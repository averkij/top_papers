{
    "paper_title": "Accelerating Streaming Video Large Language Models via Hierarchical Token Compression",
    "authors": [
        "Yiyu Wang",
        "Xuyang Liu",
        "Xiyan Gui",
        "Xinying Lin",
        "Boxue Yang",
        "Chenfei Liao",
        "Tailai Chen",
        "Linfeng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Streaming Video Large Language Models (VideoLLMs) have demonstrated impressive performance across various video understanding tasks, but they face significant challenges in real-time deployment due to the high computational cost of processing dense visual tokens from continuous video streams. In streaming video scenarios, the primary bottleneck lies in the Vision Transformer (ViT) encoding stage, where redundant processing of temporally similar frames leads to inefficiency. Additionally, inflated token sequences during LLM pre-filling further exacerbate latency and memory overhead. To address these challenges, we propose \\textbf{S}treaming \\textbf{T}oken \\textbf{C}ompression (\\textbf{STC}), a plug-and-play hierarchical framework that seamlessly integrates into existing streaming VideoLLMs, optimizing both ViT encoding and LLM pre-filling stages to accelerate processing. STC introduces two token-level accelerators: \\textbf{STC-Cacher}, which reduces ViT encoding overhead by caching and reusing features from temporally similar frames, and \\textbf{STC-Pruner}, which compresses the visual token sequence before it enters the LLM, preserving only the most salient tokens based on both spatial and temporal relevance. Extensive experiments on four baseline streaming VideoLLMs across five benchmarks demonstrate that STC outperforms other compression methods. Notably, STC retains up to \\textbf{99\\%} of accuracy on the ReKV framework while reducing ViT encoding latency and LLM pre-filling latency by \\textbf{24.5\\%} and \\textbf{45.3\\%}."
        },
        {
            "title": "Start",
            "content": "Yiyu Wang1* Xuyang Liu1,2, Xiyan Gui1,3 Xinying Lin4 Boxue Yang1 Tailai Chen1 Linfeng Zhang1(cid:12) Chenfei Liao1,5 5 2 0 2 0 3 ] . [ 1 1 9 8 0 0 . 2 1 5 2 : r 1EPIC Lab, Shanghai Jiao Tong University 2Sichuan University 3Huazhong University of Science and Technology 4Sun Yat-sen University 5Hong Kong University of Science and Technology (Guangzhou) Code: https://github.com/lern-to-write/STC"
        },
        {
            "title": "Abstract",
            "content": "Streaming Video Large Language Models (VideoLLMs) have demonstrated impressive performance across various video understanding tasks, but they face significant challenges in real-time deployment due to the high computational cost of processing dense visual tokens from continuous video streams. In streaming video scenarios, the primary bottleneck lies in the Vision Transformer (ViT) encoding stage, where redundant processing of temporally similar frames leads to inefficiency. Additionally, inflated token sequences during LLM pre-filling further exacerbate latency and memory overhead. To address these challenges, we propose Streaming Token Compression (STC), plug-and-play hierarchical framework that seamlessly integrates into existing streaming VideoLLMs, optimizing both ViT encoding and LLM pre-filling stages to accelerate processing. STC introduces two token-level accelerators: STC-Cacher, which reduces ViT encoding overhead by caching and reusing features from temporally similar frames, and STC-Pruner, which compresses the visual token sequence before it enters the LLM, preserving only the most salient tokens based on both spatial and temporal relevance. Extensive experiments on four baseline streaming VideoLLMs across five benchmarks demonstrate that STC outperforms other compression methods. Notably, STC retains up to 99% of accuracy on the ReKV framework while reducing ViT encoding latency and LLM pre-filling latency by 24.5% and 45.3%. 1. Introduction Video large language models (VideoLLMs) [8, 42, 43, 53, 54] have demonstrated strong performance across diverse video understanding tasks [14, 17, 46, 56]. Recently, emerg- *Equal contribution. Project leader. (cid:12) Corresponding author Figure 1. Inference time breakdown across components in various vision-language understanding scenarios. ViT encoding typically accounts for substantial fraction of the inference time in video understanding, about 2-3 times that in image understanding. ing applications such as live sports commentary [34] and augmented reality glasses [45] have created an urgent demand for streaming video understanding (SVU), where models must process incoming video frames continuously and generate responses with minimal latency [4, 32]. However, the computational cost of processing dense visual tokens from multi-frame inputs renders existing VideoLLMs too slow for real-time streaming scenarios, severely limiting their deployment in latency-sensitive applications [51]. To address this, two main approaches have been explored. Token compression methods reduce visual tokens either before [23, 38, 49] or within the LLM [5, 44, 47], improving both prefill and decoding efficiency. Key-Value (KV) cache compression methods [13, 21, 39] evict less important KV pairs to reduce memory overhead during decoding. However, in streaming scenarios where frames arrive continuously, the computational bottleneck lies primarily in the vision encoder (ViT [12]). Since each incoming frame must be encoded (II) Incomplete Video and Unknown Instructions: Streaming scenarios impose constraint: models cannot access complete video content in advance, nor can they know user instructions beforehand. This makes existing compression strategies ineffective. Methods relying on global video features for token selection [23, 27, 35] cannot operate efficiently, as they require the entire video for compression decisions. Instruction-aware approaches [7, 47, 55], which prune tokens based on query relevance, fail in streaming, where queries arrive only after frames are processed [50]. These challenges highlight gap: streaming video understanding demands compression methods that operate causally, exploiting temporal redundancy during encoding while adapting to incrementally arriving frames, without relying on global context or future instructions. Based on the above analysis, we propose Streaming Token Compression (STC), plug-and-play hierarchical acceleration framework that jointly optimizes the ViT encoding and LLM prefilling stages for efficient SVU. STC consists of two orthogonal modules. First, STC-Cacher reduces ViT forward passes by identifying temporally redundant frames through spatial feature similarity. It caches key frame visual features and reuses them for similar frames, avoiding redundant computations. Second, STC-Pruner compresses the visual token sequence after the vision encoder, preserving tokens salient in both spatial and temporal dimensions, and pruning redundant ones to shorten the LLM prefill sequence. Together, these modules enable low-latency video understanding while maintaining high semantic fidelity. Our contributions are four-fold: Empirical Analysis of Streaming Scenarios: We empirically analyze limitations of existing compression methods for SVU, revealing the need for causal compression that reduces ViT cost without relying on future instructions. Plug-and-Play Acceleration Framework: We propose Streaming Token Compression (STC), plug-and-play framework that seamlessly integrates into existing VideoLLMs for efficient streaming video understanding. Two Complementary Token Compressors: STC introduces two modules: STC-Cacher accelerates ViT encoding by caching features from adjacent frames, and STC-Pruner accelerates LLM prefill by pruning low-saliency tokens. Comprehensive Validation and Results: Extensive experiments demonstrate STC achieves superior performance-efficiency trade-offs, retaining 99% accuracy on ReKV while reducing LLM prefill latency by 45.3%. 2. Related Work 2.1. Streaming Video Understanding Streaming Video Understanding (SVU) requires VideoLLMs [1, 16, 42, 53, 54] to continuously process incoming video content and respond to user queries in realFigure 2. Temporal redundancy in adjacent frames in ViT encoding. Streaming videos (online) tend to show higher similarity than offline videos, indicating higher temporal redundancy. independently through ViT, the repeated ViT inference dominates overall latency before tokens even reach the LLM. Figure 1 demonstrates this phenomenon: the inference time breakdown across components reveals striking difference between image and video understanding. For models such as Qwen2-VL [41] and LLaVA-OV [16], ViT encoding in video understanding incurs 2-3 times higher computational cost compared to image understanding, making it the dominant latency contributor. Moreover, when LLaVA-OV processes 32-frame video, this results in 32 196 = 6,272 visual tokens fed into the LLM during prefilling. In comparison, image understanding tasks such as MMBench [26] typically require the LLM to process only around 1,900 visual tokens. The over three-fold increase translates directly to substantially longer latency, presenting critical barrier for real-time streaming deployment. These observations underscore that compression methods must reduce both ViT encoding overhead and context sequence length to enable efficient streaming video understanding. To better understand the unique characteristics of streaming scenarios and guide our method design, we conduct an empirical analysis comparing streaming and offline video understanding, revealing two key features in streaming videos: (I) Temporal Redundancy in ViT Encoding: Streaming videos require denser frame sampling, leading to substantial temporal redundancy. Figure 2 compares streaming (0.5fps) and offline videos (64 frames) during ViT encoding by LLaVA-OV [16]. At deeper layers (e.g., Layer 20), the mean cosine similarity of adjacent frame features in streaming reaches 0.85, compared to 0.60 in offline settings. This difference arises because consecutive frames in streaming often capture nearly identical visual content. However, existing methods fail to exploit this redundancy, as most ignore it during ViT encoding and operate only on final token representations. few methods, such as ToMe [3], perform token merging within ViT layers, but this disrupts encoding, leading to performance degradation for SVU (see Table 1, 2). time [6, 11, 20, 30]. SVU systems can generally be categorized into two main paradigms: (i) End-to-end online models trained specifically for streaming, such as Dispider [32], which decouples perception and decision-making, and StreamForest [52], which utilizes Persistent Event Memory Forest for long-term memory retention and Finegrained Spatiotemporal Window for real-time comprehension. LiveCC [6] combines streaming ASR transcripts and video frames for continuous training and real-time commentary. While these models are effective in many contexts, they come with high training costs and offer limited compatibility with offline VideoLLMs, making them less efficient for broader use cases. (ii) Offline-to-online frameworks adapt pre-trained VideoLLMs [1, 16, 41] to streaming environments without the need for retraining [29, 48, 50]. ReKV [11] pioneers this approach by maintaining framewise KV caches and employing sliding-window encoding for real-time video question answering. LiveVLM [29] and StreamChat [20] focus on preserving temporal coherence through effective memory management, enhancing the overall streaming experience. Despite their success, these approaches still process each frame through ViT encoding, feeding dense token sequences into the LLM, resulting in high computational and memory costs that limit real-time deployment. 2.2. Token Compression for VideoLLMs Token compression has emerged as promising strategy to accelerate VideoLLM inference by reducing redundant visual tokens [7, 24, 25, 35, 36, 38, 49, 55]. Existing methods can be broadly categorized into two approaches for VideoLLMs: (i) Offline compression, which assumes full access to the entire video. Methods like ToMe [2] merge similar tokens during ViT encoding, while post-encoding methods select salient tokens after feature extraction [7, 15, 49]. The former accelerates both ViT encoding and LLM prefiling, while the latter only accelerates prefiling, leaving ViT encoding inefficient. More critically, methods like SparseVLM [55], DyCoke [38], and VidCom2 [22] either rely on global visibility of the entire video or require user input, making them incompatible with causal streaming constraints. (ii) Streaming compression adapts compression for streaming settings but remains limited. LiveVLM [29] compresses the KV cache to reduce memory usage but still cannot address the high cost of ViT encoding and long sequence lengths. TimeChat-Online [51] drops tokens based on adjacent-frame similarity, but this short-horizon strategy fails to capture long-range redundancy and often retains repetitive content. In summary, prior works either overlook the causal nature of streaming or optimize only one stage. Our work is the first to jointly optimize ViT encoding and LLM prefill with streaming-native, two-stage design that respects temporal causality while maximizing redundancy removal. Figure 3. Overview of Streaming Token Compression (STC). Our framework accelerates streaming Video-LLMs in two stages. STC-Cacher employs selective recomputation to reduce computational redundancy in the ViT. STC-Pruner then reduces the token sequence to alleviate the prefilling latency for the LLM. 3. Methodology 3.1. Preliminary Streaming Inference with VideoLLMs. Most VideoLLMs, initially designed for offline video understanding (OVU) [16, 33, 37, 53, 54], can be adapted for SVU by processing video in discrete chunks. continuous video stream = {vt}T t=1 RT HW 3 is divided into segments, each corresponding to chunk Vc = {vt}(c+1)L t=cL+1, where is the chunk length. For each chunk, vision encoder (e.g., ViT) extracts visual features as token embeddings Zc = {zt}(c+1)L t=cL+1 RLN D, where is the number of patches and is the embedding dimension. and any prior textual context Xt These token embeddings Zc are projected into the LLMs embedding space for autoregressive processing. The LLM uses visual tokens Xv to predict subsequent tokens. To maintain temporal consistency under causal constraints, KV states generated from each chunk are cached in memory bank M, which serves as context for future processing. Specifically, the KV states KVc = {Kc, Vc}, where Kc and Vc represent key and value embeddings, are stored and used to condition the models future generations. This approach enables continuous video processing while respecting causal constraints, supporting real-time video understanding in streaming settings. The Redundancy Bottleneck. This sequential processing paradigm introduces computational bottlenecks. We identify two main sources of redundancy: Temporal Redundancy in ViT Encoding: In streaming video settings, adjacent frames often share similar visual Figure 4. Visualization of cache-aware selective computation by STC-Cacher. For reference frames, STC-Cacher computes and caches all tokens. For subsequent frames, only dynamic tokens are computed, while static tokens reuse cached features from reference frames. content, such as static backgrounds, leading to redundant computations in ViT encoding. Processing static information provides diminishing returns in new insights. Long Context Redundancy in LLM Prefilling: As the video progresses, redundancy accumulates in the token sequence fed to the LLM. This includes persistent temporal features (e.g., background) and low-information regions within frames. This redundancy burdens the LLM prefilling stage, characterized by the quadratic computational complexity O(N 2) of self-attention [40], and inflates the memory footprint of the KV cache without proportional gains in comprehension. These observations motivate unified compression strategy to reduce redundancy at both the vision encoding and language processing stages, while adhering to the causal constraints of streaming inference. 3.2. Streaming Token Compression To address the redundancy bottlenecks discussed above, we propose Streaming Token Compression (STC), plug-andplay acceleration framework for efficient SVU. As shown in Figure 3, STC tackles redundancy through two complementary token-level compression components: STC-Cacher combines temporal change information to cache and reuse visual features for static content, selectively computing only the temporally dynamic tokens, thus addressing temporal redundancy in ViT encoding. STC-Pruner combines causal event information to prune redundant tokens in the visual token sequence before it enters the LLM, ensuring precise token sequence compression to reduce computational cost. Both components are queryand future-agnostic, ensuring compatibility with real-time streaming constraints. Together, they reduce computational overhead while preserving causal integrity for SVU. Below, we elaborate on the Figure 5. The Mechanism of STC-Cacher. Instead of full forward pass, STC-Cacher identifies novel tokens by comparing their Key projections (Kcurr) to cached reference (Kref). It then selectively recomputes only the Query and Value representations for these dynamic tokens and scatters Value into the cached Value matrix for an efficient, low-rank update attention mechanism. detailed operations of these two components. 3.3. STC-Cacher: Selective Computation in ViT The computational cost of ViT encoding in streaming scenarios is primarily driven by the repetitive processing of temporally redundant frames. While naive strategies like token pruning or merging [3, 18] reduce computational cost, they result in significant information loss during the visual encoding process. This leads to our first research question: How can we reduce the computational cost of ViT encoding on sequential frames while preserving temporal information content from the original video stream?. To address this, we introduce STC-Cacher (Figure 5), cache-aware selective computation strategy designed to focus computation on dynamic tokens while reusing cached states for static content. The process is detailed below: (I) Reference Frame (Full Computation and Caching): For reference frame fref, we perform full forward pass, caching intermediate representations at each ViT layer to serve as reference for subsequent frames. The cached information Cl ref at layer includes: ref, Al ref = {Kl Cl ref, Vl ref, Ml ref} (1) These representations are used as reference when processing non-reference frames. (II) Non-reference Frame (Selective Computation): For subsequent frames Fnew, we leverage the cached information Cl ref to bypass most computations. Instead of full forward pass, we focus on the dynamic tokens, which contain novel information compared to the cached reference. We introduce two hyper-parameters: the cache interval and the cache reuse ratio RCacher. The cache interval determines how frequently the reference frame is updated, and RCacher controls the proportion of dynamic tokens selected. For instance, with = 4 and RCacher = 75%, every 4th frame is chosen as reference, and 25% of tokens are computed as dynamic, while the remaining 75% are reused from the cache. The process is as follows: (i) Identify Dynamic Tokens: We compute the cosine similarity between the current Key projections Kl curr,f and the cached reference Kl ref: Sf = Kl Kl curr,f Kl ref curr,f Kl ref RT (2) We select the top tokens with the lowest similarity scores as the dynamic set: If = arg top-k i{1...T } (1 Sf [i]) (3) where = r and is determined by RCacher. (ii) Selective Attention: We compute the Query and Key only for dynamic tokens indexed by If : Ql sel,f = Q(LN1(Xl ))[If ], sel,f = (LN1(Xl ))[If ] (4) We construct the full Key by initializing it from the cached reference and scattering the newly computed Key values: f Vl ref; f [If ] sel,f (5) Attention is then computed using the selective queries Ql and the full Key matrix f : sel,f Al sel,f = Attention(Ql sel,f , f , Vl curr,f ) (6) (iii) Scatter-Update Output: The attention output is updated by scattering the results into the cached attention Al ref: Al Al ref; Al [If ] Al sel,f (7) Figure 6. The Mechanism of STC-Pruner. To accelerate LLM prefilling, STC-Pruner scores each token based on its novelty. Novelty is measured as the joint dissimilarity to Temporal Context Anchor (TCA), representing historical context, and Spatial Context Anchor (SCA), representing the current frames global context. Only tokens with high novelty scores are retained. This updated tensor is passed to the MLP block, where similar process is applied. As depicted in Figure 4, by focusing computation on dynamic tokens and reusing cached values for static content, STC-Cacher significantly reduces the computational load of ViT encoding in streaming scenarios. In this way, STCCacher preserves the temporal information content while maintaining efficient processing, leading to reduced encoding time and memory usage. 3.4. STC-Pruner: Dual-Anchor Pruning for LLM Despite an accelerated ViT by our STC-Cacher, the LLM prefilling stage remains bottleneck due to the long context sequence and the quadratic complexity of self-attention. Existing token pruning methods [23, 38, 55] are often designed for offline, query-aware settings, making them unsuitable for streaming inference where decisions must be made without knowledge of the user query or future frames. This leads to our second research question: In query-agnostic and future-agnostic streaming context, what principle can serve as reliable proxy for token importance to guide token pruning?. To address this, we introduce STC-Pruner (Figure 6), dual-anchor pruning strategy to reduce context length for efficient LLM prefilling. STC-Pruner prunes tokens redundant to both the accumulated history and the current frames context. The process is below: (I) Anchor Establishment: We establish two anchors to model the past and present context in streaming videos, guiding the whole token pruning process: Temporal Context Anchor (TCA): The mean of the historical buffer, representing the aggregated context of the Methods Real-Time Visual Perception Backward Tracing Forward Active Respond. OCR ACR ATR STU FPD OJR Avg. EPM ASI HLD Avg. REC SSR CRR Avg. Overall ViT Enc. Latency LLM Pref. Latency End-to-End Online VideoLLMs with STC-Cacher Dispider(CVPR25) +STC-Cacher LiveCC(CVPR25) +STC-Cacher 49.7 44.0 60.3 43.8 59.4 48.4 51.0 47.8 54.7 50.7 39.4 55.9 41.7 58.4 48.5 49.1 47.5 53.7 5.4 5.4 36.0 16.9 40.1 45.8 35.2 17.2 36.6 45.3 68.5 39.4 62.1 43.8 68.3 59.8 57.0 58.2 59.5 89.2 69.0 30.9 56.4 72.1 61.7 36.7 59.5 43.8 67.3 53.8 53.8 56.6 56.1 87.6 66.8 29.8 54.2 70.0 StreamForest(NIPS25) +STC-Cacher 71.8 51.4 72.4 46.6 67.3 59.8 61.6 59.3 63.5 33.3 52.0 32.5 70.8 56.7 66.4 49.5 65.5 46.1 66.3 60.9 59.1 57.9 62.2 32.8 51.5 29.9 68.2 56. Offline-to-Online Framework ReKV with Token Compression Methods 73.8 56.0 74.1 51.7 70.3 60.3 64.4 54.2 57.4 28.5 46.7 25.4 64.6 53.3 ReKV(ICLR25) 61.1 49.5 52.6 42.7 62.4 50.0 53.1 49.2 50.0 29.6 42.9 19.2 60.7 50.0 +ToMe(ICLR23) 49.0 49.5 64.7 44.4 64.4 50.5 53.8 47.1 54.1 30.7 44.0 21.9 58.4 53.8 +VisionZip(CVPR25) +VidCom2 65.8 59.6 69.0 47.2 64.4 56.5 60.4 50.5 53.4 32.8 45.6 25.8 59.0 50.8 64.4 59.6 68.1 48.9 65.4 56.5 60.5 51.2 52.0 33.3 45.5 25.9 59.3 52.1 +STC-Pruner +STC-Cacher & Pruner 68.5 57.8 73.3 47.2 68.3 59.8 62.5 50.5 55.4 30.1 45.3 27.9 63.3 52.9 (EMNLP25) 34.3 33.4 53.2 51.3 53.3 51. 47.8 43.3 44.7 45.2 45.8 48.0 40.4 39.2 59.7 57.3 54.3 52.3 52.6 46.4 47.5 50.4 50.6 52.0 26.4 18.9(28.4%) 181.2 126.84(30.0%) 103.7 67.7(34.7%) 115.9 115.9 818.4 818.4 366.2 366.2 103.7 70.5(32%) 103.7 103.7 103.7 78.3 (24.5%) 482.4 257.8(46.6%) 258.3(46.5%) 259.1(46.3%) 259.2(46.3%) 263.7 (45.3%) Table 1. Comprehensive evaluation results on OVO-Bench across three categories: (i) Real-Time Visual Perception (OCR: Optical Character Recognition, ACR: Action Recognition, ATR: Attribute Recognition, STU: Spatial Understanding, FPD: Future Prediction, OJR: Object Recognition), (ii) Backward Tracing (EPM: Episodic Memory, ASI: Action Sequence Identification, HLD: Hallucination Detection), (iii)Forward Active Responding (REC: Repetition Event Count, SSR: Sequential Steps Recognition, CRR: Clues Reveal Responding). ViT Enc. Latency is the time (s) required for ViT to encode 16 frames, and LLM Pref. Latency is the time (s) required for LLM prefilling. recent past in streaming, is atemporal = 1 hH h, where = {h1, ..., ht1} is the buffer containing the mean token vectors of the past frames. (cid:80) Spatial Context Anchor (SCA): The mean of the token set from the current frame, representing its global context or background information, is given by aspatial = 1 i=1 represents the sequence of visual tokens for the current frame. i=1 zi, where = {zi}N (cid:80)N (II) Dynamics Scoring: Each token zj is scored based on its joint dissimilarity to both anchors. Using cosine distance (dcos(u, v) = 1sim(u, v)), the dynamics score S(zj) is the product of its distances to the temporal and spatial anchors. This multiplicative formulation prioritizes tokens that are distinct from both the past and the current frame: S(zj) = αdcos(zj, atemporal)+(1α)dcos(zj, aspatial) (8) where dcos is the cosine distance between the token zj and the anchors atemporal and aspatial. (III) Token Pruning: Given pruning ratio RPruner, we retain the top-k tokens with the highest novelty scores, where = (1 RPruner) is determined by the preset pruning ratio RPruner. For example, with RPruner = 25%, we prune 75% of the tokens, retaining the top 25% based on their dynamics scores. The pruned token set = {zjzj TopK(Z, k, key = S)} is then passed to the LLM. After processing, the current frames spatial context anchor aspatial is appended to the history buffer (and the oldest entry is removed) to update the context for the next time step. The token pruning process is formulated as: TopK(Z, k, key = S) (9) where the Top-k selection retains the tokens with the highest novelty scores, based on the joint dissimilarity to both the temporal and spatial anchors. In this way, STC-Pruner reduces the context length by pruning redundant tokens while preserving important temporal and spatial information. By using dual anchors, STCPruner ensures that only the most relevant tokens are retained, enabling efficient LLM prefilling and reducing the computational burden in streaming video understanding. 4. Experiments 4.1. Experimental Setup Benchmarks. We evaluate the proposed STC framework across two benchmark categories: (i) Streaming video understanding: We use OVO-Bench [31] and StreamingBench [19] to assess performance in streaming scenarios. (ii) Long video understanding: We select EgoSchema [28], MLVU-dev [56], and VideoMME [14] to evaluate effectiveness in offline long-video understanding. Implementations. We evaluate two baseline VideoLLMs: (i) End-to-End Online Models: We choose Dispider [32], LiveCC [6], and StreamForest [52], which integrate sequence compression during training. We further accelerate their ViT encoding with STC-Cacher. (ii) Offline-toOnline Frameworks: We select ReKV [10], with LLaVAOV-7B [16] as the backbone, to evaluate the impact of both STC-Cacher and STC-Pruner on ViT encoding and LLM prefill efficiency. All models follow original settings (0.5 fps protocol). Baselines. We compare our STC with token compression methods like ToMe [3] for ViT encoding, and VisionZip [49] and VidCom2 [23] for LLM prefill efficiency. For STCCacher only, we set = 4 and RCacher = 75% to accelerate Methods CS OP ATP PR ACP SU EU CT TR CR Overall End-to-End Online VideoLLMs with STC-Cacher StreamForest(NIPS25) +STC-Cacher 82.7 82.3 83.1 81. 84.3 81.6 76.9 78.7 75.6 75.6 69.1 67.9 77.5 78.8 54.4 62. 78.2 75.1 82.8 81.3 Offline-to-Online Framework ReKV with Token Compression Methods ReKV(ICLR25) +ToMe(ICLR23) +VisionZip(CVPR25) +VidCom2 +STC-Pruner +STC-Cacher & Pruner (EMNLP25) 79.2 67.5 69.5 76.0 75.4 74. 77.5 64.6 66.4 68.1 66.8 71.3 75.6 66.3 69.3 71.6 71.2 72.9 66.0 63.0 50.7 62.0 63.9 69.4 62.2 58.4 52.9 58.6 57.8 58.9 60.3 53.3 57.2 52.0 51.2 52.4 72.3 65.2 64.1 64.0 64.0 60. 43.6 19.7 33.3 42.5 45.1 44.0 69.7 57.3 54.8 60.4 63.2 66.9 79.5 76.6 73.4 76.6 76.6 77.3 77.3 76.9 69.1 59.4 60.4 63.6 63.7 65.2 ViT Enc. Latency LLM Pref. Latency 103.7 67.7(34.7%) 366.2 366.2 103.7 70.5(32.0%) 103.7 103.7 103.7 78.3(24.5%) 482.4 257.8(46.6%) 258.3(46.5%) 259.1(46.3%) 259.2(46.3%) 263.7(45.3%) Table 2. Comprehensive evaluation results on StreamingBench across real-time understanding tasks: (i) Real-Time Visual Understanding (OP: Object Perception, CR: Causal Reasoning, CS: Clips Summarization, ATP: Attribute Perception, EU: Event Understanding, TR: Text-Rich Understanding, PR: Prospective Reasoning, SU: Spatial Understanding, ACP: Action Perception, CT: Counting). Methods EgoSchema MLVU-dev VideoMME Average Short Medium Long Overall End-to-End Online VideoLLMs with STC-Cacher StreamForest(NIPS25) +STC-Cacher 60.7 59. 68.9 68.1 76.0 73.0 58.6 57.4 50.6 51.8 Offline-to-Online Framework ReKV with Token Compression Methods ReKV(ICLR25) +ToMe(ICLR23) +VisionZip(CVPR25) +VidCom2 +STC-Pruner +STC-Cacher & Pruner (EMNLP25) 57.7 55.2 55.8 60.6 60.8 59.0 68.6 63.1 63.2 67.1 67.6 67.0 70.4 59.6 59.6 68.2 68.7 67.3 55.4 52.0 51.8 55.7 56.3 53.9 47.3 43.4 43.4 46.6 46.3 48. 61.7 60.7 57.7 51.7 51.6 56.8 57.1 56.5 63.8 62.8 61.3 56.7 56.9 61.5 61.8 60.8 Table 3. Comprehensive evaluation results on three offline long video understanding benchmarks. ViT encoding. For STC-Pruner and other methods reducing sequence length, we set RPruner = 75% to compress the sequence to 25%. For STC-Cacher and STC-Pruner together (STC-Cacher & Pruner), we set = 2 and RCacher = 75%, and RPruner = 70% to compress the sequence to 30%. 4.2. Main Comparisons Table 1, 2, 3 present comprehensive comparisons of our STC framework against other compression methods in streaming and long video understanding scenarios, revealing three key advantages of our STC framework: (i) Outstanding Performance across Scenarios: Our STC framework outperforms existing methods on all benchmarks. In Table 1 and 2, compared to the previous state-of-theart method VidCom2 [23], STC improves performance by 1.6 and 1.6 on OVO-Bench [30] and StreamingBench [19], respectively, showcasing the strong performance of STC in streaming scenarios. Additionally, as shown in Table 3, STCPruner achieves state-of-the-art results across three long video understanding benchmarks, demonstrating its robust performance across diverse scenarios. (ii) Consistent Efficiency Gains: Both STC-Cacher and STC-Pruner are able to significantly improve ViT encoding and LLM prefilling efficiency. Notably, while both STCCacher and ToMe enhance ViT encoding efficiency, our method outperforms ToMe by 5.6, 5.8, and an average of 4.1 across OVO-Bench, StreamingBench, and three offline long video understanding benchmarks, demonstrating optimal performance-efficiency trade-offs. Furthermore, STC reduces ViT encoding latency and LLM prefilling latency by 24.5% and 45.3%, respectively, for ReKV on OVO-Bench. (iii) Model Compatibility: Thanks to the two complementary compressors STC-Cacher and STC-Pruner, it jointly optimizes ViT encoding and LLM prefilling stages and is plugand-play for end-to-end online models like Dispider [32], LiveCC [6], and StreamForest [52], accelerating ViT encoding. Additionally, it speeds up both ViT encoding and LLM prefilling stages for offline-to-online framework ReKV [10]. 4.3. Ablation Studies and Analysis We conduct ablation studies on STC-Cacher and STC-Pruner across three subsets of OVOBench (EPM, STU, and REC) and an offline long-video benchmark EgoSchema. Dynamic token evaluation in STC-Cacher. Figure 7 aims to explore optimal strategies for identifying dynamic tokens to determine which can be cached and reused in STC-Cacher. (i) Effects of Different Features for Dynamics Evaluation: We evaluate various features for token dynamics to idenFigure 7. Effects of Different Token Evaluation Strategies in STC-Cacher. (i) Compares various features for dynamic token evaluation to identify the optimal dynamic token set for feature caching and reuse. (ii) Further compares different metrics for token dynamics evaluation, with \"Cos Sim\" referring to cosine similarity (smaller values indicate higher dynamics), \"L1\" and \"L2\" representing L1 and L2 distances (smaller values indicate higher dynamics), and \"DP\" denoting the dot product (smaller values indicate higher dynamics), respectively. The yellow line represents the average performance gap with ToMe [3], indicating that STC-Cacher significantly outperforms ToMe. Methods OVO-Bench EPM STU REC EgoSchema Methods OVO-Bench EPM STU REC EgoSchema Attn (RCacher = 85%) MLP (RCacher = 85%) Attn and MLP (RCacher = 75%) 2.7 49.8 54.2 2.3 43.2 46. 5.3 25.3 23.4 26.2 57.1 59.0 Table 4. Effects of different reusing features in STC-Cacher. Attn and MLP are reusing features from attention and MLP. tify the optimal basis. Results show that using key-states yields the best performance, likely because they capture tokens most informative aspects, including its historical relevance and contribution to attention, making them reliable for dynamic token identification. (ii) Effects of Different Token Dynamics Metrics: We compare several metrics for token dynamics in ViT. All metrics outperform ToMe, showing that STC-Cachers feature caching-reuse strategy accelerates more gently, significantly outperforming ToMe in both streaming and offline settings. Cosine similarity consistently yields the best performance, so it is the default metric in STC-Cacher. Reusing Features across ViT Blocks. Table 4 investigates which intermediate activations should be reused during selective recomputation. Reusing only attention or MLP activations separately is insufficient: attention-only reuse leads to performance drop, while MLP-only reuse also underperforms the combined strategy. This suggests that selective recomputation must preserve both the positional/contextual information in attention pathways and the channel/semantic representations in MLP pathways. Our hybrid strategy, reusing both, achieves the right balance, keeping cached tokens informative for downstream reasoning. Dynamics Scoring for STC-Pruner. Table 5 breaks down the compression criteria of the STC-Pruner scoring mechOnly aspatial Only atemporal aspatial and atemporal 50.5 51.5 51. 47.2 47.8 48.9 25.8 24.1 25.9 59.9 59.8 59.9 Table 5. Effects on different dynamics scoring in STC-Pruner. aspatial and atemporal are using dynamics scoring by SCA and TCA. anism. Using either the SCA aspatial or TCA atemporal alone leads to unbalanced performance: the former misses interframe novelty, while the latter overlooks intra-frame redundancy. The joint scoring approach, accounting for deviation from both anchors, consistently yields the most robust results. This highlights the need for the pruner to address both intra-frame redundancy and inter-frame dynamics to retain tokens essential for complex reasoning. 5. Conclusion In this work, we propose Streaming Token Compression (STC), plug-and-play framework designed to optimize both ViT encoding and LLM pre-filling stages for real-time streaming video understanding. Through two complementary modules, STC-Cacher and STC-Pruner, STC reduces redundant computations in ViT encoding and compresses token sequences before they enter the LLM, addressing key inefficiencies in streaming video understanding. Our approach significantly enhances processing efficiency while maintaining high accuracy. STC can be easily integrated into existing streaming VideoLLMs without the need for retraining, providing practical and scalable solution for real-time deployment in latency-sensitive applications."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 2, 3 [2] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461, 2022. 3 [3] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: In Proceedings of the International Your ViT but faster. Conference on Learning Representations, 2023. 2, 4, 6, 8 [4] Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing Mao, and Mike Zheng Shou. Videollm-online: Online video large language model for streaming video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1840718418, 2024. 1 [5] Junjie Chen, Xuyang Liu, Zichen Wen, Yiyu Wang, Siteng Huang, and Honggang Chen. Variation-aware vision token dropping for faster large vision-language models. arXiv preprint arXiv:2509.01552, 2025. 1 [6] Joya Chen, Ziyun Zeng, Yiqi Lin, Wei Li, Zejun Ma, and Mike Zheng Shou. Livecc: Learning video llm with streaming speech transcription at scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2908329095, 2025. 3, 6, 7, 1 [7] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models. In Proceedings of the European Conference on Computer Vision, 2024. 2, 3 [8] Yukang Chen, Fuzhao Xue, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. Longvila: Scaling long-context visual language models for long videos. arXiv preprint arXiv:2408.10188, 2024. [9] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient In Proceedings of the exact attention with io-awareness. Advances in Neural Information Processing Systems, 2022. 2 [10] Shangzhe Di, Zhelun Yu, Guanghao Zhang, Haoyuan Li, Hao Cheng, Bolin Li, Wanggui He, Fangxun Shu, Hao Jiang, et al. Streaming video question-answering with in-context video kv-cache retrieval. In Proceedings of the International Conference on Learning Representations, 2025. 6, 7 [11] Shangzhe Di, Zhelun Yu, Guanghao Zhang, Haoyuan Li, Tao Zhong, Hao Cheng, Bolin Li, Wanggui He, Fangxun Shu, and Hao Jiang. Streaming video question-answering with in-context video kv-cache retrieval. arXiv preprint arXiv:2503.00540, 2025. 3, 2 [12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In Proceedings of the International Conference on Learning Representations, 2021. 1 [13] Yuan Feng, Junlin Lv, Yukun Cao, Xike Xie, and Kevin Zhou. Ada-kv: Optimizing kv cache eviction by adaptive budget allocation for efficient llm inference. arXiv preprint arXiv:2407.11550, 2024. 1 [14] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24108 24118, 2025. 1, [15] Yuhang Han, Xuyang Liu, Zihan Zhang, Pengxiang Ding, Donglin Wang, Honggang Chen, Qingsen Yan, and Siteng Huang. Filter, correlate, compress: Training-free token reduction for mllm acceleration. arXiv preprint arXiv:2411.17686, 2024. 3 [16] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 2, 3, 6 [17] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195 22206, 2024. 1 [18] Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song, Jue Wang, and Pengtao Xie. Not all patches are what you need: Expediting vision transformers via token reorganizations. In Proceedings of the International Conference on Learning Representations, 2022. 4 [19] Junming Lin, Zheng Fang, Chi Chen, Zihao Wan, Fuwen Luo, Peng Li, Yang Liu, and Maosong Sun. Streamingbench: Assessing the gap for mllms to achieve streaming video understanding. arXiv preprint arXiv:2411.03628, 2024. 6, 7, 1 [20] Jihao Liu, Zhiding Yu, Shiyi Lan, Shihao Wang, Rongyao Fang, Jan Kautz, Hongsheng Li, and Jose Alvare. StreamarXiv preprint chat: Chatting with streaming video. arXiv:2412.08646, 2024. [21] Xuyang Liu, Xiyan Gui, Yuchao Zhang, and Linfeng Zhang. Mixing importance with diversity: Joint optimization for kv cache compression in large vision-language models. arXiv preprint arXiv:2510.20707, 2025. 1 [22] Xiangrui Liu, Yan Shu, Zheng Liu, Ao Li, Yang Tian, and Bo Zhao. Video-xl-pro: Reconstructive token compression for extremely long video understanding. arXiv preprint arXiv:2503.18478, 2025. 3 [23] Xuyang Liu, Yiyu Wang, Junpeng Ma, and Linfeng Zhang. Video compression commander: Plug-and-play inference acceleration for video large language models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2025. 1, 2, 5, 6, 7 [24] Xuyang Liu, Ziming Wang, Junjie Chen, Yuhang Han, Yingyao Wang, Jiale Yuan, Jun Song, Linfeng Zhang, Siteng Huang, and Honggang Chen. Global compression commander: Plug-and-play inference acceleration for highresolution large vision-language models. arXiv preprint arXiv:2501.05179, 2025. 3 [25] Xuyang Liu, Zichen Wen, Shaobo Wang, Junjie Chen, Zhishan Tao, Yubo Wang, Tailai Chen, Xiangqi Jin, Chang Zou, Yiyu Wang, Chenfei Liao, Xu Zheng, Honggang Chen, Weijia Li, Xuming Hu, Conghui He, and Linfeng Zhang. Shifting ai efficiency from model-centric to data-centric compression. arXiv preprint arXiv:2505.19147, 2025. [26] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. MMBench: Is your multi-modal model an all-around player? In Proceedings of the European Conference on Computer Vision, pages 216 233, 2024. 2 [27] Junpeng Ma, Qizhe Zhang, Ming Lu, Zhibin Wang, Qiang Zhou, Jun Song, and Shanghang Zhang. Mmg-vid: Maximizing marginal gains at segment-level and token-level for efficient video llms. arXiv preprint arXiv:2508.21044, 2025. 2 [28] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very longform video language understanding. Advances in Neural Information Processing Systems, 36:4621246244, 2023. 6, 1 [29] Zhenyu Ning, Guangda Liu, Qihao Jin, Wenchao Ding, Minyi Guo, and Jieru Zhao. Livevlm: Efficient online video understanding via streaming-oriented kv cache and retrieval. arXiv preprint arXiv:2505.15269, 2025. 3 [30] Junbo Niu, Yifei Li, Ziyang Miao, Chunjiang Ge, Yuanhang Zhou, Qihao He, Xiaoyi Dong, Haodong Duan, Shuangrui Ding, Rui Qian, et al. Ovo-bench: How far is your video-llms from real-world online video understanding? In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1890218913, 2025. 3, 7, 1 [31] Junbo Niu, Yifei Li, Ziyang Miao, Chunjiang Ge, Yuanhang Zhou, Qihao He, Xiaoyi Dong, Haodong Duan, Shuangrui Ding, Rui Qian, et al. Ovo-bench: How far is your video-llms from real-world online video understanding? In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1890218913, 2025. 6 [32] Rui Qian, Shuangrui Ding, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Dahua Lin, and Jiaqi Wang. Dispider: Enabling video llms with active real-time interaction via disentangled perception, decision, and reaction. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2404524055, 2025. 1, 3, 6, [33] Minghao Qin, Xiangrui Liu, Zhengyang Liang, Yan Shu, Huaying Yuan, Juenjie Zhou, Shitao Xiao, Bo Zhao, and Zheng Liu. Video-xl-2: Towards very long-video understanding through task-aware kv sparsification. arXiv preprint arXiv:2506.19225, 2025. 3 [34] Jiayuan Rao, Haoning Wu, Hao Jiang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Towards universal soccer video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 83848394, 2025. 1 [35] Kele Shao, Keda Tao, Can Qin, Haoxuan You, Yang Sui, and Huan Wang. Holitom: Holistic token merging for fast video large language models. In Proceedings of the Advances in Neural Information Processing Systems, 2025. 2, 3 [36] Kele Shao, Keda Tao, Kejia Zhang, Sicheng Feng, Mu Cai, Yuzhang Shang, Haoxuan You, Can Qin, Yang Sui, and Huan Wang. When tokens talk too much: survey of multimodal long-context token compression across images, videos, and audios. arXiv preprint arXiv:2507.20198, 2025. 3 [37] Yan Shu, Zheng Liu, Peitian Zhang, Minghao Qin, Junjie Zhou, Zhengyang Liang, Tiejun Huang, and Bo Zhao. Videoxl: Extra-long vision language model for hour-scale video understanding. arXiv preprint arXiv:2409.14485, 2024. 3 [38] Keda Tao, Can Qin, Haoxuan You, Yang Sui, and Huan Wang. Dycoke: Dynamic compression of tokens for fast video large language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1899219001, 2025. 1, 3, [39] Keda Tao, Haoxuan You, Yang Sui, Can Qin, and Huan Wang. Plug-and-play 1. x-bit kv cache quantization for video large language models. arXiv preprint arXiv:2503.16257, 2025. 1 [40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the Advances in Neural Information Processing Systems, pages 59986008, 2017. 4 [41] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-VL: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2, 3 [42] Shihao Wang, Guo Chen, De-an Huang, Zhiqi Li, Minghan Li, Guilin Li, Jose Alvarez, Lei Zhang, and Zhiding Yu. Videoitg: Multimodal video understanding with instructed temporal grounding. arXiv preprint arXiv:2507.13353, 2025. 1, 2 [43] Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, et al. Internvideo2. 5: Empowering video mllms with long and rich context modeling. arXiv preprint arXiv:2501.12386, 2025. 1 [44] Zichen Wen, Yifeng Gao, Shaobo Wang, Junyuan Zhang, Qintong Zhang, Weijia Li, Conghui He, and Linfeng Zhang. Stop looking for important tokens in multimodal language models: Duplication matters more. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2025. 1 [45] Zichen Wen, Yiyu Wang, Chenfei Liao, Boxue Yang, Junxian Li, Weifeng Liu, Haocong He, Bolong Feng, Xuyang Liu, Yuanhuiyi Lyu, et al. Ai for service: Proactive assistance with ai glasses. arXiv preprint arXiv:2510.14359, 2025. 1 [46] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved In Proceedings of the Advideo-language understanding. vances in Neural Information Processing Systems, pages 2882828857, 2024. [47] Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu, et al. Pyramiddrop: Accelerating your large visionlanguage models via pyramid visual redundancy reduction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. 1, 2 [48] Haolin Yang, Feilong Tang, Linxiao Zhao, Xiang An, Ming Hu, Huifa Li, Xinlin Zhuang, Boqian Wang, Yifan Lu, Xiaofeng Zhang, et al. Streamagent: Towards anticipatory agents for streaming video understanding. arXiv preprint arXiv:2508.01875, 2025. 3 [49] Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li, Bei Yu, and Jiaya Jia. Visionzip: Longer is better but not necessary in vision language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. 1, 3, 6, 2 [50] Yanlai Yang, Zhuokai Zhao, Satya Narayan Shukla, Aashu Singh, Shlok Kumar Mishra, Lizhu Zhang, and Mengye Ren. Streammem: Query-agnostic kv cache memory for streaming video understanding. arXiv preprint arXiv:2508.15717, 2025. 2, 3 [51] Linli Yao, Yicheng Li, Yuancheng Wei, Lei Li, Shuhuai Ren, Yuanxin Liu, Kun Ouyang, Lean Wang, Shicheng Li, Sida Li, et al. Timechat-online: 80% visual tokens are naturally redundant in streaming videos. arXiv preprint arXiv:2504.17343, 2025. 1, 3 [52] Xiangyu Zeng, Kefan Qiu, Qingyu Zhang, Xinhao Li, Jing Wang, Jiaxin Li, Ziang Yan, Kun Tian, Meng Tian, Xinhai Zhao, Yi Wang, and Limin Wang. Streamforest: Efficient online video understanding with persistent event memory. In Proceedings of the Advances in Neural Information Processing Systems, 2025. 3, 6, 7, [53] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025. 1, 2, 3 [54] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 1, 2, 3 [55] Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, and Shanghang Zhang. SparseVLM: Visual token sparsification for efficient vision-language model inference. In Proceedings of the International Conference on Machine Learning, 2025. 2, 3, 5 [56] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. 1,"
        },
        {
            "title": "Supplementary Material",
            "content": "In this appendix, we begin by elaborating on the detailed experimental settings in Section A, which covers specific descriptions of the benchmarks, model architectures, and baseline methods employed. Subsequently, Section presents additional ablation studies, including performance comparisons on various model backbones, the impact of cache update intervals, and hyperparameter sensitivity analyses. Section and Section present the peudocode of our STC framework and more visualizations by STC-Cacher. A. Detailed Experiment Settings Benchmark Details. We evaluate our STC on various video understanding benchmarks, detailed as follows: OVO-Bench [30] is benchmark for evaluating the online video understanding capabilities of Video-LLMs, with focus on temporal awareness. It includes 644 unique videos, ranging from several minutes to half an hour in length, and features 2,814 human-curated meta-annotations with precise timestamps. The tasks are structured into three categories: Backward Tracing, Real-Time Visual Perception, and Forward Active Responding. StreamingBench [19] is comprehensive benchmark designed to evaluate the streaming video understanding capabilities of MLLMs, emphasizing the gap between offline processing and real-time human-like interaction. It consists of 900 videos covering diverse scenarios and 4,500 human-curated QA pairs, where queries are presented at specific timestamps to simulate continuous inputs. The benchmark evaluates 18 distinct tasks organized into three core categories: Real-Time Visual Understanding, OmniSource Understanding, and Contextual Understanding. EgoSchema [28] is large-scale benchmark for long-form, egocentric video question answering, containing over 250 hours of video footage. It is designed to evaluate complex causal reasoning from first-person perspective by asking why particular action was performed. The benchmark consists of multiple-choice questions that require models to infer the actors intent. VideoMME [14] comprises 900 videos and 2,700 multiple-choice questions across six domains, with durations from 11 seconds to 1 hour, categorized into short, medium, and long subsets. MLVU [56] (Massive Long Video Understanding) is benchmark for long-form video comprehension, featuring 4,000 videos that range from 30 minutes to over 2 hours, totaling 4,800 hours. It comprises 13 challenging tasks designed to test multimodal and long-context reasoning, including character-centric question answering, plot analysis, and multi-event retrieval. Model Details. We evaluate our STC on multiple Videincluding End-to-End Online VideoLLMs and oLLMs, Offline-to-Online Frameworks, detailed as follows: End-to-End Online VideoLLMs: LiveCC [6] explores the use of large-scale, cost-effective automatic speech recognition (ASR) transcripts for training Video LLMs. It proposes novel streaming training approach that densely interleaves ASR words with video frames according to their timestamps, enabling the model to learn fine-grained, temporally-aligned vision-language modeling. To support this method, the work introduces two new datasets: Live-CC-5M for pre-training and Live-WhisperX-526K for supervised fine-tuning. This approach results in strong general video question answering (QA) performance while also exhibiting novel capability for real-time video commentary. StreamForest [52] is novel architecture specifically designed for efficient online video understanding in streaming scenarios. Unlike conventional Video-LLMs that either process entire videos offline or apply aggressive compression losing spatiotemporal details, StreamForest continuously analyzes video streams at 1 fps through dualmemory mechanism. Central to the design is the Persistent Event Memory Forest (PEMF), which adaptively organizes video frames into hierarchical event-level tree structures guided by three penalty functions (temporal distance, content similarity, and merge frequency), enabling persistent long-term memory under strict token budgets. Complementing this, Fine-grained Spatiotemporal Window (FSTW) captures detailed short-term visual cues for enhanced real-time perception. The work also introduces OnlineIT, an instruction-tuning dataset tailored for streaming video tasks, and ODV-Bench, benchmark for autonomous driving scenarios. Dispider [32] is novel framework that enables active realtime interaction with Video Large Language Models by disentangling perception, decision, and reaction into asynchronous modules. Unlike previous streaming methods that alternate between video processing and response generation, Dispider features lightweight scene-based perception module that continuously monitors video streams and dynamically segments them into non-uniform clips based on visual boundaries. real-time decision module evaluates whether to trigger responses using special tokens (<TODO> and <ANS>) and historical context, while an asynchronous reaction module generates detailed responses without interrupting ongoing video processing. This non-blocking architecture ensures timely, contextually accurate responses for long-duration videos while maintaining computational efficiency. Dispider significantly outperforms existing streaming models on StreamingBench and demonstrates strong performance on conventional long-video benchmarks. Offline-to-Online Frameworks: ReKV [11] is novel, training-free framework designed to enable existing Video Large Language Models (Video-LLMs) with efficient Streaming Video QuestionAnswering (StreamingVQA) capabilities. Unlike traditional VideoQA systems that must process an entire video before responding to query, ReKV continuously analyzes video streams in streaming manner, allowing for prompt responses. During the encoding phase, the framework employs sliding-window attention mechanism to reduce computational overhead, while simultaneously storing processed video KV Cache in memory to prevent information loss. When query is posed, ReKV utilizes retrieval module to load only the query-relevant KV-Caches to serve as context, enabling efficient answer generation. This design decouples the video encoding and question-answering processes, significantly enhancing efficiency and responsiveness, particularly when handling long videos. Baseline Details. We compare our STC with below dominant token compression methods: ToMe [3] is token reduction method that improves Vision Transformer (ViT) throughput by gradually merging similar tokens across transformer layers, rather than pruning them. Unlike pruning-based approaches, ToMe can be applied off-the-shelf without retraining while maintaining compatibility with batched inference. VidCom2 [23] proposes plug-and-play inference acceleration framework centered on frame uniqueness. Unlike methods using uniform compression, it employs twostage strategy: first dynamically adjusting compression intensity based on the distinctiveness of each video frame, and then performing adaptive token compression. Notably, it addresses the implementation constraints of previous methods by maintaining full compatibility with efficient operators like Flash Attention [9]. VisionZip [49] introduces text-agnostic token reduction framework applied before the LLM input. It identifies informative dominant tokens based on the self-attention weights within the vision encoder and aggregates the remaining redundant tokens through similarity-based merging to preserve contextual details. B. Additional Ablation Studies In this section, we present further quantitative analysis of our proposed method. Results on other models. We first Methods OVO-Bench EPM STU REC Attn (RCacher = 85%) MLP (RCacher = 85%) Attn and MLP (RCacher = 75%) 30.6 56.9 57.9 33.9 43.3 46.1 11.2 27.5 29.9 Avg 25.2 42.6 44. Table 6. Effects of different reusing features in STC-Cacher with StreamForest. Attn and MLP are reusing features from attention and MLP. Methods Feature Value Key EPM 56.9 58.3 57.9 OVO-Bench STU 45.5 44.9 46.1 REC 28.9 29.8 29.9 Avg 43.8 44.3 44. Table 7. Compares various features for dynamic token evaluation to identify the optimal dynamic token set for feature caching and reuse. We compare the performance of reusing Feature, Value, and Key. Cache Interval = 1 = 4 = 7 = 10 = OVO-Bench EPM STU REC 54.6 52.2 51.9 50.8 44.1 51.1 42.7 44.4 46.1 38.2 25.5 24.9 23.8 22.9 18.3 Avg 43.7 39.9 40.0 39.9 33. Table 8. Ablation study on the cache update interval . We compare different update frequencies ranging from frame-by-frame updates (N = 1) to no updates (N = ). Results indicate that more frequent updates consistently yield better performance. present the performance comparison on additional model architectures in Table 6 and Table 7. B.1. Impact of Cache Update Interval Table 8 investigates sensitivity to cache update interval . We vary from frame-by-frame to static. Results show that more frequent updates yield better performance. Performance drops sharply as the interval becomes static, revealing feature drift. This confirms periodic updates are essential for tracking temporal dynamics. B.2. Hyperparameter Sensitivity Analysis B.2.1. Token Retention Ratios Table 9 analyzes the impact of token retention ratios in the Cacher and Pruner modules. Impact of Cacher Ratio (RCacher): As shown in Table 9 (a), higher update ratio generally yields superior performance, as refreshing more tokens preserves dynamic information against rapid changes. Impact of Joint Ratios: Table 9 (b) shows the interplay between Cacher and Pruner ratios. Performance remains Ablation Settings (a) Impact of Cacher Ratio RCacher = 85% RCacher = 75% RCacher = 50% (b) Impact of Joint Ratios RCacher = 50%, RPruner = 75% RCacher = 75%, RPruner = 50% RCacher = 75%, RPruner = 75% OVO-Bench EPM STU REC Avg 54.9 52.2 50.8 51.5 50.8 50.8 45.5 42.7 46.6 46.1 46.2 46.1 24.5 24.9 25.4 25.6 26.9 24. 41.6 39.9 40.9 41.1 41.3 40.5 Table 9. Ablation studies on Cacher and Pruner hyperparameters. We analyze (a) the impact of the Cacher update ratio RCacher, and (b) the joint effect of RCacher and the Prun ratio RPruner. Metrics atemporal +aspatial atemporal +2aspatial 2atemporal +aspatial EPM 51.2 51.1 50.1 OVO-Bench STU 48.9 48.8 47.7 REC 25.9 26.0 25. Avg 42.0 42.0 41.6 Table 10. Effects of balancing hyper-parameter α between atemporal and aspatial. robust across combinations, demonstrating that our framework does not rely on narrow hyperparameter tuning. B.2.2. Spatial-Temporal Balance Finally, we investigate the hyperparameters used to combine spatial (aspatial) and temporal (atemporal) scores in Table 10. Results show that performance is optimal when scores are balanced or slightly favor the spatial term. Heavily up-weighting the temporal score leads to degradation, implying that while temporal dynamics are useful, spatial semantics remain the fundamental basis for reasoning. C. Algorithm Pseudocode In this section, we provide the detailed pseudocode for the two stages of our Streaming Token Compression (STC) framework. Algorithm 1 details the cache-aware selective computation in the ViT (STC-Cacher), and Algorithm 2 describes the dual-anchor pruning mechanism for the LLM input (STC-Pruner). D. More Visualizations by STC-Cacher Figure 8 presents more visualization results by STC-Cacher, which intuitively demonstrates that STC-Cacher can concentrate computational overhead on temporally significant regions while compressing computation for visual tokens that remain static across temporal dimensions. Algorithm 1 STC-Cacher (ViT Acceleration) Require: Video frame sequence = {vt}T t=1, Cache Int=1 if (mod )N == 1 or == 1 then terval , Cache Reuse Ratio RCacher [0, 1) Ensure: Sequence of visual tokens = {Zt}T 1: Initialize output list 2: for = 1 do 3: 4: 5: 6: // (I) Reference Frame: Full Computation Perform full forward pass on vt Cache {Kl Zt Output from final layer Cl ref ref} for all layers representations: ref, Ml ref, Vl ref, Al 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: else // (II) Non-reference Frame: Selective Computation for each layer do // 1. Identify Dynamic Tokens Sf CosSim(Kl If arg top-k curr, Kl ref) (1 Sf [i]) where = Ntok sel, sel (1 RCacher) // 2. Selective Attention with Scatter Update Compute Query/Value for If : Ql V [If ] Vl ref; sel sel, Al sel Attention(Ql ) Al [If ] Al Al ref; sel // 3. Selective MLP with Scatter Update M end for Zt Output from final layer f sel MLP(LN( Al Ml ref; [If ])) [If ] sel Al end if Append Zt to 23: 24: 25: 26: end for 27: return Figure 8. More visualization of cache-aware selective computation by STC-Cacher. Algorithm 2 STC-Pruner (LLM Input Compression) Require: Visual tokens Zt (from STC-Cacher), History Buffer H, Pruning Ratio RPruner [0, 1), Balance Factor α Ensure: Pruned tokens t, Updated Buffer (Historical Context) (Current Frame Context) (cid:80) (cid:80) 1: // (I) Anchor Establishment 2: atemporal 1 hH 3: aspatial 1 Zt 4: // (II) Dynamics Scoring 5: for each token zj Zt do 6: 7: zZt Calculate joint dissimilarity: S(zj) α dcos(zj, atemporal) + (1 α) dcos(zj, aspatial) 8: end for 9: // (III) Token Pruning 10: Calculate retention count: Zt (1 RPruner) 11: 12: // (IV) Context Update 13: Enqueue(H, aspatial) TopK(Zt, k, key = S) (Update history for next step) 14: return t"
        }
    ],
    "affiliations": [
        "EPIC Lab, Shanghai Jiao Tong University",
        "Hong Kong University of Science and Technology (Guangzhou)",
        "Huazhong University of Science and Technology",
        "Sichuan University",
        "Sun Yat-sen University"
    ]
}