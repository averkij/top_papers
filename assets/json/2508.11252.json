{
    "paper_title": "Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information",
    "authors": [
        "Youcheng Huang",
        "Bowen Qin",
        "Chen Huang",
        "Duanyu Feng",
        "Xi Yang",
        "Wenqiang Lei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Reasoning Models (LRMs) have demonstrated remarkable problem-solving abilities in mathematics, as evaluated by existing benchmarks exclusively on well-defined problems. However, such evaluation setup constitutes a critical gap, since a genuine intelligent agent should not only solve problems (as a math quiz solver), but also be able~to ask for information when the problems lack sufficient information, enabling proactivity in responding users' requests. To bridge such gap, we proposes a new dataset consisting of two types of incomplete problems with diverse contexts. Based on the dataset, our systematical evaluation of LRMs reveals their inability in proactively asking for information. In addition, we uncover the behaviors related to overthinking and hallucination of LRMs, and highlight the potential and challenges of supervised fine-tuning in learning such ability. We hope to provide new insights in developing LRMs with genuine intelligence, rather than just solving problems."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 2 5 2 1 1 . 8 0 5 2 : r Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information Youcheng Huang12 Bowen Qin3 Chen Huang124 Duanyu Feng12 Xi Yang3 Wenqiang Lei12 Jiancheng Lv12 1Sichuan University, China 2Engineering Research Center of Machine Learning and Industry Intelligence, Ministry of Education, China 3Beijing Academy of Artificial Intelligence 4Institute of Data Science, National University of Singapore, Singapore bwqin@baai.ac.cn youchenghuang@stu.scu.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Large Reasoning Models (LRMs) have demonstrated remarkable problem-solving abilities in mathematics, as evaluated by existing benchmarks exclusively on welldefined problems. However, such evaluation setup constitutes critical gap, since genuine intelligent agent should not only solve problems (as math quiz solver), but also be able to ask for information when the problems lack sufficient information, enabling proactivity in responding users requests. To bridge such gap, we proposes new dataset consisting of two types of incomplete problems with diverse contexts. Based on the dataset, our systematical evaluation of LRMs reveals their inability in proactively asking for information. In addition, we uncover the behaviors related to overthinking and hallucination of LRMs, and highlight the potential and challenges of supervised fine-tuning in learning such ability. We hope to provide new insights in developing LRMs with genuine intelligence, rather than just solving problems."
        },
        {
            "title": "Introduction",
            "content": "Reasoning, which can be generally defined as sequences of steps to draw conclusions from premises [1, 2], poses fundamental challenge to artificial intelligence (AI) [35]. Recently, Large Reasoning Models (LRMs) [6, 7] represent remarkable mathematical reasoning abilities in solving competitive problems [8], bolstering the conviction regarding the imminent arrival of genuine AI models. However, the creation of genuine AI should not be exclusively evaluated by solving well-defined mathematical problems in existing benchmarks [811]. From the standpoint of AI, John McCarthy said that an agent is intelligent if it can get additional information from the external world when required Corresponding authors. Email: wenqianglei@scu.edu.cn Preprint. Under review. Figure 1: On an incomplete problem (i.e., lacking definition of \"distance\"), solving the problem exhibits overthinking, manifesting as protracted thinking (measured by thoughts string lengths), while asking for information reduces thinking efforts. Beyond this example, we provide systematical evaluation of: to what extend can LRMs ask for information, why they fail, and how to improve them. [3]. In addition, real-world problems are inherently characterized by incompleteness that fails to provide all necessary information [1214]. Such incompleteness makes proactive information-seeking critical for AI assistants to provide truely helpful responses. For example, user may ask My living room is 6 meters long. How many tiles (with side length of 60 centimeter) do need in total?, and any specific answer based on AI-made assumptions is meaningless as we do not know the width of the users living room and need ask for this premise.2 Therefore, being genuine intelligence in mathematics, LRMs should not only solve problems, but also acquire the ability to ask for information on incomplete problems. Otherwise, an impractical math quiz solver is what we will ultimately develop. Unfortunately, the ad-hoc example in Figure 1 (left) alerts us current LRMs being far from genuinely intelligent. Confronted with the problem (proposed by [15]) that misses precise definition, LRMs overthink to infer an answer (which is critically impossible because of incomplete premises), resulting in significant thinking efforts and delay in responding users requests. This example reveals the limitations of current LRMs in proactively identifying incomplete problems and asking for information. Beyond this example, to what extend can LRMs ask for information on incomplete problems? Why LRMs fail to ask questions? And can we train LRMs to know how to ask for information? These questions remain to be systematically evaluated and play vital roles in developing AI models. Driven by this concern, in this paper, we introduce CRITIC-math (CRItical ThInking of Completeness on mathematical problems, Figure 1 right) to provide systematical evaluation. CRITIC-math contains two types of incomplete problems through rewriting well-defined problems in open-source datasets [911]. In total, CRITIC-math generates 1.3K test and 5.3K training data, and undergoes manual verification to ensure its quality. Based on CRITIC-math, we reveal the inability of current LRMs in asking for information, and uncover the underlying causes regarding overthinking and hallucination within LRMs thinking process. We also demonstrate the potential of supervised fine-tuning (SFT) [1618] in learning such ability. In addition, during SFT, we find dilemma between problem-solving and asking for information, where the current mode of deep-thinking could weaken the ability to ask for information. These results suggest that the existing approach of developing LRMs is biased to only solve math quiz, overlooking the other aspect of intelligence of proactive information-seeking. 3 In summary, this work makes the following three-fold contributions and offer new insights to develop genuine AI in mathematics that can not only solve problems but also ask for information when needed: Propose new dataset consisting of two types of incompleteness and large-scale problems. Based on the dataset, we provide systematical evaluation of state-of-the-art LRMs, uncovering their inability to ask for information and understand how LRMs behave when they fail to ask questions. Demonstrate the potential and challenges (a dilemma between the current mode of deep-thinking in solving problems and asking for information) in training LRMs to learn to ask for information. 2Responses of different LRMs are presented in Appendix C. 3CRITIC-math is open-sourced at: https://huggingface.co/datasets/YouchengHuang/ CRITIC-math, and the SFT data is at https://huggingface.co/datasets/YouchengHuang/ CRITIC-math-sft. Both open-source datasets are under the Apache 2.0 License. 2 Figure 2: Construction pipeline of CRITIC-math. We rewrite well-defined problems in open-source datasets to incomplete ones in 3 steps: Decomposing, Incomplete Problem Constructing, Verifying."
        },
        {
            "title": "2 CRITIC-math",
            "content": "To provide systematical analysis, we introduce CRITIC-math, new benchmark consisting of two categories of incomplete problems: missing goal and missing premises. We construct CRITIC-math by transforming well-defined problems from open-source datasets into incomplete ones, followed by manual verification to ensure quality. Table 1 and 2 provide illustrative examples and data statistics. 2.1 Types of Incomplete Mathematical Problems that Need Clarification Reasoning involves sequence of steps to derive conclusions from premises. Following common practice, we focuse on problems whose goals are assigning value to target variable [9, 10, 14, 19]. Formally, we define reasoning as an assignment function A(yp1, p2, ..., pn), where is the target variable and are premises. We say problem is information-complete if is well-defined and is injective, i.e., only one specific value can be assigned given the premises. Based on this definition, as displayed in Table 1, we introduce the following two types of incomplete mathematical problems: Missing Goal. When the goal is missing, we cannot know the target space of A. Taking the example from Table 1, possible goals can be how many minutes it takes to cover 1 mile? or how many feet can be covered in 1 hour? Different goals have different values to assign. Missing Premise(s). When premises are missing, none or multiple values can be assigned to y. Taking the example from Table 1, the bicycle speeds can vary in [1, 10, 20, ] feet, different premises can result in different conclusions, and no specific value can be assigned. Table 1: Two types of incomplete problems and corresponding example. Well-defined Problem from MATH 500 [9]: bicycle is traveling at 20 feet per minute. (Sufficient Premises) What is the bicycles speed expressed in inches per second? (Clear Goal) Type of Incompleteness Incomplete Mathematical Problems Missing Goal bicycle is traveling at 20 feet per minute. Missing Premises bicycle is traveling. What is the bicycles speed expressed in inches per second? 2.2 Construction of Incomplete Mathematical Problems Overview. CRITIC-math is constructed by rewriting the well-defined problems from open-source datasets. This construction involves three key stages: I) Parsing each problem into its constituent elements (i.e., goal, premises, and Background); II) Disturbing elements and constructing incomplete problems by reassembling these disturbed elements; and III) Verifying the resulting incompleteness. Each stage is elaborated in the following sections, and the overall pipeline is illustrated in Figure 2. Data Sources. For our test set, we adopt problems from test sets of Omni-MATH [19] and MATH 500 [9]. For training set, we use problems from OpenR1-Math [11]. Given the challenging nature of Omni-MATH, where OpenAIs o1-mini [6] achieves an accuracy of 60% (compared to its 90% 3 accuracy on MATH 500 [7]), we limited our selection to Omni-MATH problems with difficulty level of 4 or lower, ensuring that the problems are rather easy for AI models to understand.4 Stage Parsing Problems into Goal and Premises. We prompt Deepseek R15 to decompose each problem from the Data Sources into three elements: Goal, Premises, and Background. The Goal represents the result that the problem asks for. Premises are essential information required to achieve the Goal. Background encompasses any remaining information, such as descriptive text and unnecessary examples. All prompts used in the data construction are provided in Appendix D.1. Stage II Constructing Incomplete Problems. We create incompleteness by introducing disturbances to each parsed problem. \"Missing Goal\" are created by replacing the original goal with an empty string. \"Missing Premises\" are generated by randomly removing one entry from the original premises list; we create up to two variants if the original list contains multiple entries. Each incomplete problem undergoes one type of disturbance (either \"Missing Goal\" or \"Missing Premises\"). After disturbances, we prompt DeepSeek R1 to reassemble modified elements and untouched elements into free-form problems. The reassembly is designed to closely mimic real-world scenarios where elements are not explicitly separated in problems. During this process, Deepseek R1 also compares the incomplete problem to its original counterpart and generates the reason for the incompleteness. Stage III Verifying Incomplete Problems. We involve multi-stage verification process. Specifically, we first prompt DeepSeek R1 to analyze the incomplete problems alongside their reasons for incompleteness (generated in Stage II). If Deepseek R1 judges problem to be genuinely incomplete, we request it to provide at least two different understanding and possible solutions of the problem. Then, human evaluators provide final confirmation by reviewing sampled rewritten problems and their AIs judgments. Details about human evaluation and results are presented in Appendix D.2. Source 243 266 234 254 380 MP MG CRITIC-math Test MATH 500 Omni-Math Table 2: Dataset Statistics of CRITIC-math. MP is \"Missing Premises\", MG is \"Missing Goal\", and Raw is well-defined problems. Raw Data Format, Statistics, and Features of CRITICmath. Each sample in CRITIC-math is tuple (p, s, r, ld), where: represents the incomplete problem; represents the solution to the original problem where is rewritten; denotes the reason of incompleteness; and ld indicates the difficulty of the original problem (ld is not applicable in training set, as OpenR1-Math dataset lacks this rating). In addition to the incomplete problems, CRITIC-math also includes the original problems, structured as (p, s, None, ld), where \"None\" indicates the absence of incompleteness. Table 2 presents the dataset statistics for CRITIC-math, showing total of 1311 incomplete problems in the test set and 5350 in the training set6. For problems difficulty, MATH 500 provides five levels (1 to 5), with the corresponding data ratios for \"Missing Premises\" (MP) being [10.3%, 17.6%, 16.8%, 25.5%, 29.7%] and for \"Missing Goal\" (MG) being [9.8%, 17.1%, 17.9%, 25.6%, 29.5%]. Omni-Math provides fine-grained levels [1, 1.5, 2, 2.25, 2.5, 3, 3.5, 4], with some levels have little samples. To ensure sufficient samples per level, we organize Omni-Math levels as follows: (1, 1.5) to 1, (2, 2.25) to 2, (2.5) to 3, (3, 3.5) to 4, and (4) to 5. This results in data ratios: MP [24.2%, 16.3%, 10.6%, 10.8%, 38.1%] and MG [25.2%, 15.4%, 11.8%, 11.42%, 36.2%]. In summary, CRITIC-math features two types of incompleteness, more challenging and diverse mathematical problems, realistic task formulation (i.e., free-form problems), and training set that allows us to examine how fine-tuning affects the ability of asking for information. CRITIC-math Training OpenR1-math 2153"
        },
        {
            "title": "3 Evaluate Asking for Information based on CRITIC-math",
            "content": "This section evaluates the ability of LRMs to ask for information when confronting the incomplete mathematical problems. Furthermore, recognizing that Supervised Fine-Tuning (SFT) is widely used to adapt pre-trained Large Language Models (LLMs) for specific tasks [1618, 2022], especially trained with the thinking process [17] of powerful LRMs, we investigate two research questions (RQs): 4MATH 500 is under MIT License, and Omni-MATH and OpenR1-Math are under Apache 2.0 License. 5We select Deepseek R1 for its high performance and suitability for our budget 6The training problems have been filtered for sampling responses to construct SFT data, cf. Section 3.2.1. 4 RQ1 (Evaluate LRMs): To what extend can existing LRMs identify incomplete mathematical problems and raise questions to ask for information? RQ2 (Evaluate SFT Effectiveness): Can SFT effectively train LLMs to acquire the ability of asking for information? Furthermore, does incorporating the deep thinking from powerful LRMs during SFT enhance LLMs ability to ask for information? 3.1 Evaluate LRMs (R1) 3.1.1 Evaluation Setups To conduct comprehensive evaluation, we introduce two levels of analysis: coarse level, focused on assessing LRMs overall performance, and fine-grained level, designed to understand the thinking characteristics of LRMs and reveal underlying phenomena that why LRMs fail to ask for information. Prompt Setups: To examine how LRMs raise clarification questions, we prompt them using two prompts: \"Implicit prompt\" requires greater proactivity from LRMs in raising questions, and are more closely resemble realistic scenarios where only the problem is provided. In contrast, \"explicit prompt\" explicitly instructs LRMs to ask for information if needed. The prompts are detailed below. Implicit Prompt: Explicit Prompt: {problem} # Task to Solve {query} # Instruction If you need ask for information, please raise clarification question and start your response STRICTLY with: \"Clarification Question\" followed by your questions. Otherwise, please provide your answer in boxed{}. LRM Models: We select Deepseek-R1 [7], Qwen3-plus with thinking enabled [23], Claude 3.7 with thinking enabled [24], OpenAI o3-mini [6], and Grok-3-mini-beta [25]. These models represent the state-of-the-art LRMs developed by various institutions, renowned for their advanced reasoning skills. Evaluation Metrics: For the coarse level evaluation of the overall performance regarding raising questions to ask for information and the corresponding thinking efforts, we consider the following metrics, with all evaluations are conducted on both well-defined and incomplete problems. Clarification Ratio (CR) is the percentage of responses that raise questions. For implicit prompts, we utilized LLM-as-a-Judge [26] (Deepseek R1) to determine whether response raised questions. For \"explicit prompt\", since it requests raising questions by starting with clarification, we simply check for the presence of the string \"clarification\". Thoughts Lengths in Clarifying (TLC), and Thoughts Lengths in No Clarifying (TLNC) are average thoughts lengths generated by LRMs when raising and not raising questions. Clarification Accuracy (ACC) is the overal percentage of responses that raise clarification questions on incomplete problems and not raise clarification questions on well-defined problems. Fine-grained level evaluates LRMs thoughts when fail to ask for information7. We split thoughts to steps by nn [27] and use the following metrics. ROR and CNR are evaluated by LLM-as-a-Judge8. Reflection Step (RS) is the number of steps that contains reflection keywords. Following [28], we select reflection keywords to include alternative, wait, but, and check. Reflection on Incompleteness Ratio (ROR) is the ratio of thoughts whose Reflection Steps specifically aimed at addressing the incompleteness of the problem. In specific, this includes making assumptions about the missing premises and imagining goal when the goal is missing. Clarification Noticing Ratio (CNR) is the ratio of thoughts that identified the need for clarification. 7This is only applicable on Deepseek-R1 and Qwen3 as other APIs do not return the thoughts contents. 8For longer thoughts, only the first ten steps are evaluated 5 Implementation Details. All tested LRMs are accessed via their respective APIs. To ensure deterministic output, we set the temperature to 0 and generate single response per problem. Appendix explores experimental results with temperature greater than 0 and multiple responses. Our analysis demonstrates there is no significant differences between sampling one and multiple responses. In addition, Appendix details the implementation of our LLM-as-a-Judges. Finally, when calculating the thoughts length, we adopt different methods depending on the return format of different APIs. For Deepseek R1, we measure the string lengths of the thoughts returned by APIs. For other LRMs, we directly use the number of \"reasoning tokens\" reported in the APIs usage data. 3.1.2 Results & Discussion Table 3: Coarse-grained assessment of overall performance. For readability, TLCs/TLNCs are reported as the multiples of the TLNC on well-defined problems, with the absolute lengths shown in parentheses. Multiples greater than 1 are marked in red; otherwise, in green. Results obtained using \"implicit prompt\" are indicated with gray lines ; otherwise, with white. In Section 3.1.1 we present detail definitions of evaluation metrics: briefly, CR denotes clarification ratio, TLC denotes thoughts lengths when clarifying, TLNC denotes thoughts lengths when not clarifying, and ACC is accuracy. Models Missing Premises Missing Goal Well-Defined CR TLC TLNC CR TLC TLNC CR TLC TLNC Deepseek R1 Qwen3 Plus o3 mini Grok 3 mini Claude 3.7 Deepseek R1 Qwen3 Plus o3 mini Grok 3 mini Claude 3. 48.68% 0.61 21.05% 2.35 51.58% 0.40 21.58% 1.81 50.70% 1.00 27.37% 1.43 55.70% 0.49 34.21% 1.14 53.42% 0.42 33.68% 0.66 44.02% 0.34 18.74% 1. 46.73% 0.30 27.65% 1.47 48.31% 0.63 20.32% 0.86 53.05% 0.36 31.83% 1.00 50.34% 0.31 23.48% 1.34 CRITIC-math (from Math 500 Data Source) 2.11 2. 1.78 1.49 2.07 2.63 1.52 1.27 2.07 1.42 48.72% 0.49 1.73 4.27% 58.12% 0.41 35.48% 1. 35.04% 1.34 19.23% 0.93 48.72% 0.48 15.81% 0.68 44.87% 0.16 11.97% 0.56 1.05 1.34 0.99 1.07 1.09 1. 0.97 1.06 0.93 0.94 1.23% 3.08 0.41% 4.24 0.00% nan 0.00% nan 0.41% 5.90 0.00% nan 0.00% nan 0.00% nan 2.47% 0.02 0.41% 6.01 1 (6387) 1 (7973) 1 (3578) 1 (4087) 1 (694) 1 (738) 1 (1525) 1 (1587) 1 (4818) 1 (3882) CRITIC-math (from Omni-Math Data Source) 1.27 1.41 1.34 1.15 1.37 1.65 1.18 1.07 1.89 1. 53.54% 0.27 1.70 3.15% 57.87% 0.24 1.98 9.45% 40.16% 0.45 20.87% 0.33 55.38% 0.38 20.47% 0.52 46.46% 0.11 14.57% 0.58 0.83 1. 1.01 0.95 0.68 0.88 0.89 0.90 1.10 1.26 3.76% 0.43 3.38% 2.53 1 (14177) 1 (15583) 5.64% 0.54 4.14% 1.54 6.39% 0.38 2.63% 0.50 8.27% 0.37 3.38% 1.55 7.89% 0.02 2.26% 2.64 1 (6284) 1 (6740) 1 (2085) 1 (2143) 1 (2433) 1 (2379) 1 (7094) 1 (4642) ACC 62.89% 38.74% 67.09% 39.91% 60.33% 45.74% 66.39% 47.84% 63.59% 46.44% 60.96% 36.14% 62.82% 37.80% 58.67% 41.74% 64.28% 46.73% 60.85% 41.64% Table 3 presents the results of our coarse-grained evaluation, while Table 4 details the fine-grained evaluation results. An integrated analysis on both tables yields the following observations: In overall speaking, LRMs lack the ability to Proactively Ask for Information. In Table 3, LRMs achieve significantly low clarification ratios (CRs, around 25%) and accuracies (ACCs, around 40%) when prompted with only problems (\"implicit prompt\"), and explicitly instructing LRMs to ask for information (\"explicit prompt\") can improve CRs from 25% to 50% and ACCs from 40% to 65%. These results indicate that LRMs lack the ability to proactively ask for information. In most cases, LRMs act as math quiz solvers to solve problems regardless that whether the problem is complete or not. Even given \"explicit prompt\", CRs are around 50%, indicating that LRMs still struggle to effectively ask for information. These phenomena are even more pronounced on difficult problems. As detailed in Appendix F.1.1, we observe negative correlation between problem difficulty and CRs: CRs decrease on harder problems, indicating that the ability of asking for information is unrobust to 6 Table 4: Fine-grained analysis of LRMs when fail to ask for information. We report RS on welldefined problems for comparison. Results obtained using \"implicit prompt\" are indicated in gray ; otherwise, in white. We average results of two data sources considering limited pages. RS denotes reflection step, ROR denotes reflection on incompleteness, and CNR is clarification noticing ratio. Missing Premises Missing Goal Well-Defined Model RS ROR CNR RS ROR CNR Deepseek R1 Qwen3 Plus 36.58 50. 79.79 84.51 74.04% 21.22% 17.21 77.27% 24.33% 29.55 97.68% 3.78% 94.77% 1.06% 76.43% 18.57% 50.37 81.40% 28.35% 59.28 98.45% 4.88% 94.60% 3.80% RS 23.39 26.41 47.79 87.39 difficult problems. Unexpectedly, in Appendix F.1.1, we find that when LRMs ask for information, their questions are rather accurate that target at the incompleteness. For such, we hypothesize that LRMs actually lack the ability to ask rather than to notice the incompleteness. LRMs tend to solve problem even know it is incomplete. Our fine-grained analysis further support this hypothesis. Fine-grained Evaluation assesses LRMs behavior modes when LRMs fail to ask for information. In total, we identify two major failure modes (thoughts-to-answer unfaithfulness and overthinking) on missing premises problems and one dominate failure mode (hallucination) on missing goal problems. Thoughts-to-Answer Unfaithfulness When Missing Premises. CNR is the ratio of thoughts that have recognized the need to ask. The results in Table 4 suggest that, confronted with missing premises problems, in approximately 20% of failure cases, LRMs recognize the need to ask for information, but ultimately fail to do so. We provide case for illustration below, where the thought recognizes the necessity (\"the best approach\") and provides candidate question (\"Are there any specific ...\"). However, the LRM fails to follow its thought, instead generating answer. This exemplifies type of thoughts-to-answer unfaithfulness in LRMs, and further support our hypothesis that LRMs know the incompleteness but tend to be math quiz solvers rather than be proactive to ask for information. Thoughts: If dont ask, might give the wrong answer. So the best approach here is to request more details. Clarification Question: Are there any specific conditions given for Answers: To determine **Final Answer**: If the equation has equal roots, then 2 . Overthinking when Missing Premises. As Table 3 shows, confronted with missing premises problems, TLNCs (the thoughts lengths in no clarifying) significantly increase. The results in Table 4 further reveal that when LRMs fail to ask for information, their extended thoughts exhibit higher RSs (reflection steps). These thoughts, as measured by RORs (Reflection on Incompleteness Ratio), contains reflection steps that focus on the incompleteness in roughly 75% cases. In together, if problems lack premises, LRMs tend to address such deficiency through their internal thinking, failing to ask for information, leading to significant delays. In contrast, TLCs (thoughts lengths in clarifying) are much shorter, demonstrating the efficiency of asking questions. In addition, we find the overthinking to be more pronounced on easy problems (detailed in Appendix F), aligning with previous findings [29]. Such overthinking also supports our hypothesis, as overthinking on incompleteness indeed indicates that LRMs have noticed the incompleteness. However, LRMs more likely to rely on internal thinking to address the incompleteness and \"solve\" the problem, rather than asking for information. Hallucinations when Missing Goals. As shown in Table 4, in more than 90% cases (RORs), the thoughts address the incompleteness by imagining goal. We consider this behavior as instruction inconsistency [30], where the output deviates from users directive. Interestingly, TLNCs are shorter compared to well-defined problems, suggesting that LRMs tend to imagine goals that can be quickly addressed without extensive thinking. For illustration, refer to Appendix for case studies. Unfaithfulness and overthinking explain 80% failures in missing premises, and hallucination explains 95% failures in missing goals. We leave analyzing other minor cases of behaviors to future works. These results help us understand how and why LRMs fail to ask questions: current LRMs are more like math quiz solvers, tending to \"solve\" incomplete problems by three major kinds of crooked behaviors instead of asking for information even explicitly prompted and recognized the incompleteness. 7 3.2 Evaluate SFT Effectiveness (RQ2) 3.2.1 Evaluation Setups SFT Training Data. Following the method of distilling LLMs [31, 32], we use Deepseek R1 with \"explicit prompt\" to generate data for SFT. Specifically, for each problem in the training set of CRITICmath: (1) For well-defined problems, we store answers and thoughts only if the answers are correct (determined by Math-Verify [33]). (2) For incomplete problems, we store answers and thoughts if the answers raise clarification questions (determined by string-matching on \"clarification\"). The training set was filtered to include only well-defined problems with correct answers and incomplete problems with answers raising clarification questions. This process results in total of 10.8K SFT samples. Models & Implementation Details: We use Qwen3-8B-Base as backbone and train two models and their variants: CRITIC-Qwen, trained using answers, and CRITIC-Qwen-thinking, trained using both thoughts and answers. For ablation studies, we train CRITIC-QwenW/I and CRITIC-QwenthinkingW/I to analyze the impact of problem types, where \"W\" indicates trained on well-defined problems and \"I\" indicates trained on incomplete problems. We use OpenRLHF [34] to SFT for 1 epoch, with learning rate of 1e-5 and batch size of 64.9 Evaluation Metrics: We follow the metrics in Section 3.1.1. Notably, according to the training dataformat, we prompt SFT models by \"explicit prompt\" according to training data-format. To further demonstrate the effectiveness of SFT in improving mathematical reasoning, we introduce Solved Ratio (SR), defined as the percentage of answers that correctly solve the well-defined problems. 3.2.2 Results & Discussion Table 5: Coarse level evaluation of SFT models. CRITIC-Qwen denotes CRITIC-Qwen-thinking. Qwen3-8B denotes the official LRM (also fine-tuned from Qwen3-8B-Base) with thinking enabled. Model Missing Premises Missing Goal Well-Defined CR TLC TLNC CR TLC TLNC CR TLC TLNC SR ACC CRITIC-Qwen CRITIC-Qwen Qwen3-8B / 78.42% 57.37% 0.22 51.58% 0.39 CRITIC-Qwen CRITIC-Qwen Qwen3-8B / 77.88% 56.66% 0.14 45.82% 0.29 Math 500 Subset / 94.87% 62.82% 0.25 50.43% 0.41 Omni-Math Subset / 97.64% 69.29% 0.14 54.72% 0.30 / 0.93 1.05 / 1.09 0.90 / 2.21 1.71 / 1.51 1.32 / 4.12% 1.23% 1.59 nan 0.00% / 1 (4554) 1 (3853) 73.39% 87.86% 80.83% 70.60% 97.53% 64.99% / 13.16% 7.52% 0.32 5.26% 0.32 / 1 (8895) 1 (7027) 41.13% 85.57% 54.88% 69.89% 83.74% 61.68% Table 6: Fine-grained analysis of SFT model when fails to ask for information. Only CRITIC-Qwenthinking can be evaluated. we average results of two data sources considering limited pages. Model Missing Premises Missing Goal Well-Defined RS ROR CNR RS ROR CNR CRITIC-Qwen 82.84 74.01% 21.75% 44.10 92.12% 6.06% Rs 41.30 Table 5 presents the results of our coarse-grained evaluation, while Table 6 details the fine-grained evaluation results. An integrated analysis on both tables yields the following observations: SFT Improves the Ability to Ask for Information. CRITIC-Qwen achieves higher CRs on incomplete problems. Although more false positives on well-defined problems, the accuracies are 87.86% and 85.57%, surpassing the highest accuracy of close-source LRMs (67.09% and 64.28% achieved by Grok 3 mini). CRITIC-Qwen-thinking also surpasses close-source LRMs. In addition, both SFT models surpass Qwen3-8B T, which fine-tuned from the same backbone but not tailored for asking for information. These results demonstrate the effectivenss of SFT in learning to ask for information. 9Other hyperparameters, like the learning rate schedule and warmup steps, are kept at their default values."
        },
        {
            "title": "4 Related Works",
            "content": "The Ability to Identify Incomplete Problem. In psychological and cognitive science, childs ability to identify missing information in mathematical problems is considered an important indicator of their intelligence [35, 36], reflecting their use of schematic knowledge (understanding problem structures) [37] and metacognition (supervising ones thoughts) [38, 39]. Current LRMs demonstrate advanced reasoning abilities, inspiring recent exploration into their capabilities from cognitive perspective [40]. Our work examines the behaviors and limitations of LRMs in identifying incomplete mathematical problems, revealing challenges in developing AI with human-level cognition. In addition, proactively identifying and gathering missing information is fundamental philosophical problem in AI [3], constituting central topics in active learning [41, 42], reinforcement learning [43], and automatical planning [44, 45]. Following this tradition, our work suggests that LRMs currently over-rely on internal thinking and struggle to effectively ask for information yet. Clarifying User Requests. Ambiguity and incompleteness frequently appear in natural language due to various factors [4648], and recognizing and resolving it has posed long-standing challenges for developing AI models [4951]. In the era of LLMs, extensive research has explored ambiguity in various scenarios, including conversation [12], semantics [52], question-answering [53], and chatbots [54]. To improve abilities of LLMs to handle ambiguity, recent approaches utilize Chain-of-Thought, few-shot prompting, or training adapters [5558], with the goal of developing helpful assistants that proactively understand users intents. In this paper, given the current trending research of LRMs, we extend the research scope of clarifying questions from general domain to mathematical reasoning, and argue that asking for information on incomplete problems stands for critical aspect to develop genuine intelligence more than just math quiz solver. We also explore the potential and challenges in learning such ability, hoping to providing insights to develop LRMs that can ask for information. Benchmarks in Asking for Information. Existing benchmarks primarily focus on users requests in general domains like task-oriented dialogue [12, 5962]. Limited efforts are dedicated to analyzing LRMs in handling incomplete math problems. Given the increasing prevalence of LRMs and current exclusive evaluation on solving well-defined problems, we propose new dataset to systematically evaluate LRMs in asking for information on incomplete problems. We acknowledge concurrent work, QuestBench [14] (released March 28, 2025 on Arxiv), that examines LLMs informationgathering in reasoning tasks. However, QuestBench only evaluates can LLMs pick up the correct question among several candidate choices. Such setting fails to evaluate the proactivity of asking for information, and does not discuss the relationship between problem-solving and asking questions, where dilemma is found in this work. CRITIC-math aims to provide more comprehensive evaluation of LRMs in proactively asking for information, extending current scope of solving problems."
        },
        {
            "title": "5 Conclusions",
            "content": "Existing evaluation of LRMs exclusively focuses on solving well-defined mathematical problems, ignoring another critic aspect of genuine intelligence regarding asking for information on incomplete problems. To bridge such gap, we introduce new benchmark, CRITIC-math, provide systematical evaluation, and find LRMs inability to identify incompleteness and proactively seek clarification. Our observed dilemma between the current mode of deep thinking and asking for information also demands fundamental shift in how we develop LRMs. We hope to offer guidance and insights for developing LRMs that transcend solely being math quiz solver, evolving into genuine intelligent agents capable requiting information, navigating uncertainties inherent in real-world scenarios."
        },
        {
            "title": "References",
            "content": "[1] Frans H. van Eemeren, Rob Grootendorst, Sally Jackson, Scott Jacobs, Agnes van Rees, Francisca Snoeck Henkemans, Eveline T. Feteris, Erik C. W. Krabbe, Agnes Verbiest, Charles Arthur Willard, and John Hayden Woods. What is reasoning? what is an argument?*. 2001. [2] Peter Adam Angeles. Dictionary of philosophy. 1981. [3] J. McCarthy and P.J. Hayes. Some philosophical problems from the standpoint of artificial intelligence. In Bonnie Lynn Webber and Nils J. Nilsson, editors, Readings in Artificial Intelligence, pages 431450. Morgan Kaufmann, 1981. [4] John McCarthy. Programs with common sense, 1959. [5] A. Newell and H. Simon. The logic theory machinea complex information processing system. IRE Transactions on Information Theory, 2(3):6179, 1956. [6] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [7] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [8] AoPS Online. 2024 aime i. URL: https://artofproblemsolving.com/wiki/index.php/2024_AIME_I, 2025. [9] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2024. [10] Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. Omni-MATH: universal olympiad level mathematic benchmark for large language models. In The Thirteenth International Conference on Learning Representations, 2025. [11] Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. [12] Tong Zhang, Peixin Qin, Yang Deng, Chen Huang, Wenqiang Lei, Junhong Liu, Dingnan Jin, Hongru Liang, and Tat-Seng Chua. Clamber: benchmark of identifying and clarifying ambiguous information In Proceedings of the 62nd Annual Meeting of the Association for needs in large language models. Computational Linguistics (Volume 1: Long Papers), pages 1074610766, 2024. [13] Yang Deng, Wenqiang Lei, Minlie Huang, and Tat-Seng Chua. Rethinking conversational agents in the era of llms: Proactivity, non-collaborativity, and beyond. In SIGIR-AP, pages 298301, 2023. [14] Zi Wang Belinda Z. Li, Been Kim. Questbench: Can llms ask the right question to acquire information in reasoning tasks? arXiv preprint arXiv:2503.22674, 2025. [15] Kunpeng Ning Jiayu Yao. Reasoning attack: Inducing llm to never-end thinking, 2025. https://github.com/PKU-YuanGroup/Reasoning-Attack/tree/master. [16] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [17] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [18] B. Bespoke-stratos Labs. of The https://hf.co/bespokelabs/Bespoke-Stratos-32B, 2025. Accessed: 2025-01-22. unreasonable effectiveness reasoning distillation. [19] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 10 [20] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. [21] NovaSky Team. Sky-t1: Train your own o1 preview model within $450. https://novaskyai.github.io/posts/sky-t1, 2025. Accessed: 2025-01-09. [22] Haotian Xu, Xing Wu, Weinong Wang, Zhongzhi Li, Da Zheng, Boyuan Chen, Yi Hu, Shijia Kang, Jiaming Ji, Yingying Zhang, et al. Redstar: Does scaling long-cot data unlock better slow-reasoning systems? arXiv preprint arXiv:2501.11284, 2025. [23] Qwen Team. Qwen3: Think deeper, act faster. https://qwenlm.github.io/blog/qwen3/, 2025. Accessed: 2025-04-29. [24] Antropic. Claude 3.7 sonnet and claude code. https://www.anthropic.com/news/claude-3-7-sonnet, 2025. Accessed: 2025-02-25. [25] xAI. Grok 3 beta the age of reasoning agents. URL: https://x.ai/blog/grok-3, 2025. [26] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024. [27] Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical reasoning. arXiv preprint arXiv:2501.07301, 2025. [28] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Openreasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. [29] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024. [30] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems, 43(2):1 55, 2025. [31] Cheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger language models In Findings of the Association for Computational with less training data and smaller model sizes. Linguistics: ACL 2023, pages 80038017, 2023. [32] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. [33] Hynek Kydlíˇcek. Math-Verify: Math Verification Library. [34] Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024. [35] Frank Dempster and Alice Corkill. Interference and inhibition in cognition and behavior: Unifying themes for educational psychology. Educational Psychology Review, 11:188, 1999. [36] Kellah Edens and Ellen Potter. How students unpack the structure of word problem: Graphic representations and problem solving. School Science and Mathematics, 108(5):184196, 2008. [37] Renae Low and R. Over. Detection of missing and irrelevant information within algebraic story problems. British Journal of Educational Psychology, 59:296305, 1989. [38] Emily Lai. Metacognition: literature review. 2011. [39] Melissa Medina, Ashley Castleberry, and Adam Persky. Strategies for improving learner metacognition in health professional education. American journal of pharmaceutical education, 81(4):78, 2017. [40] Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307, 2025. 11 [41] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Brij Gupta, Xiaojiang Chen, and Xin Wang. survey of deep active learning. ACM computing surveys (CSUR), 54(9):140, 2021. [42] Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data. In International conference on machine learning, pages 11831192. PMLR, 2017. [43] Richard Sutton, Andrew Barto, et al. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. [44] Aidan Curtis, George Matheos, Nishad Gothoskar, Vikash Mansinghka, Joshua Tenenbaum, Tomás LozanoPérez, and Leslie Pack Kaelbling. Partially observable task and motion planning with uncertainty and risk awareness. arXiv preprint arXiv:2403.10454, 2024. [45] Camille Phiquepal and Marc Toussaint. Combined task and motion planning under partial observability: An optimization-based approach. In 2019 International Conference on Robotics and Automation (ICRA), pages 90009006. IEEE, 2019. [46] Steven Piantadosi, Harry Tily, and Edward Gibson. The communicative function of ambiguity in language. Cognition, 122(3):280291, 2012. [47] Thomas Wasow, Amy Perfors, and David Beaver. The puzzle of ambiguity. Morphology and the web of grammar: Essays in memory of Steven G. Lapointe, pages 265282, 2005. [48] Tamar Degani and Natasha Tokowicz. Semantic ambiguity within and across languages: An integrative review. Quarterly Journal of Experimental Psychology, 63(7):12661303, 2010. [49] Hubert Dreyfus. What computers cant do: The limits of artificial intelligence. 1972. [50] Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter Hase, Ekdeep Singh Lubana, Erik Jenner, Stephen Casper, Oliver Sourbut, et al. Foundational challenges in assuring alignment and safety of large language models. arXiv preprint arXiv:2404.09932, 2024. [51] Chen Huang, Yang Deng, Wenqiang Lei, Jiancheng Lv, Tat-Seng Chua, and Jimmy Xiangji Huang. How to enable effective cooperation between humans and nlp models: survey of principles, formalizations, and beyond. arXiv preprint arXiv:2501.05714, 2025. [52] Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances for In The Eleventh International Conference on uncertainty estimation in natural language generation. Learning Representations, 2023. [53] Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. Ambigqa: Answering ambiguous open-domain questions. arXiv preprint arXiv:2004.10645, 2020. [54] Sanxing Chen, Sam Wiseman, and Bhuwan Dhingra. Chatshop: Interactive information seeking with language agents. arXiv preprint arXiv:2404.09911, 2024. [55] Yue Chen, Chen Huang, Yang Deng, Wenqiang Lei, Dingnan Jin, Jia Liu, and Tat-Seng Chua. Style: Improving domain transferability of asking clarification questions in large language model powered conversational agents. In Findings of the Association for Computational Linguistics ACL 2024, pages 1063310649, 2024. [56] Yang Deng, Lizi Liao, Liang Chen, Hongru Wang, Wenqiang Lei, and Tat-Seng Chua. Prompting and evaluating large language models for proactive dialogues: Clarification, target-guided, and nonIn Findings of the Association for Computational Linguistics: EMNLP 2023, pages collaboration. 1060210621, 2023. [57] Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Clam: Selective clarification for ambiguous questions with generative language models. arXiv preprint arXiv:2212.07769, 2022. [58] Jeremy Cole, Michael Zhang, Dan Gillick, Julian Eisenschlos, Bhuwan Dhingra, and Jacob Eisenstein. Selectively answering ambiguous questions. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 530543, 2023. [59] Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Iñigo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Gasic. Multiwoz-a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 50165026, 2018. 12 [60] Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan. Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 86898696, 2020. [61] Valerio Basile, Michael Fell, Tommaso Fornaciari, Dirk Hovy, Silviu Paun, Barbara Plank, Massimo Poesio, and Alexandra Uma. We need to consider disagreement in evaluation. Proceedings of the 1st Workshop on Benchmarking: Past, Present and Future, 2021. [62] Ruyuan Wan, Jaehyung Kim, and Dongyeop Kang. Everyones voice matters: Quantifying annotation In Proceedings of the AAAI Conference on Artificial disagreement using demographic information. Intelligence, volume 37, pages 1452314530, 2023."
        },
        {
            "title": "A Limitations",
            "content": "This paper contributes new dataset and analytical results of LRMs in asking for information. After thorough examination and our best in improve the quality, we acknowledge the following limitations: Exclusively Focus on Mathematical Reasoning: Mathematics well demos the characteristics of reasoning, where we need draw conclusion given the premises. In addition, mathematics receives great research interests for its nature of challenging and existing works have provided amount of well-defined mathematical problems for us to construct new dataset. Therefore, as prior work in evaluatin LRMs in asking for information, we select mathematical reasoning as our research scope. There are other tasks involve reasoning, such as code generation. However, those tasks involve more diverse contexts and skills beyond reasoning, e.g., coding skills, and less related datasets are available. We acknowledge our limited research scope and leave the exploration of other tasks to future works. Synthesized Dataset: We acknowledge that our dataset is fully AI synthesized (by Deepseek R1). To address the concern regarding dataset quality, we include two-phase of verification (cf. Section 2.2) where human evaluation are conducted. In addition, we have made our dataset open-sourced at the Huggingface (with anonymous account during any reviewing process and will be de-anonymized upon acceptance, hoping to help readers to assess the quality of our dataset. Usage of LLM-as-a-Judge in Evaluation: We acknowledge that we have utilized LLM-as-a-Judge to evaluate the results and behaviors of LRMs in asking for information. Given the free-form taskformulation of text generation, LLM-as-a-Judge is widely adopted technique in evaluation LLMs outputs. Therefore, we follow this tradition and use it in evaluation. To address concerns regarding LLM-as-a-Judge, we kindly remind readers that our evaluation contains multiple metrics, where the rule-based evaluated metrics can further support the results. And the rule-based evaluation and LLM-as-a-Judge have rather consistent results that can support each others. In addition, we have provided the prompts used for LLM-as-a-Judge and amount of cases studies in Appendix, helping readers to fully understand our evaluation process and to better assess our results. Hyperparameters in Experiments: We acknowledge that there are several hyperparameters that may affect the evalution, such as the temperature and top_p when accessing LRMs. We have conducted additional experiments to demonstrate the affect of temperature, where no significant differences are observed. However, due to the high cost of accessing LRMs, we leave other hyperparameters to its default values. Nevertheless, we hope our results can reflect behaviors of LRMs in the most common usage, where users call LRMs to generate one responses without carefully adjust the hyperparameters."
        },
        {
            "title": "B Broader Impact",
            "content": "This paper focuses on mathematical reasoning and our major argument is that genuine intelligence should not know solving problems, but also asking for information. After thorough examination, we hope other researches can use our datasets and refer to our analytical results in developing and evaluating their AI models. We do not have noticed any negative societal impact of our work yet. If noticed and received any concern, we will update this section to acknowledge the related impacts."
        },
        {
            "title": "C Responses of Different LRMs",
            "content": "In this section, we present the responses of different LRMs in responding the question My living room is 6 meters long. How many tiles (with side length of 60 centimeter) do need in total? Among LRMs, only o3-mini asks for information. The others make assumptions to solve the problem. o3-mini: To figure out the total number of tiles youll need, also need the width of your living room (in meters). With both length and width, we can compute the total area and then divide by the area of one tile (0.6 0.6 m). Whats the width of the room? Deepseek R1: The users living room is 6 meters long, and the tiles have side length of 60 centimeters (0.6 meters). To determine how many tiles are needed in total: 1. **Convert the length of the living room to centimeters**: 6 meters = 600 centimeters. 2. **Calculate the number of tiles along the length**: Each tile is 60 centimeters long, so the number of tiles needed along the length is: 600 cm 60 cm/tile = 10 tiles 3. **Consider the total number of tiles**: Since the width of the living room is not provided, the problem is ambiguous. However, if we assume the question is asking how many tiles fit along the 6-meter length, the answer is 10 tiles. Given the ambiguity but following the straightforward calculation for the length provided, the final answer is: 10 Gemini 2.5 pro: To figure out the total number of tiles, we need both the length and the width of your living room. Youve given the length as 6 meters. First, lets make sure our units are the same: Room length: 6 meters = 600 centimeters Tile side length: 60 centimeters Tiles along the length: Number of tiles = Room length / Tile side length Number of tiles = 600 cm / 60 cm = 10 tiles So, you would need 10 tiles to go along the 6-meter length of your room. To find the total number of tiles, youll also need the width of your room. Lets say, for example, your room is also 6 meters wide (making it square room): Room width: 6 meters = 600 centimeters Tiles along the width: 600 cm / 60 cm = 10 tiles Total tiles for 6m 6m room: 10 tiles (length) * 10 tiles (width) = 100 tiles If your room has different width, please provide that dimension, and can calculate the exact total for you. For instance, if your room was 3 meters (300 cm) wide: Tiles along the width: 300 cm / 60 cm = 5 tiles Total tiles: 10 tiles (length) * 5 tiles (width) = 50 tiles In summary: You need 10 tiles to span the 6-meter length. Please provide the width of your living room to calculate the total number of tiles needed. 15 Qwen3: To determine how many tiles are needed to cover the floor of living room that is **6 meters long**, we need to calculate the **total area** of the room and divide it by the **area of single tile**. ### Step 1: Convert Units for Consistency Each tile has **side length of 60 cm**, which is equivalent to: So, the **area of one tile** is: 60 cm = 0.6 0.6 0.6 = 0.36 m2 ### Step 2: Determine the Area of the Room The problem only gives the **length** of the room as 6 meters. To compute the **area**, we also need the **width**. However, since the width is not provided, and the problem asks for the **total number of tiles**, it is reasonable to **assume** that the room is **square** (i.e., length = width = 6 meters), which is common assumption in such problems when only one dimension is given. So, the **area of the room** is: 6 6 = 36 m2 ### Step 3: Calculate the Number of Tiles Now, divide the total area of the room by the area of one tile: ### Final Answer 36 m2 0.36 m2/tile = 100 tiles 100 Claude 3.7: To solve this problem, need to find how many 60 cm tiles are needed to cover length of 6 meters. First, Ill convert the measurements to the same unit: - Living room length = 6 meters - Tile side length = 60 centimeters = 0.6 meters To find the number of tiles needed, Ill divide the rooms length by the tiles side length: Number of tiles = Length of room Side length of tile Number of tiles = 6 0.6 = 10 tiles Therefore, you need 10 tiles to cover the 6-meter length of your living room. 16 CRITIC-math Construction and Case Studies D.1 Prompts Utilized in Constructing CRITIC-math CRITIC-math is constructed by rewriting the well-defined problems from open-source datasets. This construction involves three key stages: I) Parsing each problem into its constituent elements (i.e., goal, premises, and Background); II) Disturbing elements and constructing incomplete problems by reassembling these disturbed elements; and III) Verifying the resulting incompleteness. This appendix details prompts used for each stage of our process (DeepSeek R1 was used). Specifically: Prompt in Figure 3 is used for parsing problems into goals, premises and background;. Stage II Constructing Incomplete Problems is mainly completed automatically and Prompts in Figure 4 and D.1 is used for reassembly and generating reason of incompleteness. Finally, Prompt in Figure 6 is used for verify the incompleteness. Given TEXT, its \"Goal\", \"Necessary Information\", and \"Background\" are as follows: - \"Goal\" refers to the task that the TEXT requires to solve. \"Goal\" should be short and concise. - \"Necessary Information\" and \"Background\" refer to everything apart from the Goal, including data, facts, examples, etc. Among them: \"Necessary Information\" refers to the contents that are strictly needed to accomplish the Goal. \"Background\" refers to the other contents that can be ignored, such as examples and descriptions. Based on the above definitions, please SPLIT the following TEXT into \"Goal\", \"Necessary Information\", and \"Background\". *** TEXT: problem *** If Background is None, leave Background empty. Split long Necessary Information to short items. For \"Necessary Information\", provide numbered list. **Requirements** 1. **You should EXACTLY COPY contents from the TEXT to Goal, Necessary Information, and Background!!!** 2. **DO NOT ADD, REWRITE, REPHRASE, ANY CONTENT WHEN COPYING TEXT to YOUR OUTPUTS!!!** 3. **KEEP ALL SYMBOLS, such as EMPTY LINES, SPACES, MATH SYMBOLS, and FORMATTING SYMBOLS WHEN COPYING TEXT to YOUR OUTPUTS!!!** 4. **Goal, Necessary Information, and Background SHOULD NOT OVERLAP!!!** 5. **Goal, Necessary Information, and Background SHOULD COVER ALL contents of the TEXT!!!** Figure 3: The Prompt to Parse Problem into Goal and Premises. 17 # Goal {goal} # Information {information} # Background {background} # Instruction DO NOT consider rationality, assemble the Goal, into coherent text to get an Unclear Question. Because the Unclear Question contains Missing Information compared with the Original Question shown below, the Unclear Question should not be able to be answered directly. Information, and Background *** Original Question: {original_question} *** Provide the reasons that why the Unclear Question is Unclear (i.e, can not be answered directly). Generate your output STRICTLY in the following format. Unclear Question: Reason of Unclearness: # Requirements **The Unclear Question SHOULD NOT CONTAIN the title of Goal, Informa1. tion, and Background.** 2. **The Unclear Question SHOULD CONTAIN ALL contents in the Goal, Information, and Background (if the Information and Background are not empty).** 3. **DO NOT ADD ANY content that is not included in the Goal, Information and Background from the Original Question to the Unclear Question.** Figure 4: The Prompt to Construct Missing Premises Problems and Reason of Incompleteness 18 # Information {information} # Background {background} # Instruction Assemble the Information and Background into coherent text. Generate your output STRICTLY in the following format. Assembled Text: # Requirements **The Assembled Text SHOULD NOT CONTAIN the title of Information and 1. Background.** 1. **The Assembled Text SHOULD CONTAIN ALL contents in the Information and Background (if the Background is not None).** 2. **DO NOT ADD ANY other content that is not included in the Information and Background to the Assembled Text.** Figure 5: The Prompt to Construct Missing Goal Problems and Reason of Incompleteness # Task {query} # Reason of Unclearness {reason_of_unclearness} # Instruction According to the reason of unclearness, the task is classified to be unclear. Please act as an impartial judge to evaluate whether the task is truly unclear and whether the reason of unclearness is correct. If you agree that task is truly unclear, provide at least two different understandings of the unclear task corresponding to the unclearness as the verification of your judgement. Output your final verdict by strictly following this format: Analysis: [Your analysis about the task and the reason of unclearness] Judge: \"[[Yes]]\" if you agree with the reason and that the task is truly unclear, \"[[No]]\" if you do not agree with the reason and that the task is clear. Verification: [At least two different understandings of the unclear task corresponding to the unclearness] Figure 6: The Prompt to verify the incompleteness 19 D.2 Human Evaluation & Quality Control To verify the incomplete problems, we recruit three PhD students as human evaluators, two of them major in computer science and the other majors in mathematics. Since the incomplete problems have already been verified by Deepseek R1, we sample 10% of the problems and ask human evaluators to provide confirmation. Specifically, we ask human evaluators to provide their decision that whether the problem is incomplete given the incomplete problem, the counterpart well-defined problem, the reason of incompleteness, and R1s verification. We type the above information into Microsoft Word and present to human evaluators for them to verifiy. Totally, 128 incomplete problems are evaluated. All evaluators are paid at least the minimum wage in their country. The evaluation results show that human agree with the R1s verification in 95% cases (i.e., human disagrees in only 5 cases). We present two cases that we provide human evaluators to make decisions. We highlight the missing elements in red. Potential risks of Human Evaluation: Our human evaluation involves verification specific characteristic (i.e. the incompleteness) of mathematical problems. After thorough examination, we do not notice any potential risk of human subjects. We have resorted to the Institutional Review Board and got the approval regarding our human evaluation. If we notice or receive any risk regarding the human evaluation, we will update this section and provide comprehensive discussion. Incomplete Problem (Missing Premises) robot moving forward at constant speed takes 2.5 hours to travel 1 kilometer. How many meters long is the hallway? Well-defined Problem robot moving forward at constant speed takes 2.5 hours to travel 1 kilometer. moving forward at this same constant speed, it takes the robot 90 seconds to travel the length of particular hallway. how many meters long is the hallway? Reason of Incompleteness The question does not provide the time it takes for the robot to travel the hallway. While the robots speed can be calculated from the given information (2.5 hours for 1 kilometer), the hallways length cannot be determined without knowing how long the robot took to traverse the hallway. The missing information (e.g., 90 seconds in the Original Question) is critical for calculating the distance. R1 Verification Analysis: The task states the robots speed (1 km in 2.5 hours) but does not specify the time it took to traverse the hallway. To calculate the hallways length, both speed and time are required. The absence of the traversal time makes the problem unsolvable as posed. The reason for unclearness correctly identifies the missing critical information (time to traverse the hallway), rendering the task ambiguous. Judge: [[Yes]] Verification: 1. **Misinterpretation 1**: Assume the 2.5 hours mentioned is the time taken to traverse the hallway. This would incorrectly equate the hallways length to 1 km (1000 meters), conflating the speed-calibration distance with the hallways actual length. 2. **Misinterpretation 2**: Assume the problem implicitly asks for the robots speed in meters per second (e.g., converting 1 km/2.5 hours to 0.111 m/s) and answers with the speed instead of the hallway length, misaligning with the questions intent due to confusion over missing variables. 20 Incomplete Problem (Missing Goals) In school of 360 students, 15 are enrolled in calculus, physics, and chemistry simultaneously, while 15 students do not take any of these three subjects. total of 180 students take calculus. The number of students taking chemistry is twice the number taking physics. There are 75 students enrolled in both calculus and chemistry, and another 75 in both physics and chemistry. However, only 30 students take both physics and calculus. Well-defined Problem there are 360 people in my school. 15 take calculus, physics, and chemistry, and 15 dont take any of them. 180 take calculus. twice as many students take chemistry as take physics. 75 take both calculus and chemistry, and 75 take both physics and chemistry. only 30 take both physics and calculus. how many students take physics? Reason of Incompleteness The problem lacks specified goal to answer. R1 Verification Analysis: The task provides data about student enrollments in three subjects but does not specify question to answer. The reason for unclearnesslack of goalis valid. Without defined objective (e.g., finding the number of students in specific subject or verifying consistency), the problem is ambiguous. Judge: [[Yes]] Verification: 1. **Understanding 1**: The task aims to find the total number of students enrolled in physics. Using the given overlaps and relationships (chemistry = 2 physics), one might solve for physics using set theory formulas. **Understanding 2**: The task requires checking if the provided numbers are 2. mathematically consistent (e.g., ensuring overlaps do not exceed subject totals). For instance, verifying if 75 students in both physics and chemistry (with 15 in all three) do not exceed the total physics or chemistry enrollments. 21 Implementation of LLM-as-a-Judge for Evaluation In this section, we detail the prompts we used to conduct LLM-as-a-Judge in Evaluation. Prompts to Evaluate that Whether response Raises Clarification Questions when using \"Implicit Prompt\" # Question {problem} # Response {response} # Instruction Please act as an impartial judge to evaluate whether the response has raised questions to ask additional information for answering the Question. Output your final verdict by strictly following this format: Analysis: [Your analysis about the response] Judge: \"[[Yes]]\" if the response has raised questions to ask additional information, and \"[[No]]\" if the response directly provide an answer without raising clarification. Prompts to Evaluate CNR, i.e. whether the thoughts have noticed the need to raise clarification questions. # Incomplete Question {incomplete_task} # Reason of Incompleteness {reason_of_incompleteness} # Output {output} # Instruction Please act as an impartial need to ask clarification questions to address the incompleteness. Your assessment should be grounded on the Incomplete Question and the Reason of Incompleteness. judge to evaluate whether the output has noticed the Output your final verdict by strictly following this format: Analysis: [Your analysis about the Output] Judge: \"[[Yes]]\" if the output has noticed the need to ask clarification questions, otherwise \"[[No]]\" 22 Prompts to Evaluate ROR on missing premises problems, i.e. whether the reflection thoughts have focused on addressing the incompleteness. # Incomplete Question {incomplete_task} # Reason of Incompleteness {reason_of_incompleteness} # Output {output} # Instruction Please act as an impartial judge to evaluate whether the output has tried to address the incompleteness. For example, the output tried different possibilities about the incompleteness or guessing how the incompleteness arises. Your assessment should be grounded on the Incomplete Question and the Reason of Incompleteness. Output your final verdict by strictly following this format: Analysis: [Your analysis about the Output] Judge: \"[[Yes]]\" if the output has tried to address the incompleteness, otherwise \"[[No]]\" Prompts to Evaluate ROR on missing goal problems, i.e. whether the thoughts imagine goal by themselves. # Incomplete Question {unclear_task} # Reason of Incompleteness {reason_of_unclearness} # Output {output} # Instruction According to the reason of incompleteness, the question lacks specific goal. Please act as an impartial judge to evaluate whether the output has tried to address the incompleteness by imagining goal itselves. Output your final verdict by strictly following this format: Analysis: sis about the Output] Judge: \"[[Yes]]\" if the output has tried to address the incompleteness, otherwise \"[[No]]\" [Your analy-"
        },
        {
            "title": "F Additional Experimental Analysis",
            "content": "F.1 Additional Analysis for RQ1 F.1.1 The Relationship between CRs and Problem Difficulties Figure 7: CRs of different LRMs on missing premises problems with different difficulties. Figure 7 show the clarification ratios (CRs) of different LRMs on missing premises problems. We can observe negetive correlation between the problem difficulty and the CRs, which indicate that the ability to ask for information is unrobust to the prolem difficulty. F.1.2 Questions Accuracies in Addressing the Incompleteness In this section, we provide the evaluation results regarding the accuracy of asking for information, which is measured by the ratio of questions asked by LRMs that can appropriately address the incompleteness. We use LLM-as-a-Judge (Deepseek R1) as the evaluation, where the prompt are detailed in Figure 8. From the results, we can observe that all LRMs, when they successfully raise questions, can achieve rather high accuracy in asking for information. Table 7: The accuracy of asking for information, which is measured by the ratio of questions asked by LRMs that can appropriately address the incompleteness. Model Missing Premises Missing Goal CRITIC-math (from Math 500 Data Source) Deepseek R1 Qwen3 Plus o3 mini Grok 3 mini Claude 3.7 91.25% 95.41% 89.16% 92.23% 87.26% 98.25% 99.26% 90.48% 100.00% 99.12% CRITIC-math (from Omni-MATH Data Source) Deepseek R1 Qwen3 Plus o3 mini Grok 3 mini Claude 3.7 87.18% 89.86% 87.00% 85.51% 85.53% 95.59% 95.92% 91.53% 98.04% 95.71% F.1.3 Sampling Multiple Responses . In the evaluation in Section 3, we generate one response with the temperature set to 0 from closesource LRMs. To analyze that how the temperature affects the results and will the results significantly vary in different samples on the same problem, we conduct additional evaluation. Considering our budget, we conduct this evaluation only using the \"explicit prompt\" on randomly sampled 50 problems from each category of missing premises, missing goal, and well-defined in each data source. For each problem, we generate one response with temperature set to 0 (denotes as S) and 5 responses with temperature set to 0.6 (denoted as M). 24 # Incomplete Task {incomplete_task} # Reason of Incompleteness {reason_of_incompleteness} # Clarification Question {clarification_question} # Instruction Please act as an impartial judge to evaluate whether the clarification question can appropriately address the incompleteness of the incomplete question, given the content of the incomplete question and the reason of incompleteness. Output your final verdict by strictly following this format: Analysis: [Your analysis about the clarification question] Judge: \"[[Yes]]\" if the task clarification question is appropriate and sufficient, otherwise \"[[No]]\" Figure 8: Prompts to evaluate the accuracy of asking for information. Table 8 display the results. From the results, we can observe that sampling multiple responses will slightly influence the CRs, TLCs, and TLNCs. However, the influence varies in different LRMs and different types of incompleteness. Generally speaking, the results of mostly fall into the 95% confidential interval of M. Also, the main conclusion remains the same, where the CRs are around 50%, TLNCs are greater than 1 on missing premises problems, barely changed TLNCs on missing goal problems, and lower TLCs on both cases. These additional results demonstrate that our analysis and results in the main text are robust to the choice of temperature and different sampled responses. TLNCs on Problems of Different Difficulties. Figure 9: TLNCs of different LRMs on incomplete problems with different difficulties. The results are plotted in Figure 9. From the results, we can observe negative correlation between the problem difficulty and the TLNCs, where the overthink are more significant on easier problems. For example, on the difficulty level of 1 in Math 500, the thoughts lengths are in average 3.76 times longer thoughts compared with the lengths on well-defined problems. In addition, the thoughts lengths are increased more significant on Math 500 compared with Omni-MATH, given that Omni-MATH is dataset containing more challenging problems. 25 Table 8: The results of sampling multiple responses. denotes generating one response with temperature set to 0, and denotes sampling 5 responses with temperature set to 0.6. For M, 95% confidential intervals are reported. We omit the results of well-defined problems for better readability. Model Type Missing Premises Missing Goal CR TLC TLNC CR TLC TLNC Deepseek R1 Qwen3 PT o3 mini Grok 3 mini Claude 3.7 Deepseek R1 Qwen3 PT o3 mini Grok 3 mini Claude 3.7 CRITIC-math (Subset from Math 500 Data Source) 55.56% 0.53 37.60%3.24% 0.620. 44.00% 0.50 44.00%3.51% 0.550.01 46.00% 1.62 43.20%1.36% 1.780. 46.00% 0.57 48.00%3.04% 0.700.19 28.00% 0.30 38.40%17.86% 0.290. 2.25 2.380.22 2.45 2.600.13 3.20 2.980.65 1.61 1.720.20 2.56 1.761.12 46.00% 0.41 48.80%4.84% 0.490.13 64.00% 0.73 60.00%2.51% 0.470.01 1.03 1.290. 1.23 1.310.26 36.00% 38.80%0.02 1.54 1.880.33 1.33 1.320.58 54.00% 0. 49.20%2.83% 0.470.04 46.00% 0.16 48.00%8.42% 0.280.27 0.86 1.150.09 1.59 1.510. CRITIC-math (Subset from Omni-Math Data Source) 46% 0.28 50.00%3.51% 0.390.03 48.00% 0. 51.20%2.22% 0.330.03 56.00% 0.42 55.60%6.43% 0.570.39 56.00% 0. 61.60%5.38% 0.400.05 42.00% 0.45 42.00%1.76% 0.450.14 1.25 1.280.08 1.24 1.210. 1.71 1.480.14 1.07 1.060.09 1.38 1.450.10 59.00% 50.80%0.42 0.24 0.260.12 1.16 1.330. 58.00% 0.27 58.00%4.65% 0.330.07 1.26 1.210.14 40.00% 39.20%0.03 0.72 0.660. 1.08 0.870.13 52.00% 0.45 53.60%3.24% 0.410.04 38.00% 0. 36.40%4.44% 0.210.11 1.10 1.070.10 1.16 1.270.14 M S S S M F.1.4 Cases Studies about Overthinking when Missing Premises We provide 3 case studies of Deepseek R1 with \"explicit prompt\" related to the overthinking when missing premises. The case studies are provided in Figure 10, 11, and 12. From the cases, we can observe that LRMs try to address the incompleteness by themselves (marked as red) rather than asking for information. Such self-reflections on incompleteness suggests that the current mode of deep-thinking may overemphasize the internal thinking while ignoring the external interactions. These phenomena also support our analysis in Section F.2 about the dilemma between learning the current mode of deep-thinking and asking for information. F.1.5 Cases Studies about Hallucinations when Missing Goals We provide 6 case studies of Deepseek R1 (3 with \"implicit prompt\" and 3 with \"explicit prompt\") about the hallucinations when missing goals. The results are shown in Figure 13, 14, 15, 16, 17, and 18. We highlight the goal that not specified in the input but imagined by Deepseek R1 in red. In those cases, we can observe that Deepseek R1 immediately generates goal in their first paragraph of thinking, and dedicates to solve the imagined goal and finally provide corresponding answer. # Problem to Solve The greek army contained two types of soldiers: the upper class and the lower class soldiers. If there were total of 5 upper class soldiers and 10 lower class soldiers in certain part of athens, and the battle of thermopylae demands force of 4 upper class soldiers, how many different battalions can be sent? # Instruction If you need ask information, please raise clarification question and start your response STRICTLY with: \"Clarification Question\" followed by your questions. Otherwise, please reason step by step, and put your final answer within {}. ... But wait, maybe theres more to it. The problem mentions \"battalions,\" which usually consist of multiple soldiers. However, since the requirement is specifically for 4 upper class soldiers, perhaps lower class soldiers are not part of this battalion. Alternatively, maybe the problem expects that the battalion includes both upper and lower class soldiers, but with exactly 4 upper class ones. But the problem doesnt specify total number of soldiers needed for the battalion. So thats bit confusing. ... Alternatively, maybe the problem is just asking for the number of ways to choose the 4 upper class soldiers from the available 5, regardless of the lower class soldiers. Because if the battalion is required to have 4 upper class soldiers, and the number of lower class soldiers isnt specified, ... But need to make sure. Let me think again. The problem states \"a force of 4 upper class soldiers.\" If the battalion is just those 4, then yes, 5 choose 4 is 5. ... Figure 10: Case Studies of Overthinking when Missing Premises. 27 # Problem to Solve find the remainder when (n + 2)(n + 4)(n + 6) is divided by 7. # Instruction If you need ask information, please raise clarification question and start your response STRICTLY with: \"Clarification Question\" followed by your questions. Otherwise, please reason step by step, and put your final answer within {}. ... Case 1: = 0 mod 7 Then: + 2 = 2 mod 7 + 4 = 4 mod 7 + 6 = 6 mod 7 Product: 2*4*6 = 48. 48 divided by 7 is 6*7=42, remainder 6. So 48 mod7=6. ... Case 2: =1 mod7 Then: n+2 =3 mod7 n+4=5 mod7 n+6=7=0 mod7 Product: 3*5*0=0. So remainder 0. ... But wait, the problem is asking for \"the remainder\" in general. But since the remainder depends on n, unless theres more information about n, we cant give unique answer. Wait, but maybe the problem is expecting an answer that works for any n, or perhaps theres pattern that can be generalized. ... Figure 11: Case Studies of Overthinking when Missing Premises. 28 # Problem to Solve pentagon is rotated counterclockwise. what is the number of degrees it must be rotated until it coincides with its original position? # Instruction If you need ask information, please raise clarification question and start your response STRICTLY with: \"Clarification Question\" followed by your questions. Otherwise, please reason step by step, and put your final answer within {}. ... The exact wording is: \"what is the number of degrees it must be rotated until it coincides with its original position?\" So, does that mean the total rotation needed to get back to the original position? For example, if you rotate 72 degrees each time, after five such rotations, you would have 5*72 = 360 degrees, which brings it back. But is the question asking for the smallest such angle? Because 360 degrees is full rotation, which would obviously bring it back. But maybe the question is trickier than that But need to make sure. Let me think. For example, square has rotational symmetry every 90 degrees. So, rotating it 90 degrees four times brings it back to the original position. So, each 90 degrees is step. So, the minimal angle is 90 degrees. But the total rotation required to get back to original is 360 degrees, but the question is probably asking for the minimal angle that when you rotate by that angle, the figure coincides with itself. ... Figure 12: Case Studies of Overthinking when Missing Premises. proper divisor of an integer is positive divisor of that is less than n. For example, the proper divisors of 12 are 1, 2, 3, 4, and 6. Okay, so need to figure out the proper divisors of an integer. Lets start by recalling what proper divisor is. The problem statement says proper divisor of an integer is positive divisor that is less than n. For example, for 12, the proper divisors are 1, 2, 3, 4, and 6. That makes sense because those numbers divide 12 without leaving remainder and are all less than 12. ... The proper divisors of 12 are {1, 2, 3, 4, 6} . Figure 13: Case Studies of Hallucinations when Missing Goals. bc is parallel to the segment through a, and ab = bc. The diagram shows two horizontal lines connected by segments from to and to c. The angle at is labeled 124, and an angle labeled is marked on the upper horizontal line near (4.5, 3). Okay, lets see. need to figure out the value of angle in this diagram. Hmm. The problem says that segment bc is parallel to the segment through a, and that ab equals bc. There are two horizontal lines connected by segments from to and to c. The angle at is 124 degrees, and angle is on the upper horizontal line near the point (4.5, 3). ... **Final Answer** 28 Figure 14: Case Studies of Hallucinations when Missing Goals. 29 The problem must be solved without the use of calculator. Okay, so need to solve this problem without using calculator. Let me see... The problem is about triangle with vertices at points A, B, and C. The coordinates given are A(1, 2), B(4, 6), and C(6, 3). The question is asking for the area of this triangle. Hmm, right. remember there are few ways to calculate the area of triangle when you have the coordinates of the vertices. One method is using the shoelace formula, another might be using vectors or determinants. Let me recall the shoelace formula because that seems straightforward for coordinates. ... The area of the triangle is 17 2 . Figure 15: Case Studies of Hallucinations when Missing Goals. # Problem to Solve regular hexagon can be divided into six equilateral triangles. The perimeter of one of the triangles is 21 inches. # Instruction If you need ask information, please raise clarification question and start your response STRICTLY with: \"Clarification Question\" followed by your questions. Otherwise, please reason step by step, and put your final answer within boxed{}. Okay, lets see. The problem says regular hexagon can be divided into six equilateral triangles. The perimeter of one of those triangles is 21 inches. need to find something, but wait, the original problem statement isnt here. Wait, looking back. Oh, the user said \"If you need to ask information, please raise clarification question...\" So maybe the problem is to find the perimeter of the hexagon? Because that would make sense. ... So yes, the side of each triangle is equal to the side of the hexagon. Therefore, perimeter of hexagon is 6 * 7 = 42. So think thats correct Figure 16: Case Studies of Hallucinations when Missing Goals. 30 # Problem to Solve 2 (3 + 3 2)i and = 2 3i. The point is obtained by rotatLet = 2 + ing around by π 4 radians counter-clockwise. An Asymptote diagram illustrates this configuration: is plotted at the coordinate (2 + 2), at (2, 3), and is the 2, 3 3 result of rotating by 45 around c. The figure includes lines connecting z, c, and w, with labeled points and an angle label π 4 near c. # Instruction If you need ask information, please raise clarification question and start your response STRICTLY with: \"Clarification Question\" followed by your questions. Otherwise, please reason step by step, and put your final answer within boxed{}. Okay, so need to find the point w, which is the result of rotating around by π/4 radians counter-clockwise. Let me start by recalling how rotation works in the complex plane. ... Therefore, the complex number is 6 -5i. So think thats the correct answer. dont see any mistakes in the steps. So the final answer is 6 5i . Figure 17: Case Studies of Hallucinations when Missing Goals. # Problem to Solve Denali and Nate work for dog walking business and are paid for each dog they walk. Denali is responsible for 16 dogs, and Nate is responsible for 12 dogs. Under the companys new policy, they will be assigned or unassigned new dogs in groups of dogs. The ratio of Denalis pay to Nates pay would be the same if Denali started walking 4x more dogs and Nate stayed at 12 dogs or if of Nates dogs were reassigned to Denali. # Instruction If you need ask information, please raise clarification question and start your response STRICTLY with: \"Clarification Question\" followed by your questions. Otherwise, please reason step by step, and put your final answer within boxed{}. Okay, lets see. So the problem is about Denali and Nate who work for dog walking business. They get paid per dog they walk. Currently, Denali has 16 dogs and Nate has 12. The company is changing their policy so that theyll be assigned or unassigned dogs in groups of dogs. The key point is that the ratio of their pay remains the same under two different scenarios: either Denali gets 4x more dogs and Nate stays at 12, or of Nates dogs are reassigned to Denali. need to find x. ... Yes, both equal 3. So the answer is 5. So x=5. Therefore, boxed answer is 5 Figure 18: Case Studies of Hallucinations when Missing Goals. 31 F.2 Additional Analysis for RQ We conduct ablation studies to analyze the effect of different typs of problems in SFT models. The results are provided in Table 9, where the subscript indicates trained solely using well-defined problems, and the subscript indicates trained solely using incomplete problems. From the results, we have the following observations: Learning to ask for information can benefit the ability of solving problems. Comparing CRITIQwenW with CRITIC-Qwen and CRITIC-Qwen-thinkingW with CRITIC-Qwen-thinking, we can observe that SRs on well-defined problems even decreased when solely learning to solve well-defined problems (W). Therefore, learning to ask for information on incomplete problems is not contradict with, even benefit, learning the ability to solve well-defined problems Table 9: Ablation studies of SFT models trained on different types of problems."
        },
        {
            "title": "Missing Premises Missing Goal",
            "content": "Well-defined CR CR CR SR CRITIC-math (from Math 500 Data Source) CRITIC-Qwen CRITIC-QwenW CRITIC-QwenI 78.42% 0.53% 100.00% 94.87% 0.00% 100.00% 4.12% 0.00% 73.39% 73.25% 100.00% 0.00% CRITIC-math (from Omni-MATH Data Source) CRITIC-Qwen CRITIC-QwenW CRITIC-QwenI 77.88% 0.23% 100.00% 97.64% 0.00% 100.00% 13.16% 41.13% 39.85% 0.00% 100.00% 0.00%"
        }
    ],
    "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Engineering Research Center of Machine Learning and Industry Intelligence, Ministry of Education, China",
        "Institute of Data Science, National University of Singapore, Singapore",
        "Sichuan University, China"
    ]
}