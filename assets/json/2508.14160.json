{
    "paper_title": "RynnEC: Bringing MLLMs into Embodied World",
    "authors": [
        "Ronghao Dang",
        "Yuqian Yuan",
        "Yunxuan Mao",
        "Kehan Li",
        "Jiangpin Liu",
        "Zhikai Wang",
        "Xin Li",
        "Fan Wang",
        "Deli Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce RynnEC, a video multimodal large language model designed for embodied cognition. Built upon a general-purpose vision-language foundation model, RynnEC incorporates a region encoder and a mask decoder, enabling flexible region-level video interaction. Despite its compact architecture, RynnEC achieves state-of-the-art performance in object property understanding, object segmentation, and spatial reasoning. Conceptually, it offers a region-centric video paradigm for the brain of embodied agents, providing fine-grained perception of the physical world and enabling more precise interactions. To mitigate the scarcity of annotated 3D datasets, we propose an egocentric video based pipeline for generating embodied cognition data. Furthermore, we introduce RynnEC-Bench, a region-centered benchmark for evaluating embodied cognitive capabilities. We anticipate that RynnEC will advance the development of general-purpose cognitive cores for embodied agents and facilitate generalization across diverse embodied tasks. The code, model checkpoints, and benchmark are available at: https://github.com/alibaba-damo-academy/RynnEC"
        },
        {
            "title": "Start",
            "content": "RynnEC: Bringing MLLMs into Embodied World Ronghao Dang1,2, Yuqian Yuan1,3, Yunxuan Mao1,3, Kehan Li1,2, Jiangpin Liu1,3, Zhikai Wang1,2, Xin Li1,2, Fan Wang1,2, Deli Zhao1,2 1DAMO Academy, Alibaba Group 2Hupan Lab 3Zhejiang University Equal contribution, Corresponding author We introduce RynnEC, video multimodal large language model designed for embodied cognition. Built upon general-purpose vision-language foundation model, RynnEC incorporates region encoder and mask decoder, enabling flexible region-level video interaction. Despite its compact architecture, RynnEC achieves state-of-the-art performance in object property understanding, object segmentation, and spatial reasoning. Conceptually, it offers region-centric video paradigm for the brain of embodied agents, providing fine-grained perception of the physical world and enabling more precise interactions. To mitigate the scarcity of annotated 3D datasets, we propose an egocentric video based pipeline for generating embodied cognition data. Furthermore, we introduce RynnEC-Bench, region-centered benchmark for evaluating embodied cognitive capabilities. We anticipate that RynnEC will advance the development of general-purpose cognitive cores for embodied agents and facilitate generalization across diverse embodied tasks. The code, model checkpoints, and benchmark are available at: https://github.com/alibaba-damo-academy/RynnEC Date: August 21, 2025 5 2 0 2 9 1 ] . [ 1 0 6 1 4 1 . 8 0 5 2 : r Figure 1 RynnEC is video multi-modal large language model (MLLM) specifically designed for embodied cognition tasks. It can accept inputs interwoven from video, region masks, and text, and produce output in the form of text or masks based on the question. RynnEC is capable of addressing diverse range of object and spatial questions within embodied contexts and plays significant role in indoor embodied tasks."
        },
        {
            "title": "1 Introduction",
            "content": "In recent years, Multi-modal Large Language Models (MLLMs) [62, 77] have experienced rapid development, leading to the emergence of models such as Gemini [58] and GPT-4o [46] that can handle image and even video inputs. These MLLMs are attracting increasing attention from researchers due to their powerful contextual understanding [15] and generalization [82] abilities. Researchers in embodied intelligence are also beginning to explore the use of MLLMs as the brains of robots [22, 26], enabling them to perceive the real world through visual inputs like humans do. However, the current mainstream MLLMs are trained on extensive internet images and lack the foundational visual cognition to match the physical world [13, 74]. Some works have begun exploring how MLLMs can be applied to ego-centric embodied scenarios. Models like Exo2Ego [79] and EgoLM [23] enhance the understanding of ego-centric dynamic environment interactions. SpatialVLM [7] and SpatialRGPT [10] focus on addressing spatial understanding challenges within embodied contexts. However, these approaches are challenging to directly implement in physical robots to perform complex tasks. The main limitations are as follows: 1. Lack of flexible visual interaction: In complex embodied scenarios, relying solely on textual communication is prone to ambiguity or vagueness. Direct visual interaction references, such as masks or points, can more accurately and flexibly index entities within scene, facilitating precise task execution. 2. Insufficient detailed understanding of objects: During task execution, objects typically serve as the smallest operational units, making comprehensive and detailed understanding of objects crucial. As illustrated in Task 1 Step 1 in Fig. 1, recognizing the number of panes in window is essential to determine the quantity of window decals needed. 3. Absence of video-based coherent spatial awareness: For humans, spatial cognition arises from continuous visual perception [48]. Current methods in spatial intelligence [80, 64] primarily focus on single or discrete images, lacking the capacity for spatial understanding in high-continuity videos. For example, in Task 1 Step 4 in Fig. 1, the absolute distance between the teddy bear and the pillow requires spatial scale concept derived from the entire video to be properly inferred. Thus, we propose RynnEC, an embodied cognitive MLLM designed to enhance robotic understanding of the physical world. As illustrated in Fig. 1, RynnEC is large video understanding model whose visual encoder and foundational parameters are derived from VideoLLaMA3 [76]. To enable flexible visual interaction, we incorporate an encoder and decoder specifically for region masks in videos, allowing RynnEC to achieve precise instance-level comprehension and grounding. Within this framework, RynnEC is designed to perform diverse cognitive tasks in embodied scenarios. We categorize embodied cognitive abilities into two essential components: object cognition and spatial cognition. Object cognition necessitates MLLMs understanding of object attributes, quantities, and their relationships with the environment, alongside accurate object grounding. Spatial cognition is further divided into worldcentric and ego-centric perspectives. World-centric spatial cognition requires the model to grasp absolute scales and relative positions within scenes, as exemplified by object size estimations in Task 1 Step 2  (Fig. 1)  . Ego-centric spatial cognition connects the robots physical embodiment with the world, thereby assisting in behavioral decisions. For example, as depicted in Fig. 1, the reachability estimation in Task 2 Step 3 and the orientation estimation in Task 2 Step 5 assist the robot in clearly defining its relationship with interactive objects. Equipped with enhanced object and spatial reasoning, RynnEC supports more efficient execution of complex, real-world robotic tasks. Regrettably, the development of embodied cognition models has been slow due to lack of ego-centric videos and high-quality annotations. Efforts such as Multi-SpatialMLLM [64], Spatial-MLLM [61], and SpaceR [47] leverage open-source datasets with comprehensive 3D point cloud and annotations to generate training data. However, in an era of scarce 3D annotations [24, 38], this approach cannot achieve rapid and cost-effective expansion of data scale. Hence, we propose data generation pipeline that transforms ego-centric RGB videos into embodied cognition question-answering datasets. This pipeline begins with instance segmentation from videos and diverges into two branches: one generating object cognition data and the other producing spatial cognition data. Ultimately, data from both branches are integrated into comprehensive embodied cognition dataset. From over 200 households, we collect more than 20,000 egocentric videos. subset from ten households is manually verified and balanced to create RynnEC-Bench, fine-grained embodied cognition benchmark encompassing 22 tasks in object and spatial cognition. Extensive experiments demonstrate that RynnEC significantly outperforms both general [46, 4, 87] and taskspecific [72, 75, 57] MLLMs in cognitive abilities within embodied scenarios, showcasing scalable application potential. Additionally, we observe notable advantages in multi-task training with RynnEC and identify preliminary signs of emergence in more challenging embodied cognition tasks. Finally, we highlight the potential of RynnEC in facilitating robots to undertake large-scale, long-range tasks."
        },
        {
            "title": "2.1 MLLMs for Video Understanding",
            "content": "Early MLLMs primarily relied on sparse sampling and simple connectors, such as MLPs [33, 2, 39] and Q-Formers [78, 28], to integrate visual representation with large language models. Subsequently, to tackle the problem of long video understanding, [81] directly expanded the context window of language models, while [83] introduced pooling in the spatial and temporal dimensions to compress the number of video tokens. As the need for more fine-grained understanding emerged, some studies (VideoRefer [75], DAM [32] and PAM [35]) employed region-level feature encoders enabling video MLLMs to accept masked inputs and comprehend the semantic features of objects within the masks. Although these video MLLMs have demonstrated superior capabilities in high-level semantic capture and temporal modeling, they lack robust physical-world comprehension in egocentric embodied scenarios."
        },
        {
            "title": "2.2 Embodied Scene Understanding Benchmarks",
            "content": "Some studies [54, 29, 21] have begun to explore leveraging MLLMs to assist robots in solving embodied tasks. However, determining whether these MLLMs possess the ability to understand and interact with the physical world is challenging. Consequently, several benchmarks have emerged to evaluate the capability of MLLMs to perceive the physical world. OpenEQA [41] and IndustryEQA [30] focus on several key competencies in home and industrial settings, respectively, and manually designed open-vocabulary questions. VSI-Bench [69] centers on assessing the spatial cognitive abilities of MLLMs. STI-Bench [31] introduces more complex kinematic (e.g. velocity) problems. ECBench [13] systematically categorizes embodied cognitive abilities into static environments, dynamic environments, and overcoming hallucinations, offering comprehensive evaluation across 30 sub-competencies. While these benchmarks encompass wide range of abilities, they are unable to assess more fine-grained, region-level understanding capabilities in embodied scenarios. Compared to purely textual question-answering, region-level visual interaction can more accurately refer to targets in the complex real world."
        },
        {
            "title": "2.3 Improving MLLMs for Embodied Cognition",
            "content": "The aforementioned embodied benchmarks have highlighted the cognitive limitations of current MLLMs in embodied scenarios. Consequently, some studies have started to investigate diverse strategies for enhancing MLLMs understanding of the physical world. GPT4Scene [49] improves MLLMs consistent global scene understanding by explicitly adding instance marks between video frames. SAT [53] explores multi-frame dynamic spatial reasoning in simulated environments. Spatial-MLLM [61], Multi-SpatialMLLM [64], and SpaceR [47] leverage 3D datasets with detailed annotations (e.g., ScanNet [70]) to construct the suite of spatial-intelligence tasks introduced in VSI-Bench. In contrast, our data generation pipeline based on RGB videos yields more realistic and scalable training data. More importantly, RynnEC is designed not just to handle selected capabilities in embodied scenarios, but to cover broad swath of the world cognition required for embodied task execution under single paradigm. Figure 2 Embodied Cognition Question-Answer (QA) Data Generation Pipeline: First, objects within the scene are segmented from the video. Subsequently, object and spatial QA pairs are generated via two distinct branches."
        },
        {
            "title": "3 Methodology",
            "content": "RynnEC is robust video embodied cognition model capable of processing and outputting various video object proposals. This enables it to flexibly address embodied questions about objects and space. Due to paucity of research in this domain, we comprehensively present the construction process of RynnEC from four perspectives: data generation (Sec. 3.1), evaluation framework establishment (Sec. 3.2), model architecture (Sec. 3.3), and training (Sec. 3.4)."
        },
        {
            "title": "3.1 Embodied Cognition Data Generation",
            "content": "Our embodied cognition dataset construction  (Fig. 2)  begins with egocentric video collection and instance segmentation. One branch employs human-in-the-loop streaming generation approach to construct various object cognition QA pairs. The other branch utilizes monocular dense 3D reconstruction method and diverse question templates to generate spatial cognition task QA pairs. 3.1.1 Video Collection and Instance Segmentation Our egocentric video collection encompasses 200+ houses, with approximately 100 videos per house. To ensure video quality, we require resolution of at least 1080p and frame rate no less than 30fps, using gimbal to maintain shooting stability. To achieve diversity among different video trajectories, each house is divided into multiple zones, with filming trajectories categorized into single-zone, dual-zone, and tri-zone types. Cross-zone filming enhances diversity by altering the sequence of traversed zones. Additionally, we randomly vary lighting conditions and camera height under different trajectories. We require that each video includes both vertical and horizontal rotations, as well as at least two close-ups of objects, simulating the variable field of view in robotic task execution. Ultimately, we collect 20,832 egocentric videos of indoor movement. To control video length, these videos are segmented every 40 seconds. Previous works [37, 60] adopted strategy of designing separate data generation processes for each task type, leading to limited data reusability and continuity. We aim to create lineage among different types of foundational data to reduce unnecessary redundancy in data generation. Therefore, this paper proposes mask-centric embodied cognition QA generation pipeline. This pipeline initiates with the generation of object masks from video instance segmentation within scene. First, Qwen2.5-VL [4] observes the raw video and outputs an object list containing the names of all entity categories in the scene. Utilizing this object list, Grounding DINO 1.5 [55] detects objects in key frames at one-second intervals. SAM2 [51] assists in segmenting and tracking the objects detected by Grounding DINO 1.5 during the intervening one-second interval. To ensure consistency of instance IDs, the tracking results of old instances are compared with the segmentation results of newly detected instances at key frames. If an instance is found to have overlapping masks (IOU > 0.5), it retains the ID of the old tracking instance. Due to the performance limitations of Grounding DINO 1.5, newly detected object instances may already have appeared in preceding frames yet were missed. Thus, SAM2 conducts reverse four-second instance tracking for each new object in key frames, thereby achieving full lifecycle instance tracking. In total, we obtain 1.14 million video instance masks from all the egocentric videos. 3.1.2 Object QA Generation In this work, we generate three types of object-related tasks: object captioning, object comprehension QA, and referring video object segmentation. For each instance, we first divide all frames containing the instance into eight equal parts in chronological order. Within each frame group, an instance key frame is selected based on two factors: the size of the instance in the frame and the distance between the instance center and the frame center. Consequently, each instance is associated with eight instance key frames, featuring good instance visibility and diverse viewing angles. Half of these frames have the instance cropped out using mask, while the other four highlight the instance using red bounding box and background dimming technique. The final set of object cue images is displayed within the blue box in Fig. 2. Due to the limitation of SAM2 in consistent object tracking in egocentric videos, the same instance may be assigned multiple IDs if the instance appears intermittently in the video. We employ an object category filtering method that limits each video to maximum of two instances per object category, thereby minimizing duplicate instances. The presence of multiple video segments per house leads to repeated occurrences of certain salient objects, causing pronounced long-tail distribution. We downsample object categories that occur frequently to prevent extreme object distribution. After the aforementioned filtering, the cue image sets of retained instances are input into Qwen2.5-VL [4], generating object caption and object comprehension QA through various prompts. It is noteworthy that in the object comprehension QA, counting QA task is particularly unique and requires specially designed prompts. Subsequently, based on each instances caption and QAs, Qwen3 [67] generates two types of referring expressions: simple referring expressions and situational referring expressions. Simple referring expressions identify objects through combination of features such as spatial location and category. Situational referring expressions establish task scenario, requiring the model to infer the instance needed by the user within this context. Each type of QA undergoes manual filtering post-output to ensure data quality. Detailed prompts are provided in the Appendix A.2. 3.1.3 Spatial QA Generation Unlike object QA, spatial QA requires more precise 3D information concerning the global scene context. Therefore, we utilize MASt3R-SLAM [44] to reconstruct 3D point clouds from RGB videos and obtain camera extrinsic parameters. Subsequently, by projecting 2D pixel points to 3D coordinates, the segmentation of each instance in the video can be mapped onto the point cloud. However, it is important to note that the world coordinate system established by MASt3R-SLAM for the 3D point cloud is not aligned with the floor. Therefore, the Random Sample Consensus (RANSAC) [19] algorithm is implemented to identify inlier points for plane fitting through ten iterative executions. In each iteration, the detected planar surface and its inliers are removed from the point cloud for subsequent plane detection. Given that the initial camera pose was approximately horizontal but not perpendicular to the ground, the ground plane is selected based on minimal angular deviation between its normal vector and the initial camera Y-axis orientation. The point cloud is then aligned to ensure orthogonality between the world coordinate Z-axis and the detected ground plane. RynnEC dataset encompasses 10 fundamental spatial abilities, each of which is further divided into quantitative and qualitative variants. We construct spatial QA in template-based manner. Diverse QA templates are designed according to the characteristics of each task, and the missing attributes within the templates (e.g., distance, height) can be calculated from the 3D point cloud. We denote each instance in the format <Object X>. Furthermore, to obtain purely textual spatial QA pairs, we replace <Object X> with simple referring expressions generated in the above object QA pipeline. These texts are then further refined and diversified using GPT-4o, resulting in the final natural language spatial QA data. With training on these data, RynnEC is able to answer spatial questions in various input forms. Examples of the generated spatial QAs are illustrated in Fig. 2, and more examples as well as detailed templates are provided in the Appendix A.3. Figure 3 Overview of embodied cognition dimensions in RynnEC-Bench. RynnEC-Bench includes two subsets: object cognition and spatial cognition, evaluating total of 22 embodied cognitive abilities. Building on insights from prior works [61, 47], we recognize that spatial cognition tasks are highly challenging. Therefore, in addition to constructing large-scale video-based spatial QA dataset, we also develop relatively simpler image-based spatial QA dataset. This combination of tasks with varying levels of difficulty is intended to improve learning efficiency and enhance model robustness. Specifically, we collect 500k indoor images from 39k houses. Leveraging the single-image-to-3D reconstruction and calibration methods from SpatialRGPT [10], we obtain the 3D spatial relationships between objects in each image. We then select tasks from the video-based spatial cognition set that can also be addressed via single images, and design corresponding QA templates. The format of the image-based spatial QA is kept consistent with that of the video-based spatial QA."
        },
        {
            "title": "3.2 RynnEC-Bench",
            "content": "As this work is the first to propose comprehensive set of fine-grained embodied video tasks, robust evaluation framework for assessing MLLMs overall capabilities in this domain is currently lacking. To address this, we propose RynnEC-Bench, which evaluates fine-grained embodied understanding models from the perspectives of object cognition and spatial cognition in open-world scenarios. Fig. 3 provides detailed illustration of the capability taxonomy in RynnEC-Bench. 3.2.1 Capability Taxonomy Object cognition is divided into two tasks: object properties cognition and referring object segmentation. During embodied task execution, robots often require clear understanding of key objects functions, locations, quantities, surface details, relationships with the surrounding environment, etc. Accordingly, the object properties recognition tasks comprehensively and meticulously construct questions in these aspects. In the processes of robotic manipulation and navigation, identifying operation instances and target instances is an essential step. Precise instance segmentation in videos serves as the best approach to indicate the positions of these key objects. Specifically, the referring object segmentation task is categorized into direct referring problems and situational referring problems. Direct referring problems involve only combinations of descriptions for the instance, while situational referring problems are set within scenario, requiring MLLMs to perform reasoning in order to identify the target object. Spatial cognition requires MLLMs to derive 3D spatial awareness from egocentric video. We categorize it into ego-centric and world-centric spatial cognition. Ego-centric spatial cognition maintains awareness of agent-environment spatial relations and supports spatial reasoning and mental simulation; by temporal scope, we consider past, present, and future cases. World-centric spatial cognition focuses on understanding the 3D layout and scale of the physical world, which we further evaluate in terms of size, distance, and positional relations. 3.2.2 Data Balance The videos in RynnEC-Bench are collected from ten houses that do not overlap with those in the training set. When evaluating object cognition, we observe substantial variation in object-category distributions across houses, making results highly sensitive to which houses are sampled. To mitigate this bias and better reflect real-world deployment, we introduce physical-world-based evaluation protocol. We first define taxonomy of 12 coarse and 119 fine-grained indoor object categories. Using GPT-4o, we then estimate an empirical category-frequency distribution by parsing 500,000 indoor images from 39,000 houses; given the scale, this serves as close approximation to real-world indoor object frequencies. Finally, we perform frequency-proportional sampling so that the object-category distribution in RynnEC-Bench closely matches the empirical distribution, enabling more objective and realistic evaluation. Specifically, counting questions with answers of 1 or 2 are reduced by 50% to achieve more balanced difficulty distribution. All QA pairs in RynnEC-Bench are further subjected to meticulous human screening to ensure high quality. Additional implementation details are available in Appendix B. 3.2.3 Evaluation Framework The questions are categorized into three types based on the nature of their answers: numerical questions, textual questions, and segmentation questions. For numerical questions such as distance estimation and direction estimation, we directly use the formula to calculate the precise indicators. For scale-related questions, Mean Relative Accuracy (MRA) [69, 16] is used to calculate the scores. Specifically, given models prediction ˆy, ground truth y, and confidence threshold θ, relative accuracy is calculated by considering ˆy correct if the relative error rate, defined as ˆy y/y, is less than 1 θ. As single-confidence-threshold accuracy only considers relative error within narrow scope, the MRA averages the relative accuracy across range of confidence thresholds = {0.5, 0.55, . . . , 0.95}: RA = 1 (cid:88) θC (cid:32) ˆy (cid:33) < 1 θ (1) where I() is the indicator function. For angle-related questions, MRA is not suitable due to the cyclic nature of angular measurements. We therefore designed rotational accuracy metric (RoA). RoA = 1 min (cid:32) min((cid:98)y y, 360 (cid:98)y y) 90 (cid:33) , 1 (2) RoA assigns score only when the angular difference is less than 90 degrees, ensuring consistency in task difficulty across different settings. Textual questions are further categorized into close-ended and open-ended questions. For the close-ended part, we prompt GPT-4o to assign straightforward binary score of either 0 or 1. For the open-ended part, answers are scored by GPT-4o on scale from 0 to 1 in increments of 0.2. This question-type-adaptive evaluation approach enables the metrics of RynnEC-Bench to be both precise and consistent. For segmentation evaluation, prior work [72, 65] typically reports the &F measure, combining region-overlap (J ) and boundary-accuracy (F) scores. However, the conventional frame-averaged &F treats empty frames (i.e., frames with no ground-truth mask) in binary manner: if any predicted mask appears, the frame score is set to 0; otherwise it is set to 1. This evaluation method fails to account for the actual size of erroneous masks in empty frames, which can have significant impact on embodied segmentation tasks. To address this, we propose the Global IoU metric, defined as = (cid:80)N (cid:80)N i=1 Si Gi i=1 Si Gi , (3) Figure 4 Training paradigm of RynnEC. The model is trained in four progressive stages: 1) Mask Alignment, 2) Object Understanding, 3) Spatial Understanding, and 4) Referring Segmentation. where is the total number of video frames, Si denotes the predicted segmentation mask for frame i, and Gi denotes the ground truth mask for frame i. For the boundary accuracy metric F, we compute the average only over non-empty frames. The mean of and F, denoted as &F, provides an accurate reflection of segmentation quality, especially in egocentric videos where the target object appears in relatively few frames."
        },
        {
            "title": "3.3 RynnEC Architecture",
            "content": "RynnEC consists of three core components: the foundational vision-language model for basic multimodal comprehension, region-aware encoder for fine-grained object-centric representation learning, an adaptive mask decoder for video segmentation tasks. Notably, the latter two modules are designed as plug-and-play components with independent parameter spaces, ensuring architectural flexibility and modular extensibility. Foundational Vision-Language Model. We ultilize VideoLLaMA3-Image [76] as the foundational vision-language model for RynnEC, which consists of three main modules: Vision Encoder, the Projector and the Large Language Model (LLM). For the vision encoder, we use VL3-SigLIP-NaViT [76], which leverages an anyresolution vision tokenization strategy to flexibly encode images of varying resolutions. As the LLM, we employ Qwen2.5-1.5B-Instruct [66] and Qwen2.5-7B-Instruct [66], enabling scalable trade-offs between performance and computational cost. Region Encoder. Egocentric videos often feature cluttered scenes with similar objects that are difficult to distinguish using linguistic cues alone. To address this, we introduce dedicated object encoder for specific object representation. This facilitates more precise cross-modal alignment during training and enables intuitive, fine-grained user interaction at inference time. Following [73, 75], we use simple yet efficient MaskPooling for object tokenization, followed by two-layer projector to align object features with LLM embedding space. During training, object masks spanning multiple frames in video are utilized to achieve accurate representations. At inference, the encoder offers flexibility, operating effectively with either single-frame or multi-frame object masks. Mask Decoder. Accurate object localization is critical for egocentric video understanding. To incorporate robust visual grounding capabilities without degrading the models pretrained performance, we fine-tune the LLM with LoRA. Our mask decoder is based on the architecture of SAM2 [51], which has demonstrated strong generalization capabilities and prior knowledge in purely visual segmentation tasks. For given video and the instruction, we adpot [SEG] token as specifical token to trigger mask generation for the corresponding visual region. To facilitate this process, an additional linear layer is introduced to align the [SEG] token with SAM2s feature space."
        },
        {
            "title": "3.4 Training and Inference",
            "content": "As illustrated in Fig. 4, RynnEC is trained using progressive four-stage pipeline: 1) Mask Alignment, 2) Object Understanding, 3) Spatial Understanding, and 4) Referring Segmentation. The first three stages are designed to incrementally enhance fine-grained, object-centric understanding, while the final stage focuses on equipping the model with precise object-level segmentation capabilities. This curriculum-based approach Table 1 Datasets used at four training stages. IM and OM indicate whether the task involves the input mask and output mask, respectively. Training Stage Task Mask Alignment (Stage-1) General Mask Captioning Scene Instance Captioning Object Understanding (Stage-2) Basic Properties QA Object-Centric Counting Spatial Understanding (Stage-3) Our Stage-2 Spatial QA General VQA Referring Segmentation (Stage-4) Our Stage-2 & Stage-3 General Segmentation Embodied Segmentation General VQA IM (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:37) (cid:37) (cid:33) (cid:37) (cid:37) (cid:37) OM (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:33) (cid:33) (cid:37) # Samples Datasets 1.17M RefCOCO [71, 42], VideoRefer-Caption [75], DAM [32], Osprey-Caption [73], MDVPData [34], HC-STVG [56] 0.14M RynnEC-Caption 1.49M RynnEC-Object 0.25M RynnEC-Counting 0.30M RynnEC-Object, RynnEC-Counting 0.60M RynnEC-Spatial (Image), RynnEC-Spatial (Video) 0.54M VLM-3R-Data [17] 0.74M LLaVA-OV-SI [27], [84], ShareGPT-4o-video [8], VideoGPT-plus [40], FineVideo [18], CinePile [52], ActivityNet [5], YouCook2 [86], LLaVA-SFT [36] LLaVA-Video 0.60M RynnEC-Object, RynnEC-Spatial RynnEC-Counting, 0.32M ADE20K [85], COCOStuff [6], Mapillary [45], PACO-LVIS [50], PASCAL-Part [9] 0.31M RynnEC-Segmentation 0.80M LLaVA-OV-SI [27], [84], ShareGPT-4o-video [8], VideoGPT-plus [40], FineVideo [18], CinePile [52], ActivityNet [5], YouCook2 [86], LLaVA-SFT [36] LLaVA-Video ensures gradual integration of visual, spatial, and grounding knowledge without overfitting to single task. The datasets used in each stage are summarized in Tab. 1. The details of each training stage are as follows: 1) Mask Alignment. The goal of this initial stage is to encourage the model to attend to region-specific tokens rather than relying solely on global visual features. We fine-tune both the region encoder and the LLM on large-scale object-level captioning dataset, where each caption is explicitly aligned with specific object mask. This alignment training conditions the model to associate object-centric embeddings with corresponding linguistic descriptions, laying the foundation for localized reasoning in later stages. 2) Object Understanding. In this stage, the focus shifts to enriching the models egocentric object knowledge, encompassing attributes such as color, shape, material, size, and functional properties. The region encoder and the LLM are jointly fine-tuned to integrate this object-level information more effectively into the cross-modal embedding space. This stage is the basic for spatial understanding. 3) Spatial Understanding. Building on the previous stage, this phase equips the model with spatial reasoning abilities, enabling it to understand and reason about the relative positions and configurations of objects within scene. We use large amount of spatial QA we generated and the previous stage data as well as general VQA to maintain the ability to follow instructions. 4) Referring Segmentation. In the final stage, we integrate the Mask Decoder module after the LLM to endow the model with fine-grained referring segmentation capabilities. The LLM is fine-tuned via LoRA to minimize interference with its pretrained reasoning abilities. The training data includes not only segmentationspecific datasets but also samples from earlier stages to mitigate catastrophic forgetting. This multi-task mixture ensures that segmentation performance is improved without sacrificing the models object and spatial understanding. Table 2 Main evaluation results on RynnEC-Bench. We evaluate in two major categories: Object Cognition and Spatial Cognition. DR and SR represent Direct Referring and Situational Referring. PR represents Positional Relationship. Model Overall Mean GPT-4o [46] GPT-4.1 [46] Seed1.5-VL [20] Genimi-2.5 Pro [12] VideoLLaMA3-7B [76] InternVL3-78B [87] Qwen2.5-VL-72B [4] DAM-3B [32] VideoRefer-VL3-7B [75] Sa2VA-4B [72] VideoGlaMM-4B [43] RGA3-7B [59] RoboBrain-2.0-32B [57] RynnEC-2B RynnEC-7B 28.3 33.5 34.7 45.5 27.3 29.0 36.4 15.6 32.9 4.9 9.0 10.5 24.2 54.4 56. Object Cognition Spatial Cognition Object Properties Segmentation DR SR Mean Ego-Centric Pres. His. Fut. World-Centric Dis. PR Size Proprietary Generalist MLLMs 22.8 27.6 27.7 36.7 26.8 31.8 24.8 41.1 45.9 52.1 64. 36.7 45.3 54.2 33.9 37.8 42.8 52.7 13.4 17.2 8.2 9.3 Open-source Generalist MLLMs 5.1 9.0 11.3 Open-source Object-Level MLLMs 2.8 5.8 30.2 37.3 44. 14.1 29.0 18.3 36. 22.2 44.1 Referring Video Object Segmentation MLLMs 0.0 4.7 5.5 0.0 4.1 0.0 Open-source Embodied MLLMs 8.8 30.1 9.4 14.4 17.5 14.8 4.2 23.4 5.9 16.4 15.2 35.3 5.8 32. 20.7 56.3 34.1 47.2 46.2 45.3 36.9 36.1 25.1 59.3 61.4 57. 40.9 50.2 6.0 6.1 4.3 8.1 1.2 2.2 7.2 1.3 6.1 1.3 1.4 6. 24.3 35.9 32.9 47.0 30.0 10.9 27.2 28.7 38.1 0.0 0.8 1.2 16.7 30.4 19.1 29.9 19.0 30.9 22. 6.1 30.7 0.0 0.0 0.9 36.1 45.7 27.9 69.3 34.9 26.0 83.7 18.3 28.8 0.0 0.3 0. Mean 22.2 28.8 26.1 37.8 24.1 20.0 27.4 12.6 29.3 0.0 3.2 3.0 0. 23.8 22.3 37.2 67.4 67.1 30.4 31.2 3.6 85.8 28.0 52. 39.2 89.7 54."
        },
        {
            "title": "4.1 Implementation Details",
            "content": "4.1.1 Training In this part, we briefly introduce the implementation details of each training stage. For all stages, we adopt the cosine learning rate scheduler. The warm up ratio of the learning rate is set as 0.03. The maximum token length is set to 16384, while the maximum token length for vision tokens is set to 8192. In Stage 1, both the vision encoder and the LLM are initialized with pretrained weights from VideoLLaMA3-Image. During this stage, we train the LLM, the projector, and the region encoder, using learning rates of 1 105, 1 105, and 4 105, respectively. In Stages 2 and 3, the learning rates for the LLM, projector, and region encoder are adjusted to 4 105, 1 105, and 1 105, respectively. In the final stage, the LLM is fine-tuned using LoRA with the same learning rates as in Stage 3. The learning rate of Mask Decoder is set to 4 105. 4.1.2 Evaluation We present comprehensive evaluation of five MLLM categories on RynnEC-Bench, including both generalpurpose models and those fine-tuned for region-level understanding and segmentation. For models that do not accept direct region-based inputs, we uniformly highlight target objects using bounding boxes in the video. Multiple objects are distinguished by different colored boxes, which are referenced in the question prompt. We observe that general-purpose MLLMs are incapable of localizing objects in videos; thus, only specialist models fine-tuned for this ability are evaluated on the RynnEC-Bench segmentation subset. To ensure consistent evaluation protocol, videos are sampled at 1 fps up to maximum of 30 frames. If the initial sampling exceeds the 30-frame limit, these target-containing frames are kept, and the remaining frames are selected via uniform sampling from the rest of the video. Figure 5 More granular assessments of object cognition and spatial cognition. We compare the best-performing MLLM from each category with our RynnEC-7B."
        },
        {
            "title": "4.2 Embodied Cognition Evaluation",
            "content": "4.2.1 Main Results Tab. 2 presents the evaluation results of our RynnEC model and five categories of related MLLMs on the RynnEC-Bench. Although the RynnEC model contains only 7B parameters, it demonstrates robust embodied cognitive abilities, outperforming even the most advanced proprietary model, Gemini-2.5 Pro [12], by 10.7 points. Moreover, RynnEC achieves both balanced and superior performance across various tasks. For object cognition, RynnEC achieved score of 61.4 and possesses the ability to both understand and segment objects. In terms of spatial cognition, RynnEC achieves score of 54.5, which is 44.2% higher than that of Gemini-2.5 Pro. To support resource-constrained settings, we present 2B-parameter RynnEC that delivers markedly lower inference latency while maintaining near-parity performance (< 2 percentage points drop), enabling on-device deployment for embodied applications. In the following sections, we will introduce the performance of different types of MLLMs on RynnEC-Bench in detail. Proprietary Generalist MLLMs Among the four leading proprietary generalist MLLMs evaluated, Gemini-2.5 Pro establishes clear lead with an overall score of 45.5. This represents substantial performance margin of 25% over the best open-source generalist MLLM and 38.3% over the premier open-source object-level MLLM. Even more notably, it achieves remarkable score of 37.8 in the notoriously difficult domain of spatial cognition. This finding provides compelling evidence that spatial awareness can emerge as byproduct of extensive training on video comprehension tasks. Open-source Generalist MLLMs Qwen2.5-VL-72B [4] exhibits outstanding performance, achieving score of 36.4 and surpassing GPT-4.1 [46]. This suggests that, in specialized capabilities such as embodied cognition, the gap between open-source and proprietary MLLMs has been significantly narrowed. Furthermore, we observe that Qwen2.5-VL and InternVL3 [87] demonstrate superior performance in positional relationship (PR) and distance perception tasks, respectively, even outperforming Gemini-2.5 Pro. Such pronounced differences in various aspects of spatial cognition may be attributed to the distribution of training data. Open-source Object-Level MLLMs These MLLMs are capable of accepting region masks as input, enabling more direct localization of target objects and facilitating finer-grained object perception. VideoRefer-VL3-7B [75] is model fine-tuned from the base model VideoLLaMA3-7B [76]. As shown in Tab. 2, VideoRefer-VL37B consistently outperforms VideoLLaMA3-7B in both object cognition and spatial cognition tasks. This demonstrates that, in embodied scenarios, integrating mask understanding within the model is superior to explicit visual prompting. Models Qwen2.5-VL-7B [4] InternVL3-8B [87] GPT-4o [46] Magma-8B [68] Cosmos-Reason1-7B [3] VeBrain-8B [37] RoboBrain-7B-1.0 [25] RoboBrain-7B-2.0 [57] M2-Reasoning-7B [1] RynnEC-7B VSI-Bench 35.9 42.1 43.6 12.7 25.6 26.3 31.1 36.1 42.3 45.8 Figure 6 Performance on VSI-Bench [69]. Left: per-subtask comparison with VideoLLaMA3, the base model of our RynnEC. Right: overall comparison with generalist MLLMs and embodied MLLMs without explicit 3D encoding. Referring Video Object Segmentation MLLMs Recently, several studies have applied MLLMs to object segmentation tasks while retaining the original multimodal understanding capabilities of MLLMs. However, the best-performing model, RGA3-7B [59], achieves only 15.2 points on the object properties task. Although these MLLMs can still address some general video understanding tasks, their task generalization ability is significantly diminished following segmentation training. In contrast, our RynnEC model, which is specifically designed for embodied scenarios, maintains strong object and spatial understanding capabilities even after segmentation training. Open-source Embodied MLLMs With the growing demand for highly generalizable cognitive abilities in the field of embodied intelligence, number of studies have begun to develop MLLMs specifically tailored for embodied scenarios. representative model is RoboBrain-2.0 [57], which achieves 24.2 even worse than general-purpose video models such as VideoLLaMA3-7B. There are two primary reasons for this: (1) Loss of object cognition: Embodied MLLMs typically emphasize spatial perception and task planning abilities, but tend to overlook the importance of detailed object understanding. (2) Lack of fine-grained perceptual capability: In egocentric videos, RoboBrain-2.0 demonstrates limited ability to interpret region-level features. 4.2.2 Object Cognition Fig. 5 (a) presents more comprehensive evaluation of RynnECs capability in object properties cognition from multiple dimensions. Since most object properties cognition abilities are encompassed by general video understanding skills, Gemini-2.5-Pro exhibits superior performance across various competencies. However, due to the high edge deployment requirements of embodied MLLMs, the inference speed of these large-scale models becomes bottleneck. With only 7B parameters, RynnEC achieves object properties cognition comparable to that of Gemini-2.5-Pro in most categories. Notably, for attributes such as surface details, object state, and object shape, RynnEC-2B even surpasses all other MLLMs. Moreover, most MLLMs lack video object segmentation capabilities, whereas dedicated segmentation MLLMs often sacrifice understanding abilities. RynnEC, while maintaining strong comprehension capabilities, achieves 30.9% and 57.7% improvements over state-of-the-art segmentation MLLMs in direct referring and situational referring object segmentation tasks, respectively. 4.2.3 Spatial Cognition Fig. 5 (b) demonstrates RynnECs spatial cognition capabilities through more fine-grained tasks. As spatial abilities have not been formally defined or systematically explored in previous work, different MLLMs only exhibit strengths in limited set of specific skills. Overall, spatial cognition abilities such as Spatial Imagery, Movement Imagery, and Trajectory Review are typically absent in prior MLLMs. In contrast, RynnEC possesses more comprehensive set of spatial abilities, which can facilitate embodied agents in developing spatial awareness within complex environments. Figure 7 The example of RynnEC assisting robots in performing long-range tasks. The robot accomplishes the two designated tasks within the RoboTHOR simulator [14]. RynnEC facilitates the robot in achieving fine-grained environmental cognition throughout the task execution."
        },
        {
            "title": "4.3 Generalization and Scalability",
            "content": "To investigate the generalizability of RynnEC, we conduct experiments on VSI-Bench [69], purely textual spatial intelligence benchmark. As shown in Fig. 6, RynnEC-7B consistently surpasses VideoLLaMA3-7B across almost all capability dimensions. Notably, RynnEC is trained with mask-centric spatial awareness paradigm, whereas all tasks in VSI-Bench involve purely textual spatial reasoning. This demonstrates that spatial awareness need not be constrained by the modality of representation, and spatial reasoning abilities can be effectively transferred across modalities. Further observation reveals substantial performance gains of RynnEC on the Route Planning task, despite this task not being included during training. This indicates that the navigation performance of embodied agents is currently constrained by foundational spatial perception capabilities, such as the understanding of direction, distance, and spatial relationships. Only with robust foundational spatial cognition can large embodied models achieve superior performance in high-level planning and decision-making tasks. Compared to other embodied MLLMs of comparable size, RynnEC-7B also achieves leading score of 45.8. Certain tasks, such as object segmentation and movement imagery, remain significant challenges for RynnEC. We hypothesize that the suboptimal performance on these tasks stems primarily from insufficient training data. To validate this, we conduct an empirical analysis of data scalability across different task categories. As the data volume increases progressively from 20% to 100%, the models performance on all tasks improves steadily. This observation motivates further expansion of the dataset to enhance RynnECs spatial reasoning capabilities. However, it is noteworthy that the marginal gains diminish as data volume grows, indicating decreasing return on scale. Investigating strategies to enhance data diversity in order to sustain this scaling behavior remains critical open challenge for future research."
        },
        {
            "title": "4.4 Embodied Application",
            "content": "Recently, some works [11, 63] have leveraged MLLMs as the \"brain\" to assist robots in planning tasks, perceiving environments, and making decisions. However, current MLLMs lack key capabilities such as spatial awareness, fine-grained perception, and instance localization, which restricts these applications to limited and simple tasks. As illustrated in Fig. 7, RynnEC demonstrates the potential to assist robots in accomplishing long-horizon tasks within complex environments. From two real-time tasks performed by the robot equipped with RynnEC, we observe the following roles that RynnEC plays in task execution: (1) Fine-grained object localization and understanding enable robots to more quickly identify target objects and assess their states; (2) Direction and distance perception of targets improves navigation efficiency and precision; (3) Spatial scale estimation empowers robots to perform more delicate manipulations; (4) Counting ability facilitates the completion of tasks requiring mathematical reasoning. It is important to emphasize that the role of RynnEC in embodied tasks is far from limited to these examples. We hope that more researchers will integrate RynnEC models into robotic systems across wide range of tasks, thereby advancing embodied intelligence toward more valuable real-world applications."
        },
        {
            "title": "5 Conclusion and Future Works",
            "content": "In this paper, we introduce RynnEC, Video MLLM for embodied cognition. Through the architectural design of region encoder and mask decoder, RynnEC achieves flexible, fine-grained visual interaction. Meanwhile, RynnEC demonstrates robust object and spatial cognitive abilities with compact size. To address the limitations of available scene data, we employ data generation pipeline that relies solely on RGB videos. Furthermore, to supplement the lack of fine-grained embodied cognition benchmarks, we propose RynnEC-Bench, which covers 22 categories of object and spatial cognitive abilities. During training, RynnEC progressively integrates diverse skills through four-stage capability injection process. Importantly, we advocate that fine-grained video-based visual understanding is key to achieving generalizable cognition in the physical world. RynnEC will enable robots to accomplish more precise cognitive tasks, thereby advancing the practical development of embodied intelligence. We regard RynnEC as foundational step toward developing general embodied intelligence model. Looking ahead, we plan to further advance RynnEC along two primary directions. Enhancing Reasoning Capabilities: Robust visual reasoning is essential for solving any complex embodied task. An important research direction is how to effectively integrate RynnECs diverse abilities to perform joint reasoning, thereby enabling the resolution of higher-level embodied problems. Unified Perception and Planning Framework: Recent studies [57] have started to explore training unified embodied intelligence models that combine perception and planning. However, these approaches are limited in their ability to facilitate fine-grained, video-based visual interactions. In the future, we aim to endow RynnEC with more flexible planning abilities and integrate it with VLA models to form closed-loop embodied system."
        },
        {
            "title": "References",
            "content": "[1] Inclusion AI, Fudong Wang, Jiajia Liu, Jingdong Chen, Jun Zhou, Kaixiang Ji, Lixiang Ru, Qingpei Guo, Ruobing Zheng, Tianqi Li, et al. M2-reasoning: Empowering mllms with unified general and spatial reasoning. arXiv preprint arXiv:2507.08306, 2025. [2] Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Deyao Zhu, Jian Ding, and Mohamed Elhoseiny. Minigpt4-video: Advancing multimodal llms for video understanding with interleaved visual-textual tokens. arXiv preprint arXiv:2404.03413, 2024. [3] Alisson Azzolini, Junjie Bai, Hannah Brandon, Jiaxin Cao, Prithvijit Chattopadhyay, Huayu Chen, Jinju Chu, Yin Cui, Jenna Diamond, Yifan Ding, et al. Cosmos-reason1: From physical common sense to embodied reasoning. arXiv preprint arXiv:2503.15558, 2025. [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. https://arxiv.org/abs/2502.13923. [5] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: large-scale video benchmark for human activity understanding. In Proceedings of the ieee conference on computer vision and pattern recognition, pages 961970, 2015. [6] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 12091218, 2018. [7] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1445514465, 2024. [8] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Zhenyu Tang, Li Yuan, et al. Sharegpt4video: Improving video understanding and generation with better captions. Advances in Neural Information Processing Systems, 37:1947219495, 2024. [9] Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fidler, Raquel Urtasun, and Alan Yuille. Detect what you can: Detecting and representing objects using holistic models and body parts. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 19711978, 2014. [10] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision language models. arXiv preprint arXiv:2406.01584, 2024. [11] Kai Cheng, Zhengyuan Li, Xingpeng Sun, Byung-Cheol Min, Amrit Singh Bedi, and Aniket Bera. Efficienteqa: An efficient approach for open vocabulary embodied question answering. arXiv preprint arXiv:2410.20263, 2024. [12] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. [13] Ronghao Dang, Yuqian Yuan, Wenqi Zhang, Yifei Xin, Boqiang Zhang, Long Li, Liuyi Wang, Qinyang Zeng, Xin Li, and Lidong Bing. Ecbench: Can multi-modal foundation models understand the egocentric world? holistic embodied cognition benchmark. arXiv preprint arXiv:2501.05031, 2025. [14] Matt Deitke, Winson Han, Alvaro Herrasti, Aniruddha Kembhavi, Eric Kolve, Roozbeh Mottaghi, Jordi Salvador, Dustin Schwenk, Eli VanderBilt, Matthew Wallingford, Luca Weihs, Mark Yatskar, and Ali Farhadi. Robothor: An open simulation-to-real embodied ai platform, 2020. [15] Sivan Doveh, Shaked Perek, Jehanzeb Mirza, Wei Lin, Amit Alfassy, Assaf Arbelle, Shimon Ullman, and Leonid Karlinsky. Towards multimodal in-context learning for vision and language models. In European Conference on Computer Vision, pages 250267. Springer, 2025. [16] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88:303338, 2010. [17] Zhiwen Fan, Jian Zhang, Renjie Li, Junge Zhang, Runjin Chen, Hezhen Hu, Kevin Wang, Huaizhi Qu, Dilin Wang, Zhicheng Yan, et al. Vlm-3r: Vision-language models augmented with instruction-aligned 3d reconstruction. arXiv preprint arXiv:2505.20279, 2025. [18] Miquel Farré, Andi Marafioti, Lewis Tunstall, Leandro Von Werra, and Thomas Wolf. Finevideo. https: //huggingface.co/datasets/HuggingFaceFV/finevideo, 2024. [19] Martin Fischler and Robert Bolles. Random sample consensus: paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM, 24(6):381395, 1981. [20] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1.5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. [21] Xiaofeng Han, Shunpeng Chen, Zenghuang Fu, Zhe Feng, Lue Fan, Dong An, Changwei Wang, Li Guo, Weiliang Meng, Xiaopeng Zhang, et al. Multimodal fusion and vision-language models: survey for robot vision. arXiv preprint arXiv:2504.02477, 2025. [22] Xiaofeng Han, Shunpeng Chen, Zenghuang Fu, Zhe Feng, Lue Fan, Dong An, Changwei Wang, Li Guo, Weiliang Meng, Xiaopeng Zhang, et al. Multimodal fusion and vision-language models: survey for robot vision. arXiv preprint arXiv:2504.02477, 2025. [23] Fangzhou Hong, Vladimir Guzov, Hyo Jin Kim, Yuting Ye, Richard Newcombe, Ziwei Liu, and Lingni Ma. Egolm: Multi-modal language model of egocentric motions. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 53445354, 2025. [24] Xiaolu Hou, Mingcheng Li, Dingkang Yang, Jiawei Chen, Ziyun Qian, Xiao Zhao, Yue Jiang, Jinjie Wei, Qingyao Xu, and Lihua Zhang. Bloomscene: Lightweight structured 3d gaussian splatting for crossmodal scene generation. arXiv preprint arXiv:2501.10462, 2025. [25] Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, et al. Robobrain: unified brain model for robotic manipulation from abstract to concrete. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 17241734, 2025. [26] Shiyu Jin, Jinxuan Xu, Yutian Lei, and Liangjun Zhang. Reasoning grasping via multimodal large language model. arXiv preprint arXiv:2402.06798, 2024. [27] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [28] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2219522206, 2024. [29] Xiaoqi Li, Mingxu Zhang, Yiran Geng, Haoran Geng, Yuxing Long, Yan Shen, Renrui Zhang, Jiaming Liu, and Hao Dong. Manipllm: Embodied multimodal large language model for object-centric robotic manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1806118070, 2024. [30] Yifan Li, Yuhang Chen, Anh Dao, Lichi Li, Zhongyi Cai, Zhen Tan, Tianlong Chen, and Yu Kong. Industryeqa: Pushing the frontiers of embodied question answering in industrial scenarios. arXiv preprint arXiv:2505.20640, 2025. [31] Yun Li, Yiming Zhang, Tao Lin, XiangRui Liu, Wenxiao Cai, Zheng Liu, and Bo Zhao. Sti-bench: Are mllms ready for precise spatial-temporal world understanding? arXiv preprint arXiv:2503.23765, 2025. [32] Long Lian, Yifan Ding, Yunhao Ge, Sifei Liu, Hanzi Mao, Boyi Li, Marco Pavone, Ming-Yu Liu, Trevor Darrell, Adam Yala, et al. Describe anything: Detailed localized image and video captioning. arXiv preprint arXiv:2504.16072, 2025. [33] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. [34] Weifeng Lin, Xinyu Wei, Ruichuan An, Peng Gao, Bocheng Zou, Yulin Luo, Siyuan Huang, Shanghang Zhang, and Hongsheng Li. Draw-and-understand: Leveraging visual prompts to enable mllms to comprehend what you want. arXiv preprint arXiv:2403.20271, 2024. [35] Weifeng Lin, Xinyu Wei, Ruichuan An, Tianhe Ren, Tingwei Chen, Renrui Zhang, Ziyu Guo, Wentao Zhang, Lei Zhang, and Hongsheng Li. Perceive anything: Recognize, explain, caption, and segment anything in images and videos, 2025. [36] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [37] Gen Luo, Ganlin Yang, Ziyang Gong, Guanzhou Chen, Haonan Duan, Erfei Cui, Ronglei Tong, Zhi Hou, Tianyi Zhang, Zhe Chen, et al. Visual embodied brain: Let multimodal large language models see, think, and control in spaces. arXiv preprint arXiv:2506.00123, 2025. [38] Ruiyuan Lyu, Jingli Lin, Tai Wang, Xiaohan Mao, Yilun Chen, Runsen Xu, Haifeng Huang, Chenming Zhu, Dahua Lin, and Jiangmiao Pang. Mmscan: multi-modal 3d scene dataset with hierarchical grounded language annotations. Advances in Neural Information Processing Systems, 37:5089850924, 2024. [39] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. [40] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Khan. Videogpt+: Integrating image and video encoders for enhanced video understanding. arXiv preprint arXiv:2406.09418, 2024. [41] Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, et al. Openeqa: Embodied question answering in the era of foundation models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1648816498, 2024. [42] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1120, 2016. [43] Shehan Munasinghe, Hanan Gani, Wenqi Zhu, Jiale Cao, Eric Xing, Fahad Shahbaz Khan, and Salman Khan. Videoglamm: large multimodal model for pixel-level visual grounding in videos. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1903619046, 2025. [44] Riku Murai, Eric Dexheimer, and Andrew Davison. Mast3r-slam: Real-time dense slam with 3d reconstruction priors. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1669516705, 2025. [45] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and Peter Kontschieder. The mapillary vistas dataset for semantic understanding of street scenes. In Proceedings of the IEEE international conference on computer vision, pages 49904999, 2017. [46] OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, and Alan Hayes. Gpt-4o system card, 2024. [47] Kun Ouyang, Yuanxin Liu, Haoning Wu, Yi Liu, Hao Zhou, Jie Zhou, Fandong Meng, and Xu Sun. Spacer: Reinforcing mllms in video spatial reasoning, 2025. https://arxiv.org/abs/2504.01805. [48] Achille Pasqualotto and Michael Proulx. The role of visual experience for the neural basis of spatial cognition. Neuroscience & Biobehavioral Reviews, 36(4):11791187, 2012. [49] Zhangyang Qi, Zhixiong Zhang, Ye Fang, Jiaqi Wang, and Hengshuang Zhao. Gpt4scene: Understand 3d scenes from videos with vision-language models. arXiv preprint arXiv:2501.01428, 2025. [50] Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts and attributes of common objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 71417151, 2023. [51] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. [52] Ruchit Rawal, Khalid Saifullah, Miquel Farré, Ronen Basri, David Jacobs, Gowthami Somepalli, and Tom Goldstein. Cinepile: long video question answering dataset and benchmark. arXiv preprint arXiv:2405.08813, 2024. [53] Arijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan Plummer, Ranjay Krishna, Kuo-Hao Zeng, et al. Sat: Spatial aptitude training for multimodal language models. arXiv preprint arXiv:2412.07755, 2024. [54] Allen Ren, Jaden Clark, Anushri Dixit, Masha Itkina, Anirudha Majumdar, and Dorsa Sadigh. Explore until confident: Efficient exploration for embodied question answering. arXiv preprint arXiv:2403.15941, 2024. [55] Tianhe Ren, Qing Jiang, Shilong Liu, Zhaoyang Zeng, Wenlong Liu, Han Gao, Hongjie Huang, Zhengyu Ma, Xiaoke Jiang, Yihao Chen, et al. Grounding dino 1.5: Advance the\" edge\" of open-set object detection. arXiv preprint arXiv:2405.10300, 2024. [56] Zongheng Tang, Yue Liao, Si Liu, Guanbin Li, Xiaojie Jin, Hongxu Jiang, Qian Yu, and Dong Xu. Human-centric spatio-temporal video grounding with visual transformers. IEEE Transactions on Circuits and Systems for Video Technology, 32(12):82388249, 2021. [57] BAAI RoboBrain Team, Mingyu Cao, Huajie Tan, Yuheng Ji, Minglan Lin, Zhiyu Li, Zhou Cao, Pengwei Wang, Enshen Zhou, Yi Han, et al. Robobrain 2.0 technical report. arXiv preprint arXiv:2507.02029, 2025. [58] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, and Damien Vincent. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. [59] Haochen Wang, Qirui Chen, Cilin Yan, Jiayin Cai, Xiaolong Jiang, Yao Hu, Weidi Xie, and Stratis Gavves. Object-centric video question answering with visual grounding and referring. arXiv preprint arXiv:2507.19599, 2025. [60] Tai Wang, Xiaohan Mao, Chenming Zhu, Runsen Xu, Ruiyuan Lyu, Peisen Li, Xiao Chen, Wenwei Zhang, Kai Chen, Tianfan Xue, et al. Embodiedscan: holistic multi-modal 3d perception suite towards embodied ai. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1975719767, 2024. [61] Diankun Wu, Fangfu Liu, Yi-Hsin Hung, and Yueqi Duan. Spatial-mllm: Boosting mllm capabilities in visual-based spatial intelligence. arXiv preprint arXiv:2505.23747, 2025. [62] Jiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng Wan, and Philip Yu. Multimodal large language models: survey. In 2023 IEEE International Conference on Big Data (BigData), pages 22472256. IEEE, 2023. [63] Chuyan Xiong, Chengyu Shen, Xiaoqi Li, Kaichen Zhou, Jiaming Liu, Ruiping Wang, and Hao Dong. Autonomous interactive correction mllm for robust robotic manipulation. In 8th Annual Conference on Robot Learning, 2024. [64] Runsen Xu, Weiyao Wang, Hao Tang, Xingyu Chen, Xiaodong Wang, Fu-Jen Chu, Dahua Lin, Matt Feiszli, and Kevin Liang. Multi-spatialmllm: Multi-frame spatial understanding with multi-modal large language models. arXiv preprint arXiv:2505.17015, 2025. [65] Cilin Yan, Haochen Wang, Shilin Yan, Xiaolong Jiang, Yao Hu, Guoliang Kang, Weidi Xie, and Efstratios Gavves. Visa: Reasoning video object segmentation via large language models. In European Conference on Computer Vision, pages 98115. Springer, 2024. [66] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. [67] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [68] Jianwei Yang, Reuben Tan, Qianhui Wu, Ruijie Zheng, Baolin Peng, Yongyuan Liang, Yu Gu, Mu Cai, Seonghyeon Ye, Joel Jang, et al. Magma: foundation model for multimodal ai agents. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1420314214, 2025. [69] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1063210643, 2025. [70] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: high-fidelity dataset of 3d indoor scenes, 2023. [71] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expressions. In European conference on computer vision, pages 6985. Springer, 2016. [72] Haobo Yuan, Xiangtai Li, Tao Zhang, Zilong Huang, Shilin Xu, Shunping Ji, Yunhai Tong, Lu Qi, Jiashi Feng, and Ming-Hsuan Yang. Sa2va: Marrying sam2 with llava for dense grounded understanding of images and videos. arXiv preprint arXiv:2501.04001, 2025. [73] Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu. Osprey: Pixel understanding with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2820228211, 2024. [74] Yuqian Yuan, Ronghao Dang, Long Li, Wentong Li, Dian Jiao, Xin Li, Deli Zhao, Fan Wang, Wenqiao Zhang, Jun Xiao, and Yueting Zhuang. Eoc-bench: Can mllms identify, recall, and forecast objects in an egocentric world?, 2025. https://arxiv.org/abs/2506.05287. [75] Yuqian Yuan, Hang Zhang, Wentong Li, Zesen Cheng, Boqiang Zhang, Long Li, Xin Li, Deli Zhao, Wenqiao Zhang, Yueting Zhuang, et al. Videorefer suite: Advancing spatial-temporal object understanding with video llm. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1897018980, 2025. [76] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, Peng Jin, Wenqi Zhang, Fan Wang, Lidong Bing, and Deli Zhao. Videollama 3: Frontier multimodal foundation models for image and video understanding, 2025. https://arxiv.org/abs/2501.13106. [77] Duzhen Zhang, Yahan Yu, Jiahua Dong, Chenxing Li, Dan Su, Chenhui Chu, and Dong Yu. Mm-llms: Recent advances in multimodal large language models. arXiv preprint arXiv:2401.13601, 2024. [78] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. [79] Haoyu Zhang, Qiaohui Chu, Meng Liu, Yunxiao Wang, Bin Wen, Fan Yang, Tingting Gao, Di Zhang, Yaowei Wang, and Liqiang Nie. Exo2ego: Exocentric knowledge guided mllm for egocentric video understanding. arXiv preprint arXiv:2503.09143, 2025. [80] Jiahui Zhang, Yurui Chen, Yanpeng Zhou, Yueming Xu, Ze Huang, Jilin Mei, Junhui Chen, Yu-Jie Yuan, Xinyue Cai, Guowei Huang, et al. From flatland to space: Teaching vision-language models to perceive and reason in 3d. arXiv preprint arXiv:2503.22976, 2025. [81] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. [82] Xingxuan Zhang, Jiansheng Li, Wenjing Chu, Junjia Hai, Renzhe Xu, Yuqing Yang, Shikai Guan, Jiazheng Xu, and Peng Cui. On the out-of-distribution generalization of multimodal large language models. arXiv preprint arXiv:2402.06599, 2024. [83] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. [84] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. [85] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 633641, 2017. [86] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. [87] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Xuehui Wang, Yue Cao, Yangzhou Liu, Xingguang Wei, Hongjie Zhang, Haomin Wang, Weiye Xu, Hao Li, and Jiahao Wang. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, 2025. https://arxiv.org/abs/2504.10479."
        },
        {
            "title": "A Implementation Details for Data Pipeline",
            "content": "A.1 Instance Segmentation As described in Section 3.1.1, instance segmentation and tracking in videos require three-stage collaborative process. The first stage involves the extraction of an object list, which should comprehensively include the names of all objects present in the video scene. After evaluating multiple approaches, we find that directly leveraging Qwen2.5-VL to extract object category names from video frames achieves the highest efficiency and accuracy. Specifically, we uniformly sample 16 frames from each video, dividing them into two groups: even-numbered frames and odd-numbered frames. Each group is then processed independently by Qwen2.5-VL to generate list of object category names. The prompt used to guide the extraction of the object list is in Tab. 3. System Prompt: Please analyze the image sequence captured as move through an indoor environment and provide concise list of major distinct physical objects that can be detected and segmented in these scenes. You need to pay attention to the following points (1) Focus on tangible items such as furniture, appliances, and tools. Avoid nouns that denote locations and rooms like \"kitchen\" or \"bedroom\". (2) Limit the list to maximum of 20 objects, and avoid including specific components or parts of these objects. (3) Ensure the list does not have duplicates. Your output must be series of nouns separated by semicolons Table 3 Prompts for object list extraction. During our experimentation, we observe that Qwen2.5-VL occasionally produces repeated instances of the same object name or phrases sharing the same object name as prefix. To address this, we apply post-processing step to remove duplicate and semantically similar phrases from the model outputs, thereby ensuring the diversity and conciseness of the object list. The final object list is obtained by taking the union of the results from the odd-numbered and even-numbered frame groups, yielding more comprehensive and robust set of detected objects. Furthermore, generic scene-level categories such as \"wall\" and \"floor\" are explicitly excluded from the final object list, as they are not considered relevant instances for downstream instance-level tracking and segmentation tasks. A.2 Object QA Generation We generate three categories of object-related tasks: object caption, object comprehension QA, and referring video object segmentation. The pre-annotation prompts for object caption and object comprehension QA are presented in Tab. 4. Both tasks take as input set of keyframes in which the target object is highlighted; the only difference lies in the task-specific instruction prompts. The referring video object segmentation task requires generating unique referring expressions for objects. We aggregate the QAs generated in the previous stage for each object, representing various attributes of the object. Subsequently, Qwen3 utilizes these QAs to generate both direct referring expressions and situational referring expressions. The specific prompt is shown in Tab. 5. Crop Image Prompt: The above four images show crop of the object we need to describe. Bbox Image Prompt: The four images above highlight the target object with red bounding box and dimmed background. Task Prompt: Caption Task: Please provide detailed description of the specified object, focusing on its color, material, shape, state, position, function, surface detail and other information. (1) Stick to narrative format for descriptions, avoiding list-like itemizations. (2) Just output the information you are sure of, if you output the wrong information you will be fired. Comprehension QA Task: need you to generate series of question pairs for me about this object, using <object> to represent the object in the question pairs. You can focus on its category, color, material, shape, state, position, function, surface detail, size and other information. \"Output example\" Question: What color is the <object>? Answer: Mainly red, with some blue as decoration. Notes: (1) The object in all images is the same; QA should focus solely on it, without referencing specific images. (2) Ask as many questions as neededthe more details, the better. (3) Prioritize reasoning and spatial understanding questions over simple ones. (4) You can ask questions about the target object by associating it with the surrounding objects (e.g., comparison, spatial relationship, functional relationship, quantitative relationship, etc.). # Python code together with above text prompts are directly sent to LLaMA messages = [{ \" role \" : \" user \" , \" content \" : [ { \" type \" : \" text \" , \" text \" : \"Crop Image Prompt\"} + crop_image_list + { \" type \" : \" text \" , \" text \" : \"Bbox Image Prompt\"} + bbox_image_list + { \" type \" : \" text \" , \" text \" : \"Task Prompt\"}]}] Table 4 Prompts for object caption and comprehension QA generation. Separate textual instructions are provided for the cropped images and the images highlighting the object via bounding boxes, respectively. System Prompt: You are analyzing indoor objects. Given series of QAs about single object (marked as <object>), use the information to generate two referring expressions that uniquely identify it. The two expressions should be: One simple referring expression, using attributes such as category, color, material, spatial location, or function. One situational referring expression, involving contextual reasoning and diverse sentence structures. Input Example: Question: What is the primary function of the <object>? Answer: The <object> is primarily used for holding writing instruments like pens and pencils. (Additional QA pairs continue in similar fashionomitted for brevity.) Output Example: [simple expression] The cylindrical light brown pen holder on the top shelf of the desk. [complex expression] If finish writing with pencil, where is the best place to store it? Table 5 Prompt for object referring expression generation. A.3 Spatial QA Generation As outlined in Section 3.1.3, we adopt template-based approach for generating spatial QA. Specifically, we define 14 core spatial abilities and create total of 30 distinct templates, with each template encompassing at least three different question structures. Some examples of QA templates are provided in Listing 1. Listing 1 Template examples for Spatial QA generation. e _ t e _ s n = [ \" How far have you walked in total ?\" , \" What is the total distance you have covered walking ?\" , \" What is the overall distance you have walked ?\" ] s _ _ e _ s n = [ \" Which is closer to you , [ ] or [ ]?\" , \" Between [ ] and [ ] , which one is nearer to you ?\" , \" Which one is closer to you , [ ] or [ ]?\" ] s _ _ e _ s n = [ \" Which is closest to you , [ ] or [ ] or [ ]?\" , \" Among [ ] , [ ] , and [ ] , which one is nearer to you ?\" , \" Which of [ ] , [ ] , or [ ] is closest to you ?\" ] u _ e o _ e _ s n = [ \" After you turn 90 degrees to the left , where will [ ] be in relation to you ?\" , \" If you turn left by 90 - degree , in which direction will [ ] be positioned ?\" , \" Upon making 90 - degree left turn , how will [ ] be oriented with respect to you ?\" ] u _ e o _ e _ a _ s n = [ \" How many degrees clockwise do you need to turn to face the direction of [ ]?\" , \" To face towards [ ] , how many degrees should you rotate in clockwise direction ?\" , \" What degree of clockwise rotation is necessary for you to face [ ] direction ?\" ] distance_que st io ns _3 = [ \" Which of the three objects , [ ] , [ ] , or [ ] , is closest to you ?\" , \" Among [ ] , [ ] , and [ ] , which object is nearest to you ?\" , \" Between [ ] , [ ] , and [ ] , which one is the closest to you ?\" , ] g _ m _ u _ s n = [ \" What is the height difference above ground level between [ ] and [ ]?\" , \" How much higher or lower is [ ] compared to [ ] above the ground ?\" , \" By what amount does the elevation of [ ] differ from that of [ ]?\" ] t _d a _ s n = [ \" What is the distance between the centers of [ ] and [ ]?\" , \" How far apart are the centers of [ ] and [ ]?\" , \" What is the separation between the central points of [ ] and [ ]?\" ] al l_ ch ic e_ st s_ 3 = [ \" Among the three objects [ ] , [ ] , and [ ] , which one is the tallest ?\" , \" Which of the three objects [ ] , [ ] , and [ ] is tallest ?\" , \" Out of the three objects [ ] , [ ] , and [ ] , which one is the tallest ?\" , ] v _ pr c _ s n = [ \" Is [ ] above [ ]?\" , \" Does [ ] appear over [ ]?\" , \" Can you confirm if [ ] is positioned above [ ]?\" , ] Details of RynnEC-Bench Construction As described in Section 3.2.2, we adjust the object distribution in the object properties understanding evaluation set of RynnEC-Bench based on real-world object category frequencies. The detailed object categorization strategy is presented in Tab. 6. We classify common indoor objects into 12 coarse categories and 119 fine-grained categories. Objects not falling into any of these predefined categories are assigned to an \"other\" class. function-centered taxonomy is adopted: objects with similar appearances but distinct functional roles are categorized separately. To construct this evaluation set, we follow two-stage process. First, an initial, oversized pool of 20,000 question-answer (QA) pairs is randomly generated without distributional constraints. Following this, we downsample this pool to target size of 10,000 pairs. The sampling is performed according to the real-world object distribution outlined previously. Specifically, we calculate the target number of samples for each object category by multiplying its frequency in the distribution by the total target size (10,000). The final dataset is then constructed by drawing the calculated number of QA pairs for each category from the initial 20,000-pair pool. This stratified sampling strategy ensures that the final evaluation sets composition accurately mirrors the specified real-world object frequencies. Category Furniture Appliances & Electronics Fine-Grained Classes Bed, Chair, Sofa, Table, Nightstand, Cabinet, Shelf, Headboard, Wardrobe, Drawer, Wall, Door, Window, Mirror, Hanger, Hook, Handle, Hinge, Railing, Radiator, Light Switch Outlet, Refrigerator, Washing Machine, Air Conditioner, Monitor, Television, Control Panel, Fan, Speaker, Lamp, Charger, Router, Cable, Oven, Toaster, Microwave, Water heater, Range Hoods, Remote Control Kitchenware & Tableware Spice Jar, Pot, Kettle, Cup, Jar, Bowl, Spoon, Knife, Plate, Chopping board, Chopstick, Stove, Rice Cooker Containers Bag, Box, Basket, Bucket, Bottle, Trash Can, Can, Lid, Ashtray Bathroom & Cleaning Faucet, Sink, Toilet, Toilet Seat, Toilet Lid, Shower, Bathtub, Mop, Broom, Brush, Sponge, Towel, Toothbrush, Toothpaste, Comb, Soap, Toilet Paper, Hose, Razor, Hair Dryer Textiles & Bedding Quilt, Blanket, Carpet, Curtain, Pillow, Cushion, Mattress Stationery & Office Supplies Book, Clock, Calendar, Pen, Sharpener, Scissors, Calculator, Mouse, Decor & Art Daily Necessities Food Clothing Mousepad, Keyboard, LaptopPanel, Tablet Computer Plant, Painting, Picture, Poster, Label, Calendar, Vase Phone, Hat, Slipper, Shoe, Umbrella, Headphones, Glove Fruit, Vegetable Shirt, Pants, Dress, Skirt, Coat, Shorts, Socks, Underwear Fitness & Recreation Treadmill, Dumbbells, Piano, Toy Table 6 Object category taxonomy."
        },
        {
            "title": "C Qualitative examples",
            "content": "In section 4.2.1 and table 2, we show that our model can handle different types of object and spatial cognition tasks. In this section, we show more detailed qualitative examples for different abilities of our model. C.1 Object Cognition Properties (figure 8, figure 9). The model discerns wide range of object properties, including physical attributes such as size, color, and surface details, as well as functional affordances. Segmentation (figure 8). The system performs both simple and situational referring expression segmentation, enabling it to isolate target objects in the scene based on natural language queries. C.2 Spatial Cognition Trajectory Review (figure 8). The model perceives the distance traversed by its own camera, allowing for review of its past trajectory. Egocentric Direction (figure 9). It successfully determines the direction of objects relative to its own perspective. Egocentric Distance (figure 9). The system is capable of estimating the egocentric distance from itself to surrounding objects in the environment. Movement Imagery (figure 8). key capability is the imagination of prospective movements, allowing the model to reason about future paths. Spatial Imagery (figure 8). The model demonstrates an ability for spatial imagination, such as inferring the layout of unseen areas. Object Size (figure 9). Its spatial understanding extends to estimating the absolute sizes of objects and performing relative size comparisons between them. Object Height (figure 8). Similarly, the model predicts and compares the heights of different objects. Object Distance (figure 9). The system accurately gauges the distance between multiple objects within the scene (i.e., inter-object distance). Absolute Position (figure 8). The model can ascertain the absolute positional relationships between objects. Relative Position (figure 9). Furthermore, it demonstrates robust understanding of the relative positions of objects with respect to one another. Figure 8 Visualization of question answering examples. Part 1 out of 2. Figure 9 Visualization of question answering examples. Part 2 out of 2."
        }
    ],
    "affiliations": [
        "DAMO Academy, Alibaba Group",
        "Hupan Lab",
        "Zhejiang University"
    ]
}