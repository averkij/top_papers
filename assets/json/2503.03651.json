{
    "paper_title": "DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles",
    "authors": [
        "Rui Zhao",
        "Weijia Mao",
        "Mike Zheng Shou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Adapting generative models to specific domains presents an effective solution for satisfying specialized requirements. However, adapting to some complex domains remains challenging, especially when these domains require substantial paired data to capture the targeted distributions. Since unpaired data from a single modality, such as vision or language, is more readily available, we utilize the bidirectional mappings between vision and language learned by the unified generative model to enable training on unpaired data for domain adaptation. Specifically, we propose DoraCycle, which integrates two multimodal cycles: text-to-image-to-text and image-to-text-to-image. The model is optimized through cross-entropy loss computed at the cycle endpoints, where both endpoints share the same modality. This facilitates self-evolution of the model without reliance on annotated text-image pairs. Experimental results demonstrate that for tasks independent of paired knowledge, such as stylization, DoraCycle can effectively adapt the unified model using only unpaired data. For tasks involving new paired knowledge, such as specific identities, a combination of a small set of paired image-text examples and larger-scale unpaired data is sufficient for effective domain-oriented adaptation. The code will be released at https://github.com/showlab/DoraCycle."
        },
        {
            "title": "Start",
            "content": "DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles Rui Zhao, Weijia Mao, Mike Zheng Shou* Show Lab, National University of Singapore 5 2 0 2 5 ] . [ 1 1 5 6 3 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Adapting generative models to specific domains presents an effective solution for satisfying specialized requirements. However, adapting to some complex domains remains challenging, especially when these domains require substantial paired data to capture the targeted distributions. Since unpaired data from single modality, such as vision or language, is more readily available, we utilize the bidirectional mappings between vision and language learned by the unified generative model to enable training on unpaired data for domain adaptation. Specifically, we propose DoraCycle, which integrates two multimodal cycles: text-toimage-to-text and image-to-text-to-image. The model is optimized through cross-entropy loss computed at the cycle endpoints, where both endpoints share the same modality. This facilitates self-evolution of the model without reliance on annotated text-image pairs. Experimental results demonstrate that for tasks independent of paired knowledge, such as stylization, DoraCycle can effectively adapt the unified model using only unpaired data. For tasks involving new paired knowledge, such as specific identities, combination of small set of paired image-text examples and larger-scale unpaired data is sufficient for effective domain-oriented adaptation. The code will be released at https://github.com/showlab/DoraCycle. 1. Introduction The adaptation of pre-trained generative models to specific domains is an important aspect of advancing personalized content creation, from stylized media outputs to customized identity generation [52, 68]. However, effectively adapting generative models to complex domains remains challenging, particularly when these domains require extensive amounts of paired data to accurately capture the desired distributions. For instance, learning the visual styles and character identities across unique movie, which involves understanding multiple characters, their relationships, and di- *Corresponding Author. Figure 1. Training paradigms for unified generative models. (a) Traditional training involves using paired image-text data and optimizing the unified model with paired losses for both image-to-text (b) In contrast, (I2T) and text-to-image (T2I) generation tasks. the proposed multimodal cycle training framework leverages unpaired images and texts. By using cycle-consistency losses, the unified model learns to maintain consistency between input and output across modalities, enabling adaptation without the need for extensive paired datasets. verse settings, is highly complex task that demands vast amounts of paired frames and captions data. Collecting such paired data, especially for multimodal tasks involving vision and language, is often laborious and impractical, limiting the potential to adapt generative models at scale. High-quality image-text paired data is relatively rare and scarce, but unpaired images and texts are readily available in our daily lives, such as video websites, image platforms, and content from novel websites. Therefore, we aim to explore whether it is possible to adapt generative models to target domains based on unpaired data. To achieve this goal, it is crucial for the model to have an internal capability to align the two modalities to some extent. Fortunately, recent advancements in unified generative models have encouraged us to pursue this direction. Recent advanced unified generative models [15, 19, 64, 74, 85] have shown great potential in unifying multimodal 1 understanding and generation within single model. These unified models are capable of processing and generating content across different modalities, i.e. vision and language, within shared framework. By leveraging the bidirectional mappings between vision and language, which are learned by the unified generative model in its pre-training stage, we can map each data from one modality to another and then back to the original modality, as shown in Fig. 1. Through these two mappings, data can be maintained within the same modality, thereby imposing constraints on the deviation introduced in the process. This only requires computing the cross-entropy loss between the data in the same modality, without any paired data supervision. To this end, we introduce DoraCycle, framework for adapting unified multimodal generative models to target domains, through cycle-consistent learning with unpaired data. Unlike previous adaptation methods that heavily rely on paired text-image data, the proposed framework leverages the shared latent space of unified models to learn consistent transformations between modalities without requiring paired training examples. Specifically, we design two cycles, i.e. text-to-image-to-text (T cycle) and image-totext-to-image (I cycle). As shown by Fig. 1 (b), leveraging the pre-trained vision-language alignment of the unified model, each multimodal cycle involves two cross-modality mappings while optimizing in the same modality. This enables calculating loss on unpaired data while implicitly refining cross-modal alignment through the intermediate step. In practice, since there is no labeled ground-truth, mapping data to another modality requires multi-step inference, such as predicting the next token multiple times for text generation. Allowing all forward steps to participate in gradient backpropagation can lead to catastrophic gradient explosion. Therefore, we stop gradients during multi-step inference and use the generated data as pseudo labels for the model to forward once again, allowing gradients to propagate. Moreover, we found that since complete cycle requires the model to forward twice, it can lead to training instability, with the quality of pseudo data generated in the middle being compromised. To enhance the stability of pseudo data generation, we maintain slowly updated EMA (Exponential Moving Average) [63] model, which is used for inference to generate pseudo data. Additionally, we employ gradient clipping techniques to avoid conflicts in the optimization directions of the two cycles, further increasing training stability. The experiments indicate that for tasks independent of paired knowledge, such as stylization and domain-specific adaptation, DoraCycle can adapt the unified model with only unpaired data, which is both more practical and scalable. For tasks that require new paired knowledge, such as identity-specific adaptation, DoraCycle effectively utilizes small amounts of paired data along with larger-scale unpaired data, making it flexible solution for generative adaptation challenges. We conduct extensive experiments that compare DoraCycle to existing methods, showing that our approach achieves comparable or superior results while significantly reducing the need for paired data. This ability to harness large-scale unpaired data, combined with strategic usage of small paired datasets, makes DoraCycle feasible solution for personalized multimodal content generation across wide range of applications. 2. Related Works 2.1. Multimodal Generation and Understanding Generating visual contents from text and describing them through natural language have been extensively studied as core multimodal tasks. Advanced generative models [3, 5, 6, 8, 12, 14, 17, 20, 40, 41, 47, 49, 50, 54, 8284], such as DALLE [46, 48], Stable Diffusion [51], demonstrate remarkable generation capabilities, producing high-quality and diverse contents from textual prompts. Meanwhile, image captioning models [26, 28, 29, 66, 67, 69], such as mPLUG [34], and BLIP [36], push the boundaries of visual understanding, generating accurate and context-aware descriptions. Additionally, recent advancements in multimodal large language models [35], such as LLaVA [39], MiniGPT-4 [86], and InstructBLIP [11], have significantly improved the ability to understand and reason about visual content. Besides the powerful foundational generative models, adapting or customizing them attracts increasing interest, which enables more personalized and specific outputs based on user preferences [7, 9, 18, 21, 32, 33, 42, 75]. Approaches like DreamBooth [52] enable user-specific customization by fine-tuning generative models with personal data, allowing the generation of content tailored to individual needs or preferences. 2.2. Unified Multimodal Generative Models Unified multimodal generative models aim to bridge the gap between understanding and generation tasks, and integrate vision and language into single framework, enabling the model to learn shared representations across modalities [1, 2, 15, 19, 59, 62, 71, 72, 74, 76, 77, 79, 85]. SEED-X [19] utilizes unified architecture where visual features extracted from the CLIP ViT encoder [45] are combined with text tokens and fed into large language model to enable both next-word prediction and image regression tasks. DreamLLM [15] extends the generative capability of large language models by combining multimodal inputs directly into LLMs. Chameleon [64] employs discrete tokenization approach for both visual and textual inputs, converting all modalities into unified token space that is processed by transformer-based architecture. Trans2 fusion [85] introduces an advanced integration mechanism that focuses on directly fusing visual encoding with language tokens, allowing the model to effectively translate visual information into textual formats while maintaining the semantic integrity of both modalities. Show-o [74] combines autoregressive modeling with discrete diffusion process, enabling the generation of high-quality outputs that are aligned across modalities. Our work leverages the advancements made by these foundational models and explores how to adapt the foundational model to specific domains. 2.3. Cycle Consistency Cycle consistency has been utilized in computer vision and natural language processing as means to enhance model robustness and consistency [10, 16, 22, 25, 56, 56, 57, 81]. CycleGAN [87] introduced cycle consistency loss to align unpaired image domains, ensuring that mappings between domains (e.g., ABA) remain consistent. In the field of natural language processing, back-translation employs similar ideas by translating sentences between languages in both directions to improve translation quality [55]. However, the cycle consistency in these works is in single modality, i.e. vision or language. Recently, ITIT [37] was proposed to utilize cycle consistency to train visionlanguage generative models. ITIT takes in mixture of unpaired data and paired data to pre-train the foundational generative model. It is constructed with one image-text encoder and two modality-specific decoders, which operate on the encoded image-text features to generate either text or image tokens. In contrast, we utilized on single unified transformer to parse and predict text and image tokens together. Besides, we focus on adapting pre-trained foundational models to new domains efficiently rather than retraining new foundation models. 3. Method The proposed DoraCycle framework, as shown in Fig. 2, is built upon the unified generative model designed for multimodal tasks involving both vision and language [64, 74, 85]. The unified model uses single transformer to learn bidirectional mappings between vision and language, providing powerful backbone capable of processing and generating different modalities [74]. For captioning, the model takes in image tokens and predicts corresponding text tokens, while for image generation, it takes in text tokens and predicts image tokens. This versatility makes the unified model wellsuited as base for our proposed framework. In the following sections, we first introduce the design of multimodal cycles, and then discuss the stabilization of optimization, and the balance of two cycles. 3.1. Multimodal Cycles To adapt the unified model for domain-oriented adaptations using unpaired data, we design two multimodal cycles: the Image-to-Text-to-Image Cycle (I Cycle) and the Text-toImage-to-Text Cycle (T Cycle). Each cycle utilizes data from single modality, allowing the model to adapt without relying on paired data. Cycle: The cycle training involves transforming an input textual sequence into an image representation and then back into textual sequence, enforcing consistency between the generated and original text. Specifically, at each training iteration, we begin with an input text sequence = [tl]L l=1. Conditioned on this, the unified model generates pseudo-paired image tokens , representing the visual interpretation of the input text. The generated image tokens are then subjected to random masking operation, denoted as , where subset of the tokens is masked. The unified model is then called to reconstruct these masked tokens to form the complete synthesized image I. In the second half of the cycle, conditioning on image I, the model generates the corresponding text sequence. The objective of the cycle is to enforce cycle consistency between the generated text and the original input text . The cycle consistency loss is defined as follows: LT = ET Dtext (cid:88) (cid:2) l=1 log p(tl I, t0, , tl1)(cid:3), (1) where the Dtext is the set of text samples from the target domain. Cycle: The cycle training begins with an input image, which is subsequently mapped to textual representation and then transformed back to an image, enforcing consistency between the generated image and the original image tokens. At each training iteration, we start with input image tokens I. The unified model is used to synthesize pseudopaired text tokens , representing the textual description of the image. We then use in conjunction with the input image token to predict the reconstructed text tokens . In the second half of the cycle, we pass the masked image tokens IM and the text through the model to regenerate the masked image tokens. The cycle enforces consistency between the reconstructed and the original image tokens. The loss for enforcing cycle consistency is given by: LIC = EIDimage (cid:2) (cid:88) log p(ikIM , )(cid:3), (2) k:mk=1 where the Dimage is the set of image samples from the target domain, which are unpaired with the text samples. By leveraging these two cycles, our framework forces the model to refine its generative understanding of both image 3 Figure 2. The overview of cycle (text-to-image-to-text) of the proposed DoraCycle. The cycle is similar and is omitted in the figure for brevity. and text representations, ensuring consistency between the input and output while effectively leveraging unpaired data to adapt the unified towards the target domain. Efficient Training: In the intermediate steps of both cycles, generating the middle representation (i.e., captions or images) requires multiple forward passes. This is because the generation process involves either predicting the next tokens or the masked tokens multiple times. Backpropagating gradients through all these steps is computationally prohibitive. Thus, we first generate the intermediate results using the model in inference mode as pseudo-paired data, which are then used as the ground truth in the teacherforcing scheme [60, 61] for the first half of the cycles. In this way, we reduce the number of forward passes to two, i.e. one for generating the middle result and one for the final output, thus making the overall training process more memory efficient. Token Differentiability: Since the intermediate outputs in each cycle are discrete tokens, which can not directly propagate gradients, we employ the Gumbel-Softmax[31] to make these token representations differentiable. 3.2. Stabilizing Optimization Each cycle involves the same unified model twice in the forward pass, which leads to optimization instabilities. To stabilize the training process, we adopt the Exponential Moving Average (EMA) training technique [63]. Specifically, we maintain shadow version of the model, referred to as the EMA model, which is updated using an exponentially decaying average of the parameters of the main model. θEMA αθEMA + (1 α)θmain, (3) where α is decay factor (set to 0.999) that controls the update rate, and θmain represents the parameters of the main model. In each training step, the EMA version of the model is used to generate the intermediate representation tokens (e.g., pseudo image or text tokens) which serve as pseudo ground truth during training. By using these stable targets from the slower-evolving EMA model, we can mitigate the risks of optimization instability. The main model is thus able to learn from more consistent and reliable intermediate targets, rather than being affected by fluctuations during the early stages of training. 3.3. Balancing Two Cycles We observe that the cycle tends to converge faster than the cycle, primarily because textual data is inherently onedimensional and simpler to learn compared to images. This imbalance in optimization leads to kind of collapse of the model, where it tends to generate irrelevant but selfconsistent captions for images, ultimately degrading the image-text alignment capability. To address this problem, we make the gradients of the cycle orthogonal to those of the cycle, thus preventing interference. This is achieved by modifying the gradients using gradient surgery [80]. Let gT and gI represent the gradients of the cycle and the cycle, respectively. We project gT onto the orthogonal complement of gI to obtain the modified gradient , which is defined as: = gT gT gI gI gI gI , (4) where gT gI denotes the dot product between the gradients of the and cycle. Additionally, we reweight the losses to further balance the learning between the and cycles. The final loss function is as, = LIC + βLT C, (5) where the β is the weight of the cycle loss. 4 4. Experiments 4.1. Implementation Details To the best of our knowledge, Show-o [74] is currently the only fully open-source unified generative model with complete pre-trained weights and training code, including both its understanding and generation capabilities. Therefore, we base DoraCycle on Show-o and conduct experiments accordingly. The base model is unified transformer model that performs understanding and image generation by predicting discrete textual and visual tokens. We insert trainable low-rank adaptations (LoRA) [27] modules into the projection and projection of the attention layers from layers 7 to 24. The LoRA rank is set to 32. The β is set to 0.1 to balance the optimization of two cycles. The training of DoraCycle is performed on 8 NVIDIA H100 GPUs with mixed precision enabled for memory efficiency. We set the batch size to 32, with each cycle taking half of the batch when both cycles are being optimized simultaneously. The learning rate is set to 1e4 with cosine annealing schedule. The optimizer is AdamW with weight decay of 1e2. Additionally, EMA is employed to stabilize the training process, as described in Section 3.2. 4.2. Domain-Oriented Adaptations Unpaired Training: For tasks that do not require strongly related paired knowledge, our DoraCycle can fully learn the target domain using unpaired data. For example, to learn the cyberpunk style, we collected 300 cyberpunk-style images as input for the cycle, and used the text data from the base model pre-training dataset [4] for the cycle, with the keyword cyberpunk style automatically injected into text, prompting the model about the target style we want. The experimental results are shown in Fig. 3. Given the same text prompt to generate cyberpunk-style images, Fig. 3 (a) shows the images generated by the base model without additional training. It can be observed that the base model adds some cyberpunk elements, such as neon lights, but the overall atmosphere does not align well with the desired style. Fig. 3 (d) shows the images generated by the adapted model trained with DoraCycle, which aligns well with the target style. Traditional text-to-image customization or adaptation methods, such as DreamBooth [53], rely on paired data for training. Therefore, we simulate usercreated paired data by annotating the collected images with captions, and split them into two groups. One group contained only 10 paired examples, which is an acceptable workload for users, while the other group contained captions for all 300 images, which would be labor-intensive and impractical for users. The images generated by the model that trained on 10 paired examples are shown in Fig. 3 (b). It struggled to produce good stylized images, likely because the combination of indoor bookshelves with the cyberpunk Figure 3. Domain-oriented adaptation with different training setups. (a) Image generated by the base model without training for adoption. (b) Image generated by the model trained with 10 paired image-text samples. (c) Image generated by the model trained with 300 paired image-text samples. (d) Image generated by the model trained by DoraCycle on only unpaired data. (e) Image-to-Textto-Image translation performed by the adapted model trained by DoraCycle. style is too novel for the model to generalize well from limited paired data. The images generated by the model trained on 300 paired examples are shown in Fig. 3 (c), which have better outputs. In contrast, the model trained using DoraCycle does not require manual captioning, significantly reducing the workload for users. Fig. 3 (e) illustrates the adapted model trained by DoraCyle maintains semantic consistency through image-totext-to-image translation. The input image is transformed into textual description and then reconstructed into an image. The result shows that the adapted model successfully captures and retains the key visual components in the original image throughout the multimodal cycle. Notably, the identity of the characters and the details of the environment are all preserved, indicating effective bidirectional understanding and generation capabilities in the target domain. Furthermore, the newly generated image incorporates styles learned from the target domain, demonstrating the generalization of the learned knowledge to images in the wild. Learning Paired Knowledge: For tasks that require learning some paired knowledge, such as associating an identity name with its visual appearance, DoraCycle can incorporate small amount of paired data to learn such associations while leveraging large amount of unpaired 5 Figure 4. Image-to-text and text-to-image generation by the unified models that adapted for two domains. The special tokens are omitted. generating contextually accurate captions. In domain 1, the generated captions provide rich descriptions of the characters, their attributes, and the context, effectively mirroring the visual elements present in the input images. In domain 2, the captions correctly describe the characters, their actions, and their environments concisely, maintaining consistency with the visual style. The ability of the model to generate accurate descriptions highlights its robust understanding of the visual components of the domain. Additionally, an interesting phenomenon can be observed in how the model handles the visual elements that are not annotated with paired data. For instance, in Fig. 4 (w), the dorayaki (a type of sweet bean-filled pancake) was described by the model as doughnut. This may be due to the fact that the anime-style representation of the dorayaki is novel, and neither the base model nor the unpaired training provided specific textual-visual pairing knowledge about it. On the other hand, in the example shown in Fig. 4 (x), we annotate the white cat as character with paired textual and visual data, using special token for its name: <soc> white cat <eoc>. Interestingly, although no paired annotation is provided for the black cat, the model still predicts the special token for it as <soc> black cat <eoc> during the caption generation. This suggests that the model autonomously categorized the black cat as character when learning the target domain, indicating that it may have attempted to generalize learned knowledge from one type of entity to similar ones. Enhanced Learning with Special Tokens: As shown in Fig. 5, we experimentally find that the model often confused multiple novel concepts in the target domain. Fig. 5 (a) shows the image generated by the base model without training taking in the name of characters. Fig. 5 (b) shows the characters generated by the trained model. During training, the names of characters are directly included in the text without special treatment, leading to attribute confusion between characters. The varying lengths of the tokenized character names also make learning difficult. To solve this problem, we introduce simple yet efficient solution: adding special tokens around character names. We introduced start of character (<soc>) and end of character (<eoc>) tokens to enclose character names, which significantly enhance the learning of novel concepts. As shown in Fig. 5 (c), involving special tokens improves the alignments between characters and their names. 4.3. Comparisons In this section, we use the Storyboard20K [73] dataset to conduct the quantitative comparison experiments. The storyboards originating from the same data source are grouped to form domain, consisting of images and descriptive text. The data are used under three different settings, i.e. totally unpaired, only paired, and paired plus unpaired data, Figure 5. Effect of special tokens on character learning. (a) Base model without training. (b) Model trained without using special tokens, showing attribute confusion among characters. (c) Model trained with special tokens, improving character attribute alignment and reducing confusion. data to comprehensively learn general aspects of the target domain. Specifically, in each batch, for data with paired ground truth, we compute the token prediction loss and also include it in the cycle, use ground truth as the pseudo middle generations, and compute the cycle loss. For unpaired data, we compute the unpaired cycle loss. For example, when adapting the model to Domain 1: Black Myth Wukong and Domain 2: Doraemon, we annotate 1-3 images per unique identity with captions that specify the name of the identity. For each domain, we collect 2k images, which are mostly sampled from online videos, and independently collect text descriptions, which are further expanded to 1k by ChatGPT [43]. The final adapted model trained with DoraCycle demonstrates strong performance in both text-to-image generation and image-to-text generation, as shown in Fig. 4. In terms of text-to-image results, the model trained with DoraCycle effectively generated images that aligned well with the target domains. In domain 1 (Black Myth Wukong), the generated images accurately depicted domain-specific visual elements, such as the intricate details of character appearances and the overall fantasy-like atmosphere. This indicates that the model successfully learned to generalize the visual features from text prompts to realistic images within the target domain. Similarly, in domain 2 (Doraemon), the generated images preserve the iconic cartoonish aesthetics and capture key visual details of the characters and settings, demonstrating effective domain adaptation. For the image-to-text task, the model performs well in Table 1. Comparison of different training methods under various data settings. The best value is highlighted in blue , and the second-best value is highlighted in green . indicates paired data, and indicates unpaired data."
        },
        {
            "title": "I Data",
            "content": "FID-1K CIDEr"
        },
        {
            "title": "Human Eval",
            "content": "T2I Align I2T Align DreamBooth [53]"
        },
        {
            "title": "DoraCycle",
            "content": "ITIT [37] - - - - 10% 100% 100% 100% 10% 100% 100% 100% 10% + 90% 10% + 90% 10% + 90% 10% + 90% 33.22 24.93 28.93 36.63 27.44 25. 27.50 32.74 41.55 30.54 35.70 38.17 40.90 38.62 3.25 4.13 3.38 3.26 3.84 4. 3.85 1.83 3.96 1.62 2.17 3.42 3.81 3.52 as shown in Table 1. 4.4. Ablation Studies The compared methods include DreamBooth [53] and ITIT [37]. We implement DreamBooth as paired-training baseline by applying LoRA fine-tuning on the unified model. The original design of ITIT is different, in which the image and text decoders are separate models, and its code has not been released. We adjusted and re-implemented it to be suitable for our unified model architecture. We use both automatic and human evaluations to compare the performance of different methods. For automatic evaluation, we use FID to measure the distribution differences between the generated images and the target domain images [24], and CIDEr to compute the error between the generated text and the ground truth [65]. For human evaluation, we create 100 questions for the generated results of models, each rated by three different human raters. The raters are asked to evaluate the alignment between the image and text on scale from 1 to 5, where 1 indicates no relevance and 5 indicates complete alignment. The experimental results in Table 1 demonstrate that the proposed DoraCycle performs competitively under several data settings. Specifically, when using combination of paired and unpaired data, DoraCycle outperforms ITIT. Compared to DreamBooth, which heavily relies on paired data, DoraCycle outperforms it when using the same scale of paired data, i.e. 10% paired data, indicting the benefits brought by 90% unpaired data. While Dreambooth with 100% achieves the best evaluation scores, the scores of the DoraCycle with 10% paired and 90% unpaired data are comparable with them. Table 1 also shows the difference in the performance of It is shown that DoraCyle under different cycle settings. without the cycle and with only the cycle, the captioning ability of the adapted model degrades more significantly. In contrast, if only the cycle is used and without the cycle, the FID score increases substantially, indicating that the generated image distribution mismatches with the target distribution. FID-1K CIDEr w/o EMA w/o GS DoraCycle Table 2. Ablation Studies. EMA refers to the exponential moving average. GS refers to gradient surgery. that Table 2 shows removing key components from DoraCycle significantly impacts performance. Without EMA, the FID score increases from 25.37 to 27.19, indicating image quality lower due to less stable training. Removing Gradient Surgery (GS) will reduce the CIDEr score and increase the FID, indicating worse performance. This demonstrates the importance of mitigating the interference between the The complete optimization directions of two cycles. DoraCycle framework, with both EMA and GS, has the best performance across all metrics, demonstrating the importance of these components in achieving better optimization. 38.85 39.98 40.90 27.19 25.54 25.37 5. Conclusion We propose the DoraCycle to adapt the unified generative model to target domains within multimodal cycles. By leveraging both image-to-text-to-image and text-to-imageto-text cycles, DoraCycle changes the learning objectives into the same modality, allowing for effective optimization using unpaired data. Our experiments show that DoraCycle can adapt the unified model to target domains using only unpaired data, or involving small amount of paired data when necessary to learn specific concepts. Experimental results demonstrate that DoraCycle achieves advanced or comparable performance across various settings. Leveraging unpaired data broadens the application potential of DoraCycle, making it ideally suited for domain adaptation tasks where paired data is scarce or challenging to collect."
        },
        {
            "title": "References",
            "content": "[1] Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, et al. Cm3: causal masked multimodal model of the internet. arXiv preprint arXiv:2201.07520, 2022. 2 [2] Emanuele Aiello, LILI YU, Yixin Nie, Armen Aghajanyan, and Barlas Oguz. Jointly training large autoregressive multimodal models. In ICLR, 2024. 2 [3] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. 2 [4] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pretraining to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 35583568, 2021. 5 [5] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. arXiv preprint arXiv:2403.04692, 2024. 2 [6] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Zhongdao Wang, James T. Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion In transformer for photorealistic text-to-image synthesis. ICLR. OpenReview.net, 2024. [7] Li Chen, Mengyi Zhao, Yiheng Liu, Mingxu Ding, Yangyang Song, Shizun Wang, Xu Wang, Hao Yang, Jing Photoverse: Tuning-free image Liu, Kang Du, et al. customization with text-to-image diffusion models. arXiv preprint arXiv:2309.05793, 2023. 2 [8] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In ICML, pages 16911703, 2020. 2 [9] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-shot object-level imIn Proceedings of the IEEE/CVF Conage customization. ference on Computer Vision and Pattern Recognition, pages 65936602, 2024. 2 [10] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo. Stargan: Unified generative adversarial networks for multi-domain image-to-image translation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 3 [11] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Instructblip: Towards generalFung, and Steven Hoi. purpose vision-language models with instruction tuning, 2023. 2 [12] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models using photogenic needles in haystack. arXiv preprint arXiv:2309.15807, 2023. 2 [13] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme Ruiz, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin Fathy Elsayed, Aravindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Collier, Alexey A. Gritsenko, Vighnesh Birodkar, Cristina Nader Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetic, Dustin Tran, Thomas Kipf, Mario Lucic, Xiaohua Zhai, Daniel Keysers, Jeremiah J. Harmsen, and Neil Houlsby. Scaling vision transformers to 22 billion parameters. In ICML, pages 7480 7512, 2023. 1 [14] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34:87808794, 2021. [15] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, and Li Yi. DreamLLM: Synergistic multimodal comprehension and creation. In ICLR, 2024. 1, 2 [16] Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. Temporal cycleconsistency learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 18011810, 2019. 3 [17] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024. 2 [18] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 2 [19] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. 1, 2 [20] Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao, Shuning Chang, Weijia Wu, et al. Mix-of-show: Decentralized lowrank adaptation for multi-concept customization of diffusion models. Advances in Neural Information Processing Systems, 36:1589015902, 2023. [21] Jiayi Guo, Chaofei Wang, You Wu, Eric Zhang, Kai Wang, Xingqian Xu, Shiji Song, Humphrey Shi, and Gao Huang. Zero-shot generative model adaptation via image-specific In Proceedings of the IEEE/CVF conprompt learning. ference on computer vision and pattern recognition, pages 1149411503, 2023. 2 [22] Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu, TieYan Liu, and Wei-Ying Ma. Dual learning for machine trans9 lation. In Advances in Neural Information Processing Systems (NeurIPS), 2016. 3 [23] Jack Hessel, Ana Marasovic, Jena D. Hwang, Lillian Lee, Jeff Da, Rowan Zellers, Robert Mankoff, and Yejin Choi. Do androids laugh at electric sheep? Humor understanding benchmarks from The New Yorker Caption Contest. In Proceedings of the ACL, 2023. 3 [24] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 66266637, 2017. [25] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In Proceedings of the International Conference on Machine Learning (ICML), 2018. 3 [26] MD Zakir Hossain, Ferdous Sohel, Mohd Fairuz Shiratuddin, and Hamid Laga. comprehensive survey of deep learning for image captioning. ACM Computing Surveys (CsUR), 51(6):136, 2019. 2 [27] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. 5 [28] Jia Cheng Hu, Roberto Cavicchioli, and Alessandro Capotondi. Exploiting multiple sequence lengths in fast end to In 2023 IEEE Internaend training for image captioning. tional Conference on Big Data (BigData), pages 21732182. IEEE, 2023. 2 [29] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and Lijuan Wang. Scaling up vision-language pre-training for image captioning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1798017989, 2022. 2 [30] Lalit Jain, Kevin Jamieson, Robert Mankoff, Robert Nowak, and Scott Sievert. The New Yorker cartoon caption contest dataset, 2020. 3 [31] Eric Jang, Shixiang Gu, and Ben Poole. reparameterization with gumbel-softmax. arXiv:1611.01144, 2016. 4, Categorical arXiv preprint [32] Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang, and Yu-Chuan Su. Taming encoder for zero fine-tuning image customization with text-to-image diffusion models. arXiv preprint arXiv:2304.02642, 2023. 2 [33] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19311941, 2023. 2 [34] Chenliang Li, Haiyang Xu, Junfeng Tian, Wei Wang, Ming Yan, Bin Bi, Jiabo Ye, Hehong Chen, Guohai Xu, Zheng Cao, et al. mplug: Effective and efficient vision-language learning by cross-modal skip-connections. arXiv preprint arXiv:2205.12005, 2022. 2 [35] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, Jianfeng Gao, et al. Multimodal foundation models: From specialists to general-purpose assistants. Foundations and Trends in Computer Graphics and Vision, 16(1-2):1214, 2024. 2 [36] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. [37] Tianhong Li, Sangnie Bhardwaj, Yonglong Tian, Han Zhang, Jarred Barber, Dina Katabi, Guillaume Lajoie, Huiwen Chang, and Dilip Krishnan. Leveraging unpaired data for vision-language generative models via cycle consistency. arXiv preprint arXiv:2310.03734, 2023. 3, 8 [38] Yuanzhi Li, Sebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023. 1 [39] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 36, 2024. 2 [40] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024. 2 [41] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. [42] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real imIn Proceedings of ages using guided diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60386047, 2023. 2 [43] OpenAI. Chatgpt. https://chatgpt.com/, 2024. 7 [44] Dragomir Radev, Amanda Stent, Joel Tetreault, Aasish Pappu, Aikaterini Iliakopoulou, Agustin Chanfreau, Paloma de Juan, Jordi Vallmitjana, Alejandro Jaimes, Rahul Jha, and Robert Mankoff. Humor in collective discourse: Unsupervised funniness detection in the New Yorker cartoon caption contest. In LREC, 2016. 3 [45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 2 [46] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, pages 8821 8831. Pmlr, 2021. 2 [47] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with CLIP latents. CoRR, abs/2204.06125, 2022. 2 [48] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical diffusion models for text-toimage generation. arXiv preprint arXiv:2204.06125, 2022. 2 [49] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. 2 [50] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10684 10695, 2022. 2 [51] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. arXiv preprint arXiv:2112.10752, 2022. 2 [52] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint arXiv:2208.12242, 2022. 1, 2 [53] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF conference generation. on computer vision and pattern recognition, pages 22500 22510, 2023. 5, 8 [54] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:3647936494, 2022. 2 [55] Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation models with monolingual data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2016. 3 [56] Meet Shah, Xinlei Chen, Marcus Rohrbach, and Devi Parikh. Cycle-consistency for robust visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66496658, 2019. 3 [57] Sameer Shah, Siddharth Bharadwaj, Devi Parikh, and Dhruv Batra. Cycle-consistency for robust visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 3 [58] Dafna Shahaf, Eric Horvitz, and Robert Mankoff. Identifying humorous cartoon captions. jokes: 2015. 3 Inside In KDD, [59] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1439814409, 2024. [60] Sutskever. Sequence to sequence learning with neural networks. arXiv preprint arXiv:1409.3215, 2014. 4 [61] Richard Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3:944, 1988. 4 11 [62] Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal. Any-to-any generation via composable diffusion. NeurIPS, 36, 2024. 2 [63] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Advances in Neural Information Processing Systems, pages 11951204, 2017. 2, [64] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 1, 2, 3 [65] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 45664575, 2015. 8 [66] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022. 2 [67] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through simple sequence-to-sequence learning framework. In International conference on machine learning, pages 2331823340. PMLR, 2022. 2 [68] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, Anthony Chen, Huaxia Li, Xu Tang, and Yao Hu. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. 1 [69] Yuchi Wang, Shuhuai Ren, Rundong Gao, Linli Yao, Qingyan Guo, Kaikai An, Jianhong Bai, and Xu Sun. Ladic: Are diffusion models really inferior to autoregressive arXiv preprint counterparts for image-to-text generation? arXiv:2404.10763, 2024. 2 [70] Mitchell Wortsman, Peter Liu, Lechao Xiao, Katie Everett, Alex Alemi, Ben Adlam, John Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, et al. Small-scale proxies for large-scale transformer training instabilities. arXiv preprint arXiv:2309.14322, 2023. [71] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024. 2 [72] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. arXiv preprint arXiv:2309.05519, 2023. 2 [73] Jinheng Xie, Jiajun Feng, Zhaoxu Tian, Kevin Qinghong Lin, Yawen Huang, Xi Xia, Nanxu Gong, Xu Zuo, JiLearning long-form aqi Yang, Yefeng Zheng, et al. arXiv preprint video prior via generative pre-training. arXiv:2404.15909, 2024. 7 [74] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and [87] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation using cycleconsistent adversarial networks. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017. 3 generation. arXiv preprint arXiv:2408.12528, 2024. 1, 2, 3, [75] Ceyuan Yang, Yujun Shen, Zhiyi Zhang, Yinghao Xu, Jiapeng Zhu, Zhirong Wu, and Bolei Zhou. One-shot generative domain adaptation. In Proceedings of the ieee/cvf international conference on computer vision, pages 77337742, 2023. 2 [76] Hanrong Ye, De-An Huang, Yao Lu, Zhiding Yu, Wei Ping, Andrew Tao, Jan Kautz, Song Han, Dan Xu, Pavlo Molchanov, et al. X-vila: Cross-modality alignment for large language model. arXiv preprint arXiv:2405.19335, 2024. 2 [77] Haoxuan You, Mandy Guo, Zhecan Wang, Kai-Wei Chang, Jason Baldridge, and Jiahui Yu. Cobit: contrastive biarXiv preprint directional image-text generation model. arXiv:2303.13455, 2023. 2 [78] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. 1 [79] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian Karrer, Shelly Sheynin, et al. Scaling autoregressive multimodal models: Pretraining and instruction tuning. arXiv preprint arXiv:2309.02591, 2(3), 2023. 2 [80] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. In Advances in Neural Information Processing Systems, pages 58245836, 2020. 4 [81] Gengwei Zhang, Guoliang Kang, Yi Yang, and Yunchao Wei. Few-shot segmentation via cycle-consistent transformer. Advances in Neural Information Processing Systems, 34:2198421996, 2021. [82] Rui Zhao, Wei Li, Zhipeng Hu, Lincheng Li, Zhengxia Zou, Zhenwei Shi, and Changjie Fan. Zero-shot text-to-parameter translation for game character auto-creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2101321023, 2023. 2 [83] Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jia-Wei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng Shou. Motiondirector: Motion customization of text-to-video diffusion models. In European Conference on Computer Vision, pages 273290. Springer, 2024. [84] Rui Zhao, Hangjie Yuan, Yujie Wei, Shiwei Zhang, Yuchao Gu, Lingmin Ran, Xiang Wang, Jay Zhangjie Wu, David Junhao Zhang, Yingya Zhang, et al. Evolvedirector: Approaching advanced text-to-image generation with large vision-language models. Advances in Neural Information Processing Systems, 37:122104122129, 2025. 2 [85] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 1, 2, 3 [86] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. CoRR, abs/2304.10592, 2023. 2 12 DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Model Details In this section, we provide an introduction to the base unified generative model, which serves as the foundation for our approach to multimodal understanding and generation. It integrates both auto-regressive and diffusion modeling techniques to achieve joint multimodal understanding and generation [74]. The base unified generative model is built upon pretrained large language model Phi-1.5 [38]. The architecture of the base model is largely inherited from it with minimal modifications to accommodate multimodal input. Specifically, QK-Norm operation is added to each attention layer [13, 70] to enhance training stability. The embedding layer is expanded by adding learnable embeddings for discrete image tokens, enabling the joint encoding of text and image modalities. The final model consists of 24 transformer layers with total of 1.5 billion parameters. In the proposed DoraCycle, the parameters of the base model are fixed and the LoRA components introduce 4.7 million trainable parameters, accounting for approximately 0.32% of the total model parameters. The model tokenizes both text and image data into discrete tokens to create unified space, maintaining unified vocabulary. Text data is tokenized using pre-trained text tokenizer of Phi-1.5 [38]. The codebook size of text tokens is 58498. For images, quantizer like MAGVITv2 [78] is used. This quantizer maintains codebook of size = 8, 192 and encodes images at resolution of 512512 into 32 32 discrete tokens. The model utilizes the unified tokenization strategy ensuring that both modalities can be processed consistently, allowing the model to handle multimodal inputs within shared framework. Besides the text and image tokens, the model also involves different special tokens, like <sot>, <eot>, <soi>, and <soi>, which are used to denote the start and end of the text and image tokens. Among them, there are some special tokens that indicate the task to be executed, where the <mmu> indicates the model should do the understanding task and the <t2i> indicates the model should generate image tokens based on the given text. Additionally, we introduce two new special tokens, i.e. <soc> and <eoc>, to enhance the learning of new concepts. proximate the sampling in differentiable manner [31]. The Gumbel-Softmax distribution allows gradients to flow through the sampling process, making it suitable for end-toend training. The Gumbel-Softmax operation is defined as follows: yi = exp((log(πi) + gi)/τ ) j=1 exp((log(πj) + gj)/τ ) (cid:80)k , (6) where πi represents the input logits, gi is sampled from Gumbel(0, 1) distribution, and τ is the temperature parameter that controls the softness of the sampling. The Gumbel(0, 1) distribution is defined as: gi = log( log(ui)), (7) where ui is sampled from uniform distribution (0, 1). In our experiments, we set τ = 1.0 and use the hard GumbelSoftmax, where the output is discretized to be one-hot, while maintaining the gradient flow for backpropagation. C. Training Details All experiments are conducted on 8 NVIDIA H100 GPUs, with training taking 3,000 steps. Each 1K steps requires approximately 3.5 hours. We observe that the Cycle loss typically converges within 1-2K steps. The convergence of Cycle loss is more challenging to observe directly on the loss curves, while the visual inspection reveals significant convergence. We employ DeepSpeed Zero with Zero Stage 2, which optimizes memory usage by partitioning model states across devices, along with bf16 mixed precision to efficiently utilize GPU memory and computational resources. The total batch size is set to 32, with each GPU handling batch size of 4. For the inference mode to generate pseudo tokens in the middle of each cycle, we set the classifier free guidance to 5 and generation steps to 30 for the text-to-image generation and set the Top-K to 1, temperature to 1.0, and the max sequence length to 256 for the image-to-text generation. D. More Results B. Differentiable Sampling To handle the undifferentiable discrete tokens generation process, we use the Gumbel-Softmax technique to apIn Fig. 6, we provide more results generated by the unified model that is adapted by the proposed DoraCycle for two domains, respectively. We omit the special tokens in the figure for brevity. Figure 6. Image-to-text and text-to-image generation by the unified models that adapted for two domains. The special tokens are omitted. 2 Figure 7. Illustration of the progressive adaptation progress of the unified model to the target domain. For domain 1, the cartoon style, which does not need any paired knowledge, we apply DoraCycle on pure unpaired data, collected from the New Yorker Caption Contest Dataset [23, 30, 44, 58]. As shown in Fig. 6, the first and second rows present text-to-image generation results. The generated images align well with the characteristics of the target domain, and both short and long text inputs produce well-aligned images. Interestingly, the flowers in Fig. 6 (4) are depicted with bright colors, which can be traced back to the training cartoon data where some objects are highlighted with colors for emphasis, as shown in Fig. 8 (9). The third row in Figure 1 displays image-to-text generation results, where the model successfully generates accurate text descriptions for images within the domain. posed DoraCycle. In Fig. 7, we visualize the process of the model gradually adapting to the target domain, i.e. the cartoon domain. It can be observed that, given the same text prompt, the images generated by the model become increasingly aligned with the target domain characteristics as training steps increase. Notably, the three sets of examples in Fig. 7 reveal different levels of difficulty in mapping samples to the target domain. For instance, the second set, frog sitting on lily pad, watching dragonfly hovering nearby, requires more training to generate an image that fits well within the target domain, while the first set is much easier. E. Training Data Examples The fourth and fifth rows of Fig. 6 illustrate the adaptation performance of the model to the domain that requires paired knowledge. We collected 2k images from the Mr. Bean movie and annotated each character with 1-3 text descriptions. The adapted model accurately generates images given text prompts, including the character identities, and provides precise descriptions for input images, demonstrating the effective domain adaptation capabilities of the proIn Fig. 8, we present some examples of the training data of DoraCycle to adapt the unified generative models to different domains. For domains that do not require paired knowledge, DoraCycle uses purely unpaired data. The rows 1-2 in Fig. 8 show examples of the image data, while the text data uses text from the pre-trained dataset with automatically added domain-specific prompts, such as <soc> Cyberpunk Style <eoc>. For domains that require paired 3 Figure 8. Examples of the training data for DoraCycle to adapt the unified model to different target domains. knowledge, we provide large amount of unpaired data, as shown on the right side of rows 3-5 in Fig. 8, along with small amount of paired data (around 1%), as shown on the left side of rows 3-5 in Fig. 8. F. Limitations and Future Work In the current setup, the multimodal cycles only involve text prompts that directly describe visual content, i.e. captions, limiting the multimodal understanding abilities to caption generation. Extending the framework to include visual question answering is an interesting direction for future work. Directly involving question-answer in the multimodal cycles poses challenges, as the question-answer pair often covers only specific part of the image, potentially missing many visual details when performing the text-toimage generation process, leading to large cycle deviation. One potential solution is to leverage the internal language reasoning capabilities of unified models to generate more complete textual descriptions for visual content based on question-answer pair, and then proceed with the multimodal cycles."
        }
    ],
    "affiliations": [
        "Show Lab, National University of Singapore"
    ]
}