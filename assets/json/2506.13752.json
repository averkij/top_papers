{
    "paper_title": "Steering LLM Thinking with Budget Guidance",
    "authors": [
        "Junyan Li",
        "Wenshuo Zhao",
        "Yang Zhang",
        "Chuang Gan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent deep-thinking large language models often reason extensively to improve performance, but such lengthy reasoning is not always desirable, as it incurs excessive inference costs with disproportionate performance gains. Controlling reasoning length without sacrificing performance is therefore important, but remains challenging, especially under tight thinking budgets. We propose budget guidance, a simple yet effective method for steering the reasoning process of LLMs toward a target budget without requiring any LLM fine-tuning. Our approach introduces a lightweight predictor that models a Gamma distribution over the remaining thinking length during next-token generation. This signal is then used to guide generation in a soft, token-level manner, ensuring that the overall reasoning trace adheres to the specified thinking budget. Budget guidance enables natural control of the thinking length, along with significant token efficiency improvements over baseline methods on challenging math benchmarks. For instance, it achieves up to a 26% accuracy gain on the MATH-500 benchmark under tight budgets compared to baseline methods, while maintaining competitive accuracy with only 63% of the thinking tokens used by the full-thinking model. Budget guidance also generalizes to broader task domains and exhibits emergent capabilities, such as estimating question difficulty. The source code is available at: https://github.com/UMass-Embodied-AGI/BudgetGuidance."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 2 5 7 3 1 . 6 0 5 2 : r a"
        },
        {
            "title": "Steering LLM Thinking with Budget Guidance",
            "content": "Junyan Li UMass Amherst junyanli@umass.edu Wenshuo Zhao Zhejiang University zhao_ws@zju.edu.cn Yang Zhang MIT-IBM Watson AI Lab yang.zhang2@ibm.com Chuang Gan UMass Amherst chuangg@cics.umass.edu"
        },
        {
            "title": "Abstract",
            "content": "Recent deep-thinking large language models often reason extensively to improve performance, but such lengthy reasoning is not always desirable, as it incurs excessive inference costs with disproportionate performance gains. Controlling reasoning length without sacrificing performance is therefore important, but remains challenging, especially under tight thinking budgets. We propose budget guidance, simple yet effective method for steering the reasoning process of LLMs toward target budget without requiring any LLM fine-tuning. Our approach introduces lightweight predictor that models Gamma distribution over the remaining thinking length during next-token generation. This signal is then used to guide generation in soft, token-level manner, ensuring that the overall reasoning trace adheres to the specified thinking budget. Budget guidance enables natural control of the thinking length, along with significant token efficiency improvements over baseline methods on challenging math benchmarks. For instance, it achieves up to 26% accuracy gain on the MATH-500 benchmark under tight budgets compared to baseline methods, while maintaining competitive accuracy with only 63% of the thinking tokens used by the full-thinking model. Budget guidance also generalizes to broader task domains and exhibits emergent capabilities, such as estimating question difficulty. The source code is available at: https://github.com/UMass-Embodied-AGI/BudgetGuidance."
        },
        {
            "title": "Introduction",
            "content": "With the recent success of deep-thinking large language models (LLMs) such as OpenAI O1 [15], DeepSeek R1 [7], and Qwen3 [25, 26], which are capable of generating long sequences of thoughts to achieve better performance there has been growing need to control the reasoning length of these models while maintaining the performance, because many deep-thinking LLMs often incur excessive inference costs with disproportionate performance gain. For example, in Figure 1, we show response from deep-thinking model that, while correct, is unnecessarily long. Such extensive reasoning is not always desirable, and there are cases where we need to impose budget to limit the extent of reasoning, particularly in scenarios that demand real-time interaction, such as customer-facing chatbots, where excessive latency can degrade user experience and responsiveness. Existing thinking budget control methods can be roughly divided into two categories with complementary strengths. The first category is fine-tuning methods, which fine-tune deep-thinking LLMs on specially curated dataset [9] or with budget-aware reward to enable budget control capabilities [14]. Fine-tuning methods have been shown effective in changing the reasoning length while keeping competitive performance because they allow LLMs to fundamentally restructure and optimize their reasoning behavior according to the given budget. However, they come with two main drawbacks. Figure 1: Deep-thinking models often produce excessively long reasoning traces, leading to high latency and unnecessary computation. Existing inference-time methods like budget forcing rely on simplistic heuristics such as abruptly stopping, which can result in incomplete reasoning and degraded answer quality. In contrast, our method, budget guidance, steers the reasoning process toward the target budget in smoother and more natural way, without any LLM fine-tuning. First, fine-tuning an LLM is costly, requiring substantial computational resources and time. Second, directly fine-tuning the LLM may potentially alter its behavior in unexpected ways, such as compromising safety [21]. The second category of methods is the inference-time methods [19, 20], which seek to alter the reasoning behavior at inference time. While these approaches do not involve fine-tuning, they often result in sub-optimal reasoning behaviors and significant performance degradation, because the intervention at inference time are often heuristic and overly simple, breaking the integrity of the original reasoning process. For example, one well-known inference-time method is budget forcing [20] which terminates the models reasoning as soon as the thinking budget is reached, as described in Figure 1. While this method offers strict control over the number of generated tokens, abruptly interrupting the model may cut off unfinished thoughts and force premature answers, often leading to incorrect outputs. In short, an important bottleneck in the task of thinking budget control lies in the tradeoff between non-intrusiveness (in inference-time approaches) and optimality of the reasoning chain (in fine-tuning approaches). This leads to our central research question: Can we design flexible inference-time budget control approach (without fine-tuning) that still allows for wholistic, principled restructuring of the reasoning process to maintain its quality under budget? In this paper, we introduce budget guidance, novel approach that employs lightweight auxiliary module to enable test-time control over the reasoning length of LLMs. Inspired by the principle of classifier guidance in diffusion models [4], we train an auxiliary predictor that predicts the probability distribution of the remaining reasoning length at each reasoning step. The predicted length distribution is then used to modulate the LLM generation probability, effectively turning it into budget-conditional generation probability. Our method avoids the direct fine-tuning of LLMs, while providing flexible and accurate control over the reasoning process. It can be seamlessly integrated into existing inference pipelines, and adapts to wide range of models, thinking budgets, and tasks. Our experiments have revealed several key highlights of our method. First, budget guidance exhibits remarkable trade-off between thinking length and performance. For example, as shown in Figure 1, on MATH-500 benchmark [12] budget guidance can reduce the full thinking length by 37% with minimal accuracy degradation, while been 26% higher in accuracy than budget forcing baseline under tight budget. Second, the auxiliary predictor is very successful in predicting the thinking length, effectively considering task difficulty and instruction type. Thus, it can accurately guide the thinking process under various budgets. Finally, our method demonstrates surprising generalizability across domains an auxiliary predictor trained on one dataset can also work well in other datasets and domains. We summarize our contributions as follows: 2 We propose budget guidance, novel test-time method for steering the reasoning process of LLMs toward specified thinking budget, without requiring any fine-tuning of the LLM itself. We design lightweight predictor that models Gamma distribution over the remaining reasoning length based on the current generation context, and uses this signal to guide LLM generation toward target thinking budget. Budget guidance achieves strong trade-offs between thinking length and accuracy across multiple benchmarks, and demonstrates cross-domain generalization, enabling effective budget control and accurate thinking length prediction."
        },
        {
            "title": "2 Related Works",
            "content": "2.1 Efficient LLM Reasoning Efficiency is fundamental topic in machine learning, and recent work has focused on improving the token efficiency of LLMs in long chain-of-thought reasoning. For example, ThinkPrune [14] employs reinforcement learning with an iterative pruning strategy to shorten reasoning traces; Z1 [27] enables efficient test-time scaling by training on data with varying reasoning lengths and introducing shifted thinking window for hybrid-mode inference; COCONUT [10] operates in continuous space to encourage reasoning with fewer tokens. While effective, these methods typically rely on expensive LLM fine-tuning and primarily aim to reduce the length of reasoning, rather than to control it. More recent approaches [9, 20] have begun exploring methods to control the reasoning length, either through heuristic rules or model fine-tuning. In contrast, we propose simple yet effective alternative: fine-tuning-free approach that naturally steers the reasoning process to adhere to specified thinking budget, enabling more efficient and flexible inference. 2.2 Guidance and Guided Generation The term guidance originates primarily from the diffusion model literature, where it denotes the ability to steer the generative process, often through truncated or low-temperature sampling, by reducing the variance or range of noise inputs to the generative model at sampling time [13]. This effectively transforms an unconditional diffusion model into conditional one, enabling it to generate targeted outputs. One of the earliest examples is classifier guidance [4], which modifies the diffusion score by incorporating the gradient of the log-likelihood from an auxiliary classifier, thereby biasing the sampling process toward desired content. This can be viewed as form of guided generation, where image generation is conditioned on the output of classifier. similar notion of guided generation has emerged in the context of LLMs, where it typically refers to constraining the models output to satisfy structural requirements, such as regular expressions or context-free grammars, to ensure syntactic correctness for downstream applications [23]. To the best of our knowledge, our work is the first to extend the idea of guided generation to new dimension: budget-conditioned generation. Specifically, we introduce novel form of guidance that softly steers the LLMs generation to meet specified thinking budget, enabling efficient and controlled reasoning without compromising output quality."
        },
        {
            "title": "3 Budget Guidance",
            "content": "In Section 3.1, we begin by formulating the budgetWe now introduce our method in detail. conditioned generation problem and present the overall budget guidance framework, which draws inspiration from classifier guidance [4] in diffusion models. Section 3.2 describes the design of our proposed auxiliary thinking length predictor, which estimates the distribution over remaining reasoning length at each decoding step. In Section 3.3, we outline the training procedure for the predictor using reasoning traces. Section 3.4 introduces the model architecture of the predictor, which is designed to be lightweight and inference-efficient. Finally, Section 3.5 presents simple modulationskipping strategy to further reduce computational overhead during decoding. An illustration of our method is provided in Figure 2. 3 Figure 2: An overview of budget guidance. lightweight predictor uses the LLMs hidden states to predict Gamma distribution over the remaining reasoning length for each candidate token. We then use the CDF of Gamma distribution to compute predictor score, which is combined with the LLMs output score to guide generation. The result is soft, token-level steering toward budget-conditioned reasoning without any LLM fine-tuning. 3.1 The Budget Guidance Framework X, Y<t). The overall framework of our method follows the classifier guidance framework in diffusion generation [4], thus we name our framework budget guidance. Specifically, denote as the input question, Y<t as the LLMs output thinking process up to token t, and Yt as the LLMs output at token t. The LLM generation process essentially involves sampling from the following budget-unconditional distribution, p(Yt However, when there is budget constraint, we would need to draw from budget-conditional distribution. Formally, denote Lt as the random variable indicating the remaining length of the thinking process from token t. For example, if the overall thinking length is (i.e., the </think> t. Given the thinking budget limit l, the budget-conditional token occurs at token l), then Lt = distribution is defined as p(Yt According to Bayes rule, the budget-conditional distribution can be computed from the budgetunconditional distribution as follows X, Y<t, Lt t). p(Yt X, Y<t, Lt t) budget-conditional p(Yt X, Y<t) budget-unconditional r(Lt X, Y<t, Yt). (1) (cid:124) (cid:125) (cid:125) (cid:124) (cid:123)(cid:122) (cid:123)(cid:122) Therefore, at each token t, generating from the budget-conditional distribution involves three steps. First, compute the unconditional distribution, which is simply performing forward pass of the X, Y<t, Yt). Finally, use the LLM. Second, predict the remaining length distribution, r(Lt remaining length distribution to modulate the unconditional distribution and then renormalize. Therefore, within budget guidance framework, our task boils down to computing r(Lt X, Y<t, Yt). To this end, we introduce lightweight auxiliary thinking length predictor, which we describe in detail over the next three subsections. l 3.2 An Auxiliary Thinking Length Predictor . At each Denote the LLM vocabulary size as n, and denote the vocabulary as token t, the LLM outputs an n-dimensional unconditional probability vector (which we denote as ut): v1, . . . , vn} { = ut = [p(Yt = v1 X, Y<t), . . . , p(Yt = vn X, Y<t)]. (2) 4 According to Equation (1), the predictor needs to predict an n-dimensional vector (which we denote as at): at = [P r(Lt X, Y<t, Yt = v1), , r(Lt X, Y<t, Yt = vn)], (3) so that the budget-conditional probability vector, which we denote as ct, can be computed by element-wise multiplying the two vectors and renormalize: ct = normalize(ut at). (4) Equation (3) indicates that the predictor needs to accomplish rather intensive task: At each token t, given the question and all the context generated so far Y<t, the auxiliary predictor needs to ❶ traverse all possible values for Yt across the vocabulary, ❷ for each possible value, predict what would be the remaining length if Yt took on this value (that is probability distributions in total), and ❸ compute the cumulative probability up to To simplify the task, we parameterize each predicted distribution as Gamma distribution for log(Lt): for each distribution. p(Lt X, Y<t, Yt = vi) = Gamma(log(Lt); λt(vi), αt(vi)), (5) ; λ, α) represents the probability density function (PDF) of the Gamma distribution, where Gamma( with the shape parameter λ and rate parameter α. We model the distribution over log(Lt) instead of Lt directly to better capture the dynamic range of thinking lengths. With the Gamma distribution assumption, instead of predicting probability distributions, we only needs to predict two n-dimensional vectors: λt = [λt(v1), . . . , λt(vn)] and αt = [αt(v1), . . . , αt(vn)]. The cumulative probability, at, can be computed from the predicted λt and αt by the known closed-form cumulative distribution function (CDF) of the Gamma distribution. 3.3 Training the Predictor To train the predictor, we need to collect dataset of reasoning chains produced by the target LLM. Formally, the data in the dataset takes the following form: , where is the input } question, y1:l is the LLM-generated reasoning chain, and is the length of the reasoning chain. Note that the task dataset from which reasoning chain length training data are generated is not the same as the inference dataset (not even the same task), as we will show that the trained predictor has good dataset and task generalizability. For simplicity, in our training, we focus on math reasoning and use the OpenR1-Math-220k dataset [6]. (x, y1:l, l) = { For each training datum, (x, y1:l, l), we feed the information of partial reasoning chain to the predictor, truncated at different positions, and train the predictor to predict the remaining length. We adopt the maximum log likelihood objective for the gradient descent training. Formally, denote the parameters of the auxiliary predictor as θ. Then the training objective can be written as max θ (x,y1:l,l) 1 log pθ(Lt = (cid:20) t=1 (cid:88) (cid:0) = x, Y<t = y<t, Yt = yt) (cid:1) , (cid:21) (6) where pθ( ) represents the predicted PDF by the auxiliary predictor, as shown in Equation (5). 3.4 Architecture of the Predictor The predictor is designed to be lightweight enough to avoid significant computational overhead during decoding, yet expressive enough to capture both the input question and the ongoing reasoning context to produce meaningful estimate of the remaining reasoning length. To this end, we adopt BERT-base [3] as the backbone of our predictor. Its input consists of the concatenated hidden states from all layers of the last generated token of the target LLM, which encode rich semantic information about both the input question and the reasoning history. linear projection maps the LLMs hidden dimensionality to the predictors input space, and [CLS] token is used to summarize the hidden states. The final [CLS] representation is passed through another linear projection to produce an 2, where each row corresponds to the parameters λt and αt of Gamma output matrix distribution. softplus activation [5] is applied to ensure both parameters are non-negative. Rn 3.5 Skipping Modulation Ideally, probability modulation in Equation (4) would be applied at every decoding step t. To reduce computational overhead, however, we apply it only at the start of each reasoning paragraph, indicated by newline delimiters, where uncertainty is typically highest. The modulation is thus defined as: normalize(ut ut, at), if is the start of reasoning paragraph otherwise (7) ct = (cid:26) Empirically, we find that this modulation introduces only 0.6% increase in total latency for 7B-parameter LLM, which is negligible in practice."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Settings Training. We apply our method to three deep-thinking models: DeepSeek-R1-Distill-Qwen-7B (R1-7B) [7], DeepSeek-R1-Distill-Qwen-32B (R1-32B) [7], and Qwen3-8B [25, 26]. Training is conducted on OpenR1-Math-220k [6], dataset of 220k math problems from NuminaMath 1.5 [17] with reasoning traces generated by DeepSeek R1. We apply simple data augmentation technique (detailed in the Appendix) to double the dataset size. During training, the LLMs are frozen and only the predictor is updated. We train for one epoch using batch size of 8 and constant learning rate 4 after warmup. Training takes 15 hours for R1-7B and Qwen3-8B, and 35 hours for of 1.0 R1-32B, using 8 NVIDIA H100 GPUs. All evaluations are conducted on the same hardware setup. 10 Evaluation. We evaluate our method on four representative math reasoning benchmarks: MATH500 [12], AIME-2024 [1], AMC [2] (including both AMC12 2022 and AMC12 2023), and the math subset from OlympiadBench [11]. These benchmarks cover diverse mathematical topics, including arithmetic, algebra, combinatorics, etc., and span broad range of difficulty levels. Besides math benchmarks, we also extend our evaluation to broader domains to test the out-ofdomain transferability of our math-data-trained predictor. Specifically, we further evaluate on GPQA Diamond [22] for scientific reasoning, FOLIO [8] for logical reasoning, the numerical reasoning subset from TableBench [24] for tabular numerical reasoning, and LiveCodeBench [16] (2024-08 - 2025-01 following [7]) for code reasoning. All experiments are conducted in zero-shot manner, i.e., we do not perform further fine-tuning on the training sets of the evaluation benchmarks. We use greedy decoding for all evaluation. Baselines. We compare our method with other methods that also do not finetune the LLM. Our main baseline is budget forcing [20], which enforces hard token limit by appending an end-of-thinking delimiter (and optionally Final Answer:) to trigger early exit and force the model to produce its best guess. We use their open-sourced codebase for evaluation. We also include NoThinking [19] as baseline, which bypasses the reasoning stage by inserting fixed phrase as the thinking process: Okay, think have finished thinking. We also report results from the original model with full thinking as reference. 4.2 Main Results 4.2.1 Evaluation on Math Reasoning Benchmarks Since the predictor is trained on math data, we first evaluate its performance on math reasoning benchmarks to assess in-domain effectiveness. We set the thinking budget to approximately half the original models full thinking length and ensure the average thinking length (denoted as #Tokens) comparable between our method and the baseline, and report the task accuracy. Table 1 summarizes the evaluation results on math reasoning benchmarks. Across all three models and four datasets, budget guidance consistently outperforms budget forcing under comparable average thinking lengths, effectively reducing the reasoning length without causing significant accuracy degradation. Compared to NoThinking, budget guidance achieves substantially higher performance, indicating that the reasoning traces are non-trivial and contribute meaningfully to task success. 6 Table 1: Evaluation results on math benchmarks. MATH-500 AIME-2024 AMC Acc. #Tokens Acc. #Tokens Acc. #Tokens Acc. OlympiadBench #Tokens Thinking NoThinking Budget Forcing Budget Guidance Thinking NoThinking Budget Forcing Budget Guidance Thinking NoThinking Budget Forcing Budget Guidance 91.6 74.8 86.0 88.2 93.2 68.2 86.4 90.0 96.2 84.8 90.2 93.0 2598 - 1547 1329 2226 - 1525 4613 - 2545 2062 DeepSeek-R1-Distill-Qwen-7B 4338 - 1872 1768 4446 - 2015 2046 78.3 47.0 55.4 60.2 36.7 23.3 16.7 33.3 DeepSeek-R1-Distill-Qwen-32B 4156 - 1567 7694 - 2936 2873 77.1 47.0 50.6 69.9 72.6 20.0 40.0 56.7 73.3 33.3 43.3 50.0 Qwen3-8B 91.6 56.6 77.1 80.7 13660 - 4010 8740 - 3807 3869 56.9 40.4 47.1 54.2 61.9 41.9 50.7 57.8 71.7 53.5 61.6 65.6 3960 - 1844 1755 3435 - 1797 9424 - 3712 3639 Figure 3: Accuracy vs. thinking length on math benchmarks. These improvements are consistent across different model sizes (7B to 32B) and model families (DeepSeek vs. Qwen), highlighting the general applicability of our approach to diverse deep-thinking LLMs. Notably, even though the predictor for Qwen3-8B is trained on reasoning traces generated by DeepSeek-R1, it still performs well. This suggests that the training data can be model-agnostic, provided the target LLM exhibits similar reasoning style, for instance, using words like wait or alternatively to structure its reasoning process. 4.2.2 AccuracyThinking Length Tradeoff Analysis key indicator of effective control is the ability to achieve higher accuracy under the same thinking length, which we call token efficiency. To evaluate and compare the token efficiency of our method across different reasoning lengths, we vary the token budget to obtain different average thinking lengths and record the corresponding accuracy achieved by the model. We visualize this relationship through accuracy-thinking length trade-off curves. Experiments are conducted on all three models across the four math benchmarks, and the resulting plots are presented in Figure 3. Figure 4: Thinking length controllability measured on MATH-500 benchmark. Table 2: Evaluation on out-of-domain transferability. GPQA Diamond #Tokens Acc. Thinking NoThinking Budget Forcing Budget Guidance 49.1 38.4 39.9 49.0 5838 - 1895 Acc. #Tokens Acc. 63.5 46.3 60.1 61.6 849 - 372 362 37.0 16.9 22.4 26.7 906 - 379 26.9 20.7 28.8 29.4 3509 - 1135 1138 FOLIO TableBench #Tokens Acc. LiveCodeBench #Tokens From Figure 3, we observe that our method consistently achieves better token efficiency across most benchmarks, achieving higher accuracy than budget forcing under range of thinking lengths. Notably, as the average thinking length decreases, corresponding to stricter budget constraints, our method yields significantly higher accuracy, particularly on benchmarks with diverse problem difficulty such as MATH-500. We attribute this to the ability of our method to adapt the reasoning pattern under strict budgets, producing concise yet complete reasoning traces. This enables the model to arrive at correct answers more efficiently, especially for questions that are relatively easy and do not require deep reasoning. This is also reflected in the occasional worse accuracy of budget forcing compared to the NoThinking baseline under strict budgets (e.g., MATH-500 on DS-7B/32B), where the reasoning trace is abruptly truncated and the model is forced to guess prematurely. In contrast, our method avoids such incomplete reasoning and consistently outperforms the NoThinking baseline. An illustrative example of this guided reasoning behavior is provided in Section 4.4. 4.2.3 Fine-Grained Control of Thinking Length Our goal is to steer LLM reasoning to adhere to specified thinking budget. To evaluate controllability, we test on MATH-500 under varying thinking budgets, measuring the actual thinking length per sample and visualizing the distributions. We compare our method to budget forcing and include the full-thinking baseline as reference. Results across all three models are shown in Figure 4. From Figure 4, we observe that our method behaves similar to budget forcing, generally respects the specified thinking budget: for each setting, at least 75% are within the budget, and the median thinking length closely aligns with the budget. Compared to the full-thinking baseline, our method guides the model to generate budget-aligned reasoning trajectory. This behavior is notable because, unlike budget forcing, our approach does not enforce hard cutoff. Instead, it softly steers the generation process to match the desired level of detail, demonstrating flexible and controllable reasoning. 4.2.4 Out-of-Domain Transferability While we train the predictor solely on math data for simplicity, we also explore its generalization to broader task domains. To this end, we conduct an out-of-domain transferability analysis using the DS-7B model. Specifically, we evaluate our method on four benchmarks: GPQA Diamond (scientific reasoning), FOLIO (logical reasoning), TableBench (tabular reasoning), and LiveCodeBench (code reasoning). We match the average reasoning length between our method and the baseline, and report the corresponding accuracies in Table 2. Despite being trained exclusively on math data, our predictor generalizes well to non-math reasoning tasks, consistently outperforming budget forcing across all four benchmarks. These results highlight 8 Figure 5: Correlation between question difficulties and estimated thinking lengths. Figure 6: Correlation between prompt types and estimated thinking lengths. Figure 7: Sample reasoning traces generated with budget guidance under different thinking budgets. the cross-domain generalizability of our approach and its potential applicability to wide range of reasoning scenarios. While the gains on out-of-domain tasks are less pronounced than those on in-domain benchmarks, we believe performance can be further improved by incorporating reasoning traces from broader range of domains during training. We leave this direction for future work. 4.3 Insights into What the Predictor Learns To probe what the predictor has learned, we analyze its estimated thinking length at the first thinking token, interpreted as the predicted number of thinking tokens needed, against task difficulty and prompt type, using the DS-7B model. Task Difficulty. We evaluate on MATH-500 (in-domain) and LiveCodeBench (out-of-domain). Figure 5 shows that estimated thinking length increases with difficulty in both cases. This suggests that the predictor captures general understanding of difficulty, enabling effective difficulty estimation. Prompt Type. We evaluate on MATH-500 and compare two prompts: one encouraging long reasoning and one encouraging concise reasoning (listed in the Appendix). As shown in Figure 6, the long reasoning prompt yields longer estimated thinking lengths. t-test gives p-value of 0.0028, confirming the difference is statistically significant and indicating that the predictor is prompt-aware. 4.4 Case Study Figure 7 shows case study from MATH-500 illustrating reasoning traces under different thinking budgets. Rather than truncating output, our method adapts the reasoning style to the budget. With stricter budget (left), the model generates concise answers without reflection. With larger budget (right), it mirrors full-length reasoning: it starts with problem analysis and using reflective phrases like wait and double-checking. In both settings, the trace ends appropriately, highlighting our methods flexibility and controllability."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce budget guidance, simple yet effective approach for steering LLM reasoning under thinking budget. Without requiring any LLM fine-tuning, our method enables natural control over the reasoning process and significantly improves token efficiency on challenging benchmarks. Our results demonstrate that LLMs can be effectively guided to reason with budget guidance, highlighting budget-conditioned generation as promising direction for efficient and controllable LLM reasoning."
        },
        {
            "title": "References",
            "content": "[1] Art of Problem Solving. Aime problems and solutions, n.d. [2] Art of Problem Solving. Amc 12 problems and solutions, n.d. [3] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 41714186, 2019. [4] P. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [5] C. Dugas, Y. Bengio, F. Bélisle, C. Nadeau, and R. Garcia. Incorporating second-order functional knowledge for better option pricing. Advances in neural information processing systems, 13, 2000. [6] H. Face. Open r1: fully open reproduction of deepseek-r1, January 2025. [7] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [8] S. Han, H. Schoelkopf, Y. Zhao, Z. Qi, M. Riddell, W. Zhou, J. Coady, D. Peng, Y. Qiao, L. Benson, et al. Folio: Natural language reasoning with first-order logic. arXiv preprint arXiv:2209.00840, 2022. [9] T. Han, Z. Wang, C. Fang, S. Zhao, S. Ma, and Z. Chen. Token-budget-aware llm reasoning. arXiv preprint arXiv:2412.18547, 2024. [10] S. Hao, S. Sukhbaatar, D. Su, X. Li, Z. Hu, J. Weston, and Y. Tian. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. [11] C. He, R. Luo, Y. Bai, S. Hu, Z. L. Thai, J. Shen, J. Hu, X. Han, Y. Huang, Y. Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. [12] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [13] J. Ho and T. Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 10 [14] B. Hou, Y. Zhang, J. Ji, Y. Liu, K. Qian, J. Andreas, and S. Chang. Thinkprune: Pruning long chain-of-thought of llms via reinforcement learning. arXiv preprint arXiv:2504.01296, 2025. [15] A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low, A. Helyar, A. Madry, A. Beutel, A. Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [16] N. Jain, K. Han, A. Gu, W.-D. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. [17] J. Li, E. Beeching, L. Tunstall, B. Lipkin, R. Soletskyi, S. Huang, K. Rasul, L. Yu, A. Q. Jiang, Z. Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13:9, 2024. [18] H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. [19] W. Ma, J. He, C. Snell, T. Griggs, S. Min, and M. Zaharia. Reasoning models can be effective without thinking. arXiv preprint arXiv:2504.09858, 2025. [20] N. Muennighoff, Z. Yang, W. Shi, X. L. Li, L. Fei-Fei, H. Hajishirzi, L. Zettlemoyer, P. Liang, E. Candès, and T. Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [21] X. Qi, Y. Zeng, T. Xie, P.-Y. Chen, R. Jia, P. Mittal, and P. Henderson. Fine-tuning aligned arXiv preprint language models compromises safety, even when users do not intend to! arXiv:2310.03693, 2023. [22] D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. In First Conference on Bowman. Gpqa: graduate-level google-proof q&a benchmark. Language Modeling, 2024. [23] B. T. Willard and R. Louf. Efficient guided generation for large language models. arXiv preprint arXiv:2307.09702, 2023. [24] X. Wu, J. Yang, L. Chai, G. Zhang, J. Liu, X. Du, D. Liang, D. Shu, X. Cheng, T. Sun, et al. Tablebench: comprehensive and complex benchmark for table question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2549725506, 2025. [25] A. Yang, B. Yang, B. Hui, B. Zheng, B. Yu, C. Zhou, C. Li, C. Li, D. Liu, F. Huang, G. Dong, H. Wei, H. Lin, J. Tang, J. Wang, J. Yang, J. Tu, J. Zhang, J. Ma, J. Xu, J. Zhou, J. Bai, J. He, J. Lin, K. Dang, K. Lu, K. Chen, K. Yang, M. Li, M. Xue, N. Ni, P. Zhang, P. Wang, R. Peng, R. Men, R. Gao, R. Lin, S. Wang, S. Bai, S. Tan, T. Zhu, T. Li, T. Liu, W. Ge, X. Deng, X. Zhou, X. Ren, X. Zhang, X. Wei, X. Ren, Y. Fan, Y. Yao, Y. Zhang, Y. Wan, Y. Chu, Y. Liu, Z. Cui, Z. Zhang, and Z. Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. [26] A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, H. Lin, J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Lin, K. Dang, K. Lu, K. Bao, K. Yang, L. Yu, M. Li, M. Xue, P. Zhang, Q. Zhu, R. Men, R. Lin, T. Li, T. Xia, X. Ren, X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Wan, Y. Liu, Z. Cui, Z. Zhang, and Z. Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [27] Z. Yu, Y. Wu, Y. Zhao, A. Cohan, and X.-P. Zhang. Z1: Efficient test-time scaling with code. arXiv preprint arXiv:2504.00810, 2025."
        },
        {
            "title": "A Quantitative Reasoning Behavior Analysis",
            "content": "To quantitatively analyze how the predictor influences the reasoning behavior of LLMs under different budget settings, we follow the methodology proposed by [14]. Specifically, we count the frequency of reasoning-related keywords such as wait and alternatively, which are indicative of deeper reasoning processes. We compare the keyword frequencies for thinking budget of 500, 2000, and 4000 tokens using the DS-7B model on the MATH-500 benchmark. These results are contrasted with full-thinking baseline (i.e., without applying our method). The comparison is illustrated in Figure 8. Figure 8: Reasoning keywords frequency comparison under different budget settings. As shown in the figure, smaller budget substantially reduces the frequency of reasoning-related keywords, indicating more concise reasoning process. As the budget increases, the model is encouraged to engage in deeper reasoning. Notably, when the budget is set sufficiently high, the behavior closely matches that of the full-thinking baseline, suggesting minimal loss in reasoning capability. These findings demonstrate that our method can effectively steer the reasoning behavior of LLMs, while still preserving their reasoning ability under higher budget constraints."
        },
        {
            "title": "B Additional Reasoning Samples",
            "content": "We provide additional reasoning samples to facilitate clearer understanding of our method. We selected illustrative examples from MATH-500. Compared to budget forcing [20], our method demonstrates more precise and rational thinking length control. Under identical budget constraints, our method successfully solves example problems where budget forcing fails. B.1 Comparative Analysis: Budget Forcing vs. Budget Guidance We first conduct comparison between our baseline, budget forcing, and our method, budget guidance, under the same thinking budget. Figure 9 and 10 present cases with 1000-token thinking budget: The first case in Figure 9 shows our method precisely controlling thinking length to just under this limit, while the second case in Figure 10 illustrates drastically shortened reasoning process that still achieves accurate solution. These examples collectively demonstrate the superior performance of our method in effectively controlling the reasoning length within the specified thinking budget, while consistently arriving at the correct answer regardless of problem difficulty. 12 Figure 9: Budget forcing and budget guidance generate similar number of thinking tokens under specified thinking budget, but only budget guidance leads to the correct answer. B.2 Reasoning Behavior Under Varying Budgets We further demonstrate our methods ability to control thinking length under varying budget constraints. The results, illustrated in Figure 11, show that our method performs excellently across diverse budget settings, flexibly steering the reasoning while ensuring accurate reasoning process."
        },
        {
            "title": "C Dataset Description",
            "content": "We provide detailed information about the evaluation datasets used in our paper. MATH-500 [12] is 500-problem subset of the MATH dataset, selected by [18]. Each problem is labeled with difficulty level from 1 to 5. AIME-2024 [1] contains 30 problems from the 2024 American Invitational Mathematics Examination, covering topics such as algebra, combinatorics, geometry, number theory, and probability. Following budget forcing [20], we retain only the essential ASY figure code required to solve each problem, omitting non-essential diagrams. AMC [2] includes all 83 problems from AMC12 2022 and AMC12 2023. OlympiadBench [11] is challenging benchmark aimed at advancing AGI through Olympiad-level, bilingual, multimodal scientific problems. We use its math subset, which contains total of 675 problems. GPQA Diamond [22] consists of 198 high-quality, extremely difficult questions spanning broad range of scientific domains, including biology, physics, and chemistry. 13 Figure 10: Another example demonstrating that budget guidance can solve question correctly using fewer thinking tokens. FOLIO [8] is human-annotated dataset designed to evaluate complex logical reasoning in natural language. It features 1,430 unique conclusions paired with 487 sets of premises, all validated using first-order logic (FOL) annotations. We use the test set, which contains 203 unique problems. TableBench [24] is benchmark for evaluating LLMs on real-world tabular data challenges. We evaluate all models on the numerical reasoning subset, which comprises 493 problems. LiveCodeBench [16] offers holistic and contamination-free evaluation of LLM coding capabilities. Following [7], we select problems from the August 2024 to January 2025 period, totaling 323 problems."
        },
        {
            "title": "D Training Data Augmentation",
            "content": "We adopt simple data augmentation strategy to double the size of the training set. Each training sample originally follows the format: <think>THINK_MESSAGE</think>ANSWER_MESSAGE (8) Since our predictor only operates on the THINK_MESSAGE, the ANSWER_MESSAGE is not used during training. To utilize this otherwise unused information, we construct an additional training sample in the following format: <think>ANSWER_MESSAGE</think>ANSWER_MESSAGE (9) This transformation allows us to incorporate the ANSWER_MESSAGE into the predictors training process. By generating one new sample for each original sample, we effectively double the size of the training set and ensure full utilization of the available data. 14 Figure 11: Another example illustrating how budget guidance effectively steers LLM reasoning."
        },
        {
            "title": "E Prompt Description",
            "content": "In Section 4.3, we analyze the predictors estimated thinking length across different prompt types to demonstrate its prompt awareness. Below, we list the specific prompts used in our experiment. The prompt for long reasoning is: Think step by step and provide thorough reasoning before reaching conclusion. The prompt for short reasoning is: Think quickly and provide concise reasoning with minimal steps. We add these prompts as the system prompt."
        }
    ],
    "affiliations": [
        "MIT-IBM Watson AI Lab",
        "UMass Amherst",
        "Zhejiang University"
    ]
}