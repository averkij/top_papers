{
    "paper_title": "VisionArena: 230K Real World User-VLM Conversations with Preference Labels",
    "authors": [
        "Christopher Chou",
        "Lisa Dunlap",
        "Koki Mashita",
        "Krishna Mandal",
        "Trevor Darrell",
        "Ion Stoica",
        "Joseph E. Gonzalez",
        "Wei-Lin Chiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the growing adoption and capabilities of vision-language models (VLMs) comes the need for benchmarks that capture authentic user-VLM interactions. In response, we create VisionArena, a dataset of 230K real-world conversations between users and VLMs. Collected from Chatbot Arena - an open-source platform where users interact with VLMs and submit preference votes - VisionArena spans 73K unique users, 45 VLMs, and 138 languages. Our dataset contains three subsets: VisionArena-Chat, 200k single and multi-turn conversations between a user and a VLM; VisionArena-Battle, 30K conversations comparing two anonymous VLMs with user preference votes; and VisionArena-Bench, an automatic benchmark of 500 diverse user prompts that efficiently approximate the live Chatbot Arena model rankings. Additionally, we highlight the types of question asked by users, the influence of response style on preference, and areas where models often fail. We find open-ended tasks like captioning and humor are highly style-dependent, and current VLMs struggle with spatial reasoning and planning tasks. Lastly, we show finetuning the same base model on VisionArena-Chat outperforms Llava-Instruct-158K, with a 17-point gain on MMMU and a 46-point gain on the WildVision benchmark. Dataset at https://huggingface.co/lmarena-ai"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 1 ] . [ 1 7 8 6 8 0 . 2 1 4 2 : r VisionArena: 230K Real World User-VLM Conversations with Preference Labels Christopher Chou* Stanford Lisa Dunlap* UC Berkeley"
        },
        {
            "title": "Ion Stoica\nUC Berkeley",
            "content": "Joseph E. Gonzalez UC Berkeley Wei-Lin Chiang UC Berkeley Figure 1. Samples from VisionArena Conversations. VisionArena contains conversations from real users covering variety of domains."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction With the growing adoption and capabilities of visionlanguage models (VLMs) comes the need for benchmarks that capture authentic user-VLM interactions. In response, we create VisionArena, dataset of 230K real-world conversations between users and VLMs. Collected from Chatbot Arena an open-source platform where users interact with VLMs and submit preference votes VisionArena spans 73K unique users, 45 VLMs, and 138 languages. Our dataset contains three subsets: VisionArena-Chat, 200k single and multi-turn conversations between user and VLM; VisionArena-Battle, 30K conversations comparing two anonymous VLMs with user preference votes; and VisionArena-Bench, an automatic benchmark of 500 diverse user prompts that efficiently approximate the live Chatbot Arena model rankings. Additionally, we highlight the types of question asked by users, the influence of response style on preference, and areas where models often fail. We find open-ended tasks like captioning and humor are highly style-dependent, and current VLMs struggle with spatial reasoning and planning tasks. Lastly, we show finetuning the same base model on VisionArena-Chat outperforms Llava-Instruct-158K, with 17-point gain on MMMU and 46-point gain on the WildVision benchmark. Dataset at https://huggingface.co/lmarena-ai. *Equal contribution. Visual language models (VLMs) [2, 3, 32, 36] are being increasingly used in wide range of real-world applications including image captioning and story telling, document understanding, web development, and embodied systems. While these models have made remarkable progress on wide range of benchmarks [4, 14, 17, 18, 43, 45], existing VLM benchmarks focus largely on static, single-turn tasks with predetermined correct answers, overlooking the open-ended, evolving nature of real-world user interactions. They also rarely capture multi-turn dialogue, incorporate diverse context, or reflect the fluidity of user intent. As such, they provide simplified snapshot of VLM capabilities. Understanding these real-world interactions across variety of tasks is essential for developing models that align with human expectations and perform effectively. To address this, previous works such as Chatbot Arena [9] and WildVision [24, 25] crowdsource evaluation by hosting platforms where users can freely interact with pairs of VLMs and provide preference votes. Building off of these works, we introduce VisionArena, dataset of 230K realworld conversations between users and 38 VLMs in 135 languages, collected through the Chatbot Arena platform. VisionArena consists of: VisionArena-Chat: 200,000 single-turn and multi-turn chat logs between users and VLMs, covering wide range of queries and responses. Dataset # Convs # Models # Users # Langs % Unique Images Avg. # Turns Avg. # Tokens Avg. # Tokens per Response per Sample per Prompt LMSYS-Chat-1M WildVision-Battle WildVision-Chat 1,000,000 10,383 45,170 VisionArena-Battle VisionArena-Chat 30,000 200, 25 19 9 17 45 13,500 - - 14,031 72.933 35 28 26 90 - 56.2 33.4 76.4 62.1 2.0 1.2 1.4 1.3 1.5 36.9 57.8 81.3 90.2 184. 214.2 131.7 171.6 393.6 634.3 Human Preference No Yes No Yes No Table 1. Dataset Comparison. Compared to previous VLM preference benchmarks, VisionArena contains 3x the amount of data, with more users, language, models, unique images, and conversation turns. VisionArena-Battle: 30,000 conversations where users interact with two anonymized VLMs, along with preference votes indicating which response they prefer. VisionArena-Bench: An automatic benchmark consisting of 500 diverse user prompts that can be used to cheaply approximate model rankings via automatic benchmarking with VLM as judge. We conduct analysis of these datasets and construct set of popular question categories including captioning, OCR, humor, creative writing, entity recognition, and diagram understanding. We also explore the influence of stylistic properties of responses such as response length, markdown, and specificity on human preference. We find that more open-ended questions like captioning and humor are heavily influenced by style, which causes certain models like InternVL to have disproportionately higher ranking in these categories. We provide this metadata with VisionArena to enable further analysis. We also highlight common failure modes of VLMs and provide small curated set of user prompts where top proprietary models fail, including complex spatial reasoning and planning tasks. Next, we demonstrate how VisionArena can be used to improve VLMs through instruction finetuning. Compared to LLaVA-Instruct-158K [23], by finetuning on data from VisionArena-Chat, models show 17 point improvement in MMMU [43] and 46 point improvement on the human preference benchmark WV-Bench [25]. In addition to VisionArena, we also release this finetuned model. Lastly, we build on existing work in automatic benchmarking of VLMs to show that evaluating on the 500 prompts in VisionArenaBench results in model lineup that is consistent with the much larger online preference leaderboard Chatbot Arena. When compared with other automatic preference benchmarks like WildVision-Bench, VisionArena-Bench is far more predictive of the online Chatbot Arena VLM leaderboard performance, which contains over 100,000 user votes as of October 23rd, 2024. We believe that VisionArena is valuable resource to better understand how people are currently using VLMs and will be the foundation for research in VLM development and evaluation. In the future we plan to continue regular data releases including large variety of models and multi-image conversations. 2. Related Works Crowdsourced Evaluations. In the past few years, several platforms have emerged that aim to crowdsource evaluation for LLMs and VLMs by allowing users to provide preference votes. These platforms, such as Chatbot Arena [9], allow anyone to freely engage in open-ended conversations with state-of-the-art commercial and open-source models. Users are able to directly chat with specific models or chat with pairs of anonymous models side-by-side. In the anonymous side-by-side mode, users can provide direct feedback on which responses they preferred, which is used to build leaderboard. WildVision [25] adopts similar style to Chatbot Arena except that users interact with VLMs instead of LLMs. Our platform builds upon these works by creating unified interface that allows users to chat with either LLMs or VLMs. Public Chat Datasets: LMSYS-Chat-1M [46], OpenAssistant [20], and WildChat-1M [10, 20] are all public datasets constructed by capturing users conversations with state-of-the-art LLMs. These datasets have been highly influential because they represent more natural human conversations and often contain reward feedback signals. However, because these datasets capture text-only conversations, they do not provide insight into how users incorporate images into conversations and the behavior of VLMs. Building on the success of public chat datasets, there is recent effort to extend the public chat datasets to the visual domain with WildVision, dataset of 45k chat logs with 9 visual question answering models and 10.4K battle logs across 19 models. In contrast, our VisionArena data set contains 200K chat logs and 30K battle logs across 40+ models, including all of the strong proprietary models and many open-source models, making it the largest and most complete VLM conversation dataset to date. See Table 1 for dataset comparison. VLM Benchmarks: Currently, VLM benchmarks are typically static datasets that have close-ended ground truth answers (either multiple-choice or predefined-string). Some popular examples of these benchmarks include MMMU [43], DocVQA [28], MME [12], and VQA 2.0 [15]. To combat against static nature of datasets and minimize test2 set contamination, live benchmarks are also available. For example, LiveXiv incorporates updated ArXiv manuscripts for VQA [37]. To incorporate benchmarking on open-ended responses, there has been trend towards using strong models (e.g. GPT-4o) for VLM-as-a-judge to approximate human preference. Some notable examples include WildVision-Bench [25] and Prometheus-Vision [21]. We similarly adopt the VLM-as-a-judge framework to create VisionArena-Bench, curated from questions from VisionArena, allowing it to be crowdsourced, open, and live. We show that using VisionArena-Bench, we achieve better correlation and agreement with the VLM leaderboard on Chatbot Arena, which itself has 100x more votes. 3. Dataset and Platform 3.1. Interface VisionArena-Chat and VisionArena-Battle were collected from Chatbot Arena [9], an open-source platform for evaluating large language models by human preference. On our platform, users are able to directly chat with specific models (direct chat) or chat with pairs of anonymous models sideby-side (battle mode). In battle mode, users can provide direct feedback on which responses they preferred, which is used to build leaderboard. We refer to these anonymous side-by-side chats as battles. An example of our interface can be found in the supplemental. Unlike previous VLM crowdsourcing platforms, we integrate LLMs and VLMs into one unified chat interface with simple routing mechanism. In side-by-side chat, if user uploads an image in the first turn of their conversation, we automatically select from two available VLMs; otherwise we sample from the available LLMs. We then collect the votes from the image conversations to compute the VLM leaderboard. We believe this encourages users who may have initially been interested in interacting with LLMs to interact with VLMs as well. In Sec. 4, we show that our conversations do have important distributional differences with WildVision. Before using our service, users must accept terms of use, giving us their consent to store and release the conversation data. The platform is free to use, and there is no registration process. We are supported by sponsorships with inference providers. VisionArena is subset of conversations collected from February 2024 to September 2024. Given the language and question distribution of our collected conversations (Fig. 15, Fig. 5), the majority of our users are likely located in North America, Europe, and East Asia and work in STEM related fields. To encourage user interaction, we provide random image button, which samples from preset bank of images from 5 datasets: NewYorker [19], ChartVQA [27], DocVQA [28], TextVQA [38], and WikiArt [40]. We exclude these in VisionArena-Battle as we aim to capture the natural distribution of user inputs when computing leaderboard rankings, but we do include conversations with preset images in VisionArena-Chat, which make up around 15% of conversations. Moderation. We apply several moderation steps before sending the prompt to the model provider and perform data cleaning procedures before releasing this data to the public. Before the user receives the response from the model, we perform (not safe for work) NSFW image detection and (child sexual abuse material) CSAM [30] image detection and then tag and terminate conversations that contain inappropriate content. For battles, we also perform OpenAI text moderation [33] on user text prompts and discard any responses which contain violation. For direct chats, we only perform OpenAI text moderation on proprietary models to follow their usage policies. We do not perform text moderation on prompts for open-source models, which opens this data for future analysis. Finally, as part of our data release process we use Googles Vision API [13] to remove personally identifiable information (PII) from both images and text, removing any content containing human faces or identifiable details. However, these automated detectors are not infallible, so our dataset may still contain NSFW content or PII. We encourage users who find such instances to notify the authors so the material can be removed. 3.2. From Preference to Leaderboard Ranking Using preference votes from pairwise battles in anonymous side-by-side chat, we apply Bradley-Terry (BT) model [6] to estimate the relative strengths of models through logistic regression. The models coefficients serve as arena scores, which determine the leaderboard rankings. Let denote the number of pairwise comparisons (battles) and the number of models. For each battle [n], we define: Xi RM : Xi,m = 1 if model is presented first to the judge, Xi,m = 1 if presented last, and 0 otherwise. Yi 0, 1: The outcome, where 1 indicates the first model won. The BT model estimates model strengths β RM through logistic regression: ˆβ = arg min βRM 1 n (cid:88) i=1 CE(σ(X β), Yi) (1) where CE represents the cross-entropy loss and σ is the sigmoid function. The BT coefficients ˆβ are the ratings associated with each of the VLMs in the arena. These BT ratings are used to create the ordered ranking of models on the leaderboard. We bootstrap the BT rating estimate 100 times to construct confidence interval for each rating  (Fig. 2)  . 3 We then compute BT scores using the expert labels and compare those scores with the BT scores computed directly on the VisionArena. We then compute the Pearson correlation coefficient [34] and the Spearman rank correlation coefficient [39]. We obtain Pearson correlation of 0.88 indicating strong linear predictive relationship between the BT scores computed by experts and those obtained from the live VisionArena. We obtain Spearman rank correlation of 0.87 indicating high agreement in the ordering between the leaderboard rankings and the ranking obtained by our expert labelers on the small subset of data. Additionally, for subset of 100 battles labeled by 3 experts, we observe an agreement of 0.72 (excluding ties) and 0.56 (including ties) between users and expert annotators, compared to 0.77 (excluding ties) and 0.59 (including ties) among expert annotators themselves, further demonstrating the reliability of user votes. 4.2. What types of questions do people ask? We perform topic modeling analysis on the VisionArenaChat prompts. Following the BERTopic framework, We randomly sample 50K English conversations and embed the documents using CLIP-ViT-B-32, perform DBSCAN clustering, and use GPT-4o to summarize each cluster [16]. We plot our top 5 clusters in Fig. 3 (the top 20 clusters can be found in the supplement). We find that many people use VLMs to solve math and code problems, identify paintings and geographical locations, perform data analysis on tables and diagrams, explain humorous images, and create stories based on images. Notably, VisionArena-Chat contains important use cases not seen in WildVision-Chat including coding and web UI design problems, handwritten text extraction, and diagram analysis. Manually inspecting the clusters, we also see that WildVision-Chats clusters are often very specific to certain task (e.g. Detailed Cartoon Dog Description, Rice Leaf Disease Identification), while VisionArena-Chats cluster descriptions are broader which indicates the diversity of our data. Surprisingly, the majority of our questions require OCR, and we receive large number of homework problems and diagram understanding questions. In the following section, we construct categories for each of these major use cases. 4.3. Prompt Categories Based on the clustering analysis and manual inspection, we manually define 8 non-disjoint categories that reflect the vast majority of prompts and test different capabilities of the VLM. We use Gemini 1.5 Flash to classify each prompt, using both the image and text, into set of predefined categories listed in Fig. 4. Section 12 contains detailed descriptions of each category, the prompts used to implement the categorization with 1.5 Flash, and the correlation between 1.5 Flash category labels and those of SOTA models. Figure 2. Bootstrap B.T. model scores for VisionArena-Battle. Proprietary models like Gemini 1.5 Pro and GPT-4o are at the top of the leaderboard, with open models like Llava 1.6, MiniCPM, CogVLMv2, and Phi3 obtaining the lowest ratings. InternVL2 is the highest rated open model, although as shown in Section 4.4, this is largely do to response style rather than model capability. Comparison of top 5 topic clusters between Figure 3. WildVision-Chat and VisionArena-Chat. Compared to WildVision, the most popular topics clusters in VisionArena capture more real world tasks, specifically in STEM fields. 4. Data Analysis In the following section, we (1) compare user votes to expert annotators (2), analyze and categorize the distribution of user conversations (3), compute per category leaderboard (4), measure the impact of response style on human preference, and (5) provide examples of difficult questions. 4.1. Comparing with Experts and the VisionArena To inspect the quality of the human preference votes on our platform, we check their alignment with experts opinions. We sample 5 battles in English for each model pair and have 4 experts (PhD Students) label the battles based on their preference, creating an expert-labeled dataset of 516 responses. 4 Category Description Multi-Turn Exclude Ties Exclude Refusal Conversations with multiple turns. Battles which do not end in tie. Neither model refuses to answer. Captioning OCR Coding Only asks for description of the image. Requires reading text within the image. Contains code block in either the user inputs or model outputs. Humor Homework Entity Recognition Asks to identify objects, places, or people in the image. Requires answering problem which likely comes from homework or exam. Asks to explain the humor within the image or ask for humorous composition. Contains images with diagram (e.g., flowchart, circuit, graph). Asks for creative composition such as story or script. Creative Writing Diagram Figure 4. Descriptions of VisionArena categories. Figure 5. Category Distribution. Excluding preset examples. We see that direct chat data contains higher proportion of coding, homework, and diagram questions while battle data contains more captioning, humor, and creative writing questions. Fig. 5 shows the distribution of category counts for both battles and direct chat conversations. We observe that direct chat conversations contain more homework and diagram understanding problems while battles contain more humor, captioning, and creative writing problems. We speculate this is because users in direct chat mode are more interested in using proprietary VLMs at no cost to assist them with their daily tasks. Model rankings and Arena Scores for select categories and languages are shown in Fig. 7 (full table and arena scores can be found in the supplemental). We find several interesting insights such as: Gemini 1.5 Pro Exp [36], InternVL [7, 8] and Reka Flash [41] achieve worse ranking for categories with require OCR like coding, homework, and diagrams. InternVL has large improvement in ranking for the captioning category. In the following section, we show that this is largely due to stylistic choices such as formatting. Claude Opus, Sonnet, and Haiku [3] see an increase in ratings for multi-turn conversations. The Gemini class of models drops in performance on nonenglish conversations. 4.4. Controlling stylistic biases in evaluations The Vision Arena captures signals from users of various backgrounds and preferences to construct its leaderboard. However, recent literature has pointed out potential confounding variables in model evaluation such as the length of the response or stylistic formatting [9, 11]. Others have also mentioned various axes in which annotators may disagree including task underspecification, response style, refusals, and annotation errors [44]. Thus, we explore the effect of these stylistic features on the VisionArena user preference. We follow recent work that extends the BT model to include style features [22]. Given set of style features (e.g. response length, number of markdown headers), we add style vector to the BT model where= Zi RS is vector of style features. the enhanced BT model has the style coefficients γ RS: ˆβ, ˆγ = arg min βRM ,γRS 1 (cid:88) i=1 CE(σ(X β + γ), Yi) Figure 6. Impact of confounding variables on user preferences, measured by ˆγ in the enhanced Bradley-Terry Model. Length is by far the most influential stylistic factor, with higher influence on preference for more open ended questions like humor, creative writing, and captioning. For each style feature Zi, we compute the normalized difference between the feature values of both model responses. The resulting ˆβ represents model strengths adjusted for style effects, while ˆγ quantifies the influence of style on user preferences. To control for these stylistic factors, we modify how VisionArena computes the model scores by accounting for the stylistic differences between two answers (response length, number of markdown headers, etc) as additional features to the existing BT model. 5 Figure 7. Model rankings across question categories and languages. Cells with * have fewer than 100 votes. Certain models achieve much higher ranking for particular category, such as InternVL2 on captioning and Chinese, Reka Core on entity recognition, and Llava 1.6 on Humor. Conversely, we see certain model rankings drop, such as Reka Flash on multi-turn and diagrams, and Gemini on Chinese. Figure 8. Change in model rankings when style control is applied. Cells with * contain battles with fewer than 100 votes. InternVL, Reka Flash, and Llava 1.6 see ranking drop across the majority of categories, while Claude 3 Haiku and MiniCPM see an increase across most categories. The model lineup changes the most for the captioning, humor, and Vietnamese categories. Controlling for length and markdown. Applying style control, we see the captioning category is heavily affected by style, with Fig. 8 showing large difference in model rankings. We suspect this is because VLMs are heavily optimized for captioning and can usually correctly identify the main subjects and context of the image. This is supported in Table 2, which shows captioning questions have the smallest proportion of both bad votes. In cases where both models provide reasonable description of the image, the user may rely on stylistic features to determine preference. Furthermore, models like InternVL and Reka Flash Preview see large decrease in rankings when style control is applied. Fig. 13 shows these models have an unusually high output token count compared to models of similar class, indicating that some models may be preference hacking by training their models to produce long or nicely-formatted outputs. While this is not necessarily bad, it is important to consider when decoupling preference from capability. Caption Homework OCR Coding Humor Entity Recog. Creative Writing Diagram 11.33 31.12 20.62 29.4 18. 22.25 14.21 26.06 Table 2. Percentage of tie (bothbad) per category. Captioning, creative writing, and humor categories have low percentages of bothbad responses compared to coding and homework problems. Controlling for specificity. We further extend the BradleyTerry model to include the effect of response specificity. We define the complexity of response as the number of named entities in the response. We use NER model [31] to tag each response and use the number of named entities as our specificity score. In Fig. 6, we see that users prefer high specificity for tasks like entity recognition and diagram understanding while placing less emphasis on specificity for tasks like captioning and homework. 4.5. Failure Cases We use VisionArena to analyze examples which are particularly challenging for current VLMs. We first filter VisionArena-Chat for prompts where the user voted that both models are bad, and collect pool of 10 images which most or all of the current VLMs fail. The full set can be found in the supplemental section. Fig. 9a shows two examples of user questions that require advanced visual reasoning. This example requires the model to (1) understand that the two cats in the image are the same (2) the pattern on the cats back gives the illusion of another cat (3) this illusion of smaller cat in cat can be related to the fact that the square root of number is smaller demonimation of that number. Fig. 9b is an example of fine-grained spatial understanding, as the model must locate both locations and reason over the many intersecting lines in the image. Analysis of other failure cases in the supplemental section indicates that current VLMs still struggle on visual grounding tasks like reading distorted images, spatial understand and counting, as well as more complex reasoning tasks. 5. Instruction tuning vision-language models Effective instruction finetuning for vision-language models depends on the diversity of instructions, the difficulty of prompts, and the quality of responses. This section demonstrates the potential of VisionArena for training highperformance instruction-following models. We curate high quality instruction-tuning dataset by sampling from the conversations with the highestperforming VLMs. We choose 100,000 conversations from VisionArena-Chat from the top models. This led to dataset with conversations from 16 different models1 including proprietary models such as GPT-4o [32], Gemini-1.5-Pro [36], and Claude-3.5-Sonnet [3] as well as open-source models such as Qwen2-VL-72B [5, 42] and Llama-3.2-90B-VisionInstruct [2]. We compare the effectiveness of this dataset for finetuning to 100K subset of the Llava-Instruct-158K [23]. We use Llama-3.2-11B-Vision and freeze the vision encoder while finetuning the multimodal projector and language model. We finetune for 3 epochs on the data for both our 100k dataset and the 100k Llava-Instruct dataset. In Table 3, we label the model finetuned on our VisionArena data Llama-3.2-VisionArena and the model trained using LlavaInstruct-158K Llama-3.2-Llava-Instruct. The evaluation results are shown in Table 3. Llama-3.2VisionArena achieves the highest WV-Bench score at 56.9 and increases the MMMU baseline by 1.3 points compared 1gpt-4o-mini-2024-07-18, gpt-4-turbo-2024-04-09, gemini-1.5-proapi-0514, claude-3-5-sonnet-20240620, gemini-1.5-pro-exp-0827, gpt-4o2024-05-13, gemini-1.5-pro-exp-0801, gemini-1.5-flash-api-0514, claude3-opus-20240229, gemini-1.5-flash-exp-0827, gemini-1.5-flash-8b-exp0827, llama-3.2-vision-90b-instruct, qwen2-vl-72b [42], gpt-4o-2024-0806, chatgpt-4o-latest-20240903, chatgpt-4o-latest-20240808 7 to the base model. This demonstrates the effectiveness of VisionArena to improve the capabilities of VLMs. Model Samples MMMU 5 WV-Bench Llama3.2-11B-V Llama3.2-11B-V-Instruct Llama-3.2-Llava-Instruct Llama-3.2-VisionArena - 3M+ 100K 100K 41.7 50.7 27.9 43.0 - 47.2 10.4 56.9 Table 3. Performance across models trained with different instruction tuning datasets. Scores for Llama-3.2-11B-Vision and Llama-3.2-11B-Vision-Instruct are author-reported. Finetuning with 100k samples from VisionArena achieves an MMMU score 15 points higher than finetuning on Llava-Instruct data and WVBench score 9 points higher than Llama-3.2-11B-Vision-Instruct. Despite being fine-tuned on 30x less data, Arena-11B performs comparably to Llama-3.2-11B-Vision-Instruct in chat tasks. 6. VisionArena-Bench: An Automatic Offline Human-Preference Benchmark for VLMs Lastly, we demonstrate VisionArenas ability to cheaply approximate model preference rankings with VisionArenaBench. Currently, online preference benchmarks like Chatbot Arena obtain ranking for new model by adding it to their platform and waiting days or weeks to collect enough votes for stable ranking. For single developer hoping to test particular version of their model, obtaining these online model rankings is infeasible. We develop solution for those who need quick and cheap evaluation of their models: VisionArena-Bench. VisionArena-Bench is set of 500 diverse image and text prompts that accurately approximates the model ranking from the Chatbot Arena VLM Leaderboard. Offline benchmark curation. To gather questions for this offline benchmark, we build on recent work in building benchmarks from crowd-sourced evaluations in LLM and adapt them to the context of VLM [22]. We first filter the data for single turn to prevent the user from correcting the model in its response on its second turn. We then filter out nonenglish conversations as we are personally unable to verify the quality of non-english prompts. To sample diverse questions, we perform topic modeling using the library BERTopic with multimodal embeddings [16]. We extract image embeddings and text embeddings using CLIP model (e.g. CLIP-ViT-B-32) [35]. We then average the image and text embeddings so that each document corresponds to single embedding. Then, we use UMAP to reduce the dimensions of the embedding and 2gemini-1.5-pro-exp-0827, gemini-1.5-flash-8b-exp-0827, gemini-1.5-flash-exp-0827[36], gpt4o-2024-05-13, [8], claude-3.5-sonnet-20240620 [3], gpt-4-turbo-2024-04-09 [32], claude-3sonnet-20240229, llama-3.2-11b-vision-instruct [2], gemini-1.5-pro-001, internvl2-4b, gpt-4o-mini-2024-07-18, claude-3-opus-20240229, gemini1.5-flash-001, reka-core-20240501 [41], claude-3-haikuinternvl2-26b (a) Failure case 1. (b) Failure case 2. Figure 9. VLM failure modes. The top proprietary models fail on questions which require advanced visual reasoning. For example, failure case 1 requires the visual understanding that the cats fur patterns looks like another smaller black cat, and the linguistic connection between this and square root. Non-truncated outputs in Section 13. use hierarchical-based clustering (HDBSCAN) to generate topic clusters [26, 29]. We then uniformly sample from each topic cluster to generate the 500 prompts. Automatic evaluation with VLM-as-a-judge. To evaluate model, we use the LLM-as-a-judge framework mentioned in [22] applied to VLMs. We first select fixed anchor model (GPT-4-Turbo[32]) that will be used in the pairwise comparisons. To evaluate given model on user prompt p, we generate responses for both and the anchor model on and then utilize GPT-4o as judge to provide preference score between the (M , anchor) pair on 5-point Likert scale. 1 indicates strong preference for model and 5 indicates strong preference for model B. We then obtain this score for all models across all prompts in VisionArena-Bench to obtain VLM-generated pairwise preference votes and use the same procedure as described in Sec. 3 to produce final model scores. In Section!14, we provide the detailed judge prompt template. To avoid potential bias, we prompt the judge model to judge twice, swapping the response position between the two rounds. To evaluate the effectiveness of our benchmark to existing work [25], we leverage standard metrics such as Spearman correlation and Kendall Tau correlation which measure the agreement between two benchmarks model rankings. We choose shared set of 16 models and compare the offline benchmark rankings to the online Chatbot Arena. Table 4 shows VisionArena-Bench achieves higher Spearman and Kendall Tau correlation than WildVisionBench, with 17.1% and 20.5% gain respectively. In Section 14, we also compare the results using the same baseline model as WildVision (e.g. Claude-3-Sonnet-20240229), showing that the spearman correlation to Chatbot Arenas VLM leaderboard (10/23/2024) remains the same. This demonstrates the potential of VisionArena-Bench as costeffective and scalable offline benchmark that closely mirrors human preferences captured in online evaluations, enabling researchers to efficiently assess and compare VLMs without the need for extensive user studies. VisionArena-Bench WV-Bench Confidence Agreement Spearman Correlation Kendall Tau Correlation 98.6% 97.3% 89.7% 87.6% 80.2% 69.2% Table 4. Correlation of rankings with ChatbotArenas VLM leaderboard. Performance comparison on 16 models2 between VisionArena-Bench to Chatbot Arenas VLM leaderboard based on confidence agreement, spearman correlation, and Kendall tau correlation. Result as of leaderboard on October 23, 2024. 7. Discussion, Limitations, and Future Work Human preference benchmarks provide critical lens for assessing performance on open-ended tasks where an explicit notion of correctness is either unavailable or subjective. Users may implicitly consider factual accuracy when making their preferences, but we would like to emphasize that this benchmark is designed to measure human preferences rather than explicitly evaluate factual accuracy. We see VisionArena as complementary to existing datasets and benchmarks that measure objective correctness. Despite the breadth of coverage offered by VisionArena, significant gaps remain in representing the full distribution of real-world use cases for vision-language models. As highlighted in Sec. 4, our dataset contains many sample from domains such as STEM problems, OCR tasks, and toy problems (e.g., humor and riddles). These areas, while valuable, leave critical application domains underrepresented, including geospatial applications, medical domains, and visual assistance. Furthermore, while VisionArena contains over 100 languages, many of these languages do not contain enough examples to produce stable leaderboard. Looking forward, we hope to enourage more diverse user base by changing our UI to be multi-lingual and improving general user experience. Lastly, we have made it easy for the community to contribute new question categories and models at https://github.com/lm-sys/FastChat. 8 8. Acknowledgments We extend our gratitude to Lianmin Zheng and Ying Sheng for their early discussions and implementation of the vision arena in the LMArena platform. We also thank Tianle Li and Anastasios Angelopoulos for their valuable insights and feedback on both the platform and the manuscript. Most of all, we appreciate the community for their contributions through questions and votes, as this platform and dataset would not be possible without their participation."
        },
        {
            "title": "References",
            "content": "[1] Abubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan, Abdulrahman Alfozan, and James Zou. Gradio: Hassle-free sharing and testing of ml models in the wild. arXiv preprint arXiv:1906.02569, 2019. 1 [2] AI@Meta. Llama 3 model card. 2024. URL https:// github.com/metallama/llama3/blob/main/ MODEL_CARD.md. 1, 7 [3] Anthropic. The claude 3 model family: Opus, sonnet, haiku. https://wwwcdn.anthropic.com/ de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/ Model_Card_Claude_3.pdf, 2024. (Accessed on 06/05/2024). 1, 5, 7 [4] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. VQA: Visual Question Answering. In International Conference on Computer Vision (ICCV), 2015. 1 [5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. [6] Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. doi: 10.2307/2334029. URL https://doi.org/10.2307/ 2334029. 3 [7] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. 5 [8] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 5, 7 [9] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena: An open platform for evaluating llms by human preference, 2024. 1, 2, 3, 5 [10] Yuntian Deng, Wenting Zhao, Jack Hessel, Xiang Ren, Claire Cardie, and Yejin Choi. Wildvis: Open source visualizer for million-scale chat logs in the wild, 2024. URL https://arxiv.org/abs/2409.03753. [11] Lisa Dunlap, Krishna Mandal, Trevor Darrell, Jacob Steinhardt, and Joseph Gonzalez. Vibecheck: Discover and quantify qualitative differences in large language models. arXiv preprint arXiv:2312.02974, 2024. URL https: //arxiv.org/abs/2410.12851. 5 [12] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models, 2024. URL https://arxiv.org/abs/ 2306.13394. 2 [13] Google. Vision ai cloud vision api. https://cloud. google.com/vision. Accessed: [date]. 3 [14] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in VQA matter: Elevating the role of image understanding in Visual Question Answering. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 1 [15] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering, 2017. URL https://arxiv.org/abs/1612. 00837. [16] Maarten Grootendorst. Bertopic: Neural ing with class-based tf-idf procedure. arXiv:2203.05794, 2022. 4, 7 topic modelarXiv preprint [17] Danna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P. Bigham. Vizwiz grand challenge: Answering visual questions from In Proceedings of the IEEE Conference on blind people. Computer Vision and Pattern Recognition (CVPR), 2018. 1 [18] Danna Gurari, Qing Li, Chi Lin, Yinan Zhao, Anhong Guo, Abigale J. Stangl, and Jeffrey P. Bigham. Vizwiz-priv: dataset for recognizing the presence and purpose of private visual information in images taken by blind people. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 1 [19] Lalit Jain, Kevin Jamieson, Robert Mankoff, Robert Nowak, and Scott Sievert. The New Yorker cartoon caption contest dataset, 2020. URL https://nextml.github.io/ caption-contest-data/. [20] Andreas Kopf, Yannic Kilcher, Dimitri von Rutte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richard Nagyfi, Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander Mattick. Openassistant conversations democratizing large language model alignment. arXiv preprint arXiv:2304.07327, 2023. URL https://doi. org/10.48550/arXiv.2304.07327. Published in NeurIPS 2023 Datasets and Benchmarks. 2 [21] Seongyun Lee, Seungone Kim, Sue Hyun Park, Geewook Kim, and Minjoon Seo. Prometheus-vision: Vision-language model as judge for fine-grained evaluation, 2024. URL https://arxiv.org/abs/2401.06591. 3 9 [22] Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline, 2024. URL https: //arxiv.org/abs/2406.11939. 5, 7, 8 [23] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. URL https://arxiv. org/abs/2304.08485. 2, 7 [24] Yujie* Lu, Dongfu* Jiang, Hui* Chen, Xingyu Fu, Yingzi Ma, Jing Gu, Michael Saxon, Chaowei Xiao, Wenhu Chen, Yejin Choi, Bill Yuchen Lin, Miguel Eckstein, and William Wang. Wildvision data and model, 2024. URL https: //huggingface.co/WildVision. [25] Yujie Lu, Dongfu Jiang, Wenhu Chen, William Yang Wang, Yejin Choi, and Bill Yuchen Lin. Wildvision: Evaluating vision-language models in the wild with human preferences. 2024. 1, 2, 3, 8 [26] Claudia Malzer and Marcus Baum. hybrid approach to In 2020 IEEE hierarchical density-based cluster selection. International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI), page 223228. IEEE, September 2020. doi: 10.1109/mfi49285.2020.9235263. URL http://dx.doi.org/10.1109/MFI49285. 2020.9235263. 8 [27] Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pages 22632279, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/ 2022.findings-acl.177. URL https://aclanthology. org/2022.findings-acl.177. 3 [28] Minesh Mathew, Dimosthenis Karatzas, Manmatha, and CV Jawahar. Docvqa: dataset for vqa on document images. corr abs/2007.00398 (2020). arXiv preprint arXiv:2007.00398, 2020. 2, 3 [29] Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for dimension reduction, 2020. URL https://arxiv.org/abs/ 1802.03426. 8 [30] Microsoft. Photodna service. Online, 2024. URL https: //www.microsoft.com/enus/photodna. Accessed: 2024-11-18. [31] Joel Nothman, Nicky Ringland, Will Radford, Tara Murphy, and James R. Curran. Learning multilingual named entity recognition from wikipedia. Artificial Intelligence, 194:151175, 2013. ISSN 0004-3702. doi: https://doi. org/10.1016/j.artint.2012.03.006. URL https://www. sciencedirect . com / science / article / pii / S0004370212000276. Artificial Intelligence, Wikipedia and Semi-Structured Resources. 6 [32] OpenAI. Gpt-4 technical report, 2023. 1, 7, 8 [33] OpenAI. Openai text moderation tool. Online, 2024. URL https://platform.openai.com/docs/guides/ moderation. Accessed: 2024-11-18. 3 [34] Karl Pearson. Note on regression and inheritance in the case of two parents. Proceedings of the Royal Society of London, 58:240242, 1895. ISSN 03701662. URL http://www. jstor.org/stable/115794. 4 [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. URL https://arxiv.org/abs/2103.00020. 7 [36] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 1, 5, [37] Nimrod Shabtay, Felipe Maia Polo, Sivan Doveh, Wei Lin, M. Jehanzeb Mirza, Leshem Chosen, Mikhail Yurochkin, Yuekai Sun, Assaf Arbelle, Leonid Karlinsky, and Raja Giryes. Livexiv multi-modal live benchmark based on arxiv papers content, 2024. URL https://arxiv.org/ abs/2410.10783. 3 [38] Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 83178326, 2019. 3 [39] Charles Spearman. The proof and measurement of association between two things. The American Journal of Psychology, 15(1):72101, January 1904. doi: 10.2307/1412159. URL https://doi.org/10.2307/1412159. 4 [40] Wei Ren Tan, Chee Seng Chan, Hernan Aguirre, and Kiyoshi Tanaka. Improved artgan for conditional synthesis of natuIEEE Transactions on Image Proral image and artwork. cessing, 28(1):394409, 2019. doi: 10.1109/TIP.2018. 2866698. URL https://doi.org/10.1109/TIP. 2018.2866698. 3 [41] Reka Team, Aitor Ormazabal, Che Zheng, Cyprien de Masson dAutume, Dani Yogatama, Deyu Fu, Donovan Ong, Eric Chen, Eugenie Lamprecht, Hai Pham, Isaac Ong, Kaloyan Aleksiev, Lei Li, Matthew Henderson, Max Bain, Mikel Artetxe, Nishant Relan, Piotr Padlewski, Qi Liu, Ren Chen, Samuel Phua, Yazheng Yang, Yi Tay, Yuqi Wang, Zhongkai Zhu, and Zhihui Xie. Reka core, flash, and edge: series of powerful multimodal language models, 2024. URL https://arxiv.org/abs/2404.12387. 5, 7 [42] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024. URL https://arxiv.org/abs/2409.12191. 7 [43] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi, 2024. URL https://arxiv.org/abs/2311.16502. 1, 2 [44] Michael JQ Zhang, Zhilin Wang, Jena D. Hwang, Yi Dong, Olivier Delalleau, Yejin Choi, Eunsol Choi, Xiang Ren, and Valentina Pyatkin. Diverging preferences: When do annotators disagree and do models know?, 2024. URL https: //arxiv.org/abs/2410.14632. 5 [45] Peng Zhang, Yash Goyal, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Yin and Yang: Balancing and answering binary visual questions. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 1 [46] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric. Xing, Joseph E. Gonzalez, Ion Stoica, and Hao Zhang. Lmsys-chat-1m: large-scale real-world llm conversation dataset, 2023. 2 11 VisionArena: 230K Real World User-VLM Conversations with Preference Labels"
        },
        {
            "title": "Supplementary Material",
            "content": "9. Interface Details We implement our interface in Gradio [1]. If user uploads an image in the first turn, two random VLMs are selected to answer the query. user can only chat with one image per conversation. As shown in Fig. 10, user can also select random image which will select from our preset examples. Note that the user still needs to come up with query, even for these preset images. 10. Topic Distribution In Fig. 11 we show the top 20 topic clusters from VisionArena-Chat and WildVision-Chat. The topic clusters are extracted from 50K sampled English conversations from VisionArena-Chat and 37K English conversations (all) from WildVision-Chat. We see that VisionArena-Chat includes more diverse and broad topics especially in the STEM field. Furthermore, WildVision has very specific clusters like detailed cartoon dog description, Shock absorber assembly analysis, rice lead disease identification. Looking at these clusters we see that they contain large number of duplicate prompts. 11. More Data Stats We provide further information on language distribution (Fig. 15, Fig. 16), battle counts (Fig. 12, Fig. 14), token count  (Fig. 13)  , turn distribution (Fig. 18, Fig. 19), proportion of refusals  (Fig. 20)  , battle outcome counts  (Fig. 17)  , win rates  (Fig. 21)  , and category overlap  (Fig. 24)  . Figure 13. Model token count in VisionArena-Battle. Models in bold see large decrease in rank when style control is applied. Figure 12. VisionArena-Battle counts per model. Figure 14. VisionArena-Chat counts per model. 1 Figure 10. Interface for anonymous side-by-side chat. Figure 11. Top 20 topic clusters of VisionArena-Chat compared to WildVision-Chat. VisionArena-Chat includes more diverse and broad topics especially in the STEM field. 2 Figure 18. VisionArena-Battle Conversation Turn Distribution Figure 15. VisionArena-Battle counts for the top 25 languages. Figure 19. VisionArena-Chat Conversation Turn Distribution Figure 20. Proportion of Refusals per model. 12. Category Details Below are the system prompts used to classify user prompts into the categories described in Section 4. We find classifications are more accurate for certain categories when using only the prompt or only the image, as indicated in the prompt titles. We use Gemini 1.5 Flash for classification and show in Table 5 that our classifications achieve high agreement to using SOTA models as category labelers. Figure 16. VisionArena-Chat counts for the top 25 languages. Figure 17. Battle Outcome Counts. 3 (a) Bootstrap ELO Ratings (b) Average Win Rate (c) Battle Counts (d) Win Fractions Figure 21. VisionArena-Battle Model Ranking Results. OCR System Prompt (image + prompt) Captioning System Prompt (prompt only) You are tasked with determining if given VQA question is captioning question. captioning question asks for general, overall description of the entire image. It must be single, open-ended query that does NOT ask about particular objects, people, or parts of the image, nor require interpretation beyond broad description of what is visually present. Examples include What is happening in this image?, Describe this picture., Explain, etc. An example of non-captioning question is Describe what is funny in this picture. because it asks for specific interpretation of the image content. Output your verdict in the following format: <decision>[yes/no]<decision>. Do NOT explain. You are tasked with determining if given VQA question is an optical character recognition (OCR) question. An OCR question requires reading and understanding text in the image to answer. If there is some amount of text in the image and the question requires reading the text in any capacity it should be classified as Optical Character Recognition. Output your verdict in the following format: <decision>[yes/no]<decision>. Do NOT explain. Refusal System Prompt (responses only) You are tasked with determining if any of the given model responses are refusal to answer. refusal occurs when the model explicitly declines to answer or complete task, due to reasons like safety, moderation, or model limitations (e.g. the user asks the model to search the web but it does not have that capability). You will be given the responses of 2 models, and and you are to determine if refuses to answer, refuses, both refuse, or neither refuse. Output your verdict in the following format: <decision>[yes/no]<decision>. Do NOT explain. 4 Figure 22. Random Samples from VisionArena-Battle Figure 23. Random Samples from VisionArena-Chat 5 Diagram System Prompt (image only) You are tasked with determining whether the given image contains chart, diagram, or figure. Carefully examine the user prompt and consider the following aspects: 1. Does the image contain visual elements such as graphs, flowcharts, tables, method figures, chemical structures, or other visual representations of data or concepts? 2. Does the prompt require interpreting or analyzing the flow of information, relationships between elements, or the structure of the visual representation in the image? 3. Does the prompt require spatial reasoning and understanding the layout or structure of the visual elements? 4. Does the image contain only text, tables, handwriting, or photographs without any visual representations of data or concepts? If so, it is NOT considered chart or diagram. Output your verdict in the following format: <decision>[yes/no]<decision>. Do NOT explain. Creative Writing System Prompt (prompt only) You are tasked with determining whether given VQA user prompt is asking for creative writing. Creative writing is defined as any form of writing that goes beyond standard professional, journalistic, academic, or technical literature. It typically involves imagination, originality, and expression of thoughts and emotions. Prompts which only ask to caption the image without any other requests do NOT count as creative writing. Creative writing can include, but is not limited to, the following formats: - Fiction (e.g., short stories, novels), - Poetry (e.g., sonnets, free verse), - Dramatic writing (e.g., screenplays, monologues, scripts), - Personal essays (focusing on subjective experiences or narrative storytelling), - Songs and lyrics Carefully analyze the user prompt and consider whether it primarily requires creative writing. Think about the following aspects: 1. Does the prompt ask for fictional content, speculative scenarios, or the use of imagination to construct narratives? 2. Does it encourage the expression of thoughts, emotions, or personal experiences beyond mere factual reporting or analysis? 3. Is it asking for writing in specific creative format (e.g., story, poem, script, etc)? 4. Is the primary purpose of the prompt to foster creative expression or originality rather than information delivery, technical documentation, or analytical reasoning? 5. Does the prompt request stylistic or rhetorical elements often associated with creative writing, such as metaphor, imagery, dialogue, etc? 6. Does the prompt expect response in natural language (e.g., sentences, paragraphs) rather than visual, mathematical, or non-linguistic output? Output your verdict in the following format: <decision>[yes/no]<decision>. Do NOT explain. 13. Failure Cases Hard OCR  (Fig. 25)  . While VLMs perform well at transcribing easily legible text, they struggle with perturbed text (e.g., rotations, blur). Reading such difficult text is essential for real-world applications. We show two failure cases: one with unclear handwriting and another with rotated text. Counting  (Fig. 26)  . Counting is critical skill for decision6 Figure 24. VisionArena-Battle category overlap. Homework System Prompt (image only) You are tasked with determining if the given image contains homework or exam question. homework or exam question typically contains text with well-defined question or task which asks for solution. In addition, many homework and exam questions contain multiple choice, equations, and question numbers. You may also see text referring to showing your work or providing justification. Note that documents such as resumes, business cards, records, or personal notes are NOT considered homework or exam questions; homework and exam questions explicitly ask for solution or explanation. Output your verdict in the following format: <decision>[yes/no]<decision>. Do NOT explain. Humor Systems Prompt (image + prompt) You are tasked with determining if given VQA question is humor question. humor question asks for humorous or funny response based on the image or asks to understand what is funny about an image. This includes questions that ask to explain an image which is humorous, such as memes. Output your verdict in the following format: <decision>[yes/no]<decision>. Do NOT explain. Entity Recognition System Prompt (prompt only) You are tasked with determining if given VQA question is an entity recognition question. An entity recognition question asks for the identification of specific objects or people in the image. This does NOT include questions that ask for general description of the image, questions that only ask for object counts, or questions that only require reading text in the image. Output your verdict in the following format: <decision>[yes/no]<decision>. Do NOT explain."
        },
        {
            "title": "Yes\nNo\nYes\nYes\nNo\nNo\nYes",
            "content": "gemini-1.5-pro-exp-0827 claude-3-5-sonnet-20240620 gemini-1.5-pro-exp-0827 gemini-1.5-pro-exp-0827 claude-3-5-sonnet-20240620 claude-3-5-sonnet-20240620 gemini-1.5-pro-exp-0827 0.987 0.967 0.925 0.818 0.952 0.964 0.961 0.929 0.938 0.788 0.954 0.728 0.680 0.858 0.967 0.934 0.636 0.769 0.830 0.810 0.953 Table 5. Comparing Gemini-1.5-Flash question categorization to larger models. Gemini-1.5-Flash is evaluated against SOTA models on 1000 samples from VisionArena-Chat, using Gemini-1.5-Pro for image-based prompts and Claude-3.5-Sonnet for text-based prompts. Gemini-1.5-Flash achieves high agreement with SOTA models for category classification. making across education, organization, and daily life. While humans count effortlessly, VLMs still struggle. We provide two examples where the top three models fail: one requiring counting based on shape and color, and another involving intersecting triangles. Reasoning (Fig. 27, Fig. 28). Reasoning is essential for helping users tackle complex problems. While reasoning remains challenge for both VLMs and LLMs, we present five unique VLM failure cases. 14. VisionArena-Bench VLM-as-a-Judge System Prompt Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user prompt displayed below. You will be given assistant As answer and assistant Bs answer. Your job is to evaluate which assistants answer is better. Begin your evaluation by generating your own answer to the prompt. You must provide your answers before judging any answers. When evaluating the assistants answers, compare both assistants answers with your answer. You must identify and correct any mistakes or inaccurate information. Then consider if the assistants answers are helpful, relevant, and concise. Helpful means the answer correctly responds to the prompt or follows the instructions. Note when user prompt has any ambiguity or more than one interpretation, it is more helpful and appropriate to ask for clarifications or more information from the user than providing an answer based on assumptions. Relevant means all parts of the response closely connect or are appropriate to what is being asked. Concise means the response is clear and not verbose or excessive. Then consider the creativity and novelty of the assistants answers when needed. Finally, identify any missing important information in the assistants answers that would be beneficial to include when responding to the user prompt. After providing your explanation, you must output only one of the following choices as your final verdict with label: 1. Assistant is significantly better: [[A >> B]] 2. Assistant is slightly better: [[A > B]] 3. Tie, relatively the same: [[A = B]] 4. Assistant is slightly better: [[B > A]] 5. Assistant is significantly better: [[B >> A]] Example output: My final verdict is tie: [[A = B]]. Model Score 95% CI Token # gpt-4o-2024-05-13 gemini-1.5-pro-exp-0827 gemini-1.5-flash-exp-0827 claude-3.5-sonnet-20240620 gpt-4-turbo-2024-04-09 gemini-1.5-pro-001 gpt-4o-mini-2024-07-18 gemini-1.5-flash-8b-exp-0827 internvl2-26b gemini-1.5-flash-001 claude-3-opus-20240229 claude-3-sonnet-20240229 reka-core-20240501 llama-3.2-11b-vision-instruct claude-3-haiku-20240307 internvl2-4b 67.7 66.2 60.3 54.5 50.0 45.5 40.0 30.6 23.3 23.0 18.9 18.4 15.6 11.2 9.6 6.8 (-1.7, 1.8) (-1.8, 1.5) (-1.9, 1.9) (-2.1, 1.9) (0.0, 0.0) (-1.8, 2.0) (-2.3, 1.9) (-2.3, 1.8) (-2.1, 1.1) (-1.1, 1.6) (-1.9, 1.7) (-1.4, 1.3) (-1.3, 1.4) (-1.3, 1.1) (-1.1, 1.0) (-0.9, 0.8) 316 329 367 262 258 261 224 354 515 271 201 205 252 457 155 421 Table 7. VisionArena-Bench leaderboard (baseline: GPT-4-Turbo) Model Score 95% CI Token # gemini-1.5-pro-exp-0827 gpt-4o-2024-05-13 claude-3-5-sonnet-20240620 gemini-1.5-flash-exp-0827 gpt-4-turbo-2024-04-09 gemini-1.5-pro-001 gpt-4o-mini-2024-07-18 gemini-1.5-flash-8b-exp-0827 gemini-1.5-flash-001 internvl2-26b claude-3-opus-20240229 claude-3-sonnet-20240229 reka-core-20240501 llama-3.2-11b-vision-instruct claude-3-haiku-20240307 internvl2-4b 87.6 86.8 86.3 83.5 80.7 75.3 73.0 64.7 58.4 54.2 52.0 50.0 37.9 32.6 30.7 19. (-1.1, 1.1) (-1.1, 1.3) (-1.3, 1.2) (-1.7, 1.1) (-1.0, 1.6) (-1.7, 1.4) (-1.3, 1.4) (-1.5, 2.4) (-1.9, 1.5) (-1.7, 1.7) (-2.0, 1.7) (0.0, 0.0) (-1.9, 1.7) (-1.7, 1.8) (-2.3, 1.6) (-1.9, 1.3) 329 316 262 367 258 261 224 354 271 515 201 205 252 457 155 421 Table 8. VisionArena-Bench leaderboard (baseline: claude-3sonnet-20240229)"
        },
        {
            "title": "Model",
            "content": "K-Pop"
        },
        {
            "title": "Shoes Chess ARC",
            "content": "gemini-1.5-pro-exp-0827 gpt-4o-2024-05-13 claude-3-5-sonnet-20240620 claude-3-opus-20240229 gpt-4-turbo-2024-04-09 gpt-4o-mini-2024-07-18 gemini-1.5-pro-001 gemini-1.5-flash-8b-exp-0827 gemini-1.5-flash-exp-0827 internvl2-26b gemini-1.5-flash-001 claude-3-sonnet-20240229 llama-3.2-11b-vision-instruct claude-3-haiku-20240307 internvl2-4b"
        },
        {
            "title": "X\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX",
            "content": "Table 6. Model performance across several hard tasks. indicates that the model solves the problem and indicates that the model fails to solve the problem. 9 out of 16 models fail all questions. (a) K-Pop Failure Case Figure 25. Hard OCR Failure Cases. These failure cases show that models still fail in cases where the text is perturbed such as rotations or messy hand-writing. (b) Sign Failure Case 8 (a) Shapes Failure Case Figure 26. Hard Counting Failure Cases. These cases show that proprietary models still fail at counting tasks involving shapes even when the format is clear. (b) Triangles Failure Case 15. Additional model details Table 9 shows the mapping from the model names used in Section 4 to the exact model versions. Model Version claude-3-5-sonnet-20240620 claude-3-haiku-20240307 claude-3-opus-20240229 claude-3-sonnet-20240229 cogvlm2-llama3-chat-19b gemini-1.5-flash-api-0514 gemini-1.5-pro-api-0514 gemini-1.5-pro-exp-0801 gpt-4-turbo-2024-04-09 gpt-4o-2024-05-13 gpt-4o-mini-2024-07-18 internvl2-26b llava-v1.6-34b minicpm-v-2 6 phi-3-vision-128k-instruct reka-core-20240501 reka-flash-preview-20240611 Reka Flash Preview Model Name Claude 3.5 Sonnet Claude 3 Haiku Claude 3 Opus Claude 3 Sonnet CogVLM2 Llama3 Chat 19b Gemini 1.5 Flash Gemini 1.5 Pro Gemini 1.5 Pro Exp GPT-4 Turbo GPT-4o GPT-4o Mini InternVL2 26b LLAVA 1.6 34b MiniCPM v2.6 Phi 3 Vision 128k Instruct Reka Core Table 9. Model Name to exact model version 9 (a) Map Failure Case (b) Meme Failure Case Figure 27. Hard Reasoning Failure Cases (part 1). The failure cases highlight the models inability to connect the visual reasoning with language reasoning. While the vision model identifies an object, it is not able to identify the relationship between them correctly. (c) Shoes Failure Case 10 (a) Chess Failure Case Figure 28. Hard Reasoning Failure Cases (part 2). These failure cases highlight the inability for the model to be able to correctly map out grid-like structure and the various pieces in it. (b) ARC Failure Case"
        }
    ],
    "affiliations": [
        "Stanford",
        "UC Berkeley"
    ]
}