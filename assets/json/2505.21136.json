{
    "paper_title": "SageAttention2++: A More Efficient Implementation of SageAttention2",
    "authors": [
        "Jintao Zhang",
        "Xiaoming Xu",
        "Jia Wei",
        "Haofeng Huang",
        "Pengle Zhang",
        "Chendong Xiang",
        "Jun Zhu",
        "Jianfei Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The efficiency of attention is critical because its time complexity grows quadratically with sequence length. SageAttention2 addresses this by utilizing quantization to accelerate matrix multiplications (Matmul) in attention. To further accelerate SageAttention2, we propose to utilize the faster instruction of FP8 Matmul accumulated in FP16. The instruction is 2x faster than the FP8 Matmul used in SageAttention2. Our experiments show that SageAttention2++ achieves a 3.9x speedup over FlashAttention while maintaining the same attention accuracy as SageAttention2. This means SageAttention2++ effectively accelerates various models, including those for language, image, and video generation, with negligible end-to-end metrics loss. The code will be available at https://github.com/thu-ml/SageAttention."
        },
        {
            "title": "Start",
            "content": "SageAttention2++: More Efficient Implementation of SageAttention2 Jintao Zhang 1 Xiaoming Xu 1 Jia Wei 1 Haofeng Huang 1 Pengle Zhang 1 Chendong Xiang 1 Jun Zhu 1 Jianfei Chen 1 5 2 0 2 8 2 ] . [ 2 6 3 1 1 2 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The efficiency of attention is critical because its time complexity grows quadratically with sequence length. SageAttention2 addresses this by using quantization to speed up matrix multiplications (Matmul) in attention. To further accelerate SageAttention2, we propose to utilize the faster instruction of FP8 Matmul accumulated in FP16. The instruction is 2 faster than the FP8 Matmul used in SageAttention2. Our experiments show that SageAttention2++ achieves 3.9 speedup over FlashAttention while maintaining the same attention accuracy as SageAttention2. This means SageAttention2++ effectively accelerates various models, including those for language, image, and video generation, with negligible end-to-end metrics loss. The code will be available at https://github.com/ thu-ml/SageAttention. 1. Introduction The quadratic time complexity of attention necessitates efficient implementations for real-world applications with long sequences (Jiang et al., 2024). Current approaches to reduce attentions computational demands fall into three main categories: (1) linear attention methods (Wang et al., 2020; Choromanski et al., 2021; Yu et al., 2022; Katharopoulos et al., 2020; Qin et al., 2024; Yang et al., 2024) that achieve O(N ) complexity, and (2) sparse attention techniques (Liu et al., 2021; Chu et al., 2021; Li et al., 2022; Xiao et al., 2024b;a; Chen et al., 2024; Jiang et al., 2024; Venkataramanan et al., 2024; Gao et al., 2024; Fu et al., 2024; Zhang et al., 2025e; Xi et al., 2025; Zhang et al., 2025f) that process only relevant context portions. While effective, these methods often exhibit limited generality across models and tasks. (3) An alternative direction focuses on hardware-optimized attention implementations that maintain full sequence computation while achieving superior speed and accuracy. No1Department of Computer Science, Tsinghua University. Preprint. table examples include FlashAttention (Dao et al., 2022), FlashAttention2 (Dao, 2024; Shah et al., 2024), xFormers (Lefaudeux et al., 2022), and SageAttentions (Zhang et al., 2025c;c;d;b) which demonstrate strong performance across diverse applications. Motivation, problem, and our approach. For the second matrix multiplication (Matmul) in attention, SageAttention2 accelerates it by quantizing to FP8 and using the mma.f32.f8.f8.f32 instruction. However, mma.f32.f8.f8.f32 employs an FP32 accumulator and is only 2 faster than FP16. We find that the mma.f16.f8.f8.f16 instruction (using FP16 accumulator for FP8 Matmul) achieves 4 speedup over FP16 (NVIDIA, 2022). Therefore, we aim to accelerate SageAttention2 by using the faster instruction. However, directly using the faster instruction will lead to the values of exceeding the representable range of FP16. To address the problem, we propose to narrow the quantization range of and to satisfy the accumulator range in FP16. Performance. For efficiency, SageAttention2++ delivers 3.9 speedup over FlashAttention. In terms of accuracy, SageAttention2++ matches SageAttention2s performance. We conduct comprehensive evaluations on state-of-the-art models for text, image, and video generation. The results demonstrate that SageAttention2++ provides plug-and-play acceleration with negligible end-to-end metrics loss across diverse models. 2. Preliminary Table 1. Speedup compared to matrix multiplication in FP16 with an FP32 accumulator. GPU RTX4090, RTX MM Input MM Accumulator Speedup FP32 FP32 FP16 FP16 FP8 FP8 1x 2x 4x 2.1. SageAttention2 SageAttention2 (Zhang et al., 2025a) is quantization (Zhang et al., 2025g; Hu et al., 2025) method based on FlashAttention (Dao et al., 2022). FlashAttention tiles SageAttention2++: More Efficient Implementation of SageAttention2 Table 2. Average attention accuracy across all attention layers of CogvideoX. Method SageAttn2 SageAttn2++ SageAttn2++ SageAttn2++ Pr 448 448 224 112 Vr 448 2.25 4.5 9 Cossim 99.97% 99.97% 99.97% 99.97% L1 0.01862 0.01863 0.01862 0.01863 Q, K, P, into blocks ({Qi}, {Ki}, { (cid:101)Pi}, {Vi}) and uses online softmax (Milakov & Gimelshein, 2018) to compute attention progressively. For simplicity, we omit the subscripts ({Qi}, {Ki}, { (cid:101)Pi}, {Vi}) in the following content and use Q, K, (cid:101)P , to represent the tiled blocks. SageAttention2 quantizes Q, to INT4/INT8 with per-block granularity, (cid:101)P to FP8 in E4M3 with per-block granularity, and quantizes to FP8 in E4M3 with per-channel granularity. This means each Q, K, (cid:101)P has separate scale factor: δQ = max(Q)/127, δK = max(K)/127, δP = max( (cid:101)P )/448, and each channel of has separate scalar scale: δV = colmax(V )/448. By doing so, SageAttention2 accelerates matrix multiplications in attention through low-bit Tensor Core operations. For example, ˆP = (cid:101)P /δP , ˆV = /δv. Then, = ˆP ˆV δP δV . 2.2. Data Type of Accumulator for Matmul the speed of Matmul instructions deIn some GPUs, pends on the accumulator data type. For instance, mma.f32.f8.f8.f32 uses FP32 accumulator for FP8 Matmul and is only 2 faster than FP16. The instruction using FP16 accumulator for FP8 Matmul (mma.f16.f8.f8.f16) is 4 faster than FP16. Table 1 summarizes the speedup of Matmul instructions with different accumulators. 3. SageAttention2++ In this section, we introduce SageAttention2++. The workflow of SageAttention2++ is based on SageAttention2, also using the smoothing of and K, INT4/INT8 quantization for QK Matmul and FP8 quantization for Matmul. The main difference is that for , SageAttention2++ uses the faster instruction (mma.f16.f8.f8.f16), which employs an FP16 accumulator for the FP8 Matmul. To ensure the results of FP8 Matmul remain within FP16s representable range, we adjust the scale factor of the FP8 quantization. 3.1. Narrowing the FP8 Quantization Range The specific MMA (NVIDIA, 2025) instruction used for the MatMul between and is mma.m16n8k32. If we quantize and to FP8 in E4M3 (-448448) as in SageAt2 tention2, the results may exceed FP16s representable range (-6550465504). This occurs because 32 product values pv are accumulated in FP16, where and come from quantized ˆP and ˆV (derived from (cid:101)P and ). To ensure the accumulated results stay within FP16s range: 32 pv 65504 (1) For instance, choosing 224 and 9 satisfies this condition. We therefore narrow the quantization ranges of and by adjusting their scale factors: δP = max( (cid:101)P )/Pr, δV = max(V )/Vr (2) where we constrain Pr Vr 2047 (since 65504/32 = 2047). 3.2. Delayed FP32 Buffering of values accumulated transformation from The mma.m16n8k32 (in FP16) to FP32 incurs overhead because it needs additional data type conversion PTX instructions (NVIDIA, 2025) to execute. To reduce this overhead, we accumulate two consecutive mma.m16n8k32 results in FP16 before performing FP32 conversion, effectively halving the transformation overhead. Maintaining the FP16 representable range requires: Pr Vr 2047/ (3) Choice of Pr and Vr. Table 2 shows attention accuracy for feasible (Pr, Vr) pairs. The results demonstrate that narrowing quantization ranges introduces negligible error. We select Pr = 224 and Vr = 4.5 for optimal performance. 4. Experiment Main result. SageAttention2++ achieves up to 3.9 speedup over FlashAttention2 while consistently outperforming both SageAttention and SageAttention2 in computational efficiency. Importantly, these performance gains are achieved with negligible impact on end-to-end metrics across diverse model architectures. 4.1. Setup We and attentions. Models evaluate SageAttention2++ across diverse representative models spanning language, image, and video generation: Llama3.1 (8B) (Dubey et al., 2024) for text2text, CogvideoX (2B), HunyuanVideo (Kong et al., 2024), and Wan (Wan et al., 2025) for text2video, and Flux (schnell) (Black Forest Labs, 2023) and Stable-Diffusion3.5 (turbo) (Stability AI, 2023) for text2image. We compare our method with FlashAttention2 (Dao, 2024), SageAttention (Zhang et al., 2025c), SageAttention2++: More Efficient Implementation of SageAttention2 Figure 1. Speed comparison between SageAttention2++ and baselines (RTX4090, headdim=128). Figure 2. Speed comparison between SageAttention2++ and baselines (RTX4090, headdim=64). Figure 3. Speed comparison between SageAttention2++ and baselines (RTX5090, headdim=128). Figure 4. Speed comparison between SageAttention2++ and baselines (RTX5090, headdim=64). and SageAttention2 (Zhang et al., 2025a). Please note that FlashAttention3 can only run on Hopper GPUs, so FlashAttention2 is already the fastest version for RTX5090 and RTX4090. Following SageAttention2s two SageAttention2++ approach, we implement variants: SageAttn2++(8+8) (INT8 for Q, K) and SageAttn2++(4+8) (INT4 for Q, K), both using FP8 in E4M3 for (cid:101)P , . Datasets and metrics. Detailed dataset and metric information appears in Appendix A.2. Implementation. We implement SageAttention2++ using CUDA. 4.2. Speed of Kernels Kernel Speed. speed of SageAttention2++ against baselines using configurations with headdim=64 and headdim=128, both with We benchmark the 3 SageAttention2++: More Efficient Implementation of SageAttention2 Table 3. End-to-end metrics across text, image, and video generation models. Model Llama3.1 Attention Full-Precision SageAttn2(8+8) SageAttn2++(8+8) WikiText (Ppl.) 6.013 6.019 6.020 Lambda (Acc.) 0.815 0.811 0.813 NIAH (Acc.) 0.906 0.903 0. Model CogvideoX (2B) Hunyuan Video Wan Model Flux Stable-Dif fusion3.5 Attention Full-Precision SageAttn2(4+8) SageAttn2(8+8) SageAttn2++(4+8) SageAttn2++(8+8) Full-Precision SageAttn2(4+8) SageAttn2(8+8) SageAttn2++(4+8) SageAttn2++(8+8) Full-Precision SageAttn2(4+8) SageAttn2(8+8) SageAttn2++(4+8) SageAttn2++(8+8) Attention Full-Precision SageAttn2(4+8) SageAttn2(8+8) SageAttn2++(4+8) SageAttn2++(8+8) Full-Precision SageAttn2(4+8) SageAttn2(8+8) SageAttn2++(4+8) SageAttn2++(8+8) CLIPSIM 0.179 0.179 0.178 0.179 0.179 0.175 0.176 0.175 0.176 0.175 0.172 0.176 0.172 0.176 0.172 FID 165.117 164.170 163.185 164.170 163.555 166.369 164.610 164.971 164.610 165.842 CLIP-T 0.997 0.997 0.997 0.997 0.997 0.999 0.999 0.999 0.999 0.999 0.999 0.998 0.999 0.998 0. VQA-a 74.499 76.309 74.322 74.387 76.309 77.437 73.282 78.145 73.282 78.569 53.255 29.728 49.794 29.728 50.876 VQA-t 74.642 66.396 74.447 66.568 73.165 52.731 55.141 54.878 52.258 51.080 59.989 38.533 55.712 38.023 57.140 sFID 147.831 147.185 146.101 147.185 146.036 146.514 147.350 148.498 147.350 146.465 CLIP 31.401 31.358 31.453 31.358 31.445 31.876 31.912 31.964 31.912 31.968 FScore 4.974 4.386 4.899 4.333 4.386 1.169 0.968 1.176 0.968 1.192 1.843 0.994 1.870 0.994 1.902 IR 0.912 0.910 0.905 0.910 0.902 0.929 0.914 0.931 0.914 0. Figure 5. visible example of using SageAttention2++. and without Causal Mask (Vaswani, 2017). Specifically, Fig. 1 shows the speed across varying sequence lengths on RTX4090, indicating that SageAttn2++(4+8) and SageAttn2++(8+8) are approximately 3.9x and 3.0x faster than FlashAttention2, respectively. Fig. 2, 3 and 4 show more kernel speed comparison on RTX4090 and RTX5090 GPUs. 4 SageAttention2++: More Efficient Implementation of SageAttention2 Figure 6. Visible examples of using SageAttention2++ on video generation. 4.3. End-to-end Performance 5. Conclusion Metrics loss. We evaluate end-to-end model performance using SageAttention2++ against baseline methods. Detailed evaluation results are presented in Table 3. The results indicate that SageAttn2++(8+8) and SageAttn2++(4+8) match the end-to-end metrics of SageAttention2. Specifically, SageAttn2++(8+8) incurs almost no metrics loss across various models and SageAttn2++(4+8) brings little metrics loss. Visible image and video examples. Fig.5, 7, and 6 show some visible comparison examples. We introduce SageAttention2++ to further accelerate SageAttention2. We propose to utilize the faster instruction of FP8 Matmul accumulated in FP16 for the matrix multiplication of . Experiments show that SageAttention2++ achieves 3.9 speedup (SageAttention2 has 3 speedup) over FlashAttention, while maintaining the same attention accuracy as SageAttention2. This means SageAttention2++ can accelerate various models, including those for language, image, and video generation, with negligible end-to-end metrics loss. SageAttention2++: More Efficient Implementation of SageAttention"
        },
        {
            "title": "References",
            "content": "Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2023. Hu, Y., Huang, W., Liang, Z., Chen, C., Zhang, J., Zhu, J., and Chen, J. Identifying sensitive weights via postquantization integral. 2025. Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., and Jia, J. Longlora: Efficient fine-tuning of long-context large language models. In The International Conference on Learning Representations, 2024. Jelinek, F., Mercer, R. L., Bahl, L. R., and Baker, J. K. Perplexitya measure of the difficulty of speech recognition tasks. The Journal of the Acoustical Society of America, 62(S1):S63S63, 1977. Choromanski, K. M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J. Q., Mohiuddin, A., Kaiser, L., Belanger, D. B., Colwell, L. J., and Weller, A. Rethinking attention with performers. In International Conference on Learning Representations, 2021. Jiang, H., LI, Y., Zhang, C., Wu, Q., Luo, X., Ahn, S., Han, Z., Abdi, A. H., Li, D., Lin, C.-Y., Yang, Y., and Qiu, L. MInference 1.0: Accelerating pre-filling for long-context LLMs via dynamic sparse attention. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Chu, X., Tian, Z., Wang, Y., Zhang, B., Ren, H., Wei, X., Xia, H., and Shen, C. Twins: Revisiting the design of spatial attention in vision transformers. In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021. Dao, T. Flashattention-2: Faster attention with better parallelism and work partitioning. In The Twelfth International Conference on Learning Representations, 2024. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and Re, C. Flashattention: Fast and memory-efficient exact attention with IO-awareness. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Fu, T., Huang, H., Ning, X., Zhang, G., Chen, B., Wu, T., Wang, H., Huang, Z., Li, S., Yan, S., Dai, G., Yang, H., and Wang, Y. Moa: Mixture of sparse attention for automatic large language model compression. arXiv preprint arXiv:2406.14909, 2024. Gao, Y., Zeng, Z., Du, D., Cao, S., So, H. K.-H., Cao, T., Yang, F., and Yang, M. Seerattention: Learning intrinsic sparse attention in your llms. arXiv preprint arXiv:2410.13276, 2024. Hessel, J., Holtzman, A., Forbes, M., Le Bras, R., and Choi, Y. Clipscore: reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 75147528, 2021. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Kamradt, G. Llmtest needle in haystack-pressure testing llms. https://github.com/gkamradt/ LLMTest_NeedleInAHaystack, 2023. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pp. 51565165. PMLR, 2020. Kong, W., Tian, Q., Zhang, Z., Min, R., Dai, Z., Zhou, J., Xiong, J., Li, X., Wu, B., Zhang, J., Wu, K., Lin, Q., Wang, A., Wang, A., Li, C., Huang, D., Yang, F., Tan, H., Wang, H., Song, J., Bai, J., Wu, J., Xue, J., Wang, J., Yuan, J., Wang, K., Liu, M., Li, P., Li, S., Wang, W., Yu, W., Deng, X., Li, Y., Long, Y., Chen, Y., Cui, Y., Peng, Y., Yu, Z., He, Z., Xu, Z., Zhou, Z., Xu, Z., Tao, Y., Lu, Q., Liu, S., Zhou, D., Wang, H., Yang, Y., Wang, D., Liu, Y., Jiang, J., and Zhong, C. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Lefaudeux, B., Massa, F., Liskovich, D., Xiong, W., Caggiano, V., Naren, S., Xu, M., Hu, J., Tintore, M., Zhang, S., Labatut, P., Haziza, D., Wehrstedt, L., Reizenstein, J., and Sizov, G. xformers: modular and hackable transformer modelling library. https://github. com/facebookresearch/xformers, 2022. Li, K., Wang, Y., Peng, G., Song, G., Liu, Y., Li, H., and Qiao, Y. Uniformer: Unified transformer for efficient spatial-temporal representation learning. In International Conference on Learning Representations, 2022. Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pp. 740 755. Springer, 2014. 6 SageAttention2++: More Efficient Implementation of SageAttention Liu, Y., Cun, X., Liu, X., Wang, X., Zhang, Y., Chen, H., Liu, Y., Zeng, T., Chan, R., and Shan, Y. Evalcrafter: Benchmarking and evaluating large video generation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22139 22149, 2024. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1001210022, 2021. Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. In International Conference on Learning Representations, 2022. Milakov, M. and Gimelshein, N. Online normalizer calculation for softmax. arXiv preprint arXiv:1805.02867, 2018. Nvidia ada gpu architecture, 2022. https://images.nvidia.com/ NVIDIA. URL aem-dam/Solutions/geforce/ada/ nvidia-ada-gpu-architecture.pdf. Technical whitepaper. NVIDIA. 8.7. ptx_isa_8.4.pdf, 2025. Accessed: 2025-05-16. Parallel Thread Execution ISA Version https://docs.nvidia.com/cuda/pdf/ Paperno, D., Kruszewski, G., Lazaridou, A., Pham, N.-Q., Bernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and Fernandez, R. The lambada dataset: Word prediction requiring broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1525 1534, 2016. Qin, Z., Sun, W., Li, D., Shen, X., Sun, W., and Zhong, Y. Lightning attention-2: free lunch for handling unlimited sequence lengths in large language models. arXiv preprint arXiv:2401.04658, 2024. Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Improved techniques for Radford, A., and Chen, X. training gans. Advances in neural information processing systems, 29, 2016. Shah, J., Bikshandi, G., Zhang, Y., Thakkar, V., Ramani, P., and Dao, T. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Stability AI. diffusion https://stability.ai/news/ 3.5. introducing-stable-diffusion-3-5, 2023. Introducing stable Vaswani, A. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Venkataramanan, S., Ghodrati, A., Asano, Y. M., Porikli, F., and Habibian, A. Skip-attention: Improving vision In The Twelfth transformers by paying less attention. International Conference on Learning Representations, 2024. Wan, T., Wang, A., Ai, B., Wen, B., Mao, C., Xie, C.-W., Chen, D., Yu, F., Zhao, H., Yang, J., Zeng, J., Wang, J., Zhang, J., Zhou, J., Wang, J., Chen, J., Zhu, K., Zhao, K., Yan, K., Huang, L., Feng, M., Zhang, N., Li, P., Wu, P., Chu, R., Feng, R., Zhang, S., Sun, S., Fang, T., Wang, T., Gui, T., Weng, T., Shen, T., Lin, W., Wang, W., Wang, W., Zhou, W., Wang, W., Shen, W., Yu, W., Shi, X., Huang, X., Xu, X., Kou, Y., Lv, Y., Li, Y., Liu, Y., Wang, Y., Zhang, Y., Huang, Y., Li, Y., Wu, Y., Liu, Y., Pan, Y., Zheng, Y., Hong, Y., Shi, Y., Feng, Y., Jiang, Z., Han, Z., Wu, Z.-F., and Liu, Z. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. Wu, H., Zhang, E., Liao, L., Chen, C., Hou, J., Wang, A., Sun, W., Yan, Q., and Lin, W. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 20144 20154, 2023. Xi, H., Yang, S., Zhao, Y., Xu, C., Li, M., Li, X., Lin, Y., Cai, H., Zhang, J., Li, D., et al. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity. arXiv preprint arXiv:2502.01776, 2025. Xiao, C., Zhang, P., Han, X., Xiao, G., Lin, Y., Zhang, Z., Liu, Z., and Sun, M. Infllm: Training-free long-context extrapolation for llms with an efficient context memory. In First Workshop on Long-Context Foundation Models@ ICML 2024, 2024a. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2024b. Xu, J., Liu, X., Wu, Y., Tong, Y., Li, Q., Ding, M., Tang, J., and Dong, Y. Imagereward: Learning and evaluating human preferences for text-to-image generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 7 SageAttention2++: More Efficient Implementation of SageAttention Yang, S., Kautz, J., and Hatamizadeh, A. Gated delta networks: Improving mamba2 with delta rule. arXiv preprint arXiv:2412.06464, 2024. Yu, W., Luo, M., Zhou, P., Si, C., Zhou, Y., Wang, X., Feng, J., and Yan, S. Metaformer is actually what you need for vision. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10819 10829, 2022. Zhang, J., Huang, H., Zhang, P., Wei, J., Zhu, J., and Chen, J. Sageattention2: Efficient attention with thorough outlier smoothing and per-thread int4 quantization. In International Conference on Machine Learning (ICML), 2025a. Zhang, J., Huang, H., Zhang, P., Wei, J., Zhu, J., and Chen, J. Sageattention2: Efficient attention with smoothing and per-thread quantization. 2025b. Zhang, J., Wei, J., Zhang, P., Chen, J., and Zhu, J. Sageattention: Accurate 8-bit attention for plug-and-play inference acceleration. In The International Conference on Learning Representations, 2025c. Zhang, J., Wei, J., Zhang, P., Xu, X., Huang, H., Wang, H., Jiang, K., Zhu, J., and Chen, J. Sageattention3: Microscaling fp4 attention for inference and an exploration of 8-bit training. arXiv preprint arXiv:2505.11594, 2025d. Zhang, J., Xiang, C., Huang, H., Wei, J., Xi, H., Zhu, J., and Chen, J. Spargeattn: Accurate sparse attention accelerating any model inference. In International Conference on Machine Learning (ICML), 2025e. Zhang, J., Xiang, C., Huang, H., Wei, J., Xi, H., Zhu, J., and Chen, J. Spargeattn: Training-free sparse attention accelerating any model inference. 2025f. Zhang, P., Wei, J., Zhang, J., Zhu, J., and Chen, J. Accurate int8 training through dynamic block-level fallback. 2025g. Zhao, T., Fang, T., Huang, H., Liu, E., Wan, R., Soedarmadji, W., Li, S., Lin, Z., Dai, G., Yan, S., Yang, H., et al. Vidit-q: Efficient and accurate quantization of diffusion transformers for image and video generation. In International Conference on Learning Representations, 2025. Zheng, Z., Peng, X., Yang, T., Shen, C., Li, S., Liu, H., Zhou, Y., Li, T., and You, Y. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. SageAttention2++: More Efficient Implementation of SageAttention2 A. Appendix A.1. Visible Comparison Examples Figure 7. Visible examples of using SageAttention2++ on image generation. A.2. Datasets and Metrics in Experiments Datasets. Text-to-text models are evaluated on: WikiText (Merity et al., 2022) to assess the models prediction confidence, LAMBADA (Paperno et al., 2016) for contextual understanding, and Needle-in-A-Haystack (NIAH) task (Kamradt, 2023). Text-to-video models are evaluated using the open-sora (Zheng et al., 2024) prompt sets. Text-to-image models are assessed on COCO annotations (Lin et al., 2014). End-to-end metrics. For text-to-text models, we use perplexity (Ppl.) (Jelinek et al., 1977) for WikiText, accuracy (Acc.) for LAMBADA and NIAH. For text-to-video models, following Zhao et al. (2025), we evaluate the quality of generated videos on five metrics: CLIPSIM and CLIP-Temp (CLIP-T) (Liu et al., 2024) to measure the text-video alignment; VQA-a and VQA-t to assess the video aesthetic and technical quality, respectively; and Flow-score (FScore) for temporal consistency (Wu et al., 2023). For text-to-image models, generated images are compared with the images in three aspects: FID (Heusel et al., 2017) and sFID (Salimans et al., 2016) for fidelity evaluation, Clipscore (CLIP) (Hessel et al., 2021) for text-image alignment, and ImageReward (IR) (Xu et al., 2023) for human preference. Accuracy metrics. We use three metrics to assess the accuracy of quantized attention output compared to attention output in full-precision O. First, we flatten and into vectors in the shape of 1 n. Then, Cosine similarity: CosSim = (cid:80) OO/(cid:112)(cid:80) O2(cid:112)(cid:80) O2, Relative L1 distance: L1 = (cid:80) O/ (cid:80) O, Root mean square error: RM SE = (cid:112)(1/n) (cid:80)(O O)2."
        }
    ],
    "affiliations": [
        "Department of Computer Science, Tsinghua University"
    ]
}