{
    "paper_title": "Heimdall: test-time scaling on the generative verification",
    "authors": [
        "Wenlei Shi",
        "Xing Jin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "An AI system can create and maintain knowledge only to the extent that it can verify that knowledge itself. Recent work on long Chain-of-Thought reasoning has demonstrated great potential of LLMs on solving competitive problems, but their verification ability remains to be weak and not sufficiently investigated. In this paper, we propose Heimdall, the long CoT verification LLM that can accurately judge the correctness of solutions. With pure reinforcement learning, we boost the verification accuracy from 62.5% to 94.5% on competitive math problems. By scaling with repeated sampling, the accuracy further increases to 97.5%. Through human evaluation, Heimdall demonstrates impressive generalization capabilities, successfully detecting most issues in challenging math proofs, the type of which is not included during training. Furthermore, we propose Pessimistic Verification to extend the functionality of Heimdall to scaling up the problem solving. It calls Heimdall to judge the solutions from a solver model and based on the pessimistic principle, selects the most likely correct solution with the least uncertainty. Taking DeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification improves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute budget and to 83.3% with more compute budget. With the stronger solver Gemini 2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge discovery system, a ternary system where one poses questions, another provides solutions, and the third verifies the solutions. Using the data synthesis work NuminaMath for the first two components, Heimdall effectively identifies problematic records within the dataset and reveals that nearly half of the data is flawed, which interestingly aligns with the recent ablation studies from NuminaMath."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 7 3 3 0 1 . 4 0 5 2 : r Heimdall: test-time scaling on the generative verification Wenlei Shi, Xing Jin"
        },
        {
            "title": "Abstract",
            "content": "An AI system can create and maintain knowledge only to the extent that it can verify that knowledge itself [23]. Recent work on long Chain-of-Thought reasoning has demonstrated great potential of LLMs on solving competitive problems, but their verification ability remains to be weak and not sufficiently investigated. In this paper, we propose Heimdall, the long CoT verification LLM that can accurately judge the correctness of solutions. With pure reinforcement learning, we boost the verification accuracy from 62.5% to 94.5% on competitive math problems. By scaling with repeated sampling, the accuracy further increases to 97.5%. Through human evaluation, Heimdall demonstrates impressive generalization capabilities, successfully detecting most issues in challenging math proofs, the type of which is not included during training. Furthermore, we propose Pessimistic Verification to extend the functionality of Heimdall to scaling up the problem solving. It calls Heimdall to judge the solutions from solver model and based on the pessimistic principle, selects the most likely correct solution with the least uncertainty. Taking DeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification improves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16 compute budget and to 83.3% with more compute budget. With the stronger solver Gemini 2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge discovery system, ternary system where one poses questions, another provides solutions, and the third verifies the solutions. Using the data synthesis work NuminaMath [13] for the first two components, Heimdall effectively identifies problematic records within the dataset and reveals that nearly half of the data is flawed, which interestingly aligns with the recent ablation studies from NuminaMath. Date: April 15, 2025 Correspondence: Wenlei Shi at wenlei.shi@bytedance.com, Xing Jin at jinxing.9@bytedance.com"
        },
        {
            "title": "Introduction",
            "content": "In the realm of scientific and mathematical discovery, the process of logistic verification and validation is as crucial as the initial act of problem-solving. One of the most illustrative examples of this principle can be found in the famous thought experiment chasing beam of light by Albert Einstein, where he found the paradox within the established physics theories and further formulated the principle of the constancy of the speed of light, cornerstone of his Special Theory of Relativity. Recently, the problem solving ability of LLMs have been significantly improved. With the long Chain of Thought(CoT) reasoning, advanced LLMs are now able to effectively solve complex competition-level problems in both math and code domains. However, the verification ability of LLMs has not been sufficiently investigated. On one hand, although the intelligence of general purposed LLM increases rapidly with the long CoT capabilities, we find that current SOTA models with direct prompting [5, 21] are not good at verifications on complex problems, e.g., o1-mini only achieves 1 Figure 1 Scaling of Heimdall. Left: the verification accuracy scales with the response length during RL training. With more reasoning tokens, Heimdall gives more accurate judgment on the solutions on AIME2024. Middle: the verification accuracy scales with repeated sampling and Majority Voting. By sampling multiple verification trajectories and voting, the accuracy can be further improved. Right: with Heimdall scoring the solutions on AIME2025, the problem solving accuracy scales with the number of solutions. We verify 16 times on each solution and select the most likely correct one with Pessimistic Verification(16). When inter-playing with various solver models, Heimdall gives significant improvements over pure solver-based Majority Voting(MV). 80.9% on our evaluation dataset. On the other hand, some work [16, 17, 22, 25, 29, 32, 35] trains dedicated model for verification or critique but the high-quality verification data is hard to collect, which limits the verification capability and hence impedes the application to challenging problems. In this paper, we claim that verifying if solution is correct is special type of problem solving, i.e., true/false question and involves step-by-step judgment on the solution. Inspired by the recent progress on the long CoT reasoning, we propose to train long CoT verifier through reinforcement learning. We name it Heimdall, symbolizing its sharp ability to detect errors and safeguards the correctness of knowledge. We leverage PPO [20] algorithm and find that the data processing is critical to the RL training. Specifically, two types of problems hinder the optimization, i.e., easy problems with only correct solutions and hard problems with only wrong solutions, both of which lack contrastive examples and tends to guide the verifier to simply identify the hardness of problem, rather than finding the wrong position in the solution. By filtering out the two cases, the model learns the verification ability more effectively. Taking the competitive math problems as our primary experimental domain, we show that the verification ability follows the test time scaling law where the accuracy improves significantly from 62.5% to 94.5% as the response length grows, as is shown the left of Figure 1. The performance can be further improved by sampling multiple verifications and voting on the judgment results. In the middle of Figure 1, the accuracy grows from 94.5% to 97.5% on AIME2024 as the number of verifications grows from 2 to 64. Furthermore, the evaluation from human experts shows that Heimdall generalizes well on math proof problems although it is trained with only the calculation problems with explicit answers. In addition, we extend the usage of Heimdall to scale up the problem solving. Suppose the solver model gives multiple solutions for problem and Heimdall judges the correctness of each solution for multiple times. We can select the best solution based on the verification results. We frame the selection process as multi-arm bandit problem where solutions with the same conclusion are treated as multiple visits to the same arm. Based on the pessimism principle, we propose the solution selection algorithm called Pessimistic Verification that minimizes the uncertainty of selecting wrong solutions. The algorithm unifies Majority Voting and reward model based Best-of-N by balancing the contributions of the solver and the verifier, and empirically demonstrates better scaling over both algorithms. Taking DeepSeek-R1-Distill-Qwen-32B [6] as the solver model, which scores 54 on AIME2025, Pessimistic Verification raises the scores to 70 with 16 compute, matching the performance of o1, and to 83.3 with more compute. We further test with stronger solver models, including DeepSeek-R1 [6] and Gemini 2.5 Pro[3]. As is shown in the right of Figure 1, Pessimistic Verification with Heimdall consistently improves the problem solving of various models and with Gemini 2.5 Pro, the 2 accuracy on AIME2025 reaches 93%, matching the currently reported SOTA with multiple attempts by Grok3 [4]. Finally, we create prototype to demonstrate the utility of Heimdall on the automatic knowledge discovery. We use the work of math data synthesis called NuminaMath [12] as the procedure of automatically proposing new problems and the corresponding solutions, and call Heimdall to detect errors in the synthetic dataset. Human annotation demonstrates that Heimdall accurately identifies the errors in the dataset. The verification result also reveals that the quality of current synthetic dataset is poor, which is consistent with the authors finding that removing the dataset from training improve the performance of the solver model [13]. In summary, our contributions are as follows: We propose Heimdall, the long CoT verifier by reinforcement learning and demonstrate the superior accuracy than top-tier LLMs. Heimdall also shows good generalization ability on out-of-domain problems, such as math proof problems. We propose unified algorithm called Pessimistic Verification for inference time scaling on problem solving. Empirically, it scales better than the vanilla Majority Voting or the reward-model based Best-of-N and achieve SOTA accuracy on AIME2025. We create prototype to show the utility of Heimdall in the autonomous knowledge discovery, where Heimdall is used to identify the correctness of the problem-solution pairs synthesized by another LLM. Human evaluations shows that Heimdall can effectively detect the flaws in the synthetic data."
        },
        {
            "title": "2 Related Work",
            "content": "Reasoning model. Reasoning models outperform previous general-purpose models on challenging reasoning tasks. During the chain of thought(CoT) reasoning, they keep reflecting their claims and searching viable solutions, utilizing more compute budget and providing better and more robust results. OpenAI first released its reasoning models[2, 7, 30] that performs significantly better on competitive tests like AIME and CodeForces than its previous models. Work by DeepSeek[6] and Kimi[24] independently propose different ways of reinforcement learning to trigger the reflection and searching capability in their base models. Recently, Grok3 [4] and Gemini 2.5 Pro [3] also demonstrate their impressive reasoning capabilities through long CoT. Generative evaluation. Recently some work are interested in improving the verification ability of the LLMs. Some [8, 29, 32, 34] explores finetuning an LLM with synthetic verification data to improve its verification ability. However, it is hard to synthesize high-quality data if the LLM inherently lacks the verification skills. One of the related topic is LLM-as-a-Judge [5, 11] where LLM is prompted to evaluate responses from other LLMs. The work[11] leverages strong LLMs as judges to evaluate other models in various domains and reveals that strong LLM judges have good generalization ability in different domains. Some works design the judge system and analyze of the judgment behavior[19, 21, 27]. However, prompting is only effective on easy tasks, and when it comes to competitive tests, the general purposed reasoning models performs not quite well, as is reveals from our test in Section 4. Another similar topic is critique [9, 10, 14, 15, 18], which often focuses on code and math problems and is used for giving suggestions for further revision. Critique fine-tuning [25] shows that fine-tuning on high-quality critique data is beneficial to the reasoning ability of base model. Several work, e.g., CTRL [26] leverages RL to train LLMs. However, they do not leverage the long CoT ability, which limits the verification performance on complex reasoning problems."
        },
        {
            "title": "3 Approach",
            "content": "We define the verification task where we ask model to judge if solution to problem is correct in its CoT and finally put its judgment result at the end of the response. Table 1 is the template of the verification prompt and the expected format of response. 3 Here is math problem and solution of it. Think step by step and verify if the final answer in the solution is correct. The last line of your response should be of the form Answer: $Answer (without quotes) where $Answer is 1 if the final answer in the solution is correct and 0 if incorrect. **Problem** ${problem} **Solution** ${solution} Table 1 Prompt template for verification."
        },
        {
            "title": "3.1 Reinforcement learning for verification",
            "content": "RL Setup. Let = {(pi, si, yi)}N be our dataset, where pi is problem, si is solution to the problem, i=1 which may be the response from reasoning model and yi {0, 1} represents the correctness of the solution, with 1 indicating correctness and 0 indicating incorrectness. Given triplet (pi, si, yi), we prompt LLM to check the correctness of the solution step-by-step and finally give conclusion on the correctness, as is shown in Table 1. Denoting the prompt as qi, the verifier model πθ(zi, iqi) takes prompt as input and generates the CoT zi on judging the correctness of yi and at last gives boolean conclusion if is correct. The outcome reward function is as follows: R(y, y) = (cid:40) = y, 1 1 = y. Then the objective of RL is, (θ) = E(q,y)D,(z,y)πθ(q) [R(y, y)] We run the vanilla PPO algorithm on reasoning model, and propose two methods to enhance the training. Data collection and filtering. We collect the dataset by prompting one or multiple reasoning models to solve problems. For every problem in the dataset, we collect multiple solutions and construct verification prompt with each solution using the template in Table 1. However, two cases may hurt the RL training, i.e., the extremely difficult problem, which we fail to sample any correct solutions and the extremely easy problems, which we fail to sample any wrong solutions. Such unbalanced data may teach the verifier to be biased on the difficulty of the problem, i.e., be optimistic on easy problems and pessimistic on difficult problems. Therefore, we do not include the data of the two cases in the training dataset."
        },
        {
            "title": "3.2 Solution selection by Pessimistic Verification",
            "content": "When tackling challenging problems, one can sample multiple solutions and leverage the verifier to identify the most likely correct one. By sampling verification responses multiple times, we can achieve more reliable judgments, thereby improving overall problem-solving performance. We propose principled and flexible method for the inference time scaling along the two dimensions, i.e., the amount of solutions sampled from the solver model and the amount of verifications sampled from verifier model. Denote the number of solutions to problem as and the number of verifications on each solution is . We initially conceptualize the selection process as multi-arm bandit problem, where each arm corresponds to distinct answer, and each verification constitutes visit to an arm. The reward is the verification result, which can be either 1 or 0. Each time the solver generates solution, the arm representing the solutions answer receives visits and immediately accrues rewards. The straight-forward approach is to calculate the average reward each arm receives as its score and greedily select the one with the highest score. However, for those with few visits, the score fluctuates 4 and can be unreliable. Following the pessimism principle in RL, we introduce the lower-confidence-bound, which adds an uncertainty penalty to the score. Let r0, r1, ...rK be the average reward of each answer and N0, N1, ..., NK be the number of solutions that drives to certain answer. The selection algorithm is defined as: (cid:18) ˆa := arg max ai r(ai) α (cid:19) ln (N ) NiM + 1 (1) where the parameter α is hyper-parameter that balances the consideration of uncertainty in the decisionmaking process and ln is the natural logarithm. Intuitively, the first term reflects the signals from the verifier, while the second term accounts for the bias of solver in the answer space. When Ni is small, the second term dominates, which neglects the verification and in the extreme case, it collapses to Majority Voting. When Ni is large, the first term becomes more important, and in the extreme case, it simply selects the answer with the best verification score. The phase shift aligns with the fact that Majority Voting is trapped in the bias of the solver, for example when wrong answer occurs more frequently than the correct one, and as and is large, the verification scores stabilize and we tend to trust more on it, because the verification is often easier than the solution."
        },
        {
            "title": "4.1 Dataset",
            "content": "Our experiment is on the math problems. The training dataset comes from the AoPS website and official math competition homepages, similar to that of DAPO [28]. We leverage DeepSeek-R1-Distill-Qwen-32B model as the policy model to generate 16 solutions to each problem. We leverage rule-based program to check if the final answer in the solution is correct, which compares the reference answer of problem and the answer in the solution and outputs the label, i.e., 1 for correct response and 0 for the incorrect response. Then we construct the verification dataset with the prompt template in Table 1. To keep the prompt clean and short, we remove the <think> part in each solution and only use the summary part. We test the verification ability on both AIME2024 and AIME2025, 60 questions in total. During training, we monitor the performance on AIME2024 and select the best checkpoint as the final version of Heimdall. Therefore, one can treat AIME2024 as the validation dataset and AIME2025 as the test dataset."
        },
        {
            "title": "4.2 Scaling of verification",
            "content": "Figure 2 shows the accuracy and the length of response tokens during RL training. As depicted by the blue curve, both accuracy and response length increase with the number of training steps, albeit at different rates. At the early stage, the accuracy improves rapidly, but the response length fluctuates. This is because minor adjustment to the policy can significantly boost accuracy. Later, the response length grows constantly, while the accuracy gradually converges to 94.5%, because the model is learning to tackle the hardest part in the training dataset, which requires the increasingly more reasoning tokens. The red curve represents the RL training without the data filtering strategy, i.e., incorporating both extreme cases of difficult and easy problems in the dataset. As training progresses, the performance gap becomes more pronounced, indicating that the absence of contrastive examples detrimentally impacts performance. In addition, we test o1-mini with the same evaluation data, which is shown as the dash line in the left of Figure 2. Our model outperforms o1-mini in fewer than 20 steps, indicating substantial potential for enhancing the verification capabilities of general-purpose reasoning models. We further look into cases to understand what Heimdall has learned during the training. Table 3 shows the verification of correct solution to hard problem in AIME2025. Due to space limitations, we only highlight some key points. We can observe two types of checking: Forward checking. It checks if the reasoning chain in every step of the solution is correct, which is generally applicable to all problems. 5 Figure 2 Accuracy and response length during RL training. PPO w/o data filtering is the RL training with all problems in the dataset. Left: the accuracy on AIME2024 with the training steps. Right: the response length on the training dataset with the training steps. Backward checking. It checks whether conclusion, be it intermediate or final, fits the known constraints. For some types of problems like solving equations and finding the general term formula of sequence, the backward checking is efficient and easy to implement. The case exemplifies the common task of deriving general formula for sequence. As illustrated, Heimdall applies both methods of validation to confirm the correctness of the solution. Next, we investigate how the verification ability scales as the number of verifications increases. We sample 64 solutions for each problem with the solver model and 64 verifications for each solution, resulting in total of 30 64 64 responses on either AIME2024 or AIME2025 dataset. Denoting the number of verifications of each solution as , we randomly select verifications for each solution from the data collected above, and determine the final score by some aggregation operation, e.g., Majority Voting and averaging. We repeat the process for 2048 times to eliminate any fluctuations in the statistics. Taking Majority Voting as the aggregation operation, we compute the accuracy, the false positive rate and the false negative rate at every compute budget , as is shown in the top of Figure 3. In addition, we take the average of scores, decimal number in [0, 1], as the final score, and draw the curve of the AUC score in the bottom-left of Figure 3. It shows that Heimdalls performance can be significantly improved by simply repeat sampling more trajectories. As goes larger, the performance gradually converges to upper limit. For example, the accuracy converges to about 97.5% on AIME2024 and 96.0% on AIME2025, and the remaining failure cases are the bias inherent in the model that could not be eliminated by adding more compute budget. We further analyze the distribution of those failure cases. For each problem, we calculate two statistics, one is the difficulty of solving it, which is estimated by the pass rate over its 64 solutions. and the other is the difficulty of verifying its solutions, which is estimated by the total number of verification failures on its solutions. Taking the two values as the x-axis and the y-axis respectively, we visualize their correlations in the bottom-right of Figure 3. We observe that the difficulty of problem does not necessarily correlate to the difficulty of verifying its solutions. For example, Heimdall fails 17/64 times on problem whose pass-rate is 67%. In addition, clear observation is that the performance of Heimdall on AIME2025 is generally worse than that on AIME2024. We believe the main reason is that the verifying solutions on AIME2025 is harder than that on AIME2024. One evidence is that, o1-mini achieves 80.9% in AIME2024 and 75.3% in AIME2025, whose degradation is larger than that of our model."
        },
        {
            "title": "4.3 Scaling of problem solving with verification",
            "content": "In terms of problem solving, repeated sampling[1] is known to scale with the inference time compute. We evaluate multiple scaling algorithms, including both with and without the verifier. We analyze the inference 6 Figure 3 The inference-time scaling of verification ability on problem solutions in AIME2024 and AIME2025. Top-left: We show the accuracy of Heimdall when we sample multiple verification responses and make the judgment by majority voting. Top-right: We show the decreasing false-negative rate(FNR) and false-positive rate(FPR) as we scale up verification responses with majority voting. Bottom-left: We calculate the average score of verification responses and draw the AUC along each number of responses. Bottom-right: We collect the verification failure cases on every math problem and draw the relation between the difficulty of the problem and the number of verification failures, which reveals that the verification difficulty may not necessarily correlate with the difficulty of the original problem. 7 ... **Problem** Let the sequence of rationals x1, x2, . . . be defined such that x1 = 25 and xk+1 = (cid:18) xk + 1 3 1 xk (cid:19) 1 . for relatively prime positive integers and n. Find the remainder when x2025 can be expressed as + is divided by 1000. **Solution** ... + b2 akbk and bk+1 = 3akbk. **Verification** <think> Okay, so need to figure out if the given solution to this problem is correct. Let me read through it again to make sure understand. The problem is: ... The solution provided has four key steps. Let me go through each step and verify. Step 1: They let xk = ak/bk with ak and bk coprime. Then they write the recurrence relations as ak+1 = a2 Wait, lets verify that. Starting from xk+1 = (1/3)(xk + 1/xk 1). If xk = ak/bk, then 1/xk = bk/ak. So xk + 1/xk = (a2 ... Wait let me confirm with S3: a3 = a2 ... [Backward checking] ... </think> The solution correctly arrives at the conclusion despite some initial confusion in their explanation. They properly computed each part modulo 1000, leading to the correct final result. Answer: 1 k)/(akbk). ... Okay, so step 1 seems correct. [Forward checking] 2 a2b2. Wait a2 = 471, b2 = 825. So a3 = 4712 +8252 471825. + 2 +b2 Table 2 case of verification on hard problems. The verification process contains both forward checking, i.e., checking if each step is correct and backward checking, i.e., checking if conclusion matches constraints, as is shown in the position of the red texts. 8 Figure 4 The inference-time scaling of problem solving with Heimdall. The two figures show the accuracy on AIME datasets as the number of solutions scales up. Left: the problem solving accuracy on AIME2025 dataset scales with the number of solutions. The colored shaded area represents the area covered by the accuracy curves of selection algorithm as the number of verifications increases from 1 to 64. Right: the contour map of the accuracy of Pessimistic Verification as the number of solutions (x-axis) and the number of verifications (y-axis) increase. The red curve indicates the optimal configurations within various overall compute budgets. time scaling in two dimensions, i.e., the number of solutions and the number of verifications on each solution . Considering the huge computational cost, we only evaluate on AIME2025, with [2, 256] and [1, 64]. Similar to the analysis of verification accuracy, we first sample = 256 solutions for each problem and = 64 verifications for each solution as the complete data and then randomly select subset to evaluate each scaling algorithm under specific compute budget (M, ). We repeat the sampling for 2048 time to get stable average score. We set α = 0.1 in Pessimistic Verification and also evaluate the other three selection algorithms as follows. Majority Voting Majority voting is one of the most commonly used inference time scaling methods. It first categorize the solutions, e.g., by the final answers for math problems. It simply select the category that contains the largest number of solutions in it. As fore-mentioned, majority voting can be seen as special case of Pessimistic verification, where α is large enough to overshadow the signal of verification. Shortest Majority Voting. The recent work [31] observes length bias that the correct solutions are often shorter than incorrect ones for the same questions. Suppose the answer ai occurs ci times in the sampled responses and the average length of responses with the answer ai is li, the voting score for ai is si = ci li Sampling-based Search. The work [33] leverages commercial LLM as the verifier, and scales the inferencetime computation on the number of sampled solutions and the number of verifications. During the selection, it calculates the average verification score of each solution and selects the solution with the largest score. Note that it does not group the solutions based on their answers, which is different from the special case of Pessimistic verification where α equals zero. Tie-breaking rules. The selection algorithm may encounter tie situation, where multiple options have the same score. In principle, one can introduce another model to compare which option is better, but for simplicity, we leverage the length prior to break the tie, namely, selecting the option with the shortest average solution length. The left of Figure 4 shows how the accuracy of different scaling algorithms changes with the number of solutions . Majority voting are the worst among all scaling algorithms under the same . By employing the length prior, Shortest Majority Voting gives better accuracy when is small, but finally converges to 70%, the same as that of Majority Voting. The red and blue areas represents the areas covered by the group of accuracy curves with [1, 64] of Sampling-based Search and Pessimistic Verification respectively. As the figure shows, with the help of verification, the accuracy is significantly improved. In addition, Pessimistic Verification is better than Sampling-based Search when is small. The reason is that the verification process is inherently probabilities. Even for easy task, it is still possible that wrong solution is judged as correct and is finally selected as the final solution. With the second term in Equation 1, Pessimistic Verification penalize such uncertain cases and favors those with more visits. As goes larger, the verification scores stabilize and the second term becomes smaller, and the gap between the two algorithms gets smaller. Interestingly, when is large, the gap is large again. By checking the typical cases, we find that it comes from the grouping of solutions. Pessimistic Verification aggregate the solutions with their answers while Sampling-based Search treats each solution independently. Two solutions may have the same final answer, but their approaches or expressions can be entirely different. The aggregation takes this variance into consideration, so makes more robust selection. Note that such grouping is not generally applicable, e.g., grouping the solutions of proof problems is not straight-forward. In those cases, we expect the two algorithm converges to the same limit. Taking the accuracy of Pessimistic Verification as function of and , we draw the contour map in the right of Figure 4. = 0 represents the vanilla Majority Voting without verifications. We can see that increasing either or improves the performance. Each point (M, ) involves responses by the solver model and responses by the verifier, which is (N + 1) responses in total. By minimizing the overall budget, we derive the compute-optimal configurations for different compute budgets, with the constraint that and are non-negative integers. As is shown in the figure, we need to alternately increase and M, but we should increase more frequently. The reason is that the correct answer for hard problem is sparse. To get the problem solved with scaling algorithm, we first need to give sufficient budget to get the correct answer. Remaining room for improvement. Can Heimdall be better? The black dashed curve in the left of Figure 4 is Best-of-N that selects the response that equals the ground-truth answer, which is the upper-limit of any scaling algorithms. When is small, Pessimistic Verification Pessimistic Verification performs near the upper limit, but the gap widens as increases. Consider the configuration = 256, = 64. Pessimistic Verification gets score of 83.3% and the upper limit is 93.3%, so the gap is 3 problems. Looking into the individual problems, we find that there are 4 problems that have only one correct solution among the 256 solutions. Heimdall manages to identify the correct solution on one of them, which is the case fore-mentioned in Table 3, but fails on the other three problems. The failed three problems involve spatial reasoning, which the base model of Heimdall is not very skilled at. We believe that as the ability of the base model becomes better, the verification ability can reach the upper limit. Coordination with other solver models. In the previous experiments, we use DeepSeek-R1-Distill-Qwen-32B as the solver model, which is the model to collect data during the training phase. To test Heimdalls generalization to other solver models, we test on DeepSeek-R1 and Gemini 2.5 Pro. For DeepSeek-R1, we directly extract the summary in its response, while for Gemini 2.5 Pro, we leverage another LLM to summarize its solution because we observe that its responses contains many reflections, which might confuse the verification process. Due to the limited compute budget, we set the total number of verifications to 16 and the total number of solving attempts to 64. We find that the verification accuracy on DeepSeek-R1 and Gemini 2.5 Pro is 90.1% and 89.9%, respectively, close to that on DeepSeek-R1-Distill-Qwen-32B. The scaling in problem-solving, as is shown in the right of Figure 1, indicates that Heimdall consistently improves the accuracy with various solver models and using stronger solver models results in higher accuracy. Meanwhile, the improvements for Gemini 2.5 Pro is smaller compared to those of the other models. The reason is that the accuray is already high on AIME2025 and the base model of Heimdall inherently lacks some ability on certain problems."
        },
        {
            "title": "5 Generalization to math proof problems",
            "content": "Heimdall is trained on math problems with explicit answers because it is easy to determine the correctness of the solution. However, there are many other types of problems, some of which might be more challenging. 10 Here is math problem and solution of it. Think step by step and verify if each proof step in solution is correct. The last line of your response should be of the form Answer: $Answer (without quotes) where $Answer is 1 if the solution is correct and 0 if incorrect. **Problem** The players of hockey team gather to select their team captain. Initially, they stand in circle, and each person votes for the person on their left. The players will update their votes via series of rounds. In one round, each player updates their vote, one at time, according to the following procedure: At the time of the update, if is voting for b, and is voting for c, then updates their vote to c. (Note that a, b, and need not be distinct; if = c, then as vote does not change for this update.) Every player updates their vote exactly once in each round, in an order determined by the players (possibly different across different rounds). They repeat this updating procedure for rounds. Prove that at this time, all players will unanimously vote for the same person. **Solution** ... **Verification** <think> ... Wait, but since the solution didnt present this reasoning, their proof is flawed. They presented an incorrect model of the process, leading to an overestimation of the rounds needed, but the conclusion is still correct. However, the problem is to verify if each step in their solution is correct. Since their step 4s reasoning is wrong, their proof is incorrect. ... </think> The solutions step 4 incorrectly assumes that each persons vote jumps times, leading to covering the entire ring. The correct approach is to note that each round reduces the number of distinct targets by one, leading to convergence in n-1 rounds. Thus, while the conclusion is correct, the reasoning in step 4 is flawed. Answer: Table 3 case of verification on the math proof problems. The problem is P1 in Canadian Mathematical Olympiad 2025. We modify the prompt template to check the proof process rather than the final answer, which is the red text. Heimdall checks the proof step by step and finds that the solution lacks rigorous proofs in step 4. 11 In this section, we would like to test Heimdalls capability in verifying mathematical proof problems. We select 10 proof problems from Mathematics Olympiad of different countries from the years 2024 and 2025, and leverage solver model, i.e., DeepSeek-R1-Distill-Qwen-32B, to generate proof process for each problem. Considering that the solver model is not good at spatial reasoning, we do not select geometry-related problems. Heimdall is then employed to check the correctness of each proof. Finally, we have experts evaluate both the proof processes and Heimdalls verifications. The solver model correctly solves 2 problems, while the remaining 8 are incorrect. To our surprise, Heimdall correctly judges 9/10 cases, identifying 2/2 correct proofs and detecting issues in 7/8 incorrect proofs. There is 1 problem where Heimdall fail to identify the error in the proof, resulting in false-negative judgment. Looking into the specific cases, we find that Heimdall judges the correctness with both forward and backward checking, e.g., checking each step and testify with examples. It is capable of identifying most errors or unproven assumptions in the solution, but for some subtle problems, e.g., the assumption does not appear in the form of proposition but is implicitly assumed during the proof process, Heimdall might fail. We believe that introducing the proof data in the RL training would improve the performance of Heimdall and an important direction in the future would be how to generate the dataset in large scale."
        },
        {
            "title": "6 Verification on automatic knowledge discovery",
            "content": "In the process of human exploration of the unknown, some scientists pose questions, some propose solutions to these questions, and others focus on verifying the correctness of solutions provided by their predecessors. Verification itself, as crucial part of knowledge discovery, ensures the correctness of new knowledge. In this section, we design prototype that simulates the stages of posing questions and solving them, using the synthesis of math problems by NuminaMath [12], to evaluate Heimdalls effectiveness in detecting problematic knowledge. NuminaMath open-sources comprehensive collection of 860,000 pairs of math problems and reference solutions. It includes 229,982 MATH-level pairs and 62,108 AMC-AIME-level pairs that are synthesized from seed problems in MATH and AMC-AIME training dataset. We test Heimdall on the harder one, i.e., the AMC-AIME dataset. Flaws can exist either within the problem itself, such as an unsolvable problem, or within the solution provided. Theoretically, by checking if the solution satisfies all the requirements in the problem, Heimdall can detect both flaws. Meanwhile, the task also indirectly tests the generalization capabilities of Heimdall, as the qualities of the problems in the training set is much higher. We randomly sample 8, 192 pairs of questions and solutions as the test set. For each pair, we construct the prompt and query Heimdall 8 times. We calculate the sum of the verification scores, which ranges from 0 to 8 and illustrate the distribution in Figure 5. As is shown, near half of the data is labeled incorrect with high confidence, which is consistent with the experience listed in the latest NuminaMath-1.5 website [13] that by the ablation study, the authors find that this dataset hurts bit the performance and plan to remove all synthetic data until they find way to reliably generate high-quality synthetic problems. To measure the verification correctness, we randomly select 10 cases in the 0-scored group and manually check their correctness. We find that for all cases, the judgments by Heimdall are correct. Table 4 is case of the problematic data and the verification. Due to space limitations, we retain only the essential information, with the rest omitted. The question does not have the correct answer among its options. Meanwhile, the solution mistakenly uses an incorrect value in the equation and interestingly arrives at one of the given options. We hypothesize Figure 5 The distribution of verification scores on the problems of synthetic dataset. The x-axis is the sum of scores across 8 verifications and the y-axis is the number of problems corresponding to each sum. 12 Prompt: Here is math problem and solution of it. Think step by step and verify if the final answer in the solution is correct. ... **Problem** rabbit starts jumping from the point (2, 3) on rectangular board that extends from (0, 0) to (6, 5). Each jump it makes is either left, right, up, or down, and has length of 1 unit. Each jump direction is uniformly and randomly chosen. The jumping sequence continues until the rabbit reaches any border of the rectangle. Calculate the probability that the rabbits sequence of jumps ends on the top or bottom side of the rectangle. (A). 1 4 (C). 17 24 (B). 17 32 (D). 1 3 (E). 3 4 **Solution** Let P(x,y) denote the probability of the rabbit ending on the top or bottom side starting from (x, y). Using symmetry about the center line = 2.5 and = 3, and recursive relations, we analyze the movements. ... ####Solving Equations: We solve this using recursive approaches and considering border cases explicitly. Assuming symmetry simplifies P(3,2) = P(3,3). Here, lets consider only two key points (2, 3) and (3, 3) for simplicity. Solving: P(2,3) = 1 4 P(2,2) + 1 4 (1) + 1 P(1,3) + 1 4 P(3,3) ... Thus, the rabbits sequence of jumps ends on the top or bottom side with probability of 17 24 answer is (C) 17 24 . The final Response: <think> ... </think> The solution provided incorrectly assumes that P(2,4) = 1, which is not true since (2,4) is not top or bottom border. They oversimplified the equations by not accounting for the correct dependencies of other points like P(2,4) and mishandled the symmetry in y-direction due to different boundary conditions (P=0 at y=0 and P=1 at y=5). Answer: 0 Table 4 An example of problematic synthetic data and the verification. The verification correctly points out the problem in the solution. 13 that the task is somewhat challenging for the LLM used for synthetic data generation, leading the LLM to hallucinate superficially consistent but wrong output."
        },
        {
            "title": "7 Future Work",
            "content": "Firstly, the verification dataset is formed by simply extracting the summary part of reasoning model, which as we observed is sometimes overly brief, e.g., driving to an answer with only one sentence. more detailed explanation would make the verification easier. One can further improve the verification accuracy by summarizing the reasoning process with another LLM. Secondly, we mainly evaluate the verification ability on math problems that have final answers. There are many other types of tasks, e.g., coding problems and Mathematical proof problems. Although the learned ability is generalizable to other domains, we expect it beneficial to train with data in other domains. For example, in the context of coding problems, backward checking may take the alternative form of designing test cases. Lastly, we only prototype the usage of Heimdall in the automatic knowledge discovery. In real scenarios, posing valuable questions is challenging task that demands both curiosity and keen insight. Such ability is often the critical part of the scientific discovery, which however is seldom investigated. We believe that as the general capabilities of LLM continues to advance, this direction will become more and more important."
        },
        {
            "title": "8 Conclusion",
            "content": "In this paper, we propose to train long CoT verifier called Heimdall with reinforcement learning. On the competitive math problems, Heimdall achieves high accuracy and scales well along both the length of reasoning chains and the number of repeated generation. Through human evaluation, we find that Heimdall also shows impressive generalization ability on out-of-domain problems, such as math proofs. We further propose the inference time scaling algorithm called Pessimistic Verification, which incorporates solver and Heimdall for problem solving. By scaling up the compute, we can achieve the performance comparable to top-tier models on challenging math problems. Lastly, we design prototype of automatic knowledge discovery and demonstrate that Heimdall can reliably detect flaws in the synthetic data from another LLM."
        },
        {
            "title": "9 Acknowledgments",
            "content": "We thank the data annotation team for their expertise on collecting the evaluation data and analyzing the verification outputs, including Bocheng Zhou, Weijian Zhao, Tong Sun and Zhiyuan Zhang."
        },
        {
            "title": "References",
            "content": "[1] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. [2] Ahmed El-Kishky, Alexander Wei, Andre Saraiva, Borys Minaev, Daniel Selsam, David Dohan, Francis Song, Hunter Lightman, Ignasi Clavera, Jakub Pachocki, et al. Competitive programming with large reasoning models. arXiv preprint arXiv:2502.06807, 2025. [3] Google. Gemini 2.5: Our most intelligent ai model, 2025. URL https://blog.google/technology/ google-deepmind/gemini-model-thinking-updates-march-2025/#gemini-2-5-thinking. Accessed: 2025-0325. [4] Grok. Grok 3 beta the age of reasoning agents, 2025. URL https://x.ai/news/grok-3. Accessed: 2025-02-19. [5] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024. [6] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [7] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [8] Minki Kang, Jongwon Jeong, and Jaewoong Cho. T1: Tool-integrated self-verification for test-time compute scaling in small language models, 2025. URL https://arxiv.org/abs/2504.04718. [9] Tian Lan, Wenwei Zhang, Chengqi Lyu, Shuaibin Li, Chen Xu, Heyan Huang, Dahua Lin, Xian-Ling Mao, and Kai Chen. Training language models to critique with multi-agent feedback. arXiv preprint arXiv:2410.15287, 2024. [10] Tian Lan, Wenwei Zhang, Chen Xu, Heyan Huang, Dahua Lin, Kai Chen, and Xian-Ling Mao. Criticeval: Evaluating large-scale language model as critic. Advances in Neural Information Processing Systems, 37:66907 66960, 2024. [11] Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, et al. From generation to judgment: Opportunities and challenges of llm-as-a-judge. arXiv preprint arXiv:2411.16594, 2024. [12] Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13:9, 2024. [13] Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Numinamath. [https://huggingface.co/AI-MO/NuminaMath-1.5](https://github.com/ project-numina/aimo-progress-prize/blob/main/report/numina_dataset.pdf), 2024. [14] Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, and Yujiu Yang. Criticbench: Benchmarking llms for critique-correct reasoning. arXiv preprint arXiv:2402.14809, 2024. [15] Liangchen Luo, Zi Lin, Yinxiao Liu, Lei Shu, Yun Zhu, Jingbo Shang, and Lei Meng. Critique ability of large language models. arXiv preprint arXiv:2310.04815, 2023. [16] Ruotian Ma, Peisong Wang, Cheng Liu, Xingyan Liu, Jiaqi Chen, Bang Zhang, Xin Zhou, Nan Du, and Jia Li. S2r: Teaching llms to self-verify and self-correct via reinforcement learning. arXiv preprint arXiv:2502.12853, 2025. [17] Dakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, Jan-Philipp Fränken, Chelsea Finn, and Alon Albalak. Generative reward models. arXiv preprint arXiv:2410.12832, 2024. [18] Nat McAleese, Rai Michael Pokorny, Juan Felipe Ceron Uribe, Evgenia Nitishinskaya, Maja Trebacz, and Jan Leike. Llm critics help catch llm bugs. arXiv preprint arXiv:2407.00215, 2024. 15 [19] Qian Pan, Zahra Ashktorab, Michael Desmond, Martin Santillan Cooper, James Johnson, Rahul Nair, Elizabeth Daly, and Werner Geyer. Human-centered design recommendations for llm-as-a-judge. arXiv preprint arXiv:2407.03479, 2024. [20] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [21] Guijin Son, Hyunwoo Ko, Hoyoung Lee, Yewon Kim, and Seunghyeok Hong. Llm-as-a-judge & reward model: What they can and cannot do. arXiv preprint arXiv:2409.11239, 2024. [22] Linzhuang Sun, Hao Liang, Jingxuan Wei, Bihui Yu, Tianpeng Li, Fan Yang, Zenan Zhou, and Wentao Zhang. Mm-verify: Enhancing multimodal reasoning with chain-of-thought verification. arXiv preprint arXiv:2502.13383, 2025. [23] Rich Sutton. Verification, the key to ai. URL http://incompleteideas.net/IncIdeas/KeytoAI.html. [24] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [25] Yubo Wang, Xiang Yue, and Wenhu Chen. Critique fine-tuning: Learning to critique is more effective than learning to imitate. arXiv preprint arXiv:2501.17703, 2025. [26] Zhihui Xie, Liyu Chen, Weichao Mao, Jingjing Xu, Lingpeng Kong, et al. Teaching language models to critique via reinforcement learning. arXiv preprint arXiv:2502.03492, 2025. [27] Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, Pin-Yu Chen, et al. Justice or prejudice? quantifying biases in llm-as-a-judge. arXiv preprint arXiv:2410.02736, 2024. [28] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [29] Yue Yu, Zhengxing Chen, Aston Zhang, Liang Tan, Chenguang Zhu, Richard Yuanzhe Pang, Yundi Qian, Xuewei Wang, Suchin Gururangan, Chao Zhang, et al. Self-generated critiques boost reward modeling for language models. arXiv preprint arXiv:2411.16646, 2024. [30] Wojciech Zaremba, Evgenia Nitishinskaya, Boaz Barak, Stephanie Lin, Sam Toyer, Yaodong Yu, Rachel Dias, Eric Wallace, Kai Xiao, Johannes Heidecke, et al. Trading inference-time compute for adversarial robustness. arXiv preprint arXiv:2501.18841, 2025. [31] Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Yunhua Zhou, and Xipeng Qiu. Revisiting the test-time scaling of o1-like models: Do they truly possess test-time scaling capabilities? arXiv preprint arXiv:2502.12215, 2025. [32] Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240, 2024. [33] Eric Zhao, Pranjal Awasthi, and Sreenivas Gollapudi. Sample, scrutinize and scale: Effective inference-time search by scaling verification. arXiv preprint arXiv:2502.01839, 2025. [34] Jian Zhao, Runze Liu, Kaiyan Zhang, Zhimu Zhou, Junqi Gao, Dong Li, Jiafei Lyu, Zhouyi Qian, Biqing Qi, Xiu Li, et al. Genprm: Scaling test-time compute of process reward models via generative reasoning. arXiv preprint arXiv:2504.00891, 2025. [35] Jianyuan Zhong, Zeju Li, Zhijian Xu, Xiangyu Wen, and Qiang Xu. Dyve: Thinking fast and slow for dynamic process verification. arXiv preprint arXiv:2502.11157, 2025."
        }
    ],
    "affiliations": [
        "bytedance.com"
    ]
}