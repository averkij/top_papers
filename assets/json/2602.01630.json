{
    "paper_title": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks",
    "authors": [
        "Bohan Zeng",
        "Kaixin Zhu",
        "Daili Hua",
        "Bozhou Li",
        "Chengzhuo Tong",
        "Yuran Wang",
        "Xinyi Huang",
        "Yifan Dai",
        "Zixiang Zhang",
        "Yifan Yang",
        "Zhou Liu",
        "Hao Liang",
        "Xiaochen Ma",
        "Ruichuan An",
        "Tianyi Bai",
        "Hongcheng Gao",
        "Junbo Niu",
        "Yang Shi",
        "Xinlong Chen",
        "Yue Ding",
        "Minglei Shi",
        "Kai Zeng",
        "Yiwen Tang",
        "Yuanxing Zhang",
        "Pengfei Wan",
        "Xintao Wang",
        "Wentao Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "World models have emerged as a critical frontier in AI research, aiming to enhance large models by infusing them with physical dynamics and world knowledge. The core objective is to enable agents to understand, predict, and interact with complex environments. However, current research landscape remains fragmented, with approaches predominantly focused on injecting world knowledge into isolated tasks, such as visual prediction, 3D estimation, or symbol grounding, rather than establishing a unified definition or framework. While these task-specific integrations yield performance gains, they often lack the systematic coherence required for holistic world understanding. In this paper, we analyze the limitations of such fragmented approaches and propose a unified design specification for world models. We suggest that a robust world model should not be a loose collection of capabilities but a normative framework that integrally incorporates interaction, perception, symbolic reasoning, and spatial representation. This work aims to provide a structured perspective to guide future research toward more general, robust, and principled models of the world."
        },
        {
            "title": "Start",
            "content": "Bohan Zeng * 1 Kaixin Zhu * 1 Daili Hua * 1 Bozhou Li * 1 Chengzhuo Tong * 1 Yuran Wang * 1 Xinyi Huang * 1 Yifan Dai * 2 Zixiang Zhang * 1 Yifan Yang * 1 Zhou Liu 1 Hao Liang 1 Xiaochen Ma 3 Ruichuan An 1 Tianyi Bai 3 Hongcheng Gao 4 Junbo Niu 1 Yang Shi 1 Xinlong Chen 5 Yue Ding 5 Minglei Shi 4 Kai Zeng 1 Yiwen Tang 1 Yuanxing Zhang 6 Pengfei Wan 6 Xintao Wang 6 Wentao Zhang 1 6 2 0 2 2 ] . [ 1 0 3 6 1 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "World models have emerged as critical frontier in AI research, aiming to enhance large models by infusing them with physical dynamics and world knowledge. The core objective is to enable agents to understand, predict, and interact with complex environments. However, current research landscape remains fragmented, with approaches predominantly focused on injecting world knowledge into isolated tasks, such as visual prediction, 3D estimation, or symbol grounding, rather than establishing unified definition or framework. While these task-specific integrations yield performance gains, they often lack the systematic coherence required for holistic world understanding. In this paper, we analyze the limitations of such fragmented approaches and propose unified design specification for world models. We suggest that robust world model should not be loose collection of capabilities but normative framework that integrally incorporates interaction, perception, symbolic reasoning, and spatial representation. This work aims to provide structured perspective to guide future research toward more general, robust, and principled models of the world. 1. Introduction With the explosive growth of internet data and continuous advances in neural network model training, existing *Equal contribution 1Peking University 2Shanghai Jiao Tong University 3HKUST 4Tsinghua University 5School of Artificial Intelligence, University of Chinese Academy of Sciences 6Kling Team, Kuaishou Technology. Correspondence to: Bohan Zeng <bhzeng25@stu.pku.edu.cn>, Wentao Zhang <wentao.zhang@pku.edu.cn>. Preprint. February 3, 2026. large models (Achiam et al., 2023; Bai et al., 2023; Yang et al., 2025a; Liu et al., 2024; Bai et al., 2025a; Chen et al., 2024b; Team et al., 2024; Lu et al., 2024) and diffusion models (Liu et al., 2022; Lipman et al., 2022; Labs, 2024; Peebles & Xie, 2023) have achieved remarkable results in various fields. However, as model performance further improves, the bottleneck in data quality has become increasingly difficult to overcome, hindering further progress, especially in multimodal domains requiring precise analysis, such as multimodal reasoning, chemical formula recognition, 3D scene generation, and specific professional areas like healthcare (Tang et al., 2024; 2025a; Tochilkin et al., 2024; Xiang et al., 2025b;a; Cheng et al., 2025). To break through the traditional token-prediction paradigm of large models, researchers have begun to focus on the study of world models. The concept of World Model was first introduced by (Ha & Schmidhuber, 2018), which proposed strategy of constructing an interactive system between agents and the world to handle complex visual input environments. With the rapid development of large models and various multimodal generation methods, recent work (Zhu et al., 2024) has further expanded the notion of world models, viewing video generation and 3D generation as intelligent systems that simulate the real world. Researchers are considering world models as the next-generation paradigm to replace token-predicting large language models (Yang et al., 2025c). As world models attract growing interest, numerous research fields have begun to incorporate world knowledge to empower models to perform tasks that require an understanding of physical and contextual rules (Yu et al., 2025a; Team et al., 2025b; Liu et al., 2025c; Zhu et al., 2025). This trend is evident in diverse applications, including image editing (Zeng et al., 2025a; Lin et al., 2025a; Chen et al., 2025c), multimodal spatial reasoning (Chen et al., 2024a), autonomous driving (Tu et al., 2025; Zeng et al., 2025b), and even mobile communication methods such as MobileWorld (Kong et al., 2025). Several studies (Hu et al., 2025) have further provided systematic categorizations and summaries of Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks advocates for shift from task-specific adaptations to comprehensive system design. Specifically, the main contents and contributions of this work are organized as follows: We provide detailed review of recent progress in World Models, categorizing existing approaches into reasoning, content generation, and interactive agents. We examine how these fields currently incorporate world knowledge to enhance performance. We critically analyze the shortcomings of current methods that rely on injecting knowledge into isolated tasks. Through case studies in LLMs, video generation, and embodied AI, we demonstrate that these approaches often fail to achieve genuine physical understanding and long-term consistency. We propose unified and standardized World Model Framework. We define the essential components, including Interaction, Reasoning, Memory, Environment, and Multimodal Generation, and articulate how they should be integrally designed to support robust world simulation. We identify critical directions for future breakthroughs, such as physically-grounded spatiotemporal representation, embodied interaction control, and autonomous modular evolution, to guide the community toward more general and principled models. 2. Background Understanding world knowledge is crucial for enhancing the ability of artificial intelligence systems to handle complex physical environments. Existing research can be broadly categorized into three classes based on the proactivity of their interaction with the environment and their approach to knowledge integration. Although these methods have made progress in their respective fields, they collectively highlight an urgent need for unified and proactive world modeling framework in current research. 2.1. Reasoning with World Knowledge First, since Large Language Models and Vision-Language Models (LLM/VLM) have demonstrated powerful reasoning and generalization capabilities, some studies have built upon this foundation to further enhance models reasoning abilities concerning complex physical worlds and challenging logical concepts. This category of work primarily includes: general multimodal reasoning represented by OpenAI O3 (Wang et al., 2025a; Bai et al., 2025b; Liang et al., 2025), research related to spatial reasoning (Yang et al., 2025c; Chen et al., 2024a), reasoning for challenging competition problems (Chai et al., 2025; Qiu et al., 2025), and Figure 1. Comparison between current task-specific paradigms and the proposed unified framework. While current research often reduces World Models to the injection of knowledge into specific tasks, holistic World Model aims to endow AI with general capabilities to tackle multifaceted real-world challenges. these world-knowledge-infused generation methods, detailing their achieved capabilities. However, these existing methods of injecting world knowledge into tasks still rely on fine-tuning models with humancurated, task-specific data. Even the most frontier and widely-discussed research at present remains this same pattern (OpenAI, 2024; Tongyi, 2025; Sun et al., 2025; Russell et al., 2025). While this can improve performance on particular tasks, it does not break away from the inherent paradigm of the downstream tasks. Consequently, such approaches remain incapable of actively exploring, discovering, and responding to complex world environments, deviating from the original research objective of world models. The fundamental goal of world model is to enable large models and agents to enhance their understanding of the complex world through active interaction with it, thereby making more accurate analyses and responses. Overemphasis on aligning the outputs of specific tasks with world rules may impede the development of world models. To address these challenges and steer research toward more holistic understanding of the physical world, this paper Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks Figure 2. Illustration of the advocated unified world model framework. Each component served as: (a) Interaction: Enabling the model to handle multi-format inputs from the complex physical world. (b) Reasoning: Conducting logical analysis and inference derived from complex inputs. (c) Memory: Supporting long-term retention and extensive context processing. (d) Multimodal Generation: Empowering the model to generate multimodal outputs, which serve both as environmental feedback and as catalyst for superior reasoning. reasoning with multimodal inputs such as audio, 3D, and long videos (Tian et al., 2025; Xie et al., 2025b;a; Liu et al., 2025a; Shi et al., 2025a; Huang et al., 2025a; Shi et al., 2025c; Wiedemer et al., 2025; Lu et al., 2025; Chen et al., 2025b; An et al., 2024; Lin et al., 2025b; Guo et al., 2025). Meanwhile, with the advancement of reasoning capabilities in large models and agents, some methods (Park et al., 2023; Tan et al., 2025) have further strengthened interactive capabilities, enabling agents to perform long-term memory and interaction within complex virtual environments. However, despite the already formidable reasoning power of large models, they still face significant challenges in achieving accurate perception of the complex physical world, generating output representations across more modalities, and interacting with the real physical world. 2.2. World-Driven Content Generation In addition to enhancing large language models based on text token prediction by incorporating world knowledge, generative methods in other modalities also actively integrate world knowledge. The earliest attempts to introduce world knowledge into visual generation focused on navigation and abstract reasoning tasks (Bar et al., 2025), where researchers evaluated the generated image sequences or videos to assess the models accurate cognition of complex spatiotemporal relationships. With the advancement of diffusion models (Liu et al., 2022; Lipman et al., 2022; Labs, 2024; Peebles & Xie, 2023; Li et al., 2024; Shi et al., 2025b; Wang et al., 2025b; Tong et al., 2026; An et al., 2025), the quality of image, video generation and editing (Liu et al., 2025b; DeepMind, 2025; OpenAI, 2025; 2024; Tongyi, 2025; Gao et al., 2025; Zhang et al., 2025c; Wan et al., 2025) has significantly improved. To make the outputs more realistic and reliable, researchers employ techniques such as fine-tuning and reinforcement learning to guide generative models to better adhere to the physical laws of the real world (Li et al., 2025a; Tang et al., 2025a; Team et al., 2025b; Zeng et al., 2024a;b; Yang et al., 2025b; 2024; Tang et al., 2025b; Sun et al., 2025; He et al., 2025; Zhang et al., 2025b), aiming to build high-quality world simulators. However, this pixelestimation-based approach, although richer in information than text token prediction, essentially learns mapping from the 3D world to 2D rendered results. Even when the generation quality is high, results often violate common sense in details and spatio-temporal logic. Therefore, existing diffusion-based generators do not yet possess precise understanding of the spatio-temporal relationships in complex physical worlds. 2.3. Agents in Interactive Environments To realize the practical value of world models, research on agent exploration and task execution in autonomous driving, embodied intelligence, and simulated environments is crucial. This line of work aims to integrate world knowledge into the agents perception-decision loop to achieve more autonomous and physically plausible interactions. For 3 Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks instance, vision-language-action models in robotics (Black et al.; Bu et al., 2025; Agarwal et al., 2025; Team et al., 2025a), autonomous driving systems designed for complex decision-making and planning (Hu et al., 2023; Tu et al., 2025; Zeng et al., 2025b; Russell et al., 2025), and research on training agents to accomplish open-ended tasks in virtual environments (Wang et al., 2024; Zang et al., 2025) like Minecraft all require models to deeply understand environmental dynamics and perform planning. Although generalist agent frameworks (Black et al.; Bu et al., 2025; Team et al., 2025a) have demonstrated the potential to handle multimodal and multitask scenarios, current vision-languageaction systems still face limitations in long-term memory, multimodal perception in complex environments, and intricate cross-modal behavioral interactions. This underscores the urgency of co-designed integration of interaction, perception, reasoning, and memory, which forms core argument for our advocacy of unified world model framework. 3. Unified World Model Framework To address the fragmentation in current research and facilitate the development of more robust systems, this section outlines the essential components of normative world model framework. As illustrated in Fig. 2, the proposed unified framework comprises the following elements. The original World Models (Ha & Schmidhuber, 2018) primarily consisted of vision model that receives world inputs, memory model for dynamic prediction and processing, and controller that governs the models outputs. This established an effective foundational architecture for the world model framework. However, with advancements in fields such as LLMs/VLMs, diffusion models, and VLAs, this basic framework requires further expansion and refinement. Interaction. The fundamental value of world model lies in its ability to engage in bidirectional, multimodal interactions with complex environments and users. Consequently, its interaction module should evolve beyond the early frameworks vision model, and advance into unified perceptual and operational interface. As shown in Fig. 2(a), This interface requires two core capabilities: first, generalized perception, enabling the understanding and processing of multimodal inputs such as text, images, video, audio, 3D point clouds, and meshes to form unified representation of the world state; second, generalized operation, allowing the parsing and execution of diverse task instructions. These instructions include not only natural language or embodied interaction commands from users, such as movement, rotation, or dragging, but also low-level motion control signals for agents like robots or vehicles. To achieve efficient and reliable closed-loop interaction, the world models interaction module must unify the scheduling, encoding, and organization of these heterogeneous perceptual data and operational signals, providing structured input for subsequent reasoning, memory, and generation. Reasoning. To navigate the complex and dynamic nature of the real world, world model necessitates core component dedicated to reasoning about intricate dynamics and causality. Currently, LLMs/VLMs integrated with Explicit Reasoning have demonstrated remarkable analytical capabilities. mainstream and effective strategy is to employ them within world model, as illustrated in Fig. 2(b). Explicit Reasoning transforms multimodal observations and interactive information into textual descriptions or reasoning chains, leveraging the powerful symbolic reasoning and planning abilities of LLMs to infer physical laws, predict future states, or formulate high-level strategies. This textmediated reasoning offers high transparency and is relatively easy to align and verify with human intuition. For scenarios requiring the handling of sub-symbolic and continuous physical details, Explicit Reasoning may lead to information loss, making the introduction of Latent Reasoning more appropriate. This approach would enable reasoning directly within unified latent space, jointly leveraging encoded multimodal information from vision, language, action, etc. Regardless of the algorithmic approach, the reasoning module of world model should fundamentally possess the capability to perform rational inference on inputs, generating more structured and coherent content. Memory. To maintain coherence and consistency in complex, continuous physical tasks, world model must possess robust long-term memory capabilities. Memory mechanisms have evolved from implicit state storage based on recurrent networks like LSTMs (Hochreiter & Schmidhuber, 1997; Beck et al., 2024) to explicit large-scale memory utilizing the Transformer architecture with long-context windows (Beltagy et al., 2020; Dao et al., 2022; Dao, 2023; Ji et al., 2025). As illustrated in Fig. 2(c), Faced with multimodal, high-concurrency interaction streams in an open world, the memory module of world model must transcend simple sequential storage and achieve structured and dynamic management of information. This requires the system to effectively categorize, associate, and fuse experiential data from different modalities and sources, thereby constructing unified and queryable internal knowledge system. Simultaneously, constrained by computational resources, the memory system must possess the capability for key information extraction and compression (Yang et al., 2025c), actively filtering and retaining states and events core to the task. Furthermore, memory should be dynamically evolving process, as interactions progress, the system must continuously merge, update, and purge redundant stored content to ensure its timeliness and conciseness. 4 Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks Figure 3. Failure cases of various task-specific methods infused with world knowledge. Environment. The training and validation of world model are inseparable from an interactive and controllable environmental carrier. We posit that the environment should encompass both the complex physical world and simulation environments, while simultaneously serving as an integral part of the world model that receives and updates based on outputs from other components, as illustrated on the left side of Fig. 2. Traditionally, high-cost interaction with physical hardware (Black et al.; Hu et al., 2023) is the ultimate goal for world models to engage with the real world, however, acquiring training data at scale remains significant challenge. Simulation environments (Kolve et al., 2017; Li et al., 2022; Zhang et al., 2025a) have served as the cornerstone of early-stage research by providing controllable, safe, and efficient physical or rule-based simulation. However, most simulation environments rely on manually modeled limited scenes and rigid-body dynamics, creating sim-to-real gap in terms of authenticity and diversity. We advocate that the environmental architecture for world models should possess generative and extensible capabilities. Specifically, techniques such as 3D generation methods (Li et al., 2025c; Yu et al., 2025a) and procedural content generation should be leveraged to dynamically synthesize near-infinite, high-fidelity virtual scenes. Such generative environment should function not merely as scene renderer but as physically consistent simulator capable of responding to complex interactions and producing dynamic changes conforming to real-world laws. This would enable world models to be trained on an extremely rich and realistic distribution of environments, enhancing their generalization Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks and adaptation capabilities for open, unknown real-world scenarios. Multimodal Generation. While accepting complex inputs and performing reasoning, the world model must also possess multimodal generation capabilities to provide comprehensive feedback on complex environmental changes. This is crucial capability and vital means of verifying the accuracy of its world understanding and achieving intuitive alignment with humans. As shown in Fig. 2(d), complete multimodal generation capability for world model should extend beyond generating textual reports, it must be able to generate realistic video, images, audio, and even 3D geometry based on its internal states and future predictions. For instance, in an embodied navigation task, after receiving instructions and initial observations, the model should be able to synthesize 3D scene from the agents perspective based on 3D representation, such as point clouds. This constitutes an internal simulation of its own navigation strategy and scene comprehension. Multimodal generation should not be an isolated output module but should form closed loop with the reasoning and memory modules. Generated scenes can provide model-based foresight for planning, and generated data can be utilized for self-augmentation, continuously refining and enriching the models world knowledge. 4. Limitations of Existing Models"
        },
        {
            "title": "Incorporating World Knowledge",
            "content": "This section analyzes the limitations inherent in current approaches across different domains, substantiating the need for the integrated framework proposed above. For the most widely applied Large Language Models (LLMs) and Vision-Language Models (VLMs), although these models appear to possess extensive world knowledge, they fundamentally rely on statistical fitting of large-scale training data. This limitation becomes evident in complex academic reasoning, such as failing to accurately recognize chemical formulas in Chemistry Olympiad problems, and in counter-intuitive multimodal recognition. As shown in Fig.3(a), when an unnatural image depicting six fingers is input into large model, it may still assert that there are only five fingers in the picture. This indicates that large models are heavily influenced by large-scale training data and struggle to discern irregular or unnatural scenarios. These shortcomings of LLMs and VLMs suggest lack of effective perception of real-world complexity and genuine understanding of physical laws. We argue that accurately representing multimodal inputs within spatial and physical framework would significantly enhance the models comprehension of the world. Regarding image generation and editing, early methods like AnyEdit (Yu et al., 2025b) and EditWorld (Zeng et al., 2025a) primarily focused on curating task-specific datasets enriched with world knowledge to improve editing performance. However, training diffusion models directly on such data often fails to handle complex, logic-heavy instructions. Conversely, frameworks that integrate VLMs with diffusion processes have demonstrated superior representational capabilities compared to data-centric methods. This reinforces our argument that architectural advancement is more promising than mere data injection. Current editing methods still lack effective interaction with the physical world and spatiotemporal understanding. As shown in Fig.3(b), although the model successfully completes the editing task, the results do not conform to real-world lighting and shadow patterns. This indicates that possessing only logical reasoning and image generation capabilities is insufficient to produce images that align with real-world dynamics. Effectively capturing the complex, rule-based changes of the physical world remains crucial for models. In summary, developing comprehensive world model framework represents viable strategy for advancing image generation and editing. In video generation, navigation video synthesis is frequently cited as key capability of world models (Li et al., 2025a; Zhang et al., 2025b; Zhu et al., 2025; Bahmani et al., 2025; Ding et al., 2025; Wan et al., 2025). Although these models aim to function as world simulators, they often struggle with long-term memory management. As illustrated in Fig.3(c), when moving left for certain distance and then returning to the right, the objects originally present in the scene noticeably disappear, which clearly violates physical laws, this indicates that these models are merely focused on next-frame prediction in video generation, lacking effective long-term memory and real-world understanding capabilities. Furthermore, we demonstrate the performance of existing state-ofthe-art generative models in Fig. 3(d); despite their high visual quality, their outputs fail to align with real-world principles when synthesizing complex, high-speed dynamic videos. These approaches continue to fit pixel-level patterns rather than internalizing the underlying laws of the world, leading to physical inconsistencies over time. Current 3D generation methods suffer from inadequate dynamics and scalability. The resulting 3D outputs often achieve only visual plausibility without possessing genuine physical significance or interactive properties. Moreover, constrained by computational limits, directly generated 3D spaces are frequently limited in scale, leading to fragmented environments. As illustrated in Fig.3(e), although the overall quality of the 3D scene generated by existing methods appears high, details exhibit noticeable fragmentation and distortion due to the limited representational capacity of 3D point clouds (Chen et al., 2025a; Huang et al., 2025b). This further demonstrates that current 3D generation approaches remain at the level of visual alignment and struggle to handle complex 3D spaces. Merely improving 6 Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks fail to handle relatively straightforward road conditions, highlighting the considerable gap that remains before such systems can adeptly navigate complex real-world environments. Similarly, Fig. 4(c) presents robot that, despite being capable of imitating human movements with reasonable fidelity, inadvertently harms human due to its inability to deviate from pre-programmed actions. These examples collectively demonstrate that merely integrating existing models with embodied systems only enables the execution of basic, pre-defined tasks. We posit that autonomous driving and embodied agents should serve as the carriers for world model to explore the environment, while high-quality control should be an emergent capability of the model itself. Simply coupling large models with physical hardware (Li et al., 2025b) to improve task success rates deviates from the original objective of world models: to create agents capable of active exploration, discovery, and response to complex environments. 5. Discussion: Standardization and Feasibility The proposal for unified world model framework invites discussion regarding feasibility and the trade-offs with taskspecific optimization. Efficiency vs. Generalization. prevailing perspective is that fine-tuning specialized models for specific tasks (e.g., robotic grasping) yields optimal performance with clear engineering paths. Indeed, unified frameworks may incur higher training costs and complexity compared to highly optimized, task-specific systems. However, this view focuses on static performance metrics. From the perspective of dynamic interaction in open-ended environments, taskspecific models often hit performance ceiling defined by their training data. unified framework offers the structural foundation for knowledge transfer between tasks and lifelong learning, which are essential for general world understanding. Diversity vs. Integration. Another consideration is whether unified framework might stifle technological diversity. It can be argued that sub-problems like perception and reasoning are distinct and require specialized architectures. However, the unification proposed here does not imply rigid, monolithic network. Instead, it advocates modular functional specifications and standardized interfaces. By defining how core components (interaction, memory, reasoning) collaborate, standardized framework can facilitate the integration and benchmarking of diverse research efforts. This approach aims to redirect focus from redundant low-level developments to high-level system optimization, potentially accelerating the fields overall advancement. Figure 4. Illustration of the limitations of existing embodied AI and autonomous driving systems. Images sourced from internet search. memory strategies is still insufficient for capturing the laws of the real physical world. Therefore, by holistically enhancing the memory, multimodal generation, and reasoning components within world model, 3D synthesis could transcend current spatial limitations and better align with the evolutionary principles of the complex world. Finally, for autonomous driving and embodied AI, while the integration of world knowledge has yielded performance gains, these methods remain confined to narrow, task-specific domains. They often lack deep understanding of complex, long-horizon multimodal contexts. As shown in Fig. 4(a), current mainstream embodied AI research typically combines robotic arms with recognition and reasoning models to accomplish simple planning tasks. However, these tasks remain relatively basic and fail to evaluate the models capabilities in real-world complex scenarios. Meanwhile, although some efforts have deployed autonomous driving and embodied intelligence in practical applications, these achievements exhibit notable instabilities. Fig. 4(b) illustrates cases where autonomous vehicles 7 Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks 6. Future Work After analyzing the current research field of World Models and proposed unified normative framework, this section explores several critical directions that are essential for future breakthroughs in the field. Physically-Grounded Spatiotemporal Representation. Precise perception and reconstruction of temporal-spatial environment serve as the cornerstone for reasoning and generation within World Models. However, existing 3D and 4D representation techniques still face formidable challenges. While methods such as 3D Mesh, NeRF (Mildenhall et al., 2021), 3D Gaussian Splatting (Kerbl et al., 2023), and 4D representation models (Wu et al., 2024; Yang et al., 2023) have made significant strides in fitting visual appearances, enabling the synthesis of photorealistic objects or scenes, they remain essentially optical representations. They lack an intrinsic expression of real-world physical properties, such as mass, friction, elasticity, and collision volume. Furthermore, current representations struggle to support free exploration and interaction under low computational overhead. For instance, 3DGS often relies on massive point clouds to force-fit visual effects, such unstructured, discrete representations are difficult to map onto physically consistent entities, leading to logical fallacies when the model handles object deformation, fluid dynamics, or complex contacts. Consequently, future research must transcend mere appearance reconstruction and pivot toward physically-grounded representation. We need to explore novel data structures or neural implicit representations that embed physical attributes while maintaining high-fidelity visuals, significantly reducing the computational cost of rendering and interaction. This will provide World Models with spatiotemporal representation that is both freely explorable and strictly compliant with physical laws. Embodied Interaction and Control. Embodied AI serves as the ideal vehicle for World Models to explore and validate their understanding of the real world. The current bottleneck lies in the difficulty of directly transferring policies generated by World Models to physical robots, limitation rooted in the operational flexibility, sensing precision, and physical plausibility of current embodied systems. Future development should focus on enhancing the control capabilities of World Models within complex, dynamic environments. First, models must adapt to robot morphologies with higher Degrees of Freedom (DoF), extending from simple grasping tasks to fine-grained dexterous manipulation. Second, the Sim-to-Real Gap must be bridged, enabling World Models generate action sequences that respect hardware constraints, such as torque limits and joint singularities, thereby allowing embodied agents to effectively navigate diverse real-world scenarios. Furthermore, World Models should be endowed with long-horizon planning capabilities, allowing them to comprehend the causal logic of tasks and command embodied agents to complete multi-stage, complex missions in unstructured environments. Ultimately, after understanding the world, the model should be able to perform sophisticated tasks in the real-world through physical robotic platforms. Autonomous Reflection and Modular Continuous Evolution. Beyond enhancing the capacity for external exploration, improvements to the the World Model itself are equally crucial. Current systems rely heavily on offline large-scale training and lack mechanisms for active error correction or self-updating post-deployment. Future research should strive to empower World Models with metacognition and self-reflection. Specifically, models should possess the ability for uncertainty estimation regarding their own predictions. When significant discrepancy arises between prediction and actual observation, the model should autonomously trigger reflection mechanism to identify knowledge gaps. It should then spontaneously perform targeted fine-tuning by collecting specific data or replaying high-value samples, rather than passively awaiting full retraining cycle. Although current Reinforcement Learning methods assist in proactive thinking, they remain tethered to human-defined reward functions. Thus, achieving autonomous exploration within the World Model is essential. Simultaneously, to meet evolving task requirements, World Models must feature efficient and flexible modular iteration. Modules for perception, memory, reasoning, and planning should support independent fine-tuning and upgrades. This design allows researchers to iteratively improve specific weaknesses (e.g., upgrading the physical reasoning module without impairing the World Models other capabilities), thereby achieving lifelong learning and agile evolution of the entire system. 7. Conclusion In this paper, we have analyzed the current state of world model research, noting prevalence of task-specific integrations. While valuable, these approaches often lack the systemic coherence necessary for general world understanding. We proposed Unified World Model Framework that integrates interaction, perception, reasoning, memory, and generation into normative design. By discussing the limitations of existing methods and the trade-offs of standardization, we highlight the potential of this framework to foster more robust and principled research. We hope this work serves as guideline for future endeavors in physically-grounded representation, embodied control, and autonomous evolution, ultimately advancing agents capable of active and intelligent interaction with the complex world. 8 Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
        },
        {
            "title": "Impact Statement",
            "content": "This paper advocates for unified framework in world model research to enhance reproducibility and robustness. While the proposed theoretical framework poses no direct societal harm, we acknowledge the risks associated with advanced world models, including the generation of misleading content and safety issues in embodied agents. We emphasize the importance of embedding ethical considerations and safety-by-design principles in the development of these systems to ensure beneficial and secure outcomes."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Agarwal, N., Ali, A., Bala, M., Balaji, Y., Barker, E., Cai, T., Chattopadhyay, P., Chen, Y., Cui, Y., Ding, Y., et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. An, R., Yang, S., Lu, M., Zhang, R., Zeng, K., Luo, Y., Cao, J., Liang, H., Chen, Y., She, Q., et al. Mc-llava: Multi-concept personalized vision-language model. arXiv preprint arXiv:2411.11706, 2024. An, R., Yang, S., Zhang, R., Shen, Z., Lu, M., Dai, G., Liang, H., Guo, Z., Yan, S., Luo, Y., et al. Unictokens: Boosting personalized understanding and generation via unified concept tokens. arXiv preprint arXiv:2505.14671, 2025. Bahmani, S., Shen, T., Ren, J., Huang, J., Jiang, Y., Turki, H., Tagliasacchi, A., Lindell, D. B., Gojcic, Z., Fidler, S., et al. Lyra: Generative 3d scene reconstruction via video diffusion model self-distillation. arXiv preprint arXiv:2509.19296, 2025. Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025a. Bai, T., Hu, Z., Sun, F., Qiu, J., Jiang, Y., He, G., Zeng, B., He, C., Yuan, B., and Zhang, W. Multi-step visual reasoning with visual tokens scaling and verification. arXiv preprint arXiv:2506.07235, 2025b. Bar, A., Zhou, G., Tran, D., Darrell, T., and LeCun, Y. Navigation world models. In CVPR, 2025. Beck, M., Poppel, K., Spanring, M., Auer, A., Prudnikova, O., Kopp, M., Klambauer, G., Brandstetter, J., and Hochreiter, S. xlstm: Extended long short-term memory. Advances in Neural Information Processing Systems, 2024. Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Black, K., Brown, N., Driess, D., Esmail, A., Equi, M., Finn, C., Fusai, N., Groom, L., Hausman, K., Ichter, B., et al. Ï€0: vision-language-action flow model for general robot control. corr, abs/2410.24164, 2024. doi: 10.48550. arXiv preprint ARXIV.2410.24164. Bu, Q., Cai, J., Chen, L., Cui, X., Ding, Y., Feng, S., Gao, S., He, X., Hu, X., Huang, X., et al. Agibot world colosseo: large-scale manipulation platform for scalable and intelligent embodied systems. arXiv preprint arXiv:2503.06669, 2025. Chai, J., Tang, S., Ye, R., Du, Y., Zhu, X., Zhou, M., Wang, Y., Zhang, Y., Zhang, L., Chen, S., et al. Scimaster: Towards general-purpose scientific ai agents, part i. xmaster as foundation: Can we lead on humanitys last exam? arXiv preprint arXiv:2507.05241, 2025. Chen, B., Xu, Z., Kirmani, S., Ichter, B., Sadigh, D., Guibas, L., and Xia, F. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1445514465, 2024a. Chen, X., Chu, F.-J., Gleize, P., Liang, K. J., Sax, A., Tang, H., Wang, W., Guo, M., Hardin, T., Li, X., et al. arXiv preprint Sam 3d: 3dfy anything in images. arXiv:2511.16624, 2025a. Chen, X., Zhang, Y., Guan, Y., Zeng, B., Shi, Y., Yang, S., Wan, P., Liu, Q., Wang, L., and Tan, T. Versavid-r1: versatile video understanding and reasoning model from question answering to captioning tasks. arXiv preprint arXiv:2506.09079, 2025b. Chen, X., Zhang, Z., Zhang, H., Zhou, Y., Kim, S. Y., Liu, Q., Li, Y., Zhang, J., Zhao, N., Wang, Y., et al. Unireal: Universal image generation and editing via learning realworld dynamics. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1250112511, 2025c. Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S., Zhong, M., Zhang, Q., Zhu, X., Lu, L., et al. Internvl: Scaling up vision foundation models and aligning for In Proceedings of the generic visual-linguistic tasks. IEEE/CVF conference on computer vision and pattern recognition, pp. 2418524198, 2024b. 9 Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks Cheng, H. K., Ishii, M., Hayakawa, A., Shibuya, T., Schwing, A., and Mitsufuji, Y. Mmaudio: Taming multimodal joint training for high-quality video-to-audio synthesis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2890128911, 2025. Dao, T. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Dao, T., Fu, D., Ermon, S., Rudra, A., and Re, C. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in neural information processing systems, 2022. DeepMind, G. Veo 3, 2025. URL https://deepmind. google/technologies/veo. Ding, Y., Liu, J., Zhang, W., Wang, Z., Hu, W., Cui, L., Lao, M., Shao, Y., Liu, H., Li, X., et al. Klingavatar: Grounding multimodal instructions for cascaded long-duration avatar animation synthesis. arXiv preprint arXiv:2509.09595, 2025. Gao, Y., Guo, H., Hoang, T., Huang, W., Jiang, L., Kong, F., Li, H., Li, J., Li, L., Li, X., et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. Guo, Z., Chen, X., Zhang, R., An, R., Qi, Y., Jiang, D., Li, X., Zhang, M., Li, H., and Heng, P.-A. Are video models ready as zero-shot reasoners? an empirical study with the mme-cof benchmark. arXiv preprint arXiv:2510.26802, 2025. Ha, D. and Schmidhuber, J. World models. arXiv preprint arXiv:1803.10122, 2(3), 2018. He, X., Peng, C., Liu, Z., Wang, B., Zhang, Y., Cui, Q., Kang, F., Jiang, B., An, M., Ren, Y., et al. Matrix-game 2.0: An open-source real-time and streaming interactive world model. arXiv preprint arXiv:2508.13009, 2025. Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural computation, 1997. Hu, A., Russell, L., Yeo, H., Murez, Z., Fedoseev, G., Kendall, A., Shotton, J., and Corrado, G. Gaia-1: generative world model for autonomous driving. arXiv preprint arXiv:2309.17080, 2023. Hu, Y., Wang, L., Liu, X., Chen, L.-H., Guo, Y., Shi, Y., Liu, C., Rao, A., Wang, Z., and Xiong, H. Simulating the real world: unified survey of multimodal generative models. arXiv preprint arXiv:2503.04641, 2025. Huang, J., Li, Z., Zhang, H., Chen, R., He, X., Guo, Y., Wang, W., Liu, T., and Gong, M. Surprise3d: dataset for spatial understanding and reasoning in complex 3d scenes. arXiv preprint arXiv:2507.07781, 2025a. Huang, Z., Guo, Y.-C., An, X., Yang, Y., Li, Y., Zou, Z.-X., Liang, D., Liu, X., Cao, Y.-P., and Sheng, L. Midi: Multiinstance diffusion for single image to 3d scene generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2364623657, 2025b. Ji, S., Chen, X., Yang, S., Tao, X., Wan, P., and Zhao, H. Memflow: Flowing adaptive memory for consistent and efficient long video narratives. arXiv preprint arXiv:2512.14699, 2025. Kerbl, B., Kopanas, G., Leimkuhler, T., and Drettakis, G. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 2023. Kolve, E., Mottaghi, R., Han, W., VanderBilt, E., Weihs, L., Herrasti, A., Deitke, M., Ehsani, K., Gordon, D., Zhu, Y., et al. Ai2-thor: An interactive 3d environment for visual ai. arXiv preprint arXiv:1712.05474, 2017. Kong, Q., Zhang, X., Yang, Z., Gao, N., Liu, C., Tong, P., Cai, C., Zhou, H., Zhang, J., Chen, L., Liu, Z., Hoi, S., and Wang, Y. Mobileworld: Benchmarking autonomous mobile agents in agent-user interactive, and mcp-augmented environments. arXiv preprint arXiv:2512.19432, 2025. Labs, B. F. Flux, 2024. URL https://github.com/ black-forest-labs/flux. Li, J., Tang, J., Xu, Z., Wu, L., Zhou, Y., Shao, S., Yu, T., Cao, Z., and Lu, Q. Hunyuan-gamecraft: High-dynamic interactive game video generation with hybrid history condition. arXiv preprint arXiv:2506.17201, 2025a. Li, P., An, Z., Abrar, S., and Zhou, L. Large language models for multi-robot systems: survey. arXiv preprint arXiv:2502.03814, 2025b. Li, Q., Peng, Z., Feng, L., Zhang, Q., Xue, Z., and Zhou, B. Metadrive: Composing diverse driving scenarios for generalizable reinforcement learning. IEEE transactions on pattern analysis and machine intelligence, 2022. Li, S., Zeng, B., Feng, Y., Gao, S., Liu, X., Liu, J., Li, L., Tang, X., Hu, Y., Liu, J., et al. Zone: Zero-shot instruction-guided local editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 62546263, 2024. Li, X., Wang, T., Gu, Z., Zhang, S., Guo, C., and Cao, L. Flashworld: High-quality 3d scene generation within seconds. arXiv preprint arXiv:2510.13678, 2025c. Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks Liang, H., Wu, R., Zeng, B., Niu, J., Zhang, W., and Dong, B. Multimodal reasoning for science: Technical report and 1st place solution to the icml 2025 seephys challenge. arXiv preprint arXiv:2509.06079, 2025. Lin, B., Li, Z., Cheng, X., Niu, Y., Ye, Y., He, X., Yuan, S., Yu, W., Wang, S., Ge, Y., et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025a. Lin, W., Wei, X., An, R., Ren, T., Chen, T., Zhang, R., Guo, Z., Zhang, W., Zhang, L., and Li, H. Perceive anything: Recognize, explain, caption, and segment anything in images and videos. arXiv preprint arXiv:2506.05302, 2025b. Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Liu, H., Luo, K., Wang, J., Wang, W., Chen, Q., Zhao, Z., and Xue, W. Thinksound: Chain-of-thought reasoning in multimodal large language models for audio generation and editing. arXiv preprint arXiv:2506.21448, 2025a. Liu, K., Li, W., Chen, L., Wu, S., Zheng, Y., Ji, J., Zhou, F., Jiang, R., Luo, J., Fei, H., et al. Javisdit: Joint audio-video diffusion transformer with hierarchical spatio-temporal prior synchronization. arXiv preprint arXiv:2503.23377, 2025b. Liu, X., Gong, C., and Liu, Q. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Liu, Y., Min, Z., Wang, Z., Wu, J., Wang, T., Yuan, Y., Luo, Y., and Guo, C. Worldmirror: Universal 3d world reconstruction with any-prior prompting. arXiv preprint arXiv:2510.10726, 2025c. Lu, D., Liang, A., Huang, T., Fu, X., Zhao, Y., Ma, B., Pan, L., Yin, W., Kong, L., Ooi, W. T., et al. See4d: Pose-free 4d generation via auto-regressive video inpainting. arXiv preprint arXiv:2510.26796, 2025. OpenAI. Sora, 2024. URL https://openai.com/ sora. OpenAI. Sora 2: Video generation model, 2025. URL https://openai.com/sora. Park, J. S., OBrien, J. C., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442, 2023. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. Qiu, J., Shi, J., Juan, X., Zhao, Z., Geng, J., Liu, S., Wang, H., Wu, S., and Wang, M. Physics supernova: Ai agent matches elite gold medalists at ipho 2025. arXiv preprint arXiv:2509.01659, 2025. Russell, L., Hu, A., Bertoni, L., Fedoseev, G., Shotton, J., Arani, E., and Corrado, G. Gaia-2: controllable multiview generative world model for autonomous driving. arXiv preprint arXiv:2503.20523, 2025. Shi, B., Tjandra, A., Hoffman, J., Wang, H., Wu, Y.-C., Gao, L., Richter, J., Le, M., Vyas, A., Chen, S., et al. Sam audio: Segment anything in audio. arXiv preprint arXiv:2512.18099, 2025a. Shi, M., Wang, H., Zhang, B., Zheng, W., Zeng, B., Yuan, Z., Wu, X., Zhang, Y., Yang, H., Wang, X., et al. Svg-t2i: Scaling up text-to-image latent diffusion model without variational autoencoder. arXiv preprint arXiv:2512.11749, 2025b. Shi, Y., Liu, J., Guan, Y., Wu, Z., Zhang, Y., Wang, Z., Lin, W., Hua, J., Wang, Z., Chen, X., et al. Mavors: Multi-granularity video representation for multimodal large language model. arXiv preprint arXiv:2504.10068, 2025c. Sun, W., Zhang, H., Wang, H., Wu, J., Wang, Z., Wang, Z., Wang, Y., Zhang, J., Wang, T., and Guo, C. Worldplay: Towards long-term geometric consistency for real-time interactive world modeling. arXiv preprint arXiv:2512.14614, 2025. Lu, H., Liu, W., Zhang, B., Wang, B., Dong, K., Liu, B., Sun, J., Ren, T., Li, Z., Yang, H., et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024. Tan, W., Li, X., Fang, Y., Yao, H., Yan, S., Luo, H., Ao, T., Li, H., Ren, H., Yi, B., et al. Lumine: An open recipe for building generalist agents in 3d open worlds. arXiv preprint arXiv:2511.08892, 2025. Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., and Ng, R. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 2021. Tang, J., Chen, Z., Chen, X., Wang, T., Zeng, G., and Liu, Z. Lgm: Large multi-view gaussian model for highresolution 3d content creation. In European Conference on Computer Vision, pp. 118. Springer, 2024. 11 Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks Tang, J., Liu, J., Li, J., Wu, L., Yang, H., Zhao, P., Gong, S., Yuan, X., Shao, S., and Lu, Q. Hunyuan-gamecraft2: Instruction-following interactive game world model. arXiv preprint arXiv:2511.23429, 2025a. Tang, Y., Guo, Z., Zhu, K., Zhang, R., Chen, Q., Jiang, D., Liu, J., Zeng, B., Song, H., Qu, D., et al. Are we ready for rl in text-to-3d generation? progressive investigation. arXiv preprint arXiv:2512.10949, 2025b. Team, G., Georgiev, P., Lei, V. I., Burnell, R., Bai, L., Gulati, A., Tanzer, G., Vincent, D., Pan, Z., Wang, S., et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Team, G., Ye, A., Wang, B., Ni, C., Huang, G., Zhao, G., Li, H., Li, J., Zhu, J., Feng, L., et al. Gigabrain-0: world model-powered vision-language-action model. arXiv preprint arXiv:2510.19430, 2025a. Team, H., Wang, Z., Liu, Y., Wu, J., Gu, Z., Wang, H., Zuo, X., Huang, T., Li, W., Zhang, S., et al. Hunyuanworld 1.0: Generating immersive, explorable, and interactive 3d worlds from words or pixels. arXiv preprint arXiv:2507.21809, 2025b. Tian, F., Zhang, X. T., Zhang, Y., Zhang, H., Li, Y., Liu, D., Deng, Y., Wu, D., Chen, J., Zhao, L., et al. Step-audio-r1 technical report. arXiv preprint arXiv:2511.15848, 2025. Tochilkin, D., Pankratz, D., Liu, Z., Huang, Z., Letts, A., Li, Y., Liang, D., Laforte, C., Jampani, V., and Cao, Y.- P. Triposr: Fast 3d object reconstruction from single image. arXiv preprint arXiv:2403.02151, 2024. Tong, C., Chang, M., Zhang, S., Wang, Y., Liang, C., Zhao, Z., An, R., Zeng, B., Shi, Y., Dai, Y., et al. Cof-t2i: Video models as pure visual reasoners for text-to-image generation. arXiv preprint arXiv:2601.10061, 2026. Tongyi, A. Wan 2.5: Unified multi-modal video generation framework, 2025. URL https://tongyi.aliyun. com/wan. Tu, S., Zhou, X., Liang, D., Jiang, X., Zhang, Y., Li, X., and Bai, X. The role of world models in shaping autonomous driving: comprehensive survey. arXiv preprint arXiv:2502.10498, 2025. Wan, T., Wang, A., Ai, B., Wen, B., Mao, C., Xie, C.-W., Chen, D., Yu, F., Zhao, H., Yang, J., et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Wang, Y., Chen, Q., Li, Z., Wang, S., Guo, S., Zhang, Z., and Wei, Z. Simple o3: Towards interleaved vision-language reasoning. arXiv preprint arXiv:2508.12109, 2025a. Wang, Y., Zeng, B., Tong, C., Liu, W., Shi, Y., Ma, X., Liang, H., Zhang, Y., and Zhang, W. Scone: Bridging composition and distinction in subject-driven image generation via unified understanding-generation modeling. arXiv preprint arXiv:2512.12675, 2025b. Wang, Z., Cai, S., Mu, Z., Lin, H., Zhang, C., Liu, X., Li, Q., Liu, A., Ma, X. S., and Liang, Y. Omnijarvis: Unified vision-language-action tokenization enables open-world instruction following agents. Advances in Neural Information Processing Systems, 37:7327873308, 2024. Wiedemer, T., Li, Y., Vicol, P., Gu, S. S., Matarese, N., Swersky, K., Kim, B., Jaini, P., and Geirhos, R. Video models are zero-shot learners and reasoners. arXiv preprint arXiv:2509.20328, 2025. Wu, G., Yi, T., Fang, J., Xie, L., Zhang, X., Wei, W., Liu, W., Tian, Q., and Wang, X. 4d gaussian splatting for real-time dynamic scene rendering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2031020320, 2024. Xiang, J., Chen, X., Xu, S., Wang, R., Lv, Z., Deng, Y., Zhu, H., Dong, Y., Zhao, H., Yuan, N. J., et al. Native and compact structured latents for 3d generation. arXiv preprint arXiv:2512.14692, 2025a. Xiang, J., Lv, Z., Xu, S., Deng, Y., Wang, R., Zhang, B., Chen, D., Tong, X., and Yang, J. Structured 3d latents for scalable and versatile 3d generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2146921480, 2025b. Xie, Z., Lin, M., Liu, Z., Wu, P., Yan, S., and Miao, C. Audio-reasoner: Improving reasoning capability in large audio language models. arXiv preprint arXiv:2503.02318, 2025a. Xie, Z., Ma, Z., Liu, Z., Pang, K., Li, H., Zhang, J., Liao, Y., Ye, D., Miao, C., and Yan, S. Mini-omni-reasoner: Token-level thinking-in-speaking in large speech models. arXiv preprint arXiv:2508.15827, 2025b. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. Yang, L., Zhang, Z., Han, J., Zeng, B., Li, R., Torr, P., and Zhang, W. Semantic score distillation sampling for compositional text-to-3d generation. arXiv preprint arXiv:2410.09009, 2024. Yang, L., Zhu, K., Tian, J., Zeng, B., Lin, M., Pei, H., Zhang, W., and Yan, S. Widerange4d: Enabling high-quality 4d reconstruction with wide-range movements and scenes. arXiv preprint arXiv:2503.13435, 2025b. 12 Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks Yang, S., Yang, J., Huang, P., Brown, E., Yang, Z., Yu, Y., Tong, S., Zheng, Z., Xu, Y., Wang, M., et al. Cambrian-s: Towards spatial supersensing in video. arXiv preprint arXiv:2511.04670, 2025c. Zhang, Y., Yang, H., Zhang, Y., Hu, Y., Zhu, F., Lin, C., Mei, X., Jiang, Y., Peng, B., and Yuan, Z. Waver: Wave your way to lifelike video generation. arXiv preprint arXiv:2508.15761, 2025c. Yang, Z., Yang, H., Pan, Z., and Zhang, L. Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting. arXiv preprint arXiv:2310.10642, 2023. Zhu, Y., Feng, J., Zheng, W., Gao, Y., Tao, X., Wan, P., Zhou, J., and Lu, J. Astra: General interactive world model with autoregressive denoising. arXiv preprint arXiv:2512.08931, 2025. Zhu, Z., Wang, X., Zhao, W., Min, C., Li, B., Deng, N., Dou, M., Wang, Y., Shi, B., Wang, K., et al. Is sora world simulator? comprehensive survey on general world models and beyond. arXiv preprint arXiv:2405.03520, 2024. Yu, H.-X., Duan, H., Herrmann, C., Freeman, W. T., and Wu, J. Wonderworld: Interactive 3d scene generation from single image. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 59165926, 2025a. Yu, Q., Chow, W., Yue, Z., Pan, K., Wu, Y., Wan, X., Li, J., Tang, S., Zhang, H., and Zhuang, Y. Anyedit: Mastering unified high-quality image editing for any idea. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2612526135, 2025b. Zang, H., Wei, M., Xu, S., Wu, Y., Guo, Z., Wang, Y., Lin, H., Shi, L., Xie, Y., Xu, Z., et al. Rlinf-vla: unified and efficient framework for vla+ rl training. arXiv preprint arXiv:2510.06710, 2025. Zeng, B., Li, S., Feng, Y., Yang, L., Zhang, J., Li, H., Liu, J., He, C., Zhang, W., Liu, J., et al. Ipdreamer: Appearancecontrollable 3d object generation with complex image prompts. In The Thirteenth International Conference on Learning Representations, 2024a. Zeng, B., Yang, L., Li, S., Liu, J., Zhang, Z., Tian, J., Zhu, K., Guo, Y., Wang, F.-Y., Xu, M., et al. Trans4d: Realistic geometry-aware transition for compositional text-to-4d synthesis. arXiv preprint arXiv:2410.07155, 2024b. Zeng, B., Yang, L., Liu, J., Xu, M., Zhang, Y., Wan, P., Zhang, W., and Yan, S. Editworld: Simulating world dynamics for instruction-following image editing. In Proceedings of the 33rd ACM International Conference on Multimedia, pp. 1267412681, 2025a. Zeng, K., Wu, Z., Xiong, K., Wei, X., Guo, X., Zhu, Z., Ho, K., Zhou, L., Zeng, B., Lu, M., et al. Rethinking driving world model as synthetic data generator for perception tasks. arXiv preprint arXiv:2510.19195, 2025b. Zhang, J., Peng, Y., Kong, F., Yang, C., Wu, Y., Yu, Z., Xiang, J., Ruan, J., Wang, J., Song, M., et al. Autoenv: Automated environments for measuring cross-environment agent learning. arXiv preprint arXiv:2511.19304, 2025a. Zhang, Y., Peng, C., Wang, B., Wang, P., Zhu, Q., Kang, F., Jiang, B., Gao, Z., Li, E., Liu, Y., et al. Matrixgame: Interactive world foundation model. arXiv preprint arXiv:2506.18701, 2025b."
        }
    ],
    "affiliations": [
        "HKUST",
        "Kling Team, Kuaishou Technology",
        "Peking University",
        "School of Artificial Intelligence, University of Chinese Academy of Sciences",
        "Shanghai Jiao Tong University",
        "Tsinghua University"
    ]
}