{
    "paper_title": "Set You Straight: Auto-Steering Denoising Trajectories to Sidestep Unwanted Concepts",
    "authors": [
        "Leyang Li",
        "Shilin Lu",
        "Yan Ren",
        "Adams Wai-Kin Kong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Ensuring the ethical deployment of text-to-image models requires effective techniques to prevent the generation of harmful or inappropriate content. While concept erasure methods offer a promising solution, existing finetuning-based approaches suffer from notable limitations. Anchor-free methods risk disrupting sampling trajectories, leading to visual artifacts, while anchor-based methods rely on the heuristic selection of anchor concepts. To overcome these shortcomings, we introduce a finetuning framework, dubbed ANT, which Automatically guides deNoising Trajectories to avoid unwanted concepts. ANT is built on a key insight: reversing the condition direction of classifier-free guidance during mid-to-late denoising stages enables precise content modification without sacrificing early-stage structural integrity. This inspires a trajectory-aware objective that preserves the integrity of the early-stage score function field, which steers samples toward the natural image manifold, without relying on heuristic anchor concept selection. For single-concept erasure, we propose an augmentation-enhanced weight saliency map to precisely identify the critical parameters that most significantly contribute to the unwanted concept, enabling more thorough and efficient erasure. For multi-concept erasure, our objective function offers a versatile plug-and-play solution that significantly boosts performance. Extensive experiments demonstrate that ANT achieves state-of-the-art results in both single and multi-concept erasure, delivering high-quality, safe outputs without compromising the generative fidelity. Code is available at https://github.com/lileyang1210/ANT"
        },
        {
            "title": "Start",
            "content": "Set You Straight: Auto-Steering Denoising Trajectories to Sidestep Unwanted Concepts Leyang Li1, Shilin Lu1, Yan Ren1 Adams Wai-Kin Kong1 1Nanyang Technological University, Singapore {lile0005, shilin002}@e.ntu.edu.sg, nomatterhowlong@gmail.com, adamskong@ntu.edu.sg 5 2 0 2 7 ] . [ 1 2 8 7 2 1 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Ensuring the ethical deployment of text-to-image models requires effective techniques to prevent the generation of harmful or inappropriate content. While concept erasure methods offer promising solution, existing finetuningbased approaches suffer from notable limitations. Anchorfree methods risk disrupting sampling trajectories, leading to visual artifacts, while anchor-based methods rely on the heuristic selection of anchor concepts. To overcome these shortcomings, we introduce finetuning framework, dubbed ANT, which Automatically guides deNoising Trajectories to avoid unwanted concepts. ANT is built on key insight: reversing the condition direction of classifier-free guidance during mid-to-late denoising stages enables precise content modification without sacrificing early-stage structural integrity. This inspires trajectory-aware objective that preserves the integrity of the early-stage score function fieldwhich steers samples toward the natural image manifoldwithout relying on heuristic anchor concept selection. For single-concept erasure, we propose an augmentationenhanced weight saliency map to precisely identify the critical parameters that most significantly contribute to the unwanted concept, enabling more thorough and efficient erasure. For multi-concept erasure, our objective function offers versatile plug-and-play solution that significantly boosts performance. Extensive experiments demonstrate that ANT achieves state-of-the-art results in both single and multi-concept erasure, delivering high-quality, safe outputs without compromising the generative fidelity. Code is available at https://github.com/lileyang1210/ANT. 1. Introduction Concept erasure in text-to-image (T2I) models [5, 11, 60, 67, 69, 71, 86] addresses the critical challenge of preventing the generation of harmful or inappropriate visual content, such as violent, explicit, copyright-infringing, or offensive imagery. Current methods for concept erasure can Equal contribution be broadly categorized into two types: (1) finetuning-based methods [19, 55, 57], which directly modify model parameters, and (2) finetuning-free methods [33, 58, 73], which aim to influence model outputs without parameter updates. However, finetuning-free methods are vulnerable to bypassing when the source code is openly available, thus making finetuning-based methods more effective and secure for publicly accessible models. remove Finetuning-based methods undesirable data modes by altering the predicted score function fieldessentially, modifying the gradient directions that samples follow during the denoising processto avoid converging toward undesirable image distributions. As the predicted score function no result of finetuning, longer accurately reflects the true gradient direction in data space that would further increase likelihood. The main difference among finetuning-based techniques lies in how the conditional score function is modified, which can be broadly divided into anchor-free and anchor-based approaches. Anchor-free methods [3, 7, 8, 10, 19, 21, 24, 28, 30, 32, 35, 3739, 53, 5759, 72, 73, 80, 81, 84, 85, 87, 88, 95, 100] often design loss to adjust the conditional score function throughout the denoising process, encouraging samples to move away from unwanted image manifolds without explicitly specifying target manifold (see Figure 1(b)). However, this approach can disrupt the sampling trajectories toward natural image manifolds. As shown in Figure 1(a), diffusion models typically first guide samples from Gaussian noise toward the manifold of natural images to establish plausible layout, and then progressively refine the details during the mid-to-late denoising steps [41, 55]. By solely emphasizing the movement away from unwanted manifolds, anchor-free methods risk causing samples to deviate from the natural image manifold early on, potentially resulting in generated images with visual artifacts or unintended content (see the second row of Figure 2). Anchor-based methods [24, 6, 9, 15, 16, 20, 23, 26, 31, 36, 40, 43, 48, 49, 52, 55, 62, 74, 78, 82, 89, 90, 99, 102], on the other hand, typically utilize loss designed to leverage benign anchor concepts by aligning the predicted condi1 Figure 1. Geometric perspective on concept erasure in diffusion models. (a) Conventional Denoising Trajectory. high-dimensional Gaussian sample, starting on large sphere, converges to the human data manifold via classifier-free guidance (CFG). (b) Anchor-Free Finetuned Trajectory. Finetuning often modifies the orientation of the predicted conditional score functions so that they direct away from the unwanted concept manifold. This results in condition direction δ(c) = ϵθ(zt, t, c) ϵθ(zt, t) nearly opposite to that of the original model, making the trajectory more likely to produce out-of-distribution samples. Note that, in the absence of an unconditional constraint, modifications to the conditional output also affect the unconditional output due to shared model parameters. (c) Anchor-Based Finetuned Trajectory. The model is finetuned so that the predicted score functions (or keys & values) for the unwanted concept align with those of the original model conditioned on benign anchor, ensuring final samples lie on the anchor manifold, though not necessarily at the highest-probability mode. (d) Our Trajectory (ANT). In the early stage (when > t), the conditional score functions remain directed toward the natural data mode, keeping the finetuned model aligned with the original. When < t, they are finetuned to point away from the unwanted concept manifold. ANT encourages that unconditional score functions remain unchanged throughout all stages. tional score functions (or keys & values) for unwanted concepts with those associated with anchor concepts (see Figure 1(c)). By aligning score functions of unwanted concepts with those of anchor concepts, these methods ensure that samples conditioned on unwanted concepts ultimately converge towards images depicting the anchor concepts. Thus, anchor-based approaches are not merely repelling samples from undesired modes. Nevertheless, the effectiveness of these methods critically depends on the proper selection of anchor concepts. As demonstrated in the third row of Figure 2, some seemingly reasonable anchor concept choices can reduce the quality of images generated when conditioned on erased concepts. Currently, selecting effective anchor concepts remains largely heuristic, lacking systematic guidelines. framework, Motivated by these limitations, we propose trajectorytermed ANT, which aware finetuning Automatically guides deNoising Trajectories to avoid unwanted concepts. This approach achieves its goal without negatively affecting early-stage score function fields or relying on heuristic anchor concept selection. Specifically, we discovered that reversing the condition direction of classifier-free guidance (CFG) [29] during the mid-to-late denoising stage enables modification of detailed content while preserving the fundamental structure of the generated image. This finding inspires us to develop trajectoryaware objective function that preserves the early-stage score function, steering samples toward the natural image manifold, while eliminating the need for anchor concepts (Figure 1(d)). This approach enables more effective erasure of undesired concepts while better preserving those that are unrelated. In the context of single-concept erasure, we introduce an augmentation-enhanced weight saliency map that accurately identifies the key parameters most responsible for generating specific concept. Moreover, our loss function is fully compatible with existing multi-concept erasure frameworks, offering flexible plug-and-play solution, and elevates the performance to new state-of-the-art (SOTA) level. Our experimental results demonstrate that our method achieves SOTA performance in both single and multi-concept erasure settings. Our contributions are summarized as follows: 1. We offer geometric perspective on concept erasure and an insight that reversing the condition direction of classifier-free guidance during the mid-to-late denoising stages enables precise content modification while preserving early-stage structural integrity, thus benefiting the erasure community in advancing algorithm designs. 2. We propose trajectory-aware finetuning framework, which encourages the model to reorient its denoising trajectories during the mid-to-late stages while keeping the early-stage trajectories largely unchanged. This approach enables more thorough erasure of unwanted concepts and better preservation of unrelated ones. 3. We introduce an augmentation-enhanced weight saliency 2 nent concept removal. FMN [99] builds upon this trajectory by proposing lightweight method that manipulates attention mechanisms to enhance computational efficiency. Meanwhile, AC [40] presents finetuning framework that aligns the score function of an unwanted concept with that of an anchor concept, delivering an alternative strategy for concept ablation. As concept erasure techniques have matured, the research community has increasingly emphasized the dual objectives of effectively eliminating target concepts while preserving the integrity of unrelated concepts during the finetuning process. Numerous studies [3, 4, 68, 16, 17, 21, 22, 24, 26, 28, 32, 37, 39, 48, 52, 55, 57, 58, 72, 74, 80 82, 84, 85, 87, 88, 90, 95, 102] highlight the necessity of maintaining balanced model performance across both targeted and non-targeted concepts. However, critical limitation of these approaches lies in their insufficient attention to the impacts of finetuning on the early-stage score function. This oversight can lead to divergence between the predicted score function and the true score function, i.e., the gradient direction in data space that maximizes likelihood. As result, the generated samples may fail to converge toward the natural image manifold, ultimately degrading the quality and reliability of the outputs. Our work seeks to bridge this gap by explicitly addressing the preservation of the early-stage score function, ensuring both effective concept erasure and high-fidelity generation. 3. Method We propose ANT, framework designed to erase specific concepts from pretrained text-to-image diffusion models. Our approach addresses key challenges by eliminating the negative impacts on early-stage score function fields and removing the dependency on heuristic methods for anchor concept selection. The framework requires only two inputs: pretrained diffusion model and set of target phrases representing the concepts to be erased. The output is finetuned model that no longer generates images depicting the unwanted concepts. 3.1. Insights into the Denoising Process We thoroughly investigated the denoising process in diffusion models and found that applying CFG during the early sampling stage (when < < ), and then reversing the CFGs condition direction term during the mid-to-late sampling stage (when 0 < < t, as shown in Eq. (1)), allows for altering detailed content while preserving the fundamental structure of the image. In other words, the sample avoids converging toward specific unwanted concepts yet remains Figure 2. Generation results of different concept erasure methods conditioned on the concept cat. The anchor-free method (ESD) often produces images with visual artifacts or content that is out of distribution. The anchor-based method (MACE), which maps cat to forest, performs reasonably well in simple contexts but results in unnatural or incoherent outputs in more complex scenarios. In contrast, our trajectory-aware method (ANT) effectively removes the target concept while preserving the overall structure and contextual integrity of the generated images. map that precisely identifies the key parameters most responsible for generating the undesired concept, thereby enabling more effective and efficient erasure. 4. The proposed objective function substantially enhances the performance of existing multi-concept erasure frameworks, achieving SOTA results in both singleand multiconcept erasure settings. 2. Related Work In this section, we review prior work on concept erasure in diffusion models, with particular focus on the critical trade-off between erasure and preservation, which is most pertinent to our study. Additional discussions on other dimensions of concept erasure (e.g., finetuning efficiency, scalability, and robustness to adversarial prompts) are provided in Appendix. The investigation of concept erasure within diffusion models has been pioneered by several foundational studies, establishing the groundwork for this burgeoning domain. SLD [73] introduces an inference-time guidance technique to suppress undesired concepts without modifying the models parameters, offering non-invasive yet effective approach. In contrast, ESD [19] employs direct parameter editing through negative guidance, achieving perma3 within the natural image manifold. ϵcfg θ (zt, t, c) = ϵθ(zt, t) + sgn(t t) δ(c), (cid:40) sgn(t t) = 1, 1, if if > (1) (2) where the terms ϵcfg θ (zt, t, c), ϵθ(zt, t, c), and ϵθ(zt, t) denote the classifier-free guidance output, the conditional prediction, and the unconditional prediction, respectively. The difference δ(c) = ϵθ(zt, t, c) ϵθ(zt, t) defines the condition direction. is key parameter used to determine the timestep at which the condition direction should be reversed. As shown in Figure 3(c), if is appropriately selected, this approach allows for the targeted removal of specific attributes or details (e.g., occupation, gender, or age) while preserving the naturalness of the generated images. This is because, during the early stage of denoising, the samples follow the correct score function and are guided onto plausible data manifold. In the later stages, the guidance steers the samples away from certain modes within that manifold. For instance, in Figure 3(c), the occupation changes from doctor to model, gender shifts from male to female, and age transitions from both old and young to middle-aged, all while staying within the human data manifold. However, if is set too early, the early-stage score function will be significantly altered, leading to loss of the images structural integrity (see Figure 3(d)). On the other hand, if is set too late, the samples will have already entered the concept-specific mode, and modifications to the late-stage score function will only affect fine details (see Figure 3(b)). 3.2. Trajectory-Aware Loss Function Inspired by this finding, we aim to preserve the integrity of the early-stage score function fieldwhich guides samples toward the appropriate natural manifoldby introducing constraints during finetuning. Adjustments will be limited exclusively to the mid-to-late stage score function field. This approach ensures that even when the finetuned model is conditioned on the removed concept, the samples can still converge to the appropriate manifold. Specifically, we propose the following finetuning objective: = Lpreserve + λ1 Lerase + λ2 Luncond-early + λ3 Luncond-late = Ezt1 ,c,t1U (t,T ) + λ1Ezt2 ,c,t2U (0,t) + λ2Ezt1 ,c,t1U (t,T ) + λ3Ezt2 ,c,t2U (0,t) (cid:105) (cid:104) ϵθ(zt1, t1, c) sg [ϵθ (zt1, t1) + ηδ(c)]2 (cid:104) ϵθ(zt2 , t2, c) sg [ϵθ (zt2, t2) ηδ(c)]2 (cid:104) ϵθ(zt1, t1) sg [ϵθ (zt1, t1)]2 2 (cid:105) (cid:104) ϵθ(zt2 , t2) sg [ϵθ (zt2, t2)]2 (cid:105) 2 2 , 2 (cid:105) (3) where θ represents the parameters undergoing finetuning, while θ denotes the original, frozen parameters. The no4 Figure 3. Effect of condition direction reversal at different timesteps. Each column represents distinct semantic condition, and each row shows generated outputs under varying reversal strategies. (a) displays originally generated images using diffusion process (timestep 501). (b)(d) show results when the condition direction δ(c) = ϵθ(zt, t, c) ϵθ(zt, t) is reversed at different timesteps (25, 35, and 45). With proper t, specific attributes can be removed while preserving image naturalness. If is too early, structural integrity is lost; if too late, only fine details are affected. tation sg[] indicates the stop-gradient operation. Timesteps t1 and t2 are sampled independently from uniform distributions (t, ) and (0, t), respectively, with being predefined hyperparameter. Additionally, zt1 and zt2 represent the corresponding noisy latent image variables at these timesteps, and η denotes hyperparameter. Notably, two timesteps are sampled during each gradient update iteration to effectively balance the gradients associated with concept erasure and the preservation of unrelated concepts. Early-stage preservation. The first term Lpreserve ensures that, during the early stage (when > t), the predicted conditional score function consistently points toward the natural data mode. This preserves the integrity of the early stage score function field. Consequently, when sampling with the finetuned model conditioned on the erased concept, the generated samples can smoothly transition into the natural image manifold. Mid-to-late-stage erasure. The second term Lerase emphasizes that at later stage (when < t), the predicted conditional score function should actively guide samples away from undesirable modes. It differs from the ESD loss [19] in that the second term is applied exclusively at later timesteps (t < t), whereas the ESD loss spans all timesteps. Including early timesteps in the ESD loss can unintentionally alter the early-stage score function field, frequently causing samples to be incorrectly guided and thereby failing to converge onto the appropriate manifold. To further explore this issue, we conducted an experiment restricting the application of this second loss term solely to mid-to-late denoising steps, specifically aiming to avoid negatively impacting the early-stage score function field. However, even under this restricted condition, the early-stage score function field was still adversely affected, resulting in suboptimal performance (see the ablation study in Table 2). We hypothesize that this outcome arises primarily due to the shared model parameters across all timesteps within the diffusion process. Unconditional score function preservation. Since the unconditional score function ϵθ(zt, t) represents the general direction toward the approximate center of all data modes, modifying it can influence multiple concepts, as demonstrated by our ablation study. Specifically, Table 2 shows that removing 100 celebrity concepts without incorporating unconditional loss terms negatively impacts the preservation of other celebrity concepts. To address this issue, we introduce the third and fourth terms in Eq. (3). These terms align the unconditional outputs of the finetuned model with those of the original model across both stages. Figure 4. Each subplot shows the number of active parameters (yaxis) against the number of intersected saliency maps (x-axis) for four concepts: (a) Nudity, (b) Donald Trump, (c) Van Gogh Style, and (d) Dog. The number of active parameters converges across different concept types with around 100 intersected saliency maps. ing gradient maps for the model parameters. By evaluating these gradients against threshold, we obtain set of weight saliency maps: 3.3. The Heavy Hitters Among the Parameters Mci,sj = 1 (θL(zt1, zt2, t1, t2, ci, sj) γ) , (4) After determining the optimization objective, identifying the most effective parameters to optimize for achieving improved performance efficiently becomes crucial. Previous approaches typically divide the model into multiple modules, such as residual blocks, self-attention, or crossattention, and select an entire module for finetuning [19, 20, 55, 99]. Among these, finetuning cross-attention modules is most common. Inspired by saliency map techniques [13, 14, 25, 75, 76, 79], we propose concept-specific saliency map enhanced by prompt and seed augmentation to precisely identify parameters suitable for finetuning. Compared to previous methods that compute the saliency map only once, we observe that the saliency map can vary depending on the prompt context and random seed, leading to instability. However, if we take the intersection of multiple saliency maps, the parameters within this intersection gradually become more stable and consistent as the number of maps increases (see Figure 4). This approach more accurately identifies the parameters responsible for the target concept, resulting in consistent improvement in performance (as shown in the ablation study results in Table 5). Specifically, as illustrated in Figure 5, we first employ GPT-4 [61] to generate multiple prompts = {ci}Nc i=1, each accompanied by set of random seeds = {sj}Ns j=1, to produce correspondwhere 1(g γ) is an element-wise indicator function that returns 1 for the i-th element if gi γ, and 0 otherwise; denotes the element-wise absolute value operation; and γ > 0 is predefined threshold. Each weight saliency map identifies critical parameters strongly correlated with the targeted concept across diverse prompt contexts. Finally, the intersection of these weight saliency maps obtained from various prompts and seeds yields the definitive conceptspecific saliency map : = (cid:92) (cid:92) Mci,sj . ciC sj (5) As result, only crucial subset of parameters is finetuned: θ θ α θL(zt1, zt2, t1, t2, ci, sj), (6) where α is the learning rate and denotes the elementwise multiplication. Intuitively, this mechanism identifies and finetunes only those parameters consistently influential for erasing the undesired concept across diverse conditions. Concept-specific saliency map significantly narrows down the finetuning parameters, effectively preventing unnecessary perturbations to parameters unrelated to the targeted concept. 5 Figure 5. Generation of the concept-specific saliency map . GPT-4 generates prompts = {ci}Nc i=1, each paired with random seeds = {sj}Ns j=1, which are used to compute gradient maps. After thresholding, saliency maps are obtained, and their intersection across all prompts and seeds yields . 3.4. Boosting the Performance of Multi-Concept"
        },
        {
            "title": "Erasure Frameworks",
            "content": "Our proposed trajectory-aware loss function seamlessly integrates with existing multi-concept erasure frameworks, such as MACE [55], offering flexible and adaptable plug-and-play solution. Accordingly, it significantly boosts MACEs performance in multi-concept scenarios, delivering new SOTA outcomes on tasks involving the erasure of 100 celebrity concepts and 100 artistic concepts. As observed in MACE, erasing multiple concepts through either sequential or parallel finetuning often degrades performance. Sequential finetuning is susceptible to catastrophic forgetting, while parallel finetuning can lead to interference between concepts [55]. MACE addresses this by training separate LoRA module for each concept to be erased, and subsequently fusing all LoRA modules into the cross-attention layers using closed-form solution. By integrating our loss function into the MACE framework, the initial training stage can be omitted. In the second stage, we replace MACEs attention loss with our trajectory-aware loss to train individual LoRA modules Wi for each concept, eliminating the need for the large Grounded-SAM model. After training all LoRA modules, we use the following objective function to fuse them into the cross-attention layers: min (cid:88) (cid:88) i=1 j=1 (cid:13) (cid:13)W ef (cid:13) (W + Wi) ef + β p+m (cid:88) j=p+1 (cid:13) (cid:13)W ep ep (cid:13) 2 (cid:13) , (cid:13) 2 (cid:13) (cid:13) 2 (7) where denotes the original weight matrix of either the key or value projection. The embedding ef corresponds to concept-related tokens that we aim to erase, while ep represents embeddings of unrelated, prior-preservation tokens. Here, is the number of concepts to be erased, and and denote the numbers of targeted concept tokens and priorpreservation tokens, respectively. As shown in Figure 6, the objective is to find soluFigure 6. Multi-LoRA fusion for multi-concept erasure. tion that integrates multiple LoRA matrices, optimized for effective multi-concept erasure. This optimization problem has closed-form solution [55]. Table 2 shows that our trajectory-aware loss function seamlessly integrates with the MACE framework for multi-concept erasure, substantially enhancing its performance. 4. Experiments In this section, we present comprehensive evaluation of our proposed method by benchmarking it against SOTA baselines on both single-concept erasure (NSFW removal; Section 4.2) and multi-concept erasure tasks, including 100celebrity erasure (Section 4.3) and 100-artistic style erasure (Section 4.4). Finally, we perform ablation studies (Section 4.5) to assess the contribution of key components in our approach. 4.1. Implementation Details We finetune all models based on Stable Diffusion (SD) v1.4 and generate outputs using the DDIM sampler [77] over 50 inference steps. Our experimental setup follows the settings described in MACE [55]. Each LoRA module undergoes 50 gradient update steps during training. For the baselines, we adopt the configurations provided in their respective original implementations. 4.2. Erasing NSFW Content Configuration. In this experiment, we focus on removing the concept nudity from the model, representing typical NSFW category. Specifically, we follow the nudity, naked, erotic, sexual prompts introduced in [28, 55] to guide the construction of the concept-specific saliency map over the UNet. Based on , we finetune SD v1.4 to eliminate the concept. For evaluation, we use the full set of 4,703 prompts from the I2P dataset [73] along with their corresponding random seeds to generate images. We then apply NudeNet [65] with the threshold of 0.6 to detect exposed body parts in the sampled images, treating the detection results as an indicator of residual nudity in the models output. In addition, we assess the effectiveness of concept removal techniques in preserving benign content, utilizing the MS-COCO dataset [51]. We sample 30,000 captions from the validation split to generate images and compute FID [63] and CLIP score [66] as metrics for image quality and semantic alignment. Table 1. Results of Erasing NSFW Content. The left side shows the number of exposed body parts detected on the I2P dataset using the NudeNet detector, while the right side presents the FID and CLIP on the COCO dataset. M: Male. F: Female. Method FMN [99] ESD-x [19] ESD-u [19] SLD-M [73] AC [40] SA [28] EA [21] UCE [20] Receler[32] MACE [55] AdvUnlearn[100] RealEra[52] SPEED [49] SalUn [13] CE-SDWV [84] SPM [57] RECE [23] SDD [37] DuMo [24] ACE [87] Ours SD v1.4 SD v2.1 Armpits Belly Buttocks Feet Breasts (F) Genitalia (F) Breasts (M) Genitalia (M) Total FID CLIP Inappropriate Image Prompt (I2P) MS-COCO 30K 43 59 32 47 153 72 - 29 39 17 12 19 20 2 13 22 17 14 8 5 1 148 105 117 73 30 72 180 77 - 62 26 19 7 6 42 14 46 4 23 4 6 7 5 170 159 12 12 2 3 45 19 - 7 5 2 4 2 7 0 2 9 0 7 2 3 2 29 59 39 19 21 66 25 - 29 10 39 13 37 3 14 2 12 8 3 7 6 4 63 60 155 100 27 39 298 83 - 35 13 16 6 23 29 7 13 4 8 8 1 2 8 266 177 17 6 3 1 22 16 - 5 1 2 2 4 2 2 0 0 0 1 4 3 2 18 19 18 8 26 67 0 - 11 12 9 0 0 5 7 1 0 6 0 0 4 0 42 57 2 8 2 3 7 0 - 4 9 7 8 2 5 5 6 5 4 4 6 9 1 7 2 424 315 123 212 838 292 199 182 115 111 52 93 113 51 84 56 66 41 34 39 23 743 13.52 14.41 15.10 16.34 14.13 21.75 14.07 - 13.42 15.35 - 37.82 13.66 - - - - 14.69 14.44 14.04 14.87 30.39 30.69 30.21 30.90 31.37 30.24 30.85 - 29.41 29.3 - 26.29 30.80 - - - - 30.80 30.64 31.34 31.53 Results Analysis. The experimental results are presented in Table 1. Our method generates significantly less NSFW content under the I2P benchmark prompts compared to other baselines, especially in challenging regions such as breasts. At the same time, our method also achieves competitive performance in terms of FID and CLIP scores. These results demonstrate that our approach can effectively remove explicit content from the model without compromising image quality. 4.3. Erasing Celebrity Configuration. In this section, we evaluate the performance of our method on the task of simultaneously erasing multiple celebrity concepts, using the 200-celebrity dataset from MACE [55], which includes 100 celebrity concepts designated for erasure and 100 concepts intended to be preserved. We conduct experiments by finetuning SD v1.4 to erase all 100 celebrity identities in the erasure group. We evaluate the effectiveness of our method by generating portraits of the targeted celebrities. Successful erasure is indicated by low top-1 accuracy from GIPHY Celebrity Detector (GCD) [27] in identifying the erased identities. Additionally, to investigate the influence of our method on celebrities in the preservation group, we generate and evaluate their portraits in the same manner, where high top-1 GCD accuracy reflects minimal impact on these preserved identities. We also report the harmonic mean Hc metric introduced Table 2. Results of Erasing Celebrity. We report the accuracy for erased celebrities (Acce), accuracy for preserved celebrities (Accp), harmonic mean metric (Hc) and the proportion of clearly recognizable faces (Face Ratio). FID and CLIP are results based on MS-COCO dataset. SD v1.4 and SD v2.1 are used as reference base models. Method Acce Accp Hc Face Ratio FID CLIP FMN [99] ESD-x [19] ESD-u [19] SLD-M [73] AC [40] UCE [20] RECE [23] SPEED [49] MACE [55] Ours SD v1.4 SD v2.1 0.9223 0.2784 0.0406 0.8706 0.8913 0.0012 0.0243 0.0587 0.0430 0.0430 0.9648 0.9324 0.9076 0.2793 0.3909 0.7946 0.9096 0.3790 0.2371 0.8554 0.8456 0. 0.9388 0.9293 0.1431 0.4027 0.4598 0.2237 0.1977 0.5495 0.3816 0.8963 0.8979 0.9173 - - 0.9940 0.8088 0.4724 0.9093 0.9932 0.7179 - - 0.9820 0.9816 0.9876 0.9879 13.95 14.65 15.14 17.54 13.92 106.57 177.57 44.97 12.82 11. 14.04 14.87 31.31 28.90 29.02 30.93 31.23 19.17 12.09 26.22 30.21 30.40 31.34 31.53 in [55], which provides balanced evaluation of the tradeoff between successful erasure of unwanted celebrity concepts and the preservation of unrelated ones: Hc = 1 (1 Acce)1 + (Accp)1 , (8) where Hc is the harmonic mean for celebrity erasure, Acce is the accuracy for the erased celebrities, and Accp for the preserved ones. 7 Figure 7. Qualitative comparison of erasing 100 celebrities from SD v1.4. John Wayne and Tom Hiddleston are in the erasure group for evaluating erasure performance; John Lennon and Gal Gadot are in preservation group for assessing preservation performance. Preserving John Lennon is challenging due to the shared first name with John Wayne. Results Analysis. Figure 7 shows the qualitative comparison. Table 2 summarizes the performance of baselines on the celebrity concept erasure task. Our method achieves the highest Hc, outperforming all baselines and highlighting an excellent balance between concept erasure and preservation of unrelated ones. Our method obtains the lowest FID score, surpassing all compared baselines and even the original SD models. plausible reason for this improvement is that our finetuning process, while primarily intended for erasing specific concepts, implicitly regularizes the model by encouraging more consistent representations of general concepts. Additionally, the CLIP score remains competitive, indicating minimal disruption to semantic alignment. 4.4. Erasing Art Style Configuration. For art style erasure, we follow similar training procedure as described in Section 4.3, with certain 8 Table 3. Results of Erasing 100 Art Styles. We report the CLIP score for erased artistic style (CLIPe), CLIP score for preserved artistic style (CLIPp), the overall score (Ha). FID and CLIP are results based on MS-COCO dataset. Table 4. Ablation study on multiple concepts (celebrity) removal. erase: Lerase is applied at all denoising timesteps during training. Lerase: Lerase is applied only during the mid-to-late stages of the denoising process in training. Method CLIPe CLIPp Ha FID-COCO CLIP-COCO FMN [99] ESD-x [19] ESD-u [19] SLD-M [73] AC [40] UCE [20] MACE [55] Ours SD v1.4 29.63 20.89 19.66 28.49 29.26 21.31 22.59 20. 29.63 28.90 21.21 19.55 27.89 28.54 25.70 28.58 26.78 28.90 -0.73 0.32 -0.11 -0.60 -0.72 4.39 5.99 6.18 - 13.99 15.19 17.07 17.95 14.08 77.72 12.71 12. 14.04 31.31 29.52 27.76 30.87 31.29 19.17 29.51 27.63 31.34 hyperparameter adjustments detailed in the Appendix. To evaluate performance, we use the 200-artist dataset from MACE [55], which consists of two groups: an erasure group of 100 artists whose styles are targeted for removal, and preservation group of 100 artists whose styles are intended to be retained. We use the CLIP score to assess how well the generated images align with the intended artistic style. For the erasure group, lower CLIP score (CLIPe) indicates better performance, as it suggests more effective removal of the target concept. In contrast, for the preservation group, higher CLIP score (CLIPp) is desirable, as it reflects minimal disruption to unrelated concepts. The overall performance is captured by Ha = CLIPp CLIPe, where higher value indicates better balance between preservation and erasure. Results Analysis. Table 3 summarizes the performance of our method in erasing artistic styles. Our method achieves the highest Ha, substantially surpassing all baseline methods, demonstrating superior balance in effectively removing targeted art styles and preserving unrelated art styles. Considering the overall performance across other metrics, our strategy shows notable competitiveness compared to existing approaches. 4.5. Ablation Study To investigate the contribution of key components in our approach, we conduct ablation studies on both multiple concepts (celebrity removal) and single concept (NSFW removal) tasks. The experimental configurations and corresponding results are presented in Tables 4 and 5, respectively. We begin by ablating each component of our loss function in the context of celebrity removal. Config A, which applies Lerase across all stages without preserving the earlystage score function field, shows strong removal capability but clearly suffers in terms of preservation. Config builds on Config by adding Luncond to maintain the unconditional score function, resulting in improved overall performance in Config D Components Metrics Lerase erase Lpreserve Luncond-early Luncond-late Acce Accp Hc Ø 0.8680 0.8778 Ø Ø 0.8821 Ø 0.8867 Ø Ø 0.8809 0.7785 0.7848 0.8094 0.8013 0.8545 0.0192 0.0042 0.0309 0.0075 0.0910 Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ours Ø 0.0430 0.8807 0.9173 Table 5. Ablation study on single concept (NSFW) removal. Single Map: is generated using single prompt and one random seed. Multi Maps: is generated taking the intersection of saliency maps obtained using multiple prompts and multiple random seeds. Config Ours Components Inappropriate Image Prompt (I2P) Single Map Multi Maps Breasts (M&F) Genitalia (M&F) Others Total Ø Ø Ø Ø 136 83 8 11 56 148 184 12 295 323 23 terms of Hc. Next, Config applies Lerase only during the mid-to-late sampling stages, aiming to avoid disruption of the early-stage score function field. While this slightly improves Hc, the results remain unsatisfactory. Config enhances Config by adding Luncond, applied over the same timesteps as Lerase, which further improves overall performance. Config extends Config by introducing Lpreserve, which helps retain the original score function field and significantly boosts preservation performance in terms of Accp. Our full method builds upon Config by applying Luncond across all stages, resulting in superior overall performance. For NSFW content removal, Config finetunes the entire UNet, while Config finetunes only subset of parameters using saliency map obtained from single calculation. However, Config performs worse than Config F, suggesting that saliency map generated from single pass may be inaccurate. In contrast, our method derives more precise concept-specific saliency map by taking the intersection of multiple saliency maps calculated from different prompts and seeds. This allows us to more accurately identify the parameters strongly associated with the concept, leading to substantially improved performance. 5. Conclusion Our work introduces geometric perspective on concept erasure within diffusion models. Utilizing this perspective, we found that reversing the condition direction of classifierfree guidance during the mid-to-late stages of the denoising process allows for modifying detailed content without 9 compromising the overall structural integrity of the generated images. Inspired by this insight, we propose ANT, novel framework that effectively balances the removal of unwanted concepts while preserving unrelated elements. ANT demonstrates superior performance in both singleand multi-concept erasure scenarios, significantly outperforming current SOTA methods."
        },
        {
            "title": "References",
            "content": "[1] Stability AI. Stable diffusion v2.1 and dreamstudio updates 7-dec 22, 2022. 14 [2] Lucas Beerens, Alex Richardson, Kaicheng Zhang, and Dongdong Chen. On the vulnerability of concept erasure in diffusion models. arXiv preprint arXiv:2502.17537, 2025. 1, 14 [3] Anh Bui, Long Vuong, Khanh Doan, Trung Le, Paul Montague, Tamas Abraham, and Dinh Phung. Erasing undesirable concepts in diffusion models with adversarial preservation. arXiv preprint arXiv:2410.15618, 2024. 1, 3, 14 [4] Anh Bui, Trang Vu, Long Vuong, Trung Le, Paul Montague, Tamas Abraham, Junae Kim, and Dinh Phung. Fantastic targets for concept erasure in diffusion models and arXiv preprint arXiv:2501.18950, where to find them. 2025. 1, 3, 14 [5] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William Freeman, Michael Rubinstein, et al. Muse: Textto-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. 1, 14 [6] Ruchika Chavhan, Da Li, and Timothy Hospedales. Conceptprune: Concept editing in diffusion models via skilled neuron pruning. arXiv preprint arXiv:2405.19237, 2024. 1, 3, [7] Die Chen, Zhiwen Li, Mingyuan Fan, Cen Chen, Wenmeng Zhou, Yanhao Wang, and Yaliang Li. Growth inhibitors for suppressing inappropriate image concepts in diffusion models. In The Thirteenth International Conference on Learning Representations. 1 [8] Huiqiang Chen, Tianqing Zhu, Linlin Wang, Xin Yu, Longxiang Gao, and Wanlei Zhou. Safe and reliable difarXiv preprint fusion models via subspace projection. arXiv:2503.16835, 2025. 1, 3, 14 [9] Ruidong Chen, Honglin Guo, Lanjun Wang, Chenyu Zhang, Weizhi Nie, and An-An Liu. Trce: Towards reliable malicious concept erasure in text-to-image diffusion models. arXiv preprint arXiv:2503.07389, 2025. 1, 14 [10] Bartosz Cywinski and Kamil Deja. Saeuron: Interpretable concept unlearning in diffusion models with sparse autoencoders. arXiv preprint arXiv:2501.18052, 2025. 1, 14 [11] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image generation via hierarchical transformers. Advances in Neural Information Processing Systems, 35:1689016902, 2022. 1, 14 [12] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 14 [13] Chongyu Fan, Jiancheng Liu, Yihua Zhang, Eric Wong, Salun: Empowering maDennis Wei, and Sijia Liu. chine unlearning via gradient-based weight saliency in both image classification and generation. arXiv preprint arXiv:2310.12508, 2023. 5, [14] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018. 5 [15] Masane Fuchi and Tomohiro Takagi. Erasing concepts from text-to-image diffusion models with few-shot unlearning. arXiv preprint arXiv:2405.07288, 2, 2024. 1, 14 [16] Masane Fuchi and Tomohiro Takagi. Erasing with precision: Evaluating specific concept erasure from arXiv preprint text-to-image generative models. arXiv:2502.13989, 2025. 1, 3, 14 [17] Tatiana Gaintseva, Chengcheng Ma, Ziquan Liu, Martin Benning, Gregory Slabaugh, Jiankang Deng, and Ismail Elezi. Casteer: Steering diffusion models for controllable generation. arXiv preprint arXiv:2503.09630, 2025. 3, 14 [18] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 14 [19] Rohit Gandikota, Joanna Materzynska, Jaden FiottoKaufman, and David Bau. Erasing concepts from diffusion models. arXiv preprint arXiv:2303.07345, 2023. 1, 3, 4, 5, 7, [20] Rohit Gandikota, Hadas Orgad, Yonatan Belinkov, Joanna Materzynska, and David Bau. Unified concept editing in In Proceedings of the IEEE/CVF Windiffusion models. ter Conference on Applications of Computer Vision, pages 51115120, 2024. 1, 5, 7, 9, 14 [21] Daiheng Gao, Shilin Lu, Shaw Walters, Wenbo Zhou, Jiaming Chu, Jie Zhang, Bang Zhang, Mengxi Jia, Jian Zhao, Zhaoxin Fan, et al. Eraseanything: Enabling concept erasure in rectified flow transformers. arXiv preprint arXiv:2412.20413, 2024. 1, 3, 7, 14 [22] Hongcheng Gao, Tianyu Pang, Chao Du, Taihang Hu, Zhijie Deng, and Min Lin. Meta-unlearning on diffusion models: Preventing relearning unlearned concepts. arXiv preprint arXiv:2410.12777, 2024. 3, 14 [23] Chao Gong, Kai Chen, Zhipeng Wei, Jingjing Chen, and Yu-Gang Jiang. Reliable and efficient concept erasure of In European Conference text-to-image diffusion models. on Computer Vision, pages 7388. Springer, 2024. 1, 7, 14 [24] Feng Han, Kai Chen, Chao Gong, Zhipeng Wei, Jingjing Chen, and Yu-Gang Jiang. Dumo: Dual encoder modulation network for precise concept erasure. arXiv preprint arXiv:2501.01125, 2025. 1, 3, 7, 14 [25] Song Han, Huizi Mao, and William Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015. 5 [26] Tingxu Han, Weisong Sun, Yanrong Hu, Chunrong Fang, Yonglong Zhang, Shiqing Ma, Tao Zheng, Zhenyu Chen, and Zhenting Wang. Continuous concepts removal in text-to-image diffusion models. arXiv preprint arXiv:2412.00580, 2024. 1, 3, 14 [27] Nick Hasty, Ihor Kroosh, Dmitry Voitekh, and Dmytro Korduban. Giphy celebrity detector. https://github. com/Giphy/celeb-detection-oss. 7 [28] Alvin Heng and Harold Soh. Selective amnesia: continual learning approach to forgetting in deep generative models. arXiv preprint arXiv:2305.10120, 2023. 1, 3, 6, 7, 14 [29] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 2 [30] Seunghoo Hong, Juhun Lee, and Simon Woo. All but one: Surgical concept erasing with model preservation in text-to-image diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 2114321151, 2024. 1 [31] Yuepeng Hu, Zhengyuan Jiang, and Neil Zhenqiang Gong. Safetext: Safe text-to-image models via aligning the text encoder. arXiv preprint arXiv:2502.20623, 2025. 1, 14 [32] Chi-Pin Huang, Kai-Po Chang, Chung-Ting Tsai, YungHsuan Lai, Fu-En Yang, and Yu-Chiang Frank Wang. Receler: Reliable concept erasing of text-to-image diffusion models via lightweight erasers. In European Conference on Computer Vision, pages 360376. Springer, 2024. 1, 3, 7, 14 [33] Anubhav Jain, Yuya Kobayashi, Takashi Shibuya, Yuhta Takida, Nasir Memon, Julian Togelius, and Yuki Mitsufuji. Trasce: Trajectory steering for concept erasure. arXiv preprint arXiv:2412.07658, 2024. 1, [34] Abdullah Ayub Khan, Jing Yang, Asif Ali Laghari, Abdullah Baqasah, Roobaea Alroobaea, Chin Soon Ku, Roohallah Alizadehsani, Rajendra Acharya, and Lip Yee Por. Baiot-ems: Consortium network for small-medium enterprises management system with blockchain and augmented intelligence of things. Engineering Applications of Artificial Intelligence, 141:109838, 2025. 14 [35] Changhoon Kim, Kyle Min, and Yezhou Yang. Race: Robust adversarial concept erasure for secure text-to-image diffusion model. In European Conference on Computer Vision, pages 461478. Springer, 2024. 1, 14 [36] Dahye Kim and Deepti Ghadiyaram. Concept steerers: Leveraging k-sparse autoencoders for controllable generations. arXiv preprint arXiv:2501.19066, 2025. 1, 14 [37] Sanghyun Kim, Seohyeon Jung, Balhae Kim, Moonseok Choi, Jinwoo Shin, and Juho Lee. Towards safe selfdistillation of internet-scale text-to-image diffusion models. arXiv preprint arXiv:2307.05977, 2023. 1, 3, 7, 14 [38] Sanghyun Kim, Moonseok Choi, Jinwoo Shin, and Juho Lee. Safety alignment backfires: Preventing the reemergence of suppressed concepts in fine-tuned text-toimage diffusion models. arXiv preprint arXiv:2412.00357, 2024. [39] Sanghyun Kim, Seohyeon Jung, Balhae Kim, Moonseok Choi, Jinwoo Shin, and Juho Lee. Safeguard text-to-image In Eudiffusion models with human feedback inversion. ropean Conference on Computer Vision, pages 128145. Springer, 2024. 1, 3, [40] Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, and Jun-Yan Zhu. Ablating In Proceedconcepts in text-to-image diffusion models. ings of the IEEE/CVF International Conference on Computer Vision, pages 2269122702, 2023. 1, 3, 7, 9 [41] Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion models already have semantic latent space. arXiv preprint arXiv:2210.10960, 2022. 1 [42] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. Accessed: February 21, 2025. 14 [43] Byung Hyun Lee, Sungjin Lim, and Se Young Chun. Localized concept erasure for text-to-image diffusion models using training-free gated low-rank adaptation. arXiv preprint arXiv:2503.12356, 2025. 1, 14 [44] Bin Li, Yixuan Weng, Bin Sun, and Shutao Li. multitasking and multi-stage chinese minority pre-trained lanIn China Conference on Machine Translaguage model. tion, pages 93105. Springer, 2022. 14 [45] Bin Li, Yixuan Weng, Fei Xia, Bin Sun, and Shutao Li. Vpai lab at medvidqa 2022: two-stage cross-modal fusion method for medical instructional video classification. In Proceedings of the 21st Workshop on Biomedical Language Processing, pages 212219, 2022. [46] Bin Li, Bin Sun, Shutao Li, Encheng Chen, Hongru Liu, Yixuan Weng, Yongping Bai, and Meiling Hu. Distinct but correct: generating diversified and entity-revised medical response. Science China Information Sciences, 67(3): 132106, 2024. [47] Bin Li, Yixuan Weng, Fei Xia, and Hanjun Deng. Towards better chinese-centric neural machine translation for lowresource languages. Computer Speech & Language, 84: 101566, 2024. 14 [48] Feifei Li, Mi Zhang, Yiming Sun, and Min Yang. Detectand-guide: Self-regulation of diffusion models for safe text-to-image generation via guideline token optimization. arXiv preprint arXiv:2503.15197, 2025. 1, 3, 14 [49] Ouxiang Li, Yuan Wang, Xinting Hu, Houcheng Jiang, Tao Liang, Yanbin Hao, Guojun Ma, and Fuli Feng. Speed: Scalable, precise, and efficient concept erasure for diffusion models. arXiv preprint arXiv:2503.07392, 2025. 1, 7, 14 [50] Shutao Li, Bin Li, Bin Sun, and Yixuan Weng. Towards visual-prompt temporal answer grounding in instructional video. IEEE transactions on pattern analysis and machine intelligence, 46(12):88368853, 2024. 14 [51] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 6 [52] Yufan Liu, Jinyang An, Wanqian Zhang, Ming Li, Dayan Wu, Jingzi Gu, Zheng Lin, and Weiping Wang. Realera: Semantic-level concept erasure via neighbor-concept mining. arXiv preprint arXiv:2410.09140, 2024. 1, 3, 7, 11 [53] Zhili Liu, Kai Chen, Yifan Zhang, Jianhua Han, Lanqing Hong, Hang Xu, Zhenguo Li, Dit-Yan Yeung, and James Kwok. Implicit concept removal of diffusion models. In European Conference on Computer Vision, pages 457473. Springer, 2024. 1 [54] Shilin Lu, Yanzhu Liu, and Adams Wai-Kin Kong. Tf-icon: Diffusion-based training-free cross-domain image composition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22942305, 2023. 14 [55] Shilin Lu, Zilan Wang, Leyang Li, Yanzhu Liu, and Adams Wai-Kin Kong. Mace: Mass concept erasure in diffusion In Proceedings of the IEEE/CVF Conference on models. Computer Vision and Pattern Recognition, pages 6430 6440, 2024. 1, 3, 5, 6, 7, 9, 14 [56] Shilin Lu, Zihan Zhou, Jiayou Lu, Yuanzhi Zhu, and Adams Wai-Kin Kong. Robust watermarking using generative priors against image editing: From benchmarking to advances. arXiv preprint arXiv:2410.18775, 2024. 14 [57] Mengyao Lyu, Yuhong Yang, Haiwen Hong, Hui Chen, Xuan Jin, Yuan He, Hui Xue, Jungong Han, and Guiguang Ding. One-dimensional adapter to rule them all: Concepts diffusion models and erasing applications. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 75597568, 2024. 1, 3, 7, 14 [58] Zheling Meng, Bo Peng, Xiaochuan Jin, Yueming Lyu, Wei Wang, and Jing Dong. Concept corrector: Erase concepts on the fly for text-to-image diffusion models. arXiv preprint arXiv:2502.16368, 2025. 1, 3, [59] Quang Nguyen, Hoang Phan, and Khoa Doan. Unveiling concept attribution in diffusion models. arXiv preprint arXiv:2412.02542, 2024. 1, 14 [60] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 1, 14 [61] OpenAI. Hello gpt-4o, 2024. 5 [62] Yong-Hyun Park, Sangdoo Yun, Jin-Hwa Kim, Junho Kim, Geonhui Jang, Yonghyun Jeong, Junghyo Jo, and Gayoung Lee. Direct unlearning optimization for robust and safe text-to-image models. arXiv preprint arXiv:2407.21035, 2024. 1, 14 [63] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On aliased resizing and surprising subtleties in gan evaluation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1141011420, 2022. 6 [64] William Peebles and Saining Xie. Scalable diffusion modIn Proceedings of the IEEE/CVF els with transformers. international conference on computer vision, pages 4195 4205, 2023. 14 [65] platelminto. Nudenet: Neural nets for nudity classification, detection and selective censoring, 2023. 6 [66] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language super12 In International conference on machine learning, vision. pages 87488763. PMLR, 2021. 6 [67] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey text-conditional arXiv preprint Chu, and Mark Chen. image generation with clip latents. arXiv:2204.06125, 1(2):3, 2022. 1, 14 Hierarchical [68] Robin Rombach. Stable diffusion v1-4 model card. 2022. 14 [69] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 14 [70] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF conference generation. on computer vision and pattern recognition, pages 22500 22510, 2023. [71] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:3647936494, 2022. 1, 14 [72] Andrea Schioppa, Emiel Hoogeboom, and Jonathan Heek. Model integrity when unlearning with t2i diffusion models. arXiv preprint arXiv:2411.02068, 2024. 1, 3, 14 [73] Patrick Schramowski, Manuel Brack, Bjorn Deiseroth, and Kristian Kersting. Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2252222531, 2023. 1, 3, 6, 7, 9 [74] Reza Shirkavand, Peiran Yu, Shangqian Gao, Gowthami Somepalli, Tom Goldstein, and Heng Huang. Efficient finetuning and concept suppression for pruned diffusion models. arXiv preprint arXiv:2412.15341, 2024. 1, 3, 14 [75] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013. 5 [76] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viegas, and Martin Wattenberg. Smoothgrad: removing noise by adding noise. arXiv preprint arXiv:1706.03825, 2017. 5 [77] Jiaming Song, Chenlin Meng, and Stefano Ermon. DearXiv preprint noising diffusion implicit models. arXiv:2010.02502, 2020. [78] Koushik Srivatsan, Fahad Shamshad, Muzammal Naseer, and Karthik Nandakumar. Stereo: Towards adversarially robust concept erasing from text-to-image generation models. arXiv preprint arXiv:2408.16807, 2024. 1, 14 [79] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. AxIn Proceedings of iomatic attribution for deep networks. the 34th International Conference on Machine LearningVolume 70, pages 33193328. JMLR. org, 2017. 5 [80] Kartik Thakral, Tamar Glaser, Tal Hassner, Mayank Vatsa, and Richa Singh. Continual unlearning for foundational text-to-image models without generalization erosion. arXiv preprint arXiv:2503.13769, 2025. 1, 3, 14 [81] Kartik Thakral, Tamar Glaser, Tal Hassner, Mayank Vatsa, Fine-grained erasure in text-toand Richa Singh. image diffusion-based foundation models. arXiv preprint arXiv:2503.19783, 2025. 1, 14 [82] Zhihua Tian, Sirun Nan, Ming Xu, Shengfang Zhai, Wenjie Qu, Jian Liu, Kui Ren, Ruoxi Jia, and Jiaheng Zhang. Sparse autoencoder as zero-shot classifier for concept erasing in text-to-image diffusion models. arXiv preprint arXiv:2503.09446, 2025. 1, 3, 14 [83] Yu-Lin Tsai, Chia-Yi Hsu, Chulin Xie, Chih-Hsun Lin, Jia-You Chen, Bo Li, Pin-Yu Chen, Chia-Mu Yu, and Chun-Ying Huang. Ring-a-bell! how reliable are concept arXiv preprint removal methods for diffusion models? arXiv:2310.10012, 2023. [84] Jiahang Tu, Qian Feng, Chufan Chen, Jiahua Dong, Hanbin Zhao, Chao Zhang, and Hui Qian. Ce-sdwv: Effective and efficient concept erasure for text-to-image diffusion models via semantic-driven word vocabulary. arXiv preprint arXiv:2501.15562, 2025. 1, 3, 7, 14 [85] Ruipeng Wang, Junfeng Fang, Jiaqi Li, Hao Chen, Jie Shi, Kun Wang, and Xiang Wang. Ace: Concept editing in diffusion models without performance degradation. arXiv preprint arXiv:2503.08116, 2025. 1, 3, 14 [86] Yanghao Wang and Long Chen. Improving diffusion-based data augmentation with inversion spherical interpolation. arXiv preprint arXiv:2408.16266, 2024. 1 [87] Zihao Wang, Yuxiang Wei, Fan Li, Renjing Pei, Hang Xu, and Wangmeng Zuo. Ace: Anti-editing concept erasure in text-to-image models. arXiv preprint arXiv:2501.01633, 2025. 1, 3, 7, 14 [88] Jing Wu and Mehrtash Harandi. Munba: Machine unlearning via nash bargaining. arXiv preprint arXiv:2411.15537, 2024. 1, 3, 14 [89] Tianwei Xiong, Yue Wu, Enze Xie, Zhenguo Li, and Xihui Liu. Editing massive concepts in text-to-image diffusion models. arXiv preprint arXiv:2403.13807, 2024. 1, 14 [90] Yuyang Xue, Edward Moroshko, Feng Chen, Steven McDonagh, and Sotirios Tsaftaris. Crce: Coreferenceretention concept erasure in text-to-image diffusion models. arXiv preprint arXiv:2503.14232, 2025. 1, 3, [91] Jing Yang, Liangyu Li, Lip Yee Por, Sami Bourouis, Sami Dhahbi, and Abdullah Ayub Khan. Harnessing multimodal data and deep learning for comprehensive gait analysis in pediatric cerebral palsy. IEEE Transactions on Consumer Electronics, 2024. 14 [92] Jing Yang, Nika Anoosha Boroojeni, Mehran Kazemi Chahardeh, Lip Yee Por, Roohallah Alizadehsani, and Rajendra Acharya. dual-method approach using autoencoders and transductive learning for remaining useful life estimation. Engineering Applications of Artificial Intelligence, 147:110285, 2025. [93] Jing Yang, Ke Tian, Huayu Zhao, Zheng Feng, Sami Bourouis, Sami Dhahbi, Abdullah Ayub Khan, Mouhebeddine Berrima, and Lip Yee Por. Wastewater treatment monitoring: Fault detection in sensors using transductive learning and improved reinforcement learning. Expert Systems with Applications, 264:125805, 2025. [94] Jing Yang, Yuangui Wu, Yuping Yuan, Haozhong Xue, Sami Bourouis, Mahmoud Abdel-Salam, Sunil Prajapat, and Lip Yee Por. Llm-ae-mp: Web attack detection using large language model with autoencoder and multilayer perceptron. Expert Systems with Applications, 274:126982, 2025. 14 [95] Tianyun Yang, Juan Cao, and Chang Xu. Pruning for robust concept erasing in diffusion models. arXiv preprint arXiv:2405.16534, 2024. 1, 3, [96] Xinlei Yu, Ahmed Elazab, Ruiquan Ge, Hui Jin, Xinchen Jiang, Gangyong Jia, Qing Wu, Qinglei Shi, and Changmiao Wang. Ich-scnet: Intracerebral hemorrhage segmentation and prognosis classification network using clip-guided sam mechanism. In 2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), pages 2795 2800. IEEE, 2024. 14 [97] Xinlei Yu, Xinyang Li, Ruiquan Ge, Shibin Wu, Ahmed Elazab, Jichao Zhu, Lingyan Zhang, Gangyong Jia, Taosheng Xu, Xiang Wan, et al. Ichpro: Intracerebral hemorrhage prognosis classification via joint-attention fusionIn 2024 IEEE Internabased 3d cross-modal network. tional Symposium on Biomedical Imaging (ISBI), pages 1 5. IEEE, 2024. [98] Xinlei Yu, Ahmed Elazab, Ruiquan Ge, Jichao Zhu, Lingyan Zhang, Gangyong Jia, Qing Wu, Xiang Wan, Lihua Li, and Changmiao Wang. Ich-prnet: cross-modal intracerebral haemorrhage prognostic prediction method using joint-attention interaction mechanism. Neural Networks, 184:107096, 2025. 14 [99] Eric Zhang, Kai Wang, Xingqian Xu, Zhangyang Wang, Forget-me-not: Learning to forarXiv preprint and Humphrey Shi. get in text-to-image diffusion models. arXiv:2303.17591, 2023. 1, 3, 5, 7, 9 [100] Yimeng Zhang, Xin Chen, Jinghan Jia, Yihua Zhang, Chongyu Fan, Jiancheng Liu, Mingyi Hong, Ke Ding, and Sijia Liu. Defensive unlearning with adversarial training for robust concept erasure in diffusion models. Advances in Neural Information Processing Systems, 37:3674836776, 2024. 1, 7, [101] Yimeng Zhang, Jinghan Jia, Xin Chen, Aochuan Chen, Yihua Zhang, Jiancheng Liu, Ke Ding, and Sijia Liu. To generate or not? safety-driven unlearned diffusion models are still easy to generate unsafe images... for now. In European Conference on Computer Vision, pages 385403. Springer, 2024. 14 [102] Mengnan Zhao, Lihe Zhang, Xingyi Yang, Tianhang Zheng, and Baocai Yin. Advanchor: Enhancing diffusion model unlearning with adversarial anchors. arXiv preprint arXiv:2501.00054, 2024. 1, 3, 14 [103] Yuanzhi Zhu, Ruiqing Wang, Shilin Lu, Junnan Li, Hanshu Yan, and Kai Zhang. Oftsr: One-step flow for image superresolution with tunable fidelity-realism trade-offs. arXiv preprint arXiv:2412.09465, 2024."
        },
        {
            "title": "Appendix",
            "content": "A. Additional Related Work With the advancement of deep learning [34, 4447, 50, 91 94, 9698] and generative models [5, 11, 54, 56, 60, 67, 69, 71, 103], an increasing number of studies have begun to focus on the issue of concept erasure in generative models. A.1. Balancing Erasure and Preservation. With the advancement of concept erasure techniques, the community has come to recognize that concept erasure should not only focus on the target concept but also aim to minimize the impact on unrelated concepts during finetuning. Numerous studies [3, 4, 68, 16, 17, 21, 22, 24, 26, 28, 32, 37, 39, 48, 52, 55, 57, 58, 72, 74, 8082, 84, 85, 87, 88, 90, 95, 102] emphasize the models balanced performance between the target concept and unrelated concepts. MACE [55] introduces concept-focal importance sampling and modular LoRA integration, allowing for scalable multiconcept erasure while avoiding interference across modules. Several works [52, 90] explore semantic-aware preservation by modeling relationships between erased and retained concepts, improving quality retention in adjacent concept spaces. Some frameworks [81, 88] formalize the forgettingretention trade-off, offering principled mechanisms to control degradation. A.2. Finetuning Efficiency. In addition to balancing erasure and preservation, several recent methods [3, 15, 20, 23, 36, 49, 57, 58, 85] have increasingly emphasized einetuning efficiency to meet practical demands. [20, 49] achieve erasure across hundreds of concepts within seconds by leveraging low-rank adapters or null-space constraints, enabling rapid adaptation across diffusion model variants. [23, 85] introduce closed-form or structure-aware updates that reduce erasure time by orders of magnitude. These advancements demonstrate trend toward minimal-latency, high-throughput concept erasure that enables practical integration into production-scale text-toimage pipelines. A.3. Scalability. scalable multi-concept With the growing demand for safe and policy-compliant generative models, erasure techniques[8, 9, 20, 37, 43, 49, 55, 89] have emerged as key direction in diffusion model editing. [20] introduces an editing framework that supports the simultaneous modification of multiple concepts through lightweight model updates. leverages modular LoRA-based editing combined with closed-form integration to eliminate over 100 concepts with minimal interference. [89] adopts two-stage process involving self-distillation and multi-layer editing, scaling up to 1,000 concepts while preserving [55] Table 6. Training hyperparameters for NFSW content, celebrity and art style erasure tasks. Erasure Type Learning Rate Epochs NFSW Content Celebrity Art Style 5.0 104 5.0 10 5.0 104 250 400 400 λ1 1. 0.4 0.4 λ2 0.5 0.5 0. λ3 0.5 0.2 0.2 40 47 specificity and visual fidelity. Additional methods such as [8, 9] enhance scalability through embedding-space operations or adversarially robust training objectives. These techniques collectively push concept erasure toward broader, more practical deployment scenarios requiring high-volume, reliable editing. A.4. Robustness. Despite successful concept suppression, erased models remain vulnerable to adversarial prompts that can reactivate undesirable content. growing number of methods[2, 6, 9, 10, 17, 22, 23, 3133, 35, 36, 43, 48, 58, 59, 62, 78, 82, 84, 87, 88, 95, 100] have begun to address this issue explicitly, aiming to improve model reliability in the face of promptbased attacks. Methods such as [33, 78] tackle this by pairing adversarial prompt discovery with robust erasure objectives or inference-time steering, offering stronger defense without retraining. Others, like [35, 62, 100] incorporate adversarial training or preference-based optimization directly into the unlearning process to improve stability against attack. Complementary strategies from [10, 31, 59] focus on interpretable attribution or encoder-level alignment to neutralize unsafe inputs at their origin. Together, these works underscore the need for erasure techniques that are not only effective but resilient under adversarial conditions. B. Hyperparameters Setup Table 6 presents the specific hyperparameters used in the experiments for erasing different types of concepts. C. Limitations and Future Work Our work has primarily been tested on UNet-based diffusion models [1, 68]. As diffusion models increasingly adopt architectures like MMDiT [12, 42, 64], evaluating the compatibility of our approach with these new frameworks will be key focus of our next phase. Additionally, assessing the robustness of our framework against adversarial prompts [83, 101] and its ability to withstand methods for learning personalized concepts [18, 70] will be of critical importance. D. Additional Qualitative Results Figure 8 presents qualitative comparison of art style erasure and the preservation of unrelated concepts across difFigure 8. Qualitative comparison on art style erasure. The images on the same row are generated using the same random seed. Chris Van Allsburg and Claude Monet are in the erasure group, while Adriaen Van Outrecht and Adrian Ghenie are in the retention group. ferent baselines. In the erasure rows, our approach effectively eliminates the target artistic styles (Chris Van Allsburg and Claude Monet) while retaining high-quality, plausible generation. In the preservation rows, our method successfully maintains the visual characteristics of unrelated artists (Adriaen Van Utrecht and Adrian Ghenie), showing minimal unintended impact on non-target styles."
        }
    ],
    "affiliations": [
        "Nanyang Technological University, Singapore"
    ]
}