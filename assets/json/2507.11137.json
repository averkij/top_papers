{
    "paper_title": "Hashed Watermark as a Filter: Defeating Forging and Overwriting Attacks in Weight-based Neural Network Watermarking",
    "authors": [
        "Yuan Yao",
        "Jin Song",
        "Jian Jin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As valuable digital assets, deep neural networks necessitate robust ownership protection, positioning neural network watermarking (NNW) as a promising solution. Among various NNW approaches, weight-based methods are favored for their simplicity and practicality; however, they remain vulnerable to forging and overwriting attacks. To address those challenges, we propose NeuralMark, a robust method built around a hashed watermark filter. Specifically, we utilize a hash function to generate an irreversible binary watermark from a secret key, which is then used as a filter to select the model parameters for embedding. This design cleverly intertwines the embedding parameters with the hashed watermark, providing a robust defense against both forging and overwriting attacks. An average pooling is also incorporated to resist fine-tuning and pruning attacks. Furthermore, it can be seamlessly integrated into various neural network architectures, ensuring broad applicability. Theoretically, we analyze its security boundary. Empirically, we verify its effectiveness and robustness across 13 distinct Convolutional and Transformer architectures, covering five image classification tasks and one text generation task. The source codes are available at https://github.com/AIResearch-Group/NeuralMark."
        },
        {
            "title": "Start",
            "content": "Hashed Watermark as Filter: Defeating Forging and Overwriting Attacks in Weight-based Neural Network Watermarking Yuan Yao, Jin Song, Jian Jin 1 5 2 0 2 J 5 1 ] . [ 1 7 3 1 1 1 . 7 0 5 2 : r AbstractAs valuable digital assets, deep neural networks necessitate robust ownership protection, positioning neural network watermarking (NNW) as promising solution. Among various NNW approaches, weight-based methods are favored for their simplicity and practicality; however, they remain vulnerable to forging and overwriting attacks. To address those challenges, we propose NeuralMark, robust method built around hashed watermark filter. Specifically, we utilize hash function to generate an irreversible binary watermark from secret key, which is then used as filter to select the model parameters for embedding. This design cleverly intertwines the embedding parameters with the hashed watermark, providing robust defense against both forging and overwriting attacks. An average pooling is also incorporated to resist fine-tuning and pruning attacks. Furthermore, it can be seamlessly integrated into various neural network architectures, ensuring broad applicability. Theoretically, we analyze its security boundary. Empirically, we verify its effectiveness and robustness across 13 distinct Convolutional and Transformer architectures, covering five image classification tasks and one text generation task. The source codes are available at https://github.com/AIResearch-Group/NeuralMark. Index TermsNeural network watermarking, weight-based approach, hashed watermark filter, neural network ownership. I. INTRODUCTION The advancements in artificial intelligence have led to the development of numerous deep neural networks, particularly large language models [1][6]. Training such models requires investments in human resources, computational substantial power, and other resources, as exemplified by GPT-4, which costs around $40 million to train [7]. Thus, they can be regarded as valuable digital assets, necessitating urgent measures for ownership protection. To this end, neural network watermarking (NNW) approaches [8][12] have been proposed to protect model ownership by embedding watermarks within the neural network. Methods that require access to model parameters for watermark embedding and verification fall under white-box neural network watermarking (NNW) [13] [19], whereas those that do not require access to the model parameters belong to black-box NNW [20][26]. Both approaches have demonstrated significant progress in safeguarding model ownership [8] and hold promise for integration in practical applications [15], [27]. Given the distinct challenges Yuan Yao is with the Beijing Teleinfo Technology Company Ltd., China Academy of Information and Communications Technology, Beijing 100095, China. (e-mail: yaoyuan.hitsz@gmail.com) Jin Song is with the School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing 210023, China. (e-mail: jinsongresearch@outlook.com) Jian Jin is with the Research Institute of Industrial Internet of Things, China Academy of Information and Communications Technology, Beijing 100095, China. (e-mail: jin.jian@caict.ac.cn) Corresponding author: Jian Jin. inherent in white-box and black-box approaches, this article concentrates on white-box NNW, with black-box NNW left for future exploration. Existing white-box NNW approaches can be broadly categorized into three sub-branches: (i) Weight-based methods [13], [14], [18], [28], [29] embed watermarks into model parameters; (ii) Passport-based methods [15][17], [27] introduce passport layers to replace normalization layers for watermark embedding; and (iii) Activation-based methods [30][32] incorporate watermarks into the activation maps of intermediate layers. Among those methods, weight-based methods are appealing due to their inherent simplicity and practicality. By embedding watermarks into the models weights, those methods offer straightforward process that can be seamlessly integrated into various network architectures without altering the original structure. This feature renders them valuable for various practical applications. Although several state-of-theart weight-based methods [14], [18], [28], [29] can effectively resist fine-tuning and pruning attacks, they remain partially vulnerable to forging, overwriting, or both types of attacks. On one hand, forging attacks attempt to fabricate counterfeit watermarks and infer the corresponding secret key through reverse engineering, by freezing the model parameters. In this scenario, the adversary could claim the models ownership, resulting in ownership ambiguity. On the other hand, overwriting attacks aim to remove the original watermark by embedding counterfeit one. In particular, adversaries can adaptively increase the embedding strength of their watermarks without being required to match the original watermarks embedding strength. In such cases, the original watermark may be removed while the adversarys watermark is embedded, leading to the invalidation of the models ownership. This raises question: How can we design more robust and effective weighted-based method that defends against both forging and overwriting attacks? To address this challenge, we propose NeuralMark, robust weighted-based method centered on hashed watermark filter. Specifically, we use hash function to generate an irreversible binary watermark from secret key, which is then employed as filter to select the model parameters for embedding. The avalanche effect of the hash function [33] ensures that even slight changes in the input lead to significant, unpredictable variations in the output, effectively impeding gradient calculation and making reverse engineering infeasible. Moreover, because the hashed watermarks generated by the model owner and the adversary are distinct, using them as private filters reduces the overlap in selected parameters, especially when the filtering process is performed repeatedly. This mechanism significantly increases the difficulty for adversaries to identify thereby protecting and manipulate the filtered parameters, the hashed watermark the original watermark. Therefore, filter cleverly intertwines the embedding parameters with the hashed watermark, providing robust defense against both forging and overwriting attacks. Furthermore, we also apply an average pooling mechanism to the filtered parameters due to its resilience against fine-tuning and pruning attacks. Upon obtaining the resulting parameters, the hashed watermark is embedded into those parameters using lightweight watermarking embedding loss, ensuring no degradation in model performance. During verification, the embedded watermark can be extracted from the models parameters to verify model ownership. The main contributions of this article are highlighted below. We propose NeuralMark, weight-based method designed to safeguard model ownership. Also, we provide theoretical analysis of its security boundary. In NeuralMark, an elegant hashed watermark filter is developed to defend against both forging and overwriting attacks. Extensive experimental results across 13 distinct Convolutional and Transformer architectures, covering five image classification tasks and one text generation task, verify the effectiveness and robustness of NeuralMark. The remainder of this article is organized as follows. We review related work in Section II. Section III introduces the preliminary, and Section IV presents the threat model. NeuralMark is detailed in Section V. Extensive experiments in Section VI evaluate its effectiveness and robustness. Finally, we make conclusions in Section VII. II. RELATED WORK In this section, we review weight-based, passport-based, and activation-based methods, respectively. A. Weight-based Method This kind of methods [13], [14], [18], [28], [29] embeds watermarks into the model parameters of neural networks. For instance, [13] proposes the first weight-based method, which embeds the watermark into the model parameters of an intermediate layer in the neural network. Another example is [29], which presents method based on spread transform dither modulation that enhances the secrecy of the watermark. However, those two methods cannot effectively resist forging and overwriting attacks. Moreover, [28] utilizes the secret keys to pseudo-randomly select parameters for watermark embedding and apply spread-spectrum modulation to disperse the modulated watermark across different layers. This method effectively defends overwriting attacks while neglecting forging attacks. Additionally, [14] proposes to greedily choose important model parameters for watermark embedding without an additional secret key. Although this method is effective against forging attacks, it fails to provide strong resistance to overwriting attacks of varying strength levels. Recently, [18] introduces random noises into the watermarked parameters and then employs majority voting scheme to aggregate the verification results across multiple rounds. While this method enhances the watermarks robustness to some extent, it remains ineffective against forging and overwriting attacks. 2 B. Passport-based Method This group of methods [15][17], [27] integrates the watermark into the normalization layers in neural networks. Specifically, [15], [27] propose the first passport-based method, which utilizes additional passport samples (e.g., images) to generate affine transformation parameters for the normalization layers, tightly binding them to the model performance. Subsequently, [16] integrates private passport-aware branch into the normalization layers, which is trained jointly with the target model and is used solely for watermark verification. Recently, [17] argues that binding the model performance is insufficient to defend against forging attacks, and thus proposes establishing hash mapping between passport samples and watermarks. C. Activation-based Method This category of methods [30][32] incorporates watermarks into the activation maps of intermediate layers in neural networks. For instance, [30] incorporates the watermark into the mean vector of activation maps generated by predetermined trigger samples. Similarly, [31] directly integrates the watermark into the activation maps associated with the trigger samples. Additionally, [32] embeds the watermark into the hidden memory state of recurrent neural network. III. PRELIMINARY In this section, we elaborate on the principle and vulnerability of the first weight-based method [13], which we refer to as VanillaMark, serving as the foundation for subsequent weight-based watermarking methods [14], [18], [28], [29]. Specifically, it begins by selecting, averaging, and flattening subset of model parameters θ into parameter vector (cid:101)w Rk. secret key matrix Rkn is then used to derive the extracted watermark via (cid:101)b = δ( (cid:101)wK), where δ() denotes the sigmoid function. To embed target binary watermark b1 into (cid:101)w, VanillaMark optimizes the following objective: min θ Lm + λLe((cid:101)b, b), (1) where Lm denotes the main task loss (e.g., classification loss), Le(, ) represents the binary cross-entropy loss, and λ is positive trade-off hyper-parameter. Although VanillaMark is simple and pioneering, and can resist fine-tuning and pruning attacks [13], it remains vulnerable to the following two attacks: Forging Attack: An adversary can learn the secret key for any arbitrary watermark. Specifically, given counterfeit watermark ba, the attacker can learn corresponding key Ka by minimizing Le(ba, (cid:101)ba) while keeping the model parameters frozen, i.e., Ka = arg minKa Le(ba, (cid:101)ba). Overwriting Attack: VanillaMark neither protects the confidentiality of watermarked parameters nor ensures nonoverlapping usage between the model owners and the adversarys parameters. Once the watermarked parameters are identified, an adversary can forge counterfeit watermark tuple {Ka, ba} and embed ba into those parameters by optimizing the objective function: minθ Lm + λLe(ba, (cid:101)b). 1A watermark in this article refers to binary vector of ones and zeros. (a) (c) (b) (d) Fig. 1. Illustrations of different types of attacks. (a) Forging attack: the adversary aims to generate counterfeit secret keywatermark pair without modifying the model parameters. (b) Overwriting attack: the adversary embeds counterfeit watermark to overwrite the original one. (c) Fine-tuning attack: the adversary fine-tunes the model in an attempt to remove the original watermark. (d) Pruning attack: the adversary prunes the model parameters to remove the original watermark. In all cases, P(M[θ]) denotes the performance of the model parameterized by θ. Since different watermarks often induce conflicting gradients on the same parameters, the newly embedded watermark can easily overwrite the original one. IV. THREAT MODEL In this section, we present the threat model considered in this work, detailing the adversarys capabilities and the corresponding success criteria. A. Adversary Capabilities We assume that an adversary can illegally obtain watermarked model, identify the layers containing the watermark, and obtain the original training datasets, but is limited in computational resources. This constraint is reasonable, as an attacker with sufficient computational resources could simply train new model from scratch, making model theft unnecessary. As discussed above, this work primarily focuses on forging and overwriting attacks, while also considering finetuning and pruning attacks. The specific threat scenarios are detailed as follows. (1) Forging Attack: As illustrated in Fig. 1a, in forging attack, the adversary aims to generate counterfeit secret keywatermark pair without modifying the model parameters. Specifically, the adversary first randomly forges counterfeit watermark and then derives corresponding secret key by optimizing it while keeping the model parameters frozen [15], [27]. (2) Overwriting Attack: As presented in Fig. 1b, in an overwriting attack, the adversary attempts to embed counterfeit watermark to overwrite the original one [14]. (3) Fine-tuning Attack: As depicted in Fig. 1c, in fine-tuning attack, the adversary fine-tunes the model in an attempt to remove the original watermark. (4) Pruning Attack: As shown in Fig. 1d, in pruning attack, the adversary attempts to remove the original watermark by pruning the model parameters. B. Attack Success Criteria Building on insights from [15], [27], [34], [35], successful attack on watermarked model typically requires the adversary to either (i) forge counterfeit watermark without altering the model parameters, or (ii) remove the original watermark through parameter modifications, all while preserving model performance. If the adversary only embeds counterfeit watermark without removing the original one, the resulting the model owner can model contains both. In this case, submit version containing only the original watermark to an authoritative third-party for verification. In contrast, the adversary cannot provide model with only the counterfeit watermark, as the original watermark remains intact. As the adversary cannot convincingly claim ownership result, unless they train new model embedded solely with their own watermark. This not only makes stealing the original model unnecessary but also incurs significant training costs. Accordingly, we define the success criteria for each type of attack as follows. (1) Success Criteria for Forging Attack: Forge counterfeit watermark that passes verification without modifying the model parameters. (2) Success Criteria for Overwriting Attack: Remove the original watermark and embed counterfeit one by modifying the model parameters, 4 Fig. 2. Illustration of hashed watermark filter. Here, the model owners hashed watermark is [1, 0, 1, 0], while the adversarys is [0, 1, 1, 0]. Without filtering, all 16 parameters overlap. After one round of filtering, each retains eight parameters, with four overlapping. second round leaves four parameters each, with no overlap. while maintaining model performance. (3) Success Criteria for Fine-tuning Attack: Remove the original watermark through fine-tuning, while maintaining model performance. (4) Success Criteria for Pruning Attack: Remove the original watermark through parameter pruning, while maintaining model performance. V. METHODOLOGY In this section, we present NeuralMark, weight-based method designed to protect model ownership. The objective is to train watermarked model M(θ) on given training dataset such that the model parameters θ embed binary watermark while satisfying the following criteria: (i) the watermark imposes negligible impact on the model performance and remains difficult for adversaries to detect; and (ii) the embedded watermark exhibits robustness against the adversarial attacks defined in Section IV. A. Motivation As discussed in Section III, most weight-based methods struggle to simultaneously defend against both forging and overwriting attacks. On the one hand, forging attacks aim to generate counterfeit watermark and derive the corresponding secret key via gradient backpropagation, while keeping the model parameters fixed. Defending against such attacks requires disrupting gradient computation to hinder reverseengineering. On the other hand, overwriting attacks attempt to remove the original watermark by embedding counterthe feit one. Once watermarked parameters are identified, adversary can overwrite the original watermark. Since each watermark updates the model parameters in distinct and often conflicting direction, embedding new watermark can easily disrupt the original one. Therefore, to resist such attacks, it is essential to keep the watermarked parameters confidential and ensure that those used by the model owner and the adversary are non-overlapping. To address both threats, we propose hashed watermark filter that leverages an irreversible binary watermark as private filter to restrict embedding to secret subset of model parameters. Specifically, we utilize hash function to generate an irreversible binary watermark from secret key, which is then applied as filter to select the model parameters for embedding. This design provides two key properties: Gradient Obfuscation: The avalanche effect of the hash function ensures that even minor changes in the input lead to large, unpredictable changes in the output, effectively impeding gradient computation and rendering reverseengineering infeasible. Embedding Isolation: Since the hashed watermarks derived by the model owner and the adversary are inherently distinct, using them as private filters can effectively reduce the overlap in selected parameters, especially when the filtering process is performed repeatedly. As exemplified in Fig. 2, the model owners hashed watermark is [1, 0, 1, 0], while the adversarys is [0, 1, 1, 0]. Without filtering, all 16 model parameters are shared, yielding 100% overlap ratio. After the first round of filtering, each party retains eight parameters, with four overlapping, reducing the overlap to 50%. second filtering round results in four parameters per party, with zero overlap, achieving 0% overlap ratio. This progressive isolation ensures that as filtering continues, the overlap between the model owners and the adversarys selected parameters is significantly reduced. Thus, it becomes increasingly difficult for the adversary to identify and manipulate the owners watermarked parameters, even when increasing the embedding strength of their watermarks, thereby preserving the integrity of the original watermark against overwriting attacks. In summary, those properties enable the hashed watermark filter to tightly entangle the embedding parameters with the hashed watermark, providing strong resistance against both forging and overwriting attacks. This mechanism forms the core of NeuralMark, which we will elaborate on next. B. NeuralMark As depicted in Fig. 3, NeuralMark consists of three primary steps: (i) hashed watermark generation; (ii) watermark embedding; and (iii) watermark verification. Next, we detail how each step works. 1) Hashed Watermark Generation: As aforementioned, we construct hash-based mapping from secret key to binary watermark, as shown in Fig. 3a. Formally, the watermark {0, 1}n is generated by = H(K), where Rkn is secret key matrix with elements drawn from random distribution (e.g., normalized Gaussian distribution), H() denotes hash function, and indicates the length of the 5 (a) (b) (c) Fig. 3. Illustrations of the processes for watermark generation (a), embedding (b), and verification (c). watermark. To accommodate various application requirements, we adopt SHAKE-256 [36], an extendable-output function from the SHA-3 family that allows dynamic adjustment of output length. Furthermore, auxiliary content (e.g., textual descriptors or unique identifiers) can also be incorporated into the hash function, yielding = H(KC), where denotes the concatenation operation. This mechanism enables context-aware watermark generation without compromising the avalanche effect of the hash function. For simplicity, we omit auxiliary content in the subsequent experiments. 2) Watermark Embedding: As illustrated in Fig. 3b, to embed the hashed watermark into the model M(θ), we first select and flatten subset of parameters (e.g., one-layer parameters) from θ into parameter vector Rm. Then, we utilize the hashed watermark filter to select the model parameters for embedding. Specifically, let w(0) = be the initial parameter vector. In the r-th (r {1, , R}) filtering round, the watermark is repeated to match the length of w(r1), forming b(r), with any excess parameters in w(r1) discarded. The parameter vector w(r) is constructed by selecting the elements from w(r1) at positions where b(r) equals = 1}(cid:3), where w(r1) one, i.e., w(r) = (cid:2)w(r1) {j b(r) is the i-th element of w(r1), and b(r) is the j-th element of b(r). After completing the whole watermark filtering process, the filtered parameter vector w(R) is obtained. Next, we adopt the average pooling AVG() operation [37] to calculate the final parameters as (cid:101)w = AVG(w(R)) Rk. This operation aggregates parameters across broader regions, thereby enhancing robustness against parameter perturbations caused by finetuning and pruning attacks. Finally, we formulate the overall optimal objective as min θ Lm + λLe((cid:101)b, b), (2) where Lm denotes the main task loss (e.g., classification loss), Le(, ) represents the binary cross-binary loss, (cid:101)b = δ( (cid:101)wK) denotes the extracted watermark, with δ() being the sigmoid function, and λ is positive trade-off hyper-parameter. By minimizing Eq. (2), the watermark can be embedded into model parameters during the main task training. The watermark embedding process is summarized in Algorithm 1. 3) Watermark Verification: The watermark verification process is similar to the embedding process, as depicted in Fig. 3c. Concretely, upon identifying potentially unauthorized model, Algorithm 1 Watermark Embedding in NeuralMark Input: Training dataset D, secret key K, index of embedding layer Ie, hyper-parameters λ, , and filter rounds R. Output: Watermarked model M(θ). Randomly initialize the model parameter θ. Generate the watermark = H(K). for = 0 to 1 do Use Ie to select subset from θ and flatten it into w. for = 1 to do Perform watermark filtering on to obtain w(r). end for Apply average pooling on w(R) to yield (cid:101)w. Execute sigmoid mapping on (cid:101)wK to produce (cid:101)b. Update θ based on Eq. (2). end for the relevant subset of model parameters is extracted and subjected to hashed watermark filtering and average pooling to derive an extracted watermark (cid:101)b. This extracted watermark is then compared to the model owners watermark using the watermark detection rate, which is defined by ρ = 1 (cid:88) i= 1(cid:2)bi = ((cid:101)bi)(cid:3), (3) where (x) is threshold function that outputs 1 if > 0.5 and 0 otherwise, and 1(ψ) is an indicator function that returns 1 if ψ is true and 0 otherwise. The unauthorized model is confirmed to belong to the model owner if both of the following conditions are satisfied: (1) The watermark detection rate ρ exceeds theoretical security boundary ρ, which will be theoretically analyzed later. (2) The watermark must correspond to the output of the hash function applied to the secret key, ensuring cryptographic consistency with the predefined hash function. The watermark verification process is outlined in Algorithm 2. C. Theoretical Analysis We present theoretical analysis to determine the security boundary of NeuralMark in Proposition V.1. Proposition V.1. Under the assumption that the hash function for model produces uniformly distributed outputs [38], Algorithm 2 Watermark Verification in NeuralMark Input: Watermarked model M(θ), secret key K, watermark b, index of embedding layer Ie, filter rounds R, and security boundary ρ. Output: True (Verification Success) or False (Verification Failure). Use Ie to select subset from θ and flatten it to create w. for = 1 to do Perform watermark filtering on to obtain w(r). end for Apply average pooling on w(R) to yield (cid:101)w. Execute sigmoid mapping on (cid:101)wK to produce (cid:101)b. Calculate watermark detection rate ρ based on Eq. (3). if ρ ρ and H(K) = then return True else return False end if watermarked by NeuralMark with watermark tuple {K, b}, where = H(K), if an adversary attempts to forge counterfeit watermark tuple {K, b} such that = H(K) and = K, then the probability of achieving watermark detection rate of at least ρ (i.e., ρ) is upper-bounded by 1 2n (cid:80)nρn i= (cid:0)n (cid:1). The proof of Proposition V.1 is provided in Section A. Proposition V.1 provides theoretical benchmark for establishing the security boundary of the watermark detection rate. Specifically, with = 256, if the watermark detection rate ρ 88.28%, the probability of this occurring by forgery is less than 1/2128. This negligible probability allows us to confirm ownership with high confidence. Thus, we set = 256 and use 88.28% as the security bound for the watermark detection rate in the experiments. VI. EXPERIMENTS In this section, we evaluate NeuralMark across variety of datasets, architectures, and tasks. A. Experimental Setup Datasets and Architectures. We use five image classification datasets: CIFAR-10, CIFAR-100 [39], Caltech-101 [40], Caltech-256 [41], and TinyImageNet [42], as well as one text generation dataset, E2E [43]. Additionally, we utilize 11 image classification architectures, including eight Convolutional architectures: AlexNet [44], VGG-13, VGG-16 [45], GoogLeNet [46], ResNet-18, ResNet-34 [47], WideResNet-50 [48], and MobileNet-V3-L [49], as well as three Transformer architectures: ViT-B/16 [50], Swin-V2-B, and Swin-V2-S [51]. Furthermore, we adopt two text generation architectures: GPT2-S and GPT-2-M [52]. Baselines and Metrics. We compare NeuralMark with VanillaMark [13], and two state-of-the-art weight-based methods proposed in [14] and [18] (see Section II for details). For clarity, we refer to those two methods as GreedyMark and VoteMark, respectively. Additionally, we include comparison 6 with method that does not involve watermark embedding, referred to as Clean. For the image classification task, we evaluate model performance using classification accuracy, while the watermark embedding task is assessed based on the watermark detection rate. As for the text generation task, we follow [53] and evaluate model performance using BLEU, NIST, MET, ROUGE-L, and CIDEr metrics, with the watermark embedding task assessed based on the watermark detection rate. Implementation Details. We implement NeuralMark using the PyTorch framework [54] and conduct all experiments on three NVIDIA V100 series GPUs. The specific hyperparameters are summarized below. For all the image classification architectures, we train for 200 epochs with multi-step learning rate schedule from scratch, with learning rates set to 0.01, 0.001, and 0.0001 for epochs 1 to 100, 101 to 150, and 151 to 200, respectively. We apply weight decay of 5104 and set the momentum to 0.9. The batch sizes for the training and test datasets are set to 64 and 128, respectively. In addition, we set hyperparameter λ to 1 and the number of filter rounds to 4. For the GPT-2-S and GPT-2-M architectures, we use the Low-Rank Adaptation (LoRA) technique [53]. Each architecture is trained for 5 epochs with linear learning rate scheduler, starting at 2 104. We set the warm-up steps to 500, apply weight decay with coefficient of 0.01, and enable bias correction in the AdamW optimizer [55]. The dimension and the scaling factor for LoRA are set to 4 and 32, respectively, with dropout probability of 0.1 for the LoRA layers. The batch sizes for the training and test sets are 8 and 4, respectively. Moreover, we set hyper-parameter λ to 1 and the number of filter rounds to 10. B. Fidelity Evaluation Question 1. Is NeuralMark capable of reliably embedding watermarks while preserving model performance across variety of datasets, architectures, and tasks? Diverse Datasets. First, we evaluate the influence of watermark embedding on the model performance across diverse datasets. Table reports the results across five image datasets using AlexNet and ResNet-18. We observe that all methods have minimal impact on model performance while successfully embedding watermarks, indicating that NeuralMark and other methods maintain model performance across diverse datasets during watermark embedding. Various Architectures. Next, we assess the impact of NeuralMark on model performance across various architectures. Table II lists the results of NeuralMark on the CIFAR100 dataset using VGG-13, VGG-16, GoogLeNet, ResNet34, WideResNet-50, MobileNet-V3-L, ViT-B/16, Swin-V2-B, and Swin-V2-S. We find that NeuralMark maintains 100% watermark detection rate across wide range of architectures while exerting minimal impact on model performance. Those observations suggest that NeuralMark is highly generalizable across various architectures. 7 TABLE COMPARISON OF CLASSIFICATION ACCURACY (%) ACROSS DISTINCT DATASETS USING ALEXNET AND RESNET-18. WATERMARK DETECTION RATES ARE OMITTED AS THEY ALL REACH 100%. Dataset CIFAR-10 CIFAR-100 Caltech-101 Caltech-256 TinyImageNet Clean NeuralMark VanillaMark GreedyMark VoteMark AlexNet ResNet-18 AlexNet ResNet-18 AlexNet ResNet-18 AlexNet ResNet-18 AlexNet ResNet-18 91.05 68.24 68.07 44.27 42. 94.76 76.23 68.83 54.09 53.48 90.93 68.57 68.38 44.55 42.31 94.50 76.34 68.47 53.71 53.22 91.01 68.43 68.54 44.73 42.50 94.87 76.22 68.99 53.47 53.36 90.88 68.31 68.59 44.64 42. 94.69 76.14 69.08 53.28 53.31 90.86 68.53 68.88 44.43 42.50 94.79 76.74 67.91 54.71 53.47 TABLE II COMPARISON OF CLASSIFICATION ACCURACY (%) ON CIFAR-100 USING VARIOUS ARCHITECTURES. WATERMARK DETECTION RATES ARE OMITTED AS THEY ALL REACH 100%. Method ViT-B/16 Swin-V2-B Swin-V2-S VGG-16 VGG-13 ResNet-34 WideResNet-50 GoogLeNet MobileNet-V3-L Clean NeuralMark 39.07 39.22 52.99 53.57 55.88 55.87 72.75 72.61 72.71 71. 77.06 77.03 59.67 58.41 60.71 60.02 61.11 61.8 TABLE III COMPARISON ON E2E USING GPT-2-S AND GPT-2-M. WATERMARK DETECTION RATES ARE OMITTED AS THEY ALL REACH 100%. GPT-2-S BLEU NIST Clean NeuralMark 69.36 69.59 8.76 8.79 MET 46.06 46.01 ROUGE-L CIDEr GPT-2-M BLEU NIST 70.85 70.85 2.48 2.48 Clean NeuralMark 68.7 67.73 8.69 8.57 MET 46.38 46.07 ROUGE-L CIDEr 71.19 70.66 2.5 2.47 Text Generation Tasks. Finally, we evaluate the effect of NeuralMark on the text generation tasks. Table III presents the results of NeuralMark applied to the GPT-2-S and GPT2-M architectures on the E2E dataset. We can observe that NeuralMark achieves 100% watermark detection rate while maintaining nearly lossless model performance. Those results validate the potential of NeuralMark in safeguarding the ownership of generative models. C. Robustness Evaluation Question 2. Is NeuralMark capable of withstanding forging attacks? We adopt the setting detailed in Section IV-A to assess the robustness of NeuralMark against forging attacks. Concretely, for VanillaMark and VoteMark, we first randomly generate counterfeit watermark and then learn the corresponding secret key by freezing the model parameters. Since GreedyMark does not require secret key, we utilize 10 sets of randomly forged watermarks to directly verify them using the watermarked model. For NeuralMark, due to the avalanche effect of hash functions, method similar to GreedyMark is employed, where 10 sets of randomly forged watermarks are directly verified using the watermarked model. Table IV presents the watermark detection rates of forging attacks, and we present the following significant observations. (1) For VanillaMark and VoteMark, pair of counterfeited secret key and watermark can be successfully learned through reverse-engineering, indicating their vulnerability to forging attacks. (2) NeuralMark and GreedyMark demonstrate robust resistance against forging attacks, which aligns with our expectations. Those results suggest that NeuralMark effectively withstands forging attacks. TABLE IV COMPARISON OF WATERMARK DETECTION RATE (%) AGAINST FORGING ATTACKS USING RESNET-18. Dataset NeuralMark VanillaMark GreedyMark VoteMark CIFAR-10 CIFAR48.56 49.41 100.00 100.00 50.70 52.85 100.00 100.00 Question 3. Is NeuralMark robust against overwriting attacks targeting watermark-specific layers, especially under varying attack strength levels? We follow the setting outlined in Section IV-A to assess the robustness of NeuralMark against overwriting attacks. Specifically, we perform overwriting attacks targeting watermarkspecific layers, with the number of training epochs set to 100 to simulate limited computational resources, and analyze the impact of two key factors: the hyper-parameter λ in Eq. (2) and the learning rate η. Here, λ controls the strength of the watermark embedding, with larger values leading to stronger embedding, while η primarily affects model performance. Distinct Values of λ. First, we investigate the influence of λ in overwriting attacks. Specifically, we set λ to 1, 10, 50, 100, and 1000, respectively. Table presents the results on the CIFAR-100 to CIFAR-10 and CIFAR-10 to CIFAR-100 tasks using ResNet-18. We report only the original watermark detection rate, as the adversarys watermark detection rate reaches 100%. As defined in the success criterion in Section IV-B, the original watermark must be effectively removed for overwriting attacks to be deemed successful. Thus, the overwriting attack experiments focus solely on whether the original watermark can be successfully removed. We can summarize several insightful observations. (1) As λ increases, TABLE COMPARISON OF RESISTANCE TO OVERWRITING ATTACKS AT VARIOUS TRADE-OFF HYPER-PARAMETERS (λ) AND LEARNING RATES (η) USING RESNET-18. VALUES (%) INSIDE AND OUTSIDE THE BRACKET ARE WATERMARK DETECTION RATE AND CLASSIFICATION ACCURACY, RESPECTIVELY. Overwriting λ NeuralMark VanillaMark GreedyMark VoteMark η NeuralMark VanillaMark GreedyMark VoteMark 8 CIFAR-100 to CIFAR-10 CIFAR-10 to CIFAR-100 93.65 (100) 93.44 (100) 93.46 (100) 1 93.30 (100) 93.45 (48.82) 93.63 (100) 0.001 93.65 (100) 93.30 (100) 93.45 (48.82) 93.63 (100) 93.58 (100) 93.29 (51.17) 93.13 (100) 0.005 91.76 (99.60) 92.17 (73.04) 92.13 (50.00) 92.45 (78.90) 10 0.01 91.58 (92.18) 91.79 (62.10) 91.53 (49.60) 91.76 (60.15) 93.50 (100) 93.07 (55.07) 93.39 (100) 50 75.2 (50.78) 79.68 (47.26) 72.42 (53.12) 70.92 (54.29) 0.1 100 93.53 (100) 92.95 (94.53) 93.18 (54.29) 93.53 (96.48) 10.00 (44.53) 10.00 (53.51) 10.00 (48.04) 10.00 (53.51) 1 1000 93.09 (100) 92.89 (53.90) 92.85 (49.60) 92.77 (59.37) 1 10 50 100 71.49 (100) 71.92 (92.18) 72.05 (48.04) 72.72 (93.75) 1000 71.81 (100) 71.35 (57.42) 71.74 (51.95) 70.73 (56.64) 71.78 (100) 72.68 (98.82) 71.34 (55.07) 72.97 (98.43) 0.001 71.78 (100) 72.68 (98.82) 71.34 (55.07) 72.97 (98.43) 72.6 (100) 72.03 (98.04) 72.30 (49.21) 72.08 (98.04) 0.005 71.04 (99.60) 70.02 (69.53) 70.25 (48.04) 71.11 (71.09) 72.73 (100) 72.45 (95.70) 70.92 (46.87) 72.38 (97.26) 0.01 69.14 (96.48) 69.02 (59.76) 69.25 (46.09) 68.88 (62.11) 51.88 (60.54) 51.76 (53.90) 51.71 (51.56) 51.74 (56.25) 0.1 1.00 (53.51) 1.00 (44.53) 1 1.00 (53.15) 1.00 (50.00) TABLE VI COMPARISON OF RESISTANCE TO FINE-TUNING ATTACKS. VALUES (%) INSIDE AND OUTSIDE THE BRACKET ARE THE WATERMARK DETECTION RATE AND CLASSIFICATION ACCURACY, RESPECTIVELY. Fine-tuning AlexNet ResNet-18 AlexNet ResNet-18 AlexNet ResNet-18 AlexNet ResNetAlexNet ResNet-18 Clean NeuralMark VanillaMark GreedyMark VoteMark CIFAR-100 to CIFAR-10 CIFAR-10 to CIFAR-100 Caltech-256 to Caltech-101 Caltech-101 to Caltech-256 89.44 65.46 72.69 43.39 93.21 72.17 76.93 46.48 89.11(100) 93.74(100) 89.00(100) 93.59(100) 64.60(100) 71.67(100) 65.03(92.18) 72.49(97.26) 64.57(98.82) 72.06(100) 64.83(96.09) 72.27(98.04) 73.55(100) 76.60(100) 72.90(100) 77.41(100) 43.15(100) 44.42(100) 43.21(98.43) 45.69(99.60) 43.47(99.60) 45.25(100) 43.78(98.43) 45.29(100) 93.29(100) 89.34(99.21) 93.21(100) 89.03(100) 73.12(100) 77.19(100) 72.90(100) 78.48(100) the original watermark detection rate of NeuralMark remains at 100%, while those of VanillaMark, GreedyMark, and VoteMark significantly decline. In particular, when λ = 1000, the embedding strength of the adversarys watermark is 1000 times greater than that of the original watermark. At this point, the original watermark detection rates for NeuralMark, VanillaMark, GreedyMark, and VoteMark on the CIFAR-100 to CIFAR-10 tasks are 100%, 53.90%, 49.60%, and 59.37%, respectively. Those results indicate that NeuralMark exhibits strong robustness against overwriting attacks. (2) As λ increases, model performance remains relatively stable. This is because overwriting attacks jointly train both the main task and the watermark embedding task, enabling the model parameters to effectively adapt to both. Distinct Values of η. Second, we examine the impact of η in overwriting attacks. Concretely, we set η to 0.001, 0.005, 0.01, 0.1, and 1, respectively. Table lists the results on the CIFAR-100 to CIFAR-10 and CIFAR-10 to CIFAR100 tasks using ResNet-18. We have the following important observations. (1) As η increases, model performance declines due to its substantial impact on performance. Thus, the adversary cannot arbitrarily increase η to strengthen the attack. (2) At η = 0.005, the original watermark detection rates for VanillaMark, GreedyMark, and VoteMark drop dramatically, whereas NeuralMark maintains detection rate close to 100%. (3) When η = 0.01, the model performance of NeuralMark on the CIFAR-100 to CIFAR-10 task decreases by 2.07%, but its original watermark detection rate remains above the security boundary of 88.28% defined in Section V-C, while those for the other methods fall significantly. (4) For η >= 0.1, although the original watermark detection rate of NeuralMark drops below the security boundary, the model performance is completely compromised, indicating that the attack is ineffective. Question 4. Is NeuralMark robust to fine-tuning attacks? We follow the setting stated in Section IV-A to evaluate the robustness of NeuralMark against fine-tuning attacks, with training limited to 100 epochs to reflect realistic computational constraints. Fine-tuning All Model Parameters. Following [14], we adopt the same hyper-parameters for fine-tuning attacks as during training, except for setting the learning rate to 0.001. Also, we replace the task-specific classifier with randomly initialized parameters and optimize all parameters by only minimizing the main task loss Lm. Table VI reports the results of fine-tuning attacks, we find that watermarks embedded with NeuralMark maintain 100% watermark detection rate across all fine-tuning tasks. In contrast, watermarks embedded with VanillaMark, GreedyMark, and VoteMark experience slight reduction in detection rates across several tasks. Those results indicate that fine-tuning attacks cannot effectively remove watermarks embedded with NeuralMark. Fine-tuning Watermark-Specific Layer Parameters. Furthermore, we follow the aforementioned settings to perform fine-tuning attacks on the watermark-specific layers and the classifier, while keeping all other parameters frozen. As shown in Table VII, we can observe that the watermark detection rate remains at 100%, but the model performance of all methods exhibits substantial decline. Specifically, for the CIFAR-10 to CIFAR-100 task using ResNet-18, the accuracy achieved by NeuralMark is 49.77%, which is markedly lower than the 71.67% accuracy obtained when all parameters are finetuned. Similar trends are observed across other methods. Those results indicate that fine-tuning only the watermark embedding layer and classifier makes it challenging to maintain effective model performance, failing fine-tuning attacks. TABLE VII COMPARISON OF RESISTANCE TO FINE-TUNING ATTACKS AGAINST WATERMARK EMBEDDING LAYER USING RESNET-18. VALUES (%) INSIDE AND OUTSIDE THE BRACKET ARE THE WATERMARK DETECTION RATE AND CLASSIFICATION ACCURACY, RESPECTIVELY. Fine-tuning AlexNet ResNet-18 AlexNet ResNet-18 AlexNet ResNet-18 AlexNet ResNet-18 AlexNet ResNet-18 Clean NeuralMark VanillaMark GreedyMark VoteMark 9 CIFAR-100 to CIFAR-10 CIFAR-10 to CIFAR-100 Caltech-256 to Caltech-101 Caltech-101 to Caltech85.55 58.96 47.65 40.61 89.15 49.74 74.09 40.00 85.35(100) 88.83(100) 85.48(91.01) 89.35(85.93) 80.41(96.48) 76.15(94.14) 84.97(89.06) 89.66(85.54) 58.50(100) 49.77(100) 58.75(74.21) 49.97(70.31) 51.75(97.65) 19.94(82.42) 58.81(80.07) 49.08(71.87) 71.29(100) 73.12(100) 71.56(100) 71.62(100) 72.47(99.60) 40.34(100) 40.34(100) 40.71(96.09) 39.04(93.36) 40.68(100) 36.45(98.82) 39.52(95.31) 39.73(93.75) 72.04(100) 68.45(100) 74.03(100) (a) (b) (c) (d) Fig. 4. Comparison of resistance to pruning attacks under various pruning ratios on CIFAR-10 using AlexNet and ResNet-18: (a) NeuralMark, (b) VanillaMark, (c) GreedyMark, and (d) VoteMark. Question 5. Is NeuralMark robust to pruning attacks? We now verify the robustness of NeuralMark in resisting pruning attacks. Specifically, we randomly reset specified proportion of model parameters in the watermark embedding layer to zero. Fig. 4 shows the results of pruning attacks on the CIFAR-10 dataset using AlexNet and ResNet, respectively. As can be seen, as the pruning ratio increases, the performance of NeuralMark degrades while the detection rate remains nearly 100%. This indicates NeuralMarks robustness against pruning attacks. Moreover, we observe that both VanillaMark and VoteMark exhibit strong resistance to pruning attacks, while GreedyMark demonstrates relatively weak resistance. One possible reason is that GreedyMark depends on several important parameters, and their removal may affect its robustness. More results of pruning attacks across distinct datasets are provided in Appendix B. All the results suggest that NeuralMark effectively resists pruning attacks. D. Hashed Watermark Filter Analysis Question 6. How does the number of filtering rounds impact the fidelity of NeuralMark? To evaluate the impact of filtering rounds on NeuralMarks fidelity, we conduct experiments with 6 and 8 filters, compared to the default 4 filters. Table VIII presents the impact of watermark embedding on the model performance across distinct filtering rounds. The results demonstrate that NeuralMark, even with varying filtering rounds, has minimal effect on the model performance while successfully embedding watermarks. Question 7. How does the number of filtering rounds influence the robustness of NeuralMark? TABLE VIII COMPARISON OF CLASSIFICATION ACCURACY (%) WITH VARIOUS DISTINCT FILTER ROUNDS ON CIFAR-10 AND CIFAR-100 USING RESNET-18. WATERMARK DETECTION RATES ARE OMITTED AS THEY ALL REACH 100%. Dataset 4 Filters 6 Filters 8 Filters CIFAR-10 CIFAR-100 94.79 76. 94.74 75.59 94.88 76.16 To assess the influence of the number of filtering rounds on NeuralMarks robustness, we conduct experiments with 6 and 8 filters, compared to the default 4 filters. We omit forging attacks, as the hashed watermark filter is resilient to them. Overwriting Attacks. Table IX lists the results of overwriting attacks across distinct filtering rounds. From the results, we find that when the number of filtering rounds is set to 6, NeuralMark exhibits superior robustness compared to 4 and 8 filter rounds. Specifically, at η = 0.01, the original watermark detection rates for 4, 6, and 8 filter rounds on the CIFAR100 to CIFAR-10 task are 92.18%, 94.92%, and 89.84%, respectively. Those results indicate that increasing the number of filtering rounds can enhance robustness against overwriting attacks to certain extent. However, when the number of filtering rounds exceeds certain threshold, the robustness may be slightly compromised due to the reduction in the number of parameters. Moreover, we observe the same trend as shown in Table V, when η >= 0.1, the original watermark detection rate of NeuralMark falls below the security boundary, but the model performance is significantly impaired, demonstrating the attack is not effective. Fine-tuning Attacks. Table lists the results of fine-tuning attacks across distinct filtering rounds. We find that NeuralMark maintains watermark detection rate of 100% across all cases, with negligible impact on the model performance. Pruning Attacks. Fig. 5 shows the results of pruning TABLE IX COMPARISON OF RESISTANCE TO OVERWRITING ATTACKS AT VARIOUS TRADE-OFF HYPER-PARAMETERS (λ) AND LEARNING RATES (η) WITH DISTINCT FILTERING ROUNDS USING RESNET-18. VALUES (%) INSIDE AND OUTSIDE THE BRACKET ARE WATERMARK DETECTION RATE AND CLASSIFICATION ACCURACY, RESPECTIVELY. Overwriting λ 4 Filters 6 Filters 8 Filters η 4 Filters 6 Filters 8 Filters 10 CIFAR-100 to CIFARCIFAR-10 to CIFAR-100 93.65 (100) 93.13(100) 93.40(100) 0.001 93.65 (100) 1 93.44 (100) 93.06(100) 93.41(100) 0.005 91.76 (99.60) 10 93.46 (100) 93.06(100) 93.54(100) 50 100 93.53 (100) 92.88(100) 92.99(100) 1000 93.09 (100) 93.03(100) 93.39(100) 93.40(100) 93.13(100) 91.62(100) 92.10(100) 91.58 (92.18) 91.64(94.92) 90.48(89.84) 75.2 (50.78) 75.84(58.2) 74.54(51.56) 10.00 (44.53) 10.00(47.26) 10.00(50.39) 0.01 0.1 1 71.78 (100) 71.69(100) 72.63(100) 0.001 71.78 (100) 1 72.6 (100) 72.06(100) 72.81(100) 0.005 71.04 (99.60) 10 72.73 (100) 71.85(100) 72.85(100) 50 100 71.49 (100) 71.88(100) 72.00(100) 1000 71.81 (100) 72.22(100) 72.39(100) 72.63(100) 71.69(100) 71.46(100) 70.65(100) 69.14 (96.48) 69.47(97.26) 67.88(95.70) 51.88 (60.54) 55.18(62.10) 50.36(55.07) 1.00(50.39) 1.00(47.26) 1.00 (44.53) 0.01 0.1 1 TABLE COMPARISON OF RESISTANCE TO FINE-TUNING ATTACKS WITH DISTINCT FILTER ROUNDS USING RESNET-18. WATERMARK DETECTION RATES ARE OMITTED AS THEY ALL REACH 100%. Fine-tuning Clean 4 Filters 6 Filters 8 Filters CIFAR-100 to CIFAR-10 93.21 CIFAR-10 to CIFAR-100 72.17 93.74 71. 93.01 72.68 93.55 72.27 Fig. 6. Comparison of parameter overlap ratio with different filter rounds on CIFAR-100 using ResNet-18. E. Additional Analysis Question 9. How does NeuralMark affect the parameter distribution? To assess the influence of NeuralMark on the parameter distribution, Fig. 7 present the parameter distributions on the CIFAR-100 dataset with ResNet-18 and ViT-B/16 architectures. As can be seen, the parameter distributions of Clean and NeuralMark are nearly indistinguishable. Thus, it is challenging for adversaries to detect the embedded watermarks within the model. More parameter distribution results are provided Fig. 12 in Appendix C. (a) (b) Fig. 7. Comparison of parameter distributions on the CIFAR-100 dataset using distinct architectures: (a) ResNet-18, and (b) ViT-B/16. Question 10. How does NeuralMark influence the performance convergence? To examine the impact of NeuralMark on model performance convergence, Fig. 8 shows the results on the CIFAR100 dataset with ResNet-18 and ViT-B/16 architectures. We find that the performance curves of Clean and NeuralMark (a) (b) Fig. 5. Comparison of resistance to pruning attacks with distinct filter rounds on CIFAR-10 (a) and CIFAR-100 (b) using ResNet-18 at various pruning ratios. attacks on the CIFAR-10 and CIFAR-100 datasets using ResNet-18 across different filtering rounds. As can be seen, as the number of filtering rounds increases, the robustness of NeuralMark in resisting pruning attacks exhibits slight decline. One reason is that increasing the number of filter rounds reduces the number of filtered parameters, leading to smaller average pooling window size, which affects the robustness against pruning attacks. Question 8. How does the hashed watermark filter affect the overlap rate between the model owner and the adversary? To analyze the effect of the hashed watermark filter on the overlap rate between the model owner and the adversary, we generate five counterfeit watermarks and calculate the overlap ratio between the parameters filtered by those and the original watermark. As shown in Fig. 6, the overlap rate decreases towards zero with more filtering rounds, indicating that watermark filtering enhances the confidentiality of the watermarked parameters. TABLE XI COMPARISON OF THE EFFECTS OF AVERAGE POOLING ON RESISTANCE TO FINE-TUNING AND PRUNING ATTACKS USING RESNET-18. VALUES (%) INSIDE AND OUTSIDE THE BRACKET ARE WATERMARK DETECTION RATE AND CLASSIFICATION ACCURACY, RESPECTIVELY. Method 0.001 0.005 0.01 40% 60% 80% CIFAR-100 to CIFAR-10 Fine-tuning (Learning Rate) CIFAR-100 Pruning (Pruning Ratio) NeuralMark (w/o AP) NeuralMark 93.26 (100) 93.74 (100) 92.20 (100) 92.25 (100) 90.68 (81.64) 91.25 (96.87) 71.82 (90.62) 69.86 (100) 57.50 (78.51) 43.88 (99.21) 16.14 (69.92) 9.85 (99.21) TABLE XII COMPARISON OF AVERAGE TIME COST (IN SECONDS) ON CIFAR-100 USING RESNET-18. HERE, IS THE NUMBER OF FILTERING ROUNDS. Method Clean NeuralMark (R = 1) NeuralMark (R = 2) NeuralMark (R = 3) NeuralMark (R = 4) VanillaMark GreedyMark VoteMark Time (s) 23.60 24.49 24.94 25.01 25.19 24. 47.43 35.17 exhibit similar trend of change and are closely aligned, the indicating that NeuralMark does not negatively affect convergence of model performance. More performance convergence results are offered in Fig. 13 in Appendix D. VoteMark, as it avoids the multiple rounds of watermark embedding loss calculations required by VoteMark. Those results highlight the superior efficiency of NeuralMark. Question 13. How do the watermark embedding layers impact the models performance? To investigate the impact of watermark embedding layers on the model performance, we randomly choose four individual layers and all layers from ResNet-18 for watermark embedding. Table XIII presents the results on the CIFAR-100 dataset, showing that embedding different layers or all layers does not significantly affect the model performance. TABLE XIII COMPARISON OF CLASSIFICATION ACCURACY (%) ON DIFFERENT WATERMARKING LAYERS ON CIFAR-100 USING RESNET-18. HERE, LAYERS 1-4 DENOTE RANDOMLY CHOSEN LAYERS, WHILE ALL LAYER REFERS TO ALL LAYERS. WATERMARK DETECTION RATES ARE OMITTED AS THEY ALL REACH 100%. Watermarking Layer Layer 1 Layer 2 Layer 3 Layer 4 All Layer Accuracy 76.51 76.68 76. 76.73 75.86 Question 14. What is the effect of varying the watermark length on model performance? To evaluate the influence of watermark length on the model performance, we set watermark lengths to 64, 128, 256, 512, 1024, and 2048, respectively. Table XIV lists the results on the CIFAR-100 dataset using ResNet-18, indicating that NeuralMark can achieve 100% detection rate with various watermark lengths while preserving nearly lossless model performance. TABLE XIV COMPARISON OF CLASSIFICATION ACCURACY (%) FOR DISTINCT WATERMARK LENGTHS ON CIFAR-100 USING RESNET-18. WATERMARK DETECTION RATES ARE OMITTED AS THEY ALL REACH 100%. Watermark Length 64 128 256 512 1024 Accuracy 75.84 75.90 76.46 76.18 76. 76.27 (a) (b) Fig. 8. Comparison of performance convergence on the CIFAR-100 dataset using distinct architectures: (a) ResNet-18, and (b) ViT-B/16. Question 11. How does average pooling impact NeuralMark? To verify the efficacy of average pooling, we compare i.e, NeuralMark with its variant without average pooling, NeuralMark w/o AP. As shown in Table XI, both versions resist fine-tuning attacks at lower learning rates. However, at learning rate of 0.01, the detection rate for NeuralMark (w/o AP) drops to 81.64%, below the security boundary, while NeuralMark maintains at 96.87%. In addition, the detection rate of NeuralMark (w/o AP) rapidly declines with increasing pruning rates, reaching 69.92% at an 80% pruning rate, while NeuralMark achieves 99.21%. Those results confirm that average pooling enhances resistance to both fine-tuning and pruning attacks. Question 12. Does NeuralMark impose significant additional computational burden during training? Table XII list the average time cost (in seconds) per training epoch over five epochs on the CIFAR-100 dataset using ResNet-18. NeuralMarks running time is comparable to that of Clean and VanillaMark, highlighting the efficiency of NeuralMark. Also, NeuralMark outperforms GreedyMark in terms of speed due to GreedyMarks reliance on costly sorting operations for parameter selection. Moreover, NeuralMark demonstrates significantly faster running times compared to VII. CONCLUSION In this article, we present NeuralMark, white-box method designed to protect model ownership. At the core of NeuralMark is hashed watermark filter, which utilizes hash function to generate an irreversible binary watermark from secret key, subsequently employing this watermark as filter to select model parameters for embedding. This simple design cleverly intertwines the embedding parameters with the hashed watermarks, providing robust protection against both forging and overwriting attacks. Moreover, the incorporation of average pooling provides resilience against fine-tuning and pruning attacks. We provide theoretical analysis of NeuralMarks security boundary. Extensive experiments on various datasets, architectures, and tasks confirm NeuralMarks effectiveness and robustness. In the future, we plan to investigate how the proposed hashed watermark filter can be generalized and incorporated into wider range of watermarking methods. REFERENCES [1] B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal et al., Language models are few-shot learners, arXiv preprint arXiv:2005.14165, vol. 1, 2020. [2] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., Gpt-4 technical report, arXiv preprint arXiv:2303.08774, 2023. [3] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang et al., Qwen technical report, arXiv preprint arXiv:2309.16609, 2023. [4] Y. Liu, T. Han, S. Ma, J. Zhang, Y. Yang, J. Tian, H. He, A. Li, M. He, Z. Liu et al., Summary of chatgpt-related research and perspective towards the future of large language models, Meta-Radiology, p. 100017, 2023. [5] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan et al., The llama 3 herd of models, arXiv preprint arXiv:2407.21783, 2024. [6] Y. Cao, H. Zhao, Y. Cheng, T. Shu, Y. Chen, G. Liu, G. Liang, J. Zhao, J. Yan, and Y. Li, Survey on large language model-enhanced reinforcement learning: Concept, taxonomy, and methods, IEEE Transactions on Neural Networks and Learning Systems, 2024. [7] B. Cottier, R. Rahman, L. Fattorini, N. Maslej, and D. Owen, The rising costs of training frontier ai models, arXiv preprint arXiv:2405.21015, 2024. [8] Y. Sun, T. Liu, P. Hu, Q. Liao, S. Fu, N. Yu, D. Guo, Y. Liu, and L. Liu, Deep intellectual property protection: survey, arXiv preprint arXiv:2304.14613, 2023. [9] N. Lukas, E. Jiang, X. Li, and F. Kerschbaum, Sok: How robust is image classification deep neural network watermarking? in S&P. IEEE, 2022, pp. 787804. [10] M. Xue, Y. Zhang, J. Wang, and W. Liu, Intellectual property protection for deep learning models: Taxonomy, methods, attacks, and evaluations, IEEE Transactions on Artificial Intelligence, vol. 3, no. 6, pp. 908923, 2021. [11] A. T. Ngo, C. S. Heng, N. Chattopadhyay, and A. Chattopadhyay, Persistence of backdoor-based watermarks for neural networks: comprehensive evaluation, IEEE Transactions on Neural Networks and Learning Systems, 2025. [12] I. Lederer, R. Mayer, and A. Rauber, Identifying appropriate intellectual property protection mechanisms for machine learning models: systematization of watermarking, fingerprinting, model access, and attacks, IEEE Transactions on Neural Networks and Learning Systems, 2023. [13] Y. Uchida, Y. Nagai, S. Sakazawa, and S. Satoh, Embedding watermarks into deep neural networks, in ACM ICMR, 2017, pp. 269277. [14] H. Liu, Z. Weng, and Y. Zhu, Watermarking deep neural networks with greedy residuals. in ICML, 2021, pp. 69786988. [15] L. Fan, K. W. Ng, C. S. Chan, and Q. Yang, Deepipr: Deep neural network ownership verification with passports, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 10, pp. 6122 6139, 2021. 12 [16] J. Zhang, D. Chen, J. Liao, W. Zhang, G. Hua, and N. Yu, Passportaware normalization for deep model protection, in NeurIPS, vol. 33, 2020, pp. 22 61922 628. [17] H. Liu, Z. Weng, Y. Zhu, and Y. Mu, Trapdoor normalization with irreversible ownership verification, in ICML. PMLR, 2023, pp. 22 177 22 187. [18] F. Li, H. Zhao, W. Du, and S. Wang, Revisiting the information capacity of neural network watermarks: Upper bound estimation and beyond, in AAAI, 2024, pp. 21 33121 339. [19] Y. Yan, X. Pan, M. Zhang, and M. Yang, Rethinking white-box watermarks on deep learning models under neural structural obfuscation, in USENIX Security, 2023, pp. 23472364. [20] Y. Adi, C. Baum, M. Cisse, B. Pinkas, and J. Keshet, Turning your weakness into strength: Watermarking deep neural networks by backdooring, in USENIX Security, 2018, pp. 16151631. [21] E. Le Merrer, P. Perez, and G. Tredan, Adversarial frontier stitching for remote neural network watermarking, Neural Computing and Applications, vol. 32, no. 13, pp. 92339244, 2020. [22] J. Jia, Y. Wu, A. Li, S. Ma, and Y. Liu, Subnetwork-lossless robust watermarking for hostile theft attacks in deep transfer learning models, IEEE Transactions on Dependable and Secure Computing, 2022. [23] P. Li, P. Cheng, F. Li, W. Du, H. Zhao, and G. Liu, Plmmark: secure and robust black-box watermarking framework for pre-trained language models, in AAAI, vol. 37, no. 12, 2023, pp. 14 99114 999. [24] C. He, X. Bai, X. Ma, B. B. Zhu, P. Hu, J. Fu, H. Jin, and D. Zhang, Towards stricter black-box integrity verification of deep neural network models, in ACM MM, 2024. [25] Y. Quan, H. Teng, Y. Chen, and H. Ji, Watermarking deep neural networks in image processing, IEEE Transactions on Neural Networks and Learning Systems, vol. 32, no. 5, pp. 18521865, 2020. [26] G. Hua, A. B. J. Teoh, Y. Xiang, and H. Jiang, Unambiguous and high-fidelity backdoor watermarking for deep neural networks, IEEE Transactions on Neural Networks and Learning Systems, 2023. [27] L. Fan, K. W. Ng, and C. S. Chan, Rethinking deep neural network ownership verification: Embedding passports to defeat ambiguity attacks, in NeurIPS, vol. 32, 2019. [28] L. Feng and X. Zhang, Watermarking neural network with compensation mechanism, in KSEM, 2020, pp. 363375. [29] Y. Li, B. Tondi, and M. Barni, Spread-transform dither modulation watermarking of deep neural network, Journal of Information Security and Applications, vol. 63, p. 103004, 2021. [30] B. D. Rouhani, H. Chen, and F. Koushanfar, Deepsigns: an end-to-end watermarking framework for protecting the ownership of deep neural networks, in ASPLOS, vol. 3, 2019. [31] Y. Li, L. Abady, H. Wang, and M. Barni, feature-map-based largepayload dnn watermarking algorithm, in IWDW, 2021, pp. 135148. [32] J. H. Lim, C. S. Chan, K. W. Ng, L. Fan, and Q. Yang, Protect, show, attend and tell: Empowering image captioning models with ownership protection, Pattern Recognition, vol. 122, p. 108285, 2022. [33] A. F. Webster and S. E. Tavares, On the design of s-boxes, in Eurocrypt. Springer, 1985, pp. 523534. [34] R. Zhu, X. Zhang, M. Shi, and Z. Tang, Secure neural network watermarking protocol against forging attack, EURASIP Journal on Image and Video Processing, vol. 2020, pp. 112, 2020. [35] F. Li, L. Yang, S. Wang, and A. W.-C. Liew, Leveraging multi-task learning for umambiguous and flexible deep neural network watermarking. in SafeAI@ AAAI, 2022. [36] M. J. Dworkin, Sha-3 standard: Permutation-based hash and extendable-output functions, 2015. [37] H. Gholamalinezhad and H. Khosravi, Pooling methods in deep neural networks, review, arXiv preprint arXiv:2009.07485, 2020. [38] M. Bellare and P. Rogaway, Random oracles are practical: paradigm for designing efficient protocols, in CCS, 1993, pp. 6273. [39] A. Krizhevsky, G. Hinton et al., Learning multiple layers of features from tiny images, Technical report, University of Toronto, Tech. Rep., 2009. [40] L. Fei-Fei, R. Fergus, and P. Perona, Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories, in CVPRW, 2004, pp. 178178. [41] G. Griffin, A. Holub, P. Perona et al., Caltech-256 object category dataset, Technical Report 7694, California Institute of Technology Pasadena, Tech. Rep., 2007. [42] Y. Le and X. Yang, Tiny imagenet visual recognition challenge, CS 231N, vol. 7, no. 7, p. 3, 2015. [43] J. Novikova, O. Duˇsek, and V. Rieser, The e2e dataset: New challenges for end-to-end generation, arXiv preprint arXiv:1706.09254, 2017. 13 [44] A. Krizhevsky, I. Sutskever, and G. E. Hinton, Imagenet classification with deep convolutional neural networks, in NeurIPS, vol. 25, 2012. [45] K. Simonyan and A. Zisserman, Very deep convolutional networks for large-scale image recognition, in ICLR, 2015. [46] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, Going deeper with convolutions, in CVPR, 2015, pp. 19. [47] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in CVPR, 2016, pp. 770778. [48] S. Zagoruyko, Wide residual networks, in BMVC, 2016. [49] A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang, Y. Zhu, R. Pang, V. Vasudevan et al., Searching for mobilenetv3, in ICCV, 2019, pp. 13141324. [50] A. Dosovitskiy, An image is worth 16x16 words: Transformers for image recognition at scale, in ICLR, 2021. [51] Z. Liu, H. Hu, Y. Lin, Z. Yao, Z. Xie, Y. Wei, J. Ning, Y. Cao, Z. Zhang, L. Dong et al., Swin transformer v2: Scaling up capacity and resolution, in CVPR, 2022, pp. 12 00912 019. [52] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., Language models are unsupervised multitask learners, OpenAI blog, vol. 1, no. 8, p. 9, 2019. [53] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, LoRA: Low-rank adaptation of large language models, in ICLR, 2022. [54] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., Pytorch: An imperative style, high-performance deep learning library, in NeurIPS, vol. 32, 2019. [55] I. Loshchilov, F. Hutter et al., Fixing weight decay regularization in adam, arXiv preprint arXiv:1711.05101, vol. 5, 2017. APPENDIX PROOF FOR PROPOSITION V."
        },
        {
            "title": "APPENDIX B\nADDITIONAL RESULTS OF PRUNING ATTACKS",
            "content": "14 (cid:1). (cid:0)n Proposition V.1. Under the assumption that the hash function produces uniformly distributed outputs [38], for model watermarked by NeuralMark with watermark tuple {K, b}, where = H(K), if an adversary attempts to forge counterfeit watermark tuple {K, b} such that = H(K) and = K, then the probability of achieving watermark detection rate of at least ρ (i.e., ρ) is upper-bounded by 1 2n (cid:80)nρn i= Proof. Since the hash function produces uniformly distributed outputs, each bit of the counterfeit watermark matches the corresponding bit of the extracted watermark from model parameters with probability of 1 2 . The number of matching bits follows binomial distribution with parameters and = 1 2 . To achieve detection rate of at least ρ, the adversary needs at least ρn bits to match out of bits. Thus, the probability of having at least ρn matching bits is given by (cid:18)n Pr (cid:2)X ρn(cid:3) = (cid:19)i (cid:18) 1 2 (cid:19) (cid:18) 1 2 (cid:19)ni (cid:88) i=ρn = 1 2n (cid:88) i=ρn (cid:19) (cid:18)n = 1 2n nρn (cid:88) i=0 (cid:19) . (cid:18)n (4) Accordingly, the probability of an adversary forging counterfeit watermark that achieves watermark detection rate of at least ρ (i.e., ρ) is upper-bounded by 1 2n (cid:80)nρn i=0 (cid:0)n (cid:1). Fig. 9-11 provide the results from pruning attacks conducted on the CIFAR-100, Caltech-101, and Caltech-256 datasets, respectively. As can be seen, as the pruning ratio increases, the performance of NeuralMark degrades while the detection rate remains nearly 100%. This indicates NeuralMarks robustness against pruning attacks. Those results collectively suggest NeuralMark exhibits superior robustness in resisting pruning attacks compared to other methods."
        },
        {
            "title": "APPENDIX C\nADDITIONAL RESULTS OF PARAMETER DISTRIBUTION",
            "content": "Fig. 12 provides additional parameter distributions for various architectures on the CIFAR-100 dataset. As can be seen, the parameter distributions of Clean and NeuralMark closely align in each architecture. Those results further demonstrate the secrecy of NeuralMark. APPENDIX ADDITIONAL RESULTS OF PERFORMANCE CONVERGENCE Fig. 13 presents additional performance convergence plots for various architectures on the CIFAR-100 dataset. Across all architectures, the performance curves of Clean and NeuralMark exhibit similar trends and are closely aligned, further confirming that NeuralMark does not negatively affect performance convergence. (a) (b) (c) (d) Fig. 9. Comparison of resistance to pruning attacks at various pruning ratios on CIFAR-100 using AlexNet and ResNet-18: (a) NeuralMark, (b) VanillaMark, (c) GreedyMark, and (d) VoteMark. (a) (b) (c) (d) Fig. 10. Comparison of resistance to pruning attacks at various pruning ratios on Caltech-101 using AlexNet and ResNet-18: (a) NeuralMark, (b) VanillaMark, (c) GreedyMark, and (d) VoteMark. 15 (a) (b) (c) (d) Fig. 11. Comparison of resistance to pruning attacks at various pruning ratios on Caltech-256 using AlexNet and ResNet-18: (a) NeuralMark, (b) VanillaMark, (c) GreedyMark, and (d) VoteMark. (a) (b) (c) (d) (e) (f) (g) (h) Fig. 12. Comparison of parameter distributions on CIFAR-100 with distinct architectures: (a) AlexNet, (b) ResNet-18, (c) ResNet-34, (d) ViT-B/16, (e) VGG-16, (f) MobileNet-V3-L, (g) GoogLeNet, and (h) Swin-V2-B. (a) (b) (c) (d) (e) (f) (g) (h) Fig. 13. Comparison of model performance convergence on CIFAR-100 with distinct architectures: (a) AlexNet, (b) ResNet-18, (c) ResNet-34, (d) ViT-B/16, (e) VGG-16, (f) MobileNet-V3-L, (g) GoogLeNet, and (h) Swin-V2-B."
        }
    ],
    "affiliations": [
        "Beijing Teleinfo Technology Company Ltd., China Academy of Information and Communications Technology, Beijing 100095, China",
        "Research Institute of Industrial Internet of Things, China Academy of Information and Communications Technology, Beijing 100095, China",
        "School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing 210023, China"
    ]
}