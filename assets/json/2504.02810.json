{
    "paper_title": "Generative Evaluation of Complex Reasoning in Large Language Models",
    "authors": [
        "Haowei Lin",
        "Xiangyu Wang",
        "Ruilin Yan",
        "Baizhou Huang",
        "Haotian Ye",
        "Jianhua Zhu",
        "Zihao Wang",
        "James Zou",
        "Jianzhu Ma",
        "Yitao Liang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With powerful large language models (LLMs) demonstrating superhuman reasoning capabilities, a critical question arises: Do LLMs genuinely reason, or do they merely recall answers from their extensive, web-scraped training datasets? Publicly released benchmarks inevitably become contaminated once incorporated into subsequent LLM training sets, undermining their reliability as faithful assessments. To address this, we introduce KUMO, a generative evaluation framework designed specifically for assessing reasoning in LLMs. KUMO synergistically combines LLMs with symbolic engines to dynamically produce diverse, multi-turn reasoning tasks that are partially observable and adjustable in difficulty. Through an automated pipeline, KUMO continuously generates novel tasks across open-ended domains, compelling models to demonstrate genuine generalization rather than memorization. We evaluated 23 state-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO, benchmarking their reasoning abilities against university students. Our findings reveal that many LLMs have outperformed university-level performance on easy reasoning tasks, and reasoning-scaled LLMs reach university-level performance on complex reasoning challenges. Moreover, LLM performance on KUMO tasks correlates strongly with results on newly released real-world reasoning benchmarks, underscoring KUMO's value as a robust, enduring assessment tool for genuine LLM reasoning capabilities."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 0 1 8 2 0 . 4 0 5 2 : r a"
        },
        {
            "title": "Generative evaluation of complex reasoning in large\nlanguage models",
            "content": "Haowei Lin1,, Xiangyu Wang1,, Ruilin Yan1,, Baizhou Huang2, Haotian Ye5, Jianhua Zhu2, Zihao Wang1, James Zou5, Jianzhu Ma3,4, and Yitao Liang1 1Institute for Artificial Intelligence, Peking University, Beijing, China. 2Wangxuan institute of computer technology, Peking University, Beijing, China. 3Department of Electronic Engineering, Tsinghua University, Beijing, China. 4Institute for AI Industry Research, Tsinghua University, Beijing, China. 5Computer Science Department, Stanford University, California, United States. Equal contribution. *Correspondence should be addressed to: majianzhu@tsinghua.edu.cn, yitaol@pku.edu.cn."
        },
        {
            "title": "ABSTRACT",
            "content": "With powerful large language models (LLMs) demonstrating superhuman reasoning capabilities, critical question arises: Do LLMs genuinely reason, or do they merely recall answers from their extensive, web-scraped training datasets? Publicly released benchmarks inevitably become contaminated once incorporated into subsequent LLM training sets, undermining their reliability as faithful assessments. To address this, we introduce KUMO, generative evaluation framework designed specifically for assessing reasoning in LLMs. KUMO synergistically combines LLMs with symbolic engines to dynamically produce diverse, multi-turn reasoning tasks that are partially observable and adjustable in difficulty. Through an automated pipeline, KUMO continuously generates novel tasks across open-ended domains, compelling models to demonstrate genuine generalization rather than memorization. We evaluated 23 state-of-theart LLMs on 5,000 tasks across 100 domains created by KUMO, benchmarking their reasoning abilities against university students. Our findings reveal that many LLMs have outperformed university-level performance on easy reasoning tasks, and reasoning-scaled LLMs reach university-level performance on complex reasoning challenges. Moreover, LLM performance on KUMO tasks correlates strongly with results on newly released real-world reasoning benchmarks, underscoring KUMOs value as robust, enduring assessment tool for genuine LLM reasoning capabilities. Reasoningthe cognitive process of using evidence, arguments, and logic to reach conclusionsis fundamental to problem-solving, decision-making, and critical thinking [1, 2]. Enhancing the reasoning capabilities of Artificial Intelligence (AI) has been core goal since the fields inception, dating back to early developments such as automatic theorem provers and expert systems built with search algorithms and graphical models [311]. In the deep learning era, AI systems have exhibited increasingly powerful reasoning abilities. For instance, AlphaZero demonstrated superhuman performance in strategic games like chess, shogi, and Go [12]; AlphaGeometry outperformed the average gold medalist in solving Olympiad-level geometry problems [13]; and OpenAIs o3 model achieved gold medal at the 2024 International Olympiad in Informatics (IOI) [14]. Among these advancements, large language models (LLMs) have emerged as particularly significant component, enabling impressive reasoning capabilities across wide range of tasks. Looking ahead, the prospect of LLMs achieving superhuman intelligence in even more domains is highly promising. Advancing the development of LLM reasoning first requires suitable evaluation mechanism, which itself is non-trivial task. Recall that reasoning, by definition, is the cognitive process of 1 using evidence, arguments, and logic to reach conclusions. Thus, the essence of reasoning lies not merely in arriving at valid conclusions, but in the reasoning process itself. However, directly evaluating this cognitive process presents significant challenges. Firstly, cognitive processes are generally invisible, and in deep neural networks, interpreting computational flows underlying an LLMs reasoning remains difficult. Secondly, even though reasoning models such as OpenAIs o1 [14] and o3 [14], or DeepSeeks R1 [15], can explicitly articulate their reasoning thoughts[15 18], multiple valid reasoning paths may exist that lead to the same correct conclusion [1921]. Furthermore, parsing reasoning processes articulated in natural language is inherently complex. Recent work such as MR-Ben[22] demonstrates that even state-of-the-art LLMs struggle to identify errors within reasoning chains, further underscoring the difficulty of accurately assessing reasoning processes. Consequently, we argue that, for the time being, it is more practical to use conclusions as proxies for evaluating reasoning performance in LLMs, and we term this as conclusion-based evaluation. To date, numerous conclusion-based evaluation benchmarks have been proposed for assessing LLM reasoning. Existing benchmarks such as LogiQA [23], LogiQA 2.0 [24], ReClor [25], ARLSAT [26], ConTRoL [27], and AGIEval [28] are primarily derived from standardized tests (e.g., Chinese National Civil Service Exam, GMAT, LSAT, GRE, recruitment exams). These sources provide high-quality, expert-designed logical reasoning questions at considerable scale. Other datasets, such as LINGOLY [29], FOLIO [30], CLUTRR [31], and GSM8K [32], are either crafted by domain experts or constructed through crowd-sourcing efforts. With explicit correct conclusion annotation, evaluating reasoning performance is straightforward for conclusion-based benchmarks by comparing model-generated answers to the annotated ground truths. However, it is important to emphasize certain risks associated with conclusion-based evaluation. Since conclusions serve merely as proxies for actual reasoning performance, the fidelity of this evaluation approach is contingent upon the conclusion being genuinely derived through reasoning processes rather than memorization. Memorization can easily occur due to dataset contamination [3336]. Once datasets become publicly available, their content is susceptible to being incorporated into the pre-training datasets of LLMs. Recent evidence of such contamination includes observations that LLM performance on Codeforces problems sharply declines for problems published after an LLMs training cutoff date [37, 38]. Conversely, performance prior to the cutoff date strongly correlates with the frequency of problems appearance on GitHub [38]. Additionally, recently developed hand-crafted variant of the widely used math dataset GSM8K has revealed that several models have likely overfit to this benchmark [32, 39]. To counteract contamination, one recent benchmark, LiveBench [40], attempts to avoid contamination by updating questions monthly, but this demands substantial human effort. Considering these challenges, there is clear need for dynamically designed benchmark that can be efficiently updated, effectively addressing dataset contamination while balancing resource demands. In this paper, we introduce the concept of generative evaluation as dynamic benchmarking method, in which benchmark questions are algorithmically generated rather than manually curated. Prior studies, including LogicBench [37] and DYVAL [41], have applied generative methods primarily to evaluate logical expression solving and query processing tasks on synthetic directed acyclic graphs (DAGs). Such synthetic data can be efficiently compiled and synthesized, naturally supporting continuously updatable benchmarks. Inspired by these rule-based generative approaches, we propose an advanced generative evaluation framework called KUMO, designed specifically for assessing complex reasoning capabilities in LLMs. KUMO involves structured reasoning game wherein participant interacts iteratively with system to gather evidence and draw 2/28 conclusions within partially observable environments. The scenarios in this game are contextually rich and emulate real-world reasoning tasks across various domains, such as medical diagnostics, educational assessment, and chemical material detection. To ensure the evaluation isolates reasoning performance from domain-specific knowledge, each task includes knowledge guidebook provided to the participants. Tasks within KUMO are dynamically generated through an automated neuralsymbolic pipeline, where advanced LLMs collaborate closely with an SAT-based symbolic engines. This integration allows precise control over problem complexity across multiple dimensions, resulting in benchmarking approach that is both highly versatile and difficult to saturate. In our experiments, we randomly generate 5,000 tasks spanning 100 domains created by KUMO and evaluate the reasoning capabilities of 23 state-of-the-art LLMs. This allows us to establish clear ranking of their reasoning performance. We further analyze the models performance across two distinct difficulty levels and observe that reasoning-focused LLMs exhibit substantial advantages on more challenging tasks, performing comparably or even slightly better than university-level students. Additionally, we simulate data contamination scenario by fine-tuning LLMs using golden trajectories produced by an optimal search algorithm. Our results indicate that these contaminated LLMs fail to generalize effectively to out-of-domain tasks or tasks with varying difficulty levels. This demonstrates KUMOs ability to resist dataset contamination by continuously updating tasks across new domains. We also conduct several statistical analyses on KUMO, uncovering an intriguing pattern: domains exhibiting similar entity-relation graph topologies correspondingly yield similar reasoning performance among LLMs. This finding suggests that diverse set of domains is essential to comprehensively assess different facets of reasoning abilities and to enable genuine evaluation of model generalization."
        },
        {
            "title": "Results",
            "content": "KUMO benchmark We introduce complex reasoning game designed to evaluate the reasoning capabilities of Large Language Models (LLMs) based on their gameplay trajectories. single game instance is defined by the following components: Truth Set (T = {ti}N i=1 ): finite and countable set containing possible truths. Action Set (A = {ai}M i=1 ): finite and countable set containing possible actions. Outcomes (O : (cid:55) oa): mapping that associates each action with its outcome oa Oa, where Oa is the set of all possible outcomes for action a. Knowledge Book (K): document providing definitions of T, A, O, and detailing how each truth can be ruled out from observed outcomes in natural language. At the start of the game, valid truth is initialized, with all other truths in marked as invalid. Outcomes are then generated to be consistent with the valid truth t. In each round, the player selects an action A, and the game reveals the corresponding outcome oa. The objective of the game is to accurately identify the valid truth using the fewest possible actions. For instance, as illustrated in Fig. 1a, consider medical analysis game. The player is provided with medical analysis guidebook (K), which details the relationship between various diseases (T ) and diagnostic tests (A). In each round, the player can choose diagnostic test (a) to observe its corresponding 3/28 Figure 1. Overview of KUMO tasks. a. An example of the complex reasoning game. In this game, the player is presented with list of potential truths, available actions, and knowledge guidebook for specific scenario. In the illustrated case of diagnostic test scenario, the truths represent diseases, and the actions correspond to diagnostic tests. During each round, the player selects one action, observes its outcome, and uses the information to eliminate invalid truths. The objective is to identify the single valid truth using the fewest possible actions. b. The generated tasks in KUMO. This study employs an automated pipeline to generate 100 exemplar task environments across 18 topic categories. Each environment includes approximately 50 truths and 30 actions. The figure shows part of the truths and actions from the Medical environment, which corresponds to the scenario depicted in panel a. 4/28 outcome (oa) or predict the disease for the patient. The goal is to minimize the number of tests required to make correct disease prediction. Solving this game requires sophisticated reasoning due to its complexity and partial observability. We show in Method section that, assuming all truths have an equal probability of being the valid truth, the expected minimum number of actions required to identify can be computed. Optimal gameplay involves selecting actions that minimize the expected number of steps in subsequent rounds. Calculating the expected minimum steps requires recursive search process, closely tied to the players planning horizon. Additionally, as the game is partially observable (the outcome is unknown to the player in advance), the player must dynamically adjust their strategy based on observed outcomes. This interplay between planning and observation mirrors the strategic depth of games like chess, making the game an effective tool for assessing complex reasoning abilities in LLMs. KUMO offers several advantages as benchmark for evaluating LLMs. First, it provides ground truth data, enabling automatic evaluation that is both efficient and objective. Second, KUMO is highly scalable. We have developed flexible framework for semi-automatic task generation  (Fig.2)  , demonstrating that tasks within this benchmark can be easily created and updated. In this paper, we automatically generate 5000 tasks in 100 different domains to evaluate the state-of-the-art open-sourced LLMs from wide range of configurations  (Fig.4)  . Furthermore, KUMO supports adjustable difficulty levels  (Fig.3)  and our results reveal that it effectively resists saturation, as evidenced by performance gap between LLMs and the oracle search engine (Fig.3b). Lastly, this generative design significantly mitigates data contamination issues, ensuring more reliable evaluation process  (Fig.5)  . scalable framework for task generation The construction of the KUMO benchmark is organized into scalable, multi-stage pipeline designed to create complex, reasoning-intensive tasks for evaluating large language models. Each stage of the pipeline leverages the generative capabilities of LLMs and logical reasoning tools to build rich game scenarios from the ground up. The process is illustrated  (Fig.2)  and described in detail as follows. The first stage (Fig.2a) focuses on identifying diverse and challenging domains, each representing distinct scenario within the complex game. For instance, the game could involve tasks such as predicting patients disease, identifying an unknown material through chemical experiments, or diagnosing students weaknesses in an educational context. To achieve this, capable LLM is prompted to propose various domains based on the games definition using general prompt (Extended Data Fig.1). These domains establish the high-level thematic structure of the tasks, simulating real-world applications across different scenarios. Furthermore, different domains correspond to unique graph structures for truths and actions (Fig.3e), fundamentally shaping the reasoning pathways and enhancing the diversity of reasoning evaluation. Additionally, we demonstrate that the use of domains is crucial in mitigating benchmark contamination  (Fig.5)  . Once the domains are determined, the LLM generates foundational components for each one. These include situated truths (propositions relevant to the domain) and actions along with their respective outcomes. The outcomes are designed to rule out certain truths, ensuring that players must engage in careful reasoning to determine the correct truths. To instantiate concrete task within chosen domain, we first determine the size of the candidate truth set and the action set. From universal pool of truths, subset is sampled, designating one as valid and treating the others as invalid. The generation of compatible actions and corresponding outcomes is formalized as satisfiability (SAT) problem. SAT-based engine 5/28 Figure 2. The construction of KUMO benchmark consists of five stages: a. Domain proposal. capable large language model (LLM) is prompted to propose various scenarios for the complex game based on its definition. These scenarios, referred to as domains, are collected. b. Seed config generation. The LLM is further prompted to generate foundational elements for each domain, including situated truths, actions, and their corresponding outcomes. These outcomes are designed to rule out certain truths. c. Task instance generation. To create specific task instance, the sizes of its candidate truth set and action set are first determined. subset of truths is then sampled from the universal truth set, with one selected as valid while the others are treated as invalid. The generation of compatible actions and outcomes is modeled as satisfiability (SAT) problem. An SAT-based engine is employed to sample the action subset and generate outcomes. This process involves extracting related outcomes for each truth, assigning logical values based on validity, and using SAT solver to produce viable solution. d. Knowledge book generation. Once task instance is generated, an LLM is tasked with writing knowledge book and revising it if any error detected. This book translates the outcome configurations associated with the sampled truth and action subsets into detailed natural language descriptions. e. Evaluation. In each round, the player takes actions or makes truth prediction, and simulator provides observations for the action based on the outcomes of the task (which is unseen to the player). The goal is to achieve accurate truth prediction while minimizing the number of actions taken. 6/28 is then employed to pick the action subset and produce outcomes consistent with the valid truth. The process involves extracting related outcomes, assigning logical values based on validity, and using SAT solver to ensure logically coherent configuration. After task instance is established, an LLM is prompted to write knowledge book, which provides rich, natural language description of the chosen truths and actions. This knowledge book serves as narrative companion to the raw configuration, translating logical outcomes and constraints into readable, scenario-specific documentation. However, the initial knowledge book may contain logical errors or ambiguous language. It is essential to verify its accuracy and clarity. To achieve this, we randomly sample substantial number of knowledge books and identify common error patterns. We then prompt the LLM to detect any errors in the current knowledge book. If errors are found, the old version is revised, and new, improved knowledge book is rewritten. With the task instance and its corresponding knowledge book finalized, an evaluation process commences. The playerunaware of the underlying task configurationinteracts with the environment by taking actions or making predictions about the underlying truth. simulator responds with observations guided by the outcomes generated in step (c). The players objective is to correctly identify the valid truth with minimal action expenditure, testing both reasoning efficiency and accuracy. Benchmarking State-of-the-Art LLMs We generate 100 domains (Fig.1b) and conduct comprehensive evaluation of 23 state-of-the-art LLMs across two difficulty levels: Easy (#Truths = 4, #Actions = 6) and Hard (#Truths = 12, #Actions = 16). Due to cost considerations, we evaluate 15 open-source LLMs on the Easy setting across all 100 domains  (Fig.3)  , and evaluate all 23 LLMs on both Easy and Hard settings within subset of 5 domains (MedicalEnv, ChemicalEnv, MusicEnv, EducationEnv, and FantasyEnv; Fig.4). We assess reasoning performance using two metrics: (1) success rate, defined as the percentage of trajectories that correctly predict the target truth; and (2) relative action count, which quantifies the deviation between the number of actions taken and the optimal number required (see Methods). higher success rate reflects greater validity in reasoning, while lower relative action count indicates more efficient reasoning trajectories. For each domain and difficulty level, we generate 50 task instances and evaluate each LLM over 5 runs per instance. We first observe that among the evaluated LLMs, three modelsQwQ-32B, DeepSeek-R1, and o1-miniare reasoning-scaled, meaning they generate reasoning thoughts before producing the answer. The remaining models are instruction-tuned LLMs. From experiments conducted across 100 domains, we find that powerful non-reasoning-scaled models (e.g., DeepSeek-V3) are capable of solving these relatively simple reasoning tasks, achieving higher success rate (0.86) compared to the best reasoning-scaled model (DeepSeek-R1, at 0.83). Analyzing the trajectories, we observe that reasoning-scaled models tend to overthink, which can lead to incorrect predictions. When comparing relative action counts, reasoning-scaled LLMs significantly outperform instruction-tuned ones, indicating that they are better at identifying efficient reasoning paths. Interestingly, we also find that larger model size does not necessarily correlate with better reasoning performance, as seen in comparisons between the Qwen2.5 and LLaMA series. Similar observations are drawn from the five-domain experiments (Fig.4a-b). In the hard setting, reasoning-scaled LLMs demonstrate remarkable performance compared to instruction-tuned LLMsthe three reasoning-scaled models consistently rank first, with significant performance gap. Interestingly, when compared to human performance (university students), LLMs outperform humans in the easy reasoning setting, achieving both higher success rate and lower relative 7/28 Figure 3. Benchmark results for 100 environments in the Easy setting (#Truths=4, #Actions=6) using KUMO for open-sourced Large Language Models (LLMs). Left panel: Success rates of LLMs, ranked from highest to lowest from left to right. Right panel: Action counts of LLMs. Environments are ranked from top to bottom based on the average success rate across LLMs. 8/28 Figure 4. Benchmarking Large Language Models (LLMs) on KUMO and correlation with other LLM benchmarks. We evaluate 23 state-of-the-art LLMs varying in parameter counts, architectures, and organizational origins across five environments: MedicalEnv, ChemicalEnv, EducationEnv, FantasyEnv, and MusicEnv. Each environment has two difficulty levels: Easy (#Truths=4, #Actions=6) and Hard (#Truths=12, #Actions=16). a. Success rate and relative action count metrics for the Easy setting. b. Success rate and relative action count metrics for the Hard setting. Pearson Correlation of LLM performance between KUMO and c. MMLU-Pro benchmark, d. LongBench-V2 benchmark, and e. LiveBench-Reason benchmark. 9/28 action count. However, in the difficult setting, only the reasoning-scaled LLMs reach performance levels comparable to those of humans. These findings suggest that current LLMs are capable of replacing humans in many simple reasoning tasks. For more complex tasks, however, human reasoning remains more reliablethough reasoning-scaled LLMs show strong potential to surpass human performance in the future. Since KUMO is synthetic benchmark, we validate its relevance to real-world applications by computing the Pearson correlation between LLM performance on KUMO and on other established benchmarks (Fig. 4ce). The selected benchmarks MMLU-Pro, LiveBench-Reason, and LongBench V2, are all recently published and thus likely unaffected by training data contamination. Overall, we observe clear positive correlation for success rate and negative correlation for relative action count when compared to these benchmarks. Notably, the correlations with MMLU-Pro and LiveBench-Reason are significantly higher than with LongBench V2. This is likely because LongBench V2 emphasizes long-context understanding, whereas KUMO focuses more on reasoning. We also find that correlations are substantially higher under the hard setting of KUMO than the easy setting, suggesting that the selected benchmarks are themselves challenging. In fact, correlation with KUMO may serve as proxy for assessing the difficulty of reasoning benchmarks. Given that correlations exceed 0.9 in some cases, we conclude that KUMO is reliable and scalable benchmark for evaluating the reasoning abilities of LLMs, with the added benefit of being contamination-free. KUMO resists overfitting Since the code for KUMO will be publicly available, we aim to maintain faithful KUMO leaderboard, with domain updates occurring every two months. To address potential overfittingwhere models might exploit our task generation pipeline to synthesize large number of task instanceswe conduct an overfitting experiment to evaluate generalization. Specifically, we examine whether LLMs that overfit on single domain within one round can still perform well on other domains in subsequent rounds (i.e., the following two months). We fine-tune LLMs on golden trajectories generated by an optimal search algorithm (see Methods) in one domain and evaluate their performance both in-domain and out-of-domain. For this, we split MedicalEnv into two disjoint subdomainsMedicalINDEnv and MedicalOODEnvto simulate an out-of-distribution but in-domain setup. The remaining domains (EducationEnv, ChemicalEnv, MusicEnv, FantasyEnv) serve as additional out-of-domain evaluation environments. We also evaluate out-of-difficulty generalization, testing whether LLMs fine-tuned on one difficulty level can generalize to another. The experimental results  (Fig.5)  demonstrate that fine-tuned LLMs achieve strong performance on in-distribution generalization, with all models performing best in MedicalINDEnv. They also show good in-domain and hard-to-easy generalization, as evidenced by results in MedicalOODEnv and MedicalEnv. However, out-of-domain and easy-to-hard generalization remain challenging. Notably, performance varies significantly across domains: for instance, fine-tuned LLMs on FantasyEnv perform at near-random levels. These findings suggest that KUMO presents substantial challenge for both out-of-domain generalization and difficulty-based generalization, effectively mitigating overfitting to unseen domains and curbing difficulty saturation. Statistical study of KUMO In our analysis of parsing errors and token consumption for KUMO (Fig.6a-b), we observe that certain LLMssuch as LLaMA-3.2-3B, Qwen2.5-3B-instruct, LLaMA-3.1-8B-instruct, and Gemma2-9B-itconsume significantly more tokens than others while also exhibiting high parsing error rates. These models frequently fail to follow instructions and tend to generate excessively long and 10/28 Figure 5. Performance of Large Language Models (LLMs) fine-tuned on golden trajectories. The MedicalEnv environment is divided into MedicalINDEnv (in-distribution) and MedicalOODEnv (out-of-distribution), each with distinct connection components. Two LLMs, Qwen2.5-0.5B-Instruct and Qwen2.5-3B-Instruct, are fine-tuned on golden trajectories within MedicalINDEnv under Easy (#Truths=4, #Actions=6) and Hard (#Truths=12, #Actions=16) settings. a. Success rate and relative action count metrics for the Easy setting. b. Success rate and relative action count metrics for the Hard setting. Fine-tuned LLMs exhibit strong in-distribution (IND) generalization but experience severe performance degradation for out-of-domain (OOD) generalization and difficulty transitions (Easy to Hard / Hard to Easy). This demonstrates the benchmarks resistance to overfitting through diverse setting generation. 11/28 Figure 6. We use the results from benchmarking experiments to conduct statistical analysis of KUMO. a. Parsing error rates across LLMs in easy and hard settings, averaged over 5 domains. b. Token consumption (input + output) across LLMs in easy and hard settings, averaged over 5 domains. c. Raw action counts for the optimal search algorithm across 5 domains. d. Graph topology for 5 domains: nodes represent truths, edges denote actions connecting truths as possible outcomes, and colors indicate Louvain community membership. e. Cramérs between LLM performance and domain graph topology, computed over all 100 domains. 12/28 irrelevant outputs. Among the more capable models that demonstrate nontrivial performance on KUMO, token consumption in the easy setting ranges from 2,937.00 tokens (LLaMA-3.3-70B-instruct) to 8,121.88 tokens (Claude-3.5-Haiku), with an average of 6,126.35 tokens. In the hard setting, consumption spans from 5,395.78 tokens (Claude-3.5-Sonnet) to 22,605.71 tokens (Gemma-2-27B-it), with an average of 16,002.93 tokens. Overall, the number of consumed tokens scales almost proportionally with the size of the action space, indicating near-linear expansion of the search space as the number of actions increases. In the analysis of optimal raw action count (Fig.6c), we find that in the easy setting, most optimal action counts fall within the range of 3 to 5, with an expected value of 3.92. In the hard setting, optimal action counts are generally much smaller relative to the size of the action space, with two actions being the most frequent choice and an expected value of 6.69. This suggests that even in more complex scenarios, effective strategies exist for solving the task, underscoring the importance of strategic action selection. Moreover, the distribution of optimal action counts varies across domains, reflecting the diversity and domain-specific nature of the task. In our analysis of domain graph topology and its correlation with reasoning performance (Fig.6d-e), all modelsexcept LLaMA-3.2-3B-instruct and Gemma-2-27B-itexhibit statistically significant correlations (as measured by p-values) between the internal logical graph structures of tasks in KUMO and their corresponding success rates. This indicates that models ability to solve particular task in our benchmark is meaningfully influenced by the logical structure of the domain itself. One possible explanation is that similar domains require similar reasoning capabilities, which leads to correlated performance across them  (Fig.5)  . These findings highlight the importance of evaluating reasoning performance across diverse domains, as each domain embodies distinct logical patterns. Discussion As tasks in KUMO are synthesized, the presence of some counterfactual instances is inevitable. However, counterfactuality is beyond the intended scope of KUMO, which focuses solely on evaluating the reasoning capabilities of LLMs disentangled from their internal knowledgehence the inclusion of comprehensive knowledge book during the reasoning process. Counterfactual content may mislead LLMs due to the interplay between their implicit world knowledge and the externally provided knowledge book. Nevertheless, KUMO can be naturally adapted into benchmark for counterfactual reasoning by exclusively generating counterfactual samples. Similar adaptations could be made to study other aspects, such as long-context reasoning (by using long knowledge book), probabilistic reasoning (by using probabilistic rule-out relationships), or multi-truth reasoning (by allowing more than one valid truth). Overall, we believe KUMO holds strong potential as flexible generative evaluation benchmark framework that can be tailored to variety of research objectives."
        },
        {
            "title": "Methods",
            "content": "Language models In this study, we benchmark diverse set of large language models (LLMs) to assess their reasoning performance across range of tasks using KUMO. total of 23 models were included in the analysis, comprising the Qwen2.5 series (3B, 7B, 14B, 32B, 72B), LLaMA series (3.2-3B, 3.1-8B, 3.1-70B, 13/28 3.3-70B), Gemma-2 series (9B, 27B), Claude3.5 series (Haiku, Sonnet), GPT series (GPT-3.5-turbo, GPT-4o-mini, GPT-4o, o1-mini), Deepseek series (V2.5, V3, R1), QwQ-32B, Gemini-2.0-flash-exp, and Yi-lightning. The models were grouped into two broad categories: open-sourced models (LLaMA, Qwen, Deepseek, Gemma variants) and proprietary models (including those from OpenAI, Anthropic, 01.AI, and Google DeepMind). The parameter counts of these models span from few billion to several hundred billion, allowing us to explore the performance of both lightweight models suited for resource-constrained environments and cutting-edge models optimized for highperformance tasks. We found that LLMs with fewer than 3 billion parameters failed to demonstrate significant non-trivial performance on KUMO  (Fig.3)  , so these models were excluded from our benchmarking analysis. In addition to evaluation, we use OpenAIs o1-preview for domain proposal and seed configuration generation, and apply GPT-4o for knowledge book creation  (Fig.2)  . We request the LLMs via APIs from the official websites of proprietary models and the Deepseek series. For other open-sourced models, we serve them on our own machine using the VLLM framework. Both API-based and VLLM-served models use the default temperature setting for LLM decoding. Automatic domain proposal and seed configs generation Users can interact with KUMO via Jupyter notebook to propose new domains, generate seed configurations, and create registered game environment class. Domain proposals utilize LLMs to generate natural language metadata, including descriptions of reasoning goals, truths, and domain-specific actions. For selected domains, the metadata is integrated into prompt template (Supplementary), forming domain-specific prompt used for seed configuration generation (Supplementary). The LLM then produces domain-specific seed configurations, which include universal truth set univ, universal action set Auniv, outcomes O, and symbolic version of knowledge book symb. When creating task instance under specific domain, the truth set and action set will be further sampled from univ and Auniv, and symb will be rewritten in nature. To handle potential truncation issues, the LLM is iteratively queried to verify the completeness of the output. Generation continues from truncation points as necessary, up to predefined retry limit (currently set to 3 attempts by default). Generated configurations are parsed into Python data structures, stored, and validated to ensure no truth is universally excluded by available actions. Invalid configurations are regenerated as needed. Finally, another prompt template (Supplementary) is used with an LLM to generate Python file defining the registered environment class. SAT-based task generation engine The SAT-based task generation engine systematically produces consistent and diverse task instances based on predefined domain (e.g., MedicalEnv, ChemicalEnv) characterized by universal truth set univ, universal action set Auniv, and an outcomes mapping and the symbolic version of knowledge book symb. The final truth set and action set for generated task will be subset of the universal truth set and action set. The generation process is governed by three primary parameters: the total number of truths truth, the number of actions action, and the number of valid truths valid. In this study, we set valid = 1, although this parameter can generally take any positive integer value less than truth. Initially, subset of truths sub consisting of truth elements is randomly sampled without replacement from the universal truth set univ. Within sub, exactly valid truths (T valid) are randomly designated as valid, representing conditions that observations cannot contradict. For every action Auniv and each associated outcome oa Oa, consistency with respect to the 14/28 selected truths is evaluated. An outcome oa is considered contradictory if it excludes any truth from valid; otherwise, it is deemed valid. The engine utilizes SAT solver to select appropriate combinations of actions and outcomes subject to specific constraints: each action may have at most one outcome selected (unique state per action constraint), the total number of selected actions must not exceed action (action limit constraint), and each invalid truth (truths in sub but not in valid) must be excluded by at least one selected outcome (invalid truth exclusion constraint). If fewer than action actions are selected initially, additional actions are included, first choosing unused related (non-contradictory) actions, and subsequently, if required, irrelevant actions to meet the action count requirement. This procedure is iterated to generate the desired number of task instances, each of which is checked against existing instances to prevent duplication of truths, actions, and observed outcomes. Optimal search algorithm The optimal search algorithm leverages recursion to determine the minimal expected steps and the corresponding optimal action for the given truth space, Tcurrent, and action space, Acurrent, which are set as the vanilla truth set and action set A. The recursion continues until specific base conditions are met. Recursion terminates if either the current truth space size is one or fewer truths remain, or if the action space is empty. Another termination condition occurs if at least one truth in the current set is unrelated to any remaining actions (note that we set valid = 1). In both scenarios, the expected number of steps required, denoted as E[S], is one, signifying no further actions are necessary and only need to output the determined truth. If the base conditions are not satisfied, the algorithm constructs binary bitmask representation, B, to encode the current state efficiently. This bitmask assigns unique indices to each truth (indexed 0 to 1) and each action (indexed to + 1), computing the mask as: (cid:88) = 2idx(t) + (cid:88) 2idx(a). tTcurrent aAcurrent (1) The algorithm then checks if this bitmask exists in memoization table, BestActionDict. If match is found, it retrieves the stored values of expected steps E[S] and the optimal action directly from the table, avoiding redundant computation. When the bitmask is not present in BestActionDict, the algorithm proceeds with recursive computation, initializing the minimal expected steps E[S]min to infinity and the best action to undefined. It iteratively evaluates each possible action within the current action space. For each evaluated action, the algorithm adjusts the subsequent action space by excluding the selected action, thus defining Anext = Acurrent {a}. To quantify state transitions, the algorithm calculates state probabilities based on the uniform assumption across truths in the current space. Each action leads to different outcomes oa Oa, with each outcome excluding certain truths and lead to new truth set Toa Tcurrent. Outcome probabilities (oa) are calculated using weighting factor (oa), defined as the size of the current truth space Toa . The probability for each state is normalized accordingly: (oa) = Toa, (oa) = where ϵ prevents division by zero. (oa) (o (cid:80) aOa a) + ϵ , oa Oa, (2) 15/28 Subsequently, the algorithm recursively computes the cumulative expected steps for each action, E[Sa]. For every result oa resulting from action a, it generates reduced truth space Toa by excluding the truths removed by state oa. The expected steps for these reduced spaces are calculated recursively, weighted by their probabilities, and summed to update the cumulative steps E[Sa]. If at any point E[Sa] surpasses the current minimum E[S]min, the algorithm prunes further evaluation for action a. After evaluating all states for each action, if the newly computed E[Sa] is lower than the current minimum, the algorithm updates E[S]min and designates the current action as the best choice, a. The resulting minimal expected steps and corresponding optimal action are stored in the memoization table BestActionDict for future reference. Finally, the algorithm returns the optimal solution with the minimal expected number of steps, along with the optimal action a. Evaluation metrics We evaluate the reasoning performance using two metrics: (1) Success Rate measures whether the model correctly identifies the valid truth. It assigns binary score, where 1 indicates correct identification, and 0 otherwise. The success rate is computed as: Success Rate = Number of Correct Identifications Total Number of Tasks (3) Relative Action Count evaluates efficiency by comparing the number of actions taken by the model against an optimal baseline. Specifically, it measures the deviation between the models actions and the optimal number of actions (determined through an optimal search strategy). It is computed as: Relative Action Count = Model Action Count Optimal Action Count Optimal Action Count (4) lower relative action count indicates closer alignment with optimal reasoning. We use relative action counts rather than absolute counts to normalize for task-specific differences, as tasks naturally vary in complexity and inherently require differing numbers of actions. Design of the human experiment total of 92 participants were recruited for the study, comprising 48 males and 44 females. The participants ages ranged from 18 to 29 years, with 77.2% falling within the 1922 age group. They represented diverse academic backgrounds, spanning 47 different majors across 72 universities. Among the participants, 82 were undergraduate students, 8 were pursing masters degrees, and 2 were pursuing doctoral degrees. The experimental procedure was conducted on an online platform specifically designed for this study, developed using Streamlit. Participants logged in using an assigned User ID and password. Each participant was randomly assigned complete task set, consisting of 10 reasoning tasks. Half of the tasks were Easy (#Truths=4) and the other half were Hard (#Truths=12). Each task covered five domains evenly: MedicalEnv, EducationEnv, MusicEnv, FantasyEnv, and ChemicalEnv. detailed English knowledge book was presented on the left side of the screen, containing the information needed to complete the reasoning tasks. On the right side, participants selected actions from menu. After selecting an action, the system displayed an observation related to that action, which could be used to eliminate invalid truths (Fig.1a). The objective for participants was to identify the only valid truth based on the observations and the knowledge book, while minimizing the number of actions taken. Performance was also evaluated using the same metrics as those applied to LLMs. The total earnings per task set is 16/28 calculated as 25 + success rate 15 #action count 0.1, with penalties for incorrect answers and excessive actions. To ensure data quality, participant behavior was monitored throughout the study. Any participant who failed to meet the data-quality threshold (providing random answers in very short amount of time) was excluded from the analysis, and their compensation was forfeited. In the end, we collected 500 high-quality game trajectories. Domain graph visualization We use domain graph to represent and analyze the internal graph structures between truths within domain. The domain graph presented (Fig.3e) is constructed by connecting pairs of truths that co-occur within the same actions state mappings, thus forming edges between them. To uncover the underlying community structure, we apply the Louvain community detection algorithm [42]. This algorithm optimizes modularity, defined mathematically as: = 1 2m (cid:20) (cid:88) ij Aij (cid:21) kikj 2m δ(ci, cj) (5) where Aij represents the adjacency matrix of the graph, ki and kj are the degrees of nodes and j, is the total number of edges, and δ(ci, cj) is the Kronecker delta function, which equals 1 if nodes and belong to the same community c, and 0 otherwise. The Louvain algorithm iteratively optimizes this modularity measure to partition the graph into densely interconnected clusters. Each detected community is assigned distinct color in the visualization, enabling immediate identification of semantically related truth groupings. Environment split via connection analysis To partition predefined domain into two disjoint sub-domains, we use connection-based method. Each action is associated with set of related truths; truths linked to the same action are considered connected. We construct truth graph where nodes represent truths and edges connect those sharing an action. Using Depth-First Search (DFS), we identify connected componentsclusters of interrelated truths. These components are then alternately assigned to two disjoint sets to maintain balance: truth set T1 and truth set T2, ensuring that related truths stay together. Each action is then categorized: if all its related truths lie in T1, it is assigned to A1; otherwise, it goes to A2. Observations are split accordingly, preserving consistency across the data. Details of overfitting resistance experiment We perform supervised fine-tuning (SFT) on LLMs using golden trajectories generated by our optimal search algorithm to assess KUMO resistance to overfitting. We evaluate two LLMs of different scales: Qwen2.5-3B-instruct and Qwen2.5-14B-instruct. To support generalization analysis, we partition the MedicalEnv environment into two sub-domainsMedicalINDEnv and MedicalOODEnvusing our connection analysis method. Training and validation datasets are constructed from MedicalINDEnv under two difficulty settings: Easy (#Truths = 4, #Actions = 6) and Hard (#Truths = 12, #Actions = 16). For each setting, we sample 100,000 task instances, reserving the first 50 for validation and the remaining 99,950 for training. Golden trajectories for these instances are generated using the optimal search algorithm. Each sample begins with system message describing the game configurations and symbolic knowledge book (a Python dictionary capturing outcomes and rule-out information). Unlike in benchmarking experiments, this symbolic format is used to reduce the cost of LLM-generated knowledge books. During training, only the next-token prediction loss for the actions in the trajectory is included in backpropagation; losses 17/28 from system and user messages are masked out. We apply Low-Rank Adaptation (LoRA [43]) with rank of = 16 and scaling factor of α = 32. Optimization is performed using the AdamW [44, 45] optimizer with learning rate of 2 104, no weight decay, β1 = 0.9, β2 = 0.999, and ϵ = 1 108. The learning schedule includes linear warm-up over the first 3% of training steps, followed by cosine decay over the remaining 97%. We use gradient accumulation to achieve global batch size of 8 across all GPUs. Training proceeds for 100 epochs with no early stopping. Details of Chi-square test Leveraging synthetic reasoning tasks to construct KUMO offers key advantage: precise knowledge of each tasks underlying logical graph structure. This enables statistical investigation into how reasoning performance relates to domain graph properties, by analyzing the correlation between graph structures and model outcomes. In our experimental setup, we extract two core data components for each evaluated LLM based on the 100-domain experiment  (Fig.3)  : (1) task correctness, and (2) simplified representation of the tasks graph structure. For correctness, each model is evaluated on five trials per task across 100 domains (with 50 tasks per domain, totaling 5,000 tasks). task is labeled as correct if the model succeeds in at least 3 out of 5 trials, using majority voting to determine the final binary label. For the graph representation, we begin by constructing the full bipartite truth-action graph for each task. We then derive simplified structural signature by concatenating the sorted degree sequences of the truth nodes and action nodes. This signature is treated as categorical variable denoting graph structure. To assess the relationship between structure and performance, we conduct Chi-square test of independence between graph structure categories and binary correctness labels for each LLM. The p-value for the test is given by: = 1 Fχ2(X 2; k) where Fχ2 is the cumulative distribution function of the chi-square distribution with degrees of freedom, and 2 is the observed chi-square statistic. (6) To quantify the strength of association between graph structure and correctness, we compute Cramérs V: (cid:115) = 2 min{r 1, 1} (7) where is the total number of observations, and and are the number of rows and columns in the contingency table, respectively. Larger values of Cramérs indicate stronger dependence between task structure and model performance. Data availability The KUMO generated dataset and the evaluation results can be downloaded from our official Github repository https://github.com/linhaowei1/kumo. Code availability The code for KUMO, including the domain proposal, seed configuration generation, symbolic task generator, knowledge book generation, and game simulator, as well as the benchmarking of LLMs on KUMO, is available at https://github.com/linhaowei1/kumo. 18/28 References 1. Wason, P. C. & Johnson-Laird, P. N. Psychology of reasoning: Structure and content (1972). 2. Fagin, R. & Halpern, J. Y. Reasoning about knowledge and probability. In JACM (1988). 3. Polu, S. & Sutskever, I. Generative language modeling for automated theorem proving. ArXiv abs/2009.03393 (2020). 4. Jiang, A. Q. et al. Thor: Wielding hammers to integrate language models and automated theorem provers. In Oh, A. H., Agarwal, A., Belgrave, D. & Cho, K. (eds.) Advances in Neural Information Processing Systems (2022). 5. Jiang, A. Q. et al. Draft, sketch, and prove: Guiding formal theorem provers with informal proofs. In The Eleventh International Conference on Learning Representations (2023). 6. Yang, K. & Deng, J. Learning to prove theorems via interacting with proof assistants. In Chaudhuri, K. & Salakhutdinov, R. (eds.) Proceedings of the 36th International Conference on Machine Learning, vol. 97 of Proceedings of Machine Learning Research, 69846994 (PMLR, 2019). 7. Lauritzen, S. L. & Spiegelhalter, D. J. Local computations with probabilities on graphical royal statistical society series structures and their application to expert systems. J. b-methodological 50, 415448 (1990). 8. Pederson, S. P. Probabilistic networks and expert systems. Technometrics 43, 108 (2001). 9. Castillo, E. F., Gutiérrez, J. M. & Hadi, A. S. Expert systems and probabilistic network models. In Monographs in Computer Science (1996). 10. Sheikhtaheri, A., sadoughi, F. & Dehaghi, Z. H. Developing and using expert systems and neural networks in medicine: review on benefits and challenges. J. Med. Syst. 38, 16 (2014). 11. Neapolitan, R. E. Probabilistic reasoning in expert systems - theory and algorithms (2012). 12. Silver, D. et al. general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science 362, 1140 1144 (2018). 13. Trinh, T., Wu, Y., Le, Q., He, H. & Luong, T. Solving olympiad geometry without human demonstrations. Nature DOI: 10.1038/s41586-023-06747-5 (2024). 14. El-Kishky, A. et al. Competitive programming with large reasoning models. ArXiv abs/2502.06807 (2025). 15. DeepSeek-AI et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. ArXiv abs/2501.12948 (2025). 16. Wu, S. et al. comparative study on reasoning patterns of openais o1 model. CoRR abs/2410.13639 (2024). 19/28 17. Zhong, T. et al. Evaluation of openai o1: Opportunities and challenges of agi. ArXiv abs/2409.18486 (2024). 18. Qin, Y. et al. O1 replication journey: strategic progress report - part 1. ArXiv abs/2410.18982 (2024). 19. Stanovich, K. E. & West, R. F. Individual differences in reasoning: Implications for the rationality debate? Behav. Brain Sci. 645665, DOI: 10.1017/s0140525x00003435 (2000). 20. Evans, J. Intuition and reasoning: dual-process perspective. Psychol. Inq. 21, 313 326 (2010). 21. Wang, X. et al. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations (2023). 22. Zeng, Z. et al. Mr-ben: meta-reasoning benchmark for evaluating system-2 thinking in llms. In Neural Information Processing Systems (2024). 23. Liu, J. et al. Logiqa: challenge dataset for machine reading comprehension with logical reasoning. In IJCAI, 36223628 (2020). 24. Liu, H. et al. Logiqa 2.0an improved dataset for logical reasoning in natural language IEEE/ACM Trans. on Audio, Speech, Language Process. 31, 2947 understanding. (2023). 25. Yu, W., Jiang, Z., Dong, Y. & Feng, J. Reclor: reading comprehension dataset requiring logical reasoning. In International Conference on Learning Representations (2020). 26. Wang, S. et al. From lsat: The progress and challenges of complex reasoning. IEEE/ACM Trans. on Audio, Speech, Language Process. 30, 22012216 (2021). 27. Li, Z., Hua, W., Wang, H., Zhu, H. & Zhang, Y. Formal-llm: Integrating formal language and natural language for controllable llm-based agents. ArXiv abs/2402.00798 (2024). 28. Zhong, W. et al. AGIEval: human-centric benchmark for evaluating foundation models. In Duh, K., Gomez, H. & Bethard, S. (eds.) Findings of the Association for Computational Linguistics: NAACL 2024, 22992314, DOI: 10.18653/v1/2024.findings-naacl.149 (Association for Computational Linguistics, Mexico City, Mexico, 2024). 29. Bean, A. M. et al. LINGOLY: benchmark of olympiad-level linguistic reasoning puzzles in low resource and extinct languages. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track (2024). 30. Han, S. et al. FOLIO: Natural language reasoning with first-order logic. In Al-Onaizan, Y., Bansal, M. & Chen, Y.-N. (eds.) Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 2201722031, DOI: 10.18653/v1/2024.emnlp-main.1229 (Association for Computational Linguistics, Miami, Florida, USA, 2024). 31. Sinha, K., Sodhani, S., Dong, J., Pineau, J. & Hamilton, W. L. Clutrr: diagnostic benchmark for inductive reasoning from text. In Conference on Empirical Methods in Natural Language Processing (2019). 20/28 32. Cobbe, K. et al. Training verifiers to solve math word problems. ArXiv abs/2110.14168 (2021). 33. Dong, Y. et al. Generalization or memorization: Data contamination and trustworthy evaluation for large language models. In Ku, L.-W., Martins, A. & Srikumar, V. (eds.) Findings of the Association for Computational Linguistics: ACL 2024, 1203912050, DOI: 10.18653/v1/2024. findings-acl.716 (Association for Computational Linguistics, Bangkok, Thailand, 2024). 34. Roberts, M., Thakur, H., Herlihy, C., White, C. & Dooley, S. Data contamination through the lens of time. ArXiv abs/2310.10628 (2023). 35. Balloccu, S., Schmidtová, P., Lango, M. & Dusek, O. Leak, cheat, repeat: Data contamination and evaluation malpractices in closed-source LLMs. In Graham, Y. & Purver, M. (eds.) Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), 6793 (Association for Computational Linguistics, St. Julians, Malta, 2024). 36. Aiyappa, R., An, J., Kwak, H. & Ahn, Y.-y. Can we trust the evaluation on ChatGPT? In Ovalle, A. et al. (eds.) Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023), 4754, DOI: 10.18653/v1/2023.trustnlp-1.5 (Association for Computational Linguistics, Toronto, Canada, 2023). 37. Jain, N. et al. Livecodebench: Holistic and contamination free evaluation of large language models for code. In The Thirteenth International Conference on Learning Representations (2025). 38. Roberts, M., Thakur, H., Herlihy, C., White, C. & Dooley, S. To the cutoff... and beyond? longitudinal perspective on llm data contamination. In International Conference on Learning Representations (2024). 39. Zhang, H. et al. careful examination of large language model performance on grade school arithmetic. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track (2024). 40. White, C. et al. Livebench: challenging, contamination-limited LLM benchmark. In The Thirteenth International Conference on Learning Representations (2025). 41. Zhu, K. et al. Dyval: Dynamic evaluation of large language models for reasoning tasks. In International Conference on Learning Representations (2023). 42. Blondel, V. D., Guillaume, J.-L., Lambiotte, R. & Lefebvre, E. Fast unfolding of communities in large networks. J. Stat. Mech. Theory Experiment 2008, P10008 (2008). 43. Hu, J. E. et al. Lora: Low-rank adaptation of large language models. ArXiv abs/2106. (2021). 44. Kingma, D. P. & Ba, J. Adam: method for stochastic optimization. CoRR abs/1412.6980 (2014). 45. Loshchilov, I. & Hutter, F. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017). 21/"
        },
        {
            "title": "Supplementary",
            "content": "Examplar domain proposal { } Goal: Identify the being disrupting the fabric of space-time, Truths: Traits of transdimensional entities, Actions: Interaction experiments / Dimensional stability monitoring / Entity behavior analysis Seed config generation template Below is prompt for an LLM to generate configuration for reasoning game. Your task is to propose situation for the game by filling in the missing values for the prompt. For example: **TRUTH**: The domain or topic of knowledge for the game (e.g., \"Diseases\"). **ACTION**: The primary activity or process the player engages in (e.g., \"Diagnosis\"). **GOAL**: The objective or outcome the player seeks to achieve (e.g., \"identify the disease of patient\"). **<u>you only need to create the prompt instead of the configuration!!!</u>** # Prompt Template Generate configuration in Python for {DOMAIN} reasoning game. The goal of the game is to determine {GOAL} based on observed test outcomes. The configuration should follow the same format as the example. **Requirements:** 1. **Truths**: Define list of {TRUTH} for {GOAL}, such as {TRUTH_EXAMPLE1}, {TRUTH_EXAMPLE2}, {TRUTH_EXAMPLE3}. 2. **Actions**: Define list of {ACTION} for {GOAL}, such as {ACTION_EXAMPLE1}, {ACTION_EXAMPLE2}, {ACTION_EXAMPLE3}. 3. **Outcomes**: For each {ACTION}, specify the type of outcomes (e.g., \"str\" or \"float\") and define possible outcome states. Each outcome state should **rule out** certain {TRUTH} rather than confirming them. Avoid 1-to-1 mappings wherever possible; make outcomes broader and applicable to multiple {TRUTHS}. For instance: - {STATE_EXAMPLE1} on {ACTION_EXAMPLE1} could rule out lack of {TRUTH_EXAMPLE1} and {TRUTH_EXAMPLE2}. 4. Ensure that the configuration is comprehensive and maintains logical relationships between {TRUTH}, {ACTION}, and outcomes. 22/28 **Example of {TRUTHS}, {ACTIONS}, and Outcomes:** - **{TRUTH} (Truths)**: [{TRUTH_EXAMPLE1}, {TRUTH_EXAMPLE2}, {TRUTH_EXAMPLE3}, {TRUTH_EXAMPLE4}] - **{ACTIONS} (Actions)**: [{ACTION_EXAMPLE1}, {ACTION_EXAMPLE2}, {ACTION_EXAMPLE3}, {ACTION_EXAMPLE4}] - **Outcomes**: Define the outcomes for each {ACTION} with states that correspond to **ruling out** certain {TRUTH}. Use the following format: python Truths = [ # List of {TRUTHS} ] Actions = [ # List of {ACTIONS} ] Outcomes = { # Define outcomes for each test \"Test Name\": { \"type\": \"str or float\", \"states\": { \"Outcome State 1\": set(), # Set of ruled-out {TRUTH} \"Outcome State 2\": set(), # Set of ruled-out {TRUTH} ... } }, ... } Generate the configuration as requested, ensuring that: - Use tuple to represent float type. e.g., (85, 100). Do not use inf (assume all values are in reasonable range). - We allow for some states that correspond to empty set. - Float type and string type outcomes should both exist. - Each test has at least 2 possible outcome states. - The outcome states relate to several truths, and it would be better to avoid 1-to-1 mappings between actions and truths (**not enforced, sometimes 1-to-1 is allowed**). - The relationships between outcomes and truths are meaningful and logical within the {DOMAIN} domain. 23/28 - Generate at least 30 actions and 50 truths. Each action should have its outcomes in the Outcomes dict. The action cannot rule out one truth in every state. - **The set of truths for state SHOULD BE ruled out by the state outcome**. Examplar config generation prompt Generate configuration in Python for **Transdimensional Entity Identification** reasoning game. The goal of the game is to determine **the being disrupting the fabric of space-time** based on observed test outcomes. The configuration should follow the same format as the example. **Requirements:** 1. **Truths**: Define list of **traits of transdimensional entities** for identifying the being disrupting the fabric of space-time, such as **\"Dimensional Instability\"**, **\"Temporal Anomalies\"**, **\"Spatial Distortion\"**. 2. **Actions**: Define list of **diagnostic tests and analyses** for identifying the being disrupting the fabric of space-time, such as **\"Interaction Experiments\"**, **\"Dimensional Stability Monitoring\"**, **\"Entity Behavior Analysis\"**. 3. **Outcomes**: For each **test or analysis**, specify the type of outcomes (e.g., str or float) and define possible outcome states. Each outcome state should **rule out** certain **traits** rather than confirming them. Avoid 1-to-1 mappings wherever possible; make outcomes broader and applicable to multiple traits. For instance: - An **\"Unstable Reading\"** on **\"Dimensional Stability Monitoring\"** could rule out lack of **\"Dimensional Instability\"** and **\"Spatial Distortion\"**. 4. Ensure that the configuration is comprehensive and maintains logical relationships between **traits**, **tests**, and outcomes. **Example of Traits, Actions, and Outcomes:** - **Traits (Truths)**: [\"Dimensional Instability\", \"Temporal Anomalies\", \"Spatial Distortion\", \"Quantum Fluctuations\"] - **Tests and Analyses (Actions)**: [\"Interaction Experiments\", \"Dimensional Stability Monitoring\", \"Entity Behavior Analysis\", \"Quantum Field Assessment\"] - **Outcomes**: Define the outcomes for each **test** with states that correspond to **ruling out** certain **traits**. Use the following format: python 24/28 Truths = [ # List of traits ] Actions = [ # List of tests and analyses ] Outcomes = { # Define outcomes for each test \"Test Name\": { \"type\": \"str or float\", \"states\": { \"Outcome State 1\": set(), # Set of ruled-out traits \"Outcome State 2\": set(), # Set of ruled-out traits ... } }, ... } Generate the configuration as requested, ensuring that: - Use tuple to represent float type, e.g., (85, 100). Do not use inf (assume all values are in reasonable range). - We allow for some states that correspond to an empty set. - Both float type and string type outcomes should exist. - Each test has at least 2 possible outcome states. - The outcome states relate to several truths, and it would be better to avoid 1-to-1 mappings between actions and truths (**not enforced; sometimes 1-to-1 is allowed**). - The relationships between outcomes and truths are meaningful and logical within the **Transdimensional Entity Identification** domain. - Generate at least **30 actions** and **50 truths**. Each action should have its outcomes in the Outcomes dict. The action cannot rule out one truth in every state. - **The set of truths for state SHOULD BE ruled out by the state outcome.** 25/28 2 3 4 5 7 8 9 10 11 13 14 15 16 17 19 20 Algorithm 1: SAT-based Task Generation input : Universal truth set univ, universal action set Auniv, outcomes mapping O, symbolic knowledge book symb, number of truths truth, number of actions action, number of valid truths valid, required number of task instances output : collection of generated task instances 1 for 1 to do // Randomly select truths and designate valid/invalid truths sub random subset of univ of size truth; valid random subset of sub of size valid; invalid sub valid; Initialize Dvalid ; Arel ; // Assess outcomes for contradiction and relevance foreach action Auniv do foreach outcome oa Oa do // Obtain the truths excluded by oa from symb if oa excludes any truth in sub then add to Arel ; truth in sub if oa excludes any truth in valid then // At least one of its outcomes excludes mark oa as contradictory (cannot coexist with valid); else record oa and its excluded truths in Dvalid; // Invoke SAT solver to select actions/outcomes (actions, outcomes) SATSolver(cid:0)Dvalid, invalid, action(cid:1); if the SAT solver reports unsatisfiable then resample the ith task again ; // Attempt again // Ensure we have action actions if #selected actions < action then select additional actions from Arel (only outcomes not marked contradictory), then from irrelevant actions if needed, until #actions = action; // Check against duplication and save if unique if generated instance is not duplicate then save the generated instance; 26/28 Algorithm 2: SATSolver Subroutine input : Dvalid: set of outcomes (each with its list of excluded truths) that do not contradict valid, invalid: set of truths to be invalidated, action: maximum number of actions allowed output : subset of actions actions and chosen outcome for each selected action (if any) // Create boolean variables 1 For each outcome oa in Dvalid, define boolean variable xa,oa indicating selection of oa. // Enforce constraints 1. Unique state per action constraint: For each action a, at most one oa can be selected. Formally, (cid:88) xa,oa 1. oaDvalid:belongs to action 2. Action limit constraint: The total number of selected actions must not exceed action. If Achosen is the set of actions with at least one outcome selected, Achosen action. 3. Invalid truth exclusion constraint: For every invalid, there must be at least one selected outcome oa that excludes t. In terms of boolean variables, invalid, (cid:95) xa,oa = 1. oa excludes // Solve the SAT formula 2 Use any standard SAT solver to solve for {xa,oa}. If satisfying assignment is found, record the selected outcomes (those xa,oa = 1). The resulting actions are those for which at least one oa is selected. Return (actions, outcomes). If unsatisfiable, indicate failure. 27/28 Algorithm 3: OptimalSearch(T, A) input : Current Truth Space Tcurrent, Current Action Space Acureent, Truth-Action Mapping MT output : Minimum Expected Steps E[S], Best Action /* Check base cases: terminate the recursion if one of the base */ /* Encode the current state into bitmask cases is satisfied 1 if Tcurrent 1 or Acurrent = 0 then 2 3 for each Tcurrent do 4 return 0, None; if no Acurrent is related to then return 0, None; 2idx(a); tTcurrent aAcurrent 2idx(t) + (cid:80) 6 (cid:80) 7 if exists in BestActionDict then return BestActionDict[B]; 8 9 Initialize E[S]min +, 10 for each Acurrent do Anext Acurrent {a}; for each state Sa do None; (s) Tcurrent Ts Tcurrent; (cid:80) for each state Sa do (s) + ϵ; sSa (s) ; (s) E[S]a 0; for each state Sa do Tnext Tcurrent Ts; E[S]next, _ OptimalSearch(Tnext, Anext); E[S]a E[S]a + (s) E[S]next; if E[S]a E[S]min then break; if E[S]a < E[S]min then E[S]min E[S]a, a; 25 26 BestActionDict[B] (1 + E[S]min, a); 27 return 1 + E[S]min, a; 12 13 14 15 17 18 19 20 21 23 24 */ 28/"
        }
    ],
    "affiliations": [
        "Computer Science Department, Stanford University, California, United States",
        "Department of Electronic Engineering, Tsinghua University, Beijing, China",
        "Institute for AI Industry Research, Tsinghua University, Beijing, China",
        "Institute for Artificial Intelligence, Peking University, Beijing, China",
        "Wangxuan institute of computer technology, Peking University, Beijing, China"
    ]
}