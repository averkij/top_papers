{
    "paper_title": "Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred Image",
    "authors": [
        "Jerred Chen",
        "Ronald Clark"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In many robotics and VR/AR applications, fast camera motions cause a high level of motion blur, causing existing camera pose estimation methods to fail. In this work, we propose a novel framework that leverages motion blur as a rich cue for motion estimation rather than treating it as an unwanted artifact. Our approach works by predicting a dense motion flow field and a monocular depth map directly from a single motion-blurred image. We then recover the instantaneous camera velocity by solving a linear least squares problem under the small motion assumption. In essence, our method produces an IMU-like measurement that robustly captures fast and aggressive camera movements. To train our model, we construct a large-scale dataset with realistic synthetic motion blur derived from ScanNet++v2 and further refine our model by training end-to-end on real data using our fully differentiable pipeline. Extensive evaluations on real-world benchmarks demonstrate that our method achieves state-of-the-art angular and translational velocity estimates, outperforming current methods like MASt3R and COLMAP."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 8 5 3 7 1 . 3 0 5 2 : r Image as an IMU: Estimating Camera Motion from Single Motion-Blurred Image Jerred Chen University of Oxford Department of Computer Science jerred.chen@cs.ox.ac.uk Ronald Clark University of Oxford Department of Computer Science ronald.clark@cs.ox.ac.uk"
        },
        {
            "title": "Abstract",
            "content": "In many robotics and VR/AR applications, fast camera motions cause high level of motion blur, causing existing camera pose estimation methods to fail. In this work, we propose novel framework that leverages motion blur as rich cue for motion estimation rather than treating it as an unwanted artifact. Our approach works by predicting dense motion flow field and monocular depth map directly from single motion-blurred image. We then recover the instantaneous camera velocity by solving linear least squares problem under the small motion assumption. In essence, our method produces an IMU-like measurement that robustly captures fast and aggressive camera movements. To train our model, we construct largescale dataset with realistic synthetic motion blur derived from ScanNet++v2 and further refine our model by training end-to-end on real data using our fully differentiable pipeline. Extensive evaluations on real-world benchmarks demonstrate that our method achieves state-of-the-art angular and translational velocity estimates, outperforming current methods like MASt3R and COLMAP. 1. Introduction Given sequence of images, visual odometry (VO) and Structure from Motion (SfM) methods have advanced to the point where they can accurately estimate camera poses even under moderately challenging conditions. However, these methods assume that the camera remains largely stationary during exposure, allowing each image to be treated as snapshot of the scene that can be matched against subsequent images to compute relative camera poses. This assumption, however, ignores the continuous nature of camera motion and becomes especially problematic during fast movements where motion blur can severely degrade accuracy. Conventional approaches either discard motionblurred frames or use inertial measurement units (IMUs) to Figure 1. Existing methods rely on establishing correspondences between multiple frames to estimate inter-frame camera motion (a). This leads to failures during fast motion with lots of blur. We propose method that can estimate intra-frame camera motion from single image (b), making our method robust to aggressive motions. improve robustness against blur. However, using IMUs introduces additional challenges such as sensor synchronization and drift. In this work, we therefore take fundamentally different approach: rather than treating motion blur as an unwanted artifact, we exploit it as rich source of information about the cameras motion (see Figure 1). Our key observation is that the extent and direction of motion blur provide direct cues about the cameras motion during an exposure. Building on this idea, we propose model that, using just the motion-blur in an image, can estimate the relative motion that the camera undergoes during the exposure. Our model works in two stages: first we predict the motion flow field and monocular depth map, then we recover the relative pose of the camera by solving linear least squares system. Notably, our method does not require any explicit deblurring process, and with known exposure time, it yields an instantaneous rotational and translation velocity estimate that can be interpreted as an IMUlike measurement. 1 Since no existing dataset includes all the necessary data to train such model, we construct our own dataset using subset of ScanNet++v2 [47] and show how we obtain realistic motion blur that allows for generalization to in-thewild images. Furthermore, the fully differentiable nature of our method enables us to refine our model using real-world motion-blurred images, even when ground-truth flows and depths are not available for training. We evaluate our model with angular and translational velocity estimates on realworld motion blurred images to test the generalization capabilities of our model. Our method runs in real-time at 30 FPS and obtains significantly more accurate velocity estimates compared to the existing state of the art methods. In summary, our contributions are as follows: 1. new approach for estimating camera motion that uses the blur present in frame to provide extremeley robust and accurate relative pose estimates 2. pipeline for synthesizing the data needed to train the model (i.e. blurry images with ground-truth motion) from standard SLAM datasets 3. An evaluation of our method on real-world sequences, showing our method obtains state of the art results and is robust to aggressive camera motion The rest of the paper is as follows. We first discuss relevant works to our method in Section 2. We then introduce preliminary information and provide high-level intuition of our method in Section 3. In Section 4, we go into technical details about our method. Section 5 shows the dataset construction process. We evaluate our results against baseline methods in Section 6. 2. Related Works Motion from Blur. This paper is most closely related to the line of work referred to as motion from blur, which aims to extract pixel flow information only from single motion blurred image. [5] [26] are among the earliest works within this topic, where [5] introduced linear model for computing the motion vectors and [26] independently analyzed the frequency domain to identify blurred regions. However, both methods assume that there is only linear, spatially-invariant blur in the image. [32] built on top of [26] by using the three color channels of an image. The seminal work [7] proposed straightforward method where flow could be estimated using the alpha channel of an image. limitation of these aforementioned methods is the inability to distinguish directional information with the motion vectors. [11] resolves this and introduces variational approach to estimate the motion vectors and the blur segmentation in isolation. [1] proposed the first deep learned solution using spatial transformer network. These methods only focus on estimating dense flow field from the motion blurred image, whereas we are interested in the usefulness of the flow field for obtaining camera motion. Our method also estimates the depth of the scene given the motion blurred image to retrieve the metric camera motion. Similarly [16], [20], and [25] demonstrate how to recover the 3D geometry even in motion-blurred scenes. [20] is the most relevant to our work and presented visual odometry method which computes the homography during exposure time of blurred image, which demonstrates increased robustness in severely blurred videos. However, they minimize the photometric error between sharp keyframe and blurred frame, where they use off-theshelf deblurring method to do so. We do not require any deblurring and only need single image to estimate the motion. Camera Pose Estimation. Camera pose estimation is fundamental computer vision task used in 3D reconstruction which can be categorized into SfM and SLAM/VO methods. SfM methods operate on an unordered set of images and often involve estimate the camera intrinsics and extrinsincs. SLAM/VO methods can be viewed as subset of SfM with the added constraint for running in real-time, with and without loop closures respectively. For SfM methods, COLMAP [31] [30] is an incrementalSfM method that is commonly the default choice for estimating camera parameters, poses, and the scene geometry. ORB-SLAM [3] and DSO [9], on the other hand, are classic real-time methods for obtaining camera poses and mapping the environment. While highly accurate in sequences with large overlap, these traditional systems succumb to failures under adversarial conditions, including few-view reconstruction and motion blur. These failures motivate deep-learning based approaches which are more robust in these difficult scenarios. Several fully-differentiable SfM pipelines have been proposed for greatly improving 3D reconstruction and pose estimation robustness [42] [34] [2]. In particular, DUSt3R [44] and MASt3R [17] have shown that directly regressing scene geometry in the form of pointmaps enables incredible performance, which has inspired line of -3R-related works [48] [41] [33] [46] [43]. Similarly, SLAM and VO methods have also underwent deep-learning revolution and have similarly adopted learned modules with improved performance [6] [45] [37] [35] [28] [24]. For example, DROID-SLAM [37] is an endto-end solution for obtaining accurate real-time reconstructions using RAFT [36] for estimating optical flow. These approaches rely on using two or more images for estimating the camera motion, whereas our method is able to use single motion-blurred frame to estimate camera velocity. Due to there being no existing methods that estimate camera motion from single frame, we opt to use these standard camera pose estimation methods, specifically MASt3R, DROID-SLAM and COLMAP, and estimate the velocities with finite-difference approximation. 2 Figure 2. Overview of our method. Given single motion blurred image, we pass it through the network to obtain dense flow field and monocular depth prediction (Section 4.1). These are then formulated in linear system, where the optimal velocity parameters are solved for using linear least squares (Section 4.2). Because the linear solver is fully differentiable, we can train the entire network end-to-end, supervised on the camera motion. 3. Preliminaries Before introducing our method, we first revisit the image formation process. During the opening of the shutter, photons come into contact with the camera sensor and are collected to capture each pixels intensity. If the camera has moved during the exposure time, each sensor pixel receives photons from different parts of the scene and motion-blur is created. The length of the motion blur traces depends on the speed of the camera as well as the exposure time i.e. when the exposure time is lengthened, e.g. in low-illuminated settings, or if the camera moves faster relative to the scene, the blur traces are longer. More formally, let RCHW represent blurry image, such that: = (cid:18) 1 τ (cid:90) τ 0 (cid:19) ν(t)dt (1) where ν(t) RCHW represents the response of the sensor to incoming photons at timestep t, with the exposure time length denoted by τ , and g() representing the conversion from linear space to sRGB [27]. In practice, we approximate the creation of blurry image in discretized fashion: (cid:32) (cid:33) 1 (cid:88) i= ν (2) where we assume that {I ν that existed in exposure time. 1 , ..., ν } are the virtual images part of the overall blur, and the blur traces therefore act as cues that represent the motion the camera has undergone during the exposure. In essence, these blur traces can be seen as providing virtual correspondences between the first and last virtual image. We hypothesize that we can extract these virtual correspondences, revealing how each point in the scene moved across the image plane and capturing information similar to an optical flow field. Since we assume the scene is rigid, each virtual correspondence can be interpreted as match between two distinct virtual images representing the camera at the beginning and end of the exposure period (see Figure 1). Furthermore, by estimating depth map from the blurred image, we can combine the pixel motion information extracted from the motion blur traces with the scene geometry to solve for the relative pose change from the start to the end of the exposure. 4. Method An overview of our method can be seen in Figure 2. Our approach consists of two stages, the first stage takes as input single frame and predicts flow-field representing the motion the camera underwent during the exposure, and monocular depth map. The second stage uses the depth and flow to solve for the instantaneous velocity of the camera using differentiable least squares solver. 4.1. Flow and Depth Prediction The intuition for our method is that each motion-blurred image can be seen as composite of several virtual images, each capturing the camera at different pose during the exposure. Each individual virtual image contributes We follow GeoCalib [40] and use SegNeXt [12] as the backbone for our network. The blurry image is passed through shared SegNeXt encoder that maps to separate decoders, outputting the pixel-wise flow field R2HW 3 and depth map R1HW . Each flow vector = [Fx, Fy] is defined to be the pixel displacement from given pixel location in the original virtual frame ν 1 :"
        },
        {
            "title": "These equations can be expressed in matrix form as",
            "content": "Ax = b, where: = 2 p1 (3) = (cid:34) 0 0 px py pxpy (py)2+f 2 (px)2+f 2 pxpy such that = [px, py], and p1, 2 are the corresponding pixel coordinates from the first and last virtual frames ν 1 and ν respectively. We impose an L1 loss for both the flow field and the depth predictions: L1 = λF hf ( ˆFf w, ˆFbw) + λDD ˆD (4) where ˆ represents the ground truth labels, λF , λD are weights to balance the two losses, and hf is the reorientation function defined as: hf ( ˆFf w, ˆFbw; F) = (cid:40) ˆFf if ˆFf w, > ˆFbw, ˆFbw otherwise (5) 1 to ν to ν such that , is the Frobenius inner product, and ˆFf w, ˆFbw are the corresponding flow fields in the ν direction and the ν 1 direction, respectively. Because determining the flow and depth from the true start virtual frame is ill-posed, we incorporate our reorientation function hf so that the label ˆF is closest to the predicted direction of F. This stabilizes the training and enables the model to produce globally consistent outputs. 4.2. Differentiable Velocity Computation We can now use and to estimate the relative translation and rotation parameters = [tx, ty, tz] R3 and θ = [θx, θy, θz] R3 across exposure. To do so, we adapt the motion field equations described in Trucco and Verri [38] (see Supplementary 1), which we express as: Fx = + Fy = θyf + θzpy tzpx txf θxpxpy θy(px)2 + θxf θzpx tzpy tyf θypxpy + θx(py)2 . such that and is the known focal length. (6) (7) (cid:35) , py px (8) (9) = , = (cid:21) (cid:20)Fx Fy . tx ty tz θx θy θz With multiple flow vectors, the system becomes overdetermined and is solved using the least squares method: = (AA)1Ab. (10) This approach leverages the relationship between pixel displacements and camera pose, enabling closed-form estimation of the relative pose parameters and θ from the predicted flow and depths. With provided exposure time, we can then compute the instantaneous velocities and ω across exposure. convenient property of this linear least squares formulation is that this is differentiable, which allows us to train the network fully end-to-end with clear pose supervision. Therefore, our final end-to-end loss is: L2 = λRR hp( ˆR)2 + λtt hp(ˆt)2 + L1 (11) where we compute MSE on the rotations SO(3) and translations t. λR and λt help balance the loss, and hp, similar to Equation 5, reorients the ˆR or ˆt in such way that is closest to the prediction. 4.3. Direction Disambiguation While the method we have proposed so far can estimate the magnitude of the camera velocity directly, the direction has 180 ambiguity because the blurring process erases temporal cues. For instance, both leftward or rightward motion of the camera will result in the same horizontal blur traces. To address this issue, we introduce simple strategy to resolve the directional ambiguity. Recall τ is the exposure time length and let tf be the time gap between the start of frame shutter. For frame Ii, we linearly extrapolate the flow field Fi and apply the warping function Φ to Ii: = tf τ Fi = Φ(Ii; i) (12) (13) poses across exposure time. Since no public dataset meets these requirements, we create our own by adapting subset of ScanNet++v2 [47]. Our data pipeline is shown in Figure 3. The ScanNet++v2 dataset consists of about 1000 iPhone videos of various indoor scenes with COLMAP [30] [31] and ARKit-captured poses. Because we are interested in obtaining relative poses that are across very short time spans (fractions of second) and robust to fast-motion, we opt to use the ARKit poses to compute our ground truth motion. Before curating the dataset, we first preprocess the ARKit poses and filter out any jumps in the localization. , ν , ..., ν Synthesizing blur. We now detail how we select our virtual frames for creating motion blurred image B. For selected RGB image ν in given video sequence, we obi tain the following images to have collection of + 1 images {I ν i+N }. We then use an off-the-shelf frame interpolation model RIFE [13] to interpolate between each consecutive frame, resulting in total of +(N 1) virtual frames = {I ν i+N }. We follow Equation 2 and first convert in linear space g1() before averaging all images and converting back to sRGB space to create B. Rather than simply interpolating 1 virtual frames between two images, we intentionally interpolate between several real-world frames to 1) have more realistic motion trajectories to facilitate generalization for in-the-wild motion blur, and 2) to prevent RIFE from producing unreasonable blurred images. We curate images with = 3 for small motion and = 10 for large motion. , . . . , ν i+ 1 2 1 and ν Obtaining depth and flows. We only use the first and last virtual frames ν for obtaining the ground-truth flow field. We use the low-resolution ARKit depth and ν 1 in PromptDA [18] to obtain our dense, high-resolution depth map ˆD as ground truth. For all pixels, we then perform: = KT dK 1p (17) which uses ˆD to backproject all pixel points in ν 1 and subsequently project them into ν . Afterwards, we retrieve the flow field ˆF with the displacements of the projected points as expressed in Equation 3. In total, we synthesize about 120k training samples and 1.2k validation samples across 150 ScanNet++v2 sequences, with corresponding blurred images, depth maps and flow fields. 5.2. Real-world Dataset is fullyAs our method, differentiable we can train it in an end-to-end manner. This enables us to additionally train on real-world motion blur including the velocity solver, Figure 3. Overview for our synthetic dataset generation process. After preprocessing the dataset, we run selected frames through an interpolation network, which we use to synthesize our blurred image. We also take the first and last virtual frames to generate ˆD, which is subsequently used for computing ˆF. For both the forward and backwards flow fields Ff and Fbw, we compute the photometric error between the corresponding warped image with the next true frame in the video sequence Ii+1: P(I1, I2) = 1 HW (cid:88) (cid:88) u=0 v=0 I1(u, v) I2(u, v). (14) We additionally do this photometric error check with warping Fi to Ii1 and compute the sum of the photometric error between the two directions, and reorient based on the smaller photometric error. ef = P(Ii+1, ebw = P(Ii+1, i,f w) + P(Ii1, i,bw) + P(Ii1, i,bw) i,f w) (cid:40) ω, = ωf w, vf if ef < ebw ωbw, vbw otherwise. (15) (16) This straightforward photometric error check enables us to reorient the direction of the camera velocities in the correct orientation. 5. Dataset Curation 5.1. Synthetic Dataset To train our model, we require dataset of blurred images with corresponding ground-truth flows, depths, and relative"
        },
        {
            "title": "Method",
            "content": "billiards commonroom dining office avg ωx / ωy / ωz ωx / ωy / ωz ωx / ωy / ωz ωx / ωy / ωz ωx / ωy / ωz COLMAP [30] [31] D+LG [39] [19] MASt3R [44] [17] DROID-SLAM [37] 2.32 / 1.94 / 1.50 5.30 / 2.85 / 4.45 5.39 / 3.33 / 5.31 1.19 / 1.61 / 0.91 3.70 / 3.75 / 3.26 3.01 / 5.89 / 3.57 2.36 / 0.84 / 1.67 2.92 / 1.20 / 1.98 3.06 / 2.51 / 3.93 4.78 / 3.03 / 6.21 6.33 / 4.90 / 5.56 4.04 / 2.62 / 3.90 4.41 / 3.83 / 4. Ours 1.31 / 0.87 / 1.60 0.93 / 0.88 / 1.04 0.87 / 0.50 / 1.33 1.76 / 1.38 / 3.08 1.22 / 0.91 / 1. Zero-Velocity baseline 5.39 / 3.43 / 5.16 3.95 / 4.50 / 2.81 4.58 / 1.53 / 3.66 5.43 / 3.19 / 6.99 4.84 / 3.16 / 4. Table 1. RMSE for rotational velocities across each axis, in rad/s. We evaluate against multi-image (MI) and single-image (SI) methods. The best and second-best results are bolded and underlined, respectively. The represents failure to reconstruct the poses."
        },
        {
            "title": "Method",
            "content": "billiards commonroom dining office avg vx / vy / vz vx / vy / vz vx / vy / vz vx / vy / vz vx / vy / vz COLMAP [30] [31] D+LG [39] [19] MASt3R [44] [17] DROID-SLAM [37] 3.11 / 2.06 / 2.14 2.59 / 1.50 / 3.52 3.28 / 2.00 / 2.65 1.70 / 2.09 / 2.20 1.28 / 1.94 / 2.49 2.23 / 1.47 / 1.25 0.82 / 0.55 / 1.22 1.44 / 0.65 / 0.91 1.81 / 2.45 / 1.52 1.72 / 2.16 / 1.46 1.98 / 2.40 / 1.23 1.60 / 1.54 / 2.17 2.23 / 1.63 / 1. Ours 1.36 / 1.17 / 1.05 1.00 / 0.80 / 0.71 0.95 / 0.61 / 0.81 1.12 / 1.52 / 1.12 1.11 / 1.03 / 0. Zero-Velocity baseline 2.80 / 1.66 / 1.41 1.98 / 1.48 / 1.26 1.30 / 1.05 / 1.20 1.94 / 2.26 / 1.07 2.01 / 1.61 / 1. Table 2. RMSE for translational velocities across each axis, in m/s. We evaluate both multi-image (MI) and single-image (SI) methods. The best and second-best results are bolded and underlined, respectively. The represents failure to reconstruct the poses. to further close the reality-gap between our synthetically blurred dataset and in-the-wild images. We therefore collect 10k real-world motion-blurred images with corresponding ARKit poses, IMU measurements, and exposure times across about 30 scenes. We follow Liu et al. [21] and perform the Fast Fourier Transform to identify blurred images, where images below specified threshold are blurry enough to be used for training or validation. We also supplement with small amount of motionless images (with no blur) to enhance the robustness to still frames. 6. Experiments Training. Our model is trained in three stages. We first train the flow and depth decoders without pose supervision on our synthetically blurred dataset with batch size of 32. Pose supervision is added once the depths and flow estimates are reasonable, and train poses, flow and depths with batch size of 8 for 300k steps. Finally, we finetune the model on the real-world collected images to close the reality gap from synthetic images for 10k steps. We train our method using the Adam optimizer [15] on single Nvidia RTX 3090. Evaluation. To assess the accuracy of our method, we evaluate the rotational velocities ω and translational velocities in four real-world videos (see Supplementary 2). For each video sequence, we get the ARKit-estimated camera poses, IMU measurements, and the exposure times for each image. We directly compare ω predictions to the gyroscope measurements and predictions to finite-difference velocity approximation from the ARKit translations. We treat the ARKit translations as the ground-truth as their visualinertial method fuses additional sensor information, which helps mitigate the poor visual input. We measure the RMSE across each axis for ω and v. Baselines. There are no existing methods that can estimate camera motion from single blurred image, so we instead use camera pose estimation methods and estimate the relative pose (and hence velocity) between frames using centered finite-difference approximation. We evaluate against the following SfM and SLAM baselines: COLMAP [30] [31] is an incremental-SfM method that is considered to be the de-facto method for obtaining camera poses and commonly serves as ground-truth for NeRFs [23] and Gaussian Splatting [14] methods. Because standard SfM is unreliable with small triangulation angles, we run COLMAP using all images in the se6 Figure 4. Visualization of the predicted velocities for the billiards sequence using our method, MASt3R and COLMAP (w/ DISK+LightGlue). The shaded area under the curve shows the error between the predicted velocity and GT velocity. Our translations and rotations are significantly better than MASt3R. While COLMAP with DISK + LightGlue feature matching does well on rotations, our method significantly outperforms it on translations. quence. After obtaining the camera poses across all images, we scale the trajectory with the ground-truth ARKit poses. We use the hloc package [29] to run COLMAP with SIFT [22] and AdaLAM [4] as well as learning baseline using DISK [39] and LightGlue [19]. MASt3R [44] [17] is recent learning-based 3D vision method that regresses the pointmaps between pair of images. MASt3R is the current SOTA in retrieving metricscale camera poses, particularly with extremely challenging views with little-to-no-overlap. To better represent our method, we evaluate MASt3R on pairwise images two frames apart. DROID-SLAM [37] is learning-based SLAM method that utilizes RAFT [36] to estimate the optical flow across frames to help compute the camera poses, and obtains SOTA accuracy and robustness particularly in adversarial conditions. To obtain the camera poses for each frame, we set DROID-SLAM to identify each image as keyframe by setting the filter and keyframe thresholds to 0. Note that all baselines require two or more images to estimate the velocity, whereas our method only needs single image. We therefore tackle more difficult problem, and our formulation also more closely resembles the true camera state. Results. We report the rotational and translational RMSE in Tables 1 and 2. While the recorded video sequences suffer from severe motion blur, our method is able to handle the motion very robustly. Figure 5 highlight the overall robustness of our method, with the error CDFs demonstrating that our method suffers from noticably less errors than the In these sequences, the baseline methods sufbaselines. fer from major outliers and zero-prediction failures, which can be observed in Figure 4. Velocity predictions from the 7 Figure 5. Error CDFs for the billiards sequence, such that the left plot shows the distribution of translational error in the sequence and the right plot the rotational error. Curves closer to the top-left corner are better. Figure 6. Real-world application example of our method using camera attached to fast-moving RoArm-M1 robot arm platform. (a) The robot arm used for recording. (b) The predicted and GT velocity time series for the camera attached to the end-effector. Figure 7. Comparison of our method to using IMU integration. The IMU velocity estimate is accurate for few seconds until it drifts. Our method provides accurate and drift-free estimates throughout the sequence. Method Runtime (s) Realtime factor COLMAP (D+LG) MASt3R DROID-SLAM Ours 182.98 67.90 41.61 6.48 0.038 0.103 0.169 1.08 Table 3. Runtimes of all methods on the billiards sequence showing the time taken to process the sequence and how much faster/slower the method is compared to realtime (original sequence is 7.08 long). Our method is the only one that operates faster than realtime. billiards sequence are shown with the remaining sequences included in Supplementary 3. Overall, we obtain an average of 31% reduction from the second-best result in rotational velocities and 24% reduction in translational velocities. To further showcase the usefulness of our method, we test our method in estimating the velocity of the gripper on moving robot arm. We place the camera on the robot pictured in Figure 6(a) and predict the velocity of the arm solely from the images in the recorded video. The motionblurred images give salient enough signal to accurately estimate the movement of the arm as seen in Figure 6(b). In Figure 7 we show comparison between our method and an actual IMU. To do this test we first estimate the IMU bias from stationary calibration video, followed by estimating the initial pose of the camera to properly align with the gravity vector. We then preintegrate the IMU measurements following [10] using GTSAM [8] and compare the integrated velocities to our predictions. Figure 7 shows the drift that occurs in vx and vz after only 20 seconds, whereas our method is reliably drift-free for the entire video. Therefore, while our method produces inertial-like measurements, we see in this example that our method provides an even better motion estimate than an IMU. We lastly measure the runtime of our method. Even when running on every frame in the sequence, we can run in real-time at 30 FPS, where the inference and direction disambiguation for each image takes on average 0.03 seconds on an RTX 3090 GPU. 7. Conclusion In this paper we have proposed method that can estimate the instantaneous camera velocity from individual motion-blurred images producing inertial-like measurements without an IMU. Our evaluations indicate that not only do we obtain more accurate and robust results compared to an actual IMU and other vision-based solutions, but we also do so significantly more efficiently. We hope that this motivates the community to explore further how traditionally unwanted or overlooked cues can be used to enhance the performance of vision tasks."
        },
        {
            "title": "References",
            "content": "[1] Dawit Mureja Argaw, Junsik Kim, Francois Rameau, Jae Won Cho, and In So Kweon. Optical Flow Estimation from Single Motion-blurred Image. Proceedings of the AAAI Conference on Artificial Intelligence, 35(2):891900, 2021. 2 [2] Eric Brachmann, Jamie Wynn, Shuai Chen, Tommaso Cavallari, Aron Monszpart, Daniyar Turmukhambetov, and Victor Adrian Prisacariu. Scene coordinate reconstruction: Posing of image collections via incremental learning of relocalizer. In ECCV, 2024. 2 [3] Carlos Campos, Richard Elvira, Juan J. Gomez Rodrıguez, Jose M. M. Montiel, and Juan D. Tardos. Orb-slam3: An accurate open-source library for visual, visualinertial, and IEEE Transactions on Robotics, 37(6): multimap slam. 18741890, 2021. 2 [4] Luca Cavalli, Viktor Larsson, Martin Ralf Oswald, Torsten Sattler, and Marc Pollefeys. Handcrafted outlier detection revisited. In European Conference on Computer Vision, 2020. 7 [5] Wei-Ge Chen, N. Nandhakumar, and W.N. Martin. Image motion estimation from motion smear-a new computational model. IEEE Transactions on Pattern Analysis and Machine Intelligence, 18(4):412425, 1996. 2 [6] Czarnowski, Laidlow, Clark, and AJ Davison. Deepfactors: Real-time probabilistic dense monocular slam. IEEE Robotics and Automation Letters, 5:721728, 2020. 2 [7] Shengyang Dai and Ying Wu. Motion from blur. In 2008 IEEE Conference on Computer Vision and Pattern Recognition, pages 18, 2008. ISSN: 1063-6919. 2 [8] Frank Dellaert and GTSAM Contributors. borglab/gtsam, 2022. 8 [9] J. Engel, V. Koltun, and D. Cremers. Direct sparse odomIEEE Transactions on Pattern Analysis and Machine etry. Intelligence, 2018. [10] Christian Forster, Luca Carlone, Frank Dellaert, and Davide Scaramuzza. On-manifold preintegration for real-time visualinertial odometry. IEEE Transactions on Robotics, 33 (1):121, 2016. 8 [11] Jochen Gast, Anita Sellent, and Stefan Roth. Parametric Object Motion from Blur. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1846 1854, Las Vegas, NV, 2016. IEEE. 2 [12] Meng-Hao Guo, Cheng-Ze Lu, Qibin Hou, Zhengning Liu, Ming-Ming Cheng, and Shi-Min Hu. Segnext: Rethinking convolutional attention design for semantic segmentation. arXiv preprint arXiv:2209.08575, 2022. 3 [13] Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi, and Shuchang Zhou. Real-Time Intermediate Flow Estimation for Video Frame Interpolation. In Computer Vision ECCV 2022, pages 624642. Springer Nature Switzerland, Cham, 2022. Series Title: Lecture Notes in Computer Science. 5 [14] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42 (4), 2023. 6 [15] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization, 2017. 6 [16] Hee Seok Lee and Kuoung Mu Lee. Dense 3D Reconstruction from Severely Blurred Images Using Single Moving Camera. In 2013 IEEE Conference on Computer Vision and Pattern Recognition, pages 273280, Portland, OR, USA, 2013. IEEE. 2 [17] Vincent Leroy, Yohann Cabon, and Jerome Revaud. Grounding image matching in 3d with mast3r, 2024. 2, 6, 7 [18] Haotong Lin, Sida Peng, Jingxiao Chen, Songyou Peng, Jiaming Sun, Minghuan Liu, Hujun Bao, Jiashi Feng, Xiaowei Zhou, and Bingyi Kang. Prompting depth anything for 4k resolution accurate metric depth estimation. 2024. 5 [19] Philipp Lindenberger, Paul-Edouard Sarlin, and Marc Pollefeys. LightGlue: Local Feature Matching at Light Speed. In ICCV, 2023. 6, 7 [20] Peidong Liu, Xingxing Zuo, Viktor Larsson, and Marc Pollefeys. MBA-VO: Motion Blur Aware Visual Odometry, 2021. arXiv:2103.13684 [cs]. [21] Renting Liu, Zhaorong Li, and Jiaya Jia. Image partial blur In 2008 IEEE Conference on detection and classification. Computer Vision and Pattern Recognition, pages 18, 2008. 6 [22] David G. Lowe. Distinctive image features from scaleinvariant keypoints. International Journal of Computer Vision, 60:91110, 2004. 7 [23] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020. 6 [24] Riku Murai, Eric Dexheimer, and Andrew J. Davison. Mast3r-slam: Real-time dense slam with 3d reconstruction priors, 2024. 2 [25] Jiayan Qiu, Xinchao Wang, Stephen J. Maybank, and Dacheng Tao. World from blur. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 84858496, 2019. 2 [26] Ioannis M. Rekleitis. Optical flow recognition from the power spectrum of single blurred image. In International Conference in Image Processing, Lausanne, Switzerland, 1996. IEEE Signal Processing Society. [27] Jaesung Rim, Geonung Kim, Jungeon Kim, Junyong Lee, Seungyong Lee, and Sunghyun Cho. Realistic blur synthesis for learning image deblurring. In Proceedings of the European Conference on Computer Vision (ECCV), 2022. 3 [28] Antoni Rosinol, Marcus Abate, Yun Chang, and Luca Carlone. Kimera: an open-source library for real-time metricsemantic localization and mapping. In IEEE Intl. Conf. on Robotics and Automation (ICRA), 2020. 2 [29] Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and Marcin Dymczyk. From coarse to fine: Robust hierarchical localization at large scale. In CVPR, 2019. 7 [30] Johannes Lutz Schonberger and Jan-Michael Frahm. In Conference on ComStructure-from-motion revisited. puter Vision and Pattern Recognition (CVPR), 2016. 2, 5, 6 9 indoor scenes. In Proceedings of the International Conference on Computer Vision (ICCV), 2023. 2, 5 [48] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and MingHsuan Yang. Monst3r: simple approach for estimatarXiv preprint ing geometry in the presence of motion. arxiv:2410.03825, 2024. [31] Johannes Lutz Schonberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise view selection for unIn European Conference on structured multi-view stereo. Computer Vision (ECCV), 2016. 2, 5, 6 [32] Yasmina Schoueri, Milena Scaccia, and Ioannis Rekleitis. Optical Flow from Motion Blurred Color Images. In 2009 Canadian Conference on Computer and Robot Vision, pages 17, 2009. 2 [33] Brandon Smart, Chuanxia Zheng, Iro Laina, and Victor Adrian Prisacariu. Splatt3r: Zero-shot gaussian splatting from uncalibrated image pairs. 2024. 2 [34] Cameron Smith, David Charatan, Ayush Tewari, and Vincent Sitzmann. Flowmap: High-quality camera poses, intrinsics, and depth via gradient descent. In arXiv, 2024. 2 [35] Edgar Sucar, Shikun Liu, Joseph Ortiz, and Andrew J. Davison. imap: Implicit mapping and positioning in real-time. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 62296238, 2021. 2 [36] Zachary Teed and Jia Deng. RAFT: Recurrent All-Pairs In Computer Vision Field Transforms for Optical Flow. ECCV 2020, pages 402419. Springer International Publishing, Cham, 2020. Series Title: Lecture Notes in Computer Science. 2, [37] Zachary Teed and Jia Deng. DROID-SLAM: Deep Visual SLAM for Monocular, Stereo, and RGB-D Cameras. Advances in neural information processing systems, 2021. 2, 6, 7 [38] Emanuele Trucco and Alessandro Verri. Introductory Techniques for 3-D Computer Vision, pages 178184. Prentice Hall PTR, USA, 1998. 4 [39] Michał Tyszkiewicz, Pascal Fua, and Eduard Trulls. Disk: Learning local features with policy gradient. Advances in Neural Information Processing Systems, 33, 2020. 6, 7 [40] Alexander Veicht, Paul-Edouard Sarlin, Philipp Lindenberger, and Marc Pollefeys. GeoCalib: Single-image Calibration with Geometric Optimization. In ECCV, 2024. 3 [41] Hengyi Wang and Lourdes Agapito. 3d reconstruction with spatial memory. arXiv preprint arXiv:2408.16061, 2024. 2 [42] Jianyuan Wang, Nikita Karaev, Christian Rupprecht, and David Novotny. Vggsfm: Visual geometry grounded deep structure from motion. 2023. 2 [43] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei A. Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state, 2025. 2 [44] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In CVPR, 2024. 2, 6, 7 [45] Wenshan Wang, Yaoyu Hu, and Sebastian Scherer. Tartanvo: generalizable learning-based vo. 2020. 2 [46] Jianing Yang, Alexander Sax, Kevin J. Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, and Matt Feiszli. Fast3r: Towards 3d reconstruction of 1000+ images in one forward pass. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 2 [47] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: high-fidelity dataset of 3d"
        }
    ],
    "affiliations": [
        "University of Oxford Department of Computer Science"
    ]
}