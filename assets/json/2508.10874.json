{
    "paper_title": "SSRL: Self-Search Reinforcement Learning",
    "authors": [
        "Yuchen Fan",
        "Kaiyan Zhang",
        "Heng Zhou",
        "Yuxin Zuo",
        "Yanxu Chen",
        "Yu Fu",
        "Xinwei Long",
        "Xuekai Zhu",
        "Che Jiang",
        "Yuchen Zhang",
        "Li Kang",
        "Gang Chen",
        "Cheng Huang",
        "Zhizhou He",
        "Bingning Wang",
        "Lei Bai",
        "Ning Ding",
        "Bowen Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We investigate the potential of large language models (LLMs) to serve as efficient simulators for agentic search tasks in reinforcement learning (RL), thereby reducing dependence on costly interactions with external search engines. To this end, we first quantify the intrinsic search capability of LLMs via structured prompting and repeated sampling, which we term Self-Search. Our results reveal that LLMs exhibit strong scaling behavior with respect to the inference budget, achieving high pass@k on question-answering benchmarks, including the challenging BrowseComp task. Building on these observations, we introduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability through format-based and rule-based rewards. SSRL enables models to iteratively refine their knowledge utilization internally, without requiring access to external tools. Empirical evaluations demonstrate that SSRL-trained policy models provide a cost-effective and stable environment for search-driven RL training, reducing reliance on external search engines and facilitating robust sim-to-real transfer. We draw the following conclusions: 1) LLMs possess world knowledge that can be effectively elicited to achieve high performance; 2) SSRL demonstrates the potential of leveraging internal knowledge to reduce hallucination; 3) SSRL-trained models integrate seamlessly with external search engines without additional effort. Our findings highlight the potential of LLMs to support more scalable RL agent training."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 4 7 8 0 1 . 8 0 5 2 : r Preprint. SSRL: SELF-SEARCH REINFORCEMENT LEARNING Yuchen Fan1,3, Kaiyan Zhang1,, Heng Zhou3, Yuxin Zuo1,3 Yanxu Chen1 Yu Fu4 Xinwei Long1 Xuekai Zhu2 Che Jiang1 Yuchen Zhang3 Li Kang3 Gang Chen5 Cheng Huang1 Zhizhou He1 Bingning Wang6 Lei Bai3, Ning Ding1,3, Bowen Zhou1,3, 2 Shanghai Jiao Tong University 1 Tsinghua University 4 University College London Equal contributions Project leader # zhang-ky22@mails.tsinghua.edu.cn TsinghuaC3I/SSRL 5 CSCEC Third Bureau Corresponding author 6 WeChat AI 3 Shanghai AI Laboratory"
        },
        {
            "title": "ABSTRACT",
            "content": "We investigate the potential of large language models (LLMs) to serve as efficient simulators for agentic search tasks in reinforcement learning (RL), thereby reducing dependence on costly interactions with external search engines. To this end, we first quantify the intrinsic search capability of LLMs via structured prompting and repeated sampling, which we term Self-Search. Our results reveal that LLMs exhibit strong scaling behavior with respect to the inference budget, achieving high pass@k on question-answering benchmarks, including the challenging BrowseComp task. Building on these observations, we introduce Self-Search RL (SSRL), which enhances LLMs Self-Search capability through format-based and rule-based rewards. SSRL enables models to iteratively refine their knowledge utilization internally, without requiring access to external tools. Empirical evaluations demonstrate that SSRL-trained policy models provide cost-effective and stable environment for search-driven RL training, reducing reliance on external search engines and facilitating robust sim-to-real transfer. We draw the following conclusions: 1) LLMs possess world knowledge that can be effectively elicited to achieve high performance; 2) SSRL demonstrates the potential of leveraging internal knowledge to reduce hallucination; 3) SSRL-trained models integrate seamlessly with external search engines without additional effort. Our findings highlight the potential of LLMs to support more scalable RL agent training. Figure 1: Left: Prior methods like Search-R1 (Jin et al., 2025b) and ZeroSearch (Sun et al., 2025) rely on external sources (e.g., search engines, knowledge bases, or fine-tuned LLMs), representing full or semi-real search. We propose full-sim search, where policy model generates information internally (Self-Search). Right: Self-Search with test-time scaling shows strong pass@k performance as compute increases. Self-Search Reinforcement Learning (SSRL) further boosts results across models and tasks, especially with sim-to-real generalization. 1 Preprint."
        },
        {
            "title": "CONTENTS",
            "content": "1 Introduction 2 Inference-time Scaling of Self-Search 2.1 Task Formulation . 2.2 Prompt Design . . . . 2.3 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4 Performance Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.5 Further Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.5.1 Is More Reasoning Always Better? . . . . . . . . . . . . . . . . . . . . . 2.5.2 Majority Voting vs. Pass@k . . . . . . . . . . . . . . . . . . . . . . . . . 3 SSRL: Self-Search Reinforcement Learning 3.1 Task Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Training Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4 Performance Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4.1 Self-Search RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4.2 Sim2Real Generalization . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4.3 Test-Time RL . 3.5 Further Discussions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.5.1 Benefits of Information Masking . . . . . . . . . . . . . . . . . . . . . . . 3.5. Impact of Format-based Reward . . . . . . . . . . . . . . . . . . . . . . . 3.5.3 Importance of On-policy Self-Search . . . . . . . . . . . . . . . . . . . . 3.5.4 Compatibility with RL Algorithms . . . . . . . . . . . . . . . . . . . . . . 4 Related Work 4.1 Reinforcement Learning with Search Engines . . . . . . . . . . . . . . . . . . . . 4.2 Large Language Models as Search Engines . . . . . . . . . . . . . . . . . . . . . 4.3 Inference-time Scaling of LLMs and Agents . . . . . . . . . . . . . . . . . . . . . 5 Conclusion Inference-time Scaling of Self-Search A.1 Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.1.1 Instructions for Repeated Sampling . . . . . . . . . . . . . . . . . . . . . A.1.2 Instructions for LLM Providing Information . . . . . . . . . . . . . . . . . A.2 Detailed Results . A.3 Case Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.1 Case Study for Qwen3 with/without Thinking . . . . . . . . . . . . . . . . 2 4 5 5 5 6 7 8 9 9 9 10 11 12 14 15 15 17 18 18 18 18 19 24 24 24 24 26 26 Preprint. A.3.2 Case Study for Multi-turn and Reflection Repeated Sampling . . . . . . . . 26 Self-Search Reinforcement Learning B.1 Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2.1 Baseline Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2.2 Other Algorithm Implementation . . . . . . . . . . . . . . . . . . . . . . B.2.3 TTRL . . B.3 Ablation Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3.1 Model Family Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . B.3.2 Comparison between General Models and Reasoning Models . . . . . . . B.3.3 Dynamics of Training with and without Information Mask . . . . . . . . . B.3.4 Group Size Ablation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3.5 Additional Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3.6 Additional Results for Sim2Real Search . . . . . . . . . . . . . . . . . . . B.4 Case Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Format Reward Code 26 26 26 27 30 30 31 31 31 32 32 33 3 Preprint."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recently, Reinforcement Learning (RL) with verifiable rewards has substantially improved the reasoning abilities of Large Language Models (LLMs) in complex mathematical problem-solving (OpenAI, 2024; DeepSeek-AI, 2025; Petrov et al., 2025) and code generation (El-Kishky et al., 2025; Cui et al., 2025a), leading to the emergence of Large Reasoning Models (LRMs) (Xu et al., 2025). Beyond mathematics and coding, numerous studies have explored the application of RL to LLMs in agentic contexts such as tool learning (Qian et al., 2025; Feng et al., 2025). These approaches enable LLMs to learn to invoke external tools such as web search engines, perform actions, and observe states within real-world environments. Although recent models like Search-R1 (Jin et al., 2025b) and Kimi V2 (Team et al., 2025) have achieved strong performance on various benchmarks, interacting with real web search engines remains costly (Sun et al., 2025), especially given the large number of rollouts and multi-turn tool calls required during RL training. In fact, due to pre-training on massive web-scale corpora (Brown et al., 2020; Liu et al., 2024; Yang et al., 2025b), LLMs can often answer questions involving world knowledge. Some studies also suggest that LLMs can serve as world models by providing state information in response to given actions (Li et al., 2023; Hao et al., 2023; Gu et al., 2024; Tang et al., 2024). For example, recent work on ZeroSearch (Sun et al., 2025) demonstrates that fine-tuned LLM can effectively replace web search, providing stable and reliable knowledge. This finding indicates that the cost of search RL can be significantly reduced by adopting semi-real setting. Inspired by recent advances in unsupervised RL like TTRL (Zuo et al., 2025), we explore self-search RL within fully simulated RL settings (noted as full-sim), where no real search is used during training. Specifically, we focus on two key research questions: 1) What is the performance limit of LLMs on search-based QA tasks using only internal knowledge? 2) Can full-sim search RL enable effective sim-to-real transfer with real web search during inference? First, we investigate whether an LLM can generate both queries and information based on the knowledge embedded in its parameters, effectively simulating querying external search engines. To this end, we assess the intrinsic search capabilities of LLMs on benchmarks that require web searching by prompting the model to simulate the search process within single generation trajectory using multi-turn, tool-formatted outputs. Extensive sampling demonstrates that LLMs encode substantial world knowledge within their parameters, yielding high predictive pass@k scores that follow scaling law. However, reliably extracting the optimal answer remains challenging, underscoring the gap between latent knowledge and actionable retrieval. To address this challenge and explore the potential of full-sim search RL for sim-to-real transfer, we study the potential of Self-Search Reinforcement Learning (SSRL) which enhances the self-search abilities of LLMs through format-based and rule-based rewards, enabling autonomous refinement of internal knowledge utilization without relying on external searches. Our experiments show that models trained with SSRL not only outperform previous search API-based RL baselines, such as Search-R1 and ZeroSearch, across various benchmarks, but also serve as cost-effective, implicit world knowledge provider, thus reducing hallucination, for search-driven question answering. Moreover, this approach reduces dependence on external search engines and opens new avenues for sim-to-real generalization, enabling skills acquired through self-search to transfer robustly to online settings with real web access. In summary, our work demonstrates that LLMs hold significant potential as simulator of the web, resource that can be leveraged for search-driven tasks without the need for external queries. By systematically quantifying and enhancing this self-search capability with SSRL, we pave the way for more autonomous and scalable LLM agents (Leike et al., 2018; Gao et al., 2025). Takeaways 1. LLMs can serve as simulator of world knowledge, exhibiting varying upper bounds across different model families on challenging search-based benchmarks. ( 2.4) 2. Achieving the upper performance bound of Self-Search remains challenging; current methods, such as voting, are insufficient for reliably selecting optimal outputs. ( 2.5) 3. By relying on feedback from the policy model itself, Self-Search RL (SSRL) can achieve superior performance compared to training with external search engines.( 3.4.1) 4. Models trained with SSRL can be seamlessly adapted to real search scenarios without additional effort, providing preliminary evidence for sim-to-real transfer. ( 3.4.2) 4 Preprint. 2 INFERENCE-TIME SCALING OF SELF-SEARCH 2.1 TASK FORMULATION Formulation of pass@k. We consider the problem of answering information-seeking queries using only the internal knowledge of an LLM, without access to external retrieval tools such as web search engines or databases. We generate samples for problem i, and we calculate the number of accurate responses Ci. We compute pass@k using the formula below: pass@k = 1 # of problems # of problems (cid:88) (cid:32) 1 i=1 (cid:33) (cid:1) , (cid:0)KCi (cid:0)K (cid:1) (1) where correctness is defined according to the evaluation standard of the underlying benchmark (e.g., exact match, top-k accuracy, or task-specific criteria). This setup allows us to estimate the intrinsic upper bound of the models internalized search capabilities, independent of any external retriever. Formulation of Scaling Law. We present detailed formulation of the scaling law for test-time self-search. Following Brown et al. (2024), we define an explicit function to simulate the correlation between the number of samples and the coverage c. We model the log of as function of using: log akb, (2) where a, are fitted model parameters. We exponentiate each side to have straightforward prediction of the coverage c. That is: exp(akb). (3) 2.2 PROMPT DESIGN Following Jin et al. (2025a), we use an unbiased instruction without any hints for reflection. The instruction just teaches LLMs to think step by step. The prompt template is shown in Table 1. Prompt Template Answer the given question. You must conduct reasoning inside <think> and </think> first every time you get new information. After reasoning, if you find you lack some knowledge, you can call search engine by <search> query </search>, and you should return the top searched results between <information> and </information>. You can search as many times as you want. For multi-hop QA, you can break it down into pieces and search one by one. If you find no further external knowledge needed, you can directly provide the answer inside <answer> and </answer> without detailed illustrations. For example, <answer> Beijing </answer>. Question: Table 1: Prompt template. The question is appended at the end during training and inference. Our iterative reasoning framework follows structured process where the model first expresses its initial thoughts within <think>...</think> tags. When the model identifies missing information necessary for solving the problem, it formulates search queries within <search>...</search> tags. The model then auto-regressively generates relevant information to address these queries, which is incorporated within <information>...</information> tags. This cycle of thinking, searching, and information gathering continues iteratively until the model arrives at final answer. While this approach shares similarities with traditional multi-turn search systems, it fundamentally differs in its implementation: rather than conducting genuine iterative interactions with external systems, our method employs Chain-of-Thought (Wei et al., 2023) process where the language model auto-regressively generates the entire reasoning trajectory in single forward pass, including thoughts, search queries, and retrieved information. This design enables efficient self-contained search while maintaining the structured exploration benefits of iterative search processes. 2.3 EXPERIMENTAL SETUP Benchmarks. We evaluate across seven benchmarks spanning three categories of questionanswering tasks: 1) General Question Answering, which tests factual knowledge retrieval using Preprint. Knowledge Type Benchmark Time Construction Targeted Task Source Factual Reason TriviaQA Natural Questions SimpleQA 2017 2019 2024 Manual Manual Manual HotpotQA 2018 2WikiMultiHopQA 2020 2022 2022 BamBoogle MuSiQue Manual Semi-automated Manual Automated General QA General QA Factual QA Multi-hop QA Multi-hop QA Multi-hop QA Multi-hop QA Wikipedia + Web Wikipedia General knowledge Wikipedia Wikipedia + Wikidata Wikipedia Wikipedia Web browsing BrowseComp 2025 Manual Search and Browse Open Web Table 2: Benchmark concerning search. Most benchmarks are constructed manually, except 2WikiMultiHopQA and MuSiQue. Most of the benchmarks are designed for QA initially. Figure 2: The scaling curves of repeated sampling averaged on six benchmarks within three model families (Qwen2.5, Llama, and Qwen3). It indicates predictive performance gains, where average MAE for different families is 1.42%, 1.45%, and 0.95%, respectively. Natural Questions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017); 2) Multi-hop Question Answering, which requires reasoning across multiple pieces of information through HotpotQA (Yang et al., 2018), Musique (Trivedi et al., 2022), Bamboogle (Press et al., 2023), and 2WikiMultiHopQA (Ho et al., 2020); and 3) Vague Question Answering, which evaluates information synthesis from various vague restrictions using BrowseComp (Wei et al., 2025). This comprehensive evaluation framework captures capabilities ranging from direct knowledge retrieval to complex reasoning and information integration, providing robust assessment of model performance across varied question-answering scenarios. Benchmark details are listed in Table 2. Models. To ensure comprehensive evaluation of the effects of repeated sampling, we conduct experiments across three model families: Qwen2.5 (Qwen et al., 2025), Llama3 (including Llama-3.1 and Llama-3.2) (Grattafiori et al., 2024), and Qwen3 (Yang et al., 2025a). We test models spanning wide range of parameter scales from 0.6B to 72B. To ensure fair comparison across all experiments, we maintain consistent sampling parameters with temperature set to 0.7, top-k to -1, top-p to 0.95, and max token to 8192. The instruction used is shown in Appendix A.1.1. 2.4 PERFORMANCE EVALUATION Predictive Performance Improves with Sample Size. As shown in Figure 2 and Figure 3, we observe consistent and predictive performance improvements across all benchmarks as the number of samples increases. Notably, on Bamboogle, Llama-3.1-8B-Instruct achieves 87.2% accuracy for pass@1024, 150% improvement over pass@1 performance. These substantial gains are evident across all three model families (Qwen2.5, Llama, and Qwen3), with the Llama series showing particularly pronounced benefits. Figure 3 shows performance on BrowseComp, benchmark characterized by difficult search requirements but straightforward verification. While GPT-4o with search achieves only 1.9% and o1 scores 10%, Self-Search yields surprising results: Qwen2.5-14BInstruct and Llama-3.1-8B-Instruct surpass o1s performance when given sufficient samples. This finding suggests that LLMs possess substantial internal knowledge that can be effectively leveraged through repeated sampling, even in the absence of external information sources. Analysis of the 6 Preprint. upper bound further highlights the strong potential of LLMs to provide information in response to given search actions. More details are provided in Appendix A.2. Llama Outperforms Qwen, Contrary to Prior Reasoning Tasks. Previous works (Gandhi et al., 2025; Liu et al., 2025b; Wang et al., 2025a) have shown that Qwen models (including Qwen2.5 and Qwen3) possess stronger priors in mathematical reasoning and achieve greater improvements than Llama models in reinforcement learning settings. However, our findings indicate that Llama models outperform Qwen models in the Self-Search setting with respect to priors for world knowledge, as demonstrated in Figure 2 and Figure 3. This observation suggests that self-search ability and reasoning priors are not strongly correlated. We will further explore the utilization of knowledge and reasoning in Section 2.5.1. Figure 3: TTS on BrowseComp leads to consistent performance gains within all models. indicates predictive performance gains, It with average MAE for the LLaMA, Qwen 2.5, and Qwen 3 families at 0.34 %, 0.22 %, and 0.26 %, respectively. Performance Gap Narrows Between Large and Small Models with More Sampling. Remarkably, our results demonstrate that smaller models can achieve performance comparable to models with nearly 10 more parameters by leveraging repeated sampling, as measured by pass@k. For example, on TriviaQA with 1024 samples, Llama-3.1-8B-Instruct achieves score of 81.2%, while Llama-3.1-70B-Instruct achieves 81.4%, negligible difference despite the substantial gap in model size. This finding is consistent with previous studies (Snell et al., 2024; Liu et al., 2025a). 2.5 FURTHER ANALYSIS 2.5.1 IS MORE REASONING ALWAYS BETTER? Experimental Setup. As discussed in Section 2.4, we observe inconsistent results on the agentic search benchmark of reasoning and instruction models, e.g., Qwen3 vs Qwen2.5 and Llama. In this section, we begin by analyzing the utilization efficiency of thinking tokens in Qwen3 models, followed by comparison of two types of sequential scaling: multi-turn search and multi-turn reflection. Additional case studies are provided in Table 19. For implementation of token comparison, we dont truncate when the decoding response reaches predefined threshold to restrict the response length of LLMs since it may harm the ability of LLMs. Instead, we generate responses and sum up the tokens used, and compared them under the same token budget. Inefficient Utilization of Thinking Tokens. Qwen3 models support both thinking and no thinking modes (Yang et al., 2025b), allowing manual adjustment of the number of thinking tokens before the model produces final answer. To investigate the influence of increasing thinking tokens in Self-Search settings, we conduct comparative study evaluating the impact of whether enabling the thinking process during inference. We only count the token used out of <search>...</search>, <information>...</information>, and <answer>...</answer> for comparison of thinking token. As presented in Figure 4, the results demonstrate that as the number of assigned tokens increases, long CoT reasoning doesnt yield better performance, contradictory to what is observed in complex math questions. This is probably attributed to that the solution to agentic search mainly relies on the usage of knowledge, either internal or external, rather than solely thinking. These findings indicate that short-CoT should be preferred in Self-Search settings to maximize token efficiency. Multi-Turn Self-Search Hurts Performance. Following the established approach in search agent literature (Jin et al., 2025a; Sun et al., 2025) that formalizes search as multi-turn process, we perform Self-Search for each rollout. Upon generating search query, we prompt the model to provide relevant information for that query, incorporate this information into the reasoning context, 7 Preprint. Figure 4: The performance of various sizes of Qwen3 averaged on six benchmarks, when enabled forced-thinking or not. The x-axis is measured by the number of tokens used by thinking solely. Figure 5: The performance of Repeated Sampling of Self-Search, Multi-turn Self-Search, and SelfSearch with Reflection measured under the same token budget across four models. and continue the iterative reasoning process until reaching final decision. We denote the number of such interactions as . The instruction for LLMs to provide relevant information is listed in Appendix A.1.2. Since our approach eliminates the need for external search engines (Google, Bing, etc.), we avoid API costs and inference budget constraints typically associated with online search. Therefore, we set = 10 to ensure sufficient iterations for every sample to converge to final answer. As shown in Figure 5, when measured by tokens consumed, naive repeated sampling shows better performance and steady performance growth, further highlighting the upper bound of LLMs themselves as an implicit simulator of world knowledge. Self-Search with Reflection Hurts Performance. The Aha Moment, introduced by DeepseekR1 (DeepSeek-AI, 2025), demonstrates emergent reflection and exploration capabilities in LLMs, particularly in math and code generation. To investigate whether this reflective behavior extends to information search tasks without external sources, we incorporate reflection-triggering phrases into our sampling process. Specifically, we append Wait, wait, wait after each generated response to encourage the model to reconsider and explore alternative reasoning paths. Figure 5 presents the experimental results. We also find that under the same token budget, reflection doesnt yield better performance measured by pass@k compared with naive repeated sampling. In conclusion, we find that increasing the number of thinking tokens and incorporating multi-turn generation are not always beneficial in Self-Search settings. This suggests that knowledge utilization may be more advantageous than reasoning in these scenarios. Further investigation is warranted, particularly in the context of language models as world models (Hao et al., 2023; Gu et al., 2024). 2.5.2 MAJORITY VOTING VS. PASS@K In the above experiment, we found that LLMs exhibit high performance ceiling in search and question-answering tasks. However, it remains challenging to identify the correct answer from set of candidate responses, despite the correct answer being present, when the ground truth is unknown (Brown et al., 2024). This suggests that repeated sampling represents the upper limit of Test-Time Scaling (TTS), and further evaluation of alternative TTS strategies is necessary. Majority voting is widely thought of as simple but effective method to integrate with Test-time Scaling (Zuo et al., 2025). To investigate whether the performance transfers to knowledge-intensive tasks, we employ the maj@k metric. 8 Preprint. Figure 6: Majority voting results of different models averaged on six benchmarks. In essence, maj@k evaluates to 1 when the most frequently occurring answer among samples matches the ground truth, and 0 otherwise. The instruction for the majority voting experiments is detailed in Appendix A.1.1. We show the results in Figure 6. Our experiments reveal that even as we increase the number of responses for majority voting, we observe only marginal performance improvements. This limited scaling behavior suggests that naive majority voting may be insufficient for search tasks, where incorrect answers might consistently appear across multiple samples. These findings indicate that LLMs have the potential to become world models, but the world knowledge presented is vague, and how to provide precise knowledge is still challenging task."
        },
        {
            "title": "3 SSRL: SELF-SEARCH REINFORCEMENT LEARNING",
            "content": "In this section, we employ reinforcement learning (RL) to enable LLMs to export world knowledge from their own parameters. We examine the effectiveness of the policy model as simulator of world knowledge in Self-Search RL, as well as its performance in Sim2Real and TTRL settings. 3.1 TASK DEFINITION We formulate the RL objective for LLM-based search agent utilizing external search engines as: max πθ ExD,yπθ(x;R)[rϕ(x, y)] βDKL[πθ(yx; R)πref(yx; R)], (4) where πθ denotes the policy model, πref represents the reference model, rϕ is the reward function, represents retrieved information, and DKL denotes the KL divergence regularization term with coefficient β. In our approach, since the model auto-regressively retrieves knowledge from its internal parameters rather than external sources, the retrieved information follows the same distribution as πθ. This Self-Search mechanism allows us to simplify the objective function to: ExD,yπθ(x)[rϕ(x, y)] βDKL[πθ(yx)πref(yx)], (5) max πθ where πθ simultaneously functions as both the reasoning policy model and the internal search engine. We primarily leverage GRPO (Shao et al., 2024) as our training algorithm, while also experimenting with alternative RL algorithms, including PPO (Schulman et al., 2017), Reinforce++ (Hu et al., 2025), DAPO (Yu et al., 2025), and KL-Cov (Cui et al., 2025b) to validate the robustness. 3.2 TRAINING METHODOLOGY Information Token Mask Previous research (Jin et al., 2025b; Sun et al., 2025) demonstrates that masking information tokens from external search engines helps stabilize training and improve performance. However, in Self-Search, the retrieved information originates from the models own generation process rather than external sources, raising questions about whether information masking remains beneficial in this context. To investigate this, we conduct comparative experiments under two conditions: training with complete reasoning trajectories versus training with information-masked trajectories. For implementation, we extract all the tokens embraced by <information> and 9 Preprint. </information> and mask them for loss calculation. Our results reveal that information masking continues to enhance performance even when the information is self-generated by the model. We show our detailed experiments in Section 3.5.1. Reward Modeling Following DeepSeek-AI (2025); Yu et al. (2025), we employ composite reward function with two signals: format reward and outcome reward. We directly use the accuracy of the models final prediction as the outcome reward, computed using the following rule: R(ˆy, y) = (cid:26)1, is equivalent(ˆy, y) 1, otherwise (6) where is the ground-truth answer and ˆy is the predicted answer. Since our iterative search process requires models to decompose complex questions into manageable sub-problems, with each iteration focusing on searching for specific information and incrementally building toward the final answer, maintaining structured output format is crucial for effective reasoning. To address this requirement, we introduce format reward that ensures adherence to the prescribed reasoning structure, detailed in Appendix C. This format reward guides the model to produce well-organized, multi-step reasoning trajectories. The final reward combines both components as: rϕ(yi, y) = 1 1 λf λf 0 if is equivalent(ˆy, y) fformat(y) = True, if is equivalent(ˆy, y) fformat(y) = False, if !is equivalent(ˆy, y) fformat(y) = True, if !is equivalent(ˆy, y) fformat(y) = False, (7) where we set λf = 0.1 to prioritize correctness while maintaining structured reasoning, following (Wang et al., 2025b). 3.3 EXPERIMENTAL SETUP Benchmarks We conduct evaluation across six of the benchmarks described in Section 2.3. We exclude BrowseComp from evaluation due to its exceptional difficulty and limited availability of training data. To ensure fair comparison with existing baselines, we adopt the same validation sets used by Sun et al. (2025). Our evaluation employs EM, where prediction is considered correct only when it matches the ground truth answer precisely. This strict evaluation criterion ensures robust assessment of model performance. Baselines To evaluate the effectiveness of Self-Search, we compare our model with the following methods: Vanilla Prompt Methods: It includes Direct Prompt and CoT; RAG-based Methods: This category includes standard RAG and Search-o1 (Li et al., 2025b); RL-based Methods: This category includes R1, Search-R1 (Jin et al., 2025b), and ZeroSearch (Sun et al., 2025). We conduct offline evaluations of our models while enabling online testing for baseline methods where applicable. To ensure fair comparison in online settings, we limit the number of retrieved passages to 3 across all RAG-based approaches. For vanilla prompt methods, we employ instruction-tuned models as they demonstrate superior prompt-following capabilities. The implementation details of baselines are listed in Appendix B.2.1. Training Setups We conduct our RL experiments primarily on the Llama model family, specifically Llama-3.2-3B (Base/Instruct) and Llama-3.1-8B (Base/Instruct), selected based on their demonstrated effectiveness under repeated sampling conditions. We use the combination of the training dataset of NQ and HotpotQA, as in previous work, to ensure mix of general QA and multi-hop QA. Our training framework primarily utilizes GRPO as the default algorithm, while also experimenting with alternative approaches, including PPO and REINFORCE++, to validate the robustness of our findings. All training is conducted on single node equipped with 8 NVIDIA A800 GPUs. For GRPO, the training configuration includes batch size of 256, learning rate of 1e-6, and 62 warmup steps across all experiments. The max response length is 4096 across all models in our experiments. For policy optimization, we set the temperature to 1.0 and generate 5 rollouts per prompt and apply KL divergence coefficient of 0.001. We train each model for 5 epochs and select 10 Preprint. Model Search Engine General QA Multi-Hop QA NQ TQ HotpotQA Musique 2Wiki Bamboogle Direct Answer CoT RAG Search-o1 R1-Base R1-Instruct Search-R1-Base Search-R1-Instruct ZeroSearch-Base ZeroSearch-Instruct SELF-SEARCH-BASE SELF-SEARCH-INSTRUCT Direct Answer CoT RAG Search-o1 R1-Base R1-Instruct Search-R1-Base Search-R1-Instruct ZeroSearch-Base ZeroSearch-Instruct SELF-SEARCH-BASE SELF-SEARCH-INSTRUCT /- /- / / -/- -/- / / / / -/- -/- /- /- / / -/- -/- / / / / -/- -/- LLaMA-3.2-3B 29.6 44.4 57.6 48.4 44.2 52.2 60.0 53.6 63.8 58.0 45.8 58. 12.6 16.0 23.4 19.4 22.8 21.6 29.6 21.0 32.2 22.8 28.2 25.0 LLaMA-3.1-8B 52.8 46.6 62.8 37.2 48.8 59.8 62.6 59.6 52.4 68.2 49.6 62.6 21.0 18.8 37.0 21.0 23.0 30.4 40.0 36.8 26.0 36.6 30.0 34.4 16.2 26.2 30.0 24.2 28.4 35.0 41.2 37.6 43.4 40.2 35.0 43.8 21.2 23.0 40.8 26.8 21.0 39.2 41.0 39.6 38.2 48.2 41.0 48. 2.0 5.8 9.6 6.0 7.0 11.4 13.6 8.8 13.8 10.4 14.2 14.2 3.2 8.8 22.4 9.2 5.4 18.2 25.0 19.6 9.6 19.6 18.4 24.2 9.2 10.2 17.6 17.4 28.4 17.8 31.6 20.4 35.6 21.4 29.6 31.6 8.0 17.6 34.0 23.6 28.0 36.8 37.8 34.8 28.4 36.2 34.4 35.2 8.0 21.6 11.2 32.0 11.1 20.8 19.4 27.8 15.3 18.1 30.2 38.4 23.8 35.2 38.4 25.6 5.6 47.2 36.1 31.9 12.5 40.3 32.8 54. Avg 12.9 20.7 24.9 24.6 23.7 26.5 32.6 28.2 34.0 28.5 30.5 35.2 21.7 25.0 39.2 23.9 22.0 38.6 40.4 37.1 27.9 41.5 34.4 43.1 Table 3: Main results of our trained models on the six benchmarks measured by EM. The column Search Engine refers to the external search engine used in the training stage and the evaluation stage. We use to denote that the baseline does not undergo the stage and - to denote using internal knowledge. We use to denote Wikipedia for simplification. We use to denote Google. The largest score of each model is denoted using bold. to denote the Simulation LLM and the checkpoint with the highest average validation accuracy for final evaluation, ensuring optimal performance while preventing overfitting. For all the evaluation, we set the temperature to 0.0. The implementation details of other algorithms are listed in Appendix B.2.2 3.4 PERFORMANCE EVALUATION 3.4.1 SELF-SEARCH RL We present the main experimental results in Table 3 and show the case studies in Appendix (Table 28 and Table 29). We also experiment on the Qwen series, and the results of Qwen2.5 and Qwen3 are listed in Appendix B.3.1 and Appendix B.3.2. The results reveals several key insights: SSRL achieves superior performance. Our results demonstrate that models trained with autoregressive internal retrieval consistently outperform those relying on external search engines, whether using other LLMs or Google Search. We also observe better performance compared with R1-like models, which are trained with the naive CoT prompt. These findings suggest that through well-designed instruction and reward, language models can effectively function as both reasoners and knowledge retrievers simultaneously, successfully extracting relevant information from their internal parametric knowledge without external dependencies. Instruction models more effectively utilize internal knowledge. When trained on identical data for the same duration, instruction-tuned models achieve significantly better performance than their base counterparts, suggesting that additional knowledge operations may be incorporated during supervised fine-tuning. However, this advantage appears to be context-dependent: while instructiontuned models excel at leveraging internal knowledge, base models demonstrate superior performance when external information sources are available. This finding implies that different optimization strategies are required for internal versus external knowledge utilization. 11 Preprint. Figure 7: The training curves of Llama-3.2-3B-Instruct and Llama-3.1-8B-Instruct. The first figure is the training reward. The second figure is the response length, and the third figure is the number of searches included in the response. Figure 8: The training curve of Llama-3.1-8B-Instruct using SSRL and ZeroSearch. The first figure is the time used during training. The second figure is the training reward on the same training step, and the third figure is the training reward consuming the same time. Larger models show better self-search performance. Figure 7 presents the training curves for both models. We observe steady growth in training reward throughout the process. During the early training stage, both response length and search count decrease as the models adapt to the format reward constraints. In later stages, Llama-3.1-8B-Instruct develops more sophisticated strategies, learning to decompose questions and employ self-reflection to enhance performance, thus yielding better performance on our benchmarks. SSRL is more efficient and robust. Figure 8 presents the training curves for ZeroSearch and SSRL. Compared to ZeroSearch, SSRL demonstrates substantially improved training efficiency, achieving 5.53 reduction in training time. Additionally, SSRL exhibits steady reward growth throughout training without collapse, indicating robust performance. Although SSRL shows relatively lower training rewards than ZeroSearch during early training stages due to limited external knowledge, its superior efficiency and robustness compensate for this initial disadvantage. 3.4.2 SIM2REAL GENERALIZATION Although SSRL achieves strong results on static benchmarks, the inherent knowledge within these In this work, we models remains fixed, which limits their applicability to real-world scenarios. investigate whether SSRL can generalize to real-time search settings. Since our trained model follows the exact format specifications of Search-R1 (Jin et al., 2025b), we can seamlessly integrate real search capabilities. We refer to this setting as sim-to-real generalization, following terminology from prior work in Robotics RL (Kaspar et al., 2020; Da et al., 2025). Replacing Simulated Search with Real Search We replace model-generated information with results of actual searches from Google Search or local corpora, substituting up to self-generated responses, where represents the maximum turns used by Jin et al. (2025b). To ensure compatibility, we post-process the retrieved information using rule-based modifications that remove patterns absent 12 Preprint. Model General QA Multi-Hop QA NQ TQ HotpotQA Musique 2Wiki Bamboogle Zero-shot CoT SSRL Sim2Real (K=1) Sim2Real (K=3) Sim2Real (All) Zero-shot CoT SSRL Sim2Real (K=1) Sim2Real (K=3) Sim2Real (All) Zero-shot CoT SSRL Sim2Real (K=1) Sim2Real (K=3) Sim2Real (All) Zero-shot CoT SSRL Sim2Real (K=1) Sim2Real (K=3) Sim2Real (All) 26.2 43.8 44.4 44.8 44.0 23.0 48.0 39.4 33.2 39.6 15.0 23.6 35.2 37.8 37. 12.8 31.4 38.4 43.8 41.8 LLaMA-3.2-3B-Instruct 16.0 25.0 34.8 35.4 35.2 5.8 14.2 17.2 19.4 20.8 LLaMA-3.1-8B-Instruct 18.8 34.4 34.0 29.7 34. 8.8 24.2 26.8 23.4 25.0 Qwen2.5-3B-Instruct 16.2 22.4 22.0 26.4 27.4 3.6 10.4 14.8 22.4 22.4 Qwen2.5-7B-Instruct 16.2 26.0 35.6 42.0 43. 6.6 11.8 18.4 29.4 28.6 44.4 58.4 63.4 63.0 61.6 46.6 62.6 55.8 50.6 54.6 33.6 41.0 44.0 51.6 51.4 35.6 44.4 58.0 64.4 65.0 10.2 31.6 37.8 41.8 42. 17.6 35.2 39.8 39.2 36.8 18.0 26.0 36.6 36.8 36.4 22.6 31.0 36.0 53.4 50.4 21.6 38.4 42.4 47.2 46.4 35.2 54.4 53.6 36.6 50.4 12.8 32.8 26.4 21.6 22. 24.0 36.8 41.6 54.5 52.0 Avg 20.7 35.2 40.0 41.9 41.8 25.0 43.1 41.6 35.5 40.2 14.7 26.0 29.8 32.8 33.0 17.4 30.2 38.0 47.9 46. Table 4: Performance of Sim2Real Search Generalization. The largest score is denoted using bold. The second largest score is denoted using underline. from our training data. Table 4 and Figure 9 present our experimental results. Performance consistently improves with an increasing number of maximum turns across all models except Llama-3.18B-Instruct. Furthermore, compared to Search-R1 and ZeroSearch baselines, SSRL-based models under Sim2Real settings generally achieve superior performance with less online searching across various benchmarks. These findings demonstrate that search agents trained exclusively on internal knowledge can effectively leverage external knowledge sources when format alignment is maintained, thereby reducing training costs and improving efficiency. Case study is in Table 30. Combining Simulated Search with Real Search Our findings demonstrate that LLMs possess substantial internal knowledge, suggesting they should search externally only when necessary. Based on this insight, we propose an entropy-guided search strategy. For each generated sequence, we analyze the entropy trend of the initial search query: increasing entropy indicates model uncertainty, triggering external search; otherwise, we rely on internal knowledge. We use Sim2Real (All) as our baseline for fair comparison and always use external search for the first query based on the performance gains shown in Table 4 (see Appendix B.3.6 for ablation studies on first-search importance). We present our results in Table 5. The entropy-guided approach reduces search frequency by 20-42%, yielding substantial Figure 9: Pareto frontier illustrating the trade-off between performance and the number of real searches across different models. The Sim2Real models are evaluated using the maximum score within Sim2Real (K = 1), Sim2Real (K = 3) for fair comparison. 13 Preprint. Model GeneralQA Multi-HopQA NQ TQ HotpotQA Musique 2Wiki Bamboogle Avg Avg. Search Sim2Real (All) Entropy-guided Search 44.0 45.2 61.6 62.4 35.2 34.6 20.8 18.6 LLaMA-3.2-3B-Instruct LLaMA-3.1-8B-Instruct Sim2Real-guided Search (All) Entropy 39.6 43.2 54.6 56.2 34.6 33.4 25.0 26. Qwen2.5-3B-Instruct Sim2Real (All) Entropy-guided Search 37.8 36.4 51.4 54.4 27.4 30.4 22.4 19. Qwen2.5-7B-Instruct Sim2Real (All) Entropy-guided Search 41.8 40.6 65.0 63.4 43.2 39.0 28.6 23. 42.8 40.0 36.8 40.8 36.4 36.8 50.4 45.4 46.4 46.4 50.4 49. 22.4 25.6 52.0 48.0 41.8 41.2 40.2 41.7 33.0 33.9 46.8 43. 1.9 1.5 2.6 1.5 3.0 1.8 2.6 1.9 Table 5: The performance of LLaMA and Qwen2.5 models when using either full or entropybased selection over the real search engine. The average search number is the average number of <search> used during generation, i.e., online search plus self-search, if exists. cost savings while maintaining performance comparable to full external search. As we observe above, though Llama-3.1-8B-Instruct fails under Sim2Real (K = 3), it achives better performance than Sim2Real (All), indicating that Llama-3.1-8B-Instruct is hard to leverage external information easily, which may be attributed to the gap between self-search gathered information and external one. These results reinforce our key finding: LLMs can effectively leverage their internal knowledge when they possess relevant information and know how to access it, making external search unnecessary in many cases. 3.4.3 TEST-TIME RL Considering unsupervised RL algorithms, e.g., TTRL, (Zuo et al., 2025), show great potential in math and code generation, we are curious about its generalization to SELF-SEARCH. We conduct experiments on the Llama series, using the dataset consisting of NQ, TQ, HotpotQA, MusiQue, Bamboogle, 2WikiMultiHopQA, and BrowseComp 1. The implementation details are listed in Appendix B.2.3. To ensure thorough analysis, we performed ablation studies both with and without information masking, while maintaining the format reward component, which remains essential for label voting mechanisms. We measure the results using EM and show the experimental results in Table 6. We observe better performance when trained with TTRL compared with GRPO. For Llama-3.2-3B-Instruct, the average performance is improved by 59%. This phenomenon indicates the generalization of TTRL across domains and model families. When using TTRL, we find that training without the information mask yields slightly better results, which contradicts RLVR. Surprisingly, we find that simply applying TTRL on the combined benchmarks results in substantial improvement on BrowseComp, even without external search engines. The accuracy curve on BrowseComp is presented in Figure 13, and the final performance metrics are summarized in Table 7. Models There is an interesting observation that smaller models achieve higher scores through on Browsecomp TTRL, and when we delve into the cases, we find that these models to point out an entity at prefer first and check if it meets all the requirements, which is opposite to the paradigm. This further strengthens our opinion that LLMs contain information that once elicited, it can be applied to solve extremely complex questions. WebSailor-3B Qwen2.5-3B-Instruct (TTRL) Qwen2.5-3B-Instruct (TTRL-Sim2Real) Llama-3.2-3B-Instruct (TTRL) Llama-3.2-3B-Instruct (TTRL-Sim2Real) Table 7: Performance on BrowseComp. search-and-answer 2.0 3.9 1.4 6.2 4.1 BrowseComp 1For WebSailor, we sample 250 records from BrowseComp and evaluate them using substring match. response of WebSailor is considered right only if the generated prediction is in the ground truth or the ground truth is in the prediction. Preprint. Algorithm GeneralQA Multi-HopQA NQ TQ HotpotQA Musique 2Wiki Bamboogle LLaMA-3.2-3B-Instruct GRPO TTRL (w/o info) TTRL (w/ info) 43.8 58.4 76.4 58.6 +14.8 +18.0 33.8% 30.8% 74.0 57.4 +13.6 +15.6 31.1% 26.7% 25.0 47.2 +22.2 88.8% 45.2 +20.2 80.8% 14.2 31.6 37.2 +23.0 59.4 +27.8 162.0% 87.9% 36.4 +22.2 60.2 +28.6 156.3% 90.5% GRPO 48.0 62.6 TTRL (w/o info) TTRL (w/ info) 43.0 5. 64.0 +1.4 10.4% 2.2% 67.4 +4.8 7.7% 49.2 +1.2 2.5% LLaMA-3.1-8B-Instruct 34.4 35.6 +1.2 3.5% 35.4 +1.0 2.9% 24. 35.2 27.2 +3.0 47.0 +11.8 12.4% 33.5% 48.2 40.2 +16.0 +13.0 66.1% 36.9% Qwen-2.5-3B-Instruct GRPO TTRL (w/o info) TTRL (w/ info) 23.6 41.0 59.8 39.2 +13.2 +18.8 55.9% 45.9% 31.8 +8. 58.0 +17.0 34.7% 41.5% 22.4 37.8 +15.4 68.8% 33.6 +11.2 50.0% 10.4 26.0 23.8 +13. 51.2 +25.2 128.8% 96.9% 22.0 +11.6 49.0 +23.0 111.5% 88.5% Qwen-2.5-7B-Instruct GRPO TTRL (w/o info) TTRL (w/ info) 31.4 44.4 40.6 +9.2 63.2 +18.8 29.3% 42.3% 34.6 +3. 54.8 +10.4 10.2% 23.4% 26.0 40.4 +14.4 55.4% 32.6 +6.6 25.4% 11.8 31.0 28.8 +17. 53.2 +22.2 144.1% 71.6% 20.2 +8.4 43.0 +12.0 71.2% 38.7% 38.4 57.6 +19.2 50.0% 56.0 +17.6 45.8% 54. 52.0 2.4 4.4% 52.0 2.4 4.4% 32.8 49.4 +16.6 50.6% 48.8 +16.0 48.8% 36.8 64.0 +27.2 73.9% 50.4 +13.6 36.7% Avg 35.2 56.1 +20.9 59.4% 54.9 +19.7 56.0% 43.1 44.8 +1.7 3.9% 48.7 +5.6 13.0% 26.0 43.5 +17.5 67.3% 40.5 +14.5 55.8% 30.2 48.4 +18.2 60.3% 39.3 +9.1 30.1% Table 6: The performance of Llama and Qwen trained with TTRL and GRPO. w/o info and w/ info indicate without information mask and with information mask, respectively. The largest value is denoted using bold. We also experiment on Sim2Real on TTRL-trained models, and we show the results in Table 8. Though TTRL achieves better performance compared with RLVR, it introduces biases where models over-relying on its internal knowledge and are hard to adapt to real environments easily. We find that almost all queries are finished using one search query, even for BroweseComp. Therefore, in one-turn generation, the web search engine cant provide flexible information as the LLMs do. Moreover, we observe that TTRL-trained models prefer to select candidate answer and verify it rather than search based on the question sequentially. We also find that it collapses frequently than RLVR, which is attributed to the unexpected deterministic behavior of policy models. We provide case study in Table 31. 3.5 FURTHER DISCUSSIONS 3.5.1 BENEFITS OF INFORMATION MASKING Since all retrieved information originates from the reasoning model itself, jointly training the model on both the reasoning process and information generation represents natural optimization strategy. To test the effectiveness of training full trajectories, we conduct experiments for training with and without information masking during the learning process. Figure 10 presents comparative results. The experimental results demonstrate that information masking consistently enhances model performance across benchmarks. Analysis of the training dynamics, which is listed in Appendix B.3.3, reveals that masking information tokens during training encourages the model to generate more com15 Preprint. Model GeneralQA Multi-HopQA NQ TQ HotpotQA Musique 2Wiki Bamboogle LLaMA-3.2-3B-Instruct TTRL Sim2Real 58.6 56.6 76.4 74. 47.2 46.0 37.2 36.0 Qwen2.5-3B-Instruct TTRL Sim2Real 39.2 39.8 59.8 61. 37.8 40.2 23.8 22.8 59.4 59.0 51.2 51.8 57.6 54.4 49.4 41. Avg 56.1 54.5 43.5 42.9 Table 8: Performance of Sim2Real Search Generalization on TTRL. prehensive and detailed reasoning trajectories. The enhanced capability provides compelling explanation for the consistent performance improvements observed across diverse question-answering tasks. By preventing the model from simply copying retrieved information during training, the masking strategy forces deeper engagement with the reasoning process itself, ultimately leading to more robust problem-solving abilities at inference time. Figure 10: The performance of Llama-3.2-3B-Instruct and Llama-3.1-8B-Instruct when trained with and without the information mask. Figure 11: The performance of Llama-3.2-3B-Instruct and Llama-3.1-8B-Instruct when trained with and without format reward. All of the models are trained with an information mask. 3.5.2 IMPACT OF FORMAT-BASED REWARD To effectively elicit the dual capabilities of language models as both reasoners and internal search engines, we design format reward that enforces adherence to our structured reasoning framework. This reward component ensures that models consistently follow the prescribed format of thinking, searching, and information gathering throughout their reasoning process. We evaluate the effectiveness of format reward through ablation studies comparing models trained with and without this component. Figure 11 presents the comparative results, demonstrating that format reward consistently improves performance for both base and instruction-tuned models across all benchmarks. These findings highlight that structured output formatting is crucial for successfully combining reasoning and search capabilities within single model. The format reward acts as critical scaffolding mechanism, guiding the model to maintain organized reasoning trajectories that facilitate effective internal knowledge retrieval. Without this structural guidance, models tend to produce less coherent reasoning paths that underutilize their internal search capabilities, resulting in degraded overall per16 Preprint. Algorithm GeneralQA Multi-HopQA NQ TQ HotpotQA Musique 2Wiki Bamboogle LLaMA-3.2-3B-Instruct GRPO GRPO (Freezing) 58.4 43.8 46.6 28.6 11.8 15.2 34.7% 20.2% 25.0 15.8 9.2 36.8% 14.2 5.6 8. 31.6 13.8 17.8 60.6% 56.3% LLaMA-3.1-8B-Instruct GRPO GRPO (Freezing) 62.6 48.0 46.6 24.4 16.0 23.6 49.2% 25.6% 34.4 15.4 19.0 55.2% 35.2 24.2 17.2 7.6 16.6 18.0 68.6% 51.1% Qwen-2.5-3B-Instruct GRPO GRPO (Freezing) 41.0 23.6 18.4 9.8 22.6 13.8 58.5% 55.1% 22.4 7.4 15.0 67.0% 10.4 5.0 5.4 26.0 7.5 18.5 51.9% 71.2% Qwen-2.5-7B-Instruct GRPO GRPO (Freezing) 31.4 15.6 15.8 50.3% 15.8% 44.4 37.4 7.0 26.0 15.0 11.0 42.3% 11.8 7.2 4. 31.0 15.2 15.8 39.0% 51.0% 38.4 20.8 17.6 45.8% 54.4 23.2 31.2 57.4% 32.8 12.8 20.0 61.0% 36.8 23.2 13.6 37.0% Avg 35.2 21.9 13.3 37.8% 43.1 22.4 20.7 48.0% 26.0 10.2 15.8 60.8% 30.2 22.6 7.6 25.2% Table 9: Performance of Llama and Qwen2.5 with on-policy GRPO compared to freezing policy. Algorithm GeneralQA Multi-HopQA NQ TQ HotpotQA Musique 2Wiki Bamboogle GRPO DAPO KL-Cov REINFORCE++ PPO GRPO DAPO KL-Cov REINFORCE++ PPO 43.8 44.6 41.8 42.2 35.0 48.0 48.6 44.8 46.2 37.4 58.4 58.0 58.6 55.8 55.8 62.6 63.8 63.6 64.4 58.4 LLaMA-3.2-3B-Instruct 25.0 26.8 24.6 25.6 21. 14.2 12.8 12.4 12.6 11.4 LLaMA-3.1-8B-Instruct 34.4 34.4 32.6 33.4 27.0 24.2 21.6 22.8 18.4 17.0 31.6 26.6 28.6 32.0 29.6 35.2 39.2 37.4 43.2 38. 38.4 38.4 38.4 30.8 30.4 54.4 52.0 52.8 36.4 37.4 Avg 35.2 34.5 34.1 33.2 30.7 43.1 43.3 42.3 40.3 36.2 Table 10: The performance of Llama-3.2-3B-Instruct and Llama-3.1-8B-Instruct when trained with different RL algorithms. All of the models are trained with an information mask and format reward. formance. Also, we observe drastic response length growth without format reward, which indicates that format reward stabilizes the training process. 3.5.3 IMPORTANCE OF ON-POLICY SELF-SEARCH In previous work such as ZeroSearch (Sun et al., 2025), the fine-tuned LLM serves as an information provider. In contrast, we treat the policy model as an implicit simulator of world knowledge to supply information in above sections, which not only simplifies training but also significantly reduces training costs, particularly those associated with multi-turn rollouts. To gain comprehensive understanding, we examine two settings: one in which the information provider is the policy itself, and another in which the provider is the zero-step policy (i.e., frozen policy). An information mask and formatted reward are applied throughout all training procedures. We conduct experiments on four models from two different model families to evaluate their generalization capabilities. The results, presented in Table 9, reveal dramatic collapse after approximately 100 training steps, with training rewards either remaining stagnant or decreasing sharply. We also observe significant performance degradation when using frozen LLM as the information provider. Preprint. Terminology Explanation Full-Real Search Search external real engines like RAG or Google. Semi-Real Search Search external simulated engines like LLMs. Full-Sim Search Search internal engines, e.g., implicitly retrieving information from embedded knowledge. Example Search-R1 (Jin et al., 2025b) ZeroSearch (Sun et al., 2025) SELF-SEARCH Sim2Real Search Train with Full-Sim Search but inference with external real engines, such as Google Search or Bing. SELF-SEARCH Table 11: We display key concepts discussed in this paper. The terminology we mentioned above is the approach of search used during training and inference. 3.5.4 COMPATIBILITY WITH RL ALGORITHMS We present the performances when training models with different algorithms, including PPO, GRPO, Reinforce++, DAPO, and KL-Conv. We use Llama-3.2-3B-Instruct and Llama-3.1-8BInstruct as our backbones. The implementation details is listed in Appendix B.2.2. We present our results in Table 10. We observe non-trivia performance gap between different training algorithms, with GRPO-based algorithms, e.g., GRPO, DAPO, etc, performs better than PPO and REINFORCE++. The superior performance of PPO is also observed in Sun et al. (2025), which proves the effectiveness of repeated rollouts for search agent training. It is worth noting that when trained with online engines like Google, the repeated rollouts will lead to greater cost. However, since we train models totally offline, more rolloouts may result in better performance without additional cost."
        },
        {
            "title": "4 RELATED WORK",
            "content": "4.1 REINFORCEMENT LEARNING WITH SEARCH ENGINES Reinforcement Learning (RL) has emerged as powerful approach for enhancing the reasoning capabilities of LLMs (DeepSeek-AI, 2025; Cui et al., 2025a; OpenAI, 2024). RL-trained reasoning models, which utilize either process rewards or outcome rewards, demonstrate remarkable performance on complex tasks such as math and code generation through self-reflection and exploration. Several recent works have explored applying RL to improve the performance of LLM-based search agents. Search-R1 (Jin et al., 2025a) employs RL to train models for iterative searches in local text corpus with retrievers like e5. Similarly, ReSearch (Chen et al., 2025) leverages outcome rewards exclusively to enhance LLMs ability to seek additional information during reasoning processes. However, these approaches are limited by their reliance on textual corpora, such as Wikipedia, which is static and inadequately represents the complexity and noise inherent in real-world online search environments. To address these limitations, Zheng et al. (2025) introduced online-search RL coupled with browse agent, aligning trained models with web search engines like Google and Bing. While this approach yields superior performance, the training process demands extensive API calls for RL algorithms such as Group Relative Policy Optimization (GRPO) (Shao et al., 2024), resulting in substantial API costs. As cost-effective alternative, ZeroSearch (Sun et al., 2025) proposes using an LLM as search engine simulator to create synthetic online search environment, significantly reducing computational overhead while maintaining comparable or superior performance. However, considering the huge amount of training data, the potential of LLMs to serve as search world model has not been widely explored. In particular, the upper bounds of LLMs as world models for reinforcement learning in agentic search remain unknown. 4.2 LARGE LANGUAGE MODELS AS SEARCH ENGINES With the advancement of LLMs, novel paradigm called generative search has emerged, offering users flexible, multi-grained information through generation rather than traditional matching-based retrieval (Li et al., 2024b; 2025c). Current research on using LLMs as search engines primarily explores two approaches. The first, generative retrieval (Tay et al., 2022; Wang et al., 2022; Li et al., 2024c), directly generates document identifiers without explicit matching, with each identifier 18 Preprint. corresponding to specific document in the corpus. These methods operate under the assumption that LLMs have memorized the corpus, effectively functioning as an implicit knowledge base (Long et al., 2024). The second, reliable response generation, employs LLMs to summarize retrieved items, such as papers (Gao et al., 2023) and web pages (Qin et al., 2023), and generates user-centric responses as search results (Shen et al., 2023). These methods address key limitations of traditional information retrieval systems, such as rigid document granularity and relevance matching, while providing better flexibility, efficiency, and creativity for real-world applications (Li et al., 2024a; Ding et al., 2025). According to these applications, LLMs have the potential to serve as world models, providing knowledge for keyword-based searches on world knowledge. However, there has been limited exploration of using LLMs as textual world models in agentic reinforcement learning. 4.3 INFERENCE-TIME SCALING OF LLMS AND AGENTS Repeated Sampling refers to the practice of generating multiple candidate outputs from the same prompt using probabilistic sampling. Brown et al. (2024) find that the coverage of correct answers scales substantially with the number of repeated samples. This finding is further corroborated by Yue et al. (2025), who demonstrate that increasing sample numbers significantly improves the percentage of correct answers captured, even on challenging benchmarks such as AIME. Similar scaling effects have been observed in code generation tasks (Li et al., 2025a). Beyond simple repeated sampling, these approaches can be enhanced through integration with verification mechanisms. Best-of-N sampling (Liu et al., 2025a; Qiu et al., 2024) and majority voting (Zuo et al., 2025) both leverage multiple samples with different selection criteria to achieve superior performance compared to single greedy decoding. Despite these advances in reasoning and generation tasks, the effectiveness of repeated sampling strategies in information retrieval and search contexts remains underexplored. On the other hand, recent developments in TTS have also revealed vast and largely unexplored design space in language-based and embodied agent systems. Zhu et al. (2025) systematically explored various TTS strategies for language agents, demonstrating the effectiveness of parallel sampling, reflective revision, and diversified rollouts. Furthermore, agents such as web agents exhibit superior adaptive behaviors like exploration and backtracking, substantially outperforming traditional per-step scaling methods when applying scaling test-time interaction (TTI) (Shen et al., 2025). In addition to language agents, Yang et al. (2025c) introduced GUI Test-time Scaling Agent (GTA1), leveraging concurrent sampling and evaluation to significantly enhance robustness in graphical user interface (GUI) interaction tasks without relying on extensive lookahead. Complementing these strategies, Lifshitz et al. (2025) presented Multi-Agent Verification (MAV), where multiple aspect verifiers collaboratively evaluate outputs, significantly boosting overall agent performance. Collectively, these recent studies highlight diverse approaches to TTS in agent-based systems, underscoring the potential of both compute allocation and interaction strategies to enhance adaptive and robust agent behaviors across varied environments."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In conclusion, our study establishes that LLMs possess untapped capacity as implicit world models for search-driven tasks, often containing the necessary knowledge to answer complex queries internally. While reliably extracting this knowledge remains difficult, our proposed Self-Search Reinforcement Learning (SSRL) method significantly enhances self-search abilities, outperforming search API-based baselines and enabling robust sim-to-real transfer. These findings suggest promising path toward more autonomous and scalable LLM agents that can operate effectively without reliance on external search engines."
        },
        {
            "title": "REFERENCES",
            "content": "Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Re, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling, 2024. URL https://arxiv.org/abs/2407.21787. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. 19 Preprint. Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z. Pan, Wen Zhang, Huajun Chen, Fan Yang, Zenan Zhou, and Weipeng Chen. Research: Learning to reason with search for llms via reinforcement learning, 2025. URL https://arxiv.org/ abs/2503.19470. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025a. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025b. Longchao Da, Justin Turnau, Thirulogasankar Pranav Kutralingam, Alvaro Velasquez, Paulo Shakarian, and Hua Wei. survey of sim-to-real methods in rl: Progress, prospects and challenges with foundation models, 2025. URL https://arxiv.org/abs/2502.13187. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Yifan Ding, Matthew Facciani, Ellen Joyce, Amrit Poudel, Sanmitra Bhattacharya, Balaji Veeramani, Sal Aguinaga, and Tim Weninger. Citations and trust in llm generated responses. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 2378723795, 2025. Ahmed El-Kishky, Alexander Wei, Andre Saraiva, Borys Minaiev, Daniel Selsam, David Dohan, Francis Song, Hunter Lightman, Ignasi Clavera, Jakub Pachocki, et al. Competitive programming with large reasoning models. arXiv preprint arXiv:2502.06807, 2025. Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. Retool: Reinforcement learning for strategic tool use in llms, 2025. URL https://arxiv.org/abs/2504.11536. Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah D. Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars, 2025. URL https://arxiv.org/abs/2503.01307. Huan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong Liu, Jiahao Qiu, Xuan Qi, Yiran Wu, et al. survey of self-evolving agents: On path to artificial super intelligence. arXiv preprint arXiv:2507.21046, 2025. Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. Enabling large language models to generate text with citations. arXiv preprint arXiv:2305.14627, 2023. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, and et al. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/ 2407.21783. Yu Gu, Kai Zhang, Yuting Ning, Boyuan Zheng, Boyu Gou, Tianci Xue, Cheng Chang, Sanjari Srivastava, Yanan Xie, Peng Qi, et al. Is your llm secretly world model of the internet? modelbased planning for web agents. arXiv preprint arXiv:2411.06559, 2024. Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multihop qa dataset for comprehensive evaluation of reasoning steps, 2020. URL https://arxiv. org/abs/2011.01060. Jian Hu, Jason Klein Liu, and Wei Shen. Reinforce++: An efficient rlhf algorithm with robustness to both prompt and reward models, 2025. URL https://arxiv.org/abs/2501.03262. 20 Preprint. Bowen Jin, Jinsung Yoon, Priyanka Kargupta, Sercan O. Arik, and Jiawei Han. An empirical study on reinforcement learning for reasoning-search interleaved llm agents, 2025a. URL https: //arxiv.org/abs/2505.15117. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025b. Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension, 2017. URL https://arxiv.org/ abs/1705.03551. Manuel Kaspar, Juan Munoz Osorio, and Jurgen Bock. Sim2real transfer for reinforcement learning without dynamics randomization. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 43834388. IEEE, 2020. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466, 2019. doi: 10.1162/tacl 00276. URL https://aclanthology.org/Q19-1026/. Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable agent alignment via reward modeling: research direction. arXiv preprint arXiv:1811.07871, 2018. Dacheng Li, Shiyi Cao, Chengkun Cao, Xiuyu Li, Shangyin Tan, Kurt Keutzer, Jiarong Xing, Joseph E. Gonzalez, and Ion Stoica. S*: Test time scaling for code generation, 2025a. URL https://arxiv.org/abs/2502.14382. Kenneth Li, Aspen Hopkins, David Bau, Fernanda Viegas, Hanspeter Pfister, and Martin Wattenberg. Emergent world representations: Exploring sequence model trained on synthetic task. ICLR, 2023. Xiaoxi Li, Zhicheng Dou, Yujia Zhou, and Fangchao Liu. Corpuslm: Towards unified language model on corpus for knowledge-intensive tasks, 2024a. URL https://arxiv.org/abs/ 2402.01176. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models, 2025b. URL https://arxiv.org/abs/2501.05366. Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yuyao Zhang, Peitian Zhang, Yutao Zhu, and Zhicheng Dou. From matching to generation: survey on generative information retrieval. ACM Transactions on Information Systems, 43(3):162, 2025c. Yongqi Li, Xinyu Lin, Wenjie Wang, Fuli Feng, Liang Pang, Wenjie Li, Liqiang Nie, Xiangnan He, and Tat-Seng Chua. survey of generative search and recommendation in the era of large language models. arXiv preprint arXiv:2404.16924, 2024b. Yongqi Li, Nan Yang, Liang Wang, Furu Wei, and Wenjie Li. Learning to rank in generative retrieval. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 87168723, 2024c. Shalev Lifshitz, Sheila A. McIlraith, and Yilun Du. Multi-agent verification: Scaling test-time compute with multiple verifiers, 2025. URL https://arxiv.org/abs/2502.20379. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. 21 Preprint. Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, and Bowen Zhou. Can 1b llm surpass 405b llm? rethinking compute-optimal test-time scaling, 2025a. URL https://arxiv.org/abs/2502.06703. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective, 2025b. URL https: //arxiv.org/abs/2503.20783. Xinwei Long, Jiali Zeng, Fandong Meng, Zhiyuan Ma, Kaiyan Zhang, Bowen Zhou, and Jie Zhou. Generative multi-modal knowledge retrieval with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 1873318741, 2024. OpenAI. Openai o1 system card, 2024. URL https://arxiv.org/abs/2412.16720. Ivo Petrov, Jasper Dekoninck, Lyuben Baltadzhiev, Maria Drencheva, Kristian Minchev, Mislav Balunovic, Nikola Jovanovic, and Martin Vechev. Proof or bluff? evaluating llms on 2025 usa math olympiad. arXiv preprint arXiv:2503.21934, 2025. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models, 2023. URL https://arxiv. org/abs/2210.03350. Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tur, Gokhan Tur, and Heng Ji. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958, 2025. Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding, Huadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan Liu, Maosong Sun, and Jie Zhou. Webcpm: Interactive web search for chinese long-form question answering, 2023. URL https: //arxiv.org/abs/2305.06849. Jiahao Qiu, Yifu Lu, Yifan Zeng, Jiacheng Guo, Jiayi Geng, Huazheng Wang, Kaixuan Huang, Yue Wu, and Mengdi Wang. Treebon: Enhancing inference-time alignment with speculative treesearch and best-of-n sampling, 2024. URL https://arxiv.org/abs/2410.16033. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, and et al. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402. 03300. Junhong Shen, Hao Bai, Lunjun Zhang, Yifei Zhou, Amrith Setlur, Shengbang Tong, Diego Caples, Nan Jiang, Tong Zhang, Ameet Talwalkar, and Aviral Kumar. Thinking vs. doing: Agents that reason by scaling test-time interaction, 2025. URL https://arxiv.org/abs/2506.07976. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face, 2023. URL https: //arxiv.org/abs/2303.17580. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. URL https://arxiv.org/ abs/2408.03314. Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Yan Zhang, Fei Huang, and Jingren Zhou. Zerosearch: Incentivize the search capability of llms without searching, 2025. URL https://arxiv.org/abs/2505.04588. 22 Preprint. Hao Tang, Darren Key, and Kevin Ellis. Worldcoder, model-based llm agent: Building world models by writing code and interacting with the environment. Advances in Neural Information Processing Systems, 37:7014870212, 2024. Yi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, et al. Transformer memory as differentiable search index. Advances in Neural Information Processing Systems, 35:2183121843, 2022. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions via single-hop question composition, 2022. URL https://arxiv.org/abs/ 2108.00573. Yujing Wang, Yingyan Hou, Haonan Wang, Ziming Miao, Shibin Wu, Qi Chen, Yuqing Xia, Chengmin Chi, Guoshuai Zhao, Zheng Liu, et al. neural corpus indexer for document retrieval. Advances in Neural Information Processing Systems, 35:2560025614, 2022. Zengzhi Wang, Fan Zhou, Xuefeng Li, and Pengfei Liu. Octothinker: Mid-training incentivizes reinforcement learning scaling, 2025a. URL https://arxiv.org/abs/2506.20512. Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Xing Jin, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, Yiping Lu, Kyunghyun Cho, Jiajun Wu, Li Fei-Fei, Lijuan Wang, Yejin Choi, and Manling Li. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning, 2025b. URL https://arxiv.org/abs/ 2504.20073. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. URL https://arxiv.org/abs/2201.11903. Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. Measuring short-form factuality in large language models, 2024. URL https://arxiv.org/abs/2411.04368. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents, 2025. URL https://arxiv.org/abs/2504. 12516. Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, Chenyang Shao, Yuwei Yan, Qinglong Yang, Yiwen Song, Sijian Ren, Xinyuan Hu, Yu Li, Jie Feng, Chen Gao, and Yong Li. Towards large reasoning models: survey of reinforced reasoning with large language models, 2025. URL https://arxiv.org/abs/2501.09686. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, and et al. Qwen technical report, 2025a. URL https://arxiv.org/abs/2505.09388. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025b. Yan Yang, Dongxu Li, Yutong Dai, Yuhao Yang, Ziyang Luo, Zirui Zhao, Zhiyuan Hu, Junzhe Huang, Amrita Saha, Zeyuan Chen, Ran Xu, Liyuan Pan, Caiming Xiong, and Junnan Li. Gta1: Gui test-time scaling agent, 2025c. URL https://arxiv.org/abs/2507.05791. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering, 2018. URL https://arxiv.org/abs/1809.09600. Preprint. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, and et al. Dapo: An open-source llm reinforcement learning system at scale, 2025. URL https://arxiv.org/ abs/2503.14476. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model?, 2025. URL https://arxiv.org/abs/2504.13837. Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments, 2025. URL https://arxiv.org/abs/2504.03160. King Zhu, Hanhao Li, Siwei Wu, Tianshun Xing, Dehua Ma, Xiangru Tang, Minghao Liu, Jian Yang, Jiaheng Liu, Yuchen Eleanor Jiang, Changwang Zhang, Chenghua Lin, Jun Wang, Ge Zhang, and Wangchunshu Zhou. Scaling test-time compute for llm agents, 2025. URL https://arxiv.org/abs/2506.12928. Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, et al. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084, 2025. INFERENCE-TIME SCALING OF SELF-SEARCH A.1 PROMPTS A.1.1 INSTRUCTIONS FOR REPEATED SAMPLING We use the instruction in Table 12 for repeated sampling. Answer the given question. You must conduct reasoning inside <think> and </think> first every time you get new information. After reasoning, if you find you lack some knowledge, you can call search engine by <search> query </search>, and it will return the top searched results between <information> and </information>. You can search as many times as you want. If you find no further external knowledge needed, you can directly provide the answer inside <answer> and </answer> without detailed illustrations. For example, <answer> Beijing </answer>. Question: Table 12: Instruction for repeated sampling. A.1.2 INSTRUCTIONS FOR LLM PROVIDING INFORMATION We use the instruction in Table 13 when querying LLM to provide information. Given query, you need to imitate the style of the following demos and generate five useful documents for the query. [EXAMPLE] You should generate documents that can help the user find the answer. Each document should contain about 30 words. You must directly output the English documents and not output any other texts. Query: query Useful Output: Table 13: Instruction for LLM providing information. A.2 DETAILED RESULTS We introduce the results of repeated sampling of seven benchmarks, across 16 models, in Figure 12. We also list the simulated parameters for each model in Table 14 and comparison of actual vs. fitted values, residuals, and relative errors for every model across different values in Table15, 16, 17 24 Preprint. Figure 12: The results of repeated sampling of seven benchmarks Preprint. Model 1.793 0.191 Llama-3.2-3B-Instruct 1.263 0.183 Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct 0.950 0.159 Qwen3-8B Qwen3-14B Qwen3-32B Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-72B-Instruct 1.370 0.111 1.249 0.117 1.232 0.130 1.533 0.167 1.174 0.163 1.259 0.148 R2 0.986 0.987 0.976 0.984 0.984 0. 0.978 0.989 0.970 RMSE MAE 1.745% 1.583% 1.541% 1.267% 1.688% 1.415% 1.115% 0.906% 1.184% 0.987% 1.073% 0.949% 1.932% 1.674% 1.265% 1.029% 1.984% 1.660% Table 14: Fitting performance metrics for Llama and Qwen series models (RMSE and MAE are in percentage) Llama-3.2-3B-Instruct Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct Actual Fitted Residual Rel. Error (%) Actual Fitted Residual Rel. Error (%) Actual Fitted Residual Rel. Error (%) 20 21 22 23 24 25 26 27 28 29 210 13.50 19.00 25.88 31.32 36.36 41.16 46.28 50.04 52.96 56.72 59.36 16.64 20.78 25.24 29.92 34.74 39.60 44.41 49.10 53.62 57.92 61. -3.14 -1.78 0.64 1.40 1.62 1.56 1.87 0.94 -0.66 -1.20 -2.61 23.26 9.35 2.49 4.45 4.44 3.79 4.04 1.87 1.25 2.12 4.40 25.53 32.20 37.97 42.93 48.63 53.37 56.73 59.60 63.17 65.33 67.83 28.27 32.87 37.54 42.20 46.78 51.22 55.47 59.52 63.32 66.87 70.16 -2.74 -0.67 0.43 0.74 1.86 2.15 1.26 0.08 -0.15 -1.53 -2.33 10.73 2.09 1.12 1.71 3.82 4.03 2.22 0.14 0.24 2.35 3. 35.23 42.20 47.77 52.63 55.83 59.53 62.13 64.73 67.03 69.17 70.50 38.69 42.72 46.69 50.56 54.30 57.88 61.28 64.50 67.52 70.35 72.98 -3.45 -0.52 1.07 2.07 1.54 1.66 0.85 0.24 -0.49 -1.18 -2.48 9.80 1.24 2.24 3.93 2.75 2.78 1.37 0.36 0.73 1.71 3.52 Table 15: Comparison of actual vs. fitted values, residuals, and relative errors for three Llama models across different values. A.3 CASE STUDIES A.3.1 CASE STUDY FOR QWEN3 WITH/WITHOUT THINKING We show case study for Qwen3 with/without thinking mode in Table 18. When enabling thinking, the model will spend many tokens for reasoning at the beginning. A.3.2 CASE STUDY FOR MULTI-TURN AND REFLECTION REPEATED SAMPLING We show the case study for multi-turn and reflection-based repeated sampling in Table 19. SELF-SEARCH REINFORCEMENT LEARNING B.1 PROMPTS We list the instruction used for direct reasoning in Table 20, the instruction for RAG in table 21, and the instruction for R1-like model training in Table 22. B.2 IMPLEMENTATION DETAILS B.2.1 BASELINE IMPLEMENTATION Direct Answer and CoT We show the instruction used in Appendix B.1 and A.1.1 respectively. We set the temperature to 0.0 for consistent evaluation. RAG We use the instruction listed in Appendix B.1. We use Jina and Google search. We set the temperature to 0.0 for consistent evaluation. 26 Preprint. Qwen3-8B Qwen3-14B Qwen2.5-7B-Instruct Actual Fitted Residual Rel. Error (%) Actual Fitted Residual Rel. Error (%) Actual Fitted Residual Rel. Error (%) 20 21 22 23 24 25 26 27 28 29 210 22.92 27.56 31.80 34.84 37.84 40.40 42.56 45.20 47.52 50.04 51. 25.42 28.14 30.91 33.73 36.56 39.39 42.21 45.00 47.75 50.44 53.07 -2.50 -0.58 0.89 1.11 1.28 1.01 0.35 0.20 -0.23 -0.40 -1.43 10.90 2.09 2.79 3.20 3.39 2.50 0.82 0.44 0.48 0.80 2.76 26.28 30.92 35.40 38.96 41.40 45.16 46.32 49.88 52.04 53.84 56.08 28.69 31.62 34.59 37.57 40.55 43.51 46.43 49.29 52.09 54.80 57.44 -2.41 -0.70 0.81 1.39 0.85 1.65 -0.11 0.59 -0.05 -0.96 -1. 9.15 2.26 2.29 3.56 2.04 3.65 0.23 1.18 0.09 1.79 2.42 17.60 24.33 30.40 35.40 40.43 44.43 47.43 51.23 53.77 56.67 59.17 21.58 25.51 29.61 33.81 38.05 42.28 46.44 50.49 54.39 58.13 61.67 -3.98 -1.18 0.79 1.59 2.38 2.16 1.00 0.74 -0.63 -1.46 -2.50 22.63 4.83 2.61 4.50 5.89 4.85 2.10 1.45 1.17 2.58 4.23 Table 16: Comparison of actual vs. fitted values, residuals, and relative errors for Qwen3-8B, Qwen3-14B, and Qwen2.5-7B-Instruct. Qwen2.5-14B-Instruct Qwen2.5-72B-Instruct Qwen3-32B Actual Fitted Residual Rel. Error (%) Actual Fitted Residual Rel. Error (%) Actual Fitted Residual Rel. Error (%) 20 21 22 23 24 25 26 27 28 29 210 28.10 34.97 39.83 44.57 48.87 52.37 55.83 59.17 61.80 64.60 66.73 30.91 35.05 39.22 43.35 47.41 51.35 55.15 58.78 62.22 65.46 68.50 -2.81 -0.09 0.62 1.22 1.46 1.01 0.68 0.39 -0.42 -0.86 -1. 10.01 0.25 1.55 2.73 2.98 1.94 1.22 0.65 0.68 1.34 2.65 24.40 31.50 36.10 41.47 45.83 49.03 51.80 55.10 57.00 58.97 60.80 28.39 32.09 35.85 39.62 43.36 47.04 50.63 54.10 57.44 60.63 63.66 -3.99 -0.59 0.25 1.85 2.47 1.99 1.17 1.00 -0.44 -1.66 -2.86 16.34 1.88 0.69 4.45 5.39 4.06 2.26 1.81 0.77 2.82 4.70 26.96 31.68 36.64 40.24 43.44 46.76 49.64 51.76 54.48 57.36 59. 29.16 32.44 35.75 39.07 42.37 45.64 48.83 51.95 54.97 57.89 60.69 -2.20 -0.76 0.89 1.17 1.07 1.12 0.81 -0.19 -0.49 -0.53 -1.21 8.17 2.39 2.43 2.91 2.45 2.41 1.62 0.37 0.91 0.92 2.03 Table 17: Comparison of actual vs. fitted values, residuals, and relative errors for Qwen2.5-14BInstruct, Qwen2.5-72B-Instruct, and Qwen3-32B. R1 We use the instruction listed in Appendix B.1. We train for up to 5 epochs, and we stop training if any collapse is observed, including response length and training accuracy. We select the checkpoint with the best performance. The training batch size is 256, and the learning rate is 1e-6. We use KL loss coef of 0.001. We train our models based on GRPO, and for each prompt, we generate 5 responses. For evaluation, we use temperature = 0.0. ZeroSearch We use the instruction listed in Appendix A.1.1. We use the same setting as R1. The max turn is 2. The simulation LLM is Simulation LLM google 14B. The start threshold is 0.0, and the end threshold is 0.5. Search-R1 We use the same setting as R1. When training, we use e5 as the retriever and Wikipedia as the corpus. When testing, we use Google Search for consistent comparison. B.2.2 OTHER ALGORITHM IMPLEMENTATION PPO We train for up to 5 epochs, and we stop training if any collapse is observed, including response length and training accuracy. We select the checkpoint with the best performance. The training batch size is 256, and the learning rate is 1e-6. We use KL loss coef of 0.001. The learning rate for the critic model is 1e-5. We use standard GAE as our advantage estimator, with γ = 1.0 and λ = 1.0. REINFORCE++ We train for up to 5 epochs, and we stop training if any collapse is observed, including response length and training accuracy. We select the checkpoint with the best performance. The training batch size is 256, and the learning rate is 1e-6. We use KL loss coef of 0.001. DAPO We train for up to 5 epochs, and we stop training if any collapse is observed, including response length and training accuracy. We select the checkpoint with the best performance. The Preprint. Question: Who is the mother of the director of film Polish-Russian War (Film)? Golden Answer: Małgorzata Braunek Extracted Answer: Elzbieta Munk Thinking Process (With Thinking): <think> Okay, lets see. The question is asking for the mother of the director of the film Polish-Russian War. First, need to figure out who directed that film. But wait, dont remember any film specifically called Polish-Russian War. Maybe its known by different title in English? Sometimes movies have different names in different countries. Alternatively, maybe its historical film about the Polish-Russian conflicts. Let me think. There are several conflicts between Poland and Russia, like the Polish-Soviet War in the early 20th century. Could the film be about that? Maybe the user is referring to film like The Battle of Somosierra, but thats different conflict. Hmm. Wait, perhaps the user is referring to Polish film. Maybe the director is someone well-known. Let me try to search for the film Polish-Russian War to find the director. If can find the director, then can look up their mother. So, first step: search for Polish-Russian War film director. Lets see what comes up. If theres film by that name, can get the directors name. If not, maybe theres confusion with the title. Alternatively, maybe its documentary or less-known film. ... So first, need to search for Polish-Russian War film director to get the correct directors name. </think> <search> Polish-Russian War film director </search> <information> 1. Bitwa Warszawska (1975) directed by Andrzej Munk - focuses on the Polish-Soviet War. 2. The First Echelon (1968) directed by Georgy Yungvald-Khilkevich - Soviet film about Polish-Soviet War. 3. The War (2015) directed by Andrzej Munk - Polish film about the Polish-Soviet War. </information> <search> Andrzej Munk mother </search> <information> 1. Andrzej Munks mother was Elzbieta Munk, Jewish woman from Warsaw who perished in the Holocaust. </information> <search> Georgy Yungvald-Khilkevich mother </search> <information> 1. Limited information found; focus on his career in Soviet cinema. </information> <answer> Elzbieta Munk </answer> Thinking Process (Without Thinking): To answer the question, first need to identify the director of the film Polish-Russian War and then determine who their mother is. Let me start by finding out who directed the film. <search> Who directed the film Polish-Russian War? </search> <information> 1. The film Polish-Russian War (also known as Polsko-Rosyjska Wojna) was directed by Andrzej Wajda. 2. Andrzej Wajda was renowned Polish film director, screenwriter, and actor, known for his contributions to Polish cinema. </information> Now that know the director is Andrzej Wajda, can proceed to find out who his mother was. <search> Who was the mother of Andrzej Wajda? </search> <information> 1. Andrzej Wajdas mother was Jadwiga Wajda (nee Bujak). 2. Jadwiga Wajda was Polish teacher and significant influence in Andrzej Wajdas early life. </information> <answer> Jadwiga Wajda </answer> Table 18: Case study for Qwen3-32B with thinking and without thinking. Preprint. Question: Were Scott Derrickson and Ed Wood of the same nationality? Golden Answer: yes Extracted Answer: No, Scott Derrickson and Ed Wood are not of the same nationality, with Derrickson being Canadian and Wood being American. Thinking Process (Multi): <think> need to consider the nationality of Scott Derrickson and Ed Wood to determine if they are of the same nationality. Scott Derrickson is known for directing movies like Doctor Strange and The Exorcism of Emily Rose. Ed Wood is known for directing cult classic films like Plan 9 from Outer Space. To find their nationalities, Ill need to look into their birthplaces. </think> <search> Scott Derrickson nationality </search> <information> Doc 1: Scott Derrickson is Canadian film director, screenwriter, and producer known for directing Marvels Doctor Strange, released in 2016. Doc 2: Born in 1966, Derrickson grew up in Ontario, Canada, before pursuing career in filmmaking. His background in animation and visual effects influenced his work. Doc 3: Derricksons work often explores the intersection of faith and science. He co-created the TV series Eureka and wrote the screenplay for The Exorcism of Emily Rose. Doc 4: As director, Derricksons films often combine supernatural and psychological elements. His work has been praised for its unique visual style and atmospheric tension. Doc 5: Derricksons production company, Film Science, has produced several films and TV series. He continues to explore the boundaries of storytelling through his work. </information> <think> Now that know Scott Derrickson is Canadian, Ill look into Ed Woods nationality. </think> <search> Ed Wood nationality </search> <information> Doc 1: Ed Wood is an American filmmaker known for his low-budget horror and sci-fi films, including the cult classic Plan 9 from Outer Space. Doc 2: As pioneer in the low-budget film industry, Woods work often featured unique and bizarre storylines, captivating niche audience. Doc 3: Born in Poughkeepsie, New York, Ed Wood began making films as teenager and developed passion for storytelling and filmmaking. Doc 4: Woods most famous film, Plan 9 from Outer Space, is often cited as one of the worst films ever made, but also as cult classic. Doc 5: Ed Woods life and career have been the subject of numerous documentaries and books, exploring his legacy as filmmaker and outsider artist. </information> <answer> No, Scott Derrickson and Ed Wood are not of the same nationality, with Derrickson being Canadian and Wood being American. </answer> Thinking Process (Reflection): <think> need to determine the nationality of Scott Derrickson and Ed Wood to compare them. </think> <search> Scott Derrickson nationality </search> <information> Scott Derrickson is an American film director, screenwriter, and cinematographer. He was born in the United States. </information> <search> Ed Wood nationality </search> <information> Ed Wood was an American film director, screenwriter, and actor. He was born in the United States. </information> <answer> Yes, Scott Derrickson and Ed Wood are of the same nationality. They are both American. </answer> wait, wait, wait <search> nationality American </search> <information> Americans are people born or citizens of </information> <search> nationality same </search> <information> The terms same and nationality typically refer to identical or similar ethnic or cultural identity. </information> <answer> Yes, Scott Derrickson and Ed Wood are of the same nationality. They are both American. </answer> the United States. Table 19: Case study for Multi-turn and reflective Repeated Sampling. 29 Preprint. Answer the given question. Provide the answer inside <answer> and </answer> without any additional information. For example, <answer> Beijing </answer>. Table 20: Instruction for Direct Reason. You are knowledgeable assistant that utilizes the provided documents to answer the users question accurately. Question: question Documents: documents Guidelines: - Analyze the provided documents to extract relevant information. Synthesize the information to formulate coherent and accurate answer. - Ensure that your response directly addresses the users question using the information from the documents. Table 21: Instruction for RAG. training batch size is 256, and the learning rate is 1e-6. We dont use KL loss. The low clip ratio is 0.2, and the high clip ratio is 0.28. We filter groups based on accuracy. KL-Cov We train for up to 5 epochs, and we stop training if any collapse is observed, including response length and training accuracy. We select the checkpoint with the best performance. The training batch size is 256, and the learning rate is 1e-6. We dont use KL loss. We use k-percent of 0.2. B.2.3 TTRL We set the max prompt length to 1024, and the max response length to 3076. The batch size is 8, and for each prompt, we rollout 32 times. The rollout temperature is set at 0.6. We use learning rate of 5e-7 and warm up step of 62. We remove the use of KL loss as in the original paper. We train for 80 epochs and stop when the performance converges. B.3 ABLATION STUDIES B.3.1 MODEL FAMILY COMPARISON Since Qwen is widely regarded as stronger base model than Llama in math or code tasks, we aim to find out whether the conclusion holds when it relies on internal knowledge to answer the knowledgeintensive questions. We also use the default setting for training Qwen. All the training consists of the format, reward, and information token mask. The experimental results is listed in Table 23. Though we still observe the same training pattern as in Llama, for example, the scaling effect and the superior ability of instruct models, the absolute performance is relatively lower than Llama series, indicating that the ability of Qwen to serve as simulator of world knowledge is not as good as Llama. The finding conFigure 13: The performance of TTRL of various models on BrowseComp. Answer the given question. Provide the answer inside <answer> and </answer>. For example, <answer> Beijing </answer>. Lets search step by step. You can break the question into pieces and answer one by one. Table 22: Instruction for R1 Training. 30 Preprint. Model GeneralQA Multi-HopQA NQ TQ HotpotQA Musique 2Wiki Bamboogle Search-R1-base Search-R1-inst ZeroSearch-base ZeroSearch-inst SELF-SEARCH-BASE SELF-SEARCH-INSTRUCT Search-R1-Base Search-R1-Instruct ZeroSearch-Base ZeroSearch-Instruct SELF-SEARCH-BASE SELF-SEARCH-INSTRUCT 40.6 35.8 43.0 41.4 26.2 23.6 43.4 42.4 42.4 43.6 28.8 31.4 Qwen2.5-3B-Instruct 60.0 55.8 61.6 57.4 38.0 41.0 29.2 33.2 33.8 27.4 21.8 22. 11.2 7.6 13.0 30.0 8.4 10.4 Qwen2.5-7B-Instruct 61.4 63.4 66.4 65.2 44.2 44.4 31.2 32.8 32.0 34.6 25.0 26.0 18.2 17.4 34.0 18.4 11.4 11.8 32.0 26.0 34.6 9.8 30.2 26. 35.2 33.2 18.0 35.2 30.4 31.0 12.5 12.5 13.9 11.1 24.0 32.8 27.8 26.4 33.3 27.8 35.2 36.8 Avg 30.9 28.5 33.3 29.5 24.8 26.0 36.2 35.9 37.7 37.5 29.0 30. Table 23: The performance of Qwen2.5 models on General QA and Multi-Hop QA tasks. Model GeneralQA Multi-HopQA NQ TQ HotpotQA Musique 2Wiki Bamboogle Qwen2.5 Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct 23.6 31.4 41.0 44.4 22.4 26.0 Qwen Qwen3-4B Qwen3-8B 22.0 27.0 37.4 45.2 21.8 27.0 10.4 11.8 7.6 10. 26.0 31.0 24.2 31.8 32.8 36.8 34.4 36.0 Avg 26.0 30. 24.7 29.6 Table 24: The performance of Qwen2.5 and Qwen3 models on General QA and Multi-Hop QA tasks. tradicts the trend in reasoning tasks, such as math and code generation, where Qwen is always thought of as the best base model to start. B.3.2 COMPARISON BETWEEN GENERAL MODELS AND REASONING MODELS LRMs show expressive performance on reasoning tasks like math and code generation. However, few work continues to train LRMs to adapt to other fields. To have thorough overview, we compare the RL performance between general models and reasoning models. We use Qwen2.5 and Qwen3 for comparison. The experimental results is shown in Table 24. We find that the performance of Qwen3 is generally lower than Qwen2.5. Recall in Figure 2, the upper bound of Qwen3 is also lower than Qwen2.5. These findings indicate that reasoning models trained with too much math or code generation data may be hard to transfer to other domains easily. We also notice an inferior instruction-following ability during our training process, resulting in decreasing search number, which drops to 0 at later training stage. However, this may also be attributed to the format reward of certain prompt, which contradicts the initial tool call format of the Qwen3 series. B.3.3 DYNAMICS OF TRAINING WITH AND WITHOUT INFORMATION MASK We show the training dynamics with and without the information mask in Figure 14. The experimental results demonstrate that the information mask significantly enhances the models search behavior activity. This indicates that the information mask mechanism encourages the model to perform more search operations, potentially improving the models reasoning capabilities in complex tasks. B.3.4 GROUP SIZE ABLATION For GRPO, we set the group size to 5 as in Jin et al. (2025b). In this part, we ablate on the impact of group size on the training dynamics and final performance. We train for 5 epochs and stop when the final performance converges, and select the checkpoints with the largest validation score. We use the default setting as mentioned above. We experiment on Qwen2.5-3B-Instruct and Llama-3.231 Preprint. Model GeneralQA Multi-HopQA NQ TQ HotpotQA Musique 2Wiki Bamboogle LLaMA-3.2-3B-Instruct Group Size = 5 Group Size = 43.8 44.0 58.4 57.8 25.0 27.0 14.2 12.0 Group Size = 5 Group Size = 10 23.6 26. 41.0 37.8 22.4 22.6 10.4 8.4 Qwen2.5-3B-Instruct 31.6 31.4 26.0 27. 38.4 40.8 32.8 24.8 Avg 35.2 35.5 26.0 24.5 Table 25: The performance of LLaMA and Qwen2.5 models trained with different group sizes. 3B-Instruct. We show the training curve in Figure 14 and the results in Table 25. We observe comparable performance when trained with larger group size, but faster convergence rate. Figure 14: Left: Comparison of the search number with and without information mask on Llama3.1-8B-Instruct. Right: Group size comparison. B.3.5 ADDITIONAL BENCHMARKS For comprehensive evaluation of self-contained search, we further evaluate on SimpleQA with Offline Search, Online Search (We drop out = 3 for simplification), and Entropy-guided Search. We sample 200 records from SimpleQA. The results are listed in Table 27. We find that leveraging internal knowledge solely doesnt help complete tasks like SimpleQA (Wei et al., 2024), perhaps due to SimpleQA is too challenging for models to retrieve factual knowledge from their parameters. However, when accessing the external knowledge base, our models still show great potential for such complex task, indicating that SELF-SEARCH excels at organizing search queries and reasoning based on gathered information in real scenarios even if trained totally in simulated environment. B.3.6 ADDITIONAL RESULTS FOR SIM2REAL SEARCH To test the importance of the first search, we experiment on two-stage generation, where we modify the generated response with the retrieved information to replace the first or the last information part, and then re-generate to obtain final answer. The experiment results are shown in Table 26. It clearly demonstrates the importance of ensuring the quality of the first search and the corresponding information. That is, the first piece of search and the relevant information serves as an anchor for successful search-and-answer trajectory. We further show the experimental results of entropy-guided search and Sim2Real search in Table 5. B.4 CASE STUDIES We provide case study of SSRL-trained Llama-3.2-3B-Instruct and SSRL-trained Qwen2.5-7BInstruct in Table 28 and Table 29 for General QA and Multi-hop QA. After then, we show case study for Sim2Real Reasoning, providing the model with real search engine, in Table 30. At last, we show case study for TTRL-trained models on BrowseComp in Table 31. 32 Preprint. Model GeneralQA Multi-HopQA NQ TQ HotpotQA Musique 2Wiki Bamboogle LLaMA-3.2-3B-Instruct Replace First Replace Last 44.4 41. 63.4 59.6 34.8 24.8 17.2 12.8 LLaMA-3.1-8B-Instruct Replace First Replace Last 39.4 47. 55.8 62.2 34.0 34.4 26.8 22.2 Qwen2.5-3B-Instruct Replace First Replace Last 33.8 23. 49.6 37.0 28.2 22.8 12.0 7.4 Qwen2.5-7B-Instruct Replace First Replace Last 35.8 28. 56.6 45.2 34.0 25.4 17.0 11.4 37.8 32.2 39.8 39.0 33.6 29. 34.8 30.2 42.4 39.2 53.6 49.6 28.0 35.5 40.8 28.8 Avg 40.0 34.9 41.6 42.5 30.9 25.9 36.5 28.2 Table 26: The performance of LLaMA and Qwen2.5 models when replacing retrieved information at either the first or last search step using real search engine. Model SimpleQA LLaMA-3.2-3B-Instruct SSRL Sim2Real Entropy-guided Search 4.5 45.5 43.0 LLaMA-3.1-8B-Instruct SSRL Sim2Real Entropy-guided Search 7.0 35.0 30.0 Qwen2.5-3B-Instruct SSRL Sim2Real Entropy-guided Search 2.7 51.0 45.0 Qwen2.5-7B-Instruct SSRL Sim2Real Entropy-guided Search 4.0 51.0 48.5 Table 27: The performance of LLaMA and Qwen2.5 models on the SimpleQA task."
        },
        {
            "title": "C FORMAT REWARD CODE",
            "content": "In this section, we provide the code to judge whether the rollout sequence is in desired format for search agent. Listing 1: Format Reward Code. def format_reward(response: str) -> float: response = response.strip() # Check if any tag content contains disallowed tags allowed_tags = {think, search, information, answer, /think, /search, /information, /answer} all_tags = re.findall(r<([ˆ>]+)>, response) for tag in all_tags: if tag not in allowed_tags: return 0.0 33 Preprint. Question: Who got the first nobel prize in physics? Answer: Wilhelm Conrad Rontgen Thinking Process: <think> The question is asking who received the first Nobel Prize in Physics. need to find out who that is. </think> <search> first Nobel Prize in Physics </search> <information> 1. The first Nobel Prize in Physics was awarded in 1901 to Wilhelm Conrad Rontgen for his discovery of X-rays. 2. The first Nobel Prize in Physics was awarded in 1901 to Wilhelm Conrad Rontgen. 3. The first Nobel Prize in Physics was awarded in 1901 to Wilhelm Conrad Rontgen for his discovery of X-rays. </information> <think> The first Nobel Prize in Physics was awarded in 1901 to Wilhelm Conrad Rontgen for his discovery of X-rays. </think> <answer> Wilhelm Conrad Rontgen </answer> Question: If both Dictyosperma and Huernia are described as genus Answer: Yes Thinking Process: <think> The question is asking if both Dictyosperma and Huernia are described as genus. </think> <search> Dictyosperma genus </search> <information> 1. Dictyosperma is genus of plants in the asparagus family, Asparagaceae. 2. Dictyosperma is indeed genus of plants. </information> <think> Now we need to check if Huernia is also genus. </think> <search> Huernia genus </search> <information> 1. Huernia is genus of succulent plants in the family Apocynaceae. 2. Huernia is indeed genus of plants. </information> <think> Both Dictyosperma and Huernia are described as genus. </think> <answer> Yes </answer> Question: Who plays the mouse in Alice in Wonderland Answer: Barbara Windsor Thinking Process: <think> The question is asking who plays the mouse in Alice in Wonderland. need to find out who plays the mouse in the 2010 film adaptation of Alice in Wonderland. </think> <search> who plays the mouse in alice in wonderland </search> <information> 1. According to Wikipedia, the mouse in the 2010 film Alice in Wonderland is played by Barbara Windsor. 2. According to IMDb, the mouse in the 2010 film Alice in Wonderland is played by Barbara Windsor. 3. According to Wikipedia, the mouse in the 2010 film Alice in Wonderland is played by Barbara Windsor. </information> <think> The mouse in the 2010 film Alice in Wonderland is played by Barbara Windsor. However, need to verify if this information is correct. </think> <search> alice in wonderland mouse </search> <information> 1. According to IMDb, the mouse in the 2010 film Alice in Wonderland is played by Barbara Windsor. 2. According to Wikipedia, the mouse in the 2010 film Alice in Wonderland is played by Barbara Windsor. 3. According to IMDb, the mouse in the 2010 film Alice in Wonderland is played by Barbara Windsor. </information> <think> The mouse in the 2010 film Alice in Wonderland is played by Barbara Windsor. </think> <answer> Barbara Windsor </answer> Table 28: Case Study for SELF-SEARCH-Llama-3.2-3B-Instruct on General QA and Multi-hop QA. 34 Preprint. Question: When was the last time Philadelphia won the Super Bowl? Golden Answers: Super Bowl LII, 2017 Extracted Answer: 2017 Solution Process: <think> First, we need to identify the last Super Bowl win for the Philadelphia team. </think> <search> last time philadelphia won the superbowl </search> <information> - The Philadelphia Eagles last won the Super Bowl in 2017, defeating the New England Patriots in Super Bowl LII. - The Philadelphia Eagles won Super Bowl LII 41-33. - The last time the Philadelphia Eagles won the Super Bowl was in 2017. </information> <think> Based on the information, the last time the Philadelphia Eagles won the Super Bowl was in 2017. </think> <search> year philadelphia eagles last won superbowl </search> <information> - The Philadelphia Eagles last won the Super Bowl in 2017. - The 2017 Super Bowl was Super Bowl LII, which the Eagles won. - The Philadelphia Eagles have not won Super Bowl since 1980, but their last win was in 2017. </information> <think> Confirming the last Super Bowl win for the Philadelphia Eagles is indeed in 2017. </think> <search> last superbowl win philadelphia team </search> <information> - The Philadelphia Eagles last won the Super Bowl in 2017. - The last time the Philadelphia 76ers won an NBA championship was in 1983. - The last time the Philadelphia Flyers won the Stanley Cup was in 1975. </information> <think> Based on all the information, the last time the Philadelphia Eagles won the Super Bowl was in 2017. </think> <answer> 2017 </answer> Question: Who is older, Aryeh Ben-Eliezer or Jason Pociask Answer: Aryeh Ben-Eliezer Thinking Process: <think> First, we need to determine the birth years of both individuals to compare their ages. </think> <search> Who is older, Aryeh Ben-Eliezer or Jason Pociask </search> <information> - Aryeh Ben-Eliezer was born on 15 October 1937. - Jason Pociask was born on 19 July 1971. </information> <think> Now we know the birth years of both individuals. Aryeh Ben-Eliezer was born in 1937, while Jason Pociask was born in 1971. Clearly, Aryeh Ben-Eliezer is older. </think> <search> Birth year of Aryeh Ben-Eliezer </search> <information> - Aryeh Ben-Eliezer was born on 15 October 1937. </information> <think> Confirming the birth year of Aryeh Ben-Eliezer, we can now definitively state that he is older than Jason Pociask. </think> <answer> Aryeh Ben-Eliezer </answer> Table 29: Case Study for SELF-SEARCH-Qwen2.5-7B-Instruct on General QA and Multi-hop QA. 35 Preprint. Question: Who was president of the United States in the year that Citibank was founded? Answer: James Madison Ground Truth: james madison Thinking Process: <think> First, we need to determine the year Citibank was founded to identify the president at that time. </think> <search> Year Citibank was founded </search> <information> - Citibank was founded in 1812 as City Bank of New York, and later became First National City Bank of New York. </information> <think> Now that we know Citibank was founded in 1812, we need to find out who was the president of the United States in 1812. </think> <search> Who was president in 1812 United States </search> <information> - James Madison (March 16, 1751 [O.S. March 5, 1750] June 28, 1836) was an American statesman, diplomat, and Founding Father who served as the fourth. </information> <think> James Madison was the president in 1812, the year Citibank was founded. </think> <search> James Madison presidency year 1812 </search> <information> - Although he was accused of weakness in dealing with France and England, Madison won the presidency in 1808 by publishing his vigorous diplomatic dispatches. - American statesman, diplomat, and Founding Father who served as the fourth president of the United States from 1809 to 1817. - Fourth President 1809-1817. Fast Fact: James Madison, Father of the Constitution, led the inconclusive War of 1812. </information> <think> James Madison was the president from 1809 to 1817, and he was the president during the year Citibank was founded in 1812. </think> <search> Who was president in 1812 United States </search> <information> - James Madison was the fourth president of the United States, serving from 1809 to 1817. </information> <think> Based on the information, James Madison was the president of the United States in 1812 when Citibank was founded. </think> <answer> James Madison </answer> Question: What physical force is used by the machine that extracts honey from honeycombs? Answer: centrifugal force Thinking Process: <think> First, we need to identify the machine used for extracting honey and the physical force it uses. </think> <search> machine used to extract honey from honeycombs physical force </search> <information> - honey extractor extracts the honey from the honey comb without destroying the comb. Extractors work by centrifugal force. drum or container holds frame. - honey extractor is device used to extract honey from the comb by spinning it at high speed. The centrifugal force causes the honey to be flung out of. - Honey extractors are simple mechanical devices that extract honey from honeycombs. They use centrifugal force in drum or container with frame. </information> <think> Based on the information, the machine used to extract honey from honeycombs uses centrifugal force. </think> <search> physical force used in honey extractor </search> <information> - honey extractor is piece of equipment used by beekeepers in the extraction of honey from honeycombs. honey extractor works by using centrifugal force. - Extractors work by centrifugal force. drum or container holds frame basket which spins, flinging the honey out. With this method the wax comb stays intact. - Drum. The drum is the central component that holds the frames. It spins to create centrifugal force, which helps remove honey from the comb. </information> <think> It is clear that the physical force used in honey extractor is centrifugal force. </think> <search> centrifugal force in honey extractor </search> <information> - Extractors work by centrifugal force. drum or container holds frame basket which spins, flinging the honey out. With this method the wax comb stays intact. - honey extractor uses centrifugal force to extract honey from the honey comb without destroying the comb. </information> <answer> centrifugal force </answer> Table 30: Case Study for integration with real search engine. Preprint. Question: professional football player completed transfer to new club in the 2010s, in exchange for another player. This player later described his time at the new club negatively, and seven articles were written about him by Football Insider in the years 2016 and 2017. He was released from his contract by club founded in the 1920s more than 3 years after this transfer. He entered new coaching role in 2020 at football club in different country known for their attacking style. The player for which he was exchanged ended his career at football club representing town that has castle. How many league goals did the first player score for the club that he left in this transfer? Golden Answer: 76 Extracted Answer: 76 Thinking Process: <think> need to find the professional football player who completed the transfer in the 2010s. will search for the players name. </think> <search> Professional football player transferred in 2010s </search> <information> The player is Daniel Sturridge, who transferred from Manchester City to Liverpool in 2013. He was released by Liverpool in 2019. He entered new coaching role in 2020 at Middlesbrough. He ended his career at Middlesbrough, football club representing town with castle. According to his career statistics, Daniel Sturridge scored 76 league goals for Manchester City. </information> <think> Since found the players name, can now find the number of league goals he scored for the club that he left. </think> <answer> 76 </answer> Question: professional football player completed transfer to new club in the 2010s, in exchange for another player. This player later described his time at the new club negatively, and seven articles were written about him by Football Insider in the years 2016 and 2017. He was released from his contract by club founded in the 1920s more than 3 years after this transfer. He entered new coaching role in 2020 at football club in different country known for their attacking style. The player for which he was exchanged ended his career at football club representing town that has castle. How many league goals did the first player score for the club that he left in this transfer? Golden Answer: 76 Extracted Answer: 105 Thinking Process: <think> need to find the professional football player who completed the transfer in the 2010s. will search for the players name. </think> <search> Professional football player transferred in 2010s </search> <information> Heading into the 2010s, Cristiano Ronaldos C94 million move from Manchester United to Real Madrid was the world record transfer. This list of the top 10 best college football transfers dating back to 2010 to come into creation from best to worst, beginning from one to ten. </information> <think> Since found the player, can now find the club he was released from and the coaching role he entered. can also find the club he ended his career at. will search for the number of league goals he scored for the club that he left. </think> <answer> 105 </answer> Table 31: Case study of TTRL-trained Llama-3.2-3B-Instruct on BrowseComp. 37 Preprint. # Must start with <think> and end with </answer> if not (response.startswith(<think>) and response.endswith(</ answer>)): return 0.0 # Extract all tags in order tags = re.findall(r<(/?(?:thinksearchinformationanswer))>, response) # Check if any tag content is empty tag_contents = { think: re.findall(r<think>(.*?)</think>, response, re.DOTALL) , search: re.findall(r<search>(.*?)</search>, response, re. DOTALL), information: re.findall(r<information>(.*?)</information>, response, re.DOTALL), answer: re.findall(r<answer>(.*?)</answer>, response, re. DOTALL) } if len(tags) < 4: return 0.0 # Return 0 if any tag has empty content for tag_type, contents in tag_contents.items(): for content in contents: if not content.strip(): return 0.0 if tag_type == search and len(content.split(n)) != 1: return 0.0 if tag_type == search and your query in content.lower(): return 0.0 if tag_type == think and your thoughts in content.lower() : return 0.0 if tag_type == answer and your answer in content.lower(): return 0.0 if tag_type == information and your information in content.lower(): return 0.0 # Check structure if tags[0] != think or tags[1] != /think: return 0.0 if tags[-2] != answer or tags[-1] != /answer: return 0.0 # Check search-information pairing in the middle middle_tags = tags[2:-2] # Exclude initial think and final answer = 0 while < len(middle_tags): if middle_tags[i] == search: # Must be followed by /search, information, /information if (i + 3 >= len(middle_tags) or middle_tags[i + 1] != /search or middle_tags[i + 2] != information or middle_tags[i + 3] != /information): return 0.0 += 4 else: += 1 Preprint. think_num = response.count(<think>) search_num = response.count(<search>) information_num = response.count(<information>) if search_num != information_num: return 0.0 max_turn = 2 score = 1.0 / max_turn * think_num ratio = 1.0 upper_bound = 8 if think_num != search_num + 1: ratio = min(think_num, search_num + 1) / max(think_num, search_num + 1) return min(score, 1.0) * ratio if think_num <= upper_bound else 0."
        }
    ],
    "affiliations": [
        "CSCEC Third Bureau",
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "Tsinghua University",
        "University College London",
        "WeChat AI"
    ]
}