{
    "paper_title": "Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty",
    "authors": [
        "Mehul Damani",
        "Isha Puri",
        "Stewart Slocum",
        "Idan Shenfeld",
        "Leshem Choshen",
        "Yoon Kim",
        "Jacob Andreas"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "When language models (LMs) are trained via reinforcement learning (RL) to generate natural language \"reasoning chains\", their performance improves on a variety of difficult question answering tasks. Today, almost all successful applications of RL for reasoning use binary reward functions that evaluate the correctness of LM outputs. Because such reward functions do not penalize guessing or low-confidence outputs, they often have the unintended side-effect of degrading calibration and increasing the rate at which LMs generate incorrect responses (or \"hallucinate\") in other problem domains. This paper describes RLCR (Reinforcement Learning with Calibration Rewards), an approach to training reasoning models that jointly improves accuracy and calibrated confidence estimation. During RLCR, LMs generate both predictions and numerical confidence estimates after reasoning. They are trained to optimize a reward function that augments a binary correctness score with a Brier score -- a scoring rule for confidence estimates that incentivizes calibrated prediction. We first prove that this reward function (or any analogous reward function that uses a bounded, proper scoring rule) yields models whose predictions are both accurate and well-calibrated. We next show that across diverse datasets, RLCR substantially improves calibration with no loss in accuracy, on both in-domain and out-of-domain evaluations -- outperforming both ordinary RL training and classifiers trained to assign post-hoc confidence scores. While ordinary RL hurts calibration, RLCR improves it. Finally, we demonstrate that verbalized confidence can be leveraged at test time to improve accuracy and calibration via confidence-weighted scaling methods. Our results show that explicitly optimizing for calibration can produce more generally reliable reasoning models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 6 0 8 6 1 . 7 0 5 2 : r BEYOND BINARY REWARDS: TRAINING LMS TO REASON ABOUT THEIR UNCERTAINTY Mehul Damani Isha Puri Yoon Kim Jacob Andreas Massachusetts Institute of Technology Stewart Slocum Idan Shenfeld Leshem Choshen"
        },
        {
            "title": "ABSTRACT",
            "content": "When language models (LMs) are trained via reinforcement learning (RL) to generate natural language reasoning chains, their performance improves on variety of difficult question answering tasks. Today, almost all successful applications of RL for reasoning use binary reward functions that evaluate the correctness of LM outputs. Because such reward functions do not penalize guessing or low-confidence outputs, they often have the unintended side-effect of degrading calibration and increasing the rate at which LMs generate incorrect responses (or hallucinate) in other problem domains. This paper describes RLCR (Reinforcement Learning with Calibration Rewards), an approach to training reasoning models that jointly improves accuracy and calibrated confidence estimation. During RLCR, LMs generate both predictions and numerical confidence estimates after reasoning. They are trained to optimize reward function that augments binary correctness score with Brier scorea scoring rule for confidence estimates that incentivizes calibrated prediction. We first prove that this reward function (or any analogous reward function that uses bounded, proper scoring rule) yields models whose predictions are both accurate and well-calibrated. We next show that across diverse datasets, RLCR substantially improves calibration with no loss in accuracy, on both in-domain and out-of-domain evaluationsoutperforming both ordinary RL training and classifiers trained to assign post-hoc confidence scores. While ordinary RL hurts calibration, RLCR improves it. Finally, we demonstrate that verbalized confidence can be leveraged at test time to improve accuracy and calibration via confidence-weighted scaling methods. Our results show that explicitly optimizing for calibration can produce more generally reliable reasoning models."
        },
        {
            "title": "INTRODUCTION",
            "content": "Many recent advances in language models (LMs) have been driven by reasoning modelsLMs trained via reinforcement learning (RL) to think out loud in natural language before answering questions. These models have achieved state-of-the-art performance on challenging tasks like math and programming (Guo et al., 2025). The standard approach to reasoning training (often referred to as reinforcement learning with verifiable rewards, or RLVR) performs RL with simple binary correctness reward: Rcorrectness(y, y) = 1yy , where checks whether the models output matches ground-truth answer y. While simple and effective for improving accuracy, this reward comes with critical limitation: it rewards models equally whether they are confidently correct or merely guessing, and penalizes identically whether they abstain or produce incorrect answers. This incentivizes overconfident guessing, undermining trustworthiness. Consistent with this concern, studies have shown that even when initially well-calibrated, LLMs tend to become overconfident following RL training (Leng et al., 2025). Reasoning models, in particular, tend to exhibit worsened calibration and increased hallucination rates compared to base models, particularly when trained with reward signals that emphasize only correctness (Kirichenko et al., 2025; Yao et al., 2025; OpenAI, 2025). This is critical limitation in high-stakes domains such as healthcare or law, where models must not only be accurate but also communicate uncertainty when appropriate (Omar et al., 2024). Equal Contribution. Correspondence to mehul42@mit.edu. 1 Figure 1: (a): Sample chain-of-thought from model trained with RLCR, using <think>, <answer>, <analysis>, and <confidence> tags. (b) On in-domain evaluation tasks, RLCR improves on standard reasoning training (RLVR) and even slightly outperforms combination of RLVR and dedicated classifier trained to predict RLVR correctness. (c) When evaluating generalization to novel tasks, RLCR improves both accuracy and calibration, while other methods leave accuracy unchanged and sometimes harm calibration. All results shown are for HotpotQA, see Section 4 for results on math tasks and additional baselines. This paper aims to address these limitations by answering two questions: (1) Can reasoning models be optimized for both correctness and calibration? (2) Can the contents of reasoning chains themselves improve calibration? We approach these questions through the lens of statistical decision theory, specifically the theory of proper scoring rules. Given predictor that produces an output and confidence q, proper scoring rule is minimized when reflects the true probability that will agree with ground-truth outcome (Gneiting & Raftery, 2007). canonical example is the Brier score (Brier, 1950): RBrier(y, q, y) = (q 1yy )2. Proper scoring rules are widely used in forecasting (Waghmare & Ziegel, 2025), but have seen little application in training LLMs with RL. Our approach, RLCR (reinforcement learning with calibration rewards), involves modified version of reasoning training that encourages models to reason about both task correctness and uncertainty. To do so, we simply train models to output both answers and (verbalized) confidence scores q, optimizing combined reward function: RRLCR(y, q, y) = Rcorrectness(y, y) + RBrier(y, q, y) = 1yy (q 1yy )2 . (1) We show that this approach has several appealing theoretical and empirical properties: RLCR provably incentivizes both accuracy and calibration: RRLCR is maximized when models output the answer most likely to be correct, along with calibrated estimate of their probability of success. In other words, RRLCR is maximized by LM outputs (y, q) for which maximizes p(1yy ), and = p(1yy ). We show that such an objective can be constructed if any bounded, proper scoring rule is used for the calibration term. Notably, while the ubiquitous log-likelihood loss is itself proper scoring rule, it does not have this property, and can incentivize models to output incorrect answers. In experiments on factual question answering and mathematical reasoning tasks, RLCR matches the task accuracy of RLVR while substantially improving calibration, on in-domain problems, reducing expected calibration error from 0.37 0.03 on HotpotQA (Yang et al., 2018) and 0.26 0.10 on collection of math datasets. When these same models are evaluated on their generalization to out-of-domain tasks, RLVR substantially worsens calibration relative to the base model. By contrast, RLCR substantially improves calibration, outperforming both the base model, model trained with RLVR, and predictor equipped with second model fine-tuned only to output confidence scores. Verbalized confidence can be incorporated into test-time scaling methods, giving rise to improved ensembling and best-of-N methods. This may be attributed to the fact that RLVR 2 also improves the coherence of model predictions across samples: when multiple reasoning chains and predictions are generated for given question, RLCR reduces the variance in confidence scores across reasoning chains that lead to the same answer, and reduces the frequency with which models assign high confidence to contradictory answers. Together, these results show that existing reasoning training methods can be straightforwardly modified to additionally optimize for calibration, and that this improves in turn improves their accuracy, robustness, and scalability."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "Let πθ be language model that maps from prompts to outputs , both of which may be encoded as strings. Given dataset of promptoutput pairs = {(xi, )} (e.g. questions and ground-truth answers) and reward function : that compares predicted to ground-truth answers, our goal is to improve LM outputs by optimizing: arg max θ E(x,y)D, yπθ(x) R(y, y) . (2) Reinforcement learning with verifiable rewards (RLVR) When training reasoning models, standard choice of is the binary correctness reward: Rcorrectness(y, y) = 1yy , (3) where 1yy {0, 1} is the indicator function that evaluates whether is correct, i.e. equivalent (perhaps modulo formatting details) to y. Proper scoring rules Often, we want predictors that output not only an answer y, but some scalar measure of confidence in this answer.1 scoring rule measures the quality of confidence estimate. In the case of modeling binary outcomes (e.g. our confidence that given answer is correct), scoring rule is function : {0, 1} that maps confidence estimate and an outcome to scalar score. scoring rule is called proper if its expected value is minimized by confidence scores that match the true outcome probability: Eap(a) S(p(a), a) Eap(a) S(q, a) for any q. Perhaps the most familiar example of proper scoring rule is the log-loss: Logarithmic score: S(q, a) = log + (1 a) log(1 q) . But many other examples exist, including Brier score: Spherical score: S(q, a) = (a q)2 , S(q, a) = (cid:46)(cid:112)q2 + (1 q)2 . (4) (5) (6) (7) What all these scores have in common is the property that they are maximized when confidences match the true probability p(a = 1)."
        },
        {
            "title": "3 METHOD",
            "content": "The main idea behind our approach is to train language models via reinforcement learning with reward that incentivizes both correctness and calibration, by combining standard correctness reward with reward based on the Brier score. In this approach, models are first prompted to produce 1It is sometimes even more useful to train models that can place complete distribution over large set of possible answers y. But for very large answer spaces or expensive predictorslike language models performing chain-of-thought reasoningenumerating and scoring all possible answers is generally impractical. This paper mainly focuses on models that generate one answer and one confidence score, though see Section 4.6 for one way of using this approach to generate and score multiple answers. 3 Figure 2: (a): RLVR focuses solely on correctness, which can incentivize guessing. (b): RLCR uses calibrated reward that jointly optimizes for correctness and calibration. reasoning chains that produce both answers and confidences estimates (as in Fig. 1a). They are then trained to optimize: RRLCR(y, q, y) = 1yy (q 1yy )2 . (8) Intuitively, this reward incentivizes correctness but penalizes models when they output incorrect answers with high confidence or correct answers with low confidence. It is not immediately obvious that this reward function incentivizes desired LM behaviorbecause it involves tradeoff between accuracy (the first term) and calibration (the second), we might worry that models will learn to output answers certain to be wrong in order to obtain small calibration loss. But in fact, the calibration term in Eq. (8) comes at no cost in accuracy: Theorem 1. Suppose, for any possible prediction y, that the success indicator 1yy is drawn from distribution Bernoulli(py). Then RRLCR in Eq. (8) satisfies two properties: 1. Calibration incentive. For any y, the expected reward E1yy RRLCR(y, q, y) is maximized when = py. 2. Correctness incentive. Among all calibrated predictions (y, py), expected reward is maximized by the prediction whose success probability py is greatest.2 Proof is given in Appendix A. An important property of Theorem 1 is that we cannot replace the Brier term (q 1yy )2 with any proper scoring rulefor example the log loss 1yy log + (1 1yy ) log(1 q) does not incentivize correctness. However, as discussed in Appendix A, an analogous version of Theorem 1 exists for any bounded proper scoring rule."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "Our main experiments aim to evaluate how RLCR empirically changes the accuracy and calibration of LMs, both in in-domain evaluations on the task used for RL, and out-of-domain evaluations on other question-answering tasks. Additional experiments evaluate interactions between RLCR and other test-time reasoning paradigms, the role of reasoning in confidence estimation specifically, and the extent to which RLCR causes LM predictions to become more coherent across predictions. 2Note that statement of the problem does not distinguish between epistemic and aleatoric uncertainty about success. Obviously, once has been predicted, the outcome of evaluation is fully determined, and the objective probability that is either 0 or 1. But an informationor computation-constrained predictor may still possess subjective uncertainty. 4 Figure 3: (a) Reward curves for RLCR (ours) and RLVR. Both correctness and calibration rewards improve under our method, demonstrating simultaneous gains in correctness and calibration. The Brier reward is shifted upward by 1 for clarity. (b) Completion lengths during training. The completion lengths of our method gradually increase during training as uncertainty reasoning improves. 4.1 EXPERIMENTAL SETUP Training Details We use GRPO as base RL algorithm with some modifications (see Appendix B.2). Following recent work on RL training for LM reasoning (Hu et al., 2025; Guo et al., 2025), we initialize from RL from the base model and do not use any KL regularization. Specifically, we use the Qwen2.5-7B base model, part of the Qwen family recognized for strong performance in RL tasks (Hu et al., 2025; Gandhi et al., 2025). For HotPotQA, we use maximum response length of 1536 while for Math, we use 4096. To enable easier verification and more structured outputs, we augment Eq. (8) with simple format reward that encourages models to enclose CoTs within the right tags. Methods We evaluate the following methods: 1. Base: The base pre-trained model. We use Qwen2.5-7B Base in our experiments. 2. RLVR: Initialized from the base model and trained using Rcorrectness with <think> and <answer> tags. During evaluation, model is also asked to verbalize confidence. 3. RLVR + BCE Classifier: confidence classifier trained on outputs from the RLVR model. Specifically, given dataset of problems, solution CoTs (from RLVR), and correctness labels (x, y, 1yy ), we train classifier fθ(x, y) to predict the models confidence score using the binary cross-entropy (BCE) loss: LBCE(θ) = E(x,y,1yy ) [1yy log fθ(x, y) + (1 1yy ) log(1 fθ(x, y))] where fθ(x, y) (0, 1) is the classifiers confidence. The classifier is initialized from Qwen2.5-7B Base and is thus highly expressive. This approach is expensive, as it requires training and inference of two large models. (9) 4. RLVR + Brier Classifier: Instead of using binary cross-entropy (BCE) loss, we use mean squared error (MSE), which allows more direct optimization of the Brier score: LBrier(θ) = E(x,y,1yy ) (cid:104) (fθ(x, y) 1y=y )2(cid:105) (10) 5. RLVR + Probe: Given the final-layer embedding ϕ(x, y) of the RLVR model, we train linear probe to predict confidence scores: in Eq. (9), we replace the fine-tuned LM with linear model: fθ(x, y) = log σ(θϕ(x, y)) (11) where σ() denotes the sigmoid function. This method is closely related to the classifier, but replaces it with simple, less expressive linear probe. 6. Answer Probability: We generate outputs using RLVR, extract the tokens enclosed within the <answer> tags, and compute their average probability: AnswerProb(y) = 1 (cid:88) tA Pθ(yt y<t, x) (12) 5 Here, = (y1, y2, . . . , yT ) is the full generated sequence. The set {1, . . . , } denotes the token positions that appear between the <answer> tags. Pθ(yt y<t, x) represents the models probability of generating token yt. 7. RLCR (ours): Initialized from base model and trained using RRLCR. Evaluation Metrics We use the following evaluation metrics: 1. Accuracy (): measure of performance. 2. Area under ROC curve (AUROC) (): Measures ability of classifier to distinguish between positive/negative classes across thresholds. AUROC = (cid:90) 1 0 TPR(FPR1(t)) dt where TPR is the True Positive Rate and FPR is the False Positive Rate. 3. Brier Score (): Squared difference between confidence and ground truth. Brier Score = 1 (cid:88) (qi 1yiy )2 i= (13) (14) 4. Expected Calibration Error (ECE) (): Calibration metric that groups confidences into bins and computes difference between the average correctness and confidence. ECE = (cid:88) m=1 Bm acc(Bm) conf(Bm) (15) where is the number of bins , Bm is the set of samples in bin m, and is the number of samples. We use M=10. Evaluation datasets We evaluate our method on benchmarks that highlight distinct sources of uncertainty. HotPotQA (Yang et al., 2018) tests calibration under incomplete or distracting evidence, while SimpleQA (Wei et al., 2024) and TriviaQA (Joshi et al., 2017) probe overconfidence on obscure factual knowledge; GPQA (Rein et al., 2024), Math500 (Hendrycks et al., 2021), GSM8K (Cobbe et al., 2021), and Big-Math (Albalak et al., 2025) assess calibration in complex, multi-step or scientific reasoning, where uncertainty accumulates across steps. CommonsenseQA (Talmor et al., 2019) examines confidence in ambiguous, implicit reasoning scenarios. Together, these datasets capture key types of uncertainty, from ambiguous evidence to rare knowledge and complex reasoning. 4.2 HOTPOTQA Dataset We use modified HotPotQA distractor dataset (Yang et al., 2018) with multi-hop questions and 10 paragraphs (2 relevant, 8 distractors). To test uncertainty reasoning, HotPotQA-Modified removes 0, 1, or both relevant paragraphs, creating varying information completeness. The dataset is evenly split across these conditions, with 8 paragraphs per example. We train on 20,000 examples and use exact string match to compute correctness. In-distribution performance Fig. 3 shows the training curves for RLCR and RLVR. Both the correctness and calibration reward for RLCR increase smoothly, indicating that the model is able to jointly improve accuracy and calibration. Table 1 shows results on the original HotpotQA distractor dataset. As expected, RL-trained models outperform off-the-shelf models in multi-hop accuracy. RLCR matches RLVR in accuracy, showing that the calibration term does not hurt performance. However, both the base and RLVR are highly overconfident and poorly calibrated. In contrast, our method and the classifiers are much better calibrated, with our method slightly ahead. The Answer Probability baseline also exhibits poor calibration and overconfidence, as the model typically arrives at its answer during CoT reasoning, making the subsequent probability of outputting that answer within <answer> tags extremely high. 6 Method HotpotQA O.O.D (a) Models Trained on HotpotQA Base RLVR RLVR + BCE Classifier RLVR + Brier RLVR + Probe Answer Prob RLCR (ours) Acc. () 39.7% 63.0% 63.0% 63.0% 63.0% 63.0% 62.1% AUROC Brier ECE () () () 0.54 0.50 0.66 0.65 0.55 0.72 0.69 0.53 0.37 0.22 0.22 0.24 0.36 0.21 0.53 0.37 0.07 0.09 0.10 0.36 0.03 Acc. () 53.3% 53.9% 53.9% 53.9% 53.9% 53.9% 56.2% AUROC Brier ECE () () () 0.54 0.50 0.58 0.60 0.53 0.60 0.68 0.41 0.46 0.27 0.32 0.38 0.42 0. 0.40 0.46 0.24 0.33 0.38 0.42 0.21 Method Math O.O.D Averaged (b) Models Trained on Big-Math Base RLVR RLVR +Classifier RLVR +Brier-Classifier RLVR +Probe Answer Prob RLCR (ours) SFT+RLCR (ours) Acc. () 56.1% 72.9% 72.9% 72.9% 72.9% 72.9% 72.7% 72.2% AUROC Brier ECE () () () 0.56 0.47 0.78 0.78 0.65 0.52 0.67 0. 0.40 0.28 0.15 0.15 0.19 0.26 0.17 0.14 0.39 0.26 0.10 0.10 0.13 0.26 0.10 0.08 Acc. () 47.8% 52.5% 52.5% 52.5% 52.5% 52.5% 50.9% 43.8% AUROC Brier ECE () () () 0.53 0.52 0.55 0.57 0.53 0.52 0.60 0.66 0.46 0.49 0.34 0.28 0.33 0.44 0.28 0.24 0.45 0.49 0.33 0.27 0.30 0.43 0.25 0.18 Table 1: Accuracy and calibration metrics for models trained on HotpotQA and Big-Math. (a) Performance on HotpotQA and 6 out-of-distribution (O.O.D) datasets. RLCR achieves competitive accuracy and significantly outperforms all baselines in calibration, especially on O.O.D. datasets, demonstrating the benefits of jointly optimizing accuracy and calibration. (b) Performance on Math and 5 out-of-distribution (O.O.D.) datasets. Math results are averaged over 3 datasets: Math-500, GSM8K and Big-Math. SFT+RLCR variant achieves the best calibration across both in-distribution and O.O.D settings, demonstrating the benefit of SFT warmup. However, this comes at the cost of reduced generalization accuracy, possibly due to catastrophic forgetting. RLCR offers stronger trade-off in O.O.D settings, maintaining competitive accuracy while improving calibration relative to all baselines. Generalization We evaluate generalization performance on six datasets: TriviaQA, SimpleQA, MATH500, GSM8K, CommonsenseQA, and GPQA. Table 1 presents the average performance across these datasets (individual results in Appendix D). The base models accuracy closely matches that of the models trained with RL, indicating that RL training on HotpotQA does not enhance OOD reasoning. Moreover, RLVR actually hurts calibration relative to the base model on out-of-distribution tasks. In contrast, RLCR achieves substantial gains over all baselines across calibration metrics while maintaining (or slightly improving) on task accuracy. We hypothesize that better calibration generalization of RLCR could be due to: 1. Uncertainty in chain-of-thought reasoning Reasoning about uncertainty can improve calibration by allowing reflection on confidence, in line with recent work (Yoon et al., 2025). 2. Training dynamics of RL During RL training, the models confidence analysis and scores have to constantly adapt to the models improving task performance. This non-stationarity might lead to more robust learning and better generalization. 7 Figure 4: Test-time scaling curves. (a) Accuracy vs Number of Samples (N). Accuracy improves for all methods with increasing compute. Confidence-weighted majority vote outperforms both vanilla majority vote and max-confidence, highlighting complementary benefits of combining voting with confidence scores. (b) Brier Scores vs Ensemble Size (K). Here we evalute the effect of applying test-time scaling to confidence estimation alone, resampling multiple analyses (blue text in Fig. 1). Calibration improves as the size of the analysis ensemble grows. 3. Shared representations Using single model for both solution generation and calibration allows the calibration task to leverage internal representations used by the solution generating process, possibly improving generalization. 4.3 MATH Dataset We use Big-Math (Albalak et al., 2025), large, curated training dataset for RL containing over 250,000 math problems, including questions from benchmarks such as Math and GSM8K. We retain problems for which the LLaMA-8B solve rate (provided in the dataset) is between 0-70% and restrict the dataset to problems with numerical answers, enabling near-perfect automatic verification. Our final training set consists of 15,000 problems. We compute correctness using math-verify3, robust expression evaluation system. SFT Warmup While applying RL directly to the base model improves calibrated reward, the uncertainty analyses produced in Math remain qualitatively generic, often lacking reasoning tied to specific solution steps (See Appendix D.2 for an example). To enhance their quality, we train variant with lightweight SFT warmup phase before RL. Specifically, we generate solutions from the base model on 500 examples and prompt Deepseek-R1 to produce uncertainty analyses for them. Further details in Appendix B.2. Results On Math benchmarks (averaged over GSM8K, Math, and Big-Math), all RL methods improve accuracy significantly over the base model. SFT+RLCR achieves the best calibration, slightly surpassing the classifiers, while base and RLVR remain poorly calibrated. Out-of-distribution (TriviaQA, SimpleQA, CommonsenseQA, GPQA, HotPotQA), the accuracies of RLCR and RLVR are marginally better than the base model, but surprisingly the accuracy of the SFT+RLCR model drops significantly, possibly due to catastrophic forgetting induced by SFT warmup. Despite this, SFT+RLCR achieves the strongest calibration. RLCR offers stronger trade-off in O.O.D. settings, maintaining accuracy while matching or outperforming all baselines on calibration. 4.4 CAN VERBALIZED CONFIDENCES BE USED FOR TEST-TIME SCALING? We next evaluate whether confidence scores from RLCR can be incorporated into test-time scaling algorithms to yield improvements in both accuracy and calibration. 3https://github.com/huggingface/Math-Verify 8 Figure 5: Brier scores (a) and ECE (b) of baseline / analysis classifiers on HotPotQA-Modified across three model sizes. Analysis classifiers outperform baselines at smaller sizes, suggesting that uncertainty CoT is essential for better calibration when capacity is limited. Accuracy Given responses y1, y2, ...yN and reward model r(x, y), best-of-N selects the response with highest reward: ychosen = arg max {y1,...,yN }p(x) R(x, yi) . (16) Similarly, majority vote selects the most frequent answer among the answers. Prior work has also combined the two, yielding weighted majority vote, where the voting is weighted using scores from the reward model. Our key insight is that the verbalized confidence output by the model can serve as an effective proxy reward. In best-of-N, we can select the response with the highest verbalized confidence from the set of candidates. We refer to this as max-confidence. Likewise, we extend majority vote to confidence-weighted majority vote, where each responses vote is weighted by its associated confidence score. Importantly, both these approaches do not require any additional supervision or external reward models. We use the RLCR model trained on Hotpot for generation, and plot average accuracy across the 7 datasets used in Table 1. Fig. 4a shows that accuracy improves with compute across all methods. Confidence-weighted majority vote outperforms both vanilla majority voting and max-confidence, indicating that combining confidence with voting can yield complementary benefits. Although maxconfidence underperforms both voting variants, it is more flexible and remains valuable in settings where aggregation across responses (needed for voting) is infeasible, such as program synthesis or other open-ended generation tasks. Overall, the accuracy gains of these scaling algorithms are closely tied to calibrationimprovements in calibration will translate to improvements in accuracy. Calibration In our structured RLCR CoT, models first output solution, followed by an analysis and confidence score. To improve confidence scores for fixed answer y, we sample analysis CoTs z1, . . . , zK p( x, y), each producing verbalized confidence qi. We then ensemble these confidences to obtain the aggregated confidence estimate = 1 qi. Figure 4b plots Brier score (averaged over 7 datasets) as function of ensemble size K. We observe decrease in Brier score with larger ensembles, indicating that ensembling over uncertainty CoTs effectively improves calibration. For given solution and answer, ensembling verbalized confidences is lightweight and only requires additional sampling, unlike classifiers or probes, which would require training multiple heads/models. (cid:80) 4.5 DOES REASONING IMPROVE CALIBRATION? Recent work has shown that CoT reasoning can be unfaithful, with generated CoTs that do not influence their final answers (Chen et al., 2025). This raises the possibility that uncertainty analysis may not meaningfully inform the verbalized confidence score. To test this, we train two classifiers on HotPotQA-Modified: 9 Figure 6: (a): Distribution of standard deviation in confidence across multiple uncertainty reasoning chains for the same solution/answer. Most samples exhibit low deviation, indicating that the models confidence estimates are self-consistent. (b) Swarm plot of confidence sums across 3 datasets. RLCR consistently remains closer to the ideal sum of 1. Nonetheless, overconfidence remains, suggesting room for further improvement. 1. Baseline classifier: Trained on RLVR outputs (these contain no uncertainty analysis). 2. Analysis classifier: Trained on outputs of RLCR with confidence scores (present within <confidence> tags) removed to prevent direct hacking. As both RL models have comparable task accuracy, differences in classifier performance would indicate that the RLCR-trained models reasoning chains contain information specifically useful for calibration. We train classifiers for 3 different model sizes of the Qwen-base model: 0.5B, 1.5B and 7B. Figure 5 shows Brier and ECE scores on HotPotQA-Modified. Interestingly, while 7B classifiers perform similarly, the analysis classifier outperforms the baseline at smaller sizes, suggesting classifier capacity is key. For sufficiently expressive classifier (as with the 7B model), it is possible to infer confidence-relevant features directly from the solution. In contrast, smaller classifiers can make better use of RLCR reasoning chains. We believe that broader questions about the relationship between classifier capacity and CoT contents are an important topic for future work. 4.6 ARE VERBALIZED CONFIDENCES SELF-CONSISTENT? Intra-solution coherence desirable property of uncertainty-aware reasoning is that model should assign consistent confidence estimates when generating multiple uncertainty reasoning chains for the same answer. Given fixed answer y, we sample analysis CoTs z1, . . . , zK p( x, y), where each chain produces verbalized confidence score qi. Ideally, these confidence scores should exhibit low variability, indicating stable uncertainty estimates for given answer. Figure 6b plots the standard deviation across seven datasets, using analysis CoTs generated by the RLCR model trained on HotpotQA. Most samples have low standard deviation, suggesting that the models verbalized confidence estimates are generally consistent. Inter-solution consistency For tasks where answers are mutually exclusivei.e., only one answer can be correct per instanceit is desirable that the model distributes its confidence across distinct answers such that the total confidence sums to 1. Let model generate responses {yi}N i=1, where qi [0, 1]. Let = {a1, . . . , aK} denote the set of unique answers among the yi. The mean confidence assigned to answer ak is: i=1 with associated confidence scores {qi}N qk = (cid:80) (cid:80) 1yiak qi 1yiak (17) Assuming the answers in are mutually exclusive and exhaustive (i.e., exactly one is correct), the models confidences should ideally sum to one: (cid:80)K k=1 qk = 1. This captures the requirement for internal consistency in the models beliefs. Fig. 6b shows swarm plot of predicted confidence sums across three representative datasets. On the in-distribution HotpotQA dataset, RLCRs confidence sums cluster tightly around 1, indicating well-calibrated belief distribution. For out-of-distribution datasets, both RLCR and RLVR exhibit overconfidence, with sums exceeding 1, though RLCR remains significantly closer to the ideal. This reflects RLCR improved calibration, yet highlights room for improvement in inter-solution consistency, particularly out-of-distribution."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Building reliable reasoning models requires not only high accuracy but also calibrated uncertaintya property that standard RL objectives often erode (Achiam et al., 2023), leaving models over-confident (Xiong et al., 2024) and prone to hallucination (Jaech et al., 2024). To situate our proposed RLCR approach within this landscape, we survey four strands of prior work on confidence estimation in LLMs: (i) post-hoc verbalizations, (ii) sampling-based surrogates, (iii) internal signal probing, and (iv) RL-based calibration. Post-hoc verbalizations prompt models to state their confidence after answering (Xiong et al., 2024; Yang et al., 2024; Tanneru et al., 2024; Lin et al., 2022). Lin et al. (2022) fine-tune GPT-3 to predict confidence given question and answer, using GPT-3s empirical accuracy on that question as the target label. Xiong et al. (2024) find that LMs exhibit overconfidence when verbalizing their confidence, although calibration improves with increasing model capability. Tian et al. (2023) finds that RLHF models verbalized confidence scores are better calibrated than their conditional probabilities across several benchmarks. Recent work has examined the calibration of verbalized confidence in reasoning models. Mei et al. (2025) find that even reasoning models also suffer from overconfidence and can become even more overconfident with deeper reasoning. Similarly, Kirichenko et al. (2025) introduce AbstentionBench and show that LMs struggle to abstain appropriately, with reasoning fine-tuning often degrading abstention performance. Positively, Yoon et al. (2025) and Mei et al. (2025) find that reasoning models can improve their calibration by introspection and slow thinking CoT behaviors. Sampling-based methods use response agreement (e.g., majority vote or best-of-N ) as proxy for confidence, but are costly and require clear ground truth (Kang et al., 2025). Aichberger et al. (2025) estimate uncertainty by generating semantically diverse, plausible responses and measuring their consistency. Kuhn et al. (2023) propose semantic entropy, sampling-based method that accounts for linguistic invariances to better estimate uncertainty in natural language generation. Internal probing extracts confidence from model features like token probabilities (Gupta et al., 2024), offering fine-grained scores but lacking generality. Kadavath et al. (2022) prompt language models to output true or false for given answer and use the probability of predicting true (true), as proxy for the models confidence. Mielke et al. (2022) train LM to to generate responses conditioned on confidence estimates provided by an external probe. Fadeeva et al. (2024) propose Claim Conditioned Probability, token-level uncertainty method that leverages internal model signals to fact-check claims and detect hallucinations. Azaria & Mitchell (2023) train classifier on hidden layer activations to detect the truthfulness of statements based on the LLMs internal state. Orgad et al. (2025) show that internal representations encode rich, token-level truthfulness signals, which can detect and categorize errors beyond what is reflected in the output. RL-based methods train models to output calibrated verbal confidences with RL. Stengel-Eskin et al. (2024) introduce speaker-listener framework, where the speakers rewards are determined by the listeners inferred confidence in the speakers responses. Leng et al. (2025) introduce explicit confidence scores in reward model training, obtaining reward models that are better aligned to verbalized confidences. Most closely related to our work, Xu et al. (2024) and Stangel et al. (2025) train LMs with RL using proper scoring rules as reward functions. Xu et al. (2024) adopt the Brier score, while Stangel et al. (2025) use clipped version of the log loss. While effective, these approaches optimize solely for calibration, which can inadvertently harm task accuracyparticularly in larger models that may exploit the objective by outputting deliberately incorrect answers with zero confidence to achieve perfect calibration scores. Furthermore, both methods are developed and evaluated exclusively on non-reasoning tasks. In contrast, we propose calibrated reward function that provably incentivizes both correctness and calibration, and we demonstrate its effectiveness on both reasoning and non-reasoning benchmarks."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We show that incorporating proper scoring rules into RL, via an objective we call RLCR, enables reasoning models to improve both accuracy and calibration. Our approach trains models to reason about and verbalize uncertainty, preserving task performance while significantly improving calibration inand out-of-distribution. We demonstrate that reasoning about uncertainty improves calibration, and that our method improves the self-consistency of confidence, and improves with test-time scaling. However, there remains significant room for improvementeven after RLCR, out-of-domain calibration error is often high in an absolute sense, and models may still assign high confidence to multiple contradictory answers. Nevertheless, these results suggest path toward reasoning systems that are not only accurate, but reliably reason about and communicate uncertainty."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt4. arXiv preprint arXiv:2303.08774, 2023. Lukas Aichberger, Kajetan Schweighofer, Mykyta Ielanskyi, and Sepp Hochreiter. uncertainty estimation through semantically diverse language generation. International Conference on Learning Representations, 2025. Improving In The Thirteenth Alon Albalak, Duy Phung, Nathan Lile, Rafael Rafailov, Kanishk Gandhi, Louis Castricato, Anikait Singh, Chase Blagden, Violet Xiang, Dakota Mahan, et al. Big-math: large-scale, high-quality math dataset for reinforcement learning in language models. arXiv preprint arXiv:2502.17387, 2025. Amos Azaria and Tom Mitchell. The internal state of an llm knows when its lying. Empirical Methods in Natural Language Processing Findings, 2023. URL https://arxiv.org/abs/2304.13734. Glenn Brier. Verification of forecasts expressed in terms of probability. Monthly weather review, 78(1):13, 1950. Yanda Chen, Joe Benton, Ansh Radhakrishnan, Jonathan Uesato, Carson Denison, John Schulman, Arushi Somani, Peter Hase, Misha Wagner, Fabien Roger, Vlad Mikulik, Samuel R. Bowman, Jan Leike, Jared Kaplan, and Ethan Perez. Reasoning models dont always say what they think, 2025. URL https://arxiv.org/abs/2505.05410. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. URL https://arxiv.org/abs/2110.14168. Ekaterina Fadeeva, Aleksandr Rubashevskii, Artem Shelmanov, Sergey Petrakov, Haonan Li, Hamdy Mubarak, Evgenii Tsymbalov, Gleb Kuzmin, Alexander Panchenko, Timothy Baldwin, Preslav Nakov, and Maxim Panov. Fact-checking the output of large language models via token-level uncertainty quantification. CoRR, abs/2403.04696, 2024. URL https://doi.org/10.48550/ arXiv.2403.04696. Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307, 2025. Tilmann Gneiting and Adrian Raftery. Strictly proper scoring rules, prediction, and estimation. Journal of the American statistical Association, 102(477):359378, 2007. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 12 Neha Gupta, Harikrishna Narasimhan, Wittawat Jitkrittum, Ankit Singh Rawat, Aditya Krishna Menon, and Sanjiv Kumar. Language model cascades: Token-level uncertainty and beyond. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=KgaBScZ4VI. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum?id=7Bywt2mQsCe. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: large scale distantly supervised challenge dataset for reading comprehension. In Regina Barzilay and Min-Yen Kan (eds.), Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16011611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147/. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. Language models (mostly) know what they know. CoRR, abs/2207.05221, 2022. URL https://doi.org/10.48550/arXiv. 2207.05221. Zhewei Kang, Xuandong Zhao, and Dawn Song. Scalable best-of-n selection for large language models via self-certainty. In 2nd AI for Math Workshop @ ICML 2025, 2025. URL https: //openreview.net/forum?id=nddwJseiiy. Polina Kirichenko, Mark Ibrahim, Kamalika Chaudhuri, and Samuel J. Bell. Abstentionbench: Reasoning LLMs fail on unanswerable questions. In ICML 2025 Workshop on Reliable and Responsible Foundation Models, 2025. URL https://openreview.net/forum?id=kYbojsAOBj. Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=VD-AYtP0dve. Jixuan Leng, Chengsong Huang, Banghua Zhu, and Jiaxin Huang. Taming overconfidence in LLMs: Reward calibration in RLHF. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=l0tg0jzsdL. Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in words. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL https: //openreview.net/forum?id=8s8K2UZGTZ. Zhiting Mei, Christina Zhang, Tenny Yin, Justin Lidard, Ola Shorinwa, and Anirudha Majumdar. Reasoning about uncertainty: Do reasoning models know when they dont know? arXiv preprint arXiv:2506.18183, 2025. Sabrina J. Mielke, Arthur Szlam, Emily Dinan, and Y-Lan Boureau. Reducing conversational agents overconfidence through linguistic calibration. Transactions of the Association for Computational Linguistics, 10:857872, 2022. doi: 10.1162/tacl 00494. URL https://aclanthology.org/ 2022.tacl-1.50/. Omar, BS Glicksberg, GN Nadkarni, and Klang. Overconfident ai? benchmarking llm selfassessment in clinical scenarios. medRxiv, 2024. OpenAI. Openai o3 and o4-mini system card. 2025. Hadas Orgad, Michael Toker, Zorik Gekhman, Roi Reichart, Idan Szpektor, Hadas Kotek, and Yonatan Belinkov. LLMs know more than they show: On the intrinsic representation of LLM hallucinations. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=KRnsX5Em3W. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. Paul Stangel, David Bani-Harouni, Chantal Pellegrini, Ege Ozsoy, Kamilia Zaripova, Matthias Keicher, and Nassir Navab. Rewarding doubt: reinforcement learning approach to confidence calibration of large language models. CoRR, abs/2503.02623, March 2025. URL https://doi. org/10.48550/arXiv.2503.02623. Elias Stengel-Eskin, Peter Hase, and Mohit Bansal. LACIE: Listener-aware finetuning for calibration in large language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=RnvgYd9RAh. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: question answering challenge targeting commonsense knowledge. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41494158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1421. URL https://aclanthology.org/N19-1421/. Sree Harsha Tanneru, Chirag Agarwal, and Himabindu Lakkaraju. Quantifying uncertainty in natural language explanations of large language models. In International Conference on Artificial Intelligence and Statistics, pp. 10721080. PMLR, 2024. Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher Manning. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https://openreview.net/ forum?id=g3faCfrwm7. Benjamin Turtel, Danny Franklin, Kris Skotheim, Luke Hewitt, and Philipp Schoenegger. Outcomebased reinforcement learning to predict the future. arXiv preprint arXiv:2505.17989, 2025. Kartik Waghmare and Johanna Ziegel. Proper scoring rules for estimation and forecast evaluation. arXiv preprint arXiv:2504.01781, 2025. Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. Measuring short-form factuality in large language models. arXiv preprint arXiv:2411.04368, 2024. Changyi Xiao, Mengdi Zhang, and Yixin Cao. Bnpo: Beta normalization policy optimization. arXiv preprint arXiv:2506.02864, 2025. Miao Xiong, Zhiyuan Hu, Xinyang Lu, YIFEI LI, Jie Fu, Junxian He, and Bryan Hooi. Can LLMs express their uncertainty? an empirical evaluation of confidence elicitation in LLMs. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=gjeQKFxFpZ. Tianyang Xu, Shujin Wu, Shizhe Diao, Xiaoze Liu, Xingyao Wang, Yangyi Chen, and Jing Gao. Sayself: Teaching llms to express confidence with self-reflective rationales. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 59855998, 2024. Daniel Yang, Yao-Hung Hubert Tsai, and Makoto Yamada. On verbalized confidence scores for llms, 2024. URL https://arxiv.org/abs/2412.14737. 14 Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 23692380, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259/. Zijun Yao, Yantao Liu, Yanxu Chen, Jianhui Chen, Junfeng Fang, Lei Hou, Juanzi Li, and Tat-Seng Chua. Are reasoning models more prone to hallucination? arXiv preprint arXiv:2505.23646, 2025. Dongkeun Yoon, Seungone Kim, Sohee Yang, Sunkyoung Kim, Soyeon Kim, Yongil Kim, Eunbi Choi, Yireun Kim, and Minjoon Seo. Reasoning models better express their confidence. arXiv preprint arXiv:2505.14489, 2025. 15 PROOF OF THEOREM 1 We consider slightly more general family of reward functions than in the main paper body. As above, we assume that predictors produce response y, confidence [0, 1]; we now assume that scores depend additionally on an arbitrary binary correctness signal c, distributed according to some py := p(c = 1 y). We consider all reward functions of the form: R(c, q) = λc S(q, c) where S(q, c) is scoring rule and λ > 0 is scalar reward for producing the correct answer. We define the expected reward of choosing confidence for given response as: (y, q) = Ec R(c, q) = λpy [pyS(q, 1) + (1 py)S(q, 0)] . Lemma 1 (Calibration incentive). For any response y, the expected reward (y, q) is maximized at = py if and only if S(q, c) is proper scoring rule. Proof. The correctness term λpy in the reward function does not depend on q, so arg max V (y, q) = arg max pyS(q, 1) + (1 py)S(q, 0) = arg min Ec S(q, c) . But by scoring rule is proper by definition if EcBernoulli(py) S(q, c) is minimized by = py, so the statement of the lemma follows immediately. Lemma 2 (Correctness incentive). Consider y, with associated success probabilities py py. Then (y, py) (y, py) if and only if S(p, 1) S(p, 0) λ for all [0, 1] . Proof. First, define: (p) = λp pS(p, 1) + (1 p)S(p, 0) , Note that (y, py) = (py), and (p) represents the maximum reward attainable for any with associated success probability yp. Thus, to verify the statement of the lemma, it suffices to show that (p) (p) if and only if p, which in turn is equivalent to showing that (p) is nondecreasing in p. We first compute the derivative of : (p) = λ S(p, 1) + S(p, 0) pS(p, 1) (1 p)S(p, 0) . From the Savage-Dawid representation (Gneiting & Raftery, 2007) of proper scoring rules, there exists some non-negative weight function ω(t) 0 such that: S(p, 1) = (1 p) ω(p) , S(p, 0) = ω(p) . Substituting this in: (p) = λ S(p, 1) + S(p, 0) + (1 p) ω(p) (1 p) ω(p) = S(p, 1) S(p, 0) . Thus the derivative of is non-negative (W is nondecreasing) if and only if S(p, 1) S(p, 0) λ for [0, 1]. Then the main theorem statement in Section 3 follows immediately from these two lemmas: Proof of Theorem 1. First observe that RRLCR satisfies the conditions of Lemma 2 with λ = 1 and S(q, 1yy ) = (q 1yy )2, we have maxp S(p, 1) S(p, 0) = 1. Then condition 1 (calibration) follows from Lemma 1 and the fact that the Brier score is proper scoring rule, and condition 2 (correctness) follows from Lemma 2 and the boundedness of S(p, 1) S(p, 0). Corollary 1. Let S(q, c) be strictly proper scoring rule. 16 1. If S(p, 1) S(p, 0) is bounded, then there exists finite λ > 0 such that the reward function R(c, q) = λc S(q, c) satisfies the correctness condition: S(p, 1) S(p, 0) λ for all [0, 1] and thus jointly incentivizes calibration and correctness. 2. If S(p, 1) S(p, 0) is unbounded, then for any finite λ > 0, there may exist some such that (py) < (py), and RRLCR prefers (y, py) to (y, py). Examples. The Brier score is bounded: S(p, 1) = (1 p)2, S(p, 0) = p2, so: S(p, 1) S(p, 0) = 1 2p 1 for all [0, 1] Thus, the condition holds for λ = 1. In contrast, the logarithmic score is unbounded: S(p, 1) = log p, S(p, 0) = log(1p), S(p, 1)S(p, 0) = log (cid:19) (cid:18) 1 as 0 So no finite λ can satisfy the condition."
        },
        {
            "title": "B EXPERIMENTAL SETUP",
            "content": "B.1 TRAINING DATASETS HotpotQA-Modified: We use modified version of the HotPotQA distractor dataset, which contains factual questions requiring multi-hop reasoning. (Yang et al., 2018). Each example in this setting presents ten paragraphs, only two of which contain the information necessary to answer the question; the remaining eight paragraphs include closely related but irrelevant details. Consequently, solving this task requires the model to identify and reason over the pertinent passages. To more strongly develop uncertainty reasoning capability, we construct new dataset, HotPotQA-Modified, in which we systematically remove either 0, 1, or both of the key paragraphs required to answer each question. This modification introduces varying levels of informational completeness that the model must reason over. We distribute questions across three equal groups: one-third have no relevant paragraphs (0/8), one-third have 1 relevant paragraph (1/7), and one-third have both relevant paragraphs (2/6). Each question consistently contains 8 total paragraphs. Our training dataset consists of 20,000 examples. We measure correctness using exact string match. Big-Math Digits: We use Big-Math (Albalak et al., 2025), large, curated training dataset for RL containing over 250,000 math problems, including questions from benchmarks such as Math and GSM8K. To ensure an appropriate range of difficulty, we retain problems for which the LLaMA-8B solve rate (provided in the dataset) is between 0-70%. We also found that verifier noise can be significant in Math datasets and can cause training instability. To reduce verifier noise, we further restrict the dataset to problems with numerical answers, enabling near-perfect automatic verification. Our final training set consists of 15,000 problems. We compute correctness using math-verify. B.2 ADDITIONAL TRAINING DETAILS Following Turtel et al. (2025), we remove the standard deviation division in the advantage, which might help with learning on examples where there are extreme miscalibrations. We use the BNPO loss function, which aggregates token level losses using the number of active tokens in the local training batch (Xiao et al., 2025). We generate 32 responses per prompt with temperature of 0.7, and use an effective batch size of 2048. For training RLCR, we use the Long RLCR system prompt for Hotpot and the Simple RLCR prompt for Math (the long version did not provide additional benefit on Math). We use the Simple Generation prompt for RLVR. All prompts in Appendix C. Format Reward: We use format reward to encourage adherence to the structured format shown in Fig. 1. In RLVR, models must format their output in <think> and <answer> tags. In RLCR, in addition to <think> and <answer> tags, we require an <analysis> tag to enclose uncertainty reasoning and <confidence> tag for verbalized confidence. valid response must contain all these tags in the correct order. Both format and calibration rewards are weighted equally. SFT Warmup: In Math, we train variant with lightweight SFT warmup phase before RL to obtain higher quality uncertainty analyses. We generate solutions from the base model on 500 examples and prompt Deepseek-R1 with the Expert SFT Prompt to produce uncertainty analyses for them. We then perform SFT with the <think> and <answer> obtained from the base model, appended with the <analysis> obtained from Deepseek-R1. Note that we do not ask Deepseek-R1 to output confidence scores. B.3 EVALUATION DATASETS We run evaluation on large number of datasets: 1. HotPotQA (Distractor): We use 1000 validation examples from the original HotpotQA distractor dataset. We slightly modify the dataset and remove 2 non-relevant paragraphs from each question. Thus, each question has 8 paragraphs with both supporting paragraphs present. We measure correctness using exact-match (Yang et al., 2018). 2. HotPotQA-Modified: We evaluate on 500 held-out validation examples. We measure correctness using exact-match. 18 3. TriviaQA: We use 2000 examples from the validation set of the TriviaQA dataset (Joshi et al., 2017). We use the no-context split to purely test factual accuracy.We evaluate using LLM-as-a-judge. 4. SimpleQA: We use the full SimpleQA dataset consisting of 4326 factual questions (Wei et al., 2024). We evaluate using LLM-as-a-judge. 5. Math-500 We use the popular MATH-500 dataset, which contains subset of problems from the original MATH dataset (Hendrycks et al., 2021). We evaluate using math-verify, mathematical expression evaluation system released by huggingface. 6. GSM8K: We use the test set (1319 problems) of the popular Grade School Math 8K dataset (Cobbe et al., 2021). We evaluate using math-verify. 7. Big-Math-Digits: We evaluate on 1000 held-out validation examples. We evaluate using math-verify. 8. CommonSenseQA: We use the validation set (1220 problems) of the CommonsenseQA dataset (Talmor et al., 2019), multiple-choice question answering dataset that requires different types of commonsense knowledge to predict the correct answers. We evaluate using LLM-as-a-judge. 9. GPQA: We use the GPQA main dataset containing 448 multiple-choice questions written by experts in biology, physics, and chemistry (Rein et al., 2024). We evaluate using LLMas-a-judge. B.4 EVALUATION DETAILS All models are evaluated with temperature 0. For all datasets except Math and GSM8K, we use maximum token budget of 4096. The system prompt for evaluation and the pipeline to extract answer and confidence scores varies slightly based on the method we are evaluating: 1. RLCR (ours): RLCR models use <think>, <answer>, <analysis> and <confidence> tags. They are evaluated with the same system prompts they are trained on. We extract their answer from <answer> tag and their confidence from <confidence> tag. 2. RLVR: RLVR models use the <think> and <answer>. It is evaluated with the same system prompt and we extract their answer from the <answer> tag. To obtain their verbalized confidence, we append Thinking time ended. My verbalized confidence in my answer as number between 0 and 100 is equal to to their generated output. 3. Classifier/Probe: Both methods are conditioned on the question and the RLVR models generation (solution and answer). These methods thus use RLVR model as generator and their reported accuracies in the result tables are equal. 4. Base: The base model is not good at instruction following and is prompted with simpler system prompt (Simple Confidence Prompt) that guides it to use <think>, <answer> and <confidence> tags. In case no valid confidence can be extracted, we append Thinking time ended. My verbalized confidence in my answer as number between 0 and 100 is equal to to their output and call them again to extract confidence. For all methods, if we are unable to extract valid answer from the <answer> tags, we append Thinking time ended. My final answer is to their output and call them again. The main goal of these custom pipelines is to be able to fairly extract an answer and confidence level and minimize cases where incorrect formatting adversely affects performance. Note that because they are trained with format rewards, both the RL-trained models are nearly perfect in adhering to the desired format and require minimal interventions. However, the base model benefits from this full extraction pipeline. Importantly, once answers and confidences are extraced, all methods are evaluated identically and based on the dataset, exact-match, LLM-as-a-judge or math-verify is used. LLM-as-a-judge: We use Llama-3.1-8B-Instruct with temperature set to 0 as our judge. The judge is provided with the question, the ground truth answer and the answer extracted from the evaluation pipeline. It is prompted to respond with YES or NO based on the correctness of the answer. As the datasets we evaluate have short and objective answers, we do not condition the judge on the thinking traces which can add biases."
        },
        {
            "title": "Long RLCR Prompt",
            "content": "A conversation between User and Assistant. The user asks question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind, provides the user with the final answer, then analyzes its confidence about the solution and then provides the user with its confidence level. The confidence level is number between 0 and 1 (inclusive) enclosed within <confidence> </confidence> tags. The final answer is enclosed between <answer> </answer> tags. The analysis about confidence and uncertainty is enclosed within <analysis> </analysis> tags. The assistant should reason about its confidence in the solution and its uncertainty in the solution within these tags. Here are some guidelines for the analysis: 1. Your task is to point out things where the model could be wrong in its thinking, or things where there might be ambiguity in the solution steps, or in the reasoning process itself. 2. You should not suggest ways of fixing the response, your job is only to reason about uncertainties. 3. For some questions, the response might be correct. In these cases, It is also okay to have only small number of uncertainties and then explicitly say that am unable to spot more uncertainties. 4. Uncertainties might be different from errors. For example, uncertainties may arise from ambiguities in the question, or from the application of particular lemma/proof. 5. If there are alternate potential approaches that may lead to different answers, you should mention them. 6. List out plausible uncertainties, do not make generic statements, be as specific about uncertainties as possible. 7. Enclose this uncertainty analysis within <analysis> </analysis> tags. <think> reasoning process here The final </think> <answer> final answer here </analysis> <analysis> analysis about confidence and uncertainty here </analysis> <confidence> confidence level here (number between 0 and 1) </confidence> ) that must be followed is : format Simple RLCR Prompt conversation between User and Assistant. The user asks question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind, provides the user with the final answer, then analyzes its confidence about the solution and provides the user with its confidence level. The confidence level is number between 0 and 1 (inclusive) enclosed within <confidence> </confidence> tags. The final answer is enclosed between <answer> </answer> tags. The analysis about confidence and uncertainty is enclosed within <analysis> </analysis> tags. The Assistant should reason about its confidence in the solution and its uncertainty in the solution within these tags. The final format that must be followed is: <think> reasoning process here </think><answer> final answer here </answer><analysis> analysis about confidence and uncertainty here </analysis><confidence> confidence level here (number between 0 and 1) </confidence>"
        },
        {
            "title": "Simple Confidence Prompt",
            "content": "A conversation between User and Assistant. The user asks question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and analyzes its confidence about the solution and then provides the user with the final answer as well as its confidence level. The confidence level is number between 0 and 1 (inclusive) enclosed within <confidence> </confidence> tags. The final answer is enclosed between <answer> </answer> tags. The final format that must be followed is : <think> reasoning process here <//think><answer> final answer here </answer> <confidence> confidence level here (number between 0 and 1) </confidence>."
        },
        {
            "title": "RLVR Prompt",
            "content": "A conversation between User and Assistant. The user asks question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>. Expert SFT Prompt You are given question and solution to it. You have to verify if the solution is correct and enclose your verification reasoning within <analysis> </analysis> tags. Your analysis should be minimum of 300 characters and should sequentially go through the thinking solution step by step. Here are the guidelines for your analysis: 1. Your analysis should also be in form as if you wrote the solution and are now verifying it. 2. Your goal is not to solve the problem but instead to verify if the steps in the presented solution are correct. 3. If there are ambiguities in the solution steps or if step introduces uncertainty, you should mention it in the analysis. 4. Go through the solution sequentially in step-by-step manner. 5. The analysis should be 300 characters minimum. 6. Enclose this uncertainty analysis within <analysis> </analysis> tags."
        },
        {
            "title": "D RESULTS",
            "content": "D.1 MODELS TRAINED ON HOTPOTQA Method SimpleQA Trivia Acc. AUROC Brier ECE Acc. AUROC Brier ECE Base RLVR RLVR +Classifier RLVR +Brier-Classifier RLVR +Probe Answer Prob RLCR (ours) 13.5% 0.502 0.501 12.4% 0.477 12.4% 0.598 12.4% 0.506 12.4% 0.418 12.4% 0.598 12.1% 0.773 0.875 0.531 0.112 0.141 0.832 0.241 0.809 0.875 0.638 0.058 0.124 0.847 0. 0.510 57.8% 62.2% 0.502 62.2% 0.567 62.2% 0.568 62.2% 0.472 62.2% 0.504 0.731 60.8% 0.381 0.377 0.256 0.370 0.430 0.366 0.202 0.371 0.377 0.151 0.371 0.410 0.361 0.058 Table 2: Performance on SimpleQA and Trivia datasets. Best values bolded. Method CommonsenseQA GPQA Acc. AUROC Brier ECE Acc. AUROC Brier ECE Base RLVR RLVR +Classifier RLVR +Brier-Classifier RLVR +Probe Answer Prob RLCR (ours) 0.623 88.9% 90.8% 0.500 90.8% 0.635 90.8% 0.645 90.8% 0.499 90.8% 0.603 91.3% 0.728 0.099 0.092 0.119 0.261 0.748 0.083 0.167 0.004 0.092 0.183 0.418 0.812 0.031 0.302 0.492 39.5% 0.500 39.5% 0.523 39.5% 0.523 39.5% 0.503 39.5% 39.5% 0.526 41.5% 0.550 0.504 0.605 0.279 0.288 0.329 0.537 0.267 0.509 0.605 0.161 0.212 0.292 0.544 0. Table 3: Performance on CommonsenseQA and GPQA. Best values bolded. Method MATH-500 GSM8K Acc. AUROC Brier ECE Acc. AUROC Brier ECE Base RLVR RLVR +Classifier RLVR +Brier-Classifier RLVR +Probe Answer Prob RLCR (ours) 46.8% 0.591 0.502 38.6% 0.704 38.6% 0.580 38.6% 0.671 38.6% 0.786 38.6% 45.4% 0.716 0.485 0.610 0.260 0.319 0.241 0.510 0.251 0.492 0.611 0.224 0.291 0.152 0.547 0. 0.524 73.5% 0.504 80.1% 0.572 80.1% 0.659 80.1% 0.532 80.1% 0.775 80.1% 86.3% 0.740 0.240 0.198 0.162 0.548 0.413 0.163 0.142 0.215 0.198 0.067 0.629 0.479 0.169 0.197 Table 4: Performance on Math-500 and GSM8K. Best values bolded. 22 Method HotpotQA HotpotQA-Modified Acc. AUROC Brier ECE Acc. AUROC Brier ECE Base RLVR RLVR +Classifier RLVR +Brier-Classifier RLVR +Probe Answer Prob RLCR (ours) 0.540 39.7% 63.0% 0.500 63.0% 0.664 63.0% 0.653 63.0% 0.553 63.0% 0.715 62.1% 0.687 0.525 0.370 0.217 0.223 0.243 0.357 0.210 0.535 0.370 0.070 0.091 0.096 0.360 0.030 0.591 30.4% 46.0% 0.500 46.0% 0.772 46.0% 0.791 46.0% 0.571 46.0% 0.609 0.798 44.4% 0.566 0.540 0.209 0.201 0.263 0.525 0. 0.588 0.540 0.126 0.117 0.122 0.529 0.078 Table 5: Performance on HotpotQA and HotpotQA-Modified. Best values bolded. D.2 MODELS TRAINED ON MATH Method SimpleQA Trivia Acc. AUROC Brier ECE Acc. AUROC Brier ECE Base RLVR RLVR +Classifier RLVR +Brier-Classifier RLVR +Probe Answer Prob RLCR (ours) SFT+ RLCR (ours) 0.502 13.5% 15.2% 0.528 15.2% 0.446 15.2% 0.494 15.2% 0.440 15.2% 0.450 0.518 12.1% 0.595 11.4% 0.773 0.828 0.567 0.154 0.582 0.799 0.426 0.290 0.809 0.840 0.641 0.108 0.657 0.810 0.536 0.397 0.510 57.8% 0.503 58.3% 0.574 58.3% 0.608 58.3% 0.560 58.3% 58.3% 0.480 61.0% 0.673 0.719 55.5% 0.381 0.430 0.291 0.301 0.296 0.398 0.218 0.212 0.371 0.427 0.223 0.253 0.229 0.384 0.098 0.058 Table 6: Performance on SimpleQA and Trivia datasets. Best values bolded. Method CommonsenseQA GPQA Acc. AUROC Brier ECE Acc. AUROC Brier ECE Base RLVR RLVR +Classifier RLVR +Brier-Classifier RLVR +Probe Answer Prob RLCR (ours) SFT+RLCR (ours) 0.620 88.6% 89.3% 0.547 89.3% 0.614 89.3% 0.599 89.3% 0.571 89.3% 0.556 90.1% 0.619 0.726 77.6% 0.099 0.131 0.226 0.302 0.187 0.104 0.213 0.217 0.004 0.126 0.343 0.447 0.280 0.094 0.340 0.251 39.5% 0.492 50.0% 0.496 50.0% 0.514 50.0% 0.533 50.0% 0.496 50.0% 0.532 0.568 43.3% 0.602 32.6% 0.504 0.531 0.327 0.334 0.302 0.441 0.260 0.226 0.509 0.531 0.268 0.279 0.192 0.398 0.102 0.080 Table 7: Performance on CommonsenseQA and GPQA datasets. Best values bolded. 23 Method MATHGSM8K Acc. AUROC Brier ECE Acc. AUROC Brier ECE Base RLVR RLVR +Classifier RLVR +Brier-Classifier RLVR +Probe Answer Prob RLCR (ours) SFT+RLCR (ours) 0.591 46.8% 59.2% 0.451 59.2% 0.771 59.2% 0.788 59.2% 0.670 59.2% 0.544 59.8% 0.665 0.806 55.8% 0.485 0.438 0.220 0.215 0.259 0.392 0.227 0.194 0.492 0.427 0.177 0.184 0.198 0.395 0.120 0.156 0.524 73.5% 90.6% 0.465 90.6% 0.768 90.6% 0.757 90.6% 0.560 90.6% 0.479 89.6% 0.632 90.4% 0.731 0.240 0.094 0.077 0.076 0.109 0.091 0.100 0.076 0.215 0.048 0.057 0.052 0.135 0.088 0.119 0. Table 8: Performance on Math-500 and GSM8K. Best values bolded."
        },
        {
            "title": "Method",
            "content": "Big-Math Digits"
        },
        {
            "title": "HotpotQA",
            "content": "Acc. AUROC Brier ECE Acc. AUROC Brier ECE Base RLVR RLVR +Classifier RLVR +Brier-Classifier RLVR +Probe Answer Prob RLCR (ours) SFT+RLCR (ours) 48.1% 68.8% 68.8% 68.8% 68.8% 68.8% 68.7% 70.5% 0.560 0.502 0.810 0.794 0.722 0.533 0.726 0.792 0.466 0.319 0.164 0.167 0.191 0.297 0.181 0.154 0.469 0.300 0.052 0.053 0.066 0.298 0.050 0.029 39.7% 0.540 49.7% 0.505 49.7% 0.584 49.7% 0.597 49.7% 0.580 49.7% 0.599 48.3% 0.615 0.650 41.6% 0.525 0.547 0.283 0.302 0.276 0.469 0.272 0.241 0.535 0.547 0.196 0.243 0.164 0.475 0.177 0. Table 9: Performance on Big-Math Digits and HotpotQA. Best values bolded."
        },
        {
            "title": "SIMPLEQA EXAMPLE",
            "content": "Question Who was awarded the Oceanography Societys Jerlov Award in 2018? (Ground Truth Answer: Annick Bricaud) RLCR (ours) <think> The question asks who was awarded the Oceanography Societys Jerlov Award in 2018. The Jerlov Award is given by the Oceanography Society, and it is named after the late Professor Gunnar Jerlov, pioneer in the field of ocean optics. To find the winner of the 2018 award, would need to look up the most recent information on the Oceanography Societys website or other reliable sources. </think> <answer> Dr. David W. Schiedek </answer> <analysis> The Oceanography Societys website or other reliable sources do not provide information on the winner of the 2018 Jerlov Award. It is possible that the information is not readily available or that the award was not given in 2018. Additionally, the name Dr. David W. Schiedek is not well-known figure in the field of oceanography, which raises uncertainty about the accuracy of this answer. </analysis> <confidence> 0.25 </confidence> RLVR <think> The question asks for the person who was awarded the Oceanography Societys Jerlov Award in 2018. The Jerlov Award is given by the Oceanography Society to recognize outstanding contributions to the field of oceanography. The recipient for 2018 was Dr. Laurence C. Smith. </think> <answer> Laurence C. Smith </answer> <confidence> 100. </confidence> Qwen-7B-Base <think> To find out who was awarded the Oceanography Societys Jerlov Award in 2018, will search for the most recent information on the award winners from the Oceanography Societys official website or reliable source that covers scientific awards. </think> <answer> The Oceanography Societys Jerlov Award in 2018 was awarded to Dr. Lisa Levin. </answer> <confidence> 0.95 </confidence> 25 HOTPOTQA-MODIFIED EXAMPLE All supporting information from the given question has been removed. The only way to answer correctly is if the model remembers the information in its weights. Question and Supporting Information Question: Jacques Sernas, actor in Fugitive in Trieste, was of what nationality? (Ground Truth Answer: Lithuanian-born French) Your answer will be verified with exact match score. To ensure correct verification, only provide the answer within the <answer> </answer> tags. Do not put any sentences or reasoning process within the <answer> </answer> tags. Supporting Information: Paragraph 0 Man From 1997 is time travel episode of the 195657 anthology television series Conflict directed by Roy del Ruth, produced by Roy Huggins, written by James Gunn from story by Alfred Bester, and starring Jacques Sernas, Charles Ruggles, Gloria Talbott and James Garner. The music was written by David Buttolph and the cinematographer was Ted D. McCord. The show was originally telecast on November 27, 1956 and kinescope of the broadcast currently exists. Paragraph 1 Altair is 1956 Italian romantic drama film directed by Leonardo De Mitri and starring Franco Interlenghi, Antonella Lualdi and Jacques Sernas. Paragraph 2 Dieci canzoni damore da salvare (English title: Ten Love Songs) is 1953 Italian film directed by Flavio Calzavara. The plot concerns songwriter, played by Jacques Sernas, who leaves his sweetheart and publisher when he learns that he is going blind. Supporting Sernas were Brunella Bovo, Franca Tamantini, and Enrico Viarisio. Paragraph 3 Loving You Is My Sin (Italian: Amarti `e il mio peccato) is 1953 Italian melodrama film directed by Sergio Grieco and starring Jacques Sernas, Luisa Rossi and Elisa Cegani. Paragraph 4 Its All Adams Fault or In Six Easy Lessons (French: Cest la faute dAdam) is 1958 French comedy film directed by Jacqueline Audry and starring Dany Robin, Jacques Sernas and Mijanou Bardot. Paragraph 5 Balearic Caper (Spanish: Zarabanda bing bing, Italian: Baleari Operazione Oro, French: Barbouze cherie, also known as Operation Gold) is 1966 Spanish-Italian-French heist-Eurospy comedy film written and directed by Jose Marıa Forque and starring Jacques Sernas, Daniela Bianchi and Mireille Darc. It was shot in Ibiza. Paragraph 6 Finishing School (Italian: Fanciulle di lusso) is 1953 French-Italian comedy film directed by Bernard Vorhaus and starring Susan Stephen, Anna Maria Ferrero and Jacques Sernas. It was made at Cinecitt`a with sets designed by the art director Franco Lolli. It is also known by the alternative title of Luxury Girls. Paragraph 7 Jump into Hell is 1955 war film directed by David Butler. The film stars Jacques Sernas (billed as Jack Sernas) and Kurt Kasznar. The first contemporary Hollywood war film of the war in Indochina, the story is fictionalized account of the Battle of Dien Bien Phu. 26 RLCR <think> The question asks for the nationality of Jacques Sernas, the actor in Fugitive in Trieste. The information provided mentions Jacques Sernas in several films, but does not specify his nationality. However, it does mention that he starred in Altair, 1956 Italian romantic drama film, and that he was an actor in several French and Italian films. This suggests that Jacques Sernas might be of French or Italian nationality, as he starred in films produced in those countries. However, there is no direct information about his nationality in the given text. </think> <answer> French </answer> <analysis> The information provided mentions Jacques Sernas in several French and Italian films, and he starred in Altair, 1956 Italian romantic drama film. However, there is no direct information about his nationality. While it is possible that he could be of French or Italian nationality, the lack of specific information makes it uncertain. Additionally, the fact that he starred in both French and Italian films could suggest that he might be of mixed nationality or have connections to both countries. Due to the ambiguity and lack of direct information, the confidence in this answer is relatively low. </analysis> <confidence> 0.4 </confidence> RLVR <think> The question asks for the nationality of Jacques Sernas, the actor in Fugitive in Trieste. Jacques Sernas was an American actor. </think> <answer> American </answer> <confidence> 100. </confidence> Qwen-7B-Base <think> To determine Jacques Sernas nationality, will analyze the information provided in the supporting information. From the information given, can see that Jacques Sernas has appeared in various films from different countries, such as Italy, France, and Spain. However, there is no specific mention of his nationality in the provided information. Since there is no direct information about Jacques Sernas nationality, will analyze the context of the question. The question is about Jacques Sernas nationality in the context of the film Fugitive in Trieste. Trieste is city in Italy, and the film is an Italian romantic drama. Based on this context, it is reasonable to assume that Jacques Sernas is of Italian nationality. However, without any direct information, cannot be 100% confident in this assumption. </think> <answer> Italian </answer> <confidence> 0.8 </confidence>"
        },
        {
            "title": "MATH EXAMPLE",
            "content": "An example where the analysis from the vanilla RLCR is generic, while the SFT+RLCR model produces much more specific evaluation that directly references the reasoning process. Although the RLCR model achieves reasonable calibration on Math, the SFT+RLCR model outperforms it, likely because it has learned to generate more effective uncertainty-aware chain-of-thought reasoning. Question Dalton, Hunter, Alex, Bella, and Chris formed Superhero Fan Club. They set goal to watch as many Superhero movies as they could in one summer. Dalton watched 15 movies, Hunter watched 19, Alex watched 25, Bella watched 21, and Chris watched 11. They all watched 5 superhero movies together, while Dalton, Hunter, and Alex watched an additional 3 movies only among themselves, and Bella and Chris watched 2 more movies only with each other. Every other movie they watched was different. How many different movies did they see? (Ground Truth Ans: 63) RLCR (ours) <think> To determine the total number of different movies watched by Dalton, Hunter, Alex, Bella, and Chris, we need to account for the movies they watched together and the additional movies they watched among themselves or with each other. First, lets calculate the total number of movies watched if we didnt account for any overlaps: - Dalton watched 15 movies. - Hunter watched 19 movies. - Alex watched 25 movies. - Bella watched 21 movies. - Chris watched 11 movies. The sum of these is: 15 + 19 + 25 + 21 + 11 = Next, we need to subtract the overlaps. They all watched 5 movies together, so we subtract 4 overlaps (since the 5 movies are counted 5 times in the initial sum): 91 4 5 = 91 20 = 71 Now, we need to account for the additional movies watched among specific groups: - Dalton, Hunter, and Alex watched 3 additional movies only among themselves. These movies are already subtracted in the previous step, so we dont need to subtract them again. - Bella and Chris watched 2 additional movies only with each other. These movies are also already subtracted in the previous step, so we dont need to subtract them again. Since all the overlaps have been accounted, the total number of different movies they saw is 71. <answer> 71 </answer> <analysis> The solution provided involves reasonable approach to accounting for the overlaps in the number of movies watched. However, there is uncertainty in the interpretation of the problem statement, particularly the part about Every other movie they watched was different. This phrase could be interpreted in two ways: 1. That the 5 movies watched together, the 3 additional movies watched by Dalton, Hunter, and Alex, and the 2 additional movies watched by Bella and Chris are all distinct from each other and from the other movies watched by the individuals. 2. That the total number of movies watched by each individual, minus the overlaps, results in set of distinct movies. Given the information provided, the first interpretation seems more plausible, but it introduces an element of uncertainty. The problem does not clearly state that all movies watched are distinct, only that Every other movie they watched was different, which could be interpreted as subset of the total movies watched. Therefore, the confidence level in the solution is set at 0.6, acknowledging the potential for misinterpretation and the lack of clear information in the problem statement.</analysis> <confidence> 0.6 </confidence> 28 SFT+RLCR (ours) <think> To determine the total number of different movies the Superhero Fan Club members watched, we need to account for the movies watched individually and in groups, ensuring we dont double-count any movies. First, lets sum the total number of movies watched by each member without considering overlaps: - Dalton watched 15 movies. - Hunter watched 19 movies. - Alex watched 25 movies. - Bella watched 21 movies. - Chris watched 11 movies. Adding these gives: 15 + 19 + 25 + 21 + 11 = 91 Next, we need to subtract the overlaps. They all watched 5 movies together, so we subtract 4 times 5 (since the 5 movies are counted 5 times in the initial sum, once for each person): 91 4 5 = 91 20 = Now, we need to account for the additional movies watched in smaller groups. Dalton, Hunter, and Alex watched 3 more movies only among themselves. These 3 movies are already included in the initial counts for Dalton, Hunter, and Alex, but we need to ensure they are not double-counted in the all-together group. Since the 3 movies are only among these three, they are not part of the 5 movies watched together, so we dont need to subtract them again. They are already included in the individual counts and the group of 5, so we add them back to the total after the initial subtraction: 71 + 3 = 74 Bella and Chris watched 2 more movies only with each other. These 2 movies are already included in the initial counts for Bella and Chris, but we need to ensure they are not double-counted. Since the 2 movies are only between Bella and Chris, they are not part of the 5 movies watched together or the 3 movies watched by Dalton, Hunter, and Alex, so we add them back to the total after the initial subtraction: 74 + 2 = 76 At this point, we have accounted for all the overlaps, and the total number of different movies they saw is 76. To verify, lets consider the structure of the problem: - The 5 movies watched together are counted 5 times initially but subtracted 4 times, so they are correct. - The 3 movies watched by Dalton, Hunter, and Alex are included in their individual counts and the group of 5, so they are correct. - The 2 movies watched by Bella and Chris are included in their individual counts, so they are correct. The final count of 76 different movies is consistent with the problem constraints. Therefore, the total number of different movies they saw is 76. </think> <answer> 76 </answer> <analysis> Upon reviewing the solution, notice that the reasoning process involves summing the total movies watched individually, subtracting the overlaps, and then adding back the group-specific movies. The initial sum of 91 is correct, and the subtraction of 20 for the 5 movies watched together is appropriate, resulting in 71. However, the addition of the 3 movies watched by Dalton, Hunter, and Alex (3) and the 2 movies watched by Bella and Chris (2) is problematic. These movies are already included in the individual counts and the group of 5, so adding them back is redundant and unnecessary. The reasoning does not clearly explain why these movies are added back, and the final total of 76 does not seem to be derived from clear, step-by-step logical flow. The explanation about the 3 movies being only among these three and not part of the 5 is confusing, as the 3 movies are part of the individual counts and the group of 5, so they should not be added back. The 2 movies watched by Bella and Chris are also problematic, as they are only between Bella and Chris, but the reasoning does not clearly address how they are added back without double-counting. The final answer of 76 is not supported by the reasoning, and the steps are convoluted, making the solution unclear and potentially incorrect.</analysis> <confidence> 0.3 </confidence> 29 RLVR <think> To determine the total number of different movies watched by Dalton, Hunter, Alex, Bella, and Chris, we need to carefully account for the movies they watched together and the movies they watched exclusively among themselves and with each other. 1. **Total movies watched by each individual:** - Dalton: 15 movies - Hunter: 19 movies - Alex: 25 movies - Bella: 21 movies - Chris: 11 movies 2. **Movies watched together:** They all watched 5 movies together. 3. **Movies watched exclusively among themselves:** - Dalton, Hunter, and Alex watched an additional 3 movies only among themselves. - Bella and Chris watched 2 more movies only with each other. 4. **Calculating the total number of movies watched:** - First, sum the total movies watched by each individual: 15 + 19 + 25 + 21 + 11 = - Subtract the movies watched together (since they are counted multiple times in the individual totals): - Add back the movies watched together (they should only be counted once): 86 + 5 = 91 91 5 = 86 - Subtract the movies watched exclusively among themselves and with each other (since they are also counted multiple times): - Dalton, Hunter, and Alex watching 3 movies only among themselves are already subtracted when we subtracted the 5 movies watched together. - Bella and Chris watching 2 movies only with each other are already subtracted when we subtracted the 5 movies watched together. - Therefore, we do not need to make any further adjustments for these exclusive movies as they are already accounted for in the initial sum and subtraction. 5. **Final calculation:** The total number of different movies they saw is the sum of all individual movies watched, minus the movies watched together (to avoid double-counting), plus the movies watched together again (to include them once), and no further adjustments are needed for the exclusive movies: Thus, the total number of different movies they saw is: 91 5 + 5 = 91 </think> <answer> 91 </answer> <confidence> 90 </confidence>"
        }
    ],
    "affiliations": [
        "Massachusetts Institute of Technology"
    ]
}