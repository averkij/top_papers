{
    "paper_title": "True Multimodal In-Context Learning Needs Attention to the Visual Context",
    "authors": [
        "Shuo Chen",
        "Jianzhe Liu",
        "Zhen Han",
        "Yan Xia",
        "Daniel Cremers",
        "Philip Torr",
        "Volker Tresp",
        "Jindong Gu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs), built on powerful language backbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new tasks from a few multimodal demonstrations consisting of images, questions, and answers. Despite showing noticeable improvement on standard vision-language datasets, current MLLMs struggle to leverage visual information in the demonstrations. Specifically, they tend to neglect visual cues and over-rely on textual patterns, leading to mere text imitation rather than genuine multimodal adaptation. This behavior makes MICL still unimodal and largely restricts its practical utility. More importantly, this limitation is often concealed by the improved performance on tasks that do not require understanding the visual context. As a result, how to effectively enhance MICL ability and reliably evaluate the MICL performance remains underexplored. To address these issues, we first introduce Dynamic Attention Reallocation (DARA), an efficient fine-tuning strategy that encourages models to attend to the visual context by rebalancing attention across visual and textual tokens. In addition, we present TrueMICL, an MICL-dedicated dataset with both support and test sets that explicitly requires the integration of multimodal information-particularly visual content-for correct task completion. Extensive experiments demonstrate the effectiveness of our holistic solution, showcasing substantial improvements in the true multimodal in-context learning capabilities. Code and datasets are available at https://chenxshuo.github.io/true-micl-colm ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 7 0 8 5 1 . 7 0 5 2 : r Published as conference paper at COLM 2025 True Multimodal In-Context Learning Needs Attention to the Visual Context Shuo Chen1,3,5,6, Jianzhe Liu2, Zhen Han1, Yan Xia4, Daniel Cremers2,5, Philip Torr7, Volker Tresp1,5, Jindong Gu7 1LMU Munich, 2Technical University of Munich, 3Siemens AG 4University of Science and Technology of China, 5Munich Center for Machine Learning (MCML), 6Konrad Zuse School of Excellence in Reliable AI (relAI), 7University of Oxford"
        },
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs), built on powerful language backbones, have enabled Multimodal In-Context Learning (MICL)adapting to new tasks from few multimodal demonstrations consisting of images, questions, and answers. Despite showing noticeable improvement on standard vision-language datasets, current MLLMs struggle to leverage visual information in the demonstrations. Specifically, they tend to neglect visual cues and over-rely on textual patterns, leading to mere text imitation rather than genuine multimodal adaptation. This behavior makes MICL still unimodal and largely restricts its practical utility. More importantly, this limitation is often concealed by the improved performance on tasks that do not require understanding the visual context. As result, how to effectively enhance MICL ability and reliably evaluate the MICL performance remains underexplored. To address these issues, we first introduce Dynamic Attention ReAllocation (DARA), an efficient fine-tuning strategy that encourages models to attend to the visual context by rebalancing attention across visual and textual tokens. In addition, we present TrueMICL, an MICL-dedicated dataset with both support and test sets that explicitly requires the integration of multimodal informationparticularly visual contentfor correct task completion. Extensive experiments demonstrate the effectiveness of our holistic solution, showcasing substantial improvements in the true multimodal in-context learning capabilities. Code and datasets are available at here."
        },
        {
            "title": "Introduction",
            "content": "Multimodal Large Language Models (MLLMs) have extended the emergence of in-context learning (Brown et al., 2020; Dong et al., 2022) from Large Language Models to multimodal domains and enabled Multimodal In-Context Learning (MICL) (Alayrac et al., 2022; Jiang et al., 2024). These pre-trained models can rapidly adapt to vision-language (VL) tasks, given few-shot multimodal demonstrations (demos) consisting of images, questions, and answers, without heavy parameter adaptations (Ferber et al., 2024), as shown in Figure 1. Compared with zero-shot evaluation, MICL has shown noticeable improvement on standard VL tasks such as image captioning (Alayrac et al., 2022; Awadalla et al., 2023; Laurencon et al., 2024b). However, various studies have identified key limitation of current MLLMs: they struggle to effectively utilize visual information in the demonstrations (Baldassini et al., 2024; Chen et al., 2023b; Jia et al., 2024; Zong et al., 2024). Concretely, they tend to overlook visual context in the multimodal demonstrations and over-rely on textual patterns in the context, Equal contribution. Correspondence: jindong.gu@outlook.com, chenshuo.cs@outlook.com 1 Published as conference paper at COLM 2025 Figure 1: Examples of using MICL to solve image captioning from MSCOCO (Chen et al., 2015) (top) and Clock Math from our proposed dataset, TrueMICL (bottom). Generating captions relies more on the ability of task recognition, and correct caption can be answered based on the text in the demos (e.g., mimicking the caption style in the demos), without deep understanding of the demo images. However, our task requires task learning where the model needs to learn the relationship between the text and images in the demos (i.e., multiplying the two numbers pointed by the clock arrows) to correctly respond to the query. which results in textual pattern imitation rather than true multimodal adaptation. This behavior renders multimodal ICL still unimodal and largely limits both its applications and practical utility. Moreover, this disadvantage is hard to discern as it can be concealed by the improved performance on tasks where reasonable responses can be generated from text pattern following, without deep understanding of visual context. For example, Chen et al. (2023b) found that omitting the demo images during image captioning still yields comparable performance, indicating that models can generate reasonable captions for the query image even without referencing the demo images. Hence, these datasets are not suitable for evaluating MICL. To address these challenges, this study explores the following two essential research questions: 1) How to efficiently alleviate the overlooking of visual modality to truly advance MICL? 2) What kind of datasets are more suitable for improving and evaluating the true MICL ability, especially the ability to understand the visual information in the demos? For the first question, we introduce Dynamic Attention ReAllocation (DARA) to mitigate visual context neglect and reduce overreliance on textual modality. DARA introduces set of learnable attention-balancing parameters that dynamically regulate the influence of visual and textual tokens in demos during attention computation. DARA multiplies the columns of the attention score matrices corresponding to visual tokens by these learned parameters, thereby encouraging the model to emphasize the visual information in the demos. More importantly, DARA is lightweight, introducing only small number of learnable parameters for rapid adaptation. Our experiments show that with around 100 parameters, DARA can yield up to 10% improvement on downstream tasks. Consequently, DARA preserves the advantages of in-context learning while still achieving substantial performance gains. For the second question, we propose that MICL datasets should prompt models to leverage visual context, quickly infer intent, and adapt to queries, rather than merely replicating textual answer styles or learning textual label spaces. We are motivated by the ICL disentanglement framework (Pan et al., 2023), which disentangles ICL into two components: task recognitionidentifying known task from the demonstrations and applying pre-trained priorsand task learningacquiring new input-label mappings from the demos. Existing MICL datasets focus primarily on task recognition, requiring models to identify pre-learned tasks such as image captioning. However, they largely overlook task learning, where models learn an unseen mapping relationship between images and texts from the demos (Zong et al., 2024; Jia et al., 2024; Wang et al., 2024a). As shown in the bottom of Figure 1, the correct response needs the learn the task strategy of identifying numbers and doing the math calculation. To address this problem, we introduce TrueMICL, MICL-dedicated dataset 2 Published as conference paper at COLM 2025 designed with critical principle: correct responses must rely on comprehensive understanding of the multimodal context, especially the visual information. In other words, the dataset should focus more on multimodal task learning, and correct response must rely on the presence and understanding of demo images. Besides, TrueMICL is designed to be scalable and configurable for different levels of difficulty and number of samples, making it practical and flexible for MICL evaluation. Specifically, we generate dataset with both support and evaluation splits with 860 samples in total, comprising 4 types and 7 distinct tasks, covering mathematical reasoning, pattern finding, and novel visual concept learning etc. Through extensive experiments across range of MLLMs and tasks, we demonstrate the effectiveness of both DARA and TrueMICL. Our results show that current MLLMs find the evaluation tasks from TrueMICL quite challenging, and DARA significantly improves the MICL performance on both our evaluation tasks and standard VL tasks. This highlights the potential of DARA to advance MICL ability and the importance of TrueMICL for more suitable MICL evaluation. Our contributions can be summarized as follows: To enhance the MICL ability and especially to cure the visual context neglect, we propose DARA, an effective and efficient fine-tuning approach, to boost the influence of visual modality in the context. With only few additional learnable parameters, DARA achieves substantial MICL performance improvements. Towards more reliable MICL improvement and evaluation, we curate TrueMICL, MICL-specialized dataset generation pipeline focused on emphasizing the critical role of visual information in multimodal contexts to improve MICL performance. Comprehensive experiments across diverse MLLMs show the effectiveness of both DARA and TrueMICL, providing valuable empirical insights into the effective strategies for true multimodal in-context learning ability."
        },
        {
            "title": "2 Related Work",
            "content": "Multimodal In-Context Learning. Built on powerful language backbones, some MLLMs start to show MICL ability, such as Flamingo (Alayrac et al., 2022), Qwen2-VL (Wang et al., 2024b), Idefics3 (Laurencon et al., 2024a), and Phi-3.5-Vision (Abdin et al., 2024), etc. These models support interleaved multiple image-text pairs as input and can generate responses to the query image-question pair conditioned on the previous image-text demonstrations. Another line of work focuses on understanding the working mechanism of MICL (Chen et al., 2023b; Li et al., 2023c; Yang et al., 2024b; Baldassini et al., 2024; Qin et al., 2024; Luo et al., 2024; Xu et al., 2024; Zhang et al., 2024). Some focus on exploring better demonstration configurations for VQA (Li et al., 2023c) and Image Captioning (Yang et al., 2024b). Some (Baldassini et al., 2024; Luo et al., 2024; Chen et al., 2023b) examine the essential factor that contributes to MICL and also find that current MICL relies primarily on text-driven mechanisms, showing little to no influence of the visual modality. Zong et al. (2024) points out the limitations of using conventional vision-language tasks to evaluate MICL. Their benchmark indicates that most models face substantial challenges performing MICL tasks without providing alleviation methods. Given such limited performance, we propose holistic solution to advance the MICL ability by encouraging the model to see the demo images with the help of our fine-tuning method DARA and dataset TrueMICL. Fine-tuning MLLMs to enhance MICL ability. Some studies (Zhao et al., 2023; Doveh et al., 2024; Li et al., 2023b;a; Gao et al., 2025; Li et al., 2023b) have tried fine-tuning the MLLMs for better MCIL ability mainly by adjusting the dataset format. For example, Zhao et al. (2023) transforms interleaved data into unified context and Doveh et al. (2024) extends the instruction tuning dataset into the form of multi-image conversation. However, the models still suffer from over-dependency on the text modality, and the evaluation datasets are mainly traditional VL tasks that can be answered based solely on the query images. Jia et al. (2024) replaces the original text answers in the demos with semantically irrelevant strings to encourage the model to focus more on the images. However, the proposed method requires heavy direct reference optimization. Different from these, our work proposes holistic solution consisting of both lightweight fine-tuning method, DARA, and dedicated MICL dataset, TrueMICL, to improve and evaluate the MICL ability. 3 Published as conference paper at COLM"
        },
        {
            "title": "3 Methodology",
            "content": "We begin by briefly formulating MICL and introducing DARA in Section 3.1, followed by the introduction of TrueMICL in Section 3.2. 3.1 Dynamic Attention Reallocation In multimodal in-context learning, an input query q, consisting of an image Iq and question or instruction Tq, follows context prompt Cq are passed to pre-trained MLLM . The context prompt Cq includes task demonstrations drawn from support set S, where each demonstration comprises an image Ii, instruction Ti, and response Ri. The MLLM generates response Rq to the query qfor instance, answering Tqbased on both the query components Iq and Tq, and the multimodal context Cq. Formally, the process can be represented as: Rq = ([Cq, q]), where = Iq, Tq, Cq = {Ii, Ti, Ri}N. Previous studies (Chen et al., 2023b; Baldassini et al., 2024; Luo et al., 2024) have identified the limited influence of context images {Ii}N on MICL performance. Chen et al. (2023b) analyzed attention patterns and suggested that this lack of sensitivity to visual information in the context may stem from the models imbalanced attention allocation between images and texts. To address this imbalance, we propose Dynamic Attention Reallocation (DARA), which effectively amplifies the influence of visual tokens within the attention mechanism. Considering one decoder layer in the language backbone of an MLLM, it includes the crucial self-attention module (Vaswani et al., 2017), which maps input hidden representations into queries Q, keys K, and values via linear projections using weights WQ, WK, WV. The decoder layers output is then calculated as = softmax(S)V, where = QKT represents the attention score matrix. The jth column S.j in the attention score matrix indicates the relative influence of the jth token over other tokens in the input sequence. To encourage the model to focus more on context images, we directly introduce set of learnable parameters to dynamically adjust the attention scores for columns corresponding to images. Specifically, the attention scores for visual tokens from context images are scaled by these learnable parameters. We implement this by introducing an attention balancer factor to the softmax operation, represented as = SF, where = diag(f) Rll and Rl is vector of learnable parameters. Only the positions in corresponding to visual tokens from are non-zero, while other positions remain zero. The modified attention output is then computed as = softmax(S)V. Parameters in can be trained by normal cross-entropy loss and all the other parameters in MLLMs are frozen. Algorithm 1 Pseudocode of DARA. scale: divide by the square root of dimension. 1 # Compute attention scores and apply amplification 2 scores = scale(torch.matmul(query, key.T)) 3 # Adjust the attention scores of each input image 4 for img_pos_ind, img_ind in image_indexes: # multiply learned weights for each image with the corresponding columns scores[:, img_pos_index] *= DARA_weight[img_ind] 6 7 # Final attention scores and output 8 scores = softmax(scores + mask) 9 attention_output = torch.matmul(scores, value) DARA is simple to implement, with PyTorch-style pseudocode provided in Algorithm 1 on the left. After calculating the attention score matrix (Line 2) in one of the attention heads, each score column corresponding to the input images is scaled by the learned weights for those images (Lines 46). The adjusted score matrix is then used to compute the final attention output of the modified attention head. Different heads can have different DARA modules for more flexible adaptation. Relationship between DARA and Low-Rank Adaptation. Low-Rank Adaptation (LoRA) (Hu et al., 2021; Mao et al., 2025; Yang et al., 2024a; Liu et al., 2024b; Gu et al., 2023; Chen et al., 2023a) is popular approach for parameter-efficient fine-tuning, which utilizes low-rank decomposition matrices to update parameters. Appendix mathematically proves that, for any f, an equivalent low-rank decomposed update can achieve the same output. In other words, DARA can be interpreted as concise version of LoRA for MICL with many fewer trainable parameters. Experimental in Appendix D.1 also show that, given the same scale of trainable parameters, DARA can achieve better results compared to LoRA. Besides, DARA can be applied to LoRA to improve MICL performance further. 4 Published as conference paper at COLM 2025 3.2 TrueMICL, MICL-dedicated Dataset To design dataset with indispensable visual context, we are motivated by the framework from Pan et al. (2023), which disentangles in-context learning (ICL) ability into two components: task recognition and task learning. Task recognition involves identifying an already known task from demonstrations and applying pre-trained priors, such as performing general visual question answering or image captioning. In contrast, task learning requires learning new input-label mapping from demonstrations. Task learning is scale-dependent and exhibits significant discrepancies across models of varying sizes (Pan et al., 2023). Several studies (Zhao et al., 2023; Doveh et al., 2024; Li et al., 2023b;a; Liu et al., 2024a) have proposed evaluation and fine-tuning datasets for MICL. Examples include collecting more data in unified formats (Zhao et al., 2023) and converting data into multi-image conversations (Doveh et al., 2024). However, these datasets do not sufficiently encourage models to acquire new mapping relationships from demonstrations. Instead, they focus on tasks similar to pre-training or instruction tuning datasets, such as image captioning and general visual question answering. This leads to critical limitation: query questions can often be answered by either inspecting only the query image or imitating the answer style in the demo text, without utilizing the demonstration images. This approach falls short of achieving the true multimodal in-context learning, as it neglects the core task learning ability. In this setup, no new image-text pair knowledge is required to answer the query question. Instead, the model merely needs to recognize the task and align the response with the desired text label space. As result, fine-tuning on such datasets can make the model tend to ignore the demo images, as it is unnecessary for the correct final response to the query. More importantly, without proper evaluation dataset, this limitation is hard to detect. Because such an issue can be concealed by the improved performance on evaluation datasets, where the performance is mainly from textual logic imitation (i.e., task recognition) rather than truly understanding the novel multimodal context (i.e., task learning). To comprehensively evaluate and incentivize MICL ability, besides evaluating task recognition, another core but usually missing principle is that correct responses must depend on an accurate understanding of the multimodal context, particularly the visual information. In other words, the demos should present image-text pairs with relations that are less unknown to the model so that the model can conduct task learning given the context. Additionally, the task should become unsolvable if the context images are removed. To this end, we have designed novel MICL-dedicated dataset, TrueMICL, guided by the following principles: 1) Context dependency: The task must be unsolvable without the context images. 2) Novelty: The task should introduce novel image-text relationships that are uncommon in pre-training or instruction tuning, to effectively challenge task learning ability. 3) Perceivable Visual Information: The necessary information extracted from the images should not be overly complex, ensuring that the visual encoder can perceive it accurately. This allows us to focus on MICL ability rather than visual perception challenges. 4) Compatibility with Backbone: The task should push the boundaries of multimodal in-context learning without exceeding the language backbones capabilities. 5) Configurability and Extensibility: This pipeline should be easily configured to generate more data samples with different levels of difficulty. These guidelines ensure that TrueMICL effectively evaluates and enhances the models ability to learn new tasks from multimodal contexts. In the end, we have curated 867 samples in total spanning 4 different categories, consisting of 7 distinct tasks as listed in Table 1 and more detailed information is present in Table 4 and Appendix B"
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup MLLMs. We utilize 3 popular open-source MLLMs capable of MICL with varying sizes, architectures and pre-training datasets: Qwen2-VL (Wang et al., 2024b), Idefics3 (Laurencon et al., 2024a), Phi-3.5-Vision (Abdin et al., 2024). We also report the performance on GPT4o (OpenAI et al., 2024) to showcase how current closed-source models perform on our datasets. More detailed information about MLLMs is in Appendix C.1. 5 Published as conference paper at COLM 2025 Task Type Task Full Name Task Short Name # Support # Test Math Reasoning Concept Binding Pattern Finding Operator Induction Clock Math Outlier Detection CLEVR Count Operator Clock Outlier CLEVR Sudoku Palindrome Number Sudoku Palindrome Novel Concept Character Classification Character 30 30 30 30 30 30 100 100 100 100 100 100 50 Table 1: TrueMICL contains 4 different categories, including total of 7 distinct tasks. More detailed information and examples are shown in Table 4 and Appendix Datasets and Evaluation Metrics. The evaluation datasets include the test split from TrueMICL and standard VL datasets. For TrueMICL, demos for each query are selected from the support set. Standard VL datasets used in our experiments include VQAv2 (Antol et al., 2015), GQA (Hudson & Manning, 2019) and A-OKVQA (Marino et al., 2019), and MSCOCO (Chen et al., 2015). The demos for query samples from these datasets are from their corresponding training split. The evaluation metric for TrueMICL is accuracy. For numerical answers, only strict matching is considered correct, while textual answers are evaluated through keyword-based matching. As for standard VL datasets, we follow the default metric for each dataset, such as accuracy for VQAv2. more detailed introduction on datasets and metrics used in this study is present in Appendix C.2. Baselines. We have conducted experiments over the following 5 baselines: 1) Zero-shot: The model receives only the query image and text prompt, without context. 2) No-image: The model receives the query image and text prompt, with randomly selected 4-shot demos including only the text information. 3) Random: The 4-shot demos are randomly selected, including both images and text. 4) RICES: The demos are selected by Retrieval-based InContext Example Selection (Alayrac et al., 2022; Yang et al., 2022; Zebaze et al., 2024; Wang et al., 2023). For given query, we chose the 4 most similar images from the support set and used the image-label pairs as demos. 5) LoRA: We also fine-tune LoRA on the support set for each task while controlling the number of learnable parameters to the same scale as DARA. For more detailed settings, please refer to Appendix C.3. DARA. We insert the DARA module into the first transformer layer of the language backbone in the MLLM, specifically, attention score matrices in all attention heads in the first layer will be regulated by DARAs learnable parameters. The number of learnable parameters is decided by the number of demo images and the number of heads to modify. For instance, given 5 images including 4-shot and the query image, and 32 attention heads to adjust, the total number of learnable parameters is 5 32 = 160. These learnable parameters are trained using Adam on the small support set with learning rate of 0.001. More detailed hyperparameters are present in Table 6 in the Appendix. 4.2 Result Analysis on TrueMICL The main experimental results of the models performance on TrueMICL are shown in Table 2. We report the accuracy of three models across various tasks in TrueMICL, where each row represents different inference method. Except for the Zero-shot, all other methods use 4-shot setting. Additionally, the methods labeled LoRA and DARA represent the performance of models trained under the finetuning settings described in Section 4.1. Table 2 first shows that these MLLMs perform poorly on TrueMICL in zero-shot scenarios and when demonstrations contain only text without images. Some models even achieve accuracy under 10% on certain tasks, such as Phi-3.5-Vision on Operator Induction. With randomly selected 4-shot demonstrations, the models demonstrate limited improvement in accuracy across tasks. However, applying the RICE method for selecting more relevant demonstrations yields minimal additional performance gains. This result highlights two key observations: 1) Tasks in TrueMICL truly require both visual and textual information from the context to be solved correctly; 2) Due to the models limited ability to effectively make use of the visual information in demonstrations, simply improving the relevance of the shots (as done in RICE) is insufficient to enhance models performance further. Published as conference paper at COLM 2025 Model Method Operator Clock Outlier CLEVR Sudoku Palindrome Character Average Qwen2-VL Idefics3 Phi-3.5-vision 13.670.57 Zero-Shot No-image 18.670.58 Random 67.330.58 67.670.58 RICES 66.330.58 LoRA 72.671.15 DARA 6.671.53 Zero-Shot No-image 10.332.08 Random 16.001.73 17.331.53 RICES 18.670.58 LoRA 21.331.53 DARA 9.331.53 Zero-Shot No-image 11.330.58 Random 14.330.58 15.001.00 RICES 14.000.00 LoRA 17.331.53 DARA 24.002.65 27.671.53 31.001.00 31.331.53 32.670.58 37.331.53 0.670.58 2.001.00 9.330.58 9.671.53 9.671.53 14.671.53 3.671.53 6.331.53 17.330.58 17.670.58 16.670.58 20.001.73 41.332.52 46.332.08 86.670.58 87.331.53 87.330.58 91.671. 50.001.00 52.671.53 59.670.58 59.000.00 59.670.58 64.002.00 39.002.00 45.331.15 61.672.08 61.330.58 65.331.15 70.671.53 34.331.53 41.673.06 86.000.00 87.331.15 88.001.73 90.001.73 19.002.00 23.001.00 25.001.73 24.671.53 24.330.58 25.001.00 8.331.53 13.002.00 24.331.15 24.330.58 27.671.15 32.001.73 88.001.73 90.001.73 93.332.08 95.331.53 92.670.58 95.330. 53.332.08 59.332.08 86.671.53 87.001.00 87.670.58 91.331.15 46.671.15 52.671.15 84.672.08 84.001.00 84.330.58 89.331.53 30.331.15 38.333.51 96.001.00 95.001.00 98.671.53 98.001.00 28.331.15 28.672.52 29.330.58 29.330.58 28.331.15 33.673.21 32.670.58 38.001.73 42.001.00 42.330.58 41.330.58 47.332.08 23.003.61 24.331.15 83.331.15 94.002.00 96.001.53 96.000. 23.671.53 27.333.21 67.672.08 77.001.00 89.331.15 90.671.53 21.331.73 28.330.58 68.001.53 84.672.00 91.332.08 93.001.00 36.38 41.00 77.67 79.71 80.24 83.00 25.95 29.05 41.95 43.43 45.38 48.67 23.00 27.86 44.62 47.05 48.67 52.81 Table 2: Performance of MICL from 3 different MLLMs using different methods on TrueMICL. Each column demonstrates the performance from each task in TrueMICL, with the task abbreviation as the column title. The best performance for each setting is in bold, and the second-best is underlined. DARA (rows in light red) achieves the best performance in most scenarios and outperforms baselines by large margin. Experiments are averaged over 3 different seeds, and the performance is in percentage with the standard deviations. However, as shown in the last two rows for each model, the performance on TrueMICL significantly improves after finetuning, particularly with the use of DARA. Compared to LoRA, which introduces several thousand parameters but yields limited gains and remains almost the same performance as random 4-shot, DARA leads to further improvements across nearly all tasks. For tasks involving math reasoning and concept binding, each model achieves an average improvement of 3 to 5 percent per task when using DARA. Meanwhile, in tasks such as pattern finding and concept learning, models like Qwen2-VLwhich already achieve good accuracy using Random baseline due to their strong capabilitiesstill show an additional stable gain in accuracy when using DARA. Besides, the performance of the other two models on these tasks also demonstrates stable and noticeable gain when DARA is applied, further supporting its effectiveness. Overall, the results demonstrate that the proposed dataset, TrueMICL, serves as reliable benchmark for evaluating true MICL capabilities, as every task category requires the model to combine both visual and textual information to provide the correct answer. Furthermore, the proposed method, DARA, consistently achieves the highest average performance across nearly all TrueMICL tasks for each model tested. This highlights the effectiveness of DARA in enhancing the models ability to perform multimodal in-context learning. 4.3 Visualization of the DARAs Reallocation Effects To better understand how DARA affects attention, we present both qualitative and quantitative visualizations as shown in Figure 2 and 3. The spatial attention heatmap (Zhang et al., 2025) of input images (Figure 2(a)) shows that, without DARA, both demonstration and query images do not receive much attention, as indicated by predominantly blue regions in the top row. After tuning with DARA, attention over image tokens increases markedly (more red/yellow areas in the bottom row), indicating enhanced attention to the visual input. While not all focus aligns with the specific object regions, the overall shift indicates that DARA encourages greater incorporation of image information. We also quantitatively compare the attention allocation ratio over different modality tokens with and without using DARA. Figure 2(b) highlights the shift of attention on these two modalities. Without DARA, the model allocates only 28% of attention to image tokens, focusing primarily on text. With DARA, this rises to 46.7%, indicating substantial shift toward visual content during response generation. Published as conference paper at COLM 2025 (a) Attention heatmaps over input images without (top) and with (bottom) applying DARA. (b) Normalized attention ratios over image and text tokens without and with applying DARA. Figure 2: DARA enhances visual attention both qualitatively (left) and quantitatively (right). To further understand the value distribution of learned factors, Figure 3 visualizes these factors over four demos and one query image from the first transformer layer (8 heads) of Qwen2-VL. Values larger than 1 indicate attention amplification on visual tokens. DARA induces clear redistribution: (a) demo images consistently receive factors larger than 1, encouraging stronger reliance on context; and (b) different heads specializefor example, Head 1 emphasizes Demo 2 (1.27), while Head 5 emphasizes Demo 4 (1.32). These patterns confirm that DARA enables selective, context-aware visual attention during MICL. Figure 3: Learned attention amplification factors across 8 heads and 5 images in the 1st layer of Qwen2VL. DARA introduces structured visual emphasis on demonstration images. 4.4 Result Analysis on Standard VL Datasets We also analyzed the performance of DARA on standard VL datasets, following the same baseline configuration as described in Section 4.1. The results are summarized in Table 8 in the Appendix. The results show that, for standard VL tasks, model performance relies primarily on the models own ability to understand the query image and associated textual information. By comparing the first three settings, no demonstrations (Zero-shot), demonstrations without images (No-image), and random 4-shots, we observe almost no differences in performance. Taking Qwen2-VL on VQAv2 as an example, the accuracy under these settings is 78.6%, 78.7%, and 79.1%, respectively. These results align with previous findings (Chen et al., 2023b) and indicate that correct responses for these standard vision tasks do not depend on the visual context provided in the demonstrations. Once the model has extracted sufficient information from the query image and integrated it with the textual context, high-quality response can already be generated. Moreover, by examining the performance of DARA, we observe that DARAdesigned to encourage the model to better focus on visual content in the demonstrationsachieves results comparable to the 0-shot and 4-shot settings. This finding confirms, on the one hand, that standard VL benchmarks are indeed suboptimal for evaluating true multimodal in-context learning, as they do not even require visual context; and on the other hand, that the use of DARA does not lead to performance degradations on these standard VL tasks. 8 Published as conference paper at COLM 2025 Qwen2-VL Operator Baseline 67.33% Clock 31.00% Outlier 86.67% CLEVR 86.00% Sudoku 93.33% Palindrome 96.00% Operator Clock Outlier CLEVR Sudoku Palindrome 72.00% (+4.67%) 68.33% (+1.00%) 67.00% (-0.33%) 72.00% (+4.67%) 71.00% (+3.67%) 31.67% (+0.67%) 32.00% (+1.00%) 31.00% (+0.00%) 31.33% (+0.33%) 31.67% (+0.67%) 90.33% (+3.66%) 88.00% (+1.33%) 87.67% (+1.00%) 89.33% (+2.66%) 89.67% (+3.00%) 89.33% (+3.33%) 88.67% (+2.67%) 87.67% (+1.67%) 89.00% (+3.00%) 89.67% (+3.67%) 95.67% (+2.34%) 95.00% (+1.67%) 94.00% (+0.67%) 95.33% (+2.00%) 97.00% (+3.67%) 97.00% (+1.00%) 96.00% (+0.00%) 95.33% (+1.33%) 97.00% (+1.00%) 95.67% (-0.23%) Table 3: Transferability performance of DARA across tasks. Diagonal cells (self-transfer) are omitted for clarity. Each column reports the performance when DARA is trained on other tasks and evaluated on the column task. DARA shows good transferability and consistently achieves transfer gains in most settings, with improvements up to 4.67%. Figure 4: Performance comparison on Operator and Clock tasks across three models with different numbers of shots, ranging from 2 to 32. Compared to the baseline Random method, DARA consistently improves the MICL performance across different numbers of shots. 4.5 Ablation Study We further conduct various ablation studies on DARA to verify the effectiveness of our design. more detailed discussion is in Appendix Transferability on untuned tasks. We evaluate the transferability of DARA by testing whether improvements on one task can transfer to untrained ones. As shown in Table 3, training on single task yields 25% higher accuracy on unseen tasks compared to standard 4-shot baseline inference. While this does not reach the performance of models trained directly on each target task, it demonstrates that DARA can introduce transferability, enabling better generalization across related MICL tasks. Impact of Increasing Shots. We also evaluated the performance of models fine-tuned with TrueMICL under different shot settings. As shown in Figure 4, we present comparison between the performance of models trained with varying numbers of shots and the corresponding baseline at that shot level. Within certain range of shot numbers (for example, up to 10 shots for Qwen2-VL, determined by the maximum number of images the model can reliably process at once), applying DARA consistently brings performance improvements across all shot settings. However, when the number of shots becomes too large, neither DARA nor LoRA can consistently guarantee further gains. This finding suggests that as more capable models emerge in the future, DARA will still prove to be effective. Prompt Design. To test whether DARAs improvements originate from prompt engineering, we compare our original minimal promptwhich avoids leaking visual cues through textwith an enhanced version that includes detailed instructional cues (Sahoo et al., 2024). As shown in Appendix D.4, while such prompts help standardize response formatting, they 9 Published as conference paper at COLM 2025 do not lead to significant accuracy gains. This supports that DARAs effectiveness arises from improved reasoning over demonstrations, rather than prompt tuning. Human Evaluation. To assess whether TrueMICL tasks truly require MICL ability, we conducted human study involving 20 participants. In 0-shot setting, where no demos were provided, participants generally failed to answer. However, their performance improved once demonstrations were included. These results suggest that the tasks cannot be solved using prior knowledge or superficial cues alone. Instead, successful completion requires learning from the multimodal context, thereby validating the design of our benchmark. Layer Design on DARA. We further investigated applying DARA to multiple transformer layers beyond only the first layer. While extending DARA to deeper layers yields comparable accuracy, it leads to increased computational and parameter overhead. We attribute this to the first layers unique role in initiating early cross-modal fusion, as later layers process already entangled representations, limiting the impact of targeted amplification. Thus, restricting DARA to the first layer achieves better balance between effectiveness and efficiency. The result of applying different layers is shown in Appendix D.3 Hard-coded Attention Adjustment. As an additional baseline, we experimented with hard-coded attention amplification strategy. Specifically, we modified attention logits to completely ignore text tokens for half of the heads, forcing attention exclusively onto image tokens. This rigid masking led to unstable and incoherent outputs, likely due to disrupted modality balance. In contrast, DARA employs learnable amplification factor that softly increases attention to image tokens, allowing the model to dynamically adjust during training. This preserves output fluency while improving performance. Evaluation on GPT-4o. We also tested TrueMICL on GPT-4o, state-of-the-art closed-source model. In the 0-shot setting, GPT-4o fails on most tasks except few logic-based ones like Sudoku. With 4-shot demonstrations, it achieves significantly better accuracy (full results in Appendix D.2), highlighting the indispensable role of context images in TrueMICL. However, despite GPT-4os strong performance, certain fundamental MICL challenges persist; for example, GPT-4os accuracy significantly decreases when facing more challenging tasks such as our specially designed harder Sudoku variant. Furthermore, due to the closed-source nature of GPT-4o, it remains unclear whether these performance gains are driven purely by scaling or by undisclosed architectural and training strategies. Importantly, the lightweight design of DARA complements these powerful models, providing further improvements even when combined with parameter-efficient fine-tuning approaches like LoRA. Consequently, DARA remains valuable in addressing attention imbalance, especially benefiting openly accessible, resource-constrained models widely used in diverse applications. Moreover, TrueMICL serves as robust benchmark for accurately evaluating the MICL capabilities of both open-source and proprietary MLLMs."
        },
        {
            "title": "5 Conclusions",
            "content": "Despite recent advances in multimodal large language models, our study reveals critical limitation in current approaches to Multimodal In-Context Learning (MICL): the tendency to overlook visual information in demonstrations and over-rely on textual patterns. This undermines the fundamental promise of MICL, reducing it to predominantly unimodal process and limiting its real-world applicability. To tackle this issue, we propose DARA, lightweight yet effective fine-tuning method that dynamically reallocates attention to enhance the influence of visual tokens in the in-context examples. Complementing this, we introduce TrueMICL, challenging and diagnostic dataset that emphasizes true multimodal adaptation by requiring visual understanding for task success. Our extensive experiments across diverse MLLMs demonstrate that DARA consistently improves performance on both standard and TrueMICL tasks, validating its effectiveness in enhancing genuine multimodal adaptation. Furthermore, TrueMICL exposes existing blind spots in MLLMs and serves as valuable benchmark for evaluating true MICL capabilities beyond textual-level imitation. Together, DARA and TrueMICL offer holistic framework for truly advancing and evaluating MICL, paving the way for more faithful multimodal ICL in future MLLMs. 10 Published as conference paper at COLM"
        },
        {
            "title": "Acknowledgment",
            "content": "This paper is supported by the DAAD programme Konrad Zuse Schools of Excellence in Artificial Intelligence, sponsored by the Federal Ministry of Research, Technology and Space."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:2371623736, 2022. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pp. 24252433, 2015. Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An opensource framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023. Folco Bertini Baldassini, Mustafa Shukor, Matthieu Cord, Laure Soulier, and Benjamin Piwowarski. What makes multimodal in-context learning work? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15391550, 2024. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Shuo Chen, Jindong Gu, Zhen Han, Yunpu Ma, Philip Torr, and Volker Tresp. Benchmarking robustness of adaptation methods on pre-trained vision-language models. Advances in Neural Information Processing Systems, 36:5175851777, 2023a. Shuo Chen, Zhen Han, Bailan He, Mark Buckley, Qin Yao, Philip Torr, Volker Tresp, and Jindong Gu. Can multimodal large language models truly perform multimodal in-context learning? arXiv preprint arXiv:2311.18021, 2023b. Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022. Sivan Doveh, Shaked Perek, Jehanzeb Mirza, Amit Alfassy, Assaf Arbelle, Shimon Ullman, and Leonid Karlinsky. Towards multimodal in-context learning for vision & language models. arXiv preprint arXiv:2403.12736, 2024. Dyke Ferber, Georg olflein, Isabella C. Wiest, Marta Ligero, Srividhya Sainath, Narmin Ghaffari Laleh, Omar S.M. El Nahhas, Gustav uller-Franzes, Dirk Jager, Daniel Truhn, and Jakob Nikolas Kather. In-context learning enables multimodal large language models to classify cancer pathology images. arXiv preprint arXiv:2403.07407, 2024. URL https://doi.org/10.48550/arXiv.2403.07407. 40 pages, 5 figures. 11 Published as conference paper at COLM 2025 Jun Gao, Qian Qiao, Tianxiang Wu, Zili Wang, Ziqiang Cao, and Wenjie Li. Aim: Let any multimodal large language models embrace efficient in-context learning. In Proceedings of the AAAI Conference on Artificial Intelligence, number 3 in 39, pp. 30773085, 2025. Jindong Gu, Zhen Han, Shuo Chen, Ahmad Beirami, Bailan He, Gengyuan Zhang, Ruotong Liao, Yao Qin, Volker Tresp, and Philip Torr. systematic survey of prompt engineering on vision-language foundation models. arXiv preprint arXiv:2307.12980, 2023. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 67006709, 2019. Hongrui Jia, Chaoya Jiang, Haiyang Xu, Wei Ye, Mengfan Dong, Ming Yan, Ji Zhang, Fei Huang, and Shikun Zhang. Symdpo: Boosting in-context learning of large multimodal models with symbol demonstration direct preference optimization. arXiv preprint arXiv:2411.11909, 2024. Yixing Jiang, Jeremy Irvin, Ji Hun Wang, Muhammad Ahmed Chaudhry, Jonathan H. Chen, and Andrew Y. Ng. Many-shot in-context learning in multimodal foundation models. arXiv preprint arXiv:2405.09798, 2024. URL https://doi.org/10.48550/arXiv. 2405.09798. Benchmarking GPT-4o and Gemini 1.5 Pro across 14 multimodal datasets. Hugo Laurencon, Andres Marafioti, Victor Sanh, and Leo Tronchon. Building and better understanding vision-language models: insights and future directions. In Workshop on Responsibly Building the Next Generation of Multimodal Foundational Models, 2024a. Hugo Laurencon, Leo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? arXiv preprint arXiv:2405.02246, 2024b. Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Mimic-it: Multi-modal in-context instruction tuning. arXiv preprint arXiv:2306.05425, 2023a. Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023b. Li Li, Jiawei Peng, Huiyi Chen, Chongyang Gao, and Xu Yang. How to configure good in-context sequence for visual question answering. arXiv preprint arXiv:2312.01571, 2023c. Haowei Liu, Xi Zhang, Haiyang Xu, Yaya Shi, Chaoya Jiang, Ming Yan, Ji Zhang, Fei Huang, Chunfeng Yuan, Bing Li, et al. Mibench: Evaluating multimodal large language models over multiple images. arXiv preprint arXiv:2407.15272, 2024a. Yilun Liu, Yunpu Ma, Shuo Chen, Zifeng Ding, Bailan He, Zhen Han, and Volker Tresp. Perft: Parameter-efficient routed fine-tuning for mixture-of-expert model. arXiv preprint arXiv:2411.08212, 2024b. Yang Luo, Zangwei Zheng, Zirui Zhu, and Yang You. How does the textual information affect the retrieval of multimodal in-context learning? arXiv preprint arXiv:2404.12866, 2024. Yuren Mao, Yuhang Ge, Yijiang Fan, Wenyi Xu, Yu Mi, Zhonghao Hu, and Yunjun Gao. survey on lora of large language models. Frontiers of Computer Science, 19(7):197605, 2025. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 12 Published as conference paper at COLM 2025 OpenAI, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, and et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. URL https://doi.org/10.48550/arXiv.2410.21276. Covers architecture, multimodal input/output capabilities, and safety evaluations of GPT-4o. Jane Pan, Tianyu Gao, Howard Chen, and Danqi Chen. What in-context learning learns in-context: Disentangling task recognition and task learning. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 82988319, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.527. URL https://aclanthology.org/2023.findings-acl.527. Libo Qin, Qiguang Chen, Hao Fei, Zhi Chen, Min Li, and Wanxiang Che. What factors affect multi-modal in-context learning? an in-depth exploration. arXiv preprint arXiv:2410.20482, 2024. Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, and Aman Chadha. systematic survey of prompt engineering in large language models: Techniques and applications. arXiv preprint arXiv:2402.07927, 2024. URL https://doi.org/10.48550/ arXiv.2402.07927. 12 pages, 2 figures; provides taxonomy, strengths/limitations, datasets and models for prompt engineering. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Jinyin Wang, Haijing Zhang, Yihao Zhong, Yingbin Liang, Rongwei Ji, and Yiru Cang. Advanced multimodal deep learning architecture for image-text matching. arXiv preprint arXiv:2406.15306, 2024a. URL https://doi.org/10.48550/arXiv.2406.15306. Proposes novel architecture with cross-modal attention and hierarchical fusion. Liang Wang, Nan Yang, and Furu Wei. Learning to retrieve in-context examples for large language models. arXiv preprint arXiv:2307.07164, 2023. URL https://doi.org/10.48550/ arXiv.2307.07164. Accepted by EACL 2024. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024b. Nan Xu, Fei Wang, Sheng Zhang, Hoifung Poon, and Muhao Chen. From introspection to best practices: Principled analysis of demonstrations in multimodal in-context learning. arXiv preprint arXiv:2407.00902, 2024. Menglin Yang, Jialin Chen, Yifei Zhang, Jiahong Liu, Jiasheng Zhang, Qiyao Ma, Harshit Verma, Qianru Zhang, Min Zhou, Irwin King, et al. Low-rank adaptation for foundation models: comprehensive review. arXiv preprint arXiv:2501.00365, 2024a. Xu Yang, Yongliang Wu, Mingzhuo Yang, Haokun Chen, and Xin Geng. Exploring diverse in-context configurations for image captioning. Advances in Neural Information Processing Systems, 36, 2024b. Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. An empirical study of gpt-3 for few-shot knowledge-based vqa. In Proceedings of the AAAI conference on artificial intelligence, number 3 in 36, pp. 30813089, 2022. Armel Zebaze, Benoˆıt Sagot, and Rachel Bawden. In-context example selection via similarity search improves low-resource machine translation. arXiv preprint arXiv:2408.00397, 2024. URL https://doi.org/10.48550/arXiv.2408.00397. Haowei Zhang, Jianzhe Liu, Zhen Han, Shuo Chen, Bailan He, Volker Tresp, Zhiqiang Xu, and Jindong Gu. Visual question decomposition on multimodal large language models. arXiv preprint arXiv:2409.19339, 2024. Published as conference paper at COLM 2025 Jiarui Zhang, Mahyar Khayatkhoei, Prateek Chhikara, and Filip Ilievski. Mllms know where to look: Training-free perception of small visual details with multimodal llms. International Conference on Learning Representations, 2025. Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, and Baobao Chang. Mmicl: Empowering vision-language model with multi-modal in-context learning. arXiv preprint arXiv:2309.07915, 2023. Yongshuo Zong, Ondrej Bohdal, and Timothy Hospedales. Vl-icl bench: The devil in the details of benchmarking multimodal in-context learning. arXiv preprint arXiv:2403.13164, 2024."
        },
        {
            "title": "A DARA as a constrained version of LoRA",
            "content": "d Consider transformer-based attention mechanism with query, key, and value transformations defined by WQ, WK, WV, and let := QK be the attention score matrix, where := XWQ and := XWK, where is the input tensor to the transformer attention module. Suppose we introduce an attention reallocation factor to the softmax operation by defining := diag(f) RLL, where RL is vector of learnable factors. The modified scores are := SF. On the other hand, we consider the update (e.g., from gradient descent algorithm) on the module weights WQ, WK, defined as WQ, WK. After updating the weights, we have K, and the updated attention score matrix := QK . Prove that for f, WQ and WQ, so that = := WK + WK, the updated = XW := WQ + WQ, Q, = XW Proof. We want: Equivalently: = QK = QK = SF QK = QKF. (1) (2) We need to express QKF as QK with = XW and = XW K. There is no unique solution. In fact, there is family of solutions parameterized by an invertible matrix Rdd. Consider: = QR and = FKRT. Then: QK = (QR)(R1KF) = QKF. Since = XWQ and = XWK, we have: = XW = XWQR = = WQR , if is full rank or using suitable inverse/pseudoinverse. = XW = FXWKRT = K = X+FXWKRT. (3) (4) (5) (6) Here, X+ denotes left-inverse or pseudoinverse of X. Since is arbitrary and invertible, there are infinitely many such solutions. Hence, for any given F, we can always find WQ = Prove that each corresponding WQ, WK can be decomposed into low-rank matrices multiplications, such as WQ = AB, WK = CD WK to achieve = S. WQ and WK = 14 Published as conference paper at COLM 2025 Proof. Any matrix admits factorization into two matrices of potentially lower rank. For given WQ, consider its singular value decomposition (SVD): WQ = UΣV, (7) where and are orthonormal matrices and Σ is diagonal with nonnegative singular values. By retaining only the nonzero singular values (or even subset to achieve rank reduction), we obtain low-rank factorization: WQ = UrΣrV , Rrd. Define = UrΣr Rdinr and = (8) with Ur Rdinr, Σr Rrr, and Rrd. Thus: WQ = AB. similar argument applies to WK. Hence: WK = CD (9) (10) and Rrd, possibly with the same or different rank r. for some Rdinr To conclude, the updates WQ and WK obtained to achieve = can always be decomposed into products of lower-rank matrices. This establishes that these parameter updates can be implemented in low-rank form, aligning with practical low-rank adaptation methods. It is necessary to clarify that DARA can be seen as constrained variant of LoRA rather than technical equivalent, as it requires input-dependent updates and tokenposition-specific factors that deviate from the standard LoRA assumptions. Specifically, the weight updates in LoRA are input-independent and fixed after learning, whereas the desired low-rank update WK depends on the specific input as shown in the Formula 6 However, such constraints actually can provide practical benefits to MICL. While LoRA is more general, it can be indirect and suboptimal to rebalance the attention allocation for better MICL. In contrast, DARAs constrained formulation enables targeted visual attention modulation with far fewer parameters, leading to more efficient MICL improvement."
        },
        {
            "title": "B TrueMICL",
            "content": "Guided by the principles in Section 3.2, we designed and compiled four categories encompassing total of seven distinct tasks, as illustrated in Table 1 and 4. TrueMICL encompasses wide range of MICL capabilities, including mathematical reasoning, novel concept binding, and pattern interpretation, providing comprehensive dataset for multimodal in-context learning. Each task is designed with adjustable difficulty levels, such as more diverse concepts in novel concept binding, more complex visual patterns in pattern interpretation, etc. Each task in TrueMICL is divided into support set containing 30 samples and different test set containing 100 queries (except for Character Classification, due to the limited number of character images sourced from movie scenes). Operator Induction is derived from Zong et al. (2024). The dataset consists of simple arithmetic operations formed by three operators: addition, subtraction, and multiplication. Since the operator used in each image is not explicitly provided, the model must analyze the relationship between the image content and the answer to respond to the query. Clock Math is an original task inspired by the arithmetic pattern. The model must first perceive the number in the image, then learn the relationship between the number and the answer (e.g., addition), to correctly answer the query. For arithmetic tasks, considering the current capabilities of existing models, our dataset adopts the most straightforward form, using actual digits to perform the whole operations. In fact, replacing digits with arbitrary symbols to express implicit operations can all be seen as an extension of this type of task in higher levels of difficulty. Outlier Detection is originally designed to evaluate the models binding capability. Each image in this dataset contains varying numbers of two different shapes and colors. The model needs to identify the desired outlier feature based on the demo text. Published as conference paper at COLM 2025 Category Task Demo 1 Demo 2 Query Label Explanation Math Reasoning Operator Induction Clock Math 14 12 3 8 Multiplying the two numbers in the image Adding the two numbers in the clock. Concept Binding Outlier Detection Green Black Red The outlier color in the image CLEVR Count 2 Pattern Finding Sudoku Palindrome 918 7 5 217 9 The number of sphere 470 The missing number in between 9 To form palindrome number Novel Concept Character Classification Flora Walter Flora The same character in the demo Table 4: An overview of task examples in TrueMICL. The label to the query requires the model to learn the relationship between images and text in the demos. Similarly, in concept binding tasks, features such as shape, color, and size can be replaced with less explicit attributes, such as visual appearance or texture. This substitution will make the task significantly more challenging. CLEVR Count is derived from Zong et al. (2024). Each image contains objects with varying attributes such as colors and shapes. The model will be asked to count the number of objects with specific attributes based on the demonstrations. We simplified the samples to match the models visual perception capabilities. Specifically, we removed hard-to-distinguish attributes such as material and size, kept only color and shape, and redesigned the prompts to align with our intended focus on concept binding in the dataset. Sudoku and Palindromic Number are originally designed for pattern finding. They contain images of incomplete Sudoku grids or partially missing sequences of palindromic numbers. To solve these tasks, the model must recognize the underlying patterns behind the multimodal demos and apply this learned relation to correctly respond to queries. For pattern finding tasks, taking Sudoku as an example, increasing the number of missing digits or introducing additional implicit rules for number placement can substantially raise the task difficulty. High-level reasoning problems commonly used in IQ or logic tests also fall into this category as more advanced tasks of pattern finding. Character Classification is originally designed to test the ability of novel concept learning from multimodal demos. We collect movie character images after the models cutoff date and assign previously unseen names. This setup requires the model to associate novel names with unfamiliar appearances through the demonstrations in order to correctly answer the queries. For this unique type of classification task, any similarly constructed dataset that uses images collected after the release of the evaluated models can be considered valid. Published as conference paper at COLM"
        },
        {
            "title": "C More Details of Experimental Setup",
            "content": "C.1 MLLMs Model Qwen2-VL-7B-Instruct Idefics3-8B-Llama3 Phi-3.5-Vision-Instruct # Params Visual Encoder Language Model Architecture # Visual Tokens 7B Custom Patch Encoder Qwen2-7B-Instruct Autoregressive Up to 16,384 8.5B SigLIP-SO400M LLaMA3-8B-Instruct Autoregressive 169 (364364) 4.2B CLIP ViT-L/14 Phi-3.5-mini Autoregressive Dynamic (block concat) Table 5: 3 MLLMs used in this study, with various architectures, sizes, and number of visual tokens per image. Table 5 shows more details about the models used in this research. The following is brief introduction to all 3 MLLMS. Qwen2-VL is state-of-the-art multimodal model excelling in visual understanding. It supports multilingual OCR and dynamically adapts to arbitrary image resolutions for human-like visual processing. The link to the model on Hugging Face: Qwen2-VL-7BInstruct. Idefics3 is an open multimodal model that processes arbitrary sequences of images and text to generate text outputs. It supports tasks like visual question answering, image-based storytelling, and pure language modeling, with improved OCR, document understanding, and visual reasoning over previous versions. The link to the model on Hugging Face: Idefics3-8B-Llama3. Phi-3.5-Vision is lightweight, state-of-the-art multimodal model from the Phi-3 family, trained on high-quality text and vision data. It is enhanced via supervised fine-tuning and direct preference optimization to ensure strong instruction following and safety. The link to the model on Hugging Face: Phi-3.5-Vision-Instruct. C.2 Datasets and Evaluation Metrics To construct the dataset, each task type is first divided into four subsets: the query set, the support set, the training-query set, and the training-support set. These subsets are mutually exclusive and contain no overlapping samples. Depending on whether the setting is inference or fine-tuning, we then organize the data into either an inference file or training file accordingly, with the desired number of shots. While choosing demos, only those demos of the same type of query will be chosen. This structure ensures clear separation between evaluation and training data, and allows for flexible adaptation to different experimental settings. To assess the performance of our model on our proposed dataset, we primarily adopt accuracy (in percentage) as the evaluation metric. For numerical answers, only strict matching is considered correct, while textual answers are evaluated through keyword-based matching. We additionally perform manual verification on subset of the predictions to ensure the reliability of the scoring. The final accuracy is computed as: Accuracy = Ncorrect Ntotal 100% where Ncorrect denotes the number of correctly predicted answers and Ntotal represents the total number of evaluated samples. C.3 Baselines To successfully apply DARA, we first initialize new nn.Module inside the model, with dimensions [number of attention layers, number of attention heads, number of Published as conference paper at COLM 2025 images]. Then, for each input, we extract the position of each image from the input ids. By progressively passing the new parameters into the attention layer, we are able to apply scalar factor to each image token in the attention matrices of the specified layers and heads. All of the factors are first initialized to 1. As the model is finetuned using the settings described in Table 6, this parameter matrix is updated automatically. Parameter Value Batch size per device Gradient accumulation steps Epochs Learning rate Warmup steps Optimizer Weight decay Learning rate scheduler Precision Max sequence length Random seed Output directory 1 4 5 1e-3 5 AdamW (8-bit) 0.01 Linear fp16 / bf16 (auto) 2048 3407 \"outputs\" Table 6: General Supervised Fine-tuning Configuration To ensure fair comparison in parameter scale between LoRA and DARA, we restrict LoRA fine-tuning to only the first attention layer. Specifically, we disable LoRA in all other layers and retain only the two projection layers, proj and proj, in the first attention layer. Furthermore, to make the number of trainable parameters linearly controllable and scalable, we freeze proportion of the gradients in the initialized lora and lora matrices based on predefined ratio. This allows us to gradually vary the effective amount of the parameter of LoRA. Parameter Target layers Trainable modules LoRA rank (r) LoRA alpha LoRA dropout Bias setting Seed Value layers.0.self attn.q proj, layers.0.self attn.k proj Attention only 2 16 0.0 None Table 7: LoRA Configuration"
        },
        {
            "title": "D More Experimental Analysis",
            "content": "D.1 Discussion on the Performance between DARA and LoRA In this section, we provide detailed comparison between LoRA and DARA. We focus on both parameter efficiency and performance across various tasks in the TrueMICL. DARA is specifically designed to enhance models ICL ability with minimal parameter overhead. As shown in Figure 5, taking the result of Qwen2-VL on the task Operator Induction as an example, when both methods modify the same targetthe and projection layers in the first attention layerDARA significantly outperforms LoRA at low parameter scales. Specifically, LoRA with 2,252 parameters shows no improvement over random 4-shot inference. In contrast, DARA achieves better results using fewer than 225 parameters, matching the performance of LoRA configurations with 11,264 parameters. When applied 18 Published as conference paper at COLM 2025 Model Method VQAv2 GQA A-OKVQA COCO Qwen2-VL Idefics3 Phi-3.5-Vision Zero-shot No-image Random LoRA DARA Zero-shot No-image Random LoRA DARA Zero-shot No-image Random LoRA DARA 78.6 78.7 79.1 79.3 79.1 62.9 62.7 64.1 64.1 64.2 63.2 63.5 65.7 65.3 65.3 69.9 69.9 71.6 71.3 71.3 65.5 65.6 66.4 66.7 66.5 62.9 62.7 64.1 63.9 63. 87.8 87.8 89.1 89.9 89.7 79.7 80.0 80.8 80.8 80.7 80.3 80.4 81.8 81.5 81.5 120.1 119.9 120.2 120.1 119.9 117.6 116.9 117.6 116.8 117.3 115.7 115.7 116.1 115.7 115. Table 8: Results on four classical visionlanguage tasks. Accuracy is used for VQAv2, GQA, and A-OKVQA and CIDEr used for COCO. The performance is similar across all methods, suggesting that classical tasks benefit little from in-context learning, and DARA shows no signs of overfitting. Figure 5: Comparison between DARA (green point) and LoRA (blue curve). DARA outperforms the 4-shot baseline using only 140 parameters, whereas LoRA requires tens of thousands to reach similar performance, highlighting DARAs parameter efficiency. Model Method Operator Clock Outlier CLEVR Sudoku Palindrome Qwen2-VL Idefics Phi-3.5-vision LoRA 87.33 LoRA+DARA 94.67 (+1.34) 51.33 (+1.66) 89.33 (+2.00) 99.00 (+1.00) 99.00 (+1.33) 99.67 (+0.67) 93.33 99.00 97. 49.67 98.00 LoRA 81.33 LoRA+DARA 70.00 (+2.33) 37.00 (+3.00) 83.00 (+1.67) 64.33 (+1.33) 96.00 (+1.00) 80.33 (+0.66) 95.00 67. 79.67 63.00 34.00 LoRA 82.00 LoRA+DARA 68.00 (+2.67) 45.00 (-0.67) 84.00 (+2.00) 58.00 (+1.67) 94.33 (+1.66) 86.33 (+1.00) 65. 85.33 45.67 56.33 92.67 Table 9: Accuracy (%) of different models on six TrueMICL tasks, comparing full-parameter LoRA and LoRA with DARA. While full-parameter LoRA already achieves strong performance across all tasks, the integration of DARA still provides modest but consistent gains. to the same layer, LoRA needs 4050 times more parameters than DARA to achieve similar performance. These results demonstrate DARAs effectiveness in low-resource settings while maintaining exceptional parameter efficiency. We further explore DARAs behavior when combined with full-scale LoRA training. As shown in Table 9, full-parameter LoRAwhere millions of parameters are trainedundoubtedly leads to performance boost across all TrueMICL tasks. However, when DARA is applied on top of full LoRA, we still observe small but consistent improvement of 1 to 2 percent in accuracy. While this gain may appear small, it is achieved with parameter cost that is less than one ten-thousandth of that used in full LoRA, once again demonstrating the efficiency and practicality of DARA. To summarize, DARA shows clear advantages over LoRA in low-parameter regimes, achieving better performance with significantly fewer parameters. It is particularly effective for tasks that truly rely on in-context learning, as shown in TrueMICL. Moreover, DARA remains beneficial even when combined with full-parameter LoRA. These findings demonstrate that DARA serves as both an effective standalone solution and valuable complement to current adaptation approaches for multimodal in-context learning. 19 Published as conference paper at COLM 2025 D.2 Closed-source model performances We evaluate GPT-4o on the six TrueMICL tasks under both 0-shot and 4-shot settings. As state-of-the-art multimodal model, GPT-4os performance can serve as an approximate upper bound. As shown in Table 10, GPT-4o struggles in the 0-shot setting, achieving low accuracy except on Sudoku. Once demonstrations are added, performance improves dramatically across all tasks. This further confirms that TrueMICL requires models to effectively integrate and reason over visual-textual demonstrations, validating its role as multimodal in-context learning benchmark. Model Method Operator Clock Outlier CLEVR Sudoku Sudoku (hard) Palindrome GPT-4o Zero-shot 4-shot MICL 8.00 100.00 2.00 87.00 26.00 99. 4.00 96.00 94.00 100.00 0.00 91.00 33.00 97.00 Table 10: Accuracy (%) of GPT-4o on the six TrueMICL tasks under different settings. Demonstrations greatly improve performance, indicating strong multimodal in-context learning requirements. D.3 Amounts of params in DARA Figure 6: Comparison of performances when applying different settings of changed attention layers and heads. Figure 6 illustrates the models performance under different design choices of DARA, where we vary the number of modified layers and attention heads. Taking the n-shot setting as an example, the total number of trainable parameters introduced by DARA is calculated as (n + 1) (number of heads) (number of layers). As shown in the figure, within the range of few hundred parameters, different configurations have no significant difference in the impact on the final performance. D.4 Effect of Prompt Design We provide further analysis on the effect of prompt formulation. Our default prompt is intentionally minimal to avoid leaking visual information into the text and to better test models ability to reason from image demonstrations alone. To evaluate whether DARAs improvements could instead be attributed to better prompt wording, we compare this with more explicit instructional prompt. Original Prompt (Minimal and Neutral) System: Learn from the demos and give only the answer. User: <demo1> Question: What is the result of the following 20 Published as conference paper at COLM 2025 mathematical expression? Answer: 12 (repeated for 4 demos) <query> Question: What is the result of the following mathematical expression? Answer: Instructed Prompt (With Explicit Task Description) System: The following image-text pairs will show you how to answer the questions. From these demos, learn the desired operation on the numbers shown in each image. User: Here is the demo image 1: <demo1> The question is: What is the result of the following mathematical expression? The answer is: 12 (repeated for 4 demos) Here is the query where you need to answer, following the previous answers: <query> The question is: What is the result of the following mathematical expression? The answer is: Model Prompt Operator Clock Outlier CLEVR Sudoku Palindrome Qwen2-VL Original Instructed 67 67 31 30 87 87 86 87 94 96 96 Table 11: Performance comparison under two prompt settings (accuracy %). Instructional prompts offer marginal formatting consistency, but little accuracy gain. Performance Comparison Table 11 shows the performance of Qwen2-VL under both prompt settings. The instructional version yields slightly more consistent formatting, but does not improve overall accuracy. This suggests that the performance gain brought by DARA is not due to prompt wording, but due to its stronger ability to leverage visual support examples."
        }
    ],
    "affiliations": [
        "Konrad Zuse School of Excellence in Reliable AI (relAI)",
        "LMU Munich",
        "Munich Center for Machine Learning (MCML)",
        "Siemens AG",
        "Technical University of Munich",
        "University of Oxford",
        "University of Science and Technology of China"
    ]
}