{
    "paper_title": "Coupled Variational Reinforcement Learning for Language Model General Reasoning",
    "authors": [
        "Xueru Wen",
        "Jie Lou",
        "Yanjiang Liu",
        "Hongyu Lin",
        "Ben He",
        "Xianpei Han",
        "Le Sun",
        "Yaojie Lu",
        "Debing Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While reinforcement learning have achieved impressive progress in language model reasoning, they are constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the intrinsic probabilities of LLMs generating reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \\textit{\\b{Co}upled \\b{V}ariational \\b{R}einforcement \\b{L}earning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4\\% over the base model and achieves an additional 2.3\\% improvement over strong state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models."
        },
        {
            "title": "Start",
            "content": "Xueru Wen 1 2 Jie Lou 3 Yanjiang Liu 1 2 Hongyu Lin 1 2 Ben He 1 Xianpei Han 1 2 Le Sun 1 2 Yaojie Lu 1 2 Debing Zhang"
        },
        {
            "title": "Abstract",
            "content": "5 2 0 2 4 1 ] . [ 1 6 7 5 2 1 . 2 1 5 2 : r upled While reinforcement learning have achieved impressive progress in language model reasoning, they are constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the intrinsic probabilities of LLMs generating reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoningtrace sampling from answer information, leading to inefficient exploration and incoherence beIn this paper, tween traces and final answers. we propose Co einforcement earning (CoVRL), which bridges variational inL ference and reinforcement learning by coupling prior and posterior distributions through hybrid sampling strategy. By constructing and optimizing composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4% over the base model and achieves an additional 2.3% improvement over strong state-of-the-art verifierfree RL baselines, providing principled framework for enhancing the general reasoning capabilities of language models. ariational 1. Introduction Recent works (DeepSeek-AI et al., 2025; Yue et al., 2025; Yu et al., 2025a) explore reinforcement learning with verifiable rewards (RLVR) to enhance LLMs reasoning capabili1University of Chinese Academy of Sciences 2Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences 3Xiaohongshu Inc. Correspondence to: Yaojie Lu <luyaojie@iscas.ac.cn>, Jie Lou <loujie0822@gmail.com>. Figure 1. Comparison between verifier-free RL with question-only training and CoVRL. Unlike prior methods that sample reasoning traces conditioned only on the question, CoVRL couples questionconditioned prior sampling with answer-conditioned posterior sampling via hybrid variational framework, enabling efficient exploration while preserving strong traceanswer coherence. ties, demonstrating impressive results in mathematical tasks. In this paradigm, the LLM generates chain of thought (Wei et al., 2023) followed by an answer, and rule-based program evaluates the final answer, assigning binary rewards based on correctness. The model is then optimized using policy gradient methods such as GRPO (Shao et al., 2024). However, these techniques require verifiable rewards, which restricts their applicability to other domains where formal verification of model answers is challenging or impossible. natural solution to this constraint is to introduce an LLM as verifier (Ma et al., 2025), which performs role similar to that of the reward model in reinforcement learning from human feedback (Ouyang et al., 2022). However, this approach requires strong verifier and faces the risk of reward hacking (Skalse et al., 2025). As an alternative, verifier-free methods (Yu et al., 2025b; Tang et al., 2025; Zhou et al., 2025a; Chen et al., 2024) have emerged that utilize the probabilities LLMs assign to correct answers as reward signals. These methods view answer generation as process that relies on set of implicit reasoning steps. In this formulation, each potential reasoning trace is treated as latent 1 Coupled Variational Reinforcement Learning for Language Model General Reasoning variable contributing to the final answer. The probability of the correct answer is obtained by marginalizing over all plausible reasoning traces conditioned on the input question. In practice, the model is trained using reinforcement learning by sampling different intermediate thoughts for each question and using the probability of generating the correct answer from each thought as the reward. While these methods eliminate external verifiers, they typically rely on question-only generation, i.e., the model observes the question and then attempts to generate reasoning steps leading to the final answer. As shown in Figure 1, this strategy faces two fundamental challenges: (1) Low sample efficiency, particularly for difficult questions where the model struggles to produce useful reasoning traces without guidance; and (2) Potential incoherence between reasoning traces and final answers due to the lack of answer guidance during trace generation, where even correct reasoning may receive low rewards because the final answer is expressed in different format from the ground truth. In this work, we propose Coupled Variational Reinforcement Learning (CoVRL), which frames reasoning training as variational optimization problem that couples prior and posterior distributions. The core idea is to address the challenges of question-only training by establishing coupling between two complementary distributions for sampling reasoning traces during training. We leverage both prior and posterior distributions as variational components, corresponding to two complementary generation modes: (1) question-only generation, which samples traces from the prior distribution, reflecting real inference conditions; and (2) answer-guided generation, which samples traces from the posterior distribution to generate coherent reasoning that leads to the correct answer. This dual-mode strategy provides answer guidance during training while ensuring that the learned reasoning patterns transfer effectively to inference, thereby improving sample efficiency and mitigating incoherence between reasoning traces and answers. To implement this, we construct composite distribution that combines the prior and posterior probabilities, establishing coupled training framework. Since directly sampling from this composite distribution is computationally complex, we adopt hybrid sampling strategy where we randomly select between the two generation modes for each training example. We then maximize variational lower bound, which includes reconstruction term for answer prediction and regularization term to ensure transferability to inference. Through importance weighting, we enable seamless training across both modes using the same underlying language model with different prompt templates. In summary, our main contributions include: We formulate reasoning optimization as variational inference problem and introduce composite distribution that theoretically unifies prior and posterior generation modes within tractable framework. We propose hybrid sampling strategy that balances answer-guided learning and inference-time transferability, thus increasing sampling efficiency and addressing trace-answer coherence challenges. We demonstrate consistent improvements across diverse benchmarks, achieving 12.4% improvement over the base model and 2.3% improvement over the strongest baseline. 2. Methodology In this section, we introduce Coupled Variational Reinforcement Learning. Figure 2 illustrates the overall framework. 2.1. Preliminary Reinforcement Learning with Verifiable Rewards Recent work (Zhao et al., 2025; Yu et al., 2025a) has explored using reinforcement learning to optimize language models for reasoning tasks. Given reward function R(x, y) that evaluates the correctness of the generated reasoning and answer, the objective is: max θ ExD,ypθ(x)[R(x, y)] (1) where pθ denotes the model distribution parameterized by θ, is the question, and represents the generated reasoning and answer. This objective can be optimized using policy gradient methods like GRPO (Shao et al., 2024). However, these approaches require verifiable reward signals, limiting their applicability to domains without accessible verifiers. Variational Inference with Latent Variables Variational inference (Li et al., 2019; Kingma & Welling, 2022) provides principled framework for learning models with latent variables. Consider generative process where observed data depends on latent variables z: (cid:90) p(y) = p(yz)p(z)dz (2) Since this marginal likelihood is typically intractable, we introduce variational posterior qϕ(zy). By applying Jensens inequality, we obtain: log p(y) = log Eqϕ(zy) (cid:21) (cid:20) p(yz)p(z) qϕ(zy) Eqϕ(zy)[log pθ(yz)] DKL(qϕ(zy)p(z)) (3) Here, pθ(yz) is the decoder distribution that reconstructs the observed data given the latent variables, while qϕ(zy) 2 Coupled Variational Reinforcement Learning for Language Model General Reasoning Figure 2. CoVRL employs hybrid sampling between prior pϕ(zx) and posterior qψ(zx, y) to generate reasoning traces. It optimizes the reconstruction term using GRPO and NLL loss, with KL regularization applied to ensure training-inference coherence. is the encoder distribution that infers latent representations from the observed data. This lower bound is called the evidence lower bound (ELBO), which is widely used in variational autoencoders (Kingma & Welling, 2022) and diffusion models (Ho et al., 2020). This ELBO consists of two components: reconstruction term Eq(z)[log pθ(yz, x)] that encourages generating reasoning traces leading to the correct answer, and KL regularization term DKL(q(z)pϕ(zx)) that constrains deviation from the prior distribution. 2.2. Coupled Variational Reinforcement Learning In reasoning tasks, we treat reasoning traces as latent variables that mediate between questions and answers y, thus reformulating Equation 2 as: (cid:90) p(yx) = p(yz, x)p(zx)dz (4) Previous approaches generally optimize this objective by directly sampling from the prior distribution pϕ(zx). However, these approaches face two main challenges: (1) Low sampling efficiency: reasoning traces sampled from the prior pϕ(zx) may fail to effectively predict the target answer when the question is too difficult for the model, leading to inefficient exploration of the reasoning space. (2) Traceanswer incoherence: without access to the target answer, even correct reasoning traces can receive low rewards due to mismatched answer formulations or representations compared to the ground-truth answer y. In response to these limitations, we introduce variational distribution q(z). Following the standard variational inference framework, we derive the following objective: log pθ(yx) = log (cid:90) pθ(yz, x)pϕ(zx)dz (cid:90) dz q(z) = log = log Eq(z) pθ(yz, x)pϕ(zx) q(z) (cid:20) pθ(yz, x)pϕ(zx) q(z) pθ(yz, x)pϕ(zx) q(z) = Eq(z)[log pθ(yz, x)] DKL(q(z)pϕ(zx)) Eq(z) log (cid:21) (cid:20) (cid:21) (Jensens inequality) (5) 3 Previous approaches can be viewed as setting q(z) = pϕ(zx), which suffers from the aforementioned limitations. natural alternative is to use the posterior distribution as q(z), as in VAE (Kingma & Welling, 2022), which samples reasoning traces conditioned on both the question and the target answer, i.e., q(z) = qψ(zx, y). This enables generating more relevant reasoning paths and improves sampling efficiency compared to prior-only sampling. However, this introduces fundamental training-inference mismatch. During training, we optimize the posterior distribution with access to target answers, but at inference time, we must rely on the prior distribution pϕ(zx), which is conditioned only on the question. Even with KL divergence regularization DKL(qψ(zx, y)pϕ(zx)) that encourages the posterior to stay close to the prior, there is no guarantee that optimizing the posterior will improve the prior due to the asymmetric nature of reverse KL divergence (Chan et al., 2022). Specifically, while DKL(qψpϕ) constrains the posterior to avoid low-probability regions of the prior, it cannot ensure that the posterior covers all high-probability regions of the prior. This results in inadequate training for some regions of the prior, potentially leading to poor inference performance when sampling from these regions. In response to the difficulties of independently sampling from the prior pϕ(zx) and posterior qψ(zx, y) distributions, we introduce CoVRL, which establishes coupling between these distributions through complementary mechanisms. 2.3. Composite Distribution and Coupled Optimization We first construct composite distribution that unifies both question-only and answer-guided generation modes: p(zx, y) = 1 2 pϕ(zx) + 1 2 qψ(zx, y) (6) Coupled Variational Reinforcement Learning for Language Model General Reasoning This composite distribution establishes coupling between the prior pϕ(zx) and posterior qψ(zx, y) distributions. By replacing q(z) with this composite distribution p(zx, y), we reformulate the ELBO from Equation 5 as: log p(yx) = log (cid:90) p(yz, x)p(zx)dz = log Ep(zx,y) (cid:20) p(yz, x)p(zx) p(zx, y) (cid:21) Ep(zx,y)[log pθ(yz, x)] (cid:125) (cid:123)(cid:122) Reconstruction term (cid:124) DKL(p(zx, y)pϕ(zx)) (cid:123)(cid:122) (cid:125) Regularization term (cid:124) (7) This ELBO consists of two components: Reconstruction term Ep(zx,y)[log pθ(yz, x)] measures the models ability to predict the correct answer given the reasoning trace and question x, serving as the primary target for improving reasoning quality. Regularization term DKL(p(zx, y)pϕ(zx)) constrains the composite distribution to remain close to the prior distribution, ensuring that the learned reasoning patterns transfer effectively to inference. This coupled optimization framework addresses the traininginference mismatch by simultaneously optimizing both the prior and posterior distributions toward the same objective through the composite distribution. In practice, all three components can be implemented using single LLM with different prompts: pϕ(zx) conditions on the input question, qψ(zx, y) conditions on both question and answer, and pθ(yz, x) predicts the answer given the reasoning trace. However, directly sampling from the composite distribution p(zx, y) would require real-time mixing of prior and posterior distributions for generating each new token. This approach requires additional computation and complex modifications to prevalent LLM-RL frameworks (Sheng et al., 2024; Hu et al., 2024), which typically utilize efficient inference engines such as SGLang (Team, 2024) and vLLM (Kwon et al., 2023) for rollout. Therefore, instead of sampling from the actual composite distribution, we adopt off-policy hybrid sampling. For each training sample, we randomly select from the prior with probability α or the posterior with probability (1 α): phybrid(zx, y) = (cid:40) pϕ(zx) qψ(zx, y) with probability 1 α with probability α For the Composite distribution p(zx, y) = 1 1 2 qψ(zx, y), the gradient becomes: ϕ,ψEp(zx,y)[ log pθ(yz, x)] = 2 pϕ(zx)+ Ep(zx,y)[ log pθ(yz, x)ϕ,ψ log p(zx, y)] (9) Optimizing the composite distribution requires policy gradient methods due to the discrete sampling process. We employ Group Relative Policy Optimization (GRPO) (Shao et al., 2024) to optimize the reconstruction term with the following objective: (cid:105) (cid:104) Lϕ,ψ = Ep(zx,y) min(r(z) ˆA, clip(r(z), 1 ϵ, 1 + ϵ) ˆA) (10) where r(z) = new(zx,y) old(zx,y) represents the probability ratio between the new and old policies, and ϵ is the clipping parameter. The advantage ˆA = log pθold(yz, x) is computed using group relative estimation, where is the average reward across the batch. Importantly, the reward is evaluated based on the models own answer prediction probability, eliminating the need for external verifiers. However, the hybrid sampling strategy introduces fundamental difference between the hybrid sampling distribution and the target composite distribution. Nevertheless, we can still optimize the composite distribution through importance sampling. Specifically, we replace old(zx, y) in Equation 10 with the actual sampling distribution phybrid(zx, y). Thus, the importance ratio r(z) becomes: r(z) = p(zx, y) phybrid(zx, y) (11) This formulation enables mathematically principled optimization of the target composite distribution. Prior distribution For pϕ(zx), the gradient is: ϕEpϕ(zx)[log pθ(yz, x)] = Epϕ(zx)[log pθ(yz, x)ϕ log pϕ(zx)] (12) An important observation is that optimizing the prior distribution alone corresponds to standard maximum likelihood estimation, which can be efficiently computed using negative log-likelihood loss. However, applying NLL loss to all reasoning traces is problematic, as some traces may be of low quality. Since our model also serves as reward model that evaluates reasoning quality through log pθ(yz, x), we selectively reinforce high-quality traces. Therefore, we only compute the loss on samples that complete successfully and have positive advantage ˆA > 0. (8) This approach ensures that both distributions are sampled, enabling joint optimization of exploratory and answerguided reasoning. 2.4. Optimizing KL Divergence There are two approaches for optimizing the KL divergence term: (1) incorporating KL as part of the reward function 4 Coupled Variational Reinforcement Learning for Language Model General Reasoning Prior Distribution Template Posterior Distribution Template <im start>system conversation between User and Assistant. The user asks question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, i.e., <think>reasoning process respectively, here</think><answer>answer here</answer>. <im end> <im start>system conversation between User and Assistant. The user asks question, and the Assistant solves it. The assistant provides the final answer first, then follows up with comprehensive reasoning process. The answer and reasoning process are enclosed within <answer> </answer> and <think> </think> tags, respectively, i.e., <answer>answer here</answer><think>reasoning process here</think>. <im end> <im start>user {question} <im end> <im start>assistant <think> {thought} </think> <answer> {answer} </answer> <im end> <im start>user {question} <im end> <im start>assistant <answer> {answer} </answer> <think> {thought} </think> <im end> Figure 3. Prompt templates after applying chat template for Prior and Posterior distributions. The key difference lies in the order of reasoning and answer components within the assistants response. as in PPO (Schulman et al., 2017), or (2) treating KL as separate loss term as in GRPO (Shao et al., 2024). We adopt the latter approach, which we find is more stable. Optimizing KL divergence as loss function requires non-negative and low-variance KL estimator. We extend existing KL estimators to handle our composite distribution setting, which couples prior and posterior through the KL regularization term. Specifically, we need to estimate DKL(p(zx, y)pϕ(zx)) where p(zx, y) = 1 2 pϕ(zx) + 1 2 qψ(zx, y). We extend previous low-variance KL estimators (Schulman, 2020) and derive different estimators depending on the sampling distribution using importance sampling with Bregman divergence-based control variates. When sampling from the prior distribution: Dprior KL = p(zx, y) pϕ(zx) log p(zx, y) pϕ(zx) p(zx, y) pϕ(zx) + 1 (13) (cid:16) p(zx,y) The key difference between these estimators lies in the sign of the Bregman correction term . This correction serves as control variate that reduces estimation variance without introducing bias, leveraging the fact that: p(zx, y) phybrid(zx, y) phybrid(zx,y) 1 Ephybrid[ 1] = (15) (cid:17) The different signs ensure that the estimator remains nonnegative regardless of whether the trace is sampled from the prior or posterior distribution. Moreover, we apply soft clipping to the importance sampling ratios in both estimators when they exceed threshold to prevent numerical overflow from the exponential terms while still providing stable gradients. In cases where the model generation reaches the maximum response length, we skip computing the KL loss for these samples. Appendix provides more intuitive discussion of these KL estimators. When sampling from the posterior distribution: 2.5. Prompt Template and Tokenization Dposterior KL = p(zx, y) qψ(zx, y) log p(zx, y) pϕ(zx) + p(zx, y) qψ(zx, y) 1 (14) We follow the template format suggested in previous work (DeepSeek-AI et al., 2025), with simple extensions to suit our hybrid sampling strategy. The prompt templates for 5 Coupled Variational Reinforcement Learning for Language Model General Reasoning Table 1. Performance on benchmarks. Results are reported as Average@N where indicates the number of independent evaluation runs to account for sampling variance. The Overall column presents the arithmetic mean across all benchmark tasks. General Reasoning Mathematical Reasoning Method Base Model VeriFree JLB LaTRO RAVR RLPR CoVRL (Ours) GPQA MMLU-Pro TheoremQA AIME24 AQuA CARP-EN MATH-500 Minerva Avg@4 Avg@32 Avg@4 Avg@4 Avg@2 Avg@1 Avg@2 Avg@4 26. 28.9 31.6 31.0 27.6 31.3 30.4 36.7 44.1 42.7 42.7 34.2 44.9 46.5 25. 33.4 31.9 32.8 24.9 33.5 36.3 2.7 5.0 4.8 4.0 3.4 6.5 7.5 55. 72.6 69.5 67.6 59.9 72.3 77.3 54.3 62.8 63.4 50.5 53.4 62.9 65.1 44. 59.5 57.6 59.3 44.0 61.2 66.3 18.6 24.0 23.6 24.4 13.2 24.7 25.5 Overall SAT-Math Avg@32 76.5 93.3 93.7 90.1 81.2 93.8 97.1 37.8 47.1 46.5 44.7 38.0 47. 50.2 prior and posterior generation are shown in Figure 3. For tokenization, we separately encode special tokens including <think>, </think>, <answer> and </answer> to ensure consistent token ID sequences. However, during model generation, these special tokens may still be merged with adjacent tokens. Therefore, we use strings as stop signals for model generation, and after generation completes, we decode the outputs and re-encode them with separately encoded special tokens. 3. Experiment 3.1. Setup Dataset Following previous work, we use the dataset curated by (Ma et al., 2025), which was sourced from WebInstruct (Yue et al., 2024). We retain only non-mathematical questions to assess training improvements on general reasoning abilities. Beyond this selection, we apply no additional filtering to evaluate the algorithms robustness across diverse question types, difficulty levels, and quality variations. Implementation We conduct experiments primarily using Qwen2.5-7B-Base (Qwen et al., 2025). We directly finetune the base model without an intermediate supervised finetuning stage. We implement our RL training pipeline using the verl framework (Sheng et al., 2024). During training, we use batch size of 192 questions and generate 8 responses per question during rollout. For rollout sampling, we employ temperature=1.0, top p=1.0, and max tokens=2048 to ensure diverse response generation while maintaining reasonable response lengths. The clip threshold in the policy gradient loss is set to 0.3 given the off-policy nature of our algorithm. We use cosine learning rate scheduler with 64 warmup steps and peak learning rate of 1e-6. Baselines Our baseline selection focuses on other verifierfree methods that do not require external verification models during training: VeriFree (Zhou et al., 2025a), RLPR (Yu et al., 2025b), JLB (Tang et al., 2025), RAVR (Lin et al., 2025), and LaTRO (Chen et al., 2024). We summarize the gradient estimators of these methods in Appendix B. To eliminate the influence of different RL techniques and focus on the gradient estimator, we train all baselines on the Qwen2.5-7B-base model using the GRPO (Shao et al., 2024) with identical hyperparameters on the filtered dataset. Evaluation We evaluate our method on comprehensive suite of reasoning benchmarks. For general reasoning, we use MMLU-Pro (Wang et al., 2024), GPQA (Rein et al., 2023), and TheoremQA (Chen et al., 2023). For mathematical reasoning, we use AIME24 (Mathematical Association of America, 2024), AQuA (Ling et al., 2017), CARP-EN (Zhang et al., 2023), MATH-500 (Lightman et al., 2023), Minerva (Lewkowycz et al., 2022), and SAT-Math (mcaleste, 2023). For evaluation, we use temperature=0.6 and max tokens=4096. We utilize Math-Verify to check answer correctness; more details are provided in Appendix C. To reduce evaluation variance, we repeat the evaluation times for each dataset, where depends on the test set size. We report the average performance across these runs as Average@N to account for sampling variance. 3.2. Main Results Table 1 presents our main experimental results across all benchmarks. Our approach achieves consistent improvements over the base model and competitive performance against other verifier-free methods. CoVRL achieves consistent performance improvements across benchmarks. CoVRL demonstrates substantial improvements over the base model across all tasks, achieving the highest overall performance at 50.2%, representing 12.4% improvement. In comparison with other baselines, CoVRL achieves the strongest overall gains. Reasoning capabilities learned by CoVRL are generalizable to mathematical reasoning. Despite training on 6 Coupled Variational Reinforcement Learning for Language Model General Reasoning (a) Reward Score (b) Response Length (c) NLL Loss (d) KL Loss Figure 4. Training dynamics of CoVRL across different metrics. We observe stable improvements in reasoning quality alongside effective optimization of both reconstruction and regularization objectives. non-mathematical questions, our approach shows substantial gains on mathematical benchmarks. This validates that general reasoning capabilities developed through diverse problem-solving can transfer effectively, highlighting the value of general reasoning skill development. 3.3. Training Dynamics We present the evolution of key metrics in Figure 4 to better understand the algorithm. This reveals several observations: The posterior distribution is effective in providing guidance. Figure 4a shows steady reward score improvements throughout training, with the posterior distribution consistently outperforming the prior distribution. This persistent score gap validates our answer-guided sampling strategy and confirms that posterior sampling enables more efficient exploration of high-quality reasoning paths. CoVRL enhances reasoning capabilities with prolonged chain-of-thought traces. Figure 4b demonstrates stable increase in response length, indicating that the model progressively generates more detailed reasoning processes. This trend suggests that CoVRL successfully encourages elaborate chain-of-thought reasoning with more thorough step-by-step explanations. Regularization provides stable optimization dynamics. Figures 4c and 4d show stable downward trends in NLL and KL losses. The decreasing losses indicate improved answer prediction and successful regularization, confirming that our variational objective effectively balances the reconstruction and regularization terms. 3.4. Hybrid Sampling Strategy We examine the impact of our hybrid sampling strategy by varying the mixing ratio α between prior pϕ(zx) and posterior qψ(zx, y) distributions. Here α represents the probability of sampling from the prior distribution. By default, we set α = 0.5. Figure 5 demonstrates that low prior sampling probability (α = 0.1) outperforms high prior sampling probability (α = 0.9). This highlights the important Figure 5. Impact of prior sampling probability α in hybrid sampling strategy. role of the posterior distribution in our algorithm. When α = 0.9, the model primarily samples from the prior distribution, and we observe reduced reasoning chain length. We attribute this to difficulty in improving rewards, leading the model to prioritize minimizing KL loss and generating shorter sequences. When α = 0.1, reasoning chain length increases, and posterior-dominated sampling achieves better performance than prior-dominated sampling. However, due to training-inference mismatch, performance remains inferior to balanced sampling. Training dynamics under these settings are provided in Appendix D. 3.5. Training Model and Data CoVRL is robust across different base models. We evaluate CoVRL on Qwen2.5-base and Qwen3-base models ranging from 7B to 14B parameters. Note that for Qwen3base experiments, we replace <think> </think> with <thinking> </thinking> tags, as Qwen3-base fails to properly utilize <think> </think> tags in prompts while responding well to other similar tags. As shown in Table 2, CoVRL delivers consistent performance improvements across all tested models, demonstrating its robustness across model architectures. CoVRL learns generalizable reasoning abilities from training data. We also evaluate CoVRL with different training data compositions. The results in Table 3 reveal that 7 Coupled Variational Reinforcement Learning for Language Model General Reasoning Table 2. Performance comparison across different base models of various model architectures and sizes. General Mathematical Model GPQA MMLU-Pro TheoremQA AIME24 AQuA Avg@4 Avg@2 Avg@4 Avg@16 Avg@ CARP-EN MATH-500 Minerva Avg@4 Avg@2 Avg@4 SAT-Math Overall Avg@32 Qwen2.5-7B + CoVRL 26.1 30.4+4.3 36.7 46.5+9. Qwen2.5-14B 28.0 + CoVRL 36.6+8.6 Qwen3-8B + CoVRL Qwen3-14B + CoVRL 36.9 37.6+0.7 37.5 42.7+5.2 38.5 57.0+18.5 51.0 59.6+8.6 57.6 63.1+5.5 25.2 36.3+11. 25.4 42.4+17.0 32.8 43.7+10.9 37.6 46.3+8.7 2.7 7.5+4.8 3.0 7.9+4.9 4.0 9.2+5. 7.8 9.5+1.7 55.6 77.3+21.7 54.3 65.1+10.8 63.3 81.8+18.5 68.6 80.5+11.9 75.3 83.9+8. 57.3 64.0+6.7 59.4 66.1+6.7 64.6 65.9+1.3 44.7 66.3+21.6 44.8 71.5+26.7 53.6 74.5+20. 67.2 76.1+8.9 18.6 25.5+6.9 76.5 97.1+20.6 22.1 33.7+11.6 82.5 95.8+13.3 30.0 35.2+5. 33.5 38.1+4.6 90.5 97.6+7.1 93.1 97.4+4.3 37.8 50.2+12.4 40.5 54.5+14.0 47.4 56.0+8. 52.7 58.1+5.4 Table 3. Performance comparison across different training data compositions. General Mathematical Training Data GPQA MMLU-Pro TheoremQA AIME24 AQuA Avg@4 Avg@2 Avg@ Avg@16 Avg@1 CARP-EN MATH-500 Minerva Avg@4 Avg@2 Avg@4 SAT-Math Overall Avg@32 Base Model Non-Math Only Math Only 26.1 30.4+4.3 28.9+2.8 36.7 46.5+9.8 42.7+6.0 25.2 36.3+11.1 31.8+6.6 2.7 7.5+4.8 4.2+1.5 55.6 77.3+21.7 72.1+16.5 54.3 65.1+10.8 53.11. 44.7 66.3+21.6 60.1+15.4 18.6 25.5+6.9 20.6+2.0 76.5 97.1+20.6 94.3+17.8 37.8 50.2+12.4 45.3+7.5 training on both math-only and non-math-only data significantly improves base model performance. Notably, models trained solely on mathematical data demonstrate enhanced performance on non-mathematical reasoning tasks, and similarly, models trained on non-mathematical data improve on mathematical tasks. This indicates that our method enables models to acquire generalizable reasoning capabilities that transfer across different domains. 4. Related Works Verifier-free Reinforcement Learning While reinforcement learning from verifiable rewards (Cui et al., 2025; Yu et al., 2025a; Yang et al., 2025; DeepSeek-AI et al., 2025) has emerged as common practice, these approaches are restricted to domains where robust verifiers are available. Recent work explores eliminating external verification dependencies. LaTRO (Chen et al., 2024) proposes framework that formulates reasoning as sampling from latent distribution. JLB (Tang et al., 2025) attempts to scale this approach to data that cannot be easily verified. VeriFree (Zhou et al., 2025a) shares similar framework and proposes using probability directly as both reward and coefficient for negative log-likelihood loss. RLPR (Yu et al., 2025b) introduces different reward mechanism using the mean of token rewards. These approaches sample from prior distributions, potentially suffering from limited sample efficiency and reasoning-answer misalignment issues. In contrast, RAVR (Lin et al., 2025) also utilizes posterior distributions. Compared to RAVR, our approach establishes explicit coupling between prior and posterior through composite distribution and hybrid sampling strategy. Self-improving language models Recent work explores various strategies for enhancing language models by leveraging their own outputs. One line focuses on self-generated training signals, including methods that prompt models to rank their responses for preference learning (Yuan et al., 2025), apply iterative DPO (Chen et al., 2025) with implicit rewards (Rafailov et al., 2024), and employ majority voting for test-time improvements (Zuo et al., 2025). Alternative approaches construct rewards based on reference answers, such as the verifier-free methods discussed above. Among them, Zhou et al. (2025b) also introduce posterior distributions and update two separate prior and posterior models using the IWAE paradigm (Burda et al., 2016) with an EM algorithm, utilizing external verifiers for variance reduction. In comparison, we adopt online RL training to optimize single composite distribution with verifier-free rewards. 5. Conclusion In this paper, we introduce Coupled Variational Reinforcement Learning, which enables joint optimization of questiononly and answer-guided generation while maintaining theoretical grounding through coupled optimization framework based on composite distributions. Experimental results demonstrate consistent improvements across tasks, validating the effectiveness of our approach. Coupled Variational Reinforcement Learning for Language Model General Reasoning"
        },
        {
            "title": "References",
            "content": "Burda, Y., Grosse, R., and Salakhutdinov, R. Importance weighted autoencoders, 2016. URL https://arxiv. org/abs/1509.00519. Chan, A., Silva, H., Lim, S., Kozuno, T., Mahmood, A. R., and White, M. Greedification operators for policy optimization: Investigating forward and reverse kl divergences, 2022. URL https://arxiv.org/abs/ 2107.08285. Chen, C., Liu, Z., Du, C., Pang, T., Liu, Q., Sinha, A., Varakantham, P., and Lin, M. Bootstrapping language models with dpo implicit rewards, 2025. URL https: //arxiv.org/abs/2406.09760. Chen, H., Feng, Y., Liu, Z., Yao, W., Prabhakar, A., Heinecke, S., Ho, R., Mui, P., Savarese, S., Xiong, C., and Wang, H. Language models are hidden reasoners: Unlocking latent reasoning capabilities via self-rewarding, 2024. URL https://arxiv.org/abs/2411.04282. Chen, W., Yin, M., Ku, M., Lu, P., Wan, Y., Ma, X., Xu, J., Wang, X., and Xia, T. Theoremqa: theoremdriven question answering dataset, 2023. URL https: //arxiv.org/abs/2305.12524. Cui, G., Yuan, L., Wang, Z., Wang, H., Zhang, Y., Chen, J., Li, W., He, B., Fan, Y., Yu, T., Xu, Q., Chen, W., Yuan, J., Chen, H., Zhang, K., Lv, X., Wang, S., Yao, Y., Han, X., Peng, H., Cheng, Y., Liu, Z., Sun, M., Zhou, B., and Ding, N. Process reinforcement through implicit rewards, 2025. URL https://arxiv.org/abs/2502.01456. DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Ding, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J. L., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R., Zhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., Li, S. S., Zhou, S., Wu, S., Ye, S., Yun, T., Pei, T., Sun, T., Wang, T., Zeng, W., Zhao, W., Liu, W., Liang, W., Gao, W., Yu, W., Zhang, W., Xiao, W. L., An, W., Liu, X., Wang, X., Chen, X., Nie, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yang, X., Li, X., Su, X., Lin, X., Li, X. Q., Jin, X., Shen, X., Chen, X., Sun, X., Wang, X., Song, X., Zhou, X., Wang, X., Shan, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhang, Y., Xu, Y., Li, Y., Zhao, Y., Sun, Y., Wang, Y., Yu, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong, Y., Zou, Y., He, Y., Xiong, Y., Luo, Y., You, Y., Liu, Y., Zhou, Y., Zhu, Y. X., Xu, Y., Huang, Y., Li, Y., Zheng, Y., Zhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z. Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., Zhang, Z., Hao, Z., Ma, Z., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z., Xie, Z., Song, Z., Pan, Z., Huang, Z., Xu, Z., Zhang, Z., and Zhang, Z. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models, 2020. URL https://arxiv.org/ abs/2006.11239. Hu, J., Wu, X., Zhu, Z., Xianyu, Wang, W., Zhang, D., and Cao, Y. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024. Kingma, D. P. and Welling, M. Auto-encoding variational bayes, 2022. URL https://arxiv.org/ abs/1312.6114. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., Wu, Y., Neyshabur, B., Gur-Ari, G., and Misra, V. Solving quantitative reasoning problems with language models, 2022. URL https://arxiv.org/abs/2206.14858. Li, R., Li, X., Lin, C., Collinson, M., and Mao, R. stable variational autoencoder for text modelling, 2019. URL https://arxiv.org/abs/1911.05343. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Lin, T., Zhao, X., Zhang, X., Long, R., Xu, Y., Jiang, Z., Su, W., and Zheng, B. Ravr: Reference-answer-guided variational reasoning for large language models, 2025. URL https://arxiv.org/abs/2510.25206. Ling, W., Yogatama, D., Dyer, C., and Blunsom, P. Program induction by rationale generation : Learning to solve and Coupled Variational Reinforcement Learning for Language Model General Reasoning explain algebraic word problems, 2017. URL https: //arxiv.org/abs/1705.04146. Ma, X., Liu, Q., Jiang, D., Zhang, G., Ma, Z., and Chen, W. General-reasoner: Advancing llm reasoning across all domains, 2025. URL https://arxiv.org/abs/ 2505.14652. Mathematical Association of America. Aime problems and solutions. Art of Problem Solving Wiki, 2024. URL https://artofproblemsolving.com/wiki/ index.php/AIME_Problems_and_Solutions. mcaleste. Sat multiple choice math may 2023. URL https: Hugging Face Datasets, 2023. //huggingface.co/datasets/mcaleste/ sat_multiple_choice_math_may_23. Contains 32 math questions from May 2023 SAT. Accessed: 2025-01-27. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. Sheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Skalse, J., Howe, N. H. R., Krasheninnikov, D., and Krueger, D. Defining and characterizing reward hacking, 2025. URL https://arxiv.org/abs/2209.13085. Tang, Y., Wang, S., Madaan, L., and Munos, R. Beyond verifiable rewards: Scaling reinforcement learning for language models to unverifiable data, 2025. URL https: //arxiv.org/abs/2503.19618. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback, 2022. URL https: //arxiv.org/abs/2203.02155. Qwen, :, Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Tang, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report, 2025. URL https: //arxiv.org/abs/2412.15115. Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., and Finn, C. Direct preference optimization: Your language model is secretly reward model, 2024. URL https://arxiv.org/abs/2305.18290. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. Gpqa: graduate-level google-proof q&a benchmark, 2023. URL https://arxiv.org/abs/2311.12022. Schulman, J. Approximating KL divergence, 2020. URL http://joschu.net/blog/kl-approx.html. Blog post. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/ 1707.06347. Team, S. Sglang: serving framework fast large language models and vision language URL https://github.com/ for models, 2024. sgl-project/sglang. Accessed: 2025-01-23. Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., Ren, W., Arulraj, A., He, X., Jiang, Z., Li, T., Ku, M., Wang, K., Zhuang, A., Fan, R., Yue, X., and Chen, W. Mmlu-pro: more robust and challenging multitask language understanding benchmark, 2024. URL https://arxiv.org/abs/2406.01574. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-ofthought prompting elicits reasoning in large language models, 2023. URL https://arxiv.org/abs/ 2201.11903. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., Zheng, C., Liu, D., Zhou, F., Huang, F., Hu, F., Ge, H., Wei, H., Lin, H., Tang, J., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Zhou, J., Lin, J., Dang, K., Bao, K., Yang, K., Yu, L., Deng, L., Li, M., Xue, M., Li, M., Zhang, P., Wang, P., Zhu, Q., Men, R., Gao, R., Liu, S., Luo, S., Li, T., Tang, T., Yin, W., Ren, X., Wang, X., Zhang, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Zhang, Y., Wan, Y., Liu, Y., Wang, Z., Cui, Z., Zhang, Z., Zhou, Z., and Qiu, Z. Qwen3 technical report, 2025. URL https: //arxiv.org/abs/2505.09388. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., Liu, X., Lin, H., Lin, Z., Ma, B., Sheng, G., Tong, Y., Zhang, C., Zhang, M., Zhang, W., Zhu, H., Zhu, J., Chen, J., Chen, J., Wang, C., Yu, H., Song, Y., Wei, X., Zhou, H., Liu, J., Ma, W.-Y., Zhang, Y.-Q., Yan, L., Qiao, M., Wu, Y., and Wang, M. Coupled Variational Reinforcement Learning for Language Model General Reasoning Dapo: An open-source llm reinforcement learning system at scale, 2025a. URL https://arxiv.org/abs/ 2503.14476. Yu, T., Ji, B., Wang, S., Yao, S., Wang, Z., Cui, G., Yuan, L., Ding, N., Yao, Y., Liu, Z., Sun, M., and Chua, T.- S. Rlpr: Extrapolating rlvr to general domains without verifiers, 2025b. URL https://arxiv.org/abs/ 2506.18254. Yuan, W., Pang, R. Y., Cho, K., Li, X., Sukhbaatar, S., Xu, J., and Weston, J. Self-rewarding language models, 2025. URL https://arxiv.org/abs/2401.10020. Yue, X., Zheng, T., Zhang, G., and Chen, W. Mammoth2: Scaling instructions from the web, 2024. URL https: //arxiv.org/abs/2405.03548. Yue, Y., Yuan, Y., Yu, Q., Zuo, X., Zhu, R., Xu, W., Chen, J., Wang, C., Fan, T., Du, Z., Wei, X., Yu, X., Liu, G., Liu, J., Liu, L., Lin, H., Lin, Z., Ma, B., Zhang, C., Zhang, M., Zhang, W., Zhu, H., Zhang, R., Liu, X., Wang, M., Wu, Y., and Yan, L. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks, 2025. URL https://arxiv.org/abs/2504.05118. Zhang, B., Zhou, K., Wei, X., Zhao, W. X., Sha, J., Wang, S., and Wen, J.-R. Evaluating and improving toolaugmented computation-intensive math reasoning, 2023. URL https://arxiv.org/abs/2306.02408. Zhao, Y., Liu, Y., Liu, J., Chen, J., Wu, X., Hao, Y., Lv, T., Huang, S., Cui, L., Ye, Q., Wan, F., and Wei, F. Geometric-mean policy optimization, 2025. URL https://arxiv.org/abs/2507.20673. Zhou, X., Liu, Z., Sims, A., Wang, H., Pang, T., Li, C., Wang, L., Lin, M., and Du, C. Reinforcing general reasoning without verifiers, 2025a. URL https: //arxiv.org/abs/2505.21493. Zhou, X., Liu, Z., Wang, H., Du, C., Lin, M., Li, C., Wang, L., and Pang, T. Variational reasoning for language models, 2025b. URL https://arxiv.org/abs/2509. 22637. Zuo, Y., Zhang, K., Sheng, L., Qu, S., Cui, G., Zhu, X., Li, H., Zhang, Y., Long, X., Hua, E., Qi, B., Sun, Y., Ma, Z., Yuan, L., Ding, N., and Zhou, B. Ttrl: Test-time reinforcement learning, 2025. URL https://arxiv. org/abs/2504.16084. 11 Coupled Variational Reinforcement Learning for Language Model General Reasoning A. KL Divergence Estimator In this section, we provide an intuitive understanding of our KL divergence estimators and derive the specific forms used in our hybrid sampling strategy. Let = qψ(zx,y) pϕ(zx) denote the likelihood ratio between posterior and prior distributions. Our composite distribution becomes: p(zx, y) = 1 2 pϕ(zx) + 1 2 qψ(zx, y) = pϕ(zx) (cid:18) 1 2 + (cid:19) 1 2 We need to estimate DKL(p(zx, y)pϕ(zx)) using importance sampling from either the prior or posterior distribution. When sampling from the prior distribution: The importance sampling ratio is p(zx,y) becomes: pϕ(zx) = 1 2 + 1 2 r, and our KL estimator"
        },
        {
            "title": "Dprior",
            "content": "KL = (0.5 + 0.5r) ln (0.5 + 0.5r) (0.5r 0.5) When sampling from the posterior distribution: The importance sampling ratio is p(zx,y) and our KL estimator becomes: qψ(zx,y) = 0.5+0.5r = 0.5 + 0.5, Dposterior KL = (cid:18) 0.5 (cid:19) + 0.5 ln (0.5 + 0.5r) + (cid:18) 0.5 (cid:19) 0.5 Figure 6 shows the behavior of both estimators across different likelihood ratios. When > 1, the posterior assigns higher probability to the sample than the prior, indicating reasoning traces well-aligned with target answers. When 0 < < 1, the prior assigns higher probability, representing more exploratory reasoning paths. Our hybrid sampling strategy leverages this complementary behavior to ensure stable optimization across diverse reasoning patterns. Figure 6. Comparison of KL divergence estimators under different sampling strategies. Coupled Variational Reinforcement Learning for Language Model General Reasoning B. Comparison between Existing Approaches We compare the gradient estimators of various verifier-free reasoning methods: θJJLB = Ezπθ(zx) θJLaTRO = Ezπθ(zx) θJVeriFree = Ezπθ(zx) θJRLPR = Ezπθ(zx) (cid:2) log πθ(yx, z)θ log πθ(zx) + 1 θ log πθ(yx, z)(cid:3) πθ(zx) (cid:2)(log πθ(yx, z) β log πref(zx) (cid:2)πθ(yz, x)θ log πθ(zx) + πθ(yz, x)θ log πθ(yx, z)(cid:3) (cid:80)({piti y}) (cid:2) θ log πθ(y, zx)(cid:3) )θ log πθ(zx) + 1 θ log πθ(yx, z)(cid:3) (16) θJRAVR = Ezπθ(zx,y) (cid:2)R(z)θ log πθ(zx)(cid:3) + θR(z) DKL(p(zx, y)p(zx)), where R(z) = max(0, log πθ(yz, x) Ezπθ(zx)[log πθ(yx, z)]) All methods aim to improve reasoning without external verifiers but differ in their approach. JLB (Tang et al., 2025) uses log-probability as reward with fixed answer term weighting. LaTRO (Chen et al., 2024) incorporates KL regularization between policy and reference models. VeriFree (Zhou et al., 2025a) uses the probability instead of log-probability as reward and also uses it as the weight term for NLL loss. RLPR (Yu et al., 2025b) optimizes joint answer-reasoning probability using token-level probabilities as rewards. RAVR (Lin et al., 2025) samples from posterior distributions and optimizes the corresponding variational lower bound. The core differences among these methods lie in sampling distribution, reward design, and the weighting coefficients for the NLL loss terms. In contrast, our CoVRL introduces fundamentally different approach through hybrid sampling and composite distribution optimization. The gradient combines three key components: (1) importance-weighted policy gradient on the composite distribution p(zx, y) for efficient exploration-exploitation balance, (2) selective NLL loss applied only to high-quality samples (I[ ˆA > 0]), and (3) explicit KL regularization to ensure transferability from training to inference. JCoVRL = Ezphybrid(zx,y) (cid:2) p(zx, y) phybrid(zx, y) log pθ(yx, z) log p(zx, y) + I[ ˆA > 0] log pθ(yx, z)(cid:3) DKL(p(zx, y)pϕ(zx)) (17) CoVRL leverages both question-only and answer-guided sampling through phybrid(zx, y), enabling more effective training while maintaining inference compatibility. C. Evaluation Implementation For non-multiple-choice questions, we use the math-verify library to directly parse the standard answers. For multiple-choice questions, we standardize all datasets into unified multiple-choice format. When parsing model responses, we employ different strategies based on the question type: Non-multiple-choice questions: We first use the default parsing method of the math-verify library to extract results. If no parsing result is obtained, we wrap the answer with $$ to parse it as LaTeX expression. This approach handles cases where the model outputs <answer>latex expression</answer> without proper LaTeX delimiters. If there is still no valid parsing result, we perform exact string matching as fallback. Multiple-choice questions: When parsing standard answers, we include both the option labels (e.g., A, B, C, D) and the option content as potential matches, since the model sometimes outputs the option content instead of the option label. When parsing model outputs, we first attempt to match the option format. If option matching fails, we try to parse the response as LaTeX format. If LaTeX parsing also fails, we perform exact string matching detection. D. Training Dynamics of Different Sampling Ratios Figure 7 illustrates the distinct training dynamics across different sampling ratios. As shown in the figures, α = 0.9 achieves higher prior reward scores, while α = 0.1 obtains higher posterior reward scores. This aligns with expectations, as models trained with more prior sampling naturally perform better under prior evaluation conditions, while those trained with more 13 Coupled Variational Reinforcement Learning for Language Model General Reasoning (a) Prior Reward Score (b) Posterior Reward Score (c) Response Length Figure 7. Training dynamics comparison between different prior sampling probabilities (α = 0.1 vs α = 0.9). posterior sampling excel under posterior evaluation conditions. Additionally, response lengths increase with α = 0.1 but decrease with α = 0.9 during training. This shows that posterior-dominated sampling encourages longer reasoning chains guided by answer information, while prior-dominated sampling leads to shorter responses due to stronger KL regularization pressure. When prior sampling dominates, the model struggles to improve reconstruction rewards and instead favors minimizing KL divergence by generating shorter responses. In contrast, posterior guidance enables consistent reward improvements that incentivize more detailed reasoning. These dynamics provide additional evidence for the effectiveness of balanced sampling strategies. E. Influence of Loss Components We conduct comprehensive ablation studies to understand the impact of key components in our CoVRL framework, including regularization coefficients and reward function formulations. The results are presented in Table 4. Table 4. Ablation study on regularization coefficients and reward functions. General Mathematical Configuration GPQA MMLU-Pro TheoremQA AIME24 AQuA CARP-EN MATH-500 Minerva Avg@4 Avg@16 Avg@4 Avg@ Avg@1 Avg@2 Avg@2 Avg@4 SAT-Math Overall Avg@32 Regularization Coefficients Default λKL = 0.1 λN LL = 0.1 Reward Functions Log-Probability Probability Probability Sum Log-Probability Sum 30.4 31.3 31.2 30.4 31.4 28.1 31.5 46.5 32.5 42. 46.5 46.9 45.9 47.7 36.3 20.7 33.1 36.3 35.8 37.2 36.7 7.5 0.3 4.2 7.5 6.3 7.3 7.1 77.3 35.1 69. 77.3 78.2 74.8 76.8 65.1 33.8 49.6 65.1 65.3 65.0 65.1 66.3 18.1 59.5 66.3 67.6 68.7 67.1 25.5 11.9 23. 25.5 27.0 24.9 27.4 97.1 62.6 89.6 97.1 94.9 95.1 96.0 50.2 27.4 44.7 50.2 50.4 49.7 50.6 KL regularization The results show that reducing the KL divergence coefficient to 0.1 significantly degrades performance across all benchmarks, with overall accuracy dropping to 27.4%. This performance degradation stems from training instability when the KL regularization is insufficient. We observe substantial increases in KL divergence during training, indicating significant deviation between the prior and posterior distributions. This leads to both training-inference mismatch issues and training instability, as we are essentially performing off-policy optimization with an increasing distribution shift between training and inference. NLL loss In contrast, the model appears less sensitive to changes in the NLL loss coefficient. When reducing the NLL coefficient to 0.1, performance decreases moderately to 44.7%. We attribute this resilience to the fact that the RL term and NLL loss optimize essentially the same objective, both of which aim to improve answer prediction quality. The NLL loss primarily trains the models ability to summarize reasoning and produce final answers. Reward function formulation We examine different reward formulations for our variational framework, specifically focusing on (1) Length normalization: comparing averaging over sequence length versus unnormalized probability sums; 14 Coupled Variational Reinforcement Learning for Language Model General Reasoning and (2) Logarithmic transformation: examining whether to use log-probabilities or raw probabilities as reward signals. The results demonstrate that all reward formulations achieve remarkably similar overall performance, with variations of less than 1 percentage point (49.7% to 50.6%). This consistency indicates that our CoVRL framework is robust to various reward formulations."
        }
    ],
    "affiliations": [
        "Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences",
        "Xiaohongshu Inc."
    ]
}