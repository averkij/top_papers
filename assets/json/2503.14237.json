{
    "paper_title": "Make Your Training Flexible: Towards Deployment-Efficient Video Models",
    "authors": [
        "Chenting Wang",
        "Kunchang Li",
        "Tianxiang Jiang",
        "Xiangyu Zeng",
        "Yi Wang",
        "Limin Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Popular video training methods mainly operate on a fixed number of tokens sampled from a predetermined spatiotemporal grid, resulting in sub-optimal accuracy-computation trade-offs due to inherent video redundancy. They also lack adaptability to varying computational budgets for downstream tasks, hindering applications of the most competitive model in real-world scenes. We thus propose a new test setting, Token Optimization, for maximized input information across budgets, which optimizes the size-limited set of input tokens through token selection from more suitably sampled videos. To this end, we propose a novel augmentation tool termed Flux. By making the sampling grid flexible and leveraging token selection, it is easily adopted in most popular video training frameworks, boosting model robustness with nearly no additional cost. We integrate Flux in large-scale video pre-training, and the resulting FluxViT establishes new state-of-the-art results across extensive tasks at standard costs. Notably, with 1/4 tokens only, it can still match the performance of previous state-of-the-art models with Token Optimization, yielding nearly 90\\% savings. All models and data are available at https://github.com/OpenGVLab/FluxViT."
        },
        {
            "title": "Start",
            "content": "Make Your Training Flexible: Towards Deployment-Efficient Video Models Chenting Wang1,2 Kunchang Li2,3 Tianxiang Jiang2,4 Xiangyu Zeng2,5 Yi Wang2 Limin Wang2,5 1Shanghai Jiao Tong University 4University of Science and Technology of China 2Shanghai AI Laboratory 3University of Chinese Academy of Sciences 5State Key Laboratory for Novel Software Technology, Nanjing University 5 2 0 2 8 1 ] . [ 1 7 3 2 4 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Popular video training methods mainly operate on fixed number of tokens sampled from predetermined spatiotemporal grid, resulting in sub-optimal accuracy-computation trade-offs due to inherent video redundancy. They also lack adaptability to varying computational budgets for downstream tasks, hindering applications of the most competitive model in real-world scenes. We thus propose new test setting, Token Optimization, for maximized input information across budgets, which optimizes the size-limited set of input tokens through token selection from more suitably sampled videos. To this end, we propose novel augmentation tool termed Flux. By making the sampling grid flexible and leveraging token selection, it is easily adopted in most popular video training frameworks, boosting model robustness with nearly no additional cost. We integrate Flux in large-scale video pre-training, and the resulting FluxViT establishes new state-of-the-art results across extensive tasks at standard costs. Notably, with 1/4 tokens only, it can still match the performance of previous stateof-the-art models with Token Optimization, yielding nearly 90% savings. All models and data are available at https: //github.com/OpenGVLab/FluxViT. 1. Introduction Video representation learning [22, 50, 81] is fundamental task in computer vision, as it is critical in developing spatiotemporal perception and procedural reasoning in multimodal large language models [43, 46, 62, 87] and embodied AI systems [27, 88]. However, the deployment of video models is computationally intensive due to the high volume of video tokens compared to image tokens. Popular video training methods work on fixed-count patched tokens sampled from predefined spatiotemporal grid (e.g., 82242), leading to considerable redundancy in training and deployCorresponding authors. Figure 1. Flux (right) employs flexible sampling and token selection to achieve Token Optimization. Common methods(left) use rigid sampling(and use token reduction for applications directly). ment, especially for long, high-resolution videos. Moreover, this inflexible and fixed training achieves sub-optimal results for tasks that require dynamic resolution settings, including restricted computation or emphasizing frame counts rather than frame resolution. Researchers have developed different approaches for better trade-offs. First, to fit limited computation budget in inference, many works [16, 35, 67] use token reduction on the same densely sampled tokens as in training, showing sub-optimal performance. For one thing, there is trade-off between the complexity (cost) and performance of the token reduction strategy. Current strategies can not perform well with significant reduction rate. For another, the inflexibly trained models can not generalize with the sparse, masked tokens. Second, existing methods for flexible network training, such as Resformer [75] and FFN [98], have demonstrated the efficacy of flexible networks operating at different spatial or temporal resolutions but not both simultaneously. Although these approaches achieve robust performance across diverse settings, we contend that merely reducing frame counts or resolution under computational constraints fails to optimize token capacity utilization and remains suscep1 tible to redundancy. Notably, FFN exhibits substantial performance degradation when operating with only two frames in their research. We argue that the input token set with size equal to 2 frames can be optimized. Also, they are not validated in large-scale pre-training to achieve competitive results for real-world applications, and the performance gain with larger resolution input comes with quadratic computation cost increase, which is also insufficient. We propose new perspective of the desired optimal computation-and-accuracy and spatial-and-temporal tradeoffs across various settings, termed Token Optimization (TO in short), as shown in Figure 1. It involves selecting an optimized set of input tokens based on budget from more properly sampled videos for information maximization. Since there is no perfect token selector for all settings, we promote better solution of using flexible sampling, with more densely and finely sampled videos for higher computation and more sparsely sampled videos for lower budget. Spatial and temporal trade-offs are also achieved. To better achieve TO, we propose new data augmentation tool for training flexible video models, termed Flux for Flexibility in universal contexts, by utilizing the combination of flexible sampling and token selector to make training flexible while not introducing costs. We integrate it with the Unmasked Teacher [47] pre-training, supervised-training, multi-modal contrastive training, and chat-centric training to validate its efficiency. Our new recipe brings more robustness caused by Flux augmentation both in standard settings and our TO setting. Two plug-in modules are used for better feature extraction regarding the reduced token set with flexible reduction rates and input spatiotemporal resolutions. We build bottom-up ablation studies in turning the model flexible. The resulting model FluxViT achieves supreme improvements regarding the currently state-of-theart InternVideo2-Dist series models in fair settings across budgets and across tasks, as in Figure 2. Our FluxViT-S outperforms the previous state-of-theart small-scale model, InternVideo2-S [87], by 2.2% on K400 under standard computation constraints. Moreover, it achieves comparable performance to InternVideo2-S while utilizing only about 10% of inference cost. FluxViTB achieves competitive results with state-of-the-art much larger scale models on various tasks, including scenebased action recognition task [38] (90.0% top-1 accuracy on K400), motion-intensive [26] (75.8% top-1 accuracy on SSv2), long-term task [74] (94.1% top-1 accuracy on COIN), zero-shot text-to-video retrieval [1, 13, 31, 69, 91] (49.9% R@1 on MSRVTT, 53.5% R@1 on DiDeMo and 56.7% R@1 on ActivityNet). Notably, it maintains stateof-the-art performance among the similar scaled models while using only 1/4 to 1/2 of their tokens, resulting in 70% to 95% savings in computation. On chat centric tasks including MVbench [46] for general perception and Figure 2. Overview of our Flux method. The same-scaled FluxViT and InternVideo2 [87] series models are both pre-trained with the InternVideo2-1b model as the teacher using the same dataset. The FluxViT+ refers to the results using Token Optimization at test time with the same GFLOPS. Dream-1k [80] for fine-grained caption, our model surpasses widely used SigLIP, CLIP and UMT in linear prob setting across computation budgets, where only the projector between the ViT and LLM is trained for comparability regarding the ViTs capability as the common stage-1 training in chat models. This highlights Fluxs strong effect across computation constraints and tasks and underscores the robustness afforded by such flexibility. 2. Related Work Flexible Model Training. Many recent studies focus on developing method to train single model that performs well across various settings, facilitating seamless adaptation to diverse downstream deployment scenarios. These methods include training model as set of shared weights to be pruned for specific tasks [8, 94], as well as optimizing models for varied spatial resolutions [75], frame counts [98], spatiotemporal resolutions [54, 57, 83, 90], patch sizes [6], and representation dimensions [9, 41]. The Resformer [75] and FFN [98] methods propose co-training with three different resolution settings in single batch, utilizing self-distillation mechanisms to enhance performance under limited input resolutions for spatial and temporal dimensions, although separately. We argue that they cant achieve optimal performance within constrained computa2 Figure 3. Our proposed Flux method with UMT framework. We show that our proposed Flux training is easy to integrate with mainstream video training frameworks. tional budgets. Simply reducing frame counts and resolutions is suboptimal. Further, performance gains brought by high-resolution inputs lead to significant cost increases, challenge also noted in Oryx [57]. Token Optimization addresses this by optimizing the set of input tokens anytime. Masked Vision Modeling. Mask has always been seen as critical method in training and utilizing scaled vision models, especially in the video domain. Previous methods can be mainly divided into two categories. First, drawing insights from the masked language modeling [18, 19, 60], researchers use masks in vision transformer [20] pre-training as data augmentation strategy, as seen in masked autoencoder frameworks [23, 30, 76, 82] and BERT-like maskthen-predict frameworks [4, 66, 84, 100]. Works like UMT [47], MVP [89], and Milan [32] further utilize such augmentation strategy to predict the CLIP [68] models feature instead of native pixels as Teacher-Student pretraining method. Second, methods like MAR [67] and MCM [35] apply masking techniques during the fine-tuning stage and inference to save computation with minimal performance loss. We here propose that mask can be used as novel data-augmentation tool with our flexible sampling to boost performance with our Token Optimization idea. 3. Method In this section, we introduce our Flux for effectively pretraining video models that represent videos at varying resolution scales using tokens with flexible quantities robustly. 3.1. Flux-Pretraining We begin with widely adopted pre-training paradigm in which the representations of teacher model are employed as soft labels for student model. Popular approaches following this typically rely on fixed sampling grids and utilize all extracted visual tokens for both the teacher and student models. However, this methodology suffers from substantial redundancy, particularly in video data at finer resolutions. To address this, Unmasked Teacher (UMT) [47] proposes using only subset of the visual tokens for the student Figure 4. Our proposed essential modules for Flux. From the model side, Flux modules include Group-dynamic token selector, dual patch norm, and Global-Local positional embedding. while retaining the full input for the teacher. This masked alignment strategy enhances both efficiency and robustness for the student model. We propose further examination of the input tokens for the teacher and leverage flexible sampling grids combined with group-dynamic token selection mechanism. This enables us to select the same number of tokens as the original setting but from larger pool extracted at flexibly higher spatiotemporal resolutions that are more informative, thus harvesting more diversified and meaningful representations from the teacher without cost, as shown in Figure 3. The other settings, like alignment losses, are the same as the original UMT, which trains the competitive InternVideo2-Series models [87]. Flexi-Sampling. For each video in batch, we randomly select frame count ranging from Fmin to Fmax with temporal stride ts and select spatial resolution between Rmin and Rmax with spatial stride rs and enforce threshold Tthres in keeping reasonable size of visual token pool. Double Mask Module. To make the training flexible while not introducing new costs, we leverage token selection in the teacher input side. We propose simple but effective method that prioritizes tokens exhibiting significant changes between frames. We use additional sparse groups for this selection to ensure coverage of the full video and resist several rapid changes in videos, termed Group-Dynamic Selector. Formally, with pre-defined token count K, which is set to keep unchanged teacher forward cost, and given tokenized frame sequence {F1, F2, ..., FT } by teachers patch embedding layer, we uniformly sample it into sparse groups where Bi = {Ft [iB, (i + 1)B 1)}, [0, ). In each group, we compute the dynamic value as D(Ft+1,i) = Ft+1,i Ft,ip, where Ft,i means the value of ith spatial token in the tth frame and means the p-dim distance. Then we take the top( ) tokens with the highest dynamic values in each group to form visible tokens in total. This selector, along with flexible sampling, is utilized for the teacher model. We also employ another student mask based on the teachers attention score of CLS token adopted in Unmasked Teacher. The 3 usage of the two masks is referred to as the Double Mask Module. Such module greatly boosts the student models performance and brings no additional cost, showing natural integration of Flux in the UMT framework. 3.2. Flux-Tuning For the stage of supervised tuning, we keep the flexisampling method and group-dynamic selector the trained student model. Table 1 shows such methods gain even on competitive model pre-trained with standard setting, showing Flux as general augmentation tool. for Token Optimization As proposed in the introduction, we hope to maximize token information under any given budget, termed as Token Optimization at downstream tasks. Thus, during evaluation, we use subset of the train-set to find optimized sampling under given token count for the task. We provide heuristic searching method in the Appendix, which takes acceptable overhead. 3.3. FluxViT Conventional ViT architecture struggles with the variable token numbers, flexible spatiotemporal resolution, and sparsity patterns inherent in Flux training. Thus, we propose two useful plug-in augmentation modules: Global-Local Positional Embedding (GLPE) and Dual Patch Normalization (DPN), as shown in Figure 4. Global-Local Positional Embedding. As we utilize flexible sampling with various spatiotemporal resolutions and token reduction rates, it is important to encode the position information of the selected input tokens about where they are from and how they are related. Thus, Global-Local Positional Embedding (GLPE) is utilized. We enhance the global learnable positional embedding, initialized using the sine-cosine method for the maximum possible input size, by applying Depth-Wise Convolution [33]. Then, to encode local positional information at finer granularity in the position-invariant attention mechanism, we apply Linear Projection function (LPE) to the value vector V. The modified attention is formulated as: = (Softmax( QKT ) + LP E)V. (1) Dual Patch Normalization. The Patch Embedding layer plays crucial role in dynamics estimation and it must be robust to various spatiotemporal resolution distributions. We observed that an additional Layer Normalization after the standard Patch Embedding improves the accuracy of dynamic estimation, leading to more effective token selection. Furthermore, the flexible sampling strategy leads to varied distribution of input tokens. Consequently, we find the gradient norm of the Patch Embedding layer can become excessively large, as shown in the Appendix. To stabilize training, we introduce second Layer-Normalization layer before the Patch Embedding operation. 3.4. Train with Varied Input Lengths As we aim to train flexible video model that can seamlessly adjust to varied token numbers according to budgets, we propose to use multi-number co-training approach. For Flux-UMT frameworks, we suggest utilizing three distinct input numbers within single batch for the student model. This strategy maximizes the benefits of the extensive teacher-forward computation while enhancing robustness under varying conditions. Specifically, we forward the student three times using three different token numbers and compute the teacher-student alignment loss for each. In standard training frameworks without pre-trained teacher, we incorporate additional self-distillation, using the final aggregated features with the largest input as the teacher for the intermediate and the intermediate as the teacher for the smallest. We include training efficiency in the ablation. 4. Experiments We leverage InternVideo2-1B [87] as the teacher in FluxUMT due to its strong performance in general video understanding tasks. We use the same data-decomposition and similar training recipe as the currently most competitive InternVideo2-series distilled models as baselines. Note that they are also built using the Unmasked Teacher framework under the supervision of InternVideo2-1B. For notation simplification, we use Flux-Single for Flux training with single token count and Flux-Multi for Flux training with multi token counts, while Flux-PT is for Flux with UMT pretraining and Flux-FT for supervised tuning. 4.1. Ablation Study Such LPE can be generated regardless of the input token number by adding bias term for the permutation-invariant attention. This is vital in Flux to encode the discrete position information. The Value-dependent nature of LPE is more robust in Flux than others, like RoPE, in that the unmasked tokens are from different places from the source. The encoding of the relative relations between them is naturally Value-dependent. RoPE is more useful in scenarios without token selection for length extension. In the ablation study, we ablate the effect of our Flux through systematic approach, progressively transforming the original UMT framework into our Flux-UMT and getting our competitive FluxViT. We utilize K710 for pretraining, which integrates the K400 [38], K600 [10], and K700 [11] datasets with duplicates removed. Performance is evaluated using scene-based K400 and motion-intensive SSv2 [26]. We use 1 clip 1 crop for test configurations. For flexible sampling, we set the parameters as follows: 4 Method Input Size Test Token Number 3072 2048 1024 512 Avg 82242 Direct Tuned 2048 fixed count Flux-Single Tuned 122242 Direct Tuned (3072, 2048, 1024) Flux-Multi Tuned 69.5 69.5 22242 42242 80.5 73.6 77.1 82242 83.9 81.9 72.7 79.5 122242 84.6 84.2 80.9 69.8 79.9 162242 84.5 83.5 79.1 67.0 78.5 202242 84.1 82.7 77.7 64.3 77.2 242242 83.6 82.1 76.4 62.3 76.1 Avg 84.2 83.3 79.4 68.5 Max 84.6 84.3 81.9 73.6 68.1 68.1 22242 42242 79.9 74.7 77.3 82242 84.3 82.7 76.7 81.2 122242 85.1 85.1 82.8 75.8 82.2 162242 85.4 85.2 82.6 75.3 82.1 202242 85.6 85.1 82.2 74.6 81.9 242242 85.4 84.8 82.0 74.1 81.6 Avg 85.4 84.9 82.0 74.2 85.6 85.2 82.8 76.7 1.0 0.9 0.9 3.1 Max - - - - - 69.3 69.3 22242 42242 80.5 72.8 76.7 82242 83.9 81.9 72.7 79.5 122242 85.0 84.3 80.9 67.8 79.4 162242 84.9 83.9 78.9 65.2 78.2 202242 84.7 83.4 77.6 62.4 77.0 242242 84.3 82.6 76.4 60.4 75.9 Avg 84.7 83.6 79.4 67.2 Max 85.0 84.3 81.9 72.8 72.2 72.2 22242 42242 81.0 79.3 80.2 82242 84.4 82.8 80.3 82.5 122242 85.4 85.2 83.3 79.9 83.5 162242 85.7 85.1 83.5 79.2 83.4 202242 85.7 85.3 83.0 78.9 83.2 242242 85.6 85.0 82.7 78.2 82.9 Avg 85.6 85.0 82.7 78.3 85.7 85.3 83.5 80.3 0.7 1.0 1.6 7.5 Max - - - - - Table 1. Directly use Flux-Tuning with the previsou SOTA InternVideo2-S on K400. Results are reported on K400 using 1clip1crop. It shows that Flux can be used as an advanced augmentation tool directly in supervised tuning scenario. Fmin=4, Fmax=24, ts=2, Rmin=168, Rmax=252, rs=28 and Tthres=(2048, 4096) by default. Flux-Single Pre-training and Tuning. Figure 5 illustrates the performance of three models differing in their pretraining and tuning approaches. The first model, termed the FixRes Distilled FixRes Tuned model, employs fixed resolution of 82242 during both the pre-training stage and fine-tuning stage, the same as the original setting to get InternVideo2-S. We evaluate this tuned model using fixed input token count of 2048 by our token-selector on the K400 dataset, which corresponds to the number of input tokens in the standard 82242 configuration, but with varied frame counts to assess the models capability to leverage additional temporal information in fixed-number tokens for 5 Figure 5. Comparison between different training methods on K400 using fixed number of 2048 tokens. Note the three lines and all the points share similar training and inference costs. The shaded part shows results for the AnyRes Distilled AnyRes Tuned model with spatial resolution in range (196, 252), while others use fixed spatial resolution at 224. K400 w/ TO Mask Type (single res) 78.4 78.6 Random 78.8 Tube 78.7 Dynamic(L1) Dynamic(L2) 78.8 Group-Dynamic(L2)-2 78.8 Group-Dynamic(L2)-4 79. SSv2 w/ TO 78.4 65.4 79.0 (0.6) 65.3 80.0 (1.6) 65.7 79.8 (1.4) 65.7 80.0 (1.6) 65.8 80.2 (1.8) 66.0 80.3 (1.9) 66.3 65.4 65.9 (0.5) 66.7 (1.3) 66.6 (1.2) 66.7 (1.3) 67.0 (1.6) 67.3 (1.9) Table 2. Ablation on using token selection strategies We validate the effect of different token selection methods on Flux-SinglePre-training and Tuning on ViT. group size of 4 works well. enhanced performance. Thus, models trained and tuned with fixed sampling grid can hardly harvest costless performance gain with Token Optimization. In contrast, the AnyRes Distilled FixRes Tuned model utilizes Flux-UMT-Single upon the original setting. Through Flux token selection, we maintain consistent pretraining computational costs by regulating fixed quota of 2048 input tokens. The tuning method also remains unchanged. The diversified features from the teacher by the flexible sampling accumulate more valued representations for the student, gaining +0.7% improvement at the original 82242 test setting. Notably, the new model exhibits greater robustness in processing fixed number of tokens from larger spatiotemporal resolution, achieving another +0.3% gain with Token Optimization costlessly. We further test tuning the any-res trained model using Flux-FT with fixed input token number of 2048 on K400, as termed AnyRes Distilled AnyRes Tuned in the figure. As shown, this tuning method can harvest +1.9% top-1 accuracy gain on K400 compared with the normal distilled and normal tuned method while nearly bringing no computation increment throughout pre-training and fine-tuning. Method & Arch Vanilla + ViT Vanilla + FluxViT Flux-Single + ViT With new positional embeddings K400 w/ TO 78.4 79.3 79.2 SSv2 w/ TO 78.4 65.4 79.6 (1.2) 66.0 80.3 (1.9) 66.3 65.4 66.4 (1.0) 67.3 (1.9) w/ RoPE w/ GPE w/ LPE w/ GLPE"
        },
        {
            "title": "With DPN",
            "content": "w/ DPN 79.5 79.4 79.7 79.9 80.7 (0.4) 66.5 80.5 (0.2) 66.4 81.0 (0.7) 66.8 81.3 (1.0) 67.0 67.5 (0.2) 67.4 (0.1) 68.3 (1.0) 68.6 (1.3) 79.8 81.2 (0.9) 66. 68.4 (1.1) Combining the two modules Flux-Single + FluxViT 80.5 Flux-Multi + FluxViT 81.4 81.7 (3.3) 67.6 82.4 (4.0) 68.4 69.3 (3.9) 70.0 (4.6) Table 3. Results with 82242(and TO w/ 2048 tokens, which is the same token count as in 82242) on K400 and SSv2. () for absolute improvement and () for relative gain. Vanilla means PT and FT without Flux augmentation."
        },
        {
            "title": "Settings",
            "content": "Baseline Change Tthres to (2048, 6144) w/o varied spatial resolution enlarge Fmax to 32 enlarge Resmax to 336 82242 2048 + TO K400 SSv2 K400 SSv2 69.3 80.5 68.7 80.1 80.5 69.1 69.1 80.4 69.0 80.2 81.7 81.2 81.4 81.6 81.4 67.6 67.2 67.7 67.4 67.3 Table 4. Ablations on hyper-parameter on sampling designs for Flux-Single training with FluxViT. Most settings regarding flexible sampling cause only minor influences except Tthres. Pre-training Length Fine-tuning Length (w/o Flux) Single Single Multi Multi Multi Single Multi(w/ align) Single Multi Multi(w/ align)"
        },
        {
            "title": "Test Length",
            "content": "2048 1024 74.7 79.3 77.4 80.5 79.0 80.3 78.8 81.0 79.1 80.9 80.3 81.4 512 62.1 65.8 75.0 70.2 73.3 76.6 Table 5. Ablation on using varied input lengths in training FluxViT on K400. Results are tested with fixed 82242 input but varied test token lengths based on our selector. Table 1 further shows that only combining Flux-Tuning and Token Optimization on competitive model pre-trained with standard setting can achieve non-trivial gain, highlighting that the sampling and selection module in Flux can effectively serve as novel augmentation tools. Token Selection Module. We test four token selection methods for Flux-Single, including random, tube, dynamic (as also directly used in [35]) and our group-dynamic method. We further ablate the token dynamics measurement and the sparse group size. Table 2 shows that our group-dynamic strategy with token dynamics measured by Extra Data - IN-21K - JFT-3B+SMI Model TimeSformer-L [5] VideoSwin-L [56] VideoMAE-L [76] CoVeR-L [97] UniFormerV2-L [44] CLIP-400M+K710 UMT-L [47] VideoMAE2-H [82] UnlabeledHybrid ViViT-H [2] MTV-H [92] CoCa-G [95] MViTv1-B [21] MViTv2-B [49] ST-MAE-B [23] VideoMAE-B [76] - VideoSwin-B [56] UniFormer-B [45] UMT-B [47] InternVideo2-B [87] K710+MASH 121 23803 197 60412 305 395821 431 58603 354 125506 431 58603 633 119215 654 398112 JFT-300M IN-21K+WTS-60M 1000+ 613012 JFT-3B+ALIGN-1.8B 1000+ N/A12 705 - 37 2555 37 - 87 18021 K600 87 18015 88 28212 50 25912 87 18012 96 44012 #P(M) GFLOPs Top-1 80.7 83.1 86.1 87.1 90.0 90.6 88.6 84.9 89.9 88.9 80.2 81.2 81.3 81.5 82.7 83.0 87.4 88.4 IN-21k IN-1k K710 FluxViT-Be - FluxViT-Be100 K710+MASH IN-1k UniFormer-S [45] MViTv2-S [49] - VideoMAE-S [76] - VideoMAE2-S [82] - InternVideo2-S [87] K710+MASH FluxViT-Se200 - FluxViT-Se100 K710+MASH 97 97 44012 88.7 89.4 4912 84.0 86.7 44012 89.6 90.0 25512 89.3 89.7 10812 87.3 88.9 4912 84.7 87.4 424 21 645 35 5715 22 5715 22 23 15412 80.8 81.0 79.0 83.7 85. 24 24 15412 86.4 87.3 1312 79.7 84.0 15412 87.7 88.0 8312 87.3 87.7 3212 84.7 86.6 1312 80.1 84.7 Table 6. Comparison with the state-of-the-art methods with on scene-related Kinetics-400. #P is short for the number of parameters. The blue values of FluxViT show results using larger spatiotemporal resolutions but keeping fixed input token count to 3072, 2048, 1024, and 512 respectively, corresponding to the four GFLOPs listed. SMI is short for the train set of SSv2, MiT and ImageNet and MASH for MiT, ANet, SSv2 and HACS. L2-distance of tokens within adjacent frames in sparsely divided four groups is the most robust among the tokenselection methods. More advanced token selectors like Token Merging [7] and Vid-TLDR [16] can be utilized for better results but with increased complexity, cost, and tedious hyperparameters. We provide experiments using VidTLDR in the supplement material and advocate the current selector due to its simplicity. FluxViT Modules. Table 3 analyzes the impact of our proposed plug-in modules. The inclusion of Global-Local positional embedding can strengthen the models robustness when processing sparse tokens derived from larger spatiotemporal resolutions. We compare our GLPE with ROPE [72] within our Flux scheme and show better results with LPE illustrated in the Method section. Furthermore, 6 Model TimeSformer-L [5] MViTv1-B [21] MViTv2-B [49] VideoMAE-B [76] VideoMAE-L [76] UniFormerV2-B [44] CLIP-400M UniFormerV2-L [44] CLIP-400M UMT-B [47] InternVideo2-B [87] K710+MASH Extra Data GFLOPs Top-1 62.3 IN-21k 67.7 K400 70.5 K400 69.7 K400 74.0 K400 70.7 73.0 70.8 73.5 K710 FluxViT-B K710+MASH UniFormer-S [45] VideoMAE-S [76] InternVideo2-S [87] K710+MASH IN-1K K600 FluxViT-S K710+MASH Top-5 - 70.9 92.7 92.3 94.6 93.2 94.5 92.4 94.4 23803 4553 2553 1806 5966 3753 17183 1806 2536 4406 75.3 75.6 95.1 95.1 2556 75.1 75.5 94.9 95.1 1086 72.0 75.1 93.3 94.8 496 56.8 73.9 84.8 94.4 423 576 836 1546 73.4 73.8 94.1 94.1 836 72.9 73.4 94.0 94.1 326 70.0 72.5 93.4 93.8 136 55.3 70.9 83.7 93. 67.7 66.8 71.5 91.4 90.3 93.4 Table 7. Comparison with the state-of-the-art methods with on motion-intensive SSv2. Our model achieves far better results. adding dual patch norm module optimizes the performance gains in TO. Combining the two modules achieves +3.3% and +3.9% on K400 and SSv2 respectively with TO. Hyper-parameters of flexi-sampling Table 4 gives ablation studies on the results of different hyper-parameters regarding sampling. Most settings regarding flexible sampling cause only minor influences except Tthres. proper scale of the flexible space is the most powerful. Flux-Multi training. We compare our proposed method of co-training with three input numbers(2048, 1024, 512) in both pretraining and tuning in Table 5 and also validate the tuning effects on well-pretrained InternVideo2-S in table 1(3072, 2048, 1024). As shown, pre-training with multiple token numbers can not only boost performance under standard settings but also increase model consistency in input token sparsity. Tuning with different token numbers with smoothed-L1-loss based self-distillation mechanism can further enhance such alignment consistently. As the multi-token-number training brings additional computation, we report the efficiency here (i) Time: For ablation setting, Flux-Single-UMT takes 15.5 hours using 32 A100 (ii) GPU memory: with per-gpu and Multi takes 20.3. batch size of 32, Flux-Single takes 44GB GPU memory while Multi takes 70GB. Flux-Multi achieves far better results at varied token counts, with acceptable overhead, as it best leverages the heavy teacher-forward process. 4.2. Single Modality Results We scale up the training data using the K-MASH[87] dataset of 1.1 million samples, aligning with the data employed in the Internvideo2-Distilled series models, includ7 e2e BackBone Model Distant Supervision [53] TimeSformer ViS4mer [36] Turbof 32 [28] VideoMambaf 64 [48] VideoMambaf 64 [48] InternVideo2f 12 [87] MA-LMM [29] HERMES [37] FluxViT3072 FluxViT2048 FluxViT1024 FluxViT3072 FluxViT2048 FluxViT1024 Top-1 90.0 Swin-B 88.4 VideoMAE-B 87.5 VideoMamba-S 88.7 VideoMamba-M 90.4 InternVideo2-S 90.0 MLLM 93.2 MLLM 93.5 FluxViT-S 91.8 92.1 FluxViT-S 91.5 91.9 FluxViT-S 89.8 91.0 FluxViT-B 93.9 94.1 FluxViT-B 93.7 93.9 FluxViT-B 92.5 93. Table 8. Comparison with the state-of-the-art on long-form video classification COIN dataset. We report the results based on our preset token number, with the left line using unmasked 12, 8, 4 frames, and 224 spatial resolution while the blue values show results that can be achieved using more informative tokens. ing K710, SSv2[26], ANet[31], HACS[99], and MiT[64]. We train the FluxViT model using total batch size of 2048 for 100 epochs. FluxViTs performance is validated using scene-based K400, motion-intensive SSv2, and long-term COIN with Token Optimization. K400. Table 6 reports our results compared with the previous solutions on K400. We set 3072, 2048, and 1024 as our token numbers in Flux-Multi-Tuning. For our FluxViT-S, we report 88.0%(+2.2%) on K400 compared with InternVideo2-S, which is the previous SOTA solution, while still achieving 84.7% with nearly the same computation as the lightweight network UniFormer [45] but with competitive accuracy. For fair comparison without K-MASH, we pretrain FluxViT using only K400 in FluxUMT, which also shows much better results. SSv2. Table 7 shows our models performance in dealing with motion-intensive video understanding tasks. Our FluxViT-S and FluxViT-B models set new state-of-the-art performance on SSv2 with either standard or limited computation cost. For FluxViT-B, we achieve 75.6% with standard computation while still achieving 75.1% with only 25% cost of the standard setting. Previous work [98] instead gets huge performance drop using 2 frames, which corresponds to the least computation listed. COIN. Table 8 shows our models performance in dealing with long-form video understanding tasks. Our FluxViT-S and FluxViT-B models set new state-of-the-art performance on COIN with either standard or limited computation cost. 4.3. Multi Modality Retrieval Results We use the pre-trained FluxViT with the CLIP [68] framework and our Flux method to train video clip model us-"
        },
        {
            "title": "MSR DDM ANet LSMDC MSVD",
            "content": "- - - - Model Internvideo2-S2048 [87] 35.6 33.7 34.5 18.7 20.2 Frozen-B [3] 25.9 23.5 VIOLET-B [24] Singularity-B [42] 34.0 37.1 30.6 34.6 33.3 OmniVL-B [79] CLIP4Clip-B [61] 30.6 - UMT-B [47] 35.2 41.2 35.5 Internvideo2-B2048 [87] 40.3 40.3 41.5 32.0 36.9 30.9 VINDLU-L [15] 40.7 31.5 30.7 InternVideo-L [85] UMT-L [47] 40.7 48.6 41.9 42.4 18.4 15.1 ViClip-L [86] InternVideo2-L [87] 42.1 42.8 43.6 LanguageBind-L [101] 42.8 39.7 38.4 LanguageBind-H [101] 44.8 39.9 41.0 VideoCoCa-G [93] 34.5 52.7 VideoPrism-G [93] VAST-G [14] - - - 34.3 39.7 49.3 55.5 44.4 48.3 52.4 45.0 49.3 52.4 42.2 45.4 47.2 44.5 49.0 50.3 36.8 38.5 38.2 40.5 45.8 44.7 49.8 52.2 56.6 49.9 53.5 56.7 48.0 48.8 51.8 49.1 53.0 54.8 42.6 42.9 42.8 47.2 49.8 50.3 FluxViT-S FluxViT-S1024 FluxViT-S512 FluxViT-B2048 FluxViT-B1024 FluxViT-B512 14.7 - - - - 13.6 19.1 18.7 - 17.6 24.9 20.1 21.4 - - - - - 20.8 22.4 18.7 20.5 17.7 19.0 23.7 25.4 22.6 24.1 20.1 22. 41.8 - - - - 36.2 42.3 49.1 - 43.4 49.0 49.1 - 54.1 53.9 - - - 49.4 49.7 48.1 48.5 45.5 46.9 53.8 54.2 52.8 53.4 50.7 52.1 Zero-shot text-to-video retrieval on MSRVTT Table 9. (MSR), DiDeMo (DDM), AcitivityNet (ANet), LSMDC, and MSVD. We only report the R@1 accuracy. The upper line regarding FluxViT shows results with non-masked 82242, 42242 and 22242 input setting as indicated by the token count while each lower bold line shows results further using more informative tokens. We employ Dual Softmax Loss for the results. ing 27M corpus, far lower than most baseline models, including 25M coarse level caption data: Webvid10M [3], CC3M [70], COCO [51], VG [39], SBU [65], CC12M [12] and 2M high-quality data: S-MiT [63], InternVid-2MRecap [86]. We leverage MobileClip-B [77] as the text encoder and only use the vanilla VTC loss, which is short for Video-Text Contrastive loss in training. We only unfreeze the ViT projector for the first stage using the 25M coarse caption data for 3 epochs. Then we unfreeze all the modules for the second stage using the 2M high-quality data for one epoch to fully boost the ViTs capacity. By default, we train the clip model with batch size of 4096. Also, we train the clip model with three input token numbers, 2048, 1024, and 512, and utilize smoothed-L1-loss on the final aggregated vision features to perform self-distillation and compute contrastive loss for each vision feature of the three 8 #Tokens w/ TO MVbench Dream1k-F1 Encoder 8256 Clip-L [68] 8576 SigLIP336-L [96] InternVideo2-L [87] 8224 4576 SigLIP336-L [96] 4256 UMT-L [47] 8256 4256 2256 2048 1024 512 28.4 29.2 28.7 25.4 24.6 29.0 27.9 25.6 29.5 (0.5) 28.5 (0.6) 27.5 (1.9) 45.6 46.7 47.0 44.5 45.0 48.3 46.9 46.0 49.0 (0.7) 47.7 (0.8) 47.6 (1.6) FluxViT-L FluxViT-L Table 10. Results on Chat-Centric benchmarks MVbench (General perception) and Dream1k(Fine-grained caption). Models are trained in multimodal linear prob setting where both the Encoder and the LLM are frozen. #Tokens for the number of visual tokens by the vision encoder. numbers. Table 9 indicates that the clip model trained with our FluxViT method outperforms the Internvideo2-Series model with large margin and further surpasses most of the top-performing models in Large or even Giant scale. Results on zero-shot action recognition results are included in Appendix for full validation of FluxViT-CLIP. 4.4. Chat-Centric Evaluation Results We scale the pre-trained model size to FluxViT-Large and evaluate its performance under the VideoChat framework. We employ single-layer MLP projector between ViT and an LLM(Qwen2-7B[83]) and train only the projector to ensure an unbiased evaluation of the ViTs performance, termed linear probe setting in multimodal scenarios. Such probing strategy is widely used in the Stage-1 training of multi-modal chat models. We utilize large-scaled trainset including LLava-558K[55], S-MiT, 700k filtered subset of WebVid-10M, VidLN[78], and SSv2-open-ended. We then compare the models performance on two benchmarks: MVbench, which assesses general spatiotemporal perception capabilities, and Dream1k, which evaluates the models fine-grained ability to generate detailed captions. The other models tested include CLIP-L, SigLIP-L, and UMT-L, which are widely used in Video MLLMs. Given that ViT and LLM remain frozen, we maintain the original input settings as their pretraining phases to ensure comparability. We will scale the corpus size and fully tune the chat model for full performance in future work. 5. Conclusion and Future work We present token optimization process to maximize information in limited tokens within any budget. It integrates seamlessly with mainstream frameworks, offering simple and scalable method to enhance real-world applications and facilitate future video foundation models. Future work could explore more advanced token selection methods for Figure 6. Overview of Flux-Multi Tuning. Figure 8. Convergence analysis of Flux-Single tuning using 3072 tokens but different frame counts directly on K400. Figure 7. Gradient norms of main projector modules of FluxMulti trained InternVideo2 on K400. We report the L2 gradient norm using bs=32. improvement, as the group setting can ensure full video coverage but not fully resist consistent camera motions. We test advanced Vid-TLDR [16] in Appendix, but with increased cost, tedious hyper-parameter tuning, and unstable improvement. For all current tasks tested, including finegrained captioning, our method is still simple and effective. A. More Experiments A.1. More ablation studies We here provide more ablation studies on Flux, including analyzing Fluxs training stability with gradient norm, full results using different spatiotemporal resolutions and corresponding heuristic TO validation strategy, and convergence analysis and experimenting with possible token merging methods in Flux. This will provide more in-depth analysis of the whole Flux method. Training dynamics and convergence analysis As illustrated in Figure 7, we analyze the gradient norms across the main projector layers in the Flux-Multi Tuned InternVideo2-S. The Patch Embedding Layer exhibits notably elevated gradient norm values, particularly when processing higher input settings with smaller number of input tokens. This gradient magnitude disparity could potentially introduce training instability. Considering this and to genFigure 9. Overall gradient norm trend during Flux-UMT pertraining. We report the overall training dynamics with our ablation setting. The FluxViT modules can lower the overall norm. #Frame 4 6 8 10 12 16 20 Max"
        },
        {
            "title": "Max",
            "content": "Spatial Resolution 168 196 224 252 280 80.4 81.7 82.3 82.6 82.3 82.6 83.5 84.5 84.3 84.2 83.6 84.5 84.4 84.8 84.6 84.4 83.7 84.8 85.2 85.1 85.0 84.5 83.5 85.2 85.3 85.3 84.9 84.4 83.4 85.3 85.3 85.1 84.8 84.4 83.5 85.3 85.1 85.0 84.6 84.0 83.2 85.1 85.3 85.3 85.0 84.5 83.7 - Table 11. Results of FluxViT-S on K400 using 1024 tokens and different spatiotemporal resolutions. We use 1clip 1crop for testing. The blue value marks the results of the unmasked setting. The values in bold show the best resolution for each frame count. erate more stable token-selection masks as introduced before, we use the Dual Patch Norm module, which shares similar findings with DPN[40]. However, our convergence analysis, presented in Figure 8, reveals that our Flux-Single tuning with InternVideo2-S, utilizing 3072 tokens and di9 Method Our selector Input Size 1024 42242 82.3 82242 84.6 122242 84.9 162242 84.8 202242 84.6 242242 84.6 Max 84.9 #Token 512 79.5 81.3 80.7 80.7 80.3 80.3 81.3 w/ Vid-TLDR 42242 77.4 78.2 82242 83.9 81.0 79.8 122242 85.0 81.4 80.6 162242 85.3 81.5 80.9 202242 85.2 80.9 80.4 242242 85.2 80.5 80.3 Max 85.3 81.5 80. Table 12. Use token merging strategy Vid-TLDR [16] on FluxViT K400 testing. The increment achieved by ViD-TLDR is sensitive to the hyper-parameter setting, like how many tokens are to be reduced in certain layers. Method K400 K600 Top1 Top5 Top1 Top5 UCF101 MiTv FluxViT-S2048 66.7 FluxViT-S2048+ 67.0 FluxViT-S1024 64.2 FluxViT-S1024+ 65.6 FluxViT-S512 59.0 FluxViT-S512+ 62.6 FluxViT-B2048 70.2 FluxViT-B2048+ 70.7 FluxViT-B1024 68.6 FluxViT-B1024+ 69.6 FluxViT-B512 64.2 FluxViT-B512+ 67.4 88.5 88.8 86.6 87.9 82.6 85.5 90.6 90.9 89.6 90.1 86.1 87.4 65.2 65.5 62.8 64.3 57.4 61.1 68.9 69.3 67.2 68.3 62.5 65. 87.3 87.5 85.7 86.5 81.2 84.2 89.5 89.8 88.4 89.0 84.9 87.7 85.8 87.5 85.1 87.2 81.5 84.2 88.7 89.1 87.8 89.1 84.9 87.6 28.2 28.6 27.2 27.8 25.5 26.5 31.2 31.5 30.4 31.0 28.8 29. Table 13. Full Zero-shot Action Recognition Results. config optimizer optimizer momentum weight decay learning rate schedule learning rate batch size warmup epochs [25] total epochs teacher input token student input tokens input frame spatial resolution drop path [34] flip augmentation augmentation SthSth V2 Others AdamW [58] β1, β2=0.9, 0.98 0.05 cosine decay [59] 1e-3 2048 20 100 2048 2048, 1536, 1024 (4, 26, stride=2) (168, 280, stride=28) 0.05 no yes MultiScaleCrop [0.66, 0.75, 0.875, 1] Table 14. Flux-UMT pre-training settings. rect tuning over 40 epochs on the K400 dataset, demonstrates consistent convergence patterns. Specifically, configurations with varying frame counts but fixed token number exhibit normal convergence behavior during tuning. 10 config optimizer optimizer momentum weight decay learning rate schedule learning rate batch size warmup epochs [25] total epochs drop path [34] flip augmentation label smoothing [73] augmentation"
        },
        {
            "title": "COIN",
            "content": "AdamW [58] β1, β2=0.9, 0.999 0.05 cosine decay [59] 2e-4 1024+512 5+1 5e-4 512 5 35+5 (S), 20+3 (B) 40(S), 25 (B) 0.1 yes 0.0 RandAug(9, 0.5) [17] Table 15. Action recognition fine-tuning settings. The training epochs A+B on Kinetics include epochs on K710 and epochs on K400, the same notation for warmup-epochs and batch size. config optimizer optimizer momentum weight decay learning rate schedule learning rate batch size warmup epochs [25] total epochs input frame spatial resolution token threshold augmentation 25M+2.5M AdamW [58] β1, β2=0.9, 0.98 0.02 cosine decay [59] 4e-4 (25M), 2e-5 (2.5M) 4096 (image), 4096 (video) 0.6 (25M), 0 (2.5M) 3 (25M), 1 (2.5M) (4, 26, stride=2) (168, 280, stride=28) (2048, 4096) MultiScaleCrop [0.5, 1] Table 16. Flux-CLIP pre-training settings. : For FluxViT-B, we lower the batch size to 2048 for the 2.5M data training. Dataset Kinetics-710 [44] COCO [52] Visual Genome [39] SBU Captions [65] CC3M [70] CC12M [12] S-MiT0.5M [63] WebVid-2M [3] WebVid-10M [3] InternVid2M [86] #image/video #text 0 658K 567K 113K 768K 100K 860K 860K 2.88M 2.88M 11.00M 11.00M 0.5M 0.5M 2.49M 2.49M 10.73M 10.73M 2.0M 2.0M Type Video image image image image image video video video video 25M corpus = CC3M+CC12M +WebVid-10M+Visual Genome +SBU+COCO 2.5M corpus = S-MiT+InternVid2M+COCO 25.68M 26.81M video + image 2.56M 2.62M video + image Table 17. Statistics of pre-training datasets. This observation suggests that the aforementioned instability may not be the primary concern in Flux training. Consequently, we prioritize DPNs capability to generate robust masks over its stabilization properties in fine-tuning scenarios and prioritize DPNs capability in stabilized training in the pre-training stage, considering the results in Figure 9 and the results in the previous Flux-UMT ablation study. Method #Token Type FluxViT-S 1024 512 2048 FluxViT-S+ 512 2048 FluxViT-B 1024 512 FluxViT-B+ 1024 512 MSVD DiDeMo LSMDC MSRVTT ActivityNet R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 85.5 79.0 95.4 79.4 84.4 73.9 95.7 75.1 82.3 65.2 93.6 64.3 85.8 79.0 95.5 78.0 85.5 76.9 94.5 77.8 84.0 71.8 93.6 71.8 86.7 81.5 96.6 82.9 86.2 78.4 96.9 79.7 84.9 69.3 95.4 70.1 88.0 81.6 96.6 83.3 87.9 80.8 96.9 82.2 86.7 76.0 96.4 76.7 87.5 88.4 84.8 85.1 76.1 76.5 87.5 89.2 86.4 87.3 82.5 82.8 89.6 91.3 87.5 88.4 79.9 80.6 89.9 91.4 88.6 90.4 85.0 85.6 77.7 92.8 76.9 92.5 74.0 90.5 77.8 93.6 77.0 92.2 76.2 90.0 80.1 94.2 79.5 94.8 77.6 92.8 80.9 93.9 80.7 93.9 79.3 93.9 49.3 78.7 47.3 79.3 45.1 75.5 49.7 80.2 49.1 80.2 46.9 78.1 52.6 83.3 51.9 83.0 49.6 80.9 54.2 84.2 53.4 84.9 52.1 83.0 20.8 21.6 18.7 20.3 17.2 17.8 21.1 22.4 20.5 21.7 19.0 19.2 23.7 24.8 22.6 24.0 20.1 21.4 25.4 25.6 24.1 25.3 22.5 23. 82.3 83.2 79.5 80.6 74.7 75.2 82.8 82.9 82.4 82.4 80.5 79.4 84.5 86.7 82.6 83.5 77.7 77.9 86.1 86.8 84.4 86.1 82.4 83.4 52.4 53.0 47.2 48.0 38.2 38.5 52.4 53.8 50.3 50.9 44.7 44.7 56.6 57.6 51.8 53.4 42.8 43.3 56.7 58.3 55.2 57.0 50.3 50.9 74.4 75.1 72.1 71.6 65.7 65.7 74.5 74.9 73.9 74.1 71.4 71.6 77.5 78.9 75.5 76.4 68.9 70.3 77.3 78.6 77.4 78.6 74.6 75.5 48.3 50.4 45.4 47.0 38.5 40.0 49.2 51.2 49.0 50.5 45.8 47.0 52.2 53.0 48.8 50.5 42.9 44.5 53.5 54.2 53.0 54.3 49.8 51.2 75.6 77.9 74.0 74.6 69.6 70.2 75.8 76.5 74.6 76.4 71.7 73.1 80.1 81.5 78.0 78.0 73.7 74.0 79.6 82.4 79.3 79.9 77.0 78.6 44.2 45.5 43.8 45.4 41.7 41.1 46.0 46.4 44.8 45.3 40.8 41.7 49.3 49.3 48.3 49.1 45.3 46.0 50.5 50.4 49.5 50.3 47.1 47. 36.0 37.6 35.9 37.2 33.0 33.8 38.2 38.6 36.6 38.5 34.3 35.0 41.0 42.0 39.9 41.4 36.6 37.5 41.7 42.6 40.9 42.8 38.2 40.3 67.0 67.7 64.4 65.7 59.5 61.2 67.5 68.2 66.4 67.4 62.7 63.4 72.2 73.6 69.6 70.5 64.4 65.2 71.0 73.9 71.4 71.4 68.6 71.1 T2V 44.4 V2T 44.3 T2V 42.2 43.1 V2T T2V 36.8 V2T 37.0 T2V 45.0 V2T 44.9 T2V 44.5 V2T 44.2 T2V 40.5 V2T 41.1 T2V 49.8 49.3 V2T T2V 48.0 V2T 46.5 T2V 42.6 V2T 41.5 T2V 49.9 V2T 49.4 T2V 49.1 V2T 48.9 T2V 47.2 46.5 V2T Table 18. Full Zero-shot retrieval results on MSRVTT, DiDeMo, AcitivityNet, LSMDC, and MSVD. Full results using different spatiotemporal resolutions. Table 11 shows the results of FluxViT-S on K400 using different spatiotemporal resolutions but with kept 1024 number. Using lower spatial resolution but with larger frame count can further strengthen the models performance, which causes another +0.3% performance gain compared with the best result achieved using standard 224 resolution. This may reflect the datasets bias towards longer inputs and our methods preference for more dynamic tokens instead of highly informative spatial tokens. Moreover, we find that the best-performing areas are mainly located within threshold of input tokens, which can be seen as the bolded values of each frame count mainly located within an anti-diagonal line. This observation validates our approach of imposing threshold on input token numbers and suggests an optimized evaluation strategy for determining optimal input configurations. We propose systematic evaluation procedure: beginning with minimal input settings (e.g., 42242), incrementally increase frame counts until performance plateaus, then progressively reduce spatial resolution while increasing frame count until accuracy improvements cease. This linear complexity evaluation approach efficiently identifies the nearoptimal configuration for token optimization. demonstrates the performance achieved by incorporating the advanced Vid-TLDR [16] token reduction approach with our FluxViT-S model on K400. Vid-TLDR implements token merging within the initial network blocks to regulate token count. For configurations targeting 1024 tokens, we apply Vid-TLDR to progressively reduce token counts to [2048, 1536, 1024] across the first three layers. Similarly, for 512-token configurations, we evaluate two reduction sequences: [1024, 512] and an alternative [1536, 1024, 512] (denoted by gray values in the table). While results from the first two reduction strategies demonstrate the potential synergy between advanced token-selection methods and our Flux approach, the latter sequence, despite incorporating more tokens in initial layers and involving more computation overhead, underperforms our baseline that employs heuristic token-reduction method. This outcome shows Vid-TLDRs limitations in accommodating diverse token reduction requirements and highlights its ongoing need for extensive parameter searching. Thus, we only adopt our heuristic but nearly costless token selection method instead of the heavy, nonflexible, and unstable token merging method in Flux. A.2. More results Combining modern token merging strategy. The intraining-free token-merging tegration of state-of-the-art strategies during inference presents an opportunity to further enhance our Flux methods performance. Table 12 Full retrieval results. Table 18 shows more zero-shot retrieval results on MSRVTT [91], DiDeMo [1], ActivityNet [31], LSMDC [69], and MSVD [13]. We see that MSRVTT and ActivityNet enjoy only marginal perfor11 mance gain using 2048 tokens, which may be due to the information saturation for these datasets as also observed in InternVideo2 [87] when finding little gain by enlarging the frame count from 4 to 8 and 16. The other three datasets highlight our Flux methods effects more with 2048 tokens, while all these datasets demonstrate our costless performance improvement with 1024 and 512 token inputs. More zero-shot action recognition results. Table 13 shows more zero-shot retrieval results of our FluxViT on K400 [38], K600 [10], UCF101 [71], and MiTv1 [64]. B. More implementation details In this section, we introduce the detailed training hyperparameters and report the training dataset details in Table 17. Flux-UMT pre-training. In and UMT[47] framework to get our single modality FluxViT model, we follow most settings as used in deriving InternVideo2 models. Details are shown in Table 14. combining"
        },
        {
            "title": "Flux",
            "content": "Single modality fine-tuning. We adopt the Flux-UMT pretrained video encoder and add an extra classification layer for fine-tuning. Input settings are kept the same, and the details of hyperparameters are given in Table 15. combining Flux-CLIP per-training. In and CLIP[68] framework to get our multi-modality FluxViT model, we show the details in Table 16. We freeze all the modules in 25M data pretraining as Stage 1, except the vision projector. We unfreeze all the modules for the Stage 2 training on the 2.5M dataset."
        },
        {
            "title": "Flux",
            "content": "Chat Centric Training We freeze both the LLM and the Vision Encoder in the common stage-1 training of chat model. We use learning rate of 1e-3, batch size of 512, single training epoch, and cosine learning rate schedule with 0.03 warmup rate."
        },
        {
            "title": "References",
            "content": "[1] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with natural language. In ICCV, 2017. 2, 11 [2] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luˇcic, and Cordelia Schmid. Vivit: video vision transformer. In ICCV, 2021. 6 [3] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In ICCV, 2021. 8, 10 [4] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. In ICLR, 2021. 3 Is space-time attention all you need for video understanding? In ICML, 2021. 6, 7 [5] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. [6] Lucas Beyer, Pavel Izmailov, Alexander Kolesnikov, Mathilde Caron, Simon Kornblith, Xiaohua Zhai, Matthias Minderer, Michael Tschannen, Ibrahim M. Alabdulmohsin, and Filip Pavetic. Flexivit: One model for all patch sizes. In CVPR, 2022. 2 [7] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. arXiv, abs/2210.09461, 2022. 6 [8] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. Once-for-all: Train one network and specialize it for efficient deployment. In ICLR, 2019. 2 [9] Mu Cai, Jianwei Yang, Jianfeng Gao, and Yong Jae Lee. Matryoshka multimodal models. ArXiv, abs/2405.17430, 2024. 2 [10] Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. short note about kinetics600. ArXiv, abs/1808.01340, 2018. 4, [11] Joao Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman. short note on the kinetics-700 human action dataset. ArXiv, abs/1907.06987, 2019. 4 [12] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021. 8, 10 [13] David L. Chen and William B. Dolan. Collecting highly parallel data for paraphrase evaluation. In ACL, 2011. 2, 11 [14] Sihan Chen, Handong Li, Qunbo Wang, Zijia Zhao, Mingzhen Sun, Xinxin Zhu, and Jing Liu. Vast: visionaudio-subtitle-text omni-modality foundation model and dataset. In NIPS, 2023. 8 [15] Feng Cheng, Xizi Wang, Jie Lei, David J. Crandall, Mohit Bansal, and Gedas Bertasius. Vindlu: recipe for effective video-and-language pretraining. ArXiv, abs/2212.05051, 2022. 8 [16] Joonmyung Choi, Sanghyeok Lee, Jaewon Chu, Minhyuk Choi, and Hyunwoo J. Kim. vid-tldr: Training free token merging for light-weight video transformer. In CVPR, 2024. 1, 6, 9, 10, 11 [17] Ekin Dogus Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V. Le. Randaugment: Practical automated data augmentation with reduced search space. In IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 2020. [18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. ArXiv, abs/1810.04805, 2018. 3 [19] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. Unified language model pre-training for natural language understanding and generation. NeurIPS, 32, 2019. 3 [20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 3 [21] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer. Multiscale vision transformers. In ICCV, 2021. 6, [22] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In ICCV, 2019. 1 [23] Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, and Kaiming He. Masked autoencoders as spatiotemporal learners. NeurIPS, 2022. 3, 6 [24] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu. Violet: End-toend video-language transformers with masked visual-token modeling. ArXiv, abs/2111.12681, 2021. 8 [25] Priya Goyal, Piotr Dollar, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. ArXiv, abs/1706.02677, 2017. 10 [26] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The something something video database for learning and evaluating visual common sense. In ICCV, 2017. 2, 4, 7 [27] Jing Gu, Eliana Stefani, Qi Wu, Jesse Thomason, and Xin Wang. Vision-and-language navigation: survey of tasks, methods, and future directions. In ACL, 2022. [28] Tengda Han, Weidi Xie, and Andrew Zisserman. Turbo training with token dropout. In BMVC, 2022. 7 [29] Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam Lim. Ma-lmm: Memory-augmented large multimodal model for long-term video understanding. In CVPR, 2024. 7 [30] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross B. Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022. 3 [31] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: large-scale video In CVPR, benchmark for human activity understanding. 2015. 2, 7, 11 [32] Zejiang Hou, Fei Sun, Yen-Kuang Chen, Yuan Xie, and S. Y. Kung. Milan: Masked image pretraining on language assisted representation. ArXiv, abs/2208.06049, 2022. [33] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, M. Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. ArXiv, abs/1704.04861, 2017. 4 [34] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Weinberger. Deep networks with stochastic depth. In ECCV, 2016. 10 [35] Sun-Kyoo Hwang, Jaehong Yoon, Youngwan Lee, and Sung Ju Hwang. Everest: Efficient masked video autoencoder by removing redundant spatiotemporal tokens. ICML, 2022. 1, 3, 6 In [36] Md. Mohaiminul Islam and Gedas Bertasius. Long movie clip classification with state-space video models. In ECCV, 2022. [37] Faure Gueter Josmy, Yeh Jia-Fong, Chen Min-Hung, Su Hung-Ting, Hsu Winston H, and Lai Shang-Hong. Bridging episodes and semantics: novel framework for long-form video understanding. In ECCV workshops, 2024. 7 [38] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Apostol Natsev, Mustafa Suleyman, and Andrew Zisserman. The kinetics human action video dataset. ArXiv, abs/1705.06950, 2017. 2, 4, 12 [39] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV, 2017. 8, 10 [40] Manoj Kumar, Mostafa Dehghani, and Neil Houlsby. Dual patchnorm. ArXiv, abs/2302.01327, 2023. 9 [41] Aditya Kusupati, Gantavya Bhatt, Aniket Rege, Matthew Wallingford, Aditya Sinha, Vivek Ramanujan, William Howard-Snyder, Kaifeng Chen, Sham M. Kakade, Prateek Jain, and Ali Farhadi. Matryoshka representation learning. In NIPS, 2022. 2 [42] Jie Lei, Tamara Berg, and Mohit Bansal. Revealing single frame bias for video-and-language learning. ArXiv, abs/2206.03428, 2022. 8 [43] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2022. [44] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Limin Wang, and Y. Qiao. Uniformerv2: Spatiotemporal learning by arming image vits with video uniformer. ArXiv, abs/2211.09552, 2022. 6, 7, 10 [45] Kunchang Li, Yali Wang, Gao Peng, Guanglu Song, Yu Liu, Hongsheng Li, and Yu Qiao. Uniformer: Unified transformer for efficient spatial-temporal representation learning. In ICLR, 2022. 6, 7 [46] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wen Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. ArXiv, Videochat: Chat-centric video understanding. abs/2305.06355, 2023. 1, 2 [47] Kunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He, Limin Wang, and Yu Qiao. Unmasked teacher: Towards training-efficient video foundation models. In ICCV, 2023. 2, 3, 6, 7, 8, 12 [48] Kunchang Li, Xinhao Li, Yi Wang, Yinan He, Yali Wang, Limin Wang, and Yu Qiao. Videomamba: State space model for efficient video understanding. ArXiv, abs/2403.06977, 2024. 7 [49] Yanghao Li, Chaoxia Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer. Improved multiscale vision transformers for classification and detection. ArXiv, abs/2112.01526, 2021. 6, 13 [50] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efficient video understanding. In ICCV, 2019. 1 [51] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 8 [52] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 10 [53] Xudong Lin, Fabio Petroni, Gedas Bertasius, Marcus Rohrbach, Shih-Fu Chang, and Lorenzo Torresani. Learning to recognize procedural activities with distant supervision. CVPR, 2022. 7 [54] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. ArXiv, abs/2310.03744, 2023. 2 [55] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 8 [56] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. In CVPR, 2022. [57] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx mllm: On-demand spatialArXiv, temporal understanding at arbitrary resolution. abs/2409.12961, 2024. 2, 3 [58] I. Loshchilov and F. Hutter. Fixing weight decay regularization in adam. ArXiv, abs/1711.05101, 2017. 10 [59] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In ICLR, 2017. 10 [60] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. NeurIPS, 2019. 3 [61] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval and captioning. Neurocomputing, 2022. 8 [62] Muhammad Maaz, Hanoona Abdul Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. ArXiv, abs/2306.05424, 2023. 1 [63] Mathew Monfort and SouYoung Jin. Spoken moments: Learning joint audio-visual representations from video descriptions. In CVPR, 2021. 8, [64] Mathew Monfort, Bolei Zhou, Sarah Adel Bargal, Alex Andonian, Tom Yan, Kandan Ramakrishnan, Lisa M. Brown, Quanfu Fan, Dan Gutfreund, Carl Vondrick, and Aude Oliva. Moments in time dataset: One million videos for event understanding. TPAMI, 2020. 7, 12 [65] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million captioned photographs. In NeurIPS, 2011. 8, 10 [66] Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu Wei. Beit v2: Masked image modeling with vectorquantized visual tokenizers. ArXiv, abs/2208.06366, 2022. 3 [67] Zhiwu Qing, Shiwei Zhang, Ziyuan Huang, Xiang Wang, Yuehuang Wang, Yiliang Lv, Changxin Gao, and Nong Sang. Mar: Masked autoencoders for efficient action recognition. In ACMMM, 2022. 1, 3 [68] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. 3, 7, 8, 12 [69] Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon, Christopher Joseph Pal, H. Larochelle, Aaron C. Courville, and Bernt Schiele. Movie description. International Journal of Computer Vision, 2016. 2, [70] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL, 2018. 8, 10 [71] Soomro. Ucf101: dataset of 101 human actions classes from videos in the wild. ArXiv, abs/1212.0402, 2012. 12 [72] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 2024. 6 [73] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In CVPR, 2016. 10 [74] Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou. Coin: large-scale dataset for comprehensive instructional video analysis. In CVPR, 2019. 2 [75] Rui Tian, Zuxuan Wu, Qi Dai, Han Hu, Yu Qiao, and YuGang Jiang. Resformer: Scaling vits with multi-resolution training. In CVPR, 2023. 1, [76] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. VideoMAE: Masked autoencoders are data-efficient learnIn NeurIPS, ers for self-supervised video pre-training. 2022. 3, 6, 7 [77] Pavan Kumar Anasosalu Vasu, Hadi Pouransari, Fartash Faghri, Raviteja Vemulapalli, and Oncel Tuzel. Mobileclip: Fast image-text models through multi-modal reinforced training. In CVPR, 2024. 8 [78] Paul Voigtlaender, Soravit Changpinyo, Jordi Pont-Tuset, Radu Soricut, and Vittorio Ferrari. Connecting vision and language with video localized narratives. CVPR, 2023. 8 [79] Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo, Luowei Zhou, Yucheng Zhao, Yujia Xie, Ce Liu, Yu-Gang Jiang, and Lu Yuan. Omnivl: One foundation model ArXiv, for image-language and video-language tasks. abs/2209.07526, 2022. 8 [80] Jiawei Wang, Liping Yuan, Yuchen Zhang, and Haomiao Sun. Tarsier: Recipes for training and evaluating large video description models. arXiv, abs/2407.00634, 2024. 2 [81] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment networks: Towards good practices for deep action recognition. In ECCV, 2016. 1 captioners are image-text foundation models. Transactions on Machine Learning Research, 2022. 6 [96] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. CVPR, 2023. 8 [97] Bowen Zhang, Jiahui Yu, Christopher Fifty, Wei Han, Andrew M. Dai, Ruoming Pang, and Fei Sha. Co-training transformer with videos and images improves action recognition. ArXiv, abs/2112.07175, 2021. 6 [98] Yitian Zhang, Yue Bai, Chang Liu, Huan Wang, Sheng Li, and Yun Fu. Frame flexible network. In CVPR, 2023. 1, 2, 7 [99] Hang Zhao, Antonio Torralba, Lorenzo Torresani, and Zhicheng Yan. Hacs: Human action clips and segments dataset for recognition and temporal localization. In ICCV, 2019. 7 [100] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Loddon Yuille, and Tao Kong. ibot: Image bert pre-training with online tokenizer. ArXiv, abs/2111.07832, 2021. [101] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, Wang HongFa, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, Cai Wan Zhang, Zhifeng Li, Wei Liu, and Li Yuan. Languagebind: Extending video-language pretraining to nmodality by language-based semantic alignment. In ICLR, 2024. 8 [82] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and Yu Qiao. Videomae v2: Scaling video masked autoencoders with dual masking. In CVPR, 2023. 3, 6 [83] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. ArXiv, abs/2409.12191, 2024. 2, 8 [84] Rui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Yu-Gang Jiang, Luowei Zhou, and Lu Yuan. Bevt: Bert pretraining of video transformers. CVPR, 2022. 3 [85] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, and Yu Qiao. Internvideo: General video foundation models via generative and discriminative learning. ArXiv, abs/2212.03191, 2022. 8 [86] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Jian Ma, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, and Y. Qiao. Internvid: large-scale video-text dataset for multimodal understanding and generation. ArXiv, 2023. 8, [87] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Chenting Wang, Guo Chen, Baoqi Pei, Ziang Yan, Rongkun Zheng, Jilan Xu, Zun Wang, Yansong Shi, Tianxiang Jiang, Songze Li, Hongjie Zhang, Yifei Huang, Yu Qiao, Yali Wang, and Limin Wang. Internvideo2: Scaling foundation models for multimodal video understanding. In ECCV, 2024. 1, 2, 3, 4, 6, 7, 8, 12 [88] Zun Wang, Jialu Li, Yicong Hong, Yi Wang, Qi Wu, Mohit Bansal, Stephen Gould, Hao Tan, and Yu Qiao. Scaling data generation in vision-and-language navigation. In ICCV, 2023. 1 [89] Longhui Wei, Lingxi Xie, Wen gang Zhou, Houqiang Li, and Qi Tian. Mvp: Multimodality-guided visual pretraining. In ECCV, 2022. 3 [90] Chao-Yuan Wu, Ross Girshick, Kaiming He, Christoph Feichtenhofer, and Philipp Krahenbuhl. multigrid method for efficiently training video models. In CVPR, 2020. 2 [91] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging video and language. In CVPR, 2016. 2, 11 [92] Shen Yan, Xuehan Xiong, Anurag Arnab, Zhichao Lu, Mi Zhang, Chen Sun, and Cordelia Schmid. Multiview transformers for video recognition. In CVPR, 2022. 6 [93] Shen Yan, Tao Zhu, Zirui Wang, Yuan Cao, Mi Zhang, Soham Ghosh, Yonghui Wu, and Jiahui Yu. Video-text modeling with zero-shot transfer from contrastive captioners. ArXiv, abs/2212.04979, 2022. [94] Jiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Bender, Pieter-Jan Kindermans, Mingxing Tan, Thomas Huang, Xiaodan Song, Ruoming Pang, and Quoc Le. Bignas: Scaling up neural architecture search with big single-stage models. In ECCV, 2020. 2 [95] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive"
        }
    ],
    "affiliations": [
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "State Key Laboratory for Novel Software Technology, Nanjing University",
        "University of Chinese Academy of Sciences",
        "University of Science and Technology of China"
    ]
}