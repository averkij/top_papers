{
    "paper_title": "GoT: Unleashing Reasoning Capability of Multimodal Large Language Model for Visual Generation and Editing",
    "authors": [
        "Rongyao Fang",
        "Chengqi Duan",
        "Kun Wang",
        "Linjiang Huang",
        "Hao Li",
        "Shilin Yan",
        "Hao Tian",
        "Xingyu Zeng",
        "Rui Zhao",
        "Jifeng Dai",
        "Xihui Liu",
        "Hongsheng Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current image generation and editing methods primarily process textual prompts as direct inputs without reasoning about visual composition and explicit operations. We present Generation Chain-of-Thought (GoT), a novel paradigm that enables generation and editing through an explicit language reasoning process before outputting images. This approach transforms conventional text-to-image generation and editing into a reasoning-guided framework that analyzes semantic relationships and spatial arrangements. We define the formulation of GoT and construct large-scale GoT datasets containing over 9M samples with detailed reasoning chains capturing semantic-spatial relationships. To leverage the advantages of GoT, we implement a unified framework that integrates Qwen2.5-VL for reasoning chain generation with an end-to-end diffusion model enhanced by our novel Semantic-Spatial Guidance Module. Experiments show our GoT framework achieves excellent performance on both generation and editing tasks, with significant improvements over baselines. Additionally, our approach enables interactive visual generation, allowing users to explicitly modify reasoning steps for precise image adjustments. GoT pioneers a new direction for reasoning-driven visual generation and editing, producing images that better align with human intent. To facilitate future research, we make our datasets, code, and pretrained models publicly available at https://github.com/rongyaofang/GoT."
        },
        {
            "title": "Start",
            "content": "GoT: Unleashing Reasoning Capability of Multimodal Large Language Model for Visual Generation and Editing Rongyao Fang1* Chengqi Duan2 Kun Wang3 Linjiang Huang6 Hao Li1,4 Shilin Yan Hao Tian3 Xingyu Zeng3 Rui Zhao3 Jifeng Dai4,5 Xihui Liu2 Hongsheng Li1 1CUHK MMLab 2HKU 3SenseTime 4Shanghai AI Laboratory 5THU 6BUAA 5 2 0 2 3 1 ] . [ 1 9 3 6 0 1 . 3 0 5 2 : r Figure 1. Generation Chain-of-Thought (GoT) with Semantic-Spatial Reasoning. Our approach transforms input prompts into explicit reasoning chains with coordinates (middle), which guides vivid image generation and precise editing (right). This reasoning-based generation paradigm unifies spatial understanding across visual tasks: semantically-grounded visual generation (top), controllable interactive generation (middle), and localized image editing (bottom)."
        },
        {
            "title": "Abstract",
            "content": "Current image generation and editing methods primarily process textual prompts as direct inputs without reasoning about visual composition and explicit operations. We present Generation Chain-of-Thought (GoT), novel paradigm that enables generation and editing through an explicit language reasoning process before outputting images. This approach transforms conventional text-to-image generation and editing into reasoning-guided framework that analyzes semantic relationships and spatial arrangements. We define the formulation of GoT and construct large-scale GoT datasets containing over 9M samples with detailed reasoning chains capturing semantic-spatial relationships. To leverage the advantages of GoT, we implement unified framework that integrates Qwen2.5-VL for reasoning chain generation with an end-to-end diffusion model enhanced by our novel Semantic-Spatial Guidance Module. Experiments show our GoT framework achieves excellent performance on both generation and editing tasks, with significant improvements over baselines. Additionally, our approach enables interactive visual generation, allowing users to explicitly modify reasoning steps for precise image adjustments. GoT pioneers new direction for reasoningdriven visual generation and editing, producing images that better align with human intent. To facilitate future research, we make our datasets, code, and pretrained models publicly available at https://github.com/rongyaofang/ GoT. 1. Introduction Language provides the primary interface for expressing human intent in visual content generation. Traditional image generation systems [6, 21, 37], particularly diffusion models, process textual prompts by mapping semantic concepts to visual elements without explicit reasoning. These approaches struggle with complex scenes requiring precise spatial arrangements and object interactions that humans naturally consider when constructing scenes. Meanwhile, multimodal large language models (MLLMs) [2, 3, 25] excel at sophisticated reasoning tasks, including analyzing semantic structures, inferring relationships, grounding visual concepts, and processing detailed contexts through explicit reasoning chains. This gap between MLLMs advanced reasoning capabilities and the limited reasoning in current generation systems raises key question: How can we integrate the reasoning mechanisms that have revolutionized language understanding into visual generation and editing? Prior work attempted to leverage LLMs for image generation from different perspectives. One line of research [23, 55] leverages LLMs as text encoders for better prompt interpretation. However, the reasoning capabilities of LLMs are not introduced. Another line of work develops multimodal LLMs to unify understanding and generation [8, 44, 47, 50]. Although they present unified models for different tasks, there is no evidence that generation benefits from strong understanding and reasoning abilities of the models. They merely combine independent tasks rather than truly fusing language reasoning with visual generation. Additionally, layout-based methods like GLIGEN [22], LayoutGPT [9], and RPG [52] incorporate LLMs for layout planning and diffusion models for layout-guided generation. However, these methods treat planning and generation as separate stages rather than integrating reasoning throughout the endto-end process. Consequently, current image generation methods lack reasoning capabilities, emphasizing the need for framework that seamlessly combines reasoning with visual generation and editing. Inspired by chain-of-thought (CoT) reasoning of the LLMs, we introduce Generation Chain-of-Thought (GoT), novel paradigm that enables visual generation to first output step-by-step reasoning in natural language before producing images. However, implementing GoT poses two significant challenges. First, different from CoT in LLMs, the reasoning chain for visual generation and editing requires It requires new both semantic and spatial information. formulation and collecting training data in this new format. Second, existing diffusion-based models cannot leverage explicit language reasoning chains during visual generation. We need to design framework supporting end-to-end language reasoning and visual generation. To address the first challenge, we formulate GoT as multimodal reasoning chain that integrates semantic and spatial analyses to enhance image generation and editing tasks. For visual generation, GoT provides precise control over object layout, relationships, and attributes, while for editing, it leverages semantic and spatial understanding to decompose user requests into coherent grounding and modification steps. We utilize advanced MLLMs and LLMs to construct complex annotation pipelines, which capture semantic-spatial interactions across diverse visual contexts. We assembled extensive datasets comprising 8.4M images for text-to-image generation (from Laion-Aesthetics [39], JourneyDB [41], and FLUX [21]) and 920K examples for image editing (from OmniEdit [48] and SEED-EditMultiturn [12]). This computationally intensive effort produced the first large-scale dataset of reasoning chains for image generation and editing. To tackle the second challenge of architecture design supporting reasoning and generation, we construct unified end-to-end framework. Our GoT framework integrates the reasoning capabilities of MLLMs with the high-fidelity generation qualities of diffusion models. The proposed framework leverages an MLLM to generate reasoning steps and visual tokens, providing explicit guidance that incorporates semantic relationships and spatial configurations. This guidance flows into our novel Semantic-Spatial Guidance Module (SSGM), which conditions the diffusion process to ensure that generated images are closely guided by the reasoning process. This design supports end-to-end training and inference for visual generation and editing guided by explicit reasoning chains. By effectively integrating reasoning into visual generation, our GoT framework demonstrates significant improvements in both text-to-image generation quality and image editing accuracy. Additionally, GoT enables interactive generation, allowing users to control the generated image by directly modifying the explicit reasoning process according to their preferences. These advantages represent substantial advancement in reasoning-guided visual synthesis. The main contributions can be summarized as follows: We propose Generation Chain-of-Thought (GoT), paradigm that integrates reasoning into visual generation and editing tasks, enabling explicit semantic and spatial reasoning for these tasks. We define the formulation of semantic and spatial reasoning chains for visual generation and editing, and collect the first large-scale GoT datasets comprising 8.4M image generation samples and 920K image editing samples. We develop unified end-to-end framework that leverages multimodal language models and diffusion models, with novel Semantic-Spatial Guidance Module that ensures generated images follow the reasoning process. Our experimental results demonstrate significant improvements in both text-to-image generation and editing. 2. Related Work 2.1. Diffusion Models Diffusion models have revolutionized visual content creEarly approaches [30, 35] demonstrated this ation. paradigms potential, while Stable Diffusion [37] improved efficiency through latent space compression. Recent models [6, 16, 21, 32, 36, 38] have further advanced photorealism through architectural innovations and larger-scale training. Various efforts to extend diffusion models capabilities include controllable generation methods [28, 54] and instruction-based editing frameworks [5, 40]. While some researchers have explored unifying vision tasks [7, 11], these primarily focus on traditional computer vision tasks rather than general image generation. Despite these advances, current models typically process prompts through direct mapping, using text encoders like CLIP [33] or T5 [34] to condition the diffusion process via crossattention [45]. This approach treats text as static representation without explicit reasoning about scene composition or object relationships. The fundamental limitation becomes evident when generating complex scenes with multiple objects and specific spatial arrangements, necessitating more sophisticated reasoning-based approaches. 2.2. Large Language Models and Reasoning Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities through chain-of-thought (CoT)[49], enabling complex problem decomposition. This paradigm extends to MLLMs [1, 2], which integrate visual and textual understanding. Some advanced works [25, 31] have enhanced spatial understanding by grounding textual concepts to image regions, enabling analysis of object relationships. Despite these capabilities, MLLMs remain underutilized for visual generation. While models like Chameleon [44] and Emu2 [43] incorporate image generation, they lack mechanisms to decompose user intent into semantic-spatial reasoning steps. 2.3. Layout-guided Image Generation and Editing Recent research has explored layout-guided approaches for spatial control in visual synthesis. GLIGEN [22] incorporated bounding boxes through gated cross-attention layers, enhancing object placement. LayoutGPT [9] proposed two-stage pipeline converting text into scene layouts before generation. RPG [52] advanced this through recurrent planning, alternating between layout refinement and synthesis. SmartEdit [17] adapts the LLaVA [24] model to specialize in image editing tasks. FlexEdit [29] employs an MLLM to comprehend the image content, mask, and user instructions. Despite these advances, existing approaches treat layouts as static constraints or sequential plans generated before synthesis, disconnecting spatial planning from generation. 3. Generation Chain-of-Thought (GoT) During visual generation and editing, humans naturally reason about object relationships and spatial arrangements. In contrast, most current models process prompts without explicit reasoning, making it difficult to interpret complex human intentions for generating scenes with detailed object relationships and spatial configurations. Motivated by chain-of-thought (CoT) in language models, we propose Generation Chain-of-Thought (GoT), shifting the visual generation from direct mapping to reasoning-guided process. Unlike language generation, which operates primarily within semantic space, visual generation requires an integrated understanding of both semantic relationships and spatial configurations. To address this complexity, GoT employs multi-modal reasoning formulation that bridges conceptual understanding and spatial reasoning. This formulation incorporates explicit coordinate information in format (x1,y1),(x2,y2) with range [0,1000), ensuring precise management over the placement of visual elements. This unified semanticspatial reasoning chain enables fine-grained control of object placement, attributes, and inter-object relationships, ultimately supporting robust and coherent visual generation. To illustrate the formulation of GoT, Fig. 1 presents examples of both text-to-image generation and editing tasks. For text-to-image, GoT generates detailed reasoning chain specifying precise coordinates of elements. This explicit Figure 2. GoT Dataset Construction Process. Left: Text-to-image GoT annotation pipeline that labels detailed GoT with semantic content and spatial coordinates. Right: Editing GoT annotation pipeline that processes source image, target image, and instruction to generate entity-aware reasoning GoT with precise spatial grounding. Both pipelines leverage Qwen2-VL [46] and Qwen2.5 [51] models for various stages of the annotation process. spatial reasoning enables proper arrangement of all constituents while maintaining their semantic relationships, resulting in coherent and visually appealing composition. The image editing example in Fig. 1 demonstrates how GoT handles manipulation tasks through structured reasoning. When tasked with replace the giant leaf with an umbrella, GoT first analyzes the scene and then plans edits with precise coordinates. Finally, GoT describes what the image shows after editing. This decomposition into sequential steps with explicit spatial reasoning streamlines complex manipulations, contrasting with traditional editing methods that lack spatial awareness and reasoning. GoT endows image generation and editing with reasoning benefits. By decomposing complex instructions into clearly defined, sequential steps, GoT delivers results Its transthat more accurately fulfill human requests. parent process explains the intermediate reasoning behind each change and enables both image generation and editing within unified system. Implementing GoT requires two key components: Comprehensive Dataset: This dataset must consist of detailed reasoning chains that align with visual content, capturing both semantic relationships and spatial configurations. Such data provide the necessary foundation for the reasoning process. Compatible Visual Generation Model: The model needs to accommodate chain input to integrate semantic analysis and spatial reasoning, ensuring effective execution of the reasoning steps derived from the dataset. In the following sections, we elaborate on these components and discuss how they contribute to the robust performance of the GoT framework. 4. GoT Dataset: Semantic-Spatial Reasoning Chains for Visual Generation and Editing Based on the formulation presented previously, we construct large-scale training datasets using advanced LLMs and MLLMs. Our GoT dataset features meticulously crafted semantic-spatial reasoning chains for both generation and editing tasks, with each sample containing instructions, reasoning chain annotations, and corresponding images. The construction requires careful design of taskspecific annotation pipelines to ensure quality. The prompts used in the pipelines are attached in Appendix Sec. 11. 4.1. Automated Data Creation Pipeline As illustrated in Fig. 2, our annotation pipeline demonstrates the multiple stages of processing required to generate these high-quality annotations. For text-to-image, we utilize Qwen2-VL [46] to generate concise prompts that serve as text-to-image generation prompts and detailed visual descriptions that form the semantic component of GoT. Qwen2.5 [51] then performs object entity extraction, followed by Qwen2-VL establishing spatial relationships through object grounding. The detailed visual descriptions merged with precise object groundings together constitute the complete GoT annotation for text-to-image generation. For the image editing pipeline, we employ Qwen2-VL to generate comprehensive descriptions of source and target images, precisely localize editing regions through bounding boxes, and generate detailed descriptions of edited objects after cropping. We then leverage Qwen2.5 with carefully designed in-context prompting to synthesize coherent GoT reasoning chains, ensuring logical flow and completeness of the editing process. From this pipeline, we derive concise editing instructions as editing inputs while using the detailed semantic-spatial reasoning steps as GoT annotations. For the complex multi-turn editing dataset, we develFigure 3. GoT Framework with Semantic-Spatial Guidance. Left: Our dual-task framework handling both text-to-image generation (T2I) and image editing. Right: The SSGM Diffusion Module, which combines spatial layouts guidance Gs, reference image guidance Gr, and semantic guidance Gt to generate the final image with precise content and spatial control. oped related but more sophisticated protocol with Qwen2VL and Qwen2.5 to obtain intricate step-by-step reasoning chains with multiple spatial coordinates and transformation descriptions, capturing complex editing sequences. 4.2. Dataset Construction text-to-image generation, we For construct dataset Laion-Aesthetics-High-Resolution from three sources: (LAHR) [39] with 3.77M samples filtered for images larger than 512 pixels, JourneyDB [41] with 4.09M samples, and 600K FLUX.1-generated [21] images using LAHR prompts. The final datasets yield rich annotations: LAHRGoT samples with prompts averaging 110.81 characters, GoT descriptions averaging 811.56 characters, and 3.78 bounding boxes per image. Similarly, JourneyDB-GoT annotations average 149.78 characters for prompts, 906.01 characters for GoT descriptions, and 4.09 boxes image. For the single-turn image editing dataset, we build on OmniEdit [48], premier open-source image editing dataset with high-fidelity images, processing 736,691 samples covering editing operations (addition, removal, swap, changing expression/color/weather/lighting, and style transfer). The multi-turn image editing dataset is built upon SEED-EditMultiturn [12], resulting in 180,190 samples. The entire data creation process demanded substantial computational resources, requiring 100 NVIDIA A100 GPUs for over month. This comprehensive approach ensures our dataset provides the robust foundation necessary for training models capable of sophisticated image generation and editing tasks. 5. GoT Framework: Reasoning-guided Visual"
        },
        {
            "title": "Generation and Editing",
            "content": "We present the GoT framework, unified end-to-end approach embedding reasoning-guided processes into visual generation and editing tasks. GoT integrates two primary components: semantic-spatial aware MLLM generating structured reasoning chains with spatial information, and multi-guided diffusion model leveraging these reasoning outputs through our proposed Semantic-Spatial Guidance Module (SSGM) in an end-to-end manner. This design ensures that generated images precisely follow logical reasoning steps, allowing detailed control over both semantic content and spatial relationships. 5.1. Semantic-Spatial MLLM Design Our framework utilizes state-of-the-art MLLM Qwen2.5VL-3B as the backbone, chosen for its outstanding visual understanding and grounding capabilities. This MLLM functions as reasoning engine, handling both generation and editing tasks through unified architecture. As shown in Fig. 3, the MLLMs pipeline begins with task-specific input handling. For editing tasks, it processes reference images through the vision encoder to understand the source content. For both generation and editing, the MLLM produces GoT reasoning chains, capturing object attributes, relationships, modifications, and bounding box information. Following reasoning chain generation, the model processes an image start token followed by = 64 special [IMG] tokens, generating semantic guidance embeddings Gt that encapsulate information from the previous reasoning chains. Additionally, the spatial guidance Gs is Method Architecture Overall Single Obj. Two Obj. Counting Colors Position Attr. Binding Frozen Text Encoder Mapping Methods SDv1.5 [37] SDv2.1 [37] SD-XL [32] DALLE-2 [36] SD3 (d=24) [6] Unet+CLIP Unet+CLIP Unet+CLIP Unet+CLIP MMDIT+CLIP+T5 LLMs/MLLMs Enhanced Methods LlamaGen [42] Chameleon [44] LWM [26] SEED-X [13] Emu3-Gen [47] Janus [50] JanusFlow [27] Autoregressive Autoregressive Autoregressive Unet+Llama Autoregressive Autoregressive Autoregressive GoT Framework Unet+Qwen2.5-VL 0.43 0.50 0.55 0.52 0.62 0.32 0.39 0.47 0.49 0.54 0.61 0.63 0. 0.97 0.98 0.98 0.94 0.98 0.71 - 0.93 0.97 0.98 0.97 0.97 0.99 0.38 0.51 0.74 0.66 0.74 0.34 - 0.41 0.58 0.71 0.68 0.59 0. 0.35 0.44 0.39 0.49 0.63 0.21 - 0.46 0.26 0.34 0.30 0.45 0.67 0.76 0.85 0.85 0.77 0.67 0.58 - 0.79 0.80 0.81 0.84 0.83 0. 0.04 0.07 0.15 0.10 0.34 0.07 - 0.09 0.19 0.17 0.46 0.53 0.34 0.06 0.17 0.23 0.19 0.36 0.04 - 0.15 0.14 0.21 0.42 0.42 0. Table 1. Evaluation of text-to-image generation on GenEval benchmark [14]. Obj.: Object. Attr.: Attribution. derived by parsing and converting the explicit spatial information in the generated reasoning chains. This semantic-spatial aware design enables the MLLM to direct the SSGM Diffusion Module with precise control over content and layout. During training, the MLLM receives supervision through two pathways: cross-entropy loss on GoT reasoning tokens and gradient signals backpropagated from the end-to-end SSGM diffusion module through semantic guidance Gt. 5.2. Semantic-spatial Guided Diffusion Generation The end-to-end diffusion module builds upon SDXLs [32] architecture, incorporating an innovative triple-guidance mechanism that integrates semantic understanding, spatial awareness, and reference knowledge through our SemanticSpatial Guidance Module (SSGM). In SSGM, the semantic guidance pathway enhances the diffusion model by channeling = 64 MLLM-generated embeddings Gt through cross-attention layers, replacing conventional CLIP embeddings for more precise semantic control. For spatial guidance in SSGM, we extract coordinate information from the generated GoT to create color-coded masks where each object or editing region receives distinct color based on predefined order in the GoT sequence. These colored masks are processed through VAE encoder [18] and averaged to produce spatial latent features Gs, which are concatenated with the diffusion models latent representations, enabling precise spatial control during both generation and editing tasks. Following InstructPix2Pix [5], we incorporate reference image guidance as the third SSGM pathway. For editing tasks, the source image serves as reference, while for text-to-image generation, we use black reference image for architectural consistency. This design enables seamless transition between generation and editing tasks without architectural modifications. All references are processed through the VAE encoder to extract visual features Gr. 5.3. Multi-Guidance Strategy We employ classifier-free guidance strategy integrating semantic, spatial, and reference image guidance. During diffusion, the score estimation εθ is calculated through weighted combination: εθ =εθ(zt, , , )+ αt [εθ(zt, Gt, , Gr) εθ(zt, , , Gr)]+ αs [εθ(zt, Gt, Gs, Gr) εθ(zt, Gt, , Gr)]+ αr [εθ(zt, , , Gr) εθ(zt, , , )] (1) where zt is the noisy latent, Gt denotes semantic guidance embeddings, Gs indicates spatial guidance features, and Gr represents reference image features. Guidance scales αt, αs, and αr control the strength of each guidance type, while denotes null conditioning. During training, we randomly sample conditioning combinations with probability of 5%, excluding the fully-conditioned case εθ(zt, Gt, Gs, Gr), to enhance robustness. Optimal guidance parameters are introduced in Sec. 6. 5.4. Training Procedure Our training process implements two-phase approach: pretraining using LAHR-GoT, and followed by OmniEdit-GoT datasets finetuning with FLUX-GoT, OmniEdit-GoT, and SEEDEdit-MultiTurn-GoT (10,000 steps). We employ low-rank JourneyDB-GoT, (60,000 steps), Figure 4. Text-to-Image samples generated by our model. The GoT framework can plan object placement based on the input caption and generate highly aligned and aesthetic images accordingly. adaptation (LoRA) [15] to efficiently update the Qwen2.5VL decoders parameters while fully optimizing the SDXL-based diffusion module. The process operates end-to-end, jointly optimizing the MLLM GoT crossentropy token loss and diffusion MSE loss with equal weighting 1.0, demonstrating robustness without complex hyperparameter tuning. 6. Experiments We evaluate GoT framework on text-to-image generation, interactive image generation, and image editing. Experiments show quantitative improvements and qualitative benefits of our reasoning-guided approach, with ablation studies validating our design choices. 6.1. Text-to-Image Generation 6.1.1. Quantitative Results Tab. 1 presents evaluation of text-to-image generation (T2I) on GenEval [14]. The comparison spans two main categories of models: those employing frozen text encoders for direct prompt-to-image generation (primarily diffusionbased approaches) and those leveraging LLMs or MLLMs to enhance the generation process. On T2I task, GoT framework adopts αt = 7.5 and αs = 4.0, and more discussions on α tuning are shown in Appendix Sec. 9.2. As shown in Tab. 1, our framework achieves the highest overall score of 0.64, outperforming both frozen text encoder methods and LLM/MLLM-enhanced approaches. GoT framework excels particularly in single object (0.99), counting tasks (0.67), and color tasks (0.85), demonstrating the effectiveness of our reasoning-guided generation paradigm. While methods like JanusFlow [27] perform better in position and attribute binding tasks, GoT frameworks balanced performance across all metrics validates that incorporating explicit reasoning mechanisms enhances compositional generation abilities. Among the LLM/MLLM-enhanced methods, our approach outperforms recent systems like Janus [50] and JanusFlow [27] in overall performance despite their advantages in specific areas. This suggests that while autoregressive models excel in certain spatial tasks, our GoT frameworks structured reasoning provides more consistent performance across diverse generation requirements. 6.1.2. Qualitative Results In addition to the outstanding compositional text-to-image generation capability, GoT framework also exhibits high In Fig. 4, we showcase the generageneration quality. tion results of our model across diverse set of prompts. We present samples from compositional prompts containing multiple objects, incorporating object attributes, relationships, and relative spatial positions. Our model effectively plans the placement of different objects, producing coherent and aesthetically pleasing images. 6.2. Interactive Generation In our experiments, we further demonstrate the interactive capabilities of the GoT framework, as illustrated in Fig. 5. This approach enables user control over the generation process by modifying the GoT content, including both textual descriptions and bounding box positions. Users can customize their text-to-image generation through three primary interaction types: object replacement, object position adjustment, and object attribute modification. The examples showcase how the framework maintains overall scene coherence while precisely implementing the requested changes. This interactive flexibility provides an interpretable and manipulable interface for text-to-image generation that traditional black-box systems lack, allowing for precise control over the output without requiring expertise. Figure 5. Samples on interactive generation with GoT framework. By modifying GoT content (description and bounding box position), user can customize their text-to-image process with: 1. Object replacement 2. Object position adjustment 3. Object attribute modification. Method Params. Emu-Edit ImagenHub Reason-Edit CLIP-I CLIP-T GPT-4o Eval. GPT-4o Eval. IP2P [5] MagicBrush [53] MGIE [10] Emu-Edit [40] SEED-X [13] SmartEdit [17] CosXL-Edit [4] 0.9B+0.1B 0.9B+0.1B 0.9B+7B - 2.8B+14B 0.9B+7B - GoT Framework 2.8B+3B 0.834 0.838 0.783 0.859 0.825 - 0.860 0.864 0.219 0.222 0.253 0.231 0.272 - 0.274 0.276 0.308 0.513 0.392 - 0.166 - 0. 0.533 0.286 0.334 0.264 - 0.239 0.572 0.325 0.561 Table 2. Quantitative comparison on image editing benchmarks. denotes that SmartEdit mainly supports removing and replacing operation and is not designed for general editing operations. 6.3. Image Editing 6.3.1. Quantitative Results As shown in Tab. 2, we evaluate our GoT framework against state-of-the-art image editing methods across multiple benchmarks. On Emu-Edit benchmark [40], GoT framework achieves the highest scores for both CLIP-I (0.864) and CLIP-T (0.276) metrics, outperforming previous methods including CosXL-Edit [4] and Emu-Edit [40]. Since CLIP-I and CLIP-T cannot fully reflect editing accuracy, we also evaluated using GPT-4o [1], which aligns better with human evaluation [19]. On ImagenHub [20], our approach attains the highest score of 0.533. On the reasoning-based Reason-Edit benchmark [17], our model achieves strong score of 0.561, second only to SmartEdit (0.572) [17], which is specially designed for reasoning removing and replacing operations. This demonstrates our methods strong editing ability, especially in complex reasoning settings. GoT framework shows consistently superior performance while maintaining competitive parameter efficiency (2.8B+3B) compared to approaches like SEEDIn the editing task, GoT framework (2.8B+14B) [13]. adopts αt = 4.0, αs = 3.0, and αr = 1.5. The evaluation prompt of GPT-4o is shown in Appendix Sec. 11.1. 6.3.2. Qualitative Results We present qualitative comparison of image editing with other models in Fig. 6. Our approach demonstrates superior performance across diverse editing scenarios that require Figure 6. Qualitative results of image editing. Our GoT framework demonstrates superior performance in settings that require semantic-spatial reasoning. Red bounding boxes indicate the coordinates predicted by MLLM within the GoT framework. semantic-spatial reasoning. The examples highlight our frameworks distinctive capabilities: First, our model accurately identifies and localizes objects referenced through indirect descriptions. Second, our approach handles complex spatial instructions effectively, such as removing specific signage or adding delicate elements to precise locations. Third, our framework excels at multi-step editing operations, as demonstrated in the bottom example. The red bounding boxes visible in our results indicate the coordinates predicted by the MLLM within the GoT framework, providing interpretable insight into how our system reasons Method GoT SSGM Pretrain GenEval ImagenHub Baseline + GoT + SSGM GoT Framework 0.38 0.40 0.42 0.64 0.176 0.181 0.370 0.533 benchmarks while enabling unprecedented interactive control through modifiable reasoning chains. By bridging the gap between human reasoning and visual creation, GoT introduces more intuitive and powerful approach to visual synthesis that aligns with natural cognitive processes. Table 3. Ablation study of our GoT framework on GenEval overall and ImagenHub GPT-4o eval. about spatial relationships during the editing process. 6.4. Ablation Study on Framework Design We conduct an ablation study to analyze the impact of different components in our framework. Table 3 presents the results of our study, where we progressively integrate different components into the baseline and evaluate their effects on GenEval and ImagenHub benchmarks. The baseline model leverages Qwen2.5-VL-3B and SDXL but does not incorporate GoT reasoning chains. It is trained with FLUX-GoT and OmniEdit-GoT for 10,000 steps. Adding GoT reasoning chains to the baseline model enables the LLM to achieve stronger semantic guidance capabilities. The reasoning process helps LLM plan for guidance in generation. Introducing the Semantic-Spatial Guidance Module (SSGM) further enhances model performance, particularly in image editing. SSGM provides spatial control over the diffusion model, ensuring that object placement aligns more accurately with the reasoning process. This enables finegrained editing, as reflected by the significant improvement in the ImagenHub evaluation. However, in GenEval, only the position category is notably affected by SSGM, which explains the relatively minor performance gain. Our final framework, which includes GoT reasoning, SSGM, and an extensive 60,000-step pretraining phase, achieves the highest scores, demonstrating the significant benefits of prolonged pretraining and the full model design. The ablation study confirms that each added component contributes positively to the overall performance, validating our framework design choices. 7. Conclusion (GoT), We presented Generation Chain-of-Thought paradigm that integrates MLLM reasoning capabilities into visual generation through explicit semantic-spatial reasoning chains. Our approach transforms visual generation from direct mapping into reasoning-guided process with precise spatial control, addressing limitations in existing methods that lack explicit understanding of object relationships and arrangements. Through largescale dataset construction (9M+ examples), novel and an end-toSemantic-Spatial Guidance Module, end training framework, GoT achieves state-of-the-art performance on text-to-image generation and editing"
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 3, 8 [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. 2, 3 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 2 [4] Frederic Boesel and Robin Rombach. Improving image editing models with generative data refinement. In The Second Tiny Papers Track at ICLR 2024, 2024. 8 [5] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1839218402, 2023. 3, 6, 8 [6] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 2, 3, [7] Rongyao Fang, Shilin Yan, Zhaoyang Huang, Jingqiu Zhou, Hao Tian, Jifeng Dai, and Hongsheng Li. Instructseq: Unifying vision tasks with instruction-conditioned multi-modal sequence generation. arXiv preprint arXiv:2311.18835, 2023. 3 [8] Rongyao Fang, Chengqi Duan, Kun Wang, Hao Li, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, Hongsheng Puma: Empowering unified mllm Li, and Xihui Liu. arXiv preprint with multi-granular visual generation. arXiv:2410.13861, 2024. 2 [9] Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and William Yang Wang. Layoutgpt: Compositional visual planning and generation with large language models. Advances in Neural Information Processing Systems, 36:1822518250, 2023. 2, 3 [10] Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and Zhe Gan. Guiding instruction-based image editing via multimodal large language models. arXiv preprint arXiv:2309.17102, 2023. 8 [11] Yulu Gan, Sungwoo Park, Alexander Schubert, Anthony Philippakis, and Ahmed Alaa. Instructcv: Instructiontuned text-to-image diffusion models as vision generalists. arXiv preprint arXiv:2310.00390, 2023. 3 [12] Yuying Ge, Sijie Zhao, Chen Li, Yixiao Ge, and Ying Shan. Seed-data-edit technical report: hybrid dataset for instructional image editing. arXiv preprint arXiv:2405.04007, 2024. 2, [13] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. 6, 8 [14] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating textto-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. 6, 7 [15] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 7 [16] Kaiyi Huang, Chengqi Duan, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench++: An enhanced and comprehensive benchmark for compositional text-to-image generation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. 3 [17] Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang, et al. Smartedit: Exploring complex instruction-based image editing with multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8362 8371, 2024. 3, 8 [18] Diederik Kingma, Max Welling, et al. Auto-encoding variational bayes, 2013. 6 [19] Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen. Viescore: Towards explainable metrics for arXiv preprint conditional arXiv:2312.14867, 2023. 8 image synthesis evaluation. [20] Max Ku, Tianle Li, Kai Zhang, Yujie Lu, Xingyu Fu, Wenwen Zhuang, and Wenhu Chen. Imagenhub: Standardizing the evaluation of conditional image generation models. arXiv preprint arXiv:2310.01596, 2023. 8 [21] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 2, 3, [22] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2251122521, 2023. 2, 3 [23] Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. Llmgrounded diffusion: Enhancing prompt understanding of text-to-image diffusion models with large language models. arXiv preprint arXiv:2305.13655, 2023. 2 [25] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 2, 3 [26] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with ringattention. arXiv e-prints, pages arXiv2402, 2024. 6 [27] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Liang Zhao, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. arXiv preprint arXiv:2411.07975, 2024. 6, 7 [28] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI conference on artificial intelligence, pages 42964304, 2024. 3 [29] Trong-Tung Nguyen, Duc-Anh Nguyen, Anh Tran, and Flexible and controllable Cuong Pham. diffusion-based object-centric image editing. arXiv preprint arXiv:2403.18605, 2024. 3 Flexedit: [30] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. [31] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. 3 [32] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 3, 6 [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 3 [34] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. 3 [35] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. 3 [36] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. 3, 6 [24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [37] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 3 [50] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation, 2024a. URL https://arxiv. org/abs/2410.13848, 2024. 2, 6, 7 [51] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. 4 [52] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and CUI Bin. Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal llms. In Forty-first International Conference on Machine Learning, 2024. 2, 3 [53] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionguided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023. 8 [54] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. [55] Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Lirui Zhao, Fu-Yun Wang, Zhanyu Ma, et al. Lumina-next: Making lumina-t2x stronger and faster with next-dit. arXiv preprint arXiv:2406.18583, 2024. 2 the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 3, 6 [38] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 3 [39] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. 2, 5 [40] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8871 8879, 2024. 3, 8 [41] Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, et al. Journeydb: benchmark for generative image understanding. Advances in neural information processing systems, 36:4965949678, 2023. 2, 5 [42] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [43] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1439814409, 2024. 3 [44] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 2, 3, 6 [45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 3 [46] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 4 [47] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 2, 6 [48] Cong Wei, Zheyang Xiong, Weiming Ren, Xinrun Du, Ge Zhang, and Wenhu Chen. Omniedit: Building image editing generalist models through specialist supervision. arXiv preprint arXiv:2411.07199, 2024. 2, [49] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. GoT: Unleashing Reasoning Capability of Multimodal Large Language Model for Visual Generation and Editing"
        },
        {
            "title": "Supplementary Material",
            "content": "8. Training Details We pretrain our model for 60,000 steps on LAHR-GoT, JourneyDB-GoT, and OmniEdit-GoT. We adopt cosine learning rate scheduler with 500 warmup steps and maximum learning rate of 1 104. During the fine-tuning stage, we train the model on FLUX-GoT, OmniEdit-GoT, and SEED-Edit-MultiTurnGoT for 10,000 steps. In this phase, we set the warmup steps to 200 and the maximum learning rate to 5 105. For both stages, we use the Adam optimizer with β1 = 0.9, β2 = 0.98, and ϵ = 1 106. We also apply weight decay of 0.05 during training. The number of batch size is set to 128. The LLM is fine-tuned using LoRA with = 32, LoRA alpha set to 32, and LoRA dropout rate of 0.05. For diffusion, we introduce noise offset of 0.1. 9. Visualization Results 9.1. Qualitative Analysis of Image Editing and Interactive Generation We provide additional examples to demonstrate the capabilities of the GoT framework. Figure 7 illustrates the image editing performance of our model. Additionally, we present the corresponding GoT content generated alongside each sample. Further examples of interactive generation using our model are shown in Figure 8. 9.2. Visualization of Multi-Guidance Strategy Hyperparameter Selection We analyze the effect of hyperparameter selection in the Multi-Guidance Strategy on the generated images, as depicted in Figure 9. The definitions of these hyperparameters are provided in Section 5.3. 10. GoT Format and Examples This section presents examples of the GoT format in our dataset. The GoT structure varies across different tasks, including text-to-image (T2I) generation, single-turn editing, and multi-turn editing. For text-to-image generation, Figure 10 showcases examples from FLUX-GoT, JourneyDB-GoT, and LAHRGoT. Our GoT format represents the structured planning process of the upstream model in generating image content. It provides detailed breakdown of the various components within an image and their spatial relationships. To enhance spatial understanding, we append location information to key objects within the GoT representation. Figure 11 illustrates the GoT format for image editing within our dataset. For single-turn editing, GoT represents the reasoning plan of the upstream model for specific editing action. It consists of description of the source image, the object to be modified, the specific editing operation, and the resulting edited image. This structured process ensures step-by-step transformation, beginning with the original image, identifying the target object, applying the specified modification, and generating the edited image. For multi-turn editing, GoT follows more complex structure, as it must encapsulate the breakdown of an instruction into sequence of consecutive steps. In practice, we first generate description of the source image, then decompose the multi-turn instruction into series of stepby-step editing commands. At each step, GoT operates as single-turn editing process, specifying the object to be modified along with the corresponding transformation. Finally, the process concludes with description of the fully edited image. Furthermore, for image editing tasks, positional information is appended to each object to enhance spatial comprehension. 11. Prompts for Evaluation and Dataset Construction 11.1. Prompts for Evaluating Image Editing Performance We provide the prompts used for evaluating image editing performance with GPT-4o in Figure 12. We are using GPT4o-2024-11-20. The final score is the average of the minimum value of the two scores for each sample. 11.2. Prompts for Text-to-Image Data Construction Figures 13, 14, and 16 present the key prompts utilized in text-to-image data preparation. 11.3. Prompts for Image Editing Data Construction Figures 1520 illustrate the key-step prompts employed in image editing data preparation. Figure 7. More samples on image editing with the GoT content generated by our model. Figure 8. More examples on interactive generation. Figure 9. Visualization on Multi-Guidance Strategy Hyper-parameter Selection. The above are text-to-image samples generated by GoT framework under different hyper-parameters. Figure 10. Examples of GoT dataset for text-to-image generation, including FLUX-GoT, JourneyDB-GoT, and Laion-Aesthetics-HighResolution-GoT. Figure 11. Examples of GoT dataset for image editing, including OmniEdit-GoT for single-turn editing and SEED-Edit-Multiturn-GoT for multi-turn editing. Human: You are professional digital artist. You will have to evaluate the effectiveness of the AI-generated image(s) based on the given rules. You will have to give your output in this way (Keep your reasoning concise and short.): score : [...], reasoning : ... and dont output anything else. Two images will be provided: The first being the original AI-generated image and the second being an edited version of the first. The objective is to evaluate how successfully the editing instruction has been executed in the second image. Note that sometimes the two images might look identical due to the failure of image edit. From scale 0 to 10: score from 0 to 10 will be given based on the success of the editing. - 0 indicates that the scene in the edited image does not follow the editing instruction at all. - 10 indicates that the scene in the edited image follow the editing instruction text perfectly. - If the object in the instruction is not present in the original image at all, the score will be 0. second score from 0 to 10 will rate the degree of overediting in the second image. - 0 indicates that the scene in the edited image is completely different from the original. - 10 indicates that the edited image can be recognized as minimal edited yet effective version of original. Put the score in list such that output score = [score1, score2], where score1 evaluates the editing success and score2 evaluates the degree of overediting. Editing instruction: <instruction> <Image> Source Image </Image> <Image> Edited Image </Image> Assistant: Figure 12. Prompt for GPT4-o image editing evaluation. We are using GPT-4o-2024-11-20. The final score is the average of the minimum value of the two scores for each sample. Human: <Image> Image </Image> You are an advanced AI visual assistant specializing in highly detailed and comprehensive visual analysis for one image. Your role is to generate single, descriptive paragraph that encapsulates all relevant details about an image. Here is the provided image prompt for this image: <prompt>. If the provided prompt aligns with the image, enhance it by adding detailed observations about the objects, their colors, shapes, textures, numeracy, and spatial relationships. If the provided prompt does not match the image content, disregard it and craft complete description based solely on the visual elements you observe. Consider the 2D-spatial relationships (e.g., to the left of, near, aligned with) and 3D-spatial relationships (e.g., in front of, above, at distance from) when describing the scene. Include details about the overall composition, highlighting how elements are arranged relative to each other, their groupings, and any complex interactions or dynamic elements within the scene. Pay close attention to the interplay of colors, textures, and shapes, ensuring that the description reflects both the visual richness and structural composition of the image. Ensure to provide the description as one single paragraph, without preamble or additional explanation. Assistant: Figure 13. Prompt for detailed recaption for text-to-image data. Human: You are tasked with identifying and extracting all the real object names from detailed caption. An object name refers to any tangible or physical entity mentioned in the caption that can be visually grounded in the image. Ensure not to include any adjectives or single-word descriptions that do not refer to specific object, such as background. Please follow these instructions: Identify all object names in the caption in the order they appear. Maintain the exact wording of each object name as it is in the caption, including case consistency. Output the object names in Python list format. For example, consider the following caption: Example 1: In the image, person is prominently featured at vibrant pride parade, exuding confidence and pride. They are adorned in an extravagant outfit that mirrors the rainbow flag, with deep V-neck top in bold, colorful stripes of red, orange, yellow, green, blue, and purple. The persons hair is styled in striking rainbow color, complementing their outfit. They are surrounded by lively crowd, with individuals wearing various colors and accessories, adding to the festive atmosphere. The background reveals bustling street scene with buildings and trees, suggesting an urban setting. The overall composition is dynamic, with the person at the center, drawing attention to their vibrant attire and the energetic parade around them. Your output should be list of object names like this: [person, pride parade, outfit, V-neck top, The persons hair, lively crowd, individuals, street, buildings, trees] Example 2: The image depicts young boy with slender features and pale complexion, exuding an air of arrogance and coldness. His white-blonde hair is slicked back, adding to his composed demeanor. The boys eyes are striking shade of cold grey, reflecting sense of detachment and intelligence. He is dressed in white shirt with blue and white patterned collar, which contrasts with his pale skin and adds touch of elegance to his appearance. The overall composition is balanced, with the boy centrally positioned against dark background that accentuates his features and the sharpness of his expression. The interplay of colors, textures, and shapes creates visually striking and emotionally charged image. Your output should be list of object names like this: [young boy, white-blonde hair, \"The boys eyes\", white shirt] Now, given the following caption, extract the object names in the same format: <caption> Assistant: Figure 14. Prompt for identifying objects in text-to-image caption. Human: Please tell me according to the instruction: <instruction>. Which object is being replaced with another object? Please only answer the exact name of the two objects using the same words from the instruction. Use the format of Python list including the two object names. The first is the object and the second is the another object. Assistant: Figure 15. An example of prompt for parsing the edited object. This is used when the task type is replace. Human: <Image> Image </Image> Please provide the bounding box coordinates of this sentence describes: <object name> Assistant: Figure 16. Prompt for grounding object. This works for both text-to-image and image editing data. Human: <Image> Image </Image> You are an AI visual assistant, and you are seeing single image. Please describe this image in one paragraph using no more than two sentences. Always remember to include describing <object name> in the image. Assistant: Figure 17. Prompt for image description for image editing data. Human: <Image> Cropped Image </Image> Please describe <object name> briefly in several words no more than one sentence. Assistant: Figure 18. Prompt for cropped image object description for image editing. Human: You are helpful visual assistant. have an image editing data with the original instruction: <instructions>. want to augment the instruction to obtain more free-language format instructions. Your task is to rewrite this original instruction in English into three distinct, human-like, free-form instructions that convey the same meaning but use varied language and phrasing. The new instructions should reflect how humans might naturally request image edits. Please provide me with three more instructions that have the same meaning as the original instruction but in more free-language format. The new instruction can be in any format that human might input as an editing instruction. The first instruction should be relatively concise. Use the format of Python list which includes three items as strings. Assistant: Figure 19. Prompt for reinstruction for image editing data. have image editing data with the following information: Human: You are helpful assistant for designer. instruction: <instructions>, description of source image: <source desc>, description of target image: <target desc>, <coord> <object desc> Assume you are visual assistant with access to the edit instruction and the source image. Your task is to provide step-by-step chain of thought for the image editing process which only includes the image editing processes. The chain of thought can includes the following several type steps (can not in this order, not includes these words in the answer): Describe the source image; the object to be edited; the specific area to be edited; Identify the specific changes to be made; Describe the image after the edit. All information besides the instruction should be considered as derived from the source image. The output is meant to train multi-modal large language model that takes the source image and instruction as input, generates the editing chain of thought, and then outputs the edited image. Therefore, your response should consider this application and provide clear, concise reasoning in numbered steps (1. 2. 3. ... etc). The response should be purely reasoning text and formatted succinctly. Ensure your answer be brief and few steps. In context learning, example 1: 1. The source image shows grand, classical building with intricate stone carvings and statues. One prominent statue, female figure, stands on pedestal, holding torch and book. The building features arched windows and sign that reads Learning Center. 2. The object to be edited is statue of woman holding torch and book. 3. The specific area to be edited is defined by the bounding box coordinates ((554, 166), (768, 711)), which encompasses the statue. 4. Remove the statue completely from the image while maintaining the surrounding architectural details and other elements like the buildings facade, arched windows, and the Learning Center sign. 5. The edited image will show the grand, classical building with intricate stone carvings and the Learning Center sign. The statue, female figure holding torch and book, will no longer be present, and the area where the statue was will appear seamless with the surrounding architecture. The buildings arched windows and stone facade will remain intact. In context learning, example 2: 1. The source image depicts snowy mountain slope with ski board in the foreground, indicating skiing or snowboarding activity. The background features clear blue sky and rocky terrain, suggesting high-altitude or alpine setting. 2. The inserted object is skier in black jacket, complete with goggles, sitting on snowboard. This skier will be positioned in the center of the slope, facing downhill, sitting on snowboard. 3. The specific area to be edited is within the bounding box ((382, 303), (782, 813)), where the current object (a ski board) is located. This area needs to be replaced with the new skier. 4. The image now shows skier dressed in black jacket and goggles, sitting on snowboard on snowy slope. The background features clear blue sky and rocky terrain, with other skiers and equipment visible in the distance. The skier is positioned in the middle of the slope, looking downhill, seamlessly blending with the existing scene. In context learning, example 3: 1. The source image depicts group of women and child standing on beach, all dressed in vibrant, summery outfits. The scene is bright and cheerful, with the ocean and sky forming picturesque backdrop. The style of the image is casual and candid, capturing moment of joy and togetherness. 2. The edited area is ((0, 0), (999, 999)), which is the whole image. The object to be edited is the group of women and the child, along with the beach and the background elements. These need to be transformed into traditional Chinese ink painting style. 3. After the edit, the image will depict group of women and child standing in traditional Chinese ink painting style, dressed in elegant, flowing garments. They will be positioned against backdrop of serene mountains and tranquil sea, with the overall composition reflecting the classical and detailed style of traditional Chinese ink paintings. Assistant: Figure 20. In-context assembling GoT prompt for image editing data."
        }
    ],
    "affiliations": [
        "BUAA",
        "CUHK MMLab",
        "HKU",
        "SenseTime",
        "Shanghai AI Laboratory",
        "THU"
    ]
}