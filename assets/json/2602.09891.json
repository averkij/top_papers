{
    "paper_title": "Stemphonic: All-at-once Flexible Multi-stem Music Generation",
    "authors": [
        "Shih-Lun Wu",
        "Ge Zhu",
        "Juan-Pablo Caceres",
        "Cheng-Zhi Anna Huang",
        "Nicholas J. Bryan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Music stem generation, the task of producing musically-synchronized and isolated instrument audio clips, offers the potential of greater user control and better alignment with musician workflows compared to conventional text-to-music models. Existing stem generation approaches, however, either rely on fixed architectures that output a predefined set of stems in parallel, or generate only one stem at a time, resulting in slow inference despite flexibility in stem combination. We propose Stemphonic, a diffusion-/flow-based framework that overcomes this trade-off and generates a variable set of synchronized stems in one inference pass. During training, we treat each stem as a batch element, group synchronized stems in a batch, and apply a shared noise latent to each group. At inference-time, we use a shared initial noise latent and stem-specific text inputs to generate synchronized multi-stem outputs in one pass. We further expand our approach to enable one-pass conditional multi-stem generation and stem-wise activity controls to empower users to iteratively generate and orchestrate the temporal layering of a mix. We benchmark our results on multiple open-source stem evaluation sets and show that Stemphonic produces higher-quality outputs while accelerating the full mix generation process by 25 to 50%. Demos at: https://stemphonic-demo.vercel.app."
        },
        {
            "title": "Start",
            "content": "STEMPHONIC: ALL-AT-ONCE FLEXIBLE MULTI-STEM MUSIC GENERATION Shih-Lun Wu 42* Ge Zhu 2 Juan-Pablo Caceres"
        },
        {
            "title": "MIT CSAIL",
            "content": "2 2 Cheng-Zhi Anna Huang 4 Nicholas J. Bryan"
        },
        {
            "title": "Adobe Research",
            "content": "6 2 0 2 0 1 ] . [ 1 1 9 8 9 0 . 2 0 6 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Music stem generation, the task of producing musically-synchronized and isolated instrument audio clips, offers the potential of greater user control and better alignment with musician workflows compared to conventional text-to-music models. Existing stem generation approaches, however, either rely on fixed architectures that output predefined set of stems in parallel, or generate only one stem at time, resulting in slow sequential inference despite flexibility in stem combination. We propose STEMPHONIC, diffusion-/flow-based framework that overcomes this trade-off and generates variable set of synchronized stems in one inference pass. During training, we treat each stem as batch element, group synchronized stems in batch, and apply shared noise latent to each group. At inferencetime, we use shared initial noise latent and stem-specific text inputs to generate synchronized multi-stem outputs in one pass. We further expand our approach to enable one-pass conditional multi-stem generation and stem-wise activity controls to empower users to iteratively generate and orchestrate the temporal layering of mix. We benchmark our results on multiple open-source stem evaluation sets and show that STEMPHONIC produces higher-quality outputs while accelerating the full mix generation process by 2550%. Demos at: https://stemphonic-demo.vercel.app. Index Terms music audio generation, stem generation, conditional stem generation, variable stem combinations, diffusion, flow. 1. INTRODUCTION Text-to-audio music generation models are now able to produce realistic sounding music from simple text inputs [15]. They lower the barrier for music creation, enabling anyone to explore and express their creativity, but typically generate fully-mixed multi-instrument outputs that are difficult to edit and cannot easily be reused as components in new compositions [6,7]. To empower creators beyond text prompting, numerous control and editing methods have been proposed, including fine-grained temporal controls [810], music inpainting [1012], as well as music stem generation [1316]. music stem is recording of one or more instruments that collectively serve as distinct layer in mix, e.g., drums for rhythmic foundation, and basses for low-pitch progressions. Generating stems enables creators to edit each stem separately and experiment with different mixing/mastering techniques, enhancing their creative control [17, 18]. Existing stem generation methods can largely be classified into (1) models with parallelized architecture, and (2) individual-stem models that require sequential generation of stems. Parallelized models [1316] generate multiple coherent stems in single pass, but handle limited, coarse-grained stem types (e.g., only basses, drums, vocals, others) that must be fixed in advance and built into the architecture. Individual-stem models [1923], on the other hand, allow * Work done while an intern at Adobe Research. Fig. 1. Our STEMPHONIC framework for flexible multi-stem music generation. (Top) At training, each group of synchronized stems share the same noise latent. (Bottom) At inference, we use shared initial noise to generate variable multi-stem outputs in one pass. We also enable conditional stem generation and stem-wise activity controls. flexible open-vocabulary stem generations through text prompting or other conditioning mechanisms, and can often condition on existing audio to iteratively generate new accompanying stems and create mix with an arbitrary number of stems. Yet, these models generate stems one at time, leading to slower full inference processes. To unify the strengths of both paradigms and alleviate their drawbacks, we propose STEMPHONIC, latent diffusion/flow-based [24, 25] framework capable of generating variable set of musicallysynchronized stems in one inference pass, as shown in Fig. 1. We introduce two techniques applied during training. First, we treat each stem as batch element and group musically-synchronized stems in batch (Sec. 3.1). Then, we assign single shared noise latent to each group (Sec. 3.2). At inference, we use shared initial noise and different stem-specific text inputs to generate variably many synchronized stem outputs in one pass. We further expand our approach to conditional multi-stem generation, and add stem-wise activity controls (Sec. 3.3) that enable creators to iteratively generate and orchestrate the temporal layering of mix. In our experiments, we ablate our stem grouping and noise sharing core techniques, and verify the conditional generation and stem-wise activity control capabilities. We find STEMPHONIC capable of generating higher-quality multi-stem mixes, while accelerating the full inference process by 2550%, compared to the existing individual-stem iterative workflow. Our contributions can be summarized as follows: latent diffusion/flow-based framework to generate variable number of synchronized stems efficiently in one inference pass, either from-scratch, or conditioned on existing audio. method for precise activity control of individual stem outputs. Quantitative evaluation and qualitative demo examples showing high-quality outputs, and flexible composer-like workflows. Overall, we combine the speed of parallelized models with the openvocabulary freedom of individual-stem models to offer an efficient and highly-controllable stem generation framework. 2. BACKGROUND We build upon latent diffusion [26] and rectified flow (RF) [24] generative models with Transformer backbone [27], referred to as diffusion Transformer (DiT) for simplicity. We initialize our DiT with weights from base model pretrained on general music mixes, and then finetune it on isolated stems with the following RF objective: min θ (xk,Ck) data ϵ (0, ITDTD ) LogitNormalµ,σ (0,1) (cid:13) (cid:13) (cid:13)vθ (cid:0)xk(t), t, Ck (cid:1) (cid:0)xk ϵ(cid:1)(cid:13) 2 (cid:13) (cid:13) 2 , (1) where xk RTD is the variational autoencoder (VAE) audio latents being modeled, is the stem index, is the number of VAE latent frames (i.e., the time dimension), is the frame-wise dimensionality, θ is the set of trainable DiT parameters, Ck is any arbitrary conditions associated with stem xk that enable user control (e.g., stem type, audio mix of existing stems, text description, and tempo), ϵ RTD is the gaussian initial noise, (0, 1) is the sampled noise level (i.e., diffusion/flow timestep), xk(t) := (1 t)xk + tϵ is the noised VAE latents the model receives, and vθ() is the velocity predicted by the model to denoise the latents. During inference, we solve (cid:0)xk(t), Ck, t(cid:1)dt, the associated probability flow ODE, dxk(t) = vθ which deterministically transports the initial gaussian noise to realistic data via Euler discretization [28]. 3. METHOD We propose novel framework capable of generating variable number/set of musically-synchronized stems in one pass. We achieve this with two main techniques to intervene in training batch construction, i.e., stem grouping and noise sharing, which instill an inductive bias for inter-stem cohesion and synchronization directly into the model. 3.1. Stem data & grouping We finetune the DiT on dataset of isolated stem audio clips. Stems from the same composition can be linearly combined to form cohesive musical mix wmix := (cid:80) k(gk wk), where gk is the user-adjustable gain for each individual stem, and wmix, wk are raw audio waveforms of the mix and each stem. We further define sub-mix as the mixture of subset of one or more stems in full mix. We process individual stereo 44.1kHz waveforms of stems, mixes, and sub-mixes using the same VAE audio-to-latent encoder and latent-toaudio decoder [3, 14, 15, 29], obtaining the VAE latents, i.e., xks, for stem finetuning. We develop stem grouping mechanism ensuring that musically-synchronized stems appear together in single training batch. This is in contrast to the standard batch construction mechanism where individual stems are independently sampled [2022]. We let be our training dataset of multi-stem compositions (or mixes), in which each mix is denoted by its constituent stems VAE latents (m) = {x(m) k=1 , where {1, . . . , M} indexes the mix and (m) is the mth mixs number of constituent stems. We also }K(m) let be training batch (of stems) to be sampled, with fixed batch size B. To construct B, we run loop where in each iteration, indexed by N, we sample mix m(l) {1, . . . , M}, and then sample subset of the mixs constituent stems (l) (m(l)) to obtain (l) target stems as batch elements. The loop runs until the number of target stems reaches batch size B, and hence the number of groups per batch, denoted by L, is dependent on the sampled groups. We sample subsets from mix, rather than always including the entire mix, to combinatorially increase data variability. To support iterative creation workflows, we also train our model to generate new stems conditioned on an existing stem or sub-mix, similar to [21]. Operationally, we randomly select half of the groups in batch for this conditional generation task. For those selected groups, we sample subset from the left out stems of the corresponding mix, i.e., (m(l)) (l), sum their waveforms together, and then obtain the VAE latents of the sub-mix as the condition, i.e., x(l) cond RTD. Note that this is effectively treating the sub-mix as part of the conditions Ck (cf. Eqn. (1)) that is shared among each (l). This condition is channel-wise output stem in group x(l) concatenated to the noised target stem latents x(l) (t) as the input to our DiT model. For groups that are not selected (i.e., those training for the from-scratch generation task), we impute x(l) cond with zeros. 3.2. Diffusion/flow noise sharing Even with stem grouping, remaining challenge is that multiple stem groups, each corresponding to distinct mix, are present in the same batch. Since individual stems are each batch element, in the models computation graph, every stem is separate regardless of the groupings. The only shared information among stems in the same group comes from portions of Ck, which include parts of the text prompt (more details in Sec. 3.4), and the conditioning sub-mix x(l) cond in conditional generation cases. Therefore, additional signals are required to more strongly inform the grouping information for the model to learn the inter-stem alignment and orchestration well. To achieve this, we intervene in the noise sampling process. The initial noise ϵ is high-dimensional variable that provides the source of diversity for generation after conditioning on Ck. Conventionally, ϵ is sampled independently for batch elements assuming they are i.i.d. However, since our batches now contain groups of correlated stems, we can leverage the noise latents high dimensionality (RTD) as powerful signal to indicate groupings. Specifically, for each group {1, . . . , L}, we sample shared initial noise ϵ(l) to be applied to all stems in the group, i.e., the noise is paired to stems as: (ϵ(l), x(l) ) x(l) (l); ϵ(l) (0, ITDTD) . (2) By doing so, we ensure that musically-synchronized stems in group receive shared noise, while different groups (which satisfy i.i.d.) still receive independent noises, strongly enforcing the grouping property. At inference, we assign one random initial noise shared across all batch elements to generate musically-synchronized stems. Although implemented here with the RF objective, our noise sharing technique is generally applicable to any diffusionor flow-based model that utilizes high-dimensional noise latent. 3.3. Stem-wise activity control In addition to generating well-synchronized stems, to grant users more compositional agency, especially when generating large number of stems (e.g., > 5), we devise mechanism for stem-wise activity control. This empowers users to precisely layer the final mix by specifying the temporal activity of each stem. Operationally, we run simple loudness-based silence detection on the raw waveform of each stem, wk. Based on preset cutoff loudness (e.g., 60 dB), we obtain list of silence segments. We then convert/quantize it into binary, single-channel sequence denoted by ak {0, 1}T1, where 1 indicates active and 0 silent, and add it to the conditions Ck. (Note the time dimension is consistent with that of target stem latents xk.) We learn small (16-dim.) embedding for activity indicators 1 and 0, and concatenate the sequence of embeddings channel-wise to the noised target xk(t) before entering the DiT, mirroring the sub-mix conditioning mechanism (cf. Sec. 3.1). 3.4. Text conditioning Our model supports free text control as in conventional text-to-music models [2, 3, 5]. Compared to prior art on text-conditioned individual stem generation [19, 21, 22], or fixed-combination parallelized stem generation [14, 16], our method requires combining both global (i.e., shared across stems in group) and stem-wise descriptions in one text instruction. We formalize this by placing the stem-wise part first, followed by the global part. An example prompt is: Generate the stem: [GUITARS] (given context stems: [DRUMS, BASSES]) for some music described as: Relaxing country rock . . . . The given context stems clause is only used for conditional generation (i.e., not for from-scratch cases) and is randomly dropped out at training. 4. EXPERIMENTS 4.1. Implementation details Our model architecture is similar to that of Stable Audio Open [30] VAE [29] compresses stereo 44.1 kHz waveforms into latents (with = 64 dimensions) at 12 Hz frame rate. billion parametersized diffusion transformer (DiT) [27] models the VAE latents, and text conditions enter the DiT as T5-XXL [31] embeddings via cross attention. Besides the conditions introduced in Sec. 3, the model is also conditioned on tempo using bpm values. All conditions are independently dropped out 1/3 of the time to enable classifier-free guidance (CFG) [32]. We pretrain on full audio mixes before finetuning on stems with our proposed techniques. The model is trained on 32-second segments (equates to # of frames = 394), with per-GPU batch size of 1024 seconds, and an effective batch size (per gradient step) of 16K seconds. We use AdamW optimizer [33] with constant 104 lr. Stem generation training converges at 30K gradient steps, which takes 3 days on 8 A100 (80G) GPUs. At inference, we use first-order Euler sampler with 32 steps. We apply CFG to all conditions only at steps 328 [34], with the CFG scale set to 3.0. 4.2. Datasets We pretrain on 20K hours of licensed music mixes, and then finetune on licensed stems corresponding to 400 hours of mixes (with 6 stems/mix). Both datasets contain mix-level text descriptions and tempo (bpm) metadata, with an average track duration of 2.5 minutes. While our stem dataset comprises more than 50 stem types, the following 11 types constitute the vast majority: drums, basses, percussion, synths, keys, guitars, strings, SFX, vocals, synth-vocals, and winds. For evaluation, following recent literature [22], we use opensource stem-separated datasets: MoiseDB [35] and MusDB [36], both consisting of around 10 hours of mixes. To construct evaluation examples, we crop the mixes into 32-second segments, and include all stems with 50% activity in the segment. We leverage Qwen2.5Omni [37], multimodal large language model with music analysis abilities, to obtain the text descriptions, and an in-house version of Madmom beat tracker [38] to estimate the tempo (bpm). We map the stem types in MoiseDB and MusDB to our top-11 stem types above largely with rules; for those originally tagged other*, we resort to Qwen2.5-Omni labeling and then manual checking/correction. The top (>100 occurrences) stem types in the MoisesDB dataset include: drums, basses, guitars, vocals, piano, keys, percussion, and string. For MusDB, the top stem types are drums, basses, vocals, guitars, keys, and synths. After segmentation, we get around 1.5K mixes (i.e., grouped stems) for MoisesDB, and about 1K for MusDB. The mean # of stems per mix is 4.6 (stdev 1.3) for MoisesDB, and 3.5 (stdev 0.7) for MusDB. 4.3. Baselines & experimental setup Our experiments are structured in three sets. For our main ablations, we study our core capability of generating variably many musicallysynchronized stems in one inference pass. To do so, we apply stem grouping (Sec. 3.1) and noise sharing (Sec. 3.2) (we dub this setup C), or only stem grouping (B), or neither (A), resulting in three trained models. Note that, whether or not noise sharing is applied at training, we have the option to share the noise at inference among the stems meant to be synchronized, as training-free measure to promote inter-stem coherence. This gives us two inference-time variations: sharing the noise (dubbed (ii)) or not ((i)). To summarize, our full setup is C-(ii), and the most ablated setup, A-(i), represents the case of naively stretching existing individual-stem models [19, 21, 22] (with minimal operational modifications) to multi-stem generation. Next, we recognize that with the basic A-(i) setup, we can still generate variable K-stem mixes by taking inference passes. Thus, we need to demonstrate the holistic workflow improvement our C-(ii) setup brings about. Thus, we compare generating stems from scratch using (1) passes with A-(i), (2) 1 pass with C-(ii), and (3) 2 passes with C-(ii), middle ground showing our models fromscratch and conditional generation abilities,1 and the unique flexibility that lets users take however many (or few) passes they see fit. Here, we evaluate both the generation quality and total inference time here. As final experiment, we train our full model with the stemwise activity controls (Sec. 3.3). We then evaluate two key aspects: the effectiveness of the control itself, and its impact on multi-stem generation quality in scenarios both with and without the control applied at inference. We note that during all inference, no groundtruth stem audios in the evaluation sets are used; we only use the derived metadata (i.e., {text prompt, stem types, tempo}) as inputs. All inference is done on 1 A100 (80G) GPU. The generated stems are mixed in way that preserves the relative loudness (between stems) decided by the model, and then globally normalized to 16 dBFS. 4.4. Evaluation metrics We evaluate generated audios according to the following key aspects. Stem Control: We compute the Fréchet Audio Distance (FAD) [39] between reference and generated stem audios using VGGish [40] feature extractor. We treat all audios of each stem type in MoiseDB or MusDB as distinct reference set, and report the macro-average across all stem types. We abbreviate this as FADstem. Mix Quality: We also measure the FAD (shorthand FADmix), but here we treat groundtruth mixes as the reference. Note that this also 1Half of the stems are generated in the 1st pass, and the remaining half in the 2nd pass, conditioned on the sub-mix of 1st-pass output stems. If is odd, one more stem is assigned to 1st pass. Table 1. Ablations on core techniques: stem grouping and noise sharing. We generate all constituent stems in mix in one pass. Our full setting, C-(ii), generally outperforms all other ablated settings, with particularly strong gains on the more challenging MoisesDB dataset. (n is the number of mixes in each evaluation dataset.) Train Infer. MoisesDB (n = 1488) / MusDB (n = 964) Setting stem grp. noise share noise share Stem Ctrl FADstem Mix Quality FADmix Mix Text Ctrl CLAP A-(i) A-(ii) B-(i) B-(ii) C-(ii) 2.69 / 2.91 2.80 / 3.02 2.41 / 2.92 2.41 / 2.97 1.84 / 1.09 1.78 / 1.24 1.55 / 0.91 1.53 / 1.10 28.82 / 28.73 28.67 / 28.28 28.85 / 29.14 28.93 / 28.76 2.31 / 2.72 1.25 / 1.05 30.19 / 29. Table 2. Workflow improvement on generating K-stem mixes over conventional iterative (A-(i), K-passes) baseline. Our model, C-(ii), offers the unique freedom to tradeoff speed for quality: the 1-pass setup provides 50%+ speedup, while the 2-pass setup achieves the overall best generations (FADmix and CLAP) and still saves 2550% of total inference time. Stem groups & prompts are from MoisesDB. Setting A-(i) C-(ii) # infer. passes 2 1 = 3 (n = 190) = 5 (n = 379) / = 4 (n = 456) / = 6 (n = 283) FADmix CLAP time (s) 1.03 / 1.02 1.48 / 2.09 0.78 / 0.83 1.34 / 1.92 1.06 / 1.12 1.56 / 2.29 30.65 / 30.34 29.65 / 30.67 30.62 / 31.23 30.32 / 30. 29.51 / 30.44 30.26 / 29.93 4.10 / 5.50 6.88 / 8.28 3.03 / 3.27 3.70 / 4.16 2.00 / 2.52 3.13 / 3.60 evaluates stem compatibility since if the stems are high-quality but unsynchronized, the mix would sound unlike any real mixes.2 Mix Text Control: We compute the pairwise cosine similarity between the audio embeddings of the generated mix, and the text embeddings of global text description, both obtained from the contrastively trained CLAP model [42], using the music checkpoint. For stem-wise activity control, we run silence detection algorithm on the output audio to get the activity sequence, and compute the framewise F1 score w.r.t. the input control ak. Such cycle consistencybased evaluation has also been used for existing temporal controls [8]. 5. RESULTS & DISCUSSION Results of the core one-pass generation ablations are shown in Table 1. Our full C-(ii) setting overall produces the best generations, especially on the more challenging MoisesDB evaluation set with more stems per mix. Qualitatively, the baseline A-(i) setup consistently fails to produce synchronized stems, resulting in incoherent mixes. The intermediate settings (A-(ii), B-(i), B-(ii)) show marked improvement, occasionally producing musically-synchronized stems, despite being much less consistent than our full C-(ii) setup. This gives some evidence that (1) the initial noise does capture semantics 2We also investigate the COCOLA metric [41] for stem compatibility, but find it ineffective, likely due to domain mismatch between its training data (mainly synthesized audios) and ours (largely studio-recorded audios). Table 3. Evaluating stem activity control. Our mechanism (Sec. 3.3) enables near-perfect activity control (framewise F1) with the flexibility to use it or not at inference, only at moderate cost of other metrics. (F1 is evaluated only on partially active stems.) Train Infer. MoisesDB (n = 1488) / MusDB (n = 964) Setting C-(ii) C-(ii) +Act act ctrl act ctrl Stem Ctrl FADstem Mix Quality FADmix Mix Text Ctrl CLAP Act Ctrl F1 (%) 2.31 / 2.72 2.66 / 2.74 2.47 / 2. 1.25 / 1.05 1.54 / 1.08 1.46 / 1.13 30.19 / 29.27 28.78 / 28.94 29.55 / 29.14 n.a. n.a. 99.42 / 99.43 in rhythm and harmony such that sharing noise only at inference (i.e, *-(ii)) enforces some degree of cohesiveness, and (2) the shared information among grouped stems present in setting B-(*), e.g., text prompts, also induces inter-stem cohesion, though being less effective than sharing the high-dimensional noise as in C-(ii). Table 2 compares K-stem workflows between A-(i) (akin to individual-stem models [19, 21, 22]), and those made possible by our C-(ii) setup. The insights are: when given inference passes, A-(i) can generate comparable or slightly better mixes than our C-(ii), 1-pass workflow, but takes at least 2 as much time. Meanwhile, our C-(ii), 2-pass workflow achieves generally the best generations with consistently lower FADmix, while still being 2550% faster than A-(i) with the time advantage increasing with stem count K. We attribute the superior output quality of the 2-pass workflow to an inductive bias created at trainingSince our subset sampling strategy (cf. Sec. 3.1) induces binomial distribution of group sizes that peaks at K/2, the model is inherently better at generating groups of this size. Finally, Table 3 shows the performance of the model trained with stem-wise activity controls (cf. Sec. 3.3). The Frame F1 scores prove that our activity controls are near-perfect. Other metrics take slight hit, qualitatively due mainly to audio cleanliness degradation rather than loss of inter-stem cohesion, as tradeoff for the additional controls and the flexibility to optionally apply them at inference. We encourage readers to visit our demo website https: //stemphonic-demo.vercel.app, which showcases both static generations and composer-like generations, where the authors act as real-world user and iteratively shape the final mix. 6. CONCLUSIONS & FUTURE WORK We proposed STEMPHONIC, diffusion-/flow-based framework that enables one-pass generation of variable combinations of musicallysynchronized stems. This was achieved with our proposed trainingtime interventions: stem grouping and noise sharing. We further supported conditional multi-stem generation and stem-wise activity controls. Our experiments show that STEMPHONIC not only accelerates the multi-stem generation workflow by 2550% over the existing iterative baseline, but also produces higher-quality mixes. Our work opens several promising avenues for future research. deeper theoretical analysis of the noise sharing mechanism could yield fundamental insights into using high-dimensional noise as an explicit conditioning signal in generative models. For user interaction, moving beyond simple stem type tags to free-text descriptions for individual stems remains an important next step for more precise control. Furthermore, research into methods for controlling stem-wise musical novelty [43], or for agentically suggesting stem combination presets [44] given global text prompt, can significantly enhance user engagement and collaborative potential of such systems. 7. REFERENCES [1] Andrea Agostinelli, Timo Denk, Zalán Borsos, Jesse Engel, et al., MusicLM: Generating music from text, arXiv:2301.11325, 2023. [2] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre Défossez, Simple and controllable music generation, in NeurIPS, 2023. [3] Zach Evans, CJ Carr, Josiah Taylor, Scott Hawley, and Jordi Pons, Fast timing-conditioned latent audio diffusion, in ICML, 2024. [4] Zachary Novack, Ge Zhu, Jonah Casebeer, Julian McAuley, Taylor BergKirkpatrick, and Nicholas J. Bryan, Presto! distilling steps and layers for accelerating music generation, in ICLR, 2025. [5] Antoine Caillon, Brian McWilliams, Cassie Tarakajian, Ian Simon, Ilaria Manco, Jesse Engel, Noah Constant, Pen Li, Timo Denk, et al., Live music models, arXiv:2508.04651, 2025. [6] Chris Donahue, Shih-Lun Wu, Yewon Kim, Dave Carlton, Ryan Miyakawa, and John Thickstun, Hookpad Aria: copilot for songwriters, in ISMIR Late-breaking Demos, 2024. [7] Yewon Kim, Sung-Ju Lee, and Chris Donahue, Amuse: Human-AI collaborative songwriting with multimodal inspirations, in CHI, 2025. [8] Shih-Lun Wu, Chris Donahue, Shinji Watanabe, and Nicholas J. Bryan, Music ControlNet: Multiple time-varying controls for music generation, IEEE/ACM T-ASLP, 2024. [9] Or Tal, Alon Ziv, Itai Gat, Felix Kreuk, and Yossi Adi, Joint audio and symbolic conditioning for temporally controlled text-to-music generation, in ISMIR, 2024. [10] Fang-Duo Tsai, Shih-Lun Wu, Weijaw Lee, Sheng-Ping Yang, Bo-Rui Chen, Hao-Chung Cheng, and Yi-Hsuan Yang, MuseControlLite: Multifunctional music generation with lightweight conditioners, in ICML, 2025. [11] Zachary Novack, Julian McAuley, Taylor Berg-Kirkpatrick, and Nicholas J. Bryan, DITTO: Diffusion inference-time t-optimization for music generation, ICML, 2024. [12] Fang-Duo Tsai, Shih-Lun Wu, Haven Kim, Bo-Yu Chen, Hao-Chung Cheng, and Yi-Hsuan Yang, Audio Prompt Adapter: Unleashing music editing abilities for text-to-music with lightweight finetuning, in ISMIR, 2024. [13] Giorgio Mariani, Irene Tallini, Emilian Postolache, Michele Mancusi, Luca Cosmo, and Emanuele Rodolà, Multi-source diffusion models for simultaneous music generation and separation, in ICLR, 2024. [14] Yao Yao, Peike Li, Boyu Chen, and Alex Wang, JEN-1 Composer: unified framework for high-fidelity multi-track music generation, in AAAI, 2025. [15] Tornike Karchkhadze, Mohammad Rasool Izadi, and Shlomo Dubnov, Simultaneous music separation and generation using multi-track latent diffusion models, in ICASSP, 2025. [16] Simon Rouard, Robin San Roman, Yossi Adi, and Axel Roebel, MusicGen-Stem: Multi-stem music generation and edition through autoregressive modeling, in ICASSP, 2025. [17] Martin Malandro, Composers Assistant: An interactive transformer for multi-track midi infilling, in ISMIR, 2023. [18] Martin Malandro, Composers Assistant 2: Interactive multi-track midi infilling with fine-grained user control, in ISMIR, 2024. [19] Bing Han, Junyu Dai, Weituo Hao, Xinyan He, Dong Guo, Jitong Chen, Yuxuan Wang, Yanmin Qian, and Xuchen Song, InstructME: An instruction guided music edit and remix framework with latent diffusion models, in IJCAI, 2024. [20] Julian Parker, Janne Spijkervet, Katerina Kosta, Furkan Yesiler, Boris Kuznetsov, Ju-Chiang Wang, Matt Avent, Jitong Chen, and Duc Le, StemGen: music generation model that listens, in ICASSP, 2024. [21] Javier Nistal, Marco Pasini, Cyran Aouameur, Maarten Grachten, and Stefan Lattner, Diff-A-Riff: Musical accompaniment co-creation via latent diffusion models, in ISMIR, 2024. [22] Yunkee Chae and Kyogu Lee, MGE-LDM: Joint latent diffusion for simultaneous music generation and source extraction, in NeurIPS, 2025. [23] Giorgio Strano, Chiara Ballanti, Donato Crisostomi, Michele Mancusi, Luca Cosmo, and Emanuele Rodolà, Stage: Stemmed accompaniment generation through prefix-based conditioning, in ISMIR, 2025. [24] Xingchao Liu, Chengyue Gong, et al., Flow straight and fast: Learning to generate and transfer data with rectified flow, in ICLR, 2023. [25] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, et al., Scaling rectified flow transformers for high-resolution image synthesis, in ICML, 2024. [26] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer, High-resolution image synthesis with latent diffusion models, in CVPR, 2022. [27] William Peebles and Saining Xie, Scalable diffusion models with Transformers, in CVPR, 2023. [28] Jiaming Song, Chenlin Meng, and Stefano Ermon, Denoising diffusion implicit models, arXiv:2010.02502, 2020. [29] Jonah Casebeer, Ge Zhu, Zhepei Wang, and Nicholas J. Bryan, generative-first neural audio autoencoder, in ICASSP, 2026. [30] Zach Evans, Julian Parker, CJ Carr, Zack Zukowski, Josiah Taylor, and Jordi Pons, Stable Audio Open, in ICASSP, 2025. [31] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu, Exploring the limits of transfer learning with unified text-to-text transformer, Journal of Machine Learning Research (JMLR), 2020. [32] Jonathan Ho and Tim Salimans, Classifier-free diffusion guidance, arXiv:2207.12598, 2022. [33] Ilya Loshchilov and Frank Hutter, Decoupled weight decay regularization, arXiv:1711.05101, 2017. [34] Tuomas Kynkäänniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen, Applying guidance in limited interval improves sample and distribution quality in diffusion models, in NeurIPS, 2024. [35] Igor Pereira, Felipe Araújo, Filip Korzeniowski, and Richard Vogl, MoisesDB: dataset for source separation beyond 4-stems, in ISMIR, 2023. [36] Zafar Rafii, Antoine Liutkus, Fabian-Robert Stöter, Stylianos Ioannis Mimilakis, and Rachel Bittner, The MUSDB18 corpus for music separation, 2017. [37] Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, et al., Qwen2.5-omni technical report, arXiv:2503.20215, 2025. [38] Sebastian Böck, Filip Korzeniowski, Jan Schlüter, Florian Krebs, and Gerhard Widmer, Madmom: new python audio and music signal processing library, in ACM MM, 2016. [39] Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi, Fréchet audio distance: metric for evaluating music enhancement algorithms, in Interspeech, 2019. [40] Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort Gemmeke, Aren Jansen, et al., CNN architectures for large-scale audio classification, in ICASSP, 2017. [41] Ruben Ciranni, Giorgio Mariani, Michele Mancusi, Emilian Postolache, Giorgio Fabbro, Emanuele Rodolà, and Luca Cosmo, COCOLA: Coherence-oriented contrastive learning of musical audio representations, in ICASSP, 2025. [42] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor BergKirkpatrick, and Shlomo Dubnov, Large-scale contrastive languageaudio pretraining with feature fusion and keyword-to-caption augmentation, in ICASSP, 2023. [43] Mathias Rose Bjare, Stefan Lattner, and Gerhard Widmer, Controlling surprisal in music generation via information content curve matching, in ISMIR, 2024. [44] Qixin Deng, Qikai Yang, Ruibin Yuan, et al., ComposerX: Multi-agent symbolic music composition with LLMs, arXiv:2404.18081, 2024."
        }
    ],
    "affiliations": [
        "Adobe Research"
    ]
}