{
    "paper_title": "Unraveling the Capabilities of Language Models in News Summarization",
    "authors": [
        "Abdurrahman Odabaşı",
        "Göksel Biricik"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Given the recent introduction of multiple language models and the ongoing demand for improved Natural Language Processing tasks, particularly summarization, this work provides a comprehensive benchmarking of 20 recent language models, focusing on smaller ones for the news summarization task. In this work, we systematically test the capabilities and effectiveness of these models in summarizing news article texts which are written in different styles and presented in three distinct datasets. Specifically, we focus in this study on zero-shot and few-shot learning settings and we apply a robust evaluation methodology that combines different evaluation concepts including automatic metrics, human evaluation, and LLM-as-a-judge. Interestingly, including demonstration examples in the few-shot learning setting did not enhance models' performance and, in some cases, even led to worse quality of the generated summaries. This issue arises mainly due to the poor quality of the gold summaries that have been used as reference summaries, which negatively impacts the models' performance. Furthermore, our study's results highlight the exceptional performance of GPT-3.5-Turbo and GPT-4, which generally dominate due to their advanced capabilities. However, among the public models evaluated, certain models such as Qwen1.5-7B, SOLAR-10.7B-Instruct-v1.0, Meta-Llama-3-8B and Zephyr-7B-Beta demonstrated promising results. These models showed significant potential, positioning them as competitive alternatives to large models for the task of news summarization."
        },
        {
            "title": "Start",
            "content": "Abdurrahman Odabasıa, Goksel Biricikb,a aDepartment of Computer Engineering, Turkish-German University, 34820, Istanbul, Turkiye bDepartment of Computer Engineering, Yıldız Technical University, 34220, Istanbul, Turkiye 5 2 0 2 0 3 ] . [ 1 8 2 1 8 1 . 1 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Given the recent introduction of multiple language models and the ongoing demand for improved Natural Language Processing tasks, particularly summarization, this work provides comprehensive benchmarking of 20 recent language models, focusing on smaller ones for the news summarization task. In this work, we systematically test the capabilities and effectiveness of these models in summarizing news article texts which are written in different styles and presented in three distinct datasets. Specifically, we focus in this study on zero-shot and few-shot learning settings and we apply robust evaluation methodology that combines different evaluation concepts including automatic metrics, human evaluation, and LLM-as-a-judge. Interestingly, including demonstration examples in the few-shot learning setting did not enhance models performance and, in some cases, even led to worse quality of the generated summaries. This issue arises mainly due to the poor quality of the gold summaries that have been used as reference summaries, which negatively impacts the models performance. Furthermore, our studys results highlight the exceptional performance of GPT-3.5-Turbo and GPT-4, which generally dominate due to their advanced capabilities. However, among the public models evaluated, certain models such as Qwen1.5-7B, SOLAR-10.7B-Instruct-v1.0, Meta-Llama-3-8B and Zephyr-7BBeta demonstrated promising results. These models showed significant potential, positioning them as competitive alternatives to large models for the task of news summarization. Keywords: Automatic Text Summarization, News Summarization, Small Language Models, Large Language Models, Natural Language Processing, Natural Language Generation, Generative Artificial Intelligence, In-Context Learning 1. Introduction In todays digital age, the amount of data being produced has grown exponentially. The rapid increase in data, particularly in the news sector, has made it crucial to summarize information quickly and accurately to stay informed. News plays an integral role in our daily lives by keeping us updated about global events and shaping our perspectives, knowledge, and opinions. However, staying well-informed without feeling overwhelmed is challenging [1, 2]. This study, therefore, focuses on the task of News Summarization, which involves presenting the key facts and significant details from news article in clear and concise format, allowing individuals to stay effectively informed about current events. Manual summarization, while good at maintaining the original meaning of the text, is impractical due to its time-consuming nature. But one valuable solution to this problem is Automatic Text Summarization (ATS), which efficiently aims to condense lengthy articles into brief summaries, focusing exclusively on the main aspects of the original content, saving time, effort and resources by making it simpler to rapidly comprehend the primary concepts without reading the entire document [1, 3, 4, 5]. As Generative Artificial Intelligence (GenAI) technologies continue to advance, along with the increasing number of Language Models (LMs) introduced every day, there is growing interest in leveraging their capabilities to enhance the efficiency and accuracy of not only news summarization but various Natural Language Processing (NLP) tasks [6, 7]. The release and widespread use of LMs like ChatGPT have undoubtedly not only showcased AIs potential but have also increased public awareness of its capabilities [7, 8, 9]. Imagine world where you could instantly grasp the key points of any lengthy news article with just brief glancethis is what we aim to achieve with LMs. While Large Language Models (LLMs) like GPT-4, Claude, etc. have demonstrated impressive capabilities, their substantial sizewith parameters ranging from tens to hundreds of billions or maybe even moreleads to significant challenges including high computational power requirements, increased latency, costly training and maintenance, and limited flexibility. This has driven researchers to explore smaller language models. These compact models present promising alternative for LLMs since they are more computationally efficient, require less memory and storage space, and offer more cost-effective deployment options, making them particularly interesting candidates for investigating effective NLP solutions. Nevertheless, important questions remain: Can such smaller language models manage the information load and ensure that critical news reaches its audience both efficiently and effectively through summarization? How effectively can these smaller-scale language models handle news summarization tasks while balancing efficiency and performance? How well do different LMs perform in summarizing news? Which are good, which are better, and which should we avoid?"
        },
        {
            "title": "There are a few studies in the field of news summarization",
            "content": "where small number of LMs of different sizes were evaluated, but these models are becoming outdated and limited compared to recent ones [10, 11, 12]. This study attempts to fill this knowledge gap and address previous questions by evaluating the performance of various small and mid-sized LMs in the context of news summarization and comparing their performance with that of larger models, focusing on both zeroand few-shot learning scenarios. Through systematic analysis, this study seeks to identify these models strengths and drawbacks, ultimately determining the most effective LMs in summarizing news articles. This will contribute to the development of more sophisticated and reliable AI-powered news summarization systems, ensuring people stay well-informed of important news content. This work presents several original contributions to the fields of GenAI and NLP, specifically in the context of news summarization task. We conclude our researchs major contributions as follows: We conducted comprehensive benchmark of 20 contemporary language models performance in the news summarization task, considering zero-shot and few-shot incontext learning scenarios, offering new insights into the capabilities of the benchmarked models, as the large number of models evaluated simultaneously is significant compared with the number of models considered in prior works. This extensive benchmark delivers broad comprehension of the capabilities and limitations of recent models. We employed multifaceted evaluation approach, which included automatic metrics, human evaluation, and AIbased evaluation. By utilizing these diverse methods, we ensured more reliable and comprehensive analysis, offering nuanced understanding of the level of quality of LM-generated summaries. We hope our work highlights the strengths and weaknesses of various LMs for news summarization and guides future improvements in summarization tasks. 2. Related Work Automatic text summarization is one of the significant tasks in NLP, with rich research history. It began with Luhn [13], who came up with the idea of summarizing scientific documents by extracting the most significant sentences. Early research on both abstractive and extractive summarization relied on various approaches along the course of the research journey, such as statistical, graph-based, structure-based, clusteringbased, fuzzy logic-based, and machine learning approaches. Over time, deep learning techniques, including Feed Forward and Recurrent Neural Networks (RNNs), advanced summarization capabilities, with Sequence-to-Sequence (Seq2Seq) RNNs becoming notable standard [1, 2]. However, the introduction of attention mechanisms and the Transformer architecture by Vaswani et al. [14] catalyzed advancements in GenAI we see nowadays and caused an uproar in the NLP community, as Transformer models surpassed previous methods across wide range of NLP tasks, leading to numerous studies investigating earlier Transformer-based EncoderDecoder Language models such as BART, Pegasus, and T5 on news summarization tasks ([15, 16] and others). With Transformer-based language models becoming increasingly advanced, featuring decoder-only architecture, enhancing their own generative capabilities, and encompassing broader understanding of language structure and knowledge across various fields more than ever before, it has become essential to benchmark these models on specific NLP tasks, such as news summarization, considering that numerous models are being released by both commercial companies and open-source communities across different sizes and parameter scales [7, 17]. Yet recent developments have shown promising results of LMs built on non-Transformer architectures [18], which also show significant promise. However, these non-Transformer architectures fall outside the scope of our current study, which focuses exclusively on recent Transformer-based models. In 2022, Goyal et al. examined how well models of different types performed on the news summarization task. They assessed general-purpose model, GPT-3, against task-specific fine-tuned model, BRIO, and another model optimized for numerous tasks, T0. They extended the scope of generic summarization to include keyword-based summarization, specifically requesting summary of the text with an emphasis on certain keyword (topic, person, etc.). In all their experiments, it was consistently observed that GPT-3 received lower scores on automatic metrics compared to other models. However, it significantly surpassed them in terms of human evaluation [10]. Zhang et al. conducted thorough evaluation of LMs of different sizes. The number of benchmarked models has been expanded to ten, comparing different versions of OpenAI GPT-3 and InstructGPT models including Ada, Curie and Davinci versions, as well as other models like Anthropic-LM-v4, CohereXL, GLM and OPT. They performed both zero-shot, and fewshot prompting using five examples. Their research discovered that InstructGPT models especially the davinci version were capable of achieving news summarization levels that were comparable to those of human summaries [11]. Another study was carried out by Basyal and Sanghvi later in 2023 using popular news summarization benchmarks to compare several newer modelsmore precisely, tuned versions of the falcon-7b-instruct, mpt-7b-instruct, and the first model behind ChatGPT, the text-davinci-003. According to their experiments, text-davinci-003 outperformed the others [12]. To wrap up, while prior research has evaluated limited number of LMs for news summarization, some of these models are outdated and do not reflect the capabilities of the latest advancements in the Generative AI field. As previously stated, recent advancements are credited to the introduction of Transformer architecture, the availability of vast datasets, and improved computational resources. However, we note that the number of model parameters has continuously increased, from hundreds of millions to tens of billions, even to hundreds of billions. This increase in parameters is correlated with enhanced language understanding and better performance 2 Name CNN/DM [21, 22, 25] Newsroom [23] XSum [24, 26] Year 2015/2016 2018 #Sources 2 38 1 Train Set 287,113 995,041 204,045 Val Set 13,368 108,837 11,332 Test Set 11,490 108,862 11,334 Table 1: General information about the popular datasets in the field of News Summarization. on complex tasks. Thus, the ongoing trend of larger is better has led some to propose new Moores law for LMs. Nevertheless, this is not always practical due to the significant costs and complexities involved in training these large models. While large models demonstrate emergent properties such as breaking down complex tasks, reasoning, and problem-solving, smaller models are still valuable, as they can be easily fine-tuned for specific applications like reading comprehension or summarization, achieving excellent results, considering also that the training approaches for such models are evolved and new special techniques are continuously being researched [17]. Thus, our research seeks to systematically assess the effectiveness of several recent small and mid-sized LMs in In-Context learning scenarios. 3. Experimental setup 3.1. Datasets In order to be able to assess the performance of different language models on the task of news summarization, several benchmark datasets have been utilized. These benchmark datasets share certain attributes that align with our research: Firstly, our focus was primarily on English datasets, comprised of English news articles and their related summaries, due to the wide usage and availability of resources in English. Secondly, datasets must be dedicated to the Single-Document News Summarization task (SDS), which involves extracting essential points from singular news story and compressing them into succinct summary. While Multi-Document Summarization (MDS) that combines and synthesizes information from several news articles, represents another important research direction, it falls outside of the scope of this paper. Therefore, we distinguish our datasets from others such as Document Understanding Conferences (DUC) benchmarks [19] and Multi-News [20], which are intended for use in the Multi-Document News Summarization task. Thirdly, the gold summaries provided in the dataset should ideally be abstractive, human-generated, and reasonably condensed. Lastly, the datasets should be widely recognized, characterized by their large size, comprising hundreds of thousands of news articles, and publicly accessible. Three prominent benchmarks that meet all these criteria are CNN/Daily Mail, also known as (CNN/DM), Newsroom, and Extreme Summarization, also known as (XSum) (presented in Table 1) [21, 22, 23, 24]. Initially, the goal of the CNN/Daily Mail (CNN/DM) dataset was to facilitate the tasks of Passage-based Question Answering and Reading Comprehension, as it contained news articles as passages and abstractive short summaries in the form of bullet points [21]. However, in 2016 with simple straightforward modification, Nallapati et al. adapted the original dataset to 3 Figure 1: Overlap ratio distributions for the training sets of the three datasets (CNN/DM, Newsroom, XSum), visualized as normalized histograms with overlaid Kernel Density Estimate (KDE) curves. The x-axis represents the overlap ratio, while the y-axis indicates the percentage density, highlighting differences in overlap characteristics among the datasets. serve as benchmark for ATS task by concatenating the highlight bullets to form single, multi-sentence summary for each news article, where each bullet [22]. Although the method used to generate the gold summaries (highlights within the article) may not be optimal for evaluating ATS task and summaries of good quality should be in the format of coherent paragraph, not separated sentences, where each of them may explain something different, this dataset still seems to be one of the famous benchmarks in this field [27]. In 2018, Grusky et al. released the Newsroom dataset, large-scale collection sourced from 38 distinct major news publishers (such as Aljazeera, BBC, CNBC, Fox Sports, NY Daily News, Reuters, etc.), distinguishing itself from previous datasets that relied on limited number of sources [23]. With the idea of creating extremely short summaries that consist of single sentence, favoring abstractive summarization strategies over extractive ones to encourage the development of abstractive summarization models, Narayan et al. introduced the Extreme Summarization (XSum) Dataset that was created by gathering articles published in the British Broadcasting Corporation (BBC) website, where each article was paired with pre-written introductory sentence crafted by the articles author, who argues that it should succinctly address the question What is the article about? in one sentence by utilizing information from different sections of the article, and incorporating techniques of rephrasing, fusion and drawing inferences, unlike headlines, which are designed to catch the readers attention [24]. Due to the nature of the summaries being onesentence long, they exhibit greater conciseness compared to the summaries in the other datasets. In order to gain deeper understanding of the properties of these datasets, we studied and analyzed the datasets by utilizing visualizations, computing statistical measures such as overlap ratio, and viewing data point examples. Based on our comprehensive analysis, we observed the following: Invalid data points: small subset of data points was identified where the summary length (in words) exceeded the original text of the article. These instances appear to be incorrect entries, as information presented in the summary does not belong to those mentioned in the corresponding reference article or is not even mentioned in the article text. This situation was encountered in all datasets. This problem was previously mentioned by Chen et al. [27]. Data Quality Issues: Some data points demonstrated various defects and mistakes. Some summaries are totally empty or consist of special characters solely. In addition, certain article texts contain placeholders ([...]) indicating missing text, which introduces varying degrees of complexity in matching summary information with nonexistent article content, thereby posing challenges for effective summarization evaluation. The observed inconsistencies may have arisen from the followed web-based data collection approach and insufficient filtering procedures. These issues were especially encountered in Newsroom dataset. Different Approaches: Differences in summarization styles across datasets were noticed. The CNN/DM and Newsroom datasets exhibit bias towards extractive summarization. This is also evident from the high average overlap ratio (approximately 77% for CNN/DM for example) in the training set, indicating that significant portion (77%) of summary words is directly derived from the corresponding article texts. (See Figure 1) In Contrast, the XSum dataset demonstrates higher degree of abstractive summarization, where the summaries are less reliant on direct extraction from the articles and more on generating entirely novel sentences by paraphrasing the main ideas of the news articles. 3.2. Experimental design In this study, we focused exclusively on inferring LMs, employing zero-shot and few-shot in-context learning to assess the performance of various LMs on the news summarization task. We conducted our experiments in the Google Colab Environment equipped with high-performance GPU, specifically NVIDIA A100 with 40GB VRAM, which is necessary to utilize the language models. In the zero-shot setting, the models were assessed on their ability to produce accurate summaries without any helpful context or examples. Conversely, in the few-shot setting, limited number of examples were provided to give minimal guidance, aiming to help the models grasp the nuances of news summarization more effectively, thereby generating more precise and relevant summaries. The primary reason for not conducting model fine-tuning was the poor quality of the previously examined datasets summaries. Our analysis indicated that the gold summaries provided were of low quality, and fine-tuning models with such data could lead not only to suboptimal results but might also degrade the models summarization abilities. The LMs, which inherently have good capabilities in summarization and other NLP tasks, could be negatively affected by the inferior data quality. Additionally, resource limitations were significant challenge in this study. Fine-tuning requires considerable computational resources and time, yet we were restricted to the Google Colabs infrastructure, which provides finite compute units upon subscription, which are consumed based on the usage of computational resources. Although the initial amount was insufficient and we needed to purchase more and more units to conduct all inferences, the extensive resource requirements for evaluating multiple language models across three datasets in two different settings made fine-tuning both unfeasible and cost-prohibitive. Moreover, to ensure clear and focused scope, we decided to concentrate on zero-shot and few-shot settings. These approaches also provide valuable insights into the models generalization capabilities without the need for additional data. All the aforementioned reasons apply to public models. However, for the tested private models (e.g., GPT-4 or Google Gemini Pro 1.5), fine-tuning was not even an option as this feature was unavailable at the time we conducted our experiments. 3.3. Experimental settings 3.3.1. Data Integrity To maintain data integrity and to ensure robustness and consistency in our work, we implemented data cleaning procedure to identify and remove invalid data points from the training, validation, and test sets. 3.3.2. Few-Shots/Demonstrations In the few-shot setting, given the constraint of varying context window lengths across different models, it was crucial to include the instructions, the article to be summarized, and the few examples within the prompt, ensuring we did not exceed each models context window. The distinct tokenization process of each model further complicated this task, making it challenging to provide multiple examples without exceeding the context limit, particularly with longer articles. To address this, we decided to include only three examples. These examples were manually selected for their critical importance to the experiment, ensuring they were of high quality and represented variety of genres and topics. In some cases, we removed extra, misleading information within the gold summary of those demonstrations to avoid negatively impacting the models performance. By carefully choosing the shortest articles possible, we aimed to stay within the context window constraints while still providing effective guidance to the models. 3.3.3. Sampling To evaluate the models, we selected substantial sample of 1000 examples from the test set of each dataset as evaluation examples, respectively. This approach contrasts with other works that sampled only 25 to 100 examples as evaluation examples. We believe, evaluating on larger sample size of 1000 examples guarantees that the results are more statistically robust and reliable, as it reduces the impact of outliers and the variance in performance metrics, leading to more confident conclusions about the models capabilities and limitations, 4 secures that wider variety of news articles is captured, which helps in assessing the models ability to handle different topics, styles, and complexities in news summarization, provides better understanding of the models generalization capabilities, confirming that it performs well not just on small, potentially biased subset but across broader spectrum of real-world scenarios, and acts as more rigorous benchmark for future studies, setting higher standard for model evaluation in the field of news summarization. In contrast to other works (e.g., [11]) which sampled evaluation examples from the validation set and few-shot examples from the training set, we chose to sample the evaluation examples from the test set and the few-shot examples from the validation set. This approach aligns with potential future research that may involve fine-tuning, where results must be reported on the test set. By doing so, we ensure that the evaluation remains consistent and relevant for future comparisons, providing more reliable benchmark for assessing the impact of fine-tuning on model performance. 3.3.4. Prompt Design Several prompting techniques and strategies should be taken into account while designing effective prompts for In-Context Learning experiments, since these prompts play pivotal role in determining the models success by directing their interactions and outputs. This requires not only deep understanding of the models strengths and weaknesses but also domain expertise and structured method to customize prompts based on each use case. For instance, when using large models, Zeroshot Learning where the model performs tasks without any prior examples often yields satisfactory results due to their advanced capabilities. However, smaller models tend to struggle in such scenarios, where employing advanced prompt engineering techniques becomes essential [7, 17]. Therefore, we ensured through our designed prompts that all LMs comprehended the task requirements regardless of their sizes and complexities to be able to obtain the desired response. The strategies used include adopting specific role to guide the models behavior and to shape the tone and style of the output, clearly specifying the task to avoid ambiguity and help LMs to understand the scope and constraints, breaking down the task into multiple steps for better understanding, employing method akin to the Chain of Thought (CoT) technique which guides LMs through essential reasoning steps, thereby making their implicit processes explicit, and providing concise and clear instructions, and delivering the article text as input to be summarized by the model. Model Name Creator #Parameters Gemini-1.5-Pro-0409 [28] Gemma-2B [29] Gemma-7B [29] GPT-3.5-Turbo-0613 [30] GPT-4-0125-preview [31] Llama-2-7b-hf [32] Meta-Llama-3-8B [33] Meta-Llama-3-8B-Instruct [33] Mistral-7B-v0.1 [34] Mistral-7B-Instruct-v0.1 [34] Phi-3-Mini-4K-Instruct [35] Qwen1.5-0.5B [36] Qwen1.5-1.8B [36] Qwen1.5-4B [36] Qwen1.5-7B [36] SOLAR-10.7B-v1.0 [37] SOLAR-10.7B-Instruct-v1.0 [37] Yi-6B [38] Yi-9B [38] Zephyr-7B-Beta [39] Google Google Google OpenAI OpenAI Meta Meta Meta Mistral AI Mistral AI Microsoft Alibaba Cloud Alibaba Cloud Alibaba Cloud Alibaba Cloud Upstage Upstage 01.AI 01.AI Hugging Face - 2B 7B - - 7B 8B 8B 7B 7B 3.8B 620M 1.8B 4B 7B 10.7B 10.7B 6B 9B 7B Context Window 128K 8K 8K 4K 8K 4K 8K 8K 4K 4K 4K 32K 32K 32K 32K 4K 4K 4K 4K 4K Public Table 2: List of the selected language models. By incorporating these strategies, we aimed to convey our expectations to the LMs clearly and concisely. In Appendix we present one of our designed prompts. 3.4. Model Selection The selection of models for our research was guided by several key criteria, which can be broadly categorized into constraints for large models and considerations for smaller models. For large models, our selection was based on their well-known high performance and dominance in various LLM leaderboards. Specifically, we included the private OpenAI GPTs and Google Gemini models, recognizing their established performance in the NLP field and demonstrated effectiveness in similar tasks, as well as their widespread use in daily applications such as ChatGPT and Google Gemini (formerly Bard). For smaller models, we considered the public LMs published on Hugging Face platform (Until May 2024), primarily focusing on the context window length and model size. We constrained our model choices to those with context window length of at least 4096 tokens to be able to fit both prompt and demonstrations in few-shot setting in worst case. Additionally, due to the computational resources available through the Google Colab Pro+ paid plan, we restricted our selection to models with maximum parameter count of approximately 11 billion. This limitation was necessary to fit the models within the available GPU memory without requiring any type of quantization, which may lead to performance degradation. Furthermore, we prioritized the popular models due to their community support. Based on the aforementioned decisions, we considered benchmarking 20 distinct LMs in both zero-shot and three-shot settings. We list the LMs together with their details in table 2. It is important to note that the generation settings (e.g., Temperature, Top-p, etc.) were not altered for either large or small models and kept at their default settings in order to maintain consistency and ensure fair comparison across different models. By using the default settings, we acknowledge that the performance may not be optimal for our specific task, potentially leading to suboptimal results. However, our objective was to 5 evaluate the models as they are typically deployed, thereby offering an unbiased assessment of their original performance. This is important because different LMs may respond variably to tuned generation settings. 3.5. Postprocessing During the inferring of private LLMs, we encountered specific issue that some examples were blocked by the APIs (OpenAI API and Google GenerativeAI API) due to triggering content filters (such as violence, sexual content, self-harm, and hate speech). Consequently, no completions were generated for these examples, as the content or topics of the news articles activated the filters. To ensure fair and unbiased comparison across the LMs involved in this study, we include only the non-blocked examples in the evaluation process. This decision was based on several considerations; firstly, by concentrating on the non-blocked examples, we ensure that all models are assessed using the same set of inputs, thereby eliminating any potential bias introduced by content filtering mechanisms. This approach allows us to focus on the models core summarization capabilities without the confounding factor of content moderation. Secondly, this method prevents penalizing models for complying with content safety guidelines, which is crucial aspect of their deployment in real-world applications. Additionally, it is possible that one or more of the public models are also censored or equipped with similar filters behind the scenes and may likewise generate no completions for those examples, as observed with the private models. Thus, we excluded the blocked examples, resulting in final dataset comprising 827 examples from CNN/DM, 923 examples from Newsroom, and 938 examples from XSum, out of the original 1000 in each case. The evaluation of the LMs will be based on these specific subsets. Another problem faced was that some completions included hallucinated responses. For example, while generating the completion, the model would begin to summarize the provided news article but then deviate by producing irrelevant content, such as code snippets, solutions to unmentioned problems, or awkward questions. These hallucinations introduce noise into the evaluation process and detract from the models primary task of content generation. To address this issue, we implemented basic cleaning procedure to remove such irrelevant completions by identifying known specific patterns, such as (... nn ### Instructions: HALLUCINATED TEXT) or the use of triple backticks (... ```python CODE SNIPPET ```) for code snippets. However, given the complexity of natural language, it is impossible to anticipate all possible patterns. Therefore, the implemented cleaning procedure aims to minimize hallucinations in completions to the greatest extent possible. By eliminating these irrelevant continuations, we reduce the variability that could otherwise skew the evaluation outcomes. 3.6. Evaluation Framework In this study, we implemented an evaluation framework covering automatic, human, and AI-based evaluations methodologies. 3.6.1. Automatic Evaluation Since manual evaluation of generated summaries on the sampled testing sets is time-consuming, numerous automatic evaluation metrics have been proposed. These metrics typically involve comparing the generated summaries to gold reference summaries. Specifically, our study utilized ROUGE [40], METEOR [41], and BERTScore [42]. The ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metric measures the quality of candidate (AI-generated) summaries based on the lexical overlap lexical overlap through different approaches (e.g., ROUGE-L examines the Longest Common Subsequence (LCS) between the candidate and reference summaries, capturing sentence-level structural similarities without specific n-gram length). Despite its extensive use, its straightforward implementation and efficiency in measuring lexical overlap, ROUGE has its own limitations, such as its dependence on exact token matches, which means it does not account for synonymous phrases or the semantic meaning of words. To address these limitations, our study employs METEOR (Metric for Evaluation of Translation with Explicit ORdering), which incorporates more sophisticated approach by considering word order, and semantic similarity beyond exact word matches including stems, synonyms, and paraphrastic relationships in its evaluation process. However, its reliance on language-specific resources for synonym and paraphrase matching, and the complexity of its calculation led us to explore BERTScore, significant metric that leverages contextual embeddings from pre-trained language models to address many of the previous metrics limitations by capturing more nuanced semantic connections between tokens, moving beyond the constraints of exact word matches or predefined synonym sets. Unlike traditional metrics that struggle with semantic equivalence, unfairly penalizing valid paraphrases, BERTScore can more accurately assess summaries that convey identical meanings using different terminology, thereby correlating better with human judgments. Nevertheless, BERTScore faces its own challenges, including increased computational demands and potential biases inherent in the pre-trained language models used. In our study, we specifically utilized the \"roberta-large\" language model based on the implementation of the bert-score python package. We report F1-scores for these metrics in the results section. 3.6.2. Human Evaluation Protocol Given the large number of summaries generated for each news article, the evaluation task was quite challenging. To manage this, we relied on volunteer evaluators who were willing to handle the substantial workload. Each evaluator was assigned 6 articles, with 2 articles from each dataset but generated under different settings. This approach ensured that each evaluator assessed total of 120 summaries (20 summaries per article 2 settings/articles 3 datasets). This method was chosen because recruiting large number of evaluators to handle smaller pieces of work would have required significant coordination and management, increasing the complexity and cost. By assigning substantial number of summaries to each evaluator we ensured 6 that they gained comprehensive understanding of the range and quality of summaries produced by the models. This holistic view allowed evaluators to better distinguish differences in quality that might have been missed if they were only reviewing smaller subset. We understand that the task was overwhelming, and we extend our sincere gratitude to our evaluators for their dedication and hard work. We instructed five different evaluators to first read the original article carefully before evaluating its summaries. The evaluators were asked to score each summary based on three key aspects: Relevance: The summary captures the most important information covered in the original article. Factual Faithfulness: The summary accurately represents the information from the original article without introducing any errors, inconsistencies, or unfaithful details. Coherence: The summary is logically well-structured and easy to follow. We used Likert scale for scoring, ranging from 1 (worst) to 5 (best). 3.6.3. AI-based Evaluation Protocol Using one powerful LM to evaluate others commonly referred to as LLM-as-a-Judge concept provides unique way to measure their effectiveness. This method not only supports human evaluation but also offers cost-effective assessment process that could be dynamically adjusted to meet growing evaluation demands. We followed the same evaluation protocol used for human assessments but applied it to strong LLM. We chose Claude 3 Sonnet, introduced by Anthropic [43], as judge for this task due to its reputation for exceptional performance across wide range of tasks and its high ranking on leaderboards. Importantly, Claude 3 Sonnet is not related to any of the models in our study, such as GPTs and Gemini, ensuring an unbiased evaluation. We used the same three criteria for evaluation. To guide the judge LLM, we created detailed prompt. This prompt explained the task, provided the original article text and the candidate summary, and asked the judge LLM to score each criterion following specific structure. The judge LLM was asked to assess summaries of five different articles per experiment (20 summaries per article 5 articles 3 datasets 2 settings), resulting in 600 assessments in total. 4. Experimental results and discussion 4.1. Zero-shot Learning Results In this section, we present the results of our zero-shot learning experiments aimed at evaluating the performance of the LMs considered in this study for the news summarization task. 4.1.1. Zero-shot Learning on CNN/DM dataset GPT-3.5-Turbo obtained the highest scores in automated metrics on the CNN/DM dataset, with the exception of ROUGE, where Yi-9B had the greatest score. This suggests that while Yi-9B is effective at preserving lexical content and structure, it may be less capable of maintaining semantic coherence and fluency in generated summaries. Meanwhile MistralInstruct-v0.1 demonstrated the lowest performance across automatic metrics. We hypothesize that this is due to the models tendency to truncate the summary generation process too early, leading to summaries that consist of only one or two words. In this experiment, we can see that judge LLM preferred some models, such as SOLAR-Instruct-v1.0, Gemma-7B, and Yi-9B, which were not as well regarded by human assessors. Nonetheless, the remaining results were mostly comparable. Humans evaluators and the judge LLM both confirmed on the high performance of Qwen1.5-7B and Llama-3-Instruct regarding relevancy. Furthermore, both concur that Gemma-7B, Qwen1.5-7B, and the Llama-3 family produced relatively faithful summaries on the CNN/DM dataset. Finally, Llama-3Instruct and Qwen1.5-7B were recognized as the superior models in structuring their summaries. Among small models, Gemma-7B, Llama-3 models, Qwen1.5-7B, SOLAR-Instruct-v1.0 and Yi-9B perform particularly well in summarizing news articles into highlights, consistent with the CNN/DM datasets characteristics. 4.1.2. Zero-shot Learning on Newsroom dataset When analyzing the results for the Newsroom dataset shown in Table 4, we observe decline in scores compared to the CNN/DM dataset. We attribute this drop to the greater variety of styles that summaries should adhere to, as articles are obtained from larger number of sources (38 instead of 2). In addition, the lower quality of the gold summaries, which we previously discussed during our dataset analysis, is contributing factor."
        },
        {
            "title": "The automated metric scores reveal",
            "content": "that GPT-3.5-Turbo demonstrates the highest performance across most evaluation metrics. In contrast, Llama-2-hf, Llama-3-Instruct, and SOLAR-v1.0 exhibit the lowest scores, suggesting limited effectiveness in this experiment. Furthermore, upon examining the summaries generated by these models, it is evident that Llama-2-hf and Llama-3-Instruct failed to produce summaries, resulting in empty completions. Specifically, they generated 190 and 203 empty summaries out of total of 923 articles, respectively. While SOLAR-v1.0 tended to output further prompts and instructions rather than fulfilling the task of summarizing the provided news articles. These factors significantly impacted their scores in this experiment. Clearly, we continue to observe that Yi models excel at generating summaries that include n-grams present in the gold summaries, resulting in the highest ROUGE score. Additionally, Qwen1.5-7B and SOLAR-Instruct-v1.0 models still perform well in zero-shot news summarization. Overall, human evaluators favored just two models in this experiment mostly: Qwen1.5-7B and Zephyr-Beta. However, judge LLM determined that the Llama-3 family, Mistral-v0.1, Qwen1.5-4B, and SOLAR-Instruct-v1.0 are notably also capable of delivering strong performance. 7 Model Name Gemini-1.5-Pro GPT-3.5-Turbo GPT-4 Gemma-2B Gemma-7B Llama-2-hf Llama-3 Llama-3-Instruct Mistral-v0.1 Mistral-Instruct-v0.1 Phi-3-Mini-Instruct Qwen1.5-0.5B Qwen1.5-1.8B Qwen1.5-4B Qwen1.5-7B SOLAR-v1.0 SOLAR-Instruct-v1.0 Yi-6B Yi-9B Zephyr-Beta Automatic Evaluation ROUGE-L 0.189 0.2077 0.1643 0.1795 0.1927 0.1653 0.1828 0.1675 0.1698 0.1344 0.1593 0.1608 0.1617 0.1450 0.1735 0.1534 0.1692 0.1919 0.2112 0.1633 BERTScore METEOR 0.866 0.8764 0.8674 0.8542 0.8578 0.8399 0.8584 0.8495 0.8534 0.8381 0.8523 0.8534 0.8502 0.83 0.8575 0.855 0.8594 0.8539 0.8649 0.8573 0.3358 0.3613 0.3399 0.1987 0.2242 0.2151 0.2556 0.301 0.2773 0.1587 0.2604 0.279 0.268 0.2503 0.2823 0.2522 0.2887 0.2409 0.2515 0.2894 Relevance 4.6 4.4 4.8 2.0 3.6 3.8 3.2 3.8 3.2 3.4 3.8 2.8 3.4 3.4 4.2 3.4 3.6 3.6 3.2 3.6 Human Evaluation Faithfulness 5.0 4.8 5.0 2.4 4.0 4.0 4.0 4.0 2.6 2.6 2.6 1.8 3.2 3.0 4.4 3.8 3.2 3.4 3.4 2. Coherence 4.8 4.8 4.8 3.0 3.4 4.0 3.4 4.2 3.2 2.6 3.0 2.2 3.4 3.2 4.0 4.0 3.6 4.2 3.8 3.8 Relevance 5.0 5.0 4.8 3.6 4.0 3.0 4.8 4.8 4.0 3.0 3.8 3.2 3.4 3.6 4.4 4.0 4.6 3.6 4.4 4.0 LLM-as-a-Judge Faithfulness 5.0 5.0 5.0 3.4 5.0 3.6 4.8 4.6 3.6 3.2 3.8 2.6 3.2 4.0 4.2 3.4 4.6 4.0 4.4 3.6 Coherence 5.0 4.8 5.0 3.0 4.6 3.2 4.2 4.0 3.6 3.4 3.4 2.8 3.2 3.8 4.2 3.4 4.6 4.2 4.6 3.8 Table 3: Evaluation results for zero-shot LMs on CNN/DM dataset. The highest values in the automatic evaluation metrics are emphasized in bold. Model Name Gemini-1.5-Pro GPT-3.5-Turbo GPT-4 Gemma-2B Gemma-7B Llama-2-hf Llama-3 Llama-3-Instruct Mistral-v0.1 Mistral-Instruct-v0.1 Phi-3-Mini-Instruct Qwen1.5-0.5B Qwen1.5-1.8B Qwen1.5-4B Qwen1.5-7B SOLAR-v1.0 SOLAR-Instruct-v1.0 Yi-6B Yi-9B Zephyr-Beta Automatic Evaluation ROUGE-L 0.1704 0.1987 0.1684 0.1538 0.1572 0.1155 0.1152 0.101 0.1297 0.1497 0.1294 0.1342 0.1375 0.1315 0.1481 0.1079 0.1415 0.1986 0.2022 0.1188 BERTScore METEOR 0.8676 0.8714 0.8649 0.8507 0.814 0.6605 0.6791 0.8086 0.8449 0.8504 0.8476 0.8531 0.8538 0.85 0.8569 0.8168 0.8536 0.8573 0.8626 0.8478 0.2658 0.2915 0.2704 0.1978 0.226 0.1557 0.1853 0.1405 0.2231 0.1797 0.2092 0.2259 0.2299 0.2268 0.2376 0.1571 0.222 0.2098 0.1872 0. Relevance 4.4 4.2 4.6 3.2 2.6 3.8 2.4 1.2 3.2 2.4 3.4 3.6 3.6 3.4 3.8 3.2 3.6 2.6 2.4 4.2 Human Evaluation Faithfulness 5.0 4.8 4.8 3.6 3.0 3.2 2.4 1.4 2.6 3.0 3.2 2.2 2.8 3.0 3.0 1.8 2.4 3.4 3.0 4.0 Coherence 4.8 4.8 4.8 3.8 3.0 4.2 3.0 2.6 2.6 3.0 3.6 3.2 3.8 3.2 4.6 3.0 3.6 3.4 3.4 4.4 Relevance 4.8 4.6 4.6 3.0 3.8 3.4 4.2 4.6 4.0 3.6 4.0 3.8 3.8 4.0 4.2 3.6 4.0 2.8 2.2 4.2 LLM-as-a-Judge Faithfulness 5.0 5.0 5.0 4.6 4.2 4.2 5.0 5.0 4.6 4.2 4.0 4.0 4.6 4.6 5.0 4.4 5.0 3.2 2.8 4.8 Coherence 5.0 5.0 5.0 3.8 4.4 3.8 4.8 5.0 4.0 3.8 4.8 4.0 4.8 4.8 4.8 4.4 5.0 3.2 4.0 5. Table 4: Evaluation results for zero-shot LMs on Newsroom dataset. The highest values in the automatic evaluation metrics are emphasized in bold. 4.1.3. Zero-shot Learning on XSum dataset Analogous to the scores on the Newsroom dataset, this dataset also shows drop in scores, which can be attributed to the challenging task of generating highly succinct, singlesentence summaries. The automatic evaluation scores for the XSum dataset in Table 5 manifest the outstanding performance of the Yi-9B model, which achieved the highest scores across ROUGEL (0.2534), and BERTScore (0.8884) metrics. This model outperformed even the very large private models, indicating its superior capability of generating extremely concise summaries consisting of one or at most two sentences. Notably, this is the first experiment in which the Gemini-1.5Pro model outperformed the GPT models, achieving the highest METEOR score (0.2923) among all models. On the other hand, the Gemma-7B, Llama-2-hf, and Llama3-Instruct models displayed the lowest scores. These models encountered the same issues observed in the zero-shot newsroom experiment. Both Gemma-7B and Llama-2-hf generated substantial number of empty summaries, 310 and 225 out of 938 articles, respectively, while the Llama-3-Instruct model produced unrelated prompts and instructions instead of actual summaries. The judge LLM once again aligned with major human selections. Both humans and the judge LLM agreed that Zephyr-Beta, Qwen1.5-7B, Gemma-7B, and Solar are significant models for this experiment. Nonetheless, there were certain disagreements. Although humans recognized the efficacy of Mistral-v0.1, the judge LLM favored the Llama-3 family, Qwen1.5-1.8B, and Qwen1.5-4B. 4.2. Few-shot Learning Results In this section, we detail the results of our few-shot (more specifically three-shot) learning experiments in carefully organized tables, followed by an analysis of the results. 4.2.1. Three-shot Learning on CNN/DM dataset Compared to the scores of zero-shot experiment, the results of three-shot experiment for CNN/DM presented in Table 6 ensure that offering few-shot examples did not enhance the performance of the LMs; rather, it led to decline in their scores. This is explained by the low quality of the gold summaries, 8 Model Name Gemini-1.5-Pro GPT-3.5-Turbo GPT-4 Gemma-2B Gemma-7B Llama-2-hf Llama-3 Llama-3-Instruct Mistral-v0.1 Mistral-Instruct-v0.1 Phi-3-Mini-Instruct Qwen1.5-0.5B Qwen1.5-1.8B Qwen1.5-4B Qwen1.5-7B SOLAR-v1.0 SOLAR-Instruct-v1.0 Yi-6B Yi-9B Zephyr-Beta Automatic Evaluation ROUGE-L 0.2197 0.1934 0.1644 0.1645 0.1198 0.1096 0.1422 0.1042 0.1281 0.159 0.1231 0.1387 0.1322 0.1492 0.1629 0.1435 0.1433 0.2222 0.2534 0. BERTScore METEOR 0.8869 0.8791 0.8718 0.8694 0.5835 0.6522 0.7916 0.8126 0.8553 0.8564 0.8523 0.8608 0.8585 0.8635 0.8669 0.8580 0.8597 0.8809 0.8884 0.8572 0.2923 0.2617 0.2588 0.1924 0.1529 0.1356 0.2144 0.1498 0.2175 0.1993 0.1947 0.2028 0.2098 0.2281 0.2065 0.2006 0.2118 0.2325 0.2649 0.2374 Relevance 4.4 4.4 4.0 3.0 4.0 1.0 3.4 1.6 4.2 3.0 4.0 3.4 3.8 4.0 4.2 4.0 4.2 2.8 2.2 4.0 Human Evaluation Faithfulness 4.8 4.6 4.6 2.4 4.0 1.0 2.8 1.6 3.6 3.0 3.0 2.6 3.2 3.8 3.8 3.8 3.6 2.6 1.8 4.4 Coherence 4.8 4.4 4.8 3.8 4.6 1.4 3.8 2.8 4.2 3.4 4.2 3.6 3.6 4.4 4.8 4.2 3.8 3.2 2.4 4. Relevance 4.4 4.6 4.8 3.8 4.4 3.2 4.0 4.0 3.8 3.4 4.2 3.8 4.4 4.2 4.2 4.6 4.0 3.8 3.6 4.8 LLM-as-a-Judge Faithfulness 5.0 5.0 5.0 5.0 4.8 4.0 4.8 4.8 3.4 4.0 4.4 3.8 5.0 4.8 4.8 5.0 5.0 4.8 4.6 5.0 Coherence 4.8 5.0 5.0 4.8 4.8 4.0 4.8 4.8 4.2 3.8 4.6 4.4 5.0 4.8 4.6 5.0 4.6 4.6 4.8 5.0 Table 5: Evaluation results for zero-shot LMs on XSum dataset. The highest values in the automatic evaluation metrics are emphasized in bold. which likely confused the models and hindered their ability to generate accurate summaries. Yet, this statement is not valid for large models as they still perform well despite the low-quality demonstrations. In this experiment, Gemini-1.5-Pro and GPT-3.5-Turbo emerged as the best-performing models, achieving the highest scores. But Gemma-7B and Mistral-v0.1 displayed the lowest scores, indicating significant underperformance. These models responded to more than half of the articles (580 and 555 out of 827 total articles, respectively) with empty answers."
        },
        {
            "title": "Among",
            "content": "smaller models, Qwen1.5-4B, Qwen1.5-7B, SOLAR-Instruct-v1.0, Yi-6B and Yi-9B demonstrated robust performance in this experiment. Most notably, the Qwen1.50.5B model, despite having millions rather than billions of parameters, performed on par with the aforementioned models and outperformed several other larger models looking at their automated metric scores. The judge LLM gave high ranking to the Llama-3 family and SOLAR family, while humans add the Gemma family and the Qwen1.5-7B model to this list. 4.2.2. Three-shot Learning on Newsroom dataset When it came to small models in this experiment, Qwen1.57B regularly obtained outstanding scores that were consistent across evaluation methodologies. Aside from Qwen1.5-7B, models like Qwen1.5-4B, SOLAR-Instruct-v1.0, and Yi-9B illustrated significant effectiveness in automatic measures, although human assessors and judge LLM disagreed, concluding that Gemma-7B, Llama-3, and Zephyr-Beta performed excellently. Once again, both Gemma-7B and Mistral-v0.1 continue to have serious challenges in generating summaries, failing to produce summaries for 822 and 589 out of 923 total articles, respectively. 4.2.3. Three-shot Learning on XSum dataset Similarly, as observed in the zero-shot experiment on the same dataset (XSum), the Gemini-1.5-Pro model outperformed the GPT models within the category of large models. Excellent small models in this experiment that should be acknowledged are SOLAR-v1.0, Yi-6B, and Yi-9B. Furthermore, Yi-9B billion even received the highest ROUGE score among all models. Nonetheless, humans and the judge LLM hold different perspective. They recognized the strong performance of numerous models, including Gemma-2B, Llama-3, Qwen1.5-7B, SOLAR-v1.0, Zephyr-Beta, and for the first time, Phi-3-MiniInstruct. Unfortunately, it appears that certain models, specifically Gemma-7B, Mistral-v0.1, and Llama-3, continue to experience similar performance issues noted in prior experiments. They failed to generate 893, 799, and 222 out of 938 summaries, respectively. 4.3. Discussion It appears that the judge LLM (Claude 3 Sonnet) tends to assign scores more generously compared to human evaluators. But in general, the evaluator LLM aligns with human results in preferring the summaries generated by the large models. This is not surprising due to their advanced design, extensive training, and significantly larger parameter counts, which far exceed those of the public smaller models. Although GPT-3.5-Turbo achieved higher automatic evaluation scores compared to its successor GPT-4, GPT-4 slightly outperformed its predecessor in the majority of Human and AIBased evaluation metrics. This could be attributed to the nature of the summaries generated by GPT-4 since they are more sophisticated, abstractive, and comparable to those that could be authored by humans. However, upon analyzing the scores of small models only, we noticed that the promising models could be separated into three different patterns regarding the performance: Models such as Yi-6B, Yi-9B scored remarkably well on automatic evaluation metrics but received poor ratings from both human evaluators and the judge LLM. This suggests these models might be optimizing for surface-level patterns that automatic metrics can capture, rather than producing summaries that humans find useful or accurate. 9 Model Name Gemini-1.5-Pro GPT-3.5-Turbo GPT-4 Gemma-2B Gemma-7B Llama-2-hf Llama-3 Llama-3-Instruct Mistral-v0.1 Mistral-Instruct-v0.1 Phi-3-Mini-Instruct Qwen1.5-0.5B Qwen1.5-1.8B Qwen1.5-4B Qwen1.5-7B SOLAR-v1.0 SOLAR-Instruct-v1.0 Yi-6B Yi-9B Zephyr-Beta Automatic Evaluation ROUGE-L 0.2343 0.2377 0.1947 0.1653 0.0558 0.1518 0.1694 0.1612 0.0522 0.1368 0.1541 0.1654 0.1769 0.1741 0.1714 0.1546 0.1682 0.1883 0.1804 0.1624 BERTScore METEOR 0.8811 0.8806 0.8736 0.8525 0.2573 0.79 0.7939 0.8104 0.2773 0.6916 0.8436 0.8521 0.8536 0.8548 0.8537 0.8389 0.8594 0.8624 0.8574 0.8435 0.3485 0.3525 0.3484 0.1946 0.0596 0.179 0.2252 0.2432 0.0673 0.1577 0.2359 0.2723 0.2517 0.277 0.2556 0.1994 0.285 0.1857 0.1565 0.2641 Relevance 4.4 4.2 4.8 3.6 3.4 3.4 3.8 4.0 3.0 3.4 2.6 3.0 3.8 3.6 4.4 3.8 4.2 4.0 4.0 4.0 Human Evaluation Faithfulness 5.0 4.8 5.0 4.4 4.2 3.4 3.8 4.0 3.2 3.6 2.4 2.0 3.6 3.6 4.2 3.6 4.0 3.6 3.4 3. Coherence 4.8 4.8 4.6 4.0 4.2 3.4 4.2 4.0 3.2 3.8 2.6 2.8 3.2 3.6 4.2 3.6 4.2 3.8 4.0 3.8 Relevance 5.0 5.0 4.8 2.4 3.6 3.0 3.8 4.0 3.0 2.2 3.4 3.4 2.8 3.0 3.8 4.0 4.0 3.4 3.2 3.6 LLM-as-a-Judge Faithfulness 5.0 5.0 5.0 2.6 3.8 2.6 4.6 4.0 2.6 3.0 3.6 3.2 2.4 3.2 3.8 4.0 3.8 3.8 3.2 3.6 Coherence 5.0 4.8 4.8 3.0 3.6 2.4 4.4 4.0 2.8 2.8 3.2 3.0 2.6 3.2 3.4 3.6 3.8 3.4 4.0 3.8 Table 6: Evaluation results for three-shot LMs on CNN/DM dataset. The highest values in the automatic evaluation metrics are emphasized in bold."
        },
        {
            "title": "Model Name",
            "content": "Gemini-1.5-Pro GPT-3.5-Turbo GPT-4 Gemma-2B Gemma-7B Llama-2-hf Llama-3 Llama-3-Instruct Mistral-v0.1 Mistral-Instruct-v0.1 Phi-3-Mini-Instruct Qwen1.5-0.5B Qwen1.5-1.8B Qwen1.5-4B Qwen1.5-7B SOLAR-v1.0 SOLAR-Instruct-v1.0 Yi-6B Yi-9B Zephyr-Beta Automatic Evaluation ROUGE-L 0.1772 0.2144 0.1917 0.122 0.0176 0.1407 0.1118 0.1062 0.0543 0.1645 0.1342 0.1302 0.1388 0.1367 0.1379 0.1791 0.1777 0.1728 0.1905 0."
        },
        {
            "title": "BERTScore METEOR",
            "content": "0.8695 0.8744 0.8707 0.8383 0.0943 0.8042 0.5872 0.7969 0.3082 0.7966 0.8417 0.8525 0.8544 0.8542 0.8544 0.8572 0.8631 0.8511 0.8608 0.8175 0.2632 0.2892 0.2621 0.1623 0.0207 0.1728 0.1457 0.1583 0.0734 0.1708 0.1936 0.2208 0.2249 0.231 0.2258 0.1814 0.2358 0.1919 0.1798 0.2064 Relevance 4.4 4.4 4.2 2.8 4.0 2.4 4.2 2.0 4.0 3.2 3.6 3.4 3.4 4.2 4.0 4.2 4.0 2.8 2.0 4.0 Human Evaluation Faithfulness 4.6 4.8 4.8 2.4 3.6 2.6 3.8 2.2 4.2 3.8 3.8 3.2 2.6 3.6 4.4 3.8 3.6 3.0 2.2 3.6 Coherence 5.0 4.8 4.8 3.4 4.4 3.4 4.4 2.2 4.2 3.8 3.4 2.4 3.6 4.0 4.6 4.2 4.2 3.0 2.8 4.4 Relevance 4.4 4.0 4.8 3.2 3.4 3.0 4.0 4.4 3.4 2.6 3.8 3.6 3.4 4.2 4.2 3.2 3.4 2.8 3.4 4. LLM-as-a-Judge Faithfulness 5.0 5.0 5.0 4.2 4.6 3.6 4.6 5.0 3.2 3.6 4.0 3.4 3.2 3.6 4.2 3.8 3.8 3.8 4.2 3.8 Coherence 5.0 5.0 5.0 4.4 4.6 3.2 4.8 5.0 3.6 3.2 4.6 4.6 4.4 4.8 4.8 4.0 4.8 4.0 4.8 4.8 Table 7: Evaluation results for three-shot LMs on Newsroom dataset. The highest values in the automatic evaluation metrics are emphasized in bold."
        },
        {
            "title": "Model Name",
            "content": "Gemini-1.5-Pro GPT-3.5-Turbo GPT-4 Gemma-2B Gemma-7B Llama-2-hf Llama-3 Llama-3-Instruct Mistral-v0.1 Mistral-Instruct-v0.1 Phi-3-Mini-Instruct Qwen1.5-0.5B Qwen1.5-1.8B Qwen1.5-4B Qwen1.5-7B SOLAR-v1.0 SOLAR-Instruct-v1.0 Yi-6B Yi-9B Zephyr-Beta"
        },
        {
            "title": "Automatic Evaluation",
            "content": "ROUGE-L 0.2429 0.2159 0.1868 0.1464 0.0123 0.1693 0.1474 0.1082 0.0226 0.1744 0.1406 0.132 0.1447 0.1546 0.1755 0.2334 0.1599 0.2322 0.2623 0."
        },
        {
            "title": "BERTScore METEOR",
            "content": "0.8929 0.8856 0.8773 0.8497 0.0426 0.8275 0.6694 0.8298 0.127 0.8177 0.8594 0.859 0.8635 0.867 0.8717 0.8723 0.8663 0.8812 0.8915 0.8463 0.3005 0.2807 0.27 0.1674 0.0136 0.192 0.1817 0.1787 0.0311 0.2038 0.2038 0.2061 0.2075 0.2407 0.2231 0.2666 0.228 0.2515 0.2808 0.2397 Relevance 4.4 4.2 4.2 3.6 3.4 3.4 3.6 2.4 4.0 3.6 4.0 3.6 3.6 4.0 4.2 4.2 3.8 2.6 3.0 4.2 Human Evaluation Faithfulness 4.6 4.4 4.6 4.0 3.0 3.6 4.2 2.4 4.4 3.8 3.8 3.2 3.0 3.8 4.0 4.0 3.8 3.2 3.2 4.2 Coherence 4.8 4.8 4.8 4.0 4.2 3.6 4.0 2.6 4.2 4.6 4.6 4.2 4.0 4.4 4.8 4.2 4.2 3.8 3.0 4.6 Relevance 4.8 4.8 4.6 3.2 3.6 3.8 3.8 4.8 3.8 4.2 4.2 3.8 4.0 4.0 4.0 4.2 3.8 3.8 3.4 4. LLM-as-a-Judge Faithfulness 5.0 5.0 5.0 5.0 4.2 4.0 4.4 5.0 3.8 4.4 5.0 4.2 3.8 4.8 5.0 5.0 5.0 5.0 4.2 4.0 Coherence 5.0 5.0 5.0 4.6 4.4 4.6 4.6 5.0 4.4 4.6 4.8 4.6 4.4 5.0 4.8 5.0 4.6 4.8 4.0 4.8 Table 8: Evaluation results for three-shot LMs on XSum dataset. The highest values in the automatic evaluation metrics are emphasized in bold. 10 In contrast, Gemma-7B, Llama-3 family and Zephyr-Beta models received high praise from human evaluators and the judge LLM despite showing modest automatic metric scores, suggesting they better capture the nuances and coherence aspects that automated metrics might fail to quantify adequately, however humans value them in summaries. third set of models, Qwen1.5-7B and SOLAR-Instructv1.0, achieved more balanced performance profile, showing strong automatic metric scores while maintaining respectable human and LLM-judge ratings. Moreover, we observed from the scores given by the judge LLM that small models struggle to produce well-organized summaries for CNN/DM articles, when compared to the levels of coherence seen in summaries generated for Newsroom and XSum datasets in general. The structure of the reference summaries, which are highlights, may be responsible for this variation. Despite that, it is important to admit that this observation cannot be generalized, as some models, such as Llama3-Instruct, excel at generating well-structured summaries even for CNN/DM articles. Finally, we highlight specific limitations we observed in the quality of the generated summaries, based on our analysis of considerable subset of summaries generated by different LMs. The analysis identifies several key issues: Early Termination of Text Generation: One notable limitation is the tendency of some models to truncate their text generation prematurely. Despite not exceeding the context window, these models often terminate sequences illogically, resulting in incomplete summaries. Models such as Mistral-Instruct-v0.1, Yi-6B, and Yi-9B frequently exhibit this behavior. Redundancy and Repetitive Sequences: Another significant challenge is generating redundant or repetitive sequences. Some models produce reasonable initial sequences but subsequently regenerate the same content using different wordings or paraphrases, thereby diminishing the overall quality and conciseness of the summaries. Models like Phi-3-Mini-4K-Instruct and Mistral-v0.1 are particularly susceptible to this issue. Generation of Prompts Instead of Task Completions: Certain models tend to generate prompts instead of completing the given tasks. In the context of news summarization, some models respond to the initial prompt with another prompt, failing to generate the required summary. For instance, Llama-3-Instruct frequently produces candidate summaries that outline how to summarize news article rather than providing the actual summary itself. Inappropriate Continuation and Hallucinations: further issue is observed where models generate successful completions but then continue by writing questions, additional prompts or instructions, telling stories, or adding irrelevant information instead of appropriately concluding the text generation process. This problem is notably evident in models like Mistral-v0.1, Llama-3-Instruct, Phi-3-Mini4K-Instruct, and occasionally in the Qwen1.5 family. Non-Text Outputs: Lastly, apart from certain models that entirely fail to generate summaries and provide empty outputs such as Gemma-7B, the Llama-2-hf occasionally generates completions that consist solely of newline characters and/or ASCII symbols without producing any meaningful words. 5. Conclusion and future work In this work, detailed benchmarking study of 20 recent language models focusing on small models in the context of news summarization was presented using three different datasets: CNN/Daily Mail, Newsroom, and Extreme Summarization (XSum). Comprehensive experiments in both zero-shot and few-shot learning scenarios, coupled with diverse evaluation approaches, have provided significant insights into the current state of LMs in this domain. Our findings show upon analyzing the scores presented in all experiments that large models show superior performance and outperform the smaller models. Nevertheless, it appears that models like Qwen1.5-7B, SOLAR-Instruct-v1.0, Llama-3, and Zephyr-Beta are competitive with these large models, as they consistently achieve high scores across all datasets. Specifically, in the few-shot setting, adding demonstration examples did not improve model performance and even negatively impacted on the ability in extracting the primary concepts mentioned in the article texts, rather than enhancing the model capabilities, primarily due to the low quality of the gold summaries. It is noteworthy that large models proved their resilience in maintaining their performance levels across different dataset styles, even in the few-shot setting. We observed inconsistencies in performance across different datasets, with some models excelling on one dataset while underperforming on others. For example, Gemini-1.5-Pro, and SOLAR-v1.0 performed exceptionally on the XSum dataset, highlighting the potential for dataset-specific strengths among LMs. Our research has highlighted several areas where further investigation and refinement are needed to further improve the evaluation of LMs in the news summarization task: One significant factor impacting our scores was the quality of the gold summaries in the datasets we used. As detailed previously, the poor quality of these summaries limited our ability to achieve accurate assessments. Therefore, future work should focus on identifying and utilizing comprehensive datasets with high-quality summaries. Fine-tuning LMs on such datasets could yield more reliable outcomes. Alternatively, domain experts could carefully review and refine summaries generated by high-performance LLMs like GPT-4 or Gemini to create high-quality gold summaries for current datasets. 11 Another potential improvement involves adding genre attribute to news datasets. By doing so, we could determine which genres present the most challenges for LMs, allowing for targeted, topic-aware analysis and enhancements. This could lead to more nuanced understandings and better performance across different types of news content. Additionally, we used default generation settings of LMs in our experiments. Future work should investigate optimal configurations tailored to the summarization task across different models. Encouraging model creators to release best-practice configurations for specific tasks would also be beneficial. Expanding the scope of evaluation to include multidocument news summarization is another promising direction. Our current work focuses specifically on singledocument summarization, but comprehensive evaluation of LMs on multi-document summarization could provide deeper insights and broader applications. Lastly, future evaluations should not only consider English but also different languages. multilingual approach would ensure that the findings are applicable across diverse linguistic contexts, enhancing the global relevance and applicability of the research. In conclusion, this benchmarking study has shed light on the effectiveness of recent language models in the news summarization task, emphasizing the continued dominance of large models while also identifying promising small alternatives. As the field of natural language processing continues to evolve rapidly, these insights will serve as valuable foundation for future developments in LM-based summarization technologies. 6. CRediT authorship contribution statement Abdurrahman Odabası: Writing original draft, Validation, Methodology, Investigation, Visualization, Formal analysis, Conceptualization, Funding acquisition. Goksel Biricik: Writing review & editing, Supervision, Project administration, Conceptualization. 7. Acknowledgments The authors gratefully acknowledge the generous support provided by DAAD (German Academic Exchange Service), which played an essential role in facilitating the computational resources necessary for this research work. This support was instrumental in enabling the experiments and analyses that form the foundation of this study. Appendix A. Designed Prompt Template In Table A.9, we present one of the designed prompts, highlighting the components related to the utilized techniques and approaches with different colors to clarify which part corresponds to which technique."
        },
        {
            "title": "References",
            "content": "[1] W. S. El-Kassas, C. Salama, A. Rafea, and H. K. Mohamed, Automatic text summarization: comprehensive survey, Expert Systems with Applications, vol. 165, Jul. 2020. https://doi.org/10.1016/j.eswa. 2020.113679 [2] M. F. Mridha, A. A. Lima, K. Nur, et al., Survey of Automatic Text Summarization: Progress, Process and Challenges, IEEE Access, vol. 9, IEEE, Nov. 2021. https://doi.org/10.1109/ACCESS.2021. 3129786 [3] M. Zhang, G. Zhou, W. Yu, N. Huang, and W. Liu, Comprehensive Survey of Abstractive Text Summarization Based on Deep Learning, Computational Intelligence and Neuroscience, WILEY, Aug. 2022. https://doi.org/10.1155/2022/7132226 [4] Z. J. Hew, V. J. Olanrewaju, X. Chew, and K. W. Khaw, Text Summarization for News Articles by Machine Learning Techniques, Applied Mathematics and Computational Intelligence - Universiti Malaysia Perlis, vol. 11, Dec. 2022. http://dspace.unimap.edu.my/xmlui/ handle/123456789/77725 [5] D. Yadav, J. Desai, and A. K. Yadav, Automatic Text Summarization Methods: Comprehensive Review, ArXiv, Apr. 2022. https://doi. org/10.48550/arXiv.2204.01849 [6] Y. Cao, S. Li, Y. Liu, et al., Comprehensive Survey of AI-Generated Content (AIGC): History of Generative AI from GAN to ChatGPT, ArXiv, Mar. 2023. https://doi.org/10.48550/arXiv.2303.04226 [7] W. X. Zhao, K. Zhou, J. Li, et al., Survey of Large Language Models, ArXiv, Mar. 2023. https://doi.org/10.48550/arXiv.2303.18223 [8] M. U. Hadi, Q. Al Tashi, R. Qureshi, et al., Large Language Models: Comprehensive Survey of its Applications, Challenges, Limitations, and Future Prospects, ArXiv, Jul. 2023. https://doi.org/10.36227/ techrxiv.23589741.v [9] K. S. Kalyan, survey of GPT-3 family large language models including ChatGPT and GPT-4, Natural Language Processing Journal, Elsevier, vol. 6, 2024. https://doi.org/10.1016/j.nlp.2023.100048 [10] T. Goyal, J. J. Li, and G. Durrett, News Summarization and Evaluation in the Era of GPT-3, ArXiv, Sep. 2022. https://doi.org/10.48550/ arXiv.2209.12356 [11] T. Zhang, F. Ladhak, E. Durmus, et al., Benchmarking Large Language Models for News Summarization, ArXiv, Jan. 2023. https: //doi.org/10.48550/arXiv.2301.13848 [12] L. Basyal and M. Sanghvi, Text Summarization Using Large Language Models: Comparative Study of MPT-7b-instruct, Falcon-7b-instruct, and OpenAI Chat-GPT Models, ArXiv, Oct. 2023. https://doi.org/ 10.48550/arXiv.2310.10449 [13] H. P. Luhn, The Automatic Creation of Literature Abstracts, IBM Journal of Research and Development, vol. 2, Apr. 1958. https://doi. org/10.1147/rd.22.0159 [14] A. Vaswani, N. Shazeer, N. Parmar, et al., Attention Is All You Need, in Proceedings of the 31st Conference on Neural Information Processing Systems (NIPS), Jun. 2017. https://doi.org/10.48550/arXiv. 1706.03762 [15] A. Gupta, D. Chugh, Anjum, and R. Katarya, Automated News Summarization Using Transformers, ArXiv, Aug. 2021. https://doi.org/ 10.48550/arXiv.2108. [16] T. Karkera and N. Pathak, Comparative Study on News Summarization using various Transformer Based Models, International Research Journal of Engineering and Technology (IRJET), vol. 09, May 2022. https://www.irjet.net/archives/V9/i5/IRJET-V9I5294.pdf [17] S. Minaee, T. Mikolov, N. Nikzad, et al., Large Language Models: Survey, ArXiv, Feb. 2024. https://doi.org/10.48550/arXiv. 2402.06196 [18] A. Gu and T. Dao, Mamba: Linear-Time Sequence Modeling with Selective State Spaces, ArXiv, Dec. 2023. https://doi.org/10.48550/ arXiv.2312.00752 [19] D. Harman and P. Over, The Effects of Human Variation in DUC Summarization Evaluation, in Text Summarization Branches Out, Jul. 2004. https://doi.org/https://aclanthology.org/W04-1003 [20] A. Fabbri, I. Li, T. She, S. Li, and D. Radev, Multi-News: LargeScale Multi-Document Summarization Dataset and Abstractive Hierarchical Model, in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Jul. 2019. https://doi.org/10. 18653/v1/P19-1102 12 As news editor , your task is to summarize news articles in one-sentence summary, that explains the whole story. similar to how journalist would condense the long article into one sentence To accomplish this task, start by carefully reading and analyzing the provided news arti cle. Then, use your understanding to capture the most important information, events, and summary that covers the details from the article. Finally, generate key aspects mentioned in the article. one-sentence Instructions: - Ensure that the maintains the original context of the article. - Avoid incorporating redundant, unnecessary, or irrelevant information in the summary. summary is clear, coherent, informative, succinct, and one-sentence Article text: {ARTICLE TEXT} One-Sentence Summary: Role Adoption Clear Instructions Task Specification Providing Input Multi-Step Breakdown Length Constraint Table A.9: Designed Prompt for zero-shot experiments on XSum dataset Highly Capable Language Model Locally on Your Phone, ArXiv, Apr. 2024. https://doi.org/10.48550/arXiv.2404.14219 [36] J. Bai, S. Bai, Y. Chu, et al., Qwen Technical Report, ArXiv, Sep. 2023. https://doi.org/10.48550/arXiv.2309.16609 [37] D. Kim, C. Park, S. Kim, et al., SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling, ArXiv, Dec. 2023. https://doi.org/10.48550/arXiv.2312.15166 [38] 01.AI: A. Young, B. Chen, C. Li, et al., Yi: Open Foundation Models by 01.AI, ArXiv, Mar. 2024. https://doi.org/10.48550/arXiv. 2403. [39] L. Tunstall, E. Beeching, N. Lambert, et al., Zephyr: Direct Distillation of LM Alignment, ArXiv, Oct. 2023. https://doi.org/10.48550/ arXiv.2310.16944 [40] C.-Y. Lin, ROUGE: Package for Automatic Evaluation of Summaries, in Proceedings of the Association for Computational Linguistics, Jul. 2004. [41] S. Banerjee and A. Lavie, METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments, in Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, Jun. 2005. https://aclanthology.org/W05-0909 [42] T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi, BERTScore: Evaluating Text Generation with BERT, in International Conference on Learning Representations, Apr. 2019. https://doi. org/10.48550/arXiv.1904.09675 [43] Anthropic, Claude 3: Introducing the next generation of Claude, Online, https://www.anthropic.com/news/claude-3-family, accessed: May 29, 2024. [21] K. M. Hermann, T. Koˇcisky, E. Grefenstette, et al., Teaching Machines to Read and Comprehend, in Proceedings of the 28th International Conference on Neural Information Processing Systems (NIPS), Nov. 2015. https://doi.org/10.48550/arXiv.1506. [22] R. Nallapati, B. Zhou, C. dos Santos, . Gulcehre, and B. Xiang, Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond, in Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, Association for Computational Linguistics, Feb. 2016. https://doi.org/10.18653/V1/K16-1028. [23] M. Grusky, M. Naaman, and Y. Artzi, NEWSROOM: Dataset of 1.3 Million Summaries with Diverse Extractive Strategies, in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Jun. 2018. https://doi.org/10.18653/v1/N18-1065, Dataset: https: //lil.nlp.cornell.edu/newsroom/ [24] S. Narayan, S. B. Cohen, and M. Lapata, Dont Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization, in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, Aug. 2018. https://doi.org/10.18653/V1/D18-1206 [25] Hugging Face Team, Hugging Face Dataset cnn dailymail (CNN/DM), Online, https://huggingface.co/datasets/cnn_dailymail, accessed: Apr. 30, 2024. [26] Hugging Face Team, Hugging Face Dataset EdinburghNLP/xsum, Onhttps://huggingface.co/datasets/EdinburghNLP/xsum, line, accessed: Apr. 30, 2024. [27] D. Chen, J. Bolton, and C. D. Manning, Thorough Examination of the CNN/Daily Mail Reading Comprehension Task, in Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Aug. 2016. https://doi.org/10.18653/ V1/P16- [28] Google Gemini Team, Gemini: Family of Highly Capable Multimodal https://storage.googleapis.com/deepmind-media/ Models, gemini/gemini_1_report.pdf [29] T. Mesnard, C. Hardin, R. Dadashi, et al., Gemma: Open Models Based on Gemini Research and Technology, ArXiv, Mar. 2024. https://doi. org/10.48550/arXiv.2403.08295 [30] OpenAI, Introducing ChatGPT, Online, https://openai.com/ index/chatgpt/, accessed: May 26, 2024. [31] OpenAI, GPT-4 Technical Report, ArXiv, Mar. 2023. https://doi. org/10.48550/arXiv.2303.08774 [32] H. Touvron, L. Martin, K. Stone, et al., Llama 2: Open Foundation and Fine-Tuned Chat Models, ArXiv, Jul. 2023. https://doi.org/ 10.48550/arXiv.2307.09288 [33] Meta, Introducing Meta Llama 3: The most capable openly available LLM to date, Online, https://ai.meta.com/blog/ meta-llama-3/, accessed: May 26, 2024. [34] A. Q. Jiang, A. Sablayrolles, A. Mensch, et al., Mistral 7B, ArXiv, Oct. 2023. https://doi.org/10.48550/arXiv.2310.06825 [35] M. Abdin, J. Aneja, H. Awadalla, et al., Phi-3 Technical Report:"
        }
    ],
    "affiliations": [
        "Department of Computer Engineering, Turkish-German University, 34820, Istanbul, Turkiye",
        "Department of Computer Engineering, Yıldız Technical University, 34220, Istanbul, Turkiye"
    ]
}