{
    "paper_title": "Low-probability Tokens Sustain Exploration in Reinforcement Learning with Verifiable Reward",
    "authors": [
        "Guanhua Huang",
        "Tingqiang Xu",
        "Mingze Wang",
        "Qi Yi",
        "Xue Gong",
        "Siheng Li",
        "Ruibin Xiong",
        "Kejiao Li",
        "Yuhao Jiang",
        "Bo Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has propelled Large Language Models in complex reasoning, yet its scalability is often hindered by a training bottleneck where performance plateaus as policy entropy collapses, signaling a loss of exploration. Previous methods typically address this by maintaining high policy entropy, yet the precise mechanisms that govern meaningful exploration have remained underexplored. Our analysis suggests that an unselective focus on entropy risks amplifying irrelevant tokens and destabilizing training. This paper investigates the exploration dynamics within RLVR and identifies a key issue: the gradual elimination of valuable low-probability exploratory tokens, which we term \\textbf{\\textit{reasoning sparks}}. We find that while abundant in pre-trained models, these sparks are systematically extinguished during RLVR due to over-penalization, leading to a degeneracy in exploration. To address this, we introduce Low-probability Regularization (Lp-Reg). Its core mechanism regularizes the policy towards a heuristic proxy distribution. This proxy is constructed by filtering out presumed noise tokens and re-normalizing the distribution over the remaining candidates. The result is a less-noisy proxy where the probability of \\textit{reasoning sparks} is amplified, which then serves as a soft regularization target to shield these valuable tokens from elimination via KL divergence. Experiments show that Lp-Reg enables stable on-policy training for around 1,000 steps, a regime where baseline entropy-control methods collapse. This sustained exploration leads to state-of-the-art performance, achieving a $60.17\\%$ average accuracy on five math benchmarks, an improvement of $2.66\\%$ over prior methods. Code is available at https://github.com/CarlanLark/Lp-Reg."
        },
        {
            "title": "Start",
            "content": "2025-10-06 Low-probability Tokens Sustain Exploration in Reinforcement Learning with Verifiable Reward Guanhua Huang1,, Tingqiang Xu1,2, , Mingze Wang1,3,, Qi Yi1, Xue Gong1, Siheng Li1,4,, Ruibin Xiong1 Kejiao Li1, Yuhao Jiang1, Bo Zhou1 1LLM Department, Tencent 2Tsinghua University 3Peking University 4The Chinese University of Hong Kong 5 2 0 2 3 ] . [ 1 2 2 2 3 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has propelled Large Language Models in complex reasoning, yet its scalability is often hindered by training bottleneck where performance plateaus as policy entropy collapses, signaling loss of exploration. Previous methods typically address this by maintaining high policy entropy, yet the precise mechanisms that govern meaningful exploration have remained underexplored. Our analysis suggests that an unselective focus on entropy risks amplifying irrelevant tokens and destabilizing training. This paper investigates the exploration dynamics within RLVR and identifies key issue: the gradual elimination of valuable low-probability exploratory tokens, which we term reasoning sparks. We find that while abundant in pre-trained models, these sparks are systematically extinguished during RLVR due to over-penalization, leading to degeneracy in exploration. To address this, we introduce Low-probability Regularization (Lp-Reg). Its core mechanism regularizes the policy towards heuristic proxy distribution. This proxy is constructed by filtering out presumed noise tokens and re-normalizing the distribution over the remaining candidates. The result is less-noisy proxy where the probability of reasoning sparks is amplified, which then serves as soft regularization target to shield these valuable tokens from elimination via KL divergence. Experiments show that Lp-Reg enables stable on-policy training for around 1,000 steps, regime where baseline entropy-control methods collapse. This sustained exploration leads to state-of-the-art performance, achieving 60.17% average accuracy on five math benchmarks, an improvement of 2.66% over prior methods. Code is available at https://github.com/CarlanLark/Lp-Reg."
        },
        {
            "title": "Introduction",
            "content": "The advent of large reasoning models has reshaped the trajectory of artificial intelligence, with paradigmatic examples including OpenAI O1 (OpenAI et al., 2024) and DeepSeek-R1 (DeepSeek-AI et al., 2025). central technique underpinning these systems is reinforcement learning with verifiable reward (RLVR), which assigns reward to verifiable solutions through rule-based verification. These models generate extended chain-of-thought (CoT) reasoning (Wei et al., 2023) to solve challenging problems in domains like mathematical olympiads (He et al., 2024b). However, notable bottleneck emerges during RL training that limits its scalability, frequently culminating in performance plateau and subsequent collapse. This failure is consistently accompanied by rapid decay in policy entropy, indicating severe loss of exploration capacity (Yu et al., 2025; Cui et al., 2025; Wang et al., 2025). Previous approaches have recognized this declining exploration, attempting to address it through various entropy control mechanisms. Methods such as adaptive entropy regularization (He et al., 2025), high entropy change blocking (Cui et al., 2025), or selective token updates (Wang et al., 2025) aim to maintain higher entropy as proxy for exploration. However, relying on overall entropy can be an indirect and imprecise tool. An indiscriminate focus on maximizing randomness risks amplifying noise and destabilizing training ( Omer Veysel agatan & Akgun, 2025), suggesting deeper issue beyond simply the quantity of randomness. Our analysis suggests the performance bottleneck may stem from the systematic elimination of valuable lowprobability exploratory tokens. We term these tokens Reasoning Sparks; they include words like wait, however, or perhaps, which often serve as logical connectives or expressions of uncertainty that naturally initiate diverse reasoning pathways (Figure 1a). As the aggregated violin plots in Figure 1c show, standard GRPO training suppresses the low-probability sampling of these important exploratory tokens, causing the suppression of Reasoning Sparks. Furthermore, we find that indiscriminately boosting randomness amplifies the low-probability sampling of irrelevant tokens (e.g., cost, fine), which are semantically out of context for the mathematical reasoning task. We refer to the low-probability appearance of these irrelevant tokens as noise. This amplification leads to an even faster performance collapse than the baseline, as shown in Figure 1b. Equal contribution. Work completed during an internship at Tencent. Correspondence to Bo Zhou: chaysezhou@tencent.com. 1 (a) Reasoning Sparks: low-probability exploratory tokens that initiates new reasoning path. (c) Aggregated distribution of observed sampling probabilities for class of meaningful exploratory tokens (e.g., wait, however). (b) Training Dynamics (d) Aggregated distribution of observed sampling probabilities for class of irrelevant tokens (e.g., cost, fine). Figure 1: Selectively preserving low-probability tokens is key to overcoming performance plateaus in reasoning RL. (a) An illustration of reasoning spark. (b) Standard GRPO training reaches performance plateau and collapses, accompanied by decaying entropy. An indiscriminate entropy bonus (GRPO + Entropy Loss) leads to an even faster collapse. (c) We reveal the cause: GRPO systematically suppresses the low-probability sampling of important exploratory tokens (like wait), and forces these tokens sampling distributions to collapse towards high probabilities. Ent-Loss fails to fix this. In contrast, our method, Lp-Reg, successfully preserves healthy, wide distribution, sustaining exploration. (d) The failure of entropy bonuses is explained by their indiscriminate nature: they amplify the low-probability sampling of irrelevant tokens, creating noise and thereby degrading exploration quality. The aggregated statistics in (c) and (d) demonstrate systemic effect beyond single-token instances. Detailed plots for individual tokens are available in Appendix B.1. These findings present central challenge: successful exploration strategy should protect valuable reasoning sparks without simultaneously amplifying the destructive effects of irrelevant noise. To address this challenge, we introduce Low-probability Regularization (Lp-Reg). The primary goal of Lp-Reg is to preserve valuable low-probability tokens via regularization. To avoid amplifying noise, the method leverages key observation: within the low-probability range, meaningful exploratory tokens (like wait) consistently exhibit higher average probability than irrelevant noise (like cost) in the immediate next-token prediction. Based on this statistical distinction, Lp-Reg first discards low-probability tokens presumed to be noise using probability threshold. It then redistributes the probability mass from these discarded tokens among the remaining candidates. This process constructs less-noisy proxy distribution where valuable low-probability tokens are preserved and their relative probabilities amplified. Finally, Lp-Reg penalizes the deviation of the original policy from this proxy using forward KL divergence, which selectively protects the low-probability tokens that were preserved in the less-noisy proxy distribution. Our experimental evaluation demonstrates the effectiveness of Lp-Reg. Our method enables stable on-policy training for around 1,000 steps, regime where many entropy-control methods have collapsed, resulting in better performance. On five widely used math benchmarks, this results in 60.17% average accuracy on Qwen3-14B-Base, improving upon prior methods by 2.66%. Our contributions are summarized as follows: In contrast to prior work focusing on overall policy entropy, we identify the suppression of reasoning sparks as key issue and provide evidence that their preservation is crucial for sustained performance. We introduce Low-probability Regularization (Lp-Reg), method that creates more stable exploratory environment by filtering out presumed meaningless noise to protect the remaining low-probability tokens. We demonstrate through extensive experiments that Lp-Reg achieves state-of-the-art performance, while also enabling stable on-policy training over extended periods where baselines collapse. We provide comprehensive analysis showing that our approach of filtering presumed meaningless noise yields superior results compared to indiscriminate entropy-control methods."
        },
        {
            "title": "2 Related Work",
            "content": "Reinforcement learning for LLMs Recently, reinforcement learning has become the dominant framework for enhancing the reasoning abilities of large language models (LLMs) (OpenAI et al., 2024; DeepSeek-AI et al., 2025). By leveraging automatic checkers or symbolic verification, reinforcement learning with verifiable rewards (RLVR) achieved further breakthroughs in improving the reasoning capability of LLMs (Shao et al., 2024a; Yang et al., 2025; Team et al., 2025). Based on RLVR and GRPO (Shao et al., 2024a), subsequent methods such as DAPO Yu et al. (2025), VAPO (Yue et al., 2025), and other policy optimization variants (Zhao et al., 2025; Cui et al., 2025; Zheng et al., 2025) have been proposed to improve the stability, efficiency, and scalability of RL for reasoning models. Entropy collapse in RL training recurring difficulty in training reasoning models with RL is the rapid collapse of policy entropy during the early stages of training. This phenomenon, which reflects excessive exploitation and insufficient exploration, has been widely recognized as bottleneck for scaling RL in reasoning models. To mitigate collapse, researchers have explored several directions, including selectively regularizing updates at high-entropy forking tokens (Wang et al., 2025), amplifying advantages at exploratory positions (Cheng et al., 2025), modifying clipping strategies (Yu et al., 2025; Zhao et al., 2025; Cui et al., 2025; Zheng et al., 2025), or doing weight clipping (MiniMax et al., 2025; Su et al., 2025). While these methods primarily operate by monitoring policy entropy, which is correlational rather than causal to exploration, our analysis delves directly into the next-token prediction distribution. This allows for more semantically grounded and causally-informed investigation of the probabilities of individual candidates and their role in exploration dynamics. Intrinsic confidence of LLMs As the capabilities of Large Language Models (LLMs) have rapidly advanced, they have demonstrated an increasingly strong and reliable sense of intrinsic confidence (Saurav et al., 2022; Loka et al., 2024; Amir et al., 2025). Research investigates how these intrinsic confidence signals, often reflected in the next-token prediction distribution, can guide complex reasoning and exploration (Amirhosein et al., 2025; Xuezhi & Denny, 2024; Xuandong et al., 2025). Studies have shown that tokens with higher relative probabilities in the next-token prediction are often more contextually appropriate than their lower-probability counterparts (Nguyen et al., 2025; Xu et al., 2025; Fu et al., 2025). Building on this, some work has explored entropy minimization, which sharpens the models confidence distribution. This approach can improve inference performance by encouraging the model to commit to consistent and high-confidence solution paths (Gao et al., 2025; Agarwal et al., 2025). Our work builds upon similar insight, leveraging the models intrinsic confidence to distinguish between valuable reasoning sparks and irrelevant noise within this low-probability range."
        },
        {
            "title": "3 Preliminaries",
            "content": "3.1 Reinforcement Learning with Verifiable Rewards Reinforcement learning (RL) has played critical role in LLMs (Murphy, 2024). Formally, JRL(θ) = (cid:2)r(o, a)(cid:3), (q,a)D,oπθ (q) (1) where r(o, a) denotes the reward assigned to an output given reference answer a. In reinforcement learning with verifiable rewards (RLVR), this reward is computed through rule-based functions, such as Math-Verify1. Recent studies have demonstrated that large-scale RLVR encourages models to perform more deliberative reasoning by producing extended chains of thought prior to the final prediction, thereby substantially improving their capacity to solve complex problems (DeepSeek-AI et al., 2025). In practice, Eq. 1 is typically optimized using policy gradient methods, such as Proximal Policy Optimization (PPO) (Schulman et al., 2017) and Group Relative Policy Optimization (GRPO) (Shao et al., 2024b). 3.2 Group-Relative Policy Optimization GRPO is representative actor-only policy gradient method for optimizing LLMs. It directly estimates the advantage of each token by leveraging multiple samples drawn from the same prompt. Formally, the advantage is defined as Ai,t = R(oi) mean(G) std(G) , (2) where {o1, . . . , oG} are independent outputs sampled from the same prompt, with group size G, = {R(o1), . . . , R(oG)} denotes their associated rewards, and R(oi) is the reward of output oi. In this formulation, Ai,t represents the advantage of the t-th token in oi. The policy is then optimized on the basis of these advantages using the PPO 1https://github.com/huggingface/Math-Verify 3 surrogate objective: JGRPO(θ) =E (q,a)D, {oi}G i=1 πθold (q) 1 i=1 1 oi (cid:104) oi t=1 min(cid:2)ri,t Ai,t, clip(cid:0)ri,t, 1 ϵ, 1 + ϵ(cid:1) Ai,t (cid:3) β DKL (πθ πref) (3) (cid:105) , where β controls the strength of KL regularization between the current policy πθ and the reference policy πref. The probability ratio ri,t = (cid:0)oi,t q, oi,<t (cid:1) (cid:0)oi,t q, oi,<t πθ πθold (cid:1) (4) serves as the importance sampling weight for off-policy training, where πθold denotes the behavior policy. The hyperparameter ϵ specifies the clipping ratio, which constrains the updated policy from deviating excessively from the behavior policy, thereby ensuring stability during optimization."
        },
        {
            "title": "4 Low-probability Regularization",
            "content": "To address the premature elimination of valuable reasoning sparks, we propose regularization method termed Lowprobability Regularization (Lp-Reg). This method is designed to be integrated into policy gradient algorithms to create more stable exploratory environment. The central idea is to leverage the models own predictive distribution to construct less-noisy proxy for regularization, preserving low-probability tokens. 4.1 Proxy Distribution πproxy The foundation of Lp-Reg is the construction of proxy distribution, which represents filtered variant of the current policy πθ. It is constructed in two steps: Figure 2: An example of probability renormalization. πproxy assigns zero probability to tokens with πθ τ and renormalizes the probability mass to tokens with πθ > τ. 1. Filtering Noise Tokens: We first filter out set of low-confidence tokens, which are presumed to be noise, defined as those whose probability πθ(o) is under threshold τ. This threshold controls the filtering strategy, for which we explore two primary choices: Fixed threshold: simple approach where τ is constant hyperparameter, e.g., 0.02. Min-p threshold: Following (Nguyen et al., 2025), τ is defined relative to the peak probability: τ = κ maxoV πθ(o), where κ (0, 1) is hyperparameter. This makes the filtering adaptive to the distributions sharpness. Our primary experiments employ the min-p strategy for its adaptiveness, though fixed thresholds are also shown to be effective in our ablation studies. 2. Probability Renormalization: As shown in Figure 2, the proxy distribution πproxy assigns zero probability to tokens filtered out in the previous step and renormalizes the probability mass across the remaining tokens: πproxy(o) = πθ(o) s.t. πθ(o )>τ πθ(o) (cid:40) 0 if πθ(o) > τ otherwise . (5) This process effectively treats tokens with low relative probabilities as potential noise, while preserving all others to form high-confidence reference. 4 4.2 Low-probability Regularization Objective The Low-probability Regularization (Lp-Reg) penalty is integrated into the GRPO framework as selective regularization term. The final objective function is: J(θ) =E BD,(q,a)B,{oi}G i= πθold (q) (cid:104) β πθ(oi,tq, oi,<t) < δB (cid:34) 1 i=1 oi (cid:104) i=1 oi t=1 clip(ri,t(θ), 0, U) Ai,t ρ πproxy(oi,tq, oi,<t) > 0 Ai,t < 0 (cid:35) (cid:105) (cid:105) (6) DKL (πproxy(q, oi,<t)πθ(q, oi,<t)) The first term is the policy gradient objective from GRPO. We modify its clipping by removing the lower bound to avoid clipping low-probability exploratory actions and adding large upper bound for numerical stability. The second term is the Lp-Reg penalty, designed to protect reasoning sparks. It is activated by the indicator function I[] only for tokens that satisfy three conditions simultaneously: first, their sampling probability πθ is below dynamic low-percentile threshold δB ρ , which is calculated as the lowest ρ-th percentile of the sampling probabilities of all tokens within the current training batch B; second, their probability in the proxy distribution πproxy is greater than zero; and third, the token receives negative advantage signal (Ai,t < 0). This final condition ensures the regularization applies exclusively to tokens receiving negative learning signal, preventing their potential over-penalization while leaving updates from positive experiences unaffected. We use the forward KL divergence, DKL(πproxyπθ) as the regularization function. It imposes significant penalty when πθ(o) approaches zero for token with non-zero probability in πproxy, providing targeted penalty against token elimination without forcing the policy to strictly match the heuristic proxy distribution."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experimental Setup Baselines We compare Lp-Reg against suite of strong baselines, including foundational algorithm and several state-of-the-art methods designed to enhance exploration through entropy control. Our primary baseline is GRPO (Shao et al., 2024a), value-free policy optimization algorithm that employs group-relative advantage estimation, making it common choice for RLVR. To represent classical entropy regularization methods, we implement GRPO + Entropy Loss, which directly incorporates the principles of Maximum Entropy RL by adding policy entropy bonus to the GRPO objective function. We also compare against several advanced methods: Clip-Higher (Yu et al., 2025), core component of DAPO that encourages higher entropy by using an asymmetric clipping range in the PPO objective; Selective High-Entropy Training (80/20) (Wang et al., 2025), method that restricts policy gradient updates to only the top 20% of tokens with the highest generation entropy; KL-Cov (Cui et al., 2025), which prevents entropy collapse by applying selective KL-divergence penalty to tokens with the highest covariance between their log probabilities and advantages; and GSPO (Zheng et al., 2025), which modifies the clipping mechanism to operate at the sequence level to promote higher training entropy. Training Settings All experiments are conducted within the verl (Sheng et al., 2024) framework to ensure standardized and fair comparison. We use two base models for our experiments: Qwen3-14B-Base and Qwen2.532B. The 14B models are trained on 32 NVIDIA H20 GPUs, while the 32B model is trained on 64 NVIDIA H20 GPUs, with an average training step time of 600 1400 seconds. For the reinforcement learning from verifier rewards (RLVR) phase, models are trained on the Dapo-Math-17K Yu et al. (2025) dataset with maximum response length of 8, 192 tokens. We use global batch size of 256. For off-policy methods, we use mini-batch size of 32, resulting in 8 gradient updates per rollout. To ensure fair comparison, step in our experimental results consistently refers to single rollout for all methods. Consequently, each reported step for off-policy training corresponds to 8 gradient updates. constant learning rate of 1 106 is applied without warmup schedule. We set the group number as 8 for all GRPO-based methods. To ensure numerical stability, we set the policy gradients clipping by setting the upper bound of the importance sampling ratio to = 10. For our proposed Lp-Reg, which uses the min-p threshold, we set the probability percentile threshold ρ to 0.5% for Qwen2.5-32B-Base and 1% for Qwen3-14B-Base, the KL regularization coefficient β to 1.0, and the min-p ratio κ to 0.02. The proxy distribution, πproxy, is constructed from the data-generating policy (πθold in the off-policy setting and the current policy πθ in the on-policy setting). For all baseline methods, we adopt the hyperparameters specified in their original public implementations to ensure faithful reproduction. Specifically for the GRPO + Entropy Loss baseline, we set the entropy coefficient to 0.002 within the verl framework. 5 Evaluation For evaluation, we assess model performance across five diverse mathematical reasoning benchmarks: AIME24 (MAA), AIME25 (MAA), MATH-500 (Hendrycks et al., 2021), OlympiadBench (He et al., 2024a), and Minerva Math (Lewkowycz et al., 2022). Following (Cui et al., 2025), we employ distinct decoding strategies based on the benchmark. For AIME24 and AIME25, which have smaller test sets, we use sampled decoding with temperature of 0.6 and generate 16 independent responses per problem to obtain robust performance estimate. For the remaining benchmarks, including MATH-500, OlympiadBench, and Minerva, we utilize greedy decoding to evaluate performance. 5.2 Results As shown in Figure 3 and Table 1, Lp-Reg achieves state-of-the-art performance across five challenging mathematical reasoning benchmarks on both 14B and 32B model scales. On the Qwen3-14B model, on-policy Lp-Reg sets new benchmark with an average accuracy of 60.17%, surpassing the next best method, 80/20, by 2.66%. Notably, Lp-Regs advantage is more pronounced on the newer Qwen3-14B base model compared to the older Qwen2.5-32B. We hypothesize that as base models improve, their capacity for nuanced, low-probability reasoning increases, creating richer substrate for the emergence of valuable reasoning sparks, which Lp-Reg can then effectively protect and leverage. Our experiments consistently show the superiority of on-policy training over off-policy methods across 14B and 32B scales. This is due to the inherent stability of on-policy updates, which avoid distribution shifts caused by mismatched data-sampling and training policies. Off-policy methods, such as Clip-Higher, often rely on importance sampling clipping, leading to instability. While competitive on Qwen2.5-32B, Clip-Highers performance drops on Qwen3-14B, highlighting its fragility. In contrast, Lp-Regs self-contained, policy-intrinsic regularization ensures its effectiveness in both on-policy and off-policy settings, unlike competing methods that are heavily reliant on off-policy importance sampling. Beyond raw performance, Lp-Reg demonstrates distinct entropy signature indicative of healthy explorationexploitation balance. As shown in Figure 3, methods like Clip-Higher induce continuous, often artificial increase in policy entropy. Lp-Reg, however, facilitates dynamic, multi-phase entropy trajectory: entropy initially decreases as the model learns core reasoning patterns, then gradually increases to foster exploration as performance improves, and finally stabilizes within healthy range as accuracy converges. This adaptive behavior stems from our confidenceaware regularization, which selectively protects reasoning sparks without amplifying low-probability out-of-context irrelevant noise. Method AIME24 AIME25 Math-500 Minerva Olympiad Bench Avg. GRPO (Shao et al., 2024a) (off.) GSPO (Zheng et al., 2025) (off.) Clip-Higher (Yu et al., 2025) (off.) KL-Cov (Cui et al., 2025) (off.) 80/20 (Wang et al., 2025) (off.) Lp-Reg (off.) GRPO (Shao et al., 2024a) (on.) GRPO + Entropy Loss (on.) 80/20 (Wang et al., 2025)(on.) Lp-Reg (on.) GRPO (Shao et al., 2024a) (off.) GSPO (Zheng et al., 2025) (off.) Clip-Higher (Yu et al., 2025) (off.) KL-Cov (Cui et al., 2025) (off.) 80/20 (Wang et al., 2025) (off.) Lp-Reg (off.) GRPO (Shao et al., 2024a) (on.) GRPO + Entropy Loss (on.) 80/20 (Wang et al., 2025) (on.) Lp-Reg (on.) Qwen2.5-32B-Base 22.29 22.29 29.79 27.50 28.75 24.58 22.50 1.88 28.54 27.08 88.00 87.60 87.60 87.40 87.00 90.20 86.60 60.80 89.40 90.00 Qwen3-14B-Base 27.08 34.58 32.71 34.79 34.58 34.17 34.38 25.21 32.50 37. 89.20 88.60 95.00 93.00 91.80 92.40 93.00 88.20 91.60 94.40 30.63 33.33 38.33 35.62 38.12 37.71 28.54 3.75 32.50 38.12 34.38 41.46 41.67 49.17 43.96 46.25 46.04 37.29 47.29 50.83 41.18 48.53 45.22 44.49 45.22 40.81 44.85 27.94 45.59 46.32 49.26 50.74 47.43 47.43 48.16 48.16 48.53 46.32 50.37 49.26 54.37 55.56 56.44 55.11 58.37 59.70 60.30 22.22 57.63 61. 55.70 59.85 64.00 62.07 60.89 64.44 65.19 54.96 65.78 68.44 47.29 49.46 51.48 50.02 51.49 50.60 48.56 23.32 50.73 52.54 51.13 55.05 56.16 57.29 55.88 57.08 57.43 50.40 57.51 60.17 Table 1: Main results on five mathematical reasoning benchmarks across two model scales. Our method, Lp-Reg, achieves the best average performance on both models. On-policy (on.) and off-policy (off.) training methods are highlighted with distinct colors. 6 Figure 3: Training dynamics on the Qwen3-14B-Base model. To best illustrate the performance differences, we compare the top-performing methods. Lp-Reg demonstrates superior and more stable performance throughout training. Full training dynamics for the Qwen2.5-32B model are available in Appendix A.1. Figure 4: Ablation studies for core components of Lp-Reg on the Qwen3-14B-Base model. The results confirm that targeting our noise filtering threshold τ is critical for stable performance. The adaptiveness of the min-p threshold is also shown to be beneficial over fixed one. 5.3 Ablation Study We conduct series of ablation studies to analyze the core components of Lp-Reg and validate our key design choices. Importance of Noise Filtering. Lp-Reg only protects tokens deemed meaningful by the proxy distribution (πproxy > 0). To test this, we remove the filter and fork all tokens below the noise threshold τ from contributing to gradient updating (Lp-Reg w/o τ). Figure 4 shows that this leads to catastrophic performance collapse and entropy explosion. This confirms that filtering is critical to ignore the extreme tail of the distribution, which consists of irrelevant noise that destabilizes training if regularized. Dynamic vs. Fixed Threshold. We conduct comparison between the dynamic min-p noise threshold (Lp-Reg w/ dynamic τ) and the fixed noise threshold (Lp-Reg w/ fixed τ) in Section 4.1. As shown in Figure 4, the fixed threshold underperforms compared to the dynamic threshold, which we adopt as the default. However, it still significantly surpasses the standard GRPO. This indicates that while the core filtering principle is effective, the dynamic nature of min-p provides more robust estimate of the models confidence across different contexts, better preserving genuine reasoning sparks. We conduct further ablation studies on the high-entropy token regularization and reverse KL regularization. For detailed results and analysis, please refer to Appendix A.2."
        },
        {
            "title": "6 Analysis",
            "content": "To understand the mechanisms behind Lp-Regs performance, we conduct series of analyses focusing on how it overcomes the exploration bottleneck by targeting and preserving valuable reasoning tokens. 6.1 Probability-Entropy Distribution of Exploratory Tokens We begin by exploring the distinction between low-probability tokens and high-entropy tokens. Figure 5 highlights this contrast by comparing tokens from the top 1% lowest probability with those from the top 1% highest entropy. The difference is striking: low-probability tokens frequently include semantically meaningful exploratory markers such as But, Wait, Perhaps, and Alternatively, which often signal shift in the reasoning trajectory. In 7 Figure 5: The word cloud statistics. contrast, high-entropy tokens are dominated by common functional terms (e.g., sqrt, times) or formatting symbols (e.g., n), which carry little exploratory intent. This explains why entropy-based regularization often fails to enhance exploration: it confuses noise with exploration. However, the set of low-probability tokens is also not uniformly useful. It also includes noisy artifacts such as spurious newline characters (n) or formatting debris, whose regularization can destabilize training rather than enhance reasoning. To mitigate this, Lp-Reg applies threshold τ that filters out such noise. Ablation studies in Section 5.3 confirm the necessity of this step: removing the threshold results in unstable training dynamics and degraded reasoning performance. Thus, Lp-Regs effectiveness stems not only from targeting low-probability tokens but also from selectively excluding irrelevant noise. 6.2 Sampling Dynamics of Exploratory Tokens Figure 6: ProbabilityEntropy scatter plots of explorative tokens, displaying random sample of 5% of all data points. Figure 6 shows the probabilityentropy distributions of key explorative tokens (but, wait, perhaps, alternatively, and however) under three methods: GRPO, GRPO + Entropy Loss, and our Lp-Reg. With the baseline GRPO, these tokens are concentrated in low-entropy, high-probability regions. In this case, tokens like wait tend to appear only when the model is already confident, turning them into deterministic patterns rather than initiating new exploration path with uncertainty. Adding an entropy loss changes this behavior, but in an uncontrolled way. Some sampled wait tokens appear at extremely high entropy levels (sometimes exceeding 10), which superficially boosts diversity but produces little useful exploratory signal. These scattered occurrences do not integrate meaningfully into the reasoning process. Our Lp-Reg method yields more balanced dynamic. Explorative tokens are sampled across broad range of entropy values, from high probability to low probability states. This balance prevents their probabilities from collapsing under negative feedback while keeping them informative for reasoning. As result, tokens like wait remain viable options throughout training, allowing the model to explore alternative reasoning paths rather than overfitting to fixed usage patterns. Figure 7 further compares the frequency of explorative tokens (but, wait, perhaps, alternatively, and however) under GRPO and Lp-Reg. Our method consistently maintains higher fraction of these tokens, demonstrating that Lp-Reg not only broadens their probabilityentropy distribution but also sustains their practical use throughout training. 8 Figure 7: Frequency of explorative tokens during training. 6.3 Probabilistic Distinction between Reasoning Sparks and Noise Our introduction established challenge for successful exploration strategy: it must protect valuable, lowprobability reasoning sparks without simultaneously amplifying the destructive effects of irrelevant noise. This raises critical question: is there systemic, observable difference between these two classes of tokens within the low-probability range that our method can exploit? To investigate this, we analyze the next-token prediction distribution throughout the training process. Due to storage limitations, we focus our analysis on the top-64 most probable tokens, but specifically examine those within low-probability range (0 to 0.1) to isolate the phenomenon from high-probability tokens. Figure 8 plots the average probability of two distinct classes of tokens over time: group of meaningful exploratory tokens (e.g., wait, perhaps) and group of irrelevant tokens (e.g., cost, fine). Figure 8: Probabilistic distinction between exploratory and irrelevant tokens across training steps in standard GRPO training. The results reveal clear and consistent statistical distinction: across all training stages, the average next-token probability of meaningful exploratory tokens is persistently higher than that of irrelevant tokens. It can be attributed to the intrinsic confidence of LLMs (Nguyen et al., 2025; Xu et al., 2025; Fu et al., 2025). This persistent probabilistic gap provides the foundational justification for our Lp-Reg design. It suggests that while perfect separation is not possible, probability threshold τ, as defined for our proxy distribution in Section 4.1, can serve as principled filtering mechanism. By setting such threshold, we can effectively filter out substantial portion of the lowest-probability irrelevant tokens, which constitute destabilizing noise, while simultaneously retaining majority of the valuable exploratory tokens that give rise to reasoning sparks. This allows Lp-Reg to focus its regularization on tokens that are more likely to be meaningful, providing targeted and robust approach to preserving high-quality exploration."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we investigated the exploration collapse in Reinforcement Learning with Verifiable Rewards. We identified key mechanism driving this failure: the systematic elimination of class of valuable, low-probability exploratory tokens we term reasoning sparks. To address this, we introduced Low-probability Regularization (Lp-Reg), method designed to selectively preserve these crucial exploratory pathways. Lp-Reg is founded on the key insight that within the low-probability range, meaningful exploratory tokens consistently exhibit higher average probabilities than semantically irrelevant tokens, whose low-probability appearances act as noise. By leveraging this statistical distinction to filter out irrelevant noise and regularizing the policy towards the remainder, our method effectively protects valuable low-probability tokens from being extinguished. This focus on exploration quality over quantity enables stable on-policy training for around 1,000 steps, resulting in 2.66% test accuracy improvement over baselines and underscoring the importance of preserving the reasoning sparks within the policys low-probability tail."
        },
        {
            "title": "References",
            "content": "Shivam Agarwal, Zimin Zhang, Lifan Yuan, Jiawei Han, and Hao Peng. The unreasonable effectiveness of entropy minimization in llm reasoning, 2025. URL https://arxiv.org/abs/2505.15134. Taubenfeld Amir, Sheffer Tom, Ofek Eran, Feder Amir, Goldstein Ariel, Gekhman Zorik, and Yona Gal. Confidence improves self-consistency in llms. arXiv preprint arXiv:2502.06233, 2025. URL https://www.arxiv. org/abs/2502.06233. Ghasemabadi Amirhosein, Mills Keith, G., Li Baochun, and Niu Di. Guided by gut: Efficient test-time scaling with reinforced intrinsic confidence. arXiv preprint arXiv:2505.20325v1, 2025. URL https://www.arxiv. org/abs/2505.20325v1. Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective on reinforcement learning for llms, 2025. URL https: //arxiv.org/abs/2506.14758. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, Zhiyuan Liu, Hao Peng, Lei Bai, Wanli Ouyang, Yu Cheng, Bowen Zhou, and Ning Ding. The entropy mechanism of reinforcement learning for reasoning language models, 2025. URL https: //arxiv.org/abs/2505.22617. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Yichao Fu, Xuewei Wang, Yuandong Tian, and Jiawei Zhao. Deep think with confidence, 2025. URL https: //arxiv.org/abs/2508.15260. Zitian Gao, Lynx Chen, Haoming Luo, Joey Zhou, and Bryan Dai. One-shot entropy minimization, 2025. URL https://arxiv.org/abs/2505.20282. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. OlympiadBench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 38283850, Bangkok, Thailand, August 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.211. URL https://aclanthology.org/ 2024.acl-long.211/. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024b. 10 Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Bo An, Yang Liu, and Yahui Zhou. Skywork open reasoner 1 technical report, 2025. URL https://arxiv.org/abs/2505.22312. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob In Thirty-fifth Conference Steinhardt. Measuring mathematical problem solving with the MATH dataset. on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https: //openreview.net/forum?id=7Bywt2mQsCe. Aitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=IFXTZERXdM7. Li Loka, Chen Zhenhao, Chen Guangyi, Zhang Yixuan, Su Yusheng, Xing Eric, and Zhang Kun. Confidence matters: Revisiting intrinsic self-correction capabilities of large language models. arXiv preprint arXiv:2402.12563, 2024. URL https://www.arxiv.org/abs/2402.12563. MAA. American invitational mathematics examination (AIME). Mathematics Competition Series, n.d. URL https://maa.org/math-competitions/aime. MiniMax, :, Aili Chen, Aonian Li, Bangwei Gong, Binyang Jiang, Bo Fei, Bo Yang, Boji Shan, Changqing Yu, Chao Wang, Cheng Zhu, Chengjun Xiao, Chengyu Du, Chi Zhang, Chu Qiao, Chunhao Zhang, Chunhui Du, Congchao Guo, Da Chen, Deming Ding, Dianjun Sun, Dong Li, Enwei Jiao, Haigang Zhou, Haimo Zhang, Han Ding, Haohai Sun, Haoyu Feng, Huaiguang Cai, Haichao Zhu, Jian Sun, Jiaqi Zhuang, Jiaren Cai, Jiayuan Song, Jin Zhu, Jingyang Li, Jinhao Tian, Jinli Liu, Junhao Xu, Junjie Yan, Junteng Liu, Junxian He, Kaiyi Feng, Ke Yang, Kecheng Xiao, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Li, Lin Zheng, Linge Du, Lingyu Yang, Lunbin Zeng, Minghui Yu, Mingliang Tao, Mingyuan Chi, Mozhi Zhang, Mujie Lin, Nan Hu, Nongyu Di, Peng Gao, Pengfei Li, Pengyu Zhao, Qibing Ren, Qidi Xu, Qile Li, Qin Wang, Rong Tian, Ruitao Leng, Shaoxiang Chen, Shaoyu Chen, Shengmin Shi, Shitong Weng, Shuchang Guan, Shuqi Yu, Sichen Li, Songquan Zhu, Tengfei Li, Tianchi Cai, Tianrun Liang, Weiyu Cheng, Weize Kong, Wenkai Li, Xiancai Chen, Xiangjun Song, Xiao Luo, Xiao Su, Xiaobo Li, Xiaodong Han, Xinzhu Hou, Xuan Lu, Xun Zou, Xuyang Shen, Yan Gong, Yan Ma, Yang Wang, Yiqi Shi, Yiran Zhong, Yonghong Duan, Yongxiang Fu, Yongyi Hu, Yu Gao, Yuanxiang Fan, Yufeng Yang, Yuhao Li, Yulin Hu, Yunan Huang, Yunji Li, Yunzhi Xu, Yuxin Mao, Yuxuan Shi, Yuze Wenren, Zehan Li, Zelin Li, Zhanxu Tian, Zhengmao Zhu, Zhenhua Fan, Zhenzhen Wu, Zhichao Xu, Zhihang Yu, Zhiheng Lyu, Zhuo Jiang, Zibo Gao, Zijia Wu, Zijian Song, and Zijun Sun. Minimax-m1: Scaling test-time compute efficiently with lightning attention, 2025. URL https://arxiv.org/abs/2506.13585. Kevin Murphy. Reinforcement learning: an overview. arXiv preprint arXiv:2412.05265, 2024. Minh Nhat Nguyen, Andrew Baker, Clement Neo, Allen Roush, Andreas Kirsch, and Ravid Shwartz-Ziv. Turning up the heat: Min-p sampling for creative and coherent llm outputs, 2025. URL https://arxiv.org/abs/ 2407.01082. OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian Osband, Ignasi Clavera Gilaberte, Ilge Akkaya, Ilya Kostrikov, Ilya Sutskever, Irina Kofman, Jakub Pachocki, James Lennon, Jason Wei, Jean Harb, Jerry Twore, Jiacheng Feng, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joaquin Quinonero Candela, Joe Palermo, Joel Parish, Johannes Heidecke, John Hallman, John Rizzo, Jonathan Gordon, Jonathan Uesato, Jonathan Ward, Joost Huizinga, Julie Wang, Kai Chen, Kai Xiao, Karan Singhal, Karina Nguyen, Karl Cobbe, Katy Shi, Kayla Wood, Kendra Rimbach, Keren Gu-Lemberg, Kevin Liu, Kevin Lu, Kevin Stone, Kevin Yu, Lama Ahmad, Lauren Yang, Leo Liu, Leon Maksin, Leyton Ho, Liam Fedus, Lilian Weng, Linden Li, Lindsay McCallum, Lindsey Held, Lorenz Kuhn, Lukas Kondraciuk, Lukasz Kaiser, Luke Metz, 11 Madelaine Boyd, Maja Trebacz, Manas Joglekar, Mark Chen, Marko Tintor, Mason Meyer, Matt Jones, Matt Kaufer, Max Schwarzer, Meghan Shah, Mehmet Yatbaz, Melody Y. Guan, Mengyuan Xu, Mengyuan Yan, Mia Glaese, Mianna Chen, Michael Lampe, Michael Malek, Michele Wang, Michelle Fradin, Mike McClay, Mikhail Pavlov, Miles Wang, Mingxuan Wang, Mira Murati, Mo Bavarian, Mostafa Rohaninejad, Nat McAleese, Neil Chowdhury, Neil Chowdhury, Nick Ryder, Nikolas Tezak, Noam Brown, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivia Watkins, Patrick Chao, Paul Ashbourne, Pavel Izmailov, Peter Zhokhov, Rachel Dias, Rahul Arora, Randall Lin, Rapha Gontijo Lopes, Raz Gaon, Reah Miyara, Reimar Leike, Renny Hwang, Rhythm Garg, Robin Brown, Roshan James, Rui Shu, Ryan Cheu, Ryan Greene, Saachi Jain, Sam Altman, Sam Toizer, Sam Toyer, Samuel Miserendino, Sandhini Agarwal, Santiago Hernandez, Sasha Baker, Scott McKinney, Scottie Yan, Shengjia Zhao, Shengli Hu, Shibani Santurkar, Shraman Ray Chaudhuri, Shuyuan Zhang, Siyuan Fu, Spencer Papay, Steph Lin, Suchir Balaji, Suvansh Sanjeev, Szymon Sidor, Tal Broda, Aidan Clark, Tao Wang, Taylor Gordon, Ted Sanders, Tejal Patwardhan, Thibault Sottiaux, Thomas Degry, Thomas Dimson, Tianhao Zheng, Timur Garipov, Tom Stasi, Trapit Bansal, Trevor Creech, Troy Peterson, Tyna Eloundou, Valerie Qi, Vineet Kosaraju, Vinnie Monaco, Vitchyr Pong, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wes McCabe, Wojciech Zaremba, Yann Dubois, Yinghai Lu, Yining Chen, Young Cha, Yu Bai, Yuchen He, Yuchen Zhang, Yunyun Wang, Zheng Shao, and Zhuohan Li. Openai o1 system card, 2024. URL https://arxiv.org/abs/2412.16720. Kadavath Saurav, Conerly Tom, Askell Amanda, Henighan Tom, Drain Dawn, Perez Ethan, Schiefer Nicholas, Hatfield-Dodds Zac, DasSarma Nova, Tran-Johnson Eli, Johnston Scott, El-Showk Sheer, Jones Andy, Elhage Nelson, Hume Tristan, Chen Anna, Bai Yuntao, Bowman Sam, Fort Stanislav, Ganguli Deep, Hernandez Danny, Jacobson Josh, Kernion Jackson, Kravec Shauna, Lovitt Liane, Ndousse Kamal, Olsson Catherine, Ringer Sam, Amodei Dario, Brown Tom, Clark Jack, Joseph Nicholas, Mann Ben, McCandlish Sam, Olah Chris, and Kaplan Jared. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022. URL https://www.arxiv.org/abs/2207.05221. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024a. URL https://arxiv.org/abs/2402.03300. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024b. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Zhenpeng Su, Leiyu Pan, Xue Bai, Dening Liu, Guanting Dong, Jiaming Huang, Wenping Hu, Fuzheng Zhang, Kun Gai, and Guorui Zhou. Klear-reasoner: Advancing reasoning capability via gradient-preserving clipping policy optimization, 2025. URL https://arxiv.org/abs/2508.07629. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Weixin Xu, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, Zonghan Yang, and Zongyu Lin. Kimi k1.5: Scaling reinforcement learning with llms, 2025. URL https://arxiv.org/abs/2501.12599. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, Yuqiong Liu, An Yang, Andrew Zhao, Yang Yue, Shiji Song, Bowen Yu, Gao Huang, and Junyang Lin. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning, 2025. URL https://arxiv.org/abs/2506.01939. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. URL https: //arxiv.org/abs/2201.11903. 12 Zenan Xu, Zexuan Qiu, Guanhua Huang, Kun Li, Siheng Li, Chenchen Zhang, Kejiao Li, Qi Yi, Yuhao Jiang, Bo Zhou, Fengzong Lian, and Zhanhui Kang. Adaptive termination for multi-round parallel reasoning: An universal semantic entropy-guided framework, 2025. URL https://arxiv.org/abs/2507.06829. Zhao Xuandong, Kang Zhewei, Feng Aosong, Levine Sergey, and Song Dawn. Learning to reason without external rewards. arXiv preprint arXiv:2505.19590v2, 2025. URL https://www.arxiv.org/abs/2505. 19590v2. Wang Xuezhi and Zhou Denny. Chain-of-thought reasoning without prompting. arXiv preprint arXiv:2402.10200v2, 2024. URL https://www.arxiv.org/abs/2402.10200v2. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. URL https://arxiv.org/abs/2503.14476. Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, Xiangyu Yu, Gaohong Liu, Juncai Liu, Lingjun Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Ru Zhang, Xin Liu, Mingxuan Wang, Yonghui Wu, and Lin Yan. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks, 2025. URL https://arxiv.org/abs/2504.05118. Yuzhong Zhao, Yue Liu, Junpeng Liu, Jingye Chen, Xun Wu, Yaru Hao, Tengchao Lv, Shaohan Huang, Lei Cui, Qixiang Ye, Fang Wan, and Furu Wei. Geometric-mean policy optimization, 2025. URL https://arxiv. org/abs/2507.20673. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, and Junyang Lin. Group sequence policy optimization, 2025. URL https: //arxiv.org/abs/2507.18071. Omer Veysel agatan and Barıs Akgun. Failure modes of maximum entropy rlhf, 2025. URL https://arxiv. org/abs/2509.20265."
        },
        {
            "title": "A Details of Experiments",
            "content": "A.1 Further Training Dynamics The training dynamics of Lp-Reg and other RLVR methods on the Qwen2.5-32B base model are presented in Figure 9. The results show that Lp-Reg maintains comparable performance in test accuracy throughout the training process, underscoring the benefits of our low-probability token regularization strategy for preventing exploration collapse. Figure 9: Training dynamics on the Qwen2.5-32B-Base model. To best illustrate the performance differences, we compare the top-performing methods. A.2 Further Ablation Study Figure 10: Ablation study comparing low-probability token regularization versus high-entropy token regularization for Lp-Reg (on-policy) on the Qwen3-14B-Base model. The test datasets used are AIME-24 and AIME-25, with the average score reported over 16 test runs. Low Probability vs. High Entropy. To verify that targeting low-probability tokens is superior to the conventional wisdom of targeting high entropy, we conduct comparison between the high-entropy token regularization (w/ highest regularization) and the low-probability regularization (w/ lowest πθ regularization, vanilla Lp-Reg). Instead of applying Lp-Reg to the lowest 1% probability tokens, we apply an identical regularization mechanism to the tokens with the highest 1% entropy. As shown in Figure 10, this approach not only fails to improve performance but also fails to sustain high entropy, which collapses after an initial spike. This result reinforces our claim from the Introduction: high entropy is poor proxy for valuable exploration. As our analysis in Section 6.1 further corroborates, high-entropy tokens are often common function words or formatting characters, not the meaningful, low-probability exploratory tokens we term reasoning sparks. Regularizing them pollutes the learning signal without protecting the structured, low-probability reasoning paths necessary for progress. Forward KL vs. Reverse KL. We compare the performance of our chosen forward KL formulation, which is DKL(πproxyπθ), against the reverse KL formulation, DKL(πθπproxy), in Equation 6. As shown in Figure 11, our forward KL significantly outperforms the reverse KL. This result stems from the fact that our proxy distribution, πproxy, is heuristic reference derived from the current policy, not an ideal target distribution. The reverse KL, 14 DKL(πθπproxy), penalizes any deviation of πθ from πproxy, effectively forcing the policy to strictly imitate this non-ideal, heuristic target. This aggressive imitation constrains the protection of potentially valuable exploratory tokens. In contrast, the forward KL, DKL(πproxyπθ), provides much softer regularization: it only penalizes the policy for completely discarding tokens that πproxy considers plausible, without forcing strict match. This allows the policy to use πproxy as stabilizing guide while retaining the freedom to explore beyond it, which empirically leads to better performance. Figure 11: Ablation study comparing the forward and reverse KL formulations for the Lp-Reg penalty on the Qwen3-14B-Base model. The results demonstrate the superiority of the forward KL, which uses the heuristic proxy distribution as soft guide, over the reverse KL, which forces strict imitation. (a) Ablation study on ρ which defined the low-probability percentile threshold δB ρ . (b) Ablation study on κ which defined the noise threshold τ = κ maxoV πθ(o) Figure 12: Training dynamics of Lp-Reg method with different hyperparameters. A.3 Hyperparameter Sensitivity Analysis In this section, we analyze the sensitivity of two core hyperparameters in Lp-Reg to demonstrate the robustness of our method: the low-probability percentile ρ and the min-p ratio κ. The results are presented in Figure 12. The parameter ρ, as defined in our objective function (Equation 6), determines the percentile threshold for identifying low-probability tokens that are candidates for regularization. higher ρ means wider range of tokens are protected. As shown in the top panel of Figure 12, we evaluated ρ with values of 0.005, 0.010, and 0.015. The training trajectories for average test accuracy are comparable, and the final performance across all three settings is highly comparable. This indicates that Lp-Reg is not overly sensitive to the precise scope of tokens being protected within this reasonable range. The hyperparameter κ controls the adaptiveness of the min-p filtering threshold, which defines the boundary for what is treated as noise. smaller κ results in more conservative filtering strategy, removing fewer tokens. Our sensitivity analysis for κ, presented in the bottom panel of Figure 12, shows similar trend of stability. Across the tested values of 0.01, 0.02, and 0.03, the training curves and final performance remain consistently high and tightly clustered. Taken together, these results demonstrate the robustness of Lp-Reg. The methods effectiveness is not contingent on extensive, fine-grained hyperparameter tuning, highlighting its practical applicability."
        },
        {
            "title": "B Further Analysis",
            "content": "B.1 Details of Sampling Probability Density This section provides detailed, token-by-token breakdown of the aggregated distributions presented in Figure 1c and Figure 1d of the main paper, reinforcing the conclusions drawn from our analysis. Figure 14 exhibits the individual distribution of observed sampling probabilities for class of meaningful lowprobability exploratory tokens we term reasoning sparks: but, wait, perhaps, alternatively, and however. consistent trend is observable across all five tokens, validating our claims in the introduction. With standard GRPO training, the ability to sample these tokens at low probabilities is systematically eliminated, causing their distributions to collapse and shift towards higher probabilities. The indiscriminate entropy bonus (GRPO + Entropy Loss) is largely ineffective at restoring this crucial low-probability tail. In stark contrast, our proposed method, Lp-Reg, consistently maintains healthy, wide distribution for each of these tokens, demonstrating its effectiveness in preserving the models capacity for exploration. Conversely, Figure 15 details the behavior of class of what we term irrelevant tokens (e.g. cost, fine, balanced, ere, and trans). We refer to the low-probability sampling of these tokens as irrelevant noise, which can be destructive to the training process. These individual plots clearly illustrate the detrimental side effect of simple entropy bonus. For nearly every token, the GRPO + Entropy Loss baseline significantly amplifies the sampling of this irrelevant noise, which, as shown in our main analysis, contributes to faster performance collapse. Lp-Reg, by design, avoids this amplification and maintains suppressed probability distribution for these tokens, comparable to or even more constrained than the standard GRPO baseline. These detailed visualizations confirm that the elimination of reasoning sparks and the amplification of irrelevant noise are not artifacts of aggregation but are consistent patterns at the individual token level. This provides strong, granular evidence for the central challenge our paper addresses and highlights the necessity of selective preservation mechanism like Lp-Reg. B.2 Details of Probability-Entropy Distribution To supplement the aggregated analysis presented in Figure 6 of the main text, this section provides detailed breakdown of the probability-entropy distributions for individual meaningful exploratory tokens. Figure 16 shows consistent pattern across all representative tokens, ranging from but (Figure 16a) to however (Figure 16e). For frequently occurring tokens such as but, wait, and perhaps, we randomly subsample one out of every 20 instances for visualization. Under the baseline GRPO, these sparks are consistently confined to low-entropy, highprobability region, indicating collapse into deterministic usage. In contrast, the addition of an entropy loss pushes these tokens into highly scattered, often excessively high-entropy states, suggesting an uncontrolled and potentially noisy form of exploration. Our method, Lp-Reg, strikes crucial balance, maintaining structured and broad distribution across healthy range of entropy values. This consistent behavior demonstrates that the trends identified in the aggregated data are not artifacts of averaging. The individual plots offer strong, disaggregated evidence for our central claim: Lp-Reg effectively preserves the exploratory potential of reasoning sparks by preventing both the deterministic collapse seen in the baseline and the chaotic scattering induced by the indiscriminate entropy bonus. B.3 Training Dynamics of Regularized Token To better understand how Lp-Reg operates during training, we analyze the dynamics of the probability threshold δB ρ and the proportion of low-probability tokens subjected to regularization Figure 13, the threshold δB ρ gradually decreases with training steps. In parallel, the regularization ratio also declines steadily. This trend suggests that as training progresses, the extreme low-probability range becomes increasingly dominated by irrelevant tokens, constituting what we term irrelevant noise. In parallel, the semantically meaningful exploratory tokens are lifted into higher-probability regions, thus requiring less regularization. ρ πproxy(o)>0 πθ(o)<δB ρ . As shown in πθ(o)<δB B.4 Case Study To further illustrate the effect of the filter applied on low-probability tokens, Figure 17 presents case study of model-generated response, where low-probability tokens are highlighted according to whether they were preserved or filtered. Tokens with probability greater than τ are those retained by the filter, while tokens with probability 16 Figure 13: Training dynamics of the probability threshold and regularization ratio. smaller than τ are suppressed. The preserved tokens include meaningful exploratory markers such as Then, Wait, which guide the reasoning trajectory, whereas the discarded set largely consists of relatively irrelevant tokens such as We, also, that. This qualitative evidence complements our quantitative analysis, demonstrating that Lp-Reg effectively leverages the min-p transformation to distinguish between valuable low-probability exploratory tokens (reasoning sparks) and tokens that constitute destabilizing, irrelevant noise. (a) Density of observed sampling probabilities for token but. (b) Density of observed sampling probabilities for token wait. (c) Density of observed sampling probabilities for token perhaps. (d) Density of observed sampling probabilities for token alternatively. (e) Density of observed sampling probabilities for token however. Figure 14: Individual density of observed sampling probabilities for meaningful exploratory tokens: but, wait, perhaps, alternatively, and however. 18 (a) Density of observed sampling probabilities for token cost. (b) Density of observed sampling probabilities for token fine. (c) Density of observed sampling probabilities for token balanced. (d) Density of observed sampling probabilities for token ere. (e) Density of observed sampling probabilities for token trans. Figure 15: Individual density of observed sampling probabilities for irrelevant tokens: cost, fine, balanced, ere, and trans. 19 (a) Scattered probabilityentropy plot of observed sampling instances for the token but. (b) Scattered probabilityentropy plot of observed sampling instances for the token wait. (c) Scattered probabilityentropy plot of observed sampling instances for the token perhaps. (d) Scattered probabilityentropy plot of observed sampling instances for the token alternatively. (e) Scattered probabilityentropy plot of observed sampling instances for the token however. Figure 16: Individual scattered probabilityentropy plot of observed sampling instances for meaningful tokens: but, wait, perhaps, alternatively, and however. 20 Figure 17: An Example generated by Qwen3-14B-Base model trained by Lp-Reg from math reasoning."
        }
    ],
    "affiliations": [
        "LLM Department, Tencent",
        "Peking University",
        "The Chinese University of Hong Kong",
        "Tsinghua University"
    ]
}