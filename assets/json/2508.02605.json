{
    "paper_title": "ReMoMask: Retrieval-Augmented Masked Motion Generation",
    "authors": [
        "Zhengdao Li",
        "Siheng Wang",
        "Zeyu Zhang",
        "Hao Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-to-Motion (T2M) generation aims to synthesize realistic and semantically aligned human motion sequences from natural language descriptions. However, current approaches face dual challenges: Generative models (e.g., diffusion models) suffer from limited diversity, error accumulation, and physical implausibility, while Retrieval-Augmented Generation (RAG) methods exhibit diffusion inertia, partial-mode collapse, and asynchronous artifacts. To address these limitations, we propose ReMoMask, a unified framework integrating three key innovations: 1) A Bidirectional Momentum Text-Motion Model decouples negative sample scale from batch size via momentum queues, substantially improving cross-modal retrieval precision; 2) A Semantic Spatio-temporal Attention mechanism enforces biomechanical constraints during part-level fusion to eliminate asynchronous artifacts; 3) RAG-Classier-Free Guidance incorporates minor unconditional generation to enhance generalization. Built upon MoMask's RVQ-VAE, ReMoMask efficiently generates temporally coherent motions in minimal steps. Extensive experiments on standard benchmarks demonstrate the state-of-the-art performance of ReMoMask, achieving a 3.88% and 10.97% improvement in FID scores on HumanML3D and KIT-ML, respectively, compared to the previous SOTA method RAG-T2M. Code: https://github.com/AIGeeksGroup/ReMoMask. Website: https://aigeeksgroup.github.io/ReMoMask."
        },
        {
            "title": "Start",
            "content": "ReMoMask: Retrieval-Augmented Masked Motion Generation Zhengdao Li1 Siheng Wang2 Zeyu Zhang1 Hao Tang1 1Peking University 2Jiangsu University Equal contribution. Project lead. Corresponding author: bjdxtanghao@gmail.com. 5 2 0 2 4 ] . [ 1 5 0 6 2 0 . 8 0 5 2 : r Abstract Text-to-Motion (T2M) generation aims to synthesize realistic and semantically aligned human motion sequences from natural language descriptions. However, current approaches face dual challenges: Generative models (e.g., diffusion models) suffer from limited diversity, error accumulation, and physical implausibility, while Retrieval-Augmented Generation (RAG) methods exhibit diffusion inertia, partial-mode collapse, and asynchronous artifacts. To address these limitations, we propose ReMoMask, unified framework integrating three key innovations: 1) Bidirectional Momentum Text-Motion Model decouples negative sample scale from batch size via momentum queues, substantially improving cross-modal retrieval precision; 2) Semantic Spatiotemporal Attention mechanism enforces biomechanical constraints during part-level fusion to eliminate asynchronous artifacts; 3) RAG-Classier-Free Guidance incorporates minor unconditional generation to enhance generalization. Built upon MoMasks RVQ-VAE, ReMoMask efficiently generates temporally coherent motions in minimal steps. Extensive experiments on standard benchmarks demonstrate the state-of-the-art performance of ReMoMask, achieving 3.88% and 10.97% improvement in FID scores on HumanML3D and KIT-ML, respectively, compared to the previous SOTA method RAG-T2M. Code: https://github.com/ AIGeeksGroup/ReMoMask. Website: https://aigeeksgroup. github.io/ReMoMask. Introduction Human motion generation has attracted growing attention due to its broad applicability in domains such as gaming, film production (Zhang et al. 2024e), virtual reality, and robotics. Recent advancements aim to synthesize diverse and realistic motions to reduce manual animation costs and enhance content creation efficiency. Among these efforts, text-to-motion (T2M) generation (Zhang et al. 2024d,c, 2025, 2024b) has emerged as particularly intuitive paradigm, where the objective is to generate sequence of human joint positions based on textual description of the motion. Previous research on text-to-motion generation could be categorized into two directions. The first is conventional t2m models, which have explored various generative models such as generative adversarial networks (GANS) (Goodfellow et al. 2014), variational autoencoders (VAEs)(Kingma Figure 1: Comparison between t2m models. (a) The conventional t2m models. (b) The Existing RAG-t2m models. (c) The framework of our proposed ReMoMask. and Welling 2014), diffusion models (Ho, Jain, and Abbeel 2020), motion language models (Zhang et al. 2023a), or generative masked models (Li et al. 2024). Among them, generative mask models such as MoMask and MMM use motion quantizer to transform motion seuqnce into discrete tokens and train the model through randomly masked token prediction, resulting in high-fidelity motion synthesis. The second, termed RAG-t2m models, leverages retrieved text and motion knowledge from an external database to guide the motion generation, complementing the generative model, which performs well in handling uncommon text inputs. ReMoDiffuse and ReMoGPT represent two representative approaches: the former relies on the text-to-text similarity between captions using CLIP for the retrieval, while the latter adopts cross-modal retriever for improved motion alignment. As shown in Figure 1, although conventional t2m models provide precise motion synthesis and RAG-t2m models enhance generation versatility, these approaches still face two key challenges. First, the motion retriever is trained using mini-batch, which suffers from limited number of negative samples, thus limiting the learning of robust representations. Second, concatenating the text condition with 1D motion token is insufficient for modeling the relationships among the text condition, motion spatio-temporal information, and retrieved knowledge. These limitations highlight the need for new retriever training paradigm that supports larger negative samples, as well as more powerful information fusion mechanism. To address the first challenge, we propose bidirectional momentum text-motion modeling (BMM) algorithm, which provides mechanism of using two momentum encoders and maintaining dual queues that hold text and motion negative samples, respectively. At each step, the negative samples encoded by momentum encoders of the current minibatch are enqueued, while the oldest are dequeued. This design decouples the negative pool size from the mini-batch size, allowing larger negative set for contrastive learning. Furthermore, the two momentum encoders are updated via an exponential moving average of their online counterparts, ensuring temporal consistency across negatives. Moreover, to address the second challenge, we introduce semantics spatial-temporal attention (SSTA) mechanism. Unlike previous motion VQ quantizations that produce 1D token map and neglect spatial relationships between individual joints, SSTA tokenizes the motion into 2D token map via 2D RVQ-VAE, which not only captures the temporal dynamics but also aggregates the spatial information. During the later generation, the 2D token map is flattened and fused with text embedding, retrieved text features, and retrieved motion features by redefining the Q, K, matrix in the transformer layer. Compared to simply concatenating conditions, our powerful information fusion mechanism enables comprehensive alignment across text guidance, retrieved knowledge, motion temporal dynamics, and even motion spatial structure, facilitating precision and generalization simultaneously. Together, these components constitute ReMoMask, an early retrieval augmented text-to-motion masked model, text-to-motion approaches on which outperforms prior HumanML-3D and KIT-ML benchmarks. The contributions of our paper can be summarized as follows: We propose ReMoMask, an innovative RAG-t2m masked model, equipped with powerful information fusion mechanism, SSTA, which enables effective fusion of conditions with both temporal dynamics and spatial structure of motion. To enlarge the negative sample pool in text-motion contrastive learning, we proposed bidirectional momentum text-motion modeling algorithm (BMM), which decouples the number of negative samples from the mini-batch size and achieves state-of-the-art performance on textmotion retrieval. ReMoMask generates motion sequences with better generalization and precision than MoMask (Guo et al. 2024), achieving 3.14% improvement in MM Dist on HumanML3D and 32.35% in FID on KIT-ML."
        },
        {
            "title": "Related Work",
            "content": "Text-to-Motion Generation In the field of text-driven 3D human motion generation, numerous research achievements have been made. Initially, Text2Motion pioneered the establishment of the mapping between text and motion through adversarial learning. Subsequently, TM2T (Guo et al. 2022c) first introduced vector quantization (VQ), and T2M-GPT (Zhang et al. 2023b) employed autoregressive transformers for semantic control. However, T2M-GPT suffered from error accumulation during unidirectional decoding. MoMask (Guo et al. 2024) proposed hierarchical residual quantization framework, decomposing the motion into base tokens and residual tokens, and combined with bidirectional masked transformers for parallel decoding, achieving remarkable results on the HumanML3D dataset. Regarding masked modeling, MotionCLIP (Tevet et al. 2022) achieved unsupervised crossmodal alignment using CLIP but was limited by continuous representations.The diffusion models have significantly advanced this field. MotionGPT (Biao et al. 2023) discretizes motion and leverages autoregressive transformers to unify generation tasks, mitigating error accumulation. The denoising diffusion models (DDPMs) proposed by Song et al. (Song and Ermon 2020) enable high-quality parallel generation via non-autoregressive paradigms. Building on language model pretraining techniques by Radford et al. (Radford et al. 2019), researchers further enhance text semantic control. Current methods integrate discrete representations with diffusion frameworks to balance efficiency and generation quality. Moreover, for fine-grained part-level control, ParCo introduced breakthrough approach, it discretizes whole-body motion into six part motions (limbs, backbone, root) using lightweight VQ-VAEs to establish part priors. Retrieval-Augmented Generation Retrieval-augmented generation (RAG) has become powerful approach for enhancing large language models (LLMs) by incorporating external knowledge retrieved during inference (Guo et al. 2025; Qian et al. 2024; Gao et al. 2023). Initially developed for natural language processing tasks, RAG helps models generate factually grounded, contextually relevant, and domain-specific responses, addressing common issues such as hallucinations, outdated knowledge, and limited expertise in closed models. typical RAG system consists of three key components: indexing, retrieval, and generation. Data is first encoded and stored in vector database; at inference, the most relevant information is retrieved based on the input query and used to guide generation. The development of RAG builds on the evolution of information retrieval (IR) methods. Early IR systems relied on sparse vector representations, with techniques like TFIDF (Sparck Jones 1972) and BM25 (Robertson et al. 1995) ranking documents based on term frequency and inverse document frequency. These methods, however, struggled to capture semantic similarity due to their reliance on exact term matches. With the rise of deep learning, neural IR models began representing queries and documents as dense vectors, typically using bi-encoder (Karpukhin et al. 2020; Izacard et al. 2021) model or cross-encoder (Nogueira and Cho 2019) model. These representations enabled more effective semantic matching through similarity computations, laying the foundation for modern retrieval-based generation. Beyond text, RAG has been extended to multimodal domains such as image (Qi et al. 2025), video (Ren et al. 2025), and motion generation (Zhang et al. 2023c; Kalakonda, Maheshwari, and Sarvadevabhatla 2024; Yu, Tanaka, and Fujiwara 2025), where external visual or motion references are retrieved to guide the generation process. These advances highlight the flexibility and broad applicability of the RAG framework across diverse tasks and modalities."
        },
        {
            "title": "The Proposed Method",
            "content": "Framework Overview Figure 2 shows the overall architecture of ReMoMask. To ensure the quality of the motion in both temporal dynamics and spatial structure, we quantize motion sequence into 2D spatial-temporal map via 2D RVQ-VAE encoder. During generation, starting from an all masked 2D token map, ReMoMask first retrieves text and motion features using Part-Level Bidirectional Momentum Text-Motion Retriever, which is trained with the Bidirectional Momentum text-motion modeling (BMM) algorithm to enable large negative samples pool. These retrieved features are then fed into the Masked Transformer and fused by Semantics Spatial-Temporal Attention (SSTA), providing strong semantic alignment and guidance for reconstructing the core motion structure. Finally, Residual Transformer refines motion details, and the latent motion vector is decoded through 2D RVQ-VAE decoder. Bidirectional Momentum Text-Motion Modeling As shown in Figure 2(a), we adopt dual momentum encoder architecture equipped with corresponding memory queues. Let and denote the motion and text encoders, parameterized by θm and θt, respectively. To ensure temporal consistency across negative samples, we introduce two momentum counterparts ˆf and ˆf t, with parameters ˆθm and ˆθt, which are updated using exponential moving averages with momentum coefficient m: ˆθm = ˆθm + (1 m) θm, ˆθt = ˆθt + (1 m) θt, (1) (2) To decouple the size of negative samples from the mini-batch size, we employ two negative queues: Qm = {km is momentum feature vector extracted via: j=1, where each km j=1 and Qt = {kt and kt }Nq j}Nq for each sample pair and enqueue them into their respective queues, while simultaneously dequeuing the oldest Nb entries to maintain fixed queue size. For contrastive learning, each motion sample takes its paired text as the positive example, and all entries in Qt are treated as negatives. The motion-to-text contrastive loss is formulated as: Nb(cid:88) LM2T = i=1 log exp(qm exp(qm kt kt /τ ) /τ )+neg(qm ,Qt, τ ) , (4) = m(mi) and kt = ˆf t(ti). The negative term is where qm defined by: neg(qm , Qt, τ ) = (cid:88) kt Qt exp(qm kt j/τ ). (5) Analogously, we reverse the roles of motion and text to compute the text-to-motion contrastive loss: Nb(cid:88) LT2M = log exp(qt km /τ ) /τ )+neg(qt , (6) i=1 exp(qt km = ˆf m(mi). The final bidirecwhere qt tional momentum contrastive loss is the sum of both directions: = t(ti) and km , Qm, τ ) LBMM = LM2T + LT2M. (7) Algorithm 1: Bidirectional Momentum Text-Motion Modeling i=1 ˆθm θm Require: Training set Require: Online encoders t, with parameters θt, θm Require: Momentum encoders ˆf t, ˆf with ˆθt, ˆθm Require: Queues: Qt, Qm 1: Initialize: ˆθt θt, 2: while not converged do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: end while Sample mini-batch = {(mi, ti)}Nb // Feature encoding for each (mi, ti) do m(mi) qm t(ti), qt ˆf m(mi) ˆf t(ti), km kt end for // Contrastive loss Compute LM2T and LT2M using Eq.(4) and Eq.(6) LBMM LM2T + LT2M // Optimization Update θt, θm by minimizing LBMM // Momentum update ˆθt ˆθt + (1 m) θt ˆθm ˆθm + (1 m) θm // Queue update }Nb Enqueue {kt i=1 into Qm i=1 into Qt, {km Dequeue earliest Nb entries from each queue }Nb = ˆf m(mj), kt km = ˆf t(tj). (3) Given training mini-batch = {(mi, ti)}Nb i=1 (Nb = Nq), we compute the momentum features Semantics Spatial-Temporal Attention As demonstrated in Figure 2(c), for 2D token map generated by 2D RVQ-VAE (discussed in later sections), we first Figure 2: Overview of ReMoMask. (a) Bidirectional Momentum Contrastive Retrieval (BMM) uses two momentum queues, enabling large pool of negative samples for contrastive learning. (b) ReMoMask quantizes motion sequence into 2D token map, capturing not only temporal dynamics but also spatial structure. After that, Part-Level BMM Retriever retrieves relevant text and motion features based on the prompt embedding. All these conditions are fused via an SSTA module in 2D RAGMask-Transformer together with the latent motion representaion. (c) Semantic Spatial-temporal Attention (SSTA) first flattens the masked 2D token map into 1D structure, then redefines the Q, K, matrix utilizing the conditions above, providing effective semantic alignment between the conditions and the spatial-temporal information of motion add 2D position encoding (Wang and Liu 2019). The 2D position encoding RT is obtained by independently applying sinusoidal functions along the temporal and spatial axes. After that, we flatten the 2D token map to 1D structure, resulting in latent motion vector R(T J)d. Then we extract the text embedding R1d using the text encoder of Clip (Radford et al. 2021), and get the retrieved text feature Rt R1d and retrieved motion feature Rm R1d using part-level bmm retriever (as will be explained later). Here, is the number of joints, is the number of frames, and is the feature dimension. Then we perform the adapted attention in the flattened spatial-temporal dimension, as: Assta(Q, K, ) = softmax (cid:18) QK (cid:19) + V, (8) = m, = concat(z, t, [Rm; Rt]), = concat(z, t, Rm). (9) (10) (11) where [; ] denotes the concatenation of both terms, is the 2D Position embedding, and Q, K, are the query, key, and value matrices. We refer to this attention mechanism as Semantics Spatial-Temporal Attention (SSTA). simplified variant that only concatenates the text embedding with the motion vector and excludes retrieval features, is termed Spatial-Temporal Attention (STA). As shown in Figure 2(b), both attention mechanisms will be used in the subsequent motion generation module. Training: Part-Level BMM Retriever To model fine-grained motion details, we also implement part-level motion encoder inspired by (Yu, Tanaka, and Fujiwara 2025; Zou et al. 2024). Concretely, we divide the whole-body motion into six parts and embed each part, respectively. These part-level features are then concatenated and reprojected into latent dimension to produce finegrained motion feature, enabling more precise retrieval of part-specific features. For the retriever, we replacing the motion encoder and its corresponding momentum model ˆf with part-level motion encoder pl and ˆf pl in Equation 4 and Equation 6. The full training objective of Partlevel BMM Retriever is: (12) T2M. M2T + Lpl BMM = Lpl Lpl Training: Retrieval-Augmented MoMask Network Architecture As illustrated in Figure 2(b), ReMoMask includes three key components: 2D RVQVAE encode-decoder quantizes motion into discrete 2D tokens and reconstructs motion from them. 2D retrievalaugmented masked transformer generates base-layer tokens conditioned on text and retrieved features. 2D residual transformer refines the remaining token layers to capture fine-grained details. Table 1: Performance comparison on HumanML3D dataset. R-Precision FID MM Dist Diversity MultiModality Method Real Motions Top1 Top2 Top 0.511 0.703 0.797 0.002 MoCoGAN (Tulyakov et al. 2018) Dance2Music (Lee et al. 2019) Language2Pose (Ahuja and Morency 2019) Text2Gesture (Bhattacharya et al. 2021) T2M (Guo et al. 2022a) T2M-GPT (Zhang et al. 2023b) FineMoGen (zhang et al. 2023) MDM (Tevet et al. 2023) MotionDiffuse (Zhang et al. 2024a) MoMask (Guo et al. 2024) MoGenTS (Yuan et al. 2024) ReMoDiffuse (Zhang et al. 2023d) ReMoGPT (Yu, Tanaka, and Fujiwara 2024) RMD (Liao et al. 2024) MoRAG-Diffuse (Sai Shashank Kalakonda 2024) 0.037 0.033 0.246 0.165 0.457 0.491 0.504 0.491 0.521 0.529 0.510 0.501 0.524 0.511 0.072 0.065 0.387 0.267 0.639 0.680 0.690 0.681 0.713 0.719 0.698 0.688 0.715 0.699 0.106 0.097 0.486 0.345 0.740 0.775 0.784 0.611 0.782 0.807 0.812 0.795 0.792 0.811 0. ReMoMask (Ours) 0.531 0.722 0.813 94.41 66.98 11.02 7.664 1.067 0.116 0.151 0.544 0.630 0.045 0.033 0.103 0.205 0.111 0. 0.099 2.974 9.643 8.116 5.296 6.030 3.340 3.118 2.998 5.566 3.113 2.958 2.867 2.974 2.929 2.879 2.950 2.865 9. 0.462 0.725 7.676 6.409 9.188 9.761 9.263 9.559 9.410 9.570 9.018 9.763 9.527 9.536 9.535 0.019 0.043 2.090 1.856 2.696 2.799 1.553 1.241 1.795 2.816 2.604 2. 2.823 2D Residual VQ-VAE Given motion RT , we first extract 2D latent features ˆy RT using 2D convolutional encoder E2d. We then apply residual vector quantization (RVQ) (Guo et al. 2024) with L+1 levels: yl = Ql(rl), rl+1 = rl yl, (13) starting from r0 = ˆy, where Ql() denotes the vector quantization operation at level l, mapping each latent vector to its nearest code in learnable codebook. The final summed quantized representation (cid:80)L l=0 yl is then fed into 2D convolutional decoder D2d to reconstruct motion, resulting in ˆm RT . We train the model by minimizing reconstruction and embedding losses using the straight-through gradient estimator, as L2D-RVQ = ˆm1 + γ (cid:88) l=1 rl sg[yl]2 2, (14) where sg[] denotes the stop-gradient operation, and γ controls the strength of the embedding loss. 2D Retrieval-Augmented Masked-Transforemr Under the hypothesis of hierarchical RVQ-VAE, the base quantization layergenerated by masked transformercaptures the coarse motion structure, while the residual transformer layers refine fine-grained details. Since the base layer captures the main semantics of motion, we introduce retrievalaugmented context to this stage only, aiming to enhance structural reconstruction without overburdening the refinement stages. We design our 2D retrieval-augmented masked transformer to generate the base-layer motion tokens y0 RT , conditioned on the text embedding t, retrieved text feature Rt, and retrieved motion feature Rm, all of which are fused using SSTA. To train the model, we randomly mask subset of tokens in y0, replacing them with [MASK] token to obtain corrupted sequence y0 msk. We perform 2D Mask strategy (Yuan et al. 2024) by first randomly masking along the temporal dimension and then randomly masking along the spatial dimension on those unmasked frames. The model is then trained to reconstruct the original tokens by minimizing the negative log-likelihood: Lrag mask = (cid:88) [MASK] log p(y0 y0 msk, t, Rt, Rm). (15) We also employ masking ratio schedule and BERT-style remasking strategy, extending the previous method (Chang et al. 2022, 2023; Devlin et al. 2019; Guo et al. 2024). 2D Residual Transformer The architecture of our 2D residual transformer mirrors that of the 2D retrieval augmented masked transformer, except that we adopt STA instead of SSTA. During training, we randomly select quantization layer [1, L] to predict. All tokens from previous layers y0:l1 are summed to form the latent input, together with the quantizer layer index and the text condition t. The model is optimized by minimizing the negative loglikelihood: Lres = (cid:88) l=1 log (cid:0)yl y0:l1, l, t(cid:1) , (16) Inference In the inference, we start from an empty 2D token map that all tokens are masked, defined as RT . Then the 2D retrieval-augmented masked transformer repeatedly predicts the masked tokens as base quantization layer by iterations, conditioned on the caption embedding t, the retrieved motion feature Rm, and the retrieved text features Rt. Once Table 2: Performance comparison on KIT-ML dataset. R-Precision FID MM Dist Diversity MultiModality Method Real Motions Top Top2 Top3 0.424 0.649 0.779 0. MoCoGAN (Tulyakov et al. 2018) Language2Pose (Ahuja and Morency 2019) Dance2Music (Lee et al. 2019) Text2Gesture (Bhattacharya et al. 2021) T2M (Guo et al. 2022a) MotionDiffuse (Zhang et al. 2024a) T2M-GPT (Zhang et al. 2023b) MDM (Tevet et al. 2023) MoMask (Guo et al. 2024) MoGenTS (Yuan et al. 2024) 0.022 0.221 0.031 0.156 0.370 0.417 0.416 0.433 0.445 0.042 0.373 0.058 0.255 0.569 0.621 0.627 0.656 0.671 0.063 0.483 0.086 0.338 0.693 0.739 0.745 0.396 0.781 0.797 82.69 6.545 115.4 12.12 2.770 1.954 0.514 0.497 0.204 0.143 ReMoDiffuse (Zhang et al. 2023d) 0.427 0.641 0.765 0.155 ReMoMask (Ours) 0. 0.682 0.805 0.138 2.788 10.47 5.147 10.40 6.964 3.401 2.958 3.007 9.191 2.779 2.711 2. 2.682 11.08 3.091 9.073 0.241 9.334 10.91 11.10 10.92 10.85 10.918 10.80 10.83 0.250 0.062 1.482 0.730 1.570 1.907 1.131 1.239 2.017 the prediction is completed, the 2D residual transformer progressively predicts the residual tokens of the rest quantization layers. As final stage, all tokens are decoded and projected back to motion sequences through the 2D RVQ-VAE decoder. RAG Classifier Free Guidance We extend the classifierfree guidance (CFG) to incorporate the text embedding t, retrieved text feature Rt, and retrieved motion feature Rm as conditional inputs. Explicitly, we define the guidance condition as {t, Rt, Rm}, the final logits are computed as: logits = (1 + s) logitscon logitsun, con = {t, Rt, Rm} (17) (18) where logits is the output of the final linear projection layer and is the guidance scale (set to 4). During training, unconditional sampling is applied with probability of 10% to enable guidance-free learning. At inference, R-CFG is applied to the final projection layer prior to softmax."
        },
        {
            "title": "Dataset and Evaluation Metrics",
            "content": "We evaluate our model on HumanML3D (Guo et al. 2022b) and KIT-ML (Plappert, Mandery, and Asfour 2016) datasets. The HumanML3D dataset (Guo et al. 2022b) stands as the largest available dataset focused solely on 3D body motion and associated textual descriptions. It consists of 14616 motion sequences and 44970 text descriptions, and KIT-ML consists of 3911 motions and 6278 texts. The motion pose is extracted into the motion feature with dimensions of 263 and 251 for HumanML3D and KIT-ML respectively. Following previous methods (Guo et al. 2022b), the datasets are augmented by mirroring,and divided into training, testing,and validation sets with the ratio of 0.8:0.15:0.05. Evaluation Metrics We adapt standard evaluation metrics to assess various aspects of our experiments: For overall motion quality, we propose Frechet Inception Distance (FID) to measure the distributional difference between high-level features of generated and real motions. For semantic alignment between input text and generated motions, we propose RPrecision and multimodal distance. For diversity of motions generated from the same text, we propose Multimodality."
        },
        {
            "title": "Implementation Details",
            "content": "Our framework is trained on four NVIDIA H20 GPUs using PyTorch with batch size of 256 and learning rate of 2 104. For motion quantization, we employ 2D RVQVAE structure following (Yuan et al. 2024). The pose data is restructured into joint-based format of size 12 and quantized into 2D latent representations. This is achieved using two codebooks: (1) joint VQ codebook containing 256 codes (dimension 1024), and (2) global VQ codebook with 256 codes (dimension 1024) to capture holistic motion information.For motion generation, we implement two transformer architectures:M-trans (Motion Transformer): 6 layers, 8 attention heads, and 512 latent dimensions, R-trans (Residual Transformer): 6 layers, 6 attention heads, and 384 latent dimensions. The 2D Residual Transformer structure follows (Guo et al. 2024) with 5 residual layers.For retrieval enhancement, we design 2D Retrievalaugmented Masked Transformer using momentum contrastive learning. The motion encoder adopts RemoGPTs 4-layer Transformer architecture (Yu, Tanaka, and Fujiwara 2024) with 512 latent dimensions. Text embeddings are generated using DistilBERT (Victor et al. 2019). Key hyperparameters include: momentum coefficient = 0.99, temperature τ = 0.07, and dynamic queue size 65536. This module is trained on eight NVIDIA A800 GPUs with batch size 128 for 200 epochs. Main Results Evaluation of Motion Generation we compare our model with previous text-to-motion works, including combing RAG method and without RAG method. From the results reported in Table 1, 2, our method outperforms all previous methods on both the HumanML3D and the KITML datasets, which demonstrates the effectiveness of our method. Crucially, the FID is decreased by 0.093 on HumanML3D compared and is decreased by 0.066 on KIT-ML compared to MoMask (Guo et al. 2024). Moreover, the Rprecision even significantly surpasses the ground truth. Evaluation of Retriever As shown in Table 3, in the textto-motion retrieval task, BMM achieves state-of-the-art performance with significant improvements across key metrics. It obtains the highest scores in R1 (13.76), R2 (21.03), R3 (25.63), and R5 (32.40), outperforming PL-TMR with absolute gains ranging from 2.76% to 2.92%. Although its R10 score (43.27) is slightly lower than that of PL-TMR (43.43), the overall superiority of BMM is evident. Similarly, BMM demonstrates strong performance in the motion-to-text retrieval task, achieving the highest results in R1 (14.80) and R3 (25.60), with notable 2.55% absolute improvement in R1 over PL-TMR. However, it shows limitations in higherrecall metrics: its R5 (25.75) and R10 (34.61) lag behind PLTMR by 2.59% and 4.50%, respectively. Moreover, BMMs MedR (25.00) also underperforms, suggesting reduced effectiveness in retrieving multiple relevant texts per motion. Ablation Study This section conducts systematic ablation experiments to validate the contributions of ReMoMasks core modules: Bidirectional Momentum Model (BMM), Semantic Spatio-Temporal Attention (SSTA), Retrieval-Augmented Classifier-Free Guidance (RAG-CFG), and local retrieval mechanism. Results conclusively demonstrate the necessity of each innovative component. Core Module Contribution Analysis As shown in Table 4, the full model (ReMoMask) achieves comprehensive SOTA performance on HumanML3D: BMM Module: Removal causes 16.2% Top1 R-Precision drop (0.5310.445) and 50.18% FID degradation (0.4110.825), proving its irreplaceability in cross-modal alignment. SSTA Module: Replacement with feature concatenation leads to 61.2% multimodality collapse (2.8231.094) and 6.1% MM Dist increase (2.8653.04), highlighting its critical role in motion diversity. RAG-CFG: Deactivation reduces Top1 RPrecision by 22.6% (0.5310.411), confirming its efficacy in enhancing text-motion consistency. Local Retrieval: Global retrieval substitution decreases Top3 R-Precision by 9.8% (0.8130.733) and diversity by 4.8% (9.5359.08), demonstrating the superiority of local context retrieval. Bidirectional Momentum Queue Optimization Table 5 reveals the impact of momentum queue design on crossretrieval: TextMotion Retrieval: Bidirectional modal queues improve R1 by 31.3% (10.4813.76) and reduce MedR by 15.8% (1916) compared to no queues. MotionText Retrieval: Unidirectional queues cause catasTable 3: Performance comparison of text-motion retrieval tasks on HumanML3D dataset. Method Params R1 R2 R3 R5 R10 MedR Text-to-motion retrieval TMR MotionPatches PL-TMR 82M 8.92 152M 10.80 118M 11.00 12.04 14.98 17. 16.33 20.00 22.18 22.06 26.72 29.48 33.37 38.02 43.43 BMM 238M 13.76 21. 25.63 32.40 43.27 Motion-to-text retrieval TMR MotionPatches PL-TMR 82M 9.44 152M 11.25 118M 12. 11.84 13.86 14.95 16.90 19.98 21.45 22.92 26.86 28.34 32.21 37.40 39.11 BMM 238M 14. 15.63 25.60 25.75 34.61 25.00 19.00 14.00 16. 26.00 20.00 19.00 25.00 trophic failure (R1=0.70), while bidirectional queues boost R1 by 41.0% (10.5014.80), proving symmetric negative sample queues are indispensable for bidirectional retrieval. The optimal configuration significantly outperforms baselines in both tasks, validating the robustness of our bidirectional momentum design. Table 4: Ablation study 1 on HumanML3D dataset. We test ReMoMasks core modules: BMM, SSTA, RAG-CFG, and local retrieval mechanism. Method R-Precision FID MM Dist Diversity MultiModality Top1 Top2 Top3 w/o BMM w/o SSTA w/o RAG-CFG w/o Coarse-Level Retrieval 0.445 0.495 0.411 0. 0.639 0.652 0.612 0.644 0.751 0.789 0.741 0.733 ReMoMask (Ours) 0.531 0.722 0. 0.825 0.714 0.798 0.722 0.411 3.44 3.04 3.16 3.32 2.865 8.80 9.39 9.12 9.08 9. 1.017 1.094 1.088 1.044 2.823 Table 5: Ablation study 2 on HumanML3D.We explored the importance of BMM text queue motion queue R1 R2 R3 R5 R10 MedR Text-to-motion retrieval 10.48 12.44 13.76 15.80 18.98 21.03 20.90 22.79 25.63 28.59 29.72 32.40 41.25 43.02 43.27 19.00 17.00 16. Motion-to-text retrieval 10.50 0.70 14.80 13.36 1.02 15.63 18.64 1.22 25.60 24.98 1.87 25.75 36.83 3.18 34. 20.00 552.00 25.00 Conclusion In this paper, we introduce an innovative retrievalaugmented masked model, ReMoMask, for text-driven motion generation. The proposed bidirectional momentum textmotion relation modeling enlarges the set of negative samples across modalities, facilitating more effective contrastive learning for the part-level retriever. Quantizing the motion sequence into 2D token map and applying well-designed cross-attention with textual and retrieved conditions enables more expressive fusion of conditional semantics and spatio-temporal motion dynamics. Extensive experiments on HumanML3D and KIT datasets demonstrate that our model achieves SOTA performance. References Ahuja, C.; and Morency, L.-P. 2019. Language2Pose: Natural Language Grounded Pose Forecasting. In International Conference on 3D Vision (3DV), 719728. Bhattacharya, U.; Rewkowski, N.; Banerjee, A.; Guhan, P.; Bera, A.; and Manocha, D. 2021. Text2Gestures: Transformer-Based Network for Generating Emotive Body Gestures. In IEEE Virtual Reality and 3D User Interfaces (VR), 110. Biao, J.; Xin, C.; Wen, L.; Jingyi, Y.; Gang, Y.; and Tao, C. 2023. MotionGPT: Unified Human Motion Generation via Autoregressive Transformers. In NeurIPS. Chang, H.; Zhang, H.; Barber, J.; Maschinot, A.; Lezama, J.; Jiang, L.; Yang, M.-H.; Murphy, K.; Freeman, W. T.; Rubinstein, M.; Li, Y.; and Krishnan, D. 2023. Muse: TextTo-Image Generation via Masked Generative Transformers. arXiv:2301.00704. Chang, H.; Zhang, H.; Jiang, L.; Liu, C.; and Freeman, W. T. 2022. MaskGIT: Masked Generative Image Transformer. arXiv:2202.04200. Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805. Gao, Y.; Xiong, Y.; Gao, X.; Jia, K.; Pan, J.; Bi, Y.; Dai, Y.; Sun, J.; Wang, H.; and Wang, H. 2023. Retrieval-augmented arXiv generation for large language models: survey. preprint arXiv:2312.10997, 2(1). Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y. 2014. Generative adversarial nets. In NIPS, 26722680. Guo, C.; Mu, Y.; Javed, M. G.; Wang, S.; and Cheng, L. 2024. MoMask: Generative Masked Modeling of 3D Human Motions. In CVPR. Guo, C.; Zou, S.; Zuo, X.; Wang, S.; Ji, W.; Li, X.; and Cheng, L. 2022a. Generating Diverse and Natural 3D Human Motions from Text. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 51525161. Guo, C.; Zou, S.; Zuo, X.; Wang, S.; Ji, W.; Li, X.; and Cheng, L. 2022b. Generating Diverse and Natural 3D Human Motions From Text. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 51525161. Guo, C.; Zuo, X.; Wang, S.; and Cheng, L. 2022c. Tm2t: Stochastic and tokenized modeling for the reciprocal generation of 3d human motions and texts. In ECCV. Guo, Z.; Xia, L.; Yu, Y.; Ao, T.; and Huang, C. 2025. LightRAG: Simple and Fast Retrieval-Augmented Generation. arXiv:2410.05779. Ho, J.; Jain, A.; and Abbeel, P. 2020. Denoising diffusion probabilistic models. In NIPS, volume 33, 68406851. Izacard, G.; Caron, M.; Hosseini, L.; Riedel, S.; Bojanowski, P.; Joulin, A.; and Grave, E. 2021. Unsupervised dense information retrieval with contrastive learning. arXiv preprint arXiv:2112.09118. Kalakonda, S. S.; Maheshwari, S.; and Sarvadevabhatla, R. K. 2024. MoRAG Multi-Fusion Retrieval Augmented Generation for Human Motion. arXiv:2409.12140. Karpukhin, V.; Oguz, B.; Min, S.; Lewis, P. S.; Wu, L.; Edunov, S.; Chen, D.; and Yih, W.-t. 2020. Dense Passage Retrieval for Open-Domain Question Answering. In EMNLP, 67696781. Kingma, D. P.; and Welling, M. 2014. Auto-encoding variational bayes. In ICLR. Lee, H.-Y.; Yang, X.; Liu, M.-Y.; Wang, T.-C.; Lu, Y.- D.; Yang, M.-H.; and Kautz, J. 2019. Dancing to Music. In Advances in Neural Information Processing Systems (NeurIPS), volume 32. Li, J.; Wang, J.; Xu, H.; Yang, J.; Xu, M.; Yu, J.; Chen, W.; et al. 2024. Generative Masked Autoencoders for 3D Human Motion Synthesis. arXiv preprint arXiv:2403.12096. Liao, Z.; Zhang, M.; Wang, W.; Yang, L.; and Komura, T. 2024. RMD: Simple Baseline for More General Human Motion Generation via Training-free Retrieval-Augmented Motion Diffuse. arXiv preprint arXiv:2412.04343. Nogueira, R.; and Cho, K. 2019. Passage Re-ranking with BERT. arXiv preprint arXiv:1901.04085. Petrovich, M.; Black, M. J.; and Varol, G. 2023. TMR: Textto-Motion Retrieval Using Contrastive 3D Human Motion Synthesis. arXiv:2305.00976. Plappert, M.; Mandery, C.; and Asfour, T. 2016. The KIT Motion-Language Dataset. Big Data, 4(4): 236252. Qi, J.; Xu, Z.; Wang, Q.; and Huang, L. 2025. AR-RAG: Autoregressive Retrieval Augmentation for Image Generation. arXiv preprint arXiv:2506.06962. Qian, H.; Zhang, P.; Liu, Z.; Mao, K.; and Dou, Z. Memorag: Moving towards next-gen rag via 2024. arXiv preprint memory-inspired knowledge discovery. arXiv:2409.05591, 1. Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; Krueger, G.; and Sutskever, I. 2021. Learning Transferable Visual Models From Natural Language Supervision. arXiv:2103.00020. Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and Sutskever, I. 2019. Language Models are Unsupervised Multitask Learners. OpenAI blog. Ren, X.; Xu, L.; Xia, L.; Wang, S.; Yin, D.; and Huang, C. 2025. VideoRAG: Retrieval-Augmented Generation arXiv preprint with Extreme Long-Context Videos. arXiv:2502.01549. Robertson, S. E.; Walker, S.; Jones, S.; Hancock-Beaulieu, M. M.; Gatford, M.; et al. 1995. Okapi at TREC-3. Nist Special Publication Sp, 109: 109. Sai Shashank Kalakonda, R. K. S., Shubh Maheshwari. 2024. MoRAG Multi-Fusion Retrieval Augmented Generation for Human Motion. arXiv preprint arXiv:2409.12140. Song, Y.; and Ermon, S. 2020. Denoising Diffusion Probabilistic Models. arXiv:2006.11239. frame mask mamba for extended motion generation. arXiv preprint arXiv:2411.06481. Zhang, Z.; Liu, A.; Chen, Q.; Chen, F.; Reid, I.; Hartley, R.; Zhuang, B.; and Tang, H. 2024c. Infinimotion: Mamba boosts memory in transformer for arbitrary long motion generation. arXiv preprint arXiv:2407.10061. Zhang, Z.; Liu, A.; Reid, I.; Hartley, R.; Zhuang, B.; and Tang, H. 2024d. Motion mamba: Efficient and long seIn European Conference on quence motion generation. Computer Vision, 265282. Springer. Zhang, Z.; Wang, Y.; Mao, W.; Li, D.; Zhao, R.; Wu, B.; Song, Z.; Zhuang, B.; Reid, I.; and Hartley, R. 2025. Motion anything: Any to motion generation. arXiv preprint arXiv:2503.06955. Zhang, Z.; Wang, Y.; Wu, B.; Chen, S.; Zhang, Z.; Huang, S.; Zhang, W.; Fang, M.; Chen, L.; and Zhao, Y. 2024e. Motion avatar: Generate human and animal avatars with arbitrary motion. arXiv preprint arXiv:2405.11286. Zou, Q.; Yuan, S.; Du, S.; Wang, Y.; Liu, C.; Xu, Y.; Chen, J.; and Ji, X. 2024. ParCo: Part-Coordinating Text-to-Motion Synthesis. arXiv:2403.18512. Sparck Jones, K. 1972. statistical interpretation of term specificity and its application in retrieval. Journal of documentation, 28(1): 1121. Tevet, G.; Gordon, B.; Hertz, A.; Bermano, A. H.; and Cohen-Or, D. 2022. Motionclip: Exposing human motion generation to clip space. ECCV. Tevet, G.; Raab, S.; Gordon, B.; Shafir, Y.; Cohen-Or, D.; and Bermano, A. H. 2023. Human Motion Diffusion Model. In International Conference on Learning Representations (ICLR). Tulyakov, S.; Liu, M.-Y.; Yang, X.; and Kautz, J. 2018. MoCoGAN: Decomposing Motion and Content for Video Generation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 15261535. Victor, S.; Lysandre, D.; Julien, C.; and Thomas, W. 2019. DistilBERT, distilled version of BERT: smaller, faster, cheaper and lighter. arXiv. Wang, Z.; and Liu, J.-C. 2019. Translating Math Formula Images to LaTeX Sequences Using Deep Neural Networks with Sequence-level Training. arXiv:1908.11415. Yu, Q.; Tanaka, M.; and Fujiwara, K. 2024. ReMoGPT: PartLevel Retrieval-Augmented Motion-Language Models. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 112. Yu, Q.; Tanaka, M.; and Fujiwara, K. 2025. ReMoGPT: PartLevel Retrieval-Augmented Motion-Language Models. Proceedings of the AAAI Conference on Artificial Intelligence, 39(9): 96359643. Yuan, W.; Shen, W.; He, Y.; Dong, Y.; Gu, X.; Dong, Z.; Bo, L.; and Huang, Q. 2024. MoGenTS: Motion Generation based on Spatial-Temporal Joint Modeling. arXiv:2409.17686. Zhang, J.; Li, X.; Xu, S.; Yu, L.; Xu, J.; and Dai, Q. 2023a. MotionGPT: Human Motion as Foreign Language. arXiv preprint arXiv:2306.10995. Zhang, J.; Zhang, Y.; Cun, X.; Huang, S.; Zhang, Y.; Zhao, H.; Lu, H.; and Shen, X. 2023b. T2m-gpt: Generating human motion from textual descriptions with discrete representations. arXiv:2301.06052. Zhang, M.; Cai, Z.; Pan, L.; Hong, F.; Guo, X.; Yang, L.; and Liu, Z. 2024a. MotionDiffuse: Text-Driven Human Motion IEEE Transactions on Generation with Diffusion Model. Pattern Analysis and Machine Intelligence (TPAMI). Zhang, M.; Guo, X.; Pan, L.; Cai, Z.; Hong, F.; Li, H.; Yang, L.; and Liu, Z. 2023c. ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model. arXiv:2304.01116. Zhang, M.; Guo, X.; Pan, L.; Cai, Z.; Hong, F.; Li, H.; Yang, L.; and Liu, Z. 2023d. ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model. In IEEE International Conference on Computer Vision (ICCV), 110. zhang, M.; Li, H.; Cai, Z.; Ren, J.; Yang, L.; and Liu, Z. 2023. FineMoGen: Fine-Grained Spatio-Temporal Motion Generation and Editing. In NIPS. Zhang, Z.; Gao, H.; Liu, A.; Chen, Q.; Chen, F.; Wang, Y.; Li, D.; Zhao, R.; Li, Z.; Zhou, Z.; et al. 2024b. Kmm: Key User Study To comprehensively evaluate the generation capability of ReMoMask, we conducted comparative user study. We randomly selected 20 text prompts from the HumanML3D test set and generated motion sequences using ReMoMask, current state-of-the-art retrieval-augmented method (ReMoDiffuse), generative model (MoMask), and ground truth motions. We employ forced-choice paradigm in our user study, asking participants two key questions: Which of the two motions is more realistic? and Which of the two motions corresponds better to the text prompt?. The study is conducted via Google Forms interface, as illustrated in Figure 7. To ensure fairness and reduce potential bias, the names of the generative models are hidden, and the order of presentation is randomized for each question. In total, over 50 participants took part in the evaluation. Empirical results, depicted in Figure 3 and Figure 4, underscore ReMoMasks strong capability to generate motions that are not only realistic but also closely aligned with textual descriptions. Specifically, as shown in Figure 3, ReMoMask achieves 42% preference rate over ground truth (GT) in terms of realism. Although GT motions are derived from real human data, this result indicates that ReMoMask is perceived as comparably realistic by human evaluators. Moreover, the model significantly outperforms both baselines: it achieves 67% preference over MoMask and 75% over ReMoDiffuse, demonstrating its strength in producing highquality, lifelike motion sequences. In terms of text correspondence (reported in Figure 4), ReMoMask attains 47% preference rate over GT, suggesting that its generated motions exhibit nearly human-level alignment with text prompts. Compared to the baselines, ReMoMask again shows substantial improvements, with 72% preference over MoMask and 86% over ReMoDiffuse. Figure 3: Motion Quality User Study Limitation and Future Work Current Limitations: BMMs dual queues and SSTAs 2D attention significantly increase the model parameters (238M), hindering real-time deployment. Furthermore, experiments conducted primarily on short sequences (100 Figure 4: Text-Motion Correspondence User Study frames) lack validation for complex motions requiring strong spatiotemporal coherence, such as dance. Part-level retrieval also struggles with abstract textual descriptions (e.g., jumping joyfully) due to reliance on predefined motion partitions. Additionally, generated motions may violate biomechanical constraints (e.g., joint rotation limits) due to the lack of physics-based verification. Proposed Future Work: To address these limitations, we propose: (1) Adopting knowledge distillation or sparse attention mechanisms to reduce model size; (2) Decomposing long motions into sub-actions and applying phased SSTA to enhance temporal consistency; (3) Integrating Large Language Models (LLMs, e.g., GPT-4) to parse abstract texts and dynamically adapt part-level retrieval; (4) Incorporating physical constraint losses during RVQ-VAE decoding to ensure biomechanically valid motions. Visualization Figure 5 demonstrates our models capability in generating diverse human motions. The 16 randomly inferred samples exhibit complex motion patterns such as directional transitions (walks toward the front, turns to the right), rhythmic actions (raises arms three times), and semantically rich behaviors (pretending to be chicken). This showcases our models proficiency in capturing nuanced motion dynamics and temporal transitions. Figure 6 provides comparative analysis against MeGenTS, TMR, and ReMoDiffuse. While baseline models generate basic motions like walking or balancing, our approach consistently produces more natural transitions (e.g., walks forward then turns vs. simple linear motion) and physically plausible sequences (e.g., multi-step jumps forward three times). The visual comparison highlights our models superior handling of motion complexity and behavioral expressiveness. Figure 5: We randomly sample and visualize 16 motions generated by the proposed ReMoMask framework. These examples are conditioned on diverse prompts randomly selected from the HumanML3D (Guo et al. 2022b), providing qualitative evidence of the models ability to synthesize wide range of realistic and semantically coherent motions. Figure 6: Comparison of the proposed ReMoMask with three state-of-the-art methods: MoGenTS (Yuan et al. 2024), TMR (Petrovich, Black, and Varol 2023), and ReMoDiffuse (Zhang et al. 2023d) We visualize motion sequences generated in response to three distinct text prompts. Each row corresponds to specific prompt, and each column represents the output of different method. The results demonstrate that ReMoMask produces more realistic and semantically aligned motions compared to existing approaches. Figure 7: This figure illustrates the User Interface (UI) used in the ReMoMask User Study. Participants are presented with two motion videos, labeled as Motion and Motion B, alongside shared textual prompt. The motion clips are sampled from outputs generated by different models or the ground truth (GT), with model identities anonymized and video order randomized. Participants are asked to answer two evaluative questions: (1) Which of the two motions is more realistic?, assessing the visual plausibility and motion quality; and (2) Which of the two motions corresponds better to the text prompt?, evaluating the semantic alignment between the motion and the given description. This dual-question design enables comprehensive human assessment of both motion realism and text-motion correspondence."
        }
    ],
    "affiliations": [
        "Jiangsu University",
        "Peking University"
    ]
}