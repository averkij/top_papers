{
    "paper_title": "UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale High-Quality Dataset",
    "authors": [
        "Chen Zhao",
        "En Ci",
        "Yunzhe Xu",
        "Tiehan Fan",
        "Shanyan Guan",
        "Yanhao Ge",
        "Jian Yang",
        "Ying Tai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Ultra-high-resolution (UHR) text-to-image (T2I) generation has seen notable progress. However, two key challenges remain : 1) the absence of a large-scale high-quality UHR T2I dataset, and (2) the neglect of tailored training strategies for fine-grained detail synthesis in UHR scenarios. To tackle the first challenge, we introduce \\textbf{UltraHR-100K}, a high-quality dataset of 100K UHR images with rich captions, offering diverse content and strong visual fidelity. Each image exceeds 3K resolution and is rigorously curated based on detail richness, content complexity, and aesthetic quality. To tackle the second challenge, we propose a frequency-aware post-training method that enhances fine-detail generation in T2I diffusion models. Specifically, we design (i) \\textit{Detail-Oriented Timestep Sampling (DOTS)} to focus learning on detail-critical denoising steps, and (ii) \\textit{Soft-Weighting Frequency Regularization (SWFR)}, which leverages Discrete Fourier Transform (DFT) to softly constrain frequency components, encouraging high-frequency detail preservation. Extensive experiments on our proposed UltraHR-eval4K benchmarks demonstrate that our approach significantly improves the fine-grained detail quality and overall fidelity of UHR image generation. The code is available at \\href{https://github.com/NJU-PCALab/UltraHR-100k}{here}."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 1 6 6 0 2 . 0 1 5 2 : r UltraHR-100K: Enhancing UHR Image Synthesis with Large-Scale High-Quality Dataset Chen Zhao1,, En Ci1,*, Yunzhe Xu1,*, Tiehan Fan1, Shanyan Guan2, Yanhao Ge2, Jian Yang1, Ying Tai1, 1 State Key Laboratory of Novel Software Technology, Nanjing University, China 2 vivo Mobile Communication Co., Ltd., China Figure 1: Our UltraHR-100K (left) is large-scale high-quality dataset for ultra-high-resolution (UHR) image synthesis, featuring diverse range of categories. Utilizing this dataset enables the generation of high-fidelity UHR images (right)."
        },
        {
            "title": "Abstract",
            "content": "Ultra-high-resolution (UHR) text-to-image (T2I) generation has seen notable progress. However, two key challenges remain : 1) the absence of large-scale high-quality UHR T2I dataset, and (2) the neglect of tailored training strategies for fine-grained detail synthesis in UHR scenarios. To tackle the first challenge, we introduce UltraHR-100K, high-quality dataset of 100K UHR images with rich captions, offering diverse content and strong visual fidelity. Each image exceeds 3K resolution and is rigorously curated based on detail richness, content complexity, and aesthetic quality. To tackle the second challenge, we propose frequency-aware post-training method that enhances fine-detail generation in T2I diffusion models. Specifically, we design (i) Detail-Oriented Timestep Sampling (DOTS) to focus learning on detail-critical denoising steps, and (ii) Soft-Weighting Frequency Regularization (SWFR), which leverages Discrete Fourier Transform (DFT) to softly constrain frequency components, encouraging high-frequency detail preservation. Extensive experiments on our proposed UltraHR-eval4K benchmarks demonstrate that our approach significantly improves the fine-grained detail quality and overall fidelity of UHR image generation. The code is available at here. Equal Contribution. Correspondence to: Ying Tai. 39th Conference on Neural Information Processing Systems (NeurIPS 2025)."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in text-to-image (T2I) diffusion models have greatly improved image quality and controllability [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]. However, most existing models are constrained to fixed resolutions (typically 10241024), and exhibit noticeable quality degradation and structural artifacts when directly scaled to ultra-high-resolution (UHR) image generation [12, 13, 14, 15, 16, 17, 18, 19]. This limitation poses significant barrier for real-world applications that demand fine-grained detail and high visual fidelity, such as digital art, virtual content creation, and commercial design. Existing solutions to face this challenge can be grouped into two main paradigms: training-free [13, 14, 20, 21, 22, 23, 24, 25, 26] and training-based methods [12, 15, 16, 17, 27]. Training-free methods attempt to generate UHR images by modifying network architectures [20, 22, 23] or by adjusting inference schemes [14, 21]. However, these techniques exhibit excessive smoothing, produce implausible details, and incur prolonged inference timeslimitations that severely hinder their practical deployment [12]. Fundamentally, training-free methods depend on pre-trained T2I models [2, 3, 5, 7] that were not exposed to UHR data during training, and consequently lack the inherent capacity to render the fine-grained, photorealistic details essential that real-world UHR image synthesis requires. Recently, training-based models for UHR image generation have shown promising results [15, 16, 17]. However, they still face two critical challenges: 1) The absence of open-source, large-scale high-quality UHR T2I dataset. High-fidelity UHR image collection is burdensome due to the scarcity of suitable data. Although Aesthetic-4K [16] introduced the first open-source UHR T2I dataset, it remains limited in both scale (approximately 10K images) and quality (the lack of rigorous selection criterion), constraining its generalizability and high-quality generation capabilities in real-world scenarios. Consequently, constructing open-source, large-scale high-quality UHR T2I dataset represents both significant challenge and critical necessity. 2) The neglect of tailored training strategies for UHR fine-grained detail synthesis. Existing models primarily focus on training efficiency to fine-tune pre-trained T2I models [15, 17], overlooking the high-fidelity detail synthesis. Large-scale pre-training equips T2I models with strong semantic planning abilities, but they struggle to synthesize fine-grained details in the UHR setting [2, 3, 5, 7]. Thus, detail-oriented training strategy is essential for achieving high-quality UHR image synthesis. Large-Scale High-Quality Dataset for Tackling Challenge 1: We construct UltraHR-100K, large-scale high-quality UHR T2I dataset consisting of 100K UHR images paired with rich textual descriptions. As illustrated in Figure 1, UltraHR-100K offers the following key advantages: 1) Scale and Diversity: Compared to recent publicly available Aesthetic-4K [16], our UltraHR-100K is approximately 10 larger, featuring 100K images spanning broad spectrum of categories and visual concepts. 2) Higher Quality: All images in UltraHR-100K are rigorously selected from three key dimensions: detail richness, content complexity, and aesthetic quality. Notably, the minimum resolution across the proposed dataset exceeds 3K (average of width and height), ensuring highresolution content, as shown in Figure 3. 3) Fine-Grained Captions: To provide detailed textual annotations for each image, we leverage Gemini 2.0 [28], powerful commercial vision-language model (VLM), to generate high-quality captions. As shown in Figure 4, our captions are significantly more detailed and semantically rich compared to those in the Aesthetic-4K [16]. Figure 2: We perform rigorous selection of UltraHR100K by evaluating all collected images across three key dimensions: detail richness, content complexity, and aesthetic quality. Left: We present representative low-quality (bad case) examples for each dimension along with their corresponding scores, highlighting the necessity of such filtering. Right: In contrast, our UltraHR-100K exhibit superior texture details, semantic complexity, and aesthetic appeal. 2 Figure 3: Left: Resolution distribution of our UltraHR-100K. All images have minimum resolution of 3K, defined as the average of height and width exceeding 3000 pixels. Middle: Image categories across our dataset. The proportion of each category mirrors its distribution in our dataset. Right: Caption length distribution. Compared to the recent Aesthetic-4K[16], our captions are significantly longer, providing richer semantic supervision. Table 1: Overview of our data processing pipeline. The first stage involves large-scale data collection and preliminary filtering to ensure baseline level of visual quality. The second stage performs three parallel filtering procedures. The final high-quality dataset is obtained by taking the intersection of these subsets. We further employ strong VLM (Gemini 2.0) to annotate each image. Pipeline Tool Remark Data collecting Preliminary data filtering Laplacian and Sobel Python Get 400K high-resolution images Obtain subset with basic visual quality Detail richness Content complexity Aesthetic score The final dataset UHR image caption GLCM Shannon entropy LAION aesthetic predictor Get high aesthetic score set SA Obtain the set SG with rich fine-grained details Obtain the set SE with complex and diverse content Intersection Gemini 2.0 Obtain intersection: UltraHR-100K = SA SE SG Obtain long and fine-grained descriptions for the images Detail-Oriented Training Strategy for Tackling Challenge 2: To enhance UHR detail synthesis, we propose frequency-aware post-training method, which consists of detail-oriented timestep sampling (DOTS) and soft-weighting frequency regularization (SWFR). DOTS improves detail synthesis in UHR image generation by directing more training focus to timesteps associated with fine-grained details. Unlike the discrete and block-based decomposition approach used in Diffusion4K [16], which relies on DWT for frequency separation, our SWFR utilizes the continuous spectrum provided by the Discrete Fourier Transform (DFT) to enable more precise frequency control. By applying soft-weighted constraint across frequency bands, SWFR encourages the model to better reconstruct high-frequency details, without compromising low-frequency structural integrity. Through the proposed dataset and training strategy, we can enhance the synthesis capability of existing pre-trained T2I models [2, 3, 15, 7] in UHR image generation, with particular focus on improving fine-grained detail representation. Furthermore, we construct large 4K T2I benchmarks, UltraHR-eval4K (4096 4096), to comprehensively evaluate existing UHR generation models. Extensive experimental results demonstrate the effectiveness of our method."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Text-to-Image Synthesis Text-to-image (T2I) generation [1, 2, 3, 4, 5, 6, 7, 8, 29, 30, 31, 32, 33, 34, 35] has made notable progress owing to the emergence of diffusion-based frameworks [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48], which exhibit impressive ability in synthesizing visually compelling content from textual descriptions. Early methods such as Denoising Diffusion Probabilistic Models (DDPM) [49] and Denoising Diffusion Implicit Models (DDIM) [50] revealed the strength of iterative denoising procedures for producing realistic images. Subsequently, the attention to latent space diffusion [51] brought major breakthrough, significantly lowering training complexity and enhancing scalability [2]. More recently, incorporating transformer [3, 7, 15, 52, 53] into diffusion models has further 3 boosted image generation quality. In this paper, we aim to enhance the generative capability of T2I models in UHR scenarios. 2.2 Ultra-High-Resolution Image Synthesis UHR image generation plays crucial role in practical domains such as industry and entertainment [16, 54, 55]. Due to computational constraints, current advanced latent diffusion models typically operate at maximum resolution of 1024 1024 [2, 3, 6, 7, 8, 56, 57]. However, scaling to 4K resolution significantly increases computational demands, with cost growing quadratically with image size. Several training-free approaches have extended existing latent diffusion models for 4K generation by modifying the inference strategies of diffusion models. [13, 14, 58, 20, 21, 22, 23, 59]. DiffuseHigh [23] enhances the base-resolution generation by upscaling and subsequently re-denoising it, guided by structural information from the DWT. HiFlow [13] adopts cascaded generation paradigm to effectively capture and utilize low-resolution flow characteristics. However, these techniques exhibit excessive smoothing, produce implausible details, and incur prolonged inference time [12]. Pixart-σ [17] takes pioneering step by approaching direct 4K image generation through efficient token compression in DiT. Similarly, Sana [15] introduces cost-effective 4K generation pipeline. Despite these advancements, existing models primarily focus on training efficiency, overlooking the high-fidelity detail synthesis."
        },
        {
            "title": "3 Constructing UltraHR-100K",
            "content": "Dataset Number Height Width PixArt-30k [17] Aesthetic-4K [16] UltraHR-100K Table 2: Dataset statistical comparisons. To face the challenge of the lack of highquality text-image pairs at UHR image generation, we construct large-scale highquality dataset named UltraHR-100K. We begin by collecting approximately 400K highresolution images (with minimum resolution of 38402160) using custom Python crawler built with Scrapy, sourcing images from the web and various high-resolution imaging devices. However, high resolution alone does not guarantee high quality. We pose central question: What constitutes high-quality image for UHR image generation? We argue that beyond resolution, such images should exhibit rich content complexity, fine-grained visual details, and aesthetic appeal. Accordingly, we conduct rigorous filtering process based on three criteriacontent complexity, detail richness, and aesthetic qualityto curate 100K-level T2I dataset that meet these standards. The proposed UltraHR-100K provides reliable foundation for training and evaluating models in high-fidelity UHR image generation. The data processing pipeline is provided in Table 1. Aesthetic-Eval@4096 [16] UltraHR-eval4K 30,000 12,015 104,117 1,801 4,640 5,119 1,615 4,128 3,648 4,912 4,912 195 2,000 6,449 7, Preliminary Data Filtering. High-resolution images scraped from the web often suffer from blur, noise, or lack of texture, which can significantly degrade image quality. To eliminate such artifacts, we apply two-stage low-level quality filter. First, we compute the Laplacian variance to assess image sharpness and discard samples below blur threshold. Second, we apply the Sobel operator to measure edge density, removing overly flat or textureless images. This process yields cleaned subset with sufficient basic visual quality. Detail Richness. Fine-grained details are essential for training generative models to preserve highfrequency content. To quantify the aspect, we compute Gray-Level Co-occurrence Matrix (GLCM) score, including contrast, entropy, and correlation across multiple directions. These metrics capture spatial pixel relationships indicative of texture complexity. We then select the top 50% of images from with the highest aggregated GLCM scores, resulting in subset SG. Content Complexity. Visually complex images and diverse spatial structures are more valuable for guiding generation models to achieve rich content. We use Shannon entropy as proxy to measure the content complexity of each image. Images with higher entropy tend to contain more varied pixel intensities. From subset S, we retain the top 50% highest-entropy images to construct subset SE. 4 Figure 4: Comparison between our UltraHR-100K and Aesthetic-4K[16]. Captions in our UltraHR100K provide more expressive descriptions, encompassing not only global summaries of the image content but also rich details that enhance semantic alignment. Aesthetic Quality. Aesthetic appeal is an important factor in image realism and human preference. To incorporate this dimension, we adopt the LAION Aesthetic Predictor [60], neural network trained to estimate perceptual quality. It outputs scalar score reflecting visual composition, color harmony, and overall appeal. We rank all images in by their aesthetic scores and retain the top 50% to form subset SA, consisting of the most visually pleasing samples. UltraHR-100K. To ensure that the final dataset consists of high-quality UHR images with diverse content, rich textures, and strong aesthetic appeal, we take the intersection of the three selected subsets. Specifically, the final dataset is defined as: UltraHR-100K = SG SE SA (1) This intersection guarantees that each image in UltraHR-100K simultaneously meets high standards in detail richness, content complexity, and aesthetic quality, as shown in Figure 2. In addition, we construct evaluation subset from our datasetUltraHR-eval4Kcontaining 2,000 images. Table 2 compares our UltraHR-100K with Aesthetic-4K [16] and PixArt-30K [17]. These statistics highlight that UltraHR-100K not only improves dataset scale, but also provides more extensive spatial content. UHR Image Caption. UHR images typically contain significantly more visual information than standard-resolution images, making them inherently more semantically complex. However, existing datasets [16, 60] often provide only short captions, limiting the semantic expressiveness of generative models. To address this issue, we leverage Gemini 2.0 [28], state-of-the-art commercial vision-language model (VLM), to generate rich and detailed captions for our dataset. As illustrated in Figures 3 and 4, our captions are not only substantially longer but also encompass both global summaries and fine-grained descriptions, enhancing alignment with complex image content. Figure 5: Relation between weighting ratio and timesteps with beta sampling strategy."
        },
        {
            "title": "4 Frequency-Aware Post-Training",
            "content": "Pretrained T2I models, trained on large-scale datasets, exhibit strong capabilities in semantic and content planning. However, they often struggle to synthesize fine-grained details when extended to UHR scenarios [12, 15]. In this work, we focus on enhancing the detail synthesis ability of pretrained T2I models through tailored post-training strategies. To this end, we propose frequency-aware post-training method (FAPT). Specifically, FAPT consists of two parts: detail-oriented timestep sampling (DOTS) and soft-weighting frequency regularization (SWFR). DOTS improves detail synthesis in UHR image generation by directing more training focus to timesteps associated with fine-grained details. Meanwhile, SWFR imposes soft-weighted constraint across the frequency spectrum, guiding the model to better preserve and reconstruct high-frequency details. 4.1 Detail-Oriented Timestep Sampling Motivation. Existing study [61] have validated the observation that the overall image structure (low-frequency signals) is largely reconstructed in the early denoising steps, while fine-grained details (high-frequency signals) are progressively synthesized in the later stages of the denoising process. This insight motivates us to design sampling strategy that emphasizes the later stages of the denoising process, aiming to enhance the learning of fine-grained details during post-training stage. DOTS. To achieve this target, we adopt beta sampling strategy, which provides simple yet flexible mechanism to bias the sampling distribution over denoising timesteps, as shown in Figure 5. Specifically, we first draw timestep (0, 1) from Beta distribution parameterized by shape parameters α and β: Beta(α, β). (2) The Beta distribution yields rich family of unimodal or skewed distributions over the interval (0, 1), and its probability density function is given by: πbeta(t; α, β) = 1 B(α, β) tα1(1 t)β1, (3) where B(α, β) = Γ(α)Γ(β) Γ(α+β) is the Beta function. By adjusting α and β, we can control the bias of the sampling distribution. This sampling mechanism naturally supports adaptive emphasis in training: by emphasizing later denoising timesteps, we can guide the model to focus on high-frequency details. 4.2 Soft-Weighting Frequency Regularization Motivation. Large pre-trained T2I models [2, 3, 5, 7] demonstrate strong semantic planning from diverse data exposure but struggle with fine-grained detail synthesis in UHR scenarios. Existing UHR T2I models focus mainly on training efficiency [15, 17], often neglecting high-fidelity detail. Diffusion4K [16] introduces DWT-based frequency decomposition to enable 4K training, yet DWT yields coarse and discontinuous frequency separation, limiting its effectiveness for UHR modeling. To overcome this, we adopt DFT-based decomposition, which provides finer, globally coherent frequency representations better suited for capturing fine-scale structures in high-resolution synthesis. SWFR. To enhance fine-scale fidelity in UHR image synthesis, we introduce soft-weighting frequency regularization that complements the standard diffusion loss by explicitly supervising frequency consistency, with an emphasis on high-frequency components. Formally, consider the standard diffusion process: zt = αt x0 + σt ϵ, (4) where x0 denotes the data distribution, ϵ is sampled from standard normal distribution, and αt, σt are known coefficients in the diffusion formulation. Recent T2I models [5, 7, 15] adopt rectified flows to predict velocity v, with the objective as follows: vΘ(zt, t) = ϵ x0. To regularize the model in the frequency domain, we compute the 2D Discrete Fourier Transforms (DFT) of both prediction and target y: (5) ˆx = F(x), ˆy = F(y), (6) 6 Figure 6: Qualitative comparisons with SOTA methods on our UltraHR-eval4K (4096 4096). Compared with previous works, our method is capable of generating visually complex images with rich semantic content. More visual examples are available in the supplementary materials. where F() denotes the DFT. Let and denote the model prediction (e.g., = vΘ(zt, t)) and target (e.g., = ϵ x0), respectively. We define frequency-domain regularization term as: Lfreq = (cid:104) w(r) ˆx w(r) ˆy2(cid:105) , (7) where w(r) is frequency soft weighting function designed to boost high-frequency supervision: w(r) = 1 + λ exp(γr) 1 exp(γ) 1 , [0, 1], (8) and is the normalized distance from the center of the frequency plane. Hyperparameters λ and γ control the strength and steepness of high-frequency emphasis, respectively. Finally, the overall training objective is defined as: Ltotal = Ldiff + λfreq Lfreq, where Ldiff denotes the diffusion loss, which can be instantiated as velocity prediction loss (vΘ(zt, t) (ϵ x0)2). λfreq is balancing coefficient that controls the strength of frequencydomain supervision. This regularization Lfreq encourages the model to maintain consistent spectral power between prediction and target, especially in high-frequency bands. 5 Experiments (9) 5.1 Implementation Details Overall Training Setting. We adopt two-stage training strategy. In the first stage, we follow the Logit-Normal Sampling scheme introduced in SD3 [6] and perform fine-tuning on our UltraHR100K dataset, aiming to enhance the semantic planning capability in UHR generation. In the second stage, we apply our proposed frequency-aware post-training method, which focuses on high-frequency learning to further improve the fine-grained details. We use the CAMEWrapper [15] optimizer with constant learning rate of 1e-4, and employ mixed-precision training with batch size of 24. The first-stage training is conducted for 4K iterations, followed by 8K iterations in the second stage. Due to computational constraints, we conduct training solely on SANA, and all experiments are performed on four H20 GPUs. Baselines. To comprehensively evaluate our approach, we conduct extensive comparisons against SOTA methods for UHR image generation, which can be broadly categorized into three groups. The first group consists of powerful T2I models combined with super-resolution technique, BSRGAN [62]. The second group includes training-free approaches, where we evaluate FLUX[7]) using corresponding training-free generation methods, I-Max [63] and HiFlow [13]. Lastly, we compare with leading 7 Figure 7: Qualitative comparisons with SOTA methods on our UltraHR-eval4K (4096 4096). Compared with previous works, our method can generate realistic textures and fine-grained details. Figure 8: More visual comparisons demonstrate that our method consistently produces high-quality results. Additional and more diverse comparisons can be found in the supplementary material. training-based UHR generation models, including Pixart-σ [17], SANA [15], and Diffusion4K [16]. All baselines are evaluated under their official settings to ensure fair and consistent comparison. Evaluation. We employ several metrics to assess the quality of the generated images, with particular focus on our evaluation sets, UltraHR-eval4K. To evaluate image-text consistency, we calculate the long CLIP score [64] and Fine-Grained (FG) CLIP score [65]. Additionally, the Fréchet Inception Distance (FID) [66] and Inception Score (IS) [67] are computed to evaluate the overall image quality of the generated images. Following previous works [13, 12], we compute the FID-patch and IS-patch to evaluate the local quality and details of the images, which are based on local image patches. These metrics provide comprehensive evaluation of the overall quality, detail retention in the generated images. Table 3: Quantitative comparison with other baselines on our UltraHR-eval4K (4096 4096) benchmark. The best result is highlighted in bold. Method FLUX [7] + BSRGAN [62] SD3.5 [6] + BSRGAN [62] I-Max(FLUX) [63] HiFlow(FLUX) [13] Pixart-σ [17] SANA [15] Diffusion4K [16] Ours(UltraHR-100K) FID 37.651 31.870 37.667 35.892 33.171 37.070 39.857 33.995 Ours(UltraHR-100K+FAPT) 31.748 FIDpatch 43.143 25.598 37.835 38.327 32.198 38.795 38.515 20.932 15. IS ISpatch CLIP FG-CLIP 11.773 12.780 11.991 11.767 12.212 11.778 10.832 12. 12.995 5.389 5.456 4.391 4.620 5.390 5.649 3.235 5.020 5. 31.45 31.75 31.49 31.52 31.78 31.70 31.41 31.85 31.82 28.02 28. 27.78 27.75 28.65 28.60 26.48 28.65 28.68 Table 4: Left: User study results conducted on our UltraHR-eval4K. Right: Quantitative comparison on Aesthetic-Eval@4096. The results demonstrate the superior performance of our method. Method Overall Quality Quality Alignment Detail Text-Image Preference Pixart-σ [17] SANA [15] Diffusion4K [16] Ours 14% 4% 12% 70% 10% 8% 4% 78% 16% 8% 6% 72% 18% 6% 6% 70% 5.2 Comparison to State-of-the-Art Methods Method Pixart-σ [17] SANA [15] Diffusion4K [16] FID 150.593 146.027 152. FIDpatch CLIP FG-CLIP 34.88 34.62 33.99 44.702 37.031 39.729 28.48 28.61 26.06 Ours 142.965 24. 35.08 28.64 Quantitative Comparison. Table 3 summarizes the quantitative performance on our UltraHReval4K benchmark (4096 4096). Our method consistently achieves superior scores on key perceptual metrics such as FID, FID-patch and IS, indicating its strong capability in generating high-quality images with fine-grained textures. Moreover, our method achieves competitive CLIP scores, reflecting its ability to maintain semantic alignment with the input prompt. Notably, our method yields substantial improvement in FIDpatch, highlighting its effectiveness in synthesizing fine-grained details. This result demonstrates that our proposed approach significantly enhances the detail generation capability of pre-trained T2I models in UHR scenarios. Qualitative Comparison. Figure 6 presents qualitative comparisons on UltraHR-eval4K (4096 4096), focusing on the overall semantic richness and spatial layout of the generated images. While existing SOTA methods struggle to produce coherent and content-rich scenes at such ultra-high resolution, our method demonstrates strong capability in generating visually complex images with diverse and semantically meaningful elements. This highlights our models superior capacity for global spatial reasoning and semantic planning in large-scale synthesis. In Figure 7, we further compare fine-grained textures and local details. Our method produces sharper structures and more realistic textures, faithfully preserving high-frequency information that other methods tend to miss or oversmooth. These results collectively demonstrate the effectiveness of our proposes dataset and method in enhancing both the global semantics and local fidelity for ultra-high-resolution text-to-image generation. Figure 8 presents more visual comparisons. User Study. As shown in Table 4, we conducted user study with 5 volunteers evaluating 50 randomly selected cases. Images were rated on overall quality, detail quality, text-image alignment and preference. The results demonstrate the superiority of our method across all aspects. Comparisons on Public Benchmark. We conduct quantitative comparison on the publicly available Aesthetic-4K benchmark, specifically the Aesthetic-Eval@4096 subset, as reported in Table 4. This evaluation set contains 195 image-text pairs, where all images have short side greater than 4096 pixels. Due to the limited number of samples, the reported FID scores are relatively high. Nonetheless, the results clearly demonstrate the superior performance of our method, supporting its robustness and generalizability beyond our proposed benchmark. 9 Table 5: Ablation study of our key components and data scale. Model is baseline using full fine-tuning on our dataset. The comparison between (trained on partial dataset) and (full dataset) validates the effectiveness of large-scale data. Model DOTS SWFR Dataset LoRA D Full Full Full Part Full FID FIDpatch CLIP 31.80 35.02 35.07 31.85 20.93 33.99 31.79 19.95 32.57 32.75 31.74 18. 15.79 31.81 31.82 5.3 Ablation Study We conduct comprehensive ablation study to validate the effectiveness of our proposed training strategy and the importance of large-scale data. As shown in Table 5, Model serves as the baseline without our proposed DOTS and SWFR. Model introduces DOTS, resulting in consistent improvements in both FID and patch-level FID, demonstrating its effectiveness in guiding the sampling process. Further incorporating SWFR (Model D) yields substantial improvements, particularly in patch-level FID, confirming that our proposed regularization enhances the detail synthesis capability of T2I models. To evaluate the impact of training data scale, we compare Model and Model D. Model is trained with randomly sampled 15K subset of our UltraHR-100K using the same training strategy. The performance drop compared to Model clearly highlights the importance of large-scale UHR data in achieving high-fidelity and semantically aligned image generation. Table 6: Analysis for DOTS. Analysis for DOTS. The DOTS module employs Beta(α, β) distribution to guide timestep sampling, where α and β control the bias along the denoising trajectory. When α < β, sampling favors later steps (near = 0) that refine highfrequency details; when α > β, it leans toward early steps (near = 1) emphasizing global structure. In our experiments, we set α = 2, β = 4, biasing sampling toward later steps to better capture fine details crucial for ultra-highresolution generation. An ablation study  (Table 6)  varying α and β confirms this choice: larger α weakens detail learning, smaller α harms semantic consistency, and overly concentrated or flattened distributions reduce diversity. These results validate (α = 2, β = 4) as balanced and effective configuration. (α = 3, β = 4) (α = 1, β = 4) (α = 2, β = 5) (α = 2, β = 3) 22.143 25.095 23.850 24.638 33.196 33.727 33.874 33.638 31.83 31.79 31.82 31.84 FID_patch CLIP (α = 2, β = 4) Method 15.795 31.748 31.82 FID"
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we present UltraHR-100K, curated dataset of 100K UHR images with rich textual annotations. Each image is carefully selected to ensure high levels of detail, visual complexity, and aesthetic appeal. Moreover, we introduce frequency-aware post-training method, which includes: (i) Detail-Oriented Timestep Sampling (DOTS), and (ii) Soft-Weighting Frequency Regularization (SWFR). Experiments on our proposed UltraHR-eval4K benchmark confirm that our approach significantly boosts both the visual fidelity and fine-detail accuracy of UHR image synthesis. Limitations and future works. Our main limitations lie in two aspects. First, while the proposed frequency-aware post-training strategy effectively enhances fine-detail synthesis, it introduces slight degradation in textimage alignment, as shown in Table 5. Second, our dataset currently contains relatively limited amount of portrait data, which constrains the improvement in ultra-high-resolution (UHR) portrait generation, as illustrated in Figure 8. In future work, we plan to develop more balanced training strategies to alleviate the alignment issue and expand our dataset with additional high-quality UHR portrait images to further improve performance in portrait synthesis. Acknowledgments. This work was supported by Natural Science Foundation of China: No. 62406135, Natural Science Foundation of Jiangsu Province: BK20241198, and Gusu Innovation and Entrepreneur Leading Talents: No. ZXL2024362."
        },
        {
            "title": "References",
            "content": "[1] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. [2] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [3] Junsong Chen, YU Jincheng, GE Chongjian, Lewei Yao, Enze Xie, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In The Twelfth International Conference on Learning Representations. [4] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in neural information processing systems, 34:1982219835, 2021. [5] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [6] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [7] Black-Forest Labs. Flux. https://huggingface.co/black-forest-labs/FLUX.1-dev, 2024. [8] Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Chase Lambert, Joao Souza, Suhail Doshi, and Daiqing Li. Playground v3: Improving text-to-image alignment with deep-fusion large language models. arXiv preprint arXiv:2409.10695, 2024. [9] Leyang Li, Shilin Lu, Yan Ren, and Adams Wai-Kin Kong. Set you straight: Auto-steering denoising trajectories to sidestep unwanted concepts. arXiv preprint arXiv:2504.12782, 2025. [10] Daiheng Gao, Shilin Lu, Wenbo Zhou, Jiaming Chu, Jie Zhang, Mengxi Jia, Bang Zhang, Zhaoxin Fan, and Weiming Zhang. Eraseanything: Enabling concept erasure in rectified flow transformers. In Forty-second International Conference on Machine Learning, 2025. [11] Xiantao Hu, Ying Tai, Xu Zhao, Chen Zhao, Zhenyu Zhang, Jun Li, Bineng Zhong, and Jian Yang. Exploiting multimodal spatial-temporal patterns for video object tracking. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 35813589, 2025. [12] Jingjing Ren, Wenbo Li, Haoyu Chen, Renjing Pei, Bin Shao, Yong Guo, Long Peng, Fenglong Song, and Lei Zhu. Ultrapixel: Advancing ultra high-resolution image synthesis to new peaks. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. [13] Jiazi Bu, Pengyang Ling, Yujie Zhou, Pan Zhang, Tong Wu, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Dahua Lin, and Jiaqi Wang. Hiflow: Training-free high-resolution image generation with flow-aligned guidance. arXiv preprint arXiv:2504.06232, 2025. [14] Ruoyi Du, Dongliang Chang, Timothy Hospedales, Yi-Zhe Song, and Zhanyu Ma. Demofusion: Democratising high-resolution image generation with no $$$. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 61596168, 2024. [15] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, et al. Sana: Efficient high-resolution image synthesis with linear diffusion transformers. arXiv preprint arXiv:2410.10629, 2024. 11 [16] Jinjin Zhang, Qiuyu Huang, Junjie Liu, Xiefan Guo, and Di Huang. Diffusion-4k: Ultra-highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [17] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In European Conference on Computer Vision, pages 7491. Springer, 2024. [18] Chen Zhao, Zhizhou Chen, Yunzhe Xu, Enxuan Gu, Jian Li, Zili Yi, Qian Wang, Jian Yang, and Ying Tai. From zero to detail: Deconstructing ultra-high-definition image restoration from progressive spectral perspective. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1793517946, 2025. [19] Chen Zhao, Weiling Cai, Chengwei Hu, and Zheng Yuan. Cycle contrastive adversarial learning with structural consistency for unsupervised high-quality image deraining transformer. Neural Networks, 178:106428, 2024. [20] Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun, Menghan Xia, Yong Zhang, Xintao Wang, Ran He, Qifeng Chen, and Ying Shan. Scalecrafter: Tuning-free higher-resolution visual generation with diffusion models. In The Twelfth International Conference on Learning Representations, 2023. [21] Linjiang Huang, Rongyao Fang, Aiping Zhang, Guanglu Song, Si Liu, Yu Liu, and Hongsheng Li. Fouriscale: frequency perspective on training-free high-resolution image synthesis. In European Conference on Computer Vision, pages 196212. Springer, 2024. [22] Zhiyu Jin, Xuli Shen, Bin Li, and Xiangyang Xue. Training-free diffusion model adaptation for variable-sized text-to-image synthesis. Advances in Neural Information Processing Systems, 36:7084770860, 2023. [23] Younghyun Kim, Geunmin Hwang, Junyu Zhang, and Eunbyung Park. Diffusehigh: Trainingfree progressive high-resolution image synthesis through structure guidance. In Proceedings of the AAAI conference on artificial intelligence, volume 39, pages 43384346, 2025. [24] Zhengqiang Zhang, Ruihuang Li, and Lei Zhang. Frecas: Efficient higher-resolution image generation via frequency-aware cascaded sampling. The Thirteenth International Conference on Learning Representations, 2025. [25] Zihan Zhou, Shilin Lu, Shuli Leng, Shaocong Zhang, Zhuming Lian, Xinlei Yu, and Adams Wai-Kin Kong. Dragflow: Unleashing dit priors with region based supervision for drag editing. arXiv preprint arXiv:2510.02253, 2025. [26] Zhennan Chen, Yajie Li, Haofan Wang, Zhibo Chen, Zhengkai Jiang, Jun Li, Qian Wang, Jian Yang, and Ying Tai. Region-aware text-to-image generation via hard binding and soft refinement. arXiv preprint arXiv:2411.06558, 2024. [27] Nikai Du, Zhennan Chen, Shan Gao, Zhizhou Chen, Xi Chen, Zhengkai Jiang, Jian Yang, and Ying Tai. Textcrafter: Accurately rendering multiple texts in complex visual scenes. arXiv preprint arXiv:2503.23461, 2025. [28] Google. Gemini. https://gemini.google.com/, 2025. [29] Dewei Zhou, You Li, Fan Ma, Xiaoting Zhang, and Yi Yang. Migc: Multi-instance generation controller for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 68186828, 2024. [30] Li Zhang, Yan Zhong, Jianan Wang, Zhe Min, Liu Liu, et al. Rethinking 3d convolution in ℓp-norm space. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [31] Dewei Zhou, You Li, Fan Ma, Zongxin Yang, and Yi Yang. Migc++: Advanced multi-instance generation controller for image synthesis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 12 [32] Li Zhang, Zean Han, Yan Zhong, Qiaojun Yu, Xingyu Wu, et al. Vocapter: Voting-based pose tracking for category-level articulated object via inter-frame priors. In ACM Multimedia 2024, 2024. [33] Shilin Lu, Yanzhu Liu, and Adams Wai-Kin Kong. Tf-icon: Diffusion-based training-free cross-domain image composition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22942305, 2023. [34] Ji Du, Jiesheng Wu, Desheng Kong, Weiyun Liang, Fangwei Hao, Jing Xu, Bin Wang, Guiling Wang, and Ping Li. Upgen: Unleashing potential of foundation models for training-free camouflage detection via generative models. IEEE Transactions on Image Processing, 2025. [35] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. Openvid-1m: large-scale high-quality dataset for text-to-video generation. arXiv preprint arXiv:2407.02371, 2024. [36] Chenyang Si, Ziqi Huang, Yuming Jiang, and Ziwei Liu. Freeu: Free lunch in diffusion u-net. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 47334743, 2024. [37] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2. 5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint arXiv:2402.17245, 2024. [38] Shilin Lu, Zilan Wang, Leyang Li, Yanzhu Liu, and Adams Wai-Kin Kong. Mace: Mass concept erasure in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 64306440, 2024. [39] Dewei Zhou, Ji Xie, Zongxin Yang, and Yi Yang. 3dis: Depth-driven decoupled instance synthesis for text-to-image generation. arXiv preprint arXiv:2410.12669, 2024. [40] Chen Zhao, Weiling Cai, Chenyu Dong, and Chengwei Hu. Wavelet-based fourier information interaction with frequency diffusion adjustment for underwater image restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82818291, 2024. [41] Chen Zhao, Chenyu Dong, and Weiling Cai. Learning physical-aware diffusion model based on transformer for underwater image enhancement. arXiv preprint arXiv:2403.01497, 2024. [42] Chen Zhao, Weiling Cai, Chenyu Dong, and Ziqi Zeng. Toward sufficient spatial-frequency interaction for gradient-aware underwater image enhancement. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3220 3224. IEEE, 2024. [43] Zhennan Chen, Rongrong Gao, Tian-Zhu Xiang, and Fan Lin. Diffusion model for camouflaged object detection. arXiv preprint arXiv:2308.00303, 2023. [44] Dewei Zhou, Ji Xie, Zongxin Yang, and Yi Yang. 3dis-flux: simple and efficient multi-instance generation with dit rendering. CoRR, abs/2501.05131, 2025. [45] Zhiqiu Lin, Siyuan Cen, Daniel Jiang, Jay Karhade, Hewei Wang, Chancharik Mitra, Tiffany Ling, Yuhan Huang, Sifan Liu, Mingyu Chen, Rushikesh Zawar, Xue Bai, Yilun Du, Chuang Gan, and Deva Ramanan. Towards understanding camera motions in any video. CoRR, abs/2504.15376, 2025. [46] Rui Xie, Yinhong Liu, Penghao Zhou, Chen Zhao, Jun Zhou, Kai Zhang, Zhenyu Zhang, Jian Yang, Zhenheng Yang, and Ying Tai. Star: Spatial-temporal augmentation with text-to-video models for real-world video super-resolution. arXiv preprint arXiv:2501.02976, 2025. [47] Ji Du, Fangwei Hao, Mingyang Yu, Desheng Kong, Jiesheng Wu, Bin Wang, Jing Xu, and Ping Li. Shift the lens: Environment-aware unsupervised camouflaged object detection. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1927119282, 2025. 13 [48] Chen Zhao, Wei-Ling Cai, Zheng Yuan, and Cheng-Wei Hu. Multi-cropping contrastive learning and domain consistency for unsupervised image-to-image translation. IET Image Processing, 19(1):e70006, 2025. [49] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [50] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. [51] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [52] Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Lirui Zhao, Fu-Yun Wang, Zhanyu Ma, et al. Lumina-next: Making lumina-t2x stronger and faster with next-dit. arXiv preprint arXiv:2406.18583, 2024. [53] Dewei Zhou, Zongxin Yang, and Yi Yang. Pyramid diffusion models for low-light image enhancement. arXiv preprint arXiv:2305.10028, 2023. [54] Lucy Chai, Michael Gharbi, Eli Shechtman, Phillip Isola, and Richard Zhang. Any-resolution training for high-resolution image synthesis. In European conference on computer vision, pages 170188. Springer, 2022. [55] Li Zhang, Weiqing Meng, Yan Zhong, Bin Kong, Mingliang Xu, Jianming Du, Xue Wang, Rujing Wang, and Liu Liu. U-cope: Taking further step to universal 9d category-level object pose estimation. In European Conference on Computer Vision, pages 254270. Springer, 2025. [56] Dewei Zhou, Mingwei Li, Zongxin Yang, and Yi Yang. Dreamrenderer: Taming multi-instance attribute control in large-scale text-to-image models. arXiv preprint arXiv:2503.12885, 2025. [57] Shilin Lu, Zihan Zhou, Jiayou Lu, Yuanzhi Zhu, and Adams Wai-Kin Kong. Robust watermarking using generative priors against image editing: From benchmarking to advances. arXiv preprint arXiv:2410.18775, 2024. [58] Li Zhang, Mingliang Xu, Dong Li, Jianming Du, and Rujing Wang. Catmullrom splines-based regression for image forgery localization. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 71967204, 2024. [59] Shilin Lu, Zhuming Lian, Zihan Zhou, Shaocong Zhang, Chen Zhao, and Adams Wai-Kin Kong. Does flux already know how to perform physically plausible image composition? arXiv preprint arXiv:2509.21278, 2025. [60] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. [61] Mingyang Yi, Aoxue Li, Yi Xin, and Zhenguo Li. Towards understanding the working mechanism of text-to-image diffusion model. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. [62] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. Designing practical degradation model for deep blind image super-resolution. In Proceedings of the IEEE/CVF international conference on computer vision, pages 47914800, 2021. [63] Ruoyi Du, Dongyang Liu, Le Zhuo, Qin Qi, Hongsheng Li, Zhanyu Ma, and Peng Gao. I-max: Maximize the resolution potential of pre-trained rectified flow transformers with projected flow. arXiv preprint arXiv:2410.07536, 2024. [64] Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, and Jiaqi Wang. Long-clip: Unlocking the long-text capability of clip. In European Conference on Computer Vision, pages 310325. Springer, 2024. 14 [65] Chunyu Xie, Bin Wang, Fanjing Kong, Jincheng Li, Dawei Liang, Gengshen Zhang, Dawei Leng, and Yuhui Yin. Fg-clip: Fine-grained visual and textual alignment. arXiv preprint arXiv:2505.05071, 2025. [66] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [67] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016."
        }
    ],
    "affiliations": [
        "State Key Laboratory of Novel Software Technology, Nanjing University, China",
        "vivo Mobile Communication Co., Ltd., China"
    ]
}