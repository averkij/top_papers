{
    "paper_title": "Are You Getting What You Pay For? Auditing Model Substitution in LLM APIs",
    "authors": [
        "Will Cai",
        "Tianneng Shi",
        "Xuandong Zhao",
        "Dawn Song"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The proliferation of Large Language Models (LLMs) accessed via black-box APIs introduces a significant trust challenge: users pay for services based on advertised model capabilities (e.g., size, performance), but providers may covertly substitute the specified model with a cheaper, lower-quality alternative to reduce operational costs. This lack of transparency undermines fairness, erodes trust, and complicates reliable benchmarking. Detecting such substitutions is difficult due to the black-box nature, typically limiting interaction to input-output queries. This paper formalizes the problem of model substitution detection in LLM APIs. We systematically evaluate existing verification techniques, including output-based statistical tests, benchmark evaluations, and log probability analysis, under various realistic attack scenarios like model quantization, randomized substitution, and benchmark evasion. Our findings reveal the limitations of methods relying solely on text outputs, especially against subtle or adaptive attacks. While log probability analysis offers stronger guarantees when available, its accessibility is often limited. We conclude by discussing the potential of hardware-based solutions like Trusted Execution Environments (TEEs) as a pathway towards provable model integrity, highlighting the trade-offs between security, performance, and provider adoption. Code is available at https://github.com/sunblaze-ucb/llm-api-audit"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 5 1 7 4 0 . 4 0 5 2 : r Preprint. Are You Getting What You Pay For? Auditing Model Substitution in LLM APIs Will Cai, Tianneng Shi, Xuandong Zhao, Dawn Song University of California, Berkeley {wicai,stneng,xuandongzhao,dawnsong}@berkeley.edu"
        },
        {
            "title": "Abstract",
            "content": "The proliferation of Large Language Models (LLMs) accessed via black-box APIs introduces significant trust challenge: users pay for services based on advertised model capabilities (e.g., size, performance), but providers may covertly substitute the specified model with cheaper, lower-quality alternative to reduce operational costs. This lack of transparency undermines fairness, erodes trust, and complicates reliable benchmarking. Detecting such substitutions is difficult due to the black-box nature, typically limiting interaction to input-output queries. This paper formalizes the problem of model substitution detection in LLM APIs. We systematically evaluate existing verification techniques, including output-based statistical tests, benchmark evaluations, and log probability analysis, under various realistic attack scenarios like model quantization, randomized substitution, and benchmark evasion. Our findings reveal the limitations of methods relying solely on text outputs, especially against subtle or adaptive attacks. While log probability analysis offers stronger guarantees when available, its accessibility is often limited. We conclude by discussing the potential of hardware-based solutions like Trusted Execution Environments (TEEs) as pathway towards provable model integrity, highlighting the trade-offs between security, performance, and provider adoption. Code is available at https://github.com/sunblaze-ucb/llm-api-audit"
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities, leading to their widespread adoption through cloud-based APIs (OpenAI, 2022; Anthropic, 2023; Google, 2023; TogetherAI, 2023). Users typically select and pay for services based on advertised model specifications, such as parameter count (e.g., Llama 405B vs 70B), training data, or benchmark performance (Touvron et al., 2023; Bai et al., 2023; Mistral, 2023). However, the operational complexity and substantial computational cost associated with hosting state-of-the-art LLMs create incentives for service providers to optimize resource usage. This optimization can sometimes manifest as model substitution, where the provider covertly uses different model than the one advertised and billed to the user. Such substitutions can occur for various reasons. provider might replace large, expensive model (e.g., Llama-3.1-405B) with smaller, cheaper one (e.g., Llama-3.1-70B) or heavily quantized version (e.g., INT8 or FP8) to reduce GPU memory and compute costs, thereby increasing profit margins (Gao et al., 2024; Sun et al., 2024b). Alternatively, substitutions might happen for operational reasons like load balancing; if the requested model variant is under heavy load, requests might be rerouted to different, less loaded variant (e.g., version fine-tuned on different data). While potentially benign in intent, undisclosed substitutions break the implicit contract with the user, compromise the reliability of services built upon these APIs, hinder reproducible research, and invalidate benchmarks performed through third-party APIs. Furthermore, subtle changes from quantization or fine-tuning might inadvertently affect model safety or introduce biases. Imagine researcher relying on specific models advertised capabilities for scientific analysis, only to unknowingly receive results from substituted, less capable model, potentially invalidating their findings. 1 Preprint. Detecting model substitution is challenging in black-box setting. Users typically only have query access, receiving text outputs and sometimes limited metadata like token log probabilities (Carlini et al., 2024). They lack direct access to model weights or the underlying inference infrastructure. This information asymmetry makes it difficult to verify the providers claims. Dishonest providers might further employ active countermeasures, such as detecting and evading benchmark queries or randomly mixing outputs from the correct and substituted models to obscure the signal from auditing attempts. This paper addresses the critical problem of auditing model substitution in black-box LLM APIs. We formalize the verification task and systematically investigate the efficacy and robustness of various detection methodologies under realistic adversarial scenarios. Our contributions are: We formalize the problem of LLM model substitution detection from the perspective of black-box user/auditor. We design and analyze several practical adversarial attack scenarios relevant to model substitution, including quantization, randomized substitution, benchmark evasion, and limiting information disclosure. We empirically evaluate the effectiveness of existing and potential detection techniques (text classification, identity prompting, model equality testing, benchmark analysis, log probability verification, embedding subspace analysis) against these attacks. We analyze the robustness of these methods, showing that output-based statistical tests like Maximum Mean Discrepancy (MMD) are resilient to certain attacks but vulnerable to others, while log probability analysis offers stronger guarantees when available. We discuss the potential of hardware-based solutions like Trusted Execution Environments (TEEs) as path towards provable integrity, albeit requiring provider adoption."
        },
        {
            "title": "2 Related work",
            "content": "LLM API auditing and monitoring. Several studies have monitored the behavior and performance of commercial LLM APIs over time. Chen et al. (2023) tracked changes in ChatGPTs capabilities, highlighting behavioral drift. Eyuboglu et al. (2024) characterized updates to API-accessed ML models. Gao et al. (2024) specifically tested API output text distributions against reference distributions using Maximum Mean Discrepancy (MMD). We extend this by evaluating MMD and other methods against adversarial substitution attacks. Complementary work by Sam et al. (2025) predicts black-box LLM performance using self-queries, aligning with our goal of verifying model identity. Detecting LLM-generated text. Detection of LLM-generated text has been extensively studied, including post-hoc methods and proactive watermarking techniques (Yang et al., 2023; Ghosal et al., 2023). Zero-shot detection approaches use stylistic features or modelspecific fingerprints to distinguish AI-generated content without specialized training data (Mitchell et al., 2023; Bao et al., 2023; Yang et al., 2024). Trained classifiers (Hu et al., 2023; Sun et al., 2025) utilize datasets from various sources to differentiate model outputs. Although effective against stylistically distinct models, these approaches may struggle with subtle substitutions within the same model family. LLM watermarking techniques embed hidden signals in outputs to trace content ownership (Kirchenbauer et al., 2023; Zhao et al., 2024a;b), but they are provider-centric and not intended for end-user verification of model identity, particularly as the same watermark might span different backend models. Verifiable computation for ML. Cryptographic techniques like Zero-Knowledge Proofs (ZKPs) (Feng et al., 2021; Sun et al., 2024a; Xie et al., 2025) and hardware-based Trusted Execution Environments (TEEs) (NVIDIA, 2023) offer provable verification of ML inference. ZKPs allow proof of correct inference without revealing inputs or weights but face substantial overhead for large LLMs (Sun et al., 2024a). TEEs provide hardware-level guarantees with lower performance costs but depend heavily on specific infrastructure (NVIDIA, 2 Preprint. 2023). We highlight TEEs as promising yet infrastructure-dependent, focusing primarily on methods practical today with existing API structures."
        },
        {
            "title": "3 Problem formulation and threat models",
            "content": "3.1 Problem formulation We consider scenario involving three main entities: User/Auditor: Wishes to use an LLM service for task involving input prompts sampled from distribution π(x). Service provider: Offers access to an LLM via black-box API. The provider advertises that the API uses specific target model Mspec (e.g., Llama-3-70B-Instruct). Target model (Mspec): The model the user expects and pays for, characterized by conditional probability distribution Pspec(yx) over output sequences y. The core problem arises because the provider might, intentionally or unintentionally, use an alternative model Malt while still claiming to use Mspec. The alternative model Malt could be: (1) smaller model from the same family (e.g., 8B instead of 70B). (2) quantized version of Mspec (e.g., INT8 or FP8). (3) model from an entirely different family (e.g., Mistral instead of Llama) (4) fine-tuned variant of Mspec, etc. The user interacts with the API by sending prompt and receiving an output generated according to the providers actual backend distribution Pactual(yx). Depending on the API, the user might also receive additional information, such as log probabilities of the generated tokens. The user typically has reference access (e.g., via local execution or trusted provider) to the target distribution Pspec(yx) and possibly some alternative distributions Palt(yx). Auditing goal. The user/auditor aims to determine, with high confidence, whether the provider is faithfully using the specified model. Formally, given samples (x, y) from the providers API (y Pactual(x)) and potentially reference samples from Pspec(x), the auditor wants to test the null hypothesis: H0 : Pactual(yx) = Pspec(yx) π(x) (Honest Provider) against the alternative hypothesis: H1 : Pactual(yx) = Pspec(yx) for some π(x) (Substitution Occurred) An effective verification method should achieve: High accuracy: Low False Positive Rate (incorrectly flagging an honest provider) and low False Negative Rate (failing to detect substitution). Efficiency: Require reasonable number of queries and computational overhead. Robustness: Remain effective even when the provider employs countermeasures. Generality: Apply across different models, tasks (prompts π), and levels of API access. 3.2 Adversarial attack scenarios dishonest provider aiming to perform model substitution while avoiding detection might employ several evasion strategies. We analyze the following practical attack scenarios: Quantization substitution. The provider replaces the full-precision target model Mspec with quantized version (e.g., INT8, FP8, NF4). Quantization significantly reduces memory footprint and often accelerates inference, lowering costs. While preserving much of the models capabilities, it slightly alters the output distribution Palt(yx). The attack relies on this distributional difference being too small for naive detection methods to pick up, especially with limited samples. Randomized model substitution. To make detection harder, especially if simpler substitutions prove detectable, the provider can employ randomized substitution. query is 3 Preprint. routed to the original model Mspec with probability and to cheaper substitute Malt with probability 1 p. The resulting output distribution observed by the user is mixture: Pmixed(yx) = Pspec(yx) + (1 p) Palt(yx). As approaches 1 (low substitution rate), Pmixed becomes statistically closer to Pspec, making detection via sampling-based tests increasingly difficult. sophisticated provider might adaptively adjust p, perhaps increasing it (reducing substitution) if query patterns suggest an audit (e.g., repeated prompts, known benchmark queries), aiming to balance cost savings with evasion. Benchmark evasion (cached/routed output). This attack specifically targets verification methods relying on known, fixed prompts, such as benchmark datasets or simple identity queries (What model are you?). The provider implements detection mechanism (e.g., prompt hashing, string matching, embedding similarity with known benchmark prompts) to identify likely audit queries. Upon detection, the provider serves response generated by (or routes the query to) genuine Mspec instance. If necessary, corresponding metadata like log probabilities can also be cached or generated on-demand from Mspec. For all other normal user queries, the cheaper Malt is used. This allows the provider to pass audits based on known inputs while still achieving cost savings on the majority of traffic. Limiting information disclosure. Providers might proactively limit the information exposed through their APIs to thwart known verification techniques that rely on richer metadata. For instance, after researchers demonstrated that full logit distributions could be reconstructed using logit biasing parameters (Carlini et al., 2024; Finlayson et al., 2024), several major API providers removed or restricted this feature."
        },
        {
            "title": "4 Model verification techniques and robustness analysis",
            "content": "LLM service providers offer wide range of interfaces, from basic text-only web chats to advanced APIs that allow control over decoding parameters and access to token log probabilities (Table 1 and Table 5). This variation determines which verification techniques can be applied. Service Provider Open Source Models Decoding Parameters Logprobs Output Anyscale Together.ai Hugging Face AWS Bedrock Nebius AI Vertex AI Mistral DeepSeek OpenAI Cohere Anthropic Yes Yes Yes Yes Yes Yes Yes Yes No No No Full Control Full Control Full Control Full Control Full Control Full Control Partial Control Partial Control Partial Control Full Control Partial Control Top 5 Single Token Top 5 No No Top 5 No Top 20 Top 20 Single Token No We evaluate each technique based (1) on its required access level: text-only output, (2) access to additional information (e.g., log probabilities) or control via APIs, and (3) reliance on provider-supplied information (e.g., TEEs discussed in Section 4.3). For each method, we assess its effectiveness and robustness against the attacks described in Section 3.2. Table 1: Transparency and control across LLM API providers (March 2025). Full Control implies typical parameters like temperature, top-p, top-k, etc. 4.1 Text-output-based verification These methods rely solely on the textual completions generated by the API, making them universally applicable but potentially less sensitive. 4.1.1 Text classifier (Sun et al., 2025) Method. This approach leverages the idea that different LLMs possess unique stylistic fingerprints. Using text outputs from different source models, an auditor can construct labeled dataset = {(xi, zi)}N i=1, where each xi is generated text sample and zi {1, . . . , K} indicates the model that generated it. classifier is then trained by fine-tuning 4 Preprint. an encoder LLM along with classification head, formally to find: θ = arg min θ (xi,zi)D L(cid:0)Mθ(xi), zi (cid:1), where denotes the cross-entropy loss. During auditing, generated text from the API is fed to the classifier to predict its source. Attack: Quantization substitution. We investigate if text classifiers can distinguish between full-precision model and its quantized versions, common cost-saving substitution. We generated text using Llama-3.1-70B, Gemma2-9B, Mistral-7B-v0.3, and Qwen2-72B, comparing their original versions against FP8 and INT8 quantized counterparts. Using prompts from Ultrachat (Ding et al., 2023) and sampling with temperature 0.6, we created datasets (10k train, 1k validation samples per model version). We trained binary classifiers using BERT (Devlin et al., 2019), T5 (Raffel et al., 2020), GPT-2 (Radford et al., 2019), and LLM2Vec (BehnamGhader et al., 2024) embeddings. BERT Acc T5 Acc GPT2 Acc LLM2Vec Acc 50.10 49.90 50.30 49.70 49.75 50.75 50.25 50. 50.55 51.60 49.95 49.00 50.55 49.70 50.05 50.75 Model Llama3-70B-Instruct-FP8 Llama3-70B-Instruct-INT8 Gemma2-9b-it-FP8 Gemma2-9b-it-INT8 Mistral-7b-v3-Instruct-FP8 Mistral-7b-v3-Instruct-INT8 Qwen2-72B-Instruct-FP8 Qwen2-72B-Instruct-INT8 Analysis. Table 2 shows that classification accuracies across all model pairs and embedding models consistently hover around 50. This indicates that regardless of original model series, parameter size, and quantization methods, there are no distinguishing features that allow current state-of-the-art embedding models to perform better than random guessing. This differs significantly from the baseline results reported in Sun et al. (2025), where the same classifier setup achieved approximately 98% accuracy for cross-model classification and around 80% accuracy when comparing larger and smaller models within the same series. Our experiments with different training and testing data configurations yielded similar results, providing strong evidence that text classification approaches are ineffective for identifying quantized model substitution. Table 2: Binary classification accuracy between outputs of quantized and original models on the Ultrachat dataset. Accuracies near 50% indicate failure to differentiate. 50.45 51.30 51.20 51.65 48.70 50.50 48.75 49.45 49.90 50.25 49.80 49.55 48.75 51.15 49.80 50. 4.1.2 Identity prompting (Huang et al., 2025) Method. naive method is to prompt the model to reveal its identity, leveraging the fact that many LLMs are trained to disclose their name or developer. Repeated querying using identity-eliciting prompts (e.g., Who are you?, Who developed you?) followed by simple string matching against known model identifiers has proven effective in classifying models based on their responses (Huang et al., 2025). Attack: Quantization substitution. We tested if quantization affects the models ability to self-identify accurately. We prompted base and FP8 versions of several models (Llama3.170B-Instruct, Gemma2-9b-it, Mistral-7b-v3-Instruct, and Qwen2-72B-Instruct) 500 times each with identity queries at temperature 0.6. We then calculated detection accuracy by string matching for model family names (e.g., Llama), developer information (e.g., Meta). Attack: System prompt override. System prompt overrides can circumvent identity-based detection methods. By prepending instructions like You are model XX, adversaries can manipulate models to suppress their trained identity information. We evaluated this attack by querying Gemma2-9b-it, Mistral-7b-v3-Instruct, and Qwen2-72B-Instruct about their identity using the same methodology as our previous experiments, but with the system prompt You are LLaMA, an AI assistant developed by Meta AI. This approach significantly reduced the accuracy of naive identity prompting across all tested models. 5 Preprint. Analysis. Our results demonstrate consistent limitations in model self-identification. Across all tested models, responses contained only high-level identity information (e.g., family name Gemma and developer Google DeepMind), lacking specifics like parameter count or quantization status. Direct queries about these technical specifications yielded unreliable results, with success rates below 5% and frequent refusals or hallucinated values. While system prompt overrides generally succeeded, certain models occasionally produced inconsistent responses that contradicted the injected identity (e.g., am an AI model trained by Mistral AI, not Meta AI). These contradictions, along with potential side effects on model behavior for unrelated queries, make quantization-based substitution more reliable attack. Overall, standard identity prompting proves inadequate for detecting either quantization status or parameter size variations within the same model family."
        },
        {
            "title": "4.1.3 Model equality testing (Gao et al., 2024)",
            "content": "Method. Given the assumption that all decoding parameters are the same and we have access to models text output only, we follow Gao et al. (2024) to use the Maximum Mean Discrepancy (MMD) to compare two distributions over text completions, and Q. Let k(, ) be positive definite kernel function. Then the squared MMD is defined by: (cid:2)k(x, x)(cid:3) + MMD2(P, Q) = (cid:2)k(x, x)(cid:3) 2E (cid:2)k(x, x)(cid:3). xP,xQ x,xQ x,xP To tailor the MMD to text data generated by LLM, we will empirically use Hamming Kernel: khamming(x, x) = are the i-th tokens of sample and x, respectively, and is the the maximum sequence length. i}, where xi and i=1 1{xi = To determine statistical significance after calculating the MMD statistic, we compare the observed MMD value against its null distribution (MMD when = Q). We estimate this null distribution through permutation testing. Specifically, we generate multiple random permutations by repeatedly shuffling samples between the and sets and recalculating the MMD statistic for each permutation. The p-value is then the proportion of permutation MMD values that are greater than or equal to our observed MMD value. We reject the null hypothesis (that = Q) if this p-value is below our chosen significance level α. This approach allows us to detect distribution differences with relatively small sample sizes, making it practical for API auditing. Attack: Randomized model substitution. For the experiment, we used the completions from dataset of outputs generated by Llama3-70B-Instruct and Llama-3.1-70BInstruct from the INT8 quantized model and the original full-precision model. Throughout all experiments, we maintained identical decoding parameters by using maximum completion length of 50 tokens and temperature = 1. The prompt distribution is uniform over 25 random strings sampled from multilingual Wikipedia. For each model, we created mixed distributions with parameter {0.0, 0.2, 0.4, 0.6, 0.8, 1.0}, where represents the proportion of INT8 samples in the mixture. We then evaluated the statistical power of our test, based on the Hamming MMD, to distinguish the mixed output distribution under different parameter from the original models distribution for each of the three models with significance level α = 0.05. For each comparison, we performed 100 Monte Carlo simulations and used 1000 permutations to estimate p-values, calculating the statistical power as the proportion of simulations that correctly rejected the null hypothesis. Figure 1: Statistical power of model equality testing for the smaller model under quantization attack and the larger model under substitution attack. Analysis. As shown in Figure 1, model equality testing on smaller-sized models has statistical power consistently remains below 0.2 and usually close to 0.05, even without 6 Preprint. randomized substitution, indicating poor detection capability. Consequently, our random model substitution analysis focuses on larger models where these tests initially demonstrate greater detection capability. When the proportion of substituted samples reaches 0.2, the tests statistical power decreases to approximately the significance level α = 0.05. This indicates that with just 20% substitution, the tests performance becomes statistically indistinguishable from random chance, showing that random substitution attack is successful in such auditing scenario. Empirically, the MMD distance between instances of the same model can be quite large even when auditing from reputable API provider like Together.ai1. This demonstrates that different implementations of inference frameworks can lead to false test results, indicating that such testing cannot be fully trusted."
        },
        {
            "title": "4.1.4 Benchmark-based detection (Chen et al., 2023)",
            "content": "Method. An auditor may deploy benchmark-based detection by querying the API using series of standardized benchmark tasks with known results. By comparing the models predictions against these established baselines on the model of suspect, deviations can indicate that the model is substituted. MMLU 62.76 0.18 62.30 0.19 78.17 0.14 77.84 0.15 71.86 0.12 71.89 0.12 82.17 0.12 81.99 0.12 59.12 0.19 58.70 0.18 Model Meta-Llama-3-8B-Instruct Meta-Llama-3-8B-Instruct-FP8 Meta-Llama-3-70B-Instruct Meta-Llama-3-70B-Instruct-FP8 Gemma-2-9b-it Gemma-2-9b-it-FP8 Qwen2-72B-Instruct Qwen2-72B-Instruct-FP8 Mistral-7B-Instruct-v0.3 Mistral-7B-Instruct-v0.3-FP8 Attack 1: Quantization substitution. We evaluate benchmarkbased detection using four tasks: MMLU (Hendrycks et al., 2020), Math (Hendrycks et al., 2021), GPQA (Rein et al., 2024), and GSM8k (Cobbe et al., 2021). The MMLU benchmark is simulated 100 times by restricting the models output to multiplechoice tokens (e.g., A, B, etc.), computing their probabilities, and resampling proportionally after normalizing these probabilities to sum to 1. The GPQA and GSM8k tasks are evaluated using string matching with regular expressions, while the Math benchmark employs the math-verify library to verify answer correctness with these three tasks running 4 times. Our goal is to assess whether quantized models exhibit performance differences across these benchmarks and evaluation methods. Table 3: Mean standard deviation of benchmark results on different models at temperature = 0.5. GPQA Diamond 22.60 0.25 20.20 0.00 29.55 0.51 33.21 0.25 28.79 4.70 27.78 6.06 30.05 4.55 31.06 2.04 21.59 0.25 22.73 0. MATH 20.43 1.85 15.24 0.00 35.57 1.82 35.61 1.83 33.34 0.44 32.56 0.64 37.40 1.44 37.66 0.02 9.01 1.78 7.62 0.00 GSM8K 65.43 4.70 61.64 0.00 88.36 1.59 87.49 1.97 81.84 2.13 79.51 1.48 86.83 1.10 86.85 0.31 35.96 6.41 32.30 0.00 As shown in Table 3, benchmark performance between quantized and original fullprecision models is generally close, showing inherent randomness in evaluations and suggesting quantized models perform nearly indistinguishably from their fullprecision counterparts. Auditors typically cannot repeat evaluations extensively due to rate limitations and budget constraints, making such evaluations more susceptible to quantized model substitution. Furthermore, benchmark evasion and randomized model substitutions can further shift results closer to the original model, allowing less capable quantized models to perform similarly. While usually sufficient, this further undermines benchmark verification effectiveness. 1Discussed in Gao et al. (2024) Figure 2: Benchmark Accuracy (y-axis) vs. Temperature (x-axis) on different models. Preprint. Attack 2: Hiding temperature parameter. In Figure 2, we observe that for temperature 1, the standard deviation across most benchmarks is very small (especially for MMLU, where we have larger sample size). Single-run benchmark results are fairly reliable; if service provider claims to serve state-of-the-art model, an auditor can easily detect substitution when observed performance falls slightly below published benchmark results. However, by not disclosing the temperature parameter used, the result could be attributed to either higher temperature setting or an actual model substitution, making benchmark verification less effective."
        },
        {
            "title": "4.1.5 Greedy decoding output",
            "content": "When given the option to do greedy decoding, users could theoretically compare API outputs directly with locally deployed models using identical parameters. However, our experiments with Llama-3-8B and Gemma-2-9B models showed that outputs never matched exactly between various API service and our local generation, across 100 UltraChat queries under greedy decoding. These discrepancies likely result from the non-deterministic of GPU computation and the algorithmic differences across different API providers. Therefore, exact output matching proves unreliable for detecting model substitution in practice. This results also align with the MMD difference from service provider shown in Gao et al. (2024). 4.2 Log probability verification Method. Log-probability comparison can be used for verifying if the underlying model is as claimed. When comparing the outputs of deployed model against known reference model, the log probabilities produced by the same model should be identical. Empirically, there exist small differences that can be attributed software and hardware difference. Thus, significant differences in log-probability outputs should indicate model substitution. Evaluation. To evaluate the stability of log probabilities across different environments, we conducted testing using token-level log probabilities from greedy decoding on UltraChat Queries (Ding et al., 2023). We compared outputs across different versions of different inference frameworks (vLLM (Kwon et al., 2023) and Hugging Face Transformers), hardware configurations (H100 and A100 GPUs). In Figure 3, we show the log probabilities differences for the first 20 generated tokens of 2 sample generations. More samples can be found in Figure 4. Weakness. In Figure 3, we can observe that even in controlled environment, logprobs still exhibit instability in early tokens across different software and hardware configurations. In practice, API providers serve multiple requests simultaneously, and different optimization techniques like continuous batching may also affect the results. As discussed in Section 4.1.5, the results from different API providers can even show token-level differences. In addition, this method cannot work on proprietary model since it requires white-box access to the reference model. Almost all API providers do not give the full logprobs access. If the full logprobs is provided, auditors may verify the served model by reconstructing the hidden dimension or output subspace, which is discussed in Appendix B. Figure 3: Log probability of generating first 20 shared tokens under greedy decoding for UltraChat Queries under different environment. 8 Preprint. 4.3 Hardware-assisted verification: Trusted Execution Environment (TEE) Method. Trusted Execution Environment (TEE) technology provides secure and isolated zone within the processor, safeguarding data processed within the secure enclave from unauthorized access and alterations. This allows users to gain clear understanding of how their data is handled and processed, as the TEE guarantees integrity and confidentiality throughout computations by hardware. NVIDIA Confidential Computing (NVIDIA, 2023) brings TEE technology to their latest GPUs in the Hopper and Blackwell architectures, making model inference within TEEs possible with minimal performance cost. This method enables end users to verify the authenticity of the model in use, ensuring that the advertised model is indeed operational as the backend model. In addition to ensuring the integrity of the model, this technology can also provide privacy guarantees for user input with proper settings. Appendix includes more details about building an LLM inference API endpoint with TEE. This method shifts trust assumptions from the API providers operational honesty to the integrity of the hardware manufacturer. Metric TEE no TEE Overhead First token latency (ms) Evaluation. To evaluate the performance overhead of using TEEs, we conduct performance benchmark for vLLM API endpoint with and without TEE. We host Meta-Llama-3-8B-Instruct model with single H100 GPU and enable TLS for the HTTP endpoint. As can be observed in Table 4, the overheads are relatively low, especially the overall throughput under concurrent requests, which has an overhead of less than 3%. In real-world scenarios, API providers serve multiple users simultaneously and receive large number of concurrent requests. They charge users based on the number of tokens. Therefore, the overall throughput under concurrent requests is the metric that API providers care about most. Concurrent requests # 1 64 1 64 Table 4: Performance comparison. Overall throughput (token/s) 9.23% 16.18% 15.38% 2.88% 65 68 117 971 71 79"
        },
        {
            "title": "5 Discussion and Conclusion",
            "content": "Our investigation into auditing model substitution in LLM APIs reveals challenging landscape for users seeking verification. Limitations of software-based verification. Methods relying solely on text output (classification, identity prompting, MMD, benchmarks) face significant hurdles. They struggle to detect subtle substitutions like quantization (Table 2, Table 3) and can be undermined by adversarial strategies. Randomized substitution significantly degrades the power of statistical tests like MMD (Figure 1), while benchmark evasion can trivially defeat methods relying on known prompts (identity prompting, benchmarks). Furthermore, the effectiveness of benchmarks is highly sensitive to undisclosed decoding parameters like temperature (Figure 2), making reliable comparisons difficult without full transparency. Strength and weakness of log probability analysis. Access to token log probabilities provides much stronger signal (Figure 3). Logprob distributions are highly sensitive to model weights, allowing reliable detection of even quantization or minor version changes, assuming the auditor has reference Mspec instance. However, this methods applicability is entirely dependent on the provider offering logprobs access  (Table 1)  . Providers can easily thwart this technique by simply not providing logprobs (limiting information disclosure) or potentially serving correct logprobs only for detected audit queries (benchmark evasion). In addition, optimization techniques like continuous batching may affect the results and the need for the user to perform reference computations also adds overhead. The promise and challenge of TEEs. Hardware-assisted verification using TEEs (Section 4.3) offers the most robust solution against software-level substitution and evasion tactics. It provides strong integrity guarantee about the code and data (model weights) 9 Preprint. being executed. Performance overhead appears manageable, especially under high concurrency  (Table 4)  . However, the primary barrier is infrastructure dependency and provider adoption. TEEs require specific hardware and software setup, and providers must be willing to offer attestation services. Establishing trust also requires verifiable measurements for the inference stack and model weights, necessitating open-source software components and mechanisms for distributing or verifying model hashes. Overall landscape and future directions. Currently, no universally applicable and robust software-based method exists for users to reliably audit model substitution against determined adversary. Log probability verification is the strongest software technique but it is not sufficiently accurate and relies on API features. TEEs offer technical path to provable integrity but require ecosystem support. This highlights critical need for: Increased transparency: Providers could voluntarily increase transparency by consistently providing detailed metadata like log probabilities, exact model versions (including quantization details), and potentially TEE attestations. More robust auditing techniques: Research into statistical methods less susceptible to randomization or requiring fewer samples, potentially combining multiple weak signals, could be valuable. Techniques analyzing finer-grained output properties beyond standard text features might also yield improvements. Standardized protocols: Development of standardized protocols for verifiable LLM inference, perhaps incorporating lightweight cryptographic commitments or attested execution, could build trust between users and providers. User awareness and tools: Empowering users with tools and knowledge about potential substitutions and available verification methods is crucial."
        },
        {
            "title": "References",
            "content": "AMD. AMD Secure Encrypted Virtualization (SEV), 2016. URL https://www.amd.com/en/ developer/sev.html. Accessed: 2025. Anthropic. Introducing claude, 2023. introducing-claude. URL https://www.anthropic.com/news/ Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, and Yue Zhang. Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature. In The Twelfth International Conference on Learning Representations, 2023. Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. LLM2vec: Large language models are secretly powerful text encoders. In First Conference on Language Modeling, 2024. URL https://openreview.net/ forum?id=IW1PR7vEBf. Nicholas Carlini, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas Steinke, Jonathan Hayase, A. Feder Cooper, Katherine Lee, Matthew Jagielski, Milad Nasr, Arthur Conmy, Eric Wallace, David Rolnick, and Florian Tram`er. Stealing part of production language model. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, 2024. 10 Preprint. Lingjiao Chen, Matei Zaharia, and James Zou. How is chatgpts behavior changing over time?, 2023. URL https://arxiv.org/abs/2307.09009. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training In Proceedings of the of deep bidirectional transformers for language understanding. 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41714186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/ N19-1423. URL https://aclanthology.org/N19-1423/. Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations, 2023. URL https://arxiv.org/abs/2305.14233. Sabri Eyuboglu, Karan Goel, Arjun Desai, Lingjiao Chen, Mathew Monfort, Chris Re, and James Zou. Model changelists: Characterizing updates to ml models. In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency, pp. 24322453, 2024. Boyuan Feng, Lianke Qin, Zhenfei Zhang, Yufei Ding, and Shumo Chu. Zen: An optimizing compiler for verifiable, zero-knowledge neural network inferences. Cryptology ePrint Archive, 2021. Matthew Finlayson, Xiang Ren, and Swabha Swayamdipta. Logits of api-protected llms leak proprietary information, 2024. URL https://arxiv.org/abs/2403.09539. Irena Gao, Percy Liang, and Carlos Guestrin. Model equality testing: Which model is this api serving?, 2024. URL https://arxiv.org/abs/2410.20247. Soumya Suvra Ghosal, Souradip Chakraborty, Jonas Geiping, Furong Huang, Dinesh Manocha, and Amrit Singh Bedi. Towards possibilities & impossibilities of ai-generated text detection: survey. arXiv preprint arXiv:2310.15264, 2023. Google. Introducing gemini: our largest and most capable ai model, 2023. URL https: //blog.google/technology/ai/google-gemini-ai/. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Xiaomeng Hu, Pin-Yu Chen, and Tsung-Yi Ho. Radar: Robust ai-text detection via adversarial learning. Advances in neural information processing systems, 36:1507715095, 2023. Yangsibo Huang, Milad Nasr, Anastasios Angelopoulos, Nicholas Carlini, Wei-Lin Chiang, Christopher A. Choquette-Choo, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Ken Ziyu Liu, Ion Stoica, Florian Tramer, and Chiyuan Zhang. Exploring and mitigating adversarial manipulation of voting-based leaderboards, 2025. URL https://arxiv.org/ abs/2501.07493. Intel. Intel Trust Domain Extensions (Intel TDX), 2021. URL https://www.intel.com/ content/www/us/en/developer/tools/trust-domain-extensions/overview.html. Accessed: 2025. John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. watermark for large language models. In International Conference on Machine Learning, pp. 1706117084. PMLR, 2023. 11 Preprint. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pp. 611626, 2023. Mistral. Mistral 7b: The best 7b model to date, apache 2.0, 2023. URL https://mistral.ai/ news/announcing-mistral-7b/. Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher Manning, and Chelsea Finn. Detectgpt: Zero-shot machine-generated text detection using probability curvature. In International Conference on Machine Learning, pp. 2495024962. PMLR, 2023. Dov Murik and Hubertus Franke. surement, 2021. securing-linux-vm-boot-with-amd-sev-measurement.pdf. Accessed: 2025. Securing Linux VM boot with AMD SEV meaURL https://static.sched.com/hosted files/kvmforum2021/ed/ NVIDIA. AI Security With Confidential Computing. NVIDIA Technical Blog, 2023. URL https://www.nvidia.com/en-us/data-center/solutions/confidential-computing/. Accessed: 2025. OpenAI. Introducing chatgpt, 2022. URL https://openai.com/index/chatgpt/. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 2019. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level googleproof q&a benchmark. In First Conference on Language Modeling, 2024. Dylan Sam, Marc Finzi, and Zico Kolter. Predicting the performance of black-box llms through self-queries. arXiv preprint arXiv:2501.01558, 2025. Haochen Sun, Jason Li, and Hongyang Zhang. zkllm: Zero knowledge proofs for large language models. In Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security, pp. 44054419, 2024a. Mingjie Sun, Yida Yin, Zhiqiu Xu, J. Zico Kolter, and Zhuang Liu. Idiosyncrasies in large language models. arXiv preprint arXiv:2502.12150, 2025. Yifan Sun, Yuhang Li, Yue Zhang, Yuchen Jin, and Huan Zhang. Svip: Towards verifiable inference of open-source large language models. arXiv preprint arXiv:2410.22307, 2024b. TogetherAI. Announcing together inference engine the fastest inference available, 2023. URL https://www.together.ai/blog/together-inference-engine-v1. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Tiancheng Xie, Tao Lu, Zhiyong Fang, Siqi Wang, Zhenfei Zhang, Yongzheng Jia, Dawn Song, and Jiaheng Zhang. zkpytorch: hierarchical optimized compiler for zero-knowledge machine learning. Cryptology ePrint Archive, 2025. Xianjun Yang, Liangming Pan, Xuandong Zhao, Haifeng Chen, Linda Petzold, William Yang Wang, and Wei Cheng. survey on detection of llms-generated content. arXiv preprint arXiv:2310.15654, 2023. 12 Preprint. Xianjun Yang, Wei Cheng, Yue Wu, Linda Ruth Petzold, William Yang Wang, and Haifeng Chen. DNA-GPT: divergent n-gram analysis for training-free detection of gpt-generated text. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id= Xlayxj2fWp. Xuandong Zhao, Prabhanjan Vijendra Ananth, Lei Li, and Yu-Xiang Wang. Provable robust watermarking for ai-generated text. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024a. URL https://openreview.net/forum?id=SsmT8aO45L. Xuandong Zhao, Sam Gunn, Miranda Christ, Jaiden Fairoze, Andres Fabrega, Nicholas Carlini, Sanjam Garg, Sanghyun Hong, Milad Nasr, Florian Tram`er, Somesh Jha, Lei Li, Yu-Xiang Wang, and Dawn Song. Sok: Watermarking for ai-generated content. arXiv preprint arXiv:2411.18479, 2024b. Preprint."
        },
        {
            "title": "A Additional details",
            "content": "Service Provider Reference documentation Anyscale Together.ai Hugging Face AWS Bedrock Nebius AI Vertex AI Mistral DeepSeek OpenAI Cohere Anthropic https://docs.anyscale.com/endpoints/text-generation/logprobs/ https://docs.together.ai/docs/logprobs https://huggingface.co/docs/api-inference/tasks/chat-completion#request https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html https://docs.nebius.com/studio/inference https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/content-generation-parameters#log-probabilities-output-tokens https://docs.mistral.ai/api/#operation/createChatCompletion https://api-docs.deepseek.com/api/create-completion https://platform.openai.com/docs/api-reference/chat/create#chat-create-top logprobs https://docs.cohere.com/v2/reference/chat#request.body.logprobs https://docs.anthropic.com/en/api/messages Table 5: LLM API service providers documentations. Figure 4: Log probability of generating first 20 shared tokens under greedy decoding for UltraChat Queries under different environment."
        },
        {
            "title": "B Model stealing and embedding verification",
            "content": "Background. Researchers have explored extracting model information (e.g., weights, architecture details) from black-box APIs (Carlini et al., 2024; Finlayson et al., 2024). While successful extraction could reveal substitutions, these methods often require vast number of queries, significant computational resources, and may still yield incomplete information, making them impractical for routine auditing by typical users. Finlayson et al. (2024) specifically demonstrated how logits analysis could expose model details, which we consider as potential verification method for detecting model substitutions. 14 Preprint. Method. The embedding size detection relies on the fact that logits generated by an LLM are restricted to d-dimensional subspace of the full v-dimensional output space where is the hidden embedding size. So the auditor can analyze set of logit outputs from different prompts and reconstruct this subspace and the hidden embedding size. Given set of output logit vectors {ℓi}n i=1 (or log probabilities, since it preserves linear relationship up to constant addition), we construct logit matrix with each vectors as its columns. Applying SVD to yields = UΣVT, where Rvv, Σ Rvn (with singular values λ1 λ2 . . .), and Rnn. The magnitude of the singular values will drastically decreases after the first dimensions. Formally, the embedding size can be identified by detecting the singular value index at which the magnitude drops the most: = arg maxi(log λi log λi+1), Once identified, the d-dimensional subspace is constructed using the first left singular vectors from U: Ud = [u1, u2, . . . , ud] Rvd, forming unique signature of the model. Empirical evaluations confirm that this method accurately recovers the embedding dimensions. Furthermore, monitoring this subspace allows auditors to detect subtle changes, such as hidden system prompts, fine-tuning, or entire model substitutions from the service provider. Weakness. This verification method is impractical because no well-known service provider gives full log probability access."
        },
        {
            "title": "C Build an LLM inference API endpoint with TEE",
            "content": "In this section, we provide solution for building an LLM inference API endpoint with TEE. Considering the main program is still running on the CPU, the GPU TEE technology needs to be paired with the VM-based CPU TEE (e.g., Intel TDX (Intel, 2021), AMD SEVSNP (AMD, 2016)) to work properly. The latest CPU TEE can launch confidential virtual machine inside and ensure the integrity and confidentiality of the VMs memory, preventing the host which is controlled by the API provider from manipulating the data inside the confidential VM. The GPU TEE (e.g., NVIDIA Confidential Computing (NVIDIA, 2023)) can build secure communication channel between the CPU and GPU and ensure the integrity and confidentiality of the data on the GPU memory, thus we can add the GPU to the trusted computing base (TCB). By combining these two TEEs, we can ensure the integrity and confidentiality of programs and data processed inside them. To ensure integrity and confidentiality, the program inside the TEE needs to generate key pair for identifying itself and establishing secure communication channels with end users. This key can help end users verify that they are connecting to the program inside the TEE and ensure the API providers cannot interfere the communication traffic. Once the data is transferred to TEE, the data processing inside TEE will be encrypted by hardware to ensure integrity and confidentiality. One last question is how we can provide proof of the integrity of the inference program and add this proof to the attestation report. First, the VM-based CPU TEE ensures memory protection and the measured direct boot (Murik & Franke, 2021) extends the measurement to kernel, initrd, and kernel command line. At this point, the measurement value in the attestation report can validate the integrity of the kernel, initrd, and kernel cmdline. However, ensuring the integrity of the operating system and program on the disk requires additional steps. To achieve this, we include program into the initrd that verifies and encrypts disk data. This action incorporates the disk into the trusted computing base (TCB). In this setting, the LLM inference program needs to be open-sourced for verification, while the model can remain private if needed. The inference program inside the TEE can verify the hash of the model weights and ensure that every time model with specific name is used, it has the same hash. For open-sourced models, it is easy for anyone to calculate the desired hash. For proprietary models, the name serves as an alias for specific model weights, if we can confirm that it always has the same hash value, we can consider it to be integrous. 15 Preprint. Figure 5: screenshot of the API inference endpoint with TEE. As we can see in Figure 5, the program is transparent and verifiable to the end users. End users can check the attestation report and verify the signature on it to ensure that the API provider is running the desired program."
        }
    ],
    "affiliations": [
        "University of California, Berkeley"
    ]
}