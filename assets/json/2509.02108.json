{
    "paper_title": "DivMerge: A divergence-based model merging method for multi-tasking",
    "authors": [
        "Touayouch Brahim",
        "Fosse Loïc",
        "Damnati Géraldine",
        "Lecorvé Gwénolé"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multi-task learning (MTL) is often achieved by merging datasets before fine-tuning, but the growing availability of fine-tuned models has led to new approaches such as model merging via task arithmetic. A major challenge in this setting is task interference, which worsens as the number of tasks increases. We propose a method that merges models trained on different tasks into a single model, maintaining strong performance across all tasks. Our approach leverages Jensen-Shannon divergence to guide the merging process without requiring additional labelled data, and automatically balances task importance. Unlike existing methods, our approach remains robust as the number of tasks grows and consistently outperforms prior work."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 8 0 1 2 0 . 9 0 5 2 : r DivMerge: divergence-based model merging method for multi-tasking Brahim Touayouch1,2 Loïc Fosse1,3 Géraldine Damnati1 Gwénolé Lecorvé1 1Orange Research, Lannion, France 2École polytechnique, Institut polytechnique de Paris, Palaiseau, France 3CNRS, LIS, Aix Marseille Université, France Contact: first.last@orange.com"
        },
        {
            "title": "Abstract",
            "content": "Multi-task learning (MTL) is often achieved by merging datasets before fine-tuning, but the growing availability of fine-tuned models has led to new approaches such as model merging via task arithmetic. major challenge in this setting is task interference, which worsens as the number of tasks increases. We propose method that merges models trained on different tasks into single model, maintaining strong performance across all tasks. Our approach leverages Jensen-Shannon divergence to guide the merging process without requiring additional labelled data, and automatically balances task importance. Unlike existing methods, our approach remains robust as the number of tasks grows and consistently outperforms prior work."
        },
        {
            "title": "Introduction",
            "content": "based transformer Current language models (Vaswani et al., 2017; Brown et al., 2020) have demonstrated remarkable efficiency in handling wide range of tasks within single unified architecture. This led to the creation of the so-called Instruct models (Shengyu et al., 2023), which are now state-of-the-art in almost all NLP tasks. However, creating these models is very expensive and relies on large data collections and models. With this goal of reducing costs in mind, paradigm has been revived: model merging. This paradigm rooted in ensemble methods consists in combining parameters of models specialized on specific tasks in order to create new one that has new properties such as multi-tasking to only cite this one. This paradigm is all the more interesting given the large number of specialized models for specific tasks available on collaborative platforms such as HuggingFace (Wolf et al., 2019). Thus, since the work of Ilharco et al. (2022) whole series of studies on model merging have been published, proposing many different methods to combine model parameters in order to produce multi-tasking type model (Goddard et al., 2024; Yang et al., 2024b). The motivation behind the creation of these different methods is to answer the following question: how can models trained on different tasks be combined without losing performance on each task? In the literature this problem is also mentioned as interference between models. However, the wide variety of methods available and the lack of consensus in the production of methods seem to suggest that this question remains unanswered. In this paper, we propose new model merging method that allows us to create so-called multi-task model. Our method formally addresses the interference problem mentioned above, which can adversely affect the model resulting from the combination. More formally, the contributions of this paper are the following three points: 1. novel merging method. We propose merging method that is grounded in information theory (Cover and Thomas, 1991). This method automatically learns how to combine parameters of different models in data driven but reference free setting to achieve the multi-task property. We formally demonstrate that our method is linked to classical multi-task learning (Caruana, 1997) and is constructed in way that it minimizes interferences between models by respecting weight disentanglement defined by OrtizJimenez et al. (2023). 2. Better performances. On classical set-up with only two tasks to merge we illustrate that our method is the best one in average among classical state of the art methods. 3. Better scalability. We empirically show that our method scales better when the number of tasks to merge increases. This clearly illustrates that our method is more effective at limiting the possible effects of interference between the different models we wish to combine."
        },
        {
            "title": "2 Related Work",
            "content": "Multi-Task Learning. Multi-task learning (Caruana, 1997) refers to methods that produce model capable of solving several tasks. Classical multitask learning generally consists in combining datasets from multiple tasks and training model on this union, which is justified by classical results such as Steins paradox (Stein, 1956). This philosophy is at the core of the production of most current models such as T5 (Raffel et al., 2020) and more recently Instruct type models (Shengyu et al., 2023). Although the results of these models are now considered state-of-the-art in NLP, this approach is not without its issues. Indeed, several studies (Baxter, 2000; Ben-David and Schuller, 2003; Fifty et al., 2021; Standley et al., 2020; Jeong and Yoon, 2025; Maurer et al., 2016) show that the choice of tasks is important in order to limit effects such as interference (Yu et al., 2020): some tasks may have negative effect on others. In this study we propose novel method, grounded in Information Theory (Cover and Thomas, 1991), to produce multi-task model. Our method is theoretically connected to the classical multi-task learning, and model merging. Model Merging. While the idea of combining several models has its roots in ensemble methods originally developed for variance reduction (Dietterich, 2000), model merging refers to methods that combine models in the parameter space to produce new model that has new properties that go beyond variance reduction. While most applications of model merging are described by Yang et al. (2024c), we recall here the most popular ones. One objective of model merging is of course variance reduction (i.e. better generalization) (Jin et al., 2022; Matena and Raffel, 2022; Ferret, 2025; Izmailov et al., 2018; Wortsman et al., 2022), with the goal of producing more reliable model on single target task by merging models trained on the same task with different initializations or hyperparameter settings. The most popular objective in model merging is the multi-task one (Ilharco et al., 2022; Yang et al., 2024d; Yu et al., 2024; Pfeiffer et al., 2021): by merging models specialized on different tasks, the resulting model should achieve good performance on all tasks. Zhou et al. (2024) and (Ortiz-Jimenez et al., 2023) provide theoretical insights into why model merging can work well for this multi-task objective. Another objective is task unlearning (Ilharco et al., 2022; Kuo et al., 2025; Kim et al., 2024): by merging certain models (via addition and negation), we seek to remove or \"forget\" specific components. Finally, modular learning (Ballard, 1987; Pfeiffer et al., 2023; Chronopoulou et al., 2023) is an interesting but less explored objective, which consists of creating model that performs well on task by merging models trained on other (possibly unrelated) tasks, leveraging the notion of transfer between tasks. In this study, we propose new model merging method with the goal of multi-task learning. Merging Methods. While the literature offers wide range of merging methods, some of them stand out with interesting results and properties. Since this study is focused on multi-task learning, we give quick overview of methods designed for this. The most straightforward and simple approach is model averaging (Wortsman et al., 2022), also known as isotropic merging, which simply consists in taking the uniform average of the models parameters. In (Ilharco et al., 2022) the notion of task vector is introduced which is the shift in the parameter space from pre-trained model to finetuned one1. This concept has led to the framework of task arithmetic (TA) which is now extensively used (Ortiz-Jimenez et al., 2023) and has proven its efficiency across wide range of applications. To enhance TA, SLERP (Spherical Linear Interpolation) (Jang et al., 2024) proposes merging method that preserves certain geometric properties of the task vectors, thereby helping to mitigate interference as described earlier. This notion of interference is also at the heart of wide range of methods that attempt to address this problem by following two-step process: first, task vectors are modified using techniques such as masking or singular value decomposition (SVD) (Wang et al., 2024; Yadav et al., 2023; Stewart, 1993); second, the preprocessed task vectors are interpolated to produce the merged model. This is the case of methods such as TIES (Yadav et al., 2023), AdaMerging++ (Yang et al., 2024d), or DARE (Yu et al., 2024). Among these methods, AdaMerging stands out as it is data-driven, i.e. this method automatically learns the best way to combine each models parameters. It requires learning algorithm that uses data from the different tasks. In this study, we propose novel multi-task merging method rooted in task arithmetic and, like AdaMerging, leverages reference-free data-driven optimization. 1We provide more formal definition in Sec."
        },
        {
            "title": "3 Formalism",
            "content": "Notations. Random variables are denoted by capital letters (e.g., X), their spaces by calligraphic letters (e.g., ), and elements by lowercase letters (e.g., ). P(X ) is the set of probability measures on , and P(YX ) the set of conditional probabilities on given . For , PX P(X ) is its law, and SX its support. task is probability measure PXt,Yt P(X Y), following the formalism of (Fosse et al., 2025). For sake of simplicity, we hypothesis that all tasks share the same space Y, and that = Y, which is true in most generative tasks: both inputs and outputs are texts. We note language model with parameters θ with ( ; θ) P(YX ). For task t, the specialized model on is ( ; θt) or Mt(). Remark 1. We identify language model with conditional probability: (x; θ) is probability distribution over texts. For sake of simplicity we will refer to language model by (x; θ) when the model is given the data as input. In other words language model is communication channel."
        },
        {
            "title": "3.1 Task vectors and model merging",
            "content": "Task vectors have become essential objects in modern machine learning, given the vast number of finetuned models available on collaborative platforms such as HuggingFace (Wolf et al., 2019). These objects were first defined in (Ilharco et al., 2022) as follows: if we denote by θ0 Rd the parameters of pre-trained model (d being the number of parameters in the model), and by θt Rd the parameters of the same model after fine-tuning on task t, the task vector is given by, τt θt θ0 Rd. (1) The main approach in model merging is to combine such task vectors to create new one that has new properties. In the following, we denote any task vector-based merging algorithm as, where Γt are real values (possibly negative). In the following we will refer to the TA method as ΦΓ n, with the number of task vectors, and Γ the merging coefficients. Estimating the parameters Γ can be challenging, computationally intensive, and it strongly depends on the objective we aim to achieve with model merging (e.g., multi-task learning, modularity, task unlearning, etc.). In this study, we focus on the multi-task setup in model merging: the merged model must be capable of solving all the tasks on which the individual component models have been fine-tuned. In this set-up, as stated in (Ortiz-Jimenez et al., 2023), one of the main properties we hope to achieve is weight disentanglement, which is defined in Definition 1. Definition 1 (Weight Disentanglement (Ortiz-Jimenez et al., 2023)). Let (.; θ) be model parametrized by θ. Consider set of tasks, {(Xt, Yt), }, and their corresponding task vectors {τt, } relatively to the model . If tasks have non overlapping supports i.e. SXt SXt = for all = t, we say that merging method satisfies weight disentanglement iff, (x; (θ0, {τt}, Γ)) = (cid:40) (x; θ0 + τt) (x; θ0) if SXt, if / (cid:83)T t=1 SXt. In other words, merging method satisfies Definition 1 if adding τt does not affect models output outside the corresponding task support SXt. merging method satisfying weight disentanglement assures that the performance on all the merged tasks will be preserved, assuring thus good behaviour in terms of multi-tasking. In this study we will focus on TA merging algorithm denoted as ΦΓ n. We choose this method because, as explained in App. E, most current model merging approaches are based on task arithmetic. However, as we will demonstrate, our theoretical framework is not limited to task arithmetic, and most of our results can be extended to more general methods. (θ0, {τt} , Γ) Rd, (2)"
        },
        {
            "title": "3.2 Our Method",
            "content": "where θ0 are the pre-trained model parameters, {τt} is the set of task vectors to be merged, and Γ is the set of parameters for the merging method. For example, task arithmetic (Ilharco et al., 2022; Ortiz-Jimenez et al., 2023) (TA) can be formulated as, (θ0, {τt} , Γ) = θ0 + (cid:88) Γt τt, (3) Based on the Kullback and Leibler (KL) (Kullback and Leibler, 1951) and Jensen-Shannon (JS) (Wong and You, 1985) divergences, we propose method to automatically estimate the merging coefficients in Eq. 3 in way that forces weight disentanglement (c.f. Definition 1), which translates into preserving performance (as much as possible) on the different task components. First, we recall some basic properties of the KL divergence. 3 Given two discrete probability distributions µ and ν over some discrete space we have, KL(µν) (cid:88) µ(i) log iI (cid:18) µ(i) ν(i) (cid:19) . In this definition, we refer to µ as the reference distribution. From this, we also define the JS divergence, which is symmetric version of the KL divergence, JS(µ, ν) 1 2 (cid:0)KL (cid:0)µ (cid:13) (cid:13) µ+ν 2 (cid:1) + KL (cid:0)ν (cid:13) (cid:13) µ+ν 2 (cid:1)(cid:1) . Both divergences are positive real numbers that quantify notion of distance between the measures µ and ν (lower values indicate closer distributions). We can extend these definitions to define divergence between language models. Given two LMs, M1 and M2, and reference input dataset X, we define the divergence (either KL or JS) between these models as, DX (M1M2) EX (cid:2)D(cid:0)M1( X)M2( X)(cid:1)(cid:3) . This is one possible expression for the divergence between transition probabilities. Since, in NLP tasks, the model generates sequences of text, we can naturally extend the formula above to sequencelevel distributions. See Sec. B.1 for more details. Since this work focuses on task vectors and model parameters, in the following, we will denote language model (LM) by its parameters, i.e. θt Mt. Given divergence (either KL or JS), we provide in Eq. 4 an optimization problem to automatically find coefficients in task arithmetic. Γ arg min Γ (cid:88) t="
        },
        {
            "title": "DXt",
            "content": "(cid:0)θt ΦΓ (cid:1) . (4) Remark 2. (Nielsen, 2020) provides intuition for the solution of Eq. 4. In the case where = JS, the solution corresponds to the probabilistic centroid of the different output distributions generated by the models {θt}, which is concept firstly introduced in the framework of Information Geometry (Amari In App. we give more and Nagaoka, 2000). details about properties of some centroids in the case of model merging. Remark 3. Eq. 4 is defined for the task arithmetic merging function. However, we can replace ΦΓ with any merging method (θ0, {τt}, Γ) (not only task arithmetic). Despite the simplicity of the optimisation problem in Eq. 4, we propose several results that connect with weight disentanglement and multi-task learning theory. Proposition 1. For either DX = KL or DX = JS, the objective function defined in Eq. 4 has minimum value of 0 if and only if the merging method satisfies weight disentanglement around θ0 on the merged tasks. The proof of this result is given in Sec. A.1. Therefore, the optimization problem in Eq. 4 admits minimum value of 0 iff there exists choice of merging parameters Γ that respects the weight disentanglement property, giving added weight to this property. Furthermore, the following proposition establishes an interesting link between Eq. 4 and classical multi-task learning setups (i.e., when datasets are merged before fine-tuning), further emphasizing the relevance of the objective function defined in Eq. 4. Proposition 2. For = KL, the optimization problem defined in Eq. 4 is Moment projection (M-projection) approximation of the multi-task objective when tasks are merged before training. We give the proof of Proposition 2 in Sec. A.2. This proposition is noteworthy as it links our method to the classical multi-task learning objective considered so far as the strongest baseline. In other words our method is an approximation of the classical multi-task learning objective. Moreover, the demonstration of Proposition 2 indicates that the effectiveness of this approximation depends on the performance of the individual fine-tuned models. Finally, we give an illustration of our procedure for merging two models in Figure 1, and we provide the general algorithm for our approach in Algorithm 1."
        },
        {
            "title": "4 Experimental Protocol",
            "content": "Models and datasets. As stated in Sec. 3, our proposed merging method is designed for generative language models. Therefore, we evaluate our approach on two architectures: decoderonly architecture using the Qwen2.5-0.5B (Yang et al., 2024a) model and an encoder decoder architecture using the T5-Base (Raffel et al., 2020) model. We apply our method on both classification and generative tasks. For classification tasks, we used the GLUE benchmark (Wang et al., 2019), as is common in recent studies. We fine-tuned 4 Figure 1: Illustration of the divergence-based model merging method. This figure shows the merging loss associated to one task to be merged, which is the task t. Our method consists in doing this procedure for every task and, as it is written on the left side, to sum all the associated loss. For more details, we refer to Algorithm 1. Description Data Prompt Data Labels Answer Is the following sentence linguistically acceptable or unacceptable in english? Sodium is little too peppy for me to want to try mixing and water in teacup. Answer with acceptable or unacceptable. Answer: unacceptable<endoftext> Table 1: Data example for the fine-tuning on the GLUE Benchmark. This example is derived from the CoLA dataset. Fine-tuning is done in completion only fashion (SFT) after the pattern Answer: Qwen2.5-0.5B on 7 classification tasks, resulting in 7 distinct checkpoints for our experiments. Finetuning was performed using standard supervised fine-tuning (SFT), where the model generates the classification labels. An example of the fine-tuning data is shown in Table 1. For generative tasks, we used the T5-Base model and existing checkpoints on several tasks, including IMDB (Maas et al., 2011), QASC (Khot et al., 2020), SQuAD (Rajpurkar et al., 2016), and CommonGen (Lin et al., 2020)2. This setup allows us to evaluate our method in more realistic scenario, where we do not control the fine-tuning process and only have access to the fine-tuned models. We demonstrate that our method is effective in this setting as well. Evaluation Metrics. To assess the quality of merging method, we define metric to quantify how the merged model performs compared to each fine-tuned model. This metric is the Average Normalized Performance (ANP), defined as: ANP 1 (cid:88) t=1 PERF (f (θ0, {τi}, Γ) ; t) PERF(θt; t) , (5) where is the number of merged tasks, PERF (f (θ0, {τi}, Γ) ; t) is the performance of the merged model on task using the merging method , and PERF(θt; t) is the performance of the fine-tuned model on task t. In our work, PERF is measured by classical accuracy for classification tasks, and by the ROUGE1 score (Lin, 2004) for generation tasks. Each performance metric is computed on separate test set. This metric quantifies the performance of specific merging experiment. In practice, multiple merging experiments can be conducted (for example, by sampling different sets of tasks to merge). In such cases, we compute the ANP metric for each merging experiment and report the average. It is worth noting that the ANP metric is well suited for the multitasking setup, as it automatically provides value relative to the baselines.3 Merging method. As stated previously, in this study we focus on the task arithmetic merging function ΦΓ n. We define two versions of this merging method. The first, described in Eq. 3, is what we call Task Level (TL) since one coefficient Γi is given for each task. The granularity of the method can be further refined: since the models we use are deep neural networks organized in layers, we can define different parameter Γi for each layer for task l, resulting in merging coefficients (where is the number of layers). We refer to this method as the Layer Level (LL) approach. In the following, we present results for both methods and always specify which one is used. Moreover, our method requires input data for each task; that is, when merging tasks and j, we need data from Xi and Xj. For this purpose, we use the validation set of each task. 2All checkpoints are available here 3If the merging method verifies Definition 1 ANP = 1. 5 Algorithm 1 Model Merging via Divergence-Based Optimization. In this procedure, for sequence and model , Logits(y, ) denotes the logits (soft probs) of the sequence given by the model . From technical point of view, this is only forward pass of through . In Sec. B.1 we propose more details about the computation of divergences. Require: : {Xt } fΓ (Merging method) θ0 (Pre-trained model) {θt } (Fine-tuned models) // Get logits of the data by generation for do for Xt do ˆyx {y1, . . . , ym, eos} (x; θt) ℓ(t, x) Logits(ˆyx , (x, θt)) end for end for // Compute coefficients Γi 1 for each epoch do i (Init. of coefficients) for tXt do // Batch sampling for do ℓ(f, x) Logits(ˆyx , (x, fΓ)) end for LΓ 0 for each do for do // Choose right task // Compute pointwise Loss LΓ LΓ + D(ℓ(t, x)ℓ(f, x)) end for end for Γ Γ ΓLΓ // Gradient update end for end for return Γ Baselines. Since we propose new merging method, we compare our method with existing ones that are widely used such as model averaging (Wortsman et al., 2022), Multi-SLERP (Goddard et al., 2024), TIES (Yadav et al., 2023), and AdaMerging (Yang et al., 2024d) in its two variants: Task Level and Layer Level. For TIES, we followed the recommended recipe from (Yadav et al., 2023). For Multi-SLERP, the weights associated to each tasks were set to 1 . For optimization-based methods (AdaMerging and ours), for each method we used the same hyper-parameters across all merging experiments, with batch size of 4 for each iteration. In Table 9 we provide more training details."
        },
        {
            "title": "5.1 Divergence justification",
            "content": "As stated in Sec. 3, our method is based on computing divergences between the merged model and the various fine-tuned ones. Before applying this method, we decided to illustrate an interesting re6 ask (θi) KL JS CoLA SST-2 QQP QNLI MNLI RTE MRPC Avg. 0.812 0.920 0.313 0.774 0.947 0.769 0.877 0.925 0.887 0.340 0.796 0.990 0.915 0.875 0.773 0.818 2: Spearmans beTable {DXi (θiθj), } tween and {PERF(θj, i), } for all (The negation sign is added to have positive correlations). The \"Avg.\" row reports the mean correlation across all tasks. correlation sult: divergence and performance are well correlated. Given set of tasks , for each pair of tasks (i, j) , we compute DXi(θiθj) on development set, and the performance of the model θj on task on test set, denoted as PERF(θj, i). Then, for all tasks we compute the correlation between {DXi(θiθj), } and {PERF(θj, i), }. We report Spearmans correlations in Table 2 for classification tasks. We clearly observe that we obtain high correlations indicating that KL and JS are interesting proxies for performance: if DXi(θiθj) is low, it may suggest that PERF(θj, i) will be high (without stating that this is causation relation). We also observe that the JS divergence achieves the highest correlation; therefore, unless stated otherwise, we use the JS divergence criterion in our experiments. Although the JS divergence consistently outperforms KL divergence, the difference is not always significant. Since our method involves minimizing (cid:80)n n), it thus can be viewed as aiming for merging model that performs well on each task. This aligns directly with Proposition 2, which states that our method is an approximation of the classical multi-task learning objective. For more details, we refer to Sec. B.2. i=1 DXi(θiΦΓ"
        },
        {
            "title": "5.2 Effective Model Merging",
            "content": "We first illustrate our method in pairwise model merging setup i.e. we merge only two tasks at time and compute ANP as defined in Eq. 5. We computed this metric for every possible pairwise merging experiment and for each task types (classification and generation). For example, for classification tasks, we have 7 distinct tasks, meaning that we can perform (cid:0)7 (cid:1) = 21 pairwise merging 2 experiments and thus compute 21 distinct values Merging Method Classif. Gen. Model Averaging Multi-SLERP TIES TL Adamerging LL Adamerging TL KL (ours) LL KL (ours) TL JS (ours) LL JS (ours) 88.48 ( 3.17) 91.54 ( 2.98) 94.06 ( 1.81) 93.62 ( 2.53) 94.06 ( 2.95) 97.68 ( 1.94) 99.16 ( 0.50) 97.73 ( 2.01) 99.18 ( 0.51) 94.38 ( 2.6) 76.39 ( 21.04) 95.53 ( 4.44) 93.42 ( 10.08) 83.20 ( 9.94) 93.97 ( 3.46) 97.50 ( 1.73) 97.29 ( 1.94) 98.93 ( 1.05) Table 3: Average ANP (%) for various merging methods across GLUE and T5 task pairs. The best results per benchmark are boldfaced, and the second best is underlined. See Table 6 and Table 7 in the appendix for more detailed results. of ANP (for generation tasks we have (cid:0)4 (cid:1) = 6). 2 Table 3 presents the average ANP metric across all pairwise merging experiment. We can clearly observe that our method achieves the best average performance. Moreover we can see that the Layer Level variant outperforms the Task Level variant, which is expected since the former has greater number of merging coefficients (one for each layer of each task), giving higher degree of granularity."
        },
        {
            "title": "5.3 Robustness to Number of Tasks",
            "content": "A major limitation of existing merging methods, as noted in (Yadav et al., 2023), is their lack of robustness as the number of merged models increases. We thus decided to empirically assess whether our method is robust to an increasing number of merged tasks by varying this number in our experiments and testing all possible combinations of tasks. For example, in the classification setup, we have 7 different tasks. We test our merging method by merging between 2 and 7 tasks. Moreover, to ensure the reliability of our conclusions, we follow the same procedure as before: for each number of merged tasks, we perform all possible merging combinations. For instance, when merging three tasks, we consider all possible combinations, i.e., (cid:0)7 (cid:1) = 35 merging experiments. We then 3 compute the average ANP metric. In this analysis, we focus on the use of the JS divergence, as it demonstrated slightly better results. Figure 2 shows the average ANP metric across all possible merging experiments as function of the number of tasks, for both the classification setup (Figure 2a) and the generation setup (Figure 2b). First, we observe that regardless of the used method, increasing the number of tasks generally leads to degrada- (a) Classification (b) Generation Figure 2: Evolution of the average ANP metric as function of the number of merged tasks. For each number of tasks on the x-axis, several merging experiments were conducted ((cid:0)n (cid:1) in total), and we report the 95% confidence interval. tion in the average ANP, which is consistent with the fact that task interference becomes more apparent. We can also observe that, regardless of the number of merged tasks, our method (both tasklevel and layer-level) consistently provides better results, with curves that remain higher throughout the graph. Moreover, the drop in performance as the number of tasks increases is less pronounced for our method, illustrating its robustness with respect to the number of tasks. In addition to the average ANP, Figure 2 presents confidence intervals (CI) across experiments for various tasks with fixed number of tasks to merge. Our method demonstrates strong stability, both in classification and generation tasks. For generation tasks, some SOTA methods show large, overlapping confidence intervals, so we re7 Merging Method 2 Tasks 3 Tasks Model Averaging Multi-SLERP TIES TL Adamerging LL Adamerging TL JS (ours) LL JS (ours) 2.60 2.96 21.04 27.97 4.44 19.01 10.08 23.94 19.03 9.94 1.94 1.05 5.37 1.09 Table 4: Confidence interval (CI) margins for different merging methods when merging 2 or 3 tasks. For each method, the margin is measured across multiple merging experiments performed on different task combinations. port CI margins in Table 4. Notably, our method exhibits greater stability compared to Adamerging, another optimization-based approach. 5.4 Method Behaviour Analysis In this section, we analyse our method in greater depth by examining its convergence behaviour. We focus here exclusively on classification tasks. Performance Convergence. Since our method is data-driven, it requires training procedure, which we described in Algorithm 1. We decided to further investigate its training dynamics by studying the evolution of the ANP metric over different training iterations. Here, we focus on pairwise merging, and Figure 3 shows the evolution of the ANP metric across training iterations for both the Task Level and Layer Level variants. First we can notice once again that Task level and Layer level provide similar results which is in line with our previous findings. Then, in all cases, the merging process converges smoothly without signs of over-fitting (i.e., sudden drop in performance), indicating that the proposed methods are effective and stable, consistently merging task-specific representations over training iterations. Dataset Size Influence. As described in Sec. 3, when merging set of task vectors {τi}, we require some data derived from {Xi}. natural and important question is how much data is needed for our method to achieve strong performance. To investigate this, we studied the evolution of the ANP metric as function of the amount of data used by our method. For this experiment, we focused on classification tasks and considered three merging scenarios: (CoLA, SST-2), (QNLI, MNLI), and (RTE, MRPC). In Figure 4, we plot the evolution of the ANP metric. We observe that our method Figure 3: ANP metric of merged task pairs with Task Level and Layer Level JS Divergence as function of training iterations. Figure 4: Impact of dataset size on the performance of our approach compared to data-free baselines. The average ANP is computed across three task pairs. outperforms state-of-the-art methods with as few as 25 samples, which corresponds to only 0.4% of the training corpus used for fine-tuning the merged models and 5% of the validation dataset."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we propose new, data-driven but reference free, merging method which consists in finding the probabilistic centroid of fine-tuned models in order to produce multi-task model. After showing that, theoretically, our method is directly linked to multi-task learning and the concept of weight disentanglement, we demonstrate, empirically, that our method consistently outperforms most state-of-the-art methods on pairwise model merging set-up. Furthermore, we show that our method seems to better handle interference issues considering that it is the best one when the number of merged tasks increases. Finally, we show that our method has high training stability and requires relatively small amount of data to work."
        },
        {
            "title": "References",
            "content": "Despite interesting results, our method present several limitations that we tempt to address here. Shun-ichi Amari and Hiroshi Nagaoka. 2000. Methods of information geometry, volume 191. American Mathematical Soc. Other fine-tuning methods. Our method has been extensively tested when the specialized models were constructed using full-finetuning. In this setup, the task vectors are sparse, and as consequence, the interference problem is more limited. However, in low rank adaptation (LoRA) (Hu et al., 2022) fine-tuning, task vectors (i.e., LoRA matrices) affect the task arithmetic paradigm and are responsible for significant performance loss when merging. limitation of our work is that we have not experimented within this constrained setup. Dataset influence. Our method assumes that for each task t, we have access to sample of the distribution PXt corresponding to the input data for task t. However, in some setups, we may not have access to such distribution, but only to an approximation of it (P Xt ). We have not addressed this case here. On the other hand, we propose an initial theoretical analysis of such case in Sec. A.3. We believe that pushing in this direction will provide more robust results for model merging. Dana Ballard. 1987. Modular learning in neural netIn Proceedings of the sixth National conworks. ference on Artificial intelligence-Volume 1, pages 279284. J. Baxter. 2000. Model of Inductive Bias Learning. Journal of Artificial Intelligence Research, 12:149 198. Shai Ben-David and Reba Schuller. 2003. Exploiting Task Relatedness for Multiple Task Learning. In Gerhard Goos, Juris Hartmanis, Jan Van Leeuwen, Bernhard Schölkopf, and Manfred K. Warmuth, editors, Learning Theory and Kernel Machines, volume 2777, pages 567580. Springer Berlin Heidelberg, Berlin, Heidelberg. Yochai Blau and Tomer Michaeli. 2018. The perceptiondistortion tradeoff. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 62286237. Yochai Blau and Tomer Michaeli. 2019. Rethinking lossy compression: The rate-distortion-perception tradeoff. In International Conference on Machine Learning, pages 675685. PMLR. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and 1 others. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901. Rich Caruana. 1997. Multitask Learning. Machine Learning, 28(1):4175. Alexandra Chronopoulou, Jonas Pfeiffer, Joshua Maynez, Xinyi Wang, Sebastian Ruder, and Priyanka Agrawal. 2023. Language and Task Arithmetic with Parameter-Efficient Layers for Zero-Shot Summarization. Preprint, arXiv:2311.09344. T. M. Cover and J. A. Thomas. 1991. Elements of Information Theory. John Wiley & Sons, Inc. Imre Csiszár. 1975. I-divergence geometry of probability distributions and minimization problems. The annals of probability, pages 146158. Thomas G. Dietterich. 2000. Ensemble methods in machine learning. In Multiple Classifier Systems, pages 115, Berlin, Heidelberg. Springer Berlin Heidelberg. Olivier Ferret. 2025. Projeter pour mieux fusionner: une histoire de bandit et de lit. Chris Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan Anil, and Chelsea Finn. 2021. Efficiently Identifying Task Groupings for Multi-Task Learning. In Advances in Neural Information Processing Systems, 9 volume 34, pages 2750327516. Curran Associates, Inc. Loïc Fosse, Frédéric Béchet, Benoît Favre, Géraldine Damnati, Gwénolé Lecorvé, Maxime Darrin, Philippe Formont, and Pablo Piantanida. 2025. Statistical deficiency for task inclusion estimation. arXiv preprint arXiv:2503.05491. Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vlad Karpukhin, Brian Benedict, Mark McQuade, and Jacob Solawetz. 2024. Arcees mergekit: toolkit for merging large language models. arXiv preprint arXiv:2403.13257. Karsten Grove and Hermann Karcher. 1973. How to conjugate 1-close group actions. Mathematische Zeitschrift, 132(1):1120. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, and 1 others. 2022. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3. Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. 2022. Editing models with task arithmetic. arXiv preprint arXiv:2212.04089. Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. 2018. Averaging weights leads to wider optima and better generalization. arXiv preprint arXiv:1803.05407. Young Kyun Jang, Dat Huynh, Ashish Shah, Wen-Kai Chen, and Ser-Nam Lim. 2024. Spherical linear interpolation and text-anchoring for zero-shot composed image retrieval. Preprint, arXiv:2405.00571. Wooseong Jeong and Kuk-Jin Yoon. 2025. Selective task group updates for multi-task optimization. arXiv preprint arXiv:2502.11986. Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, and Pengxiang Cheng. 2022. Dataless knowledge fusion by merging weights of language models. arXiv preprint arXiv:2212.09849. Tushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, and Ashish Sabharwal. 2020. Qasc: dataset for question answering via sentence composition. arXiv:1910.11473v2. Hyoseo Kim, Dongyoon Han, and Junsuk Choe. 2024. Negmerge: Consensual weight negation arXiv preprint for strong machine unlearning. arXiv:2410.05583. Diederik Kingma and Jimmy Ba. 2014. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980. Kevin Kuo, Amrith Setlur, Kartik Srinivas, Aditi Raghunathan, and Virginia Smith. 2025. Exact unlearning of finetuning data via model merging at scale. arXiv preprint arXiv:2504.04626. Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. 2020. CommonGen: constrained text generation challenge for generative commonsense reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 18231840, Online. Association for Computational Linguistics. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142150, Portland, Oregon, USA. Association for Computational Linguistics. Michael Matena and Colin Raffel. 2022. Merging models with fisher-weighted averaging. Advances in Neural Information Processing Systems, 35:17703 17716. Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. 2016. The benefit of multitask representation learning. Journal of Machine Learning Research, 17(81):132. Frank Nielsen. 2020. On generalization of the jensen shannon divergence and the jensenshannon centroid. Entropy, 22(2):221. Guillermo Ortiz-Jimenez, Alessandro Favero, and Pascal Frossard. 2023. Task arithmetic in the tangent space: Improved editing of pre-trained models. Advances in Neural Information Processing Systems, 36:6672766754. Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. 2021. AdapterFusion: Non-Destructive Task Composition for Transfer Learning. Preprint, arXiv:2005.00247. Jonas Pfeiffer, Sebastian Ruder, Ivan Vulic, and Edoardo Maria Ponti. 2023. Modular Deep Learning. Preprint, arXiv:2302.11529. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. 2020. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167. Solomon Kullback and Richard Leibler. 1951. On information and sufficiency. The annals of mathematical statistics, 22(1):7986. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of 10 the 2016 Conference on Empirical Methods in Natural Language Processing, pages 23832392, Austin, Texas. Association for Computational Linguistics. 2022. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. Preprint, arXiv:2203.05482. Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. 2023. Ties-merging: Resolving interference when merging models. Preprint, arXiv:2306.01708. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, and 22 others. 2024a. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115. Enneng Yang, Li Shen, Guibing Guo, Xingwei Wang, Xiaochun Cao, Jie Zhang, and Dacheng Tao. 2024b. Model merging in llms, mllms, and beyond: Methods, theories, applications and opportunities. Preprint, arXiv:2408.07666. Enneng Yang, Li Shen, Guibing Guo, Xingwei Wang, Xiaochun Cao, Jie Zhang, and Dacheng Tao. 2024c. Model merging in llms, mllms, and beyond: Methods, theories, applications and opportunities. arXiv preprint arXiv:2408.07666. Enneng Yang, Zhenyi Wang, Li Shen, Shiwei Liu, Guibing Guo, Xingwei Wang, and Dacheng Tao. 2024d. Adamerging: Adaptive model merging for multi-task learning. Preprint, arXiv:2310.02575. Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. 2024. Language models are super mario: Absorbing abilities from homologous models as free lunch. Preprint, arXiv:2311.03099. Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. 2020. Gradient surgery for multi-task learning. Advances in neural information processing systems, 33:5824 5836. Yuefeng Zhang. 2023. rate-distortion-classification approach for lossy image compression. Digital Signal Processing, 141:104163. Yuyan Zhou, Liang Song, Bingning Wang, and Weipeng Chen. 2024. Metagpt: Merging large language models using model exclusive task arithmetic. arXiv preprint arXiv:2406.11385. Henry Scheffé. 1947. Useful Convergence Theorem for Probability Distributions. The Annals of Mathematical Statistics, 18(3):434438. Zhang Shengyu, Dong Linfeng, Li Xiaoya, Zhang Sen, Sun Xiaofei, Wang Shuhe, Li Jiwei, Runyi Hu, Zhang Instruction Tianwei, Fei Wu, and 1 others. 2023. tuning for large language models: survey. arXiv preprint arXiv:2308.10792. Trevor Standley, Amir Zamir, Dawn Chen, Leonidas Guibas, Jitendra Malik, and Silvio Savarese. 2020. Which tasks should be learned together in multi-task learning? In International conference on machine learning, pages 91209132. PMLR. Charles Stein. 1956. Inadmissibility of the usual estimator for the mean of multivariate normal distribution. In Proceedings of the third Berkeley symposium on mathematical statistics and probability, volume 1: Contributions to the theory of statistics, volume 3, pages 197207. University of California Press. Gilbert Stewart. 1993. On the early history of the singular value decomposition. SIAM review, 35(4):551 566. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. Glue: multi-task benchmark and analysis platform for natural language understanding. Preprint, arXiv:1804.07461. Ke Wang, Nikolaos Dimitriadis, Guillermo OrtizJimenez, François Fleuret, and Pascal Frossard. 2024. Localizing task information for improved model merging and compression. arXiv preprint arXiv:2405.07813. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and 1 others. 2019. Huggingfaces transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771. Andrew KC Wong and Manlai You. 1985. Entropy and distance of random graphs with application to structural pattern recognition. IEEE transactions on pattern analysis and machine intelligence, (5):599 609. Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt."
        },
        {
            "title": "A Theoretical results",
            "content": "A.1 Proof of Proposition 1 Proof. Since the divergences we use are non-negative, we have the following equivalence: (cid:88) t=1 DXt (cid:0)θt ΦΓ (cid:1) = 0 t, DXt (cid:0)θt ΦΓ (cid:1) = 0. Moreover, by the properties of the KL and JS divergences, we have: DXt (cid:0)θt ΦΓ (cid:1) = 0 SXt, (x; θt) = (x; ΦΓ n), where the last equality is understood in the sense of equality of measures. By transitivity, we obtain: (cid:88) t=1 DXt (cid:0)θt ΦΓ (cid:1) = 0 t, SXt, (x; θt) = (x; ΦΓ n), which concludes the proof. Remark 4. In this demonstration, we stated that this was due thanks to some properties of the KL or JS divergence. However, we have the same result if we use any -divergence, any divergence than can be expressed as following, Df (µν) = (cid:90) (cid:19) (cid:18) dµ dν dν, which is of course the case of the Jensen Shannon and the Kullback ones. In fact this proof is valid for any divergence which satisfies the following property, A.2 Proof of Proposition 2 Df (µν) = 0 µ = ν Definition 2 (Multi task objective). Let {(Xt, Yt) } be set of tasks, the cross-entropy loss function, and (; θ) model parameterized by θ. We define the multi-task loss function as follows: LMT(θ) 1 (cid:88) tT (cid:0)PYtXt, (Xt; θ)(cid:1) , Lemma 1. Let (Xt, Yt) be task, the cross-entropy loss function, and (; θ) model parameterized by θ. Then, the following relation holds: H(PYtXt, (Xt; θ)) = H(YtXt) + KL(PYtXtM (Xt; θ)), where denotes Shannons entropy. We now provide the proof of Proposition 2: Proof. By hypothesis, we have By Lemma 1, this is equivalent to θt = arg min θ H(PYtXt, (Xt; θ)). θt = arg min θ KL(PYtXtM (Xt; θ)). Thus, (Xt; θt) is the moment projection (M-projection) (Csiszár, 1975) of PYtXt onto the set {M (Xt; θ) θ Rd}. Based on this, we define the M-projection multi-task objective as follows: MT(θ) 1 LM (cid:88) tT (M (Xt; θt), (Xt; θ)) . Again, by Lemma 1, we have arg min θ This concludes the proof. LM MT(θ) = arg min θ KL (M (Xt; θt)M (Xt; θ)) . 12 A.3 Distribution shift The objective function we proposed in Eq. 4 supposed that for each task we have access to the input data distribution denoted as PXt. However, in some cases we can have no access to PXt but to an approximation of it, denoted as Xt . For example, we have model trained on sentiment analysis and we do not have access the true data. We can thus use existing data for such task as an approximation. We show in the following that we can in fact control the behaviour of our method with respect to the quality of the approximation. Proposition 3. Considering set of approximated distribution (cid:110) Xt converge in uniform way with (cid:111) . (cid:110) Xt (cid:111) , for = JS, our method will Proof. We recall that in Eq. 4 for given task we have the following, DXt (cid:0)θt ΦΓ (cid:1) = (cid:90) (cid:0)θt(.x) ΦΓ n(.x)(cid:1) PXt(dx). Then, (cid:12) (cid:12) (cid:12)DXt (cid:0)θt ΦΓ (cid:1) Xt (cid:0)θt ΦΓ (cid:1)(cid:12) (cid:12) (cid:12) = (cid:12) (cid:90) (cid:12) (cid:12) (cid:12) (cid:90) If we use the Jensen Shannon divergence we then have, (cid:0)θt(.x) ΦΓ (cid:0)θt(.x) ΦΓ n(.x)(cid:1) (PXt(dx) Xt n(.x)(cid:1) (cid:12) (cid:12) (cid:12)PXt(dx) Xt (cid:12) (cid:12) (dx)) (cid:12) (cid:12) (cid:12) (cid:12) (dx) (cid:12) (cid:12) (cid:12) (cid:12)JSXt (cid:0)θt ΦΓ (cid:1) JS Xt (cid:0)θt ΦΓ (cid:1)(cid:12) (cid:12) (cid:12) log(2) (cid:90) (cid:12) (cid:12) (cid:12)PXt(dx) Xt (cid:12) (cid:12) (dx) (cid:12) Then by Scheffes Theorem (Scheffé, 1947), we have: JSXt (cid:0)θt ΦΓ (cid:1) JS Xt (cid:0)θt ΦΓ (cid:1) 2 log(2)TV(PXt, Xt ), where TV stands for total variation distance. Then we have, (cid:16) (cid:12) (cid:12) (cid:88) (cid:12) (cid:12) (cid:12) t"
        },
        {
            "title": "DXt",
            "content": "(cid:0)θt ΦΓ (cid:1) Xt (cid:0)θt ΦΓ (cid:1)(cid:17) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 2 log(2) (cid:88) TV(PXt, Xt ), which concludes the proof. Remark 5. In Proposition 3 we state that the convergences is uniform in the sense that if the approximations we have converge uniformally to the true distribution i.e. in the sense of the total variation, then we have convergence of our objective function."
        },
        {
            "title": "B Divergence Details",
            "content": "B.1 Divergence Between Models on Sequence Outputs In this section, we give more details on the computation of divergence between autoregressive models. In this study, we recall that we defined the divergence between two LMs M1 and M2 as following, DX (M1M2) EX (cid:2)D(cid:0)M1( X)M2( X)(cid:1)(cid:3) . Since model prompted with input generates sequence of symbols in an auto-regressive set-up, we propose here to detail more the way divergence is computed between models. For each input in X, we generate the next tokens in greedy manner using the reference model M1, while storing the softened logits (probability distributions) of the generated tokens at each step, until the end-of-sequence (EOS) token is produced. We then append the generated token sequence to the original input x, forming (a) KL (b) JS Figure 5: DXi(θiθj) values for different divergences. (i corresponds to the row index, while corresponds to the column index.) the extended sequence + y. Next, we forward propagate this extended sequence through the second model M2 once, and obtain the probability distributions of the tokens generated by M1. This allows us to compare the token-level distributions of M1 and M2 on the same generated sequence. Using this procedure, we can measure the divergence DX (M1M2) over sequences generated by M1. In more formal way we propose to compute divergence in recurrent. Let be sequence, be an index, and we suppose that we can sample D(cid:0)M1( x, y<t)M2( x, y<t)(cid:1). Then the final divergence is given by, 1 (cid:88) x"
        },
        {
            "title": "1\nTx",
            "content": "Tx(cid:88) t=1 D(cid:0)M1( x, y<t)M2( x, y<t)(cid:1), Where for each x, Tx is the maximum number of token to generate before the end of sequence token (it can be viewed as some sort of stopping time). For greater accuracy, this calculation should be performed as follows, 1 (cid:88) x"
        },
        {
            "title": "1\nTx",
            "content": "Tx(cid:88) (cid:88) t=1 y<tM1(x) D(cid:0)M1( x, y<t)M2( x, y<t)(cid:1), where the sum over y<t M1( x), would correspond to sampling procedure of sequences of size less than with respect to the model M1( x). In our work, for sake of simplicity we stick to some greedy procedure. B.2 Correlation between Divergence Variants and Model Relatedness. In Sec. 5.1, we proposed an experiment to investigate links between the divergences we used and the notion of performance on the different tasks. In Figure 5, we propose the heat map defined by DXi(θiθj), and in Table 5, we proposed the matrix of values PERF(θj, i). Correlations computed in Table 2 in this study, correspond to correlations compute between rows of Figure 5 and rows of Table 5."
        },
        {
            "title": "C Additional experiments",
            "content": "C.1 Task Vectors Cosine Similarities As is well known in the model merging literature, and more specifically within the task arithmetic framework, cosine similarities between task vectors are typically close to zero. This indicates that the 14 Eval Dataset CoLA SST-2 QQP QNLI MNLI Model RTE MRPC CoLA SST-2 QQP QNLI MNLI RTE MRPC 82.20 44.00 33.00 46.20 33.80 34.80 32.80 77.80 92.80 50.20 76.40 61.40 48.20 57.40 50.60 71.20 84.60 39.20 65.40 63.60 66.20 44.60 37.00 40.80 86.40 45.00 50.20 41.20 8.80 10.20 8.60 9.20 78.00 14.00 11.60 28.88 26.71 28.88 43.68 28.16 75.81 45. 70.00 66.75 69.75 69.00 67.50 69.00 85.00 Table 5: Accuracies (%) of each model checkpoint (rows) evaluated on the seven GLUE tasks (columns). Each row corresponds to model fine-tuned on specific task. The highest accuracy for each task is highlighted in bold, and corresponds each time to the specialized model. Figure 6: Cosine similarity matrices of task vectors between different tasks. Left: Similarity between GLUE benchmark tasks (CoLA, SST2, QQP, QNLI, MNLI, RTE, MRPC). Right: Similarity between diverse generative tasks tasks (IMDB, QASC, SQUAD, commonGen). Lower similarity values indicate greater orthogonality between task vectors, suggesting less interference when merging models fine-tuned on these tasks. tasks are sufficiently disentangled and can be effectively merged using task arithmetic methods. In Figure 6, we present the cosine similarity matrices of the task vectors used in our experiments. C.2 Details on Figure 2 On Table 6, we propose the values that are plotted on Figure 2a, and on Table 7 we propose the values that are plotted on Figure 2b. C.3 Parameter convergence In Sec. 5 we provided an analysis of the convergence of our method by displaying the evolution of our loss function through training iterations and we concluded that our method smoothly converges to an local optimum value. We decided to go further and analyse the evolution of the coefficients associated to each task. As recall we used the framework of task arithmetic and in this framework the merged model is given by the following, (cid:88) θ0 + Γi τi, and we are here interested into the evolution of the coefficients Γi. In Figure 7, we provide the evolution of Γ1 (left) and Γ2 (right) through training iterations, on different pairwise merging set-up on the benchmark GLUE. We can mainly observe that the dynamic of our method is also smooth in the coefficients Γi, with an interesting convergence of the parameters. We can also go further by observing in some settings 15 # Tasks Model Averaging Multi-SLERP TIES Task Level Layer Level Adamerging KL (ours) JS (ours) Adamerging KL (ours) JS (ours) 2 3 4 5 6 7 Average 93.89 89.10 75.12 60.92 56.98 60. 72.75 94.20 92.18 79.31 66.86 70.48 68.89 78.65 96.02 94.08 79.37 73.61 67.23 68.39 79.78 91.16 90.86 78.73 66.84 67.97 67. 77.14 98.18 97.40 80.12 83.82 83.45 83.45 87.73 98.28 97.39 79.21 85.37 84.45 84.70 88.23 92.41 90.26 78.47 64.93 62.85 63. 75.33 98.85 98.42 95.20 95.60 93.11 92.53 95.62 98.85 98.26 95.19 95.50 93.42 93.06 95.71 Table 6: ANP for merged tasks obtained via different merging methods. Values are normalized as percentages, with separate evaluations for KL and JS Divergence variants. # Tasks Model Averaging Multi-SLERP TIES Task Level Layer Level Adamerging Forward (ours) JS (ours) Adamerging Forward (ours) JS (ours) 2 3 4 Average 97.92 82.62 53. 77.97 98.96 67.89 64.85 77.23 98.43 87.87 56.42 80.91 99.74 79.92 60. 80.18 96.34 92.99 87.52 92.28 98.96 96.36 91.75 95.69 100.00 89.26 83. 90.80 98.96 95.91 94.97 96.61 99.48 97.79 97.44 98.24 Table 7: ANP for merged tasks obtained via different merging methods. Values are normalized as percentages, with separate evaluations for KL and JS Divergence variants. that the values of Γ1 and Γ2 seem to be independent meaning that when merging two tasks the merging coefficient associated to one task seems to strongly depend on the task itself and not the task with which we merge. To better support this fact, we decided to add visualization. In the framework of task arithmetic, each merging experiment can be represented by point in an euclidean space defined by the following coordinates (Γ1, Γ2, . . . , Γn)t. In the case of pairwise merging experiments, these points are in plan and we decided to visualize this plan on Figure 8, for classification tasks, and Figure 9 for generative tasks. On these figures, we can mainly observe that we have different scenarios. For tasks such as QNLI, the factor associated with the QNLI task seems not to depend on the other tasks, while for some other tasks such as MRPC and CoLA we have another scenario where the value of the coefficient associated to the task seems to depend on the value associated to the other tasks. This seems to be an interesting observation, to be considered alongside the fact that some tasks may be independent, while others may have statistical dependency, i.e., completing one task may have positive or negative impact on another."
        },
        {
            "title": "D Training Settings",
            "content": "D.1 Data details As explained in Sec. 4, we used the GLUE Benchmark (Wang et al., 2019) to perform our experiments. We recall on Table 8 the description of tasks from this benchmark. natural language inference CoLA detection of the linguistic acceptability of sentence MNLI MRPC paraphrase detection QNLI QQP RTE SST2 question answering converted into natural language inference detection of equivalence between questions natural language inference sentiment analysis Table 8: Description of the GLUE Benchmark 16 Figure 7: Evolution of the task coefficients across training iterations. The first graph shows the coefficient assigned to the first task in each task pair (as indicated in the legend), while the second graph shows the coefficient assigned to the second task."
        },
        {
            "title": "Method",
            "content": "Level Batch Size Epochs Dataset Size Scheduler (LR) Init. Param. KL/JS (Classif) Task KL/JS (Classif) Layer KL/JS (Gen.) Task KL/JS (Gen.) Layer Task AdaMerging (Classif) AdaMerging (Classif) Layer Task AdaMerging (Gen.) Layer AdaMerging (Gen.) 4 # tasks 4 # tasks 4 # tasks 4 # tasks 4 # tasks 4 # tasks 4 # tasks 4 # tasks 4 4 4 4 5 5 5 5 200 400 200 400 200 400 200 Table 9: Training configurations. 1e2 1e2 1e2 1e2 1e3 1e3 1e2 1e2 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 D.2 Training details We propose in Table 9 training hyper-parameters we chose for our method, as well as for the Adamerging one since it also requires training procedure. All the optimizations were done using the Adam optimizer (Kingma and Ba, 2014) with default moments hyper-parameters. From practical standpoint, the hyperparameters we choose, both for the Adamerging method and for our own, allow us to maximize multi-task performance on evaluation sets. While the choice of hyperparameters is relatively sensitive in the Adamerging method, our method appears to be more robust in terms of hyperparameter selection. The other methods we used that had hyper-parameters were TIES and Multi-SLERP, for which we basically used the recommended recipes: TIES: We used the recommended recipe from (Yadav et al., 2023), with λ = 1 and mask rate of 0. (i.e., 80% zeros in the mask). Multi-SLERP: The weights were set to 1/N , where is the number of tasks."
        },
        {
            "title": "E Everything is task arithmetic",
            "content": "Many different merging methods have emerged in the landscape of machine learning. Among them, task arithmetic (Ilharco et al., 2022) is probably the most widely used. As reminder, the merging function in 17 Figure 8: Visualization of coefficient values for fixed reference task versus coefficient values for the remaining GLUE tasks. Each subplot corresponds to different reference task. the case of task arithmetic is defined as follows: (θ0, {τt} , Γ) = θ0 + (cid:88) Γt τt. An interesting question that naturally arises is the following: Given merging method (θ0, {τt}, ), can we find coefficients Γ such that (θ0, {τt}, ) = (θ0, {τt}, Γ)? If this is true, then we can state that min Γ (cid:88) DXt (θt (θ0, {τt}, Γ)) min (cid:88) DXt (θt g(θ0, {τt}, )) . This inequality would highlight the strength of our method, as it encompasses the entire range of task arithmetic. As stated in Remark 3, our method can be applied to any hyper-parameter differentiable merging approach. As pointed out in (Goddard et al., 2024), wide range of merging methods are based on task arithmetic, with the main differences lying in the estimation of the merging coefficients. In the following, we provide an analysis of other merging methods to show that they can be expressed as model merging. This demonstrates that our method has the potential to achieve better results. SLERP. Spherical linear interpolation (SLERP) (Wortsman et al., 2022) is classical method used to combine vectors on spherical manifold. For this method, we introduce hyperparameter [0, 1], and define SLERP as follows: (θ0, {τ1, τ2}, t) θ0 + sin((1 t)Ω) sin Ω τ1 τ1 + sin(tΩ) sin Ω τ2 τ2 , where Ω is the angle between τ1 and τ2. Fisher Weight Averaging. The Fisher weight averaging method, introduced in (Matena and Raffel, 2022), is merging technique that reduces to task arithmetic in the case of linear interpolation between specialized models. The merging coefficients for each model are based on the Fisher Information matrix 18 Figure 9: Coefficient values for each task at different T5 checkpoints. Each plot fixes reference task and compares its coefficient to those of the other tasks. and are determined by solving an optimization problem related to finding centroid between models. One limitation, as pointed out in the original paper, is the high computational cost of estimating the Fisher Information matrix to obtain the merging coefficients. This estimation can also be numerically unstable, as the coefficients in the matrix can be close to zero. Additionally, this method introduces extra scaling hyperparameters that must be tuned. RegMean. The RegMean merging method, proposed in (Jin et al., 2022), also reduces to task arithmetic, as it performs linear interpolation between specialized models. This interpolation aims to minimize the L2 distance between the merged model and the individual models, whereas our method is designed to minimize the JS (or KL) divergence between models. The L2 distance is restrictive measure. Moreover, as stated in (Blau and Michaeli, 2018, 2019; Zhang, 2023), L2 distance is distortion measure, while KL and JS are perception measures. Minimizing perception distance appears to be more suitable for downstream applications, such as performing other tasks. Kracher Mean. The Kracher mean (or Riemannian centroid), originally formulated in (Grove and Karcher, 1973) can be used as merging method which consists in finding some sort of centroid of finite set of task vectors, denoted as {τt}. To do so, we suppose that task vectors lies in Finite dimension Hilbert Space (H, < , >), where < , > is the standard dot product onto this space and thus is the associated norm. The Kracher mean is defined as following, τF arg min τ (cid:88) τ τt2. The following proposition holds, Proposition 4. Let (H, < , >) be finite dimension Hilbert space. Then for all set of point {τt} H, representing task vectors, the solution of the Kracher mean (or equivalently the centroid) can be expressed in the task arithmetic framework. Proof. Let {τt} H. Let Span ({τt}). Let τ H. We have the following result, τ = p1 + p2, s.t. p1 F, p2 . 19 Then we have, ψ(τ ) (cid:88) τ τi2, = = = (cid:88) (cid:88) (cid:88) < τ τt, τ τt >, τ 2 2 < τ, τi > +τi2, p12 + p22 2 < p1, τi > +τi2. Then by taking, τ = p1, we have ψ(τ ) ψ(τ ), which leads to the following statement: τ H, τ , such that, Then arg minτ ψ(τ ) , which concludes the proof. ψ(τ ) ψ(τ ). Remark 6. As stated in Remark 2, the method we proposed in this study can also be viewed as centroid. However the framework we used does not allow to connect directly to the theory of Kracher mean. In fact, if one would want to formulate our method as Kracher mean, the \"distance\" defined over the space of task vectors would be the following, d(τi, τj) = DXi (θiθj) . However, even in the case of the Jensen Shannon divergence this \"distance\" is not mathematical one as it does not respect the property of the distance. Consequently it does not define metric space and therefore even less Hilbert space. Then an interesting line of research would be to identify the possible distances one could define over the space of task vectors. From the result we just demonstrated, if we can verify that the distance can be derived from dot product and thus induce Hilbert Space, then we can conclude that the optimal solution lies in the framework of task arithmetic, giving thus added weight to this method and possibly offering more theoretical explanations as to why this method is in many cases the state of the art. new distance. We would like to provide first sketch of answer to questions raised in Remark 6. Based on relaxation of the total variation distance (TV), we can give first distance between task vectors, which, as our method, is based on probability distances between language models. First we can notice that for every sequence (In the following will denote the space of sequences language model can take as input), language model prompted with x, denoted as ( x, θ) is discrete probability measure on , meaning that it is dominated by the counting measure, that we denote by λ. From this we can propose the following form between two task vectors (it is form since the results of the following operation is in R), < τi, τj > sup xX (cid:90) yX (cid:18) dM (yx, θi) dλ dM (yx, θj) dλ (cid:19) λ(dy), where dM (yx,θi) dλ dλ denotes the Radon Nikodym derivative with respect to the measure λ. In more concrete way, , denotes the probability that the sequence follows the sequence accordingly to the model ( , θi), which is thus the probability given by the language model. Since the Radon Nikodym is linear operator, if we add the linearity hypothesis defined in (Ortiz-Jimenez et al., 2023) and defended in (Zhou et al., 2024) we can clearly state that < τi, τj > defines scalar dot over the space of task vectors. The associated distance will thus be, d2(τi, τj) =< τi τj, τi τj > (cid:90) (cid:18) dM (yx, θi) dλ = sup xX dM (yx, θj) dλ (cid:19) λ(dy), 20 which is L2 approximation of the total variation distance between ( , θi) and ( , θj), in the sense that, TV (M ( , θi), ( , θj)) = sup xX (cid:90) (cid:12) (cid:12) (cid:12) (cid:12) dM (yx, θi) dλ dM (yx, θj) dλ (cid:12) (cid:12) (cid:12) (cid:12) λ(dy) In this case of L2 approximation, following Proposition 4, the Kracher mean is thus given by task arithmetic. The direct use of the total variation distance would not work in that case since it is well known result that the L1 norm cannot be derived from any scalar dot and thus it cant define Hilbert Space."
        }
    ],
    "affiliations": [
        "CNRS, LIS, Aix Marseille Université, France",
        "Orange Research, Lannion, France",
        "École polytechnique, Institut polytechnique de Paris, Palaiseau, France"
    ]
}