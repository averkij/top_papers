{
    "paper_title": "Test-Time Scaling with Reflective Generative Model",
    "authors": [
        "Zixiao Wang",
        "Yuxin Wang",
        "Xiaorui Wang",
        "Mengting Xing",
        "Jie Gao",
        "Jianjun Xu",
        "Guangcan Liu",
        "Chenhui Jin",
        "Zhuo Wang",
        "Shengzhuo Zhang",
        "Hongtao Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce our first reflective generative model MetaStone-S1, which obtains OpenAI o3-mini's performance via the new Reflective Generative Form. The new form focuses on high-quality reasoning trajectory selection and contains two novelties: 1) A unified interface for policy and process reward model: we share the backbone network and use task-specific heads for reasoning trajectory predicting and scoring respectively, introducing only 53M extra parameters for trajectory scoring. 2) Eliminating the reliance on process-level annotation: we provide a self-supervised process reward model, which can directly learn the high-quality reasoning trajectory selection from the outcome reward. Equipped with the reflective generative form, MetaStone-S1 is naturally suitable for test-time scaling, and we provide three reasoning effort modes (low, medium, and high) based on the controllable thinking length. Experiments demonstrate that our MetaStone-S1 achieves comparable performance to OpenAI o3-mini's series with only 32B parameter size. To support the research community, we have open-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1."
        },
        {
            "title": "Start",
            "content": "Test-Time Scaling with Reflective Generative Model Zixiao Wang2*, Yuxin Wang1*, Xiaorui Wang1, Mengting Xing1, Jie Gao1, Jianjun Xu2, Guangcan Liu1, Chenhui Jin2, Zhuo Wang2, Shengzhuo Zhang1, Hongtao Xie2 MetaStone-AI1 & USTC"
        },
        {
            "title": "Abstract",
            "content": "We introduce our first reflective generative model MetaStone-S1, which obtains OpenAI o3minis performance via the new Reflective Generative Form. The new form focuses on highquality reasoning trajectory selection and contains two novelties: 1) unified interface for policy and process reward model: we share the backbone network and use task-specific heads for reasoning trajectory predicting and scoring respectively, introducing only 53M extra parameters for trajectory scoring. 2) Eliminating the reliance on process-level annotation: we provide self-supervised process reward model, which can directly learn the high-quality reasoning trajectory selection from the outcome reward. Equipped with the reflective generative form, MetaStone-S1 is naturally suitable for test-time scaling, and we provide three reasoning effort modes (low, medium, and high) based on the controllable thinking length. Experiments demonstrate that our MetaStone-S1 achieves comparable performance to OpenAI o3-minis series with only 32B parameter size. To support the research community, we have open-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1. 5 2 0 2 9 ] . [ 2 1 5 9 1 0 . 7 0 5 2 : r Figure 1 Benchmark performance of MetaStone-S1. *Equal Contribution. Corresponding author. 1. Introduction Over the past two years, the field of Large Language Models (LLMs) has experienced rapid advancements, marked by the emergence of increasingly sophisticated models. Notable developments include OpenAIs GPT-4, Googles Gemini, Metas LLaMA series, Alibabas Qwen, and DeepSeeks R1, which have collectively pushed the boundaries of natural language understanding and generation. This progress is attributed to innovations in model architectures and training techniques, enabling LLMs to process and generate content across various formats. Recent analyses suggest that OpenAIs o3 model achieves its advanced reasoning and coding capabilities through Test-Time Scaling (TTS) techniques such as massive sampling, candidate scoring, and search over multiple reasoning paths (Labs, 2025; Zeff, 2024). For instance, during ARC-AGI and competitive coding evaluations, o3 was shown to generate up to 1024 candidate samples for each query (Chollet, 2024; OpenAI, 2025). These inference-time strategies mark significant shift from traditional one-pass models, enabling o3 to adapt dynamically to novel tasks and achieve near-human performance in reasoning benchmarks. TTS approaches can be categorized into two types: internal TTS and external TTS. Internal TTS (also called sequential TTS in Zeng et al. (2025)) strategies use CoT for longer thinking processes (Guo et al., 2025; OpenAI, 2024), which benefits from Long-CoT Supervised FineTuning and reinforcement learning. Recent internal TTS methods (Guo et al., 2025) mainly suffer from the false positive reasoning process, as the outcome reward will misclassify the correct answer with incorrect reasoning during the training stage. External TTS (also called parallel TTS in Zeng et al. (2025)) is proposed for selecting the correct reasoning process, which is proved to be more effective in performance boosting compared with outcome reward (Lightman et al., 2023). Prominent external TTS algorithms include Best-of-N sampling, Beam Search, and Diverse Verifier Tree Search, using the verifier such as the Process Reward Model(PRM) to select high-quality reasoning trajectories. For example, Best-of-N firstly generates multiple outputs, and then sequentially uses PRM-based to score and select the best solution. Liu et al. (2025) point out that the best external TTS solution varies from policy model with different parameters. When the parameter of the policy model is less than 32B, the search methods (Beam Search and Diverse Verifier Tree Search) achieve better results than the sampling one(Best-of-N). However, when the parameter size is equal to or greater than 32B, the sampling method can achieve better performance. Based on the above analysis, internal and external TTS are two individual methods and can benefit each other, e.g. using the Best-of-N to boost the Long-CoT model with PRM. This paper focuses on external TTS and proposes new Reflective Generative Form for highquality reasoning trajectory selection. Specially, the proposed new form shares the backbone of the policy model and process reward model, and uses self-supervised training to eliminate the reliance on process-level supervision. Based on the Reflective Generative Form, the proposed MetaStone-S1 contains high, medium, and low reasoning modes with the controllable thinking length. Experiment results show that MetaStone-S1 achieves comparable performance to OpenAI o3-minis series with only 32B parameters. 1.1. Contributions new Reflective Generative Form: We systematically review the existing Test-Time Scaling (TTS) paradigms, and provide clear definition of the Reflective Generative Form for high-quality reasoning trajectory selection. The proposed Reflective Generative Form enables single network to achieve both reasoning trajectory prediction and selection (with Zero process-level annotation). 2 The comprehensive analysis about the aha moment, scaling law and robustness of the new form: We provide both qualitative and quantitative analysis for the aha moment, scaling law, and robustness of the proposed new form. These exhaustive discussions will effectively benefit the community for future research. The State-of-the-art performance: MetaStone-S1 achieves comparable performance as the OpenAI o3-minis series with only 32B parameter. Specially, MetaStone-S1-low outperforms OpenAI o3-mini-low in mathematical(AIME24&25), coding(LiveCodeBench) and Chinese reasoning(C-Eval) tasks respectively. MetaStone-S1-medium obtains similar results to OpenAI o3-mini-medium. Finally, MetaStone-S1-high further improves the intelligence ceiling and achieves SOTA results in series of open-source and closed-source models. 2. Related Works 2.1. Test-Time Scaling Test-Time Scaling (TTS) is technique that leverages additional computational resources at inference time to tackle challenging problems. With the remarkable performance improvements demonstrated by OpenAI o1 (Jaech et al., 2024), TTS has become research hot spot for enhancing the reasoning capabilities of LLMs. TTS can be divided into two categories: internal TTS and external TTS. Internal TTS introduces the long Chain-of-Thought (CoT) to generate answers based on the detailed reasoning process. OpenAI o1(Jaech et al., 2024) and DeepSeek R1(Guo et al., 2025) introduce thinking process to plan the solution and guide the final answer. Jin et al. (2024); Yeo et al. (2025) have shown that long CoT can help models correct mistakes by themselves and decompose complex problems more effectively, thereby improving performance. DeepScaleR(Luo et al., 2025) demonstrates that by carefully extending the context length during training, only 1.5B-parameter model can surpass the o1-Preview. However, Chen et al. (2024a,b) have highlighted the risk of overthinking, where excessively long reasoning trajectories may lead to performance degradation. On the other hand, external TTS scales up inference through search-based strategies and auxiliary reward models. common approach is the Best-of-N strategy (Brown et al., 2024; Lightman et al., 2023; Wang et al., 2023), which generates multiple candidates and selects the best one based on scores from the pretrained reward model. Moreover, fine-grained methods have also been explored, including, such as Beam Search (Liu et al., 2025; Snell et al., 2024), Diverse Verifier Tree Search (Beeching et al.) and Monte Carlo Tree Search (MCTS) (Guan et al., 2025; Luo et al., 2024; Zhang et al., 2024). These methods search at the step level and utilize Process Reward Models (PRMs) to guide the reasoning trajectory step-by-step. Beyond search strategies, recent work emphasizes that the quality of the reward model is crucial factor in external TTS (Guan et al., 2025). straightforward and effective way to enhance models reasoning ability is to develop high-quality reward model. 2.2. Process Reward Model Process Reward Models (PRMs) focus on evaluating LLMs at the step level. Lightman et al. (2023) unveil that this fine-grained guidance can lead to better TTS performance compared with the global-level Outcome Reward Model (ORM). However, accurately identifying logical errors in LLM outputs remains challenging, and PRMs require high-quality task-specific annotated data for training. To this end, recent works Wang et al. (2023) leverage Monte Carlo estimation to automatically assign step-level scores using only the final answers as supervision. Guan et al. (2025); Zhang et al. (2024) iteratively synthesizes data by MCTS and fine-tuning both LLMs 3 and PRMs, improving performance across both models. Tan et al. (2025) follow the LLM-asa-judge method and introduce new LLM to annotate the reward of each step. Nonetheless, Zhang et al. (2025) point out that labels generated by Monte Carlo estimation can be noisy, as incorrect reasoning processes may still yield correct final answers. They further propose hybrid approach that combines both Monte Carlo estimation with the LLM-as-a-judge. Despite these advances, existing PRMs still suffer from several challenges. First, the PRMs are trained on new large-scale LLM model, resulting in significant training and inference costs. Second, most PRM training methods typically follow an off-policy strategy, which limits their ability to directly discriminate outputs generated by the target LLM. The unseen distribution during inference time may further degrade performance. To address these issues, we propose Reflective Generative Form, which shares most parameters between the PRM and the target LLM, and supports on-policy optimization with only outcome rewards, enabling more efficient and aligned training. 3. Problem Formulation This paper aims to find high-quality reasoning trajectory more efficiently at inference time based on TTS. We first summarize the general inference forms for standard LLMs (policy models) and existing TTS methods, and then formally define our proposed Reflective Generative Form. 1) Basic LLMs. The model directly generates an answer based on the input query. This basic inference form can be formulated as: answer = ğ¿ğ¿ğ‘€answer( query ) . (1) TTS based methods can be categorized into two types: sequential scaling based internal TTS and parallel scaling based external TTS. 2) Internal TTS (e.g. DeepSeek R1(Guo et al., 2025)). The internal TTS first generates reasoning trajectory by Long-CoT using ğ¿ğ¿ğ‘€thinking, and then predicts the final answer based on this trajectory using ğ¿ğ¿ğ‘€answer. This procedure can be expressed as: answer = ğ¿ğ¿ğ‘€answer( ğ¿ğ¿ğ‘€thinking( query )) . (2) To be specific, recent methods (e.g. DeepSeek R1(Guo et al., 2025)) use the same policy model for both ğ¿ğ¿ğ‘€thinking and ğ¿ğ¿ğ‘€answer. 3) External TTS (e.g. Lightman et al. (2023); Liu et al. (2025); Zhang et al. (2024)). Firstly, the Long-CoT generation is extended by generating multiple reasoning trajectories and answers in parallel. Then, reward model (e.g. PRM) is used to score and select the best result. This inference form can be described as: answer = arg max ğ¿ğ¿ğ‘€ğ‘ƒğ‘…ğ‘€ 1,ğ‘˜ ğ‘– [ ] (cid:16) ğ¿ğ¿ğ‘€answer( ğ¿ğ¿ğ‘€thinking( [ query ))] ğ‘– , (cid:17) (3) where [] ğ‘– denotes the ğ‘–-th candidate among ğ‘˜ parallel generations. Though existing external TTS methods have been proven to obtain considerable performance enhancement, they still encounter several problems: (1) Extra Computation: PRM contains individual parameters from the policy model(ğ¿ğ¿ğ‘€think and ğ¿ğ¿ğ‘€answer), which introduces additional huge computation. (2) Expensive Annotation: It is difficult to obtain the large-scale reasoning trajectory annotations for PRM training. 4 Figure 2 The training and inference framework of Reflective Generative Models. Reflective Generative Form. To address the extra computation and expensive annotation issues, we propose new Reflective Generative Form focusing on the efficient and label-free reasoning trajectory selection. The proposed Reflective Generative Form is shown in Eq.4. answer = ğ¿ğ¿ğ‘€answer (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) share backbone (cid:16) (cid:124) (cid:123)(cid:122) (cid:125) arg max 1,ğ‘˜ ğ‘– [ ] ğ¿ğ¿ğ‘€ğ‘†ğ‘ƒğ‘…ğ‘€ (cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32) share backbone (cid:16) (cid:123)(cid:122) (cid:125) (cid:124) ğ¿ğ¿ğ‘€thinking (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) [ ( query )] ğ‘– share backbone (cid:17)(cid:17) (4) (cid:124) (cid:123)(cid:122) (cid:125) Firstly, we share the backbone of the policy model and PRM in single network, which enables reasoning trajectory generation and scoring in unified interface for parallel prediction. The score measures the quality of each reasoning trajectory, and the trajectory with higher score is selected as the high-quality candidate in TTS. This unified interface is proved to be effective for parameter reduction in our experiments. Secondly, we introduce novel Self-supervised Process Reward Model (SPRM) to eliminate the reliance on process-level annotation, which can be optimized with only outcome-level annotation in self-supervised manner. In particular, we only implement the SPRM for the ğ¿ğ¿ğ‘€think selection, which can further improve the inference efficiency during the real implementation. 4. Approach 4.1. Unified Interface in Reflective Generative Form Our proposed Reflective Generative Form establishes unified interface for the policy model and the PRM. For the policy model, we employ reasoning LLMs that contain the thinking process in response, delineated by the <think> and </think> tokens. For the PRM, we introduce Self-supervised Process Reward Model (SPRM), which shares the same backbone as the policy 5 model but incorporates an additional lightweight SPRM head. The SPRM head is implemented by binary classifier consisting of two linear layers and dropout layer. An overview of the joint framework is illustrated in Fig. 2(a). Within this unified form, the policy model first generates multiple thinking processes as the reasoning trajectories. Subsequently, the SPRM evaluates each thinking process for reasoning trajectory selection. The evaluation procedure contains two steps: (1) Segmenting the reasoning trajectory into discrete steps and (2) Predicting trajectory score based on evaluation in each step. 1. Step Segmentation. We segment each reasoning trajectory using tokens that are already supported by the policy models tokenizer, eliminating the need to introduce additional step-specific tokens or fine-tune the LLM for step-format outputs. Specifically, we treat tokens containing .nn as step-tokens and split the trajectory accordingly. Additionally, we retain only the first token in any sequence of consecutive step-tokens and ignore the step-token appearing at the beginning of the trajectory, as it does not contain substantive solution information. 2. Trajectory Score Prediction. After using step-tokens to mark the end of individual reasoning steps, we evaluate each step based on the representation of the corresponding step-token. Since the representation in the last layer mainly captures the logits prediction for single token, we use the hidden representations from the second-to-last layer of the policy model to provide richer contextual information of the entire step. These representations are then fed into the SPRM head to predict process scores for each step. The final score for the entire reasoning trajectory is computed as the geometric mean of the individual process scores: Sfinal = ğ‘› (cid:32) (cid:214)ğ‘–=1 Scoreğ‘– 1 ğ‘› = (cid:33) ğ‘› (cid:32) (cid:214)ğ‘–= 1 ğ‘› , ğ‘†ğ‘ƒğ‘…ğ‘€ ğ‘“ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘–)(cid:33) ( (5) where ğ‘› denotes the total number of steps, and ğ‘“tokenğ‘– is the representation of the ğ‘–-th step-token obtained from the policy model. Scoreğ‘– is the SPRMs process score for ğ‘–-th step. Through this unified interface, single network can generate reasoning trajectories and score them in parallel, enabling joint training in an end-to-end manner. This design facilitates straightforward and efficient training pipeline for on-policy PRM learning, where both the policy model and the SPRM continuously refine their parameters from shared experiences, thereby improving the overall quality of the generated trajectories. 4.2. Optimization of Reflective Generative Form During optimization, we train the policy model and the SPRM head simultaneously. For optimizing the policy model, we adopt Group Relative Policy Optimization (GRPO) following Shao et al. (2024). To optimize the SPRM head, we propose Self-supervised Process Reward Loss (SPR Loss), which enables learning process discrimination ability only from outcome reward (e.g. final answer correctness). The SPR Loss is formulated as follows, LSPR = 1 ğ‘ ğ‘ ğ‘–=1 ğ‘¤ğ‘– ğµğ¶ğ¸ğ¿ğ‘œğ‘ ğ‘  Scoreğ‘–, ğ‘¦ğ‘–) ( , where ğ‘¤ğ‘– = if ğ‘¦ğ‘– = 1 & Scoreğ‘– > 0.5 if ğ‘¦ğ‘– = 0 & Scoreğ‘– < 0.5 1, 1, 0, others , (6) where ğ‘– denotes the step-tokens, ğ‘¤ğ‘– is token-level weight, Scoreğ‘– is SPRMs process score on step ğ‘–, and ğ‘¦ğ‘– denotes whether the final answer from the policy model is correct. In Eq.6, the 6 process score is optimized based on the correctness of the final answer. However, since correct final answer may include incorrect intermediate steps and vice versa (Lightman et al., 2023), we introduce the self-supervised dynamic weight ğ‘¤ğ‘– to mitigate supervision noise. Specifically, we use the SPRM heads own prediction on each step as the pseudo label and set ğ‘¤ğ‘– = 1 only if the pseudo label is consistent with the final answers correctness. This dynamic filtering allows the model to avoid noisy samples and focus on the most representative steps of correct and incorrect solutions. Thus, by enlarging the score gap between correct and incorrect steps, SPRM can progressively learn the process evaluation ability with only final annotations. 4.3. Inference with Reflective Generative Form In the inference stage, our Reflective Generative Form is naturally suitable for TTS where the SPRM can provide guidance for selecting the high-quality reasoning trajectory from the policy model. The total inference process divides into three steps(shown in Fig.2(b)): (1) For the given question, the policy model first samples ğ‘˜ thinking processes as the candidate reasoning trajectories: ğ‘¡â„ğ‘–ğ‘›ğ‘˜1, ğ‘¡â„ğ‘–ğ‘›ğ‘˜2, . . . , ğ‘¡â„ğ‘–ğ‘›ğ‘˜ğ‘˜. (2) The SPRM evaluates the steps in each process and obtains the final score by the geometric mean of corresponding process scores: ğ‘†1, ğ‘†2, . . . , ğ‘†ğ‘˜. (3) The reasoning trajectory with the highest final score is chosen and guides the policy model to answer the question (Eq.7). answer = ğ¿ğ¿ğ‘€answer( ğ‘¡â„ğ‘–ğ‘›ğ‘˜ ğ‘—) , where ğ‘— = ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥ ğ‘†1, ğ‘†2, . . . , ğ‘†ğ‘˜) ( (7) 5. Experiment 5.1. Baseline&Dataset We conduct experiments on the models with three different sizes: MetaStone-S1-1.5B, 7B, and 32B, which are initialized from DeepSeek-R1-Distill-Qwen-1.5B/7B (Guo et al., 2025), and QWQ-32B (Team, 2025) with continual reinforcement training. After adding the SPRM head, only 5M/26M/53M extra parameters are introduced for MetaStone-S1-1.5B/7B/32B. Our training dataset is constructed from multiple publicly available math-related sources, including NuminaMath (Li et al., 2024), OpenR1-Math-220k, DeepScaleR (Luo et al., 2025), LIMR (Li et al., 2025), and OREAL-RL (Lyu et al., 2025). We apply multi-agent data cleaning framework to ensure data quality, resulting in final dataset of 40k high-quality examples. In the inference stage, we set 3 reasoning efforts with ğ‘˜ = 2, 8, 32 in Eq.7, named MetaStone-S1-low, -medium, and -high. We evaluate our models on mathematical benchmarks: AIME2024 and AIME2025 (AIME, 2025) , and out-of-distribution benchmarks: LivecodeBench(240801-250201) (Jain et al., 2024) and C-Eval (Huang et al., 2023). AIME2024 and AIME2025 are challenging mathematical benchmarks, designed to assess the mathematical reasoning capabilities of LLMs. LivecodeBench(240801250201) contains high-quality programming problems from coding competition websites (LeetCode, AtCoder, and CodeForces), designed to assess the code generation and problem-solving abilities of LLMs. C-Eval is comprehensive Chinese evaluation benchmark designed to assess the Chinese knowledge and reasoning abilities of LLMs. We adopt Pass@1 as the evaluation metric. For each problem, the model generates only one final answer, and the Pass@1 score is computed as the proportion of correctly solved problems. To improve the stability of the results, we repeat the evaluation 64 times and report the average accuracy as the final score. 7 Model Mathematical Out-of-Distribution AIME24 AIME25 LiveCodeBench C-Eval Small-size Open-Source Models DeepScaleR-1.5B-Preview R1-Distill-Qwen-1.5B R1-Distill-Qwen-7B R1-Distill-Llama-8B Baseline-1.5B MetaStone-S1-1.5B-low MetaStone-S1-1.5B-medium MetaStone-S1-1.5B-high Baseline-7B MetaStone-S1-7B-low MetaStone-S1-7B-medium MetaStone-S1-7B-high 43.1 28.9 55.5 50.4 39.3 44.0 53.1 57.9 54.7 60.7 66.3 70.2 30.0 22.8 - - 29.9 32.6 35.7 40.4 41.2 45.4 48.3 48. s1-32B QwQ-32B R1-Distill-Qwen-32B GLM-Z1-32B-0414 DeepSeek-R1-671B Claude-3.5-Sonnet1022 GPT-4o-0513 OpenAI o1-mini OpenAI o1-1217 OpenAI o3-mini* Baseline-32B MetaStone-S1-32B-low MetaStone-S1-32B-medium MetaStone-S1-32B-high Large-size Open-Source Models 56.7 79.5 72.6 80.8 79.8 50.0 69.5 49.6 63.6 70.0 Closed-Source Models 16.0 9.3 63.6 79.2 79.6 79.9 82.0 84.2 85.2 7.4 11.6 50.7 - 74.8 70.5 72.0 73.4 73.6 - 16.9 37.6 39.6 22.4 24.2 26.6 28. 39.4 41.7 44.1 44.4 - 63.4 57.2 59.1 65.9 37.2 32.9 53.8 63.4 67.4 63.4 63.8 64.0 64.2 - 27.1 - - 41.8 43.6 43.9 44. 51.3 55.1 57.5 57.8 - 88.4 82.2 - 91.8 76.7 - 68.9 - 75.9 89.4 89.5 89.6 89.7 Comparison of MetaStone-S1 models and other comparable models. * denotes medium Table 1 level of OpenAI o3-mini. The best and second-best results are shown in bold and underlined. 5.2. Main Results Table 1 summarizes the performance of our MetaStone-S1 models across four representative benchmarks. We denote baseline as the only policy model without using the Reflective Generative Form. Across different model scales, our proposed Reflective Generative Form consistently enhances the baseline, particularly on mathematical reasoning benchmarks. Specifically, compared with the baseline, MetaStone-S1 achieves performance gains of 18.6/15.5/5.3 points on AIME24 and 10.5/7.4/3.1 points on AIME25 for the 1.5B/7B/32B sizes, respectively. For other tasks, our Reflective Generative Form continues to offer stable improvements, yielding gains of 5.7/5.0/0.8 points on LiveCodeBench and 2.3/6.5/0.3 points on C-Eval. We further compare MetaStone-S1 against both advanced open-source and closed-source models. Among the open-source models, we consider DeepScaleR-1.5B-Preview (Luo et al., 2025), DeepSeek-R1-Distill-Qwen (1.5B/7B/32B), DeepSeek-R1-Distill-LLaMA-8B (Guo et al., 8 2025), QwQ-32B (Team, 2025), GLM-Z1-32B-0414 (GLM et al., 2024), s1-32B (Muennighoff et al., 2025), and DeepSeek-R1-671B (Guo et al., 2025). Among the closed-source models, we include Claude-3.5-Sonnet-1022, GPT-4o-0522, OpenAI o1-mini, OpenAI o1-1217, and OpenAI o3-mini-medium. At the small scale, MetaStone-S1-1.5B and MetaStone-S1-7B consistently outperform the listed open-source models with comparable or larger parameter sizes. Especially, MetaStone-S11.5B-low surpasses DeepScaleR-1.5B-Preview and R1-Distill-Qwen-1.5B on all datasets. And MetaStone-S1-1.5B-high further outperforms R1-Distill-Qwen-7B and R1-Distill-Llama-8B on AIME24 (57.9% vs 55.5%), demonstrating strong efficiency and capability of our lightweight SPRM. For MetaStone-S1-7B, its low reasoning effort has outperformed R1-Distill-Qwen-7B and R1-Distill-Llama-8B on AIME24 and LiveCodeBench. And MetaStone-S1-7B-high further gains the improvement of 14.7 points on AIME24 (70.2% vs 55.5%) and 4.8 points on LiveCodeBench (44.4% vs 39.6%). At the larger scale, MetaStone-S1-32B-high achieves superior results on mathematical reasoning tasks, outperforming all listed open-source models of comparable or even larger size by +4.4% on AIME24 (85.2% vs 80.8%) and +3.6% on AIME25 (73.6% vs 70.0%). For other out-of-distribution tasks, MetaStone-S1-32B-high surpasses other 32B-sized models by +0.8% on LiveCodeBench (64.2% vs 63.4%) and +1.3% on C-Eval (89.7% vs 88.4%). Compared to closed-source models, medium and high levels of MetaStone-S1-32B outperform Claude-3.5Sonnet-1022 and GPT-4o-0522, and achieving comparable performance with medium level of OpenAI o3-mini (85.2% vs 79.6% on AIME24, 73.6% vs 74.8% on AIME25, 64.2% vs 67.4% on LiveCodeBench, 89.7% vs 75.9% on C-Eval,), highlighting its strong competitiveness in both mathematical and general reasoning tasks. 5.3. Scaling Law of the Reflective Generative Model In Fig. 3, we present the scaling law for reflective generative models, which shows the relationship between the total reasoning computation in TTS and the final performance. Following Snell et al. (2024), we define the computation budget ğ¶ as the product of the models parameter (B) and the total number of reasoning tokens (k): ğ¶ = ğ‘ƒğ‘ğ‘Ÿğ‘ğ‘š ğ‘ğ‘œğ‘™ğ‘–ğ‘ ğ‘¦ ğ‘‡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘–ğ‘› ğ‘“ ğ‘’ğ‘Ÿ. Notably, when the total reasoning length is scaled to more than 32 times the baseline (e.g., Best-of-64), the Figure 3 The scaling law of Reflective Generative Models. 9 performance improves slowly. Therefore, we mainly focus on TTS results up to Best-of-32 for each model scale. We observe that the final performance shows positive correlation with the logarithm of the computation budget (the specific scaling factor depends on the baseline model architecture). This indicates that the final performance of our model can be enhanced by exponentially scaling on parameter size or the reasoning length. 5.4. Aha Moment of the Reflective Generative Model In Fig. 4, we present the final evaluation scores from MetaStone-S1 for both correct and incorrect reasoning trajectories throughout the training process. During the initial phase of training, the optimization trajectories for all samples follow similar trend, indicating that the model has not yet learned to distinguish between correct and incorrect reasoning trajectories. However, after Figure 4 The training process of SPRM. The blue and red curves denote the final score on correct and incorrect reasoning trajectories. The green dashed line indicates the \"aha moment\". Figure 5 Comparison of SPRMs predictions before and after the aha moment. Only key steps are listed. The error steps are marked in red. 10 Model Reward Model Extra Params AIME24 AIME25 LiveCodeBench MetaStone-S1-1.5B-high MetaStone-S1-7B-high Qwen2.5-Math-RM Qwen2.5-Math-PRM SPRM Qwen2.5-Math-RM Qwen2.5-Math-PRM SPRM 72B 72B 5M 72B 72B 26M 55.8 56.7 57.9 63.5 68.8 70. 35.3 40.0 40.4 46.7 48.3 48.6 26.8 26.8 28.1 42.3 42.8 44.4 Table 2 Comparison of SPRM and other PRM models. Figure 6 Evaluation of varying numbers of candidate reasoning trajectories on AIME24. certain number of training steps (e.g., around 10/20/50 steps, 1280/2560/6400 samples for MetaStone-S1-1.5B/7B/32B, respectively), we observe distinct \"aha moment\" point where the optimization trends of different reasoning trajectories begin to diverge. This suggests that the model is starting to judge the correctness based on the reasoning contents. With this aha moment, MetaStone-S1 can progressively refine its SPRM head through our proposed self-supervised SPRLoss, leading to clear score gap between correct and incorrect reasoning trajectories, and enabling effective TTS. Specially, we show an example in Fig.5. We fix the reasoning trajectory and use MetaStone-S1 before and after the aha moment for scoring. In this case, the model mistakenly confuses ğœ€ğ‘˜ and 1, resulting in an incorrect solution. Our model fails to recognize the error before the aha ğœ€ğ‘˜ + moment, while the model after the aha moment can correctly discriminate it. 5.5. Ablation Study Effectiveness of SPRM. As shown in Table.2, our SPRM, with only around 26M extra parameters, achieves higher performance compared with 72B ORM and PRM. This demonstrates that sharing parameters between the reward model and policy model is able to generate high-quality guidance without the requirement of additional large-scale reward models. To further analyze the efficacy of different reward models, we compare their performance under different numbers of candidate reasoning trajectories ğ‘˜ in Fig.6. Across different ğ‘˜ and model sizes, SPRM consistently outperforms other methods, indicating its strong ability to distinguish between high and low quality reasoning trajectories. Moreover, we evaluate the generalization ability of SPRM on tasks from the other do11 Figure 7 SPRMs predictions on reasoning trajectories. Only key steps are listed. Correct and error steps are marked in green and red. 12 Model Loss AIME24 LiveCodeBench MetaStone-S1-1.5B-high MetaStone-S1-7B-high BCELoss SPRLoss BCELoss SPRLoss 56.7 57.9 69.1 70. 27.9 28.1 43.9 44.4 Table 3 Evaluation on SPRLoss. Figure The prediction score gap between correct and incorrect solutions. The blue curve shows the SPRLoss. The red curve shows the BCELoss. main (LiveCodeBench). Without any task-specific fine-tuning or in-domain data, our SPRM still achieves superior performance compared to separate reward models. This demonstrates SPRMs strong zero-shot generalization capability, suggesting that it can capture domain-agnostic patterns to evaluate the reasoning trajectories. Fig.7 shows the visualization of step-wise evaluation scores from SPRM. It can be observed that SPRM effectively identifies low-quality processes generated by the policy model, including logical error (e.g. the misunderstanding of in step 58 of example 1) and calculation error (e.g., the incorrect computation 9 21 + 7 = 193 in step 32 of example 2). SPRM assigns low scores to these low-quality steps. Since SPRM only outputs process scores, we additionally provide post-descriptions within the dashed boxes for better clarity. Effectiveness of self-supervised optimization. We evaluate the effectiveness of SPRLoss in Table.3. Compared with using the final answer correctness as process-level supervision for PRM training, our proposed self-supervised optimization method achieves larger performance gains on both 1.5B and 7B models. Furthermore, Fig.8 shows the prediction score gap between correct and incorrect solutions. Compared to the BCELoss, SPRLoss demonstrates stronger discriminative capability with larger score gap. This indicates that treating final answer correctness as process-level labels introduces substantial label noise, which harms the optimization. In contrast, SPRLoss leverages self-supervised signals to reduce the impact of noisy supervision, leading to more stable and accurate training. 13 Searching Tokens(k) 0 40 120 160 Accuracy(%) 39.3 48.8 50. 51.7 52.8 Table 4 Performance of MetaStone-S1-1.5B with MCTS on AIME24. 5.6. Extend on MCTS Since SPRM produces process scores for each step, our reflective generative models can be naturally used in step-level search-based TTS methods such as Monte Carlo Tree Search (MCTS). In our setup, instead of performing full simulations to the end of reasoning trajectory, we directly use SPRM to estimate the value of each node during the search. This enables more efficient evaluation at each expansion step. In the expanding stage, we expand 4 children for the selected node and generate 1024 tokens in each child node. To balance the computation cost, we set the maximum number of tokens in the MCTS process from 0 (without MCTS) to 160k for each question. Table.4 presents the performance of MCTS on the AIME24 dataset. By increasing the maximum number of searching tokens, the performance improved from 39.3 to 52.8, demonstrating the effectiveness of SPRM in providing high-quality step-level guidance in the reasoning stage. However, the performance of MCTS is still lower than the results under the Best-of-N strategy reported in Table 1. This is primarily due to the computational overhead inherent in tree-based search methods, which leads to incomplete searching in our setting. Nonetheless, the observed gains over the baseline validate the potential of our reflective generative models for integration with more advanced search-based inference methods. 6. Conclusion In this work, we propose novel Reflective Generative Form, which enables single LLM to both generate and select high-quality reasoning trajectories for Test-Time Scaling (TTS). Based on this form, we introduce our first reflective generative model, MetaStone-S1, which achieves comparable performance to the OpenAI o3-mini series. Specifically, we design unified interface that integrates the policy model and process reward model (PRM) within single network, resulting in low parameter overhead and efficient TTS inference. To further reduce the dependence on costly process-level annotations for PRM training, we present self-supervised process reward model (SPRM) that learns process-level evaluation using only final answer annotations. MetaStone-S1, with 32B parameters, achieves strong performance across mathematics, coding, and Chinese reasoning benchmarks, outperforming series of opensource and closed-source models. In addition, our experimental analyses of the aha moment and scaling law further demonstrate the effectiveness of our Reflective Generative Form. In future work, we plan to explore its capability for more efficient step-level search-based TTS for real-time reasoning enhancement."
        },
        {
            "title": "References",
            "content": "AIME. AIME problems and solutions, 2025. URL https://artofproblemsolving.com/wi ki/index.php/AIME_Problems_and_Solutions. E. Beeching, L. Tunstall, and S. Rush. Scaling test-time compute with open models. URL 14 https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-tim e-compute. B. Brown, J. Juravsky, R. Ehrlich, R. Clark, Q. V. Le, C. RÃ©, and A. Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. Q. Chen, L. Qin, J. Wang, J. Zhou, and W. Che. Unlocking the capabilities of thought: reasoning boundary framework to quantify and optimize chain-of-thought. Advances in Neural Information Processing Systems, 37:5487254904, 2024a. X. Chen, J. Xu, T. Liang, Z. He, J. Pang, D. Yu, L. Song, Q. Liu, M. Zhou, Z. Zhang, et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024b. F. Chollet. Openai o3 breakthrough high score on arc-agi-pub, 2024. URL https://arcprize .org/blog/oai-o3-pub-breakthrough#::text=o3%27s%20improvement%20over %20the%20GPT,progress%20is%20about%20new%20ideas. Accessed: 2024-12-20. T. GLM, A. Zeng, B. Xu, B. Wang, C. Zhang, D. Yin, D. Rojas, G. Feng, H. Zhao, H. Lai, H. Yu, H. Wang, J. Sun, J. Zhang, J. Cheng, J. Gui, J. Tang, J. Zhang, J. Li, L. Zhao, L. Wu, L. Zhong, M. Liu, M. Huang, P. Zhang, Q. Zheng, R. Lu, S. Duan, S. Zhang, S. Cao, S. Yang, W. L. Tam, W. Zhao, X. Liu, X. Xia, X. Zhang, X. Gu, X. Lv, X. Liu, X. Liu, X. Yang, X. Song, X. Zhang, Y. An, Y. Xu, Y. Niu, Y. Yang, Y. Li, Y. Bai, Y. Dong, Z. Qi, Z. Wang, Z. Yang, Z. Du, Z. Hou, and Z. Wang. Chatglm: family of large language models from glm-130b to glm-4 all tools, 2024. X. Guan, L. L. Zhang, Y. Liu, N. Shang, Y. Sun, Y. Zhu, F. Yang, and M. Yang. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519, 2025. D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, Y. Fu, et al. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models. Advances in Neural Information Processing Systems, 36:6299163010, 2023. A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low, A. Helyar, A. Madry, A. Beutel, A. Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. N. Jain, K. Han, A. Gu, W.-D. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. M. Jin, Q. Yu, D. Shu, H. Zhao, W. Hua, Y. Meng, Y. Zhang, and M. Du. The impact of reasoning step length on large language models. arXiv preprint arXiv:2401.04925, 2024. A. Labs. Inside reasoning models openai o3 and deepseek r1, 2025. URL https://labs.ada line.ai/p/inside-reasoning-models-openai-o3/. Accessed: 2025-02-18. J. Li, E. Beeching, L. Tunstall, B. Lipkin, R. Soletskyi, S. Huang, K. Rasul, L. Yu, A. Q. Jiang, Z. Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13:9, 2024. 15 X. Li, H. Zou, and P. Liu. Limr: Less is more for rl scaling. arXiv preprint arXiv:2502.11886, 2025. H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. R. Liu, J. Gao, J. Zhao, K. Zhang, X. Li, B. Qi, W. Ouyang, and B. Zhou. Can 1b llm surpass 405b llm? rethinking compute-optimal test-time scaling. arXiv preprint arXiv:2502.06703, 2025. L. Luo, Y. Liu, R. Liu, S. Phatale, M. Guo, H. Lara, Y. Li, L. Shu, Y. Zhu, L. Meng, et al. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592, 2024. M. Luo, S. Tan, J. Wong, X. Shi, W. Y. Tang, M. Roongta, C. Cai, J. Luo, L. E. Li, R. A. Popa, and I. Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl. https: //pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview-wit h-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2, 2025. Notion Blog. C. Lyu, S. Gao, Y. Gu, W. Zhang, J. Gao, K. Liu, Z. Wang, S. Li, Q. Zhao, H. Huang, et al. Exploring the limit of outcome reward for learning mathematical reasoning. arXiv preprint arXiv:2502.06781, 2025. N. Muennighoff, Z. Yang, W. Shi, X. L. Li, L. Fei-Fei, H. Hajishirzi, L. Zettlemoyer, P. Liang, E. CandÃ¨s, and T. Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. OpenAI. Learning to reason with llms, 2024. URL https://openai.com/index/learnin g-to-reason-with-llms/. Accessed: 2025-04-13. OpenAI. Openai o3-mini evaluation, 2025. URL https://openai.com/index/openai-o 3-mini/. Accessed: 2025-01-31. Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. C. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. X. Tan, T. Yao, C. Qu, B. Li, M. Yang, D. Lu, H. Wang, X. Qiu, W. Chu, Y. Xu, et al. Aurora: Automated training framework of universal process reward models via ensemble prompting and reverse verification. arXiv preprint arXiv:2502.11520, 2025. Q. Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https: //qwenlm.github.io/blog/qwq-32b/. P. Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv preprint arXiv:2312.08935, 2023. E. Yeo, Y. Tong, M. Niu, G. Neubig, and X. Yue. Demystifying long chain-of-thought reasoning in llms. arXiv preprint arXiv:2502.03373, 2025. 16 M. Zeff. Openais o3 suggests ai models are scaling in new ways but so are the costs, 2024. URL https://techcrunch.com/2024/12/23/openais-o3-suggests-ai-models-are-s caling-in-new-ways-but-so-are-the-costs/?guccounter=1#::text=AI%20mo dels%20were%20showing%20diminishing,with%20drawbacks%20of%20its%20own. Accessed: 2024-12-23. Z. Zeng, Q. Cheng, Z. Yin, Y. Zhou, and X. Qiu. Revisiting the test-time scaling of o1-like models: Do they truly possess test-time scaling capabilities? arXiv preprint arXiv:2502.12215, 2025. D. Zhang, S. Zhoubian, Z. Hu, Y. Yue, Y. Dong, and J. Tang. Rest-mcts*: Llm self-training via process reward guided tree search. Advances in Neural Information Processing Systems, 37: 6473564772, 2024. Z. Zhang, C. Zheng, Y. Wu, B. Zhang, R. Lin, B. Yu, D. Liu, J. Zhou, and J. Lin. The lessons of developing process reward models in mathematical reasoning. arXiv preprint arXiv:2501.07301, 2025."
        }
    ],
    "affiliations": [
        "MetaStone-AI",
        "USTC"
    ]
}