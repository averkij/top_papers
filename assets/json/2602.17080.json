{
    "paper_title": "Adam Improves Muon: Adaptive Moment Estimation with Orthogonalized Momentum",
    "authors": [
        "Minxin Zhang",
        "Yuxuan Liu",
        "Hayden Schaeffer"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Efficient stochastic optimization typically integrates an update direction that performs well in the deterministic regime with a mechanism adapting to stochastic perturbations. While Adam uses adaptive moment estimates to promote stability, Muon utilizes the weight layers' matrix structure via orthogonalized momentum, showing superior performance in large language model training. We propose a new optimizer and a diagonal extension, NAMO and NAMO-D, providing the first principled integration of orthogonalized momentum with norm-based Adam-type noise adaptation. NAMO scales orthogonalized momentum using a single adaptive stepsize, preserving orthogonality while improving upon Muon at negligible additional cost. NAMO-D instead right-multiplies orthogonalized momentum by a diagonal matrix with clamped entries. This design enables neuron-wise noise adaptation and aligns with the common near block-diagonal Hessian structure. Under standard assumptions, we establish optimal convergence rates for both algorithms in the deterministic setting and show that, in the stochastic setting, their convergence guarantees adapt to the noise level of stochastic gradients. Experiments on pretraining GPT-2 models demonstrate improved performance of both NAMO and NAMO-D compared to the AdamW and Muon baselines, with NAMO-D achieving further gains over NAMO via an additional clamping hyperparameter that balances the competing goals of maintaining a well-conditioned update direction and leveraging fine-grained noise adaptation."
        },
        {
            "title": "Start",
            "content": "Adam Improves Muon Adam Improves Muon: Adaptive Moment Estimation with Orthogonalized Momentum Minxin Zhang Yuxuan Liu Hayden Schaeffer Department of Mathematics University of California, Los Angeles Los Angeles, CA 90095, USA minxinzhang@math.ucla.edu yxliu@math.ucla.edu hayden@math.ucla.edu"
        },
        {
            "title": "Abstract",
            "content": "Efficient stochastic optimization typically integrates an update direction that performs well in the deterministic regime with mechanism adapting to stochastic perturbations. While Adam uses adaptive moment estimates to promote stability, Muon utilizes the weight layers matrix structure via orthogonalized momentum, showing superior performance in large language model training. We propose new optimizer and diagonal extension, NAMO and NAMO-D, providing the first principled integration of orthogonalized momentum with norm-based Adam-type noise adaptation. NAMO scales orthogonalized momentum using single adaptive stepsize, preserving orthogonality while improving upon Muon at negligible additional cost. NAMO-D instead right-multiplies orthogonalized momentum by diagonal matrix with clamped entries. This design enables neuron-wise noise adaptation and aligns with the common near block-diagonal Hessian structure. Under standard assumptions, we establish optimal convergence rates for both algorithms in the deterministic setting and show that, in the stochastic setting, their convergence guarantees adapt to the noise level of stochastic gradients. Experiments on pretraining GPT-2 models demonstrate improved performance of both NAMO and NAMO-D compared to the AdamW and Muon baselines, with NAMO-D achieving further gains over NAMO via an additional clamping hyperparameter that balances the competing goals of maintaining well-conditioned update direction and leveraging fine-grained noise adaptation."
        },
        {
            "title": "1 Introduction",
            "content": "Stochastic optimization is central to modern large-scale learning, where algorithms must update iterates using only noisy estimates of first-order information. key challenge is to balance two goals: selecting an update direction that is effective in the noise-free regime, and incorporating mechanisms that adapt to stochastic perturbations. From this viewpoint, an efficient stochastic optimizer can be understood as combining two ingredients: (i) principled direction-selection rule that performs well when gradients are exact, and (ii) an adaptive stepsize mechanism that stabilizes the iterates by attenuating updates under gradient uncertainty. This perspective offers useful lens for interpreting existing methods and for guiding the development of new algorithms with strong convergence behavior. In this paper, we design new optimization algorithm that couple the structural advantages of an orthogonalized update direction with adaptive moment estimation to account for gradient noise. We provide theoretical convergence guarantees and demonstrate strong empirical performance in training large language models (LLMs). Adaptive methods such as Adam (Kingma and Ba, 2014) and its decoupled weight-decay variant AdamW (Loshchilov and Hutter, 2017) have long been the de facto optimizers for large-scale The code is available at: https://github.com/minxin-zhg/namo. 1 6 2 0 2 0 ] . [ 2 0 8 0 7 1 . 2 0 6 2 : r Zhang, Liu and Schaeffer training. Their coordinate-wise adaptive stepsizes promote training stability and reduce sensitivity to hyperparameter choices. Given momentum coefficients β1, β2 [0, 1), Adam iteratively updates biased first-moment estimate of the stochastic gradient: and biased second raw-moment estimate: mt = β1mt1 + (1 β1)gt, vt = β2vt1 + (1 β2)g2 , where gt denotes the stochastic gradient at iteration t, and the square is applied elementwise. With the standard bias corrections ˆmt := mt/(1 βt 2), the parameters are updated via: 1) and ˆvt := vt/(1 βt θt = θt1 η ˆmt ˆvt + ϵ , where ϵ > 0 is small fixed scalar that avoids division by zero, and η > 0 is the learning rate. Throughout the paper, we refer to the hyperparameter η as the learning rate, and to its combination with an adaptive scaling as the effective stepsize. The ratio of the firstand second-moment ˆvt, is often interpreted as signal-to-noise ratio (SNR). When the update direcestimates, ˆmt/ tion is dominated by noise, or as the iterates approach stationary point, the resulting effective stepsize decreases, which is desirable for stable convergence. When β1 = β2, (Orvieto and Gower, 2025) interprets Adam as performing online estimation of the mean and variance of the stochastic gradients. Moreover, Adams update rule can be viewed as combining sign descent direction with variance adaptation component. Theoretical analysis and experiments in (Balles and Hennig, 2018) show that the sign-descent component can lead to Adams adverse generalization behavior (Wilson et al., 2017), indicating that coupling Adam-type noise adaptation with different descent direction may yield improved generalization relative to Adam. While Adam and other standard variants of stochastic gradient descent (SGD) treat trainable parameters of neural networks as flattened vectors, the Muon optimizer (Jordan et al., 2024b) exploits their matrix structure by updating weight matrices with orthogonalized momentum. Given matrix Rmn, if = ΣV is its reduced singular value decomposition (SVD), then its orthogonalization is given by: Orth (M ) := . The orthogonalization of , Orth (M ), is also called the orthogonal factor in the polar decomposition and is the nearest orthogonal matrix to in the Frobenius norm (Higham, 2008). Muon updates matrix-structured parameters Θ Rmn by: Θt = Θt1 ηOt, where η > 0 is the prescribed learning rate, and Ot Orth (Mt) denotes an approximate orthogonalization of the momentum matrix Mt at iteration t, computed via Newton-Schultz iterations (Bernstein, 2025). Growing evidence demonstrates that Muon achieves superior empirical performance compared to Adam in LLM training (Jordan et al., 2024a; Team et al., 2025; Liu et al., 2025). Recent studies show that orthogonalized updates can accelerate convergence, facilitate more reliable hyperparameter transfer across model sizes (Boreiko et al., 2025; Shah et al., 2025), and learn more effectively from heavy-tailed data (Wang et al., 2025). In the deterministic setting without momentum, Muons orthogonalized update direction can be interpreted as the steepest descent direction under the spectral norm, which is shown to be effective for training in deep learning 2 Adam Improves Muon (Davis and Drusvyatskiy, 2025). In stochastic settings, however, matrix orthogonalization is an unbounded operation (Higham, 1986) that may amplify the impact of noise in the original momentum matrix, leading to training instability (He et al., 2025) and increased sensitivity to hyperparameter choices (Crawshaw et al., 2025). This suggests that pairing Muons orthogonalized updates with an explicit noise-adaptation mechanism could improve training robustness and further boost performance. In this work, we develop theoretically principled integration of Adam-type variance adaptation with an orthogonalized update direction. Prior work shows that orthogonalization decouples the direction and magnitude of matrix update through norm-duality characterization (Bernstein and Newhouse, 2024, Proposition 5), suggesting that norm-based rescaling of the orthogonalized update is natural design choice. We pair norm-based moment estimation with orthogonalized momentum and propose new optimizer, NAMO (Norm-Based Adaptive Moment Estimation with Orthogonalized Momentum), together with diagonal extension, NAMO-D. NAMO scales the orthogonalized momentum with norm-based adaptive scalar, preserving the orthogonality of the update direction while adapting the effective stepsize to the noise level. NAMO-D scales the orthogonalized momentum with right-multiplied diagonal matrix, allowing an individual adaptive stepsize for each neuron. We establish theoretical convergence rates for both algorithms, and demonstrate that they outperform the AdamW and Muon baselines in GPT-2 pretraining."
        },
        {
            "title": "1.1 Related work",
            "content": "Unlike Adams coordinate-wise adaptive stepsizes, our methods introduce structured stepsize adaptation for orthogonalized updates: NAMO scales the orthogonalized momentum using single adaptive stepsize, whereas NAMO-D employs column-wise adaptive stepsize for the orthogonalized momentum. Related simplifications of Adam have also been studied. In (Chezhegov et al., 2024), clipped norm-based Adam-type stepsize is applied to the momentum, and convergence is analyzed under the assumptions of bounded gradients and heavy-tailed noise. Motivated by the near block-diagonal Hessian structure of neural networks, Adam-mini (Zhang et al., 2024) partitions parameters into blocks and assigns single adaptive learning rate per block, matching Adams performance while reducing memory cost. Several adaptive variants of Muon have been proposed as well. AdaMuon (Si et al., 2025) and NorMuon (Li et al., 2025) combine Muons orthogonalized momentum with Adam-type scaling variants, though without theoretical convergence guarantees. AdaGO (Zhang et al., 2025) scales the orthogonalized momentum with an adaptively decaying stepsize and achieves optimal theoretical convergence rates, although its performance in LLM training has yet to be investigated. Layerwise adaptive learning rates for Muon are proposed in (Hao et al., 2025) based on gradientvariance estimation that requires evaluating nuclear norm, thereby increasing the per-iteration cost. Tuning-robust variants of Muon are explored in (Crawshaw et al., 2025). PRISM (Yang, 2026) augments Muon with moment-based adaptive preconditioner, but incurs higher additional computational cost than our proposed algorithms, and does not provide theoretical convergence guarantees. DeVA (Song et al., 2026) decouples variance adaptation from scale-invariant sign descent, yielding an Adam-style scaling for Muons orthogonalized momentum that require high computational and memory overhead for maintaining Kronecker preconditioners and periodic eigendecompositions. 3 Zhang, Liu and Schaeffer 1.2 Contributions and Organization We propose new optimization algorithm and diagonal extension for problems with matrixstructured parameters, NAMO and NAMO-D, which provide the first theoretically principled integration of an orthogonalized update direction with noise adaptation based on Adam-type moment estimation. NAMO scales the orthogonalized momentum using single adaptive stepsize, incurring only negligible O(mn) additional computational cost and no additional memory overhead, thus providing useful improvement for Muons performance. On the other hand, NAMO-D scales the orthogonalized momentum by right-multiplying with diagonal matrix Dt, thereby assigning neuron-wise adaptive stepsize determined by the column norms of the stochastic gradient and momentum matrices. This column-wise scaling enables finer-grained noise adaptation and aligns with the near block-diagonal Hessian structure commonly observed in neural networks (Dong et al., 2025; An et al., 2025), while no longer strictly preserving the orthogonalized update direction. The diagonal entries of Dt admit an upper bound and are clamped toward the average, ensuring that NAMO-Ds update direction remains well-conditioned. Under standard smoothness and unbiased bounded-variance noise assumptions, we establish optimal convergence rates for both algorithms in the deterministic setting and show that, in the stochastic setting, their convergence guarantees adapt to the noise level of the stochastic gradients and attain the optimal rate when the batch size is sufficiently large. Experiments on pretraining GPT-2 models demonstrate that both NAMO and NAMO-D outperform AdamW and Muon baselines. NAMO-D achieves further gains over NAMO through an additional clamping hyperparameter c, which balances the competing goals of maintaining well-conditioned update direction and leveraging finer-grained noise adaptation. The rest of the paper is organized as follows. Section 2 describes the proposed algorithms, and Section 3 provides the convergence analysis, with detailed proofs deferred to Appendices AE. Section 4 presents experiments on GPT-2 pretraining, and Section 5 concludes the paper."
        },
        {
            "title": "2 New Optimization Algorithms: NAMO and NAMO-D",
            "content": "In this section, we describe the proposed algorithm NAMO and its diagonal extension NAMO-D. We consider optimization problems with matrix-structured parameters of the form: min ΘRmn L(Θ), where : Rmn is the (nonconvex) loss function. We impose the following standard smoothness of loss function and bounded-variance noise assumptions. Assumption 1 The gradient of L(Θ) is Lipschitz continuous, i.e., for arbitrary Θ, Θ Rmn: (cid:13)L(Θ) L(Θ)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)Θ Θ(cid:13) (cid:13)2 , (1) for some constant > 0, where and 2 denote the nuclear norm and the spectral norm respectively. Note that gradient of the loss, L(Θ), is in Rmn and recall that the nuclear norm is the sum of the singular values while the spectral norm is the maximum singular value. Assumption 2 At each iteration t, the stochastic gradient Gt is an unbiased estimate of the true gradient, i.e., E[Gt] = L(Θt1), with uniformly bounded variance: (cid:104) Gt L(Θt1)2 (cid:105) σ2 , 4 Adam Improves Muon where 1 is the batch size and denotes the Frobenius norm. Assumption 1 is equivalent to more commonly seen smooth assumption: (cid:13)L(Θ) L(Θ)(cid:13) (cid:13) (cid:13)F (cid:13) (cid:13)Θ Θ(cid:13) (cid:13)F (2) for different Lipschitz constant > 0. Since orthogonalized gradient descent can be interpreted as steepest descent under the spectral norm (Shen et al., 2025), we state the smoothness assumption in the equivalent form (1). Details of NAMO and NAMO-D are summarized in Algorithms 1 and 2, respectively. For the theoretical analysis in Section 3, we assume exact orthogonalization in both algorithms, as is standard in existing works (Shen et al., 2025; Sato et al., 2025; Li and Hong, 2025; Chen et al., 2025; Kovalev, 2025; Pethick et al., 2025). In practice, exact orthogonalization can be expensive to compute, and for the experiments in Section 4 we use NewtonSchulz iterations to obtain an approximate orthogonalization, as in the Muon optimizer. NAMO maintains biased second raw-moment estimate of the squared Frobenius norm: vt = µ2vt1 + (1 µ2) Gt2 , where Gt is the stochastic gradient matrix at iteration t, and the momentum matrix Mt is biased first moment estimate of the stochastic gradient. Applying bias correction yields ˆMt := Mt/(1µt 1) and ˆvt := vt/(1 µt 2). The parameters Θ Rmn are updated by: Θt = Θt1 ηαtOt, (3) where η > 0 is prescribed learning rate, and the orthogonalized momentum Ot is adaptively scaled by scalar: αt := 1 2 (1 µt 2) 1 µt 1 Mt vt + ϵ = (cid:13) (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) ˆvt + ϵt , with ϵt := ϵ/(cid:112)1 µt approach stationary point, αt is small, promoting stable convergence. 2 for small ϵ > 0. When stochastic gradients are noisy, or when the iterates Algorithm 1 NAMO: Norm-Based Adaptive Moment Estimation with Orthogonalized Momentum Require: Learning rate η, momentum µ1, µ2 [0, 1), batch size b, ϵ > 0 1: Initialize Θ0 Rmn, M0 = 0, v0 = 0 2: for = 1, 2, . . . , do 3: Sample minibatch of size and compute stochastic gradient Gt = Lt(Θt1) Mt µ1Mt1 + (1 µ1)Gt vt µ2vt1 + (1 µ2) Gt2 Ot Orth (Mt) 1µt 2 1µt 1 MtF vt+ϵ αt Update parameters Θt Θt1 ηαtOt 4: 5: 6: 7: 8: 9: end for 10: return ΘT While NAMO scales the orthogonalized momentum using single adaptive stepsize, NAMO-D applies column-wise scaling based on the column norms of the momentum matrix. This design 5 Zhang, Liu and Schaeffer assigns an individual adaptive stepsize to each neuron and is consistent with the near block-diagonal Hessian structure commonly observed in neural networks (Dong et al., 2025; An et al., 2025). For each = 1, 2, , n, NAMO-D maintains biased second raw-moment estimate of the squared norm of the j-th colum of the stochastic gradient: [vt]j = µ2 [vt]j1 + (1 µ2) (cid:13) (cid:13) (cid:13)[Gt]:j (cid:13) 2 (cid:13) (cid:13) . To write this in an equivalent vector form, define the operator Nc : Rmn Rn by: where denotes the Euclidean norm. Then the iterates can be written as: [Nc(M )]j := M:j , = 1, 2, , n, vt = µ2vt1 + (1 µ2)Nc(Gt) Nc(Gt). Applying bias correction yields ˆMt := Mt/(1 µt 1) and ˆvt := vt/(1 µt (cid:17) (cid:16)(cid:112) ˆvt + ϵt , 2). Now let: dt = Nc( ˆMt) where ϵt := ϵ/(cid:112)1 µt entries of dt are bounded above by: 2 for small ϵ > 0, and denotes entrywise division. By Lemma 7, the dt = max [dt]j (cid:114) 1 µ1 1 µ2 . Compared with NAMO, NAMO-Ds column-wise scaling enables finer-grained noise adaptation, but it no longer strictly preserves orthogonality of the update direction. Let dt := dt1 /n be the average of the entries of dt. To ensure that the scaled direction remains well-conditioned, we clamp the adaptive stepsizes toward this average via: dt := min (cid:8)max (cid:8)dt, dt1(cid:9) , dt/c(cid:9) , for prescribed constant (0, 1], where the maximum and minimum operations are applied entrywise, and 1 Rn denotes the vector of all-ones. Let Dt := diag . NAMO-D updates the parameters by: (cid:16)dt (cid:17) Θt = Θt1 ηOtDt. This update rule combines column-wise, noise-adaptive scaling with simple clamping safeguard, tempering updates when gradients are noisy or the iterates are near stationary point, while keeping the scaled update direction well-conditioned."
        },
        {
            "title": "3 Convergence Analysis",
            "content": "In this section, we establish convergence rates for NAMO (Algorithm 1) and NAMO-D (Algorithm 2) in both the deterministic and stochastic settings under the standard Assumptions 12. The analysis of NAMO is presented in Section 3.1, and the analysis of NAMO-D is presented in Section 3.2. We provide proof sketch for each of the theorems below and include detailed proofs in Appendices BE. Proofs of useful lemmata are in Appendix A. The proofs repeatedly use the biascorrected moment estimates and their convex-combination representations with coefficients: w1,t,τ := 1 µ1 1 µt 1 µtτ 1 , w2,t,τ := 1 µ2 1 µt 2 µtτ 2 , for 1 and τ {1, . . . , t}, 6 Adam Improves Muon Algorithm 2 NAMO-D: Diagonal exension of NAMO Require: Learning rate η, momentum µ1, µ2 [0, 1), batch size b, ϵ > 0, (0, 1] 1: Initialize Θ0 Rmn, M0 = 0, v0 = 0 2: for = 1, 2, . . . , do 3: Sample minibatch of size and compute stochastic gradient Gt = Lt(Θt1) Mt µ1Mt1 + (1 µ1)Gt vt µ2vt1 + (1 µ2)Nc(Gt) Nc(Gt) Nc(Mt) (cid:0) vt + ϵ(cid:1) 4: 5: 6: 7: 8: 9: 10: 11: end for 12: return ΘT 1µt 2 1µt dt dt dt1 /n Ot Orth (Mt) dt1(cid:9)(cid:1) Dt diag (cid:0)min (cid:8)max (cid:8)dt, dt1(cid:9) , 1 Update parameters Θt Θt1 ηOtDt τ =1 w1,t,τ = (cid:80)t which satisfy (cid:80)t τ =1 w2,t,τ = 1. This representation allows one to control the deviation between the bias-corrected momentum and the current gradient via Assumption 1, as well as the magnitude of the bias-corrected second-moment estimate by elementary geometric-series bounds."
        },
        {
            "title": "3.1 Analysis of NAMO",
            "content": "We first analyze the convergence of NAMO in the deterministic case where gradients are evaluated exactly. The O(T 1/2) rate established in the theorem below is the best possible for deterministic first-order methods under Assumption 1; see (Carmon et al., 2020, Theorem 2). Theorem 3 (NAMO in the deterministic case) Suppose Assumptions 1 holds. Let {Θt} Rmn be the sequence of iterates generated by Algorithm 1 with full-batch gradients and 0 µ1 µ2 < 1. If choosing η = 2 ), µ1 = Θ(1) and µ2 = Θ(1), then for large > 0: , ϵ = O(T 1 1 (cid:16) (cid:17)"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 L(Θt1)F (cid:16) 1 (cid:17) . 1) and ˆvt := vt/(1µt τ =1 w1,t,τ L(Θτ 1) and ˆvt = (cid:80)t Proof sketch. Let ˆMt := Mt/(1µt 2) denote the bias-corrected moments, so that ˆMt = (cid:80)t . Define the adaptive scaling αt as in (3). Applying Lemma 7 to the vectorization of the gradient matrix sequence {L(Θτ 1)} yields uniform upper bound on αt (Appendix B, Step 1). Next, using the orthogonalized descent inequality from (Zhang et al., 2025, Lemma B.1) and Assumption 1, one obtains an average inequality involving: τ =1 w2,t,τ L(Θτ 1) ˆMtF L(Θt1) ˆvt + ϵ/(cid:112)1 µt 2 and L(Θt1) ˆMt (Appendix B, Step 2). The deviation of the bias-corrected momentum from the gradient is then controlled using the convex-combination representation of ˆMt together with Assumption 1 and telescoping bound on Θτ 1 Θt12 (Appendix B, Step 3). Similarly, ˆvt is upper bounded by L(Θt1) plus constant obtained from geometric-series estimates (Appendix B, Step 4). Combining these bounds and applying Lemma 8 converts the bound on an averaged surrogate ϕϵ 7 Zhang, Liu and Schaeffer into the stated bound on the average gradient norm (Appendix B, Steps 56). The full proof is provided in Appendix B. Next we analyze NAMO in the stochastic case where stochastic gradients have bounded variance. The best possible convergence rate for stochastic first-order methods under Assumptions 12 is O(T 1/4); see (Arjevani et al., 2023, Theorem 3). The following result shows that NAMO adapts to the noise level of the stochastic gradients and achieves the optimal convergence rate when the batch size is sufficiently large. Theorem 4 (NAMO in the stochastic case) Suppose Assumptions 12 hold. Let {Θt} Rmn be the sequence of iterates generated by Algorithm 1. For large > 0, if we set η = O(T 3 4 ), 1 µ1 = Θ(T 2 ), 0 µ1 µ2 < 1, and ϵ = O(T 1 2 ), 1 µ2 = Θ(T 1 2 ), then: 1 (cid:88) t= [L(Θt1)F ] (cid:16) 1 4 + where is the batch size. σb 1 4 1 8 (cid:17) , Proof sketch. The proof again works with the convex-combination representations of the biascorrected moment estimates ˆMt and ˆvt with weight coefficients w1,t,τ and w2,t,τ respectively. Let Et[] be the expectation conditioned on the iterate Θt1, and define the deviation term Et := ˆMt L(Θt1). Using (Zhang et al., 2025, Lemma B.1) and taking conditional expectations yields an average bound on E[αt ˆMt] in terms of [Et] and (cid:2)α2 (cid:3) (Appendix C, Step 2). The quantity [Et] is controlled by decomposing Et into weighted sum of noise terms Gτ L(Θτ 1) and drift term L(Θτ 1) L(Θt1). The noise term is bounded above via Assumption 2, the drift term is bounded above using Assumption 1 along with telescoping argument, and then Lemma 9 is applied to bound the averaging over (Appendix C, Step 3). In parallel, Minkowski and (cid:105) ˆvt + ϵ/(cid:112)1 µt Jensens inequalities yield an upper bound on the expected denominator term by [L(Θt1)] plus an explicit additive term containing σ/ b, then Lemma 10 is applied to bound the averaging over (Appendix C, Step 4). Finally, CauchySchwarz argument relates E[αt ˆMtF ] to E[L(Θt1)F ], leading to quadratic inequality in [L(Θt1)F ] that can be solved explicitly (Appendix C, Step 5). Substituting the averaged bounds and the parameter choices yields the stated rate. The full proof is provided in Appendix C. (cid:104) 2 This result indicates that the convergence rate of NAMO is adaptive to the noise level of the stochastic gradients, and that choosing the batch size as = Ω(σ2 ) recovers the optimal O(T 1/4) rate."
        },
        {
            "title": "3.2 Analysis of NAMO-D",
            "content": "NAMO-D introduces column-wise diagonal scaling Dt, defined in Algorithm 2, instead of the scalar αt. We first establish its optimal (cid:0)T 1/2(cid:1) rate for the deterministic case where gradients are evaluated exactly in the theorem below. Theorem 5 (NAMO-D in the deterministic case) Suppose Assumption 1 holds. Let {Θt} Rmn be the sequence of iterates generated by Algorithm 2 using full batch with 0 µ1 µ2 < 1. If we choose η = O(T 1 2 n1), and 2 ), 1 µ1 = Θ(1), 1 µ2 = Θ(1), 0 µ1 µ2 < 1, ϵ = O(T 1 8 Adam Improves Muon 1 T (cid:88) t=1 L(Θt1)F (cid:16) 1 (cid:17) , = Θ(1), then: for large > 0. Proof sketch. The proof works with the diagonal scaling matrix Dt defined as in Section 2 and its extremal diagonal entries dt,max and dt,min. Applying Lemma 7 column-wise yields uniform upper bound on the entries of Dt (Appendix D, Step 1). The clamping rule implies dt,min c2 dt,max and hence bounds the condition number of Dt. Applying the orthogonalized descent inequality (Zhang et al., 2025, Lemma B.1) gives an average bound on dt,max L(Θt1) in terms of the j=1 ˆvj deviation are controlled using similar arguments as in the proof of Theorem 3, together with geometric-series bounds (Appendix D, Steps 34). The result again follows by combining these bounds and applying Lemma 8 (Appendix D, Steps 56). The full proof is provided in Appendix D. (Appendix D, Step 2). The deviation term and the term (cid:13) (cid:13)L(Θt1) ˆMt (cid:13) (cid:113)(cid:80)n (cid:13) (cid:13) (cid:13) In the stochastic case, we show that the convergence of NAMO-D adapts to the noise level of stochastic gradients, and achieves the optimal (cid:0)T 1/4(cid:1) rate when the batch size is sufficiently large. Theorem 6 (NAMO-D in the stochastic case) Suppose Assumptions 12 holds. Let {Θt} Rmn be the sequence of iterates generated by Algorithm 2. For large > 0, if we set η = O(T 3 4 ), 1 µ1 = Θ(T 2 ), 0 µ1 µ2 < 1, ϵ = O(T 1 2 ), 1 µ2 = Θ(T 1 2 ), and = Θ(1), then:"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 [L(Θt1)F ] (cid:16) 1 4 + where is the batch size. σb 1 4 1 8 (cid:17) , (cid:80)T (cid:13) (cid:13) (cid:13) Proof sketch. Define ˆMt, ˆvt, and Dt as before, and let Et [] denotes the expectation conditioned on iterate Θt1, and Et := ˆMt L(Θt1). The expected descent step uses Lemma 11 to lower (cid:13) bound ˆMt, OtDt by dt,min ˆMt (cid:13) , and the clamping rule controls the ratio of dt,max and dt,min (cid:13) [Et] is bounded by the same (Appendix E, Step 2). The averaged deviation term 1 variance-plus-drift decomposition as in the proof of Theorem 4, using Assumption 2 to control the noise term and Assumption 1 to control the drift term via telescoping argument; then averaging uses Lemma 9 (Appendix E, Step 3). For the denominator, Minkowski and Jensens inequalities yield an upper bound on E[ ] by [L(Θt1)] plus an explicit additive term containb; then the averaging over is bounded using Lemma 10 (Appendix E, Step 4). Finally, ing σ/ (cid:13) (cid:105) ˆMt CauchySchwarz argument relate (cid:13) to the expected gradient norm, yielding (cid:13) quadratic inequality in [L(Θt1)] that can be solved explicitly (Appendix E, Step 5). Combining these bounds with the specified hyperparameter choices gives the stated convergence rate (Appendix E, Step 6). The full proof is provided in Appendix E. (cid:104) dt,max j=1 ˆvj (cid:113)(cid:80)n (cid:13) (cid:13) (cid:13)F t=1 This result indicates that the convergence rate of NAMO-D, like that of NAMO, is adaptive to the noise level of the stochastic gradients, and that the optimal O(T 1/4) rate is recovered when choosing the batch size as = Ω(σ ), 9 Zhang, Liu and Schaeffer The results of this section demonstrate that both NAMO and NAMO-D can achieve optimal convergence rates in terms of the order on . This occurs in both the deterministic and stochastic In the deterministic setting, the O(T 1/2) bound matches the settings under Assumptions 1-2. known lower complexity bound for smooth nonconvex optimization with first-order methods, thus demonstrating that utilizing orthogonalized update and adaptive scaling does not incur any deterioration in the convergence or complexity. In the stochastic regime, the bounds decompose naturally into an optimization term of order O(T 1/4) and an explicit variance-dependent term scaling as (cid:0) σ b1/4T 1/8(cid:1), and thus quantifying the precise affect of gradient noise and batch size. Specifically, when = Ω(σ2T 1/2), the variance term becomes asymptotically dominated and the optimal O(T 1/4) rate is recovered. The analysis further shows that the orthogonalized descent inequality, combined with bias-corrected moment estimates and controlled adaptive scaling (scalar or diagonal), yields unified proof strategy that is robust to both drift and stochastic perturbations. Altogether, these guarantees provide rigorous complexity characterization of the proposed methods and confirm that their matrix-aware adaptive scaling preserves optimal first-order performance while maintaining stability under noise."
        },
        {
            "title": "4.1 Experimental setup",
            "content": "Baselines. We compare our proposed algorithms, NAMO and NAMO-D, against two popular baselines: AdamW (Loshchilov and Hutter, 2017) and Muon (Jordan et al., 2024b). Since Muon, NAMO, and NAMO-D are designed specifically for matrix parameters, we use AdamW to optimize all remaining scalar and vector parameters across all models. For brevity, we refer to these hybrid optimizers simply as Muon, NAMO, and NAMO-D. Model Architectures. We evaluate all optimizers on GPT-2 pretraining experiments, which are based on the nanoGPT (Karpathy, 2022) implementation of the GPT-2 architecture (Radford et al., 2019). Two model sizes are considered: small (124M parameters), and medium (355M parameters). All experiments are conducted on 4 NVIDIA H100 GPUs. Dataset. All experiments are conducted on the OpenWebText dataset (Gokaslan et al., 2019), which contains approximately 9 billion training tokens and 4.4 million validation tokens. Training details. We use standard hyperparameter settings for the baselines. For AdamW, we set the momentum coefficients to β1 = 0.9 and β2 = 0.95. For Muon, the momentum coefficient is set to β = 0.95. For NAMO and NAMO-D, the momentum coefficients are set to µ1 = 0.95 and µ2 = 0.99. The weight decay coefficient is set to λ = 0.01 for all optimizers. The Muon optimizer applies decoupled weight decay as AdamW: Θt = Θt1 η (Ot + λΘt1) , where η is the prescribed learning rate. For NAMO and NAMO-D, as part of the effective stepsize, the adaptive scaling is applied to the decoupled weight decay as well. Specifically, NAMO updates by: where αt is given in Algorithm 1, and NAMO-D updates by: Θt = Θt1 ηαt (Ot + λΘt1) , Θt = Θt1 η (Ot + λΘt1) Dt, 10 Adam Improves Muon where Dt is given in Algorithm 2. For GPT-2 (124M) model pretraining, we train with context length = 1024 and an effective batch size of 480 sequences (491,520 tokens) per optimizer update for all optimizers (micro-batch size = 60 and gradient accumulation = 8). For GPT-2 (355M) model, we also train with context length = 1024 and an effective batch size of 480 sequences (491,520 tokens) per optimizer update for all optimizers (micro-batch size = 40 and gradient accumulation = 12). For both models and all optimziers, we use the following learning rate scheduler: 2000-step linear warm-up followed by constant learning rate. Also, for both models, we sweep for optimal learning rate η for each optimizer through grid search. For NAMO-D, we fix the clamping hyperparameter = 0.1 for GPT-2 (124M) experiments, and sweep for optimal for GPT-2 (355M) experiments. 4.2 Empirical Results on GPT-2 For pretraining the GPT-2 (124M) model, we perform grid search for each optimizer to find the optimal learning rate (LR) that achieves the lowest validation loss after 10K steps. The sweeping results are presented in Figure 1, which shows that NAMO and NAMO-D achieve lower training and validation losses across wider range of learning-rate choices, demonstrating both accelerated convergence and improved tuning robustness comparing to Muon and AdamW. The optimal learning rate selected from the 10K-step sweep for each optimizer is listed in Table 1. While NAMO and NAMO-D exhibit similar training and validation losses in the 10K-step learningrate sweep, Figure 2 shows that when training is extended to 50K steps using the selected optimal learning rate, NAMO-D attains lower training and validation losses than NAMO, demonstrating advantages of its finer-grained neuron-wise adaptive scaling. The training and validation losses at termination for each optimizer are reported in Table 2. We further conduct experiments on pretraining the GPT-2 (355M) model. For each optimizer, we sweep from five learning rates that are around the chosen optimal learning rate for GPT-2 (124M) model listed in Table 1. Specifically, for Muon and AdamW, we sweep for the optimal LR from the following set: {0.0006, 0.0009, 0.0013, 0.0018, 0.0025} ; for NAMO and for NAMO-D, we sweep for the optimal LR from the following set: {0.005, 0.007, 0.009, 0.012, 0.015} . The optimal LR is the one that achieves the lowest validation loss after 10K steps. Additionally, for NAMO-D, we observe that its performance on the GPT-2 (355M) model can vary for different choices of the clamping hyperparameter c. Therefore, we also sweep for an optimal from the following set: {0.12, 0.40, 0.75, 0.90} . The optimal hyperparameters for each optimizer for the GPT-2 (355M) model are listed in Table 1, and the training and validation losses at termination for each optimizer are reported in Table 2, together with those for the GPT-2 (124M) model. Figure 3 show that NAMO and NAMO-D outperforms both Muon and AdamW, with NAMO-D providing further gains over NAMO through the additional hyperparameter c. This observation is consistent with the algorithmic design of the two proposed algorithms. NAMO augments Muon with adaptivity while preserving the orthogonality of the update direction. On the other hand, NAMO-D enables neuron-wise adaptivity while no longer strictly preserving the orthogonalized direction, and suitable choice of the clamping parameter balances the two competing goals of maintaining the structural advantages of well-conditioned update direction and benefiting from finer-grained noise adaptation. 11 Zhang, Liu and Schaeffer Table 1: Optimal hyperparameters for all optimizers and pretraining of two model sizes. Grid search is used to determine the optimal choices. Optimizer GPT-2 (124M) Hyperparameters GPT-2 (355M) Hyperparameters AdamW Muon NAMO NAMO-D η = 0.0013 η = 0.0013 η = 0.012 η = 0.009, = 0.1 η = 0.0009 η = 0.0009 η = 0.007 η = 0.009, = 0. (a) Training loss (b) Validation Loss Figure 1: Hyperparameter sweeping results for GPT-2 (124M). The training and validation losses at step 10K are reported, where the x-axis is the learning rate. (a) Training loss (b) Validation Loss Figure 2: Pretraining GPT-2 (124M) for 50K steps. The optimal LR from sweeping for 10K steps is used. 12 Adam Improves Muon Table 2: Final training and validation losses for GPT-2 (124M) and GPT-2 (355M). GPT-2 (124M) is trained for 50K steps, and GPT-2 (355M) is trained for 10K steps. Optimizer AdamW Muon NAMO NAMO-D GPT-2 (124M) GPT-2 (355M) Training Loss Validation Loss Training Loss Validation Loss 3.0456 3.0265 2.9272 2. 3.0643 3.0435 3.0351 3.0246 2.9760 2.9524 2.9359 2.9351 2.9914 2.9684 2.9516 2.9507 (a) Training loss (b) Validation Loss Figure 3: Pretraining GPT-2 (355M) for 10K steps. The optimal LR (and optimal for NAMO-D) from sweeping for 10K steps are used. 13 Zhang, Liu and Schaeffer"
        },
        {
            "title": "5 Conclusions and Future Work",
            "content": "In this work, we propose new optimizer and diagonal extension, NAMO and NAMO-D, which provide the first theoretically principled integration of an orthogonalized update direction with norm-based adaptive moment estimation for noise adaptation. NAMO rescales Muons orthogonalized momentum by single adaptive scalar, thereby preserving the orthogonality of the update direction while yielding performance improvements over Muon at negligible additional computational cost. NAMO-D instead right-multiplies the orthogonalized momentum by diagonal matrix, enabling finer-grained, neuron-wise noise adaptation for further improving performance, albeit without strictly preserving orthogonality of the update direction. Under standard smoothness and unbiased bounded-variance noise assumptions, we establish optimal convergence rates for both algorithms in the deterministic setting and show that, in the stochastic setting, their convergence guarantees adapt to the noise level of the stochastic gradients and attain the optimal rate when the batch size is sufficiently large. Experiments on pretraining GPT-2 (124M) and GPT-2 (355M) models demonstrate the improved performance of both NAMO and NAMO-D compared to the AdamW and Muon baselines. NAMO-D provides further gains over NAMO through an additional clamping hyperparameter, which can be tuned to balance two competing goals: maintaining the structural advantages of well-conditioned update direction and leveraging finer-grained noise adaptation. Future work includes evaluating NAMO and NAMO-D on larger LLMs, developing tuning-light variants of NAMO-D, and further investigating theoretical and practical advantages of noise-adaptive scaling for orthogonalized updates."
        },
        {
            "title": "Acknowledgement",
            "content": "Funding. This work was supported in part by NSF DMS 2502561."
        },
        {
            "title": "References",
            "content": "Kang An, Yuxing Liu, Rui Pan, Yi Ren, Shiqian Ma, Donald Goldfarb, and Tong Zhang. Asgo: Adaptive structured gradient optimization. arXiv preprint arXiv:2503.20762, 2025. Yossi Arjevani, Yair Carmon, John Duchi, Dylan Foster, Nathan Srebro, and Blake Woodworth. Lower bounds for non-convex stochastic optimization. Mathematical Programming, 199(1):165 214, 2023. Lukas Balles and Philipp Hennig. Dissecting adam: The sign, magnitude and variance of stochastic gradients. In International Conference on Machine Learning, pages 404413. PMLR, 2018. Jeremy Bernstein. The modula docs, 2025. URL https://docs.modula.systems/. Jeremy Bernstein and Laker Newhouse. Old optimizer, new norm: An anthology. arXiv preprint arXiv:2409.20325, 2024. Valentyn Boreiko, Zhiqi Bu, and Sheng Zha. Towards understanding of orthogonalization in muon. In High-dimensional Learning Dynamics 2025, 2025. Yair Carmon, John Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary points i. Mathematical Programming, 184(1):71120, 2020. Lizhang Chen, Jonathan Li, and Qiang Liu. Muon optimizes under spectral norm constraints. arXiv preprint arXiv:2506.15054, 2025. 14 Adam Improves Muon Savelii Chezhegov, Yaroslav Klyukin, Andrei Semenov, Aleksandr Beznosikov, Alexander Gasnikov, Samuel Horvath, Martin Takaˇc, and Eduard Gorbunov. Clipping improves adam-norm and adagrad-norm when the noise is heavy-tailed. arXiv preprint arXiv:2406.04443, 2024. Michael Crawshaw, Chirag Modi, Mingrui Liu, and Robert Gower. An exploration of noneuclidean gradient descent: Muon and its many variants. arXiv preprint arXiv:2510.09827, 2025. Damek Davis and Dmitriy Drusvyatskiy. When do spectral gradient updates help in deep learning? arXiv preprint arXiv:2512.04299, 2025. Zhaorui Dong, Yushun Zhang, Jianfeng Yao, and Ruoyu Sun. Towards quantifying the hessian structure of neural networks. arXiv preprint arXiv:2505.02809, 2025. Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus, 2019. Jie Hao, Xiaochuan Gong, Jie Xu, Zhengdao Wang, and Mingrui Liu. Noise-adaptive layerwise learning rates: Accelerating geometry-aware optimization for deep neural network training. arXiv preprint arXiv:2510.14009, 2025. Wei He, Kai Han, Hang Zhou, Hanting Chen, Zhicheng Liu, Xinghao Chen, and Yunhe Wang. Root: Robust orthogonalized optimizer for neural network training. arXiv preprint arXiv:2511.20626, 2025. Nicholas Higham. Computing the polar decompositionwith applications. SIAM Journal on Scientific and Statistical Computing, 7(4):11601174, 1986. Nicholas Higham. Functions of matrices: theory and computation. SIAM, 2008. Keller Jordan, Jeremy Bernstein, Ben Rappazzo, Vlado, Jiacheng, Cesista, and Koszarsky. Modded-nanogpt: Speedrunning the nanogpt baseline. GitHub repository, 2024a. Keller Jordan, Yuchen Jin, Vlado Boza, You Jiacheng, Franz Cesista, Laker Newhouse, and Jeremy Bernstein. Muon: An optimizer for hidden layers in neural networks, 2024b. URL https: //kellerjordan.github.io/posts/muon/. Andrej Karpathy. Nanogpt. https://github.com/karpathy/nanoGPT, 2022. Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Dmitry Kovalev. Understanding gradient orthogonalization for deep learning via non-euclidean trust-region optimization. arXiv preprint arXiv:2503.12645, 2025. Jiaxiang Li and Mingyi Hong. note on the convergence of muon. arXiv preprint arXiv:2502.02900, 2025. Zichong Li, Liming Liu, Chen Liang, Weizhu Chen, and Tuo Zhao. Normuon: Making muon more efficient and scalable. arXiv preprint arXiv:2510.05491, 2025. Jingyuan Liu, Jianlin Su, Xingcheng Yao, Zhejun Jiang, Guokun Lai, Yulun Du, Yidao Qin, Weixin Xu, Enzhe Lu, Junjie Yan, et al. Muon is scalable for llm training. arXiv preprint arXiv:2502.16982, 2025. Zhang, Liu and Schaeffer Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Antonio Orvieto and Robert Gower. In search of adams secret sauce. arXiv preprint arXiv:2505.21829, 2025. Thomas Pethick, Wanyun Xie, Kimon Antonakopoulos, Zhenyu Zhu, Antonio Silveti-Falls, and Volkan Cevher. Training deep learning models with norm-constrained lmos. arXiv preprint arXiv:2502.07529, 2025. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Naoki Sato, Hiroki Naganuma, and Hideaki Iiduka. Analysis of muons convergence and critical batch size. arXiv preprint arXiv:2507.01598, 2025. Ishaan Shah, Anthony M. Polloreno, Karl Stratos, Philip Monk, Adarsh Chaluvaraju, Andrew Hojel, Andrew Ma, Anil Thomas, Ashish Tanwer, Darsh J. Shah, et al. Practical efficiency of muon for pretraining, 2025. Wei Shen, Ruichuan Huang, Minhui Huang, Cong Shen, and Jiawei Zhang. On the convergence analysis of muon. arXiv preprint arXiv:2505.23737, 2025. Chongjie Si, Debing Zhang, and Wei Shen. AdaMuon: Adaptive muon optimizer. arXiv preprint arXiv:2507.11005, 2025. Zitao Song, Cedar Site Bai, Zhe Zhang, Brian Bullins, and David Gleich. Decoupling variance and scale-invariant updates in adaptive gradient descent for unified vector and matrix optimization. arXiv preprint arXiv:2602.06880, 2026. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. Shuche Wang, Fengzhuo Zhang, Jiaxiang Li, Cunxiao Du, Chao Du, Tianyu Pang, Zhuoran Yang, Mingyi Hong, and Vincent YF Tan. Muon outperforms adam in tail-end associative memory learning. arXiv preprint arXiv:2509.26030, 2025. Ashia Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal value of adaptive gradient methods in machine learning. Advances in neural information processing systems, 30, 2017. Yujie Yang. Prism: Structured optimization via anisotropic spectral shaping. arXiv preprint arXiv:2602.03096, 2026. Minxin Zhang, Yuxuan Liu, and Hayden Schaeffer. Adagrad meets muon: Adaptive stepsizes for orthogonal updates. arXiv preprint arXiv:2509.02981, 2025. Yushun Zhang, Congliang Chen, Ziniu Li, Tian Ding, Chenwei Wu, Diederik Kingma, Yinyu Ye, Zhi-Quan Luo, and Ruoyu Sun. Adam-mini: Use fewer learning rates to gain more. arXiv preprint arXiv:2406.16793, 2024. 16 Adam Improves Muon Appendix A. Useful Lemmata This section contains proofs of lemmata used in the analysis of NAMO and NAMO-D. The following lemma is used in the proofs of Theorems 36 for the derivation of uniform upper bound of the adaptive stepsizes. Lemma 7 Assume µ1, µ2 (0, 1] with µ1 µ2, > 0, and g1, g2, , gt Rd. Define mt by m0 = 0 and: mτ = µ1mτ 1 + (1 µ1)gτ , τ = 1, 2, , 1. Define vt by v0 = 0 and: vτ = µ2mτ 1 + (1 µ2) gτ 2 , τ = 1, 2, , 1. Let ˆmt := mt/(1 µt 1) and ˆvt := vt/(1 µt 2). Then: ˆmt ˆvt (cid:114) 1 µ1 1 µ2 . Proof Write w1,t,τ := 1µ1 1µt 1 µtτ 1 and w2,t,τ := 1µ2 1µt 2 µtτ 2 . Then: and ˆmt = 1 µ1 1 µt 1 (cid:88) τ =1 µtτ 1 gτ = (cid:88) τ =1 w1,t,τ gτ , ˆvt = 1 µ2 1 µt 2 (cid:88) τ =1 µtτ 2 gτ 2 = (cid:88) τ =1 w2,t,τ gτ 2 . Since (cid:80)t τ =1 w1,t,τ = (cid:80)t τ =1 w2,t,τ = 1. Under the assumption that µ1 µ2, w1,t,τ = 1 µ1 1 µ2 1 µt 2 1 µt (cid:18) µ1 µ2 (cid:19)tτ w2,t,τ 1 µ1 1 µ2 w2,t,τ . By Cauchy-Schwarz inequality, (cid:88) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) τ =1 (cid:32) (cid:88) ˆmt2 = w1,t,τ gτ (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:33) (cid:32) w1,t,τ (cid:88) (cid:33) w1,t,τ gτ 2 Hence, τ =1 τ =1 w2,t,τ gτ 2 = 1 µ1 1 µ2 1 µ1 1 µ (cid:88) τ =1 ˆvt. αt ˆmt ˆvt (cid:114) 1 µ1 1 µ2 . 17 Zhang, Liu and Schaeffer The lemma below is used in the analysis of NAMO and NAMO-D in the deterministic setting, i.e. in the proofs of Theorem 3 and Theorem 5. Lemma 8 For ϵ > 0 and 0, define ϕϵ(x) := x2 x+ϵ . Then: ϕϵ(x) + (cid:112)ϵϕϵ(x). Proof By the definition of ϕϵ(x), Solving for gives: x2 ϕϵ(x)x ϵϕϵ(x) 0. ϕϵ(x) + (cid:112)ϕϵ(x)2 + 4ϵϕϵ(x) 2 ϕϵ(x) + (cid:112)ϵϕϵ(x). The following two lemmas are used in the proofs of Theorem 4 and Theorem 6. Lemma 9 For µ (0, 1) and > 0, (cid:88) t=1 1 1 µt + µ 1 µ 1 ln µ ln (cid:18) 1 µT 1 µ (cid:19) . Proof For 1, define: Since: the integral test gives: (x) := µx 1 µx . (x) = µx ln µ (1 µx)2 < 0, µt 1 µt = To compute the integral, write := µx, then dy = µx ln µdx = ln µdx, and (t) (1) + (x)dx. t= t=1 1 (cid:90) (cid:88) (cid:88) (cid:90) 1 µx 1 µx dx = (cid:90) µT µ 1 dy ln µ = 1 ln µ (cid:90) µT µ ln(1 y) = 1 ln µ ln (cid:18) 1 µT 1 µ (cid:19) . Hence, It then follows that: (cid:88) t=1 µt 1 µt µ 1 µ 1 ln µ ln (cid:18) 1 µT 1 µ (cid:19) . (cid:88) t=1 1 1 µt = (cid:88) t=1 (cid:18) 1 + (cid:19) µt 1 µt + µ 1 µ 1 ln µ ln (cid:18) 1 µT 1 µ (cid:19) . 18 Adam Improves Muon Lemma 10 For µ (0, 1) and > 0, (cid:88) t=1 1 (cid:112)1 µt 2 ln(1 + (cid:112)1 µT ) ln µ . Proof For 1, define: Since: (x) := 1 1 µx . (x) = µx ln µ 2(1 µx)3/2 < 0, is strictly decreasing on [1, ). Hence, (cid:88) t= 1 (cid:112)1 µt (cid:90) 0 (x)dx. Let := 1 µx, then dy = µx ln µ 1µx dx = (y21) ln µ 2 dx. It follows that: (cid:88) t=1 1 (cid:112)1 µt (cid:90) 1 1 µx dx = 2 ln µ (cid:90) 1µT 1 y2 dy = 1 ln µ ln (cid:32) 1 + (cid:112)1 µT 1 (cid:112)1 µT (cid:33) . Since: 1 ln µ ln (cid:32) 1 + (cid:112)1 µT 1 (cid:112)1 µT (cid:33) = 1 ln µ ln 1 + (cid:112)1 µT (cid:17)2 1 (1 µT ) = 2 ln(1 + (cid:112)1 µT ) ln µ , 2y (cid:16) Hence, (cid:88) t=1 1 (cid:112)1 µt 2 ln(1 + (cid:112)1 µT ) ln µ . The lemma below is used for the analysis for NAMO-D, i.e., in the proofs of Theorem 5 and Theorem 6. Lemma 11 Let Rmn have reduced singular value decomposition (SVD) = ΣV , and let = . Let = diag (d1, , dn) with di 0 for all i, and write dmin := mini di. Then M, OD dmin . Proof Note that: M, OD = Tr (cid:0)M OD(cid:1) = Tr (cid:0)(U ΣV )T D(cid:1) = Tr (cid:0)V ΣV D(cid:1) = Tr (cid:0)ΣV DV (cid:1) . Since dminIn, (D dminIn)V 0. 19 Zhang, Liu and Schaeffer It follows that: M, OD = dmin Tr (Σ) + Tr (cid:0)ΣV (D dminIn)V (cid:1) dmin . Appendix B. Proof of Theorem 3 This section contains the detailed proof of Theorem 3 for the convergence of NAMO in the deterministic setting. Proof Step 1: uniform upper bound on stepsize. For each 0 and τ t, define: w1,t,τ := 1 µ1 1 µt 1 µtτ 1 and w2,t,τ := 1 µ2 1 µt 2 µtτ . It satisfies that (cid:80)t then: τ =1 w1,t,τ = (cid:80)t τ =1 w2,t,τ = 1. Let ˆMt := Mt/(1 µt 1) and ˆvt := vt/(1 µt 2), and Define: ˆMt = 1 µ1 1 µt 1 (cid:88) τ = µtτ 1 L(Θτ 1) = (cid:88) τ =1 w1,t,τ L(Θτ 1), ˆvt = 1 µ2 1 µt (cid:88) τ =1 µtτ 2 L(Θτ 1)2 = (cid:88) τ = w2,t,τ L(Θτ 1)2 , αt := (cid:112)1 µt 2 1 µt 1 MtF vt + ϵ = (cid:13) (cid:13) ˆMt (cid:13) (cid:13) (cid:13)F (cid:13) ˆvt + ϵ/(cid:112)1 µt 2 . Since the Frobenius norm of matrix coincides with the Euclidean norm of its vectorization, by Lemma 7, (cid:13) (cid:13) ˆMt (cid:13) (cid:13) (cid:13)F (cid:13) ˆvt αt = (cid:114) 1 µ1 1 µ2 . (4) Step 2: descent inequality and averaging. By (Zhang et al., 2025, Lemma B.1), L(Θt) L(Θt1) L(Θt1), ηαtOt + 2 η2α2 ηαt L(Θt1) + 2ηαt (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) L(Θt1) 1 µ2 ˆvt + ϵ/ = η (cid:13) (cid:13)L(Θt1) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) + 2 η2α2 + 2ηαt (cid:13) (cid:13)L(Θt1) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) + 2 η2α2 . 20 Adam Improves Muon Rearranging the terms gives: (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) L(Θt1) 1 µ2 ˆvt + ϵ/ 1 T (cid:88) t=1 ηT ηT + + 2 2 (cid:88) t= αt (cid:13) (cid:13)L(Θt1) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) + ηL 2T (cid:88) t=1 α2 (cid:114) 1 µ1 1 µ2 (cid:88) t=1 (cid:13) (cid:13)L(Θt1) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) + ηL 2 (cid:18) 1 µ1 1 µ2 (cid:19) , (5) where := L(Θ0) minΘ L(Θ). Step 3: Bounding the distance between bias-corrected momentum and true gradient. The difference between the scaled momentum ˆMt and the gradient L(Θt1) can be bounded by: (cid:13) ˆMt L(Θt1) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:88) τ =1 (cid:88) τ =1 (cid:88) τ =1 (cid:88) τ =1 w1,t,τ L(Θτ 1) L(Θt1) w1,t,τ Θτ 1 Θt12 w1,t,τ ηαsOs (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2 (cid:33) αs . t1 (cid:88) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) s=τ (cid:32) t1 (cid:88) s=τ w1,t,τ ηL Then by (4) and the definition of w1,t,τ , (cid:13) (cid:13) ˆMt L(Θt1) (cid:13) (cid:13) (cid:13) (cid:13) = (cid:88) τ =1 (cid:88) τ =1 w1,t,τ ηL (t τ ) (cid:114) 1 µ1 1 µ ηL (t τ ) µtτ 1 1 µ1 1 µt 1 (cid:114) 1 µ1 1 µ2 (cid:114) 1 µ1 1 µ2 (cid:88) j=1 jµj 1 ηL ηL 1 µ1 1 µt 1 (cid:114) 1 µ1 1 µ2 µ1 (1 µ1)2 . (6) Step 4: Bounding ˆvt. For ˆvt, it satisfies that: ˆvt = (cid:88) τ =1 (cid:88) τ =1 (cid:88) τ = w2,t,τ L(Θτ 1)2 w2,t,τ (L(Θt1) + Θτ 1 Θt12)2 (cid:32) w2,t,τ L(Θt1) + ηL (cid:33)(cid:33)2 (cid:32) t1 (cid:88) s=τ αs 21 Zhang, Liu and Schaeffer Then by (4) and the definition of w2,t,τ , ˆvt (cid:88) τ =1 (cid:18) w2,t,τ L(Θt1) + ηL (t τ ) (cid:19)2 (cid:114) 1 µ1 1 µ2 = L(Θt1)2 + 2ηL L(Θt1) (cid:114) 1 µ1 1 µ (cid:88) τ =1 w2,t,τ (t τ ) + η2L2 (cid:18) 1 µ1 1 µ2 (cid:19) (cid:88) τ =1 w2,t,τ (t τ )2. By Cauchy-Schwarz inequality, it follows that: ˆvt L(Θt1)2 + 2ηL L(Θt1) + η2L2 (cid:18) 1 µ1 1 µ2 (cid:19) (cid:32) (cid:88) τ =1 w2,t,τ (t τ )2 (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) τ =1 (cid:114) 1 µ1 1 µ2 (cid:33) w2,t,τ (t τ )2 L(Θt1) + ηL (cid:114) 1 µ1 1 µ2 (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) τ =1 2 w2,t,τ (t τ )2 = (L(Θt1) + at)2 , at := ηL (cid:114) 1 µ1 1 µ2 (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) τ =1 w2,t,τ (t τ )2. η2L2 a2 η2L2 η2L (cid:18) 1 µ1 1 µ2 (cid:18) 1 µ1 1 µ2 (cid:18) 1 µ1 1 µ2 (cid:33) µtτ 2 (t τ )2 (cid:19) (cid:32) (cid:88) τ = (cid:33) (cid:19) (cid:18) 1 µ2 1 µt 2 (cid:19) (cid:32) (cid:88) 2τ 2 µτ τ =1 (cid:19) (cid:18) µ2(1 + µ2) (1 µ2) (cid:19) := a2. (cid:112) ˆvt L(Θt1) + a. 22 where"
        },
        {
            "title": "Since",
            "content": "Hence, (7) (8) Adam Improves Muon Step 5: Relating αtL(Θt1) to L(Θt1). Combining (6) and (8) gives: αt L(Θt1) = (cid:13) (cid:13) ˆMt (cid:13) (cid:13) L(Θt1 (cid:13) (cid:13)F ˆvt + ϵ/(cid:112)1 µt 2 (cid:13) ˆMt L(Θt1) (cid:13) L(Θt1F (cid:13) (cid:16) (cid:17) (cid:13) (cid:13) (cid:13)F L(Θt1) + ϵ/(cid:112)1 µt 2 + (cid:13) (cid:13) ˆMt L(Θt1) (cid:13) (cid:13) (cid:13)F (cid:13) L(Θt1 2 + (cid:18) 1 L(Θt1F L(Θt1 L(Θt1) + ϵ/(cid:112)1 µt L(Θt1F L(Θt1 L(Θt1) + ϵ/(1 µt 2) L(Θt1F L(Θt1 L(Θt1) + ϵ/(1 µt 2) L(Θt1F L(Θt1 L(Θt1) + ϵ/(1 µt 2) L(Θt12 / L(Θt1) + ϵ ηL L(Θt1) + ϵ/(1 µt (cid:13) (cid:13) ˆMt L(Θt1) (cid:13) (cid:13) (cid:13)F (cid:13) (cid:114) 1 µ1 1 µ2 µ1 (1 µ1) µ1 (1 µ1)2 ηL (cid:114) 1 µ1 1 µ2 (cid:19) (cid:13) (cid:13) ˆMt L(Θt1) (cid:13) (cid:13) (cid:13)F (cid:13) 2) + where ϵ := ϵ/(1 µ2), := min{m, n} and the constant is given in (7). Define ϕϵ(x) := x2 0, it then follows from (5) and (6) that: x+ϵ for"
        },
        {
            "title": "1\nT",
            "content": "(cid:88) ϕϵ(L(Θt1) t=1"
        },
        {
            "title": "1\nT",
            "content": "(cid:13) (cid:13) ˆMt (cid:13) (cid:13) (cid:13) L(Θt1 (cid:13) ˆvt + ϵ/(1 µt 2) (cid:88) t=1 + + ηL (cid:114) 1 µ1 1 µ2 µ1 (1 µ1)2 (cid:13) (cid:13)L(Θt1) ˆMt (cid:13) (cid:19) ηL 2 (cid:18) 1 µ1 1 µ + + ηL (cid:13) (cid:13)L(Θt1) ˆMt (cid:13) t=1 (cid:19) 2ηLµ1 (1 µ1)2 + ηL 2 (cid:18) 1 µ1 1 µ ηL 2 + ηL (cid:19) + + ηL (cid:18) 1 µ1 1 µ2 (cid:112)µ2(1 µ1)(1 + µ2) (1 µ2)2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) + + (cid:19) (cid:114) 1 µ1 1 µ2 (cid:114) 1 µ1 1 µ2 (cid:33) (cid:33) µ1 (1 µ1)2 µ1 (1 µ1)2 + (cid:114) 1 µ1 1 µ2 ηLµ1 (1 µ1) (cid:33) (cid:114) 1 µ1 1 µ2 (cid:114) 1 µ1 1 µ"
        },
        {
            "title": "2\nT",
            "content": "T (cid:88) t=1 (cid:88) (cid:32) (cid:32) (cid:32) ηT ηT ηT + + + ηT = where (cid:18) 1 µ1 1 µ2 + ηLCµ r, Cµ := (cid:19) (cid:18) 1 µ1 1 µ2 2µ1 (1 µ1)2 + 1 2 (cid:19) (cid:18) 1 µ1 1 µ2 + (cid:112)µ2(1 µ1)(1 + µ2) (1 µ2)2 + (cid:114) 1 µ1 1 µ2 µ1 (1 µ1)2 is constant that depends on µ1 and µ2. Zhang, Liu and Schaeffer Step 6: Deriving the convergence rate. By Lemma 8, 1 (cid:88) t=1 L(Θt1 1 1 (cid:88) t= (cid:88) t=1 ηT ϕϵ (L(Θt1) + 1 T (cid:88) (cid:113) ϵϕϵ (L(Θt1) t=1 (cid:118) (cid:117) (cid:117) (cid:116) 1 ϵ 2 T (cid:88) ϕϵ (L(Θt1) + ϕϵ (L(Θt1) + ηLCµ + ϵ 1 2 (cid:115) ηT t= + ηLCµ r. In particular, if choosing η = (cid:16) 2 (cid:17) , µ1 = Θ(1), µ2 = Θ(1), and ϵ = O(T 1 2 ), then: 1 (cid:88) t=1 L(Θt1)F 1 (cid:88) t=1 L(Θt1) (cid:16) 1 2 (cid:17) for large > 0. The proof is thus completed. Appendix C. Proof of Theorem This section contains the detailed proof of Theorem 4 for the convergence of NAMO in the stochastic setting."
        },
        {
            "title": "Proof",
            "content": "Step 1: uniform upper bound on stepsize For each 0 and τ t, define: w1,t,τ := 1 µ1 1 µt 1 µtτ 1 and w2,t,τ := 1 µ2 1 µt µtτ 2 . It satisfies that (cid:80)t then: τ =1 w1,t,τ = (cid:80)t τ =1 w2,t,τ = 1. Let ˆMt := Mt/(1 µt 1) and ˆvt := vt/(1 µt 2), and Define: ˆMt = 1 µ1 1 µt 1 (cid:88) τ =1 µtτ 1 Gτ = (cid:88) τ =1 w1,t,τ Gτ , ˆvt = 1 µ2 1 µt 2 (cid:88) τ =1 µtτ 2 Gτ 2 = (cid:88) τ =1 w2,t,τ Gτ 2 , αt := (cid:112)1 µt 2 1 µt 1 MtF vt + ϵ = (cid:13) (cid:13) ˆMt (cid:13) (cid:13) (cid:13)F (cid:13) ˆvt + ϵ/(cid:112)1 µt 2 . Since the Frobenius norm of matrix coincides with the Euclidean norm of its vectorization, by Lemma 7, (cid:13) (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13)F ˆvt αt 24 = (cid:114) 1 µ1 1 µ2 . (9) Adam Improves Muon Step 2: Expected descent and an average bound on αt ˆMt. Let Et[] := E[Θt1] denote the conditional expectation given the previous iterates Θ0, , Θt1, and write Et := ˆMt L(Θt1). Then by (Zhang et al., 2025, Lemma B.1), Et [L(Θt) L(Θt1)] Et [ L(Θt1), ηαtOt] + η2L 2 (cid:69)(cid:105) (cid:68) L(Θt1) ˆMt, ηαtOt (cid:104) Et (cid:3) (cid:2)α2 Et (cid:104) ηαt (cid:13) ˆMt (cid:13) (cid:13) (cid:104) ηαt (cid:13) (cid:13)L(Θt1) ˆMt (cid:13) (cid:105) (cid:13) (cid:13) (cid:13) Et (cid:104) ηαt (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) =Et (cid:32) Et (cid:105) (cid:13) (cid:13) (cid:13) (cid:33) (cid:105) + + η2L 2 η2L 2 Et (cid:3) (cid:2)α2 Et (cid:3) (cid:2)α2 Et (cid:104) ηαt (cid:105) (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) + Et [ηαt Et] + η2L 2 Et (cid:2)α2 (cid:3) . Rearranging the terms gives: (cid:104) αt Et (cid:105) (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) Et [L(Θt1) L(Θt)] + Et [αt Et] + ηL 2 Et (cid:2)α2 (cid:3) . Then by the law of total expectation and (9),"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 (cid:104) αt (cid:105) (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) ηT ηT + +"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 [αt Et] + ηL 2T (cid:88) t= (cid:2)α2 (cid:3) (cid:114) 1 µ1 1 µ2 (cid:88) t=1 [Et] + ηL(1 µ1) 2(1 µ2) . (10) Step 3: Bounding the distance between bias-corrected momentum and true gradient. For each t, it satisfies: Et = ˆMt (cid:105) (cid:104) ˆMt + (cid:105) (cid:104) ˆMt L(Θt1) = (cid:88) τ =1 w1,t,τ (Gτ L(Θτ 1)) + (cid:88) τ =1 w1,t,τ (L(Θτ 1) L(Θt1)) 25 Hence, by (9), (cid:104) Et2 (cid:105) Zhang, Liu and Schaeffer w1,t,τ (Gτ L(Θτ 1)) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) + (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:88) τ =1 w1,t,τ (L(Θτ 1) L(Θt1)) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) t (cid:88) + w1,t,τ L(Θτ 1) L(Θt1)2 (cid:88) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) τ =1 (cid:18) 1 µ1 1 µt 1 (cid:18) 1 µ1 1 µt 1 (cid:18) 1 µ1 1 µt 1 (cid:18) 1 µ1 1 µt 1 (cid:18) 1 µ1 1 µt 1 (cid:18) 1 µ1 1 µt 1 (cid:18) 1 µ1 1 µt 1 = (cid:19) (cid:88) τ = 1 L2 Θτ 1 Θt12 µtτ 2 (cid:19) (cid:88) L2η2 µtτ τ =1 (cid:88) L2η2 (cid:33)2 (cid:32) t1 (cid:88) s=τ αs (cid:35) µtτ 1 (t τ )2 (cid:19)2 (cid:32) (cid:88) (cid:33) µ2(tτ ) σ2 τ =1 (cid:19)2 (cid:18) 1 µ2t 1 1 µ2 1 (cid:19) σ2 + τ =1 (cid:18) 1 µ1 1 µt 1 (cid:18) 1 µ1 1 µt 1 (cid:18) 1 µ1 1 µ2 (cid:18) 1 µ1 1 µ + + (cid:19) (cid:19) (cid:19) σ2 + (cid:19) (cid:18) 1 + µt 1 1 + µ1 (cid:19) (cid:34)(cid:18) 1 + µt 1 1 + µ1 (cid:19) (cid:34)(cid:18) 1 + µt 1 1 + µ1 (cid:19) (cid:20)(cid:18) 1 + µt 1 1 + µ1 (cid:19) σ2 (cid:19) σ2 (cid:19) σ2 (cid:19) σ2 + µ1(1 + µ1)L2η2 (1 µ2) (1 µ1)(1 µt 1) . L2η2 τ =1 (cid:88) τ =1 (cid:35) 1τ 2 µτ (cid:18) + µ1(1 + µ1) (1 µ2) (1 µ1)2 (cid:19) (cid:21) L2η Then by Lemma 9, it follows that:"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 (cid:104) Et2 (cid:105) (cid:32) (cid:18)"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 1 1 µt 1 (cid:33) (cid:18) σ2(1 µ1) + µ1(1 + µ1)L2η2 (1 µ1)(1 µ2) (cid:19) 1 + µ1 (1 µ1)T"
        },
        {
            "title": "1\nT ln µ1",
            "content": "ln (cid:18) 1 µT 1 1 µ1 (cid:19)(cid:19) (cid:18) σ2(1 µ1) + µ1(1 + µ1)L2η2 (1 µ1)(1 µ2) (cid:19) . By Cauchy-Schwarz inequality and Jensens inequality, (cid:118) (cid:117) (cid:117) (cid:116) [Et]"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 (cid:88) (cid:104) (cid:105) Et2 t=1 (cid:115) 1 + µ1 (1 µ1)T"
        },
        {
            "title": "1\nT ln µ1",
            "content": "ln (cid:18) 1 µT 1 1 µ1 (cid:19) (cid:32) σ(cid:112)r(1 µ1) (cid:115) + Lη (cid:33) rµ1(1 + µ1) (1 µ2) (1 µ1) (11) . 26 Adam Improves Muon Step 4: Bounding E[ ˆvt]. For ˆvt, by Minkowski inequality and Jensens inequality, (cid:104)(cid:112) (cid:105) ˆvt =E (cid:118) (cid:117) (cid:117) (cid:116) (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) τ =1 (cid:88) τ = w2,t,τ Gτ 2 w2,t,τ L(Θτ 1) Gτ 2 + (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) w2,t,τ L(Θτ 1) + [L(Θt1)F ] + (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) τ =1 w2,t,τ L(Θτ 1) L(Θt1)2 σ σ (9) σ σ σ σ σ + [L(Θt1)F ] + ηLE τ =1 (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) (cid:32) (cid:88) w2,t,τ τ = τ =1 (cid:33)2 αs + [L(Θt1)F ] + ηL + [L(Θt1)F ] + ηL + [L(Θt1)F ] + ηL + [L(Θt1)F ] + ηL (cid:115) (cid:115) (cid:115) (cid:115) + [L(Θt1)F ] + 2ηL 1 µ1 1 µt 2 1 µ1 1 µt 2 1 µ1 1 µt 2 (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) τ =1 (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) τ =1 (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) τ =1 µtτ 2 (t τ )2 µtτ 2 (t τ ) µτ 2τ 2 (1 µ1)µ2(1 + µ2) (1 µt 2)(1 µ2)3 (cid:115) (1 µ1) 2)(1 µ2)3 . (1 µt Hence, where Then by Lemma 10, (cid:34) (cid:112) ˆvt + (cid:35) ϵ (cid:112)1 µt 2 [L(Θt1)] + at, (12) at := (cid:115) 2ηL + σ (1 µ1) 2)(1 µ2)3 + (1 µt ϵ (cid:112)1 µt ."
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 at σ (cid:88) t= 1 + (cid:32) (cid:115) 2ηL (1 µ1) (1 µ2)3 + ϵ (cid:33) 1 2 ln(1 + (cid:113) 1 µT 2 ) ln µ . (13) 27 Zhang, Liu and Schaeffer Step 5: Relating (cid:104) αt (cid:105) (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F to [L(Θt1)F ]. By Cauchy-Schwarz inequality, (cid:16) (cid:104)(cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F (cid:105)(cid:17)2 = (cid:16)(cid:112) ˆvt + ϵt (cid:17) 1 (cid:1) 1 2 2 (cid:13) (cid:13) (cid:13)F (cid:13) ˆMt (cid:13) (cid:13) (cid:0) (cid:13) ˆMt (cid:13) (cid:13) ˆvt + ϵt (cid:13) (cid:105) (cid:13) (cid:13)F (cid:104) αt (cid:104)(cid:112) ˆvt + ϵt (cid:105) , where ϵt := ϵ/(cid:112)1 µt 2. Combining the above with (12) gives: (cid:16) (cid:105)(cid:17)2 (cid:104)(cid:13) ˆMt (cid:13) (cid:13) [L(Θt1)F ] + at (cid:13) (cid:13) (cid:13)F (E [L(Θt1)F ] [EtF ])2 [L(Θt1)F ] + at . (cid:104) αt (cid:105) (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F Rearranging the terms gives: (cid:16) (E [L(Θt1)F ])2 2E [EtF ] + (cid:104) αt (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F (cid:105)(cid:17) [L(Θt1)F ]atE (cid:104) αt (cid:105) (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F +E [EtF ]2 0. Then solving for [L(Θt1)F ] gives: [L(Θt1)F ] (cid:104) αt 2E [EtF ] + (cid:114)(cid:16) (cid:105) (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F + 2E [EtF ] + (cid:104) αt (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F (cid:105)(cid:17)2 + 4atE (cid:104) αt (cid:105) (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F 4E [EtF ]2 [EtF ] + (cid:104) αt (cid:13) ˆMt (cid:13) (cid:13) 1 2 [EtF ] + (cid:104) αt (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F (cid:105) + (cid:114) 1 4 (cid:114) (cid:13) (cid:13) (cid:13)F (cid:105) + (E [EtF ] + at) (cid:104) αt (cid:105) (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F 2 (cid:105)2 (cid:104) αt (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F + (E [EtF ] + at) (cid:104) αt (cid:105) (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F Step 6: Deriving the convergence rate. By Cauchy-Schwarz inequality,"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 (cid:88) t=1 [L(Θt1)F ] [EtF ] +"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 (cid:104) αt (cid:105) (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F + (cid:118) (cid:117) (cid:117) (cid:116)"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 (E [EtF ] + at) (cid:118) (cid:117) (cid:117) (cid:116)"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 (cid:104) αt (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F (cid:105) . 28 Adam Improves Muon Combining the above with (10) and (13) gives: (cid:88) t=1 (cid:88) [L(Θt1)F ] [Et] + ηT + 1 1 + 1 + t=1 (cid:118) (cid:117) (cid:117) (cid:116) 1 (cid:88) t=1 t=1 (cid:118) (cid:117) (cid:117) (cid:117) (cid:116) 1 T (cid:88) t=1 + (cid:114) 1 µ1 1 µ2 1 (cid:88) t=1 [Et] ηL(1 µ1) 2(1 µ2) (cid:118) (cid:117) (cid:117) (cid:116) ηT (E [EtF ] + at) + (cid:114) 1 µ1 1 µ2 1 (cid:88) t=1 [Et] + ηL(1 µ1) 2(1 µ2) (cid:88) [Et] + ηT + ηL(1 µ1) 2(1 µ2) + (cid:114) 1 µ1 1 µ2 1 (cid:88) t=1 [Et] [EtF ] + (cid:32) (cid:115) 2ηL σ + (1 µ1) (1 µ2)3 + ϵ (cid:33) 1 2 ln(1 + (cid:113) 1 µT 2 ) ln µ (cid:118) (cid:117) (cid:117) (cid:116) ηT + (cid:114) 1 µ1 1 µ"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 [Et] + ηL(1 µ1) 2(1 µ2) . In particular, for large > 0, if choosing η = O(T 3 0 µ1 µ2 < 1, and ϵ = O(T 2 ), then, by (11), 4 ), 1 µ1 = Θ(T 1 2 ), 1 µ2 = Θ(T 1 2 ), and it follows that:"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 [Et] (cid:16) 1 (cid:17) , (cid:88) [L(Θt1)F ] (cid:16) 4 + σb 1 4 1 8 (cid:17) ,"
        },
        {
            "title": "1\nT",
            "content": "t=1 where is the batch size. The proof is thus completed. Appendix D. Proof of Theorem 5 This section contains the detailed proof of Theorem 5 for the convergence of NAMO-D in the deterministic setting."
        },
        {
            "title": "Proof",
            "content": "Step 1: uniform upper bound on stepsize. For each 0 and τ t, define: w1,t,τ := µtτ 1 and w2,t,τ := 1 µ1 1 µt 1 τ =1 w2,t,τ = 1. For each = 1, , and > 0, let 1 µ2 1 µt µtτ 2 . denote the j-th element of vt. Write ˆMt := 1 1µt 1 Mt, ˆvt := 1 1µt 2 denote vt, It satisfies that (cid:80)t the j-th column of Mt, and vj and τ =1 w1,t,τ = (cid:80)t (cid:18) (cid:26) Dt := diag min max (cid:8)dt, dt1(cid:9) , (cid:27)(cid:19) dt1 , 1 29 Zhang, Liu and Schaeffer where dt and dt are given in Algorithm 2. Then: ˆMt = 1 µ1 1 µt 1 (cid:88) τ =1 µtτ 1 L(Θτ 1) = (cid:88) τ =1 w1,t,τ L(Θτ 1), (14) (15) (16) and ˆvt = 1 µ2 1 µt 2 (cid:88) τ =1 By Lemma 7, Write: µtτ 2 Nc(L(Θτ 1)) Nc(L(Θτ 1)) = (cid:88) τ =1 w2,t,τ Nc(L(Θτ 1)) Nc(L(Θτ 1)). [Dt]jj (cid:13) ˆM (cid:13) (cid:13) (cid:113) (cid:13) (cid:13) (cid:13) ˆvj (cid:114) 1 µ1 1 µ2 , j. dt,max := max [Dt]jj , and dt,min := min [Dt]jj . Then the condition number of Dt is given by: κ (Dt) = dt,max dt,min (cid:26) min κt, (cid:27) , 1 c2 where (0, 1] is the fixed clamping hyperparameter, κt := κ (diag (dt)) denotes the condition number of diag (dt) . Step 2: Descent inequality and averaging. By (Zhang et al., 2025, Lemma B.1), L(Θt) L(Θt1) L(Θt1), ηOtDt + 2 ηdt,min L(Θt1) + 2ηdt,max η2 Dt2 2 (cid:13) (cid:13)L(Θt1) ˆMt (cid:13) 2 (cid:13) (cid:13)L(Θt1) ˆMt (cid:13) η2 Dt2 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) + + 2 η2 Dt2 2 . = η max{κ1 , c2}dt,max L(Θt1) + 2ηdt,max Rearranging the terms gives:"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 dt,max L(Θt1) ηT c2 +"
        },
        {
            "title": "2\nT c2",
            "content": "T (cid:88) t=1 dt,max (cid:13) (cid:13)L(Θt1) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) + ηL 2T c2 ηT c2 +"
        },
        {
            "title": "2\nT c2",
            "content": "(cid:114) 1 µ1 1 µ2 (cid:88) t=1 (cid:13) (cid:13)L(Θt1) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) + 30 (cid:88) d2 t,max t=1 (cid:18) 1 µ1 1 µ2 ηL 2c2 (cid:19) . (17) Adam Improves Muon Step 3: Bounding ˆMt L(Θt1). The difference between the scaled momentum ˆMt and the gradient L(Θt1) can be bounded by: (cid:13) ˆMt L(Θt1) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:88) τ =1 (cid:88) τ =1 (cid:88) τ =1 (cid:88) τ =1 w1,t,τ L(Θτ 1) L(Θt1) w1,t,τ Θτ 1 Θt12 w1,t,τ ηOsDs (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2 (cid:33) t1 (cid:88) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) s=τ (cid:32) t1 (cid:88) s=τ w1,t,τ ηL ds,max . Then by (4) and the definition of w1,t,τ , (cid:13) (cid:13) ˆMt L(Θt1) (cid:13) (cid:13) (cid:13) (cid:13) = (cid:88) τ =1 (cid:88) τ =1 w1,t,τ ηL (t τ ) (cid:114) 1 µ1 1 µ ηL (t τ ) µtτ 1 1 µ1 1 µt 1 (cid:114) 1 µ1 1 µ2 (cid:114) 1 µ1 1 µ2 (cid:88) j=1 jµj 1 ηL ηL 1 µ1 1 µt 1 (cid:114) 1 µ1 1 µ2 µ1 (1 µ1)2 . (18) Step 4: Bounding (cid:113)(cid:80)n j=1 ˆvj . For ˆvt, it satisfies that: (cid:88) j=1 ˆvj = (cid:88) τ = (cid:88) τ =1 (cid:88) τ =1 w2,t,τ L(Θτ 1)2 w2,t,τ (L(Θt1F + Θτ 1 Θt12)2 (cid:32) w2,t,τ L(Θt1F + ηL (cid:33)(cid:33)2 ds,max (cid:32) t1 (cid:88) s=τ Then by (4) and the definition of w2,t,τ , (cid:88) j=1 ˆvj (cid:88) τ =1 (cid:18) w2,t,τ L(Θt1F + ηL (t τ ) (cid:19) (cid:114) 1 µ1 1 µ2 = L(Θt1)2 + 2ηL L(Θt1)F (cid:114) 1 µ1 1 µ2 (cid:88) τ = w2,t,τ (t τ ) + η2L2 (cid:18) 1 µ1 1 µ2 (cid:19) (cid:88) τ = w2,t,τ (t τ )2. 31 Zhang, Liu and Schaeffer By Cauchy-Schwarz inequality, it follows that: (cid:88) j= ˆvj L(Θt1)2 + 2ηL L(Θt1)F (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) τ =1 (cid:114) 1 µ1 1 µ2 (cid:33) w2,t,τ (t τ )2 (cid:18) 1 µ1 1 µ2 (cid:19) (cid:32) (cid:88) τ =1 w2,t,τ (t τ )2 + η2L L(Θt1)F + ηL (cid:114) 1 µ1 1 µ2 (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) τ =1 2 w2,t,τ (t τ )2 where Since: Hence, = (L(Θt1)F + at)2 , at := ηL (cid:114) 1 µ1 1 µ2 (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) τ =1 w2,t,τ (t τ )2. η2L2 a2 η2L2 η2L2 (cid:18) 1 µ1 1 µ2 (cid:18) 1 µ1 1 µ2 (cid:18) 1 µ1 1 µ (cid:33) µtτ 2 (t τ )2 (cid:19) (cid:32) (cid:88) τ =1 (cid:33) (cid:19) (cid:18) 1 µ2 1 µt 2 (cid:19) (cid:32) (cid:88) 2τ 2 µτ τ =1 (cid:19) (cid:18) µ2(1 + µ2) (1 µ2)3 (cid:19) := a2. (19) (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) j=1 ˆvj L(Θt1)F + at L(Θt1)F + a. (20) 32 Adam Improves Muon Step 5: Lower-bounding dt,maxL(Θt1). Combining (18) and (20) gives: (cid:13) ˆMt (cid:13) (cid:13) (cid:113)(cid:80)n j=1 ˆvj dt,max L(Θt1) (cid:13) (cid:13) (cid:13) L(Θt1 + nϵ/(cid:112)1 µt 2 (cid:13) ˆMt L(Θt1) (cid:13) (cid:13) L(Θt1F (cid:16) (cid:17) (cid:13) (cid:13) (cid:13) L(Θt1)F + nϵ/(cid:112)1 µt 2 + L(Θt12 (cid:13) ˆMt L(Θt1) (cid:13) (cid:13) L(Θt1 (cid:13) (cid:13) (cid:13) 2 + (cid:32) L(Θt1)F + nϵ/(cid:112)1 µt L(Θt12 L(Θt1)F + nϵ/(cid:112)1 µt L(Θt12 L(Θt1)F + nϵ/(cid:112)1 µt L(Θt12 L(Θt1)F + nϵ/(cid:112)1 µt F 2 2 2 ηL (cid:114) 1 µ1 1 µ µ1 (1 µ1)2 L(Θt12 L(Θt1)F + nϵ ηL (cid:114) 1 µ1 1 µ2 µ1 (1 µ1) 1 L(Θt1)F + nϵ/(cid:112)1 µt (cid:13) ˆMt L(Θt1) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2 + (cid:33) (cid:13) (cid:13) ˆMt L(Θt1) (cid:13) (cid:13) (cid:13) (cid:13) where ϵ := ϵ/ 1 µ2 and is given in (19). Step 6: Upper-bounding the gradient norm and deriving rate. Now define ϕnϵ(x) := x2 it then follows from (17) and (18) that: x+nϵ ,"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 ϕnϵ(L(Θt1F )"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 dt,max L(Θt1 + + ηL (cid:114) 1 µ1 1 µ2 µ1 (1 µ1)2 (cid:88) (cid:13) (cid:13)L(Θt1) ˆMt (cid:13) (cid:13) (cid:13)L(Θt1) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) + ηL 2c2 (cid:18) 1 µ1 1 µ (cid:19) (cid:13) (cid:13) (cid:13) + ηL 2c2 (cid:18) 1 µ1 1 µ2 (cid:19) (cid:19) ηL 2c2 (cid:114) 1 µ1 1 µ2 (cid:18) 1 µ1 1 µ2 ηLµ1 (1 µ1)2 ηT c2 + + + ηL ηT c2 + (cid:114) 1 µ1 1 µ"
        },
        {
            "title": "2\nT c2\n(cid:114) 1 − µ1\n1 − µ2",
            "content": "+ + ηL ηT c2 + (cid:18) 1 µ1 1 µ2 t=1 µ1 (1 µ1)2 (cid:88) t=1 µ1 (1 µ1)2 (cid:19) 2ηLµ (1 µ1)2c2 + + ηL (cid:112)µ2(1 µ1)(1 + µ2) (1 µ2)2 + = ηT c2 + ηLCµ c2 , 33 Zhang, Liu and Schaeffer where Cµ := (cid:19) (cid:18) 1 µ1 1 µ2 2µ1 (1 µ1)2 + 1 2 (cid:19) (cid:18) 1 µ1 1 µ + c2(cid:112)µ2(1 µ1)(1 + µ2) (1 µ2)2 + (cid:114) 1 µ1 1 µ2 µ1c2 (1 µ1)2 is constant that depends on µ1 and µ2. Then by Lemma 8, 1 (cid:88) t=1 L(Θt1F 1 1 (cid:88) t=1 (cid:88) t=1 ϕnϵ (L(Θt1F ) + (nϵ) 1 2 ϕnϵ (L(Θt1F ) + 1 2 (nϵ) 1 (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) t=1 (cid:88) t=1 (cid:113) ϕnϵ (L(Θt1F ) ϕnϵ (L(Θt1F ) ηT c2 + ηLCµ c2 + (nϵ) 1 2 (cid:115) ηT c2 + ηLCµ c2 . In particular, if choosing η = O(T 1 ϵ = O(T 1 2 n1), and = Θ(1), then: 2 ), 1 µ1 = Θ(1), 1 µ2 = Θ(1), 0 µ1 µ2 < 1,"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 L(Θt1) (cid:16) 1 (cid:17) for large > 0. The proof is thus completed. Appendix E. Proof of Theorem 6 This section contains the detailed proof of Theorem 6 for the convergence of NAMO-D in the stochastic setting."
        },
        {
            "title": "Proof",
            "content": "Step 1: uniform upper bound on stepsize. For each 0 and τ t, define: w1,t,τ := 1 µ1 1 µt 1 µtτ 1 and w2,t,τ := 1 µ2 1 µt µtτ 2 . It satisfies that (cid:80)t the j-th column of Mt, and vj and τ =1 w1,t,τ = (cid:80)t τ =1 w2,t,τ = 1. For each = 1, , and > 0, let denote the j-th element of vt. Write ˆMt := 1 1µt Mt, ˆvt := 1 1µt 2 denote vt, Dt := diag min (cid:18) (cid:26) max (cid:8)dt, dt1(cid:9) , (cid:27)(cid:19) dt1 , 1 where (0, 1] is the fixed clamping hyperparameter, dt and dt are given in Algorithm 2. Then: ˆMt = 1 µ1 1 µt 1 (cid:88) (cid:88) µtτ 1 Gτ = w1,t,τ Gτ , τ =1 τ =1 and ˆvt = 1 µ2 1 µt 2 (cid:88) τ =1 µtτ 2 Nc(Gτ ) Nc(Gτ ) = (cid:88) τ =1 w2,t,τ Nc(Gτ ) Nc(Gτ ). Adam Improves Muon [Dt]jj (cid:13) ˆM (cid:13) (cid:13) (cid:113) (cid:13) (cid:13) (cid:13) ˆvj (cid:114) 1 µ1 1 µ , j. By Lemma 7, Write: dt,max := max [Dt]jj , and dt,min := min [Dt]jj . Then the condition number of Dt is given by: κ (Dt) = dt,max dt,min (cid:26) min κt, (cid:27) , 1 (21) (22) (23) where κt := κ (diag(dt)) denotes the condition number of diag(dt). Step 2: Expected descent inequality and averaging. Let Et[] := E[Θt1] denote the conditional expectation given the previous iterates Θ0, , Θt1, and write Et := ˆMt L(Θt1). Then, by Lemma 11, Et [L(Θt) L(Θt1)] Et [ L(Θt1), ηOtDt] + η2L 2 (cid:69)(cid:105) L(Θt1) ˆMt, ηOtDt (cid:68) (cid:104) Et (cid:104) Dt2 2 (cid:105) Et (cid:104) η (cid:68) ˆMt, OtDt (cid:69)(cid:105) (cid:104) ηdt,max (cid:13) (cid:13)L(Θt1) ˆMt (cid:13) (cid:105) (cid:13) (cid:13) (cid:13) Et (cid:104) ηdt,min (cid:105) (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) =Et (cid:32) Et + (cid:33) Et η2L 2 η2L 2 + (cid:104) Dt2 2 (cid:105) Et (cid:2)d2 t,max (cid:3) (cid:104) Et ηdt,min (cid:105) (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) + Et [ηdt,max Et] + η2L 2 Et (cid:2)d2 t,max (cid:3) , where dt,max and dt,min are defined in (22). Rearranging the terms gives: (cid:104) Et dt,min (cid:105) (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) Et [L(Θt1) L(Θt)] + Et [dt,max Et] + ηL Et (cid:2)d2 t,max (cid:3) . Then by the law of total expectation and (21),"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 (cid:104) dt,min (cid:105) (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) ηT ηT + +"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 [dt,max Et] + ηL 2T (cid:88) t= (cid:2)d2 t,max (cid:3) (cid:114) 1 µ1 1 µ2 (cid:88) t= [Et] + ηL(1 µ1) 2(1 µ2) . (24) Step 3: Bounding the distance between bias-corrected momentum and true gradient. For each t, it satisfies: Et = ˆMt (cid:105) (cid:104) ˆMt + (cid:105) (cid:104) ˆMt L(Θt1) = (cid:88) τ =1 w1,t,τ (Gτ L(Θτ 1)) + (cid:88) τ = w1,t,τ (L(Θτ 1) L(Θt1)) 35 Hence, by (21), (cid:104) Et (cid:105) Zhang, Liu and Schaeffer w1,t,τ (Gτ L(Θτ 1)) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) + (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:88) τ =1 w1,t,τ (L(Θτ 1) L(Θt1)) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:88) + w1,t,τ L(Θτ 1) L(Θt1)2 (cid:88) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) τ =1 (cid:18) 1 µ1 1 µt 1 (cid:18) 1 µ1 1 µt 1 (cid:18) 1 µ1 1 µt 1 (cid:18) 1 µ1 1 µt 1 (cid:18) 1 µ1 1 µt 1 (cid:18) 1 µ1 1 µt 1 (cid:18) 1 µ1 1 µt 1 = (cid:19) (cid:88) τ =1 1 L2 Θτ 1 Θt12 µtτ 2 (cid:19) (cid:88) L2η2 µtτ 1 τ =1 (cid:88) L2η2 (cid:33)2 (cid:32) t1 (cid:88) s=τ Ds2 (cid:35) µtτ 1 (t τ )2 (cid:19)2 (cid:32) (cid:88) (cid:33) µ2(tτ ) 1 σ2 τ =1 (cid:19)2 (cid:18) 1 µ2t 1 1 µ2 1 (cid:19) σ2 + τ =1 (cid:18) 1 µ1 1 µt 1 (cid:18) 1 µ1 1 µt 1 (cid:18) 1 µ1 1 µ2 (cid:18) 1 µ1 1 µ2 + + (cid:19) (cid:19) (cid:19) σ2 + (cid:19) (cid:18) 1 + µt 1 1 + µ1 (cid:19) (cid:34)(cid:18) 1 + µt 1 1 + µ1 (cid:19) (cid:34)(cid:18) 1 + µt 1 1 + µ1 (cid:19) (cid:20)(cid:18) 1 + µt 1 1 + µ1 (cid:19) σ2 (cid:19) σ2 (cid:19) σ2 (cid:19) σ2 + µ1(1 + µ1)L2η2 (1 µ2) (1 µ1)(1 µt 1) . L2η2 τ =1 (cid:88) τ = (cid:35) 1τ 2 µτ (cid:18) + µ1(1 + µ1) (1 µ2) (1 µ1)2 (cid:19) (cid:21) L2η2 Then by Lemma 9, it follows that:"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 (cid:104) Et2 (cid:105) (cid:32) (cid:18)"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 1 1 µt 1 (cid:33) (cid:18) σ2(1 µ1) + µ1(1 + µ1)L2η2 (1 µ1)(1 µ2) (cid:19) 1 + µ1 (1 µ1)T"
        },
        {
            "title": "1\nT ln µ1",
            "content": "ln (cid:18) 1 µT 1 1 µ1 (cid:19)(cid:19) (cid:18) σ2(1 µ1) + µ1(1 + µ1)L2η2 (1 µ1)(1 µ2) (cid:19) . By Cauchy-Schwarz inequality and Jensens inequality, (cid:118) (cid:117) (cid:117) (cid:116) [Et]"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 (cid:88) (cid:104) (cid:105) Et2 t=1 (cid:115) 1 + µ1 (1 µ1)T"
        },
        {
            "title": "1\nT ln µ1",
            "content": "ln (cid:18) 1 µT 1 1 µ1 (cid:19) (cid:32) σ(cid:112)r(1 µ1) (cid:115) + Lη (cid:33) rµ1(1 + µ1) (1 µ2) (1 µ1) (25) . 36 Step 4: Bounding (cid:20)(cid:113)(cid:80)n j=1 ˆvj Adam Improves Muon (cid:21) . For each j, let Gj τ denote the j-th column of Gτ . By Minkowski inequality and Jensens inequality, (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) =E ˆvj (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) (cid:88) j=1 j=1 τ =1 w2,t,τ (cid:13) (cid:13)Gj (cid:13) τ (cid:13) 2 (cid:13) (cid:13) = (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) w2,t,τ Gτ 2 (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) w2,t,τ L(Θτ 1) Gτ 2 τ = + (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) w2,t,τ L(Θτ 1) σ σ (21) σ σ σ σ σ τ =1 τ =1 + [L(Θt1)F ] + (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) w2,t,τ L(Θτ 1) L(Θt1)2 + [L(Θt1)F ] + ηLE τ =1 (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) τ =1 (cid:33)2 Ds2 (cid:32) t1 (cid:88) s=τ w2,t,τ + [L(Θt1)F ] + ηL + [L(Θt1)F ] + ηL + [L(Θt1)F ] + ηL + [L(Θt1)F ] + ηL (cid:115) (cid:115) (cid:115) (cid:115) + [L(Θt1)F ] + 2ηL 1 µ1 1 µt 2 1 µ1 1 µt 2 1 µ1 1 µt (cid:118) (cid:117) (cid:117) (cid:116) (cid:118) (cid:117) (cid:117) (cid:116) (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) τ =1 (cid:88) τ =1 (cid:88) τ =1 µtτ 2 (t τ )2 µtτ (t τ )2 µτ 2τ 2 (1 µ1)µ2(1 + µ2) (1 µt 2)(1 µ2)3 (cid:115) (1 µ1) 2)(1 µ2)3 . (1 µt (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) j=1 ˆvj + nϵ (cid:112)1 µt [L(Θt1)] + at, (26) Hence, where at := (cid:115) 2ηL + σ (1 µ1) 2)(1 µ2)3 + (1 µt nϵ (cid:112)1 µt 2 . Then by Lemma 10,"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 at σ + (cid:32) (cid:115) 2ηL (1 µ1) (1 µ2)3 + nϵ (cid:33) 1 2 ln(1 + (cid:113) 1 µT 2 ) ln µ2 . (27) 37 Zhang, Liu and Schaeffer Step 5: Lower-bounding Schwarz inequality, (cid:104) dt,max (cid:105) (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F and relating to [L(Θt1)F ] . By Cauchy- (cid:16) (cid:104)(cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F (cid:105)(cid:17)2 = (cid:13) (cid:13) (cid:13)F (cid:13) ˆMt (cid:13) (cid:13) (cid:18)(cid:113)(cid:80)n j=1 ˆvj (cid:13) (cid:13) 2 ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:113)(cid:80)n j=1 ˆvj + nϵt + nϵt E (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) ˆvj + nϵt j=1 (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) j=1 (cid:19) 1 2 1 ˆvj + nϵt 2 (cid:118) (cid:117) (cid:117) (cid:117) (cid:116) (cid:80)n j= (cid:13) (cid:13) 2 ˆM (cid:13) (cid:13) (cid:13) (cid:13) j=1 ˆvj + nϵt (cid:80)n (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F (cid:104) dt,max (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F (cid:105) (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) j=1 ˆvj + nϵt , (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) ˆvj + nϵt j= where ϵt := ϵ/(cid:112)1 µt 2. Combining the above with (26) gives: (cid:104) dt,max (cid:105) (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F (cid:16) (cid:105)(cid:17)2 (cid:104)(cid:13) ˆMt (cid:13) (cid:13) [L(Θt1)F ] + at (cid:13) (cid:13) (cid:13)F (E [L(Θt1)F ] [EtF ])2 [L(Θt1)F ] + at . Rearranging the terms gives: (cid:16) (E [L(Θt1)F ])2 2E [EtF ] + Then solving for [L(Θt1)] gives (cid:104) dt,max (cid:105)(cid:17) (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F [L(Θt1)F ]atE (cid:104) dt,max (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F (cid:105) +E [EtF ]2 0. [L(Θt1)F ] (cid:104) 2E [EtF ] + dt,max (cid:114)(cid:16) (cid:105) (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F + 2E [EtF ] + (cid:104) dt,max (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F (cid:105)(cid:17)2 + 4atE (cid:104) dt,max (cid:105) (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F 4E [EtF ]2 (cid:105) + (cid:114) 1 4 (cid:114) (cid:13) (cid:13) (cid:13)F (cid:105) [EtF ] + (cid:104) dt,max (cid:13) ˆMt (cid:13) (cid:13) 1 [EtF ] + [EtF ] + (cid:104) (cid:104) dt,max dt,max (cid:13) ˆMt (cid:13) (cid:13) (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F (cid:13) (cid:13) (cid:13)F + (E [EtF ] + at) (cid:105) + (cid:114) (E [EtF ] + at) (cid:104) dt,max (cid:13) ˆMt (cid:13) (cid:13) (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F (cid:13) (cid:13) (cid:13)F (cid:105) (cid:105) . 2 (cid:105)2 (cid:13) (cid:13) (cid:13)F (cid:104) dt,max (cid:104) dt,max (cid:13) ˆMt (cid:13) (cid:13) + (E [EtF ] + at) (cid:104) dt,max (cid:105) (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F Step 6: Deriving the convergence rate By Cauchy-Schwarz inequality and (23), it then follows that:"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 [L(Θt1)F ]"
        },
        {
            "title": "1\nT",
            "content": "+ (cid:88) [EtF ] +"
        },
        {
            "title": "1\nT",
            "content": "(cid:26) min κt, 1 c2 (cid:27) (cid:88) (cid:104) dt,min (cid:105) (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F t=1 (cid:118) (cid:117) (cid:117) (cid:116)"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 (E [EtF ] + at) (cid:118) (cid:117) (cid:117) (cid:116)"
        },
        {
            "title": "1\nT",
            "content": "38 t=1 (cid:26) min κt, 1 (cid:27) (cid:88) t=1 (cid:104) dt,max (cid:13) ˆMt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F (cid:105) . Adam Improves Muon Combining the above with (24) and (27) gives: [L(Θt1)F ] 1 1 + 1 + (cid:88) t=1 (cid:88) t=1 (cid:118) (cid:117) (cid:117) (cid:116) 1 (cid:88) t= t=1 (cid:118) (cid:117) (cid:117) (cid:117) (cid:116) 1 (cid:88) t=1 [Et] + ηT c2 + ηL(1 µ1) 2c2(1 µ2) + 1 c2 (cid:114) 1 µ1 1 µ2 (cid:88) t= [Et] (E [EtF ] + at) (cid:118) (cid:117) (cid:117) (cid:116) ηT c2 + 1 c2 (cid:114) 1 µ1 1 µ (cid:88) t=1 [Et] + ηL(1 µ1) 2c2(1 µ2) (cid:88) [Et] + ηT c2 + ηL(1 µ1) 2c2(1 µ2) + 1 c2 (cid:114) 1 µ1 1 µ2 (cid:88) [Et] [EtF ] + (cid:32) (cid:115) 2ηL σ + (1 µ1) (1 µ2)3 + nϵ t=1 (cid:33) 1 2 ln(1 + (cid:113) 1 µT 2 ) ln µ2 (cid:118) (cid:117) (cid:117) (cid:116) ηT c2 +"
        },
        {
            "title": "1\nT c2",
            "content": "(cid:114) 1 µ1 1 µ2 (cid:88) t=1 [Et] + ηL(1 µ1) 2c2(1 µ2) . In particular, for large > 0, if choosing η = O(T 3 ϵ = O(T 1 2 ), and = Θ(1), then, by (25), 4 ), 1 µ1 = Θ(T 1 2 ), 1 µ2 = Θ(T 1 2 ),"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 [Et] (cid:16) 1 (cid:17) , σb 1 4 1 8 (cid:17) , and it follows that:"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 [L(Θt1)] (cid:16) 1 4 + where is the batch size. The proof is thus completed."
        }
    ],
    "affiliations": [
        "University of California, Los Angeles"
    ]
}