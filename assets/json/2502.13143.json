{
    "paper_title": "SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation",
    "authors": [
        "Zekun Qi",
        "Wenyao Zhang",
        "Yufei Ding",
        "Runpei Dong",
        "Xinqiang Yu",
        "Jingwen Li",
        "Lingyun Xu",
        "Baoyu Li",
        "Xialin He",
        "Guofan Fan",
        "Jiazhao Zhang",
        "Jiawei He",
        "Jiayuan Gu",
        "Xin Jin",
        "Kaisheng Ma",
        "Zhizheng Zhang",
        "He Wang",
        "Li Yi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Spatial intelligence is a critical component of embodied AI, promoting robots to understand and interact with their environments. While recent advances have enhanced the ability of VLMs to perceive object locations and positional relationships, they still lack the capability to precisely understand object orientations-a key requirement for tasks involving fine-grained manipulations. Addressing this limitation not only requires geometric reasoning but also an expressive and intuitive way to represent orientation. In this context, we propose that natural language offers a more flexible representation space than canonical frames, making it particularly suitable for instruction-following robotic systems. In this paper, we introduce the concept of semantic orientation, which defines object orientations using natural language in a reference-frame-free manner (e.g., the ''plug-in'' direction of a USB or the ''handle'' direction of a knife). To support this, we construct OrienText300K, a large-scale dataset of 3D models annotated with semantic orientations that link geometric understanding to functional semantics. By integrating semantic orientation into a VLM system, we enable robots to generate manipulation actions with both positional and orientational constraints. Extensive experiments in simulation and real world demonstrate that our approach significantly enhances robotic manipulation capabilities, e.g., 48.7% accuracy on Open6DOR and 74.9% accuracy on SIMPLER."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 3 4 1 3 1 . 2 0 5 2 : r SOFAR: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation Zekun Qi13 Wenyao Zhang237 Yufei Ding34 Runpei Dong5 Xinqiang Yu3 Jingwen Li4 Lingyun Xu4 Baoyu Li5 Xialin He5 Guofan Fan1 Jiazhao Zhang3 Jiawei He3 Jiayuan Gu6 Xin Jin7 Kaisheng Ma1 Zhizheng Zhang3 He Wang34 Li Yi189 Equal Contribution Corresponding authors 1Tsinghua University 2Shanghai Jiao Tong University 3Galbot 4Peking University 6ShanghaiTech University 7Eastern Institute of Technology 8Shanghai Qi Zhi Institute https://qizekun.github.io/sofar 5UIUC 9Shanghai AI Laboratory Fig. 1: Overview. We present SOFAR, the first system to incorporate object orientation into spatial reasoning and robotic manipulation. We introduce the concept of Semantic Orientation, which refers to natural language-grounded object orientations, such as the cutting direction of knife or the handle direction of cup. To support this, we construct OrienText300K, large-scale dataset of object-text-orientation pairs. SOFAR enables accurate 6-DoF object understanding and excels in tasks such as visual question answering, object manipulation, and navigation. AbstractSpatial intelligence is critical component of embodied AI, promoting robots to understand and interact with their environments. While recent advances have enhanced the ability of VLMs to perceive object locations and positional relationships, they still lack the capability to precisely understand object orientationsa key requirement for tasks involving finegrained manipulations. Addressing this limitation not only requires geometric reasoning but also an expressive and intuitive way to represent orientation. In this context, we propose that natural language offers more flexible representation space than canonical frames, making it particularly suitable for instruction-following robotic systems. In this paper, we introduce the concept of semantic orientation, which defines object orientations using natural language in reference-frame-free manner (e.g., the plug-in direction of USB or the handle direction of knife). To support this, we construct OrienText300K, largescale dataset of 3D models annotated with semantic orientations that link geometric understanding to functional semantics. By integrating semantic orientation into VLM system, we enable robots to generate manipulation actions with both positional and orientational constraints. Extensive experiments in simulation and real world demonstrate that our approach significantly enhances robotic manipulation capabilities, e.g., 48.7% accuracy on Open6DOR and 74.9% accuracy on SIMPLER. I. INTRODUCTION Open-world spatial intelligence is crucial for embodied AI, as robot must understand not only what an object is but also its precise where for effective interaction. To this end, vision-language-models (VLMs) [1, 30, 68, 79] with spatial understanding [10, 12, 15] that comprehend spatial concepts [70, 142] and relationships [41, 66, 135] have been built. These models incorporate spatial knowledge into their architecture design or training data, enabling them to perform tasks such as distinguishing left from right [12, 15], counting objects [10, 33], and even planning for position-only manipulations [10, 160]. Despite the remarkable achievements, we ask: What is the missing cornerstone of such spatial understanding? Given the original intent of seeing is for doing [40], how can we push spatial understanding further? We observe that current VLMs struggle with understanding object orientation, making them insufficient for generic robot manipulation planning. Consider some everyday scenarios: inserting pen into pen holder, righting tilted wine glass, or plugging cord into power strip. Previous approaches [10, 12, 15] primarily focused on understanding where is the pen or where is the wine glass while ignoring their orientations, making them insufficient for accomplishing these seemingly simple object manipulation tasks. More importantly, different orientations of an object hold varying semantic significance. The capability of connecting specific orientations to their semantic meanings is essential for language-guided robot manipulations. For example, inserting pen into pen holder requires aligning the pen tip with the direction of the pen holders opening; righting wine glass necessitates aligning the glasss top with the z-axis in the world coordinate frame; and plugging into power strip involves understanding the insertion direction, which is perpendicular to the power strips surface. However, translating specific language description into desired object orientation is challenging for existing VLMs. To move forward, we introduce language-grounded orientation that bridges spatial reasoning and object manipulation, characterized by the following: From Position Awareness to Orientation Awareness. While prior works [10, 12, 15] emphasize position relationship, orientation understanding is equally critical for defining the full six degrees of freedom (6-DoF) of object or end-effector poses [18, 34, 71, 141, 146, 158]. Orientation awareness involves understanding object orientations and their relationships in the open world, enabling robots to complete tasks requiring precise alignment and rearrangement together with position awareness. From Orientation to Semantic Orientation. Traditional orientation, defined relative to base frame or template model [18, 67, 122, 141, 146], is insufficient for openworld manipulation guided by language instructions [57, 119, 127]. We introduce semantic orientation, linking orientational vectors of an object to open-vocabulary prompts (e.g., the handle direction of knife or plug-in direction of USB). This bridges geometric reasoning with functional semantics, enabling robots to interpret task-specific orientation changes. Achieving such spatial awareness requires addressing two key challenges: acquiring semantic orientation knowledge in the open world and integrating it with VLMs. To tackle the first, we propose PointSO, generalizable cross-modal 3D Transformer [29, 107, 109, 134], which serves as robust and versatile framework for open-world spatial orientation understanding. To train PointSO effectively, we construct OrienText300K, large-scale orientation-text paired dataset curated from internet sources. This dataset, devoid of expensive robot data, is automatically labeled by prompting GPT-4o [100] with extensive and diverse language-grounded semantic orientation queries. These queries encompass intra-object spatial understanding and inter-object interaction-related semantics, such as manipulation orientations. OrienText300K comprises over 350K 3D models of diverse everyday objects. Powered by OrienText300K, PointSO can reliably infer semantic orientation for an arbitrary object without being restricted to known category or instance. We further develop an integrated reasoning system, SOFAR, to coordinate our proposed PointSO and existing position foundation models for achieving more comprehensive spatial understanding, where Florence-2 [147] and SAM [65] handle the object positions, while our PointSO focuses on understanding and outputting orientations complimentary. Specifically, we parse an input RGB-D observation as an orientation-aware 3D scene graph using SAM-segmented object point clouds and our PointSO. The RGB-D observation, together with the scene graph, is then input to the VLM, which outputs chain-ofthought [140] spatial reasoning for both position and orientation commands. These commands can then serve as visual planning outcomes to support robotic manipulation tasks. To assess our system, we introduce Open6DOR V2, largescale robot manipulation benchmark designed for 6-DoF object rearrangement in simulation. This benchmark demands robust positional and orientational reasoning in open-world settings and supports both open-loop and closed-loop robotic control. Our experiments demonstrate that our system considerably outperforms state-of-the-art vision-language models and popular vision-language-action (VLA) modelseven those trained with extensive and costly robot trajectories. These performance gains are also observed in real-world experiments. Additionally, we establish new spatial visual-question-answering benchmark, that confirms the systems exceptional open-world spatial reasoning capabilities. that In summary, our contributions are fourfold. First, we infers introduce PointSO, an orientation base model the semantic directions of novel objects in an open-world context. Second, we curate OrienText300K, large-scale 3D model dataset annotated with semantic directions to support the training of orientation models. Third, we develop an integrated system that enhances powerful VLMs with advanced spatial understanding, facilitating robot manipulations that require both positional and orientational spatial knowledge. Fig. 2: Data Construction of OrienText300K. Fourth, we establish 6-DoF SpatialBench & Open6DOR V2, an orientation-aware spatial visual-question-answering benchmark and comprehensive robot manipulation benchmark designed to evaluate both open-loop and closed-loop open-world 6D rearrangement strategies. Extensive experiments demonstrate the superior performance of our method together with series of perception and robot manipulation benchmarks. II. SEMANTIC ORIENTATION: CONNECTING LANGUAGE AND OBJECT ORIENTATION A. Definition of Semantic Orientation Traditionally, the orientation of an object is defined within reference frame, using quaternions or Euler angles to represent relative rotations. Intuitively, object orientations commonly correspond to some specific semantics in most interactive behaviors. This aligns with the fact that humans typically understand an objects orientation in more semantic, referencefree way. For instance, when plugging plug into charger, we accomplish the action of plugging in by matching the metal prongs direction with the outward direction of the chargers socket. Drawing on this observation, we define an objects Semantic Orientation as follows. Given an object and description ℓ, the corresponding semantic orientation sX ℓ S(2) is an object-centric direction represented as unit vector semantically matching the description ℓ. sX ℓ = F(X, ℓ). (1) ℓ is an open-vocabulary language description that should have clear semantic correspondence to general orientation (e.g., front, top), an object part (e.g., handle, cap), or specific manipulation goal (e.g., pour out, plug-in). For an object X, it may have multiple semantic orientations corresponding to different functions or attributes via changing the language description ℓ, forming semantic orientation set SX = {sX }. Based on this set, the rotation of ℓ1 can be characterized by transforming its semantic orientations. , . . . , sX ℓn , sX ℓ B. Robotic Manipulation via Semantic Orientation Semantic orientations are powerful representations that help characterize various orientation-related knowledge. What relates Fig. 3: OrienText300K data filtering and annotating accuracy on human-labeled validation sets. SO denotes the annotation quality for semantic orientation. All VLMs achieve high accuracy and GPT-4o consistently yields the best result. most to robot manipulation is the knowledge of object reorientation. Given an initial observation of an object and task command that specifies the desired reorientation, semantic orientations can be used to determine the necessary object rotation. First, we identify task-related semantic orientation descriptions {lc} from the task command whose desired directions are clearly outlined in the command. For instance, command such as turn bottle over and put it on the ground necessitates identifying the up direction of the bottle as semantic orientation, with the desired direction being (0, 0, 1) in world coordinate system where the z-axis is perpendicular to the ground. Then, by extracting the semantic orientations from the initial observation and calculating the rotation needed to align these with the desired directions, we can effectively determine how the object should be reoriented. Beyond object reorientation, semantic orientations can be linked to traditional instance-level and category-level orientations, and even facilitate cross-category orientation. Specifically, by orthogonalizing set of semantic orientations, we can establish reference frames that define instance-level orientations for individual objects. For objects within the same category, using consistent set of linguistic descriptions aligns their semantic orientation sets, resulting in category-level consistent references from which category-level orientations can be derived. Moreover, applying the same linguistic descriptors across different categories creates cross-category consistent semantic orientation sets, enabling the derivation of crosscategory reference frames. Thus, our approach builds orientation understanding on semantic orientations, with an emphasis on learning to estimate these directions for open-world objects based on open-ended descriptions. C. OrienText300K: Orientation-Text Paired Data at Scale Our goal is to learn an orientation model on large-scale 3D model dataset so that it can identify semantic orientations in open-world scenarios. To achieve it, we first introduce OrienText300K, newly curated 3D model dataset with diverse and extensive semantic orientation labels for training our language-conditional orientation model. visualization. ❸ Reasonable objects that have sufficient spatial reasoning potentials. ❹ High-quality objects. Low-quality objects such as blurry and wrong samples are filtered. ❺ Distinguishable objects. Abstract objects such as meaningless solids are filtered. ❻ Non-scene objects. Samples that describe 3D scene are filtered for object-centric understanding purposes. However, it is non-trivial to conduct filtering on such big data using manual labor. Inspired by recent works showing large VLMs are human-aligned 2D or 3D image-based judgers [103, 143, 170], we employ GPT-4o [100] by prompting requirements above. To be specific, the multi-view images of 3D objects are concatenated together with our designed prompts into GPT-4o, and GPT-4o will decide whether one sample should be filtered. The filtered dataset yields 350K+ clean samples, significantly reducing data noise. 4) Data Annotation: Like data filtering, we annotate OrienText300K with semantic orientations using GPT-4o. GPT-4o will take rendered object views as input and generate the language description and one best-grounded direction from the 6 standard orthogonal direction candidates. We render the six standard orthogonal view images in Blender, which are concatenated as input of GPT-4o. Besides, four 45-degree oblique orthogonal views are rendered and concatenated as an additional input, ensuring more robust annotation. 5) Quality Validation: To validate the accuracy of our prompted GPT agents as the data filter and annotator, we construct validation set containing 208 samples with manually labeled filtering criteria and semantic orientation labels, respectively. From Fig. 3, we observe that GPT-4o achieves an average accuracy of 88.3% and 97.1% accuracy on filtering and annotating, respectively. This provides quality guarantee of our OrienText300K. D. PointSO: Cross-Modal 3D Transformer for Semantic Orientation Prediction We introduce PointSO, plain Transformer-based architecture [134] with cross-modal 3D-language fusion as our orientation model. As illustrated in Fig. 4, PointSO takes the objects 3D point clouds and language description as inputs, and predicts the corresponding semantic orientation. 1) 3D and Language Embeddings: Given an objects point cloud = {xi R3i = 1, 2, . . . , } with 3D points defined in (x, y, z) Cartesian space, and an arbitrary language description ℓ, we first embed both into discrete token embeddings. For the 3D point clouds, we follow [29, 107, 159] to first sample Ns seed points using farthest point sampling (FPS) and then group inputs with KNN for point feature embedding with local geometric extraction network such as lightweight PointNet [104, 105]. An MLP head is used which maps special [CLS] token [31] to predicted direction. As for the language inputs, we adopt OpenAIs CLIP [115] and use the global token as cross-modal fusion inputs. Like Vision Transformer [31], we explore three model configurations, i.e., small, base, and large versions. Each configuration is of different number of Transformer layers, dimensions, and CLIP models, detailed in the Appendix B. Fig. 4: PointSO architecture for SO prediction. PointSO takes 3D and language embeddings as inputs of B-layer plain Transformer model, and uses simple token-level sum as cross-modal fusion in all layers. An MLP head predicts the corresponding orientation vector normalized in unit sphere. 1) Scalability Analysis: Before discussing the data curation process, it is important to note that semantic orientations are typically defined within the six standard orthogonal views of an object from canonical setup. For example, the orientation for placing pen into pencil holder is opposite to the holders upright direction. Object canonicalization often involves specifying the up and front directions of an object. Fortunately, most 3D models available in web datasets are already canonicalized, except for potential axis flipping. Based on this observation, we propose to scale up the semantic orientation annotations by leveraging readily available web 3D datasets with automatic labeling using GPT. Specifically, we can use the six orthogonal directions as candidates and generate descriptions from rendered multi-view images. This associates various language descriptions with salient directions on objects, providing the supervision needed for learning an orientation model. 2) Data Source: To scale up, we build the OrienText300K dataset from Objaverse [22], which originally contains 800K Internet 3D models across substantial categories. However, such Internet data contains lot of noisy annotations or low-quality samples that cannot be used. Based on Blender, we carefully set up the light and rendered more than 8M high-fidelity rendering images. 3) Data Filtering: To clean the data that can better be used for generating semantic orientation annotations, we first clean the data by using dedicated filtering strategy that filters data to preserve samples that satisfy the following 6 requirements. ❶ Standard orthogonal view only. Samples in random views will be filtered. ❷ Clean objects without the ground for auxiliary Fig. 5: Overview of SOFAR system. SOFAR takes RGB-D images as inputs, where the depth images can be obtained from depth senor or metric 3D prediction [157]. Given the language instruction, SOFAR prompts the VLM to obtain task-oriented object phases and semantic orientation descriptions. Then, SOFAR leverage foundation models Florence-2 [147] and SAM [65] to segment depth point clouds and our PointSO (Section II-D) to obtain semantic orientations. Summarizing 3D object-centric information, an orientation-aware scene graph is constructed and encoded into languages (Section III). The VLM takes the RGB image and the scene graph and outputs the queried spatial understanding VQA or translation for manipulation. 2) Cross-Modal Fusion: We adopt dense layer-wise feature fusion strategy by injecting the global text features into the Transformer layers of the 3D Transformer, where cross-modal fusion is conducted. Generally, this fusion operation can be implemented in various fashions such as cross-attention, adapter, or concatenating features along spatial/channel dimensions. Empirically, however, we find that the sum of the text token to every point token is the most simple but effective solution (see Appendix H2). This may be due to the relatively short length of the languages, where per-token sum enhances attention to languages. 3) Optimization: Let FSO represent the PointSO model parameterized by θSO (the CLIP is kept frozen and thus its parameters are not included). Given every object point cloud Xi DOrienText300K in the OrienText300K dataset, where each object is labeld with language set Li = {ℓi j, = 1, 2, . . . , Q} and the corresponding ground truth semantic orientation set, Si = {si j, = 1, 2, . . . , Q}. The optimization is to minimize the negative cosine similarity Lcos(v, k) = 1 vk vk between predicted and the ground truth semantic orientations: (cid:17) (cid:16) (cid:88) (cid:88) Lcos FSO(Xi, ℓi j), si . (2) min θSO XiDOrienText300K ℓi Li III. SOFAR: SEMANTIC ORIENTATION BRIDGES SPATIAL REASONING AND OBJECT MANIPULATION Our proposed PointSO model now paves the off-the-shelf for object-centric spatial orientation understanding. However, it is still unclear how to leverage such object-centric spatial understanding for scene-level spatial reasoning both in the digital world (e.g., orientation-aware visual question answering, VQA) and in the physical world (e.g., robot manipulations). To enable such applications, we build an integrated reasoning system where powerful VLM acts as an agent and reasons about the scene while communicating with off-the-shelf models including PointSO and SAM [65]. Fig. 5 illustrates an overview of our proposed framework, aiming at Semantic Orientation For Autonomous Robotic manipulation (SOFAR). SOFAR consumes an RGB-D image and language query as input and first leverages off-the-shelf models including SAM and PointSO to convert the image into an orientation-aware 3D scene graph. Then SOFAR leverages VLM agent to produce planning outcomes based upon the scene graph and the input language query, which can be later used for robot manipulation. We will introduce the construction of the orientation-aware 3D scene graph in Section III-A and how to perform spatial-aware task reasoning and plan for robot manipulation in Section III-B. A. Orientation-Aware Scene Graph from RGB-D To convert the input RGB-D image into an orientation-aware 3D scene graph, we first segment the RGB image to obtain object-level 3D point clouds using SAM [65] and then construct scene graph with object-attribute nodes. 1) Task-Oriented Object Segmentation: Given language query Q, we first prompt VLM model FVLM to abstract the task-oriented object phrase set. Thus, set = {pii = 1, 2, . . . , } with object phrases in language will be generated from Q. With set P, we use language-conditioned object segmentation with SAM to obtain an object set = {Xii = 1, 2, . . . , }, where Xi is the 3D point cloud of the i-th object. Besides, we assign individual IDs to objects which are used for Set-of-Mark (SoM) prompting [151] on VLMs image input. Next, we prompt the VLM to generate every objects corresponding task-oriented language description set Li. We predict the semantic orientation using pretrained PointSO for each description in the description set Li, forming the semantic orientation set Si for the i-th object. Fig. 6: Qualitative results of real world language-grounded manipulation. SOFAR doesnt require any robotic data training or human annotation of task-specific prompts, and it can generalize across various embodiments, tasks and environments. TABLE I: 6-DoF object rearrangement evaluation on Our Proposed Open6DOR V2 Benchmark. Method Position Track Rotation Track 6-DoF Track Level 0 Level 1 Overall Level 0 Level 1 Level 2 Overall Position Rotation Overall Time Cost (s) Perception Tasks on Issac Sim [95] GPT-4V [98] Dream2Real [62] VoxPoser [54] Open6DOR-GPT [28] SOFAR-LLaVA SOFAR Octo [126] OpenVLA [64] SOFAR 46.8 17.2 35.6 78.6 86.3 96. 51.2 51.6 72.1 39.1 11.0 21.7 60.3 57.9 81.5 32.1 32.4 47.6 45.2 15.9 32.6 74.9 78.7 93.0 47.2 47.6 67.0 9.1 37.3 - 45.7 62.5 68. 6.9 27.6 - 32.5 30.2 42.2 11.7 26.2 - 49.8 67.1 70.1 Execution Tasks on Libero [76] 10.7 11.0 28.3 18.3 18.5 18.3 29.9 30.6 34. 9.2 31.3 - 41.1 48.6 57.0 17.2 17.6 25.7 - 26.2 - 84.8 83.0 92.7 45.6 46.2 63.7 - 18.7 - 40.0 48.2 52.7 8.0 8.2 25. - 13.5 - 35.6 40.3 48.7 8.0 8.2 18.4 - 358.3s - 126.3 9.6s 8.5s - - 40s 2) Orientation-Aware 3D Scene Graph: From segmented object set , we construct an orientation-aware scene graph = (V, E) with object nodes. Each node oi consists of following semantic and spatial attributes: ❶ object name pi with an individual ID that distinguishes instances; ❷ object position as 3D centroid coordinate ci = (x, y, z) R3 from segmented depth; ❸ objects 3D bounding box size bi = (h, w, l) R3; ❹ semantic orientation set Si and the corresponding task-oriented language-description set Li. Each edge eij stores the relative translation and bounding box size ratio between the connected two objects oi and oj. B. Spatial-Aware Task Reasoning We convert the orientation-aware scene graph into descriptive language and input this, along with the RGB image and query Q, into VLM. With spatial knowledge accurately encoded in the scene graph, the VLM can leverage its robust image and language understanding capabilities to produce highquality spatial reasoning results. 1) Chain-of-Thought Spatial Reasoning: Since most of the robot manipulation problems involving rigid bodies can be abstract as changing the position and orientation of objects, we especially focus on instructing the VLM to output goal transformation as the plan for robot manipulation. To realize this, we use chain-of-thought (CoT) reasoning [140] that regularizes the VLM to derive transformation given language-described manipulation goal in 3 steps: i) scene analysis of the language query and object nodes V; ii) manipulation analysis to provide an analytical calculation process of the target objects desired position and orientation; iii) output task-desired object position ci and semantic orientation set Si for each object. Afterward, given each objects initial position ci and semantic orientation set Si, the 6-DoF transformation matrix Pi can be derived. Specifically, the translation transformation ti = ci ci, and we solve the rotation transformation Ri from Si and Si using Kabsch-Umeyama algorithm [60, 61, 132]. 2) Low-Level Motion Execution: Similar to CoPa [51], our model includes task-oriented grasping and task-aware motion planning. We first segment the manipulated objects or parts using Florence-2 [147] and SAM [65] to obtain the object point cloud, and then we use GSNet [38] to generate grasp pose candidates. We select the most effective grasp pose by balancing the grasp score and the angle between the robots approaching direction and the world frames z-axis. Conditioned on the text instruction, SOFAR predicts the target objects translation and rotation matrix, which define the transformation from the grasp pose to the placement pose. An open-source motion planning module [121] is then used to generate collision-free trajectory. In addition, we set the initial joint position as the midpoint to achieve smooth motion while reducing collisions between the manipulated object and the environment. IV. EXPERIMENTS A. Benchmarks We propose two benchmarks to demonstrate the effectiveness of our SOFAR in spatial reasoning and robotic manipulation. 1) Open6DOR V2: For simulation experiments, we choose the Open6DOR[28] Benchmark to comprehensively evaluate our spatial understanding abilities. Beyond the perception tasks that it originally proposes, we further construct an execution track to enable comparison with close-loop policies. We name the new combined benchmark Open6DOR V2. Perception tasks. In line with Open6DORs definition, the model takes an RGB-D scene image along with language instruction as input and directly outputs the translation and orientation of the target object. Execution tasks. We replicate Open6DOR scenes and ground them into robosuite simulation environment for execution, excluding single-object scenes to better assess the understanding of spatial relationships. The model takes the RGB-D image with language instruction as input and completes the entire execution process. We build it based on robosuite [173] and adopt the format established by LIBERO [76]. Evaluation is conducted according to the final position and orientation of the target object. 2) 6-DoF SpatialBench: To further evaluate spatial understanding that requires 6-DoF awareness, we propose VQA TABLE II: SimplerEnv [74] simulation valuation results for the Google Robot setup. We present success rates for the Variant Aggregation and Visual Matching approaches. Top-1 & Top-2 accuracies are represented using different colors, bold text, and underlines. OXE: Open X-Embodiment dataset [96]. Google Robot Evaluation Setup Policy Training Data Pick Coke Can Move Near Open / Close Drawer Horizontal Laying Vertical Laying Standing Average Average Open Close Average Average Variant Aggregation Visual Matching RT-1-X [96] RT-2-X [174] Octo-Base [126] OpenVLA [64] OXE OXE OXE OXE SOFAR Zero-Shot RT-1-X [96] RT-2-X [174] Octo-Base [126] OpenVLA [64] OXE OXE OXE OXE SOFAR Zero-Shot 0.569 0.822 0.005 0. 0.861 0.820 0.740 0.210 0.270 0.770 0.204 0.754 0.000 0.271 0.698 0.893 0.013 0.653 0.490 0.823 0.006 0. 0.323 0.792 0.031 0.477 0.069 0.519 0.333 0.372 0.000 0.021 0.158 0.195 0.294 0.353 0.011 0.177 0.397 0.661 0.012 0.411 0.960 0. 0.907 0.740 0.200 0.394 0.297 0.676 0.330 0.740 0.210 0. 0.550 0.880 0.090 0.190 0.567 0.787 0.170 0.163 0.317 0.779 0.042 0.462 0.296 0.891 0.157 0.343 0.009 0.444 0.194 0.518 0.597 0.250 0.227 0.356 0.534 0.606 0.168 0. 1.000 1.000 0.923 0.917 0.227 0.578 0. 0.749 TABLE III: SimplerEnv [74] simulation evaluation results for the WidowX + Bridge setup. We report both the final success rate (Success) along with partial success (e.g., Grasp Spoon). Top-1 & Top-2 accuracies are represented using different colors, bold text, and underlines. OXE: Open X-Embodiment dataset [96]. Bridge: BridgeData V2 dataset [136]. Put Spoon on Towel Put Carrot on Plate Stack Green Block on Yellow Block Put Eggplant in Yellow Basket Policy Training Data RT-1-X [8] Octo-Base [126] Octo-Small [126] OpenVLA [64] RoboVLM [72] RoboVLM [72] SpatialVLA [112] SpatialVLA [112] OXE OXE OXE OXE OXE Bridge OXE Bridge Grasp Spoon 0.167 0.347 0.778 0.041 0.375 0.542 0.250 0. Success 0.000 0.125 0.472 0.000 0.208 0.292 0.208 0.167 Grasp Carrot 0.208 0.528 0.278 0.333 0.333 0.250 0.417 0.292 0.042 0.083 0.097 0.000 0.250 0.250 0.208 0.250 SOFAR Zero-Shot 0.625 0.583 0.750 0.667 Success Grasp Green Block Success Grasp Eggplant Success 0.083 0.319 0.403 0.125 0.083 0.458 0.583 0.625 0. 0.000 0.000 0.042 0.000 0.083 0.125 0.250 0.292 0.708 0.000 0.667 0.875 0.083 0.000 0.583 0.792 1.000 0.667 0.000 0.431 0.569 0.041 0.000 0.583 0.708 1.000 0. Average 0.011 0.160 0.300 0.010 0.135 0.313 0.344 0.427 0.583 benchmark for spatial understanding evaluation, named 6DoF SpatialBench. Previous benchmarks [10, 15, 33, 124] for assessing VLMs primarily focus on understanding spatial positions, with little attention given to orientation. Additionally, most assessments emphasize relative and imprecise spatial relationships (e.g., to the left, nearest) while lacking quantitative metrics. In contrast, 6-DoF SpatialBench focuses on both positional and orientational understanding, encompassing 223 manual annotated samples, divided into the position track and orientation track depending on the question. Each task includes an RGB image of scene and multiple-choice questions with four options. Specifically, the tasks cover numerical queries (e.g., counting), positional relationships (e.g., left / right), and orientation (the facing direction of an object). The model needs to analyze the image and select the correct answer from four options. All questions and ground-truth answers are carefully designed through human annotation. B. 6-DoF object rearrangement evaluation in Simulation We conduct experiments on the proposed Open6DOR V2 benchmark, with results presented in Table I. In perception tasks, we compare our results against the same baselines used in the original Open6DOR [28] experiments. SOFAR outperforms all baselines, demonstrating effective spatial understanding and zero-shot generalizability. In the execution tasks, we record the initial and final poses of the objects to evaluate execution success. We use the original pretrained Octo [126] and the LIBERO-finetuned OpenVLA [64] as baselines, conducting all experiments in the same LIBERO environment where OpenVLA was fine-tuned to minimize the domain gap. Despite this, both Octo and OpenVLA show lower success rates, indicating their poor generalizability. In contrast, SOFAR achieves about 40% success, even with vanilla execution implementation. It is worth noting that some of the objects are inherently difficult to grasp, which significantly hampers execution success. Fig. 7: Quantitative evaluation of zero-shot real-world language-grounded rearrangement. We design 60 diverse real-world tasks involving over 100 diverse objects (detailed in Table XIII). The Success (%) is obtained with 3 trials per task and method. TABLE IV: Spatial comprehension evaluation on our proposed 6-DoF SpatialBench. Depth-Esti: Use monocular depth estimation methods like Metric3D [157] or Moge [138]. rel.: Relative metric evaluation, abs.: Absolute metric evaluation. Method Depth-Esti Position Orientation rel. abs. rel. abs. Total Blind Evaluation with Large Language Models GPT-3.5-Turbo [9] GPT-4-Turbo [99] 24.5 27. 24.9 27.3 26.7 29.2 27.5 27.9 25.7 27.8 General Vision Language Models LLaVA-1.5 [80] GPT-4o-mini [100] GPT-4V [98] GPT-4o [100] 30.9 33.3 37.7 49.4 24.5 26.9 32.7 28.4 28.3 32.5 36.7 44.2 25.8 23.8 27.5 25.8 27.2 31.0 33.9 36. Vision Language Models with Spatial Awareness SpaceLLaVA [12] SpaceMantis [12] SpatialBot [10] RoboPoint [160] SOFAR 32.4 33.6 50.9 43.8 59.6 30.5 29.2 21.6 30.8 33.8 30.9 27.2 39.6 33.8 54. 24.9 25.0 22.9 25.8 31.3 28.2 28.9 32.7 33.5 43.9 TABLE V: Semantic Orientation evaluation on our proposed OrienText300K dataset test spilt. Method 45 30 15 5 Average PointSO-S PointSO-B PointSO-L 77.34 79.69 81.25 74.22 77.34 78. 67.97 70.31 72.66 60.94 62.50 65.63 70.12 72.46 74.42 TABLE VI: Zero-shot Semantic Direction evaluation of robustness on OrienText300K test split. Single-View: randomly select camera viewpoint within the unit sphere and generate single viewpoint within the FoV on polar coordinates. Jitter: Gaussian jittering with noise ϵ (0, σ2) and σ = 0.01. Rotate: random SO(3) rotation sampling over X-Y-Z Euler angle (α, β, γ) U(θ, θ) and θ = π. All: All the corruptions. Method PointSO-S PointSO-B PointSO-L OrienText300K-C Variants Single-View Jitter Rotate All 72.66 75.00 76. 76.56 78.90 81.25 73.43 75.78 77.34 67.19 71.09 74.22 We call for more robust execution policies and manipulation strategies, such as prehensile grasping and adaptive techniques, to demonstrate better performance on Open6DOR V2. C. Zero-shot Object Manipulation Evaluation in Simulation SIMPLER [74] is suite of open-source simulated evaluation environments designed for real-world robot manipulation setups. SIMPLER provides standardized platform for benchmarking manipulation tasks, emphasizing reproducibility and alignment with real-world scenarios. We conduct quantitative evaluations of SOFARs zero-shot execution performance on Google Robot tasks & Widow-X tasks and compare it to baselines including Octo [126], OpenVLA [64] and more concurrent works [72, 112]. The robot follows the planned trajectory generated by the planning module, as described in Sec. III-B2, to execute the task. Furthermore, leveraging the error detection and re-planning capabilities of VLMs [100, 125], we can make multiple attempts following single-step execution failure to approximate achieve closed-loop effect. For fairness, we limit the maximum number of attempts to three. Detailed visualizations and analyses are provided in the Appendix C. As shown in Tables II and III, despite the training data for Octo and OpenVLA including Google Robot tasks, SOFAR demonstrates superior zero-shot performance compared to most baselines. D. Zero-shot Real-world Manipulation 1) Hardware Setup: We set up real-world tabletop environment utilizing Franka Panda robotic arm equipped with parallel gripper. For perception, we integrate single RGB-D camera (Intel RealSense D415) mounted on the end-effector. The details of the environmental visualization and additional real-world robot setup are provided in the Appendix A. Fig. 8: Realworld Orientation-Aware Navigation. We present both the third-person view and the egocentric view, annotating the predicted orientation of the interacted objects. 2) Tasks and Evaluations: To comprehensively evaluate the generalization of SOFAR, we design 60 diverse real-world experimental tasks involving over 100 diverse objects. Following Open6DOR [28]. These tasks are categorized into three tracksposition, orientation, and comprehensive & 6-DoF, each with simple and hard levels. The position-simple track focuses on basic spatial relationships (front/back/left/right), while the position-hard track involves more complex spatial concepts (between/center/customized). The orientation-simple track targets object part orientations, whereas the orientation-hard track requires precise angle judgments for upright or flipped objects. Comprehensive tasks demand intricate instruction understanding and long-horizon operations, and 6-DoF tasks require simultaneous position and orientation control. Each task is executed three times to ensure statistical reliability. Detailed visualization and analysis are provided in the Appendix A. 3) Results: The quantitative results are shown in Fig. 7, SOFAR outperform the baseline across all tracks, particularly in the challenging orientation and comprehensive & 6-DoF tasks. Meanwhile, SOFAR also utilizes the minimal planning time overhead. In addition, SOFAR is not limited to single embodiment. In qualitative experiments, we also test different embodiments, such as dexterous hand and suction cup, as shown in Fig. 6. Additional robot setups and generalization experiments are included in the Appendix A. E. Visual Question Answer on 6-DoF SpatialBench We evaluate SOFAR on our proposed 6-DoF SpatialBench, as shown in Table IV. We choose several VLMs and comparable methods as baselines. SOFAR achieves superior performance across both position-track and orientation-track, outperforming baselines by over 18%. F. Semantic Orientation Prediction Using free-text descriptions to extract semantic orientations from object point clouds is challenging. In Objaverse [22], we manually annotate 128 diverse objects and form an OrienText300K test split to evaluate the directional accuracy of PointSO. We trained different model variants on OrienText300K, and the results in Table V, report varying accuracies between 45 5. In the real world, acquiring complete point clouds is difficult even impractical. To assess the robustness, we introduce random rotations, single-sided point clouds, and Gaussian noise. The accuracy at 45, as shown in Table VI, demonstrates the models performance under these conditions. G. Additional Applications 1) Cross Embodiedment Generalization: Our approach determines grasp poses by generating masks and plans the target pose and transformation using our PointSO and large language model. It does not rely on trajectory data specific to any robotic arm, making SOFAR embodiment-agnostic. Fig. 6 illustrates the diverse embodiments employed in our real-world experiments. Leveraging the GSNet [137] algorithm based on LeapHand [116], we perform 6-DoF object manipulation experiments on dexterous hands. We conduct three positionrelated and three rotation-related experiments. Leveraging the PointSO and large language models, SOFAR is capable of performing complex 6-DoF manipulation tasks, such as Upright the fallen wine glass and arrange it neatly in row with the other wine glasses. 2) Long Horizon Planning & Close-Loop Execution: Similar to ReKep [56], SOFAR leverages VLMs [100, 125] to perform long-horizon decomposition of complex tasks and employs dualsystem VLMs [100, 125] to determine the success of execution between tasks and subtasks, enabling closed-loop execution. When discrepancy between the results and expectations is detected, SOFAR re-percepts and re-executes the current subtask. Detailed visualization and analysis can be found in Appendices and D. 3) Orientation-Aware Robotic Navigation: Semantic orientation can not only be applied to manipulation tasks but also to robotic navigation tasks. As shown in Fig. 8, we conduct experiments using robotic dog for orientationaware robotic navigation tasks. Unlike traditional navigation approaches, the dog is required to face specific direction during navigation. This orientation-aware constraint enhances the navigation process by ensuring precise alignment with the desired orientation, thereby improving task performance in scenarios where directionality is critical. V. RELATED WORKS VII. CONCLUSIONS A. Vision-Language Models for Spatial Understanding Vision-Language Models(VLMs) are rapidly being developed in research community, driven by the storming lead in extending GPT-style [9, 113, 114] Large Language Models (LLMs) like LLaMA [130, 131] to VLMs [1, 4, 23, 30, 32, 79, 80, 123, 129, 165, 167]. SpatialVLM [12] pioneers this direction by constructing VQA data in spatial understanding from RGBD, which is used for training an RGB-only VLM. Following SpatialVLM, SpatialRGPT [15] extends RGB-based spatial understanding to RGB-D by constructing spatial understanding data using 3D scene graphs. SpatialBot [10] explores RGB-D spatial reasoning through hierarchical depth-based reasoning. Some other works propose visual prompting for improving GPT-4Vs spatial understanding [77, 88, 151]. Meanwhile, another line of works explores VLMs using 3D representations such as point clouds for 3D scene [42, 50] and objectcentric [109, 110, 149] understanding. Despite the remarkable progress, these works are limited in 3-DoF understanding which is not actionable. In contrast, we explore spatial understanding in 6-DoFs from RGB-D via VLMs. Unlike vanilla 3D scene graphs used by SpatialRGPT for data construction, we propose orientation-aware 3D scene graphs realized by our proposed PointSO. In addition, we formulate spatial understanding as graph learning, where the scene graph nodes are directly input during inference. B. Language-Grounded Robot Manipulation Language-grounded robot Manipulation adopts the human language as general instruction interface. Existing works can be categorized into two groups: i) End-to-end models like RTseries [5, 8, 174] built upon unified cross-modal Transformers with tokenized actions [7, 45, 73, 78, 92, 117, 168], large visionlanguage-action (VLA) models built from VLMs [64], or 3D representations [13, 160, 169]. Training on robot data such as Open X-Embodiment [96] and DROID [63], remarkable process has been made. However, the data scale is still limited compared to in-the-wild data for training VLMs. ii) Decoupled high-level reasoning and low-level actions in large VLMs and small off-the-shelf policy models, primitives [39, 51, 53 55, 57, 59, 75, 90, 94, 153, 161], or articulated priors [52, 71]. Our SOFAR lies in this group, where an open-world generalization property emerges from VLMs and our proposed PointSO empowered by orientation-aware spatial understanding. VI. LIMITATIONS One notable limitation for decoupled systems like SOFAR is that the execution may fail due to sub-module error, i.e., robots may place target objects with an error transformation because of unstable grasping or inaccurate visual perception. For example, the pen will be placed in an unexpected pose due to the rotation during execution. Future works include integrating scalable data and more advanced models and exploring the potential of combining end-to-end and such decoupled methods, and expanding SOFAR to more applications. In this paper, we propose semantic orientation, languagegrounded representation that defines object orientations via intuitive descriptors (e.g., plug-in direction), bridging geometric reasoning and functional semantics. To enable this, we introduce OrienText300K, large-scale dataset of 3D models annotated with semantic orientation. Through PointSO and the integrated SOFAR system, we significantly enhance robotic manipulation capabilities, as demonstrated by strong performance in both simulated and real-world experiments."
        },
        {
            "title": "REFERENCES",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: visual language model for few-shot learning. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2022. 2, 11 [2] Iro Armeni, Ozan Sener, Amir Zamir, Helen Jiang, Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3d semantic parsing of large-scale indoor spaces. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2016. 27 [3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 24, 27 [4] Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan L. Yuille, Trevor Darrell, Jitendra Malik, and Alexei A. Efros. Sequential modeling enables scalable learning for large vision models. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2024. 11 [5] Suneel Belkhale, Tianli Ding, Ted Xiao, Pierre Sermanet, Quon Vuong, Jonathan Tompson, Yevgen Chebotar, Debidatta Dwibedi, and Dorsa Sadigh. RT-H: action hierarchies using language. CoRR, abs/2403.01823, 2024. 11 [6] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen Creel, Jared Quincy Davis, Dorottya Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, and et al. On the opportunities and risks of foundation models. CoRR, abs/2108.07258, 2021. 26 [7] Konstantinos Bousmalis, Giulia Vezzani, Dushyant Rao, Coline Manon Devin, Alex X. Lee, Maria Bauza Villalonga, Todor Davchev, Yuxiang Zhou, Agrim Gupta, Akhil Raju, Antoine Laurens, Claudio Fantacci, Valentin Dalibard, Martina Zambelli, Murilo Fernandes Martins, Rugile Pevceviciute, Michiel Blokzijl, Misha Denil, Nathan Batchelor, Thomas Lampe, Emilio Parisotto, Konrad Zolna, Scott Reed, Sergio Gomez Colmenarejo, Jonathan Scholz, Abbas Abdolmaleki, Oliver Groth, Jean-Baptiste Regli, Oleg Sushkov, Thomas Rothorl, Jose Enrique Chen, Yusuf Aytar, David Barker, Joy Ortiz, Martin Riedmiller, Jost Tobias Springenberg, Raia Hadsell, Francesco Nori, and Nicolas Heess. Robocat: self-improving generalist agent for robotic manipulation. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. 11 [8] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J. Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael S. Ryoo, Grecia Salazar, Pannag R. Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong T. Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. RT-1: robotics transformer for real-world control at scale. In Robotics: Science and Systems XIX, Daegu, Republic of Korea, July 10-14, 2023, 2023. 8, 11 [9] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2020. 9, 11, 31 [10] Wenxiao Cai, Yaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang, Hao Dong, and Bo Zhao. Spatialbot: Precise spatial understanding with vision language models. CoRR, abs/2406.13642, 2024. 2, 8, 9, 11, 22, 23, 24 [11] Matthew Chang, Theophile Gervet, Mukul Khanna, Sriram Yenamandra, Dhruv Shah, So Yeon Min, Kavit Shah, Chris Paxton, Saurabh Gupta, Dhruv Batra, Roozbeh Mottaghi, Jitendra Malik, and Devendra Singh Chaplot. GOAT: GO to Any Thing. In Proceedings of Robotics: Science and Systems, Delft, Netherlands, July 2024. 30 [12] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter, Danny Driess, Pete Florence, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing visionlanguage models with spatial reasoning capabilities. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2024. 2, 9, [13] Shizhe Chen, Ricardo Garcia Pinel, Cordelia Schmid, and Ivan Laptev. Polarnet: 3d point clouds for languageguided robotic manipulation. In Conference on Robot Learning, CoRL 2023, 6-9 November 2023, Atlanta, GA, USA, volume 229 of Proceedings of Machine Learning Research, pages 17611781. PMLR, 2023. 11 [14] Zixuan Chen, Xialin He, Yen-Jen Wang, Qiayuan Liao, Yanjie Ze, Zhongyu Li, Shankar Sastry, Jiajun Wu, Koushil Sreenath, Saurabh Gupta, et al. Learning smooth humanoid locomotion through lipschitz-constrained policies. arXiv preprint arXiv:2410.11825, 2024. 30 [15] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision language model. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2024. 2, 8, 11 [16] Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, and Ying Shan. Yolo-world: Real-time open-vocabulary object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1690116911, 2024. 26, 27 [17] Xuxin Cheng, Yandong Ji, Junming Chen, Ruihan Yang, Ge Yang, and Xiaolong Wang. Expressive WholeBody Control for Humanoid Robots. In Proceedings of Robotics: Science and Systems, Delft, Netherlands, July 2024. 30 [18] Jaime Corsetti, Davide Boscaini, Changjae Oh, Andrea Cavallaro, and Fabio Poiesi. Open-vocabulary object 6d pose estimation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 1807118080. IEEE, 2024. 2 [19] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2017. [20] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven C. H. Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2023. 24 [21] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi. Objaverse-xl: universe of 10m+ 3d objects. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2023. 26, 30 [22] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. In Objaverse: universe of annotated 3d objects. IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2023. 4, 10, 27 [23] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, Yen-Sung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, KuoHao Zeng, Jon Borchardt, Dirk Groeneveld, Jen Dumas, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross B. Girshick, Ali Farhadi, and Aniruddha Kembhavi. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. CoRR, abs/2409.17146, 2024. 11 [24] Jiajun Deng, Shaoshuai Shi, Peiwei Li, Wengang Zhou, Yanyong Zhang, and Houqiang Li. Voxel r-cnn: Towards high performance voxel-based 3d object detection. In AAAI Conf. Artif. Intell. (AAAI), 2021. [25] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACLHLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 41714186. Association for Computational Linguistics, 2019. 29 the 2019 Conference of [26] Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang, Song Bai, and Xiaojuan Qi. PLA: language-driven openvocabulary 3d scene understanding. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2023. 29 [27] Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang, Song Bai, and Xiaojuan Qi. Lowis3d: Language-driven open-world instance-level 3d scene understanding. IEEE Trans. Pattern Anal. Mach. Intell. (TPAMI), pages 116, 2024. 29 [28] Yufei Ding, Haoran Geng, Chaoyi Xu, Xiaomeng Fang, Jiazhao Zhang, Songlin Wei, Qiyu Dai, Zhizheng Zhang, and He Wang. Open6DOR: Benchmarking openinstruction 6-dof object rearrangement and VLM-based approach. In IEEE/RSJ Int. Conf. Intell. Robot. and Syst. (IROS), 2024. 7, 8, 10, 26, 27, [29] Runpei Dong, Zekun Qi, Linfeng Zhang, Junbo Zhang, Jianjian Sun, Zheng Ge, Li Yi, and Kaisheng Ma. Autoencoders as cross-modal teachers: Can pretrained 2d image transformers help 3d representation learning? In Int. Conf. Learn. Represent. (ICLR), 2023. 2, 4, 27, 29, 30 [30] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, and Li Yi. DreamLLM: Synergistic multimodal comprehension and creation. In Int. Conf. Learn. Represent. (ICLR), 2024. 2, 11 [31] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image In Int. Conf. Learn. Represent. recognition at scale. (ICLR), 2021. 4, 29 [32] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An In Int. Conf. embodied multimodal language model. Mach. Learn. (ICML), 2023. 11 [33] Mengfei Du, Binhao Wu, Zejun Li, Xuanjing Huang, and Zhongyu Wei. Embspatial-bench: Benchmarking spatial understanding for embodied tasks with large visionlanguage models. arXiv preprint arXiv:2406.05756, 2024. 2, 8, 22, 23, 24 [34] Jiafei Duan, Wentao Yuan, Wilbert Pumacay, Yi Ru Wang, Kiana Ehsani, Dieter Fox, and Ranjay Krishna. Manipulate-anything: Automating real-world robots using vision-language models. In 8th Annual Conference on Robot Learning, 2024. [35] Ben Eisner, Harry Zhang, and David Held. Flowbot3d: Learning 3d articulation flow to manipulate articulated objects. arXiv preprint arXiv:2205.04382, 2022. 24 [36] Nico Engel, Vasileios Belagiannis, and Klaus Dietmayer. Point transformer. IEEE Access, 9:134826134840, 2021. 27, 29 [37] Guofan Fan, Zekun Qi, Wenkai Shi, and Kaisheng Ma. Point-gcc: Universal self-supervised 3d scene CoRR, pre-training via geometry-color contrast. abs/2305.19623, 2023. 29 [38] Haoshu Fang, Chenxi Wang, Minghao Gou, and Cewu Lu. Graspnet-1billion: large-scale benchmark for general object grasping. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 11441 11450. Computer Vision Foundation / IEEE, 2020. 7 [39] Kuan Fang, Fangchen Liu, Pieter Abbeel, and Sergey Levine. MOKA: Open-World Robotic Manipulation through Mark-Based Visual Prompting. In Proceedings of Robotics: Science and Systems, Delft, Netherlands, July 2024. 11 [40] Li Fei-Fei. The Worlds See: Curiosity, Exploration, and Discovery at the Dawn of AI. Flatiron books: moment of lift book, 2023. 2 [41] Li Fei-Fei, Asha Iyer, Christof Koch, and Pietro Perona. What do we perceive in glance of real-world scene? Journal of vision, 7(1):1010, 2007. [42] Rao Fu, Jingyu Liu, Xilun Chen, Yixin Nie, and Wenhan Xiong. Scene-llm: Extending language model for 3d visual understanding and reasoning. CoRR, abs/2403.11401, 2024. 11 [43] JJ Gibson. The theory of affordances. Perceiving, acting and knowing: Towards an ecological psychology/Erlbaum, 1977. 29 [44] Mohit Goyal, Sahil Modi, Rishabh Goyal, and Saurabh Gupta. Human hands as probes for interactive object understanding. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 32833293. IEEE, 2022. 29 [45] Markus Grotz, Mohit Shridhar, Yu-Wei Chao, Tamim Asfour, and Dieter Fox. Peract2: Benchmarking and learning for robotic bimanual manipulation tasks. In CoRL 2024 Workshop on Whole-body Control and Bimanual Manipulation: Applications in Humanoids and Beyond, 2024. 11 [46] Abdullah Hamdi, Silvio Giancola, and Bernard Ghanem. MVTN: multi-view transformation network for 3d shape recognition. In Int. Conf. Comput. Vis. (ICCV), pages 111. IEEE, 2021. 27 [47] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross B. Girshick. Masked autoencoders are scalable vision learners. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2022. 29, [48] Tairan He, Zhengyi Luo, Xialin He, Wenli Xiao, Chong Zhang, Weinan Zhang, Kris Kitani, Changliu Liu, and Guanya Shi. Omnih2o: Universal and dexterous humanto-humanoid whole-body teleoperation and learning. In Annu. Conf. Robot. Learn. (CoRL), 2024. 30 [49] Xialin He, Runpei Dong, Zixuan Chen, and Saurabh Gupta. Learning getting-up policies for real-world humanoid robots. arXiv preprint arXiv:2502.12152, 2025. 30 [50] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3dllm: Injecting the 3d world into large language models. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2023. 11 [51] Haoxu Huang, Fanqi Lin, Yingdong Hu, Shengjie Wang, and Yang Gao. Copa: General robotic manipulation through spatial constraints of parts with foundation models. CoRR, abs/2403.08248, 2024. 7, 11, 23, 28 [52] Siyuan Huang, Haonan Chang, Yuhan Liu, Yimeng Zhu, Hao Dong, Peng Gao, Abdeslam Boularias, and Hongsheng Li. A3vlm: Actionable articulation-aware vision language model. arXiv preprint arXiv:2406.07549, 2024. 11 [53] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Tomas Jackson, Noah Brown, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue: Embodied reasoning through planning with language models. In Conference on Robot Learning, CoRL 2022, 14-18 December 2022, Auckland, New Zealand, volume 205 of Proceedings of Machine Learning Research, pages 17691782. PMLR, 2022. [54] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. Voxposer: Composable 3d value maps for robotic manipulation with language models. In Annu. Conf. Robot. Learn. (CoRL), 2023. 7, 31 [55] Wenlong Huang, Fei Xia, Dhruv Shah, Danny Driess, Andy Zeng, Yao Lu, Pete Florence, Igor Mordatch, Sergey Levine, Karol Hausman, and Brian Ichter. Grounded decoding: Guiding text generation with In Advances grounded models for embodied agents. in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. 11 [56] Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, and Li Fei-Fei. Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. In Annu. Conf. Robot. Learn. (CoRL), 2024. 10, 22, 23, 28, 29, 30, 31 [57] Brian Ichter, Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Dmitry Kalashnikov, Sergey Levine, Yao Lu, Carolina Parada, Kanishka Rao, Pierre Sermanet, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Mengyuan Yan, Noah Brown, Michael Ahn, Omar Cortes, Nicolas Sievers, Clayton Tan, Sichun Xu, Diego Reyes, Jarek Rettinghouse, Jornell Quiambao, Peter Pastor, Linda Luu, Kuang-Huei Lee, Yuheng Kuang, Sally Jesmonth, Nikhil J. Joshi, Kyle Jeffrey, Rosario Jauregui Ruano, Jasmine Hsu, Keerthana Gopalakrishnan, Byron David, Andy Zeng, and Chuyuan Kelly Fu. Do as can, not as say: Grounding language in robotic affordances. In Conference on Robot Learning, CoRL 2022, 14-18 December 2022, Auckland, New Zealand, volume 205 of Proceedings of Machine Learning Research, pages 287318. PMLR, 2022. 2, 11, 29 [58] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [59] Hanxiao Jiang, Binghao Huang, Ruihai Wu, Zhuoran Li, Shubham Garg, Hooshang Nayyeri, Shenlong Wang, and Yunzhu Li. Roboexp: Action-conditioned scene graph via interactive exploration for robotic manipulation. In Annu. Conf. Robot. Learn. (CoRL), 2024. 11 [60] Wolfgang Kabsch. solution for the best rotation to relate two sets of vectors. Acta Crystallographica Section A: Crystal Physics, Diffraction, Theoretical and General Crystallography, 32(5):922923, 1976. 7 [61] Wolfgang Kabsch. discussion of the solution for the best rotation to relate two sets of vectors. Acta Crystallographica Section A: Crystal Physics, Diffraction, Theoretical and General Crystallography, 34(5): 827828, 1978. 7 [62] Ivan Kapelyukh, Yifei Ren, Ignacio Alzugaray, and Edward Johns. Dream2real: Zero-shot 3d object reIn IEEE arrangement with vision-language models. International Conference on Robotics and Automation, ICRA 2024, Yokohama, Japan, May 13-17, 2024, pages 47964803. IEEE, 2024. 7 [63] Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, Peter David Fagan, Joey Hejna, Masha Itkina, Marion Lepert, Yecheng Jason Ma, Patrick Tree Miller, Jimmy Wu, Suneel Belkhale, Shivin Dass, Huy Ha, Arhan Jain, Abraham Lee, Youngwoon Lee, Marius Memmel, Sungjae Park, Ilija Radosavovic, Kaiyuan Wang, Albert Zhan, Kevin Black, Cheng Chi, Kyle Beltran Hatch, Shan Lin, Jingpei Lu, Jean Mercat, Abdul Rehman, Pannag Sanketi, Archit Sharma, Cody Simpson, Quan Vuong, Homer Rich Walke, Blake Wulfe, Ted Xiao, Jonathan Heewon Yang, Arefeh Yavary, Tony Z. Zhao, Christopher Agia, Rohan Baijal, Mateo Guaman Castro, Daphne Chen, Qiuyu Chen, Trinity Chung, Jaimyn Drake, Ethan Paul Foster, Jensen Gao, David Antonio Herrera, Minho Heo, Kyle Hsu, Jiaheng Hu, Donovon Jackson, Charlotte Le, Yunshuang Li, Roy Lin, Zehan Ma, Abhiram Maddukuri, Suvir Mirchandani, Daniel Morton, Tony Nguyen, Abigail ONeill, Rosario Scalise, Derick Seale, Victor Son, Stephen Tian, Emi Tran, Andrew E. Wang, Yilin Wu, Annie Xie, Jingyun Yang, Patrick Yin, Yunchu Zhang, Osbert Bastani, Glen Berseth, Jeannette Bohg, Ken Goldberg, Abhinav Gupta, Abhishek Gupta, Dinesh Jayaraman, Joseph Lim, Jitendra Malik, Roberto Martın-Martın, Subramanian Ramamoorthy, Dorsa Sadigh, Shuran Song, Jiajun Wu, Michael C. Yip, Yuke Zhu, Thomas Kollar, Sergey Levine, and Chelsea Finn. DROID: Large-Scale InThe-Wild Robot Manipulation Dataset. In Proceedings of Robotics: Science and Systems, Delft, Netherlands, July 2024. 11 [64] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Paul Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An open-source vision-languageaction model. CoRR, abs/2406.09246, 2024. 7, 8, 9, 11 [65] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. 2, 5, 7, 24, 26 [66] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense image annotations. Int. J. Comput. Vis., 123(1):3273, 2017. 2 [67] Yann Labbe, Lucas Manuelli, Arsalan Mousavian, Stephen Tyree, Stan Birchfield, Jonathan Tremblay, Justin Carpentier, Mathieu Aubry, Dieter Fox, and Josef Sivic. Megapose: 6d pose estimation of novel objects via render & compare. In Conference on Robot Learning, CoRL 2022, 14-18 December 2022, Auckland, New Zealand, volume 205 of Proceedings of Machine Learning Research, pages 715725. PMLR, 2022. 2 [68] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. BLIP: bootstrapping language-image pre-training for unified vision-language understanding and generation. In Int. Conf. Mach. Learn. (ICML), 2022. 2 [69] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. CoRR, abs/2301.12597, 2023. 24 [70] Li-Jia Li and Li Fei-Fei. What, where and who? classifying events by scene and object recognition. In IEEE 11th International Conference on Computer Vision, ICCV 2007, Rio de Janeiro, Brazil, October 14-20, 2007, pages 18. IEEE Computer Society, 2007. 2 [71] Xiaoqi Li, Mingxu Zhang, Yiran Geng, Haoran Geng, Yuxing Long, Yan Shen, Renrui Zhang, Jiaming Liu, and Hao Dong. Manipllm: Embodied multimodal large language model for object-centric robotic manipulation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 1806118070. IEEE, 2024. 2, 11, 23, [72] Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, Hanbo Zhang, and Huaping Liu. Towards generalist robot policies: What matters in building vision-language-action models. arXiv preprint arXiv:2412.14058, 2024. 8, 9 [73] Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, Hang Li, and Tao Kong. Visionlanguage foundation models as effective robot imitators. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. 11 [74] Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, et al. Evaluating real-world robot manipulation policies in simulation. arXiv preprint arXiv:2405.05941, 2024. 8, 9, 23, 30 [75] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. In IEEE International Conference on Robotics and Automation, ICRA 2023, London, UK, May 29 - June 2, 2023, pages 94939500. IEEE, 2023. 11 [76] Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, qiang liu, Yuke Zhu, and Peter Stone. LIBERO: Benchmarking knowledge transfer for lifelong robot learning. In Thirtyseventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. 7, 23, 30 [77] Dingning Liu, Xiaomeng Dong, Renrui Zhang, Xu Luo, Peng Gao, Xiaoshui Huang, Yongshun Gong, and Zhihui Wang. 3daxiesprompts: Unleashing the 3d spatial task capabilities of GPT-4V. CoRR, abs/2312.09738, 2023. [78] Hao Liu, Lisa Lee, Kimin Lee, and Pieter Abbeel. Instruction-following agents with jointly pre-trained vision-language models. CoRR, abs/2210.13431, 2022. 11 [79] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2023. 2, 11, 24, 27 [80] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2024. 9, 11 [81] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pages 3855. Springer, 2025. 25, 26, 27 [82] Yunze Liu, Yun Liu, Che Jiang, Kangbo Lyu, Weikang Wan, Hao Shen, Boqiang Liang, Zhoujie Fu, He Wang, and Li Yi. HOI4D: 4d egocentric dataset for categoryIn IEEE/CVF Conf. level human-object interaction. Comput. Vis. Pattern Recog. (CVPR), 2022. 30 [83] Yunze Liu, Changxi Chen, and Li Yi. Interactive humanoid: Online full-body motion reaction synthesis with social affordance canonicalization and forecasting. ArXiv, 2023. 30 [84] Yunze Liu, Junyu Chen, Zekai Zhang, Jingwei Huang, and Li Yi. Leaf: Learning frames for 4d point cloud sequence understanding. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 604613, 2023. 30 [85] Yunze Liu, Changxi Chen, Zifan Wang, and Li Yi. Crossvideo: Self-supervised cross-modal contrastive learning for point cloud video understanding. 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 1243612442, 2024. URL https://api.semanticscholar.org/CorpusID:267027937. 30 [86] Ze Liu, Zheng Zhang, Yue Cao, Han Hu, and Xin Tong. Group-free 3d object detection via transformers. In Int. Conf. Comput. Vis. (ICCV), 2021. 29 [87] Shiyang Lu, Haonan Chang, Eric Pu Jing, Abdeslam Boularias, and Kostas E. Bekris. OVIR-3D: openvocabulary 3d instance retrieval without training on 3d data. In Annu. Conf. Robot. Learn. (CoRL), 2023. 29 [88] Chenyang Ma, Kai Lu, Ta-Ying Cheng, Niki Trigoni, and Andrew Markham. SpatialPIN: Enhancing spatial reasoning capabilities of vision-language models through prompting and interacting 3d priors. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 11 [89] Jiageng Mao, Yujing Xue, Minzhe Niu, Haoyue Bai, Jiashi Feng, Xiaodan Liang, Hang Xu, and Chunjing Xu. Voxel transformer for 3d object detection. In Int. Conf. Comput. Vis. (ICCV), 2021. [90] Weixin Mao, Weiheng Zhong, Zhou Jiang, Dong Fang, Zhongyue Zhang, Zihan Lan, Fan Jia, Tiancai Wang, Haoqiang Fan, and Osamu Yoshie. Robomatrix: skillcentric hierarchical framework for scalable robot task planning and execution in open-world. arXiv preprint arXiv:2412.00171, 2024. 11 [91] Daniel Maturana and Sebastian A. Scherer. Voxnet: 3d convolutional neural network for real-time object recognition. In IEEE/RSJ Int. Conf. Intell. Robot. and Syst. (IROS), pages 922928. IEEE, 2015. 27 [92] Atharva Mete, Haotian Xue, Albert Wilcox, Yongxin Chen, and Animesh Garg. Quest: Self-supervised skill abstractions for learning continuous control. CoRR, abs/2407.15840, 2024. 11 [93] Kaichun Mo, Leonidas Guibas, Mustafa Mukadam, Abhinav Gupta, and Shubham Tulsiani. Where2act: In From pixels to actions for articulated 3d objects. Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 68136823, 2021. 24 [94] Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita Dasgupta, Annie Xie, Danny Driess, Ayzaan Wahid, Zhuo Xu, Quan Vuong, Tingnan Zhang, Tsang-Wei Edward Lee, Kuang-Huei Lee, Peng Xu, Sean Kirmani, Yuke Zhu, Andy Zeng, Karol Hausman, Nicolas Heess, Chelsea Finn, Sergey Levine, and Brian Ichter. PIVOT: iterative visual prompting elicits actionable knowledge for vlms. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. 11 [95] NVDIA. Nvidia isaac sim. https://developer.nvidia.com/ isaac-sim, 2021. [96] Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alexander Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben BurgessLimerick, Beomjoon Kim, Bernhard Scholkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Buchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Paul Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Haoshu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I. Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, Joao Silverio, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi Jim Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J. Joshi, Niko Sunderhauf, Ning Liu, Norman Di Palo, Nur Muhammad (Mahi) Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R. Sanketi, Patrick Tree Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martın-Martın, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham D. Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Liangwei Xu, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, and Zipeng Lin. Open x-embodiment: Robotic learning datasets and RT-X models : Open x-embodiment In IEEE International Conference on collaboration. Robotics and Automation, ICRA 2024, Yokohama, Japan, May 13-17, 2024, pages 68926903. IEEE, 2024. 8, 11 [97] OpenAI. Introducing chatgpt. 2022. URL https://openai. com/blog/chatgpt. 27 [98] OpenAI. Gpt-4v(ision) system card, 2023. URL https: //openai.com/research/gpt-4v-system-card. 7, 9, 24 [99] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. URL https://openai.com/research/gpt-4. 9 [100] OpenAI. Introducing gpt-4o and more tools to chatgpt free users. 2024. URL https://openai.com/index/ gpt-4o-and-more-tools-to-chatgpt-free/. 2, 4, 9, 10, 24, 26 [101] Yatian Pang, Wenxiao Wang, Francis E. H. Tay, Wei Liu, Yonghong Tian, and Li Yuan. Masked autoencoders for point cloud self-supervised learning. In Eur. Conf. Comput. Vis. (ECCV), 2022. [102] Songyou Peng, Kyle Genova, Chiyu Max Jiang, Andrea Tagliasacchi, Marc Pollefeys, and Thomas A. Funkhouser. Openscene: 3d scene understanding with In IEEE/CVF Conf. Comput. Vis. open vocabularies. Pattern Recog. (CVPR), 2023. 29 [103] Yuang Peng, Yuxin Cui, Haomiao Tang, Zekun Qi, Runpei Dong, Jing Bai, Chunrui Han, Zheng Ge, Xiangyu Zhang, and Shu-Tao Xia. Dreambench++: humanaligned benchmark for personalized image generation. CoRR, abs/2406.16855, 2024. 4 [104] Charles Ruizhongtai Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), pages 7785, 2017. 4, 26, 27 [105] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J. Guibas. Pointnet++: Deep hierarchical feature learning on point sets in metric space. In Adv. Neural Inform. Process. Syst. (NIPS), pages 50995108, 2017. 4, 27 [106] William Qi, Ravi Teja Mullapudi, Saurabh Gupta, and Deva Ramanan. Learning to move with affordance In 8th International Conference on Learning maps. Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. [107] Zekun Qi, Runpei Dong, Guofan Fan, Zheng Ge, Xiangyu Zhang, Kaisheng Ma, and Li Yi. Contrast with reconstruct: Contrastive 3d representation learning guided by generative pretraining. In Int. Conf. Mach. Learn. (ICML), 2023. 2, 4, 27, 29, 30 [108] Zekun Qi, Muzhou Yu, Runpei Dong, and Kaisheng Ma. VPP: efficient conditional 3d generation via voxelpoint progressive representation. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2023. 27 [109] Zekun Qi, Runpei Dong, Shaochen Zhang, Haoran Geng, Chunrui Han, Zheng Ge, Li Yi, and Kaisheng Ma. Shapellm: Universal 3d object understanding for embodied interaction. In Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29October 4, 2024, Proceedings, Part XLIII, volume 15101 of Lecture Notes in Computer Science, pages 214238. Springer, 2024. 2, 11, 27 [110] Zhangyang Qi, Ye Fang, Zeyi Sun, Xiaoyang Wu, Tong Wu, Jiaqi Wang, Dahua Lin, and Hengshuang Zhao. Gpt4point: unified framework for point-language In Proceedings of the understanding and generation. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2641726427, 2024. 11 [111] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Abed Al Kader Hammoud, Mohamed Elhoseiny, and Bernard Ghanem. Pointnext: Revisiting pointnet++ with improved training and scaling strategies. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2022. 27 [112] Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, et al. Spatialvla: Exploring spatial representations for visual-language-action model. arXiv preprint arXiv:2501.15830, 2025. 8, 9 [113] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018. [114] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 1(8): 9, 2019. 11 [115] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Int. Conf. Mach. Learn. (ICML), volume 139 of Proceedings of Machine Learning Research, pages 87488763. PMLR, 2021. 4, 29, 30 [116] Kenneth Shaw, Ananye Agarwal, and Deepak Pathak. Leap hand: Low-cost, efficient, and anthropomorarXiv preprint phic hand for arXiv:2309.06440, 2023. 10 learning. robot [117] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-actor: multi-task transformer for robotic manipulation. In Conference on Robot Learning, CoRL 2022, 14-18 December 2022, Auckland, New Zealand, volume 205 of Proceedings of Machine Learning Research, pages 785799. PMLR, 2022. 11 [118] Tianmin Shu, Michael S. Ryoo, and Song-Chun Zhu. Learning social affordance for human-robot interaction. In Int. Joint Conf. Artif. Intell. (IJCAI), 2016. 29 [119] Austin Stone, Ted Xiao, Yao Lu, Keerthana Gopalakrishnan, Kuang-Huei Lee, Quan Vuong, Paul Wohlhart, Sean Kirmani, Brianna Zitkovich, Fei Xia, Chelsea Finn, and Karol Hausman. Open-world object manipulation using pre-trained vision-language models. In Conference on Robot Learning, CoRL 2023, 6-9 November 2023, Atlanta, GA, USA, volume 229 of Proceedings of Machine Learning Research, pages 33973417. PMLR, 2023. 2 [120] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik G. Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition. In Int. Conf. Comput. Vis. (ICCV), 2015. 27 [121] Ioan A. ucan, Mark Moll, and Lydia E. Kavraki. The IEEE Robotics & Open Motion Planning Library. Automation Magazine, 19(4):7282, December 2012. doi: 10.1109/MRA.2012.2205651. https://ompl.kavrakilab. org. 7 [122] Jiaming Sun, Zihao Wang, Siyu Zhang, Xingyi He, Hongcheng Zhao, Guofeng Zhang, and Xiaowei Zhou. Onepose: One-shot object pose estimation without CAD models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 68156824. IEEE, 2022. 2 [123] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Emu: Generative In Int. Conf. Learn. pretraining in multimodality. Represent. (ICLR), 2024. [124] Emilia Szymanska, Mihai Dusmanu, Jan-Willem Buurlage, Mahdi Rad, and Marc Pollefeys. Space3d-bench: Spatial 3d question answering benchmark. arXiv preprint arXiv:2408.16662, 2024. 8 [125] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 9, 10, 24, 26, 27 [126] Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, You Liang Tan, Lawrence Yunliang Chen, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, and Sergey Levine. Octo: An open-source generalist robot policy. CoRR, abs/2405.12213, 2024. 7, 8, 9 [127] Stefanie Tellex, Thomas Kollar, Steven Dickerson, Matthew R. Walter, Ashis Gopal Banerjee, Seth J. Teller, and Nicholas Roy. Understanding natural language commands for robotic navigation and mobile manipulation. In AAAI Conf. Artif. Intell. (AAAI), 2011. 2 [128] Stefanie Tellex, Nakul Gopalan, Hadas Kress-Gazit, and Cynthia Matuszek. Robots that use language. Annu. Rev. Control. Robotics Auton. Syst., 3:2555, 2020. 29 [129] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: fully open, vision-centric exploration of multimodal llms. CoRR, abs/2406.16860, 2024. 11 [130] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023. 11, [131] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023. 11, 27 [132] Shinji Umeyama. Least-squares estimation of transformation parameters between two point patterns. IEEE Transactions on Pattern Analysis & Machine Intelligence, 13(04):376380, 1991. 7 [133] Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Thanh Nguyen, and Sai-Kit Yeung. Revisiting point cloud classification: new benchmark dataset and classification model on real-world data. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), pages 1588 1597, 2019. 27 [134] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Adv. Neural Inform. Process. Syst. (NIPS), pages 59986008, 2017. 2, 4, 29 [135] Johanna Wald, Helisa Dhamo, Nassir Navab, and Federico Tombari. Learning 3d semantic scene graphs In 2020 IEEE/CVF from 3d indoor reconstructions. Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 39603969. Computer Vision Foundation / IEEE, 2020. 2 [136] Homer Rich Walke, Kevin Black, Tony Zhao, Quan Vuong, Chongyi Zheng, Philippe Hansen-Estruch, Andre Wang He, Vivek Myers, Moo Jin Kim, Max Du, et al. Bridgedata v2: dataset for robot learning at scale. In Conference on Robot Learning, pages 1723 1736. PMLR, 2023. [137] Chenxi Wang, Haoshu Fang, Minghao Gou, Hongjie Fang, Jin Gao, and Cewu Lu. Graspness discovery in clutters for fast and accurate grasp detection. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pages 1594415953. IEEE, 2021. 10 [138] Ruicheng Wang, Sicheng Xu, Cassie Dai, Jianfeng Xiang, Yu Deng, Xin Tong, and Jiaolong Yang. Moge: Unlocking accurate monocular geometry estimation for open-domain images with optimal training supervision. arXiv preprint arXiv:2410.19115, 2024. 9 [139] Zehan Wang, Ziang Zhang, Tianyu Pang, Chao Du, Hengshuang Zhao, and Zhou Zhao. Orient anything: Learning robust object orientation estimation from rendering 3d models. arXiv preprint arXiv:2412.18605, 2024. 22, 30 [140] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits In Adv. Neural reasoning in large language models. Inform. Process. Syst. (NeurIPS), 2022. 2, 7, 31 [141] Bowen Wen, Wei Yang, Jan Kautz, and Stan Birchfield. Foundationpose: Unified 6d pose estimation and tracking of novel objects. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 1786817879. IEEE, 2024. 2 [142] Jiajun Wu. Physical scene understanding. AI Mag., 45 (1):156164, 2024. 2 [143] Tong Wu, Guandao Yang, Zhibing Li, Kai Zhang, Ziwei Liu, Leonidas J. Guibas, Dahua Lin, and Gordon Wetzstein. Gpt-4v(ision) is human-aligned evaluator for text-to-3d generation. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2024. [144] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: deep representation for volumetric shapes. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), pages 19121920, 2015. 27 [145] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, Li Yi, Angel X. Chang, Leonidas J. Guibas, and Hao Su. SAPIEN: simulated part-based interactive environment. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2020. 23, 24 [146] Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter Fox. Posecnn: convolutional neural network for 6d object pose estimation in cluttered scenes. In Robotics: Science and Systems XIV, Carnegie Mellon University, Pittsburgh, Pennsylvania, USA, June 26-30, 2018, 2018. 2 [147] Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, and Lu Yuan. Florence-2: Advancing unified representation In Proceedings of the for variety of vision tasks. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 48184829, 2024. 2, 5, 7, 24, 25, 26, 27 [148] Saining Xie, Jiatao Gu, Demi Guo, Charles R. Qi, Leonidas J. Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. In Eur. Conf. Comput. Vis. (ECCV), volume 12348 of Lecture Notes in Computer Science, pages 574591. Springer, 2020. [149] Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, and Dahua Lin. Pointllm: Empowering large language models to understand point clouds. CoRR, abs/2308.16911, 2023. 11 [150] Zhenjia Xu, Zhanpeng He, and Shuran Song. Universal manipulation policy network for articulated objects. IEEE robotics and automation letters, 7(2):24472454, 2022. 24 [151] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in GPT-4V. CoRR, abs/2310.11441, 2023. 5, 11, 27 [152] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. arXiv preprint arXiv:2412.14171, 2024. 30 [153] Sherry Yang, Yilun Du, Seyed Kamyar Seyed Ghasemipour, Jonathan Tompson, Leslie Pack Kaelbling, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. 11 [154] Sriram Yenamandra, Arun Ramachandran, Karmesh Yadav, Austin Wang, Mukul Khanna, Theophile Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander William Clegg, John Turner, et al. Homerobot: Open-vocabulary mobile manipulation. arXiv preprint arXiv:2306.11565, 2023. [155] Li Yi, Vladimir Kim, Duygu Ceylan, I-Chao Shen, Mengyan Yan, Hao Su, Cewu Lu, Qixing Huang, Alla Sheffer, and Leonidas Guibas. scalable active framework for region annotation in 3d shape collections. ACM Trans. Graph., 35(6):112, 2016. 27 [156] Li Yi, Hao Su, Xingwen Guo, and Leonidas J. Guibas. Syncspeccnn: Synchronized spectral CNN for 3d shape segmentation. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2017. 27 [157] Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaixuan Wang, Xiaozhi Chen, and Chunhua Shen. Metric3d: Towards zero-shot metric 3d prediction from single image. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pages 90099019. IEEE, 2023. 5, 9 [158] Tsuneo Yoshikawa. Manipulability of robotic mechanisms. The international journal of Robotics Research, 4(2):39, 1985. 2 [159] Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, and Jiwen Lu. Point-bert: Pre-training 3d point cloud transformers with masked point modeling. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2022. 4, 29 [160] Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan Murali, Arsalan Mousavian, and Dieter Fox. Robopoint: visionlanguage model for spatial affordance prediction for robotics. CoRR, abs/2406.10721, 2024. 2, 9, 11 [161] Zhecheng Yuan, Tianming Wei, Shuiqi Cheng, Gu Zhang, Yuanpei Chen, and Huazhe Xu. Learning to manipulate anywhere: visual generalizable framework for reinforcement learning. In 8th Annual Conference on Robot Learning, 2024. 11 [162] Jiazhao Zhang, Kunyu Wang, Shaoan Wang, Minghan Li, Haoran Liu, Songlin Wei, Zhongyuan Wang, Zhizheng Zhang, and He Wang. Uni-navid: video-based visionlanguage-action model for unifying embodied navigation tasks. arXiv preprint arXiv:2412.06224, 2024. 23 [163] Junbo Zhang, Runpei Dong, and Kaisheng Ma. CLIPFO3D: learning free open-world 3d scene representations In IEEE/CVF International from 2d dense CLIP. Conference on Computer Vision, ICCV 2023 - Workshops, Paris, France, October 2-6, 2023, pages 20402051. IEEE, 2023. [164] Linfeng Zhang, Xin Chen, Runpei Dong, and Kaisheng Ma. Region-aware knowledge distillation for efficient image-to-image translation. In Brit. Mach. Vis. Conf. (BMVC), 2023. 29 [165] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. In Int. Conf. Learn. Represent. (ICLR), 2024. 11 [166] Shaochen Zhang, Zekun Qi, Runpei Dong, Xiuxiu tuning for arXiv preprint Bai, and Xing Wei. efficient 3d representation learning. Positional prompt Florence, Chelsea Finn, Kumar Avinava Dubey, Danny Driess, Tianli Ding, Krzysztof Marcin Choromanski, Xi Chen, Yevgen Chebotar, Justice Carbajal, Noah Brown, Anthony Brohan, Montserrat Gonzalez Arenas, and Kehang Han. RT-2: vision-language-action models transfer web knowledge to robotic control. In Conference on Robot Learning, CoRL 2023, 6-9 November 2023, Atlanta, GA, USA, volume 229 of Proceedings of Machine Learning Research, pages 21652183. PMLR, 2023. 8, arXiv:2408.11567, 2024. 29 [167] Liang Zhao, En Yu, Zheng Ge, Jinrong Yang, Haoran Wei, Hongyu Zhou, Jianjian Sun, Yuang Peng, Runpei Dong, Chunrui Han, and Xiangyu Zhang. Chatspot: Bootstrapping multimodal llms via precise referring instruction tuning. In Int. Joint Conf. Artif. Intell. (IJCAI), 2024. 11 [168] Tony Z. Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware. In Robotics: Science and Systems XIX, Daegu, Republic of Korea, July 10-14, 2023, 2023. 11 [169] Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong, and Chuang Gan. 3d-vla: 3d vision-language-action generative world model. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. 11 [170] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-ajudge with mt-bench and chatbot arena. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. 4 [171] Chengliang Zhong, Yuhang Zheng, Yupeng Zheng, Hao Zhao, Li Yi, Xiaodong Mu, Ling Wang, Pengfei Li, Guyue Zhou, Chao Yang, et al. 3d implicit transporter for temporally consistent keypoint discovery. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38693880, 2023. [172] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing visionlanguage understanding with advanced large language models. In Int. Conf. Learn. Represent. (ICLR), 2024. 24 [173] Yuke Zhu, Josiah Wong, Ajay Mandlekar, Roberto Martın-Martın, Abhishek Joshi, Kevin Lin, Abhiram Maddukuri, Soroush Nasiriany, and Yifeng Zhu. robosuite: modular simulation framework and benchmark for robot learning. In arXiv preprint arXiv:2009.12293, 2020. 7 [174] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Quan Vuong, Vincent Vanhoucke, Huong T. Tran, Radu Soricut, Anikait Singh, Jaspiar Singh, Pierre Sermanet, Pannag R. Sanketi, Grecia Salazar, Michael S. Ryoo, Krista Reymann, Kanishka Rao, Karl Pertsch, Igor Mordatch, Henryk Michalewski, Yao Lu, Sergey Levine, Lisa Lee, TsangWei Edward Lee, Isabel Leal, Yuheng Kuang, Dmitry Kalashnikov, Ryan Julian, Nikhil J. Joshi, Alex Irpan, Brian Ichter, Jasmine Hsu, Alexander Herzog, Karol Hausman, Keerthana Gopalakrishnan, Chuyuan Fu, Pete 2 3 3 3 3 4 4 4 4 4 4 5 5 5 5 5 7 7 7 7 7 7 7 7 8 VIIConclusions Appendix Appendix A: Robot Setups"
        },
        {
            "title": "A\nB",
            "content": "Simulation Robot Setups . . . . . . . . . . . . . . . . . Real World Robot Setups . . . . . . Appendix B: Additional Experiments 23 23 23 23"
        },
        {
            "title": "C\nD\nE\nF\nG\nH",
            "content": ". . . . Articulated Objects Manipulation Evaluation . Spatial VQA on EmbSpatial-Bench [33] & 23 . SpatialBot-Bench [10] . . . . . . . . . . Close-Loop Execution Experiment . . . . 23 . Long Horizon Object Manipulation Experiment 24 In the Wild Evaluation of Semantic Orientation 24 25 Cross-View Generalization . . . . . . . . . 25 Failure Case Distribution Analysis . . . . . 26 . . . . . . . . . . Ablation Study . . . . . 26 . . . . . . . . . . Scaling Law . H1 . 26 . . . Cross-Modal Fusion Choices H2 Open Vocabulary Object Detection H3 . Module . . . . . . . . . . . . . . . . . . . . . . . 26 . . . Appendix C: Additional Implementation Details Detail Real World Experiment Results . PointSO Model Details . . . . . . . . . . SoFar-LLaVA Model Details . . . . . . . . . . . . . . . . . . ChatGPT API Costs C . . . . . . . . . . . . 26 26 26 27 27 Appendix D: Additional Benchmark Statistic Analysis 27 27 6-DoF SpatialBench Analysis . Open6DOR V2 Analysis . . . . . . . . . . . . . . . . . . . . Appendix E: Additional Related Works 3D Representation Learning . . . . . . . . . . 27 27 Appendix F: Additional Discussions 29 Relation to Affordance & 6-DoF Pose Estimation 29 29 Comparison to Concurrent Works . . . . Comparison with ReKep [56] . . B1 29 Comparison with Orient Anything [139] 30 B2 30 . . . . . . . . . . . . . . Future Works . . . . . . . . . . C Appendix G: Additional Visualizations Robotic Manipulation . . . . . . . . . . . . 6-DoF SpatialBench . . System Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 30"
        },
        {
            "title": "Contents",
            "content": "I Introduction II Semantic Orientation: Connecting Language and . . . . . . . . at Scale . . . II-C1 Scalability Analysis . . II-C2 Data Source . . II-C3 Data Filtering . . II-C4 Data Annotation . . II-C5 Quality Validation . Object Orientation . . . II-A Definition of Semantic Orientation . II-B Robotic Manipulation via Semantic Orientation II-C OrienText300K: Orientation-Text Paired Data . . . . . . . . . . . . . . . . . . . . . . . . . . II-D PointSO: Cross-Modal 3D Transformer for . . . . . . . . . . . . . . . . Semantic Orientation Prediction . . II-D1 3D and Language Embeddings . II-D2 Cross-Modal Fusion . . . II-D3 Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . III SOFAR: Semantic Orientation Bridges Spatial Reasoning and Object Manipulation III-A Orientation-Aware Scene Graph from RGB-D III-A1 Task-Oriented Object Segmentation . . III-A2 Orientation-Aware 3D Scene Graph . . . . . . III-B1 Chain-of-Thought Spatial Reasoning . . . . . III-B2 Low-Level Motion Execution . III-B Spatial-Aware Task Reasoning . . . . . IV Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ulation . Simulation . IV-A Benchmarks . . . IV-A1 Open6DOR V2 . . IV-A2 6-DoF SpatialBench . . . . . . . . . . . . . IV-B 6-DoF object rearrangement evaluation in Sim- . . . . . . IV-C Zero-shot Object Manipulation Evaluation in . . . . . . . . . . . . . . . . . . . . . . 9 9 9 10 10 . IV-E Visual Question Answer on 6-DoF SpatialBench 10 10 IV-F Semantic Orientation Prediction . 10 . IV-G Additional Applications 10 . . IV-D Zero-shot Real-world Manipulation . . . . . . . . . . . . IV-G1 Cross Embodiedment Generalization . IV-G2 Long Horizon Planning & Close-Loop . . . . . IV-G3 Orientation-Aware Robotic Navigation . IV-D1 Hardware Setup . IV-D2 Tasks and Evaluations . . . IV-D3 Results Execution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Related Works V-A Vision-Language Models for Spatial Under- . . . . . V-B Language-Grounded Robot Manipulation . . . standing . . . . . . . . . . . . . . . . VI Limitations 10 10 11 11 11"
        },
        {
            "title": "APPENDIX A\nROBOT SETUPS",
            "content": "A. Simulation Robot Setups To ensure fairness, we utilize the same Franka Panda arm for evaluations in both the LIBERO [76] and our Open6DOR V2 benchmarks. For SIMPLER [74], we use the Google Robot exclusively to conduct the baseline experiments, adhering to all configurations outlined in SIMPLER, as presented in Table II. B. Real World Robot Setups As for manipulation tasks, in Fig. 9, we perform 6-DoF rearrangement tasks using the Franka Panda equipped with gripper and the UR robot arm with LeapHand, while articulated object manipulation is conducted using the Flexiv arm equipped with suction tool. All the robot arms mount Realsense D415 camera to its end for image capturing. Fig. 11: Navigation robot setup. We use Unitree GO2 as our embodiment, and we mount RealSense D455, portable Wi-Fi and LiDAR-L1. Note that, our model only takes RGB frames as input. The portable Wi-Fi is used for communication with the remote server and the Lidar is used for the local controller API of Unitree Dog. Fig. 9: The robots used in our real-world experiments. In Fig. 10, we present the workspace and robotic arm for real-world 6-DoF rearrangement. Unlike Rekep [56], CoPa [51] et al., we utilize only single RealSense D415 camera. This setup significantly reduces the additional overhead associated with environmental setup and multi-camera calibration, and it is more readily reproducible. Fig. 10: 6-DoF rearrangement robot setup. As for navigation tasks, we provide visualization of our robotic dog in Fig. 11. Following Uni-Navid [162], our robotic dog is Unitree GO2 and we mount RealSense D455 camera on the head of the robotic dog. Here, we only use the RGB frames with resolution of 640 480 in the setting of 90 HFOV. We also mount portable Wi-Fi at the back of the robot dog, which is used to communicate with the remote server (send captured images and receive commands). Unitree GO2 is integrated with LiDAR-L1, which is only used for local motion planning. APPENDIX ADDITIONAL EXPERIMENTS A. Articulated Objects Manipulation Evaluation We further integrate SOFAR with articulated object manipulation, as illustrated in Table VII, and evaluate its practicality in robotic manipulation tasks using the PartNet-Mobility Dataset within the SAPIEN [145] simulator. Our experimental setup follows ManipLLM [71], employing the same evaluation metrics. Specifically, we directly utilize the segmentation centers provided by SAM as contact points, leverage PointSO to generate contact directions, and use VLM to determine subsequent motion directions. The results demonstrate significant improvements over the baseline. Notably, our model achieves this performance without dividing the data into training and testing sets, operating instead in fully zero-shot across most tasks. This underscores the robustness and generalization of our approach. B. Spatial VQA on EmbSpatial-Bench [33] & SpatialBotBench [10] To further demonstrate SOFARs spatial reasoning capabilities, we conducted Spatial VQA tests within the EmbSpatialBench [33] and SpatialBot-Bench [10]. As shown in Tables VIII and IX, SOFAR significantly outperformed all baselines, achieving more than 20% performance improvement in EmbSpatialBench. C. Close-Loop Execution Experiment We demonstrate the closed-loop replan capabilities of SOFAR within Simpler-Env [74] in Fig. 12. The instruction TABLE VII: Zeroshot articulate object manipulation evaluation within the SAPIEN [145] simulator using PartNet-Mobility Dataset. Notably, while the baseline methods use distinct training and testing splits, our model achieves robust performance without fine-tuning on the SAPIEN samples. Method Where2Act [93] UMPNet [150] FlowBot3D [35] Implicit3D [171] ManipLLM [71] 0.26 0.46 0.67 0.53 0.68 0.36 0.43 0.55 0.58 0.64 0.19 0.15 0.20 0.35 0.36 0.27 0.28 0.32 0.55 0. SOFAR 0.75 0.88 0.43 0.85 Method Where2Act [93] UMPNet [150] FlowBot3D [35] Implicit3D [171] ManipLLM [71] 0.13 0.22 0.17 0.27 0.41 0.18 0.33 0.53 0.65 0.75 0.13 0.26 0.29 0.20 0.44 0.40 0.64 0.42 0.33 0.67 SOFAR 0.35 0.68 0.62 0.73 0.23 0.54 0.27 0.28 0.43 0. AVG 0.26 0.35 0.37 0.46 0.59 0.64 0.11 0.32 0.31 0.66 0.62 0.54 0.18 0.42 0.23 0.45 0. 0.15 0.28 0.61 0.58 0.65 0.75 0.35 0.20 0.10 0.17 0.22 0.47 0.56 0.68 0.51 0.61 0.49 0.38 0.35 0.60 0.80 0. 0.68 0.45 0.90 0.14 0.44 0.15 0.52 0.65 0.24 0.40 0.28 0.57 0.52 0.13 0.10 0.36 0.45 0. 0.58 0.72 0.69 0.28 0.42 0.39 0.53 0.86 0.77 0.05 0.29 0.27 0.15 0. 0.55 0.21 0.20 0.42 0.69 0.85 0.79 0.12 0.23 0.18 0.34 0.40 0.42 0.17 0.26 0.28 0.41 0. 0.48 0.56 0.18 0.21 0.41 0.64 0.68 0.54 0.70 0.54 0.71 0.07 0.20 0.18 0.39 0.60 0.40 0.42 0.26 0.43 0.64 0. 0.81 0.58 0.63 0.20 0.28 0.51 0.31 0.83 0.80 0.15 0.25 0.13 0.30 0. 0.15 0.15 0.23 0.31 0.38 0.56 0.44 AVG 0.21 0.28 0.32 0.41 0.54 0. TABLE VIII: Zero-shot performance of LVLMs in EmbSpatialBench [33]. Bold indicates the best results."
        },
        {
            "title": "Likelihood",
            "content": "BLIP-2 [69] InstructBLIP [20] MiniGPT4 [172] LLaVA-1.6 [79] GPT-4V [98] Qwen-VL-Max [3] SOFAR 37.99 38.85 23.54 35.19 36.07 49.11 70.88 35.71 33.41 31.70 38.84 - - - D. Long Horizon Object Manipulation Experiment Fig. 13 illustrates the execution performance of our model on long-horizon tasks. Through the VLM [100, 125], complex instructions such as making breakfast and cleaning up the desktop can be decomposed into sub-tasks. In the second example, we deliberately chose complex and uncommon objects as assets, such as Aladdins lamp and puppets, but SOFAR is able to successfully complete all tasks. TABLE IX: Zero-shot performance of LVLMs in SpatialBotBench SpatialBot-Phi2-3B-RGB, SpatialBot-8B: SpatialBot-Llama3-8B-RGB. SpatialBot-3B: [10]. Model Pos Exist Count Reach Size Avg ChatGPT-4o [100] 70.6 85.0 SpatialBot-3B [10] 64.7 80.0 SpatialBot-8B [10] 55.9 80.0 84.5 88.0 91.2 51.7 61.7 40.0 43.3 67.0 28.3 64.5 20.0 57.4 SOFAR 76.5 87.5 80.0 57.5 40.0 68.3 for both tasks is pick the coke can In Fig. 12 (a), the model initially misidentified the coke can as Fanta can. After correction by the VLM, the model re-identified and located the correct object. In Fig. 12 (b), the model accidentally knocks over the Coke can during motion due to erroneous motion planning. Subsequently, the model re-plans and successfully achieves the grasp. Fig. 12: Close-loop execution of our SOFAR. E. In the Wild Evaluation of Semantic Orientation We provide qualitative demonstration of the accuracy of PointSO under in-the-wild conditions, as shown in Fig. 14, where the predicted Semantic Orientation is marked in the images. We obtained single-sided point clouds by segmenting objects using Florence-2 [147] and SAM [65] and fed them into PointSO. It can be observed that our model achieves good performance across different views, objects, and instructions, which proves the effectiveness and generalization of PointSO. Fig. 13: Long-horizon object manipulation experiment of our SOFAR. Fig. 14: In the wild evaluation of PointSO. F. Cross-View Generalization SOFAR gets point clouds in the world coordinate system using an RGB-D camera to obtain grasping poses, and it is not limited to fixed camera perspective. In addition, PointSO generates partial point clouds from different perspectives through random camera views to serve as data augmentation for training data, which also generalizes to camera perspectives in the real world. Fig. 15 illustrates SOFARs generalization capability for 6-DoF object manipulation across different camera poses. It can be observed that whether its front view, side view, or ego view, SOFAR can successfully execute the upright the bottle instruction. Fig. 15: Cross view generalization of our SOFAR. G. Failure Case Distribution Analysis Based on the failure cases from real-world experiments, we conducted quantitative analysis of the failure case distribution for SOFAR, with the results shown in Fig. 16. It can be observed that 31% of the failures originated from grasping issues, including objects being too small, inability to generate reasonable grasping poses, and instability after grasping leading to sliding or dropping. Next, 23% were due to incorrect or inaccurate Semantic Orientation prediction. For tasks such as upright or upside - down, highly precise angle estimation (5) is required for smooth execution. Object analysis and detection accounted for approximately 20% of the errors. The instability of open-vocabulary detection modules like Florence2 [147] and Grounding DINO [81] often led to incorrect detection of out-ofdistribution objects or object parts. In addition, since our Motion TABLE X: Data scaling property of semantic orientation with different training data scales evaluated on OrienText300K test split. All experiments are conducted with PointSO-Base. Data Scale 45 30 15 5 Average 5% 10% 50% 100% 57.03 61.72 76.56 79. 46.09 53.13 72.66 77.34 39.84 43.75 66.41 70.31 27.34 30.47 56.25 62.50 42.58 47.27 67.97 72.46 TABLE XI: Ablation of multi-modal fusion in PointSO. All the experiments are under the PointSO-Base variant. Data Scale 45 30 15 5 Average Cross-attention Multiplication Addition Concat 74.22 74.22 79.69 66.41 70.31 69.53 77.34 60.94 63.28 60.16 70.31 52.34 57.03 56.25 62.50 43.75 66.21 65.04 72.46 55.86 Planning did not take into account the working space range of the robotic arm and potential collisions of the manipulated object, occasional deadlocks and collisions occurred during motion. Finally, there were issues with the Task Planning of the VLM [100, 125]. For some complex Orientations, the VLM occasionally failed to infer the required angles and directions to complete the task. Employing more powerful, thoughtenabled VLM [58] might alleviate such errors. Fig. 16: Failure case distribution analysis of our SOFAR. H. Ablation Study 1) Scaling Law: The scaling capability of models and data is one of the most critical attributes today and core feature of foundation models [6]. We investigate the performance of PointSO across different data scales, as illustrated in Table X. We obtain the subset for OrienText300K from ObjaverseLVIS, which consists of approximately 46,000 3D objects with category annotations. The selection was based on the seven criteria mentioned in the main text. Objects meeting all seven criteria formed the strict subset, comprising around 15k objects. When including objects without textures and those of lower quality, the total increases to approximately 26k objects. It can be seen that the increase in data volume is the most significant factor driving the performance improvement of PointSO. It can be anticipated that with further data expansion, such as Objaverse-XL [21], PointSO will achieve better performance. 2) Cross-Modal Fusion Choices: We further conduct an ablation study on the multi-modal fusion methods in PointSO, testing commonly used feature fusion techniques such as crossattention, multiplication, addition, and concatenation, as shown in Table XI. The results indicate that simple addition achieves the best performance. This may be attributed to the fact that instructions in the semantic domain are typically composed of short phrases or sentences, and the text CLS token already encodes sufficiently high-level semantic information. 3) Open Vocabulary Object Detection Module: SOFAR utilize detection foundation model to localize the interacted objects or parts, then generate masks with SAM [65]. Although not the SOTA performance on the COCO benchmark, Florence2 [147] exhibits remarkable generalization in in-the-wild detection tasks, even in simulator scenarios. Table XII illustrates the performance of various detection modules in Open6DOR [28] Perception, where Florence-2 achieves the best results and outperforms Grounding DINO [81] and YOLO-World [16]. APPENDIX ADDITIONAL IMPLEMENTATION DETAILS A. Detail Real World Experiment Results To fully demonstrate the generalization of SOFAR rather than cherry-picking, we carefully design 60 different realworld experimental tasks, covering more than 100 different and diverse objects. Similar to the Open6DOR [28] benchmark in the simulator, we divide these 60 tasks into three parts: position-track, orientation-track, and the most challenging comprehensive & 6-DoF-track. Each track is further divided into simple and hard levels. The position-simple track includes tasks related to front & back & left & right spatial relationships, while the position-hard track includes tasks related to between, center, and customized. The orientation-simple track includes tasks related to the orientation of object parts, and the orientation-hard track includes tasks related to whether the object is upright or flipped (with very strict requirements for angles in both upright and flipped cases). Comprehensive tasks involve complex instruction understanding and long-horizon tasks; 6-DoF tasks simultaneously include requirements for both object position and orientation instructions. In Table XIII, we present the complete task instructions, as well as the performance metrics of SOFAR and the baseline. Due to the large number of tasks, we performed each task three times. It can be seen that SOFAR achieves the best performance in all tracks, especially in the orientation-track and comprehensive & 6-DoF-track. We also show all the objects used in the real-world experiments in Fig. 17, covering wide range of commonly and uncommonly used objects in daily life. B. PointSO Model Details For PointSO, we utilize FPS + KNN to perform patchify and employ small PointNet [104] as the patch encoder. Subsequently, standard Transformer encoder is adopted as the backbone, followed by single linear layer to map the output to three-dimensional vector space. All parameter TABLE XII: Ablation study of open vocabulary detection modules on Open6DOR [28] perception tasks. Method Position Track Rotation Track 6-DoF Track Level 0 Level 1 Overall Level 0 Level 1 Level 2 Overall Position Rotation Overall Time Cost (s) YOLO-World [16] Grounding DINO [81] Florence-2 [147] 59.0 92.2 96. 37.7 71.5 81.5 53.3 86.7 93.0 48.3 64.7 68.6 36.1 41.1 42.2 62.0 69.8 70.1 44.9 55.5 57. 53.4 87.2 92.7 44.6 51.6 52.7 27.8 44.6 48.7 7.4s 9.2s 8.5s GPTs filtering and comprehension capabilities. To generate semantic direction annotations, we filter the 800K dataset of Objaverse [22] and apply ChatGPT to approximately 350K of the filtered data to generate semantic text-view index pairs. The OpenAI official API was used for these calls, with the GPT-4o version set to 2024-08-06 and the output format configured as JSON. The total cost for debugging and execution amounted to approximately $10K. APPENDIX ADDITIONAL BENCHMARK STATISTIC ANALYSIS A. 6-DoF SpatialBench Analysis We conduct statistical analysis of the manually constructed 6-DoF SpatialBench, with category comparisons and word cloud visualizations shown in Fig. 19. We collect diverse image data from the internet, encompassing scenes such as indoor, outdoor, and natural landscapes. The questions may involve one or multiple objects, with varying levels of uncertainty in image resolution. Most importantly, we are the first to propose VQA benchmark for orientation understanding, focusing on both quantitative and qualitative evaluation of orientation. B. Open6DOR V2 Analysis Open6DOR V2 builds upon Open6DOR V1 by removing some incorrectly labeled data and integrating assets and metrics into Libero, enabling closed-loop policy evaluation. The detailed number of tasks is presented in Table XVI, comprising over 4,500 tasks in total. Notably, we remove level 2 of the position track in Open6DOR V1 [28] because it requires manual inspection, which is not conducive to open-source use and replication by the community. Besides, due to the randomness of object drops in the scene, approximately 8% of the samples already satisfy the evaluation metrics in their initial state. APPENDIX ADDITIONAL RELATED WORKS A. 3D Representation Learning Research on 3D Representation Learning encompasses various methods, including point-based [104, 105], voxelbased [91], and multiview-based approaches [46, 120]. Pointbased methods [36, 111] have gained prominence in object classification [133, 144] due to their sparsity yet geometryinformative representation. On the other hand, voxel-based methods [24, 108, 156] offer dense representation and translation invariance, leading to remarkable performance in object detection [19] and segmentation [2, 155]. The evolution of Fig. 17: The real-world assets used in our real-world experiments. More than 100 diverse objects are used in our 6-DoF rearrangement experiments. configurations follow prior work on point cloud representation learning [29, 107, 109]. Detailed hyperparameter and model configurations are provided in Tables XIV and XV. C. SoFar-LLaVA Model Details In addition to leveraging the extensive knowledge and strong generalization capabilities of closed-source/open-source pretrained VLMs [3, 97, 125] for zero-shot or in-context learning, SOFAR can also enhance the planning performance of open-source models through visual instruction tuning for rapid fine-tuning. The pipeline of the model is illustrated in Fig. 18. JSON-formatted 6-DoF scene graph, processed through text tokenizer, along with the image refined by SoM [151], is fed into an LLM (e.g., LLaMA [130, 131]) for supervised fine-tuning [79]. In the Open6DOR [28] task, we supplement the training dataset with additional samples retrieved and manually annotated from Objaverse [22], ensuring alignment with the object categories in the original benchmark. This dataset includes approximately 3,000 6-DoF object manipulation instructions. Using this data, we construct dialogue-style training data based on ChatGPT and train the SOFAR-LLaVA model. The training hyperparameters are detailed in Table XV. Similarly, we finetune PointSO on this training dataset and achieve superior performance on the Open6DOR task. D. ChatGPT API Costs The knowledge of OrienText300K is derived from the annotations of 3D modelers on Sketchfab, combined with ChatTABLE XIII: Detailed zero-shot real-world 6-DoF rearrangement results. Task CoPa [51] ReKep-Auto [56] SOFAR-LLaVA (Ours) SOFAR (Ours) Positional Object Manipulation Move the soccer ball to the right of the bread. Place the doll to the right of the lemon. Put the pliers on the right side of the soccer ball. Move the pen to the right of the doll. Place the carrot on the left of the croissant. Move the avocado to the left of the baseball. Pick the pepper and place it to the left of the charger. Place the baseball on the left side of the mug. Arrange the flower in front of the potato. Put the volleyball in front of the knife. Place the ice cream cone in front of the potato. Move the bitter melon to the front of the forklift. Place the orange at the back of the stapler. Move the panda toy to the back of the shampoo bottle. pick the pumpkin and place it behind the pomegranate. Place the basketball at the back of the board wipe. Put the apple inside the box. Place the waffles on the center of the plate. Move the hamburger into the bowl. Pick the puppet and put it into the basket. Drop the grape into the box. Put the doll between the lemon and the USB. Set the duck toy in the center of the cart, bowl, and camera. Place the strawberry between the Coke bottle and the glue. Put the pen behind the basketball and in front of the vase. Total success rate 2/3 3/3 1/3 3/3 2/3 3/3 1/3 3/3 2/3 3/3 2/3 2/3 3/3 2/3 3/3 2/3 3/3 3/3 2/3 1/3 2/3 2/3 2/3 2/3 2/3 74.7% 3/3 3/3 1/3 2/3 3/3 2/3 2/3 2/3 3/3 3/3 3/3 1/3 2/3 3/3 2/3 2/3 2/3 2/3 2/3 2/3 3/3 2/3 1/3 2/3 1/3 72.0% Orientational Object Manipulation Turn the yellow head of the toy car to the right. Adjust the knife handle so it points to the right. Rotate the cap of the bottle towards the right. Rotate the tip of the screwdriver to face the right. Rotate the stem of the apple to the right. Turn the front of the toy car to the left. Rotate the cap of the bottle towards the left. Adjust the pears stem to the right. Turn the mug handle to the right. Rotate the handle of the mug to towards right. Rotate the box so the text side faces forward. Adjust the USB port to point forward. Set the bottle upright. Place the coffee cup in an upright position. Upright the statue of liberty Stand the doll upright. Right the Coke can. Flip the bottle upside down. Turn the coffee cup upside down. Invert the shampoo bottle upside down. Total success rate 2/3 2/3 2/3 0/3 0/3 0/3 2/3 1/3 1/3 2/3 0/3 0/3 0/3 1/3 0/3 0/3 0/3 0/3 0/3 0/3 21.7% 2/3 1/3 2/3 0/3 1/3 0/3 1/3 1/3 1/3 1/3 1/3 0/3 1/3 1/3 0/3 1/3 0/3 0/3 0/3 0/3 23.3% Comprehensive 6-DoF Object Manipulation Pull out tissue. Place the right bottle into the box and arrange it in 33 pattern. Take the tallest box and position it on the right side. Grasp the error bottle and put it on the right side. Take out the green test tube and place it between the two bottles. Pack the objects on the table into the box one by one. Rotate the loopy doll to face the yellow dragon doll Right the fallen wine glass and arrange it neatly in row. Grasp the handle of the knife and cut the bread. Pick the baseball into the cart and turn the cart to facing right. Place the mug on the left of the ball and the handle turn right. Aim the camera at the toy truck. Rotate the flashlight to illuminate the loopy. Put the pen into the pen container. Pour out chips from the chips cylinder to the plate. Total success rate 3/3 0/3 1/3 1/3 2/3 1/3 0/3 0/3 0/3 0/3 0/3 1/3 0/3 0/3 0/3 20.0% 3/3 0/3 1/3 2/3 2/3 1/3 1/3 0/3 0/3 0/3 0/3 0/3 0/3 1/3 1/3 26.7% 3/3 3/3 3/3 3/3 2/3 2/3 2/3 2/3 2/3 3/3 2/3 2/3 3/3 3/3 1/3 3/3 3/3 3/3 2/3 2/3 3/3 2/3 2/3 3/3 2/3 81.3% 1/3 2/3 2/3 1/3 1/3 2/3 1/3 1/3 2/3 2/3 0/3 1/3 0/3 2/3 1/3 0/3 1/3 0/3 1/3 0/3 35.0% 2/3 0/3 3/3 1/3 3/3 0/3 1/3 0/3 0/3 1/3 1/3 1/3 1/3 0/3 1/3 33.3% 3/3 3/3 2/3 3/3 2/3 3/3 2/3 3/3 3/3 3/3 3/3 2/3 3/3 2/3 2/3 2/3 3/3 3/3 3/3 2/3 2/3 3/3 2/3 3/3 2/3 85.3% 2/3 2/3 2/3 1/3 2/3 2/3 2/3 1/3 2/3 1/3 1/3 1/3 1/3 2/3 0/3 1/3 1/3 1/3 1/3 0/3 43.3% 3/3 1/3 3/3 2/3 3/3 1/3 1/3 0/3 1/3 2/3 1/3 1/3 1/3 1/3 1/3 48.9% Fig. 18: Pipeline of SOFAR-LLaVA, fine-tuned vision language model based on visual instruction tuning. TABLE XIV: Details of PointSO model variants. This table format follows Dosovitskiy et al. [31]. Model CLIP Layers Hidden size MLP size Heads #Params Small ViT-B/32 Base ViT-B/32 Large ViT-B/32 12 12 12 256 384 512 1024 1536 2048 4 6 8 11.4M 19.0M 43.6M attention mechanisms [134, 164] has also contributed to the development of effective representations for downstream tasks, as exemplified by the emergence of 3D Transformers [36, 86, 89]. Notably, 3D self-supervised representation learning has garnered significant attention in recent studies. PointContrast [148] utilizes contrastive learning across different views to acquire discriminative 3D scene representations. Innovations such as Point-BERT [159] and Point-MAE [101] introduce masked modeling [25, 47] pretraining into the 3D domain. ACT [29] pioneers cross-modal geometry understanding through 2D or language foundation models such as CLIP [115] or BERT [25]. Following ACT, RECON [107] further proposes learning paradigm that unifies generative and contrastive learning. PPT [166] highlights the significance of positional encoding in 3D representation learning Additionally, leveraging foundation vision-language models like CLIP [29, 115] has spurred the exploration of new direction in open-world 3D representation learning. This line of work seeks to extend the applicability and adaptability of 3D representations in diverse and openworld/vocabulary scenarios [26, 27, 37, 87, 102, 163]. APPENDIX ADDITIONAL DISCUSSIONS A. Relation to Affordance & 6-DoF Pose Estimation Conceptually, this semantic orientation is counterpart of affordance [43, 44, 106, 118] but beyond, as SO and affordance all present potential actions and interactions with objects. However, SO also contains the spatial understanding of intra-object part-level attributes more than affordance learning. Compared to vanilla 6-DoF pose estimation, our proposed SO Fig. 19: 6-DoF SpatialBench statistics. (a) Statistical analysis of the task type, question type, and object relation. (b) Word cloud visualization. combined with the 3-DoF translation understanding has the same DoF completeness. The difference is, our proposed SO is grounded by languages, making it useful for open-world manipulation requiring complicated spatial reasoning [28, 57, 128]. In addition, our Semantic Orientation can be auto-labeled from Internet 3D data that achieves higher scalability, introduced in the next section. B. Comparison to Concurrent Works 1) Comparison with ReKep [56]: Recently, ReKep has succeeded in executing complex robotic tasks, such as longhorizon manipulation, based on the relationships and constraints between spatial key points. Its structural design offers many insights that SOFAR can draw upon, yet it also presents several issues: (1) Overly customized prompt engineering. ReKep requires manually designed complex system prompts for each task during inference. While this approach may be described as no training, it cannot be considered true zero-shot transfer. In contrast, SOFAR achieves genuine zero-shot transfer by eliminating the need for any human involvement during inference; (2) Using constraints based solely on key points fails to capture the full 6-DoF pose integrity of objects. For example, in the pouring water task, merely bringing the spout of the kettle close to the cup may lead to incorrect solutions, such as the kettle overturning; (3) ReKep requires all key points to be present in the first frame, and each step of the processfrom TABLE XV: Training recipes for PointSO and SOFAR-LLaVA. PointSO SOFAR-LLaVA Config optimizer learning rate weight decay learning rate scheduler training epochs warmup epochs batch size drop path rate number of points number of point patches point patch size augmentation GPU device Small AdamW 5e-5 5e-2 cosine 300 10 256 0. 10000 512 32 Base AdamW 5e-5 5e-2 cosine 300 10 256 0.2 10000 512 32 Large AdamW 2e-5 5e-2 cosine 300 10 256 0. 10000 512 32 Finetune AdamW 5e-5 5e-2 cosine 50 5 256 0.2 10000 512 32 Rot&Part&Noise Rot&Part&Noise Rot&Part&Noise Rotation SFT AdamW 2e-5 0 cosine 2 0.03 128 - - - - - 8H800 8H800 8H800 8H800 8H800 Fig. 20: An example of SOFAR how to finish move near task in SIMPLER [74]. mask extraction to feature dimensionality reduction, clustering, and filteringintroduces additional hyperparameters. 2) Comparison with Orient Anything [139]: Recently, Orient Anything also highlighted the importance of orientation in spatial perception and adopted training data construction approach similar to Our PointSO. Our primary distinction lies in semantic orientation, which is language-conditioned orientation. In contrast, Orient Anything is limited to learning basic directions such as front and top. By aligning with textual information, semantic orientation better enhances spatial perception, understanding, and robotic manipulation. C. Future Works Future work includes further expanding the OrienText300K with larger datasets like Objaverse-XL [21], enhancing the performance of semantic orientation through self-supervised learning and pretraining methods [29, 47, 107, 115], and demonstrating its effectiveness in broader range of robotic scenarios, such as navigation [11], mobile manipulation [154], lifelong learning [76], spatio-temporal reasoning [56, 84, 85, 152], humanoid [14, 17, 48, 49], and human-robot interaction [82, 83]. APPENDIX ADDITIONAL VISUALIZATIONS A. Robotic Manipulation As shown in Fig. 20, we present visualization of executing task named move near. According to the input image and task instruction - move blue plastic bottle near pepsi can, SOFAR can predict the center coordinate of the target object (bottle) and relative target (pepsi can), and it would infer the place coordinate and produce series of grasp pose. B. 6-DoF SpatialBench To further evaluate 6-DoF spatial understanding, we construct 6-DoF SpatialBench. We present examples of question-answer pairs from the 6-DoF SpatialBench, with quantitative and qualitative questions shown in Figs. 21 and 22, respectively. The benchmark we constructed is both challenging and practical, potentially involving calculations based on the laws of motion, such as Assuming moving speed of 0.5 m/s, how many seconds would it take to walk from here to the white flower? Moreover, it covers wide range of spatially relevant scenarios across both indoor and outdoor environments. TABLE XVI: Statistics of Open6DOR V2 Benchmark. The entire benchmark comprises three independent tracks, each featuring diverse tasks with careful annotations. The tasks are divided into different levels based on instruction categories, with statistics demonstrated above. Track Level Position-track Rotation-track 6-DoF-track Totel Level 0 Level 1 Level Level 1 Level 2 - - 1810 - - 4535 4535 Task Catog. Left Right Top Behind Front Between Center Geometric Directional Semantic Task Stat. 296 266 209 297 193 159 318 Benchmark Stat. 1698 1027 134 C. System Prompts Prompt engineering significantly enhances ChatGPTs capabilities. The models understanding and reasoning abilities can be greatly improved by leveraging techniques such as Chain-of-Thought [140] and In-Context Learning [9]. Figs. 23 and 24 illustrate the system prompt we used in constructing OrienText300K. Fig. 25, Fig. 26, and Fig. 27 illustrate the system prompt we used when evaluating SOFAR on Open6DOR (simulation), object manipulation (both simulation and real worlds), and VQA, respectively. Note that different from previous methods [54, 56], SOFAR does not require complicated in-context examples. Fig. 21: Visualization example of 6-DoF SpatialBench data samples. Fig. 22: Visualization example of 6-DoF SpatialBench data samples. Fig. 23: The system prompt of ChatGPT-4o used for filtering Objaverse data. Fig. 24: The system prompt of ChatGPT-4o used for generating Semantic Direction-Index pairs. Fig. 25: The system prompt of Open6DOR tasks. Fig. 26: The system prompt of general manipulation tasks. Fig. 27: The system prompt of visual-question-answering tasks."
        }
    ],
    "affiliations": [
        "Eastern Institute of Technology",
        "Galbot",
        "Peking University",
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "Shanghai Qi Zhi Institute",
        "ShanghaiTech University",
        "Tsinghua University",
        "UIUC"
    ]
}