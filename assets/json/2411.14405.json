{
    "paper_title": "Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions",
    "authors": [
        "Yu Zhao",
        "Huifeng Yin",
        "Bo Zeng",
        "Hao Wang",
        "Tianqi Shi",
        "Chenyang Lyu",
        "Longyue Wang",
        "Weihua Luo",
        "Kaifu Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Currently OpenAI o1 has sparked a surge of interest in the study of large reasoning models (LRM). Building on this momentum, Marco-o1 not only focuses on disciplines with standard answers, such as mathematics, physics, and coding -- which are well-suited for reinforcement learning (RL) -- but also places greater emphasis on open-ended resolutions. We aim to address the question: \"Can the o1 model effectively generalize to broader domains where clear standards are absent and rewards are challenging to quantify?\" Marco-o1 is powered by Chain-of-Thought (CoT) fine-tuning, Monte Carlo Tree Search (MCTS), reflection mechanisms, and innovative reasoning strategies -- optimized for complex real-world problem-solving tasks."
        },
        {
            "title": "Start",
            "content": "2024-11-22 Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions Yu Zhao*, Huifeng Yin*, Bo Zeng , Hao Wang , Tianqi Shi , Chenyang Lyu , Longyue Wang , Weihua Luo and Kaifu Zhang MarcoPolo Team, Alibaba International Digital Commerce Currently OpenAI o1 has sparked surge of interest in the study of large reasoning models (LRM). Building on this momentum, Marco-o1 not only focuses on disciplines with standard answers, such as mathematics, physics, and codingwhich are well-suited for reinforcement learning (RL)but also places greater emphasis on open-ended resolutions. We aim to address the question: Can the o1 model effectively generalize to broader domains where clear standards are absent and rewards are challenging to quantify? Marco-o1 is powered by Chain-of-Thought (CoT) fine-tuning, Monte Carlo Tree Search (MCTS), reflection mechanisms, and innovative reasoning strategiesoptimized for complex real-world problem-solving tasks. Figure 1 classic strawberry question reasoned by our Marco-o1 model: How many rs are in strawberry. 1. Introduction OpenAI recently introduced the groundbreaking o1 model [OpenAI, 2024, Zhong et al., 2024], renowned for its exceptional reasoning capabilities. This model has demonstrated outstanding performance on platforms such as AIME and CodeForces, surpassing other leading models. Inspired by this success, we aimed to push the boundaries of LLMs even further, enhancing their reasoning abilities to tackle complex, real-world challenges. Equal Contribution. Email: wanglongyue.wly@alibaba-inc.com 2024 Alibaba. All rights reserved 4 2 0 2 1 2 ] . [ 1 5 0 4 4 1 . 1 1 4 2 : r Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions Marco-o1 leverages advanced techniques like CoT fine-tuning [Wei et al., 2022], MCTS [Wei et al., 2022, Feng et al., 2023, Silver et al., 2017], and Reasoning Action Strategies to enhance its reasoning power. As shown in Figure 2, by fine-tuning Qwen2-7B-Instruct [Yang et al., 2024] with combination of the filtered Open-O1 CoT dataset [Team, 2024], Marco-o1 CoT dataset, and Marco-o1 Instruction dataset, Marco-o1 improved its handling of complex tasks. MCTS allows exploration of multiple reasoning paths using confidence scores derived from softmax-applied log probabilities of the top-𝑘 alternative tokens, guiding the model to optimal solutions. Moreover, our reasoning action strategy involves varying the granularity of actions within steps and mini-steps to optimize search efficiency and accuracy. Marco-o1 achieved accuracy improvements of +6.17% on the MGSM (English) dataset and +5.60% on the MGSM (Chinese) dataset, showcasing enhanced reasoning capabilities [Shi et al., 2022]. Additionally, in translation tasks, we demonstrate that Marco-o1 excels in translating slang expressions. For example, the model correctly translates colloquial expression in Chinese that literally means This shoe offers stepping-on-poop sensation to English This shoe has comfortable sole, demonstrating its superior grasp of colloquial nuances. Our work is distinguished by the following contributions: Fine-Tuning with CoT Data: We develop Marco-o1-CoT by performing full-parameter finetuning on the base model using open-source CoT datasets combined with our self-developed synthetic data. Solution Space Expansion via MCTS: We integrate LLMs with MCTS (Marco-o1-MCTS), using the models output confidence to guide the search and expand the solution space. Reasoning Action Strategy: We implement novel reasoning action strategies and reflection mechanism (Marco-o1-MCTS mini-step), including exploring different action granularities within the MCTS framework and prompting the model to self-reflect, thereby significantly enhancing the models ability to solve complex problems. Application in Translation Tasks: We are the first to apply Large Reasoning Models (LRM) to Machine Translation tasks, exploring inference-time scaling laws in the multilingual and translation domain. 2. Marco Reasoning Datasets To enhance the reasoning capabilities of the Marco-o1 model, we employed Supervised Fine-Tuning (SFT) strategy using variety of datasets. Open-O1 CoT Dataset (Filtered) [Team, 2024]: We refined the Open-O1 projects CoT Dataset by applying heuristic and quality filtering processes. This enhancement allowed the model to adopt structured reasoning patterns effectively. Marco-o1 CoT Dataset (Synthetic): We generated the Marco-o1 CoT Dataset using MCTS, which helped to formulate complex reasoning pathways, further bolstering the models reasoning capabilities. Marco Instruction Dataset: Recognizing the critical role of robust instruction-following capabilities in executing complex tasks, we incorporated set of instruction-following data. This integration ensures the model remains competent across wide range of tasks, maintaining its general effectiveness while significantly boosting its reasoning flair. Download: Marco Reasoning Dataset (Our Partial Dataset). 2 Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions Dataset Number of Samples Open-O1 CoT Dataset (Filtered) [Team, 2024] Marco-o1 CoT Dataset (Synthetic) Marco Instruction Dataset Total 45,125 10,000 5,141 60,266 Table 1 Overview of Marco Reasoning Datasets 3. Solution Space Expansion via MCTS Figure 2 The overview of Marco-o1. We integrated LLMs with MCTS to enhance the reasoning capabilities of our Marco-o1 model: Nodes as Reasoning States: In the MCTS framework, each node represents reasoning state of the problem-solving process. Actions as LLM Outputs: The possible actions from node are the outputs generated by the LLM. These outputs represent potential steps or mini-steps in the reasoning chain. Rollout and Reward Calculation: During the rollout phase, the LLM continues the reasoning process to terminal state. Guiding MCTS: This reward score 𝑅 is used to evaluate and select promising paths within the MCTS, effectively guiding the search towards more confident and reliable reasoning chains. Furthermore, we obtain the value of each state by computing confidence score. For each token 𝑡𝑖 generated during the rollout, we calculate its confidence score by applying the softmax function to 3 Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions its log probability and the log probabilities of the top 5 alternative tokens. This is given by: 𝑐𝑖 = exp( 𝑝(𝑡𝑖)) 𝑘=1 exp( 𝑝(𝑡𝑘)) (cid:205)5 where 𝑐𝑖 is the confidence score for the 𝑖𝑡ℎ token in the rollout. 𝑝(𝑡𝑖) is the log probability of the 𝑖𝑡ℎ token generated by the LLM. 𝑝(𝑡𝑘) for 𝑘 = 1 to 5 are the log probabilities of the top 5 predicted tokens at the 𝑖𝑡ℎ step. 𝑛 is the total number of tokens in the rollout sequence. This equation ensures that the confidence score reflects the relative probability of the chosen token compared to the top alternatives, effectively normalizing the scores between 0 and 1. After obtaining the confidence scores for all tokens in the rollout sequence, we compute the average confidence score across all tokens to derive the overall reward score: 𝑣 = 1 𝑛 𝑛 𝑖=1 𝑐𝑖 where 𝑣 is the overall reward score for the rollout path. This average serves as the reward signal that evaluates the quality of the reasoning path taken during the rollout. higher 𝑣 indicates more confident and likely accurate reasoning path. By employing this method, we effectively expand the solution space, allowing the model to explore vast array of reasoning paths and select the most probable ones based on calculated confidence scores. 4. Reasoning Action Strategy 4.1. Action Selection We observed that using actions as the granularity for MCTS search is relatively coarse, often causing the model to overlook nuanced reasoning paths crucial for solving complex problems. To address this, we explored different levels of granularity in the MCTS search. Initially, we used steps as the unit of search. To further expand the models search space and enhance its problem-solving capabilities, we experimented with dividing these steps into smaller units of 64 or 32 tokens, referred to as mini-step. This finer granularity allowed the model to explore reasoning paths in greater detail. While token-level search offers theoretical maximum flexibility and granularity, it is currently impractical due to the significant computational resources required and the challenges associated with designing an effective reward model at this level. In our experiments, we implemented the following strategies within the MCTS framework: Step as Action: We allowed the model to generate complete reasoning steps as actions. Each MCTS node represents an entire thought or action label. This method enables efficient exploration but may miss finer-grained reasoning paths essential for complex problem-solving. Mini-step as Action: We used mini-steps of 32 or 64 tokens as actions. This finer granularity expands the solution space and improves the models ability to navigate complex reasoning tasks by considering more nuanced steps in the search process. By exploring the solution space at this level, the model is better equipped to find correct answers that might be overlooked with larger action units. 4 Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions 4.2. Reflection after Thinking We introduced reflection mechanism by adding the phrase Wait! Maybe made some mistakes! need to rethink from scratch. at the end of each thought process. This prompts the model to self-reflect and reevaluate its reasoning steps. Implementing this reflection has yielded significant improvements, especially on difficult problems that the original model initially solved incorrectly. With the addition of reflection, approximately half of these challenging problems were answered correctly. From the self-critic perspective [Valmeekam et al., 2023], this approach allows the model to act as its own critic, identifying potential errors in its reasoning. By explicitly prompting the model to question its initial conclusions, we encourage it to re-express and refine its thought process. This self-critical mechanism leverages the models capacity to detect inconsistencies or mistakes in its own output, leading to more accurate and reliable problem-solving [Madaan et al., 2024, Li et al., 2024, Huang et al., 2022]. The reflection step serves as an internal feedback loop, enhancing the models ability to self-correct without external intervention. 5. Experiments Based on Qwen2-7B-Instruct, we performed SFT using our training data to create Marco-o1-CoT. Besides, we employed Marco-o1-CoT within the framework of MCTS tree search, differentiating by actions: Marco-o1-MCTS (step): using each inference step as an action (step). Marco-o1-MCTS (mini-step of 64 tokens): using 64-token mini-step as an action (64 tokens). Marco-o1-MCTS (mini-step of 32 tokens): using 32-token mini-step as an action (32 tokens). During testing, each model utilized CoT prompt to ensure consistency in reasoning processes. We then tested these configurations on the English (En) and Chinese (Zh) subsets of the MGSM dataset, obtaining the following results: Model MGSM-En (Acc.) MGSM-Zh (Acc.) Qwen2-7B-Instruct Marco-o1-CoT Marco-o1-MCTS (step) Marco-o1-MCTS (mini-step of 64 tokens) Marco-o1-MCTS (mini-step of 32 tokens) 84.23% 85.60% 90.40% 88.40% 87.60% 76.80% 71.20% 80.00% 80.40% 82.40% Table 2 Experimental results on MGSM datasets These results demonstrate that: In the MGSM-en dataset, Marco-o1-CoT shows an advantage over Qwen2-7B-Instruct, as shown in Figure 4, which is expected due to the fine-tuning with English CoT data. In the MGSM-zh dataset, however, Marco-o1-CoT exhibits decrease in performance compared to Qwen2-7B-Instruct. This decline is attributed to the fact that the CoT data used for fine-tuning was in English, which may not transfer effectively to the Chinese dataset. The three MCTS-enhanced models demonstrate improvements over Marco-o1-CoT, indicating that incorporating MCTS helps to expand the models solution space and increase the probability of obtaining correct answers. However, since we use the Confidence Score as the reward, the tree search 5 Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions Figure 3 The main results of Marco-o1. results exhibit significant randomness. In MGSM-en, the step as Action strategy performs the best, while in MGSM-zh, the mini-step as Action (32) strategy yields the highest accuracy. Currently, as shown in Figures 4, 5, and 6, we cannot draw definitive conclusions about which action strategy is superior. We believe that as the reward becomes more accurate, the larger solution space provided by MCTS will demonstrate greater potential. These results demonstrate the effectiveness of our approach in enhancing the reasoning capabilities of the model across different languages and configurations. 6. Case Study on Translation Tasks To demonstrate the capabilities of our Marco-o1 model in translation tasks, we conducted case study comparing its performance with Google Translate on translating colloquial and slang expressions. Our model excels in understanding context and nuances, providing more accurate and natural translations. As shown in Figure 7, 8, and 9, these cases illustrate the advanced understanding and reasoning capabilities of Marco-o1 in handling complex translation tasks, especially with colloquial and slang language, outperforming standard translation tools like Google Translate. 7. Conclusions and Future Work Our Marco-o1 enhances the reasoning ability by integrating Chain-of-Thought (CoT) fine-tuning, Monte Carlo Tree Search (MCTS), and novel reasoning action strategies. Marco-o1s integration of MCTS allowed for expanded solution spaces, and experimentation with different action granularities (steps and mini-steps) showed the potential of finer search resolutions in enhancing accuracy. Our approach demonstrated significant improvements in reasoning tasks, as well as success in translating complex slang expressions. Looking ahead, we aim to refine the reward signal for MCTS through Outcome Reward Modeling (ORM) and Process Reward Modeling (PRM) [Lightman et al., 2023], which will reduce randomness and further improve performance. Additionally, reinforcement learning techniques are being explored to fine-tune the decision-making processes of Marco-o1, ultimately enhancing its ability to tackle complex real-world tasks. 6 Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions Figure 4 MCTS Expands the Solution Space for Correct Answers. Comparison between Marco-o1-CoT (left) and Marco-o1-MCTS (step) (right) on the MGSM dataset. While Marco-o1-CoT failed to provide the correct answer, integrating MCTS with step-level actions allowed the model to explore broader solution space, increasing the likelihood of arriving at the correct solution."
        },
        {
            "title": "References",
            "content": "X. Feng, Z. Wan, M. Wen, S. M. McAleer, Y. Wen, W. Zhang, and J. Wang. Alphazero-like tree-search can guide large language model decoding and training. arXiv preprint arXiv:2309.17179, 2023. J. Huang, S. S. Gu, L. Hou, Y. Wu, X. Wang, H. Yu, and J. Han. Large language models can self-improve. arXiv preprint arXiv:2210.11610, 2022. Z. Li, B. Peng, P. He, M. Galley, J. Gao, and X. Yan. Guiding large language models via directional stimulus prompting. Advances in Neural Information Processing Systems, 36, 2024. H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36, 2024. 7 Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions Figure 5 Finer Granularity with mini-steps Enhances Problem-Solving. Comparison between Marcoo1-MCTS (step) (left) and Marco-o1-MCTS (mini-step of 32 tokens) (right) on the MGSM dataset. The step-level action strategy did not yield the correct answer, but by using finer-grained mini-step of 32 tokens, the model successfully navigated the solution space to find the correct answer, demonstrating the effectiveness of increased action granularity. OpenAI. Learning to reason with llms. https://openai.com/index/ learning-to-reason-with-llms/, 2024. [Accessed 19-09-2024]. F. Shi, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. Vosoughi, H. W. Chung, Y. Tay, S. Ruder, D. Zhou, et al. Language models are multilingual chain-of-thought reasoners. arXiv preprint arXiv:2210.03057, 2022. D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, et al. Mastering the game of go without human knowledge. nature, 550(7676):354359, 2017. O. Team. Open-o1. https://github.com/Open-Source-O1/Open-O1, 2024. [Accessed 19-11-2024]. K. Valmeekam, M. Marquez, and S. Kambhampati. Can large language models really improve by self-critiquing their own plans? arXiv preprint arXiv:2310.08118, 2023. J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-of-thought 8 Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions Figure 6 Optimal Action Granularity Depends on Problem Complexity. Comparison between Marcoo1-MCTS (mini-step of 64 tokens) (left) and Marco-o1-MCTS (step) (right) on the MGSM dataset. The model with mini-step of 64 tokens failed to find the correct answer, whereas using step-level actions enabled the model to correctly solve the problem. This highlights that we cannot draw definitive conclusions about which action strategy is superior. We believe that as the reward becomes more accurate, the larger solution space provided by MCTS will demonstrate greater potential. prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. A. Yang, B. Yang, B. Hui, B. Zheng, B. Yu, C. Zhou, C. Li, C. Li, D. Liu, F. Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. T. Zhong, Z. Liu, Y. Pan, Y. Zhang, Y. Zhou, S. Liang, Z. Wu, Y. Lyu, P. Shu, X. Yu, et al. Evaluation of openai o1: Opportunities and challenges of agi. arXiv preprint arXiv:2409.18486, 2024. 9 Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions Figure 7 Demonstration of translation task using Marco-o1 of colloquial expression This shoe has comfortable sole and is highly recommended for purchase. Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions Figure 8 Translation comparison of colloquial expression Its so beautiful that its captivating, the upper part has distinctly Korean style, the soft and fluffy material is perfectly thick, and its complemented by base layer, creating unique and everyday-wear outfit. 11 Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions Figure 9 Translation comparison of colloquial expression Its so beautiful! And its so cheap, super straight and doesnt curl. Buy it, buy it!."
        }
    ],
    "affiliations": [
        "MarcoPolo Team, Alibaba International Digital Commerce"
    ]
}