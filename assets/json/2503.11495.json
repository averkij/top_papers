{
    "paper_title": "V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning",
    "authors": [
        "Zixu Cheng",
        "Jian Hu",
        "Ziquan Liu",
        "Chenyang Si",
        "Wei Li",
        "Shaogang Gong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Human processes video reasoning in a sequential spatio-temporal reasoning logic, we first identify the relevant frames (\"when\") and then analyse the spatial relationships (\"where\") between key objects, and finally leverage these relationships to draw inferences (\"what\"). However, can Video Large Language Models (Video-LLMs) also \"reason through a sequential spatio-temporal logic\" in videos? Existing Video-LLM benchmarks primarily focus on assessing object presence, neglecting relational reasoning. Consequently, it is difficult to measure whether a model truly comprehends object interactions (actions/events) in videos or merely relies on pre-trained \"memory\" of co-occurrences as biases in generating answers. In this work, we introduce a Video Spatio-Temporal Reasoning (V-STaR) benchmark to address these shortcomings. The key idea is to decompose video understanding into a Reverse Spatio-Temporal Reasoning (RSTR) task that simultaneously evaluates what objects are present, when events occur, and where they are located while capturing the underlying Chain-of-thought (CoT) logic. To support this evaluation, we construct a dataset to elicit the spatial-temporal reasoning process of Video-LLMs. It contains coarse-to-fine CoT questions generated by a semi-automated GPT-4-powered pipeline, embedding explicit reasoning chains to mimic human cognition. Experiments from 14 Video-LLMs on our V-STaR reveal significant gaps between current Video-LLMs and the needs for robust and consistent spatio-temporal reasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 5 9 4 1 1 . 3 0 5 2 : r V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning Zixu Cheng1, Jian Hu1*, Ziquan Liu1, Chenyang Si2, Wei Li3, Shaogang Gong1 1Queen Mary University of London, 2Nanjing University, 3Nanyang Technological University {zixu.cheng,jian.hu,ziquan.liu,s.gong}@qmul.ac.uk,chenyang.si@nju.edu.cn,wei.l@ntu.edu.sg https://V-STaR-Bench.github.io/ Figure 1. Illustration of the challenge in evaluating spatio-temporal reasoning ability. In both examples, the model correctly identifies objects, but its performance on related temporal and spatial questions varies greatly. This inconsistency suggests that correct answers may result from pretraining co-occurrence biases rather than true understanding. Existing benchmarks focus on object identification but fail to determine whether models truly engage in spatio-temporal reasoning. Our V-STaR benchmark fills this gap by evaluating how models integrate spatial, temporal, and causal relationships in video understanding."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Human processes video reasoning in sequential spatiotemporal reasoning logic, we first identify the relevant frames (when) and then analyse the spatial relationships (where) between key objects, and finally leverage these relationships to draw inferences (what). However, can Video Large Language Models (Video-LLMs) also reason through sequential spatio-temporal logic in videos? Existing Video-LLM benchmarks primarily focus on assessing object presence, neglecting relational reasoning. Consequently, it is difficult to measure whether model truly comprehends object interactions (actions/events) in videos or merely relies on pre-trained memory of co-occurrences as biases in generating answers. In this work, we introduce Video Spatio-Temporal Reasoning (V-STaR) benchmark to address these shortcomings. The key idea is to decompose video understanding into Reverse Spatio-Temporal Reasoning (RSTR) task that simultaneously evaluates what objects are present, when events occur, and where they are located while capturing the underlying Chain-of-thought (CoT) logic. To support this evaluation, we construct dataset to elicit the spatial-temporal reasoning process of Video-LLMs. It contains coarse-to-fine CoT questions generated by semi-automated GPT-4-powered pipeline, embedding explicit reasoning chains to mimic human cognition. Experiments from 14 Video-LLMs on our V-STaR reveal significant gaps between current Video-LLMs and the needs for robust and consistent spatio-temporal reasoning. *Corresponding author. When answering video question, humans first identify the relevant moment (when), then establish spatial and temporal relationships (where-when dependencies) of the key objects. Finally, we use these relationships to infer the answer (what) [21]. This reflects humans natural ability to construct sequential spatio-temporal reasoning logic by progressively organizing events across time and space [34]. This structured reasoning process has inspired the AI communitys development of Chain-of-Thought (CoT) reasoning [12, 18, 36]. Due to the inherent sequential logic of language, CoT can not only enhance model performance but also serve as tool for evaluating the reasoning capabilities of LLMs. However, unlike text-based tasks, visual tasks often lack clear logical steps, making it more challenging to design effective CoT strategies for both reasoning training and evaluation [5]. This challenge is further compounded in video reasoning, where understanding requires not only recognizing objects (what) but also establishing their spatial (where) and temporal (when) relationships. Some video spatio-temporal benchmarks attempt to evaluate models reasoning abilities. But current benchmarks only measure models output on object names (answering what) without assessing models capacity for relational reasoning. As result, as shown in Fig. 1, models can achieve high accuracy in question answering tasks by leveraging pre-trained co-occurrence biases [14] rather than truly understanding object interactions spatio-temporally. We argue it is essential to quantify models spatio-temporal reasoning ability. This helps reveal Video-LLMs true limiBenchmark Venue CVPR20 TCSVT22 CVPR24 CVPR25 ACL24 CVPR24 - VidSTG [48] - HC-STVG [37] MVBench [23] VideoMME [9] TempCompass [26] Movie-Chat-1k [35] MMBench-Video [7] NeurIPS24 LongVideoBench [39] NeurIPS24 NeurIPS24 HourVideo [3] EMNLP18 TVQA [19] CVPRW22 QAEgo4D [2] CVPR24 NeXT-GQA [40] NeurIPS24 REXTIME [4] NeurIPS24 E.T. Bench [27] ArXiv24 GCG [30] ACL20 TVQA+ [20] - Ours VQA with Grounding VQA Temporal Spatial - - - - - - - - - - - - - - - - - - - - CoT Questions - - - - - - - - - - - - - - - - Tasks Metrics MCQ MCQ MCQ MCQ Grounding Rule-based Grounding Rule-based Accuracy Accuracy MCQ or Y/N Accuracy Open-ended LLM-based Open-ended LLM-based Accuracy Accuracy Open-ended Rule-based Open-ended Rule-based Open-ended Rule-based Open-ended Rule-based Open-ended Rule-based Open-ended Rule-based Open-ended Rule-based Open-ended Rule-based Table 1. Comparison of spatial-temporal understanding datasets. tations and potential in video understanding tasks. However, existing datasets lack structured framework to assess spatio-temporal reasoning ability. As shown in Tab.1, numerous datasets [4, 19, 23, 27, 39] typically focus on three aspects: what objects are present, when events occur, and where objects are located. However, they either cover only one aspect or treat such questions in isolation as separate sub-tasks [20, 24], failing to measure models ability of logical spatio-temporal reasoning. Effective video understanding requires integrating what, when, and where through CoT-style reasoning. In this work, we introduce new Video Spatio-Temporal Reasoning (V-STaR) benchmark, to evaluate explicitly the capacity of current Video-LLMs on spatio-temporal reasoning comprehensively. There are two distinct designs in our V-STaR. First, we propose Reverse Spatio-Temporal Reasoning (RSTR) task, to break down and quantify models spatio-temporal reasoning ability. RSTR simultaneously assesses models output on what objects are present, when events occur, and where objects are located while also examining how model constructs CoT logic during reasoning. Second, to support this evaluation, we construct finegrained reasoning dataset using semi-automated GPT-4powered pipeline. To mimic human-thought cognitive process, we embed explicit reasoning chains within custom <think><think> tags for each question. To mitigate error propagation in model reasoning, we further decompose these reasoning chains into structured CoT tasks with increasing granularity, enabling more systematic and finegrained assessment of spatio-temporal video understanding. Additionally, we propose new metric, the Logarithmic Geometric Mean (LGM), which combines model score at each step of the reasoning chain, offering comprehensive assessment of spatio-temporal reasoning. We conducted experiments on 14 contemporary and state-of-the-art models, providing an inclusive assessment of the Video-LLMs reasoning capabilities. Our contributions are as follows: 1) We are among the first to investigate the spatio-temporal reasoning ability of state-of-the-art Video-LLMs, revealing their unreliable inference in such tasks. To support the eval2 uation, we propose V-STaR, the first benchmark explicitly designed to evaluate Video-LLMs spatio-temporal reasoning ability in answering questions explicitly in the context of when, where, and what. 2) We construct fine-grained reasoning dataset with coarse-to-fine CoT questions, enabling structured evaluation of spatio-temporal reasoning. Specifically, we introduce Reverse Spatio-Temporal Reasoning (RSTR) task to quantify models spatio-temporal reasoning ability. 3) Experiments from 14 Video-LLMs on V-STaR reveal although many models perform well on what, some struggle to ground their answers in time and location. This finding highlights fundamental weakness in existing VideoLLMs regarding causal spatio-temporal reasoning and inspires research in improving trustworthy spatio-temporal understanding in future Video-LLMs. 2. Related Works Spatio-temporal understanding in Video-LLMs VideoLLMs [13, 15, 22, 33, 38, 43, 47] have made rapid progress in video understanding, enabling them to answer diverse range of questions about videos, e.g. framed as video question answering (VQA) problems. Many opensource Video-LLMs demonstrate competitive results to the proprietary commercial models, e.g., GPT-4o [32] and Gemini-2-Flash [11], across multiple Video-LLM Benchmarks [7, 9, 39]. Recent studies have explored the ability of Video-LLMs in video temporal and spatial understanding. TimeChat [33], VTimeLLM [33], and Trace [13] were among the first to develop specialized models for video temporal grounding, which involves localizing event timestamps in video given text description. Additionally, general-purpose models, such as Qwen2.5-VL [1] and VideoLlama3 [44], also exhibit strong temporal grounding capability in video, achieving comparable performance of classic models [45, 46] on temporal grounding datasets [10, 17]. While certain Video-LLMs [1, 38, 44] claim to support object detection [25] and referring expression comprehension [42] on image inputs, their video spatial grounding capabilities remain largely unexplored. Munasinghe et al. [29] first introduces spatial grounding to Video-LLMs, later extended to video segmentation [30, 41, 43]. However, most existing Video-LLMs evaluate their performance on VQA, temporal grounding, and spatial grounding tasks separately, without validating their ability for spatio-temporal reasoning. It is unclear whether Video-LLMs correctly understand and use spatio-temporal information in video reasoning. Video-LLM Benchmarks Recently, numerous benchmarks have been proposed to evaluate the general video understanding and reasoning capabilities of Video-LLMs. These benchmarks span diverse range of tasks [23, 24, 26], types [7, 9, 35] and durations [3, 39, 49]. However, they primarily focus on Video Question Answering (VQA), Figure 2. Illustration of the semi-automated data construction pipeline of V-STaR. GPT-4 generates spatio-temporal reasoning CoT chain to answer VQA questions, along with set of RSTR questions. The RSTR questions are independent temporal or spatial grounding challenges, decomposed from the CoT reasoning chain, designed to evaluate the models spatio-temporal reasoning capabilities. essentially addressing the what question in videos while overlooking whether model correctly understands and leverages spatio-temporal context in their reasoning process. To bridge this gap, some studies [4, 30, 40] have begun incorporating temporal or spatial grounding to validate the reasoning pathways of Video-LLMs. TVQA [19] proposed Grounded Video Question Answering (GVQA), requiring models to answer not only multiple-choice questions but also temporal grounded evidence in TV series videos. Expanding upon GVQA, benchmarks such as QAEgo4D [2], Next-GQA [40], and ReXTime [4] have extended these tasks to ego-centric videos, real-world videos, and complex reasoning questions. Grounded Conversation Generation (GCG) [30] was designed to challenge models in reasoning and identifying specific objects for segmentation in videos. VidSTG [48] further integrated spatio-temporal grounding with interrogative queries to reason the referred object in videos. TVQA+ [20] then introduced spatiotemporal grounding for VQA, but treated it as three independent sub-tasks, without investigating how models utilize temporal and spatial relationships in their reasoning process. Building on these works, our benchmark introduces CoT reasoning and employs temporal and spatial grounding as structured reasoning chain, aiming to explicitly investigate the spatio-temporal reasoning abilities of Video-LLMs, providing more comprehensive evaluation framework. 3. V-STAR Benchmark In this section, we first define the Reverse Spatio-Temporal Reasoning (RSTR) task for evaluating the spatio-temporal reasoning capabilities of Video-LLMs. Then, we introduce semi-automatic pipeline using GPT-4 [31], to generate coarse-to-fine RSTR questions to construct the dataset. 3.1. Task Definition Most existing reasoning tasks require model to directly produce answers to complex sequential problems. These benchmarks [9, 23] often fail to reveal the models underlying reasoning process, and the model may exploit pretrained biases rather than engage in genuine reasoning on given video. To truly assess models ability, we propose the Reverse Spatio-Temporal Reasoning (RSTR) task. Specifically, the task is based on three fundamental elements: what, when, and where. Based on the spirit of human problem-solving [34], when faced with complex video spatio-temporal reasoning challenge, people typically start by (1) identifying the relevant frames (when), (2) then determining and analyzing the positions of objects in those frames (where), and (3) finally answering the what question. In contrast, due to pre-training cooccurrence biases, even if Video-LLM produces correct answer, it is hard to tell whether it did so via its own reasoning process or by relying on prior knowledge [14], causing inconsistency and sensitivity to prompting in models answers, e.g. hallucinations at the wrong time in the wrong place. To evaluate this, we adopt Reverse CoT strategy: the model is first prompted to answer the what question, and then, based on that answer, coarse-to-fine reasoning chain following the order what-when-where evaluates the models spatial-temporal reasoning capability. We also design parallel chain in the order what-where-when to examine how different logical sequences impact the final results. Our RSTR task not only evaluates the models spatiotemporal reasoning ability, but also quantifies the influence of various logical sequences. 3.2. Dataset Construction significant challenge in constructing this new dataset is to obtain videos accompanied by precise, coarse-to-fine CoT questions. To ease the burden of manual annotation, we propose hybrid approach that leverages annotated data from existing datasets while incorporating semi-automated annotation pipeline. This approach unfolds in three stages: data collection, pipeline construction, and metric design. Data Collection. We collected videos from datasets that offer spatial and temporal grounding. We used VidSTG [48], TVQA+ [20], and GOT-10K [16] datasets. VidSTG provides spatio-temporal grounding. TVQA+ offers temporal grounding for certain objects through question-answer pairs. GOT-10k gives spatial grounding details. However, these datasets do not include CoT reasoning chains, and their video durations are mostly between 0 and 3 minutes. Such rather short video durations are much narrower than what is seen in real-world scenarios. To ensure diverse range of video durations, we started with the GOT-10k dataset because it has complete spatial grounding informaFigure 3. Dataset statistics of video domain and length, and visualization of objects in video."
        },
        {
            "title": "Entertainment Daily Life",
            "content": "Avg Length(s) Avg Moment(s) Avg M/L Ratio(%) Num of BBox Num of Objects 104.60 9.32 15.16 2097 255 88.21 8.68 20.29 4351 38 Indoor 45.24 10.40 22.98 4621 29 Sports Animals Vehicles Nature 44.19 128.00 8.96 6.99 20.49 20.76 789 1409 12 37 42.16 7.70 20.01 806 38.07 8.10 21.30 1471 26 Shows 258.14 10.71 18.34 840 18 Tutorial Overall 110.23 1512.05 9.06 10.45 19.32 2.02 16793 409 342 29 Table 2. Statistical comparison of different domains. tion. We then collected additional videos from YouTube that range from 3 minutes to 1 hour. We randomly inserted selected GoT videos into various positions within these videos. It ensures that the final dataset shows high degree of diversity in both duration and content. Once we gathered videos of various lengths and types, we built the coarse-to-fine CoT questions for reasoning. Pipeline Construction. In the previous stage, we collected diverse set of videos with complete spatio-temporal grounding labels. However, our goal is to evaluate the models spatio-temporal reasoning ability in fine-grained manner. To achieve this, we leveraged GPT-4-turbo [31] to construct semi-automated pipeline for generating CoT reasoning chains and questions with coarse-to-fine granularity. Specifically, as shown in Fig. 2, we first automatically filter out samples where the video length is too short or the moment ratio to video length is too large, ensuring that the questions remain sufficiently challenging. Next, we input video question along with its answer, as well as the corresponding temporal and spatial annotations, into GPT4-turbo. This process generates spatio-temporal reasoning chain for answering the video question, which is then decomposed into two independent fine-grained sub-questions focusing on temporal and spatial localization. These subquestions evaluate whether the models spatial and temporal reasoning is correct. Finally, we manually verify the reasoning chain and the decomposed localization questions, assigning the temporal and spatial labels to each sub-question. to comprehensively investigate how model leverages temporal and spatial relationships in the reasoning, we formulate our generated questions into two RSTR task chains:what-when-where and what-wherewhen. In each reasoning chain, the subsequent question incorporates the ground truth of the previous question. For instance, in the what-when-where chain, the when quesFurthermore, 4 tion contains the ground truth of the what question, and the where question includes the ground truths of both the when and what questions. This design prevents the model from making errors in earlier reasoning steps and propagating to the final result, allowing for an independent and fairer evaluation of temporal and spatial reasoning. Ultimately, each sample is associated with one spatiotemporal CoT reasoning chain and two RSTR task chains. Metric Design. To evaluate the models spatio-temporal reasoning ability, we have decomposed the task into finegrained CoT reasoning questions. Each question targets one of the what, when, and where components, which are independently assessed using Acc (accuracy), tIoU (mean temporal IoU), and vIoU (mean visual IoU). Although this method effectively measures the performance of each individual question, it only considers the correctness of each answer in isolation, ignoring the interconnections between different answers within CoT reasoning. To overcome this problem, we propose evaluating the models overall performance across these three questions using the Arithmetic Mean (AM) (Eq. 1) and modified logarithmic Geometric Mean (LGM) (Eq. 3). Specifically, AM is given as: 1 3 AM = (Acc + tIoU + vIoU), (1) while AM effectively assesses the models overall performance across different metrics, it is susceptible to extreme values. To mitigate this issue, we employ the Geometric Mean (GM) to evaluate model performance: GM = (Acc tIoU vIoU) (2) However, when any of the metrics is zero, GM will become zero, which fails to reflect the contribution of the remaining metrics. To alleviate it, we transform GM into logarithmic 1 3 , Figure 4. An example illustrating the construction of CoT questions. Each sample contains thinking chain and two RSTR question chains. GM (LGM) as follows: LGM = (cid:110) ln (cid:0)1 Acc + 系(cid:1) + ln (cid:0)1 tIoU + 系(cid:1) + ln (cid:0)1 vIoU + 系(cid:1)(cid:111) , (3) 1 3 where 系 is small constant to prevent ln(0) when any metric reaches 1. Eq.3 maps the metric range from 0 to positive infinity and ensures higher performance corresponds to higher LGM score. Since the logarithm transformation results in values that are typically small in magnitude, we multiply LGM by linear scaling factor of 100 to ensure numerical clarity, allowing finer distinctions between different methods while preserving relative ranking. Moreover, when the same questions appear in different CoT chains, the order in which they occur can lead to significant variations in the results. To assess the overall performance of the model across different chains, we propose the mean AM (mAM) and mean LGM (mLGM) as follows: mAM = 1 (cid:88) k=1 AMk, mLGM = 1 (cid:88) k=1 LGMk. (4) where denotes the number of different chains. The mAM and mLGM effectively evaluate the combined impact of the various chains on the models performance. 3.3. Dataset Statistics Here, we present detailed statistics of our dataset, including video information, meta information, qualitative analyses, and comparisons with previous works. Video Information. Our dataset comprises 2094 videos totalling 64.12 hours of footage. As shown in Fig.3(a), to ensure the inclusion of varied video genres, we categorized the videos into 9 domains: Entertainment, Daily Life, Indoor, Sports, Animals, Vehicles, Nature, Shows, and Tutorial. The length distribution of the videos, illustrated in Fig.3(b), demonstrates considerable diversity. The videos range in length from 15.02 seconds to 59.2 minutes with average 110.23 seconds, satisfying the requirement for diverse video lengths and better reflecting real-world scenarios. Meta Information. To further assess the completeness of 5 our dataset, we assessed the meta-information annotations. Each video is accompanied by temporal moment annotations, with an average duration of 9.06 seconds and individual durations ranging from 1.7 seconds to 47 seconds. These temporal moments account for an average of 19.3% of the total video duration, ensuring reasonable level of difficulty for the temporal grounding subtask. For the spatial grounding subtask, we annotated 342 objects with total of 16,793 bounding boxes, covering approximately 19.8% of the video resolution. This proportion is similar to that of the temporal grounding, ensuring consistent challenge levels across both tasks. Additionally, we visualized the object categories with word cloud (Fig.3(c)), demonstrating that our questions robustly capture wide diversity of objects. Tab.2 provides further detailed statistics. Qualitative analyses. Fig. 4 shows an example from our VSTaR benchmark. It contains one spatio-temporal CoT reasoning thinking chain and two RSTR task chains. For each RSTR task chain, the CoT evaluation starts with coarsegrained question about what in the video. In the whatwhen-where chain, the subsequent when question incorporates the answer of what and its answer is included in the where question. In the other chain, the subsequent where question contains the answer of what and the bounding boxes answer of where will be provided without time information in the when question. Comparisons with previous benchmarks. We compared our V-STaR to previous Video-LLM benchmarks in Tab. 1. Most existing datasets only focused on what question in VQA [3, 7, 9, 23, 26, 35, 39], failed to validate the models spatio-temporal reasoning ability. Some partially cover on when [2, 4, 19, 27, 40] or where [30], without complete spatio-temporal reasoning chain. Only TVQA+ [20] covered all of the three, but it ignored their inner spatiotemporal reasoning relationship. Instead, our V-STaR provides two CoT question chains for each sample to reveal the spatio-temporal reasoning ability of Video-LLMs. Model Venue Parameters - - GPT-4o [32] Gemini-2-Flash [11] Video-LLaMA3 [44] ArXiv25 ArXiv25 Qwen2.5-VL [1] ArXiv24 Qwen2-VL [38] ArXiv24 InternVL-2.5 [6] ArXiv24 Llava-Video [47] CVPR24 VideoChat2 [23] Oryx-1.5 [28] ICLR25 Video-CCAM-v1.2 [8] ArXiv24 CVPR24 TimeChat [33] CVPR24 VTimeLLM [15] ICLR25 TRACE [13] ArXiv25 Sa2VA [43] - - 7B 7B 7B 8B 7B 7B 7B 7B 7B 7B 7B 8B LGM AM Where (Spatial Grounding) What (VQA) When (Temporal Grounding) Score Acc R1@0.3 R1@0.5 R1@0.7 tIoU AP@0.1 AP@0.3 AP@0.5 vIoU 1.71 60.78 1.59 53.01 1.38 41.94 1.61 54.53 1.03 25.91 1.46 44.18 1.50 49.48 1.27 36.21 0.94 20.47 1.75 59.35 1.06 26.38 1.45 41.46 0.90 17.60 0.70 16.36 16.67 24.54 22.97 11.48 19.18 8.72 10.52 13.69 13.54 1.50 12.01 17.13 19.74 0.11 10.35 15.84 19.80 8.92 17.94 4.87 6.30 13.07 4.48 0.00 8.68 10.88 14.17 0.00 23.14 31.63 35.73 17.03 27.96 11.98 15.12 20.47 17.03 1.15 17.80 25.24 28.53 0. 8.36 3.82 0.76 19.92 12.21 0.27 0.94 1.31 11.60 - - 0.14 - 42.68 2.75 0.93 0.11 8.36 3.89 0.04 0.18 0.14 2.17 - - 0.03 - 34.18 19.92 15.67 3.17 35.89 28.59 2.18 5.23 10.06 35.58 - - 0.62 - 52.16 39.51 27.97 6.47 36.14 27.39 4.63 0.89 27.12 21.93 13.59 35.20 26.53 20.35 18.13 9.31 22.69 17.85 0.65 27.11 20.64 1.92 2.51 20.74 17.47 10.14 16.05 14.72 30.51 20.28 14.47 12.80 24.18 19.60 13.78 12.45 32.31 19.00 16.26 5.10 9.45 8.68 3.72 9.16 2.34 1.43 6.49 1.72 0.00 3.48 3.15 6.73 0.00 - - 0.21 - Table 3. Performance on the chain of what-when-where. The top result is highlighted in bold, while the second is underlined. - denotes model failed to generate formatted answers. The score ranges from 0 to 4. Model Venue Parameters - - GPT-4o [32] Gemini-2-Flash [11] Video-LLaMA3 [44] ArXiv25 Qwen2.5-VL [1] ArXiv25 ArXiv24 Qwen2-VL [38] ArXiv24 InternVL-2.5 [6] Llava-Video [47] ArXiv24 CVPR24 VideoChat2 [23] Oryx-1.5 [28] ICLR25 Video-CCAM-v1.2 [8] ArXiv24 CVPR24 TimeChat [33] CVPR24 VTimeLLM [15] TRACE [13] ICLR25 ArXiv25 Sa2VA [43] - - 7B 7B 7B 8B 7B 7B 7B 7B 7B 7B 7B 8B LGM AM Where (Spatial Grounding) When (Temporal Grounding) What (VQA) Score Acc AP@0.1 AP@0.3 AP@0.5 vIoU R1@0.3 R1@0.5 R1@0.7 tIoU 1.71 60.78 1.59 53.01 1.38 41.94 1.61 54.53 1.03 25.91 1.46 44.18 1.50 49.48 1.27 36.21 0.94 20.47 1.75 59.35 1.06 26.38 1.45 41.46 0.90 17.60 0.70 16.36 10.04 15.22 20.42 5.39 16.32 3.77 5.49 12.07 5.58 0.00 8.54 4.53 12.02 0.00 17.13 31.58 35.11 11.02 24.62 10.83 16.89 18.08 18.99 2.19 20.42 8.44 24.52 0. 9.29 7.49 0.62 5.15 7.11 0.42 4.29 3.08 11.50 - - 0.00 - 58.47 1.19 0.58 0.02 1.40 1.14 0.00 0.25 0.30 0.96 - - 0.00 - 40.42 3.01 2.21 0.19 2.00 2.41 0.14 1.31 0.97 3.50 - - 0.00 - 37.48 4.18 1.89 0.17 2.87 3.55 0.03 1.23 0.91 4.32 - - 0.00 - 49.47 12.82 36.79 25.53 23.83 34.99 26.35 23.14 26.96 21.76 7.61 29.58 21.38 17.52 17.23 15.28 7.75 27.15 17.36 12.21 27.54 21.00 12.50 19.77 16.56 14.81 14.16 12.93 2.26 30.88 20.54 13.60 15.08 13.33 19.90 15.81 5.96 17.11 12.71 11.57 21.61 17.95 0.00 7.25 8.54 9.21 2.48 8.25 1.57 2.00 6.20 2.72 0.00 2.53 2.10 5.73 0. Table 4. Performance on chain of what-where-when. The top result is highlighted in bold, while the second is underlined. - denotes model failed to generate formatted answers. The score ranges from 0 to 4. 4. Experiments 4.1. Setting and Metrics Implementation Details. We tested 14 Video-LLMs, involving 2 commercial models GPT-4o [32] and Gemini-2Flash [11], and 12 open-source models. The open-source models include (i) 8 generic models: Video-LLaMA3 [44], Qwen2.5-VL [1], Qwen2-VL [38], InternVL-2.5 [6], LLaVA-Video [47], VideoChat2 [23], Oryx-1.5 [28], and Video-CCAM-v1.2 [8]); (ii) 3 time-aware models: TimeChat [33], VTimeLLM [15], and Trace [13]; and (iii) 1 segmentation model, Sa2VA [43]. We followed their official configurations and sampled the video frames at 1fps for all models. If video exceeded the models input limitations, we applied uniform sampling to select the maximum allowable number of frames. We investigated the models spatio-temporal reasoning ability using two RSTR task chains: what-when-where and what-where-when. Experiments were run on 2 NVIDIA A100 80G GPUs. Metrics. To evaluate the open-ended what question, we follow MMBench-Video [7] and use Qwen2.5-72B-Instruct to score answers from 0 to 4, denoting entirely incorrect, Short Medium Long All Model mAM mLGM mAM mLGM mAM mLGM mAM mLGM 27.49 38.56 26.96 40.58 14.86 19.28 26.75 38.15 GPT-4o [32] 24.97 32.07 28.99 40.35 37.81 56.14 26.87 35.57 Gemini-2-Flash [11] 21.68 26.62 21.84 27.23 22.46 28.83 21.66 27.04 Video-LLaMA3 [44] 23.96 32.39 25.51 34.84 23.67 32.87 Qwen2.5-VL [1] 15.78 17.50 18.47 21.22 14.09 17.53 16.71 18.79 Qwen2-VL [38] 17.94 22.90 17.94 23.06 9.58 11.19 17.60 24.92 InternVL-2.5 [6] Llava-Video [47] 22.37 30.23 18.28 22.77 18.23 25.23 20.82 27.33 17.57 21.02 17.20 20.50 17.02 20.26 VideoChat2 [23] Oryx-1.5 [28] 13.17 14.25 14.83 16.46 11.89 13.99 15.11 13.83 Video-CCAM-v1.2 [8] 21.66 34.09 19.62 28.36 12.61 15.80 20.41 30.70 13.07 14.78 TimeChat [33] 13.70 15.56 13.22 15.06 18.31 23.19 18.15 22.44 17.71 22.04 VTimeLLM [15] 11.77 12.96 12.49 13.87 13.59 15.30 12.01 13.25 TRACE [13] 17.11 20.31 18.14 22.01 16.32 18.92 Sa2VA [43] 3.37 5.89 3.24 5. 9.70 2.27 5.64 2.20 5.28 8. Short, Performance on different video lengths. Table 5. Medium and Long denote video durations of [0, 1] min, (1, 3] min, and (3, 60] min, respectively. The top result is highlighted in bold, while the second is underlined. largely incorrect, largely correct, and entirely correct. Answers scoring above 2 are considered correct, allowing us to compute accuracy. For the when question, we follow the commonly used temporal grounding metrics, R@n, tIoU=m, which refers to the percentage of top-n prediction with temporal IoU score larger than m, and mean temporal IoU score (m tIoU). For the where question, we 6 AcctIoU@0.3 AccvIoU@0.1 AcctIoU@0.3, vIoU@0. Model Chain 1 Chain 2 Chain 1 Chain 2 Chain 1 Chain 2 15.12 GPT-4o [32] Gemini-2-Flash [11] 19.70 Video-LLaMA3 [44] 15.41 10.73 Qwen2.5-VL [1] 8.06 Qwen2-VL [38] 5.92 InternVL-2.5 [6] 7.92 Llava-Video [47] 8.78 VideoChat-2 [23] 3.58 Oryx-1.5 [28] 7.59 4.53 7.68 3.48 4.2 4.48 0.19 0.52 1.15 4.25 19.99 4.68 2.29 5.06 2.05 0.19 0.62 0.19 0.67 0.19 2.86 0.29 2.1 1.24 1.24 4.2 2.05 11.16 3.96 15.27 19.04 0.66 8.68 14.89 0.52 1.34 7.20 3.53 24.24 6.68 1.38 7.11 4.77 1.15 0.81 8.97 1.05 3.05 7.73 1.05 3.34 3.77 0.19 6.25 3.91 2.24 0.05 1.15 1.53 0.05 0.86 0.62 0.67 0.62 1.24 0.47 3.53 0.76 0.14 0.19 0.33 0. Figure 5. The performance of each domain. follow TVQA+ [20] and VidSTG [48] to use the Average Precision score (AP@vIoU=m) and mean visual Intersection over Union (m vIoU) of every annotated frame. We follow the proposed LGM (Eq.3) and AM (Eq.1) to measure models spatial-temporal reasoning ability. higher LGM indicates better overall spatio-temporal reasoning ability of the model, and higher AM indicates more average performance of the model on the three metrics. 4.2. Quantitative Results Performance on what-when-where chain. As shown the what-where-when chain evaluates in Tab.3, models spatial-temporal reasoning ability. Overall, GPT4o, Gemini-2-Flash, and Qwen2.5-VL demonstrate the strongest spatio-temporal reasoning capabilities, ranking as the top-3 models. Their scores for LGM and AM are 39.15/36.14/35.20 and 27.97%/27.39%/26.53%, respectively. At the lower end, Trace, TimeChat, and Oryx1.5 rank as the bottom-3 models, with LGM and AM scores of 13.78/14.47/16.05 and 12.45%/12.80%/14.72%, respectively. The remaining models, ranked in descending order based on their LGM scores, are Video-CCAM, Video-LLaMA3, LLaVA-Video, VTimeLLM, InternVL2.5, VideoChat2, Qwen2-VL, and SA2VA. Among them, Video-LLaMA3 demonstrates the most balanced performance, with an AM score of 21.93%. it (m tIoU:1.50%) In open-source models, Video-CCAM-v1.2 leads in VQA accuracy (59.35%) but struggles with fine-grained temporal and spatial understanding (fail). While VideoLLaMA3 leads in temporal grounding (m tIoU:22.97%), lacks consistency across the other two tasks. Sa2VA excels in spatial grounding (m vIoU: 32.31%), but performs poorly in VQA (16.36%) and temporal grounding (m tIoU: 0.11%). In contrast, Qwen2.5-VL shows the most balanced performance across all three tasks, leading open-source models in overall performance. They highlight that maintaining consistency across whatwhen-where reasoning is crucial, as weaknesses in earlier steps propagate and affect overall performance. Performance on what-where-when chain. Tab.4 presents the models performance across the other reasoning chain what-where-when. In this chain, GPT4o and Gemini-2-Flash achieve the top-2 overall perforTable 6. Joint performance evaluation across models. mances, with LGM scores of 36.79 and 34.99, respectively. However, Gemini-2-Flash exhibits more balanced performance than GPT-4o, with an AM score of 26.35% compared to 25.53%. Although Video-CCAM-v1.2 outperforms Qwen2.5-VL in overall performance (LGM: 30.88 vs. 29.58), it is less consistent across tasks (AM: 20.54% vs. 21.38%). The bottom-3 models remain Trace, Oryx-1.5, and TimeChat, with LGM and AM scores of (12.71/14.16/15.08) and (11.57%/12.93%/13.33%), respectively. The remaining models, ranked by LGM from high to low, are LLaVA-Video, InternVL-2.5, Video-LLaMA3, SA2VA, VTimeLLM, VideoChat-2, and Qwen2-VL. In this reasoning chain, without temporal grounding as prerequisite step, models exhibit general performance drop in spatial grounding. The most significant decline is observed in Qwen2.5-VL, whose vIoU score drops sharply to 2.00%. In temporal grounding, excessive spatial information in the prompts leads to substantial performance drop in VTimeLLM, reducing its tIoU score to 5.96%. Interestingly, Llava-video, Video-LLaMA3, Oryx1.5 and TimeChat show slight improvements in this setting. Performance on each domain. We visualize the performance on each domain in Fig.5 using mAM (left) and mLGM (right). Gemini-2-Flash (orange) and LLaVA-Video (green) demonstrate relatively balanced performance across domains. GPT-4o (blue-green) performs best in the Animals, Nature, Daily Life, and Sports but lags in the Tutorial. Qwen2.5-VL (pink) shows strong performance in Vehicle but also lags in Tutorials, whereas Video-CCAM-v1.2 (yellow) shows strong advantage in Shows, Vehicles, and Nature domains but weaker performance in others. Overall, it indicates that current Video-LLMs do not generalize well across all domains, emphasizing the need for domainspecific evaluation in spatio-temporal reasoning tasks. Effect of different video length. From the results in Table 5, the models spatial-temporal reasoning ability is evaluated across different video lengths, showing how performance shifts as reasoning complexity increases. GPT4o performs well on short videos (mLGM: 38.56, mAM: 27.49%) but struggles with long sequences, suggesting weaker long-range dependency modelling. In medium videos, GPT-4o achieves the highest performance with 40.58% in mLGM, while Gemini-2-Flash demonstrates greater balance with 28.99% in mAM. Gemini-2-Flash consistently outperforms others in long videos, achieving the 7 Figure 6. An example showcasing the performance of five models. highest mLGM (56.14) and mAM (37.81%), indicating strong temporal reasoning over extended sequences. It also leads in medium (mLGM: 40.35, mAM: 28.99%) and overall performance (mLGM: 35.57, mAM: 26.87%), highlighting its robustness across different durations. Overall, GPT4o achieves the highest overall performance with 38.15 in mLGM, with competitive mAM score. Among opensource models, Qwen2.5-VL, InternVL-2.5 and VideoChat2 perform moderately well but show noticeable declines as video length increases. Video-CCAM-v1.2, TimeChat, and TRACE struggle across all durations, with mLGM scores below 15, indicating weak spatial-temporal integration. These results suggest that handling longer sequences remains challenge, requiring models to improve long-range dependency modelling to maintain reasoning continuity across extended video durations. Results on joint performance. To further evaluate spatiotemporal reasoning, we analyze models joint performance on RSTR tasks in Tab.6. Using thresholds (tIoU = 0.3, vIoU = 0.1), we measure the percentage of samples where models correctly use temporal, spatial, or both cues to infer answers. Results show that Gemini-2-Flash excels in temporal reasoning, while Qwen2.5-VL leads in spatial reasoning for Chain 1 and SA2VA for Chain 2. For combined spatio-temporal reasoning, Qwen2.5-VL ranks highest in Chain 1, and Gemini-2 in Chain 2, but both still with low accuracy (4.68% and 2.24%). This highlights the limited spatio-temporal reasoning abilities of current Video-LLMs. From the changes between the two chains in the joint performance, GPT-4o and Qwen2.5-VL are the most affected. Qualitative analysis. Fig.6 presents visualization of five models performance on the V-STaR benchmark. While all correctly answer the what questions, their In the whatspatio-temporal reasoning remains weak. when-where chain, Qwen2.5-VL achieves the best spatial grounding but struggles with temporal localization, whereas InternVL-2.5 excels in temporal grounding but fails in spatial accuracy. GPT-4o, Gemini-2-Flash, and VideoLlama3 show more balanced understanding of both aspects. In the what-where-when chain, Qwen2.5-VL maintains stable spatial grounding despite missing temporal cues, while others degrade. When given spatial information, GPT-4o and Qwen2.5-VL improve in temporal grounding, VideoLlama3 remains unchanged, and Gemini-2-Flash and InternVL-2.5 perform worse. Notably, Video-LLMs often analyse each frame independently, overlooking dynamic relationships among frames and treating objects as static, revealing key limitation in their motion perception. Supplementary material. App. and provides more codes and benchmark details. App. includes additional implementation information. App. presents an in-depth experiment results with 24 tables, and App. is limitation. 5. Conclusion This work introduces new Video-LLM spatio-temporal reasoning benchmark, V-STaR, the first benchmark for comprehensively assessing spatio-temporal reasoning ability of Video-LLMs. We constructed dataset with coarseto-fine CoT questions for structured evaluation and introduced new Logarithmic Geometric Mean (LGM) metric for scoring video spatio-temporal reasoning performance. Experiments on 14 Video-LLMs provide insights into their reasoning capabilities and future improvements."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 2, 6, 7 [2] Leonard Barmann and Alex Waibel. Where did leave my keys?-episodic-memory-based question answering on egocentric videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1560 1568, 2022. 2, 3, 5 [3] Keshigeyan Chandrasegaran, Agrim Gupta, Lea Hadzic, Taran Kota, Jimming He, Cristobal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, and Fei-Fei Li. Hourvideo: 1-hour video-language understanding. Advances in Neural Information Processing Systems, 37:5316853197, 2025. 2, 5 [4] Jr-Jen Chen, Yu-Chien Liao, Hsi-Che Lin, Yu-Chu Yu, YenChun Chen, and Frank Wang. Rextime: benchmark suite for reasoning-across-time in videos. Advances in Neural Information Processing Systems, 37:2866228673, 2025. 2, 3, 5 [5] Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, and Ajay Divakaran. Measuring and improving chain-ofthought reasoning in vision-language models. arXiv preprint arXiv:2309.04461, 2023. 1 [6] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 6, 7 [7] Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbench-video: long-form multi-shot benchmark for holistic video understanding. Advances in Neural Information Processing Systems, 37:8909889124, 2025. 2, 5, [8] Jiajun Fei, Dian Li, Zhidong Deng, Zekun Wang, Gang Liu, and Hui Wang. Video-ccam: Enhancing video-language understanding with causal cross-attention masks for short and long videos. arXiv preprint arXiv:2408.14023, 2024. 6 [9] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 2, 3, 5 [10] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall: Temporal activity localization via language query. In Proceedings of the IEEE international conference on computer vision, pages 52675275, 2017. 2 [11] Google. Google, gemini-2-flash. Technical report, Google, 2024. 2, 6, 7 [12] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 1 [13] Yongxin Guo, Jingyu Liu, Mingda Li, Xiaoying Tang, Qingbin Liu, and Xi Chen. Trace: Temporal grounding video llm via causal event modeling. arXiv preprint arXiv:2410.05643, 2024. 2, [14] Jian Hu, Jiayi Lin, Junchi Yan, and Shaogang Gong. Leveraging hallucinations to reduce manual prompt dependency in promptable segmentation. Advances in Neural Information Processing Systems, 37:107171107197, 2025. 1, 3 [15] Bin Huang, Xin Wang, Hong Chen, Zihan Song, and Wenwu Zhu. Vtimellm: Empower llm to grasp video moments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1427114280, 2024. 2, 6 [16] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: large high-diversity benchmark for generic object tracking in the wild. IEEE transactions on pattern analysis and machine intelligence, 43(5):15621577, 2019. 3 [17] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In Proceedings of the IEEE international conference on computer vision, pages 706715, 2017. 2 [18] Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, et al. Measuring faithfulness in chain-of-thought reasoning. arXiv preprint arXiv:2307.13702, 2023. 1 [19] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara Berg. Tvqa: Localized, compositional video question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 13691379, 2018. 2, 3, [20] Jie Lei, Licheng Yu, Tamara Berg, and Mohit Bansal. Tvqa+: Spatio-temporal grounding for video question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 82118225, 2020. 2, 3, 5, 7 [21] Jiangtong Li, Li Niu, and Liqing Zhang. From representation to reasoning: Towards both evidence and commonsense reasoning for video question-answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2127321282, 2022. 1 [22] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 2 [23] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195 22206, 2024. 2, 3, 5, 6, 7 [24] Yunxin Li, Xinyu Chen, Baotian Hu, Longyue Wang, Haoyuan Shi, and Min Zhang. Videovista: versatile benchmark for video understanding and reasoning. arXiv preprint arXiv:2406.11303, 2024. 2 [25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence 9 In Zitnick. Microsoft coco: Common objects in context. Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. 2 [26] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv preprint arXiv:2403.00476, 2024. 2, 5 [27] Ye Liu, Zongyang Ma, Zhongang Qi, Yang Wu, Ying Shan, and Chang Wen Chen. Et bench: Towards open-ended event-level video-language understanding. arXiv preprint arXiv:2409.18111, 2024. 2, 5 [28] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx mllm: On-demand spatial-temporal understanding at arbitrary resolution. arXiv preprint arXiv:2409.12961, 2024. 6, 7 [29] Shehan Munasinghe, Rusiru Thushara, Muhammad Maaz, Hanoona Abdul Rasheed, Salman Khan, Mubarak Shah, and Fahad Khan. Pg-video-llava: Pixel grounding large videolanguage models. arXiv preprint arXiv:2311.13435, 2023. [30] Shehan Munasinghe, Hanan Gani, Wenqi Zhu, Jiale Cao, Eric Xing, Fahad Shahbaz Khan, and Salman Khan. Videoglamm: large multimodal model for pixel-level visual grounding in videos. arXiv preprint arXiv:2411.04923, 2024. 2, 3, 5 [31] OpenAI. Gpt-4 technical report. Technical report, OpenAI, 2023. 3, 4 [32] OpenAI. Openai, gpt-40. Technical report, March 2024. 2, 6, 7 [33] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1431314323, 2024. 2, [34] Camilo Miguel Signorelli, Selma Dundar-Coecke, Vincent Wang, and Bob Coecke. Cognitive structures of space-time. Frontiers in Psychology, 11:527114, 2020. 1, 3 [35] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1822118232, 2024. 2, 5 [36] Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, and Greg Durrett. To cot or not to cot? chain-of-thought helps mainly on math and symbolic reasoning. arXiv preprint arXiv:2409.12183, 2024. 1 [37] Zongheng Tang, Yue Liao, Si Liu, Guanbin Li, Xiaojie Jin, Hongxu Jiang, Qian Yu, and Dong Xu. Human-centric spatio-temporal video grounding with visual transformers. IEEE Transactions on Circuits and Systems for Video Technology, 32(12):82388249, 2021. 2 [38] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models 10 perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2, 6, 7 [39] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. Advances in Neural Information Processing Systems, 37:2882828857, 2025. 2, 5 [40] Junbin Xiao, Angela Yao, Yicong Li, and Tat-Seng Chua. Can trust your answer? visually grounded video question In Proceedings of the IEEE/CVF Conference answering. on Computer Vision and Pattern Recognition, pages 13204 13214, 2024. 2, 3, [41] Cilin Yan, Haochen Wang, Shilin Yan, Xiaolong Jiang, Yao Hu, Guoliang Kang, Weidi Xie, and Efstratios Gavves. Visa: Reasoning video object segmentation via large language models. In European Conference on Computer Vision, pages 98115. Springer, 2024. 2 [42] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expresIn Computer VisionECCV 2016: 14th European sions. Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 6985. Springer, 2016. 2 [43] Haobo Yuan, Xiangtai Li, Tao Zhang, Zilong Huang, Shilin Xu, Shunping Ji, Yunhai Tong, Lu Qi, Jiashi Feng, and Ming-Hsuan Yang. Sa2va: Marrying sam2 with llava for dense grounded understanding of images and videos. arXiv preprint arXiv:2501.04001, 2025. 2, 6 [44] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025. 2, 6, 7 [45] Hao Zhang, Aixin Sun, Wei Jing, and Joey Tianyi Zhou. Span-based localizing network for natural language video localization. arXiv preprint arXiv:2004.13931, 2020. 2 [46] Songyang Zhang, Houwen Peng, Jianlong Fu, and Jiebo Luo. Learning 2d temporal adjacent networks for moment localization with natural language. In Proceedings of the AAAI conference on artificial intelligence, pages 1287012877, 2020. 2 [47] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 2, 6, [48] Zhu Zhang, Zhou Zhao, Yang Zhao, Qi Wang, Huasheng Liu, and Lianli Gao. Where does it exist: Spatio-temporal video grounding for multi-form sentences. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1066810677, 2020. 2, 3, 7 [49] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024."
        }
    ],
    "affiliations": [
        "Nanjing University",
        "Nanyang Technological University",
        "Queen Mary University of London"
    ]
}