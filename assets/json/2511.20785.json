{
    "paper_title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling",
    "authors": [
        "Zuhao Yang",
        "Sudong Wang",
        "Kaichen Zhang",
        "Keming Wu",
        "Sicong Leng",
        "Yifan Zhang",
        "Bo Li",
        "Chengwei Qin",
        "Shijian Lu",
        "Xingxuan Li",
        "Lidong Bing"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 2 5 8 7 0 2 . 1 1 5 2 : r LongVT: Incentivizing Thinking with Long Videos via Native Tool Calling Zuhao Yang,1,2,5, Sudong Wang,1,3,5, Kaichen Zhang,1,2,5, Keming Wu1,4,5, Sicong Leng2, Yifan Zhang1, Bo Li2,5, Chengwei Qin3, Shijian Lu(cid:66),2, Xingxuan Li(cid:66),1, Lidong Bing1 1MiroMind AI, 2NTU, 3HKUST(GZ), 4THU, 5LMMs-Lab Team Email Contact: {yang0756,zhan0564}@e.ntu.edu.sg, {swang886}@connect.hkust-gz.edu.cn Project Page: https://evolvinglmms-lab.github.io/LongVT/ Figure 1. Interleaved Multimodal Chain-of-Tool-Thought (iMCoTT). Compared to prior text-based Chain-of-Thought (CoT) reasoning, iMCoTT in our proposed LongVT can natively perform self-reflection via calling crop video(start time, end time) tool. It proposes time window after global preview, proactively fetches the corresponding short clip, rethinks based on the new evidence, and determines whether to refine or answer directly. Such tool-augmented reasoning behaviors ground each step in what is actually seen rather than blindly rephrasing in text-only CoT, which mitigates hallucination and leads to enhanced temporal localization and answer correctness."
        },
        {
            "title": "Abstract",
            "content": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videosby first skimming globally and then examining relevant clips for detailswe introduce LongVT, an end-to-end agentic framework that enables Thinking with Long Videos via interleaved Multimodal *Equal Contribution (cid:66)Corresponding Author Chain-of-Tool-Thought. Specifically, we exploit LMMs inherent temporal grounding ability as native video cropping tool to zoom in on specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained questionanswering (QA) data for the long video reasoning task, we curate and will release data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for toolintegrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through semi-automatic data pipeline with human-in-the-loop validation. With meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https: //github.com/EvolvingLMMs-Lab/LongVT. 1. Introduction Understanding long-form videos (>15 minutes) poses major challenge in multimodal intelligence [9, 13, 48, 51]. Compared with short clips, long videos contain complex event structures and require sustained comprehension across thousands of frames, supporting tasks such as video question answering (QA) [2, 23, 26, 48, 51], temporal grounding [10, 18, 34, 54, 57], and dense captioning [14, 18, 65]. These capabilities further underpin real-world applications such as soccer event spotting [25] and long-range film understanding [38]. Recent LMMs [8, 24, 46, 49, 58] exhibit promising short video reasoning, yet most rely on the R1-style paradigm [11]supervised fine-tuning (SFT) with textual Chain-of-Thought (CoT), followed by Group Relative Policy Optimization (GRPO)-based reinforcement learning (RL) [35]. Such pipelines remain largely languagecentric, limiting visual reasoning and increasing hallucinations in long-video scenarios [59]. Moreover, their uniform sampling fails to adaptively capture key visual evidence, often missing fine-grained or decisive moments critical for long-video reasoning. This motivates our central question: Can LMMs reliably reason over long videos by performing human-like visual operations to guide their reasoning? Let us consider the following scenario: testee is asked to answer the question, Which foot did the French player use to execute the volley, equalizing the score? using only the silent video of football match. Without audio, metadata, or timeline markers, the testee must rely purely on visual inspection. Based on common viewing habits, human would typically jump through the video in coarse intervals, searching for strong visual indicators of goalsuch as crowd reactions, player celebrations, referee gestures, or scoreboard updates. After locating likely scoring segment, the testee would rewind slightly and examine the surrounding frames more carefully to pinpoint the exact equalizing moment, and then verify the scoring foot using close-up shots. Notably, when we prompt two state-of-the-art proprietary LMMs (i.e., GPT-5 [42] and Gemini 2.5 Pro [5]) with the same task, the strategies they propose closely mirror this human-intuitive procedure (see Section 7). As illustrated in Figure 1, the testee, seeking to save time, avoids scanning the entire video frame by frame. Instead, they first perform coarse global skim and then zoom in on promising segments. When projected to the LMM setting, this global-to-local reasoning strategy enables models with limited context length to process extremely long videos effectively. To implement such strategy, we design interleaved Multimodal Chain-of-Tool-Thought (iMCoTT) that enables LMMs to naturally interleave reasoning with on-demand temporal retrieval via dynamically selecting and re-inspecting interested video segments. Such LMM behaviors stem from their native temporal grounding capabilities, without an auxiliary expert model or external retriever. Our designed iMCoTT enables looking again by proposing more robust time window, examining that snippet, and revising its hypothesis when necessary. Such capability helps reduce hallucinations and reveals more fine-grained details, akin to human self-reflection after realizing that an initially inspected segment was erroneous. This human-inspired Thinking with Long Videos paradigm is naturally suitable for queries that either require aggregating clues across multiple shots or hinge on brief and evidence-bearing segment within hours-long footage. Yet, the open-source community lacks training and evaluation data with such fine-grained queries: most public datasets emphasize general and high-level questions but rarely train and evaluate reasoning capability under Video Segment-In-A-Haystack setting. We address this grand challenge by constructing VideoSIAH that comprises high-quality QA pairs and tool-augmented reasoning traces. VideoSIAH comprises 247.9K samples for SFT, 1.6K samples for agentic RL, and 15.4K samples for reinforcement fine-tuning (RFT), respectively. Besides, we curate dedicated evaluation benchmark, VideoSIAH-Eval, comprising 1,280 QA pairs that have undergone human-in-the-loop validation [3], where each questions supporting evidence lies within narrow window relative to the full video duration. In this paper, we introduce LongVT, an end-to-end agentic framework that elicits LMMs ability for Thinking with Long Videos via three-stage training strategy with large-scale and high-quality Tool-augmented data from VideoSIAH. The first stage performs cold-start SFT that empowers the base LMM with three fundamental capabilities: (1) proposing precise window for relevant event(s), (2) reasoning over densely resampled frames within the window, and (3) self-correcting when the window is suboptimal. The second stage adopts agentic RL for enhancing the models generalization over open-ended QA tasks. Unlike existing work that relies on answer-only rewards for video QA and IoU rewards for temporal grounding [8, 49], we design joint answer-temporal grounding reward function that explicitly encourages exploratory rollouts with improved temporal localization while preserving answer correctness. The third stage leverages agentic RFT where the model is further optimized by utilizing filtered rollout traces distilled from its own RL-trained policy. This stage stabilizes agentic behaviors learned during RL and consolidates fine-grained temporal localization and multi-step reasoning. The contributions of our work can be summarized in three major aspects. First, we introduce an end-to-end agentic paradigm that natively interleaves multimodal toolaugmented CoT with on-demand clip inspection over hourslong videos, thereby enabling LMMs to perform more effective and reliable long-video reasoning. Second, to facilitate training and evaluation of evidence-sparse long-video reasoning, we construct scalable data pipeline that produces diverse and high-quality QAs and tool-integrated reasoning traces, and dedicated benchmark under video segment-in-a-haystack setting. Third, we conduct comprehensive ablations on data recipes, training strategies, and design choices, together with extensive analysis on training dynamics, establishing state-of-the-art baseline for Thinking with Long Videos with invaluable insights. 2. Related Work RL-Based Multimodal Reasoning. Inspired by large reasoning models such as OpenAI o1 [17] and DeepSeekR1 [11], recent studies extend GRPO-style RL from textonly reasoning to multimodal domains. In vision, methods enhance reasoning for image QA [15, 20, 31, 61], grounding [7, 28, 36], and segmentation [27]. Video-centric approaches further tackle temporal reasoning tasks such as video QA [8, 46], temporal grounding [49], and spatiotemporal grounding [24], including recent efforts to scale RL to long videos [4, 45]. Audio-aware methods similarly apply RL to audio QA [21, 50] and broader omnimodal reasoning [64]. Collectively, these works demonstrate that RLbased reasoning improves cross-modal understanding. Tool-Augmented Agentic LMMs. Complementing RLbased reasoning, another line of research incorporates tools to incentivize LMMs agentic capabilities. For images, recent methods [39, 52, 56, 63] interleave pixel-level operations (e.g., zooming in, drawing auxiliary lines, generative imagery) to reason over finer details while reducing hallucinations. For videos, VITAL [59] shows that tool-augmented RL improves video QA and temporal grounding. Our approach differs from VITAL in two key aspects. First, LongVT targets video segment-in-a-haystack reasoning and contributes large-scale, high-quality dataset and benchmark. VideoSIAH not only triggers tool-integrated reasoning but also reveals emergent human-like self-reflection in long-form video understanding. Second, we propose three-stage closed-loop training paradigm combining SFT cold start, RL, and dedicated RFT stage leveraging highquality rollout traces for iterative self-refinement. Moreover, unlike prior work relying on multi-task objectives [8, 24] or explicit tool rewards [59, 63], LongVT shows that single-task RL with decoupled temporal-grounding reward can still achieve state-of-the-art performance in long video reasoning. 3. VideoSIAH: Fine-Grained Data Suite for Evidence-Sparse Long-Video Reasoning Long-video reasoning presents fundamentally different challenge from previous video QA settings: LMMs must locate sparse, fine-grained, and causally decisive moments embedded within hours-long content. However, existing tool-augmented LMMs [39, 59] are mostly trained with coarse-grained and clip-level data. This mismatch leaves modern LMMs lacking the supervision needed to learn how temporal hypotheses are formed, verified, or reviseda critical yet underexplored capability for agentic long-video reasoning. Moreover, most existing video understanding benchmarks [9, 48, 51] only offer multiple-choice QAs, which can be solved without genuine temporal grounding and are vulnerable to dataset leakage or shortcut exploitation. Evidence and discussion can be found in Section 8. To fill this gap, we introduce VideoSIAH, large-scale, diverse, and high-quality data suite that serves collectively as training dataset capturing the reasoning dynamics required for segment-in-a-haystack question-answering, and fine-grained evaluation benchmark, VideoSIAH-Eval, with human-in-the-loop validation for long-video open-ended question-answering. 3.1. Data Pipeline As illustrated in Figure 2, VideoSIAH is curated through semi-automatic, human-in-the-loop pipeline that constructs temporally grounded reasoning traces aligned with human cognitive processes during evidence-sparse long-video reasoning. We begin with automatic scene detection on long videos and merge consecutive segments shorter than 10 seconds to obtain semantically stable units for downstream QA generation. For each segment, Qwen2.5-VL-72B [1] generates detailed descriptions capturing salient objects, spatial relations, and evolving events. These captions serve as the semantic basis for generating temporally grounded QA pairs. Initial QAs are created from the captions, covering temporal events, spatial layouts, motion, object attributes, and scene transitions, ensuring broad coverage at scale. To ensure quality, we employ two filtering stages: (1) text-based QA filtering, which removes low-quality or illposed QAs (e.g., answer leakage) using linguistic heuristics and model agreement; and (2) multimodal QA filtering, where GLM-4.5V [12] verifies answer consistency against the video segment, eliminating hallucinated and visually unsupported claims. Annotator feedback further refines prompting rules for QA generation, filtering, and iMCoTT construction. This prompt-feedback refinement loop boosts 3 Figure 2. Data Pipeline of VideoSIAH. We construct semi-automatic data pipeline that integrates several state-of-the-art LMMs [1, 5, 12, 43] to sequentially perform long video segmentation, video clip captioning, segment-in-a-haystack QA generation, cross-modal QA filtering, and iMCoTT generation. Icons with human silhouettes denote human-in-the-loop validation, where annotators inspect small set of representative failures to refine prompting rules for QA generation, QA filtering, and iMCoTT generation. Note that iMCoTT traces are generated only for the cold-start SFT stage, whereas RL training operates solely on the filtered QA pairs. reliability without heavy manual annotation, yielding highfidelity, temporally grounded, and scalable data. 3.2. Dataset Curation SFT Data Curation. Our SFT data is constructed from (1) tool-augmented multi-round three major categories: data, (2) image reasoning data, and (3) video reasoning data, with the goal of enhancing both tool-calling capability and general reasoning performance. We curate toolaugmented QA pairs following the pipeline illustrated in Figure 2. When processing hours-long videos, we find that sparsely sampled frames from single round often fail to capture the correct temporal segment, which makes multiround tool-calling necessary. To address this limitation, we generate multi-round tool-calling traces in an adaptive manner based on video length. Specifically, we define the probability of selecting sample for multi-round curation as Pmulti = 1 Lmax clip(Lvideo, Lmax, Lmin) Lmax Lmin , where Pmulti denotes the probability of choosing given data sample for multi-round generation, Lvideo represents the video length, and Lmax and Lmin are the maximum and minimum video length thresholds, respectively. The function clip(x, a, b) restricts to the range [b, a]. Videos selected under this criterion undergo multi-round data generation to ensure that longer videos receive proportionally Split Source Purpose Samples Total SFT (w/o tool) SFT (w/ tool) RL RFT LongVideo-Reason CoT [4] Video-R1 CoT [8] Image-based CoT Reasoning-augmented Open-ended QA Reasoning-augmented Video QA Reasoning-augmented Image QA Gemini-distilled iMCoTT Qwen-distilled iMCoTT Gemini-distilled QAs Self-distilled iMCoTT Tool-augmented Open-ended QA Tool-augmented Temporal Grounding Open-ended QA over Long Videos Agentic Behaviors 5, 165,575 228,835 58,022 12,766 6,395 1, 15,353 19,161 17,020 Table 1. Dataset Statistics of VideoSIAH. Our proposed dataset contains non-tool SFT data, tool-augmented SFT data, RL QAs, and self-distilled RFT traces. more tool-calling rounds, improving temporal coverage and reasoning completeness. We further gather mixture of diverse video and image reasoning datasets. RL Data Curation. For RL, the split is built from the filtered segment-in-a-haystack QA pairs produced by our data pipeline in Section 3.1. Each QA is associated with the length of its source video, and we partition candidates into several duration bands (short, medium, long). From 4 these bands, we sample length-balanced subset, ensuring the RL data is not dominated by very short clips and instead covers diverse range of video durations. On top of this length-balanced pool, we apply simple difficultyaware filter based on multi-turn tool runs. For each question, we draw rollouts of the current policy; if all trajectories answer correctly (too easy) or all fail (too hard), we discard the item and retain only questions with mixed outcomes. This focuses RL on middle band of difficulty and avoids degenerate reward signals, yielding more informative and stable optimization process. RFT Data Curation. To construct the RFT traces, we filter trajectories from early RL runs and retain only highquality cases. Concretely, trajectory is kept if the model produces the correct final answer and its predicted temporal span attains an Intersection over Union (IoU) of at least 0.3 with the annotated ground-truth window. This dual criterion enforces both semantic correctness and sufficiently accurate temporal grounding, ensuring the curated traces reflect genuinely successful long-video reasoning rather than reward hacking or lucky guesses. We then convert these filtered trajectories into supervised training examples for post-RL refinement. Training on this self-generated, wellgrounded subset provides high-precision in-distribution supervision, stabilizes optimization, and further strengthens the models grounding and tool-calling behavior beyond what SFT alone can provide. 3.3. Dataset Statistics As shown in Table 1, VideoSIAH comprises 228,835 SFT samples with normal (non-tool) CoT annotation, 19,161 tool-augmented SFT samples, and 17,020 instances used for RL and RFT. In the SFT split, the non-tool portion is dominated by long-video reasoning data [4], complemented by Video-R1-CoT [8] and smaller amount of hard image-based CoT supervision. Detailed breakdown can be found in Section 9. The tool-augmented subset combines Gemini 2.5 Flash [5] distilled CoT traces (i.e., iMCoTT) for open-ended QA and Qwen2.5-VL-72B-Instruct [1] distilled traces for temporal grounding, providing joint supervision for tool usage and timestamp prediction. For the RL split, we filtered high-quality subset of QA instances from Section 3.1. For RFT, we further select high-quality RL rollout traces for post-RL refinement, providing dense supervision that enables the policy to go well beyond the SFT-only performance ceiling. Together, these components yield large-scale and diverse dataset spanning SFT, RL, and RFT, covering high-level reasoning, temporal grounding, and tool-integrated behaviors. For evaluation, we introduce the VideoSIAH-Eval benchmark, which consists of 244 videos and 1,280 carefully filtered QA pairs via human-in-the-loop validation. This benchmark is specifically designed for long-form video reasoning with an average video duration of approximately 1,688 seconds. The duration distribution is concentrated in the 15-30 minute range (71.84%), with the remaining 28.16% of videos being longer than 30 minutes. 4. Training Strategy To make full use of the VideoSIAH and elicit robust Thinking with Long Videos behaviors, LongVT adopts three-stage training pipeline: (1) cold-start supervised finetuning, which teaches the base model to propose temporal windows, invoke video tools, and compose multimodal evidence; (2) agentic reinforcement learning, which optimizes joint answertemporal-grounding reward to refine toolusing rollouts; and (3) agentic reinforcement fine-tuning, which distills high-quality RL trajectories back into supervised data to stabilize these behaviors and consolidate longhorizon reasoning. 4.1. Cold-Start Supervised Fine-Tuning As shown in Figure 3-(b), our preliminary RL experiments using Qwen2.5-VL-7B [1] as the baseline model reveal that the model fails to improve during RL and ultimately collapses with continued training. This analysis of training dynamics indicates two major deficiencies of the base LMM: (1) the inability to correctly localize the relevant temporal window within long video, and (2) insufficient reasoning capability when integrating tool outputs. We also present straightforward example in Figure 14 that illustrates the necessity of cold-start SFT stage. These limitations highlight that the models native tool-calling abilities are too weak for direct RL training. Therefore, cold-start stage is indispensable for establishing reliable foundation. After applying SFT cold start, the models tool-calling activeness improves substantially and continues to increase steadily during RL, supported by results in Table 3. 4.2. Agentic Reinforcement Learning In this stage, we treat the model as tool-using agent that decides when to inspect the video, how long to crop, and how to integrate the retrieved evidence into its reasoning. We employ GRPO [35] to achieve this objective. In addition, we introduce three-part reward modeling that jointly optimizes answer accuracy, format compliance, and temporal grounding precision of sampled trajectories, namely, joint answer-temporal grounding reward. Prior work [8, 49] typically targets either answer correctness or time alignment in isolation. We take further step toward unifying these signals within single reward function for open-ended longvideo QA. This coupling ties answer selection to where the evidence lies in time, improving final-answer correctness and promoting more effective tool use at inference, with more reliable and precise timestamp proposals. 5 Figure 3. Ablations on Reward Design. The left panel shows training dynamics under different accuracy and time rewards, and the right panel shows the effect of tool-call reward on tool usage. Answer Accuracy. Let be the number of sampled rollouts in group. For the k-th rollout (k {1, . . . , K}), let ˆa(k) denote its generated answer and let denote the ground-truth answer. We employ LLM-as-a-Judge [55] to obtain categorical verdict (k) = JudgeLLM (cid:0)ˆa(k), a(cid:1) {F, P, I}, where = fully consistent (semantically equivalent to a), = partially consistent (contains some correct information but is incomplete or imprecise), and = inconsistent (incorrect or contradictory)."
        },
        {
            "title": "The accuracy reward is then defined as the normalized",
            "content": "score R(k) acc = 1, 0.5, 0, if (k) = F, if (k) = P, if (k) = I. Format Compliance. Let y(k) denote the full textual output of the k-th rollout and let be the required output schema. Define R(k) format = (cid:40) 1, 0, if y(k) matches S, otherwise. Temporal Overlap. Following previous temporal grounding work [8, 24], we use standard temporal IoU as the reward function for temporal localization. For prediction s, [ts, te] and ground truth [t e], [ts, te] [t [ts, te] [t IoU = s, s, e] e] ."
        },
        {
            "title": "We set",
            "content": "R(k) time = IoU(k). Hence R(k) time = 1 only when the predicted span matches the ground-truth interval exactly, and R(k) time = 0 when there is no temporal overlap. This simple form proved sufficient to drive grounded cropping and tighter timestamp proposals during tool use. 6 Overall Reward. R(k) = R(k) acc + R(k) format + R(k) time. 4.3. Agentic Reinforcement Fine-tuning Recent work [40] argues that RFT has become key ingredient for equipping large language models and their multimodal counterparts with strong reasoning capabilities, since it optimizes sequence-level rewards that directly reflect task success rather than token-level likelihood, and consistently improves performance across diverse modalities and tasks. Motivated by these findings, we further leverage RFT to stabilize models agentic behaviors and consolidate multimodal reasoning. Specifically, we select high-quality cases from early RL rollouts that exhibit both accurate temporal localization and coherent reasoning toward the final answer, and incorporate these trajectories back into the supervised fine-tuning curriculum as privileged and self-distilled demonstrations. Empirically (see Section 5.3), we find that learning from these in-distribution high-quality trajectories helps the model internalize robust grounding and toolcalling patterns complementary to large-scale agentic RL, effectively guiding optimization toward policies that better align answer accuracy, temporal grounding, and tool usage. 4.4. Overall Framework As visualized in Figure 4, LongVT operates in an iterative hypothesis-verification cycle. This behavioral capability is incentivized via cold-start SFT, enabling the model to skim global frames and proactively invoke the crop video tool to resample fine-grained evidence. In cases where the initial retrieval (e.g., at T1) proves insufficient, the model leverages learned self-correction to reinvoke the tool (e.g., at T2) with refined parameters. Crucially, this entire decision-making trajectory is consolidated via agentic RL, which optimizes the policy against the joint answer-temporal grounding reward (Racc + Rformat + Rtime), Figure 4. Overall Framework of LongVT. Our approach processes long-form videos in human-like two-stage manner. Specifically, LongVT is augmented with interleaved Multimodal Chain-of-Tool-Thought (iMCoTT): first performs global skim over sampled video frames to form coarse hypothesis about when evidence likely occurs; then invokes native video tool crop video(start time, end time) to resample finer-grained frames from short clip via hypothesized window and reasons again. Our model itself determines whether to directly answer after one turn (T1) or continue for multiple turns (up to T5) with self-reflection. During reinforcement learning, we jointly optimize answer correctness (Racc), clean formatting (Rformat), and precise temporal grounding (Rtime). enhancing the models generalization ability to further align with human-like verification strategies. 5. Experiments 5.1. Experimental Setup We utilize Qwen2.5-VL-7B [1] as the baseline model in all experiments. We report the performance of three LongVT variants based on their training stages against Qwen2.5-VL7B and other open-source video-centric LMMs including Video-R1-7B [8], VideoRFT-7B [46], and Video-Thinker7B [47] plus proprietary LMMs such as GPT-4o [16] and Gemini 1.5 Pro [41]. Note that we do not include direct comparisons to the concurrent tool-augmented videocentric LMM [59], since its model checkpoints are not publicly available, which hinders fair and reproducible experiments. We evaluate all models on four long-video understanding and reasoning benchmarks, namely VideoMME [9], VideoMMMU [13], LVBench [48], and our self-curated VideoSIAH-Eval, leveraging unified evaluation framework [60] for fair comparison. Results are reported under two frame-sampling regimes: Sparse Frame Sampling (64 uniformly sampled video frames) and Dense Frame Sampling (512 or 768 uniformly sampled frames; the better result among the two is reported). Reasoning Prompt indicates whether standard reasoning-style prompt () or direct question-answering prompt () is applied; Tool Calling denotes whether native tool calling is enabled () or disabled () in the prompt. More implementation details can be found in Section 12. 5.2. Main Results As shown in Table 2, our approach achieves new state-ofthe-art among open-source video-centric LMMs under both sparse and dense frame sampling settings. When evaluating at 64 frames, LongVT-7B-RL slightly surpasses the best existing open-source baseline. Under dense frame sampling, both LongVT-7B-RL and LongVT-7B-RFT yield more dominant performance, outperforming existing methods by large margin. On the challenging VideoSIAH-Eval, which involves open-ended QAs that require the retrieval of finegrained evidence from hours-long videos, LongVT-7B-RFT reaches 42.0, outperforming the second-best model by 6 points. This confirms that LongVT achieves stronger longvideo reasoning and exhibits an emergent ability to invoke native tools for temporal localization. Notably, the gap between open-source and proprietary LMMs has narrowed substantially: LongVTs best-performing checkpoint lies within roughly four points of GPT-4o on average, marking significant step forward in long-video reasoning capability among open-source LMMs. 7 Model Reasoning Tool VideoMME (1018 sec) [9] VideoMMMU (506 sec) [13] LVBench [48] VideoSIAH-Eval Average Prompt Calling w/ subtitle adaptation comprehension perception (4101 sec) (1688 sec) Score GPT-4o [16] Gemini 1.5 Pro [41] Qwen2.5-VL-7B [1] Video-R1-7B [8] VideoRFT-7B [46] Video-Thinker-7B [47] LongVT-7B-SFT (Ours) LongVT-7B-RL (Ours) Qwen2.5-VL-7B [1] Video-R1-7B [8] VideoRFT-7B [46] Video-Thinker-7B [47] LongVT-7B-SFT (Ours) LongVT-7B-RL (Ours) LongVT-7B-RFT (Ours) Proprietary LMMs 77.2 81.3 66.0 59.0 62.0 53.3 Open-Source LMMs with Sparse Frame Sampling 62.6 61.0 60.9 61.0 12.5 66.1 37.3 36.3 36.7 34.3 37.7 32. 28.0 40.7 42.0 44.7 46.0 44.7 Open-Source LMMs with Dense Frame Sampling 64.3 60.5 49.2 60.8 64.9 66.1 67.0 35.7 37.3 37.7 37.7 32.3 37.7 35.7 44.3 38.7 40.7 42.7 42.0 42.3 43.7 55.7 49.3 36.7 52.3 53.0 53.0 58.3 50.0 56.7 46.3 48.7 55.3 49.7 56.3 56.7 30.8 33.1 30.7 37.2 34.7 52.2 36.0 37.8 40.9 40.1 18.7 54.3 41.1 41.4 41.3 17.4 - 28.1 27.9 26.5 10.4 26.8 31.0 33.8 33.1 26.9 6.6 34.8 35.9 42. 51.5 55.2 37.2 42.6 42.3 42.6 36.2 43.7 46.0 42.7 37.0 42.9 44.1 46.6 47.7 Table 2. Performance Comparison with Existing Video-Centric LMMs across Various Long Video Understanding and Reasoning Benchmarks. The best and second-best result among open-source models in each column is marked in bold and underlined, respectively. The numbers with denote the average video duration of each benchmark. indicates results sourced from official reports [9, 13, 48]. Setting VideoMME [9] VideoMMMU [13] LVBench [48] VideoSIAH-Eval Average w/ subtitle adaptation comprehension perception test test Score SFT w/o self-curated iMCoTT SFT w/ self-curated iMCoTT (LongVT-7B-SFT) RL w/o self-curated QAs RL w/ self-curated QAs (LongVT-7B-RL) SFT only (LongVT-7B-SFT) RL only SFT+RL (LongVT-7B-RL) SFT+RL+RFT (LongVT-7B-RFT) 8.4 64.9 55.1 66. 64.9 52.7 66.1 67.0 Data Recipe 33.6 32.3 30.6 37.7 Training Stage 32.3 35.33 37.7 35.7 41.6 42.0 42.0 42. 42.0 43.0 42.3 43.7 46.0 49.7 45.6 56.3 49.7 55.1 56.3 56.7 15.1 41.1 38.4 41.4 41.1 37.1 41.4 41.3 4.1 34.8 30.8 35. 34.8 28.2 35.9 42.0 RL w/o Decoupled Reward RL w/ Recall Reward RL w/ IoU Reward Decoupled Temporal Grounding Reward Charades-STA [10] IoU@0.3 IoU@0. IoU@0.7 mIoU 31.5 32.0 41.0 19.9 20.4 25.8 9.1 9.6 11.7 21.2 21.6 27. 24.8 44.1 40.4 46.6 44.1 41.9 46.6 47.7 Average Score 20.4 20.9 26.4 Table 3. Ablation Studies. The best result among each comparison group is in bold. We examine Data Recipe where we remove selfcurated iMCoTTs during SFT or self-curated QAs during RL to test the dependence on fine-grained supervision; Training Stage where SFT, RL, and RFT are ablated individually and in combination to test their complementary effect; Decoupled Temporal Grounding Reward where Recall-based and IoU-based reward functions are compared, together with variant without decoupled temporal grounding reward. 5.3. Ablation Studies Fine-grained reasoning data matters. As shown in Table 3, our self-curated training data plays crucial role in shaping the models reasoning behavior when dealing with In the SFT stage, removing the selflong-form videos. curated iMCoTTs (SFT w/o self-curated iMCoTT) leads to consistent performance drop in long-form video understanding. In addition, when self-curated QAs are removed during RL (RL w/o self-curated QAs), models performance drops quickly on VideoSIAH-Eval, with lower answer accuracy, weaker temporal localization, and less systematic tool use, which can also be observed in Figure 3-(b). Recall encourages coverage; IoU demands precision. As shown in Figure 3-(a), using Recall as the reward function during RL presents drawback: the policy can enlarge the predicted span to envelop the ground-truth interval, which monotonically raises the Recall-based score while ignoring boundary quality. This plateau in the curve of Recall Accu8 racy Score further validates our hypothesized reward hacking. Quantitatively, in the reward-choice rows of Table 3, IoU-rewarded training outperforms Recall on the temporal grounding benchmark [10], while Recall is only marginally above the RL w/o Decoupled Reward variant, pointing to IoUs tighter handling of boundary agreement. Optimizing with IoU provides smooth shaping over overlap and implicitly penalizes span inflation via the union term, yielding better-aligned boundaries and more disciplined tool use. Is tool reward really necessary? As shown in Figure 3-(b), the Qwen2.5-VL-7B baseline collapses to near-zero tool calls after training in both configurations (w/ and w/o tool reward), indicating that the model does not internalize the tools function. After performing cold-start SFT to obtain LongVT-7B-SFT, tool-call frequency rises during training under both configurations and accuracy improves in tandem. Hence, the tool reward is not required for basic competence: once SFT grounds the tools semantics, the model learns when to invoke the tool and when to abstain. Moreover, introducing the tool reward brings little benefit. In the later training stage, the configuration without the tool reward even exhibits slightly higher tool-use frequency, indicating that the binary bonus does not encourage usage and may suppress exploration, while accuracy remains essentially unchanged. Given these observations, we discard the tool reward in our final recipe and rely on the standard accuracy, format, and decoupled IoU reward modeling. SFT builds competence; RL optimizes decisions; RFT stabilizes behaviors. We ablate each training stage individually and in combination, finding that strong performance emerges only with the full three-stage pipeline. As shown in Figure 3-(b), removing SFT leaves the model with poor it cannot reliably invoke crop video tool-use ability: tool or integrate cropped evidence into its reasoning. Consistently, the RL-only variant achieves the lowest scores on all four benchmarks  (Table 3)  and exhibits behavioral inconsistencies during trainingoften following surface instructions and becoming confused by the returned crop rather than using it as supporting evidence. SFT teaches the intended tool-use paradigmselecting temporal windows, inspecting their content, and incorporating the resulting evidence into the final answer. However, SFT remains imitation-driven [22]: it fits demonstrated formats, suffers from exposure bias, and fails to generalize under distribution shift. On long-video QA, SFT alone yields only modest gains. We therefore introduce RL with temporal-grounding reward, optimized via GRPO. RL enables the policy to learn when to inspect, how long to crop, and how to integrate retrieved evidence. This stage pushes performance beyond the supervised ceiling on heldout videos and unseen question templates  (Table 3)  , aligning with prior findings that GRPO improves reasoning and generalization [11]. Finally, RFT distills high-reward trajectories back into the supervised corpus, providing additional performance gains. On VideoSIAH-Eval, it surpasses the RL-only plateau by substantial margin and yields our bestperforming model, while still delivering consistent improvements on other benchmarks. This demonstrates that consolidating successful rollouts is essential for fully realizing the benefits of temporal-grounding feedback. 6. Conclusion In this work, we present LongVT, an end-to-end agentic framework that enables LMMs to reliably reason over long videos. By interleaving multimodal tool-augmented CoT with on-demand temporal inspection, LongVT transforms long-video understanding from passive frame consumption into active, evidence-seeking reasoning. Supported by selfcurated VideoSIAH, large-scale, fine-grained data suite built specifically for evidence-sparse long-video reasoning tasks, our proposed three-stage training pipeline yields substantial and consistent improvements compared to existing strong baselines."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 3, 4, 5, 7, 8, 1, 2 [2] Mu Cai, Reuben Tan, Jianrui Zhang, Bocheng Zou, Kai Zhang, Feng Yao, Fangrui Zhu, Jing Gu, Yiwu Zhong, Yuzhang Shang, et al. Temporalbench: Benchmarking finegrained temporal understanding for multimodal video models. arXiv preprint arXiv:2410.10818, 2024. 2 [3] Maya Cakmak and Andrea Thomaz. Eliciting good teaching from humans for machine learners. Artificial Intelligence, 217:198215, 2014. 2 [4] Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, Xiaojuan Qi, et al. Scaling rl to long videos. arXiv preprint arXiv:2507.07966, 2025. 3, 4, 5 [5] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 2, 4, 5, 1 [6] Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: An early exploration to complex vision-language reasoning via iterative self-improvement. arXiv preprint arXiv:2503.17352, 2025. [7] Yue Fan, Xuehai He, Diji Yang, Kaizhi Zheng, Ching-Chen Kuo, Yuting Zheng, Sravana Jyothi Narayanaraju, Xinze Guan, and Xin Eric Wang. Grit: Teaching mllms to think with images. In Advances in Neural Information Processing Systems, 2025. 3 9 [8] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. 2, 3, 4, 5, 6, 7, 8 [9] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2410824118, 2025. 2, 3, 7, 8, 5 [10] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall: Temporal activity localization via language query. In Proceedings of the IEEE international conference on computer vision, pages 52675275, 2017. 2, 8, 9 [11] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 2, 3, [12] Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, et al. Glm-4.5v and glm-4.1v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv preprint arXiv:2507.01006, 2025. 3, 4 [13] Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei Liu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos. arXiv preprint arXiv:2501.13826, 2025. 2, 7, 8, 5 [14] Gabriel Huang, Bo Pang, Zhenhai Zhu, Clara Rivera, and Radu Soricut. Multimodal 2gpretraining for dense video captioning. arXiv preprint arXiv:2011.11760, 2020. 2 [15] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. 3 [16] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 7, 8 [17] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. 3 [18] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In Proceedings of the IEEE international conference on computer vision, pages 706715, 2017. [19] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th symposium on operating systems principles, pages 611626, 2023. 5 [20] Sicong Leng, Jing Wang, Jiaxi Li, Hao Zhang, Zhiqiang Hu, Boqiang Zhang, Yuming Jiang, Hang Zhang, Xin Li, Lidong Bing, et al. Mmr1: Enhancing multimodal reasoning with variance-aware sampling and open resources. arXiv preprint arXiv:2509.21268, 2025. 3 [21] Gang Li, Jizhong Liu, Heinrich Dinkel, Yadong Niu, Junbo Zhang, and Jian Luan. Reinforcement learning outperforms supervised fine-tuning: case study on audio question answering. arXiv preprint arXiv:2503.11197, 2025. 3 [22] Jiaxiang Li, Siliang Zeng, Hoi-To Wai, Chenliang Li, Alfredo Garcia, and Mingyi Hong. Getting more juice out of the sft data: Reward learning from human demonstration improves sft for llm alignment. In Advances in Neural Information Processing Systems, pages 124292124318, 2024. 9 [23] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195 22206, 2024. 2 [24] Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal arXiv preprint perception via reinforcement fine-tuning. arXiv:2504.06958, 2025. 2, 3, 6 [25] Yixuan Li, Changli Tang, Jimin Zhuang, Yudong Yang, Guangzhi Sun, Wei Li, Zejun Ma, and Chao Zhang. Improving llm video understanding with 16 frames per second. arXiv preprint arXiv:2503.13956, 2025. [26] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv preprint arXiv:2403.00476, 2024. 2 [27] Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, and Jiaya Jia. Seg-zero: Reasoning-chain guided segmentation via cognitive reinforcement. arXiv preprint arXiv:2503.06520, 2025. 3 [28] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visualrft: Visual reinforcement fine-tuning. In Proceedings of the IEEE international conference on computer vision, 2025. 3 [29] LMMs-Lab. Lmms engine: simple, unified multimodal framework for pretraining and finetuning., 2025. 4 [30] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. 4 [31] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, et al. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. [32] Zhanfeng Mo, Xingxuan Li, Yuntao Chen, and Lidong Bing. Multi-agent tool-integrated policy optimization. arXiv preprint arXiv:2510.04678, 2025. 6 [33] Runqi Qiao, Qiuna Tan, Peiqing Yang, Yanzi Wang, Xiaowan Wang, Enhui Wan, Sitong Zhou, Guanting Dong, Yuchen Zeng, Yida Xu, et al. We-math 2.0: versatile 10 mathbook system for incentivizing visual mathematical reasoning. arXiv preprint arXiv:2508.10433, 2025. 3 [34] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1431314323, 2024. 2 [35] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 2, 5, [36] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. 3 [37] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pages 12791297, 2025. 4 [38] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1822118232, 2024. 2 [39] Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning. arXiv preprint arXiv:2505.15966, 2025. 3 [40] Haoyuan Sun, Jiaqi Wu, Bo Xia, Yifu Luo, Yifei Zhao, Kai Qin, Xufei Lv, Tiantian Zhang, Yongzhe Chang, and Xueqian Wang. Reinforcement fine-tuning powers reasoning capability of multimodal large language models. arXiv preprint arXiv:2505.18536, 2025. 6 [41] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 7, [42] OpenAI Team. Introducing gpt-5. https://openai. com/index/introducing-gpt-5/, 2025. 2, 1 [43] OpenAI Team. https : / / openai.com/index/thinkingwithimages/, 2025. 4 Thinking with images. [44] Qwen Team. Qwen3-vl: Sharper vision, deeper thought, broader action. https://qwen.ai/blog?from= research . latest - advancements - list & id = 99f0335c4ad9ff6153e517418d48535ab6d8afef, 2025. 1, 2 [45] Shulin Tian, Ruiqi Wang, Hongming Guo, Penghao Wu, Yuhao Dong, Xiuying Wang, Jingkang Yang, Hao Zhang, Hongyuan Zhu, and Ziwei Liu. Ego-r1: Chain-of-toolthought for ultra-long egocentric video reasoning. arXiv preprint arXiv:2506.13654, 2025. 3 11 [46] Qi Wang, Yanrui Yu, Ye Yuan, Rui Mao, and Tianfei Zhou. Videorft: Incentivizing video reasoning capability in mllms via reinforced fine-tuning. arXiv preprint arXiv:2505.12434, 2025. 2, 3, 7, 8, 5 [47] Shijian Wang, Jiarui Jin, Xingjian Wang, Linxin Song, Runhao Fu, Hecheng Wang, Zongyuan Ge, Yuan Lu, and Xuelian Cheng. Video-thinker: Sparking thinking with videos via reinforcement learning. arXiv preprint arXiv:2510.23473, 2025. 7, 8, 5 [48] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao Dong, et al. Lvbench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024. 2, 3, 7, 8, [49] Ye Wang, Ziheng Wang, Boshen Xu, Yang Du, Kejun Lin, Zihan Xiao, Zihao Yue, Jianzhong Ju, Liang Zhang, Dingyi Yang, et al. Time-r1: Post-training large vision language model for temporal video grounding. arXiv preprint arXiv:2503.13377, 2025. 2, 3, 5 [50] Cheng Wen, Tingwei Guo, Shuaijiang Zhao, Wei Zou, and Xiangang Li. Sari: Structured audio reasoning via curriculum-guided reinforcement learning. arXiv preprint arXiv:2504.15900, 2025. 3 [51] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. In Advances in Neural Information Processing Systems, pages 2882828857, 2024. 2, 3 [52] Junfei Wu, Jian Guan, Kaituo Feng, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, and Tieniu Tan. Reinforcing spatial reasoning in vision-language models with interwoven thinking and visual drawing. arXiv preprint arXiv:2506.09965, 2025. 3 [53] Guowei Xu, Peng Jin, Ziang Wu, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language models reason step-by-step. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2087 2098, 2025. 3 [54] Antoine Yang, Arsha Nagrani, Ivan Laptev, Josef Sivic, and Cordelia Schmid. Vidchapters-7m: Video chapters at scale. Advances in Neural Information Processing Systems, 36: 4942849444, 2023. [55] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 6, 5 [56] Zhongyu Yang, Junhao Song, Siyang Song, Wei Pang, and Yingfang Yuan. Mermaid: Multi-perspective self-reflective agents with generative augmentation for emotion recogniIn Proceedings of the 2025 Conference on Empirition. cal Methods in Natural Language Processing, pages 24650 24666, 2025. 3 [57] Zuhao Yang, Yingchen Yu, Yunqing Zhao, Shijian Lu, and Song Bai. Timeexpert: An expert-guided video llm for video In Proceedings of the IEEE/CVF Intemporal grounding. ternational Conference on Computer Vision, pages 24286 24296, 2025. 2 [58] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025. 2, 3 [59] Haoji Zhang, Xin Gu, Jiawen Li, Chixiang Ma, Sule Bai, Chubin Zhang, Bowen Zhang, Zhichao Zhou, Dongliang He, and Yansong Tang. Thinking with videos: Multimodal toolaugmented reinforcement learning for long video reasoning. arXiv preprint arXiv:2508.04416, 2025. 2, 3, 7 [60] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, et al. Lmms-eval: Reality check on the evaluation of large multimodal models. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 881916, 2025. 7, [61] Kaichen Zhang, Keming Wu, Zuhao Yang, Kairui Hu, Bin Wang, Ziwei Liu, Xingxuan Li, and Lidong Bing. Openmmreasoner: Pushing the frontiers for multimodal reaarXiv preprint soning with an open and general recipe. arXiv:2511.16334, 2025. 3 [62] Lianmin Zheng, Zhiqiang Xie, Liangsheng Yin, Chuyue Livia Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph Gonzalez, et al. Sglang: Efficient execution of structured language information model programs. processing systems, pages 6255762583, 2024. 4 In Advances in neural [63] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing thinking with images via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. 3 [64] Hao Zhong, Muzhi Zhu, Zongze Du, Zheng Huang, Canyu Zhao, Mingyu Liu, Wen Wang, Hao Chen, and Chunhua Shen. Omni-r1: Reinforcement learning for omnimodal arXiv preprint reasoning via two-system collaboration. arXiv:2505.20256, 2025. [65] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI conference on artificial intelligence, 2018. 2 12 LongVT: Incentivizing Thinking with Long Videos via Native Tool Calling"
        },
        {
            "title": "Outline",
            "content": "This Supplementary Material complements the main paper, providing comprehensive experimental details, indepth analyses of training dynamics, and extensive qualitative visualizations. The content is organized as follows: Strategic Alignment & Motivation. We first demonstrate the conceptual alignment between LongVT and state-of-the-art proprietary large multimodal models (LMMs) in Section 7. Subsequently, we present rigorous data contamination study in Section 8 to underscore the necessity of our proposed VideoSIAH-Eval benchmark, followed by detailed statistics of the curated dataset in Section 9. Formulation & Training Dynamics. We elaborate on the theoretical formulations of our training objectives in Section 10 for both supervised fine-tuning (SFT) and reinforcement learning (RL). Crucially, in Section 11, we visualize the economy of thinkinga distinct evolutionary trajectory where the model learns to internalize tool usage. Section 12 then provides the exact hyperparameters and infrastructure details for reproducibility. Efficiency & Qualitative Analysis. We report detailed inference latency comparison in Section 13, countering the intuition that multi-turn agentic frameworks are inherently slower. In Section 14, we provide prompt templates, diverse qualitative examples, and workflow demonstration, while Section 15 analyzes specific failure modes to highlight the importance of the cold-start training stage. Discussion. Finally, we discuss the architectural limitations and future multi-agent directions in Section 16, followed by discussion on the broader impact and ethical considerations in Section 17 and Section 18, respectively. 7. LongVT Performs Human-Aligned Thinking like Leading Proprietary LMMs The core philosophy of our proposed interleaved Multimodal Chain-of-Tool-Thought (iMCoTT) entails globalto-local thinking pattern: the model first performs coarse skim to formulate hypothesis, and subsequently invokes the native crop video() tool to inspect specific temporal windows for fine-grained verification. While this design was inspired by human intuition, we observe striking convergence between our approach and the reasoning behaviors emerging in state-of-the-art proprietary LMMs when they are prompted to perform fine-grained analysis. To validate this alignment, we queried two leading models, Gemini 2.5 Pro [5] and GPT-5 Thinking [42], regarding their optimal strategies for analyzing fine-grained video details. As illustrated in Figure 5a, Gemini 2.5 Pro explicitly advocates for two-stage process: Step 1: Coarse Scan to efficiently locate the general event (e.g., searching for scoreboard changes or crowd reactions), followed by Step 2: Fine Scan to isolate the exact moment and verify details (e.g., scrubbing back 30-60 seconds). This directly mirrors the workflow of our proposed LongVT, where the Coarse Scan corresponds to our global preview stage, and the Fine Scan is functionally identical to our agentic crop video() tool calling. Similarly, Figure 5b demonstrates that the GPT-series model adopts hierarchical CoarseMediumFine search strategy. These examples confirm that the Thinking with Long Videos paradigm we propose in this work is natural and necessary evolution for reliable long-form video reasoning, given that such human-aligned reasoning capabilities are currently exclusive to top-tier proprietary models. 8. What Motivates VideoSIAH? Unveiling the Data Contamination in Qwen-VL Series With the rapid advancements of LMMs, model performance on various benchmarks has steadily improved. However, the black-box nature of training data raises critical question: Do these improvements reflect genuine reasoning capability, or are they partly due to the model memorizing the benchmark samples? To investigate this, we conduct rigorous contamination study on the Qwen-VL series [1, 44] across two probing settings: (1) No Visual, where we feed the text prompt without video frames to test for direct memorization; (2) Rearranged Choices, where we randomize the mapping between option labels and their textual content (e.g., assigning the original answer to B) for multiplechoice questions (MCQs) to detect label memorization. Our experimental results reveal significant vulnerabilities in existing benchmarks and highlight the necessity of our proposed VideoSIAH-Eval: Observation 1: No Visual Performance Indicates Severe Leakage in Existing Benchmarks. As shown in Table 4, both Qwen2.5-VL and Qwen3-VL achieve remarkably high scores on VideoMME and VideoMMMU even without seeing any video frames. Notably, for VideoMME, we specifically evaluate without subtitles to ensure there is no textual leakage, yet Qwen2.5VL still achieves 40.1%, far exceeding random guessing (25%) for such four-option MCQs. Similar patterns of potential data leakage are observed on VideoMMMU. While the No Visual scores of 38.3% (Comprehension) and 39.3% (Perception) might appear similar to VideoMME, they are statistically more improbable given the dataset 1 (a) Watching Strategy of Gemini 2.5 Pro. (b) Watching Strategy of GPT-5 Thinking. Figure 5. Comparison of Watching Strategies Proposed by Gemini 2.5 Pro [5] and GPT-5 Thinking [42]. Best viewed when zoomed in. Setting Original No Visual Rearranged Choices Original No Visual Rearranged Choices VideoMME [9] VideoMMMU [13] VideoSIAH-Eval w/o subtitle adaptation comprehension perception Qwen2.5-VL-7B-Instruct [1] 35.7 27.0 31.6 44.3 38.3 40.3 Qwen3-VL-8B-Instruct [44] 40.7 35.1 38.7 60.3 39.3 47. 64.3 40.1 56.0 69.3 44.1 69.0 56.7 39.3 67.0 71.3 46.7 69.3 test 33.8 12.7 - 46.6 0.00 - Table 4. Contamination Tests for Qwen-VL Series on Long Video Understanding and Reasoning Benchmarks. Results are reported across different perturbation settings. The best result in each block column is in bold, and the second-best is underlined. The VideoSIAHEval column shows - entries for Rearranged Choices since our proposed benchmark is fully open-ended QA, where random optionanswer mapping is not applicable. Note that adaptation is evaluated exclusively on multiple-choice questions. composition. Our statistics reveal that these subsets are overwhelmingly dominated by MCQs with 10 options (e.g., 286 out of 300 for Comprehension and 279 out of 300 for Perception), implying random guessing baseline of only 1016%. The fact that the model achieves scores significantly above this threshold absent any visual context indicates high probability of benchmark memorization. In contrast, performance on VideoSIAH-Eval drops significantly in the No Visual setting. Specifically, Qwen3VL collapses to score of 0.00. Upon manual inspection, we find that without visual grounding, the model generates repetitive code or refusal messages, which is the expected behavior for clean and non-contaminated benchmark. Observation 2: Rearranged Choices Reveals Overfitting to Option Patterns. For MCQ-based benchmarks, we observe distinct performance drops when answer choices are rearranged. For instance, Qwen2.5-VL drops from 64.3 to 56.0 on VideoMME. This indicates that they heavily rely on memorizing specific option mappings (e.g., the answer to this question is usually A) rather than understanding the content. Since VideoSIAH-Eval utilizes fully open-ended QA format, it is inherently immune to this type of option hacking, providing more robust assessment of the models capabilities. 2 These findings confirm that existing benchmarks are compromised by data contamination (high No Visual scores), option bias (sensitive to Rearranged Choices). This motivates the introduction of VideoSIAH-Eval, which ensures: (1) Zero leakage as verified by the 0.00 blind score, and (2) Immunity to option bias via open-ended QA format. 9. Additional VideoSIAH Details Source Purpose LLaVA-CoT [53] OpenVLThinker [6] Complex Reasoning We-Math 2.0 [33] Mathematical Reasoning General Visual Reasoning Samples 54,591 2,829 602 Table 5. Detailed Statistics of Image-based CoT Data for ColdStart SFT. Breakdown of Image-based CoT Data. As detailed in Table 5, we construct diverse mixture of image-based CoT data for the cold-start SFT stage, spanning general visual reasoning [53], complex logical inference [6], and mathematical problem-solving [33]. Drawing on insights from recent work [8, 58], we leverage these image-based reasoning traces to strengthen the models fundamental perceptual capabilities. This strategy exploits the inherent synergy between image and video modalities, where robust spatial grounding serves as critical foundation for complex temporal reasoning. for"
        },
        {
            "title": "Distribution",
            "content": "comprises 244 videos VideoSIAH-Eval. Category VideoSIAH-Eval and 1,280 high-quality QA pairs. As illustrated in Figure 6a, the video corpus encompasses diverse spectrum of domains, ranging from Travel & Events to Gaming, ensuring broad coverage of real-world scenarios. Furthermore, Figure 6b highlights our deliberate emphasis on dynamic video reasoning: Action Recognition and Temporal Reasoning (17% in total) constitute large portion of queries, rigorously benchmarking the models capacity for fine-grained event perception and causal understanding in the temporal dimension. 10. Additional Methodological Details Next-Token Prediction. During SFT, we train our model by minimizing the negative log-likelihood of the target tokens given their preceding context. For sequence of tokens = (x1, x2, . . . , xT ) and model parameterized by θ that defines conditional probabilities pθ(xt x<t), the loss function is defined as L(θ) = (cid:88) t=1 log pθ(xt x<t), 3 which encourages the model to assign higher probability to the ground-truth next token. Group Relative Policy Optimization. During RL, we adopt GRPO [35] for optimization. For each prompt D, we draw group of responses from the behavior policy πθold . y(k) πθold ( x), y(k) = (y(k) 1 , . . . , y(k) Tk We use group baseline and advantages: ), Tk = len(y(k)). = 1, . . . , K, = 1 K (cid:88) k=1 R(k), A(k) = R(k) b, where R(k) is the scalar return of response y(k). The policy maximizes length-normalized, tokenconditional KL-regularized objective: (θ)= (cid:88) k=1 (cid:20) 1 Tk(cid:88) xD {y(k)}πθold (x) (cid:20) 1 (cid:88) 1 Tk β ExD k=1 t= 1 Tk Tk(cid:88) t=1 A(k) log πθ (cid:0)y(k) (cid:21) (cid:1) x, y(k) <t (cid:16) DKL πθ( x, y(k) <t ) πref ( x, y(k) <t ) (cid:17)(cid:21) , (1) with {1, . . . , Tk}, πref frozen reference policy, and β > 0 controlling KL strength. 11. Reflection Trajectory: From Verbose SelfCorrection to Internalized Tool Usage We visualize the evolution of the models internal thought process in Figure 7 (left). Echoing the training dynamics observed in DeepEyes [63], the trajectory of reflection token proportion discloses distinct three-phase evolution from exploratory correction to efficient tool exploitation: (1) Verbose Self-Correction (Steps 050): Initially, reflection density remains high. Due to insufficient localization accuracy, the model relies on extensive self-correction and iterative verbal reasoning to compensate for sub-optimal (2) Efficiency Optimization (Steps 5080): tool usage. significant drop follows as the policy matures. As the models intrinsic grounding capability improves, it identifies prolonged reflection to be redundant, autonomously pruning unnecessary linguistic fillers to maximize reward (3) Internalized Proficiency (After 80 Steps): efficiency. The curve stabilizes at concise baseline, indicating shift toward selective reasoningthe model invokes explicit reflection only when resolving ambiguity, having internalized the core semantics of tool interaction. Complementing this, the word cloud (right) confirms that the remaining reflection tokens are semantically grounded (e.g., segment, confirm), serving as functional anchors for temporal reasoning rather than generating generic linguistic fillers. (a) Video Category Distribution (b) Question Category Distribution Figure 6. Category Distribution of VideoSIAH-Eval. We present the distribution of video types (a) and question types (b), highlighting the diversity of our proposed benchmark. Figure 7. Trend of Reflection-Related Words and the Corresponding Word Cloud across All Rollouts. 12. Additional Implementation Details Component SFT RL RFT Optimizer Learning Rate (LR) LR Scheduler Weight Decay No. of Training Steps No. of Warmup Steps Max Length Dynamic Batch Size Remove Padding Liger Kernel No. of GPUs No. of Frames AdamW [30] AdamW AdamW 5e-5 cosine 0.0 3000 300 51200 True True True 32 512 1e-6 constant 1e-2 160 0 52384 False True False 64 5e-5 cosine 0.0 1600 160 51200 True True True 64 512 Table 6. Detailed Hyperparameters across Training Stages. Unless otherwise specified, all experiments are conducted on NVIDIA A800-SXM4-80GB GPUs."
        },
        {
            "title": "The full set of experimental hyperparameters is detailed",
            "content": "in Table 6. SFT. We initialize the cold-start SFT phase using Qwen2.5-VL-7B-Instruct [1], utilizing the LMMs-Engine [29] framework. To optimize training throughput and minimize memory overhead, we employ an online stream packing strategy on iterable datasets. Specifically, instead of padding individual sequences, we concatenate input samples to fill fixed buffer size of 51,200 tokens, thereby eliminating redundant computation on padding tokens. Incoming data is dynamically batched to maximize GPU utilization. Given the streaming nature of this pipeline, we train the model until convergence rather than adhering to predetermined epoch count. RL. For the RL stage, we build upon the verl library [37], extending it to support multi-turn and multimodal tool-augmented rollouts via SGLang [62]. We configure global batch size of 16 and sample 16 rollouts per prompt. To manage context limitations effectively, we restrict the maximum number of new tokens to 16,384 and impose hard cap of 36,000 tokens on the total prompt length. constant temperature of 1.0 is maintained across all experi4 Model VideoMMMU [13] LVBench [48] VideoMME [9] VideoSIAH-Eval Average Qwen2.5-VL-7B [1] Video-R1-7B [8] VideoRFT-7B [46] Video-Thinker-7B [47] LongVT-7B-RFT (Ours) 2108.6 1341.8 1937.9 3153.8 1329.8 2014.7 1550.6 2154.3 3834.9 1509.3 3031.6 2483.3 3544.2 2475.1 2754.0 1834.3 1900.3 2052.6 1899.2 1891.1 2247.3 1819.0 2422.3 2840.8 1871. Table 7. Inference Latency (in seconds) Comparison Across Various Long Video Understanding and Reasoning Benchmarks. For each benchmark, the lowest latency is shown in bold, and the second-lowest is underlined. Intermediate variants such as LongVT-7BSFT and LongVT-7B-RL are excluded to focus on representative baselines and final-stage models. All experiments are conducted using uniform 64-frame sampling and online inference served via vLLM [19], with latency measured through LMMs-Eval [60] on 8 NVIDIA A800-SXM4-80GB GPUs. ments to encourage exploration. Given the significant computational cost associated with reinforcement learning, we adopt an early stopping strategy, terminating training once the reward metrics saturate. RFT. The RFT stage serves to consolidate the agentic behaviors emerging from RL. We adhere to the same efficient training infrastructure and stream-packing protocols established in the SFT stage. However, critically, we initialize this stage using the best-performing checkpoint obtained from RL, rather than the base model. The training corpus contains high-quality, self-distilled trajectories filtered from the RL rollouts. To accommodate this augmented dataset and speed up the refinement process, we scale our computational resources from 32 to 64 GPUs. Accordingly, the training span is adjusted to approximately 1,600 steps, ensuring the model sufficiently internalizes the precise temporal grounding and reasoning patterns present in the selfgenerated traces. Evaluation. We conduct comprehensive evaluations using the LMMs-Eval framework [60], maintaining consistent testing environment across SFT, RL, and RFT checkpoints. To robustly assess tool-calling capabilities, we deploy standard Model Context Protocol server paired with an online inference engine [19] that supports continuous batching for asynchronous requests. We inject special delimiter tags into the generation stream to rigorously parse reasoning steps, tool invocations, and final answers. Performance is quantified using hybrid scoring mechanism that integrates deterministic rule-based validators with semantic evaluation via an LLM-as-a-Judge [55] approach. 13. Inference Efficiency Analysis Efficiency Analysis. We present comparative analysis of inference latency across four benchmarks in Table 7. Despite incorporating multi-turn tool interactions, LongVT7B-RFT demonstrates remarkable efficiency, achieving latency on VideoMMMU (1329.8 seconds) the lowest and LVBench (1509.3 seconds), and maintaining highly competitive speeds on VideoMME and VideoSIAH-Eval. This counter-intuitive efficiencywhere multi-turn agentic framework outpaces single-turn baselinescan be attributed to the precision of our reasoning. Upon checking the inference results, we found that baselines like Qwen2.5VL often has higher chance of hallucinations, generating redundant descriptions by blindly rephrasing uncertain visual memories (as discussed in Figure 1 of main paper), LongVT proactively seeks evidence. By grounding its answer in retrieved frames, our model circumvents the need for verbose, uncertainty-driven fabrication, resulting in more concise and faster token generation overall. Note on Efficiency Context. Our criterion for fastest does not imply skipping content arbitrarily. Instead, it aligns with human-like viewing: we do not expect the testee to watch the entire video frame-by-frame from start to finish before answering. In the context of LMMs, this translates to the ability to strategically sample and encode relevant segments, avoiding the prohibitive computational cost and context overflow associated with encoding extremely long sequences in their entirety. 14. Examples Prompts and Data Examples. To enhance reproducibility and transparency, we provide concrete examples of the key resources used in our experiments. Figure 8 shows the RL prompt template, while Figure 9 presents the evaluation prompts used in LLM-as-a-Judge [55] for measuring answers accuracy during RL. One representative sample from both SFT and RFT stages is shown in Figure 10. Reasoning and Inference Examples. Beyond static prompts and data, we visualize the models inference process to illustrate its reasoning and self-correction behavior. Figure 11 highlights single-turn case where the model uses internal monologue to re-check visual evidence and successfully self-correct an initial hallucination. Figure 12 furwithout succumbing to context overflow. We leave the exploration of this scalable, divide-and-conquer architecture to future work. 17. Broader Impact LongVT advances the field of long-video understanding by introducing an agentic framework capable of proactive evidence seeking and self-correction. By enabling LMMs to dynamically inspect and re-examine video segments, this work addresses critical reliability issuessuch as hallucinations and temporal misalignment that hinder the deployment of AI in high-stakes domains. As video-based AI systems become integral to applications ranging from automated surveillance and content moderation to educational analytics and assistive technologies for the visually impaired, the improved factual grounding and transparency offered by LongVT support safer and more trustworthy interactions. 18. Ethical Considerations Advancing Reliability and Safety. LongVT is explicitly designed to enhance the reliability of video LMMs by mitigating hallucinations through on-demand visual verification. By grounding answers in retrieved video evidence, the system reduces the likelihood of fabricating events or misinterpreting context, thereby fostering more trustworthy predictions in complex, long-form video scenarios. tool invocation, Transparency and Interpretability. By decomposing into observable stepsglobal the reasoning process skimming, and self-reflectionLongVT inherently supports transparent decision-making. This explicit chain of tool-augmented thought facilitates auditing and debugging, allowing users to trace why model arrived at specific conclusion and which video segments informed that decision. retrieval, evidence Responsible Use of Data. The system does not access private or surveillance feeds, and no additional personally identifiable information is introduced. We advocate for the strict adherence to privacy standards and ethical guidelines when deploying such long-video analysis tools in realworld settings. ther shows multi-turn example in which tool interactions iteratively refine the temporal window. Finally, Figure 13 compares our approach with standard textual CoT baseline: while the latter hallucinates unseen visual details (e.g., incorrect object appearance), our method follows an active verify-and-correct proceduredetecting that the retrieved segment lacks the queried object, adjusting the crop region, and ultimately locating the correct evidence to produce the accurate answer. 15. Failure Case Analysis To further illustrate the instability of the RL-only variant discussed in Section 5.3 of the main paper, we present representative failure case. As shown in Figure 14, the model correctly recognizes the need to invoke tool to inspect the glass coffee table. However, after receiving the resampled video frames, it fails to integrate the returned evidence to answer the specific question (which video-game device). Instead of performing the required reasoning, the model becomes confused by the context shift and reverts to generic video captioning, merely restating superficial scene descriptions. This behavior underscores the importance of the SFT cold start in teaching the model the intended semantics of tool usage, enabling it to correctly interpret tool outputs and incorporate them into its reasoning process. 16. Limitation and Future Direction While our efficiency analysis in Section 13 confirms that multi-turn tool interactions do not impose significant latency penalties, the memory footprint of such recursive reasoning remains bottleneck. The single-agent architecture of LongVT is constrained by the inherent context window of the underlying LMM: as the number of interaction turns increasesdriven by the need for multiple crop video calls to inspect ultra-long or infinite video streamsthe accumulation of history tokens (including dense visual features returned by tools) can rapidly exhaust the context budget. This accumulation poses risk of Out-of-Memory errors during training and imposing performance degradation due to truncation. promising future direction to resolve this limitation lies in multi-agent collaboration. Inspired by recent advancements in multi-agent reinforcement learning such as MATPO [32], we envision hierarchical framework where context management is decoupled from reasoning. In this future paradigm, Manager Agent could orchestrate high-level planning and dispatch sub-tasks to specialized Worker Agents, each responsible for inspecting distinct temporal segments or executing specific tool calls. By enabling workers to summarize their observations into concise natural language updates for the manager, such system could theoretically support infinite-horizon reasoning loops Figure 8. Prompt Template Utilized for RL. This template outlines the structural guidelines and system instructions provided to the model during the RL training phase. Figure 9. Evaluation Prompt for LLM-as-a-Judge. We present the full system instruction used to query the judge model. This prompt defines the scoring criteria and guidelines to ensure consistent evaluation of the models generated responses. 7 Figure 10. Representative Data Example for SFT and RFT. The example illustrates the input format and the corresponding ground-truth response used to train the model across both fine-tuning stages. 8 Figure 11. An Example of Single-turn Inference with Self-Correction. The model initially misidentifies the basin color as pink. However, through the reasoning process (highlighted in the Thinking block), it explicitly decides to double-check the frames, corrects the hallucinations, and outputs the correct answer (Blue). 9 Figure 12. An Example of Multi-step Inference Involving Tool Interaction. In this complex query, the model initially crops an incorrect time window (297s-305s) which lacks the target visual information. Recognizing this error during the reasoning phase, it refines the parameters and calls the tool again with the correct window (344s-372s) to successfully identify the US flag. 10 Figure 13. Qualitative Comparison between Textual CoT and Our Designed iMCoTT. The baseline textual CoT (left) relies on hallucinated memory, confidently providing an incorrect answer regarding the cars colors (Black and Yellow). In contrast, our model (right) actively engages with the video content via tool usage. Despite an initial mis-localization (90s-120s), the model explicitly detects the absence of the target object, self-corrects its temporal search window to the correct range (174s-190s), and accurately identifies the cars as White and Yellow. 11 Figure 14. Failure Case of the RL-only Variant. This example demonstrates the models inability to maintain the logical flow after tool interaction without prior SFT. Although the model initiates tool call to inspect the blurred region, it fails to utilize the returned observation to answer the users question. Instead, it loses the conversational context and hallucinates repetition of the general video description."
        }
    ],
    "affiliations": [
        "HKUST(GZ)",
        "LMMs-Lab Team",
        "MiroMind AI",
        "NTU",
        "THU"
    ]
}