{
    "paper_title": "DressRecon: Freeform 4D Human Reconstruction from Monocular Video",
    "authors": [
        "Jeff Tan",
        "Donglai Xiang",
        "Shubham Tulsiani",
        "Deva Ramanan",
        "Gengshan Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present a method to reconstruct time-consistent human body models from monocular videos, focusing on extremely loose clothing or handheld object interactions. Prior work in human reconstruction is either limited to tight clothing with no object interactions, or requires calibrated multi-view captures or personalized template scans which are costly to collect at scale. Our key insight for high-quality yet flexible reconstruction is the careful combination of generic human priors about articulated body shape (learned from large-scale training data) with video-specific articulated \"bag-of-bones\" deformation (fit to a single video via test-time optimization). We accomplish this by learning a neural implicit model that disentangles body versus clothing deformations as separate motion model layers. To capture subtle geometry of clothing, we leverage image-based priors such as human body pose, surface normals, and optical flow during optimization. The resulting neural fields can be extracted into time-consistent meshes, or further optimized as explicit 3D Gaussians for high-fidelity interactive rendering. On datasets with highly challenging clothing deformations and object interactions, DressRecon yields higher-fidelity 3D reconstructions than prior art. Project page: https://jefftan969.github.io/dressrecon/"
        },
        {
            "title": "Start",
            "content": "DressRecon: Freeform 4D Human Reconstruction from Monocular Video Jeff Tan, Donglai Xiang, Shubham Tulsiani, Deva Ramanan, Gengshan Yang* Carnegie Mellon University, USA 4 2 0 2 8 ] . [ 2 3 6 5 0 2 . 9 0 4 2 : r Figure 1. Given an input video of human, DressRecon reconstructs time-consistent 4D body model, including shape, appearance, time-varying body articulations, as well as deformation of extremely loose clothing or accessory objects. We propose hierarchical bag-of-bones deformation model that allows body and clothing motion to be separated. We leverage image-based priors such as human body pose, surface normals, and optical flow to make optimization more tractable. The resulting neural fields can be extracted into timeconsistent meshes, or further optimized as explicit 3D Gaussians for high-fidelity interactive rendering."
        },
        {
            "title": "Abstract",
            "content": "We present method to reconstruct time-consistent human body models from monocular videos, focusing on extremely loose clothing or handheld object interactions. Prior work in human reconstruction is either limited to tight clothing with no object interactions, or requires calibrated multi-view captures or personalized template scans which are costly to collect at scale. Our key insight for highquality yet flexible reconstruction is the careful combination of generic human priors about articulated body shape (learned from large-scale training data) with video-specific articulated bag-of-bones deformation (fit to single video via test-time optimization). We accomplish this by learning neural implicit model that disentangles body versus clothing deformations as separate motion model layers. To capture subtle geometry of clothing, we leverage imagebased priors such as human body pose, surface normals, and optical flow during optimization. The resulting neural fields can be extracted into time-consistent meshes, or *Corresponding author: jefftan@andrew.cmu.edu further optimized as explicit 3D Gaussians for high-fidelity interactive rendering. On datasets with highly challenging clothing deformations and object interactions, DressRecon yields higher-fidelity 3D reconstructions than prior art. Project page: https://jefftan969.github.io/dressrecon/ 1. Introduction We aim to reconstruct animatable dynamic human avatars from videos of people wearing loose clothing or interacting with objects, such as in-the-wild monocular videos recorded on phone or from the Internet. High-quality reconstructions in this setting traditionally require calibrated multi-view captures [41, 64], which are costly to obtain. From only single viewpoint, recovering freelydeforming humans with arbitrary topology is highly underconstrained, and thus prior works often rely on domainspecific constraints which struggle to support loose clothing. Template-based human reconstruction [18, 19, 63] requires personalized scanned templates, which works well for single instance but cannot reconstruct unseen clothing and body shapes and clothing. Methods that regress 3D surfaces from single image [61, 62] can produce highquality geometry at observed regions, but the results are inconsistent across frames and sometimes fail to produce coherent body shapes. Human-specific methods [16, 24, 55] can achieve high quality on tight clothing, but often use fixed human skeleton or parametric body template and thus cannot handle extreme deformations outside the body. More broadly, generic methods for humans and animals [66, 67] can support arbitrary deformations, but often produce lower quality results than human-specific methods. This paper presents DressRecon, which reconstructs freeform 4D humans with loose clothing and handheld objects from monocular videos. Our key insight is the careful combination of generic human-level priors about articulated body shape (learned from large-scale training data) with video-specific articulated bag-of-bones clothing models (fit to single video via test-time optimization). We accomplish this by learning neural implicit model that disentangles body and clothing deformations as separate motion layers. To capture subtle geometry of clothing, we leverage image-based priors such as masks, normals, and body pose during optimization. When the goal is shape reconstruction, we extract time-consistent meshes from the optimized neural fields. Otherwise, to enable high-quality interactive rendering, we propose refinement stage that converts our implicit neural body into 3D Gaussians while maintaining the motion field design. On datasets with highly challenging clothing and object deformations, DressRecon yields higher-fidelity 3D reconstructions than prior art. 2. Related Work Humans from multi-view or depth. With sufficient information as input, multi-view methods [9, 13, 26, 37, 41, 46, 59] can reconstruct human shape and appearance of very high fidelity, but the reliance on dense capture studio limits their applicability at consumer level. Depthbased methods [10, 60, 73] follow the seminal DynamicFusion work [44] to integrate human shape from monocular depth stream into canonical space with the help of deformation model. However, their application scenarios are also limited because they require specialized depth sensors. Monocular human reconstruction. Monocular RGBbased reconstruction is challenging due to the 3D ambiguity of monocular input. Early work [2, 15, 28, 58] aims to reconstruct 3D human keypoints or skeletal poses using deformable human model [27, 40]. Compared with sparse keypoints, reconstructing dense human surfaces is even more challenging, especially when clothing is considered. Trained on ground truth 3D scans, pixel-aligned implicit functions [34, 48, 62] regress clothed human surfaces from monocular image, but their output on video tends to be less temporally coherent. Another line of work aims Table 1. Related work in monocular 3D body reconstruction. (1)Methods based on human body and pose models. (2)General methods for humans and animals. Dense: Dense deformation fields. Bob: Bag-of-bones. H: Human body and pose priors. F: Optical flow. N: Surface normal. ϕ: Features. Our method combines the best of human-specific and general methods by fitting flexible motion model initialized from off-the-shelf 3D human poses, using dense image-based priors. Method Motion model Prior Input (1) (2) ECON [62] NeuMan [24] Vid2Avatar [16] SelfRecon [22] HumanNeRF [55] MagicPony [57] LASR [66] BANMo [68] RAC [69] N.A. Skeleton Skeleton Skeleton+Dense Skeleton+Dense Skeleton Bob Bob Skeleton+Dense H,N H,N ϕ F,ϕ F,ϕ Image Video Video Video Video Image Video Video Video DressRecon (Ours) Hierarchical Bob H,F,N,ϕ Video to reconstruct dynamic human shapes from video input, using deformable human model [16, 22, 55] or pre-scanned personalized templates [19, 25, 63] and often achieving significant speedups [20, 23, 33]. Generic human models (e.g. SMPL) help resolve monocular 3D ambiguity, but without personalized clothed template, few works can handle dynamic clothing that does not closely follow body motion. HOSNeRF [39] reconstructs objects rigidly attached to the human body (e.g., hand) by introducing new object bones into the human skeleton hierarchy. Our method took step further and introduces novel representation that not only leverages human-specific model priors, but also simultaneously enjoys the flexibility to handle loose garments. concurrent work, ReLoo [17], also applies two layer deformation model to account for the motion of loose garments. Monocular nonrigid 3D reconstruction. Non-rigid structure from motion (NRSfM) methods [4] reconstruct nonrigid 3D shapes from 2D point trajectories in classagnostic way. However, due to the oversimplified motion model and the difficulties in estimating long-range correspondences [50], they do not work well for videos with challenging deformations. Recent work applies differentiable rendering to reconstruct articulated objects from videos [47, 56, 66, 67] or images [14, 29, 57, 72]. However, they cannot reconstruct challenging body articulations and large deformations beyond the body, due to the lack of flexible motion representation and sufficient measurement signals. As shown in Tab. 1, we introduce hierarchical bag-of-bones motion model that is capable of representing the deformation of loose garments and accessories, fitted using rich signals from pretrained vision models such as human body pose, surface normals, and optical flow. Figure 2. Method Overview: We represent 3D humans in loose clothing as temporally consistent 4D neural fields (Sec. 3.1). Central to our approach is flexible motion representation that captures fine-grained clothing deformations as well as limb motions, while effectively utilizing domain-specific priors such as 3D human body pose (Sec. 3.2). We perform video-specific optimization that fits this model to dense image-based priors via differentiable rendering (Sec. 3.3). After optimization, our neural implicit surface can be extracted into time-consistent mesh via marching cubes, or converted into explicit 3D Gaussians for high-fidelity interactive rendering (Sec. 3.4). 3. Method Our goal is to reconstruct time-varying 3D humans in loose clothing from in-the-wild monocular videos  (Fig. 2)  . We represent humans with clothing as 4D neural fields and perform per-video optimization with differentiable rendering (Sec. 3.1). Key to our approach is hierarchical motion model (Sec. 3.2) capable of representing large limb motions as well as clothing and object deformations. We leverage image-based priors (Sec. 3.3) such as body pose, surface normals, and optical flow to make optimization more stable and tractable. The resulting neural fields can be extracted into time-consistent meshes via marching cubes, or converted into explicit 3D Gaussians for high-fidelity interactive rendering (Sec. 3.4). 3.1. Preliminary: Consistent 4D Neural Fields To represent time-varying 3D human, we construct timeinvariant canonical shape that is warped by time-varying deformation field. Canonical shape. We represent the body shape as neural signed distance field in the canonical space, with the following properties: signed distance d, color c, and universal features ϕ. The canonical fields are defined as (d, ϕ) = MLPSDF(X), ct = MLPcolor(X, ωt), (1) (2) where is 3D point in canonical space and ωt is timevarying appearance code specific to each frame. Space-time warpings. We represent time-varying motion using continuous 3D deformation fields. forward deformation field W(t)+ : Xt maps canonical 3D point to time t. During volume rendering, rays at time are traced Figure 3. Visualization of two-layer deformation. The body and clothing deformation layers each contribute separate types of motion. In this sequence, the clothing Gaussians deform the womans dress to be larger, while the body Gaussians move her right arm forward. During forward warping, we start from the canonical shape (left), and first apply the forward warp described by clothing Gaussians, then the forward warp described by body Gaussians. The same process happens in reverse during backward warping. back to the canonical space using backward deformation field W(t) : Xt X. We use 3D cycle loss Lcyc to ensure that W(t)+ W(t) is close to identity [35, 68]. Volume rendering. Neural fields can be optimized via differentiable volume rendering [42], which renders images and minimizes reconstruction errors (e.g. photometric loss). To provide additional supervision on geometry and motion, we augment the training data with additional signals obtained from off-the-shelf networks, detailed in Sec. 3.3. 3.2. Hierarchical Gaussian Motion Fields In monocular 4D reconstruction, it is challenging to find motion representation that is both sufficiently flexible and easy to optimize. Recent methods are either not flexible enough to model dynamic structures outside the body [22], or struggle to robustly reconstruct dynamic motions at high quality [66]. We introduce hierarchical motion fields to strike balance between flexibility and robustness. Bag-of-bones skinning deformation. Our motion model is inspired by deformation graphs and its extension to Gaussian blend skinning models [3, 51, 66]. The idea is to use the motion of bones (defined as 3D Gaussians, typically = 25) to drive the canonical geometrys motion. Each Gaussian maintains time-varying trajectory of its 3D centers µt RT 3 and orientations Vt RT 3 over frames, as well as axis-aligned scales Λ R3 that are time-invariant. Given the 3D Gaussians, dense forward deformation field can be computed by blending the SE(3) transformations of Gaussians with forward skinning weights W+. Similarly, dense backward deformation field is produced by blending with backward skinning weights : Xt = +(X, t) = (cid:32) (cid:88) W+,bGb (cid:0)Gb(cid:1)1 (cid:33) (3) = (Xt, t) = b=1 (cid:32) (cid:88) b=1 W,b Gb (cid:0)Gb (cid:33) (cid:1)1 Xt (4) Here and Gt are the SE(3) transformations of the canonical and time Gaussians, respectively. Forward skinning weights W+ Rb are computed using the Mahalanobis distance from to each canonical Gaussian G. We use coordinate MLP to refine the weights (similar to [68]), and use negative softmax such that farther Gaussians are assigned lower weight. In the same way, backward skint RT are computed using the Maning weights halanobis distance from Xt to each time Gaussian Gt, followed by MLP refinement. This bag-of-bones representation can represent large non-rigid deformations due to its flexibility, but can be challenging to optimize. For example, most Gaussians can get concentrated in local region, which limits the ability to deform the other parts of the target. Carefully initializing the Gaussians and spatially distributing them during optimization can help avoid such bad local minima. Our key idea is to divide the Gaussians into body and clothing layers, which can be initialized and regularized separately. Body Gaussians are intended to represent skeletal motions of the target. With recent advances in human and animal body pose [15, 43], 3D joint locations can be robustly estimated from images and used to initialize the body Gaussian trajectories. This allows body Gaussians to start from close-to-optimal solution and get locally refined throughout differentiable rendering. The resulting body Gaussians exhibit less temporal jitter than the single-frame predictor, and are better aligned to physical bone locations. Clothing Gaussians are intended to represent free-form deformations not explained by body Gaussians, such as cloth deformation and the motion of handheld objects. To encourage that clothing Gaussians only deform structures outside the scope of body Gaussians, we add regularization term to minimize the impact of clothing Gaussians: Lcl = (cid:13) (cid:13)W + cloth(X, t) X(cid:13) 2 (cid:13) (5) Compositional two-layer deformation. The final deformation fields are the composition of body and clothing layer deformations  (Fig. 3)  , each with about 25 Gaussian bones. During forward warping we apply the clothing deformation before the body deformation, and during backward warping we perform the reverse: +(t) = + (t) = body(t) + cloth(t) cloth(t) body(t) (6) (7) We optimize the body and clothing Gaussians jointly. To encourage body and clothing Gaussians to be welldistributed in 3D space, we use Sinkhorn divergence loss Lsink [12] to match the spatial distribution of Gaussians with the body shape. The Sinkhorn divergence is computed between 1k random points on the canonical rest surface, and 3D points on the Gaussians of each deformation layer. With proper initialization and regularization, body and clothing motion can be properly disentangled. In Fig. 3, the clothing Gaussians deform the dress while the body Gaussians deform the womans arm. On the supplementary webpage, we show video examples where body and clothing motion is properly decomposed by two-layer deformation. 3.3. Optimization with Image-Based Priors Optimizing time-varying 3D geometry from monocular videos is challenging due to its under-constrained nature. Recent advances in surface normals [11], optical flow [52, 65], image features [5, 45], and zero-shot segmentation [32] provide additional interpretations of raw pixel values. This knowledge is not only generic, but also highly correlated with the geometry and motion of the underlying scene, making it suitable for our reconstruction task. We introduce an optimization routine that uses foundational image-based priors as supervision to make the problem tractable. Surface normals. Without multi-view inputs, it is challenging to distinguish shape from appearance. For example, detailed structures such as clothing wrinkles can just as easily be painted as colors on flat surface, leading to inaccurate surface geometry. To counteract this, we use normal estimators [31] trained on large datasets to provide signal to improve the geometry. We can take spatial derivatives of signed distance with respect to Xt to compute the surface normal of point Xt in deformed space. We normalize the rendered and estimated surface normals and compute normal loss as L2 error between them. Similar to prior work on neural surface reconstruction [71], we also compute an eikonal loss Leik to regularize the neural surface. = normalize(d(W (Xt, t))) Ln = n2 = 2 2n, Leik = (cid:13) (cid:13)norm(d(W (Xt, t))) 1(cid:13) (cid:13) (8) (9) (10) Normals with numerical gradients. Most prior work uses analytical gradients (e.g. auto-diff) to compute normals of signed distance fields. However, these are computed within an infinitesimally small neighborhood of Xt and suffer from noise in both the estimated backward warping fields and signed distances [7]. This leads to unstable optimization when dealing with deformable objects. To avoid this, we compute normals by numerical gradients [36] with fixed 1mm step size during optimization. Normals with eikonal filtering. Although numerical normal computation works well on static scenes, it is more challenging in deformable scenes where the warping fields influence can cause Xt + δ Xt to be very different from (Xt + δ) (Xt). For example, the hand and waist might be close in deformed space but far in canonical space, causing exploding gradients due to large change in signed distance gradient over small neighborhood. To avoid this problem, we clip the normal direction to 0 after Eq. 8 whenever the gradient magnitude exceeds some threshold, in our case > 10. Optical flow. We use optical flow [52, 65] to learn the non-rigid deformation and relative camera transform between two frames. We compute 3D scene flow vectors by backward warping deformed points to canonical space, then forward-warping to another timestamp. We use the camera matrix to project 3D flow vectors into 2D, and compute L2 error between rendered flow and estimated flow f, Lf = f. Here, t = {1, 2, 4, 8}: f3D(Xt, t) = +(W (Xt, t), t) Xt (11) Universal features. Deep neural features are useful for registering pixels to 3D model [38, 67], while allowing better convergence at textureless regions or under deformation. Prior work relies on category-specific image features, but we find DINOv2 [45]) to be robust and universal feature descriptor that works well for clothing and accessories. We choose the small DINOv2 model with registers, as it produces fewer peaky feature artifacts [8]. We obtain pixellevel features from DINOv2s patch descriptors by evaluating DINOv2 on an image pyramid, averaging features across pyramid levels, and reducing the dimension to 16 via PCA [1]. We compute feature loss as L2 error between rendered and estimated features, Lϕ = ϕ ϕ2. Zero-shot Inspired by shape-fromsilhouette [53], we use image segmentation to carve out the 3D boundary of the target. We leverage the foundational 2D segmentation model SAM [32] and its extension segmentation. to tracking [70] to predict accurate silhouettes of humans with clothing and accessories. We pass different prompts according to different scenarios we aim to reconstruct, such as human wearing cloth and human holding an object. We compute silhouette loss as the L2 error between rendered and estimated silhouettes, Ls = s2. Losses. Our final loss is weighted sum of reconstruction and regularization terms. Loss weights λ are searched once and kept across all experiments. Lrec = λcLc + λf Lf + λnLn + λϕLϕ + λsLs Lreg = λeikLeik + λcycLcyc + λsinkLsink + λclLcl (12) (13) 3.4. Refinement with 3D Gaussians Representation. Neural SDFs are ideal for extracting surfaces, but can be difficult to optimize as adding new geometry requires making global changes. In light of this, we introduce refinement procedure that replaces the canonical shape representation with 3D Gaussians [30] while keeping the two-layer motion model as is. To render an image, we warp Gaussians forward from canonical space to time (Eq. 17) and call the differentiable Gaussian rasterizer. Initialization. We use 40k Gaussians, each parameterized by 14 values, including its opacity, RGB color, center location, orientation, and axis-aligned scales. Gaussians are initialized on the surface of the neural SDF with isotropic scaling. To initialize the color of each Gaussian, we query the canonical color MLP (Eq. 2) at its center. Optimization. We update both the canonical 3D Gaussian parameters and the motion fields by minimizing Lrec = λcLc + λf Lf + λsLs Lreg = λsinkLsink. (14) (15) Notably, the 3D cycle loss Lcyc can be dropped since rasterization does not require computing backward warps. 4. Experiments We evaluate DressRecons ability to reconstruct both 3D shape and appearance given challenging monocular videos. Video results are available on the supplementary webpage. 4.1. Datasets Dynamic clothing and accessories. To evaluate DressRecons ability to reconstruct dynamic clothing and objects, we select 14 sequences from DNA-Rendering [6] with challenging cloth deformation and/or handheld objects (e.g. playing cello, swinging cloth, waving brush). As DNA-Rendering does not provide ground-truth meshes, we compute pseudo-ground-truth 3D meshes by using all 48 available cameras to optimize separate NeuS2 [54] instance at each timestep. To overcome the limited viewpoint range of each individual camera, we assemble turntable monocular videos by rendering these per-frame NeuS2 instances along smooth 360-degree camera trajectory. Avatars from casual videos. We also evaluate DressRecons ability to recover high-fidelity human avatars from casual turntable videos. We evaluate our method on ActorsHQ [21] and select subsets of the first 4 sequences for evaluation, each about 200 frames. As ActorsHQ cameras have small fields of view and often do not cover the whole body, we colorize the provided ground-truth meshes and render turntable monocular videos with 360 of camera rotation. 4.2. Results Reconstructing dynamic clothing and accessories. Tab. 2 reports the 3D chamfer distance (cm, ) for reconstructing dynamic clothing and handheld objects, evaluated across 14 DNA-Rendering sequences. We compare with Vid2Avatar [16], BANMo [68], RAC [69], and ECON [62], and show qualitative results in Fig. 4. The project page contains corresponding video results. DressRecon reconstructs finer details and more accurate body shape than prior art, and is able to handle challenging scenarios such as the tip of the cello (image 1), the hair tassels (image 2), and the detailed cloth wrinkles on the martial arts uniform (image 4). Reconstructing avatars from casual videos. Tab. 4 reports 3D chamfer distance (cm, ) and F-score at {1, 2, 5}- cm thresholds for recovering avatars from turntable videos, evaluated across 4 ActorsHQ sequences. We compare with Vid2Avatar [16] and show qualitative results in Fig. 5. DressRecon performs on par with Vid2Avatar in tight clothing scenarios, and reconstructs higher-fidelity geometry on sequences with challenging clothing such as dresses. Rendering dynamic clothing and accessories. Tab. 3 reports the RGB PSNR (), SSIM (), LPIPS (), and mask IoU () on test views by holding out every 8-th training view. We compare against Vid2Avatar [16], BANMo [68], and RAC [69], and show qualitative results in Fig. 7. The project page contains extensive video results. DressRecon produces more accurate renderings than prior art. 4.3. Diagnostics 3D Gaussian refinement. In Tab. 6, we show results from optimizing neural implicit model from scratch, 3D Gaussian model from scratch, and 3D Gaussian model initialized from neural implicit model. The same computational budget is allocated to all three experiments. The highest rendering quality is achieved with neural implicit optimization followed by 3D Gaussian refinement. This suggests that the neural implicit model helps produce good initialization of shape and deformation, making it easier for 3DGS to converge to better local optima. Choice of deformation model. In Tab. 5, we swap our hierarchical two-layer deformation model with several alternatives in the literature. Swapping to skeleton+dense Figure 4. 3D reconstruction results on DNA-Rendering. We demonstrate DressRecons ability to reconstruct challenging sequences with large cloth deformation. DressRecons predictions align well with the image evidence, even in the presence of rapid clothing and object deformations. Vid2Avatar often outputs spurious shape artifacts and is unable to reconstruct challenging structures, such as the white cloth (row 2), brown brush (row 3), and detailed sleeves (row 4). BANMo and RAC produce hollow cellos on the first row, and tend to output over-smoothed surfaces for the other cases. ECON produces highly detailed textures, but it performs the worst numerically (Tab. 2) as the outputs often have an incorrect overall shape (e.g. Row 1). We encourage readers to view the video results on the supplementary webpage. warping field [22, 55], skeleton alone [69], or bag-of-bones alone [68] reduces the geometry quality. Alternative deforTable 2. 3D reconstruction metrics on DNA-Rendering sequences. We evaluate 3D chamfer distance (cm, ) on fourteen DNA-Rendering sequences with challenging clothing deformation or handheld objects. DressRecon outperforms all baselines, and is the best or second-best method on all sequences. Sequence DressRecon Vid2Avatar BANMo RAC ECON 0008 01 0047 01 0047 12 0102 02 0113 06 0121 02 0123 02 0128 04 0133 07 0152 01 0166 04 0188 02 0206 04 0239 01 Average (Ours) 7.300 8.542 6.064 5.421 6.872 5.520 6.725 7.803 6.194 6.437 4.356 5.403 7.555 5.559 6. [16] 6.786 11.216 9.334 7.812 8.517 6.478 8.343 8.184 6.314 7.465 4.608 5.887 8.392 5.503 [68] [69] [62] 7.768 7.808 6.914 5.131 8.362 7.453 7.418 8.913 7.017 6.815 5.969 6.341 8.404 6. 9.420 8.112 17.102 9.106 7.541 6.618 10.181 7.278 11.282 8.391 9.334 6.926 9.683 12.108 10.005 11.569 7.320 7.449 6.170 8.735 6.562 6.286 9.026 5.829 9.987 9.644 8.026 7.831 7.489 7.201 7.809 9.871 Table 3. Rendering metrics on DNA-Rendering sequences. We evaluate RGB PSNR (), SSIM (), LPIPS (), and mask IoU (), averaged across fourteen DNA-Rendering sequences with challenging clothing deformation or handheld objects. DressRecon outperforms all baselines, particularly when 3D Gaussian refinement is used to improve the rendering quality. Method PSNR SSIM LPIPS Mask IoU DressRecon w/ 3DGS refinement Vid2Avatar [16] BANMo [68] RAC [69] 22.03 22.27 19.61 19.78 19. 0.9375 0.1059 0.9506 0.0860 0.8948 0.1167 0.9341 0.1257 0.9351 0.1157 0.9544 0.9786 0.7931 0.8988 0.9044 Table 4. 3D reconstruction metrics on ActorsHQ sequences. We evaluate 3D chamfer distance (cm, ) and F-score at {1, 2, 5}- cm thresholds (%, ) on the four ActorsHQ sequences shown in Fig. 5. DressRecon outperforms Vid2Avatar on most sequences. Sequence CD DressRecon F@2 F@5 F@1 CD Vid2Avatar F@2 F@5 a1s1 a2s1 a3s1 a4s 3.212 1.838 2.647 2.247 94.74 99.96 97.15 98.88 72.46 92.72 79.91 85.02 48.99 62.14 51.38 57.00 3.204 2.891 4.376 2.039 98.22 97.59 90.93 98. 69.69 79.35 56.83 89.26 F@1 39.25 40.65 30.33 63.84 Average 2.486 97. 82.53 54.88 3.128 96.39 73.78 43. analytical gradients, shape optimization becomes unstable. Figure 5. 3D reconstruction results on ActorsHQ. DressRecon is on par with Vid2Avatar for standard clothing (Rows 2 and 4), and higher fidelity than Vid2Avatar for loose clothing (Rows 1 and 3). Vid2Avatars reconstructed skirts often contain shape artifacts. We attribute DressRecons improved performance to its flexible shape and deformation representation, which is capable of representing non-standard geometry and deformation. Figure 6. Qualitative ablation of numerical normals. We show the difference between optimizing with numerical and analytical normals. Using analytical normals causes training to be unstable, resulting in flat shape with no surface detail. The quality of surface details is reduced when normal loss is disabled (Tab. 5). mation models are also less interpretable, as skeleton-only and bag-of-bones do not separate body and clothing motion. Choice of image-based priors. In Tab. 5, we run the optimization routine and remove one of the image-based priors each time. Without mask loss, the surface geometry has an incorrect overall structure. Without normal loss, the reconstructed surface has lower detail. Without flow loss, the shape is less sensible and camera optimization is less stable. Choice of normal supervision. In Fig. 6, we show the benefit of using normal loss with numerical gradients. With Figure 7. RGB rendering results on DNA-Rendering. For each sequence, we show DressRecons and Vid2Avatars renderings at both the input view and 90-degree novel view. DressRecons renderings are shown with and without 3D Gaussian refinement. We find (similar to Tab. 3) that refinement significantly improves the textures, especially the flowers on the yellow dancers sleeve. Vid2Avatars renderings are less detailed, and fail to accurately depict structures that substantially deviate from the body, such as the cello and white stool. Table 5. Ablation study for 3D reconstruction. We ablate the importance of motion field representation and choice of image-based priors, by evaluating 3D chamfer distance (cm, ) and F-score at {1, 2, 5}-cm thresholds (%, ) on 14 DNA-Rendering sequences. DressRecon performs worse after switching motion representations (skeleton-only [16], bag-of-bones [68], skeleton+dense [22]) and after removing any image-based prior. Table 6. Ablation study for Gaussian refinement. We ablate the impact of 3D Gaussian refinement, by evaluating RGB PSNR (), SSIM (), LPIPS (), and mask IoU () on 14 DNA-Rendering sequences. We perform experiments where only an implicit SDF is optimized, where 3D Gaussians are optimized without initializing from an SDF, and where neural SDF is used to initialize 3D Gaussians. The best rendering quality is obtained by initializing 3D Gaussians from an SDF. Sequence CD F@5 F@2 F@ DressRecon 6.411 81.16 47.66 25.62 skeleton-only bag-of-bones skeleton+dense 7.340 6.942 7.526 78.10 44.55 22.67 80.19 47.21 25.49 76.33 41.58 21. w/o normal w/o mask 10.647 65.73 33.61 17.42 77.70 43.46 23.53 79.03 45.72 24.41 78.87 46.21 25.11 79.25 46.75 25.34 7.206 w/o flow 7.094 6.938 w/o pose 6.829 w/o feat 5. Discussion We present DressRecon, which reconstructs humans with loose clothing and accessory objects from monocular videos. DressRecon uses hierarchical bag-of-bones deformation to model clothing and body deformation separately, and leverages off-the-shelf priors such as masks and surface Sequence PSNR SSIM LPIPS Mask IoU Implicit-only 3DGS-only 22.03 21.31 Implicit3DGS 22.27 0.9375 0.1059 0.9455 0.0939 0.9506 0.0860 0.9544 0.9737 0.9786 normals to make optimization more tractable. To improve the rendering quality, we introduce refinement stage that converts the implicit neural body into 3D Gaussians. Limitations. DressRecon requires sufficient view coverage to reconstruct complete human, and cannot hallucinate unobserved body parts. It also has no understanding of cloth deformation physics. As result, clothing may deform unnaturally if we reanimate with novel body motion. We leave reanimating human-cloth and human-object interactions as future work. Moreover, specifying inaccurate segmentation, e.g. by passing the wrong prompt to SAM [32], could result in failure to reconstruct some details."
        },
        {
            "title": "References",
            "content": "[1] Shir Amir, Yossi Gandelsman, Shai Bagon, and Tali Dekel. Deep vit features as dense visual descriptors. arXiv preprint arXiv:2112.05814, 2021. 5 [2] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael Black. Keep it smpl: Automatic estimation of 3d human pose and shape from single image. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part 14, pages 561578. Springer, 2016. 2 [3] Aljaˇz Boˇziˇc, Pablo Palafox, Michael Zollhofer, Justus Thies, Angela Dai, and Matthias Nießner. Neural deformation graphs for globally-consistent non-rigid reconstruction. CVPR, 2021. 4 [4] Christoph Bregler, Aaron Hertzmann, and Henning Biermann. Recovering non-rigid 3d shape from image streams. In CVPR, 2000. 2 [5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, pages 96509660, 2021. 4 [6] Wei Cheng, Ruixiang Chen, Wanqi Yin, Siming Fan, Keyu Chen, Honglin He, Huiwen Luo, Zhongang Cai, Jingbo Wang, Yang Gao, Zhengming Yu, Zhengyu Lin, Daxuan Ren, Lei Yang, Ziwei Liu, Chen Change Loy, Chen Qian, Wayne Wu, Dahua Lin, Bo Dai, and Kwan-Yee Lin. DNARendering: Diverse Neural Actor Repository for HighFidelity Human-centric Rendering. arXiv, 2023. [7] Aditya Chetan, Guandao Yang, Zichen Wang, Steve Marschner, and Bharath Hariharan. Accurate differenarXiv preprint tial operators for hybrid neural fields. arXiv:2312.05984, 2023. 5 [8] Timothee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers, 2023. 5 [9] Paul Debevec, Tim Hawkins, Chris Tchou, Haarm-Pieter Duiker, Westley Sarokin, and Mark Sagar. Acquiring the In Proceedings of the reflectance field of human face. 27th annual conference on Computer graphics and interactive techniques, pages 145156, 2000. 2 [10] Mingsong Dou, Sameh Khamis, Yury Degtyarev, Philip Davidson, Sean Ryan Fanello, Adarsh Kowdle, Sergio Orts Escolano, Christoph Rhemann, David Kim, Jonathan Taylor, et al. Fusion4d: Real-time performance capture of challenging scenes. ACM Transactions on Graphics (ToG), 35(4): 113, 2016. 2 [11] Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir. Omnidata: scalable pipeline for making multitask mid-level vision datasets from 3d scans. In ICCV, pages 1078610796, 2021. 4 [12] Jean Feydy, Thibault Sejourne, Francois-Xavier Vialard, Shun-ichi Amari, Alain Trouve, and Gabriel Peyre. Interpolating between optimal transport and mmd using sinkhorn divergences. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 26812690, 2019. 4 [13] Chen Geng, Sida Peng, Zhen Xu, Hujun Bao, and Xiaowei Zhou. Learning neural volumetric representations of dynamic humans in minutes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 87598770, 2023. [14] Shubham Goel, Angjoo Kanazawa, and Jitendra Malik. In ECCV, 2020. Shape and viewpoints without keypoints. 2 [15] Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa*, and Jitendra Malik*. Humans in 4D: Reconstructing and tracking humans with transformers. In ICCV, 2023. 2, 4 [16] Chen Guo, Tianjian Jiang, Xu Chen, Jie Song, and Otmar Hilliges. Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition. CVPR, 2023. 2, 6, 7, 8 [17] Chen Guo, Tianjian Jiang, Manuel Kaufmann, Chengwei Zheng, Julien Valentin, Jie Song, and Otmar Hilliges. Reloo: Reconstructing humans dressed in loose garments from In European conference on monocular video in the wild. computer vision (ECCV), 2024. 2 [18] Marc Habermann, Weipeng Xu, Michael Zollhoefer, Gerard Pons-Moll, and Christian Theobalt. LiveCap: Real-time Human Performance Capture from Monocular Video. ACM TOG, 2019. [19] Marc Habermann, Weipeng Xu, Michael Zollhoefer, Gerard Pons-Moll, and Christian Theobalt. DeepCap: Monocular Human Performance Capture Using Weak Supervision. CVPR, 2020. 1, 2 [20] Liangxiao Hu, Hongwen Zhang, Yuxiang Zhang, Boyao Zhou, Boning Liu, Shengping Zhang, and Liqiang Nie. Gaussianavatar: Towards realistic human avatar modeling from single video via animatable 3d gaussians. 2024. 2 [21] Mustafa Isık, Martin Runz, Markos Georgopoulos, Taras Khakhulin, Jonathan Starck, Lourdes Agapito, and Matthias Nießner. HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion. ACM TOG, 2023. 6 [22] Boyi Jiang, Yang Hong, Hujun Bao, and Juyong Zhang. Selfrecon: Self reconstruction your digital avatar from monocular video. In CVPR, 2022. 2, 3, 6, 8 [23] Tianjian Jiang, Xu Chen, Jie Song, and Otmar Hilliges. Instantavatar: Learning avatars from monocular video in 60 seconds. arXiv, 2022. 2 [24] Wei Jiang, Kwang Moo Yi, Golnoosh Samei, Oncel Tuzel, and Anurag Ranjan. Neuman: Neural human radiance field In ECCV, pages 402418. Springer, from single video. 2022. 2 [25] Yue Jiang, Marc Habermann, Vladislav Golyanik, and Christian Theobalt. Hifecap: Monocular high-fidelity and expressive capture of human performances. arXiv preprint arXiv:2210.05665, 2022. [26] Hanbyul Joo, Tomas Simon, Xulong Li, Hao Liu, Lei Tan, Lin Gui, Sean Banerjee, Timothy Godisart, Bart Nabbe, Iain Matthews, et al. Panoptic studio: massively multiview system for social interaction capture. TPAMI, 41(1):190 204, 2017. 2 [27] Hanbyul Joo, Tomas Simon, and Yaser Sheikh. Total capture: 3d deformation model for tracking faces, hands, and bodies. In CVPR, 2018. 2 [28] Angjoo Kanazawa, Michael Black, David Jacobs, and Jitendra Malik. End-to-end recovery of human shape and pose. In CVPR, 2018. 2 [29] Angjoo Kanazawa, Shubham Tulsiani, Alexei A. Efros, and Jitendra Malik. Learning category-specific mesh reconstruction from image collections. In ECCV, 2018. 2 [30] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42 (4), 2023. 5 [31] Rawal Khirodkar, Timur Bagautdinov, Julieta Martinez, Su Zhaoen, Austin James, Peter Selednik, Stuart Anderson, and Shunsuke Saito. Sapiens: Foundation for human vision models, 2024. 4, [32] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. 4, 5, 8, 2 [33] Jiahui Lei, Yufu Wang, Georgios Pavlakos, Lingjie Liu, and Kostas Daniilidis. Gaussianavatar: Towards realistic human avatar modeling from single video via animatable 3d gaussians. 2024. 2 [34] Ruilong Li, Yuliang Xiu, Shunsuke Saito, Zeng Huang, Kyle Olszewski, and Hao Li. Monocular real-time volumetric perIn Computer VisionECCV 2020: 16th formance capture. European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XXIII 16, pages 4967. Springer, 2020. 2 [35] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-time view synthesis of dynamic scenes. In CVPR, 2021. 3, 1 [36] Zhaoshuo Li, Thomas Muller, Alex Evans, Russell Taylor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin. Neuralangelo: High-fidelity neural surface reconstruction. In CVPR, pages 84568465, 2023. 5 [37] Zhe Li, Zerong Zheng, Lizhen Wang, and Yebin Liu. Animatable gaussians: Learning pose-dependent gaussian maps arXiv preprint for high-fidelity human avatar modeling. arXiv:2311.16096, 2023. 2 [38] Philipp Lindenberger, Paul-Edouard Sarlin, Viktor Larsson, and Marc Pollefeys. Pixel-Perfect Structure-from-Motion with Featuremetric Refinement. In ICCV, 2021. 5 [39] Jia-Wei Liu, Yan-Pei Cao, Tianyuan Yang, Zhongcong Xu, Jussi Keppo, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Hosnerf: Dynamic human-object-scene neural raIn Proceedings of the diance fields from single video. IEEE/CVF International Conference on Computer Vision, pages 1848318494, 2023. [40] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. SMPL: skinned multiperson linear model. SIGGRAPH Asia, 2015. 2 [41] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis. 3DV, 2024. 1, 2 [42] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020. 3 [43] Tanmay Nath*, Alexander Mathis*, An Chi Chen, Amir Patel, Matthias Bethge, and Mackenzie Mathis. Using deeplabcut for 3d markerless pose estimation across species and behaviors. Nature Protocols, 2019. 4 [44] Richard Newcombe, Dieter Fox, and Steven Seitz. Dynamicfusion: Reconstruction and tracking of non-rigid scenes in real-time. In CVPR, pages 343352, 2015. 2 [45] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, ShangWen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. 4, 5, 2 [46] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes In CVPR, for novel view synthesis of dynamic humans. 2021. [47] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-NeRF: Neural Radiance Fields for Dynamic Scenes. In CVPR, 2020. 2 [48] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization. In ICCV, pages 23042314, 2019. 2 [49] Shunsuke Saito, Jinlong Yang, Qianli Ma, and Michael J. Black. SCANimate: Weakly supervised learning of skinned clothed avatar networks. In CVPR, 2021. 1 [50] Peter Sand and Seth Teller. Particle video: Long-range motion estimation using point trajectories. In IJCV, 2008. 2 [51] Robert Sumner, Johannes Schmid, and Mark Pauly. Embedded deformation for shape manipulation. In ACM SIGGRAPH 2007 papers. 2007. 4 [52] Zachary Teed and Jia Deng. RAFT: Recurrent all-pairs field transforms for optical flow. In ECCV, 2020. 4, [53] Daniel Vlasic, Ilya Baran, Wojciech Matusik, and Jovan Popovic. Articulated mesh animation from multi-view silhouettes. In SIGGRAPH 2008. 2008. 5 [54] Yiming Wang, Qin Han, Marc Habermann, Kostas Daniilidis, Christian Theobalt, and Lingjie Liu. Neus2: Fast learning of neural implicit surfaces for multi-view reconIn Proceedings of the IEEE/CVF International struction. Conference on Computer Vision (ICCV), 2023. 5 [55] Chung-Yi Weng, Brian Curless, Pratul Srinivasan, Jonathan Barron, and Ira Kemelmacher-Shlizerman. Humannerf: Free-viewpoint rendering of moving people from monocular video. In CVPR, pages 1621016220, 2022. 2, 6 [56] Shangzhe Wu, Tomas Jakab, Christian Rupprecht, and Andrea Vedaldi. Dove: Learning deformable 3d objects by watching videos. arXiv preprint arXiv:2107.10844, 2021. 2 [73] Tao Yu, Kaiwen Guo, Feng Xu, Yuan Dong, Zhaoqi Su, Jianhui Zhao, Jianguo Li, Qionghai Dai, and Yebin Liu. Bodyfusion: Real-time capture of human motion and surface geometry using single depth camera. In Proceedings of the IEEE International Conference on Computer Vision, pages 910919, 2017. 2 [57] Shangzhe Wu, Ruining Li, Tomas Jakab, Christian Rupprecht, and Andrea Vedaldi. MagicPony: Learning articulated 3d animals in the wild. 2023. 2 [58] Donglai Xiang, Hanbyul Joo, and Yaser Sheikh. Monocular total capture: Posing face, body, and hands in the wild. In CVPR, 2019. [59] Donglai Xiang, Fabian Prada, Timur Bagautdinov, Weipeng Xu, Yuan Dong, He Wen, Jessica Hodgins, and Chenglei Wu. Modeling clothing as separate layer for an animatable human avatar. ACM Transactions on Graphics (TOG), 40(6): 115, 2021. 2 [60] Donglai Xiang, Fabian Prada, Zhe Cao, Kaiwen Guo, Chenglei Wu, Jessica Hodgins, and Timur Bagautdinov. Drivable avatar clothing: Faithful full-body telepresence with dynamic clothing driven by sparse rgb-d input. arXiv preprint arXiv:2310.05917, 2023. 2 [61] Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and Michael J. Black. ICON: Implicit Clothed humans Obtained from Normals. CVPR, 2022. 2 [62] Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, and Michael J. Black. ECON: Explicit Clothed humans Optimized via Normal integration. CVPR, 2023. 2, 6, [63] Weipeng Xu, Avishek Chatterjee, Michael Zollhofer, Helge Rhodin, Dushyant Mehta, Hans-Peter Seidel, and Christian Theobalt. MonoPerfCap: Human Performance Capture from Monocular Video. ACM TOG, 2018. 1, 2 [64] Zhen Xu, Sida Peng, Haotong Lin, Guangzhao He, Jiaming Sun, Yujun Shen, Hujun Bao, and Xiaowei Zhou. 4K4D: arXiv, Real-Time 4D View Synthesis at 4K Resolution. 2023. 1 [65] Gengshan Yang and Deva Ramanan. Volumetric corresponIn NeurIPS, 2019. 4, 5, dence networks for optical flow. 2 [66] Gengshan Yang, Deqing Sun, Varun Jampani, Daniel Vlasic, Forrester Cole, Huiwen Chang, Deva Ramanan, William Freeman, and Ce Liu. LASR: Learning articulated shape reconstruction from monocular video. In CVPR, 2021. 2, 3, 4, 1 [67] Gengshan Yang, Deqing Sun, Varun Jampani, Daniel Vlasic, Forrester Cole, Ce Liu, and Deva Ramanan. Viser: Videospecific surface embeddings for articulated 3d shape reconstruction. In NeurIPS, 2021. 2, [68] Gengshan Yang, Minh Vo, Neverova Natalia, Deva Ramanan, Andrea Vedaldi, and Hanbyul Joo. Banmo: Building animatable 3d neural models from many casual videos. In CVPR, 2022. 2, 3, 4, 6, 7, 8 [69] Gengshan Yang, Chaoyang Wang, Dinesh Reddy, and Deva Ramanan. Reconstructing Animatable Categories from Videos. CVPR, 2023. 2, 6, 7, 1 [70] Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything: Segment anything meets videos, 2023. 5 [71] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces. NeurIPS, 2021. 4, 1 [72] Yufei Ye, Shubham Tulsiani, and Abhinav Gupta. Shelfsupervised mesh prediction in the wild. In CVPR, 2021. DressRecon: Freeform 4D Human Reconstruction from Monocular Video"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Video Results Please see the attached webpage for video results. 7. Implementation Details 7.1. Consistent 4D Neural Fields Signed distance fields. We initialize canonical signed distance fields as sphere with radius 0.1m. Following standard practice, we apply positional encodings to all 3D points (Lxyz = 10) and timestamps (Lt = 6) before passing into MLPs. The appearance code ωt has 32 channels. After MLPSDF computes the signed distance at 3D point, we convert the signed distance to volumetric density σ [0, 1] for volume rendering. Similar to VolSDF [71], this is done using the cumulative Laplace distribution σ = Γβ(d), where β is global learnable scalar parameter that controls the solidness of the object, approaching zero for solid objects. This representation allows us to extract mesh as the zero level-set of the SDF. Cycle consistency regularization. Given forward warping field +(t) : Xt and backward warping field (t) : Xt X, we introduce cycle consistency term, similar to NSFF [35]. sampled 3D point in camera coordinates should return to its original location after passing through backward and forward warping: Lcyc = (cid:88) Xt +(W (Xt, t), t) Xt2 2 (16) 7.2. Hierarchical Gaussian Motion Fields Bag-of-bones skinning deformation. Our motion model uses the motion of bones (defined as 3D Gaussians, typically = 25) to drive the motion of canonical geometry. Given 3D Gaussians, we compute dense 3D motion fields by blending the SE(3) transformations of canonical Gaussians with skinning weights W: Xt = +(X, t) = (cid:32) (cid:88) W+,bGb (cid:0)Gb(cid:1)1 (cid:33) (17) = (Xt, t) = b=1 (cid:32) (cid:88) b=1 W,b Gb (cid:0)Gb (cid:33) (cid:1)1 Xt (18) where G+ Gaussians, cal Gaussians, and W+ are forward skinning weights. are forward warps from canonical to time are backward warps from time to canoniSimilar to SCANimate [49] and LASR [66], we define forward skinning weight function + : RB which computes the normalized influence of each Gaussian bone on canonical 3D point. At coarse level, skinning weights are defined as the Mahalanobis distance from to the canonical Gaussians: W+ σ = (X µ)Q(X µ), (19) where µ RB3 are canonical bone centers, = VΛV are canonical bone precision matrices, RBSO(3) are canonical bone orientations, and ΛB33 are time-invariant axis-aligned diagonal scale matrices. In addition to coarse component, we find it helpful to use delta skinning weights to model fine geometry. Delta skinning weights are computed by coordinate MLP: W+ = MLP,+(X, t) RB (20) The final skinning function is normalized sum of coarse and fine components: W+ = +(X, t) = softmax(W+ σ W+ ), (21) where the negative sign ensures that faraway Gaussian bones (which have larger Mahalanobis distance) are assigned lower skinning weight after softmax. Backward skinning weights are computed analogously with the time Gaussians, which have center µt, orientation Vt, and time-invariant scale Λ. We also need the transformation from each time Gaussian to the canonical Gaussian, as well as the backward skinning MLP . 7.3. Optimization Sampling. Due to the expensive per-ray computation in volume rendering, optimization with batch gradient descent is challenging. As result, previous methods randomly sample entire images [69] to compute the reconstruction terms, leading to small batch sizes (typically 16 images per batch) and noisy gradients. We implement an efficient data-loading pipeline with memory-mapping that allows per-pixel measurements (e.g., RGB, flow, features) to load directly from disk without accessing the full image. This allows loading pixels from significantly more images in single batch (e.g. 256 images on GPU). Hyperparameters. We use the Adam optimizer with learning rate 0.0005. We use 48k iterations of optimization for all experiments. On single RTX 4090 GPU, it takes about 8 hours to optimize the neural implicit body model and 15 seconds to render each frame. 3D Gaussian refinement is performed for another 48k iterations of optimization, taking about 8 hours to optimize and 0.1 seconds to render each Table 7. Summary of losses and loss weights. Our final loss is weighted sum of reconstruction terms (color, optical flow, normal, feature, and segmentation) and regularization terms (eikonal, cycle-consistency, gaussian consistency, camera prior, and joint prior)."
        },
        {
            "title": "Description",
            "content": "Lc Lf Ln Lϕ Ls λc = 0.1 λf = 0.5 λn = 0.03 λϕ = 0.01 λs = 0.1 L2 loss, rendered RGB vs. the input image L2 loss, rendered 2D flow vs. computed flow from VCNPlus [65] L2 loss, rendered normals vs. computed normals from Sapiens [31] L2 loss, rendered features vs. computed features from DINOv2 [45] L2 loss, rendered masks vs. computed masks from SAM [32]"
        },
        {
            "title": "Leik\nLcyc\nLgauss\nLcloth",
            "content": "λeik = 0.01 λcyc = 0.05 λgauss = 0.2 λcloth = 0.1 Minimize the magnitude of clothing deformation Encourage numerical gradients of canonical SDF to have unit norm Encourage backward and forward warping fields to be inverses Sinkhorn divergence between canonical 3D Gaussians and SDF frame. Our loss weights are described in Tab. 7. At each iteration, we sample 72 images and take 16 pixel samples per image. For training efficiency, input images are cropped to tight bounding box around the object and resized to 256x256. To prevent floater artifacts from appearing outside the tight crop, 90% of pixel samples are taken from the tight bounding box and 10% of pixel samples are taken from the full un-cropped image."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University, USA"
    ]
}