{
    "paper_title": "Listener-Rewarded Thinking in VLMs for Image Preferences",
    "authors": [
        "Alexander Gambashidze",
        "Li Pengyi",
        "Matvey Skripkin",
        "Andrey Galichin",
        "Anton Gusarov",
        "Konstantin Sobolev",
        "Andrey Kuznetsov",
        "Ivan Oseledets"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Training robust and generalizable reward models for human visual preferences is essential for aligning text-to-image and text-to-video generative models with human intent. However, current reward models often fail to generalize, and supervised fine-tuning leads to memorization, demanding complex annotation pipelines. While reinforcement learning (RL), specifically Group Relative Policy Optimization (GRPO), improves generalization, we uncover a key failure mode: a significant drop in reasoning accuracy occurs when a model's reasoning trace contradicts that of an independent, frozen vision-language model (\"listener\") evaluating the same output. To address this, we introduce a listener-augmented GRPO framework. Here, the listener re-evaluates the reasoner's chain-of-thought to provide a dense, calibrated confidence score, shaping the RL reward signal. This encourages the reasoner not only to answer correctly, but to produce explanations that are persuasive to an independent model. Our listener-shaped reward scheme achieves best accuracy on the ImageReward benchmark (67.4%), significantly improves out-of-distribution (OOD) performance on a large-scale human preference dataset (1.2M votes, up to +6% over naive reasoner), and reduces reasoning contradictions compared to strong GRPO and SFT baselines. These results demonstrate that listener-based rewards provide a scalable, data-efficient path to aligning vision-language models with nuanced human preferences. We will release our reasoning model here: https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 2 3 8 2 2 . 6 0 5 2 : r Listener-Rewarded Thinking in VLMs for Image Preferences Alexander Gambashidze1,2, Li Pengyi1,2, Matvey Skripkin1,2, Andrey Galichin1,2, Anton Gusarov1,2, Konstantin Sobolev1, Andrey Kuznetsov1,2, Ivan Oseledets1,2 1Artificial Intelligence Research Institute, Moscow, Russia 2Skolkovo Institute of Science and Technology, Moscow, Russia"
        },
        {
            "title": "Abstract",
            "content": "Training robust and generalizable reward models for human visual preferences is essential for aligning text-to-image and text-to-video generative models with human intent. However, current reward models often fail to generalize and supervised fine-tuning leads to memorization, demanding complex annotation pipelines. While reinforcement learning (RL)specifically Group Relative Policy Optimization (GRPO)improves generalization, we uncover key failure mode: significant drop in reasoning accuracy occurs when models reasoning trace contradicts that of an independent, frozen vision-language model (listener) evaluating the same output. To address this, we introduce listener-augmented GRPO framework. Here, the listener re-evaluates the reasoners chain-of-thought to provide dense, calibrated confidence score, shaping the RL reward signal. This encourages the reasoner not only to answer correctly, but to produce explanations that are persuasive to an independent model. Our listener-shaped reward scheme achieves best accuracy on the ImageReward benchmark (67.4%), significantly improves out-of-distribution (OOD) performance on large-scale human preference dataset (1.2M votes, up to +6% over naive reasoner), and reduces reasoning contradictions compared to strong GRPO and SFT baselines. These results demonstrate that listener-based rewards provide scalable, data-efficient path to aligning vision-language models with nuanced human preferences. We will release our reasoning model here https://huggingface.co/alexgambashidze/qwen2. 5vl_image_preference_reasoner."
        },
        {
            "title": "Introduction",
            "content": "Optimizing powerful visionlanguage models (VLMs) to robustly capture human visual preferences is key open problem in generative modeling. Preference models are central to downstream applications in text-to-image and text-to-video generation [32], as they determine both alignment with user intent and the ability to generalize beyond training distributions. However, common approaches like supervised fine-tuning (SFT) for reward modeling are fundamentally limited: SFT often leads to memorization [8], and general reward models frequently lack robustness to distribution shifts [13], necessitating expensive, large-scale annotation pipelines that are difficult to scale and insufficiently capture subjective, nuanced human choices. Reinforcement learning (RL) offers more scalable paradigm for preference alignment. Indeed, the development of highly capable models such as xAIs Grok [29], OpenAIs O1 and O3 [20, 21], Googles Gemini 2.5 Pro [9] and DeepSeeks models [10] (which introduced large-scale Group alexandergambashidze@gmail.com Preprint. Figure 1: While naive GRPO provides good generalization and already outperforms supervised fine-tuning, we observe that the reasoning model often contradicts itself, giving correct answers that do not actually align with reasoning trace. Our Listener mechanism helps with this issue: the model aligns reasoning with the answer better which gives boost in OOD accuracy. Relative Policy Optimization - GRPO), has highlighted RLs potential to improve generalization, particularly when paired with chain-of-thought reasoning [? 30]. GRPO, for instance, has demonstrated improved generalization in visual domains by stabilizing learning via group-wise normalization and removing the value network bottleneck [10]. Nevertheless, our analysis uncovers systematic failure mode in naive RL-based preference reasoners: when generating chain-of-thought explanations, models frequently produce answers that contradict the evaluations of frozen, instruction-tuned VLM (listener) on the same reasoning trace (illustrated in Figure 1). We quantify this listener disagreement and show it is strongly correlated with significant reductions in final accuracyhighlighting key misalignment between explanation plausibility and decision correctness. To address this, we propose listener-augmented GRPO framework for preference reasoning. Specifically, we extend the GRPO objective with listener-shaped soft reward: frozen VLM listener independently re-processes the reasoners chain-of-thought (excluding the final answer token) and outputs calibrated confidence score for the correct choice. This score is integrated into the RL reward signal, directly penalizing explanations that fail to convince an independent model, and providing dense, data-efficient supervision without the need for extra human annotation. We evaluate our approach using the Qwen 2.5 VL architecture on both in-domain and challenging outof-distribution (OOD) datasets. Our method achieves state-of-the-art accuracy on the ImageReward test set (67.4%) and substantially outperforms strong GRPO and SFT baselines on the large-scale (1.2M vote) Rapidata-HSP benchmark [23], while reducing contradictory reasoning events. Furthermore, we demonstrate that listener-shaped rewards yield more calibrated outputs and enhanced OOD robustness, even when trained on fraction of available preference data. In summary, our technical contributions are: To the best of our knowledge, we are the first to train chain-of-thought style reasoning models to predict human visual preferences for outputs of generative models. Empirical identification and quantification of listener disagreement as principal failure mode in RL-based visual preference modeling. Design of novel listener-shaped soft reward mechanism for GRPO, using independent model disagreement to align both reasoning traces and final decisions. Experimental validation on benchmarks, demonstrating superior accuracy, OOD generalization, and reduced reasoning contradictions. Our results suggest that listener-augmented RL is an effective, practical tool for preference alignment in VLMs, offering scalable solution for next-generation text-to-image and text-to-video systems."
        },
        {
            "title": "2 Related Work",
            "content": "Visual preference models. Early reward models such as ImageReward [32] and PickScore [17] boost in-domain alignment yet struggle on modern generative outputs. HPSv2 [28] improves robustness with larger, biascontrolled dataset, but the collection process is costly and its generalization is already limited. The recent VisionReward framework [31] tackles generalization by annotating multiobjective sub-criteria (aesthetics, realism, prompt fidelity) and fine-tuning VLMs. While effective, its hierarchical markup pipeline further increases annotation complexity. Industry adoption therefore still seeks light-weight preference tuning strategies that generalize without human annotations. RL vs. SFT for alignment. growing evidence shows that reinforcement learning gets better generalization than supervised fine-tuning. [8] demonstrate that SFT memorizes training distributions, whereas RL variants transfer better across tasks. The DeepSeek line adopts Group Relative Policy Optimization (GRPO) to scale reasoning models with minimal overhead [25, 1]. GRPOs groupnormalized update stabilizes learning without value head, making it attractive for vision-language fine-tuning. Direct preference optimization. DPO removes the need to fit separate reward model by directly optimizing contrastive preference objective [22]. It has been successfully adapted beyond language to diffusion and video generators: DIFFUSION-DPO significantly improves Stable Diffusion XL [27], they report even better results when ranking images with reward model; the 30B-parameter STEP-VIDEO T2V model applies Video-DPO to cut artifacts and lift FID/CLIPScores across new video benchmark [11]; HUNYUANVIDEO fine-tunes its 13 text-to-video backbone with preference-optimized loss [18]; LTX-VIDEO reports +15% preference swing after lightweight DPO pass, while retaining real-time generation speed [16]. These industry results highlight the demand for simple preference methods that scale without complex annotation pipelinesan objective our listener-shaped GRPO strategy fulfills while further improving reasoning consistency. Our contribution. Building on the insight that RL generalizes better than SFT, we uncover novel listener disagreement failure mode: VLM accuracy consistently declines as the distance between its predictions and those of an independent listener increases. To address this, we propose listenershaped GRPO objective that improves out-of-distribution generalization without requiring additional annotations or complex multi-objective markupoffering precisely the kind of simplicity needed for scalable T2I/T2V systems."
        },
        {
            "title": "3 Preliminaries",
            "content": "Vision-Language Models (VLMs) are large language models that take both text and image embeddings as input and generate text, enabling multimodal reasoning. Visual generative models, such as diffusion models (e.g., Stable Diffusion, DALLE 3, Flux), synthesize images by iteratively denoising random noise, with text conditioning provided by text encoders, LLMs, or VLMs. Our work focuses on aligning the outputs of such generative models with human preferences using VLM-based reasoner. 3.1 Predicting Human Preferences Given text prompt and pair of images I, we learn scoring function fθ : (I, ) (cid:55) [0, 1], where higher values indicate stronger human preference. For set of images {I1, . . . , IN } associated with the same prompt, pairwise preference is inferred by score comparison; i.e. Ia is preferred over Ib if fθ(Ia, ) > fθ(Ib, ). Modelling scalar score instead of direct binary label enables the soft reward scheme introduced in 4. 3.2 Datasets Benchmarking visual preference models requires careful curation of both training and evaluation datasets to ensure fair comparison, generalization assessment, and reproducibility. The protocol in this paper involves: 3 Training on established datasets: Models are trained using widely adopted datasets, such as IMAGEREWARD and HPSV2. These contain large numbers of annotated image pairs, generated from variety of older diffusion models (e.g., Stable Diffusion versions 1.x, 1.4/1.5). Out-of-Distribution (OOD) evaluation: Generalization is assessed using newer, more challenging datasets. For instance, RAPIDATA-HSP is constructed from the outputs of state-of-the-art generative models (e.g., DALLE 3, Midjourney v6, Flux), which are not present in the training data, providing robust test for OOD performance. Pairwise and absolute scoring: Evaluation considers both pairwise preference (which of two images is preferred for prompt) and, where applicable, absolute scoring (assigning continuous score to each image), following community best practice. This protocol is motivated by the need to evaluate models under distribution shift, to prevent overfitting to older generative models and to provide large and robust benchmarking. Table 1 summarises the specific datasets used in this work, detailing their scale, source models, and how they are utilized for training and evaluation. Dataset Scale Source models Split ImageReward [32] HPSv2 [28] Rapidata-HSP[23] 137 pairs 798 pairs 1.2 votes SD 1.x SD 1.4/1.5 train / val / test train Flux, DALLE 3, MJ v6 whole train (OOD) Table 1: Datasets used for training and evaluation. Usage train/eval train eval 3.3 Group Relative Policy Optimisation (GRPO) We train the preference reasoner with Group Relative Policy Optimisation (GRPO) [10], variant of PPO that removes the value network by normalising rewards within rollout group of size G. For each state we sample actions {ai, ri}G i=1 and compute group-normalised2 advantage Ai = ri µ σ + ε , µ = 1 (cid:88) ri, σ2 = 1 (ri µ)2. (cid:88) The policy update maximises the clipped objective L(θ) = Ei (cid:2)min(cid:0)ρiAi, clip(ρi, 1ϵ) Ai (cid:1) λ KL(cid:0)πθ(s) πref(s)(cid:1)(cid:3) (1) (2) where ρi = πθ(ais)/πθold (ais). GRPOs group-based normalisation stabilises training and eliminates the memory overhead of separate value head. In the following section, we describe the main failure mode observed when applying standard RL methods to visual preference reasoning, and introduce our proposed listener-shaped reward framework to address it."
        },
        {
            "title": "4 Soft–Listener Rewarding",
            "content": "4.1 Motivation: The Listener Disagreement Failure Mode While reinforcement learning (RL) via GRPO improves the generalization of visual preference models, we observe critical failure mode unique to reasoning-based reward learning: When the reasoners generated chain-of-thought (CoT) is evaluated by an independent, frozen visionlanguage model (VLM), the \"listener\", reasoning traces that are plausible to the reasoner may still fail to convince the listener, leading to sharp drops in final accuracy. Empirically, we find that accuracy even on in-distribution data (ImageReward test set) consistently declines as the divergence between the reasoners and listeners assessments of reasoning trace increases (see Figure 2). This suggests that robust alignment demands not only correct answers, but explanations that are independently persuasive and consistent across VLMs. 2We use = 10. 4 Figure 2: Listenerreasoner disagreement is strong error signal. Each point aggregates ImageReward test pairs whose ℓ2 distance (sinstr )2 falls in bin. Accuracy drops as the two score vectors diverge. ) (sreason , sreason 2 , sinstr 2 1 1 4.2 Soft rewards Let = {V, P, T, A<t} denote the conditioning context (visual input , prompt , reasoning tokens and partial answer A<t). The policy πθ outputs logits (xt C). Single-image scoring. With rating vocabulary = {r1, . . . , rn} the penultimate step (t = tans1) produces p(ri C) = exp (ri C) exp (rj C) (cid:80) , (cid:88) ˆs = ri p(ri C) [0, 1]. (3) The scalar ˆs serves as dense reward during RL and continuous metric at test time. Pairwise preference with an anchor. During training we learn binary classifier {0, 1} for image pairs (Ia, Ib). At inference, to score >2 images we anchor-sample: pick one image Ianc, assign it score 0.5, and compare every other image Ik to the anchor in single forward pass: p(1 C) = σ(cid:0)f (1 C) (0 C)(cid:1), σ(z) = 1 1 + ez . (4) 4.3 Listener-augmented Reward pure RL reasoner can produce fluent yet incorrect explanations. To quantify this, we feed each reasoning trace (excluding the final answer token) to the frozen listener πref and record its soft-score for the winner. Figure 2 shows that accuracy collapses as the listeners soft scores diverge from the reasonersrevealing an actionable failure signal. Listener-augmented reward. Let pcorr = πref(y C) be the listeners probability for the correct image y. Define rlist = max(cid:0)0, pcorr 0.5(cid:1), and combine with formatting check rfmt {0, 1}: racc = I[a = y], = rfmt + 0.5 racc + 0.5 rlist . (5) The strict term preserves unbiased supervision, whereas the listener term supplies dense rewards that push the reasoner to produce explanations that are convincing to an independent judge. In 5 we show that listener-shaped rewards improve in-domain accuracy over strong GRPO baseline, and significantly raise OOD accuracy. 5 Figure 3: Accuracy on the high-quality [23] modern dataset at different human agreement thresholds. Listener mechanism consistently improves generalization beyond the strong GRPO baseline. Supervised Fine-Tuning and Reasoners are initialized from the same Qwen2.5-VL-7B-Instruct checkpoint."
        },
        {
            "title": "5 Experiments",
            "content": "We initialize our models with Qwen 2.5-VL-7B-Instruct)[26] and evaluate on the ImageReward test set and large, high-quality modern dataset [23], using different training strategies for absolute and pairwise preference modeling. For absolute score prediction, we train on the ImageReward train split using soft scalar rewards applied during both training and inference. We combine three components: (1) formatting reward [10], (2) an exact match reward, and (3) an approximate match reward based on the distance between predicted and ground-truth scores (reward = 1.0 if distance is 0; 0.75 if 1; 0.5 if 2; 0 otherwise). However, we find that training to predict absolute scores generalizes poorly. Because score distributions are tightly tied to dataset quality, models trained on older ImageReward data assign maximum scores to most high-quality images, failing to differentiate among them. The result is near-random behavior on out-of-distribution (OOD) prompts, it only slightly surpasses original ImageReward which itself suffers from very poor generalization. In pairwise setting, we train reasoner model using 16% randomly sampled HPSv2 pairs (due to compute constraints). For the base GRPO model, we apply two rewards: formatting check and exact match. To train our listener-augmented model, we reshape the reward just as we discussed in previous section. For inference in the pairwise setting, we use an anchor-based comparison strategy: for each prompt with images, we randomly select one image as the anchor, assign it score of 0.5, and compute soft win probabilities for each remaining image relative to the anchor. This reduces the complexity of the inference from O(n2) to O(n). We do not evaluate straightforward O(n2) for ImageReward inference due to its impractical unreasonable complexity. We summarize ImageReward results in Table 2 and 3. 6 Training Dataset Mean@1 (%) Mean@3 (%) ImageReward HPDv2 + Listener 65.8 67.4 67.7 65.4 67.2 67.4 Table 2: Accuracy of Qwen 2.5 VL trained with GRPO. Averaging scores across multiple reasoning rollouts improves performance, consistent with trends in LLMs. Training on only 16% of HPSv2 (randomly sampled) preserves strong performance, indicating that RL-based reasoning is data-efficient and robust. Reasoning steps also act like ensembling: multiple rollouts improve accuracy further. We also compare zero-shot performance and listener-enhanced models on the ImageReward test set  (Table 3)  : Method Single Human [28] HPSv2.1 [28] Zero-Shot Reasoning ImageReward GRPO ImageReward (abs score) SFT on HPDv2 GRPO on HPDv2 + Listener Accuracy (%) 65.3 66.8 55.4 65.1 65.4 66.9 67.2 67.4 Table 3: Performance on the ImageReward test set. GRPO-trained reasoner surpasses previous methods; listener reward further improves generalization. We find that using absolute score regression trained on ImageReward generalizes poorly to highquality image distributions. The model tends to predict maximal scores for all samples, failing to discriminate due to overfitting to older, lower-quality generations. In contrast, listener-based soft rewards lead to better calibrated outputs and more stable generalization across domains. Generalization to modern datasets. To test generalization, we evaluate our models on modern preference dataset generated by models such as Flux Pro, DALLE 3, and Stable Diffusion 3.5. We use two versions of it: Vote-distribution dataset: includes soft preference labels (e.g., 60/40 vote splits). Since ambiguous votes reduce evaluation reliability, we bin samples by human agreement threshold (from 50% to 100%) and report accuracy per bin (Figure 3). Binarized: contains only examples with clear majority votes. We select pairs where the winner has at least 80% of votes and total number of votes is greater than 15. We observe that integrating the instruct model as listener yields consistent gains across all evaluation setups. Notably, the \"Reasoner + Listener\" model surpasses both the GRPO-only and soft-reward-only versions, supporting our hypothesis that aligning reasoning traces with an external reference improves generalization and robustness. To further show that Listener mechanism actually gives more robust answers, we evaluate the number of contradictions that appear in reasoning traces. We simply inference Qwen2.5-14B-Instruct [26] with the following system prompt: You are an expert factual verifier. Determine whether the models final answer contradicts its reasoning. Reply with the single word YES if it contradicts, otherwise NO. Answer only YES or NO. Then we provide reasoning traces and answer. We observe that Listener mechanism indeed pushes the model to be more consistent. We observe that there is still room for further research. This highlights that plausible reasoning traces are not enoughmodels must also produce reasoning that leads to correct decisions. Listener supervision helps enforce that alignment. Do reasoning traces actually help generalization? To test whether reasoning itself is required, we follow [19] and replace the chain-of-thought with the fixed string have finished thinking. All other 7 Model Contradictory Examples (%) Reasoner Reasoner + Listener 10.1 8.3 Table 4: Evaluation on contradictory samples from the high-quality OOD set. Listener-enhanced model shows marginally less contradictions in its answers. Here we also use 5139 most confident samples with at least 20 votes. We use Qwen2.5-14B-Instruct [26] as contradiction judge. settings are kept constant. Surprisingly, we do not observe drop in accuracy on baseline reasoner, while strategy with Listener drops it significantly, results in Table 5 confirm that the Listener pushes the Reasoner to rely more on reasoning traces. Model Accuracy (%) Reasoner w/ thinking Reasoner w/o thinking with Listener w/ thinking with Listener w/o thinking 72 1.2 73 1.2 76 1.2 70 1.2 Table 5: We evaluate accuracy with \"I have finished thinking\" instead of full reasoning traces. We use the 5139 most confident samples from high-quality out-of-distribution dataset that have at least 20 votes. Results show that reasoning traces positively influence generalization, and the listener model relies on them more effectively. Furthermore, we observe poor majority voting improvements in OOD, Figure4. This supports observations of [33] that reasoners majority vote effect strongly correlates with the base model majority voting. Figure 4: Majority voting across multiple reasoning rollouts improves models insignificantly in OOD. Training setup. We train full model (no adapter) with GRPO using 8H100 GPUs, constant learning rate of 1e6, batch size of 1 with 4 gradient accumulation steps, and sequence length of 512, we set temperature 1.1, other parameters are default. Group size for RL is 10 reasoning rollouts. During inference, if soft score is below 0.5, we set the reward to zero; otherwise, we assign it 1 winner_score and add 0.5 times the strict reward. All models within same train dataset have the same system prompt: \"The user has two images and textual prompt. You need to reason carefully and produce an answer with reasoning in <think>...</think> where you should choose best image.\" and the same user prompt: f\"User prompt: {prompt} Which image is better given the prompt? Analyze aesthetics, composition, prompt alignment and other factors. Provide your reasoning in <think>. . . </think> tags and the final JSON answer in <answer>\"preferred\":\"second\"</answer> or \"preferred\":\"first\". We train reasoning models for 4000 training steps."
        },
        {
            "title": "6 Conclusion",
            "content": "We presented reinforcement learning framework for training visual language models to reason about human preferences. By combining test-time chain-of-thought generation with soft, listener-shaped rewards, our method improves both accuracy and generalization across in-distribution and out-ofdistribution preference datasets. We showed that reasoning traces alone are not sufficientmodels must generate explanations that are also persuasive to an independent listener. This insight allows us to transform model disagreement into dense and informative reward signal. Our experiments demonstrate that naive GRPO already outperforms existing baselines, and that incorporating frozen instruction-tuned model as listener leads to further improvements. This shows listener-shaped rewards as practical tool for aligning generative models with nuanced human judgments, even under limited supervision. We believe this direction opens new opportunities for preference-based training in both vision and language domains, especially where data is noisy, supervision is scarce, and generalization is critical. We also believe that our ideas will help significantly improve general-purpose language models and we will explore adaptations of listener mechanism in the future work."
        },
        {
            "title": "7 Limitations & Future Work",
            "content": "While listener-shaped GRPO delivers state-of-the-art performance on IMAGEREWARD and strong gains on modern out-of-distribution dataset, several open directions remain: 1. Residual reasoning inconsistencies. The listener mechanism reducesbut does not eliminatecases where the reasoners explanation contradicts its final decision  (Table 4)  . More targeted feedback, such as contradiction or hallucination detection, could further improve consistency but will require extra compute overhead. 2. General-purpose applicability. While our method is evaluated on visual preferences, the listener-shaped reward strategy is model-agnostic. Extending this approach to other domainse.g., math, programming, instruction followingoffers an exciting direction for aligning reasoning traces in LLMs. We also note that our results were achieved without large-scale hyperparameter tuning, full use of available preference data (e.g., only 16% of HPDv2) and huge compute budget. This suggests that listener-shaped GRPO can be an efficient and scalable component for broader preference optimization pipelines in T2I/T2V and beyond."
        },
        {
            "title": "Societal Impacts",
            "content": "Our methods for improving vision-language models understanding of human visual preferences have several societal implications. Positive Impacts. By enabling models to better generalize from limited and subjective preference data, our approach can help create more aligned and user-intent-aware generative models. This benefits creative industries, personalized content, and accessibility. The listener mechanism also encourages models to provide transparent reasoning, supporting interpretability and trust. Dataefficient RL further lowers barriers to developing such systems. Potential Risks and Considerations. More capable generative models can be misused for disinformation, non-consensual imagery, or may reinforce societal biases present in training data. While our focus is technical, we recognize the dual-use nature of these advances and the need for ongoing research into safety and ethical safeguards. We hope our work contributes to more controllable and ethically-aligned AI systems, and encourage the community to further study both the benefits and risks of such technologies."
        },
        {
            "title": "References",
            "content": "[1] Deepseek-r1: Scaling reasoning with reinforced learning. Technical report, DeepSeek-AI, 2024. Technical report. 9 [2] Hunyuan-video: Scaling text-to-video generation with large-scale rlhf. Technical report, Tencent AI Lab, 2024. [3] Ltx-video: Large transformer for controllable video generation. Technical report, LTX Lab, 2024. [4] Sora: Multi-modal video generation at scale. Technical report, OpenAI, 2024. Technical report. [5] Want2v: High-fidelity text-to-video synthesis via direct preference optimization. Technical report, WanAI, 2024. [6] Yanda Chen, Joe Benton, Ansh Radhakrishnan, Jonathan Uesato, Carson Denison, John Schulman, Arushi Somani, Peter Hase, Misha Wagner, Fabien Roger, Vlad Mikulik, Samuel R. Bowman, Jan Leike, Jared Kaplan, and Ethan Perez. Reasoning models dont always say what they think. arXiv preprint arXiv:2505.05410, 2025. [7] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, et al. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. [8] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, jan 2025. Accessed: March 24, 2025. [9] Google DeepMind. Gemini 2.5 pro: Advanced multimodal reasoning model. https:// deepmind.google/technologies/gemini/pro/, 2025. Product page and capability demo. Accessed 2025-05-16. [10] DeepSeek. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Technical report, DeepSeek, 2023. [11] Guoqing Ma et al. Step-video-t2v technical report: The practice, challenges, and future of video foundation model. Technical report, StepFun, 2025. arXiv:2502.10248. [12] Chun et al. The poison of alignment. arXiv preprint arXiv:2308.13449, aug 2023. Accessed: March 24, 2025. [13] Jordan Hoffmann et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, March 2022. [14] Alexander Gambashidze, Pavel Kulikov, Maxim Sosnin, and Ivan Makarov. Aligning diffusion models with noise-conditioned perception. arXiv preprint arXiv:2406.17636, 2025. [15] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. arXiv preprint arXiv:2210.10760, oct 2022. Accessed: March 24, 2025. [16] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, and Dudu Moshe et al. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. [17] Yuval Kirstain, Adam Polyak, Uriel Singer, et al. Pick-a-pic: An open dataset of user preferences for text-to-image generation. In NeurIPS, 2023. [18] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, and Jin Zhou et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [19] Wenjie Ma, Jingxuan He, Charlie Snell, Tyler Griggs, Sewon Min1, and Matei Zaharia. Reasoning models can be effective without thinking. 2025. [20] OpenAI. OpenAI o1: Learning to reason with reinforcement learning. https://openai. com/index/learning-to-reason-with-llms, 2024. System card released Dec 5 2024. Accessed 2025-05-16. [21] OpenAI. OpenAI o3: multimodal model for math, science, coding, and visual reasoning. https://platform.openai.com/docs/models/o3, 2025. Model announcement Apr 2025. Accessed 2025-05-16. [22] Rafael Rafailov, Archit Sharma, Eric Mitchell, et al. Direct preference optimization: Your language model is secretly reward model. arXiv preprint arXiv:2305.18290, 2023. [23] Rapidata. Rapidata human style preferences for images. https://huggingface.co/ datasets/Rapidata/human-style-preferences-images, 2025. [24] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, July 2017. [25] Zhihong Shao, Peiyi Wang, Qihao Zhu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [26] Qwen Team. Qwen2.5: party of foundation models, September 2024. [27] Bram Wallace, Meihua Dang, Rafael Rafailov, et al. Diffusion model alignment using direct preference optimization. arXiv preprint arXiv:2311.12908, 2023. [28] Xiaoshi et al. Wu. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, June 2023. [29] xAI. Grok-3: The age of reasoning agents. https://x.ai/blog/grok-3, 2025. System card and model overview. Accessed 2025-05-16. [30] Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let visionlanguage models reason step-by-step. arXiv preprint arXiv:2411.10440, nov 2024. Accessed: March 24, 2025. [31] Jiazheng Xu, Yu Huang, Jiale Cheng, et al. Visionreward: Fine-grained multi-dimensional human preference learning for image and video generation. arXiv preprint arXiv:2412.21059, 2024. [32] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. arXiv preprint arXiv:2304.05977, April 2023. [33] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. [34] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Joseph E. Gonzalez, Ion Stoica, and Hao Zhang. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, June 2023. [35] Huaisheng Zhu, Teng Xiao, and Vasant G. Honavar. Dspo: Direct score preference optimization for diffusion model alignment. In International Conference on Learning Representations (ICLR), 2025. OpenReview xyfb9HHvMe."
        }
    ],
    "affiliations": [
        "Artificial Intelligence Research Institute, Moscow, Russia",
        "Skolkovo Institute of Science and Technology, Moscow, Russia"
    ]
}