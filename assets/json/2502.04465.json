{
    "paper_title": "FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks",
    "authors": [
        "Luca Della Libera",
        "Francesco Paissan",
        "Cem Subakan",
        "Mirco Ravanelli"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models have revolutionized natural language processing through self-supervised pretraining on massive datasets. Inspired by this success, researchers have explored adapting these methods to speech by discretizing continuous audio into tokens using neural audio codecs. However, existing approaches face limitations, including high bitrates, the loss of either semantic or acoustic information, and the reliance on multi-codebook designs when trying to capture both, which increases architectural complexity for downstream tasks. To address these challenges, we introduce FocalCodec, an efficient low-bitrate codec based on focal modulation that utilizes a single binary codebook to compress speech between 0.16 and 0.65 kbps. FocalCodec delivers competitive performance in speech resynthesis and voice conversion at lower bitrates than the current state-of-the-art, while effectively handling multilingual speech and noisy environments. Evaluation on downstream tasks shows that FocalCodec successfully preserves sufficient semantic and acoustic information, while also being well-suited for generative modeling. Demo samples, code and checkpoints are available at https://lucadellalib.github.io/focalcodec-web/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 5 6 4 4 0 . 2 0 5 2 : r FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks Luca Della Libera 1 2 Francesco Paissan 3 2 Cem Subakan 4 1 2 Mirco Ravanelli 1 2 Abstract Large language models have revolutionized natural language processing through self-supervised pretraining on massive datasets. Inspired by this success, researchers have explored adapting these methods to speech by discretizing continuous audio into tokens using neural audio codecs. However, existing approaches face limitations, including high bitrates, the loss of either semantic or acoustic information, and the reliance on multi-codebook designs when trying to capture both, which increases architectural complexity for downstream tasks. To address these challenges, we introduce FocalCodec, an efficient low-bitrate codec based on focal modulation that utilizes single binary codebook to compress speech between 0.16 and 0.65 kbps. FocalCodec delivers competitive performance in speech resynthesis and voice conversion at lower bitrates than the current state-of-the-art, while effectively handling multilingual speech and noisy environments. Evaluation on downstream tasks shows that FocalCodec successfully preserves sufficient semantic and acoustic information, while also being well-suited for generative modeling. Demo samples, code and checkpoints are available at https://lucadellalib.github.io/focalcodec-web/. 1. Introduction Recent advancements in large language models (OpenAI et al., 2023; Chowdhery et al., 2024; Jiang et al., 2024a; Grattafiori et al., 2024) have led to significant progress in natural language processing, enabling breakthroughs in tasks such as summarization, translation, question answering, code generation, and retrieval. Building on this success, the research community has extended these methods to other modalities, with speech emerging as major area of interest. The impressive performance of text-conditioned 1Concordia University, Montreal, Canada 2Mila-Quebec AI Institute, Montreal, Canada 3Fondazione Bruno Kessler, Trento, Italy 4Universite Laval, Quebec, Canada. Correspondence to: Luca Della Libera <luca.dellalibera@mail.concordia.ca>. audio and speech generation models (Borsos et al., 2023; Copet et al., 2023; Kreuk et al., 2023; Wang et al., 2023; Kim et al., 2024), along with recent speech language models (Zhang et al., 2023; Hassid et al., 2023; Defossez et al., 2024; Nguyen et al., 2024), highlights the potential of tokenbased approaches for speech processing. key component of these pipelines is the neural audio codec, which compresses speech into tokens that downstream models can process. These tokens must preserve acoustic and semantic information to ensure effective representations for downstream tasks while maintaining high reconstruction quality. Another important requirement is low token rate. As sequence length increases, capturing long-term dependencies becomes more challenging, and computational costs increase. Despite recent progress, current codecs still face several challenges. Acoustic codecs (Defossez et al., 2023; Kumar et al., 2023; Ji et al., 2024; Xin et al., 2024) achieve highquality reconstruction but often rely on multiple codebooks, adding complexity to the design of downstream models. Additionally, they typically lack strong semantic representations. Hybrid codecs (Zhang et al., 2024; Liu et al., 2024; Defossez et al., 2024; Parker et al., 2024) aim to combine both acoustic and semantic information while maintaining high-quality resynthesis. Still, they often depend on complex multi-codebook designs, explicit disentanglement, distillation losses, or supervised fine-tuning. Single-codebook designs (Li et al., 2024; Guo et al., 2024; Ji et al., 2024; Xin et al., 2024; Wu et al., 2024a) offer simpler architecture but struggle to balance compression while maintaining both reconstruction quality and effective representations for downstream tasks, especially at low bitrates. To address these limitations, we introduce FocalCodec, an efficient lowbitrate codec based on focal modulation (Yang et al., 2022) that compresses speech into the space of single binary codebook. FocalCodec achieves competitive performance in reconstruction at lower bitrates than the current state-ofthe-art under variety of conditions while also preserving sufficient semantic and acoustic information for downstream tasks. Our contributions are as follows: We introduce FocalCodec, novel hybrid codec featuring unique compressor-quantizer-decompressor archiFocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks Figure 1. FocalCodec architecture. The encoder extracts features containing both acoustic and semantic information. These features are then mapped to low-dimensional space by the compressor, binary quantized, and projected back by the decompressor. The decoder resynthesizes the waveform from these features. tecture that compresses speech using single binary codebook at ultra-low bitrates (0.16 to 0.65 kbps). We propose focal modulation-based architecture with strong inductive biases for speech, offering an efficient and scalable solution for tokenization. We demonstrate the versatility of FocalCodec through comprehensive evaluations of reconstruction quality and performance in downstream tasks, highlighting its potential for both discriminative and generative speech modeling. Demo samples, code and checkpoints are available on our project page1. 2. Related Work Acoustic Codecs. Acoustic codecs, built on the VQVAE (van den Oord et al., 2017) framework, aim for highfidelity reconstruction. Notable advancements include hierarchical RVQ (Zeghidour et al., 2021), lightweight architectures (Defossez et al., 2023), improved RVQ techniques (Kumar et al., 2023), and efficiency-driven designs (Yang et al., 2023; Ren et al., 2024; Ai et al., 2024). Recent methods explore scalar quantization (Mentzer et al., 2024; Yang et al., 2024a), Mel-spectrogram discretization (Bai et al., 2024), and novel paradigms like diffusionand flow-based decoding (Wu et al., 2024b; Yang et al., 2024b; Pia et al., 2024). To reduce bitrate without compromising performance, multiscale RVQ (Siuzdak et al., 2024; Qiu et al., 2024) achieves improved compression by varying frame rates in deeper quantizers. However, its hierarchical design adds complexity to downstream applications, as it requires flattening the token sequences. Single-codebook designs (Li et al., 2024; Guo et al., 2024; Ji et al., 2024; Xin et al., 2024; Wu et al., 2024a) have emerged as simpler, efficient alternative, de1https://lucadellalib.github.io/focalcodec-web/ livering robust performance at low bitrates. Our codec aligns with this trend, leveraging novel focal modulation architecture and pretrained self-supervised encoder to efficiently unify semantic and acoustic representation learning. Semantic Codecs. Semantic codecs leverage selfsupervised features from large models trained with contrastive objectives (Baevski et al., 2020; Hsu et al., 2021; Chen et al., 2022) and k-means clustering (Lloyd, 1982) for quantization, either from single layer (Polyak et al., 2021; Wang et al., 2024) or multiple layers (Mousavi et al., 2024b; Shi et al., 2024). Improvements upon this paradigm include replacing k-means with RVQ (Huang et al., 2024), noise-aware (Messica & Adi, 2024) and speaker-invariant tokenization (Chang et al., 2024). While these approaches effectively capture linguistic and content-related information, they often discard much of the acoustic detail, resulting in low speaker fidelity when vocoder is trained to resynthesize speech from these representations. Our codec adopts self-supervised architecture similar to semantic codecs but retains acoustic detail through its novel compressorquantizer-decompressor architecture and decoupled training strategy, ensuring high-quality reconstruction while preserving the advantages of semantic representations. Hybrid Codecs. Hybrid codecs combine semantic and acoustic features to balance reconstruction quality and content representation. Some methods (Ju et al., 2024; Jiang et al., 2024b; Zheng et al., 2024) employ multiple codebooks to disentangle speech into distinct subspaces, such as content, prosody, and timbre, while others (Liu et al., 2024) utilize dual encoders to separately capture content and finegrained acoustic information. Semantic distillation (Zhang et al., 2024; Defossez et al., 2024) has also been explored to enrich the first RVQ codebook with semantic information from HuBERT (Hsu et al., 2021) and WavLM (Chen et al., 2022). More recently, Parker et al. (2024) trained largescale transformer-based VQ-VAE, achieving exceptional FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks reconstruction quality at ultra-low bitrates. To enhance semantic content, they employed supervised fine-tuning on force-aligned phoneme data. Our codec also belongs to this category but instead of relying on complex multi-codebook designs with explicit disentanglement, distillation losses, or supervised fine-tuning, it is purely based on self-supervised learning. It compresses both semantic and acoustic information into single codebook, pushing the boundaries of hybrid codec design at low bitrates. 3. FocalCodec 3.1. Architecture The proposed codec is largely based on the VQ-VAE framework but incorporates novel compressor and decompressor modules between the encoder and decoder (see Figure 1). The discriminator is used only during training and is discarded afterward. Encoder. To build hybrid codec with simple design, without relying on distillation losses or multiple encoders, the encoder must capture both acoustic and semantic information. This ensures high-quality reconstructions and expressive tokens for training downstream models. Selfsupervised models like HuBERT and WavLM retain significant acoustic information in their lower layers (Chen et al., 2022), making them suitable for hybrid codecs. For instance, Baas et al. (2023) show that high-quality vocoder can be trained using continuous representations from layer6 of WavLM-large. Following this approach, we use the first 6 layers of WavLM-large2 as our encoder. However, effective quantization is critical for approximating continuous representations with sufficient granularity. Standard k-means clustering typically fails to preserve essential acoustic details (van Niekerk et al., 2022). To address this, we introduce the novel compressor-quantizer-decompressor design based on focal modulation, which allows for granular quantization that preserves both semantic and acoustic information. Compressor. The compressor maps the encoder representations to compact, low-dimensional latent space. Optionally, it can perform temporal downsampling to further reduce the frame rate. Prior work typically relies on convolutional, recurrent, or transformer-based architectures for compression. In contrast, we introduce novel focal downscaling module, which combines downscaling operation with focal block. The downscaling step applies linear projection to compress the feature dimension, while 1D convolution can be used instead to additionally downsample along the time dimensions. To better capture periodic patterns, we follow (Kumar et al., 2023) and apply Snake activations (Ziyin et al., 2020) after the projection. 2https://github.com/microsoft/unilm/tree/master/wavlm To build focal block, we replace the self-attention mechanism in the standard transformer block with focal modulation. Focal modulation (Yang et al., 2022) is an efficient alternative to self-attention that enables fine-to-coarse modeling and introduces useful inductive biases such as translation equivariance, explicit input dependency, time and channel specificity, and decoupled feature granularity. While originally designed for image and video processing, these properties also benefit speech modeling (Della Libera et al., 2024). Unlike self-attention, which directly computes token-wise interactions, focal modulation first aggregates the global context and then modulates local interactions based on this aggregated representation. This ensures that interactions are guided by the overall context rather than being dominated by individual tokens. Formally, focal modulation computes output representation yi for each input feature xi in sequence x1:n as: yi = q(xi) (cid:33) gℓ zℓ (cid:32)L+1 (cid:88) ℓ=1 (1) zℓ gℓ where q() and h() are linear projections, and zℓ 1:n and gℓ 1:n are the context and gating vectors at position and focal level ℓ {1, . . . , + 1}, with denoting element-wise multiplication. The context sequence z1:n is obtained via stack of depth-wise convolutions with exponentially increasing kernel sizes to capture dependencies from short to long range, with average pooling applied to the last level feature map to incorporate global information. Then, for each focal level, point-wise convolution is used to compute the gating sequence g1:n. This hierarchical approach, operating at multiple granularities, makes focal modulation well-suited for processing speech features, enabling efficient and scalable representation learning in linear time while preserving long-range dependencies. Quantizer. FocalCodec maps latent representations from the compressor into the codebook space of single quantizer, eliminating the need for hierarchical designs in downstream models. To achieve this, while maintaining both reconstruction quality and efficiency, the quantizer should satisfy the following requirements: 1) given that the original waveform is already significantly compressed into short sequence of latents, the quantizer must compensate by using sufficiently large codebook size to reduce the quantization error; 2) the quantizer should make efficient use of the codebook capacity, avoiding under-utilization; 3) code lookup must remain efficient, despite the increased codebook size, to ensure fast inference. To address these challenges, we employ binary spherical quantization (BSQ) (Zhao et al., 2024), originally introduced for compression of images and videos. To the best of our knowledge, this is the first successful application of binary quantization in the speech domain. BSQ belongs to the category of lookup-free quantization (LFQ) methods (Yu 3 FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks et al., 2024), i.e. it utilizes an implicit codebook, defined as: (cid:26) (cid:27)L , , (2) = 1 1 which represents an L-dimensional hypercube projected onto unit hypersphere. The codebook size is determined by the latent representation dimension as = 2L. For example, latent representations of dimension 13 correspond to codebook size of 8192. The quantization process consists of two steps. First, the input vector of dimension is normalized to lie on the unit hypersphere: = v2 . (3) Second, binary quantization with normalization factor of is applied independently to each dimension of u: ˆu = sign(u) , (4) where sign() denotes the sign function, with sign(0) remapped to 1 to ensure the output always lies on the hypersphere. To make the quantization differentiable, we use the straight-through estimator (Bengio et al., 2013). BSQ offers several advantages over traditional quantization methods. First, the parameter-free implicit codebook is lightweight and computationally efficient. Second, empirical evidence (Zhao et al., 2024) shows that the binary quantization bottleneck encourages high codebook utilization, even for large values of L. Third, the quantization error is bounded, resulting in faster convergence compared to vanilla LFQ, which does not normalize the representations. Finally, tying the codebook size to the latent dimension helps prevent performance degradation in downstream generative models when using larger codebooks (Yu et al., 2024). Decompressor. The decompressor reconstructs the encoder continuous representations from the quantizer output. It closely mirrors the structure of the compressor, with the downscaling layers replaced by upscaling layers. Decoder. Most codecs use symmetric architectures, where the decoder mirrors the encoder. However, some works (Bai et al., 2024; Ji et al., 2024; Liu et al., 2024) explore asymmetric designs with larger decoders to improve reconstruction quality. In this work, we adopt an asymmetric design but prioritize the encoder, allocating 5x more parameters to it than the decoder. We argue that strong encoder is essential for extracting robust, disentangled representations for downstream tasks. Even with high compression rate, smaller decoder can still generate high-quality audio while offering faster inference, which is beneficial for streaming applications. For the decoder, we choose the more efficient Vocos (Siuzdak, 2024) architecture over HiFi-GAN (Kong et al., 2020). Vocos maintains consistent feature resolution and uses inverse STFT for upsampling, minimizing aliasing and improving computational efficiency. The decoder 4 processes features through ConvNeXt (Liu et al., 2022) blocks and projects the sequence of hidden representations to Fourier coefficients for waveform reconstruction. The final audio is synthesized using inverse STFT. Discriminator. Following (Kong et al., 2020), we employ multi-period discriminator and multi-scale discriminator. This approach slightly differs from prior work (Zeghidour et al., 2021; Defossez et al., 2023; Siuzdak, 2024; Kumar et al., 2023; Ji et al., 2024), which utilize multiresolution and/or STFT-based discriminators in place of multi-scale discriminator. The multi-resolution and STFTbased discriminators are particularly useful for mitigating over-smoothing artifacts in high-frequency components (Kumar et al., 2023), which are more critical for music and environmental sounds. Since our focus is on speech (i.e. medium frequency range), we stick to the simpler HiFi-GAN setup. 3.2. Training The training process consists of two stages. In the first stage, the compressor, quantizer, and decompressor are jointly trained to reconstruct the encoder continuous representations, ensuring that the tokens retain both semantic and acoustic information from the encoder, which is kept frozen. The training objective includes reconstruction loss and entropy loss. The reconstruction loss is computed as the squared L2 distance between the reconstructed and original encoder features. The entropy loss, defined as in (Yu et al., 2024; Zhao et al., 2024), encourages both confident predictions and uniform code utilization. Note that we omit the commitment loss used in standard VQ, as for BSQ there is no concern of embedding divergence (quantization error is bounded). In the second stage, the decoder is trained to resynthesize audio from the encoder continuous representations. This approach enables us to perform this stage in parallel with the first, simplifying the training setup. The training objective includes adversarial loss, reconstruction loss, and feature matching loss, as in (Kong et al., 2020). However, following (Zeghidour et al., 2021), we use hinge loss formulation instead of least squares. The reconstruction loss is computed as the L1 distance between the reconstructed and original log-Mel spectrograms, while the feature matching loss is the mean of the distances between the l-th feature maps of the k-th subdiscriminator. This decoupled training approach ensures that both semantic and acoustic information are preserved in the tokens, which is crucial for downstream tasks while maintaining high reconstruction quality. If trained end-to-end without additional constraints on the hidden representations (e.g. distillation loss), the reconstruction loss prioritizes acoustic features, as observed in (Defossez et al., 2023; Kumar et al., 2023). FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks 4. Experiments 4.1. FocalCodec We train FocalCodec on LibriTTS (Zen et al., 2019), resampled to 16 kHz. We train three variants of the model with codebook size of 8192 and token rates of 50 Hz, 25 Hz, and 12.5 Hz by adjusting the temporal downsampling factors in the compressor layers to (1, 1, 1), (2, 1, 1), and (2, 2, 1), respectively. These patterns are mirrored in the decompressor layers for upsampling. Information about hyperparameters and training details can be found in Appendix D.1. 4.2. Baselines We compare our models to recent state-of-the-art low-bitrate codecs across acoustic, semantic, and hybrid categories. Since the paper focuses on low-bitrate codecs, when multiple quantizers are available, we configure them to achieve bitrate below 1.50 kbps, ensuring fair comparison. For acoustic codecs, we compare against EnCodec (Defossez et al., 2023), DAC (Kumar et al., 2023), WavTokenizer (Ji et al., 2024), and BigCodec (Xin et al., 2024). Among these, BigCodec is the current state-of-the-art for low-bitrate speech reconstruction quality (Wu et al., 2024a). We use the official checkpoints for these models. We do not include the recent TS3-Codec (Wu et al., 2024a), which matches BigCodec performance at an even lower bitrate, as it is not publicly available. However, we contacted the authors to request reconstructed samples for comparison. Additional results related to TS3-Codec can be found in Appendix F.2. For semantic codecs, we adopt the approach introduced in (Wang et al., 2024), which quantizes layer-6 representations from WavLM-large using k-means clustering with 512 centroids. These representations are fed into Conformer (Gulati et al., 2020) encoder to reconstruct continuous representations, followed by HiFi-GAN decoder. This baseline, referred to as WavLM6-KM, provides direct comparison between our codec and another model leveraging WavLM layer-6 features but differing in design and training methodology. Since the code and checkpoints for WavLM6-KM are not publicly available, we reimplemented the model using subset of LibriSpeech (Panayotov et al., 2015). Note that we do not include additional baselines from this category, as semantic codecs typically underperform in terms of reconstruction quality (Parker et al., 2024) or require much higher bitrates to be competitive in this regard (Mousavi et al., 2024a). Furthermore, most hybrid codecs are already built on top of semantic representations. Therefore, we prioritize the hybrid category, to which our codec also belongs. For hybrid codecs, we compare against SpeechTokenizer (Zhang et al., 2024), SemantiCodec (Liu et al., 2024), Mimi (Defossez et al., 2024), and Stable Codec (Parker et al., 2024), using their official checkpoints. The configurations and details of each model are summaTable 1. Compared codecs. Codec EnCodec DAC WavLM6-KM SpeechTokenizer SemantiCodec Mimi WavTokenizer BigCodec Stable Codec FocalCodec@50 FocalCodec@25 FocalCodec@12.5 Bitrate (kbps) Sample Rate (kHz) Token Rate (Hz) Codebooks Code Size Params (M) MACs (G) 1.50 1.00 0.45 1.00 0.65 0.69 0.48 1.04 0.70 0.65 0.33 0. 24 16 16 16 16 24 24 16 16 16 16 16 75.0 50.0 50.0 50.0 25.0 12.5 40.0 80.0 25.0 50.0 25.0 12.5 2 1024 2 1024 1 512 2 1024 2 8192 5 2048 1 4096 1 8192 2 15625 1 8192 1 8192 1 128 8 1024 1024 1536 256 512 8 6 13 13 13 15 74 127 108 1033 82 85 160 95 142 144 145 2 56 28 17 1599 11 3 61 37 9 9 rized in Table 1. Multiply-accumulate operations per second (MACs) are measured using ptflops3. Additional information about the baselines is provided in Appendix C. 4.3. Speech Resynthesis We evaluate FocalCodec on speech resynthesis, considering both English and multilingual speech. For English speech, we use LibriSpeech (Panayotov et al., 2015) test-clean. For multilingual speech, following (Xin et al., 2024), we randomly select 100 utterances from each of the 7 foreign languages in Multilingual LibriSpeech (Pratap et al., 2020) (Dutch, French, German, Italian, Polish, Portuguese, and Spanish), resulting in total of 700 utterances. We also consider the more realistic scenario of speech contaminated with environmental noise. For this, we use the test splits of VoiceBank (ValentiniBotinhao et al., 2016) and the more challenging Libri1Mix, which is constructed by mixing clean utterances from the first speaker of LibriMix (Cosentino et al., 2020) with noise from WHAM! (Wichern et al., 2019). We evaluate the models using objective metrics. To measure naturalness, we employ UTMOS (Saeki et al., 2022) for clean speech and DNSMOS (Reddy et al., 2022) for noisy speech. Note that we do not include signal-level metrics such as SNR, PESQ (Rix et al., 2001), or STOI (Taal et al., 2011), as these metrics do not correlate well with perceived reconstruction quality (Parker et al., 2024; Wang et al., 2024). To evaluate speaker fidelity, we compute the cosine similarity (Sim) between speaker embeddings extracted from the reconstructed audio and the target audio. These embeddings are obtained using WavLM-base (Chen et al., 2022) fine-tuned for speaker verification4. To assess intelligibility, we compute the differential word error rate (dWER) (Wang et al., 2021), which measures the difference in word error rate between the reconstructed and target audio, using transcriptions from Whisper small5 (Radford et al., 2023). To ensure fairness in evaluation, we do 3https://pypi.org/project/ptflops/0.7.4/ 4https://huggingface.co/microsoft/wavlm-base-sv 5https://huggingface.co/openai/whisper-small FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks Table 2. Clean speech resynthesis. Codec Reference EnCodec DAC WavLM6-KM SpeechTokenizer SemantiCodec Mimi WavTokenizer BigCodec Stable Codec FocalCodec@50 FocalCodec@25 FocalCodec@12.5 Reference EnCodec DAC WavLM6-KM SpeechTokenizer SemantiCodec Mimi WavTokenizer BigCodec Stable Codec FocalCodec@50 FocalCodec@25 FocalCodec@12.5 UTMOS dWER Sim Code Usage Norm Entropy RTF LibriSpeech test-clean 0.00 8.08 20.04 6.20 5.14 8.97 5.73 11.55 2.55 4.97 2.18 3.30 7.94 100.0 93.8 89.2 90.0 91.6 96.0 96.0 95.4 98.5 94.7 97.4 96.3 93.9 93.4 100.0 26.4 95.9 75.9 95.6 100.0 100.0 98. 100.0 99.8 98.2 82.1 91.7 95.4 97.0 94.4 91.8 96.7 98.6 94.7 98.9 98.4 97.4 Multilingual LibriSpeech 700 0.00 29.60 56.08 44.54 56.32 36.21 30.96 49.73 15.24 56.99 12.57 19.78 54. 100.0 95.5 89.1 89.5 92.0 97.7 96.7 97.0 99.1 95.9 98.3 97.3 95.2 93.4 100.0 28.1 96.1 76.4 95.9 97.6 100.0 92.9 100.0 99.2 96.4 79.2 90.0 0.91 94.0 94.7 89.0 95.6 97.9 93.8 98.1 97.4 96. 109 89 85 63 0.62 137 181 22 103 185 195 208 140 97 125 74 0.74 239 290 24 144 269 292 296 4.09 1.58 1.29 3.75 2.28 2.91 3.29 3.78 4.11 4.32 4.05 4.14 4. 2.84 1.33 1.24 2.97 1.55 1.87 2.08 2.64 2.86 3.47 2.96 3.16 3.37 Table 3. Noisy speech resynthesis. Codec Reference EnCodec DAC WavLM6-KM SpeechTokenizer SemantiCodec Mimi WavTokenizer BigCodec Stable Codec FocalCodec@50 FocalCodec@25 FocalCodec@12. Reference EnCodec DAC WavLM6-KM SpeechTokenizer SemantiCodec Mimi WavTokenizer BigCodec Stable Codec FocalCodec@50 FocalCodec@25 FocalCodec@12.5 DNSMOS dWER Sim Code Usage Norm Entropy RTF VoiceBank test 100.0 87.7 79.8 82.9 82.2 90.6 87.8 89.8 92.3 88.8 91.3 90.1 84.7 77.5 98.7 24.8 88.1 52.4 78.6 94.8 99.8 75.7 98.0 89.6 77. Libri1Mix test 100.0 86.3 76.6 85.9 82.8 89.9 89.4 86.3 88.3 90.0 91.6 90.7 88.9 84.4 99.1 26.8 93.5 64.7 90.8 96.4 100.0 95.8 100.0 99.6 97.2 0.00 28.16 63.90 20.67 34.51 31.46 28.00 42.12 20.67 20. 8.08 11.75 27.97 0.00 55.17 90.92 36.60 57.26 51.18 49.14 70.10 53.26 43.52 27.89 34.27 42.59 78.1 88.4 92.3 88.4 92.6 85.5 94.0 96.8 95.4 96.2 96.0 95.5 78.7 88.8 95.5 96.5 90.8 90.1 95.4 98.2 93. 98.5 97.9 97.2 44 48 44 42 0.28 47 63 17 39 80 81 79 97 91 65 63 91 104 165 19 68 155 161 164 3.56 2.76 2.72 3.06 2.74 3.13 3.01 3.09 3.19 3. 3.16 3.17 3.22 3.73 2.40 2.40 2.87 2.58 2.67 2.65 2.53 2.75 2.91 2.93 2.91 2.92 not use more powerful ASR models (e.g. Whisper largev3), as these models can correct pronunciation mistakes and are more robust to noise, potentially hiding flaws in the reconstruction. We also report code usage, i.e. the ratio of unique tokens used to the codebook size (averaged over codebooks for multi-codebook models), and normalized entropy (Cover & Thomas, 2006; Parker et al., 2024), Table 4. One-shot voice conversion. UTMOS dWER Sim RTF Codec Reference EnCodec DAC WavLM6-KM SpeechTokenizer SemantiCodec Mimi WavTokenizer BigCodec Stable Codec FocalCodec@50 FocalCodec@25 FocalCodec@12.5 4.09 1.24 1.25 2.90 1.49 2.02 2.40 3.13 1.31 3.76 3.38 3.40 3.43 VCTK 0.00 86.52 104.00 26.68 20.32 106.00 110.00 43.15 99.96 27.63 21.27 23.59 29.93 100.0 57 72.2 60 67.2 57 92.4 33 81.2 0.60 72.8 71 89.7 89 73.4 13 68.9 65 71.1 92.2 92.6 92.6 116 118 117 where higher values indicate more uniform codebook usage. For inference speed, we measure the real-time factor (RTF), i.e. the ratio of the reconstructed audio duration to the processing time. An RTF greater than 1 indicates fasterthan-real-time performance, measured on an NVIDIA V100 GPU with 32 GB of memory. Discussion Results are presented in Table 2 and Table 3. FocalCodec shows strong performance across both clean and noisy speech resynthesis tasks. On clean speech, FocalCodec@50 achieves the best trade-off of quality, intelligibility, and efficiency. Notably, FocalCodec is the best in terms of dWER, surpassing BigCodec, which is currently stateof-the-art. It also generalizes well to multilingual speech, obtaining the lowest dWER and high Sim. Note that FocalCodec, WavLM6-KM, SpeechTokenizer, BigCodec and Stable Codec were trained exclusively on English speech. In noisy speech resynthesis, FocalCodec@50 again excels, achieving the lowest dWER by large margin on both VoiceBank and Libri1Mix, while maintaining high speaker similarity. Meanwhile, FocalCodec@25 and FocalCodec@12.5 exhibit some degradation in dWER and speaker similarity, particularly in multilingual settings, due to their significantly lower bitrates. Nevertheless, despite operating at just 0.16 kbps, FocalCodec@12.5 remains competitive with several baselines that use much higher bitrates (e.g. EnCodec). Finally, the high code usage and normalized entropy across all FocalCodec variants indicate efficient token utilization, contributing to their strong overall performance. Additional results on reconstruction quality, including subjective evaluations, can be found in Appendix F.1. 4.4. Voice Conversion We conduct one-shot voice conversion experiments to verify that FocalCodec can effectively disentangle speaker information from content despite its single-codebook design. This task involves converting speech from source speaker to an arbitrary target speaker using reference speech from the target speaker. For single-codebook baselines, including FocalCodec, we use k-nearest neighbors search in the codec feature space, as in (Baas et al., 2023). Specifically, we 6 FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks replace each frame in the reconstructed feature sequence (right before the decoder) with the average of the = 4 closest matches in terms of cosine distance from continuous features extracted from the reference. For multi-codebook baselines, instead, we follow the procedure in (Zhang et al., 2024). The source and reference speech are tokenized, and the first codebook tokens from the source are concatenated with the second-to-last codebook tokens from the reference. The resulting sequence is then forwarded to the decoder. If sequence lengths differ, the reference is truncated or circularly padded as needed. Effective disentanglement of content and speaker information between first and subsequent codebooks is expected to yield fair voice conversion performance. We conduct voice conversion experiments on VCTK (Yamagishi et al., 2017), which includes parallel utterances from different speakers. To create the test set, we randomly select an utterance from source speaker, the corresponding utterance from target speaker, and an utterance with different content from the same target speaker to act as the reference. Among available reference utterances, we select the longest to minimize padding issues. We repeat this process for each speaker, for each of the 24 parallel utterances, resulting in dataset with 2521 samples. To evaluate performance, we use UTMOS, dWER, Sim, and RTF as defined in Section 4.3. Discussion As reported in Table 4, FocalCodec achieves the highest speaker similarity while maintaining good intelligibility, confirming its suitability for voice conversion tasks. This is particularly impressive, especially compared to other hybrid codecs like SpeechTokenizer and Mimi, which are explicitly optimized to disentangle semantic information in the first codebook and acoustic information in the following. Despite this, FocalCodec outperforms these models, excelling in both speaker identity preservation and intelligibility, striking remarkable balance of quality, efficiency, and speaker similarity. WavLM6-KM ranks as the secondbest performing model, which is expected since it shares the same encoder as FocalCodec. In contrast, acoustic codecs struggle with this task, as they do not separate speaker and content information. 4.5. Downstream Tasks To evaluate the quality of the learned discrete representations, we train downstream models on both discriminative and generative tasks. Discriminative Tasks. We evaluate performance on automatic speech recognition (ASR), speaker identification (SI), and speech emotion recognition (SER). These tasks allow us to assess token quality along three axes: semantic information retention (ASR), acoustic information retention (SI), and emotion information retention (SER, which requires non-trivial combination of semantic and acoustic clues). To focus on the disentanglement of learned representations, we employ shallow downstream models, aiming to stay as close as possible to linear probing. Following (Zhang et al., 2024), we employ shallow BiLSTM for all tasks. For ASR, we use LibriSpeech (Panayotov et al., 2015) train-clean-100 and train-clean-360 for training, dev-clean for validation, and test-clean for testing. The word error rate (WER) is reported. For SI, we also use LibriSpeech, grouping utterances from train-clean-100 and train-clean-360 by speaker ID. Data are randomly split into training, validation and test sets in ratio of 80% / 10% / 10%. The speaker error rate (ER) is reported. For SER, we use the IEMOCAP dataset (Busso et al., 2008), focusing on four emotions: sadness, happiness, anger, and neutral. Sessions 14 are used for training, session 5F for validation, and session 5M for testing. The emotion ER is reported. Details about the model architecture, hyperparameters, and training procedure are provided in Appendix D.2. Discussion Table 5 shows the results. In ASR, FocalCodec@50 achieves the third lowest WER. While SpeechTokenizer and Stable Codec perform slightly better, the former operates at 1.5x higher bitrate using two codebooks, while the latter was fine-tuned on force-aligned phoneme data to enhance semantic representations. In contrast, our model is purely self-supervised. In SI, FocalCodec@50 achieves marginally higher error rate (2%) than codecs such as BigCodec and WavTokenizer. However, these models perform significantly worse in ASR due to being trained solely with reconstruction-based objectives. On the other hand, the purely semantic WavLM-KM6 codec performs competitively in ASR but exhibits the highest ER in SI despite using the same encoder as FocalCodec. This further confirms the effectiveness of our codec design, as it improves WER over WavLM-KM6 while preserving speaker information. Interestingly, Stable Codec also performs poorly in SI, likely because semantic fine-tuning tends to remove acoustic information from the representations. In SER, no codec clearly excels, with FocalCodec@50 performing on par with the best models. Overall, FocalCodec@50 shows competitive performance across all discriminative tasks, rivaling hybrid codecs with more complex multi-codebook designs and higher bitrates. The more compressed variants, FocalCodec@25 and FocalCodec@12.5, still achieve acceptable performance while operating at ultra-low bitrates. Generative Tasks. We evaluate performance on speech enhancement (SE), speech separation (SS), and text-tospeech (TTS). For these tasks, we employ more powerful transformer-based downstream models, focusing on generation quality. For SE we use VoiceBank (ValentiniBotinhao et al., 2016). To form validation set, we randomly select two speakers from the training set. The input tokens are extracted from noisy utterances, while the 7 FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks Codec Reference EnCodec DAC WavLM6-KM SpeechTokenizer SemantiCodec Mimi WavTokenizer BigCodec Stable Codec FocalCodec@50 FocalCodec@25 FocalCodec@12. Discriminative Tasks ASR WER 27.89 35.89 19.04 14.97 41.42 22.98 35.62 26.41 16.85 17.63 21.12 33.24 SI ER 3.00 3.27 22.30 2.73 15.90 5.43 2.44 2.34 16.50 4.48 6.07 11.69 SER ER 47.00 45.90 42.90 41.50 51.60 44.70 49.80 47.50 46. 45.60 46.80 46.30 Table 5. Evaluation on downstream tasks. SE Generative Tasks SS TTS DNSMOS dWER Sim DNSMOS dWER Sim UTMOS dWER Sim 3.56 3.11 3.03 3.52 3.21 3.59 3.30 3.41 3.52 3.55 3.47 3.49 3.58 0.00 37.10 67.65 22.85 29.82 102.00 53.98 51.75 26.68 35. 10.93 14.74 36.98 100.0 85.9 81.7 83.6 85.9 83.3 84.6 88.6 93.2 82.8 91.4 90.0 86.9 3.77 3.11 2.76 3.49 3.13 3.59 3.41 3.54 3.54 3.61 3.71 3.69 3.57 0.00 78.51 106.00 76.91 83.99 123.00 93.23 105.00 89.24 103. 73.87 99.96 116.00 100.0 87.3 83.3 85.0 87.3 84.4 88.1 86.4 89.4 78.2 89.0 85.4 80.8 4.09 1.69 1.36 3.71 2.63 2.72 3.05 3.65 3.24 2.86 4.05 4.12 4.16 Table 6. Ablation studies. 0.00 74.07 61.11 48.51 47.81 59.85 39.50 59.22 63.83 56.97 39.58 30.28 29.91 100.0 79.1 84.1 88.2 88.3 90.8 93.3 89.6 87.8 84.3 92.9 91.4 91.5 target tokens come from clean utterances. Performance metrics include DNSMOS, dWER, and Sim. For SS, we use Libri2Mix (Cosentino et al., 2020) train-100, dev, and test sets. The setup mirrors that of speech enhancement: input tokens are derived from speech mixtures, while target tokens correspond to the two individual sources. For TTS, we use LibriSpeech train-clean-100 and train-clean-360 for training, dev-clean for validation, and test-clean for testing. The input consists of character-based text tokens, while the target tokens are derived from the corresponding utterances. Performance is evaluated using UTMOS, dWER, and Sim. Details about the model architecture, hyperparameters, and training procedure are provided in Appendix D.2. Discussion From Table 5, we observe that in SE, FocalCodec@50 significantly outperforms all other baselines in terms of dWER. similar trend is observed for SS, where FocalCodec@50 is consistently superior to the other baselines. However, the absolute performance is still far from practical utility, likely due to the loss of information crucial for SS during quantization. Additional results can be found in Table 10. As with discriminative tasks, FocalCodec@25 and FocalCodec@12.5 show degraded performance, likely due to their ultra-low bitrates. However, this trend is reversed for TTS, with FocalCodec@12.5, followed closely by FocalCodec@25, achieving the best overall results. This can be attributed to the fact that, in autoregressive modeling, shorter sequences reduce the computational burden and simplify the task of predicting the next token. FocalCodec@12.5, operating at frame rate close to that of text with single codebook, makes next-token prediction easier and more computationally efficient than other methods. This highlights the importance of having compact, powerful, discrete representations for downstream tasks. It is important to note, however, that we trained on only 460 hours of speech, which explains why TTS performance is not state-of-the-art. 8 Compression/ Decompression Block Down/Upscale Activation Focal modulation Focal modulation Focal modulation Focal modulation Leaky ReLU Snake Snake Snake Conformer AMP Linear Snake Snake Snake 4.6. Ablation Studies Quantizer Decoder UTMOS dWER Sim BSQ BSQ LFQ LFQ LFQ LFQ LFQ Vocos HiFi-GAN HiFi-GAN HiFi-GAN HiFi-GAN HiFi-GAN HiFi-GAN 4.14 3.73 3.74 3.72 3.74 3.70 2. 2.54 2.54 2.75 2.85 3.58 4.52 9.37 95.3 95.7 95.4 95.2 94.3 94.3 82.5 Due to limited computational resources, we perform ablation studies on smaller variant of FocalCodec. This variant is similar to the 50 Hz model, with the main difference being the model size, as detailed in Appendix D.1. We focus on the speech resynthesis task. The results are shown in Table 6. Replacing Vocos with HiFi-GAN significantly degrades UTMOS, despite achieving slightly better speaker fidelity. Also note that Vocos offers from 2x to 3x faster inference. Replacing BSQ with vanilla LFQ results in the degradation of dWER and Sim despite similar UTMOS scores. Replacing Snake activations with leaky ReLU causes only minor degradation in performance. The most significant performance drop occurs when the focal modulation blocks are replaced with Conformer (Gulati et al., 2020) blocks, antialiased multi-periodicity (AMP) (Lee et al., 2023) blocks, or linear layers, in this order. This leads to notable decrease in both dWER and Sim. This analysis further validates our design choices, highlighting the importance of the selected components for achieving optimal performance. 5. Conclusions In this work, we introduced FocalCodec, low-bitrate singlecodebook speech codec that employs novel architecture based on focal modulation. It delivers competitive performance in speech resynthesis and voice conversion at low and ultra-low bitrates while maintaining robustness across diverse conditions, including multilingual and noisy speech. Furthermore, FocalCodec effectively preserves both semantic and acoustic information and provides powerful discrete representations for generative downstream tasks. FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks"
        },
        {
            "title": "References",
            "content": "Ai, Y., Jiang, X.-H., Lu, Y.-X., Du, H.-P., and Ling, Z.-H. APCodec: neural audio codec with parallel amplitude and phase spectrum encoding and decoding. IEEE/ACM Transactions on Audio, Speech and Language Processing, 32:32563269, 2024. Baas, M., van Niekerk, B., and Kamper, H. Voice conversion with just nearest neighbors. In Interspeech, pp. 2053 2057, 2023. Baevski, A., Zhou, Y., Mohamed, A., and Auli, M. wav2vec 2.0: framework for self-supervised learning of speech representations. In International Conference on Neural Information Processing Systems (NeurIPS), pp. 12449 12460, 2020. Bai, H., Likhomanenko, T., Zhang, R., Gu, Z., Aldeneh, Z., and Jaitly, N. dMel: Speech tokenization made simple. arXiv preprint arXiv:2407.15835, 2024. Bengio, Y., Leonard, N., and Courville, A. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. Borsos, Z., Marinier, R., Vincent, D., Kharitonov, E., Pietquin, O., Sharifi, M., Roblek, D., Teboul, O., Grangier, D., Tagliasacchi, M., and Zeghidour, N. AudioLM: language modeling approach to audio generation. IEEE/ACM Transactions on Audio, Speech and Language Processing, 31:25232533, 2023. Busso, C., Bulut, M., Lee, C.-C., Kazemzadeh, A., Mower, E., Kim, S., Chang, J. N., Lee, S., and Narayanan, S. S. IEMOCAP: Interactive emotional dyadic motion capture database. Language Resources and Evaluation, 42(4): 335359, 2008. Chang, H.-J., Gong, H., Wang, C., Glass, J., and Chung, DC-Spin: speaker-invariant speech tokarXiv preprint Y.-A. enizer for spoken language models. arXiv:2410.24177, 2024. Chen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li, J., Kanda, N., Yoshioka, T., Xiao, X., Wu, J., Zhou, L., Ren, S., Qian, Y., Qian, Y., Wu, J., Zeng, M., Yu, X., and Wei, F. WavLM: Large-scale self-supervised pre-training for full stack speech processing. IEEE Journal of Selected Topics in Signal Processing, pp. 15051518, 2022. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., et al. PaLM: scaling language modeling with pathways. Journal of Machine Learning Research, 24, 2024. Copet, J., Kreuk, F., Gat, I., Remez, T., Kant, D., Synnaeve, G., Adi, Y., and Defossez, A. Simple and controllable music generation. In International Conference on Neural Information Processing Systems (NeurIPS), volume 36, pp. 4770447720, 2023. Cosentino, J., Pariente, M., Cornell, S., Deleforge, A., and Vincent, E. LibriMix: An open-source dataset arXiv preprint for generalizable speech separation. arXiv:2005.11262, 2020. Cover, T. M. and Thomas, J. A. Elements of information theory. Wiley-Interscience, 2006. Defossez, A., Copet, J., Synnaeve, G., and Adi, Y. High fidelity neural audio compression. Transactions on Machine Learning Research (TMLR), 2023. Della Libera, L., Subakan, C., and Ravanelli, M. Focal modulation networks for interpretable sound classification. In IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW), pp. 853857, 2024. Defossez, A., Mazare, L., Orsini, M., Royer, A., Perez, P., Jegou, H., Grave, E., and Zeghidour, N. Moshi: speech-text foundation model for real-time dialogue. arXiv preprint arXiv:2410.00037, 2024. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., Yang, A., Fan, A., et al. The Llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Graves, A., Fernandez, S., Gomez, F., and Schmidhuber, J. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In International Conference on Machine Learning (ICML), pp. 369376, 2006. Gulati, A., Qin, J., Chiu, C.-C., Parmar, N., Zhang, Y., Yu, J., Han, W., Wang, S., Zhang, Z., Wu, Y., and Pang, R. Conformer: Convolution-augmented transformer for speech recognition. In Interspeech, pp. 50365040, 2020. Guo, Y., Li, Z., Du, C., Wang, H., Chen, X., and Yu, K. LSCodec: Low-bitrate and speaker-decoupled discrete speech codec. arXiv preprint arXiv:2410.15764, 2024. Hassid, M., Remez, T., Nguyen, T. A., Gat, I., Conneau, A., Kreuk, F., Copet, J., Defossez, A., Synnaeve, G., Dupoux, E., Schwartz, R., and Adi, Y. Textually pretrained speech language models. In International Conference on Learning Representations (ICLR), 2023. FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks Hsu, W.-N., Bolte, B., Tsai, Y.-H. H., Lakhotia, K., Salakhutdinov, R., and Mohamed, A. HuBERT: Selfsupervised speech representation learning by masked prediction of hidden units. IEEE/ACM Trans. Audio Speech Lang. Process., 29:34513460, 2021. Huang, Z., Meng, C., and Ko, T. RepCodec: speech representation codec for speech tokenization. In Annual Meeting of the Association for Computational Linguistics (ACL), pp. 57775790, 2024. Ji, S., Jiang, Z., Wang, W., Chen, Y., Fang, M., Zuo, J., Yang, Q., Cheng, X., Wang, Z., Li, R., Zhang, Z., Yang, X., Huang, R., Jiang, Y., Chen, Q., Zheng, S., Wang, W., and Zhao, Z. WavTokenizer: An efficient acoustic discrete codec tokenizer for audio language modeling. arXiv preprint arXiv:2408.16532, 2024. Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., de las Casas, D., and others, E. B. H. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024a. Jiang, X., Peng, X., Zhang, Y., and Lu, Y. Universal speech token learning via low-bitrate neural codec and pretrained representations. IEEE Journal of Selected Topics in Signal Processing, pp. 113, 2024b. Ju, Z., Wang, Y., Shen, K., Tan, X., Xin, D., Yang, D., Liu, Y., Leng, Y., Song, K., Tang, S., Wu, Z., Qin, T., Li, X.- Y., Ye, W., Zhang, S., Bian, J., He, L., Li, J., and Zhao, S. NaturalSpeech 3: Zero-shot speech synthesis with factorized codec and diffusion models. arXiv preprint arXiv:2403.03100, 2024. Kahn, J., Riviere, M., Zheng, W., Kharitonov, E., Xu, Q., Mazare, P.-E., Karadayi, J., Liptchinsky, V., Collobert, R., Fuegen, C., et al. Libri-Light: benchmark for ASR with limited or no supervision. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 76697673, 2020. Kim, J., Lee, K., Chung, S., and Cho, J. CLam-TTS: Improving neural codec language model for zero-shot text-to-speech. In International Conference on Learning Representations (ICLR), 2024. Kolbæk, M., Yu, D., Tan, Z.-H., and Jensen, J. Multitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 25:19011913, 2017. Kong, J., Kim, J., and Bae, J. HiFi-GAN: generative adversarial networks for efficient and high fidelity speech synthesis. In International Conference on Neural Information Processing Systems (NeurIPS), 2020. Kreuk, F., Synnaeve, G., Polyak, A., Singer, U., Defossez, A., Copet, J., Parikh, D., Taigman, Y., and Adi, Y. AudioGen: Textually guided audio generation. In International Conference on Learning Representations (ICLR), 2023. Kumar, R., Seetharaman, P., Luebs, A., Kumar, I., and Kumar, K. High-fidelity audio compression with improved RVQGAN. In International Conference on Neural Information Processing Systems (NeurIPS), 2023. Lee, S., Ping, W., Ginsburg, B., Catanzaro, B., and Yoon, S. BigVGAN: universal neural vocoder with largescale training. In International Conference on Learning Representations (ICLR), 2023. Li, H., Xue, L., Guo, H., Zhu, X., Lv, Y., Xie, L., Chen, Y., Yin, H., and Li, Z. Single-Codec: Single-codebook speech codec towards high-performance speech generation. arXiv preprint arXiv:2406.07422, 2024. Liu, H., Xu, X., Yuan, Y., Wu, M., Wang, W., and Plumbley, M. D. SemantiCodec: An ultra low bitrate semantic audio codec for general sound. arXiv preprint arXiv:2405.00233, 2024. Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., and Xie, S. ConvNet for the 2020s. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Lloyd, S. P. Least squares quantization in PCM. IEEE Transactions on Information Theory, pp. 129137, 1982. Loshchilov, I. and Hutter, F. Decoupled weight decay regIn International Conference on Learning ularization. Representations (ICLR), 2019. Mentzer, F., Minnen, D., Agustsson, E., and Tschannen, M. Finite scalar quantization: VQ-VAE made simple. In International Conference on Learning Representations (ICLR), 2024. Messica, S. and Adi, Y. NAST: Noise aware speech tokenization for speech language models. In Interspeech 2024, pp. 41694173, 2024. Mousavi, P., Della Libera, L., Duret, J., Ploujnikov, A., Subakan, C., and Ravanelli, M. DASB - discrete audio and speech benchmark. arXiv preprint arXiv:2406.14294, 2024a. Mousavi, P., Duret, J., Zaiem, S., Della Libera, L., Ploujnikov, A., Subakan, C., and Ravanelli, M. How should we extract discrete audio tokens from self-supervised models? In Interspeech, pp. 25542558, 2024b. Nguyen, T. A., Muller, B., Yu, B., Costa-jussa, M. R., Elbayad, M., Popuri, S., Duquenne, P.-A., Algayres, R., 10 FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks Mavlyutov, R., Gat, I., Synnaeve, G., Pino, J., Sagot, B., and Dupoux, E. SpiRit-LM: Interleaved spoken and written language model. arXiv preprint arXiv:2402.05755, 2024. OpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, arXiv preprint J., et al. GPT-4 technical report. arXiv:2303.08774, 2023. Panayotov, V., Chen, G., Povey, D., and Khudanpur, S. LibriSpeech: An ASR corpus based on public domain audio books. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 52065210, 2015. Parker, J. D., Smirnov, A., Pons, J., Carr, C., Zukowski, Z., Evans, Z., and Liu, X. Scaling transformers for low-bitrate high-quality speech coding. arXiv preprint arXiv:2411.19842, 2024. Pia, N., Strauss, M., Multrus, M., and Edler, B. FlowMAC: Conditional flow matching for audio coding at low bit rates. arXiv preprint arXiv:2409.17635, 2024. Polyak, A., Adi, Y., Copet, J., Kharitonov, E., Lakhotia, K., Hsu, W.-N., Mohamed, A., and Dupoux, E. Speech resynthesis from discrete disentangled self-supervised representations. In Interspeech, pp. 36153619, 2021. Pratap, V., Xu, Q., Sriram, A., Synnaeve, G., and Collobert, R. MLS: large-scale multilingual dataset for speech research. In Interspeech, pp. 27572761, 2020. Qiu, K., Li, X., Chen, H., Sun, J., Wang, J., Lin, Z., Savvides, M., and Raj, B. Efficient autoregressive audio modeling via next-scale prediction. arXiv preprint arXiv:2408.09027, 2024. Radford, A., Kim, J. W., Xu, T., Brockman, G., Mcleavey, C., and Sutskever, I. Robust speech recognition via largeIn International Conference scale weak supervision. on Machine Learning (ICML), volume 202, pp. 28492 28518, 2023. Ravanelli, M., Parcollet, T., Plantinga, P., Rouhe, A., Cornell, S., Lugosch, L., Subakan, C., Dawalatabad, N., Heba, A., Zhong, J., Chou, J.-C., Yeh, S.-L., Fu, S.- W., Liao, C.-F., Rastorgueva, E., Grondin, F., Aris, W., Na, H., Gao, Y., Mori, R. D., and Bengio, Y. SpeechBrain: general-purpose speech toolkit. arXiv preprint arXiv:2106.04624, 2021. Champion, P., Rouhe, A., Braun, R., Mai, F., ZuluagaGomez, J., Mousavi, S. M., Nautsch, A., Nguyen, H., Liu, X., Sagar, S., Duret, J., Mdhaffar, S., Laperri`ere, G., Rouvier, M., Mori, R. D., and Est`eve, Y. Open-source conversational AI with SpeechBrain 1.0. Journal of Machine Learning Research (JMLR), 25(333):111, 2024. Reddy, C. K., Gopal, V., and Cutler, R. DNSMOS P.835: non-intrusive perceptual objective speech quality metric In IEEE International to evaluate noise suppressors. Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022. Ren, Y., Wang, T., Yi, J., Xu, L., Tao, J., Zhang, C. Y., and Zhou, J. Fewer-token neural speech codec with timeinvariant codes. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1273712741, 2024. Rix, A., Beerends, J., Hollier, M., and Hekstra, A. Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 749752, 2001. Saeki, T., Xin, D., Nakata, W., Koriyama, T., Takamichi, S., and Saruwatari, H. UTMOS: UTokyo-SaruLab system for VoiceMOS challenge 2022. In Interspeech, pp. 4521 4525, 2022. Schoeffler, M., Bartoschek, S., Stoter, F.-R., Roess, M., Westphal, S., Edler, B., and Herre, J. webMUSHRA - comprehensive framework for web-based listening tests. Journal of Open Research Software, 2018. Shi, J., Ma, X., Inaguma, H., Sun, A., and Watanabe, S. MMM: Multi-layer multi-residual multi-stream discrete speech representation from self-supervised learning model. In Interspeech, pp. 25692573, 2024. Siuzdak, H. Vocos: Closing the gap between time-domain and fourier-based neural vocoders for high-quality audio synthesis. In International Conference on Learning Representations (ICLR), 2024. Siuzdak, H., Grotschla, F., and Lanzendorfer, L. A. SNAC: Multi-scale neural audio codec. In Audio Imagination: NeurIPS 2024 Workshop AI-Driven Speech, Music, and Sound Generation, 2024. Ravanelli, M., Parcollet, T., Moumen, A., de Langen, S., Subakan, C., Plantinga, P., Wang, Y., Mousavi, P., Della Libera, L., Ploujnikov, A., Paissan, F., Borra, D., Zaiem, S., Zhao, Z., Zhang, S., Karakasidis, G., Yeh, S.-L., Taal, C. H., Hendriks, R. C., Heusdens, R., and Jensen, J. An algorithm for intelligibility prediction of timefrequency IEEE Transactions on Audio, weighted noisy speech. Speech and Language Processing, pp. 21252136, 2011. 11 FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks Valentini-Botinhao, C., Wang, X., Takaki, S., and Yamagishi, J. Investigating RNN-based speech enhancement methods for noise-robust text-to-speech. In Speech Synthesis Workshop, pp. 146152, 2016. Yang, D., Liu, S., Huang, R., Tian, J., Weng, C., and Zou, Y. HiFi-Codec: Group-residual vector quantization for high fidelity audio codec. arXiv preprint arXiv:2305.02765, 2023. van den Oord, A., Vinyals, O., and Kavukcuoglu, K. NeuIn International ral discrete representation learning. Conference on Neural Information Processing Systems (NeurIPS), pp. 63096318, 2017. Yang, D., Wang, D., Guo, H., Chen, X., Wu, X., and Meng, H. SimpleSpeech: Towards simple and efficient text-tospeech with scalar latent transformer diffusion models. In Interspeech, pp. 43984402, 2024a. van Niekerk, B., Carbonneau, M.-A., Zaıdi, J., Baas, M., Seute, H., and Kamper, H. comparison of discrete and soft speech units for improved voice conversion. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 65626566, 2022. Wang, C., Chen, S., Wu, Y., Zhang, Z., Zhou, L., Liu, S., Chen, Z., Liu, Y., Wang, H., Li, J., He, L., Zhao, S., and Wei, F. Neural codec language models are zero-shot text to speech synthesizers. arXiv preprint arXiv:2301.02111, 2023. Wang, Z., Zhu, X., Zhang, Z., Lv, Y., Jiang, N., Zhao, G., and Xie, L. SELM: Speech enhancement using discrete tokens and language models. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1156111565, 2024. Wang, Z.-Q. et al. Sequential multi-frame neural beamforming for speech separation and enhancement. In IEEE Spoken Language Technology Workshop (SLT), pp. 905 911, 2021. Wichern, G., Antognini, J., Flynn, M., Zhu, L. R., McQuinn, E., Crow, D., Manilow, E., and Roux, J. L. WHAM!: Extending speech separation to noisy environments. In Interspeech, pp. 13681372, 2019. Wu, H., Kanda, N., Eskimez, S. E., and Li, J. TS3-Codec: Transformer-based simple streaming single codec. arXiv preprint arXiv:2411.18803, 2024a. Wu, Y.-C., Markovic, D., Krenn, S., Gebru, I. D., and Richard, A. ScoreDec: phase-preserving high-fidelity audio codec with generalized score-based diffusion postfilter. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 361365, 2024b. Xin, D., Tan, X., Takamichi, S., and Saruwatari, H. BigCodec: Pushing the limits of low-bitrate neural speech codec. arXiv preprint arXiv:2409.05377, 2024. Yang, H., Jang, I., and Kim, M. Generative de-quantization for neural speech codec via latent diffusion. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2024b. Yang, J., Li, C., Dai, X., and Gao, J. Focal modulation networks. In International Conference on Neural Information Processing Systems (NeurIPS), 2022. Yu, L., Lezama, J., Gundavarapu, N. B., Versari, L., Sohn, K., Minnen, D., Cheng, Y., Gupta, A., Gu, X., Hauptmann, A. G., Gong, B., Yang, M.-H., Essa, I., Ross, D. A., and Jiang, L. Language model beats diffusion - tokenizer is key to visual generation. In International Conference on Learning Representations (ICLR), 2024. Zeghidour, N., Luebs, A., Omran, A., Skoglund, J., and Tagliasacchi, M. SoundStream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp. 495507, 2021. Zen, H., Dang, V., Clark, R., Zhang, Y., Weiss, R. J., Jia, Y., Chen, Z., and Wu, Y. LibriTTS: corpus derived from LibriSpeech for text-to-speech. In Interspeech, 2019. Zhang, D., Li, S., Zhang, X., Zhan, J., Wang, P., Zhou, Y., and Qiu, X. SpeechGPT: Empowering large language models with intrinsic cross-modal conversational abilities. In Findings of the Association for Computational Linguistics: EMNLP, pp. 1575715773, 2023. Zhang, X., Zhang, D., Li, S., Zhou, Y., and Qiu, X. SpeechTokenizer: Unified speech tokenizer for speech large language models. In International Conference on Learning Representations (ICLR), 2024. Zhao, Y., Xiong, Y., and Krahenbuhl, P. Image and video tokenization with binary spherical quantization. arXiv preprint arXiv:2406.07548, 2024. Zheng, Y., Tu, W., Kang, Y., Chen, J., Zhang, Y., Xiao, L., Yang, Y., and Ma, L. FreeCodec: disentangled neural speech codec with fewer tokens. arXiv preprint arXiv:2412.01053, 2024. Yamagishi, J., Veaux, C., and MacDonald, K. CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit. University of Edinburgh. The Centre for Speech Technology Research (CSTR), 6:15, 2017. Ziyin, L., Hartwig, T., and Ueda, M. Neural networks fail to learn periodic functions and how to fix it. In International Conference on Neural Information Processing Systems (NeurIPS), 2020. 12 FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks A. Limitations The proposed codec comes with some inherent trade-offs. From an architectural perspective, it is not causal. While it can be streamed in chunk-wise fashion (see Appendix F.3), its latency remains too high for real-time applications. Future work should explore alternative designs and/or training strategies to enable fully causal architecture. Regarding the dataset, aside from the 94,000 hours of pretraining data used for the WavLM encoder, our compressor, quantizer, decompressor, and decoder were trained only on few hundred hours of clean English speech sampled at 16 kHz. Expanding the dataset to include more data, broader range of domains (e.g. multilingual speech, mixtures, etc.) and increasing the sampling rate (e.g. 24 kHz) could further improve quality, robustness, and versatility of the model. Additionally, the model is limited to speech and does not support high-fidelity reconstruction of music or environmental sounds. B. Datasets The following datasets were used in this work: LibriSpeech (Panayotov et al., 2015) is large-scale corpus of English read speech derived from audiobooks in the LibriVox project. It contains approximately 1000 hours of speech sampled at 16 kHz, with predefined training, validation, and test splits. LibriTTS (Zen et al., 2019) is corpus designed for text-to-speech research, constructed from the same source as LibriSpeech. It consists of 585 hours of transcribed speech with predefined training, validation, and test splits. Multilingual LibriSpeech (Pratap et al., 2020) is an extension of LibriSpeech to multiple languages, including English, German, Dutch, French, Spanish, Italian, Portuguese and Polish. It provides approximately 44,500 hours of transcribed English speech and about 6000 hours from other languages. VoiceBank (Valentini-Botinhao et al., 2016) is dataset primarily used for speech enhancement, including 11,572 utterances from 28 speakers in the training set (noise at 0 dB, 5 dB, 10 dB, and 15 dB), and 872 utterances from 2 unseen speakers in the test set (noise at 2.5 dB, 7.5 dB, 12.5 dB, and 17.5 dB). LibriMix (Cosentino et al., 2020) is dataset for speech separation and enhancement, created by mixing LibriSpeech utterances with noise from the WHAM! (Wichern et al., 2019) corpus. It provides mixtures of two or three speakers at different signal-to-noise ratios. VCTK (Yamagishi et al., 2017) is corpus of English speech recordings from 110 speakers with various accents. It is widely used for speaker adaptation, text-to-speech, and voice conversion tasks. IEMOCAP (Busso et al., 2008) is dataset designed for emotion recognition, consisting of scripted and improvised dialogues performed by 10 actors. It includes audio, video, and textual transcriptions with emotion labels such as happiness, sadness, and anger. C. Baselines Additional information about the baseline codecs is provided in Table 7. For our WavLM6-KM (Wang et al., 2024) reproduction, we use LibriSpeech train-clean-100 and train-clean-360. First, we train k-means quantizer with 512 centroids on top of layer-6 representations from WavLM-large. We train on audio chunks of 16,000 samples with large batch size of 512 for improved stability, and we stop training when cluster centroids stop changing significantly. Then, we train dequantizer to minimize the L2 loss between quantized and original WavLM features. We employ Conformer (Gulati et al., 2020) encoder with 6 layers, 4 attention heads, hidden dimension of 512, and feed-forward layer dimension of 512. We train on audio chunks of 7040 samples with batch size of 16. We use the AdamW (Loshchilov & Hutter, 2019) optimizer with an initial learning rate of 0.0005, β1 of 0.8, β2 of 0.99, weight decay of 0.01, and dropout of 0.1. The learning rate is reduced by factor of 0.9 if validation loss does not improve within margin of 0.0025. Gradients are clipped to maximum L2 norm of 5. Training stops when validation loss does not decrease for several consecutive epochs. Finally, we train HiFi-GAN V1 (Kong et al., 2020) decoder on audio chunks of 7040 samples with batch size of 16. We use the AdamW optimizer with an initial learning rate of 0.0002, β1 of 0.8, β2 of 0.99, and weight decay of 0.01. The learning rate follows an exponential decay schedule with factor of 0.999. Training continues until perceived audio quality stops improving. 13 FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks Codec Causal Training Datasets Hours Multilingual Audio Domain Checkpoint Table 7. Baseline codecs. EnCodec (Defossez et al., 2023) Optional DNS, CommonVoice, AudioSet, FSD50K, Jamendo 17,000+ DAC (Kumar et al., 2023) WavLM6-KM (Wang et al., 2024) SpeechTokenizer (Zhang et al., 2024) SemantiCodec (Liu et al., 2024) Mimi (Defossez et al., 2024) WavTokenizer (Ji et al., 2024) BigCodec (Xin et al., 2024) No No No No Yes No No DAPS, DNS, CommonVoice, VCTK, MUSDB, Jamendo 10,000+ Subset of LibriSpeech (in addition to Libri-Light, GigaSpeech, and VoxPopuli English for WavLM pretraining) 460 (+ 94,000) LibriSpeech GigaSpeech, subset of OpenSLR, Million Song Dataset, MedleyDB, MUSDB18, AudioSet, WavCaps, VGGSound 960 20,000+ Yes Yes No No Yes General General encodec 24khz weights 16khz.pth Speech discrete-wavlm-codec Speech speechtokenizer hubert avg General semanticodec tokenrate 50 Predominantly English speech (in addition to Libri-Light, GigaSpeech, and VoxPopuli English for WavLM pretraining) 7,000,000 (+ 94,000) Likely Speech mimi LibriTTS, VCTK, subset of CommonVoice, subset of AudioSet, Jamendo, MUSDB LibriSpeech 8000 960 Yes No No General WavTokenizer-large-unify-40token Speech Speech bigcodec.pt stable-codec-speech-16k Stable Codec (Parker et al., 2024) Optional Libri-Light, Multilingual LibriSpeech English 105, D. Hyperparameters and Training Details D.1. FocalCodec The compressor processes 1024-dimensional WavLM features and forwards them through 3 focal downscaling blocks with hidden dimensions of 1024, 512, and 256, respectively. Each block has two focal levels, window size of 7, focal factor of 2, and layer scale initialization of 0.0001. final projection maps the 256-dimensional hidden states to latent representations of dimension 13, which are then quantized with binary spherical codebook of 213 = 8192 codes. The decompressor mirrors the compressor, replacing focal downscaling blocks with focal upscaling blocks to reconstruct the 1024-dimensional continuous representations from the quantized latent codes. We use weight of 1.0 for the reconstruction loss and weight of 0.1 for the entropy loss. We train on LibriTTS (Zen et al., 2019) (585 hours from 2456 speakers) using full utterances rather than fixed-length chunks, which differs from related work. This approach allows us to fully exploit the unlimited receptive field of focal modulation. This is in line with our vision that the encoder should be as powerful as possible to extract high-quality representations, while the decoder can be lightweight and use limited context windows. For this stage, we use the AdamW (Loshchilov & Hutter, 2019) optimizer with an initial learning rate of 0.0005, β1 of 0.8, β2 of 0.99, and weight decay of 0.01. The learning rate is reduced by factor of 0.9 if validation loss does not improve within margin of 0.0025. Gradients are clipped to maximum L2 norm of 5. Training stops when validation loss does not decrease for several consecutive epochs. The decoder processes 1024-dimensional WavLM features and forwards them through 8 ConvNeXt blocks with hidden dimension of 512, feed-forward dimension of 1536, kernel size of 7, and padding of 3. For the STFT, we set the FFT size to 1024 samples and the hop length to 320. The feature matching loss is calculated using 80-dimensional log-Mel spectrograms with the same STFT configuration. The discriminator adopts the convolutional architecture introduced in (Kong et al., 2020). We train on LibriTTS using audio chunks of 7040 samples with batch size of 16. Due to resource constraints, our training is limited to the train-clean-100 split. We found this amount of data sufficient to obtain high-quality reconstructions. We use the AdamW optimizer with an initial learning rate of 0.0002, β1 of 0.8, β2 of 0.99, and weight decay of 0.01. The learning rate follows an exponential decay schedule with factor of 0.999. Training continues until perceived audio quality stops improving, which occurs around 3M steps. For the smaller variant of FocalCodec used in the ablation studies, we employ the same setup with the following modifications: the hidden sizes in the three focal downscaling blocks are reduced from 1024, 512, 256 to 512, 256, 128; the codebook size is decreased to 1024; the model is trained on LibriSpeech train-clean-100 using batch size of 4. D.2. Downstream Tasks Automatic Speech Recognition (ASR). The model architecture is 2-layer BiLSTM with 512-dimensional hidden states. CTC (Graves et al., 2006) head is stacked on top and trained to predict either characters or BPE units. Experiments use characters and BPE vocabularies of sizes 250, 500, and 1000, with the best result reported. Note that for Mimi and FocalCodec@12.5, training on characters is infeasible due to the low token rate (12.5 Hz), which results in hidden sequences shorter than the target, making them incompatible with CTC loss. For all models except Mimi, performance improves 14 FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks monotonically with increasing BPE sizes up to 1000, while Mimi achieves the best results with BPE-500. If the codec employs multiple codebooks, we compute weighted sum of the embeddings from each codebook, with the weights learned during training, as done in (Chen et al., 2022). The embedding layer is initialized using the discrete embeddings from the codec quantizer. Speaker Identification (SI). The SI setup closely mirrors that of ASR. The only difference is that the BiLSTM output sequence is aggregated using statistics pooling, followed by cross-entropy classification head. Speech Emotion Recognition (SER). The SER setup is the same as SI, where only the number of output classes is different. Speech Enhancement (SE). The model architecture is Conformer (Gulati et al., 2020) encoder with 6 layers, 4 attention heads, model dimension of 512, and feed-forward layer dimension of 2048. Codecs with multiple codebooks use weighted sum of embeddings for the input, with independent linear heads for each codebook in the output. The embedding layer is initialized using the discrete embeddings from the codec quantizer. Training is performed using cross-entropy loss between predicted and target tokens. Speech Separation (SS). The SS setup closely mirrors that of SE. The only difference is that training is performed using cross-entropy loss with permutation invariant training (Kolbæk et al., 2017), and the number of output heads is doubled to account for predicting two sources in parallel. Text-to-Speech (TTS). The model architecture is an autoregressive Llama 3 (Grattafiori et al., 2024) decoder with 12 layers, 4 attention heads, 1 key-value head, model dimension of 512, feed-forward layer dimension of 2048, and base RoPE frequency of 10,000. To provide speaker information, we extract speaker embeddings from the target utterance using WavLM-base (Chen et al., 2022), fine-tuned for speaker verification. The pooled speaker embedding is prepended to the text embeddings to condition the model on speaker identity. The embedding layer is initialized using the discrete embeddings from the codec quantizer. Training is performed with next-token prediction, where the input sequence consists of pooled speaker embedding, text embeddings, and speech token embeddings. The cross-entropy loss is computed only on speech tokens, while the text and speaker embeddings are excluded from loss computation. For inference, we use top-p sampling with = 0.9 and temperature of 1.0. Training Details. For all tasks, we use AdamW (Loshchilov & Hutter, 2019) optimizer with batch size of 16, an initial learning rate of 0.0001, β1 = 0.8, β2 = 0.99, weight decay of 0.01, and dropout of 0.1. The learning rate is reduced by factor of 0.9 if validation loss does not improve within margin of 0.0025. Gradients are clipped to maximum L2 norm of 0.01. Training stops if validation loss does not decrease for several consecutive epochs. E. Implementation and Hardware Software for the experimental evaluation was implemented in Python using the SpeechBrain (Ravanelli et al., 2021; 2024) toolkit. Each model is trained on single GPU, with the choice between V100 GPUs (16 or 32 GB) and A100 GPUs (40 GB), depending on cluster resource availability. F. Additional Results F.1. Subjective Evaluation We conduct subjective test with 21 participants who rate total of 10 reconstructions from LibriSpeech test-clean. Following prior work (Defossez et al., 2023; Zhang et al., 2024; Liu et al., 2024; Parker et al., 2024), we employ the MUSHRA (Schoeffler et al., 2018) format without hidden anchor. Listeners compare multiple versions of an example at once, including labeled reference and hidden reference. They are asked the following question: Please evaluate the quality proximity between an audio sample and its reference. Please listen carefully to the reference audio and then rate the quality of each test audio clip compared to the reference. Use the scale where 0 indicates no resemblance to the reference, and 100 means perfectly the same as the reference. Participants were recruited online by sharing link to the test across various public channels. To keep the subjective test short, we selected subset of baselines based on their overall performance in objective metrics6. As showcased in Figure 2, FocalCodec can achieve extremely low bitrates without 6All the baselines are presented on the project page to foster reproducibility and fair comparison. 15 FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks compromising performance significantly. We observe that FocalCodec is Pareto-optimal until 0.50 kbps. Its performance is superior to most baselines and remains comparable to BigCodec, Stable Codec and WavTokenizer. Finally, we emphasize that FocalCodec strikes the optimal trade-off between MOS and downstream task performance, balancing perceived audio quality and objective metrics. Figure 2. Subjective evaluation from 21 participants averaged over 10 samples. Left. Trade-off between mean opinion score and bitrate. The green dashed line highlights the reference score. Right. Distribution of mean opinion score. The red lines highlight the median. FocalCodec@50 median is marginally lower than BigCodec, Stable Codec and WavTokenizer. However, user preference remains comparable when accounting for variability. F.2. Comparison to TS3-Codec TS3-Codec (Wu et al., 2024a) is recent transformer-only architecture designed for low-bitrate streaming speech coding. Despite its lower bitrate and streamable architecture, it remains competitive with BigCodec, the current state-of-the-art. Like FocalCodec, it utilizes single quantizer. However, its fully transformer-based architecture prioritizes reconstruction, focusing on acoustic representations. The model was trained on Libri-Light (Kahn et al., 2020). Since the model is not publicly available, we reached out to the authors to obtain reconstructions of the LibriSpeech test-clean for comparison. Table 8 shows the results. FocalCodec@50 surpasses TS3-Codec across all evaluated metrics, while FocalCodec@25, despite operating at significantly lower bitrate, still achieves superior performance in terms of UTMOS and dWER. These findings further highlight the effectiveness of the proposed models. Table 8. Clean speech resynthesis on LibriSpeech test-clean. Codec Bitrate (kbps) Sample Rate (kHz) Token Rate (Hz) Codebooks Code Size Params (M) MACs (G) UTMOS dWER Sim Reference TS3-Codec (X2) FocalCodec@50 FocalCodec@25 FocalCodec@12.5 0.85 0.65 0.33 0. 16 16 16 16 50.0 50.0 25.0 12.5 1 131072 1 8192 1 8192 1 16 13 13 13 204 142 144 145 8 9 9 4.09 3.84 4.05 4.14 4.22 0.00 4.51 2.18 3.30 7.94 100.0 97.1 97.4 96.3 93. 16 FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks F.3. Chunk-Wise Streaming Inference Although our codec is non-causal, it can be made streamable via chunked inference. This involves splitting the input signal into fixed-size chunks with certain amount of overlap to reduce boundary artifacts. To assess the streamability of our codec, we use left context of 48,000 samples (3 seconds) and an overlap of 250 samples while varying the chunk size. The reconstructed chunks are stitched together using the overlapadd method with linear fade-in/fade-out. As shown in Table 9, FocalCodec@50 maintains acceptable performance when streamed with chunk size of 8000 (500 milliseconds). In contrast, for FocalCodec@25 and FocalCodec@12.5, performance drops significantly. This is because, due to the 2x and 4x downsampling along the time dimension, they require much larger look-ahead. Table 9. Offline vs chunk-wise streaming inference. Codec Chunk Size FocalCodec@50 FocalCodec@25 FocalCodec@12.5 Inf Inf Inf FocalCodec@50 2000 (125 ms) FocalCodec@50 4000 (250 ms) FocalCodec@50 8000 (500 ms) FocalCodec@25 8000 (500 ms) FocalCodec@12.5 8000 (500 ms) UTMOS dWER Sim LibriSpeech test-clean 4.05 4.14 4.22 2.17 2.71 3.16 2.95 2.84 2.18 3.30 7.94 6.06 4.62 4.55 12.17 47.43 97.4 96.3 93.9 95.9 96.6 96.9 95.6 91. F.4. Speech Separation with Continuous Input Although our codec outperforms the baselines in speech separation (see Section 4.5), its performance remains insufficient for practical use. We hypothesize that this limitation stems not from the lack of expressive power of the continuous representations since WavLM is pretrained on simulated noisy and overlapping speech, its lower layers retain the ability to represent mixtures (Chen et al., 2022) but rather from the fact that our compressor, quantizer, decompressor, and decoder are not explicitly trained on mixtures. As result, the quantized representations struggle to disentangle speakers. To investigate this further, we replaced the discrete input representations with continuous representations from the encoder. We focused on FocalCodec@50 and WavLM-KM6, the top two models in terms of intelligibility of reconstructed speech, both of which share the same encoder (WavLM-large layer-6). As shown in Table 10, using continuous representations as input instead of discrete ones significantly improves performance, with FocalCodec@50 outperforming WavLM-KM6 in both dWER and Sim. This suggests that WavLM-large layer-6 retains substantial information relevant to speech separation. Training the quantizer with mixtures could further improve the quality of the learned tokens for this task. Table 10. Discrete vs continuous input features for speech separation. Codec Reference WavLM6-KM WavLM6-KM Input Features Discrete Continuous FocalCodec@50 FocalCodec@50 Continuous Discrete DNSMOS dWER Sim Libri2Mix 3.77 3.49 3. 3.71 3.76 0.00 76.91 23.09 73.87 17.35 100.0 85.0 89.4 89.0 93.8 FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks F.5. Mel-Spectrogram Analysis Figure 3 shows examples of reconstructed Mel-spectrograms from LibriSpeech (left) and Libri1Mix (right), using the 3 top-performing codecs. The reconstructed speech from LibriSpeech is almost indistinguishable from the ground truth. For Libri1Mix, the first row shows audio contaminated with noise, while the second row shows the original clean audio. It can be observed that BigCodec, purely acoustic codec trained for reconstruction, attempts to reconstruct the noise, resulting in poor intelligibility. In contrast, Stable Codec and FocalCodec, which have semantically meaningful representations, are able to perform basic denoising. Notably, FocalCodec assigns more energy to the frequency bands corresponding to speech, even more than in the original clean audio, leading to improved intelligibility. On the other hand, Stable Codec, while providing good denoising, introduces some artifacts and static noise in the lower part of the spectrogram, which degrades intelligibility. Figure 3. Reconstructed Mel-spectrograms from LibriSpeech (left) and Libri1Mix (right)."
        }
    ],
    "affiliations": [
        "Concordia University, Montreal, Canada"
    ]
}