{
    "paper_title": "How to Steer LLM Latents for Hallucination Detection?",
    "authors": [
        "Seongheon Park",
        "Xuefeng Du",
        "Min-Hsuan Yeh",
        "Haobo Wang",
        "Yixuan Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Hallucinations in LLMs pose a significant concern to their safe deployment in real-world applications. Recent approaches have leveraged the latent space of LLMs for hallucination detection, but their embeddings, optimized for linguistic coherence rather than factual accuracy, often fail to clearly separate truthful and hallucinated content. To this end, we propose the Truthfulness Separator Vector (TSV), a lightweight and flexible steering vector that reshapes the LLM's representation space during inference to enhance the separation between truthful and hallucinated outputs, without altering model parameters. Our two-stage framework first trains TSV on a small set of labeled exemplars to form compact and well-separated clusters. It then augments the exemplar set with unlabeled LLM generations, employing an optimal transport-based algorithm for pseudo-labeling combined with a confidence-based filtering process. Extensive experiments demonstrate that TSV achieves state-of-the-art performance with minimal labeled data, exhibiting strong generalization across datasets and providing a practical solution for real-world LLM applications."
        },
        {
            "title": "Start",
            "content": "How to Steer LLM Latents for Hallucination Detection? Seongheon Park 1 Xuefeng Du 1 Min-Hsuan Yeh 1 Haobo Wang 2 Yixuan Li 1 5 2 0 2 1 ] . [ 1 7 1 9 1 0 . 3 0 5 2 : r Abstract Hallucinations in LLMs pose significant concern to their safe deployment in real-world applications. Recent approaches have leveraged the latent space of LLMs for hallucination detection, but their embeddings, optimized for linguistic coherence rather than factual accuracy, often fail to clearly separate truthful and hallucinated content. To this end, we propose the Truthfulness Separator Vector (TSV), lightweight and flexible steering vector that reshapes the LLMs representation space during inference to enhance the separation between truthful and hallucinated outputs, without altering model parameters. Our twostage framework first trains TSV on small set of labeled exemplars to form compact and wellseparated clusters. It then augments the exemplar set with unlabeled LLM generations, employing an optimal transport-based algorithm for pseudolabeling combined with confidence-based filtering process. Extensive experiments demonstrate that TSV achieves state-of-the-art performance with minimal labeled data, exhibiting strong generalization across datasets and providing practical solution for real-world LLM applications. 1. Introduction Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation, showcasing their potential as general-purpose task solvers (Zhao et al., 2023). Despite their success, LLMs can generate hallucinated outputsstatements that appear plausible but factually inaccurate or unsupported. Such hallucinations can undermine user trust and lead to potentially harmful consequences, especially in high-stake applications (Zhang et al., 2023; Pal et al., 2023). Therefore, to be truly trustworthy, an LLM must not only generate text that is consistent with user prompts but also possess the ability to detect hallucinations and alert users when they occur. 1Department of Computer Sciences, University of Wisconsin-Madison, USA 2School of Software Technology, Zhejiang University, China. Correspondence to: Yixuan Li <sharonli@cs.wisc.edu>. Figure 1. T-SNE visualization (Van der Maaten & Hinton, 2008) of the last-token embeddings from the final layer of LLaMA-3.1-8B on the TruthfulQA test set. (a) Pre-trained models embeddings exhibit significant overlap, whereas (b) adding TSV to latent states of an intermediate LLM layer effectively separates the embeddings of truthful and hallucinated data. Recent work has explored leveraging the latent space of LLMs to identify hallucinations (Burns et al., 2023; Azaria & Mitchell, 2023; Marks & Tegmark, 2024; Yin et al., 2024a; Du et al., 2024; Chen et al., 2024a; Li et al., 2024; Kossen et al., 2024). These approaches typically rely on the embeddings of off-the-shelf LLMs to classify outputs as truthful or hallucinated. However, pre-trained LLMs are optimized for linguistic coherence using next-token prediction objective, often prioritizing fluency and syntactic correctness over factual accuracy (Radford et al., 2019). As result, their internal representations, while powerful for general text generation, can fail to provide clear separation between truthful and hallucinated content (see real-world example in Figure 1a). This motivates key question: How can we shape the latent space of an LLM for hallucination detection? Instead of fine-tuning the LLMs, which is computationally expensive and alters the models parameters (Gekhman et al., 2024), we propose learning lightweight vector, called Truthfulness Separator Vector (TSV). As illustrated in Figure 1b, this learnable vector is introduced during inference and adjusting the internal representations of the LLM to enhance the separation between truthful and hallucinated generations, without modifying the models parameters. TSV focuses on reshaping the latent space for classifying halluci1 How to Steer LLM Latents for Hallucination Detection? nated responses, fundamentally different objective from mitigating hallucinated generations (Li et al., 2024; Chen et al., 2024b; Marks & Tegmark, 2024). To the best of our knowledge, this is the first exploration of steering representations for hallucination detection. Learning TSV is appealing yet challenging due to the lack of large-scale human-labeled datasets with truthfulness annotations for LLM generation, which are costly and timeintensive to create. To overcome this, we propose twostage training framework. In the initial stage, small exemplar set of labeled data is used to guide the learning process. The objective in this stage is to encourage the steered embeddings to form compact clusters around class prototypes, representing truthful and hallucinated generations. In the second stage, we augment the training data by leveraging unlabeled LLM generations, which are freely available for deployed LLM systems through user queries and interactions (Du et al., 2024). To assign pseudo-labels to these unlabeled samples, we propose an optimal transportbased algorithm, which aligns unlabeled data embeddings with class prototypes by minimizing transport costs while accounting for the imbalanced class proportions. Furthermore, confidence-based sample selection is then used to include only the most reliable pseudo-labeled samples in the training process. Together, these stages enable TSV to effectively separate truthful and hallucinated representations while significantly reducing the reliance on human labeling. Extensive experiments demonstrate the strong performance of our method across diverse datasets. On the challenging TruthfulQA benchmark (Lin et al., 2022a), our approach achieves significant +12.8% improvement in hallucination detection accuracy (AUROC) compared to state-of-the-art methods. Notably, our method reaches performance comparable to the fully-supervised upper bound (e.g., 84.2% vs. 85.5% on TruthfulQA), while using small labeled exemplar set with only 32 examples. TSV also exhibits strong generalization capabilities, maintaining competitive performance when applied to unseen datasets. Our key contributions are summarized as follows: 1. We propose the Truthfulness Separator Vector (TSV), lightweight approach to separate truthful and hallucinated representations without fine-tuning the LLMs, which is largely unexplored in hallucination detection. 2. We develop an optimal transport-based pseudo-labeling framework with confidence-based sample selection to leverage unlabeled LLM generations effectively. 3. We demonstrate TSVs superior performance and perform in-depth ablation studies to evaluate the impact of various design choices in TSV and validate its scalability across larger LLMs and diverse datasets. These findings provide systematic understanding of leverag2 ing steering vector and limited labeled data for hallucination detection, paving the way for future research. 2. Related Works Hallucination detection has emerged as critical area of research, addressing safety concerns of LLMs and their deployment in real-world applications (Huang et al., 2023). plethora of works address hallucination detection by designing uncertainty scoring functions. For instance, logit-based methods utilize token-level probability as an uncertainty score (Ren et al., 2022; Malinin & Gales, 2021; Kuhn et al., 2023), verbalized methods prompt LLMs to express their uncertainty in human language (Lin et al., 2022b; Xiong et al., 2024), and consistency-based methods assess uncertainty by evaluating the consistency across multiple responses (Manakul et al., 2023; Chen et al., 2024a). Recently, internal state-based methods leverage hidden activations, employing techniques such as contrast-consistent search (Burns et al., 2023) and identifying hallucination subspace (Du et al., 2024). However, these approaches often rely on default LLM embeddings that do not inherently separate truthful and hallucinated data. In contrast, our method aims to shape the latent space through learnable steering vector for enhanced separation between the two types of data. On the other hand, supervised methods leverage labeled data to train the classifier, assuming that pre-trained LLMs encode the truthfulness of responses within their internal states (Azaria & Mitchell, 2023; Marks & Tegmark, 2024). However, these methods require extensive labeling efforts. In contrast, our method performs hallucination detection with minimal human supervision, which is more practical for real-world applications. Activation engineering enables control over the LLM generation during inference, applying task-specific steering vectors into the models internal representation (Zou et al., 2023). For example, several studies mitigate hallucination by shifting activations along the truthful direction identified by analyzing activation differences between contrastive pairs (Li et al., 2024; Chen et al., 2024b; Marks & Tegmark, 2024). Concurrently, representation fine-tuning methods introduce learning task-specific interventions on linear subspaces of hidden representations (Wu et al., 2024) or sparse subsets of attention heads (Yin et al., 2024b). Our approach differs in the following key aspects: (1) We learn steering vector specifically for hallucination detection, focusing on separating representations rather than mitigating hallucinated generations, and (2) while previous methods rely on large labeled datasets, our method achieves strong performance under minimal human supervision. How to Steer LLM Latents for Hallucination Detection? Figure 2. Overall framework. In the initial training phase, Truthfulness Separator Vector (TSV) is trained on an exemplar set. After initial training, (1) we assign soft pseudo-labels to the unlabeled data, (2) select confident pseudo-labeled samples, and (3) augment the exemplar set with selected samples. Finally, we retrain TSV with the augmented set. Best viewed in color. 3. Problem Setup Definition 3.1 (Hallucination detector). We define the truthful distribution Ptrue as the joint distribution over pairs of the input prompts and their corresponding truthful generations. Let denote vocabulary space of causal LLM, where each individual token is denoted as V. Given an input prompt xprompt = (x1, . . . , xn) and model generation = (xn+1, . . . , xn+m), the task of hallucination detection aims to learn binary predictor : {0, 1}: G(xprompt, x) = (cid:40) if (xprompt x) Ptrue 1, 0, otherwise , (1) where xprompt = (x1, . . . , xn, xn+1, . . . , xn+m) represents the ordered concatenation of the prompt xprompt and the generation x. Following the practical setup in recent work (Du et al., 2024), we utilize unlabeled LLM generations in the wild, which can be collected in vast quantities through user interactions with LLMs. This data can be freely collectible for any deployed LLM system, yet often contains mixture of truthful and hallucinated content. Formally, Definition 3.2 (Unlabeled data). We define the unlabeled pairs of input prompt xi prompt and LLM generation in the wild xi to be the following mixture of distributions: Punlabeled = (1 π)Ptrue + πPhallucination, where π [0, 1] is the fraction of hallucinated generation. promptx1), . . . , (xM The unlabeled dataset, DU = {(x1 prompt xM )}, is independently and identically sampled from the mixture distribution Punlabeled. Here, is the total number of unlabeled samples, and the tilde symbolizes the uncertain nature of the generation. Exemplar set. In addition to the unlabeled data, we incorporate small, practical-to-annotate set of labeled exemplars to guide the learning of hallucination detector. Specifically, pairs of input prompt ei prompt and LLM-generated responses ei can be annotated with ground-truth labels ci = {truthful, hallucinated}. This forms the labeled exemplar set: DE = {(e1 prompt eN , cN )}, prompt e1, c1), . . . , (eN where is the total number of labeled exemplars. In this paper, we will show that can be kept very small (e.g., 32) to minimize annotation costs while still providing valuable guidance for the learning process. 4. Method Overview. Since LLMs do not inherently produce optimal embeddings to separate truthful and hallucinated data, our framework introduces learnable vector, named Truthfulness Separator Vector (TSV), designed to enhance this separation within the representation space of the LLM. As illustrated in Figure 2, TSV is added into the latent states of the model during inference, which avoids the computational overhead associated with retraining or fine-tuning the model. In what follows, we describe how to learn TSV using unlabeled data and small exemplar set. 4.1. How to learn TSV? Initial training phase TSV is defined as single trainable vector Rd, which can be plugged into pre-trained LLMs after the generation is completedwithout compromising their original language capabilities. Given sequence of tokens (e.g., input prompt and generation pair), we add to h(l), which represent the d-dimensional latent states at an intermediate layer l: h(l) h(l) + λv, (2) where λ is hyperparameter which controls the strength of the steering, and is shared across all token positions. This intervention affects the embeddings in subsequent layers + 1, . . . , via the non-linear transformations inherent in LLM architecture. The last-token embedding at the final layer after applying TSV is: Φfinal(h(l) + λv) = ϕL ϕL1... ϕl+1(h(l) + λv), where ϕl indicates the non-linear transformation in layer of the transformer model. In Section 5.3, we perform ablations on different layers of applying TSV. How to Steer LLM Latents for Hallucination Detection? Training objective of TSV. To effectively detect hallucinations, it is crucial to establish clear decision boundary between truthful and hallucinated data. To this end, we propose training objective that learns TSV to separate embeddings between two classes = {truthful, hallucinated}. This is achieved by performing maximum likelihood estimation (MLE) on the exemplar set DE: arg max DE (cid:89) i=1 p(ci Φfinal(h(l) + λv)), (3) where is the index of training sample in DE. To realize the MLE objective, we need to explicitly model the probability distribution p(ci Φfinal(h(l) + λv)). In particular, we model the last-token embeddings at the final layer using hyperspherical distribution with the unit norm, where truthful and hallucinated data each form distinct clusters. This modeling aligns with the structure of embeddings typically observed after the RMSNorm layer in practical Transformer models (Dubey et al., 2024; Yang et al., 2024), where the norms of the embeddings are similar but directions can vary (see verification in Appendix E). This can be naturally characterized by the von Mises-Fisher distribution, classical probability distribution in directional statistics (Mardia & Jupp, 2009), which is analogous to spherical Gaussian distributions for features with unit norms. Under this model, the class conditional probability is given by: p(c rv) = rv(cid:1) exp (cid:0)κµ crv(cid:1) , exp (cid:0)κµ (cid:80) (4) where rv = Φfinal(h(l) + λv)/Φfinal(h(l) + λv)2, represents the normalized last-token embedding at the final layer, µc Rd is the class prototype for class c, κ 0 is the concentration parameter controlling how tightly the distribution is clustered around the mean direction µc. Empirical loss function. Under the probability model defined above, our MLE objective in Eq. 3 is equivalent to minimizing the negative log-likelihood over the exemplar set DE. This encourages embeddings within each class to cluster tightly around their respective class centroids: = 1 DE DE (cid:88) (cid:88) i=1 cC q(c rv ) log p(c rv ) (5) where q( rv can be either ground-truth or pseudo-label. ) denotes the target label distribution, which In practice, the prototype vector µc Prototype update. can be efficiently updated using exponential moving average (Wang et al., 2022): µc normalize[αµc + (1 α)rv], (6) where α is the decay rate, and rv = (cid:80) the mean of the normalized embeddings from class c. )rv q(crv q(crv (cid:80) ) denotes 4.2. How to learn TSV? Augmented training phase While we demonstrate that leveraging few labeled examples helps hallucination detection (Section 5.3), these examples may not fully capture the diversity inherent in the truthful and hallucinated data distributions. To address the limitation, we propose to further incorporate unlabeled training data to augment the learning process. Label assignment via optimal transport. Assigning labels (truthful vs. hallucinated) to unlabeled data is non-trivial task, particularly because we aim to generate pseudo-labels that align with the class distribution of LLM generations, which are naturally imbalanced (Hu et al., 2024). To this end, we propose leveraging Optimal Transport (OT) (Villani et al., 2009), which provides principled approach to label assignment. This approach aligns unlabeled data embeddings with class prototypes by minimizing transport costs while respecting the imbalanced class proportions. Given unlabeled dataset DU with samples, the optimization problem is formulated as: min Q[0,1]M 2 (cid:88) (cid:88) Qm,c log Pm,c ϵH(Q) m=1 cC s.t. Q12 = 1 1M , Q1M = w, (7) q(crv p(crv where 1M RM denotes an -dimensional vector of ones, Qm,c = 1 m) represents an entry of the matrix RM 2 for assigned joint pseudo-label probabilities, m) denotes an entry of RM 2 and Pm,c = 1 for joint probabilities estimated by our model after initial training, where p(crv m) is computed with Eq. 4. The first constraint ensures that for each unlabeled sample, the total probability mass of being assigned to two classes adds up to 1. The second constraint ensures that the number of samples assigned to each class matches the expected class probability distribution R2. Here, H(Q) = (cid:80) ij Qij log Qij is the entropy function, and ϵ is hyperparameter controlling the smoothness of the assignment. The entropy regularization term enables the computationally efficient Sinkhorn algorithm (Cuturi, 2013) to solve the problem. The minimizer of Eq. 7 can be expressed as: = diag(α)P1/ϵdiag(β), (8) where α RM and β R2 are scaling coefficient vectors ensuring that the resulting forms valid probability matrix. These scaling coefficients are determined iteratively using the following updates: α 1 1M P1/ϵβ , β (P1/ϵ)α . (9) 4 How to Steer LLM Latents for Hallucination Detection? We use the class distribution of the exemplar set as proxy for w, assuming missing completely at random (MCAR) scenario, which is natural assumption for data collected in real-world settings (Van Buuren, 2018). Confident data selection. Since pseudo-labels predicted for the unlabeled data may be incorrect and thus introduce noise into the learning process, we propose selecting only the most confident pseudo-labeled samples from the unlabeled dataset DU, which are most likely to be correct. We measure the models predictive uncertainty using the crossentropy between the assigned pseudo-label distribution and the models predicted distribution p. Specifically, for each unlabeled sample ri, we define: (cid:40) Ω = (cid:88) cC q(c rv ) log p(c rv ) (cid:41) IDU , (10) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) where IDU denotes the index set of DU. We then select samples from DU to form the subset DS: TopKiIDU where TopK denotes the indices of the samples with the lowest uncertainty values, and Dj DS = {Dj is j-th data in DU. (Ωi)}, (11) Exemplar set augmentation. Finally, we augment the original training dataset DE by incorporating the selected samples DS along with their pseudo-labels: DE DE DS. (12) The learning process described in Section 4.1 is then repeated using the augmented dataset until convergence. We summarize the full algorithm in Appendix F. 4.3. Inference-time hallucination detection During inference, we leverage the learned class prototypes µc to perform hallucination detection. Specifically, we compute the truthfulness score as the normalized probability of test inputs embedding vector rv test being assigned to the truthful class. The scoring function is defined as: S(x) = exp (cid:0)κµ (cid:80) truthfulrv test exp (cid:0)κµ crv test (cid:1) (cid:1) . (13) Based on the scoring function, the hallucination detector is Gζ(xtest) = 1{S(xtest) ζ}, where 1 indicates the truthful class and 0 indicates otherwise. The task can be seamlessly switched back to the original text generation by simply removing TSV v, restoring the models initial generation capabilities without additional modifications. 5. Experiments 5.1. Setup Datasets. We evaluate our method on four generative three open-domain QA question-answering (QA) tasks: 5 datasetsTruthfulQA (Lin et al., 2022a), TriviaQA (Joshi et al., 2017), and NQ Open (Kwiatkowski et al., 2019); and domain-specific QA datasetSciQ (Welbl et al., 2017). For evaluation, 25% of the QA pairs from each dataset are reserved for testing. Consistent with Du et al. (2024), 100 QA pairs are used for validation, while the remaining samples simulate the unlabeled training dataset. We randomly sample = 32 pairs from TruthfuQA, and 64 pairs from the other datasets to construct an exemplar set, with = 128 used for all experiments. Implementation details are provided in Appendix A. Models. We evaluate our method using two families of widely adopted open-source LLMs which provide accessible internal representations: LLaMA-3.1-8b & 70b (Dubey et al., 2024), and Qwen-2.5-7b & 14b (Yang et al., 2024). By default, we used greedy sampling for the generation. Baselines. We evaluate our approach against diverse set of 11 baseline methods, including existing state-ofthe-art. The baselines are categorized as follows: (1) logit-based methodsPerplexity (Ren et al., 2022), LengthNormalized Entropy (LN-entropy) (Malinin & Gales, 2021) and Semantic Entropy (Kuhn et al., 2023); (2) consistencybased methodsLexical Similarity (Lin et al., 2024), SelfCKGPT (Manakul et al., 2023) and EigenScore (Chen et al., 2024a); (3) verbalized methodsVerbalize (Lin et al., 2022b) and Self-evaluation (Kadavath et al., 2022); and (4) internal state-based methodsContrast-Consistent Search (CCS) (Burns et al., 2023), HaloScope (Du et al., 2024), and SAPLMA (Azaria & Mitchell, 2023). To ensure fair comparison, all methods are evaluated on the same test dataset, using their default experimental configurations as specified in the respective literature. Evaluation. Following previous works (Kuhn et al., 2023; Du et al., 2024), we evaluate the performance with the area under the curve of the receiver operator characteristic (AUROC). We consider the generation truthful when the similarity score between the generation and the reference answer is larger than threshold of 0.5. Following Lin et al. (2022a), we utilize BLEURT (Sellam et al., 2020) to measure the similarity. Additionally, we show that our method is robust when evaluated using GPT-4o (Hurst et al., 2024) in Appendix C.2. 5.2. Main results In Table 1, we compare TSV with competitive hallucination detection methods from the literature. TSV demonstrates state-of-the-art performance, significantly outperforming other methods on both the LLaMA-3.1-8b and Qwen-2.5-7b models. We show that unsupervised methods often struggle with inconsistent performance across different models and data distributions as the representations in LLMs are not inherently aligned with the hallucination detection task, How to Steer LLM Latents for Hallucination Detection? Table 1. Main results. Comparison with competitive hallucination detection methods on different datasets. Single sampling indicates whether the approach requires multiple generations during inference. For our method, the mean and standard deviation are computed across three different random seeds. denotes methods trained on fully labeled datasets. All values are percentages (AUROC), and the best results are highlighted in bold. Single Sampling TruthfulQA TriviaQA Model LLaMA-3.1-8b Qwen-2.5-7b Method Perplexity LN-Entropy Semantic Entropy Lexical Similarity EigenScore SelfCKGPT Verbalize Self-evaluation CCS HaloScope SAPLMA TSV (Ours) TSV (Ours) Perplexity LN-Entropy Semantic Entropy Lexical Similarity EigenScore SelfCKGPT Verbalize Self-evaluation CCS HaloScope SAPLMA TSV (Ours) TSV (Ours) 71.4 62.5 59.4 49.1 45.3 57.0 50.4 67.8 66.4 70.6 78.2 84.20.2 85.50.1 65.1 66.7 66.1 49.0 53.7 61.7 60.0 73.7 67.9 81.3 81.7 87.30.4 88.70.1 76.3 55.8 68.7 71.0 69.1 80.2 51.1 50.9 60.1 76.2 83.7 84.00.5 87.20.2 50.2 51.1 58.7 63.1 61.3 62.3 54.3 50.9 53.0 73.4 82.0 79.80.9 84.20. SciQ 52.6 57.6 68.2 61.0 59.6 67.9 53.4 54.6 77.1 76.1 77.3 85.80.4 88.60.1 53.4 52.4 65.9 62.2 63.2 58.6 51.2 53.8 51.9 76.6 81.5 82.00.4 84.80.3 NQ Open 50.3 52.7 60.7 60.9 56.7 60.0 50.7 52.2 62.6 62.7 62.8 76.10.7 78.00.2 51.2 54.3 65.3 61.2 57.4 63.4 51.2 52.4 51.2 65.7 67.9 73.80.7 76.20.3 making them less reliable for safety-critical applications. In contrast, our method achieves robust and superior performance across both models and all four datasets. In particular, TSV outperforms HaloScope by 13.6% on TruthfulQA with LLaMA-3.1-8b. While both methods use the same validation set and unlabeled data, HaloScope relies on default LLM embeddings. By contrast, our method leverages small exemplar set and shapes the latent space to better align with the hallucination detection task, enabling significantly improved performance while remaining practical. Our method is also computationally efficient at the inference stage with complexity of O(m2), where is the number of generated tokens. In contrast, some logit and consistency-based methods require multiple sampling, resulting in higher complexity of O(Am2), where can be over 10 in practice. Qualitative results are in Appendix D, and experiments with larger models (LLaMA-3.1-70b & Qwen-2.5-14b) are provided in Appendix C.1. Comparison with fully supervised methods. We compare our approach with fully supervised method SAPLMA, which trains binary classifier using the default embeddings, fully labeled as truthful or hallucinated. As shown in Table 1, with only 32 labeled examples, TSV outperforms SAPLMA with full supervision by 6.0% on TruthfulQA, emphasizing the importance of shaping the latent space and the label-efficiency of our method. We further evaluate our method by comparing it with fully supervised upper bound (TSV). Specifically, all unlabeled data is annotated with ground-truth labels, and TSV is trained on this fully labeled dataset. We then compare our default setting (with small exemplar set) to this supervised oracle on the same test set, using the AUROC metric to measure performance. Our evaluation, based on the LLaMA-3.1-8b model, demonstrates that our method with 32 examples achieves hallucination detection AUROC of 84.2% on TruthfulQA, closely matching the performance of the fully supervised oracle (AUROC: 85.5%). These results underscore that our approach can achieve reliable hallucination detection accuracy with small labeling costs, offering an effective and efficient alternative to fully-supervised approaches. 5.3. Ablation studies How does the steering location affect performance? We investigate the impact of the location where TSV is applied on overall performance using LLaMA-3.1-8b. In Figure 3a, we present the effects of two factors on performance: (1) the index of the layer, and (2) the component of the multihead attention (MHA) architecture where TSV is applied. 6 How to Steer LLM Latents for Hallucination Detection? (a) Effect of the steering location (b) Effect of the steering strength (λ) (c) Effect of the number of exemplars (N ) Figure 3. (a) Effect of steering location (layer index and MHA components) on TruthfulQA performance, (b) effect of steering strength λ (Section 4.1), and (c) effect of the number of labeled exemplars. All results are reported as AUROC using LLaMA-3.1-8b. In particular, the MHA can be conceptually expressed as: fi+1 = fi + QiAttni(fi), (14) where fi represents the output of the i-th transformer block, Attni(fi) denotes the output of the self-attention module in the i-th block, and Qi is the weight of the feedforward layer. We train and apply TSV at three distinct locations within the MHA architecture: (1) residual stream , (2) MLP output QAttn(f ), and (3) attention output Attn(f ). We find that applying TSV in the early-middle layers (e.g., 4th10th layers) is the most effective for guiding representations in the hallucination detection task. Performance improves as TSV is applied from the top layers towards the early-middle layers but gradually declines in later layers. Moreover, the choice of location within MHA shows minimal impact on performance. Our findings suggest that tuning the layer position is likely more critical than the specific MHA location for effectively separating representations in the hallucination detection task. How does the steering strength affect the performance? To better understand the characteristics of TSV, we vary the steering strength λ {0.1, 0.5, 1, 5, 10} and analyze its effect on the models performance, as demonstrated in Figure 3b. The results show that performance improves with moderate steering strength (e.g., λ = 5), but declines as λ increases further. small λ does not provide sufficient signal to meaningfully separate representations in the final layer, while large λ disrupts the representation space by dominating it, resulting in suboptimal performance. Table 2. Camparison on pseudo-labeling accuracy (PL ACC) on selected unlabeled generations and hallucination detection performance (HD AUROC) on the test dataset. Results are reported based on LLaMA 3.1-8b. Dataset Metric 32 128 256 512 TruthfulQA TriviaQA PL ACC (%) 100 HD AUROC (%) 83.5 PL ACC (%) 100 HD AUROC (%) 78.3 98.4 84.0 91.2 82.8 95. 84.2 89.1 84.0 89.8 84.7 87. 82.2 81.8 84.2 87.0 81.0 small number of labeled exemplars, which are practical to obtain. However, when the number of labeled exemplars is too small (N = 8), the performance becomes suboptimal. Pseudo-labeling accuracy and the number of selected unlabeled data. We analyze the effect of the number of selected unlabeled samples, K, for augmenting the training data. In Table 2, we report (1) the pseudo-labeling accuracy on selected unlabeled generations (PL ACC), and (2) the overall hallucination detection performance on the test dataset (HD AUROC). Our optimal transport-based pseudolabeling achieves near-perfect accuracy up to = 64, with gradual decline as increases further. The hallucination detection performance peaks at = 128 and decreases thereafter. This trend indicates that while our learning framework is relatively robust to the number of selected samples, including too many false-positive samples can introduce noise into the learning process, potentially affecting performance. How does number of exemplars affect the performance? In Figure 3c, we examine the impact of the number of labeled exemplars on performance. We evaluate {8, 16, 32, 64} and compare them to the fully-supervised upper bound (FS), where all samples in DU are labeled with ground truth. Our results indicate that small exemplar set is effective for modeling the truthfulness distribution when = {32, 64}, achieving performance almost comparable to the fully-supervised oracle. This demonstrates that reliable hallucination detector can be designed using only Can TSV generalize across data distributions? While TSV shows superior performance, we are also interested in its capability to generalize across different data distributions. As shown in Figure 4, we evaluate the generalization capability of TSV using LLaMA-3.1-8b model by learning it from source in-distribution (ID) dataset, directly applying it to different target out-of-distribution (OOD) datasets, and computing the corresponding hallucination detection scores. The results demonstrate the robust transferability of our approach across diverse datasets, specifically achiev7 How to Steer LLM Latents for Hallucination Detection? Figure 4. Generalization results on out-of-distribution datasets. Table 3. Component analysis. TSV: Truthfulness Separator Vector, IT: Initial Training phase, and AT: Augmented Training phase. Index (a) (b) (c) Ours Component Dataset TSV IT AT TruthfulQA TriviaQA SciQ NQ Open 52.2 52.0 80. 84.2 50.8 50.2 80.8 84.0 54.1 57.1 82.0 85.8 50.8 52.1 71. 76.1 ing hallucination detection AUROC of 79.8% on TriviaQA when TSV is learned from TruthfulQA, exhibiting performance close to that obtained directly from TriviaQA (84.0%). This strong transferability highlights TSVs potential for real-world LLM applications, effectively detecting hallucinations even under domain shifts. Component analysis. In Table 3, we present ablation results for the components of our approach using LLaMA3.1-8b model. Comparing (a) and (b), which update the class prototypes without using TSV, we observe that training performance remains close to 50%, and even with the augmented training phase, performance does not improve. In contrast, comparing (a) and (c), we find that incorporating TSV improves AUROC by 28.7% on TruthfulQA. This demonstrates that shaping representations with TSV is critical for hallucination detection, as it makes the representations more separable. Further, comparing (c) with our full approach, we see that the augmented training phase enhances performance by an additional 3.3% on TruthfulQA, achieving the best performance among all configurations. Unlike (a) and (b), this highlights that the augmented training phase is effective only when supported by well-structured representations and accurate pseudo-labels, underscoring the importance of learning TSV. Overall, integrating all components achieves the best performance across all datasets, indicating that each component is effective for addressing the hallucination detection task. Computational efficiency of TSV. To evaluate the costefficiency of our method, we compare TSV with parameterefficient fine-tuning (PEFT) approaches in Table 4. Specifically, we train LoRA (Hu et al., 2022) and LoReFT (Wu et al., 2024) using our training framework, leveraging small labeled exemplar set along with the unlabeled dataset. 8 (a) Scores of HaloScope (b) Scores of Ours Figure 5. Score distributions for HaloScope vs. our method. Table 4. Performance comparison with PEFT methods. % Params is calculated by dividing the number of trainable parameters by the total number of parameters in the base LLM. Model Method Trainable Parameters Datasets # Params % Params TruthfulQA TriviaQA Llama 3.1-8b Qwen2.5-7b LoRA LoReFT Ours LoRA LoReFT Ours 3.4M 32K 4K 2.5M 28K 3.6K 0.0424 % 0.0004 % 0.00005% 0.0331 % 0.0004 % 0.00005% 83.6 77.5 84. 85.9 81.5 87.3 82.0 76.0 84.0 76.0 79.3 79.8 Our method achieves superior performance while utilizing 8 to 800 fewer parameters, demonstrating that TSV can effectively shape representations for the hallucination detection task while significantly reducing computational and annotation costs. Training time is detailed in Appendix G. Visualization of truthfulness score distributions. Figure 5 visualizes the score distributions for HaloScope (Du et al., 2024) and our method on TruthfulQA based on LLaMA-3.18b model. Our approach demonstrates more distinct separation between truthful and hallucinated data distributions. This enhanced differentiation is attributed to the effectiveness of shaping latent space with TSV, which contributes to more reliable detection performance than other methods. 6. Conclusion In this work, we tackle the challenge of hallucination detection in LLM by introducing the Truthfulness Separator Vector (TSV), lightweight and modular approach that reshapes the latent space during inference to enhance the separation between truthful and hallucinated outputs without altering the models parameters. Through two-stage training framework that combines small labeled exemplar set with unlabeled LLM generations, TSV achieves superior performance while minimizing reliance on human labeling and computational cost. Our experiments demonstrate TSVs effectiveness, achieving state-of-the-art accuracy with strong generalization across datasets. This work not only advances the state of hallucination detection but also lays the groundwork for scalable and practical solutions to improve the reliability of LLMs in real-world applications. How to Steer LLM Latents for Hallucination Detection?"
        },
        {
            "title": "Impact Statement",
            "content": "Ensuring the reliability of LLM is paramount as they are increasingly integrated into high-stakes applications like healthcare, law, and education. This work tackles the critical challenge of hallucination detection, which identifies factually inaccurate outputs for enhanced user trust. We propose practical method that minimizes computational and labeling costs while enabling plug-and-play approach for pre-trained LLMs. This research not only advances the technical landscape of hallucination detection but also lays the groundwork for scalable and reliable AI systems, fostering broader trust and adoption of LLMs in critical domains. Our study does not involve human subjects, complies with all legal and ethical standards, and we do not anticipate any potential harmful consequences resulting from our work. Code will be released for reproducibility."
        },
        {
            "title": "Acknowledgement",
            "content": "We gratefully acknowledge Maxim Khanov, Shawn Im, and Shrey Modi for their valuable comments on the draft. This work is supported by the AFOSR Young Investigator Program under award number FA9550-23-1-0184, National Science Foundation (NSF) Award No. IIS-2237037 and IIS2331669, Office of Naval Research under grant number N00014-23-1-2643, and Philanthropic Fund from SFF."
        },
        {
            "title": "References",
            "content": "Azaria, A. and Mitchell, T. The internal state of an llm knows when its lying. In EMNLP Findings, 2023. Burns, C., Ye, H., Klein, D., and Steinhardt, J. Discovering latent knowledge in language models without supervision. In ICLR, 2023. Chen, C., Liu, K., Chen, Z., Gu, Y., Wu, Y., Tao, M., Fu, Z., and Ye, J. Inside: Llms internal states retain the power of hallucination detection. In ICLR, 2024a. Chen, Z., Sun, X., Jiao, X., Lian, F., Kang, Z., Wang, D., and Xu, C. Truth forest: Toward multi-scale truthfulness in large language models through intervention without tuning. In AAAI, 2024b. Cuturi, M. Sinkhorn distances: Lightspeed computation of optimal transport. In NeurIPS, 2013. Du, X., Xiao, C., and Li, Y. Haloscope: Harnessing unlabeled llm generations for hallucination detection. In NeurIPS, 2024. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Gekhman, Z., Yona, G., Aharoni, R., Eyal, M., Feder, A., Reichart, R., and Herzig, J. Does fine-tuning llms on new knowledge encourage hallucinations? In EMNLP, 2024. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. In ICLR, 2022. Hu, X., Zhang, Y., Peng, R., Zhang, H., Wu, C., Chen, G., and Zhao, J. Embedding and gradient say wrong: white-box method for hallucination detection. In EMNLP, 2024. Huang, L., Yu, W., Ma, W., Zhong, W., Feng, Z., Wang, H., Chen, Q., Peng, W., Feng, X., Qin, B., et al. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems, 2023. Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L. TriviaQA: large scale distantly supervised challenge dataset for reading comprehension. In ACL, 2017. Kadavath, S., Conerly, T., Askell, A., Henighan, T., Drain, D., Perez, E., Schiefer, N., Hatfield-Dodds, Z., DasSarma, N., Tran-Johnson, E., et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022. Kossen, J., Han, J., Razzak, M., Schut, L., Malik, S., and Gal, Y. Semantic entropy probes: Robust and cheap hallucination detection in llms. arXiv preprint arXiv:2406.15927, 2024. Kuhn, L., Gal, Y., and Farquhar, S. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. In ICLR, 2023. Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., et al. Natural questions: benchmark for question answering research. In TACL, 2019. Li, K., Patel, O., Viegas, F., Pfister, H., and Wattenberg, M. Inference-time intervention: Eliciting truthful answers from language model. In NeurIPS, 2024. Lin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring how models mimic human falsehoods. In ACL, 2022a. Lin, S., Hilton, J., and Evans, O. Teaching models to express their uncertainty in words. In TMLR, 2022b. How to Steer LLM Latents for Hallucination Detection? Lin, Z., Trivedi, S., and Sun, J. Generating with confidence: Uncertainty quantification for black-box large language models. In TMLR, 2024. Welbl, J., Liu, N. F., and Gardner, M. Crowdsourcing multiple choice science questions. arXiv preprint arXiv:1707.06209, 2017. Wu, Z., Arora, A., Wang, Z., Geiger, A., Jurafsky, D., Manning, C. D., and Potts, C. Reft: Representation finetuning for language models. In NeurIPS, 2024. Xiong, M., Hu, Z., Lu, X., Li, Y., Fu, J., He, J., and Hooi, B. Can llms express their uncertainty? an empirical In ICLR, evaluation of confidence elicitation in llms. 2024. Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. Yin, F., Srinivasa, J., and Chang, K.-W. Characterizing truthfulness in large language model generations with local intrinsic dimension. In ICML, 2024a. Yin, F., Ye, X., and Durrett, G. Lofit: Localized fine-tuning on llm representations. In NeurIPS, 2024b. Zhang, Y., Li, Y., Cui, L., Cai, D., Liu, L., Fu, T., Huang, X., Zhao, E., Zhang, Y., Chen, Y., et al. Sirens song in the ai ocean: survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023. Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., et al. survey of large language models. arXiv preprint arXiv:2303.18223, 2023. Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging llm-as-a-judge with mt-bench and chatbot arena. In NeurIPS, 2023. Zou, A., Phan, L., Chen, S., Campbell, J., Guo, P., Ren, R., Pan, A., Yin, X., Mazeika, M., Dombrowski, A.-K., et al. Representation engineering: top-down approach to ai transparency. arXiv preprint arXiv:2310.01405, 2023. Loshchilov, I. Decoupled weight decay regularization. In ICLR, 2019. Malinin, A. and Gales, M. Uncertainty estimation in autoregressive structured prediction. In ICLR, 2021. Manakul, P., Liusie, A., and Gales, M. J. Selfcheckgpt: Zeroresource black-box hallucination detection for generative large language models. In EMNLP, 2023. Mardia, K. V. and Jupp, P. E. Directional statistics. John Wiley & Sons, 2009. Marks, S. and Tegmark, M. The geometry of truth: Emergent linear structure in large language model representations of true/false datasets. In COLM, 2024. Pal, A., Umapathi, L. K., and Sankarasubbu, M. Medhalt: Medical domain hallucination test for large language models. In CoNLL, 2023. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog, 2019. Ren, J., Luo, J., Zhao, Y., Krishna, K., Saleh, M., Lakshminarayanan, B., and Liu, P. J. Out-of-distribution detection and selective generation for conditional language models. In ICLR, 2022. Sellam, T., Das, D., and Parikh, A. P. Bleurt: Learning robust metrics for text generation. In ACL, 2020. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023. Van Buuren, S. Flexible imputation of missing data. CRC press, 2018. Van der Maaten, L. and Hinton, G. Visualizing data using t-sne. Journal of machine learning research, 2008. Villani, C. et al. Optimal transport: old and new. Springer, 2009. Wang, H., Xiao, R., Li, Y., Feng, L., Niu, G., Chen, G., and Zhao, J. Pico: Contrastive label disambiguation for partial label learning. In ICLR, 2022. How to Steer LLM Latents for Hallucination Detection?"
        },
        {
            "title": "Contents",
            "content": "A Implementation Details and Hyperparameters A.1 Implementation details (ours) . A.2 Hyperparameters . . . . . . . . . . . A.3 Implementation details (baselines) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . More Details of the Benchmarks"
        },
        {
            "title": "C Ablation Studies",
            "content": "C.1 Scalability to larger language models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Evaluation results with GPT-4o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Design choices for the class distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.4 Results with LLaMA-2-chat-7b . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Qualitative Results Embedding Norms Algorithms F.1 Overall training framework . F.2 Sinkhorn algorithm . . . . . Compute Resources and Time G.1 Software and hardware . . . G.2 Training and inference time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 11 12 12 13 13 14 14 15 15 16 16 17 17 17 A. Implementation Details and Hyperparameters A.1. Implementation details (ours) Following Kuhn et al. (2023), we generate the most likely answer using beam search with 5 beams. Class prototypes µc and TSV are randomly initialized, and trained in two stages: 20 epochs using only the exemplar set, followed by an additional 20 epochs after augmentation. Training is performed using the AdamW optimizer (Loshchilov, 2019), with learning rate of 5e-03 and batch size of 128. We set steering strength λ to 5, the concentration parameter of the vMF distribution κ to 10, and the EMA decay rate α to 0.99. The number of iterations in the Sinkhorn algorithm is 3, and the regularization parameter ϵ is set to 0.05. The hyperparameters are tuned based on testing performance on the validation set. The steering location for each model is detailed in Appendix A.2. For generating responses, we utilize the following input prompt: 11 How to Steer LLM Latents for Hallucination Detection?"
        },
        {
            "title": "Input prompt for generating responses",
            "content": "Prompt: Answer the question concisely: Q: {question} A: A.2. Hyperparameters Table 5. Steering layer index for LLaMA-3.1-8b and Qwen-2.5-7b. Model Datasets TruthfulQA TriviaQA SciQ NQ Open LLaMA-3.1-8b Qwen-2.5-7b 9 4 4 6 8 11 9 6 Table 6. Hyperparameter search space. The hyperparameters used in our method are underlined. Hyperparameters Steering MHA component Steering strength (λ) Optimizer Learning rate Batch size Initial training epochs (ninitial) Augmented training epochs (naugmented) EMA decay rate (α) Concentration parameter (κ) Search space {mlp, attn, res} {0.1, 0.5, 1, 5, 10} {SGD, Adam, AdamW} {1e-04, 2e-04, 5e-04, 1e-03, 2e-03, 5e-03, 1e-02} {32, 64, 128} {5, 10, 20, 40} {5, 10, 20, 40} {0, 0.5, 0.9, 0.95, 0.99, 1} {0.1, 1, 5, 10, 100} The steering layer index for applying TSV is provided in Table 5. We select the steering layer index based on the models performance on the validation set for each dataset, and we consistently apply TSV to the residual stream of MHA component. The search space of hyperparameters is outlined in Table 6. The training configuration is determined using the performance on the TruthfulQA dataset with LLaMA-3.1-8b and is uniformly applied across all experiments. A.3. Implementation details (baselines) For Perplexity1 (Ren et al., 2022), we evaluate the average perplexity score based on the generated tokens. For baselines requiring multiple generations (Malinin & Gales, 2021; Kuhn et al., 2023; Lin et al., 2024; Manakul et al., 2023; Chen et al., 2024a), we utilize multinomial sampling to generate 10 samples (A = 10) per question, setting the temperature to 0.5, and adhering to the default configurations outlined in the original paper. For Verbalize (Lin et al., 2022b), we implement the following prompt: Verbalized Prompt: Q: {question} A: {answer} The proposed answer is true with confidence value (0-100) of The generated confidence value is directly utilized as the uncertainty score during testing. For the Self-evaluation (Kadavath et al., 2022), we adhere to the approach outlined in the original paper and use the following prompt: 1https://huggingface.co/docs/transformers/en/perplexity 12 How to Steer LLM Latents for Hallucination Detection? Self-evaluation Prompt: Q: {question} A: {answer} Is the proposed answer: (A) True (B) False The proposed answer is: In line with the original paper, we evaluate hallucination detection performance by using the log probability of the output token as the uncertainty score. We implement SAPLMA (Azaria & Mitchell, 2023) using an MLP classifier consisting of three hidden layers with decreasing numbers of hidden units (256, 128, and 64). Each layer employs ReLU activations, consistent with the original paper. We set the LoRA2 (Hu et al., 2022) rank to 8, α to 32, and the dropout rate to 0.1. We use the AdamW optimizer with learning rate of 5e-04. For LoReFT3 (Wu et al., 2024), we set the rank to 4 and apply to the same layer as ours and all input positions to ensure consistency with ours. B. More Details of the Benchmarks We evaluate our method on four publicly available generative question-answering (QA) tasks: TruthfulQA4 (Lin et al., 2022a), TriviaQA5 (Joshi et al., 2017), SciQ6 (Welbl et al., 2017), and NQ Open7 (Kwiatkowski et al., 2019). TruthfulQA focuses on assessing models truthfulness and robustness in generating false or unsupported responses; we use its generation track with 817 QA pairs. TriviaQA includes fact-based questions from trivia websites, making it useful for testing factual accuracy; we use the deduplicated validation split of the rc.nocontext subset, comprising 9,960 QA pairs. SciQ is domain-specific dataset with science-related QA pairs, suitable for evaluating hallucinations in specialized domains, and we use its validation split with 1,000 QA pairs. NQ Open, with 3,610 QA pairs in its validation split, challenges models on open-domain reasoning and general knowledge. Together, these datasets provide comprehensive benchmark for evaluating hallucination detection across diverse tasks. C. Ablation Studies C.1. Scalability to larger language models Table 7. Hallucination detection results on larger LLMs. Qwen-2.5-14b LLaMA-3.1-70b Method Perplexity CCS HaloScope SAPLMA Ours TruthfulQA SciQ TruthfulQA SciQ 76.0 80.0 78.9 83.8 89.7 53.5 55.5 60.3 58.4 75. 64.7 69.4 74.6 83.1 83.6 52.3 63.1 67.4 70.2 76.6 We evaluate our method on larger LLMs, including the LLaMA-3.1-70b and Qwen-2.5-14b models, to illustrate its scalability. Specifically, we apply TSV to the residual stream of the 31st layer in LLaMA-3.1-70b and the 23rd layer in Qwen-2.5-14b. Results in Table 7 demonstrate that our approach consistently outperforms four strong baselines including the fully-supervised method (SAPLMA) while also improving upon the performance achieved with smaller LLMs. For instance, on the SciQ dataset, our approach achieves an AUROC of 89.7% with the Qwen-2.5-14b model, compared to 82.0% with the Qwen-2.5-7b model, reflecting performance gain of 7.7%. 2https://github.com/microsoft/LoRA 3https://github.com/stanfordnlp/pyreft 4https://huggingface.co/datasets/truthfulqa/truthful qa 5https://huggingface.co/datasets/mandarjoshi/trivia qa 6https://huggingface.co/datasets/allenai/sciq 7https://huggingface.co/datasets/google-research-datasets/nq open 13 How to Steer LLM Latents for Hallucination Detection? C.2. Evaluation results with GPT-4o Table 8. Hallucination detection results using labels generated by GPT-4o. Method Perplexity CCS HaloScope SAPLMA TSV (Ours) LLaMA-3.1-8b Qwen-2.5-7b TruthfulQA SciQ TruthfulQA SciQ 55.3 50.3 77.0 79.6 85.6 41.3 52.7 66.9 72.6 81.6 56.3 58.6 63.4 67.7 87.2 65.8 59.5 63.3 70.7 75.1 In our main paper, generation is considered truthful if its BLEURT score with the gold standard answer exceeds predefined threshold. In addition to the threshold-based evaluation using BLEURT (Sellam et al., 2020), we employ GPT-4o (Hurst et al., 2024) to determine the truthfulness of the generation, under the paradigm of LLM-as-a-judge (Zheng et al., 2023). Specifically, we assess the truthfulness of LLM-generated responses by determining their semantic equivalence to the given gold standard answers. In Table 8, we demonstrate that our method consistently outperforms competitive baselines, highlighting its robustness across different approaches for determining the truthfulness of the model generation. We utilize the following input prompt: Input prompt for GPT-4o based evaluation Prompt: You are tasked with evaluating whether generated answer is truthful compared to provided list of gold standard answers. Your response should be Yes if the generated answer is consistent with any of the gold standard answers, and No otherwise. Provide clear justification for your decision. Question: {question} Gold Standard Answers: {gold standard answers} Generated Answer: {generated answer} Response Format: - Answer: [Yes/No] - Justification: [Explain briefly why the answer is correct or incorrect.] We consider generation truthful when the answer is Yes and hallucinated when the answer is No. C.3. Design choices for the class distribution Table 9. Hallucination detection results using different w. Qwen-2.5-7b LLaMA-3.1-8b Method Uniform Estimation Oracle Ours TruthfulQA SciQ TruthfulQA SciQ 81.2 80.6 82.6 82.0 84.0 83.9 85.0 85.8 87.0 87.3 87.5 87.3 83.2 83.7 84.3 84.2 We ablate the various design choices for the class distribution of the unlabeled dataset when formulating the optimal transport problem in Equation (7). We evaluate the following configurations: (1) uniform class distribution (Uniform), (2) an estimated class distribution obtained via pseudo-labeling with nearest neighbor classification (Estimation), (3) the ground-truth class distribution of the unlabeled dataset (Oracle), and (4) the class distribution derived from the exemplar set (Ours). In Table 9, our proposed design choice achieves performance comparable to the Oracle approach. Notably, the robustness to design choices of appears to stem from our confident data selection procedure in Equation (11), which plays an important role in ensuring stable performance across different configurations. Additionally, we demonstrate that the pseudo-labeling approach is also effective, highlighting the scalability and adaptability of the algorithm. 14 How to Steer LLM Latents for Hallucination Detection? C.4. Results with LLaMA-2-chat-7b Table 10. Experiment results with LLaMA-2-chat-7b. All results are directly copied from HaloScope. Method Perplexity LN-Entropy Semantic Entropy Lexical Similarity EigenScore SelfCKGPT Verbalize Self-evaluation CCS HaloScope TSV (Ours) TruthfulQA TriviaQA 56.77 61.51 62.17 55.69 51.93 52.95 53.04 51.81 61.27 78.64 80.93 72.13 70.91 73.21 75.96 73.98 73.22 52.45 55.68 60.73 77.40 85.20 We evaluate our method using the LLaMA-2-chat-7b model (Touvron et al., 2023), following the experimental setup outlined in HaloScope (Du et al., 2024). Specifically, we apply TSV to the residual stream of the 9th layer and adopt the same training configurations as in the main experiments. Our results demonstrate that TSV is effective even when applied to legacy models such as LLaMA-2-chat-7b, showcasing the versatility and robustness of our approach. D. Qualitative Results Figure 6. Qualitative examples from (a) TruthfulQA and (b) TriviaQA. We compare the truthfulness scores S(x) across different test inputs x. green checkmark indicates ground truth labeled as truthful, while red cross denotes ground truth labeled as hallucinated. We present qualitative examples of the models truthfulness score, S(x), for various input query and generated text pairs. Using questions sampled from (a) TruthfulQA and (b) TriviaQA, we generate responses with the LLaMA-3.1-8b model. As illustrated in Figure 6, our approach accurately assigns scores that align with the truthfulness of the answers, demonstrating the effectiveness of the method. E. Embedding Norms We model the last-token embeddings at the final layer using hyperspherical distribution with unit norm. This approach aligns with the structure of embeddings commonly observed after the RMSNorm layer in practical Transformer models, where the embedding norms remain consistent while their directions vary. These characteristics can be naturally characterized by the von Mises-Fisher (vMF) distribution, which we employ to represent the probability distribution in the MLE objective in Equation (3). To validate our modeling, we visualize the L2 norms of the last-token embeddings at the final layer for the 15 How to Steer LLM Latents for Hallucination Detection? Figure 7. L2 norms of the last token embeddings at the final layer from LLaMA-3.1-8b. Figure 8. L2 norms of the last token embeddings at the final layer from Qwen-2.5-7b. pre-trained LLaMA-3.1-8b (Figure 7) and Qwen-2.5-7b (Figure 8). The visualizations show that the embedding norms are uniformly distributed around 140 for the LLaMA-3.1-8b model and around 300-330 for the Qwen-2.5-7b model, supporting the validity of our modeling approach. F. Algorithms F.1. Overall training framework Compute the loss L(DE) Update with gradient step Update class prototypes µc using EMA Algorithm 1 Overall training framework Parameters: ninitial, naugmented, l, Input: Exemplar set DE, unlabeled dataset DU Initialize TSV and class prototypes µc with random weights. Apply TSV to the intermediate layer l: h(l) h(l) + λv 1. Initial training phase 1: for = 1 to ninitial do 2: 3: 4: 5: end for 2. Augmented training phase 1: Compute pseudo-labels for DU using the Sinkhorn algorithm 2: Select confident samples DS from DU 3: Augment exemplar set: DE DE DS. 4: for = 1 to naugmented do 5: 6: 7: 8: end for Compute the loss L(DE) Update with gradient step Update class prototypes µc using EMA 16 Equation (5) Equation (6) Equations (7) to (9) Equations (10) and (11) Equation (12) Equation (5) Equation (6) How to Steer LLM Latents for Hallucination Detection? F.2. Sinkhorn algorithm Algorithm 2 Sinkhorn algorithm for entropic-regularized optimal transport Parameters: ϵ, niter Input: Unlabeled dataset DU, class distribution w, cost matrix log Initialize β 12 1: for = 1 to niter do 2: 3: 4: end for Return = diag(α)P1/ϵdiag(β) α 1 β 1M P1/ϵβ (P1/ϵ)α Equation (9) Equation (8) G. Compute Resources and Time G.1. Software and hardware We conducted all experiments using Python 3.8.15 and PyTorch 2.3.1 (Paszke et al., 2019) on NVIDIA A100 GPUs. For evaluation with GPT-4o, we utilized the OpenAI API. G.2. Training and inference time Based on tracked runs, the estimated total training and inference time is notably low: approximately 0.1 GPU-hours for LLaMA-3.1-8b and Qwen-2.5-7b, 0.2 GPU-hours for Qwen-2.5-14b, and 1 GPU-hours for LLaMA-3.1-70b. These highlight the computational efficiency of our approach, achieving practical training and inference time even for large-scale models. To further contextualize this, we compare the wall-clock time for training and inference computed on the same split of TruthfulQA with LLaMA-3.1-8b, as shown in Figure 9. We evaluate three hallucination detection methods requiring training: HaloScope (Du et al., 2024), SAPLMA (Azaria & Mitchell, 2023), and TSV (Ours); and one training-free method: Semantic Entropy (Kuhn et al., 2023). All methods are tested using the same software and hardware setup, and runtime is measured after completing the sampling process. While TSV incurs slightly higher computational costs compared to HaloScope, it achieves significant performance improvement of 13.6%. Furthermore, TSV demonstrates superior performance compared to the fully-supervised method: SAPLMA, achieving both lower computational and annotation costs. Additionally, TSV outperforms Semantic Entropy which involves computationally expensive semantic clustering across multiple samples. We also compare wall-clock time with PEFT methods: LoRA (Hu et al., 2022) and LoReFT (Wu et al., 2024); trained using our pipeline. Despite using fewer trainable parameters and lower time costs, our approach demonstrates superior performance in hallucination detection. These results demonstrate TSVs effectiveness as high-performing hallucination detection method that balances detection performance, computational efficiency, and annotation costs, offering flexibility across different cost budgets. Figure 9. AUROC and wall-clock time for training and inference."
        }
    ],
    "affiliations": [
        "Department of Computer Sciences, University of Wisconsin-Madison, USA",
        "School of Software Technology, Zhejiang University, China"
    ]
}