{
    "paper_title": "TADA! Tuning Audio Diffusion Models through Activation Steering",
    "authors": [
        "Łukasz Staniszewski",
        "Katarzyna Zaleska",
        "Mateusz Modrzejewski",
        "Kamil Deja"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Audio diffusion models can synthesize high-fidelity music from text, yet their internal mechanisms for representing high-level concepts remain poorly understood. In this work, we use activation patching to demonstrate that distinct semantic musical concepts, such as the presence of specific instruments, vocals, or genre characteristics, are controlled by a small, shared subset of attention layers in state-of-the-art audio diffusion architectures. Next, we demonstrate that applying Contrastive Activation Addition and Sparse Autoencoders in these layers enables more precise control over the generated audio, indicating a direct benefit of the specialization phenomenon. By steering activations of the identified layers, we can alter specific musical elements with high precision, such as modulating tempo or changing a track's mood."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 1 ] . [ 1 0 1 9 1 1 . 2 0 6 2 : r Preprint. Preliminary work. TADA! THROUGH ACTIVATION STEERING"
        },
        {
            "title": "TUNING AUDIO DIFFUSION MODELS",
            "content": "Łukasz Staniszewski1,2 Katarzyna Zaleska1 Mateusz Modrzejewski1 Kamil Deja1,2 1Warsaw University of Technology 2IDEAS Research Institute Code (cid:192) Audio Examples"
        },
        {
            "title": "ABSTRACT",
            "content": "Audio diffusion models can synthesize high-fidelity music from text, yet their internal mechanisms for representing high-level concepts remain poorly understood. In this work, we use activation patching to demonstrate that distinct semantic musical concepts, such as the presence of specific instruments, vocals, or genre characteristics, are controlled by small, shared subset of attention layers in state-of-the-art audio diffusion architectures. Next, we demonstrate that applying Contrastive Activation Addition and Sparse Autoencoders in these layers enables more precise control over the generated audio, indicating direct benefit of the specialization phenomenon. By steering activations of the identified layers, we can alter specific musical elements with high precision, such as modulating tempo or changing tracks mood. Figure 1: We study localized steering in Audio Diffusion Models. By localizing functional layers, we enable precise steering of generations with Contrastive Activation Addition and Sparse Autoencoders."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advancements in generative audio have led to Diffusion Models (DMs) capable of synthesizing high-fidelity music from textual descriptions (Gong et al., 2025). While impressive, significant limitation of those methods is that interaction relies solely on prompting, which acts as relatively blunt instrument. While user can ask for samba song, prompts lack the precision needed for subtle creative adjustments. For instance, it is nearly impossible to use text to express that track should have slightly slower tempo or marginally lower vocal pitch without triggering the model to regenerate completely different song. This creates significant gap for creators who need smooth, precise control over the text-to-music model that goes beyond the limitations of language. Corresponding author: luks.staniszewski@gmail.com 1 Preprint. Preliminary work. Behind this limitation lies an architectural challenge: current audio DMs operate as black boxes that entangle various musical skills and semantic attributes within complex set of millions of parameters. Because these internal mechanisms remain opaque, researchers and creators cannot easily isolate or adjust specific characteristics without affecting the entire global composition. In this work, we shed light on the inner workings of these audio diffusion models. Drawing inspiration from interpretability methods for language (Meng et al., 2022) and vision (Basu et al., 2024a; Staniszewski et al., 2025), we localize the functional components responsible for generating specific audio concepts across state-of-the-art text-to-music models. Our investigation reveals that, surprisingly, semantic music features such as the presence of male or female vocalist, instruments, genre, mood, or tempo are governed by remarkably small and specialized subsets of attention layers. As illustrated in Fig. 1, we build on this observation by adapting activation steering techniques within localized layers, offering new tool for controllable generation. We show that restricting interventions to identified functional layers yields significantly higher precision and control than baselines that either apply steering to all layers or to the non-functional set exclusively. Our experiments confirm that this targeted approach effectively modulates attributes such as tempo, mood, voice gender, or instrument presence, while preserving audio fidelity, thereby avoiding the quality degradation observed with standard steering. Furthermore, this localization enables efficient training of Sparse Autoencoders (SAEs) within influential regions, revealing highly semantic and interpretable features. This allows fine-grained control over musical attributes, surpassing the limitations of coarse text prompting. Our contributions can be summarized as follows: 1. We construct dataset of counterfactual prompt pairs spanning diverse musical concepts to assess the role of layers building modern text-to-music diffusion models. 2. We show that semantic musical attributes, such as tempo, vocals, instruments, mood, and genres, are controlled by small, shared subsets of cross-attention layers across diverse diffusion architectures. 3. We leverage localization to apply Contrastive Activation Addition and Sparse Autoencoders for targeted steering, enabling fine-grained control over musical attributes while preserving overall audio quality."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Diffusion Model Interpretability and Steering. The goal of Causal Mediation Analysis (Pearl, 2001; Meng et al., 2022) is to understand how the model output changes under interventions in its computational graph. Recently, this technique has been applied to the image domain (Basu et al., 2024b;a; Staniszewski et al., 2025; Zarei et al., 2025), uncovering the mechanistic role of DMs Attention layers. Precisely, Basu et al. (2024b) employ activation patching and Basu et al. (2024a) use prompt injection to localize layers controlling model knowledge in U-Net-based DMs. Staniszewski et al. (2025) combine these techniques to find layers controlling text generated on images with Diffusion Transformers (DiTs). Finally, Zarei et al. (2025) show that important attentions in DiTs can be efficiently traced through the magnitude of their outputs. The linear representation hypothesis (Park et al., 2024) posits that neural networks encode highlevel concepts as linear directions in the activation space. By contrasting hidden states from model runs with different prompts, an activation difference is yielded that encodes the semantic change between prompts. This approach has been widely applied to Large Language Models (LLMs), e.g., for steering model behavior (Chen et al., 2025) or reducing toxicity (Rodriguez et al., a). In text-toimage models, Rodriguez et al. (a) and Rodriguez et al. (b) steer generations by training affine maps representing directions between distributions of activations. Similarly, difference-based methods have been applied in the text encoder (Baumann et al., 2025) or Attention layers (Gaintseva et al., 2025) of the DMs to remove concepts from generations. Beyond activation contrasting, Sparse Autoencoders (SAEs) (Olshausen & Field, 1997) have recently (Huben et al., 2024; Bricken et al., 2023) been applied to LLMs to decompose activations into sparse, interpretable features by training an autoencoder with sparsity constraint. In the image domain, Surkov et al. (2025) demonstrated that SAEs can capture meaningful features in DMs, and Cywinski & Deja (2025) ablated features within the SAE latent space for concept unlearning. Yet another approach (Gandikota et al., 2024) to steering is to train low-rank adapters to represent directions as weight updates. 2 Preprint. Preliminary work. Interpretability of Audio Generation Models. Recent research has begun adapting interpretability techniques to the audio domain to understand and control generative models. Initial efforts include the Insider Whisper case study, where Sadov (2024) identified interpretable circuits in the ASR model via feature discovery techniques. Prior works have also studied text-to-music models, predominantly focusing on autoregressive models (Music LLMs). In particular, Wei et al. (2024) introduced dataset to probe whether music foundation models encode specific music-theory concepts, such as intervals and chords. Similarly, Vasquez et al. (2024) probe for the information about instruments and genres. Moving towards control, Koo et al. (2025) developed SMITIN, which uses classifier probes to steer attention heads for specific musical traits, while Facchiano et al. (2025), closest to our work, use activation patching to manipulate binary attributes. Singh et al. (2025) use Sparse Autoencoders (SAEs) in MusicGENs residual stream, demonstrating their utility for steering, while Paek et al. (2025) mapped SAE features to acoustic properties such as pitch and loudness in popular music autoencoders. Finally, in the case of Audio Diffusion Models, Yang et al. (2025) manipulate self-attention maps of the AudioLDM to edit attributes of the audio, while Lee et al. (2026) proposes method for timbre transfer. In contrast to these works, which primarily focus on autoregressive architectures or specific editing tasks, we systematically localize functional layers within audio diffusion models and demonstrate that restricting activation steering (via CAA or SAEs) to these specific bottlenecks yields significantly higher controllability and fidelity."
        },
        {
            "title": "3 BACKGROUND & METHODOLOGY",
            "content": "αt x0 + 1 αt ϵ, t)2 Audio Diffusion Models. Diffusion models (DMs, Dhariwal & Nichol (2021)) learn to reverse gradual noising process by predicting, at given timestep t, the noise ϵ (0, I) added to clean data x0, minimizing Et,x0,ϵϵ ϵθ( 2. Modern audio DMs, such as AceStep (Gong et al., 2025), operate in compressed latent space: an encoder maps waveforms to latent representations z0 = E(x0), where the diffusion process occurs, and decoder reconstructs the latents back to audio. These models employ U-Net (Ronneberger et al., 2015) or Transformer (Vaswani et al., 2017; Dosovitskiy et al., 2021) as their backbone architecture. Unlike text-to-image models processing spatial patches, audio latents are structured as sequences of temporal frames zt RF d. Thus, each token corresponds to distinct timeframe of the audio. Cross-attention role is to introduce, the text information into the hidden states RF through the prompt embedding RCdc. The output of the l-th Cross-Attention block at step is then CrossAttn(h(t) l1, c) = softmax (cid:19) (cid:18) QKT dk VWO, (1) where = h(t) weights. This output is further added to the residual stream as h(t) l1WQ, = cWK, = cWV , and {WQ, WK, WV , WO} are the learned l1 + CrossAttn(h(t) = h(t) l1, c). Activation Patching. To identify which layers control specific musical concepts, we employ activation patching (Meng et al., 2022) illustrated in Fig. 2. For concept (e.g., female vocal), we define set of counterfactual prompt pairs (Pc, Pc) where Pc contains the concept and Pc does not. First, we generate audio with the target prompt Pc, caching the cross-attention keys Kl = cWK and the values Vl = cWV at each layer l. Then, while generating audio with the source prompt Pc, we patch layer by substituting its keys and values with the cached ones from Pc run. Finally, we compare how the intervention affects similarity between the generated audio and the prompt describing the concept c. If the output audio for the patched run exhibits concept c, we identify as functional layer for c. Contrastive Activation Addition (CAA). We compute steering vectors following CASteer (Gaintseva et al., 2025) to enable fine-grained control over musical attributes. Given contrastive prompt pairs {(P (i) , (i) i=1 for concept c, we collect cross-attention outputs and compute the steering vector vCAA as: )}N vCAA = vc vc2 , where vc = 1 (cid:88) i=1 (cid:16)h(i) h(i) (cid:17) , (2) with denoting cross-attention outputs averaged across temporal frames. 3 Preprint. Preliminary work. Figure 2: Layer localization via Activation Patching. For given music concept (e.g., male voice), we perform (a) target run with prompt Pc and cache the cross-attention keys and values. In (b) source run, we generate with prompt Pc, which represents counterfactual concept (e.g., male voice) or does not contain c. We patch layer by substituting cross-attention key (K) and value (V) matrices with those cached from the Pc run. In such case, other layers receive Pc. If patching layer produces audio containing concept (d), we identify it as functional layer. Otherwise (c), the layer does not control the concept. During generation, we steer by modifying cross-attention outputs at functional layers as = , hl), where α controls steering strength, with positive values adding ReNorm(hl + α vCAA and negative values removing the concept. ReNorm is re-normalization operation, ensuring that the norm of the output activations l matches the one before intervention hl: h l2 l, hl) = hl2. ReNorm(h (3) Sparse Autoencoders (SAEs). To further discover interpretable features within cross-attention activations, we train TopK SAE (Gao et al., 2025; Bussmann et al., 2024) on the functional layer with the highest response during activation patching. The SAE with encoder weights Wenc Rmdd, decoder weights Wdec Rdmd, and bias bpre Rd maps activations Rd to sparse code Rmd (with being the expansion factor): = TopK (Wenc(h bpre)) , (4) and further reconstructs them: ˆh = Wdecf + bpre. (5) The TopK() operation retains only the largest activations in the autoencoders latent space, zeroing out the rest. The SAE is trained to minimize reconstruction error ˆh2 2. To identify concept-specific features, we use two contrastive sets of prompts (Pc, Pc) for concept to compute importance scores using TF-IDF-based criterion: (cid:18) (cid:19) score(j, c) = µj(Pc) (cid:124) (cid:123)(cid:122) (cid:125) TF log 1 + (cid:124) 1 µj(Pc) + ϵ (cid:123)(cid:122) IDF , (cid:125) (6) (cid:80) where µj(P) = 1 fj(h) is the mean activation of feature in the SAEs latent space on generated audio with prompts P. Features that activate highly for concept but rarely for other samples receive high scores. We select the top-τc scoring features Fc and, by summing their corresponding decoder columns, we construct the steering vector vSAE hP as vSAE = (cid:88) jFc Wdec[:, j], and add it directly to the output of the cross-attention layer: = hl + α vSAE . 4 (7) (8) Preprint. Preliminary work."
        },
        {
            "title": "4.1 LAYER LOCALIZATION",
            "content": "To measure the importance of individual cross-attention layers in text-to-audio models, we construct dataset of counterfactual prompt pairs. We consider the following musical concepts: vocal gender (female vs. male), tempo (slow vs. fast), mood (happy vs. sad), and categories such as instruments (drums, flute, guitar, maracas, trumpet, violin), and genres (jazz, techno, reggae). For each contrasting pair (c, c), we select captions from the MusicCaps (Agostinelli et al., 2023) dataset that contain keywords associated with concept (e.g., female voice) but do not contain any terms related to the alternative variant (e.g., male voice). For genres and instruments, we replace concepts (e.g., violin) with alternatives (e.g., trumpet). We select up to 256 such prompts as target ones Pc and use GPT-4 (Achiam et al., 2023) to generate alternatives Pc by replacing concept-associated terms with their counterparts while preserving all other content. Examples of prompt pairs and concept replacements are provided in the App. A. We apply the activation patching procedure described in Section 3 to three state-of-the-art audio diffusion models: AudioLDM2 (Liu et al., 2024), Stable Audio Open (Evans et al., 2025), and AceStep (Gong et al., 2025). We generate waveforms of 10 seconds (AudioLDM2, Stable Audio Open) and 30 seconds (Ace-Step), using 8 different random seeds per prompt, resulting in 2048 generations per concept. The impact of layer on concept is calculated as: Impact(l, c) = sim(l c, c) sim(l c, c) sim(l c, c) sim(l c, c) , (9) where sim(l c1, c2) denotes the audio-text similarity between the concept name and generation where layer inputs prompt c1 while all other layers = {l} receive c2. We use MuQ (Zhu et al., 2025) for assessing mood, tempo, instruments, and genres, and CLAP (Wu et al., 2022) for distinguishing vocal gender. Fig. 3 presents the layer-wise impact scores for each model. Across all three architectures, we observe that small subset of layers concentrates the control over musical concepts. In AudioLDM2, with the U-Net architecture, we localize key components in the decoder, specifically the layers {44,45,50,51} (4 out of 64 cross-attentions), with slight contribution from the layers in-between (4649). In transformer-based architectures, we observe an intense concentration of control in the middle layers (2 out of 24). Namely, in Ace-Step, cross-attentions {6,7} exhibit strong influence across all concept categories, suggesting their role as semantic bottleneck. In Stable Audio Open, similar concentration arises in layers {11,12}. These findings suggest that audio diffusion models develop interpretable, functionally specialized layers that are shared across various audio concepts. Additionally, our experiments indicate that such specialization phenomenon is not limited to single model, but is general property of text-to-music DMs. 4.2 STEERING AUDIO DIFFUSION MODELS Given strong specialization of cross-attention layers, we further evaluate their usefulness on downstream task of concept steering with audio DMs. The goal is to modulate the generated audio to increase the likelihood of including the concept according to the steering strength α, while preserving other audio characteristics untouched. Evaluation Metrics. We evaluate steering across four dimensions, over range of steering strengths α {αmin, . . . , αmax}. Preservation measures how well the original audio characteristics are maintained with steering using LPAPS (Iashin & Rahtu, 2021) and FAD (Kilgour et al., 2019) computed against the unsteered baseline (α = 0), averaging metrics over all steering strengths α. Alignment quantifies steering effectiveness as the difference in audio-text similarity between the maximum and minimum steering strengths, measured using CLAP (Wu et al., 2022) and MuQ (Zhu et al., 2025) models; higher values indicate higher degree of concept manipulation, and thus higher steering expressiveness. Smoothness captures the consistency of transitions, computed as the standard deviation of consecutive alignment differences across α, where lower values indicate smoother interpolation. Finally, Audio Quality is assessed using Audiobox Aesthetics (Tjandra et al., 2025), scoring average Content Enjoyment (CE), Content Usefulness (CU), Production Complexity (PC), and Production Quality (PQ). Preprint. Preliminary work. Figure 3: Functional cross-attention layers in AudioLDM2 (Liu et al., 2024), Stable Audio Open (Evans et al., 2025), and ACE-Step (Gong et al., 2025) models. We demonstrate that singular layers control different musical concepts, including vocal gender, tempo, mood, instruments, and genres across diverse audio diffusion architectures. Details. We evaluate steering methods with Ace-Step, generating 30-second audio clips with 60 diffusion steps. For Contrastive Activation Addition (CAA), we compute steering vectors leveraging contrastive prompt pairs (see App. for examples) and apply them with uniform strengths α {100, 90, . . . , 90, 100}. Evaluation is performed on 100 diverse prompts spanning diverse music styles and concepts, allowing us to assess the steering effect on wide range of generations. 6 Preprint. Preliminary work. We train SAE on layer {7} activations from generations from the MusicCaps (Agostinelli et al., 2023) prompts. We conduct hyperparameter search over expansion factors (m {2, 4, 8, 16}) and sparsity parameters (k {16, 32, 64}). We select the best configuration based on the lowest reconstruction error and fraction of both dead and high-frequency features, which is SAE (m = 4, = 64) trained for 15 epochs. Concept-specific features are selected based on generations from contrastive prompt pairs (same as in the case of CAA), using TF-IDF scoring (Eq. (6)) with the following top-τ values: τ = 20 for piano, τ = 20 for vocal gender, τ = 20 for tempo, and τ = 40 for mood. Results. Table 1 presents the qualitative results of our steering experiments on the Ace-Step model, comparing our targeted intervention strategy against global baselines. We compare steering exclusively the identified functional layers ({6, 7}) against steering all layers (L) and, crucially, an ablation setting where we steer all layers except the functional ones (L {6, 7}). The results provide striking validation of our localization hypothesis. We observe semantic bottleneck, consisting of controlling layers, within which the models steering capacity for high-level concepts is almost entirely concentrated. As shown in Table 1, steering these two layers alone ({6, 7}) yields high alignment scores across all concepts. Conversely, when we apply steering to the remaining 22 layers while leaving the functional layers untouched (L {6, 7}), the ability to control the generation collapses. Beyond successful alignment, targeted steering demonstrates superior preservation of the original audios characteristics. By limiting the intervention to the semantic bottleneck, we minimize collateral damage to unrelated acoustic features. This is evidenced by the Preservation metrics (LPAPS and FAD), where our steering consistently maintains lower distances to the original audio compared to using the vast majority of non-functional layers. Furthermore, while global steering (L) often degrades the overall fidelity of the output, our targeted approach maintains Audio Quality scores (CE, CU, PC, PQ) closer to the original, unsteered generations, showing less degradation or even improvement in production value or listening experience. Finally, the results using Sparse Autoencoders (SAE({7})) further corroborate these findings. By steering along specific feature directions within single functional layer (Layer 7), we achieve alignment scores competitive with, and occasionally surpassing, the raw activation steering of combined layers, offering the highest degree of interpretability. This is noteworthy case, given the recent studies by Kantamneni et al. (2025) questioning the practical utility of SAEs compared to standard baselines in LLMs. Concept Layers (#) No steering (α = 0) Piano Vocal Gender Tempo Mood {6, 7} {6, 7} SAE({7}) {6, 7} {6, 7} SAE({7}) {6, 7} {6, 7} SAE({7}) {6, 7} {6, 7} SAE({7}) 24 22 2 1 24 22 2 1 24 22 2 1 24 22 2 1 Preservation () Alignment () LPAPS CLAP MuQ - 2.707 2.524 2.188 1.973 3.334 2.983 2.532 2.600 3.065 2.613 2.633 2.390 3.067 2.710 2.433 2.359 - - 0.106 0.174 0.011 0.016 0.190 0.100 0.191 0.112 0.110 0.244 0.051 0.145 0.078 0.174 0.096 0.154 0.195 0.164 0.056 0.036 0.158 0.143 0.096 0.119 0.080 0.162 0.020 0.046 0.078 0.136 0.035 0.132 FAD - 0.486 0.467 0.276 0.262 1.148 1.406 0.309 0.546 0.858 0.426 0.533 0.494 1.155 0.683 0.406 0.459 Smoothness () CLAP MuQ - 0.004 0.004 0.004 0.002 0.010 0.006 0.006 0.002 0.005 0.003 0.003 0.004 0.006 0.004 0.004 0.007 - 0.004 0.002 0.005 0.002 0.008 0.003 0.003 0.001 0.004 0.002 0.003 0.003 0.002 0.002 0.003 0.002 Audio Quality CE CU PC PQ 7.40 6.79 7.29 6.61 7.28 6.59 7.30 6.64 7.40 6.79 7.13 6.58 7.08 6.50 7.31 6.67 7.46 6.72 7.17 6.52 7.25 6.59 7.23 6.59 7.35 6.76 7.08 6.47 7.11 6.52 7.30 6.64 7.33 6.67 5.51 5.44 5.45 5.47 5.22 5.87 5.88 5.87 5.25 5.44 5.47 5.48 5.23 5.42 5.43 5.49 5.20 7.62 7.50 7.47 7.52 7.63 7.06 6.99 7.17 7.27 7.41 7.46 7.46 7.60 7.34 7.37 7.51 7.56 Table 1: Steering Ace-Step with CAA and SAEs. denotes steering all cross-attention layers, {6, 7} excludes layers 6 and 7, and SAE({7}) uses the SAE trained on layer 7 activations. 7 Preprint. Preliminary work."
        },
        {
            "title": "5 CONCLUSIONS",
            "content": "In this work, we demonstrated that distinct semantic musical concepts, such as instrument presence, vocals, and genre, are controlled by small, shared subset of attention layers in audio diffusion architectures. By identifying these functional regions through activation patching, we showed that targeted interventions using Contrastive Activation Addition and Sparse Autoencoders enable precise manipulation of attributes such as tempo, mood, vocal gender, and piano presence without degrading audio quality. Our results confirm that this layer-specific steering outperforms global baselines in both precision and fidelity, offering robust method for fine-grained musical control that overcomes the limitations of text prompting."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Andrea Agostinelli, Timo I. Denk, Zalan Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, Matt Sharifi, Neil Zeghidour, and Christian Frank. Musiclm: Generating music from text. arXiv preprint arXiv: 2301.11325, 2023. Samyadeep Basu, Keivan Rezaei, Priyatham Kattakinda, Vlad Morariu, Nanxuan Zhao, Ryan Rossi, Varun Manjunatha, and Soheil Feizi. On mechanistic knowledge localization in text-toimage generative models. In Forty-first International Conference on Machine Learning, 2024a. Samyadeep Basu, Nanxuan Zhao, Vlad I. Morariu, Soheil Feizi, and Varun Manjunatha. LocalIn The Twelfth International izing and editing knowledge in text-to-image generative models. Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024b. URL https://openreview.net/forum?id=Qmw9ne6SOQ. Stefan Andreas Baumann, Felix Krause, Michael Neumayr, Nick Stracke, Melvin Sevi, Vincent Tao Hu, and Bjorn Ommer. Continuous, subject-specific attribute control in t2i models by identifying semantic directions. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1323113241, 2025. Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Brayden McLean, Josiah Burke, Tristan Hume, Shan Carter, Tom Henighan, and Christopher Olah. Towards monosemanticity: Decomposing language models with dictionary learning. https://transformercircuits.pub/2023/monosemantic-features/index.html. Transformer Circuits Thread, 2023. Bart Bussmann, Patrick Leask, and Neel Nanda. Batchtopk sparse autoencoders, 2024. URL https://arxiv.org/abs/2412.06410. Runjin Chen, Andy Arditi, Henry Sleight, Owain Evans, and Jack Lindsey. Persona vectors: Monitoring and controlling character traits in language models. arXiv preprint arXiv:2507.21509, 2025. Bartosz Cywinski and Kamil Deja. SAeuron: Interpretable concept unlearning in diffusion models with sparse autoencoders. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/forum?id=6N0GxaKdX9. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL https: //openreview.net/forum?id=YicbFdNTTy. 8 Preprint. Preliminary work. Zach Evans, Julian D. Parker, CJ Carr, Zack Zukowski, Josiah Taylor, and Jordi Pons. Stable audio open. In ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 15, 2025. doi: 10.1109/ICASSP49660.2025.10888461. Simone Facchiano, Giorgio Strano, Donato Crisostomi, Irene Tallini, Tommaso Mencattini, Fabio Galasso, and Emanuele Rodol`a. Activation patching for interpretable steering in music generation. arXiv preprint arXiv:2504.04479, 2025. Tatiana Gaintseva, Andreea-Maria Oncescu, Chengcheng Ma, Ziquan Liu, Martin Benning, Gregory Slabaugh, Jiankang Deng, and Ismail Elezi. Casteer: Steering diffusion models for controllable generation. arXiv preprint arXiv:2503.09630, 2025. Rohit Gandikota, Joanna Materzynska, Tingrui Zhou, Antonio Torralba, and David Bau. Concept sliders: Lora adaptors for precise control in diffusion models. In European Conference on Computer Vision, pp. 172188. Springer, 2024. Leo Gao, Tom Dupre la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, and Jeffrey Wu. Scaling and evaluating sparse autoencoders. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/ forum?id=tcsZt9ZNKD. Junmin Gong, Sean Zhao, Sen Wang, Shengyuan Xu, and Joe Guo. Ace-step: step towards music generation foundation model. arXiv preprint arXiv: 2506.00045, 2025. Robert Huben, Hoagy Cunningham, Logan Riggs Smith, Aidan Ewart, and Lee Sharkey. Sparse autoencoders find highly interpretable features in language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=F76bwRSLeK. Vladimir E. Iashin and Esa Rahtu. Taming visually guided sound generation. British Machine Vision Conference, 2021. doi: 10.5244/c.35.336. Subhash Kantamneni, Joshua Engels, Senthooran Rajamanoharan, Max Tegmark, and Neel Nanda. Are sparse autoencoders useful? case study in sparse probing. arXiv preprint arXiv:2502.16681, 2025. Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi. Frechet audio distance: reference-free metric for evaluating music enhancement algorithms. In Proc. Interspeech 2019, pp. 23502354, 2019. Junghyun Koo, Gordon Wichern, Francois Germain, Sameer Khurana, and Jonathan Le Roux. IEEE Smitin: Self-monitored inference-time intervention for generative music transformers. Open Journal of Signal Processing, 2025. Ching Ho Lee, Javier Nistal, Stefan Lattner, Marco Pasini, and George Fazekas. Diffusion timbre transfer via mutual information guided inpainting. arXiv preprint arXiv:2601.01294, 2026. Haohe Liu, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Qiao Tian, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark Plumbley. Audioldm 2: Learning holistic audio generation with self-supervised pretraining. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024. Kevin Meng, David Bau, Alex Andonian, editing factual associations in gpt. D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural ing Systems, volume 35, pp. 1735917372. Curran Associates, https://proceedings.neurips.cc/paper_files/paper/2022/file/ 6f1d43d5a82a37e89b0665b33bf3a182-Paper-Conference.pdf. Locating and In S. Koyejo, S. Mohamed, A. Agarwal, Information ProcessURL Inc., 2022. and Yonatan Belinkov. Bruno Olshausen and David Field. Sparse coding with an overcomplete basis set: strategy employed by v1? Vision research, 37(23):33113325, 1997. Nathan Paek, Yongyi Zang, Qihui Yang, and Randal Leistikow. Learning interpretable features in audio latent spaces via sparse autoencoders. arXiv preprint arXiv:2510.23802, 2025. Preprint. Preliminary work. Kiho Park, Yo Joong Choe, and Victor Veitch. The linear representation hypothesis and the geometry of large language models. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=UGpGkLzwpP. Judea Pearl. Direct and indirect effects. In Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence, UAI01, pp. 411420, San Francisco, CA, USA, 2001. Morgan Kaufmann Publishers Inc. ISBN 1558608001. Pau Rodriguez, Arno Blaas, Michal Klein, Luca Zappella, Nicholas Apostoloff, Xavier Suau, et al. Controlling language and diffusion models by transporting activations. In The Thirteenth International Conference on Learning Representations, a. Pau Rodriguez, Michal Klein, Eleonora Gualdoni, Valentino Maiorca, Arno Blaas, Luca Zappella, Xavier Suau, et al. Lineas: End-to-end learning of activation steering with distributional loss. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, b. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention (MICCAI), pp. 234241, 2015. Konstantine Sadov. Feature discovery in audio models whisper case study, 2024. URL https: //builders.mozilla.org/insider-whisper/. Nikhil Singh, Manuel Cherep, and Pattie Maes. Discovering interpretable concepts in large generative music models. arXiv preprint arXiv:2505.18186, 2025. Łukasz Staniszewski, Bartosz Cywinski, Franziska Boenisch, Kamil Deja, and Adam Dziedzic. Precise parameter localization for textual generation in diffusion models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview. net/forum?id=gdHtZlaaSo. Viacheslav Surkov, Chris Wendler, Antonio Mari, Mikhail Terekhov, Justin Deschenaux, Robert West, Caglar Gulcehre, and David Bau. One-step is enough: Sparse autoencoders for text-toimage diffusion models. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https://openreview.net/forum?id=MBJJ9Wcpg9. Andros Tjandra, Yi-Chiao Wu, Baishan Guo, John Hoffman, Brian Ellis, Apoorv Vyas, Bowen Shi, Sanyuan Chen, Matt Le, Nick Zacharov, Carleigh Wood, Ann Lee, and Wei-Ning Hsu. Meta audiobox aesthetics: Unified automatic quality assessment for speech, music, and sound. 2025. URL https://arxiv.org/abs/2502.05139. Marcel Velez Vasquez, Charlotte Pouw, John Ashley Burgoyne, and Willem Zuidema. Exploring the inner mechanisms of large generative music models. ISMIR, 2024. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Megan Wei, Michael Freeman, Chris Donahue, and Chen Sun. Do music generation models encode International Society for Music Information Retrieval Conference, 2024. doi: music theory? 10.48550/arXiv.2410.00872. Yusong Wu, K. Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and S. Dubnov. Largescale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation. IEEE International Conference on Acoustics, Speech, and Signal Processing, 2022. doi: 10.1109/ICASSP49357.2023.10095969. Yi Yang, Haowen Li, Tianxiang Li, Boyu Cao, Xiaohan Zhang, Liqun Chen, and Qi Liu. Melodia: Training-free music editing guided by attention probing in diffusion models. arXiv preprint arXiv:2511.08252, 2025. 10 Preprint. Preliminary work. Arman Zarei, Samyadeep Basu, Keivan Rezaei, Zihao Lin, Sayan Nag, and Soheil Feizi. Localizing knowledge in diffusion transformers. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https://openreview.net/forum?id= SiBVbL7rsX. Haina Zhu, Yizhi Zhou, Hangting Chen, Jianwei Yu, Ziyang Ma, Rongzhi Gu, Yi Luo, Wei Tan, and Xie Chen. Muq: Self-supervised music representation learning with mel residual vector quantization. arXiv preprint arXiv:2501.01108, 2025. 11 Preprint. Preliminary work."
        },
        {
            "title": "CONTENTS",
            "content": "A Tracing Dataset Steering Experiment Details B.1 Contrastive Prompts for Steering Vectors and SAE Features . . . . . . . . . . . . . B.2 Evaluation Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Audio-Text Alignment Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 13"
        },
        {
            "title": "A TRACING DATASET",
            "content": "Table 2 presents examples of counterfactual prompt pairs used in our localization experiments. Each pair consists of an original prompt Pc from the MusicCaps dataset and modified prompt Pc where concept-associated terms are replaced with their counterparts. Table 3 lists the keywords used to filter MusicCaps captions for each concept and the corresponding replacement terms used to generate counterfactual prompts. Concept Vocal Gender (female male) Original Prompt Pc Counterfactual Prompt Pc The low quality recording features ballad song that contains sustained strings, mellow piano melody, and soft female vocal singing over it. It sounds sad and soulful. The low quality recording features ballad song that contains sustained strings, mellow piano melody, and soft male vocal singing over it. It sounds sad and soulful. Tempo (slow fast) This is country music piece. There is fiddle playing the main melody. The acoustic guitar and electric guitar are playing gently. The song has slow tempo. The atmosphere is sentimental. This is country music piece. There is fiddle playing the main melody. The acoustic guitar and electric guitar are playing gently. The song has fast tempo. The atmosphere is sentimental. Mood (happy sad) This is Hindustani classical music piece. There is harmonium playing the main tune. bansuri joins in to play, supporting melody. The rhythmic background consists of tabla percussion and electronic drums. The atmosphere is joyful. This is Hindustani classical music piece. There is harmonium playing the main tune. bansuri joins in to play, supporting melody. The rhythmic background consists of tabla percussion and electronic drums. The atmosphere is sorrowful. Instrument (violin trumpet) This folk song features male voice singing the main melody in an emotional mood. This is accompanied by an accordion playing fills in the background. violin plays droning melody. This folk song features male voice singing the main melody in an emotional mood. This is accompanied by an accordion playing fills in the background. trumpet plays droning melody. Genre (reggae metal) The low quality recording features reggae/dub song that consists of flat male vocal singing over punchy 808 bass, punchy snare, shimmering hi hats and groovy piano chords. It sounds energetic, groovy and the recording is noisy and in mono. The low quality recording features metal/dub song that consists of flat male vocal singing over punchy 808 bass, punchy snare, shimmering hi hats and groovy piano chords. It sounds energetic, groovy and the recording is noisy and in mono. Table 2: Examples of counterfactual prompt pairs. For each concept category, we show an original prompt from MusicCaps and its counterfactual version with the target concept replaced. Modified terms are highlighted in bold. 12 Preprint. Preliminary work. Category Concept Filter Keywords Example Replacements Vocal Tempo Mood female male slow fast happy sad drums flute Instrument maracas female, woman, girlish male, man slow, slowed, slower, slowly, slowpaced, leisurely fast, rapid, dynamic, quick, energetic, groovy, lively happy, cheerful joyful, upbeat, uplifting, sad, depressing, dejected drums, drum, percussion flute, flutes maracas, maraca trumpet trumpet, trumpets violin jazz reggae Genre violin, fiddle jazz, blues reggae, jamaican, tropical, beach female male woman man girlish boyish man woman his her himself herself slow fast slower faster slow-paced fast-paced fast slow quickly slowly lively slowly joyful sorrowful cheerful gloomy warm cold sad happy lonely connected emotional lighthearted drums saxophone percussionist saxophonist flute maracas flutist maracast maracas flute maraca flute trumpet violin trumpeter violinist violin trumpet string jazz opera band choir reggae metal chill heavy beach concert Table 3: Keywords for dataset construction. For each concept, we list the keywords used to filter captions from MusicCaps (selecting prompts containing these terms) and the replacement keywords used to generate counterfactual prompts."
        },
        {
            "title": "B STEERING EXPERIMENT DETAILS",
            "content": "This section provides details on the steering experiments. B.1 CONTRASTIVE PROMPTS FOR STEERING VECTORS AND SAE FEATURES To compute steering vectors and identify concept-specific SAE features, we generate audio from contrastive prompt pairs. For each concept, we construct positive prompts Pc containing the target attribute and negative prompts Pc with the contrasting attribute. Table 4 shows the prompt templates used for each concept. The base prompts span 50 diverse musical styles and genres, including: song, melody, music, tune, track, instrumental music, pop song, rock song, jazz piece, classical piece, electronic music, acoustic music, orchestral music, hip hop music, country music, blues music, folk music, reggae music, ambient music, lofi music, ballad, love song, energetic music, calm music, dramatic music, among others. 13 Preprint. Preliminary work. Concept Positive Prompt Pc Negative Prompt Pc Piano Mood Tempo {base} with piano happy {base} fast {base} {base} sad {base} slow {base} Vocal Gender {base} with female vocals {base} with male vocals Table 4: Contrastive prompt templates for steering. The {base} placeholder is filled with diverse musical descriptions (e.g., song, jazz piece, electronic music). B.2 EVALUATION PROMPTS For steering evaluation, we use 100 diverse prompts that cover wide range of musical styles, for example: Pop & Rock: Upbeat indie pop track with jangly guitars and handclaps, 90s grunge with fuzzy guitars, angst-filled dynamics Electronic: Dark synthwave anthem with pulsing bass, retro analog synths, Techno industrial with distorted kicks, metallic textures Jazz & Blues: Melancholic jazz ballad with smooth saxophone, walking bassline, Delta blues with slide guitar, stomping rhythm World Music: Traditional Irish jig with fiddle, tin whistle, bodhran drums, Afrobeat groove with polyrhythmic percussion, brass stabs Classical: Romantic piano nocturne with expressive dynamics, Epic orchestral score with full brass, timpani rolls Hip Hop & R&B: Boom bap with punchy drums, scratched samples, Neo-soul with warm keys, silky bassline Country & Folk: Acoustic folk song with fingerpicked guitar, gentle harmonies, Bluegrass breakdown with banjo rolls, fiddle solo B.3 AUDIO-TEXT ALIGNMENT PROMPTS To measure concept alignment ( Alignment metric in Table 1), we compute audio-text similarity between steered generations and concept-specific text queries using CLAP (Wu et al., 2022) and MuQ (Zhu et al., 2025) models. Concept Piano Mood (sad happy) Tempo (slow fast) Vocal Gender (male female) Text Query piano song cheerful track fast track This is music of female vocal singing Alignment Model MuQ MuQ MuQ CLAP Table 5: Text queries for audio-text alignment measurement. These prompts are used to compute similarity scores between generated audio and target concepts."
        }
    ],
    "affiliations": [
        "IDEAS Research Institute",
        "Warsaw University of Technology"
    ]
}