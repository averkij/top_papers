{
    "paper_title": "Exploring the Abilities of Large Language Models to Solve Proportional Analogies via Knowledge-Enhanced Prompting",
    "authors": [
        "Thilini Wijesiriwardene",
        "Ruwan Wickramarachchi",
        "Sreeram Vennam",
        "Vinija Jain",
        "Aman Chadha",
        "Amitava Das",
        "Ponnurangam Kumaraguru",
        "Amit Sheth"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Making analogies is fundamental to cognition. Proportional analogies, which consist of four terms, are often used to assess linguistic and cognitive abilities. For instance, completing analogies like \"Oxygen is to Gas as <blank> is to <blank>\" requires identifying the semantic relationship (e.g., \"type of\") between the first pair of terms (\"Oxygen\" and \"Gas\") and finding a second pair that shares the same relationship (e.g., \"Aluminum\" and \"Metal\"). In this work, we introduce a 15K Multiple-Choice Question Answering (MCQA) dataset for proportional analogy completion and evaluate the performance of contemporary Large Language Models (LLMs) in various knowledge-enhanced prompt settings. Specifically, we augment prompts with three types of knowledge: exemplar, structured, and targeted. Our results show that despite extensive training data, solving proportional analogies remains challenging for current LLMs, with the best model achieving an accuracy of 55%. Notably, we find that providing targeted knowledge can better assist models in completing proportional analogies compared to providing exemplars or collections of structured knowledge."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 ] . [ 1 9 6 8 0 0 . 2 1 4 2 : r Exploring the Abilities of Large Language Models to Solve Proportional Analogies via Knowledge-Enhanced Prompting Thilini Wijesiriwardene1, Ruwan Wickramarachchi1, Sreeram Vennam2, Vinija Jain4,5*, Aman Chadha3,5, Amitava Das1, Ponnurangam Kumaraguru2, Amit Sheth 1, 1AI Institute, University of South Carolina, USA, 2IIIT Hyderabad, India 3Amazon GenAI, USA, 4Meta, USA 5Stanford University, USA Correspondence: thilini@sc.edu"
        },
        {
            "title": "Abstract",
            "content": "Making analogies is fundamental to cognition. Proportional analogies, which consist of four terms, are often used to assess linguistic and cognitive abilities. For instance, completing analogies like Oxygen is to Gas as <blank> is to <blank> requires identifying the semantic relationship (e.g., type of) between the first pair of terms (Oxygen and Gas) and finding second pair that shares the same relationship (e.g., Aluminum and Metal). In this work, we introduce 15K Multiple-Choice Question Answering (MCQA) dataset for proportional analogy completion and evaluate the performance of contemporary Large Language Models (LLMs) in various knowledge-enhanced prompt settings. Specifically, we augment prompts with three types of knowledge: exemplar, structured, and targeted. Our results show that despite extensive training data, solving proportional analogies remains challenging for current LLMs, with the best model achieving an accuracy of 55%. Notably, we find that providing targeted knowledge can better assist models in completing proportional analogies compared to providing exemplars or collections of structured knowledge."
        },
        {
            "title": "Introduction",
            "content": "The ability to form analogies enables humans to transfer knowledge from one domain to another, making it core component of human cognition (Hofstadter, 2001; Holyoak et al., 2001; Minsky, 1988). Specifically, in analogy-making, the emphasis is on the relations among objects, as it is the system of relations that is compared across domains rather than the specific objects and their attributes (Gentner, 1983). Researchers have identified several types of analogies within the domain of NLP, such as proportional analogies (analogies among word/term pairs) (Brown, 1989; Chen et al., * Work does not relate to position at Meta. Work does not relate to position at Amazon. 1 2022; Ushio et al., 2021; Szymanski, 2017; Drozd et al., 2016), sentence-analogies (Jiayang et al., 2023; Afantenos et al., 2021; Zhu and de Melo, 2020; Wang and Lepage, 2020) and analogies of longer text (Sultan and Shahaf, 2022; Sultan et al., 2024). Proportional analogies, which is the focus of this paper, are presented in the form A:B::C:D, meaning is to as is to D. These analogies involve four terms, where the relationship between the first pair of terms (A and B) is similar to the relationship between the second pair of terms (C and D). Generative Artificial Intelligence (GenAI) models, particularly those recognized for their capacity to generate high-quality textual outputs1, have emerged as focal point of research in contemporary Natural Language Processing. The capabilities of these models are typically evaluated through range of tasks, including question answering (Arora et al., 2022; Kasai et al., 2023), reasoning (Zhang et al., 2024), paraphrasing (Witteveen and Andrews, 2019), sentiment analysis (Kheiri and Karimi, 2023) and, more recently, analogical reasoning (Bhavya et al., 2024; Wijesiriwardene et al., 2023). Notably, Wijesiriwardene et al. (2023) have demonstrated that SAT-style2 Proportional analogies pose significant challenges for LLMs, particularly when solved using intrinsic distance-based similarity measures. Conversely, Webb et al. (2023) have shown that GPT-3 can surpass human performance in solving proportional analogies, though these findings were based on dataset with limited size (774 data points) and narrow range of distinct semantic relations among term pairs (seven semantic relation types). Motivated by the need 1In this work, Generative AI models refer to Large Language Models (LLMs) capable of producing high-quality textual content. Therefore, we use the term GenAI Models and LLMs interchangeably. 2SAT is US college admission test where proportional analogies were used to assess linguistic and cognitive abilities of examinees. Figure 1: Knowledge-enhanced Prompting. An illustration of our knowledge-enhanced prompting approach with types of knowledge and prompting techniques. The question consists of two terms (Oxygen and Gas), and answer choices consist of term pairs that are analogous to the question term pair. Each model is queried using the prompting techniques illustrated. to broaden the scope of research, we scale up the evaluation by assessing diverse set of GenAI models on larger, more comprehensive proportional analogy dataset. Additionally, we employ various prompting techniques enhanced with multiple types of knowledge to understand model capabilities in completing proportional analogies. Our primary contribution lies in conducting comprehensive evaluation of nine GenAI models, specifically assessing their performance in solving proportional analogies presented in multiplechoice format. Considering the limitations of existing proportional analogy datasets, which typically comprise fewer than thousand data points and restricted range of relation types, we present substantially larger dataset. Our dataset contains 15K proportional analogies with 238 distinct relation types. We evaluate the nine GenAI models on the 15K dataset using four distinct prompting techniques: (i) Zero-shot Prompting, where no additional knowledge is incorporated into the prompt, (ii) Few-shot Prompting, where exemplar knowledge in the form of examples from the dataset is included in the prompt, (iii) Structured Knowledge Prompting (SKP), where the prompt is augmented with structured knowledge in the forms of lexical, commonsense, and world knowledge drawn from WordNet (McCrae et al., 2019), ConceptNet (Speer et al., 2017), and Wikidata (Vrandeˇcic and Krötzsch, 2014) respectively and (iv) Targeted Knowledge Prompting (TKP), which integrates targeted knowledge in the form of specific semantic relationships necessary for solving proportional analogies, along with the cognitive process behind such reasoning. To the best of our knowledge, this study is the first to explore knowledge-enhanced prompting strategies for solving proportional analogies. Our findings indicate that completing proportional analogies is highly challenging for current LLMs and incorporating targeted knowledge significantly enhances model performance, with the best-performing model showing an improvement of approximately +21% compared to prompts without any knowledge, and around +45% relative to prompts enhanced with structured knowledge. The underperformance of SKP relative to Zero-shot Prompting suggests that the mere inclusion of relevant knowledge may not always improve model performance."
        },
        {
            "title": "2 Related Work",
            "content": "In this section, we introduce related literature on the main topics of our paper: proportional analogies and LLMs, prompting techniques, and knowledge-enhancement in LLM prompting."
        },
        {
            "title": "2.1 Proportional Analogies and LLMs",
            "content": "One of the earliest methods for solving proportional analogies was Latent Relational Analysis (LRA), introduced by Turney (2005). LRA determines analogy by measuring the similarity in semantic relationships shared between word pairs, considering them analogous if they exhibit high degree of relational similarity. With the advent of neural networks, vector difference-based methods (Vylomova et al., 2016; Allen and Hospedales, 2019; Mikolov et al., 2013) were used to address proportional analogies. As LLMs based on the Transformer architecture (Vaswani et al., 2017a) gained prominence, researchers began investigating the potential of LLMs, particularly Generative 2 Artificial Intelligence (GenAI) models, for solving proportional analogies (Brown, 2020; Ushio et al., 2021; Webb et al., 2023). Specifically, Webb et al. (2023) demonstrated strong performance using single model (GPT-3) on four relatively small proportional analogy datasets. Our study extends this work by scaling up the evaluation to substantially larger dataset and by assessing nine contemporary GenAI models across six distinct prompting approaches. Additionally, we introduce novel exploration of the impact of incorporating various types of knowledge when evaluating GenAI models on proportional analogies."
        },
        {
            "title": "Prompting",
            "content": "GenAI models are built on LLMs that are trained on extensive datasets and optimized for various tasks, including question-answering. This training implies that these models encapsulate the knowledge in the data, allowing them to effectively answer natural language queries (Roberts et al., 2020; Zhu and Li, 2023). Prompting involves transforming an input query into structured natural language statement (prompt) and presenting it to the model, which then guides the output generation process of the model. (Schulhoff et al., 2024; Hadi et al., 2023; Liu et al., 2023). Generating outputs through prompting requires only forward passes during inference time, without any weight updates. Prompts can be created either manually (Wei et al., 2022; Schulhoff et al., 2024) or automatically (Ye et al., 2023; Reynolds and McDonell, 2021; Deng et al., 2022); in this work, we employ the more intuitive manual approach. Prompts can be categorized based on the context they provide. Zero-shot prompts (Brown, 2020) contain only instructions related to solving specific task, whereas Few-shot prompts (Brown, 2020) include both the instructions and one or more examples. Providing examples when querying models is paradigm broadly known as In-context Learning (ICL) (Brown, 2020). Chainof-Thought (CoT) Prompting is designed to guide models through the reasoning process required to solve task by presenting an exemplar that includes the question, reasoning path, and correct answer (Wei et al., 2022) or by just incorporating thoughtinducing phrase such as Lets think step by step (Kojima et al., 2022) (Zero-shot-CoT). Unlike conventional CoT prompting, which often includes an exemplar, our adaptation termed TKP does not provide an exemplar. Instead, it enhances the prompt with the targeted knowledge specific to solving proportional analogies. As result, TKP is more akin to Zero-shot-CoT (Kojima et al., 2022) than to traditional CoT (Wei et al., 2022). The enhancement of LLM performance through the integration of external knowledge, both unstructured and structured, has been extensively studied (Yu et al., 2022). Some approaches transform external knowledge from multiple documents into graph structures and utilize these graphs to enhance LLM querying (Wang et al., 2024). Additionally, some methods directly employ structured knowledge (Baek et al., 2023). Retrieval-augmented generation (RAG) has recently emerged as an umbrella term encompassing all these techniques, where user queries are enriched with content retrieved from external sources to enhance model performance (Lewis et al., 2020; Ding et al., 2024; Mialon et al., 2023; Schulhoff et al., 2024). In this work, we utilize multiple types of knowledge, including targeted and structured knowledge (from three sources), to assess the impact on LLM performance in solving proportional analogies. To the best of our knowledge, this is the first study to explore the capabilities of LLMs in solving proportional analogies using knowledge-enhancement approaches."
        },
        {
            "title": "3 Approach",
            "content": "As illustrated in Figure 1, given proportional analogy MCQ where the question consists of single term pair (e.g., Oxygen and Gas), the GenAI model is required to provide the correct answer choice from five, four or three choices. Zero-shot Prompting, only include the MCQ and simple instruction on how to produce the output without any knowledge enhancement added to the prompt. Next, we enhance the Zero-shot Prompt with exemplars of solved MCQs from the dataset. We consider this approach as enhancing the prompt with exemplar knowledge and refer to this prompting technique as Few-shot Prompting. We experiment with one exemplar (One-shot Prompting) and five exemplars (Fiveshot Prompting). Then combination of lexical, commonsense, and world knowledge from structured sourcesWordNet, ConceptNet, and Wikidata, respectivelyis added to the Zero-shot Prompts for knowledge enhancement, resulting in what we call SKP. Finally, the zero-shot prompt 3 is enhanced with targeted knowledge and we identify this prompting technique as TKP. Targeted knowledge is composed of, the semantic relationship shared between the question term pair and the cognitive process behind solving the proportional analogy. We detail the prompting techniques in Section 3.3."
        },
        {
            "title": "3.1 Dataset Creation",
            "content": "We introduce 15K dataset of proportional analogies containing 5-way, 4-way and 3-way MCQs. Table 1 presents the dataset statistics along with examples from the dataset. We generate 14K questions out of the 15K based on the work by (Yuan et al., 2023). Yuan et al. (2023) introduced an automatically generated million-scale analogy knowledge base with diverse relational structures among the analogous term pairs. We adopt this resource to develop n-way (n=[3, 4, 5]) MCQs as follows. single n-way MCQ consist of pair of terms representing the question and five term pairs representing the answer choices, among which only one term pair is the correct answer. The semantic relationship between the term pair in the question is the same as the semantic relationship shared between the term pair which is the correct answer. The rest of the incorrect answer choices consist of term pairs with different semantic relationships among them. Thousand data points out of the 15K are borrowed from work by Ushio et al. (2021); Turney and Littman (2003); Boteanu and Chernova (2015)3 and contain 5-way, 4-way and 3-way MCQs. We highlight that, compared to previous proportional analogy MCQ datasets used for research (Webb et al., 2023; Ushio et al., 2021), the current dataset provides significant increase in question quantity (15-times) and diversity (with regard to diversity of semantic relations among terms). Our dataset also includes the semantic relationship shared by the question term pair compared to other datasets that do not include this information (Ushio et al., 2021; Turney and Littman, 2003; Boteanu and Chernova, 2015)."
        },
        {
            "title": "3.2 Model Details",
            "content": "GenAI models are designed to generate content that are often indistinguishable from human-produced 3Unlike the 14K MCQs created based on AnalogyKB, these 1K data points do not provide the semantic relationship shared between the question term pair explicitly, therefore we employ two NLP researchers to discuss and manually identify the shared semantic relationship. output. Current state-of-the-art GenAI models are largely based on the Transformer architecture (Vaswani et al., 2017b). In this work we compare the following popular open-source and proprietary GenAI models for their ability to solve proportional word analogy MCQs by incorporating variety of knowledge: (i) Falcon, causal decoder-only model (Almazrouei et al., 2023), (ii) FlanT5 (Longpre et al., 2023), T5 (Raffel et al., 2020a) based model trained on the Flan collection of datasets, (iii) GPT2 (Radford et al., 2019a), the first series of models to popularize in-context instructions, (vi) Mistral (Jiang et al., 2023), leveraging transformers architecture (Vaswani et al., 2017b) with several new introductions such as sliding window attention and pre-fill chunking, (v) Orca (Mukherjee et al., 2023), based on LLaMA model family (Touvron et al., 2023) and fine-tuned on complex explanation traces obtained from GPT-4 (Achiam et al., 2023), (vi) Zephyr (Tunstall et al., 2023), fine-tuned version of Mistral trained on public datasets and optimized with knowledge distillation techniques. (vii) CodeT5 (Wang et al., 2021c), unified pretrained encoder-decoder transformer model leveraging code semantics and finally (viii) CodeParrot (Jain, 2023), model based on GPT-2 and trained to generate python code (ix) GPT-3.5-Turbo 4. Further details of the models used are presented in Appendix A."
        },
        {
            "title": "3.3 Prompting Techniques",
            "content": "Currently, the most popular approach to Multiple Choice Question Answering (MCQA) is via clozestyle prompting (Brown, 2020; Robinson et al., 2023) where each answer choice is concatenated to the question separately and scored independently by the language model (LM). This style of prompting is problematic since it prevents the LM from comparing and contrasting all available options simultaneously. Additionally, it is computationally expensive, as it requires multiple forward passes through the LM to identify the correct answer (Robinson et al., 2023). To address these limitations, we adopt the prompt phrasing introduced by Robinson et al. (2023) with task-specific modifications. Specifically, the question and its symbolenumerated candidate answers are provided to the model as single prompt. Robinson et al. (2023) do not include specific instructions in the prompt for the model to output only the choice symbol. But 4https://platform.openai.com/docs/models/ gpt-3-5-turbo 4 Question Type (MCQ) 5-way 4-way 3-way Top 5 Relation Types # Data Points Questions Relations Example Question: Tenable: Indefensible Choices: (1) Unique : Unprecedented (2) Dire : Pressing (3) Bleak : Desolate (4) Theoretical : Concrete (5) Recondite : Scholarly Question: Haiku: Poem Choices: Question:Ancient: Old Choices: (1) Song : Musician (2) Novel : Book (3) Artist : Painting (4) Page : Typeface (1) Crazy : Unhealthy (2) Delicious : Tasty (3) Smart : Intelligent part of is at location follows producer Amount 14386 610 4 Total # relation types 1020 702 518 376 374 238 Table 1: Dataset statistics. The dataset consist of 15K MCQs that share 238 semantic relation types among them. we observe that adding such specific instructions reduce the model hallucinations. Therefore we use specific, non-ambiguous language to instruct the model to only output the relevant choice symbol. The prompting techniques are detailed below (See example prompts in appendix C)."
        },
        {
            "title": "3.3.1 Zero-shot Prompting",
            "content": "In Zero-shot Prompting, the question, all multiple choice answers and the instructions are provided in natural language (no knowledge is provided)."
        },
        {
            "title": "3.3.2 Few-shot Prompting",
            "content": "We demonstrate the task to the model by providing several exemplars in the form of question, answer choices and the correct answer choice. Then the actual question and answer choices are provided requiring the model to choose the correct answer choice. We employ one-shot and five-shot prompting under the few-shot prompting strategy where one example and five examples are provided respectively. We select these quantities of exemplars to strike balance between the models maximum accepted context length and the computational resources required. To obtain the exemplars, we employ semantic similarity based filtering mechanism as follows. We encode each proportional analogy MCQ in the dataset using SOTA sentence encoding transformer model5, and identify the most semantically similar single example/ five examples based on Cosine similarity."
        },
        {
            "title": "3.3.3 Structured Knowledge Prompting (SKP)",
            "content": "We retrieve knowledge from structured sources, filter it, and then integrate the resulting refined knowledge into the prompts. We detail this process in the subsequent sections. Knowledge Retrieval. We leverage the following widely-used large knowledge sources to obtain three types of knowledge: (i) Wikidata (Vrandeˇcic and Krötzsch, 2014), which provides world knowledge in the form of explicit information about specific instances, encompassing billions of nodes and edges (Wang et al., 2021a); (ii) ConceptNet (Speer et al., 2017), general-domain commonsense knowledge graph with 799,273 nodes and 2,487,810 edges; and (iii) WordNet (McCrae et al., 2019), lexical database for the English language containing 161,338 words, 120,135 synsets, and 415,905 semantic relations. We retrieve knowledge from above sources as follows. Since analogies focus on relations oppose to entities or entity attributes (Gentner, 1983), when retrieving knowledge from knowledge sources we focus on path finding approaches oppose to subgraph extraction approaches. To extract both world and commonsense knowledge, we utilize the pathfinding approach by Lin et al. (2019) that identifies connections between each term pair (in both the question and answer choices). Specifically, we extract paths of length k6 from ConceptNet and Wikidata. When retrieving lexical knowledge from WordNet, we extract the shortest path between term pairs. Knowledge Filtering. For each term pair in the question and answer choices, multiple knowledge paths may be retrieved. To ensure the prompts stay within the maximum context length limit of the evaluated language models, we filter the retrieved paths and retain single path for Wikidata and ConceptNet (See Figure 2). Filtering is not performed on WordNet since single path (shortest) is always retrieved. The filtering mechanisms we employ are as follows: (i) Random Filtering, where one path is randomly selected from the list of available paths; and (ii) Semantic Filtering, which selects the path most semantically similar to the term 5https://huggingface.co/sentence-transformers/ all-mpnet-base-v2 6k is set to 2 for Wikidata and 3 for ConceptNet, as longer paths tend to introduce excessive noise and reduce efficiency. 5 prompting technique (Wei et al., 2022) to provide the model with targeted knowledge in the form of (i) semantic relationship shared by the question term pair (ii) cognitive process used by humans when evaluating such analogies, via the prompt."
        },
        {
            "title": "4 Experimental Setting",
            "content": "We have conducted comprehensive set of experiments across nine GenAI models over six prompt variants on 15K dataset, totalling to 54 (9X6) experiments. 4."
        },
        {
            "title": "Implementation Details",
            "content": "We use API requests for GPT-3.5-Turbo and checkpoints from Hugging face9 for open-source models. The models are evaluated with following hyper parameter settings, temperature = 0.1, top_p=0.1 and repetition_penalty=1.2 to elicit more concrete answers for the MCQs. We use Sentence Transformers10 to identify semantically similar exemplars and to perform semantic knowledge filtering. We utilize Wikidata knowledge from (Wang et al., 2021b), ConceptNet knowledge from conceptnet511 and WordNet knowledge from Open English WordNet (2023)12."
        },
        {
            "title": "5 Results and Discussion",
            "content": "Proportional analogy multiple-choice questions (MCQs) are presented to each GenAI model using the previously described prompts. The models response is extracted from the generated output, and accuracy is measured using Exact Match Accuracy (EMA) (Rajpurkar et al., 2016). While more flexible evaluation metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) are commonly used to assess GenAI-generated outputs, we employ EMA because MCQs are inherently evaluated in binary manner, where partial correctness is not rewarded. We report EMA as percentage for each model and prompt variant. The results are presented in Table 2."
        },
        {
            "title": "Techniques",
            "content": "The highest overall performance was attained by GPT-3.5-Turbo, achieving an EMA of 55.25%. 9https://huggingface.co/models 10https://sbert.net/ 11https://github.com/commonsense/conceptnet5/ wiki/Downloads 12https://github.com/globalwordnet/ english-wordnet?tab=readme-ov-file Figure 2: An illustration of the knowledge filtering approach. Random indicates Random Filtering and Semantic indicates Semantic Filtering. pairs. The term pairs (in question and answer choices) are formatted to term pair sentences in the following form <TERM_1> IS SEMANTICALLY RELATED TO <TERM_2> and returned paths are also formatted to path sentences in the form of [<NODE1_NAME> <RELATION1_NAME> <NODE2_NAME>, <NODE2_NAME> <RELATION2_NAME> <NODE3_NAME>, ...]. Both term pair sentences and path sentences are then encoded using SOTA sentence encoding transformer model7 and the path sentence with the highest cosine similarity to term pair sentence is filtered as relevant knowledge and referred to as knowledge paths8. Generating Prompt. The filtered knowledge paths are appended to the zero-shot prompt after the question and the answer choices to create the SKP and the model is instructed to use the knowledge if necessary. Based on the knowledge filtering mechanism SKP can be referred to as SKP[random] or SKP[semantic]."
        },
        {
            "title": "3.3.4 Targeted Knowledge Prompting (TKP)",
            "content": "When solving proportional analogies, humans typically examine the question term pair, identify the semantic relationship between the two terms, and select the answer pair that shares the same or similar relationship. Inspired by this cognitive process, we modify the traditional Chain-of-Thought (CoT) 7https://huggingface.co/sentence-transformers/ all-mpnet-base-v2 8specific format of Wikidata knowledge paths is <node2_name>, <relation1_name> [<node1_name> <node3_name>] <relation2_name> <node2_name> and ConceptNet knowledge path is [<node1_name> <node2_name> <relation1_name> <relation2_name> <node3_name> <relation3_name> <node4_name>] <node2_name>, <node3_name>, Model Name Zero-shot Prompting Few-shot Prompting Five-shot One-shot Structured Knowledge Prompting Random Semantic Targeted Knowledge Prompting Falcon FlanT5 GPT2 Mistral Orca Zephyr CodeT5 CodeParrot GPT-3.5-Turbo 24.17 36.47 22.65 26.59 24.54 29.46 20.64 0 45.7 23.21 40.09 22.49 26.22 23.28 34.05 24.33 10.11 31.79 22.61 38.07 7.19 27.34 14.11 35.87 0 12.6 41.21 24.75 14.43 6.29 24.58 18.48 16.13 16.15 0 38.29 25.01 14.62 6.17 24.42 18.81 17.22 17.47 0.01 38. 25.4 44.26 21.64 27.37 24.2 15.83 21.64 2.09 55.25 Table 2: MCQ Performance of models. Performance is reported in EMA percentage. Best performance of each model is indicated in bold and the second best performance is indicated by underline. This result underscores the challenge that proportional analogies pose for current state-of-theart GenAI models. This accuracy was obtained through Targeted Knowledge Prompting where the prompt was enhanced with targeted knowledge. Interestingly, the same model, when enhanced with structured knowledge, underperformed with an accuracy of 38% (EMA for SKP[random] is 38.29% and SKP[semantic] is 38.79%), compared to Zero-shot prompting (EMA 45.7%). This suggests that simply adding knowledge, even from diverse sources, may not be beneficial for cognitively demanding tasks such as proportional analogy completion. Out of the nine models four (Falcon, Flan, Mistral and GPT-3.5-Turbo) performs the best when prompted with Targeted Knowledge Prompts and two (GPT2 and Orca) performs the best with Zero-shot prompts with no knowledge enhancement. CodeT5 performs the best with oneshot prompts and Zephyr and CodeParrot performs the best with five-shot prompts. We also observe that models trained specifically on code generation such as CodeT5 and CodeParrot (specially CodeParrot) perform at the lower end of the spectrum despite the demonstrated abilities of them to perform well on other MCQ datasets Robinson et al. (2023). We believe this is due to the challenging nature of the proportional analogy completion task."
        },
        {
            "title": "Performance",
            "content": "Although enhancing prompts with structured knowledge does not consistently improve model performance compared to other prompting techniques, SKP[semantic] leads to slight increases in EMA values (ranging from 0.01% to 1.32%) compared to SKP[random], across all models except GPT-2 and Mistral (see Table 4). We identified subset of MCQs (19.96%) where all three types of knowledge were available and conducted additional experiments to evaluate the individual contribution of each knowledge type to EMA (we employed SKP[semantic ] prompting). Our results show (See table 3 in Appendix B) that incorporating each of the three knowledge types separately into prompts leads to very similar EMA values (when averaged across all nine models). Specifically, prompts enhanced only with Wikidata knowledge resulted in an average EMA of 14.57%, while using only WordNet or only ConceptNet yielded average EMAs of 14.41% and 14.34%, respectively. We also observed that incorporating all three types of knowledge simultaneously into the prompts, compared to using them individually, produced varying results. For example, Falcon, CodeT5 and GPT-3.5-Turbo perform marginally better when single knowledge type is incorporated into the prompt, compared to including all three knowledge types simultaneously (see Figure 3). Providing FlanT5 with single knowledge type compared to all three knowledge types contributes to significant increases of percentage points in EMA (WordNet +18.51 , ConceptNet +15.96 and WikiData +7.44). In contrast, GPT-2, Mistral, and Orca perform better when all knowledge types are integrated into the prompt. Notably, Orca demonstrates an average EMA increase of +11.14 percentage points compared to using only single knowledge source."
        },
        {
            "title": "Performance",
            "content": "Brown (2020) demonstrated that the accuracy of large language models improves with an increase in the number of exemplars. However, Liu et al. (2022) found that the benefits diminish beyond 20 exemplars in certain cases. Similarly, in our study, increasing exemplars from one to five decreases EMA in six out of nine models (see Table 2), leading us to limit exemplars to maximum of five. 7 Figure 3: Perfromance with structured knowledge. Performance of each model when Structured Knowledge Prompting with semantic filtering (SKP[semantic]) is used. All indicates the prompt is enhanced with all three types of knowledge (Wikidata, ConceptNet and WordNet). EMA values are reported on 20% of the 15K dataset where all three knowledge types available."
        },
        {
            "title": "Performance",
            "content": "In this study, we utilize three types of knowledge to enhance prompts: exemplar knowledge, structured knowledge, and targeted knowledge. Among these, exemplar knowledge has the least acquisition cost since it is readily available from the dataset itself requiring no additional resources. Structured knowledge, on the other hand, is more expensive to acquire because it necessitates accessing external knowledge bases or graphs and filtering knowledge, which incurs computational overhead. Targeted knowledge is the costliest to acquire, as it involves identifying the specific semantic relationship between the question term pairs. This semantic relationship is not always readily available, requiring human annotation (for instance, in our dataset of 15K data points, 1K data points lacked this semantic information, necessitating human annotation). As shown in Table 2, targeted knowledge, being the most expensive to acquire, led to the best performance in four models (Falcon, FlanT5, Mistral and GPT-3.5-Turbo) including the peak performance (55% EMA) from GPT-3.5-Turbo. In contrast, structured knowledge, the second most costly, did not result in any models best performance. Although exemplar knowledge is the least expensive, three models performed best with it (Zephyr and CodeParrot in Five-shot; CodeT5 in One-shot)."
        },
        {
            "title": "6 Conclusion and Future Work",
            "content": "We explore the ability of nine LLMs to solve proportional analogies via different knowledgeenhanced prompting techniques on 15K MCQ dataset that we introduce. Our extensive experiments show that most LLMs perform their best when targeted knowledge is incorporated into prompts as opposed to exemplar knowledge and Figure 4: Best and least performing models for each prompting technique. structured knowledge. Even though the several models used in the current study are instruction finetuned versions of their base models, they are not necessarily finetuned for the proportional analogy completion task. Therefore, there is room to explore the performance of models specifically finetuned for proportional analogy completion. Our study currently evaluate manual prompting techniques which are known to be brittle. We believe that evaluating automatic prompting approaches for the same task would be compelling."
        },
        {
            "title": "7 Limitations",
            "content": "When incorporating knowledge paths in SKP, the exact semantic relationship between the question term pair may sometimes be retrieved from the knowledge sources. We define this specific information as targeted knowledge. As result, these instances can be categorized as SKP with targeted knowledge. However, we do not currently verify or adjust for them. In this work, we employed manual prompt cre8 ation, where slight variations can lead to significant differences in model outputs (Zhao et al., 2021). We did not account for this variability in the current study by experimenting with several prompt templates for each prompting technique."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Stergos Afantenos, Tarek Kunze, Suryani Lim, Henri Prade, and Gilles Richard. 2021. Analogies between sentences: Theoretical aspects-preliminary experiments. In Symbolic and Quantitative Approaches to Reasoning with Uncertainty: 16th European Conference, ECSQARU 2021, Prague, Czech Republic, September 2124, 2021, Proceedings 16, pages 318. Springer. Carl Allen and Timothy Hospedales. 2019. Analogies explained: Towards understanding word embeddings. In International Conference on Machine Learning, pages 223231. PMLR. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. 2023. The falcon series of open language models. arXiv preprint arXiv:2311.16867. Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar. 2023. Gpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo. https: //github.com/nomic-ai/gpt4all. Simran Arora, Avanika Narayan, Mayee Chen, Laurel Orr, Neel Guha, Kush Bhatia, Ines Chami, and Christopher Re. 2022. Ask me anything: simple strategy for prompting language models. In The Eleventh International Conference on Learning Representations. Jinheon Baek, Alham Fikri Aji, and Amir Saffari. 2023. Knowledge-augmented language model prompting for zero-shot knowledge graph question answering. In Proceedings of the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE), pages 78106, Toronto, Canada. Association for Computational Linguistics. Bhavya Bhavya, Shradha Sehgal, Jinjun Xiong, and ChengXiang Zhai. 2024. AnaDE1.0: novel data set for benchmarking analogy detection and extraction. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 17231737, St. Julians, Malta. Association for Computational Linguistics. Adrian Boteanu and Sonia Chernova. 2015. Solving and explaining analogy questions using semantic networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 29. Tom Brown. 2020. Language models are few-shot learners. Annual Conference on Neural Information Processing Systems. William Brown. 1989. Two traditions of analogy. Informal Logic, 11(3). Jiangjie Chen, Rui Xu, Ziquan Fu, Wei Shi, Zhongqiao Li, Xinbo Zhang, Changzhi Sun, Lei Li, Yanghua Xiao, and Hao Zhou. 2022. E-kar: benchmark for rationalizing natural language analogical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pages 39413955. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling instruction-finetuned language models. arXiv preprint. Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric Xing, and Zhiting Hu. 2022. RLPrompt: Optimizing discrete text prompts with reinforcement learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 33693391, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Yujuan Ding, Wenqi Fan, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. 2024. survey on rag meets llms: Towards retrievalaugmented large language models. arXiv preprint arXiv:2405.06211. Aleksandr Drozd, Anna Gladkova, and Satoshi Matsuoka. 2016. Word embeddings, analogies, and machine learning: Beyond king-man+ woman= queen. In Proceedings of coling 2016, the 26th international conference on computational linguistics: Technical papers, pages 35193530. Dedre Gentner. 1983. Structure-mapping: theoretical framework for analogy. Cognitive science, 7(2):155 170. Muhammad Usman Hadi, Rizwan Qureshi, Abbas Shah, Muhammad Irfan, Anas Zafar, Muhammad Bilal Shaikh, Naveed Akhtar, Jia Wu, Seyedali Mirjalili, et al. 2023. Large language models: comprehensive survey of its applications, challenges, limitations, and future prospects. Authorea Preprints. Douglas Hofstadter. 2001. Analogy as the core of cognition. The analogical mind: Perspectives from cognitive science, pages 499538. 9 Holyoak, Dedre Gentner, and Kokinov. 2001. The place of analogy in cognition. The analogical mind: Perspectives from cognitive science, 119. Chin-Yew Lin. 2004. Rouge: package for automatic In Text summarization evaluation of summaries. branches out, pages 7481. Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436. Royal Jain. 2023. codeparrot (CodeParrot). Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Cheng Jiayang, Lin Qiu, Tsz Chan, Tianqing Fang, Weiqi Wang, Chunkit Chan, Dongyu Ru, Qipeng Guo, Hongming Zhang, Yangqiu Song, et al. 2023. Storyanalogy: Deriving story-level analogies from large language models to unlock analogical understanding. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1151811537. Jungo Kasai, Keisuke Sakaguchi, yoichi takahashi, Ronan Le Bras, Akari Asai, Xinyan Velocity Yu, Dragomir Radev, Noah A. Smith, Yejin Choi, and Kentaro Inui. 2023. Realtime QA: Whats the answer right now? In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Kiana Kheiri and Hamid Karimi. 2023. Sentimentgpt: Exploiting gpt for advanced sentiment analysis and its departure from current machine learning. arXiv preprint arXiv:2307.10234. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199 22213. Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Hoi. 2022. CodeRL: Mastering code generation through pretrained models and deep reinforcement learning. In Advances in Neural Information Processing Systems. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:94599474. Bill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang Ren. 2019. KagNet: Knowledge-aware graph networks for commonsense reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 28292839, Hong Kong, China. Association for Computational Linguistics. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100114, Dublin, Ireland and Online. Association for Computational Linguistics. Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023. Pretrain, prompt, and predict: systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):135. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc Le, Barret Zoph, Jason Wei, and Adam Roberts. 2023. The flan collection: Designing data and methods for effective instruction tuning. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 2263122648. PMLR. Pankaj Mathur. 2023. tuned orca_mini_7b: An excusand dolly datasets. openllama-7b model plain tom wizardlm, alpaca, https://github.com/pankajarm/wizardlm_ alpaca_dolly_orca_open_llama_7b, https://https://huggingface.co/psmathur/ wizardlm_alpaca_dolly_orca_open_llama_7b. on John P. McCrae, Alexandre Rademaker, Francis Bond, Ewa Rudnicka, and Christiane Fellbaum. 2019. English WordNet 2019 an open-source WordNet for English. In Proceedings of the 10th Global Wordnet Conference, pages 245252, Wroclaw, Poland. Global Wordnet Association. Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, et al. 2023. Augmented language models: survey. arXiv preprint arXiv:2302.07842. Tomáš Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word representations. In Proceedings of the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies, pages 746751. Marvin Minsky. 1988. Society of mind. Simon and Schuster. 10 Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318. Sander Schulhoff, Michael Ilie, Nishant Balepur, Konstantine Kahadze, Amanda Liu, Chenglei Si, Yinheng Li, Aayush Gupta, HyoJung Han, Sevien Schulhoff, et al. 2024. The prompt report: systematic survey of prompting techniques. arXiv preprint arXiv:2406.06608. Robyn Speer, Joshua Chin, and Catherine Havasi. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Proceedings of the AAAI conference on artificial intelligence, volume 31. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116. Oren Sultan, Yonatan Bitton, Ron Yosef, and Dafna Shahaf. 2024. Parallelparc: scalable pipeline for generating natural-language analogies. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 59005924. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019a. Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019b. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020a. Exploring the limits of transfer learning with unified text-to-text transformer. J. Mach. Learn. Res., 21(1). Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. 2020b. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250. Laria Reynolds and Kyle McDonell. 2021. Prompt programming for large language models: Beyond the In Extended Abstracts of the few-shot paradigm. 2021 CHI Conference on Human Factors in Computing Systems, CHI EA 21, New York, NY, USA. Association for Computing Machinery. Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the parameters of language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 54185426, Online. Association for Computational Linguistics. Joshua Robinson, Christopher Michael Rytting, and David Wingate. 2023. Leveraging large language models for multiple choice question answering. Preprint, arXiv:2210.12353. Oren Sultan and Dafna Shahaf. 2022. Life is circus and we are the clowns: Automatically finding analogies between situations and processes. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 35473562. Terrence Szymanski. 2017. Temporal word analogies: Identifying lexical replacement with diachronic word embeddings. In Proceedings of the 55th annual meeting of the association for computational linguistics (volume 2: short papers), pages 448453. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, et al. 2023. Zephyr: Direct distillation of lm alignment. arXiv preprint arXiv:2310.16944. Peter Turney. 2005. Measuring semantic similarity by latent relational analysis. arXiv preprint cs/0508053. Peter Turney and Michael Littman. 2003. Combining independent modules in lexical multiple-choice problems. In Recent Advances in Natural Language Processing III, page 101110. Asahi Ushio, Luis Espinosa Anke, Steven Schockaert, and Jose Camacho-Collados. 2021. Bert is to nlp what alexnet is to cv: Can pre-trained language models identify analogies? In Proceedings of the 59th Annual Meeting of the Association for Computational 11 Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 36093624. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, and Aidan Gomez. 2017a. L. u. kaiser, and i. polosukhin,attention is all you need,. Advances in neural information processing systems, 30:59986008. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017b. Attention is In Proceedings of the 31st Internaall you need. tional Conference on Neural Information Processing Systems, NIPS17, page 60006010, Red Hook, NY, USA. Curran Associates Inc. Denny Vrandeˇcic and Markus Krötzsch. 2014. Wikidata: free collaborative knowledgebase. Communications of the ACM, 57(10):7885. Ekaterina Vylomova, Laura Rimell, Trevor Cohn, and Timothy Baldwin. 2016. Take and took, gaggle and goose, book and read: Evaluating the utility of vector differences for lexical relation learning. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16711682, Berlin, Germany. Association for Computational Linguistics. Chenhao Wang, Yubo Chen, Zhipeng Xue, Yang Zhou, and Jun Zhao. 2021a. Cognet: Bridging linguistic knowledge, world knowledge and commonsense knowledge. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 16114 16116. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Thilini Wijesiriwardene, Ruwan Wickramarachchi, Bimal Gajera, Shreeyash Gowaikar, Chandan Gupta, Aman Chadha, Aishwarya Naresh Reganti, Amit Sheth, and Amitava Das. 2023. ANALOGICAL - novel benchmark for long text analogy evaluation in large language models. In Findings of the Association for Computational Linguistics: ACL 2023, pages 35343549, Toronto, Canada. Association for Computational Linguistics. Sam Witteveen and Martin Andrews. 2019. Paraphrasing with large language models. In Proceedings of the 3rd Workshop on Neural Generation and Translation, pages 215220, Hong Kong. Association for Computational Linguistics. Qinyuan Ye, Maxamed Axmed, Reid Pryzant, and Fereshte Khani. 2023. Prompt engineering prompt engineer. arXiv preprint arXiv:2311.05661. Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang. 2022. survey of knowledge-enhanced text generation. ACM Computing Surveys, 54(11s):138. Siyu Yuan, Jiangjie Chen, Changzhi Sun, Jiaqing Liang, Yanghua Xiao, and Deqing Yang. 2023. Analogykb: Unlocking analogical reasoning of language models with million-scale knowledge base. arXiv preprint arXiv:2305.05994. Liyan Wang and Yves Lepage. 2020. Vector-toIn 2020 sequence models for sentence analogies. International Conference on Advanced Computer Science and Information Systems (ICACSIS), pages 441446. IEEE. Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian de Wynter, Yan Xia, Wenshan Wu, Ting Song, Man Lan, and Furu Wei. 2024. Llm as mastermind: survey of strategic reasoning with large language models. arXiv preprint arXiv:2404.01230. Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2021b. Kepler: unified model for knowledge embedding and pre-trained language representation. Transactions of the Association for Computational Linguistics, 9:176194. Yu Wang, Nedim Lipka, Ryan Rossi, Alexa Siu, Ruiyi Zhang, and Tyler Derr. 2024. Knowledge graph prompting for multi-document question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1920619214. Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021c. Identifier-aware unified pre-trained encoder-decoder models for code unarXiv preprint derstanding and generation. arXiv:2109.00859. Codet5: Taylor Webb, Keith Holyoak, and Hongjing Lu. 2023. Emergent analogical reasoning in large language models. Nature Human Behaviour, 7(9):15261541. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In International conference on machine learning, pages 1269712706. PMLR. Xunjie Zhu and Gerard de Melo. 2020. Sentence analogies: Linguistic regularities in sentence embeddings. In Proceedings of the 28th International Conference on Computational Linguistics, pages 33893400, Barcelona, Spain (Online). International Committee on Computational Linguistics. Zeyuan Allen Zhu and Yuanzhi Li. 2023. Physics of language models: Part 3.1, knowledge storage and extraction. arXiv preprint arXiv:2309.14316."
        },
        {
            "title": "A Model Details",
            "content": "Falcon (Almazrouei et al., 2023): The Falcon model used in this work is the Falcon-7B-Instruct 12 model 13 which is causal decode-only model, instruction finetuned on top of the base Falcon-7B. The fine-tuning dataset is made up of 250M tokens from various conversational datasets (Baize14), (Anand et al., instruction datasets (GPT4All 2023), GPTeacher15) and common crawl data (RefinedWeb (Penedo et al., 2023))from the web. Falcon-7B tokenizer is used for tokenization. The architecture of Falcon is broadly adapted from GPT3 with changes in positional embeddings used, attention mechanisms used and decoder block architecture. FlanT5 (Chung et al., 2022): We use the FlanT5-XXL version with 11B parameters. This version is based on pretrained T5 (Raffel et al., 2020b) and instruction finetuned on mixture of tasks. This model is finetuned specifically with Chain-of-Thought data. GPT2 (Radford et al., 2019b): We use the XL version with 1.5 parameters. The model is pretrained with English language data (40 GB of text from the web) and causal language modeling objective. Interestingly the model is not trained on articles from Wikipedia. Mistral (Jiang et al., 2023): This is decoder only transformer model and we use the Mistral-7BInstruct version with 7B parameters. This version is finetuned on publicly available instruction datasets. Mistral introduce Sliding Window Attention, Rolling Buffer Cache and Pre-fill Chunking in its architecture. Orca (Mathur, 2023): We employ orca_mini_7b, 7B parameter version of Orca, which is based on OpenLLaMA-7B. The model is trained on datasets with explanation tuning, where the response from the <query, response> pair is augmented with detailed responses from the base (teacher) model (Mukherjee et al., 2023). The explanation tuning datasets used are WizardLM16, Alpaca dataset (Taori et al., 2023) and Dolly17 and system prompts are used to elicit step-by-step explanations. 13https://huggingface.co/tiiuae/ falcon-7b-instruct 14https://github.com/project-baize/ baize-chatbot/tree/main/data 15https://github.com/teknium1/GPTeacher 16https://github.com/nlpxucan/WizardLM 17https://github.com/databrickslabs/dolly Zephyr18: We use the Zephyr-7B-alpha with 7B parameters finetuned from Mistral-7B-v0.1. The finetune datasets contain synthetic dialogues ranked by GPT-4 and prompt completion dataset where completions are ranked by GPT-4. CodeT5 (Le et al., 2022): The CodeT5 model wwe use is codet5-large model with 770M parameters. The model is trained on Masked Span Prediction objective on CodeSearchNet dataset (Husain et al., 2019) CodeParrot (Jain, 2023): We use the 1.5B parameter CodeParrot model based on GPT-2. The model is trained to generate python code on python files dataset from GitHub19. GPT-3.5-Turbo20: We use OpenAI API to access the model, gpt-3.5-turbo-0125. Figure 5: Prompt Lengths vs. Peak Performance"
        },
        {
            "title": "B Performance and Additional Results",
            "content": "B.1 Model Performance vs. Prompt Length (PL) We calculated the average prompt lengths across models for each prompting technique (PL for SKP is calculated by averaging SKP[random] and SKP[semantic]) (See Figure 5). According to (Liu et al., 2024), longer prompts (with important information placed in the middle) tend to negatively affect performance. Based on such literature, one 18https://huggingface.co/HuggingFaceH4/ zephyr-7b-alpha 19https://huggingface.co/datasets/codeparrot/ codeparrot-clean 20https://platform.openai.com/docs/models/ gpt-3-5-turbo 13 Model Name WD knowledge only CN knowledge only WN knowledge only All knowledge available Falcon FlanT5 GPT2 Mistral Orca Zephyr CodeT5 CodeParrot GPT-3.5-Turbo 25.14 19.63 4.11 22.84 14.62 16.16 21.47 0 7.13 25.38 28.15 2.4 22.07 9.52 12.49 21.64 0 7.43 25.04 30.7 1 23.34 9.28 9.75 22.7 0 7.9 24.14 12.19 5.84 24.11 22.24 14.76 20.7 0 7.2 Table 3: Performance of models based on provided knowledge types. Performance values are reported in EMA percentage and calculated using 2995 (20%) data points that had all three knowledge types available. might suggest that Zero-shot prompts yield better results in our study because they are short, but this is not the case. Despite being longer than Zeroshot prompts, higher peak model performance is achieved by TKP."
        },
        {
            "title": "C Prompts",
            "content": "Figues 6, 7, 8, 9 and 10 illustrates example prompts provided to models. 14 Figure 6: Example of Zero-shot prompt used on our dataset Figure 7: Example of One-shot prompt used on our dataset 15 Figure 8: Example of Five-shot prompt used on our dataset Figure 9: Example of Structured Knowledge Prompt[semantic] used on our dataset 16 Figure 10: Example of Targeted Knowledge Prompt used on our dataset"
        }
    ],
    "affiliations": [
        "AI Institute, University of South Carolina, USA",
        "Amazon GenAI, USA",
        "IIIT Hyderabad, India",
        "Meta, USA",
        "Stanford University, USA"
    ]
}