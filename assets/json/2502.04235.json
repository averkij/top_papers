{
    "paper_title": "MAGA: MAssive Genre-Audience Reformulation to Pretraining Corpus Expansion",
    "authors": [
        "Xintong Hao",
        "Ke Shen",
        "Chenggang Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite the remarkable capabilities of large language models across various tasks, their continued scaling faces a critical challenge: the scarcity of high-quality pretraining data. While model architectures continue to evolve, the natural language data struggles to scale up. To tackle this bottleneck, we propose \\textbf{MA}ssive \\textbf{G}enre-\\textbf{A}udience~(MAGA) reformulation method, which systematic synthesizes diverse, contextually-rich pretraining data from existing corpus. This work makes three main contributions: (1) We propose MAGA reformulation method, a lightweight and scalable approach for pretraining corpus expansion, and build a 770B tokens MAGACorpus. (2) We evaluate MAGACorpus with different data budget scaling strategies, demonstrating consistent improvements across various model sizes (134M-13B), establishing the necessity for next-generation large-scale synthetic pretraining language models. (3) Through comprehensive analysis, we investigate prompt engineering's impact on synthetic training collapse and reveal limitations in conventional collapse detection metrics using validation losses. Our work shows that MAGA can substantially expand training datasets while maintaining quality, offering a reliably pathway for scaling models beyond data limitations."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 5 3 2 4 0 . 2 0 5 2 : r MAGA: MASSIVE GENRE-AUDIENCE REFORMULATION TO PRETRAINING CORPUS EXPANSION Xintong Hao, Ke Shen, Chenggang Li Seed-LLM, ByteDance haoxintong@bytedance.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Despite the remarkable capabilities of large language models across various tasks, their continued scaling faces critical challenge: the scarcity of high-quality pretraining data. While model architectures continue to evolve, the natural language data struggles to scale up. To tackle this bottleneck, we propose MAssive GenreAudience (MAGA) reformulation method, which systematic synthesizes diverse, contextually-rich pretraining data from existing corpus. This work makes three main contributions: (1) We propose MAGA reformulation method, lightweight and scalable approach for pretraining corpus expansion, and build 770B tokens MAGACorpus1. (2) We evaluate MAGACorpus with different data budget scaling strategies, demonstrating consistent improvements across various model sizes (134M-13B), establishing the necessity for next-generation large-scale synthetic pretraining language models. (3) Through comprehensive analysis, we investigate prompt engineerings impact on synthetic training collapse and reveal limitations in conventional collapse detection metrics using validation losses. Our work shows that MAGA can substantially expand training datasets while maintaining quality, offering reliably pathway for scaling models beyond data limitations."
        },
        {
            "title": "INTRODUCTION",
            "content": "The success of Large Language Models(LLMs) can be largely attributed to the scale of model parameters and training data (Kaplan et al., 2020; Hoffmann et al., 2022). However, recent studies indicate that we are approaching critical point where high-quality natural language data is becoming increasingly scarce (Villalobos et al., 2022). This data exhaustion phenomenon poses fundamental challenge to the continued scaling of language models, as model size growth begins to outpace the availability of diverse, high-quality training data. Recent efforts to enhance LLMs under data-constrained scenarios have explored several promising directions. Researchers have made significant progress in optimizing data curation workflows to reduce filtration losses while maintaining quality (Su et al., 2024; Penedo et al., 2024). Another straight-forward approach is controlled data repetition, which shows that moderate repetition (up to 4) yields negligible loss changes compared to unique data (Muennighoff et al., 2023). Perhaps most intriguingly, researchers have begun leveraging LLMs themselves to synthesize high-quality training data (Abdin et al., 2024; Su et al., 2024), creating bootstrapping effect for model improvement. Among these approaches, data synthesis presents particularly promising direction, as it offers the potential to generate unlimited training data while maintaining control over quality and distribution. However, existing methods heavily rely on large-scale models for synthetic data generation. Nemotron-CC (Su et al., 2024) employs 12B dense model, while Phi series (Abdin et al., 2024) depends on GPT-4-level capabilities. Additionally, Phi introduces sophisticated seed curation system to control data diversity, an approach later adopted by Cosmopedia (Ben Allal et al., 2024). The reliance on large-scale models and careful curation creates both computational bottlenecks and scalability challenges for pretraining data synthesis. 1https://huggingface.co/datasets/bytedance-research/MAGACorpus Figure 1: Overview of MAGA framework. Our method expands the original corpus through twostage synthesis process. Each document is reformulated to 5 new documents, achieving 3.9 token number expansion while maintaining diversity through massive (genre, audience) pairs. In this work, we propose more efficient approach MAGA. As illustrated in Figure 1, our approach leverages 3.3B MoE model that adaptively expands datasets using only raw documents as input, keeping it lightweight and scalable. Our main contributions are: We build 770 billion tokens MAGACorpus based on existing high-quality text collections, demonstrating superior performance across various sizes (134M/377M/1.7B/7B/13B parameters) compared to original corpus. We perform representative evaluation of data budget scaling strategies, revealing that MAGACorpus yields consistent improvements compared to data repetition and upsamling, demonstrating the necessity of synthetic data in next-generation language model training. We analyze synthetic data collapse from two key perspectives, characterizing prompt engineerings mitigating effects while revealing limitations of validation loss as collapse detection metric, providing insights for future synthetic data optimization."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Data Curation While web-crawled data contains hundreds of trillions of tokens, stringent quality filters typically remove the majority of this content. Popular datasets like C4, Gopher, Dolma, RefinedWeb (Raffel et al., 2020; Rae et al., 2021; Penedo et al., 2023; Soldaini et al., 2024) use nonlearned heuristics method. And recently FineWeb-Edu (Penedo et al., 2024), DCLM (Li et al., 2024), FineFineWeb (Zhang et al., 2024) focus on aggressive model-based and retrieval-based filtering. Such heavy filtering results in removal of 90% of tokens, some researchers turn their attention to balance accuracy and data quantity (Su et al., 2024). However, this does not alter the fact that the total amount of high-quality data remains limited. Repetition Training Studies on subset repetition training have revealed that model divergence tends to occur earlier as model parameters increase (Hernandez et al., 2022). For scenarios training on entire datasets repeated, limiting to 4 epochs or fewer results in minimal efficiency degradation (Muennighoff et al., 2023; Taylor et al., 2022). Furthermore, (Xue et al., 2024) shows that some regularization techniques (e.g., dropout) and leveraging MoE architecture can help efficient LLM development on broader scale. Overall, this topic remains understudied across different model architectures, data distributions, and repetition ratios. Synthetic Pretrain Current synthetic data generation methods for language model pretraining can be primarily categorized into two approaches: seed based synthesis and raw text based rephrasing. The seed based method, exemplified by Phi-4 (Abdin et al., 2024) and Cosmopedia (Ben Allal et al., 2024), employs predefined seed systems and task templates to precisely control the type and structure of generated content. The rephrasing method, represented by WRAP (Maini et al., 2024) and 2 Figure 2: Implementation details. From high-quality corpus, we sample subset to serve as input for the LLM labeler and judger. Through iterative filtering, we train and quantize SLM tool models for each stage to improve inference efficiency, which are used to generate the reformulated corpus. Nemotron-CC (Su et al., 2024), generates data by rephrasing web content into QA pairs and wikistyle texts, demonstrating significant effectiveness in processing noisy web text, though its benefits may be limited when applied to high-quality source data (Pieler et al., 2024). Additionally, Ge et al. (2024) introduce an innovative text generation method based on billion personas, offering new insights into enhancing the diversity of synthetic data. While existing approaches have made significant progress, they face key limitations: seed-based methods require complex initialization systems, limiting investigation of their scaling properties, while rephrasing-based approaches struggle to effectively augment high-quality corpus at scale. To address these limitations, our framework MAGA bridges this gap by leveraging and extending the inherent diversity of existing corpus. Specifically, it adaptively generates multiple Genre and Audience seeds for each document of SmolLM-Corpus (Ben Allal et al., 2024), enabling 3.9x token expansion while maintaining diversity and quality."
        },
        {
            "title": "3 MASSIVE GENRE-AUDIENCE REFORMULATION",
            "content": "Preserving core knowledge while adapting content presentation for diverse audiences is the key motivation behind MAGA reformulation. As shown in Figure 2, the framework expands original corpus through two-stage synthesis process followed by an extra heuristic cleaning stage. Our implementation consists of three key components2: (1) large language model serving as both LLM labeler and judger; (2) Task specific tool models applying W8A8 quantized (Xiao et al., 2023) for efficiency; (3) balanced quality assessment mechanism defined as Limited Consistency. Genre-Audience Pairs For pretraining corpus, there is consensus among researchers to ensure diversity and quality. Inspired by Maini et al. (2024);Ge et al. (2024), we expand the simple rephrasing method from only few styles to massive genre-audience pairs. Genre defines the knowledge expression framework through multiple dimensions: communication purpose (e.g., education, analysis), content structure (e.g., step-by-step tutorials, analytical reports), language style, and knowledge depth control. This framework guides how information should be reconstructed while preserving core concepts. Audience profiles combine demographic factors (age, education, profession) with knowledge background and motivational characteristics. For example, beginner-level first-aid guide would be reformulated differently for medical students versus office workers, while maintaining essential medical accuracy. Our framework supports genres and audience types, theoretically enabling NM unique reformulation patterns. To balance diversity and computational efficiency, we generate 5 genre-audience pairs per inference pass. And as each run is conditioned by different document, we can maximize the coverage of different genre-audience combinations. 2Tool model details presented in Appendix B. Prompts and case studies in Appendix E. 3 Reformulation Once the genre-audience pairs are determined, the reformulation process follows straightforward approach, as prompt presented in Appendix E.2. The key factor of reformulation is how to evaluate the output text, so we introduce the concept of Limited Consistency as criterion for quality controlling. This framework seeks to establish an optimal balance between textual variation and information preservation as shown in Prompt 1. # Detailed Requirements For scoring judgment, the following standards must be followed: 1. The scoring range is 15 points. You need to analyze and grasp each point mentioned in #Thought Process#, and give scores with distinction. Be strict, dont be too lenient with scoring! 2. The Reformulated Text is allowed to differ from the Original Text in writing style, expression style, and focus points! This cannot be basis for deducting points! 3. The Reformulated Text is allowed to omit some information from the Original Text! Not all information from the Original Text needs to be reflected in the Reformulated Text! The following situations will [NOT REDUCE] the score: 1. The Reformulated Text can include information points not present in the Original Text 2. The additional content in the Reformulated Text deviates significantly from the core information of the Original Text 3. The expression style, order, and focus points of the Reformulated Text differ from the Original Text The following situations will [REDUCE] the score: 1. The information points in the Reformulated Text differ so greatly from the Original Text that its not apparent it was Reformulated from the Original Text 2. The Reformulated Text lacks every information points present in the Original Text Prompt 1: LLM judger prompt snippet. In practice, we use the proportion of samples (score 3) as our primary metric during both labeler LLM prompt engineering and tool model development. As shown in Table 1, both our LLM and SLM achieve over 92% with only minor performance gap (-1.05%). Table 1: Performance comparison between SLM and LLM on reformulation quality evaluation. Models Total 5 3 2 1 Rate( 3) Diff Labeler LLM 15,355 4,120 7,143 3,034 661 214 15,355 3,788 7,124 3,224 736 285 Tool SLM 93.11% 92.06% -1.05% - This trade-off between flexibility and fidelity is critical for maintaining reformulation quality while ensuring meaningful content adaptation. The empirical effects of different consistency levels are further explored in our ablation studies (Section 5.1), where we demonstrate how models perform when deviating from this balanced approach. Cleaning Stage Similar to previous synthesis work (Maini et al., 2024; Su et al., 2024), we involve final cleaning stage to filter out the high frequency patterns, for example, Notes: ..., Please note that ..., The above is as required ..., The following is..., etc. And remove the documents which have an extremely low keyword coverage to raw documents."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "Having established our MAGACorpus generation framework, we now evaluate its effectiveness through series of experiments designed to answer two key research questions: RQ1: How effective is MAGACorpus as synthetic expansion to original dataset? RQ2: How does MAGACorpus aid model scaling under data-constrained scenarios? 4.1 SETUP Datasets To ensure reproducibility, we build MAGACorpus based on SmolLM-Corpus3 (Ben Allal et al., 2024), expanding fineweb-edu-dedup source from 195B tokens to 770B tokens. 3https://github.com/huggingface/smollm/tree/main/text/pretraining 4 Models and Hyperparams The architecture of pretraining model follows llama3 (Dubey et al., 2024). Experiments across various sizes (134M/377M/1.7B/7B/13B) are running with WSD lr scheduler (Hu et al., 2024) where 0.1% warmup steps, 75% stable and final 25% decay phase. Detailed model specifications are provided in Appendix and Table 6. Then, we conduct two groups of experiments: (1) to validate MAGACorpus quality in Section 4.2, and (2) to demonstrate how MAGA reformulation aids scaling under data-constrained scenarios in Section 4.3. Evaluation In Section 4.2, we follow popular practice of LIGHTEVAL (Fourrier et al., 2023), evaluating on comprehensive suite of open benchmarks aligned with Fineweb/SmolLM/Cosmopedia settings4, include ARC-E+C (Clark et al., 2018), HellaSwag (Zellers et al., 2019), Winogrande (Sakaguchi et al., 2021), MMLU (Hendrycks et al., 2020), GSM8K (Cobbe et al., 2021), etc. While model performance is influenced by multiple factors, we list some recently SOTA models as reference, all the models are evaluated in the same environment except Llama-3.2-1B5. Then in Section 4.3 and Section 5 we will report the scores and training dynanmics provided by our in-house evaluation kit, where all benchmarks are grouped into knowledge, reasoning and math categories."
        },
        {
            "title": "4.2 EFFECTIVENESS OF MAGACORPUS",
            "content": "Data Recipe Models of 134M/377M/1.7B sizes are trained from scratch for up to 1000 billion tokens. Our baseline is trained on SmolLM-Corpus (Ben Allal et al., 2024) dataset, and the other two experiments use MAGACorpus as incremental/replacement data. In contrast to SmolLMs recipe, we use unique token number from each source as the mixing ratio shown in Table 2. This ensures that different sources have consistent repetition epochs during training. Table 2: MAGACorpus experiments data source weight (%). experiments fineweb-edu-dedup cosmopedia-v2 python-edu open-web-math maga-corpus Baseline MAGA-Only MAGA-Mix 80.89 - 16.29 11.65 11.65 11.65 1.66 1.66 1. 5.80 5.80 5.80 - 80.89 64.59 Performance training on MAGACorpus. As shown in Table 3, the MAGA-Mix group shows consistent improvements across different model sizes, with larger performance gains as model size increases, +0.26/+0.95/+2.15 for 134M/377M/1.7B models respectively. Notably, MAGA-Mix achieved substantial gains in TriviaQA(+2.03/+6.99/+15.47) and GSM8K(+0.15/+0.22/+6.06). Additional insights and explanations regarding metric changes are provided in Appendix D. These results strongly demonstrate the effectiveness and quality of MAGACorpus. Table 3: Benchmark MAGA with SOTA small LMs. Models of similar size are grouped. All results are obtained through LIGHTEVAL (Fourrier et al., 2023). Best results in each group are highlighted in bold, the second in underline, and in green for that MAGA-Mix wins under fair comparison. Model #Params. #Tokens ARC(C+E) Wino. Hella. MMLU MMLU-PRO CSQA OpenBookQA PIQA TriviaQA GSM8K Avg. SmolLM2-135M 135M SmolLM-135M 135M 134M Baseline 134M MAGA-Mix Qwen2.5-0.5B 360M SmolLM2-360M 360M SmolLM-360M 360M 377M Baseline 377M MAGA-Mix Qwen2.5-1.5B SmolLM2-1.7B Llama-3.2-1B OLMo-1B-0724 SmolLM-1.7B Baseline MAGA-Mix 1.3B 1.7B 1.2B 1B 1.7B 1.7B 1.7B 2T 600B 600B 600B 18T 4T 600B 600B 600B 18T 11T 9T 3.05T 1T 1T 1T 44.12 42.47 41.71 43.01 45.16 53.4 49.99 48.57 49.39 58.36 60.42 49.2 44.71 59.95 59.63 60.36 51.07 42.03 51.54 41.08 52.41 40.69 41.25 51.7 53.99 51.16 52.33 54.58 52.96 51.67 52.64 51.02 52.64 51. 58.64 66.39 59.59 68.73 57.8 61.2 56.04 64.38 62.83 54.7 57.38 65.19 57.46 65.52 31.27 29.93 30.03 30.1 33.51 35.29 33.84 33.63 34.09 40.23 41.4 36.63 32.3 39.35 39.4 40.79 11.06 11.4 11.37 11.76 11.97 11.17 11.42 11.25 11. 13.85 19.61 11.7 11.8 10.92 12.11 14.1 33.82 32.51 34.32 32.68 31.61 37.92 34.81 36.77 37.1 34.4 43.65 41.2 33.09 38 42.59 41.11 35 33.2 35.4 36.4 37.6 37.6 37.6 39 39.6 42.6 38.4 38 42.6 45.6 42.8 68.23 68.17 67.85 67.3 69.97 71.76 71.87 71 72.31 75.95 77.53 74.8 75.24 75.9 76.88 77.53 1.91 1.08 0.02 2.05 3.96 16.73 2.27 0.29 7. 20.51 36.68 28.1 13.82 13.14 4.95 20.42 1.52 0.99 1.29 1.44 32.9 2.96 1.97 1.52 1.74 60.8 29.04 7.2 2.43 4.62 7.81 13.87 32.00 31.24 31.51 31.77 37.18 37.37 34.84 34.57 35. 46.87 47.93 40.62 37.18 40.20 41.15 43.4 4.3 SCALING RESULTS Scaling Strategies decay phase, the hyperparams except data scheduler are algined with Section 4.2. In this section, we train models from 377M to 13B without the learning rate 4https://github.com/huggingface/cosmopedia/blob/main/evaluation 5Our access request is rejected by repo authors, so we use scores reported by SmolLM. Table 4: Scaling experiments data recipe, values represent #unique tokens #epochs. Repetition Experiments Training fineweb-edu maga corpus dedup Budget fineweb random Design Rationale EntireSet Subset Baseline Full-Fineweb-Edu MAGA Expansion Baseline Upsample-EDU MAGA Expansion 500B 500B 500B 700B 700B 700B 50 10 195 2.56 50 2 - - 200 2 - - - What if we could collect more unique data. Add maga to reduce the repetition num. 50 1.4 50 5 50 1 - - 450 1.4 450 1 Upsample to get 200B more budget. 200 1 450 1 Add maga to achieve the same target. We simulate two common scenarios under data-constrained conditions: Scenario 1: Expanding 50B high-quality dataset to 500B training budget. Scenario 2: Expanding 500B mixed-quality dataset to 700B training budget. The experiment design for different strategies is presented in Table 4, which involves three datasets: (1) 50B-token random sample from fineweb-edu-dedup, (2) corresponding filtered subset from MAGACorpus, and (3) 450B-token corpus obtained from Fineweb (Penedo et al., 2024) after global min-hash deduplication. Figure 3: The first and last two figures illustrate the training dynamics of EntireSet and SubSet data recipes, respectively. Benchmark details are provided in Appendix D. Scaling Results As shown in Figure 3, the EntireSet experiments, increasing unique token count through Full-Fineweb-Edu shows marginal improvements, while MAGA expansion demonstrates consistent gains. Similarly, the Subset experiments, while both upsampling and MAGA expansion improve upon the baseline, their scaling properties differ significantly. The performance advantage of upsampling remains relatively constant across model sizes (+0.89/+1.53/+1.23/+1.41), whereas MAGA expansion demonstrates superior scaling characteristics, its performance gains amplify with increasing model scale (+1.46/+2.67/+3.59/+3.73). This widening gap with model size suggests that synthetic data expansion is more promising approach for scaling language models effectively. Validation Losses Although MAGACorpus demonstrates superior benchmark performance, we observe increasing validation losses compared to baseline models. While higher validation losses might seem concerning at first glance, its important to note that validation loss may not fully reflect model performance, as token-level perplexity is inherently biased by the frequency distribution of the validation set, and in-domain validation metrics may not necessarily correlate with out-of-domain generalization capabilities. This observation, combined with recent studies linking loss degradation to model collapse phenomena (Dohmatob et al., 2024b;a; Zhu et al., 2024), calls for more nuanced analysis, which we provide in Section 5.2."
        },
        {
            "title": "5 ABLATIONS AND DISCUSSIONS",
            "content": "5.1 HOW IMPORTANT IS TO CONSTRUCT PROPER PROMPT ENGINEERING TARGET? In this section, we provide further explanation of Limited Consistency introduced in Section 3. The concept centers on balancing variance and invariance in the reformulation process. To quantitatively understand this trade-off, we investigate two prompt engineering strategies through controlled experiments. 6 5.1.1 INFORMATION PRESERVATION TRADE-OFF As shown in Figure 14, we design two prompt variants: (1) strict version that enforces high fidelity to source information, and (2) relaxed version that allows substantial deviations while maintaining basic topical relevance. Using these prompts, we collect training data from the same samples to train two SLM models, denoted as SLM-Strict and SLM-Relaxed respectively. The SLM performance comparison is presented in Table 5. Table 5: Performance comparison of different SLM variants on reformulation quality metrics. Models Total 5 4 SLM-Base SLM-Strict SLM-Relaxed 15,355 15,355 3,788 7,124 3,224 15,355 6,814 5,220 2,384 2 736 520 1 Rate( 2) Rate( 4) Rate(= 5) 285 227 6.65% - 71.06% 24.67% 78.37% 44.38% 408 1,685 3,889 4,156 5,086 60.19% - - For this ablation study, we sample an additional 20B tokens from real data and generate three synthetic datasets: 80B tokens using SLM-Base, 80B tokens using SLM-Strict, and 40B tokens using SLM-Relaxed. Following the experimental setup in Section 4.2, we compare MAGA-Mix and MAGA-Only data recipe against baseline created by replicating the original 20B tokens 10 times. IMPACT ON MODEL TRAINING 5.1.2 As shown in Figure 4, our MAGA-Mix setting experiments reveal distinct patterns across training configurations. Both SLM-Base and SLM-Strict demonstrate performance improvements, while the SLM-Relaxed configuration leads to significant collapse. more pronounced performance gap is observed in the MAGA-Only setting presented in Appendix D.3. Figure 4: Benchmark results and validation losses in the MAGA-Mix setting. The sensitivity to data repetition varies across capability domains, with knowledge dimension showing greater resilience. Despite the apparent effectiveness of strict information preservation, can it fundamentally address the challenges posed by data repetition? Our examination of validation loss trajectories reveals critical distinction: SLM-Base maintains healthy optimization characteristics throughout training, whereas SLM-Strict exhibits degraded scaling behavior at higher iteration steps, reminiscent of the limitations observed with data repetition. Figure 5: t-SNE visualization results. Base (left) maintains distribution that overlaps with but extends beyond the original data. Strict (middle) clusters also extend original data, but indicating limited diversity compared to left. Relaxed (right) shows significant distributional shift, explaining its poor performance. To provide deeper insights into these behaviors, we analyze the embedding space distributions through t-SNE visualization (Figure 5). SLM-Strict achieves moderate distribution expansion be7 yond the source domain, yet more constrained compared to SLM-Bases balanced extension. Meanwhile, SLM-Relaxeds excessive drift demonstrates the importance of maintaining appropriate distributional constraints. These findings highlight the critical importance of proper prompt design in synthetic data generation. Too strict preservation may limit the benefits of data synthesis, while too loose constraints can lead to model collapse. Our Base strategy achieves an effective balance, enabling both quality preservation and beneficial variations that contribute to improved model performance. 5.2 IS THERE MODEL COLLAPSE PHENOMENON TRAINING WITH MAGA? In Section 4.3 and 5.1, we both observe that the validation loss of MAGA group is significantly higher than that of the real data group. This raises an important question: does this loss degradation indicate potential model collapse, as suggested by recent work using loss as measurement (Dohmatob et al., 2024b;a; Zhu et al., 2024). Our investigation reveals more nuanced picture."
        },
        {
            "title": "5.2.1 MULTI-PERSPECTIVE VALIDATION ANALYSIS",
            "content": "Our analyses across different validation sets reveal varying patterns in model behavior (Figure 6). As expected, MAGA groups substitution of fineweb-edu data results in adverse effects on corresponding loss, with similar deterioration observed in open-web-math. Interestingly, the synthetic dataset Cosmopedia-V2 demonstrates improved loss metrics. notable contrast emerges in pythonedu performance: while MAGA groups exhibit negative impact at the 134M and 377M parameter, this trend reverses at 1.7B, suggesting scale-dependent effects on model behavior. Figure 6: validation losses of experiments in Section 4.2. 5.2.2 FINE-GRAINED PATTERN ANALYSIS To better understand whether the increased validation loss truly indicates model collapse, we conduct fine-grained analysis of loss patterns. Specifically, we compare token-level losses between models trained on real data and MAGA-Mix data (800B checkpoint), using samples from both Fineweb-Edu and MAGACorpus. As illustrated in subfigures 1 and 3 of Figure 7, each point represents samples average token loss, consistent with the overall loss discrepancy shown in Figure 6. Figure 7: Losses pattern analysis. Subfigures 1 and 3 shows comparison between models trained on different data settings, with lossreal on y-axis and losssynt on x-axis. Subfigures 2 and 4 track synt lossi the position where lossi diff) first becomes significantly higher than the sequences average difference (detailed definition in Appendix D.3). real (lossi The distribution of first anomaly positions (subfigures 2 and 4) reveals crucial insight: when processing real data, models trained on synthetic data show performance degradation (measured 8 by lossdiff) that predominantly manifests in later sequence positions, which intensifies as lossdiff increases. However, this positional bias disappears when evaluating on synthetic data. The systematic pattern suggests that rather than experiencing model collapse, the synthetic-trained model may have developed different learning strategy (examples shown in Figure 11 and Figure 12). While it shows higher validation losses on certain real-world datasets, its strong performance in our main experiments indicates potential trade-off: the model may prioritize learning generalizable patterns from context over memorizing specific sequence dependencies. This shift in learning process could explain both the improved performance on benchmark tasks and the increased losses on validation sets that potentially require more memorization-based processing."
        },
        {
            "title": "5.2.3 ANALYSIS AND IMPLICATIONS",
            "content": "Our findings suggest two key insights into model learning with synthetic data: Limitations of Loss Metrics. Validation loss alone may not fully capture the complex dynamics of model learning. Different validation sets, whether synthetic or real, represent distinct sampling approaches to human knowledge and skills, making it challenging to establish truly unbiased evaluation metrics for model performance. The strong performance of MAGA-trained models on downstream tasks, despite higher validation loss on certain datasets, suggests that the relationship between validation loss and model capabilities is more complex than previously assumed. Synthetic Learning Hypothesis. Similar to Abdin et al. (2024), synthetic data, generated through next-token prediction, provides more efficient learning pathways by compelling models to focus on core reasoning and contextual understanding. In contrast, real data contains patterns from nonlinear human editing processes, which might require additional model capacity to capture but may not directly contribute to core reasoning capabilities. Therefore, the relative advantages of synthetic data in certain aspects may stem from the resource competition between capability acquisition and memory storage in models. While our investigation provides initial insights, we believe this phenomenon merits deeper examination in future research to fully understand the interplay between synthetic and real training data."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we present MAGA, novel approach for expanding pre-training corpus through modelassisted generation. Through extensive experiments across different model scales (134M to 13B parameters), we demonstrate that MAGA effectively addresses data scarcity challenges in typical scenario. Our empirical results show consistent improvements across various benchmarks, highlighting the robustness and scalability of our approach. The effectiveness of MAGA highlights the potential of synthetic data in modern language model training. While current experiments focus on moderate-scale scenarios, our findings suggest promising directions for addressing larger-scale training challenges. The frameworks flexibility in balancing data quality and quantity offers new possibilities for resource-efficient training strategies. As the field continues to explore solutions for data-constrained scenarios, MAGA provides practical approach that complements existing data curation methods. ACKNOWLEDGEMENTS We are grateful to Chao He, Zhixin Yao, Yue Chen and Runyu Shi for their help with prompt templates and case studies, and to Seed-Foundation team for providing the stable training/inference platform, which enabled us to build the synthetic pipeline and corpus within reasonable timeframe. The icons shown in Figure 1 are designed by Freepik."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Jyoti Aneja, Harkirat Behl, Sebastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905, 2024. 9 Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von URL https://huggingface.co/datasets/ Werra. HuggingFaceTB/smollm-corpus. Smollm-corpus, 2024. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Elvis Dohmatob, Yunzhen Feng, Arjun Subramonian, and Julia Kempe. Strong model collapse. arXiv preprint arXiv:2410.04840, 2024a. Elvis Dohmatob, Yunzhen Feng, Pu Yang, Francois Charton, and Julia Kempe. tale of tails: Model collapse as change of scaling laws. arXiv preprint arXiv:2402.07043, 2024b. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Clementine Fourrier, Nathan Habib, Thomas Wolf, and Lewis Tunstall. Lighteval: lightweight URL https://github.com/huggingface/ framework for llm evaluation, 2023. lighteval. Tao Ge, Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, and Dong Yu. Scaling synthetic data creation with 1,000,000,000 personas. arXiv preprint arXiv:2406.20094, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and arXiv preprint Jacob Steinhardt. Measuring massive multitask language understanding. arXiv:2009.03300, 2020. Danny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, et al. Scaling laws and interpretability of learning from repeated data. arXiv preprint arXiv:2205.10487, 2022. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. An empirical analysis of compute-optimal large language model training. Advances in Neural Information Processing Systems, 35:3001630030, 2022. Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar. Datacomp-lm: In search of the next generation of training sets for language models, 2024. Pratyush Maini, Skyler Seto, He Bai, David Grangier, Yizhe Zhang, and Navdeep Jaitly. RephrasarXiv preprint ing the web: recipe for compute and data-efficient language modeling. arXiv:2401.16380, 2024. Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. Advances in Neural Information Processing Systems, 36:5035850376, 2023. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Hamza Alobeidli, Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: Outperforming curated corpora with web data only. Advances in Neural Information Processing Systems, 36:7915579172, 2023. Guilherme Penedo, Hynek Kydlıˇcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the finest text data at scale, 2024. URL https://arxiv.org/abs/2406.17557. Michael Pieler, Marco Bellagente, Hannah Teufel, Duy Phung, Nathan Cooper, Jonathan Tow, Paulo Rocha, Reshinth Adithyan, Zaid Alyafeai, Nikhil Pinnaparaju, et al. Rephrasing natural text data with different languages and quality levels for large language model pre-training. arXiv preprint arXiv:2410.20796, 2024. Jack Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et al. Dolma: An open corpus of three trillion tokens for language model pretraining research. arXiv preprint arXiv:2402.00159, 2024. Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Nemotron-cc: Transforming common crawl into refined long-horizon pretraining dataset. arXiv preprint arXiv:2412.02595, 2024. Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: large language model for science. arXiv preprint arXiv:2211.09085, 2022. Pablo Villalobos, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, and Anson Ho. Will we run out of data? an analysis of the limits of scaling datasets in machine learning. arXiv preprint arXiv:2211.04325, 1, 2022. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: In International Accurate and efficient post-training quantization for large language models. Conference on Machine Learning, pp. 3808738099. PMLR, 2023. Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, and Yang You. To repeat or not to repeat: Insights from scaling llm under token-crisis. Advances in Neural Information Processing Systems, 36, 2024. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019. Ge Zhang, Xinrun Du, Zhimiao Yu, Zili Wang, Zekun Wang, Shuyue Guo, Tianyu Zheng, Kang Zhu, Jerry Liu, Shawn Yue, Binbin Liu, Zhongyuan Peng, Yifan Yao, Jack Yang, Ziming Li, Bingni Zhang, Minghao Liu, Tianyu Liu, Yang Gao, Wenhu Chen, Xiaohuan Zhou, Qian Liu, Taifeng Wang, and Wenhao Huang. Finefineweb: comprehensive study on fine-grained domain web corpus, December 2024. Xuekai Zhu, Daixuan Cheng, Hengli Li, Kaiyan Zhang, Ermo Hua, Xingtai Lv, Ning Ding, Zhouhan Lin, Zilong Zheng, and Bowen Zhou. How to synthesize text data without model collapse? arXiv preprint arXiv:2412.14689, 2024."
        },
        {
            "title": "A LIMITATIONS AND OPPORTUNITIES",
            "content": "While our experimental results demonstrate the effectiveness of MAGA in both quality validation and scaling scenarios, several important aspects warrant further investigation. We identify three key areas for future research: The tool model (SLM) employed in this work relies on an early internal version with relatively moderate capabilities. While Pieler et al. (2024) suggests that model size may not be the determining factor for rephrasing tasks, the influence within MAGA framework remains unexplored. Understanding the relationship between SLM capacity and corpus quality is crucial for optimizing the effectiveness-efficiency trade-off. Our current experiments demonstrate effectiveness up to 13B parameters and 1,000B tokens of training budget. Extending this approach to long-horizon training and larger-scale models requires additional validations, particularly for next-generation models which require hundreds of trillions of training tokens. Regarding data repetition strategies, we present preliminary explorations under computational resource constraints. The underlying patterns and their sensitivity to various factorsrepetition ratio, data distribution, and data qualityrequire systematic investigation. Future research should examine how these factors collectively determine optimal data strategies across different training scenarios."
        },
        {
            "title": "B TOOL MODEL IMPLEMENTATION",
            "content": "Our implementation pipeline consists of three key components: (1) large language model serving as both LLM labeler and judger; (2) Task specific tool models applying W8A8 quantized (Xiao et al., 2023) for efficient inference; (3) comprehensive evaluation framework. High Quality Corpus To ensure reproducibility, we conduct our reformulated corpus based on SmolLM-Corpus6 (Ben Allal et al., 2024), expanding fineweb-edu-dedup source from 195B tokens to 770B tokens. Then we setup additionally experiments on FineWeb and FineWeb-Edu (Penedo et al., 2024), which constitute solid foundation for research on data scaling approaches. Prior to these experiments, we have validated our approach on our in-house datasets. The results demonstrate consistent performance across both datasets, suggesting broad applicability of our method. Tool Models Training Initialized from pretrained SLM (an in-house 3.3B MoE model), we collect 50,000 training samples through iterative filtering and training, where 15,000 of raw text to genre-audience pairs, 35,000 of raw text to reformulated output. Each models validation responses are scored by capable LLM judger, that ensures the SLMs achieve comparable synthesis quality to the original LLM as shown in Table 1. The sequence length is 8192 with maximum prompt/response length 4,096 tokens, each model is trained 3 epochs on the samples with cosine lr scheduler. Resource Analysis To generate 770B synthetic tokens, it takes 25664 and 1024130 NVIDIA H100 GPU hours to process two stages, or 4 more hours when using Huawei Ascend910B2. In our practice, we use Huawei Ascend910B2 synthesis most tokens of MAGACorpus, which significantly reduce the cost of synthesis."
        },
        {
            "title": "C PRETRAINING DETAILS",
            "content": "Evaluation The LightEval results provided in Section 4.2 follow SmolLM setting, that with GSM8K 5-shot and all the others 0-shot. The benchmarks presented in Figure 8 and Figure 9 follow few-shot evaluation settings, specifically ARC(8-shots), TriviaQA(5-shots), Winogrande(5-shots) and similar configurations for other tasks. 6https://github.com/huggingface/smollm/tree/main/text/pretraining 13 Figure 8: Detail evaluation results of EntireSet described in Table 4. MAGACorpus group demonstrats advantages over other groups across most evaluation sets, consistently across models of sizes. Figure 9: Detail evaluation results of Subset described in Table 4. As the model size increases, the performance gap between the upsampling group and MAGACorpus gradually widens in ARC, DROP, GSM8K, RACE, but with some variations observed in TriviaQA and WinoGrande. Datasets and Models We sample 100 million tokens from SmolLM-Corpus as the validation dataset. The hyperparams are presented in Table 6, tokenzier used for training and computing token counts is same as SmolLM17 with vocab size of 49,152. 7https://huggingface.co/HuggingFaceTB/cosmo2-tokenizer 14 Table 6: Hyperparams of different model size. model batch learning hidden size size size rate ffn inner num num shared layers head heads seq len total tie emb params 134M 128 377M 320 1.7B 512 7B 1,024 13B 1,024 3e-3 1.5e-3 5e-4 4e-4 4e-4 1,204 1,536 2,560 4,096 4,096 4,096 6,144 10,240 8,192 12,288 8 12 20 32 32 8 10 16 32 1 1 1 4 4 8,192 false 8,192 false 8,192 false 8,192 false 8,192 false 134M 377M 1.68B 6.98B 12.9B"
        },
        {
            "title": "D FURTHER ANALYSIS OF EXPERIMENTS",
            "content": "D.1 BENCHMARK IMPROVEMENT In our experimental observations  (Table 7)  , notable performance improvements are demonstrated in both TriviaQA and GSM8k benchmarks, warranting detailed examination of these score variations. Table 7: Benchmark results. copy of SmolLM1/SmolLM1-Ours/MAGA-Mix in Table 3. Model #Params. #Tokens ARC(C+E) Wino. Hella. MMLU MMLU-PRO CSQA OpenBookQA PIQA TriviaQA GSM8K Avg. SmolLM-1.7B Baseline MAGA-Mix 1.7B 1.7B 1.7B 1T 1T 1T 59.95 59.63 60.36 54.7 62.83 57.38 65.19 57.46 65.52 39.35 39.4 40.79 10.92 12.11 14. 38 42.59 41.11 42.6 45.6 42.8 75.9 76.88 77.53 13.14 4.95 20.42 4.62 7.81 13.87 40.20 41.15 43. The enhanced TriviaQA performance exhibited by SmolLM1-1.7B relative to our baseline can be attributed to the larger proportion of Cosmopedia in its training configuration. Both MAGACorpus and Cosmopedia employ synthetic methodologies, which contribute to improved model learning efficiency. The observed gains in GSM8K performance can be traced to the target genres, including teaching schemas and problem-solving exemplars, embedded within the Reformulation component. This early exposure to structured problem-solving approaches facilitates more effective performance on analogous mathematical reasoning tasks. D.2 WHAT IF USE MAGACORPUS ALONE? While our method primarily aims to expand existing datasets, aligning with recent trends in combining synthetic and real data, we explore the effects of training exclusively with MAGACorpus. As shown in Table 8, the absence of real data leads to performance degradation across most tasks (average -0.95), particularly in two tasks, Hellaswag(-1.23/-1.69/-2.85) and CommonsenseQA(-3.11/- 4.83/-4.50). This decline can be attributed to our design choice, which focuses on diversity and overall quality rather than requiring the preservation of all information from each raw documents. Table 8: Comparison between MAGA-Mix and MAGA-Only Model #Params. #Tokens ARC(C+E) Wino. Hella. MMLU MMLU-PRO CSQA OpenBookQA PIQA TriviaQA GSM8K Avg. MAGA-Mix MAGA-Only 134M 134M MAGA-Mix MAGA-Only 377M 377M 600B 600B 600B 600B MAGA-Mix MAGA-Only 1.7B 1.7B 1T 1T 43.01 41.98 -1.03 49.39 47.95 -1. 60.36 59.02 -1.34 30.1 51.7 41.25 51.38 40.02 29.87 -0.32 -1.23 -0.23 34.09 52.64 51.34 53.35 49.65 33.31 0.71 -1.69 -0.78 40.79 57.46 65.52 57.06 62.67 40.34 -0.40 -2.85 -0.45 11.76 11.5 -0.26 11.35 11.38 0. 14.1 13.51 -0.59 32.68 29.57 -3.11 37.1 32.27 -4.83 41.11 36.61 -4.50 36.4 33 -3.40 38 38 - 42.8 45.2 2.40 67.3 68.01 0.71 72.31 70.95 -1.36 77.53 76.71 -0.82 2.05 2.26 0.21 7.28 6.83 -0. 20.42 19.78 -0.64 31.77 1.44 1.06 30.87 -0.38 -0.90 35.52 1.74 1.59 34.53 -0.15 -0.99 43.40 13.87 13.57 42.45 -0.30 -0.95 D.3 ABLATION DETAILS MAGA-Only Setting of PE Ablation Upon relaxing the information preservation requirements for PE objectives in the MAGA-Only setting, we observe complete collapse in knowledge-based 15 dimensions while maintaining modest improvements in reasoning and mathematical capabilities. This divergence suggests that different cognitive capabilities have distinct requirements for the richness and nature of training data content. Figure 10: Corresponding benchmark results described in Section 5.1. Further Discussion of Section 5.2 For our analysis method in Figure 7, we define the token loss difference as lossi real, where is the token index, synt/real is dataset used for model training. Note that we consistently use synthetic minus real, where positive value indicates poorer prediction performance by the synthetic model on given sample. diff = lossi synt lossi Since next token prediction is computed based on preceding context, we define the first anomaly position to identify where models prediction for tokens within the window begins to significantly deteriorate. The definition is as follows: first anomaly position = min{p (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 p+w1 (cid:88) i=p lossi diff (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) > µ + kσ}, where = max(0.05 seq length, 1), µ = mean(lossi diff). Here, we employ the absolute value of the windowed average loss to identify significant performance degradation in either model. This approach enables the detection of notable prediction quality drops regardless of which model (synthetic or real) experiences the deterioration. diff), σ = std(lossi Finally, we define the normalized position, enabling fair comparisons across various sequence lengths: normalized position = (cid:40) first anomaly position seq length 1 100% if anomaly found otherwise Below are example cases from English and Chinese documents. Figure 11 presents the token loss difference on each position. Example 2 and Example 3 show similar anomaly pattern, we can get the reason in Figure 12, that they are from the same website source contain identical boilerplate text about region selection and website localization at the end of their content. Figure 11: Random examples sampling from where mean(lossi model fail to predict the tokens in later sequence positions. diff) > 0.5, the synthetic-trained This suggests potential noise in the data preprocessing pipeline, specifically in handling website navigation elements and localization prompts that should have been removed during content extraction. 16 While these examples demonstrate clear patterns of model behavior differences in handling noisy web data, we acknowledge that this analysis is limited to selected cases with apparent preprocessing artifacts. more comprehensive evaluation across diverse data sources and quality levels would be necessary to fully understand the impact of synthetic training data on model performance. Figure 12: Corresponding cases sampled from Fineweb-Edu, which align with the loss patterns shown in Figure 11, with higher loss by synthetic-trained model highlighted in red . Figure 13: Chinese corpus samples with higher loss by synthetic-trained model in red ."
        },
        {
            "title": "E PROMPTS AND CASES",
            "content": "E.1 CASES Table 9: Example outputs of SLM variants. 18 E.2 PROMPTS Although the term rewrite is used in some prompt templates as the editing instruction, it serves the same function as reformulate discussed in sections above, which aims to maintain the core meaning of the documents while only optimizing its expression. # strict version You are text polishing expert. You will polish text based on the given [Genre] and [Audience]. When polishing, you must follow these 4 rules: 1. Read through the entire text and polish it according to the requirements of the given [Genre] and [Audience] 2. The degree of polishing should not be too heavy just aim to satisfy the requirements of [Genre] and [Audience] as much as possible 3. Doublecheck that the polished text is suitable for the audience described in [Audience]! 4. Pay attention to the frequency of modal particles the text should not contain too many modal particles # relaxed version You are creative expert skilled at transforming materials into creative inspiration and building independent, complete, and highly original texts. Requirements: 1. Read through the original text thoroughly, extract several key themes/keywords, transform to abstract or universal concept inspiration, then generate entirely new text constructions. 2. Extract content from [audience] and [genre] sections, but dont be constrained by them directly, just use them as creative inspiration. 3. Create and reformulated text around points 1/2, and build new meaning from details to the whole structure. Figure 14: two different prompt templates, we keep the input aligned with MAGA strategy, using raw text, genre, audience to fill the template. ############# #Identity and Capabilities# You are content creation expert, specializing in text analysis and rewriting, capable of adapting content based on varying genres and audiences to produce diverse and highquality texts. Your English writing is at native editor level, and you will output your rewritten texts in English. International audiences particularly enjoy your work, which receives widespread readership and circulation, earning unanimous acclaim from the industry for your capabilities! ############# #Workflow# Please utilize your analytical and writing abilities to rewrite the text based on the original content and given genre and audience. Before beginning the rewrite, you will consider the following requirements: 1. First, read through the original text thoroughly, identify its information content and value, and consider how to prevent any loss of information points and value in the rewritten text 2. Focus on the original content, combine it with the given genre requirements, and rewrite the text following the descriptions, content modules, language requirements, and other stylistic elements specified in the genre, to form an initial draft 3. Polish the initial draft according to the given audience requirements, and generate the final rewritten text in English 4. Refine the rewritten text to match native English speakers reading habits and expression patterns ############# #Detailed Requirements# Please ensure you follow the three workflow requirements above, then generate the final English rewritten text according to these detailed requirements. The given audience is <<<{audience}>>>. The given genre is <<<{genre}>>>. ############# #Raw Text# {raw text} Prompt 2: reformulation prompt template. 19 ############# #Identity and Capabilities# You are content creation expert, specializing in text analysis and rewriting, skilled at adapting content based on varying [genres] and [audiences] to produce diverse and highquality texts. Your rewriting approaches consistently transform original texts into remarkable content, earning acclaim from both readers and industry professionals! ############# #Workflow# Please utilize your imagination and creativity to generate 5 pairs of [genre] and [audience] combinations suitable for the original text. Your analysis should follow these requirements: 1. First, analyze the characteristics of the source text, including writing style, information content, and value 2. Then, consider how to preserve the primary content and information while exploring possibilities for broader audience engagement and alternative genres ############# #Detailed Requirements# Ensure adherence to the workflow requirements above, then generate 5 pairs of [genre] and [audience] combinations according to these specifications: Your provided [genres] should meet the following requirements: 1. Clear Genre Definition: Demonstrate strong diversity; include genres youve encountered, read, or can envision 2. Detailed Genre Description: Provide 23 sentences describing each genre, considering but not limited to type, style, emotional tone, form, conflict, rhythm, and atmosphere. Emphasize diversity to guide knowledge adaptation for specific audiences, facilitating comprehension across different backgrounds. Note: Exclude visual formats (picture books, comics, videos); use textonly genres. Your provided [audiences] should meet the following requirements: 1. Clear Audience Definition: Demonstrate strong diversity; include both interested and uninterested parties, those who like and dislike the content, overcoming bias toward positive audiences only 2. Detailed Audience Description: Provide 2 sentences describing each audience, including but not limited to age, occupation, gender, personality, appearance, educational background, life stage, motivations and goals, interests, and cognitive level ############# #Response# { audience 1: audience1, genre 1: genre1, audience 2: audience2, genre 2: genre2, audience 3: audience3, genre 3: genre3, audience 4: audience4, genre 4: genre4, audience 5: audience5, genre 5: genre5 } ############# #Input# {raw text} Prompt 3: genre-audience pairs prompt template. 20 ############# #Identity and Capabilities# You are Content Reviewer, skilled at analyzing texts and keenly identifying and analyzing the relationships, similarities, and differences between two texts. Your thorough analysis of each pair of texts, with attention to every detail, provides great convenience for subsequent review work! ############# #Thinking Process# Please fully utilize your analytical abilities, review capabilities, and deep thinking skills to analyze the Rewritten Text against the Original Text as benchmark, ultimately providing analysis and scoring for [ A]. You will follow these steps for detailed consideration: 1. First, you will read through the original text thoroughly, identifying the information points in the Original Text 2. You will also read through the rewritten text thoroughly, identifying the information points in the Rewritten Text 3. Compare the information in both texts content. The Rewritten Text is allowed to have new information points, different writing styles, expression styles, order, and focus from the Original Text. As long as it is created based on some information points from the Original Text, it is considered good for [A] 4. After careful analysis and review, please clearly list the connections and differences between the two texts, and based on this, provide final analysis and scoring for [A] ############# #Detailed Requirements# The scoring judgment for [A] must follow these standards: 1. The scoring range is 15 points. You need to analyze and grasp each aspect mentioned in #Thinking Process#, and differentiate scores accordingly. Be strict, dont be too lenient with scoring! 2. The Rewritten Text is allowed to differ from the Original Text in writing style, expression style, and focus! This cannot be basis for deducting points! 3. The Rewritten Text is allowed to omit some information from the Original Text! It is not required that all information from the Original Text appears in the Rewritten Text! This also cannot be basis for deducting points! If this is the only issue, please give full score of 5 points. In scoring [A], the following situations will **NOT reduce** the score for [A]: 1. The Rewritten Text can include information points not present in the Original Text 2. The added content in the Rewritten Text significantly deviates from the core information of the Original Text 3. The expression style, order, and focus of the Rewritten Text differ from the Original Text In scoring [A], the following situations **WILL reduce** the score for [A]: 1. The information points in the Rewritten Text differ so greatly from the Original Text that its not recognizable as being rewritten from the Original Text 2. The Rewritten Text contains none of the information points from the Original Text ############# #Original Text# {raw text} #Rewritten Text# {rewritten text} ############# #Response Format# { A:{ analysis: xxx, provide reasons for point deductions score: 1, 2, 3, 4, or 5 }, } ############# Prompt 4: Full LLM judger prompt."
        }
    ],
    "affiliations": [
        "Seed-LLM, ByteDance"
    ]
}