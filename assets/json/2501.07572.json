{
    "paper_title": "WebWalker: Benchmarking LLMs in Web Traversal",
    "authors": [
        "Jialong Wu",
        "Wenbiao Yin",
        "Yong Jiang",
        "Zhenglin Wang",
        "Zekun Xi",
        "Runnan Fang",
        "Deyu Zhou",
        "Pengjun Xie",
        "Fei Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Retrieval-augmented generation (RAG) demonstrates remarkable performance across tasks in open-domain question-answering. However, traditional search engines may retrieve shallow content, limiting the ability of LLMs to handle complex, multi-layered information. To address it, we introduce WebWalkerQA, a benchmark designed to assess the ability of LLMs to perform web traversal. It evaluates the capacity of LLMs to traverse a website's subpages to extract high-quality data systematically. We propose WebWalker, which is a multi-agent framework that mimics human-like web navigation through an explore-critic paradigm. Extensive experimental results show that WebWalkerQA is challenging and demonstrates the effectiveness of RAG combined with WebWalker, through the horizontal and vertical integration in real-world scenarios."
        },
        {
            "title": "Start",
            "content": "WebWalker: Benchmarking LLMs in Web Traversal Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Deyu Zhou, Pengjun Xie, Fei Huang"
        },
        {
            "title": "Tongyi Lab",
            "content": ", Alibaba Group jialongwu@{alibaba-inc.com, seu.edu.cn}"
        },
        {
            "title": "Code",
            "content": "5 2 0 2 3 1 ] . [ 1 2 7 5 7 0 . 1 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Retrieval-augmented generation (RAG) demonstrates remarkable performance across tasks in open-domain question-answering. However, traditional search engines may retrieve shallow content, limiting the ability of LLMs to handle complex, multi-layered information. To address it, we introduce WebWalkerQA, benchmark designed to assess the ability of LLMs to perform web traversal. It evaluates the capacity of LLMs to traverse websites subpages to extract high-quality data systematically. We propose WebWalker, which is multi-agent framework that mimics humanlike web navigation through an explore-critic paradigm. Extensive experimental results show that WebWalkerQA is challenging and demonstrates the effectiveness of RAG combined with WebWalker, through the horizontal and vertical integration in real-world scenarios."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have demonstrated impressive capabilities across wide range of natural language processing tasks (Ouyang et al., 2022; OpenAI, 2022b). While their knowledge base remains static post-training, integrating external search engines via retrieval-augmented generation (RAG) allows LLMs to retrieve up-to-date information from the web, enhancing their utility in dynamic, knowledge-intensive scenarios (Lewis et al., 2020). However, traditional online search engines, e.g., Google or Bing, perform horizontal searches of queries and may not effectively trace the deeper content embedded within websites. Interacting with the web pages and digging through them can effectively address this issue. Previous works related to web pages focus on addressing action-based requests, such as Mind2Web (Deng et al., 2023) and WebArena (Zhou et al., 2024a); these HTML-based instruction-action benchmarks face challenges such 1 Figure 1: multi-source QA1example from WebWalkerQA that requires traversing web pages to gather information for answering the given question. as excessively noisy information and overly long inputs, which can significantly hinder performance due to limitations in long-context understanding. Additionally, they fail to capture the complexities of real-world scenarios where relevant information is buried deep within web pages and requires multiple layers of interaction. To fill this gap, new task Web Traversal is proposed, given an initial website corresponding to query, systematically traverses web pages to uncover information. We propose WebWalkerQA, designed specifically to evaluate LLMs on their ability to handle queries embedded in complex, multi-step web interactions on given root website. WebWalkerQA focuses on text-based reasoning abilities, using Question-Answer format to evaluate traversal and problem-solving capabilities in web scenarios. We constrain actions to click to evaluate the agents navigation and information-seeking capabilities. This paradigm is more targeted and aligns better with practical applications. WebWalkerQA reflects real-world 1In our paper, multi-source refers to the requirement of information from multi distinct web pages. challenges, emphasizing the depth of the source information across education, conference, organization, and game domains, where official sources are published and paths to information are more structured with clickable buttons and reasoning logic. Several types, including multi-source and single-source QAs, are developed to evaluate the ability of LLMs to mimic different human webnavigation paradigms. Additionally, we introduce strong baseline WebWalker, multi-agent framework designed to emulate human-like web navigation through vertical exploration. The framework consists of an explorer agent and critic agent. Given the need for reasoning capabilities to navigate and interact with web pages effectively, the explorer agent is built upon the ReAct framework (Yao et al.), leveraging thought-action-observation paradigm, while the critic agent is responsible for maintaining memory and generating responses based on the exploration conducted by the explorer agent. We evaluate the performance of the WebWalker, built on various mainstream LLMs, including both closedsource and open-sourced, using WebWalkerQA as the benchmark. However, even with the most powerful LLMs as the backbone, its performance on WebWalkerQA remains suboptimal, thereby validating the challenge posed by WebWalkerQA. We then conduct further experiments to validate the integration with the RAG for informationseeking QA tasks. Our findings are as follows: (i) Web navigation still requires efforts in tasks that demand planning and reasoning; (ii) By combining RAG with the WebWalker, this horizontal and vertical coordination proves effective; (iii) Vertical exploration of pages offers promising direction for scaling inference time in RAG systems. The contributions of our work are as follows: We construct challenging benchmark, WebWalkerQA, which is composed of 680 queries from four real-world scenarios across over 1373 webpages. To tackle the challenge of web-navigation tasks requiring long context, we propose WebWalker, which utilizes multi-agent framework for effective memory management. Extensive experiments show that the WebWalkerQA is challenging, and for informationseeking tasks, vertical exploration within the page proves to be beneficial."
        },
        {
            "title": "2.1 Web-Oriented Benchmark",
            "content": "Before the era of LLMs, several web-oriented benchmarks had already been proposed (Liu et al., 2018; Xu et al., 2021; Humphreys et al., 2022; Yao et al., 2022; Mialon et al., 2024; Xu et al., 2024). LLMs are capable of interacting with complex environments, like the open web in HTML or DOM format (Tan et al., 2024), leading to the development of an increasing number of benchmarks aimed at evaluating the interaction capabilities of LLMs with web content. The widely used benchmark today, Mind2Web (Deng et al., 2023), is dataset designed for evaluating web agents that follow instructions to complete complex tasks, typically through multiple-choice questions. Subsequent works have extended the interaction to the vision domain, incorporating information from screenshots (Zheng et al., 2024; He et al., 2024a; Koh et al.; Cheng et al., 2024). The web-oriented benchmark is becoming progressively more human-like, vision-centric, and increasingly broad, complex, and realistic (Liu et al.; Hong et al., 2024; Kim et al., 2024; Zhang et al., 2024c). The most closely to ours are the MMInA (Zhang et al., 2024c) and AssistantBench (Yoran et al., 2024), both of which focus on time-consuming tasks that require navigation across multiple pages. In our work, WebWalkerQA takes the form of QA pairs. Unlike all previous works, we construct both single-source and multi-source queries from the width perspectives of the website, aiming to simulate two types of page exploration patterns typically exhibited by humans. The comparison between WebWalkerQA and other benchmarks is shown in Table 2."
        },
        {
            "title": "2.2 Agents on Web-Navigation",
            "content": "Based on web-oriented benchmarks, numerous web agents have been proposed (Nakano et al., 2021; Liu et al., 2023; Zhou et al., 2023; Lai et al., 2024; Zhou et al., 2024b). Web agents primarily follow two lines of development: one leverages small language model trained specifically to filter actions or identify relevant HTML elements (Zheng et al.; Deng et al., 2024; Furuta et al., 2024). The other line focuses on prompting LLMs (Reddy et al., 2024; Song et al., 2024; Koh et al., 2024), where different agentic modules are used to guide the model in accomplishing complex web navigation tasks more effectively. In addition, with the rise of visual web-oriented benchmarks, many agents now"
        },
        {
            "title": "Format",
            "content": "Mind2Web (Deng et al., 2023) WebArena (Zhou et al., 2024a) AssistantBench (Yoran et al., 2024) MMInA (Zhang et al., 2024c) GAIA (Mialon et al., 2024)"
        },
        {
            "title": "En\nEn\nEn\nEn\nEn",
            "content": "Multi-choice Action QA Action QA Depth Width Hop"
        },
        {
            "title": "WebWalkerQA",
            "content": "En&Zh QA # Pages 100 6 525 100 - 1373 Table 1: Comparison between WebWalkerQA and other benchmarks. Depth refers to the extent of exploration required on given website. Width denotes whether answering query necessitates multiple sources. Hop indicates whether multiple steps are required to complete the task. #Pages refers to the number of webpages involved. use screenshots as sensory input (He et al., 2024b; Abuelsaad et al.; Iong et al., 2024). Unlike previous works, WebWalker specializes in informationseeking by reasoning over HTML button data. It emulates human-like page interactions with web pages to access reliable, authoritative information utilizing multi-agent framework."
        },
        {
            "title": "3 WebWalkerQA",
            "content": "We present WebWalkerQA in this section, starting with an overview of the data collection process to ensure quality (3.1), followed by discussion of WebWalkerQAs statistics (3.2) and introduction of new task, Web Traversal (3.3). Finally, we describe the evaluation metrics for WebWalkerQA(3.4)."
        },
        {
            "title": "3.1 Data Collection",
            "content": "To make the annotation process cost-efficient and accurate, we employ two-stage funnel annotation strategy, combining LLM-based and human annotation. In the first stage, GPT-4o (OpenAI, 2022a), performs initial annotations, followed by second stage, where crowd-sourced human annotators conduct quality control and filtering to refine the final results. The overall data collection pipeline is illustrated in Figure 2. LLM-based Annotation The collection pipeline is outlined as follows: Step1: Traverse official websites recursively, collecting information on accessible sub-links and their respective pages. Step2: Construct queries based on the provided page information and specified role, such as focusing on the solo page or considering both pages simultaneously. Step3: Verify and filter for legitimate queries that deviate from natural, human-like phrasing, retaining only QA pairs with short answers containing entities. The additional details, including step-specific prompts and case examples, are provided in Appendix D. As illustrated in Figure 2 (b), our dataset construction includes both multi-source and single-source types, corresponding to two types of human information-seeking behaviours within web pages. The single-source type simulates user deeply exploring single piece of information hidden within web pages, while the multi-source type simulates multi-source scenarios where users rely on multiple pages to solve query. Notably, the multi-source QA tasks can not be easily exploited by search engine shortcuts (Mavi et al., 2024). Human Annotation After the synthetic queries are generated by LLM, human annotators can rewrite and calibrate the questions and answers to ensure the QA pairs are correct and consistent."
        },
        {
            "title": "3.2 Data Statistics",
            "content": "Through such data construction method with LLM and human participation, we obtain 680 questionanswer pairs for WebWalkerQA. The annotated case is shown in Figure 10. We will provide comprehensive statistics on WebWalkerQA, categorized by type, domain, and language. two Type WebWalkerQA contains types multi-source and single-source of data: Single-source QAs are labeled as QAs. single-sourcei, where [2, 4], denoting the depth of the corresponding subpage. Similarly, Multi-source QAs are labeled as multi-sourcei, where [2, 8], representing the sum of the depths of the two associated subpages2. 2Taking multi-source6 as an example, it may refer to query constructed from two 3rd level pages or from one page at the 2nd level and another at the 4th level. 3 Figure 2: Data Generation Pipeline for WebWalkerQA. We first collect root official websites across conference, organization, education, and game domains. Then we mimic human behavior by systematically clicking and collecting subpages accessible through sublinks on the root page. Using predefined rules, we leverage GPT4o to generate synthetic QA-pairs based on the gathered information, followed by manual verification to ensure accuracy and relevance. Single-source QAs Multi-source QAs"
        },
        {
            "title": "Hard",
            "content": "80 140 120 80 140 Table 2: Dataset statistics on data difficulty level. In other words, answering this query requires reading both pages simultaneously. Difficulty Level We categorize the questions into three difficulty levels: easy, medium, and hard, based on the value of i. Specifically, single-source2, single-source3, and single-source4 correspond to the easy, medium, for respectively. and hard levels, Similarly, multi-source24, multi-source multi-source46, and multi-source68 correspond to the easy, medium, and hard levels, respectively. The data statistics for the different data types are presented in Table 1. questions, Domain WebWalkerQA encompasses four realworld domains: conference, organization, education, and game. These domains are selected because they provide authoritative information relevant to their respective fields, and their pages contain rich clickable content, offering substantial depth for exploration. Language WebWalkerQA is bilingual dataset that includes both Chinese and English3, reflecting the most widely used and universal languages in real-world web environments. The statistics of WebWalkerQA on domain and language are illustrated in Figure 3. The proportions of the conference, organization, education, 3Classification based on the language of the root webpages. 4 Figure 3: The language and domain distribution. and game domains are 24.0%, 7.9%, 46.3%, and 24.0%, respectively. In terms of language distribution, Chinese and English account for 60.5%, 39.5%, respectively. WebWalkerQA features diverse distribution of languages and domains to ensure comprehensive evaluation."
        },
        {
            "title": "3.3 Web Traversal Task",
            "content": "Formally, given an initial website URL Uroot and query Q, which needs to be answered by exploring the website. The goal of this task is to gather enough information through page traversal to ultimately answer the query Q. The task is to navigate the website to find the corresponding information."
        },
        {
            "title": "3.4 Evaluation",
            "content": "WebWalkerQA can be evaluated from both performance and efficiency perspectives. using questionanswering accuracy (acc.) as the performance metric and the action count (A.C.) of successful agentic executions answering correctly as the efficiency metric. Due to the varying lengths of generated text, it is challenging to perform exact match evaluation, even though we have controlled for short answers. We use GPT-4 as the evaluator, which determines the correctness of responses by comparing the predicted answer with the ground truth using CoT prompting strategy (Wei et al., 2022)4. 4https://api.python.langchain.com/en/latest/langchain/ evaluation.html, Details of the prompt for the evaluator are provided in Appendix Figure 4: The overall framework of WebWalker."
        },
        {
            "title": "4 WebWalker",
            "content": "We introduce WebWalker, multi-agent framework designed to interact with web environments to answer queries. The WebWalker framework consists of two agents: an explorer agent and critic agent. As illustrated in Figure 4, the explorer agent traverses the web pages in Thought-ActionObservation (T , A, O) paradigms. The critic agent updates the memory until sufficient information is accumulated to effectively address the query. The details regarding prompts for both agents are presented in Appendix D.3."
        },
        {
            "title": "4.1 Think then Explore",
            "content": "The explorer agent explores the subpages by interacting with HTML buttons on the page. At time step t, the explorer agent receives an observation Ot from the web environment and takes an action At, following the policy π(AtHt). The observation Ot = (pt, lt) consists of the information from the current page pt and set of clickable sublinks lt = {buttoni}K i=1, where each buttoni describes HTML button information for one of the sublinks and have an associated URL. The action At involves selecting URL of subpage to explore and does not encompass answering the question. Specifically, we utilize the web pages markdown content along with clickable HTML buttons (and corresponding URL) extracted using Beautiful Soup as the observation for the current page. The context Ht = (T1, A1, O1, , Ot1, Tt, At, Ot) represents the sequence of past observations and actions leading up to the current step t. The context will be updated, and this exploration process will continue until the critic agent determines to answer the query or the maximum number of steps is reached."
        },
        {
            "title": "4.2 Think then Critique",
            "content": "Due to the policy π(AtHt) being implicit and the potentially large size of Ht, motivated by pair programming (Williams et al., 2000; Noori and Kazemifard, 2015), we incorporate critic agent into the WebWalker framework to address these challenges. The critic agent operates after each execution of the explorer agent. Its input consists of the query and the explorers current observation. The critic initializes memory to incrementally accumulate relevant information. Formally, at each step, t, following the execution of the explorer agent, the critic agent takes the query and the explorers current observation and action (Ot, At) as input. It then updates the memory M, evaluates whether the gathered information is sufficiently complete to answer the query, and provides an answer once the required information is deemed sufficient. 5 Single-source QA Multi-source QA Overall Backbones Method Easy Medium Hard Easy Medium Hard acc. A.C. acc. A.C. acc. A.C. acc. A.C. acc. A.C. acc. A.C. acc. A.C. GPT-4o Qwen-Plus Qwen-2.5 -7B Qwen-2.5 -14B Qwen-2.5 -32B Qwen-2.5 -72B ReAct Reflexion WebWalker ReAct Reflexion WebWalker ReAct Reflexion WebWalker ReAct Reflexion WebWalker ReAct Reflexion WebWalker ReAct Reflexion WebWalker 53.75 56.25 55.00 48.75 53.75 55. 37.50 37.50 41.25 36.25 46.25 41.25 47.50 42.50 41.25 47.50 57.50 58.75 2.53 2.91 2.97 1.67 3.66 3. 3.36 4.03 3.39 1.86 2.21 2.42 2.21 2.52 2.69 1.68 3.04 2.70 45.00 51.43 50.00 48.57 40.00 47. 18.5 7 25.00 24.71 32.14 34.29 41.43 35.71 32.86 34.29 38.57 44.29 48.57 Closed-Sourced LLMs 30.00 30.83 30. 28.33 24.17 30.00 5.61 5.75 6.02 4.00 5.88 6.13 32.50 35.00 47.50 35.00 47.50 35.00 Open-Sourced LLMs 9.17 11.67 12.50 15.00 15.00 23.33 16.67 16.67 22.50 20.00 28.33 25.83 5.45 4.57 5.93 3.61 4.44 4. 3.55 3.90 5.14 4.04 5.82 5.77 17.50 30.00 18.75 27.50 36.25 30.00 36.25 31.25 27.50 45.00 36.25 35. 3.34 3.88 3.43 2.69 3.79 3.19 4.88 3.48 3.86 2.75 2.83 3.24 3.20 2.65 4.14 2.79 3.88 3. 2.34 3.67 4.00 2.60 3.28 3.89 3.42 2.66 3.00 2.31 2.51 3.95 2.68 2.84 3.13 2.25 3.62 3. 31.43 27.14 34.29 27.86 30.00 27.14 11.43 15.71 20.71 22.86 22.86 22.86 18.57 23.57 25.00 32.14 25.00 29. 3.97 4.13 3.85 3.11 4.07 4.39 3.62 5.45 3.34 3.00 3.34 3.56 3.00 3.12 3.51 3.13 3.60 4. 15.00 16.67 15.83 14.17 15.00 15.00 5.83 4.17 5.83 5.00 5.83 10.00 8.33 5.83 10.00 10.00 12.50 15. 6.77 7.05 6.57 6.55 7.11 7.38 4.57 7.8 7.28 5.00 5.42 6.16 3.70 5.00 6.08 5.41 6.26 7. 33.82 35.29 37.50 33.08 33.23 33.82 16.02 19.11 19.85 22.35 25.14 27.50 25.44 23.26 26.02 30.73 32.50 33. 3.83 4.27 4.67 3.03 4.32 4.36 2.99 4.07 3.94 2.76 3.01 3.60 2.93 3.00 3.90 2.86 4.09 4. Table 3: Main results of three methods across closed-sourced and open-sourced LLMs as the backbone. Acc. and A.C. refer to accuracy and action count, respectively."
        },
        {
            "title": "5.1 Experimental Setting",
            "content": "Baselines We choose widely recognized state-ofthe-art agent frameworks, ReAct and Reflexion, as our baselines. ReAct (Yao et al.) is general paradigm that combines reasoning and acting with LLMs by multiple thought-action-observation steps. Reflexion (Shinn et al., 2024) is singleagent framework designed to reinforce language agents through feedback. Backbones To thoroughly assess the web traversal capabilities of existing LLM-based agents, we select models with context window of at least 128K to accommodate the extensive length of page information. Given the inherent complexity of the task, we opt for models with at least 7B parameters. We validate total number of nine models, including both closed-sourced and open-sourced ones: Closed-sourced LLMs GPT-4o5 (OpenAI, 2022a); Qwen-Plus6 (Team, 2024); Open-sourced LLMs Qwen2.5 series models (Yang et al., 2024) specifically, Qwen2.5-{7,14,32,72}B-Instruct.7 5https://platform.openai.com/docs/models#gpt-4o 6https://www.alibabacloud.com/help/en/model-studio/ 7The LLaMA series models (Dubey et al., 2024) demonstrate limited ability to handle react-format instructions in our preliminary experiments. Figure 5: represents WebWalker using various models as backbones, represents Reflextion with different backbone models, and denotes ReAct employing various backbone models. Implementation Details Considering the context limitation of models, our proposed WebWalker, along with two baselines, all operate in zero-shot setting. We limit the number of actions for the explorer agent to 15, meaning that the explorer agent can explore at most 15 steps. More implementation details are presented in Appendix A."
        },
        {
            "title": "5.2 Main Results",
            "content": "The main results across six LLMs are presented in Table 3. The closed-source models outperform the open-source models in both performance and"
        },
        {
            "title": "Overall",
            "content": "Single-source QA Multi-source QA"
        },
        {
            "title": "Hard",
            "content": "Close Book (No Retrieval) Gemini-1.5-Pro o1-preview 12.50 16.25 7.86 10.00 8.33 9.17 11.25 7. 6.43 10.71 5.00 6.67 8.08 9.85 Doubao Gemini-Search ERNIE-4.0-8K Kimi Tongyi"
        },
        {
            "title": "Naive RAG\nMindSearch",
            "content": "Avg. 45.00 40.00 52.50 77.50 41.25 37.50 15.00 37."
        },
        {
            "title": "Commerical Systems",
            "content": "15.00 32.14 30.00 41.43 45.00 18.33 29.17 28.33 40.83 41.67 13.75 30.00 21.25 26.25 40.00 Open-Sourced Systems 25.71 11.43 24. 24.17 10.83 20.00 8.75 23.42 19.86 8.57 23.57 18.57 26.43 41.43 14.29 12. 18.02 10.00 17.50 30.00 22.50 34.17 12.50 10.00 16.48 16.76 27.94 28.97 37.35 40.73 20.73 11. - Table 4: Accuracy results on Commercial and Open-sourced Searched-enhanced RAG systems. Figure 6: Performance across domains and languages of WebWalker building upon Qwen-14B and Qwen-Plus. efficiency. For open-source models, performance and efficiency improves as the model size increases. Our proposed WebWalker framework outperforms Reflexion, which in turn outperforms React. We only counted the action count (A.C.) from correct executions, and as the model size increases, the A.C. grows, indicating that larger LLMs have enhanced long-range information-seeking ability. Even the best-performing WebWalker using GPT-4o as its backbone does not surpass 40%, highlighting the challenge posed by WebWalkerQA. It can be observed that as the depth increases or the number of sources required increases, the difficulty of acquiring the information needed to resolve the query becomes greater, resulting in decline in accuracy performance. The performance distribution of accuracy and action count for different methods across various models is shown in Figure 5. The further towards the top-right corner, the more effective and prolonged the web traversal becomes. We observe that increasing the model size or introducing reflection on the process of each action can address certain Figure 7: Predication distribution of WebWalker and React method building on Qwen-14B and Qwen-Plus. problems requiring multi-step solutions, thereby enabling long-distance task-solving capabilities in web traversal tasks."
        },
        {
            "title": "5.3 Results across Domains and Languages",
            "content": "WebWalkerQA is bilingual dataset encompassing both Chinese and English and spans multiple domains, including games, conferences, education, and organizations. The performance across different domains and languages is shown in Figure 6. In the domain of conference, the framework demonstrates relatively superior performance, likely due to the more explicit and directive nature of the button information, which facilitates more straightforward inferences. The framework performs similarly in both Chinese and English, as the models we employed are both pre-trained and supervisedfine-tuned in bilingual setting."
        },
        {
            "title": "5.4 Error Assessment",
            "content": "In Table 3, we only report the action count of the successful agentic executions (A.C.). For incorrect execution, errors can also be categorized into three types: refusal to answer or locating wrongly, reasoning error, and exceeding the maximum number of steps K. The prediction distribution is shown in Figure 7. The model with relatively small number of parameters using the ReAct framework lacks the capacity to explore the depth of information, making judgments within just few iterations of taking action, regardless of whether relevant information has been found. It tends to give up and exhibits characteristics of impatience. Introducing memory to manage the long context, along with an increase in model parameters, provides evidence that this phenomenon stems from the interference of long contexts having noisy information and the inherent capabilities of the model itself, consistent with the analysis drawn in 5.2. Some errors are categorized as reasoning errors, where the golden page has been found in the visited pages but is still incorrectly marked. This underscores the challenge of reasoning on page information in certain cases."
        },
        {
            "title": "6.1 RAG Performance on WebWalkerQA",
            "content": "We evaluate the performance of RAG systems in tackling WebWalkerQAs challenges, specifically, whether they can retrieve deep information, presented in Table 4. We first evaluate the performance under Close Book settings using the state-of-the-art model OpenAI o1 (OpenAI, 2024) and Gemini-1.5-Pro without retrieval. We then access the performance of several commercial and open-sourced RAG systems9. Without performing the search, even the strongest models exhibit very poor performance. WebWalkerQA is built on official websites with dynamically updated information, while pre-trained models rely on static knowledge limited by cutoff date and lack dynamic updates10. Both commercial and open-sourced RAG systems exhibit relatively poor performance on WebWalkerQA, with the best result coming from Tongyi, which only reaches 40%. Commercial RAG systems are typ8The corresponding case is presented in Appendix F.1. 9The commercial RAG systems are accessed through business-oriented API. The details of RAG systems are provided in Appendix B. 10The case study is shown in Appendix F.2. ically modular, consisting of various components such as rewrite, router, reranker, and others. Some systems, like ERNIE, may have stronger search capabilities for Chinese, resulting in higher values. For open-sourced RAG systems, Multi-source queries have lower accuracy than Single-source queries, which validates the challenge posed by WebWalkerQA, as search engines are unable to retrieve all relevant information in one or several single horizontal search attempts. Furthermore, as the difficulty increases, e.g. the depth of information growing deeper, the performance tends to deteriorate. Overall, search engines still face challenges when retrieving content that is buried deeper. Findings (i): RAG systems struggle with key challenges that require effective web traversal."
        },
        {
            "title": "6.2 WebWalker Combined with RAG System",
            "content": "Figure 8: Performance under standard RAG and RAG combined with WebWalker configurations. SS and MS denote single-source and multi-source QAs. The standard RAG system can be viewed as horizontal search for relevant documents in response to query, while WebWalker can be considered as vertical exploration approach. WebWalker can seamlessly integrate into standard RAG systems to acquire deep information and enhance problem-solving capabilities. We integrate WebWalker building upon Qwen-2.5-Plus into the naive RAG system, and the detailed results are shown in Figure 8. The core contribution of WebWalker is providing useful information for question answering; specifically, the memory of the critic agent is append to the relevant documents to aid in generation. It is observed that, after the integration, performance has improved across all difficulty levels, especially in the multi-source category. 8 Findings (ii): WebWalker can be module in agentic RAG system, enabling vertical exploration. Figure 9: Overall performance on WebWalker and RAG combined WebWalker at varying values of K, using Qwen-Plus as backbones."
        },
        {
            "title": "6.3 Scaling Up on Action Count K",
            "content": "Previous work (Yue et al., 2024) explored the inference scaling laws for the RAG system by examining the impact of increasing retrieved documents. We scale up the amount of {5, 10, 15, 20, 25} to study the impact of scaling during the inference phase when tracing source information. Figure 9 shows the results of scaling up, where larger values of lead to better performance, validating the feasibility of vertical scaling within certain range. Findings (iii): Scaling the process of digging through links could represent potential direction for vertical exploration in RAG systems."
        },
        {
            "title": "7 Conclusion",
            "content": "We introduce WebWalkerQA, benchmark for evaluating LLMs web traversal abilities in complex, multi-step information-seeking tasks. We also proposed WebWalker, multi-agent framework that mimics human-like web navigation, combining exploration and critique. Experiments show that WebWalkerQA effectively challenges RAG systems, and combining RAG with WebWalker improves web navigation performance. Our work highlights the importance of deep, vertical exploration in web-based tasks, paving the way for more scalable and reliable LLM-based information retrieval integrated with RAG."
        },
        {
            "title": "Limitations and Discussion",
            "content": "the web-agent domain, similar to benchmarks such as AssistantBench (Yoran et al., 2024) (214) and MMIna (Zhang et al., 2024c) (1,050), GAIA (Mialon et al., 2024) (466), our proposed WebWalkerQA currently comprises 680 high-quality QA pairs. Additionally, we possess collection of approximately 14k silver QA pairs, which, although not yet carefully human-verified, can serve as supplementary training data to enhance agent performance, leaving room for further exploration. Multimodal Environment: In this work, we only utilize HTML-DOM to parse clickable buttons. In fact, visual modalities, such as screenshots, can also assist and provide more intuitive approach (Nguyen et al., 2024; Zhang et al., 2024a; He et al., 2024b). We leave this for future work. Agent Tuning: WebWalker is driven by prompting without additional training. We can use agent tuning to help LLMs learn web traversal. This involves fine-tuning models with golden trajectories, enabling them to take effective actions for completing information-seeking tasks (Zeng et al., 2024; Chen et al., 2024b; Zhang et al., 2024b; Qiao et al., 2024; Zhu et al., 2024). Better Integration with RAG Systems: In , the root url is provided for the WebWalker to execute. To better integrate with the RAG system, one approach could be to first rewrite the query within the RAG system to refine the search, directing it to the querys official websites likely to contain relevant information. The WebWalker can then be used to extract useful information. Both the knowledge retrieved from the RAG system and the information mined by the WebWalker can be combined as augmented retrieval knowledge for generation, leading to better result. WebWalker can function independently as web information retrieval assistant for given webpage or seamlessly integrate with RAG systems to expand their scope. Under the agentic RAG paradigm, the click action proves to be highly effective."
        },
        {
            "title": "References",
            "content": "Tamer Abuelsaad, Deepak Akkil, Prasenjit Dey, Ashish Jagmohan, Aditya Vempaty, and Ravi Kokku. Agente: From autonomous web navigation to foundational In NeurIPS design principles in agentic systems. 2024 Workshop on Open-World Agents. We discuss the following limitations: Dataset Size: Due to the complexity of queries in Zehui Chen, Kuikun Liu, Qiuchen Wang, Jiangning Liu, Wenwei Zhang, Kai Chen, and Feng Zhao. 2024a. 9 Mindsearch: Mimicking human minds elicits deep ai searcher. arXiv preprint arXiv:2407.20183. Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, and Feng Zhao. 2024b. Agent-FLAN: Designing data and methods of effective agent tuning for large language models. In Findings of the Association for Computational Linguistics: ACL 2024, pages 9354 9366, Bangkok, Thailand. Association for Computational Linguistics. Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. 2024. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. 2023. Mind2web: Towards generalist agent for the web. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Yang Deng, Xuan Zhang, Wenxuan Zhang, Yifei Yuan, See-Kiong Ng, and Tat-Seng Chua. 2024. On the multi-turn instruction following for conversational web agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 87958812, Bangkok, Thailand. Association for Computational Linguistics. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, and et al. 2024. The llama 3 herd of models. CoRR, abs/2407.21783. Hiroki Furuta, Kuang-Huei Lee, Ofir Nachum, Yutaka Matsuo, Aleksandra Faust, Shixiang Shane Gu, and Izzeddin Gur. 2024. Multimodal web navigation with instruction-finetuned foundation models. In The Twelfth International Conference on Learning Representations. Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. 2024a. WebVoyager: Building an end-toend web agent with large multimodal models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 68646890, Bangkok, Thailand. Association for Computational Linguistics. Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Hongming Zhang, Tianqing Fang, Zhenzhong Lan, and Dong Yu. 2024b. Openwebvoyager: Building multimodal web agents via iterative real-world exploration, feedback and optimization. arXiv preprint arXiv:2410.19609. Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. 2024. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1428114290. Peter Humphreys, David Raposo, Tobias Pohlen, Gregory Thornton, Rachita Chhaparia, Alistair Muldal, Josh Abramson, Petko Georgiev, Adam Santoro, and Timothy Lillicrap. 2022. data-driven approach for learning to control computers. In International Conference on Machine Learning, pages 94669482. PMLR. Iat Long Iong, Xiao Liu, Yuxuan Chen, Hanyu Lai, Shuntian Yao, Pengbo Shen, Hao Yu, Yuxiao Dong, and Jie Tang. 2024. Openwebagent: An open toolkit to enable web agents on large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 7281. Geunwoo Kim, Pierre Baldi, and Stephen McAleer. 2024. Language models can solve computer tasks. Advances in Neural Information Processing Systems, 36. Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. In ICLR 2024 Workshop on Large Language Model (LLM) Agents. Jing Yu Koh, Stephen McAleer, Daniel Fried, and Ruslan Salakhutdinov. 2024. Tree search for language model agents. arXiv preprint arXiv:2407.01476. Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, et al. 2024. Autowebglm: large language model-based web navigating agent. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 52955306. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:94599474. Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. 2018. Reinforcement learning on web interfaces using workflow-guided exploration. arXiv preprint arXiv:1802.08802. Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023. Webglm: Towards an efficient web-enhanced question answering system with human preferences. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 45494560. Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. In The Twelfth International Conference on Learning Representations. Vaibhav Mavi, Anubhav Jangra, and Adam Jatowt. 2024. Multi-hop question answering. Preprint, arXiv:2204.09140. Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. 2024. GAIA: benchmark for general AI assistants. In The Twelfth International Conference on Learning Representations. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webgpt: Browser-assisted questionanswering with human feedback. arXiv preprint arXiv:2112.09332. Dang Nguyen, Jian Chen, Yu Wang, Gang Wu, Namyong Park, Zhengmian Hu, Hanjia Lyu, Junda Wu, Ryan Aponte, Yu Xia, et al. 2024. Gui agents: survey. arXiv preprint arXiv:2412.13501. Fariba Noori and Mohammad Kazemifard. 2015. Simulation of pair programming using multi-agent and mbti personality model. In 2015 Sixth International Conference of Cognitive Science (ICCS), pages 29 36. IEEE. OpenAI. 2022a. Gpt-4 system card. OpenAI. 2022b. Introducing ChatGPT. OpenAI. 2024. Introducing openai o1. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Jiang, Chengfei Lv, and Huajun Chen. 2024. AutoAct: Automatic agent learning from scratch for QA via self-planning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 30033021, Bangkok, Thailand. Association for Computational Linguistics. Revanth Gangi Reddy, Sagnik Mukherjee, Jeonghwan Kim, Zhenhailong Wang, Dilek Hakkani-Tur, and Heng Ji. 2024. Infogent: An agent-based framework for web information aggregation. arXiv preprint arXiv:2410.19054. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2024. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36. Yueqi Song, Frank Xu, Shuyan Zhou, and Graham Neubig. 2024. Beyond browsing: Api-based web agents. arXiv preprint arXiv:2410.16464. Jiejun Tan, Zhicheng Dou, Wen Wang, Mang Wang, Weipeng Chen, and Ji-Rong Wen. 2024. Htmlrag: Html is better than plain text for modeling retrieved knowledge in rag systems. arXiv preprint arXiv:2411.02959. Qwen Team. 2024. Qwen2.5: party of foundation models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Laurie Williams, Robert Kessler, Ward Cunningham, and Ron Jeffries. 2000. Strengthening the case for pair programming. IEEE software, 17(4):1925. Kevin Xu, Yeganeh Kordi, Tanay Nayak, Ado Asija, Yizhong Wang, Kate Sanders, Adam Byerly, Jingyu Zhang, Benjamin Van Durme, and Daniel Khashabi. 2024. Tur[k]ingbench: challenge benchmark for web agents. Preprint, arXiv:2403.11905. Nancy Xu, Sam Masling, Michael Du, Giovanni Campagna, Larry Heck, James Landay, and Monica Lam. 2021. Grounding open-domain instructions to automate web support tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 10221032, Online. Association for Computational Linguistics. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, and et al. 2024. Qwen2 technical report. CoRR, abs/2407.10671. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2022. Webshop: Towards scalable realworld web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:2074420757. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations. Ori Yoran, Samuel Joseph Amouyal, Chaitanya Malaviya, Ben Bogin, Ofir Press, and Jonathan Berant. 2024. AssistantBench: Can web agents solve realistic and time-consuming tasks? In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 89388968, Miami, Florida, USA. Association for Computational Linguistics. 11 Zhenrui Yue, Honglei Zhuang, Aijun Bai, Kai Hui, Rolf Jagerman, Hansi Zeng, Zhen Qin, Dong Wang, Xuanhui Wang, and Michael Bendersky. 2024. Inference scaling for long-context retrieval augmented generation. arXiv preprint arXiv:2410.04343. Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. 2024. AgentTuning: Enabling generalized agent abilities for LLMs. In Findings of the Association for Computational Linguistics: ACL 2024, pages 30533077, Bangkok, Thailand. Association for Computational Linguistics. Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Qingwei Lin, Saravan Rajmohan, et al. 2024a. Large language model-brained gui agents: survey. arXiv preprint arXiv:2411.18279. Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen, Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang, and Weiming Lu. 2024b. Agentpro: Learning to evolve via policy-level reflection and optimization. arXiv preprint arXiv:2402.17574. Ziniu Zhang, Shulin Tian, Liangyu Chen, and Ziwei Liu. 2024c. Mmina: Benchmarking multihop multimodal internet agents. Preprint, arXiv:2404.09992. Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v (ision) is generalist web agent, if grounded. In Forty-first International Conference on Machine Learning. Boyuan Zheng, Boyu Gou, Scott Salisbury, Zheng Du, Huan Sun, and Yu Su. 2024. WebOlympus: An open platform for web agents on live websites. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 187197, Miami, Florida, USA. Association for Computational Linguistics. Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. 2024a. Webarena: realistic web environment for building autonomous agents. In The Twelfth International Conference on Learning Representations. Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, et al. 2023. Agents: An open-source framework for autonomous language agents. arXiv preprint arXiv:2309.07870. Wangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, Shuai Wang, Xiaohua Xu, Ningyu Zhang, et al. 2024b. Symbolic learning enables self-evolving agents. arXiv preprint arXiv:2406.18532. Yuqi Zhu, Shuofei Qiao, Yixin Ou, Shumin Deng, Ningyu Zhang, Shiwei Lyu, Yue Shen, Lei Liang, Jinjie Gu, and Huajun Chen. 2024. Knowagent: Knowledge-augmented planning for llm-based agents. Preprint, arXiv:2403.03101."
        },
        {
            "title": "Annotated Data Format",
            "content": "In this study, we utilize Qwen-Agent11 as the foundational codebase for building and developing the baselines proposed WebWalker. The details of LLM hyperparameters for generation are as follows: topp = 0.8. We sincerely thank the contributors and maintainers of ai4crawl12 for their open-source tool, which helped us get web pages in Markdown-like format. We release the code of WebWalker in our GitHub Codebase."
        },
        {
            "title": "B Details for RAG Systems",
            "content": "We select five mainstream commercial systems and two open-source systems for evaluation. B.1 Commercial Systems Doubao13, ERNIE-4.0-8K14, Tongyi, Kimi, and Gemini-Search are all accessed through their business-oriented API interfaces to ensure reproducibility. The detailed configuration of each API can be found in our codebase. B.2 Open-sourced Systems (a) Mindsearch (Chen et al., 2024a) is to mimic the human minds in web information seeking and integration, which can be instantiated by multiagent framework consisting of WebPlanner and WebSearcher. (b) Naive RAG built from scratch We use Google to query the relevant terms and concatenate the information from the Top-10 returned links with the query to provide instructions for the Qwen-Plus to generate response."
        },
        {
            "title": "C Annotated Case",
            "content": "An annotated case is shown in Figure 10. The WebWalkerQA dataset is available at HuggingFace Datasets."
        },
        {
            "title": "D Details on Annotation",
            "content": "D.1 Sources of Root Page The root page is initially identified through Google search using keywords such as conference official website or game official website, followed by manual filtering. For the education domain, we choose the official websites of various university computer science departments, closely 11https://github.com/QwenLM/Qwen-Agent 12https://github.com/unclecode/crawl4ai 13https://www.volcengine.com/docs/82379/1302004 14https://cloud.baidu.com/doc/WENXINWORKSHOP/s/ clntwmv7t 1 ## JSON Format 2 The keys in the JSON include: 3 Question, Answer, Root_Url, and Info. The Info field contains 4 more detailed information, including Hop, Domain, Language, 5 Difficulty_Level, Source Website, and Golden_Path. 6 ``` 7 { 8 \"Question\": \"When is the paper submission deadline for the ACL 2025 Industry Track, and what is the venue address for the conference?\", 9 10 11 12 13 14 15 16 17 \"Answer\": \"The paper submission deadline for the ACL 2025 Industry Track is March 21, 2025. The conference will be held in Brune-Kreisky-Platz 1.\", \"Root_Url\": \"https://2025.aclweb.org/\", \"Info\":{ \"Hop\": \"multi-source\", \"Domain\": \"Conference\", \"Language\": \"English\", \"Difficulty_Level\": \"Medium\", \"Source_Website\": [\"https://2025.aclweb.org/calls/ industry_track/\",\"https://2025.aclweb.org/venue/\"], \"Golden_Path\": [\"root->call>student_research_workshop\" , \"root->venue\"] } 18 19 } 20 ``` Figure 10: JSON-format case in WebWalkerQA. reflecting real-world scenarios. The distribution of the domain is shown in Figure 3. D.2 Details on Prompts for Annotation The prompts for GPT-4o-based initial annotation are presented below. Prompts for Multi-source Data Annotation"
        },
        {
            "title": "Question Generate",
            "content": "You are professional web content analyst. Based on the provided material, construct query statement: Sublink 1 URL; Sublink 1 INFO Sublink 2 URL; Sublink 2 INFO ... Sublink URL; Sublink INFO ### Requirements: 1. **Core Goal of the Query**: Create multistep standalone query where the user needs to integrate information from at least two sublinks to find the final answer. The answer should be single, clear, concise, and precise entity. 2. **Relevance of Sublinks**: The selected sublinks must have an intrinsic connection, and the answer should be derived by combining information from these two sublinks. 3. **Logical and Complex**: The constructed query should be as complex and specific as possible , challenging, and can leverage time, sequence, or commonly mentioned topics to construct naturally coherent reasoning process. Avoid questions about browsing history, browsing paths, etc., which have no practical value. 4. **Accuracy of the Answer**: Ensure the answer is accurate, concise, and closely connected to the logical chain constructed in the query. Please return in JSON format, structured as follows: 13 { \"sublink_reason\": \"Describe why these specific sublinks were chosen and how they are interrelated.\", \"sublinks\": [\"Selected sublink URL\", \"Selected sublink URL\"], \"reason\": \"Explain the reason for designing this query and how it encourages the user to engage in multistep reasoning.\", \"query\": \"Your query statement\", \"answer\": \"The answer to the query\" } Sublink 1 URL; Sublink 1 INFO Sublink 2 URL; Sublink 2 INFO ... Sublink URL; Sublink INFO Question-Answer Verify You will act as strict judge. You need to evaluate whether the given query can be accurately answered only by combining the information from two documents (doc1 and doc2) and the provided answer. Additionally, check if the answer is concise (as an entity or judgment) and correct. If the answer is incorrect, can be answered using only one document, or is not concise enough, you should return false. If any document (doc1 or doc2) does not contain the necessary key information for the answer and only provides context for the query, you should return false. If any document merely provides query background information unrelated to the answer and does not require combining information from both documents, you should return false. If the answer is long answer and not of an entity type, you should return false. If the query is unnatural, doesn't appear as complete query, or has harsh tone, you should return false. Each question should require combining information from both documents, meaning the answer results from multihop reasoning or multi step reasoning, and it is concise for you to return true. You are very strict, and any case failing to meet the above criteria should result in false. Please return your result in JSON format as follows: { \"reason\": \"Consider each of the conditions above in sequence to assess whether the query and answer meet the criteria. If they do meet the criteria, list the helpful parts from each doc for answering the question.\", \"decision\": \"true/false\" } {Doc1 INFO}; {Doc2 INFO} Prompts for Single-source Data Annotation"
        },
        {
            "title": "Question Generate",
            "content": "Question-Answer Verify You will act as strict judge. You need to assess whether current knowledge from doc2 is required to accurately answer the given query based on the two provided documents (doc1 and doc2) and the given answer. Doc1 represents known knowledge, while doc2 represents current knowledge. Your task is to determine if the answer relies on doc2 to be accurately provided. Additionally, evaluate whether the answer is short (an entity or judgment) and correct. If the answer is incorrect or not concise, return false . If the necessary key information is found in the known knowledge doc1, also return false. If the answer is long answer and not of entity type , return false. If the query is unnatural, not complete query, or awkwardly phrased, return false. The answer should result from multihop reasoning or multistep reasoning, where multistep reasoning indicates that the generated query is challenging and requires reasoning or calculation to answer, and only if the answer is concise should you return true. You are extremely strict, and any requirements not met should result in return of false. Please return the result in JSON format as follows: { \"reason\": \"Evaluate against the above conditions step by step, considering whether the query and answer meet the conditions. Use English to justify, and if they do, list the sections from doc2 that assist in answering the query.\", \"decision\": \"true/false\" } D.3 Details Prompts for Agents The prompts for the Expoloer Agent Agent are shown below. and Critic"
        },
        {
            "title": "The Expoloer Agent",
            "content": "Digging through the buttons to find quailty sources and the right information. You have access to the following tools: {tool_descs} Use the following format: Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [{ tool_names}] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can be repeated zero or more times) Begin! {query}"
        },
        {
            "title": "The Critic Agent",
            "content": "14 CoT-QA Evaluator You are teacher grading quiz. You are given question, the context the question is about, and the student's answer. You are asked to score the student's answer as either CORRECT or INCORRECT, based on the context. Write out in step by step manner your reasoning to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset. Example Format: QUESTION: question here CONTEXT: context the question is about here STUDENT ANSWER: student's answer here EXPLANATION: step by step reasoning here GRADE: CORRECT or INCORRECT here Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! QUESTION: {query} CONTEXT: {answer} STUDENT ANSWER: {result} EXPLANATION GRADE:\"\"\" Figure 11: The prompt for evaluation. even when the source page is successfully located, errors might still occur if the system fails to process the time correctly. F.2 Time Cut-off As shown in Table 6, the cutoff date for o1s temporal data is October 2023, rendering it unable to provide answers regarding web information published beyond this point. Critic You are critic agent. Your task is to analyze the given observation and extract information relevant to the current query. You need to decide if the observation contains useful information for the query. If it does, return JSON object with \" usefulness\" value of true and an \"information\" field with the relevant details. If not, return JSON object with \"usefulness\" value of false. **Input:** Query: \"<Query>\" Observation: \"<Current Observation>\" **Output (JSON):** { \"usefulness\": true, \"information\": \"<Extracted Useful Information>\" } Or, if the observation does not contain useful information: { \"usefulness\": false } Query: {Query} Observation: {Observation} Answer You are critic agent. Your task is to evaluate whether the accumulated useful information is sufficient to answer the current query. If it is sufficient, return JSON object with \"judge\" value of true and an \"answer\" field with the answer. If the information is insufficient, return JSON object with \"judge\" value of false. **Input:** Query: \"<Query>\" Accumulated Information: \"<Accumulated Useful Information>\" **Output (JSON):** { \"judge\": true, \"answer\": \"<Generated Answer>\" } Or, if the information is insufficient to answer the query: { \"judge\": false } Query: {Query} Accumulated Information: {Information}"
        },
        {
            "title": "E Details for Evaluation",
            "content": "E.1 Evaluator The evaluator prompt is shown in Figure 11."
        },
        {
            "title": "F Case Study",
            "content": "F.1 Reasoning Error As shown in Table 5, this question requires first locating the webpage related to the Inclusive Connections Lounge, followed by comprehensive understanding of the information on the page to calculate the required time. In such cases, it is also necessary to account for the systems ability to perform time calculations or reasoning. Consequently,"
        },
        {
            "title": "Question",
            "content": "https://www.mrs.org/ How many hours in total would person spend if they attended the Inclusive Connections Lounge activities from December 1 to 6, 2024, at the MRS Fall Meeting?"
        },
        {
            "title": "Answer",
            "content": "66 hours"
        },
        {
            "title": "Source Website",
            "content": "https://www.mrs.org/meetings-events/annual-meetings/ 2024-mrs-fall-meeting/meeting-events/broadening-participation/ inclusive-connections-lounge"
        },
        {
            "title": "Website Information",
            "content": "Table 5: The case requiring reasoning capability in web traversal task. Question Where and when will the 2025 MRS Fall Meeting take place?"
        },
        {
            "title": "Answer",
            "content": "Boston, Massachusetts; November 30 to December 5, 2025. Prediction As of my knowledge cutoff in October 2023, the MRS has not yet announced the exact dates or location for the 2025 MRS Fall Meeting. Table 6: The case of time cutoff in predictions generated by o1."
        }
    ],
    "affiliations": [
        "Alibaba Group"
    ]
}