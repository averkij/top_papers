{
    "paper_title": "Skywork Open Reasoner 1 Technical Report",
    "authors": [
        "Jujie He",
        "Jiacai Liu",
        "Chris Yuhao Liu",
        "Rui Yan",
        "Chaojie Wang",
        "Peng Cheng",
        "Xiaoyu Zhang",
        "Fuxiang Zhang",
        "Jiacheng Xu",
        "Wei Shen",
        "Siyuan Li",
        "Liang Zeng",
        "Tianwen Wei",
        "Cheng Cheng",
        "Bo An",
        "Yang Liu",
        "Yahui Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The success of DeepSeek-R1 underscores the significant role of reinforcement learning (RL) in enhancing the reasoning capabilities of large language models (LLMs). In this work, we present Skywork-OR1, an effective and scalable RL implementation for long Chain-of-Thought (CoT) models. Building on the DeepSeek-R1-Distill model series, our RL approach achieves notable performance gains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench from 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%) for the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and Qwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable results on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models demonstrate competitive reasoning capabilities among models of similar size. We perform comprehensive ablation studies on the core components of our training pipeline to validate their effectiveness. Additionally, we thoroughly investigate the phenomenon of entropy collapse, identify key factors affecting entropy dynamics, and demonstrate that mitigating premature entropy collapse is critical for improved test performance. To support community research, we fully open-source our model weights, training code, and training datasets."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 2 1 3 2 2 . 5 0 5 2 : r Skywork Open Reasoner 1 Technical Report Jujie He, , Jiacai Liu , Chris Yuhao Liu , Rui Yan , Chaojie Wang , Peng Cheng , Xiaoyu Zhang , Fuxiang Zhang , Jiacheng Xu , Wei Shen , Siyuan Li , Liang Zeng , Tianwen Wei , Cheng Cheng , Bo An , Yang Liu , and Yahui Zhou Skywork AI, Kunlun Inc GitHub: https://github.com/SkyworkAI/Skywork-OR1 HuggingFace: https://huggingface.co/Skywork/Skywork-OR1-32B Abstract The success of DeepSeek-R1 underscores the significant role of reinforcement learning (RL) in enhancing the reasoning capabilities of large language models (LLMs). In this work, we present Skywork-OR1, an effective and scalable RL implementation for long Chain-of-Thought (CoT) models. Building on the DeepSeek-R1-Distill model series, our RL approach achieves notable performance gains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench from 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%) for the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and Qwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable results on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models demonstrate competitive reasoning capabilities among models of similar size. We perform comprehensive ablation studies on the core components of our training pipeline to validate their effectiveness. Additionally, we thoroughly investigate the phenomenon of entropy collapse, identify key factors affecting entropy dynamics, and demonstrate that mitigating premature entropy collapse is critical for improved test performance. To support community research, we fully open-source our model weights, training code, and training datasets. Figure 1: The performance curve of Skywork-OR1-32B during RL training for AIME 2024 and AIME 2025. The red stars indicate the selected final checkpoints. Equal contribution. Corresponding author: jujie.he@kunlun-inc.com"
        },
        {
            "title": "3 MAGIC in Skywork-OR1",
            "content": "3.1 MAGIC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Effectiveness of MAGIC Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.1 Data Mixture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.2 Multi-Stage Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.3 Advantage Mask for Truncated Responses . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.4 High-temperature Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.5 Adaptive Entropy Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.6 No KL Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "4 Empirical Studies on Mitigating Policy Entropy Collapse",
            "content": "4.1 Ablation Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Premature Entropy Collapse Generally Manifests as Worse Performance . . . . . . . . . . . . 4.3 The Impact of Rollout-Diversity-Related Hyperparameters . . . . . . . . . . . . . . . . . . . . 4.4 The Impact of Off-policy Update by Increasing NSGD . . . . . . . . . . . . . . . . . . . . . . 4.5 Preventing Premature Entropy Collapse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Empirical Studies on Training Resource Allocation 5.1 5.2 Improving Training Efficiency with Fixed Computational Resources . . . . . . . . . . . . . . . Improving Test Performance with More Computational Resources . . . . . . . . . . . . . . . . 6 Dataset Preparation 6.1 Data Source Selection and Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2 Model-Aware Difficulty Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.3 Quality Assessment via Human and LLM-as-a-Judge . . . . . . . . . . . . . . . . . . . . . . . 7 Math & Code Verifiers 7.1 Math Verifiers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2 Code Sandboxes 8 Experiments 8.1 Training and Evaluation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.2 Evaluation Results of Skywork-OR1 models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 Conclusion 6 7 7 9 9 10 11 14 16 16 18 19 20 20 21 24 27 28 29 30 31 32 33 34 34 35 35 36 37"
        },
        {
            "title": "Introduction",
            "content": "In recent months, post-training techniques based on reinforcement learning (RL) have achieved groundbreaking success in enhancing the reasoning capabilities of large language models (LLMs). Representative models such as OpenAI-o1 [9], DeepSeek-R1 [6], and Kimi-K1.5 [24] demonstrate RLs remarkable ability to significantly improve performance in mathematics and coding. While prior RL approaches have primarily relied on Monte Carlo Tree Search (MCTS) or Process Reward Models (PRMs) to improve reasoning over supervised fine-tuning (SFT) models, the success of DeepSeek-R1 demonstrates conclusively that online RL with simple rule-based reward is sufficient to substantially enhance the reasoning capabilities of base models. As model capabilities continue to advance, Chains-of-Thought (CoT) have grown progressively longer. For example, the DeepSeek-R1-Distill model series [6] generates CoT sequences averaging over 10K tokens on the AIME24 benchmark, significantly surpassing earlier popular SFT models such as the Qwen 2.5 model series [33] and the Llama 3.1 model series [5]. Despite several reproduction efforts (e.g., Logic-RL [31], Open-Reasoner-Zero [8], DAPO [34], VAPO [35]) following the success of DeepSeek-R1, most have focused on applying RL to base models rather than to long CoT models that have already undergone SFT. As result, it remains unclear how to improve the reasoning abilities of long CoT models using RL in an efficient and scalable manner. While recent works such as DeepScaleR [17], Light-R1 [28], and DeepCoder [16] have made preliminary progress toward efficient RL optimization for long CoT models, their analyses do not systematically disentangle the contributions of distinct algorithmic components during RL training. In this work, we introduce Skywork Open Reasoner 1 (abbreviated as Skywork-OR1 throughout the report), an efficient and scalable RL recipe for long CoT models. Our experiments are based on the DeepSeekR1-Distill model series and open-source datasets with rigorous preprocessing and filtering. As shown in Figure 1 and Table 13, the Skywork-OR1 model series achieves significant performance improvements over base models, demonstrating the effectiveness of our RL implementation. Specifically, Skywork-OR1-32B achieves scores of 82.2 on AIME24, 73.3 on AIME25, and 63.0 on LiveCodeBench[10] (2024-08 - 2025-02), outperforming DeepSeek-R1 and Qwen3-32B in the math domain. Skywork-OR1-7B achieves 70.2 on AIME24, 54.6 on AIME25, and 47.6 on LiveCodeBench, exhibiting competitive performance relative to similarly sized models in both math and coding tasks. Our previously released model, Skywork-OR1-Math-7B, also delivers strong performance among similarly sized models, scoring 69.8 on AIME24, 52.3 on AIME25, and 43.6 on LiveCodeBench. We conducted exhaustive ablation experiments to validate the effectiveness of the core components in the training pipeline. Balancing exploration and exploitation is crucial in RL training [22]. We conducted comprehensive study on premature entropy collapse, phenomenon associated with excessive exploitation, and found that mitigating premature entropy collapse is essential for achieving better test performance. Through exhaustive ablation experiments, we identified key factors that influence entropy dynamics. To ensure full reproducibility and support ongoing research within the LLM community, we release all of our training resources, including source code, the post-training dataset, and model weights . Furthermore, we conducted extensive ablation studies across both data and algorithmic dimensions to elucidate effective RL implementations for long CoT models. As follow-up to our previously released Notion blog post [7], we present this more detailed technical report, with our key findings summarized as follows: https://github.com/SkyworkAI/Skywork-OR1 https://huggingface.co/datasets/Skywork/Skywork-OR1-RL-Data https://huggingface.co/Skywork/Skywork-OR1-7B https://huggingface.co/Skywork/Skywork-OR1-32B"
        },
        {
            "title": "Data Collection",
            "content": "1. To ensure stable and effective training, it is crucial to incorporate problems from diverse set of sources. We observe that, in the absence of consistent quality assessment and filtering procedures, previously successful datasets exhibit several failure modes with larger models (Section 6). 2. Rigorous filtering and quality control of training data significantly accelerate learning. Our proposed data mixture, constructed with stringent filtering criteria, outperforms baseline mixture assembled with looser quality thresholds (Section 3.2.1)."
        },
        {
            "title": "Training Strategy",
            "content": "1. Multi-stage training significantly improves training efficiency in the initial phase while preserving scalability for later stages (Section 3.2.2). 2. Addressing noisy training signals introduced by truncated trajectories in Stage does not lead to better scaling at large context lengths, e.g., 32K (Section 3.2.3). 3. High-temperature sampling results in lower test accuracy during the early training steps but ultimately yields greater performance improvements (Section 3.2.4). 4. On-policy training mitigates entropy collapse and leads to higher test performance (Section 4). Loss Function 1. Adaptive entropy control effectively keeps the models entropy lower-bounded by the target entropy throughout training, maintaining the models exploration ability and high learning plasticity, with test performance steadily improving (Section 3.2.5). 2. The KL penalty hinders further improvements in test performance during multi-stage training. Therefore, we omit KL loss from our training pipeline (Section 3.2.6). Empirical Results of Our Entropy Collapse Study 1. Faster entropy collapse generally correlates with poorer test performance (Section 4.2). Appropriate entropy control that mitigates premature convergence can improve test outcomes (Section 4.5). 2. Increasing rollout diversity by enlarging the batch and group sizes has only minor effects on entropy dynamics (Section 4.3), whereas using higher sampling temperature significantly impacts initial entropy and learning dynamics (Section 3.2.4). 3. Off-policy training via increased mini-batches or data reuse accelerates entropy collapse and generally leads to degraded test performance compared to on-policy updates, due to the introduction of off-policy data (Section 4.4). 4. The entropy loss exhibits high sensitivity to both the training data and the coefficient. By either adaptively adjusting the entropy loss coefficient or applying clip-higher trick with an appropriate clip ratio, entropy dynamics become slower and more stable, leading to improved test performance. Nevertheless, entropy still converges faster than in on-policy training (Section 4.5). Organization In Section 2, we introduce the preliminaries of several important policy optimization methods in RL. Section 3 elaborates on our training pipeline, including comprehensive ablation studies that validate the effectiveness of its core components. systematic investigation of entropy collapse is presented in Section 4, demonstrating that mitigating premature policy convergence is critical in RL training for enhancing exploration and achieving better test performance. We discuss training resource allocation in Section 5. The implementation details of our training data preparation and rule-based reward are provided in Sections 6 and 7. Finally, Section 8 presents comprehensive description of the training and evaluation details for our three released models: Skywork-OR1-Math-7B, Skywork-OR1-7B, and Skywork-OR1-32B. Figure 2: Performance of Skywork-OR1-32B on challenging mathematics and coding benchmarks. Figure 3: Performance of Skywork-OR1-7B on challenging mathematics and coding benchmarks."
        },
        {
            "title": "2 Preliminaries",
            "content": "The success of Deepseek-R1 demonstrates that Policy Gradient (PG) methods [22], especially Group Relative Policy Optimization(GRPO) [21], can effectively enhance the reasoning abilities of LLMs. Generally speaking, the RL objective is to find policy π that maximizes the reward, i.e.: (cid:8)J (π) := ExDEyπ(x) [r (x, y)](cid:9) , max π (2.1) where is the training prompt, is the sampling distribution of x, is the response sampled by the policy π for input prompt x, and denotes the reward function. In practice, we estimate surrogate objective for (π) at the batch level for tractable optimization. At each training step k, we sample batch of prompts x1, . . . , xN from the data distribution D, denoted as Tk, and generate the corresponding responses y1, . . . , yN using the current policy π with context length and temperature τ . The batch-level surrogate objective at step can be formulated as: max π (cid:8)Jk (π) := ExiTk Eyiπ(xi) [r (xi, yi)](cid:9) , (2.2) where πk is shorthand for the policy πθk parameterized by θk. Vanilla Policy Gradient For parameterized policy πθ, vanilla PG [23] uses gradient ascent to obtain the optimal parameter θ, i.e. valid first-order surrogate policy loss for vanilla PG at each iteration is given by: θ θ + θJ (πθ) . LPG (θ) = ExiTk Eyiπk(xi) yi1 (cid:88) t=0 Aπk (cid:0)st i, at (cid:1) , πθ (at πk (at ist i) ist i) (2.3) where the response yi = (a0 , ..., at1 := (xi, a0 st , ..., ay1 ) consists of tokens, at ) is the prefix context when generating at Aπk (cid:0)st, at(cid:1) := Eyπk(x) (cid:2)r (x, y) st, at(cid:3) Eyπk(x) (cid:2)r (x, y) st(cid:3) . is the t-th token in the sequence yi, , and Aπk is the advantage function defined as One can easily show that θLPG (θk) = θJk (πk). Proximal Policy Optimization (PPO) At each training step k, PPO [20] performs multiple gradient descent steps on the policy loss Lk with clip trick to keep the new policy restricted within the trust region of πk. The policy loss employed in PPO is formulated as: LPPO (θ) = ExiTk Eyiπk(xi) yi1 (cid:88) min (cid:0)ρt (θ) Aπk (cid:0)st i, at (cid:1) , clip (cid:0)ρt (θ) , 1 ε, 1 + ε(cid:1) Aπk (cid:0)st i, at (cid:1)(cid:1) , t=0 i) ist where ρt i) ist estimate the token-level advantage Aπk (st πθ(at πk(at i(θ) := i, at i). , and ε is the clip hyperparameter. In practice, PPO generally uses GAE [19] to Group Relative Policy Optimization (GRPO) Suppose i.i.d. responses yi1, .., yiM are sampled for each prompt xi. GRPO [21] estimates the token-level advantage using the group-normalized rewards and 6 introduces an additional length normalization term 1 GRPO is formulated as: yij for each response yij. The policy loss employed in"
        },
        {
            "title": "LGRPO",
            "content": "k (θ) = ExiTk {yij }M j=1πk(x)"
        },
        {
            "title": "1\nM",
            "content": "M (cid:88) i=1 1 yij yij 1 (cid:88) t=0 min (cid:0)ρt ij (θ) At ij, clip (cid:0)ρt ij (θ) , 1 ε, 1 + ε(cid:1) At ij (cid:1) βDt ij (θ) , (2.4) ij, ..., ayij 1 where yij = (a0 πθ(at ij) ij) πk(at keep the policy πθ stay in the trust region of reference policy πref, i.e. ij := (xi, a0 is the token-level k3 loss [21] applied in at ij is the t-th token in the sequence yij, st , ε is the clip hyperparameter, Dt ij ij st ij st ), at ij ij ij, ..., at ij ), ρt ij(θ) := with coefficient β to Dt ij (θ) := πref πθ (cid:0)at (cid:0)at (cid:1) ijst ij (cid:1) log ijst ij πref πθ (cid:0)at (cid:0)at (cid:1) ijst ij (cid:1) 1, ijst ij For each prompt-response pair (xi, yij), binary reward (xi, yij) {0, 1} is given by rule-based verifier. The token-level advantage At ij is estimated by : At ij = (xi, yij) mean (r (xi, yi1) , ..., (xi, yiM )) std (r (xi, yi1) , ..., (xi, yiM )) . (2.5)"
        },
        {
            "title": "3 MAGIC in Skywork-OR1",
            "content": "We employ training pipeline built upon modified version of GRPO [21], referred to as Multi-stage Adaptive entropy scheduling for GRPO In Convergence (MAGIC). In the following sections, we first introduce the recipe of MAGIC and then analyze the effectiveness of each of its components."
        },
        {
            "title": "3.1 MAGIC",
            "content": "In the following, we present the MAGIC framework by detailing its components in terms of Data Collection, Training Strategy, and Loss Function. Data Collection To ensure the quality of queries during post-training, we construct the initial dataset through stringent data preparation, as described in Section 6, and adopt more accurate verifiers to provide reward signals, as outlined in Section 7. Additionally, we employ the following strategies to further improve sample efficiency: 1. Offline and Online Filtering. We apply data filtering both before and during training. Prior to training, we remove prompts with base model correctness rates of 1 (fully correct) or 0 (completely incorrect). During training, at the beginning of each stage, we also discard training prompts for which the actor model achieved correctness of 1 in the previous stage. This dynamic online filtering mechanism ensures that the actor model is consistently trained on challenging problems at each stage. 2. Rejection Sampling. Responses in the zero-advantage group (as defined by Equation (2.5)) do not contribute to the policy loss but may influence the KL loss or entropy loss, potentially leading to more unstable training process due to the implicitly increased relative weight of these losses. To mitigate this issue, our training batches include only groups with non-zero advantages; specifically, the samples of prompt xi are filtered out if / Tk, where Tk := (cid:111) (cid:110) [N ] : [M ] ˆAij = . 7 Training Strategy We made the following refinements to the training strategy of vanilla GRPO: 1. Multi-Stage Training. Inspired by DeepScaleR [17], we progressively increase the context length and divide the training process into multiple stages. We found that multi-stage training significantly reduces computational costs while preserving scalability, as supported by the evidence presented in Section 3.2.2. 2. Advantage Mask for Truncated Responses. To address potential noise in training signals when outcomes cannot be derived from truncated responses since assigning negative advantages in such cases may introduce bias we experimented with an advantage mask during the early stages of multi-stage training, when many responses are truncated. However, as shown in Section 3.2.3, penalizing truncated responses does not hinder later-stage improvements and enhances token efficiency. Based on these results, we do not employ any advantage mask strategy in our training pipeline. 3. High-Temperature Sampling. We set the rollout temperature to τ = 1 to enhance the models exploration capability and improve learning plasticity. This decision was motivated by our observation that the sampling policy either immediately enters (in the case of math data) or quickly transitions into (in the case of code data) low-entropy state when using smaller sampling temperature (e.g., τ = 0.6). See Section 3.2.4 for further details. 4. On-Policy Training. We adopted on-policy training for Skywork-OR1-7B and Skywork-OR1-32B, as we found that on-policy updates significantly slow entropy collapse and lead to higher test performance. See Section 4 for our detailed findings on entropy collapse. In contrast, Skywork-OR1-Math-7B was trained with two gradient steps per training step (and was therefore not strictly on-policy). This setup preceded our complete understanding of the relationship between off-policy updates and premature entropy collapse. Nevertheless, adaptive entropy control (Section 3.2.5) effectively mitigated collapse, allowing the model to achieve strong performance. Loss Function To mitigate implicit length bias, we adopt token-level policy loss by removing the length normalization term 1/ yij from each response. The policy loss is averaged across all tokens in training batch, formulated as follows: LMAGIC (θ) = 1 Tk (cid:88) (cid:88) Tk j=1 yij 1 (cid:88) t= min (cid:8)ρt ij (θ) At ij, clip (cid:0)ρt ij (θ) , 1 ε, 1 + ε(cid:1) At ij (cid:9) + αkHt ij (θ) , (3.1) ij, ..., ayij 1 where yij := (a0 context when generating at ij ij ), at ij , ρt is the t-th token in the sequence yij, st ij st (cid:0)st ij st , αk 0 is the coefficient of the entropy, Tk := (cid:80) ij(θ) := (cid:0)πθ ij(θ) := πθ(at πk(at ij) ij) , Ht ij policy of token at j=1 yij is the total number of ij tokens in the training batch. Meanwhile, we also introduce the following characteristics into the loss function: Tk (cid:80)M ij ) is the prefix ij, ..., at1 ij := (xi, a0 (cid:1)(cid:1) is the entropy of the generation 1. Adaptive Entropy Control. To preserve the models exploration capability and maintain high learning plasticity, it is common to include an additional entropy loss to prevent entropy collapse. An appropriately weighted entropy loss can enhance generalization. However, our experiments show that selecting suitable coefficient in advance is often challenging, as the entropy loss is highly sensitive to both the coefficient and the training data. To address this, we introduce an additional hyperparameter, tgt-ent, representing the target entropy. This hyperparameter dynamically adjusts the coefficient αk based on the difference between the current entropy and the target entropy, ensuring that the current entropy remains lower-bounded by tgt-ent. See Section 3.2.5 for more details. 2. No KL Loss. We found that including KL loss term hinders performance gains, particularly in the later stages of multi-stage training. Therefore, we omit the KL loss from our training recipe. See Section 3.2.6 for further discussion."
        },
        {
            "title": "3.2 Effectiveness of MAGIC Components",
            "content": "In this section, we present results from extensive experiments conducted to examine how various components of our MAGIC recipe influence the performance improvement of reinforcement learning during post-training."
        },
        {
            "title": "3.2.1 Data Mixture",
            "content": "(a) (b) Figure 4: Left: Comparison of our data mixture with DeepScaleRs mixture. The experiment was conducted on an earlier version of the 32B variant, using only math data. Right: Comparison of AIME 24 performance between two mixtures: our official mixture (default) and version with additional data selected using lower verification criteria (i.e., with potential errors in ground truth answers). Although the quality is lower, we observe only slower learning progress compared to the clean counterpart. In our formal training recipe, we include additional hard problems filtered from NuminaMath-1.5 [13] to construct our final data mixture. We conduct the following ablation study to demonstrate the effectiveness of this design choice. We primarily compare against DeepScaleRs data mixture [17], as existing models trained on it have shown strong performance. Ablation Experiments 1: Existing Mixture vs. Our Data Mixture 1. DeepScaleR mixture [17]: Comprises problems from previous years AIME, AMC, Omni-MATH [4], and STILL [26]. 2. Skywork-OR1 mixture: Our custom mixture described in Section 6, incorporating problems from more diverse sources (e.g., NuminaMath-1.5) and selected via difficulty filtering and quality control. We use the same hyperparameters and approximately the same number of training steps across both experiments to control for the effect of data size. Results are shown in Figure 4. Although the DeepScaleR dataset performs well with smaller model variants, we observed slight initial improvement on AIME24. However, performance degraded sharply after 300 training steps, eventually returning to the same accuracy as before training. Additionally, in Figure 4(b), we test our data mixture combined with an extra subset obtained via less stringent verification procedure. This extra subset contains hard problems from NuminaMath-1.5 that were previously excluded due to potential mismatches between extracted and provided solutions. We find that the performance difference between the two mixtures is negligible within the first 900 steps. The version including the extra subset exhibits slightly slower early 9 (a) (b) Figure 5: Left: Comparison of From-Scratch vs. Multi-Stage training. Top left: Response length during RL training. Bottom left: AIME24 avg@8 performance at temperature 1 (left y-axis) and cumulative training hours (right y-axis). Multi-stage training achieves the same final accuracy with significantly fewer training hours due to smaller context length in the early stages. Right: AIME24 avg@32 vs. response length for Skywork-OR1-Math-7B and DeepSeek-R1-Distill-Qwen-7B with 32K context length. The Stage checkpoint of Skywork-OR1-Math-7B reaches comparable performance to DeepSeek-R1-Distill-Qwen-7B with notably better token efficiency; further performance gains are seen in Stages II &III. progress, possibly due to noise in the provided answers. We hypothesize that RL training is robust to small amounts of ground truth noise, consistent with findings in [36]. Therefore, we adopt the default data composition described in Section 6 for all subsequent exploration experiments. 3.2.2 Multi-Stage Training One of the major challenges in optimizing long Chain-of-Thought (CoT) models with RL is managing Inspired by excessively long outputs, which can lead to slow convergence and high training variance. DeepScaleR [17], we incorporated multi-stage training in all our released models to improve training efficiency. Specifically, we used shorter context length in the initial stages. Once the models performance converged, we increased in the subsequent stage. This approach led to significant performance improvements on benchmarks while also enhancing training efficiency. Same Improvement, Higher Efficiency. To demonstrate the effectiveness of multi-stage training, we conducted two experiments based on DeepSeek-R1-Distill-Qwen-7B with different schedules for : Ablation Experiments 2: From-Scratch vs. Multi-Stage 1. From-Scratch: We started with = 16K at step 0 and kept it fixed during training. 2. Multi-Stage: We started with = 8K at step 0. At later step (i.e., step 540), we switched to Stage II and increased to 16K. The other hyper-parameters were kept same for both experiments and are reported in Table 1. The results are presented in Figure 5(a) and Figure 5(b). Figure 5(a) illustrates how AIME24 accuracy, generated response length, and cumulative training hours evolve with the number of training steps in Ablation Experiments 2. As shown, the AIME24 accuracy in both experiments converges to approximately 60 when the number of training steps is sufficiently large. However, in the multi-stage experiment, the context length in Stage (i.e., 8K) is only half that used in the from-scratch experiment (i.e., 16K). As result, the average response length in the multi-stage experiment is Batch Size Mini-batch Size Group Size Entropy Control KL Loss 64 16 target-entropy 0.2 No Table 1: Shared hyperparameters in Ablation Experiments 1 based on Deepseek-R1-Distill-Qwen-7B. significantly shorter during Stage and the initial steps of Stage II, leading to more efficient training due to reduced inference and computational costs (approximately 100 training hours are saved over 1000 training steps). After transitioning to Stage II, both the response length and AIME24 accuracy begin to increase immediately. Within roughly 500 training steps in Stage II, the accuracy of the multi-stage experiment reaches the same level as that of the from-scratch experiment. Improving Token Efficiency While Preserving Scaling Potential. Truncated responses are labeled as negative samples in RL training because they lack final answers. potential concern with multi-stage training is that using short context windows may bias the model toward generating shorter responses, potentially limiting its exploratory capacity and reducing its ability to solve complex problems. Our findings demonstrate that multi-stage training not only improves token efficiency in the initial stage but also preserves scaling ability. In Figure 5(b), we observe that training with an 8K context length in Stage maintains comparable AIME24 accuracy under 32K context length while significantly improving token efficiency (reducing the average response length from approximately 12.5K to 5.4K tokens). In Stages II and III, Skywork-OR1-Math-7B steadily increases response length while concurrently improving performance. 3.2.3 Advantage Mask for Truncated Responses In practice, responses are sampled within fixed context length . When response lengths exceed , the outcomes cannot be derived, and accuracy rewards are set to 0, resulting in negative advantages for these truncated responses, which may introduce bias. To mitigate this issue, we investigated several advantage mask strategies aimed at reducing the influence of truncated responses. However, our findings show that assigning negative advantages to truncated samples not only improves token efficiency but also preserves the models scaling ability in later stages. As result, we did not apply any mask strategies in our final training pipeline. Figure 6: Training accuracy and clip ratio during RL training of Skywork-OR1-Math-7B in Stage I. accuracy: Mean accuracy reward on training batch. accuracy_nontruncated: Mean accuracy of non-truncated samples. clip_ratio: Ratio of truncated responses. Two Optimization Directions in Short Context Length. In our Stage training of Skywork-OR111 Math-7B, we set the context length to = 8K, and approximately 40% of responses were truncated at the initial steps. Although overall training accuracy continued to increase during RL training, we observed that the accuracy of non-truncated samples initially declined sharply within the first 100 training steps before showing slight upward trend. See Figure 6 for details. truncated response typically receives an accuracy reward of 0 because the final answer is missing due to truncation, even if it would be correct if fully generated. Therefore, reducing the number of truncated responses improves achievable accuracy. Figure 6 shows that the initial increase in training accuracy (steps 0-100) is primarily due to sharp decrease in the clip ratio. After step 100, the algorithm begins to improve accuracy for non-truncated responses as well. Brief Explanation from Theoretical Perspective. We now use mathematical language to clarify this phenomenon further in formal way. Recall the objective of RL training in (2.1), π argmax π {J (π) := ExDEyπ(x) [r (x, y)]}, where is the prompt, is the distribution of prompts, is the response sampled from actor π, r(x, y) {0, 1} is the binary accuracy reward. Note that the response is sampled under the context length . For these truncated responses whose lengths are greater than , i.e. > , the accuracy reward is r(x, y) = 0 since the outcome can not be derived from the response. Based on this observation, one can easily shows that the objective function (π) satisfies (π) = ExDEyπ(x) [r (x, y)] = ExDEyπ(x) [r (x, y) {y }] = ExD = ExD = ExD (cid:20) nontrunc (x) Eyπ(x) pπ nontrunc (x) EyˆπT (x) [r (x, y)](cid:3) nontrunc (x) rπ nontrunc (x)(cid:3) , (cid:20) {y } pπ nontrunc (x) (cid:2)pπ (cid:2)pπ (cid:21)(cid:21) (x, y) nontrunc (x) := Pyπ(x) (y ) is the probability that response is not truncated by the limit nontrunc (x) := EyˆπT (x) [r (x, y)] is the {y } . This where pπ of context length (we assume pπ π(yx) accuracy of the non-truncated responses output by policy π and ˆπT (yx) := pπ nontrunc(x) implies that the accuracy on training distribution, i.e. (π), can be increased by: nontrunc (x) > 0 for simplicity), rπ increasing pπ nontrunc (x), which means the number of the responses that receive accuracy reward of 0 erroneously decreases. increasing rπ nontrunc (x), which means the response quality within the context length will be improved. Advantage Mask for Truncated Responses. To encourage the algorithm to focus on optimizing accuracy nontrunc (x) rather than merely shortening responses to avoid within the context length i.e., increasing rπ erroneously receiving zero accuracy reward i.e., increasing pπ nontrunc (x) we explored various advantage mask strategies. These strategies were designed to mitigate the impact of noisy training signals introduced by truncated samples. We conducted ablation experiments using DeepSeek-R1-Distill-Qwen-7B in Stage to evaluate the effects of different advantage mask strategies. (a) (b) Figure 7: Left: The clip ratio of generated responses during reinforcement learning training was analyzed after applying various advantage mask strategies in Ablation Experiments 2. Using an advantage mask mitigates the decay in response length. The clip ratio even increased after applying Adv-Mask-Before. Middle: Training accuracy of responses influenced by different advantage mask strategies in Ablation Experiment 2 shows distinct patterns. After applying the Adv-Mask-Before, training accuracy decreases. In contrast, it continues to increase when using the Adv-Mask-After or No-Adv-Mask strategies. Right: Training accuracy of non-truncated responses induced by different advantage mask strategies in Ablation Experiment 2 showed distinct outcomes. After applying the Adv-Mask-Before strategy, the training accuracy of non-truncated responses continued to rise. In contrast, both the Adv-Mask-After and No-Adv-Mask strategies resulted in sharp decrease during the early steps. (c) Ablation Experiments 3: Different Advantage Mask Strategies 1. No-Adv-Mask: We do not employ any advantage mask strategy. 2. Adv-Mask-Before: The truncated responses are not involved in the group advantage calculation for non-truncated responses, and the advantage of these truncated responses are set to 0 (thus not contributing to the policy loss): : At ij = r(xi,yij )mean(ˆRi) std(ˆRi) y > Here ˆRi is the accuracy rewards group of non-truncated responses of prompt xi. 3. Adv-Mask-After: The truncated responses are not involved in the group advantage calculation for non-truncated responses, and the advantage of these truncated responses are set to 0 (thus not contributing to the policy loss): : At ij = r(xi,yij )mean(ˆRi) std(ˆRi) 0 y > Here ˆRi is the accuracy rewards group of non-truncated responses of prompt xi. The other hyperparameters remain the same for both experiments and are reported in Table 2. The results can be found in Figure 7(a), Figure 7(c) and Figure 8. Figure 7 shows the clip ratio, overall accuracy, and accuracy on non-truncated responses in Ablation Experiments 2. We observe that although the response quality within the context length (i.e., the accuracy of non-truncated responses) increases as expected after applying the Adv-Mask-Before strategy, the overall training accuracy continues to decline, and the clip ratio increases steadily. This appears to be form of reward hacking from our perspective. More importantly, as shown later in Figure 8, the accuracy of the 13 Batch Size Mini-batch Size Group Size Context Length Entropy Control KL Loss 256 128 Stage 8K target-entropy 0.2 No Table 2: Shared hyperparameters in Ablation Experiments 2 based on Deepseek-R1-Distill-Qwen-7B. Figure 8: AIME24 avg@32 performance vs. context length for different advantage mask strategies in Ablation Experiments 3. All strategies achieve the same accuracy at the 32K context length. The accuracy was further improved after the training of Stage II even though the noisy training signals from truncated responses were introduced in Stage I. Adv-Mask-Before strategy under large context lengths where responses are typically not truncated (e.g., 32K) shows no improvement. This may be attributed to the smaller effective training batch size caused by the increased clip ratio under the Adv-Mask-Before strategy. The behavior of Adv-Mask-After serves as an intermediate point between Adv-Mask-Before and No-Adv-Mask. Advantage Mask Does Not Exhibit Better Performance Given Larger Inference Budget. Although Ablation Experiments 2 demonstrate that rπ untrunc (x) is optimized under short context lengths when applying advantage masks, we find that accuracy does not improve when the context length is large enough to avoid truncation (i.e., 32K). We compare the test-time scaling behavior on AIME24 for models trained with different advantage mask strategies (see Figure 8). The results show that applying an advantage mask does not improve test-time scaling behavior in Stage I, and accuracy at 32K remains unchanged-even though rπ nontrunc (x) is optimized during training. In contrast, RL training without an advantage mask in Stage not only maintains accuracy at large context lengths but also significantly improves token efficiency. Moreover, the shorter response lengths learned in Stage do not hinder the simultaneous improvements in both response length and accuracy observed in Stage II. Based on these findings, we did not apply any advantage mask to address noisy training signals from truncated samples in our final training recipe. 3.2.4 High-temperature Sampling The group-wise nature of GRPO implies that the sampling procedure for responses directly affects the quality and diversity of each group, which in turn influences learning. Prior work suggests that higher temperatures generally lead to slightly worse performance due to increased randomness. If the temperature is set too 14 Figure 9: AIME25 avg@8 performance and entropy versus the number of training steps in Ablation Experiments 3. Training with temperature of 0.6 starts with the lowest entropy and learns more slowly than at temperature of 1.0. Note that the entropy in the right plot remains around 0.2 because adaptive entropy control is enabled. This experiment was conducted on an earlier version of the 32B variant using only math data. Note also that in the left plot, the two temperatures indicate the rollout temperatures used during training. The scores of AIME25 were obtained by evaluating both models at temperature of 0.6 to ensure fair comparison. high, it may increase the likelihood of sampling groups containing only incorrect responses, thereby reducing training efficiency due to the absence of advantageous signals. On the other hand, using low temperature reduces group diversity, resulting in solutions that are highly similar or potentially all correct. Therefore, selecting an appropriate temperature is critical to ensure sufficient in-group solution diversity. We conducted ablation experiments on the choice of sampling temperature τ , and the results are presented in Figure 9. Ablation Experiments 4: Different Online Sampling Temperatures τ We compared two different sampling temperatures in online RL training: 1. High Temperature: We set the temperature hyperparameter τ = 1.0. 2. Low Temperature: We set the temperature hyperparameter τ = 0.6. The other hyperparameters were kept the same for both experiments and are reported in Table 3. The results can be found in Figure 9. Batch Size Mini-batch Size Group Size 32 16 Context Length Stage 16K Entropy Control KL Loss target entropy 0. 0 Table 3: Shared hyperparameters in Ablation Experiments 4 In our experiments, we identified an additional entropy-related phenomenon: when low temperature is used (e.g., 0.6), the model either begins with extremely low entropy or its entropy quickly collapses to near zero within approximately 100 steps. This behavior initially slows learning progress and ultimately leads to stagnation. We hypothesize that with less diverse group of solutions despite containing both correct and incorrect responses the policy update becomes overly focused on narrow subset of tokens. This results in large probability mass being assigned to specific tokens that frequently appear in the sampled responses. When we increased the rollout temperature to 1.0, the models initial entropy rose to more desirable range. Although entropy still eventually converges, the higher temperature substantially enhances the learning signal in the early stages and preserves greater potential for continued training, as shown in the figure above."
        },
        {
            "title": "3.2.5 Adaptive Entropy Control",
            "content": "Building on the findings from Section 4 which suggest that while preventing premature entropy collapse via entropy regularization is beneficial, selecting an appropriate entropy loss coefficient is challenging we introduce Adaptive Entropy Control, method that adaptively adjusts the entropy loss coefficient based on the target and current entropy. Specifically, we introduce two additional hyperparameters: tgt-ent (the desired target entropy) and (the adjustment step size for the entropy loss coefficient). We initialize the adaptive coefficient with c0 = 0. At each training step k, let denote the current entropy of the actor (estimated from the rollout buffer). If is less than tgt-ent, we increase ck by (i.e., ck+1 = ck + ). If exceeds tgt-ent, we decrease ck by . To alleviate instability caused by unnecessary entropy loss, we activate the entropy loss only when tgt-ent, i.e., αk = ck I{e tgt-ent}, ensuring that the current entropy remains lower-bounded by the target entropy. By leveraging adaptive entropy control, we maintain the models entropy at reasonable level throughout training and effectively prevent premature collapse. Figure 10 illustrates the entropy trajectory of SkyworkOR1-Math-7B across all training stages. In our experiments, we set tgt-ent = 0.2 and = 0.005. To further validate the effectiveness of adaptive entropy control, we conducted an ablation study detailed in Section 4.5. αk = ck I{ek tgt-ent}, ck+1 = (cid:40) ck + , ck , if ek < tgt-ent if ek > tgt-ent , c0 = 0 (3.2) Figure 10: Entropy of generated responses (left) and avg@8 performance on AIME24 (right) of SkyworkOR1-Math-7B across all stages. We use adaptive entropy control with tgt-ent=0.2 and = 0.005 . Under adaptive entropy control, the entropy of Skywork-OR1-Math-7B is generally lower-bounded by the target entropy 0.2 and the performance on AIME24 has been steadily improving. 3.2.6 No KL Loss To investigate the impact of the KL loss, we conducted the following ablation experiments. Ablation Experiments 5: KL Loss vs. No KL Loss We consider token-level k3 loss in our ablation and the KL-regularized policy loss we employed is: Lβ (θ) = (θ) + β Tk (cid:88) (cid:88) yij 1 (cid:88) (cid:32) iTk j=1 t=0 πref πθ (cid:0)at (cid:0)at (cid:1) ijst ij (cid:1) log ijst ij πref πθ (cid:0)at (cid:0)at (cid:1) ijst ij (cid:1) 1 ijst ij (cid:33) , where L(θ) is the original policy loss defined in (3.1), β is the KL coefficient. We first run stage 1 experiment with β=1e-3 based on DeepSeek-R1-Distill-Qwen-7B (reference policy). Then in stage 2, we conducted ablations based on the stage 1 checkpoint, comparing β = 1e-3 with β = 0. The other hyper-parameters are reported in Table 4. The results can be found in Figure 11(a) and Figure 11(b). Batch Size Mini-batch Size Group Size 256 128 Context Length Stage II 16K Entropy Control target entropy 0.2 Table 4: Shared hyperparameters in Ablation Experiments 5 based on stage1 checkpoint We observe that, in Stage 2, the KL loss strongly pulls the actor models policy back toward the reference model, causing the KL divergence to rapidly decrease toward zero (see Figure 11(a)). As result, performance on AIME24 fails to improve significantly once the actors policy becomes too similar to the reference policy (see Figure 11(b)). Based on this observation, we set β = 0 for all training stages of our released models. (a) (b) Figure 11: Results of Ablation Experiments 5. Left: KL divergence between the actor model and the reference model during RL training with different KL loss coefficient β in Ablation Experiments 5. Setting β = 1e-3 pulls the actor model back towards the reference model strongly in stage 2. Right: The AIME24 avg@8 performance at temperature 1 during RL training of different β in Ablation Experiments 5."
        },
        {
            "title": "4 Empirical Studies on Mitigating Policy Entropy Collapse",
            "content": "Figure 12: Overview of our empirical studies on mitigating policy entropy collapse. Gray and green blocks: The potential benefits and possible approaches to enhance the models exploration capability and mitigate entropy collapse. Yellow blocks: The experimental variables in our empirical studies on keeping the models exploration capability and maintaining high plasticity. Exploration and exploitation represent one of the most fundamental dilemmas in RL training [22], particularly in on-policy algorithms. In brief, achieving better performance requires sufficient exploration. However, if the agents policy prematurely converges to specific solution, that policy may be suboptimal, and such convergence hinders the exploration of diverse trajectories. An important metric for monitoring the convergence of RL algorithms is policy entropy. In general, when models policy entropy converges to very small value (e.g., near zero), the policy stabilizes. At this point, the models generation behavior becomes resistant to updates from training data, leading to reduced learning efficiency and diminished output diversity. To expose the model to more effective training signals and improve its out-of-distribution (OOD) performance, it is therefore critical to prevent premature entropy collapse in practice. This section investigates which hyperparameters and components of the policy update process help prevent entropy collapse and, in turn, improve OOD generalization. The overall framework of our empirical study on alleviating policy entropy collapse is illustrated in Figure 12. Initially, we hypothesize that the following two sources may influence the models entropy and convergence behavior: Rollout diversity. If the rollout data contain greater diversity of correct responses, this prevents the model from overfitting to single correct trajectory. We examine how sampling-related hyperparameters such as sampling temperature, rollout batch size, and group size affect the models policy entropy during RL training. Policy update. We also investigate how different components of the policy update influence entropy. In this section, we focus primarily on the number of stochastic gradient descent (SGD) steps per training step and the use of additional entropy control methods (e.g., entropy loss). 18 After conducting exhaustive ablation experiments, we present our main results below."
        },
        {
            "title": "Empirical Results of Our Entropy Collapse Study",
            "content": "1. Faster entropy collapse generally leads to worse test performance. In Section 4.2 and Section 4.5, we show that appropriate entropy control, which prevents premature policy convergence, can yield improved test performance. 2. Increasing rollout diversity by enlarging the batch size and group size has only minor effect on entropy dynamics, whereas using higher sampling temperature significantly impacts initial entropy. See Section 4.3 for details. 3. Increasing the number of SGD steps per training step whether by using more mini-batches or increasing data reuse significantly accelerates entropy collapse and generally results in degraded test performance due to the introduction of off-policy data. See Section 4.4 for more information. 4. Our ablation experiments in Section 4.5 show that the entropy loss is highly sensitive to both the training data and the loss coefficient. By either adaptively adjusting the entropy loss coefficient or appropriately applying the clip-higher trick [34], entropy dynamics can be stabilized and lowerbounded, leading to improved test performance."
        },
        {
            "title": "4.1 Ablation Setup",
            "content": "All ablation experiments presented in Section 4 are conducted using the training pipeline described in Section 3.1. We start from the following baseline experiment based on DeepSeek-R1-Distill-Qwen-7B with its hyperparameters reported in Table 5, the key symbols used are defined as follows: DR is the rollout batch size (the number of prompts used to generate responses in one training step). DT is the mini-batch size (the number of prompts corresponding to the responses used per policy update step). Nreuse is the number of times the rollout buffer is traversed. gs is the group size (the number of responses generated for each prompt). is the context length. τ is the sampling temperature. DR DT 64 64 Nreuse 1 gs 16K τ 1.0 Learning Rate Entropy Control KL loss 1e-6 No No Table 5: Hyperparameters of our baseline experiment in the ablation study presented in Section 4. Unless otherwise specified, the default training configurations for all ablation experiments in this section are aligned with those of the baseline experiment presented above. We use AIME24, AIME25, and LiveCodeBench [10] (2024.082025.02) as evaluation sets. The test performance reported in our ablation study is computed as the empirical mean of avg@8 performance on AIME24/25 and pass@1 performance on LiveCodeBench. Notably, the baseline experiment achieves 69.2% avg@8 on AIME24, 53.3% avg@8 on AIME25, and 50.5% pass@1 on LiveCodeBench after 2,700 training steps using 32 H800 GPUs. These results, which closely approximate the performance of our final Skywork-OR1-7B release, establish strong baseline for analyzing key factors that affect test performance and contribute to entropy collapse. 19 Figure 13: Preliminary experiments on mitigating entropy collapse by introducing entropy loss. We tested two different coefficients αk = 1e-3 and 5e-3, and found that the entropy loss with the higher coefficient αk, i.e., 5e-3, more effectively prevents entropy collapse and achieves higher test performance. Left: Accuracy curves on test benchmarks during RL training. Right: Entropy of generated responses during RL training."
        },
        {
            "title": "4.2 Premature Entropy Collapse Generally Manifests as Worse Performance",
            "content": "As previously noted, entropy dynamics during RL training reflect the degree of policy convergence. When the actor converges to specific policy and enters low-entropy state, both learning efficiency and rollout diversity tend to decline. In our preliminary experiments, we observed that the entropy of the actor model often decreased rapidly during training. To mitigate premature entropy collapse, we introduced an entropy loss term, hypothesizing that it would allow the actor to converge toward better policy. Our results confirmed this hypothesis: test performance improved with the addition of entropy loss. Figure 13 presents the accuracy curves on test benchmarks and the entropy of generated responses from two preliminary experiments using different values of the entropy loss coefficient αk (1e-3 vs. 5e-3). The results show that using higher coefficient (i.e., 5e-3) more effectively prevents entropy collapse and leads to better generalization performance. Furthermore, our ablation experiments in Section 4.4 reinforce this finding, showing that RL training accompanied by premature entropy collapse generally results in worse test performance. These observations motivate our integration of entropy control mechanisms into the training pipeline, as well as our systematic investigation into how hyperparameters and other RL components influence entropy dynamics."
        },
        {
            "title": "4.3 The Impact of Rollout-Diversity-Related Hyperparameters\nWe investigated how the rollout batch size DR, group size gs, and sampling temperature τ influence entropy\ndynamics. Note that increasing the rollout batch size DR and group size gs during the rollout stage results\nin a larger rollout budget, which typically requires greater computational resources to accelerate training.\nTherefore, we provide a detailed discussion of the impact of DR and gs in Section 5, which focuses on\ntraining-time computational resource allocation for improved test performance. Here, we present only the\nexperimental results related to policy entropy. Specifically, we conducted ablation experiments using rollout\nbatch sizes DR = 16, 32, 64 and group sizes gs = 4, 8, 16, based on the baseline experiment described in\nSection 4.1 and analyzed in Section 5. Our results (Figure 14) indicate no significant differences in entropy\ndynamics across these on-policy configurations. Notably, none of these experiments exhibited entropy collapse.\nRegarding the sampling temperature τ , we found that using a properly chosen but relatively high temperature\nled to lower test accuracy during the initial training steps, but ultimately resulted in greater performance\nimprovements. For further details, please refer to Section 3.2.4.",
            "content": "20 Figure 14: Entropy of generated responses during on-policy updates with different rollout batch sizes DR (left) and group size gs (right). All the experiments exhibit similar entropy dynamics. Figure 15: Illustration of on-policy vs. off-policy update in PPO-style policy loss. On-policy update applies single SGD step to the entire rollout batch, whereas off-policy update implements multiple SGD steps mini-batches, with through rollout batch decomposition and reuse. The rollout batch is partitioned into DR DT each mini-batch undergoing an independent SGD step. Then, one can iterate over the rollout batch Nreuse times. Thus, the total number of SGD steps performed on one rollout batch is DR DT Nreuse."
        },
        {
            "title": "4.4 The Impact of Off-policy Update by Increasing NSGD\nNote that the policy loss (3.1) in MAGIC is PPO-style, which naturally allows for performing multiple SGD\nsteps through rollout batch decomposition and reuse (as illustrated in Figure 15). Recalling the definitions of\nDR,DT and Nreuse from Section 4.1, it is clear that the number of SGD steps performed in one training step,\ni.e. NSGD, satisfies",
            "content": "Nreuse. (4.1) NSGD = DR DT 21 When DR = DT and Nreuse = 1, the policy update is purely on-policy since NSGD = 1. In contrast, when DT < DR or Nreuse 2, NSGD 2 and the off-policy data is introduced into the policy update. In this section, we investigate how NSGD affects the entropy dynamics and the test performance improvement. More SGD Steps, Faster Convergence with Worse Test Performance. We conducted the following ablation experiments on different NSGD values by decreasing DT or increasing Nreuse given fixed DR. Ablation Experiments 6: The Impact of Different Numbers of SGD Steps NSGD Consider the quadruple (NSGD, DR, DT , Nreuse). We started from the baseline experiment (1,64,64,1) presented in Section 4.1 and adjusted either DT or Nreuse to increase NSGD. The experiments are listed below: 1. NSGD = 1: The baseline experiment with the quadruple (1,64,64,1). 2. NSGD = 2: We ran two experiments with the quadruples (2,64,32,1) and (2,64,64,2). 3. NSGD = 4: We ran two experiments with the quadruples (4,64,16,1) and (4,64,64,4). The experimental results can be found in Figure 16. Figure 16: Results of Ablation Experiments 6. Off-policy training with increased NSGD by either decreasing DT or increasing Nreuse accelerates entropy collapse and exhibits worse test performance. Left: Entropy of generated responses during RL training. Right: Test performance during RL training. As shown in Figure 16, experiments with NSGD {2, 4} exhibit faster policy convergence, with entropy decaying to very small values within few training steps. As result, test performance fails to improve consistently once the model enters low-entropy state. In contrast, using an on-policy update with the configuration (1, 64, 64, 1) significantly alleviates this issue, leading to gradual decline in entropy and steady, albeit slower, improvement in test performance. Ultimately, the on-policy update with configuration (1, 64, 64, 1) achieves superior test performance when the number of training steps is sufficiently large. Off-Policy Data Harms Test Performance. We now investigate which factor in off-policy updates is more likely to contribute to degraded test performance. We identify the following two potential contributors that may influence the gradient direction in each SGD step: (1) the mini-batch size DT , and (2) the use of off-policy data. In the data reuse experiments with Nreuse {2, 4}, since DT is held constant and matches the value used in the on-policy setting, we attribute the degraded test performance to the use of off-policy data introduced through rollout batch reuse. In experiments that involve more mini-batches (i.e., DT {16, 32}), the performance drop compared to the on-policy update may be due to both the smaller mini-batch size 22 leading to greater gradient variance and the presence of off-policy data. To better understand which factor contributes more significantly, we conducted the following ablation experiments. Ablation Experiments 7: On-policy vs. Off-policy with the Same SGD Data Size DT Consider the quadruple (NSGD, DR, DT , Nreuse). 1. Off-policy update: We considered two off-policy experiments in Ablation Experiments 6 with the quadruples (2,64,32,1) and (4,64,16,1), which have smaller DT compared to the baseline (1,64,64,1). 2. On-policy update: We ran two experiments, configured with the quadruples (1,32,32,1) and (1,16,16,1) respectively as the on-policy counterparts to the off-policy update. These were based on the baseline configuration from Section 4.1. The experimental results are reported in Figure 17. Figure 17: Results of Ablation Experiments 7. On-policy experiments, i.e. NSGD = 1, do not exhibit premature entropy collapse and finally outperform the off-policy counterparts with the same DT when training step is sufficiently large. Left: Entropy of generated responses during RL training. Right: Test performance at temperature 1 during RL training. The experimental results shown in Figure 17 indicate that the on-policy update with smaller DT relative to the baseline experiment still yields steady improvements in test performance, and premature entropy collapse does not occur. Ultimately, the on-policy update outperforms the off-policy update with the same DT when the number of training steps is sufficiently large. Based on these observations, we hypothesize that the degraded test performance in the off-policy update is primarily caused by the introduction of off-policy data in each SGD step. Can Large DR in Off-Policy Updates Prevent Premature Entropy Collapse? Consider the off-policy experiment in Ablation Experiments 6 with the quadruple (NSGD, DR, DT , Nreuse) = (4, 64, 16, 1). We attempted to increase the rollout batch size DR from 64 to 256 while keeping NSGD = 4 fixed (i.e., resulting in the configuration (NSGD, DR, DT , Nreuse) = (4, 256, 64, 1)), with the expectation that this would introduce more diverse samples and prevent convergence on single trajectory. However, our results in Figure 18 indicates that even with larger DR, premature entropy collapse not only still occurs but may even do so more rapidly. 23 Figure 18: Keeping DR = 4 and Nreuse = 1, off-policy training with larger DR, i.e., DR = 256, does not DT prevent the premature entropy collapse. Both off-policy experiments, i.e. NSGD = 4, exhibit faster entropy convergence compared with the on-policy experiment with NSGD = 1."
        },
        {
            "title": "4.5 Preventing Premature Entropy Collapse",
            "content": "As previously discussed, premature entropy collapse is often associated with degraded test performance. It is therefore reasonable to expect that proper entropy control can lead to improved outcomes. As shown earlier, increasing NSGD and introducing off-policy data accelerate entropy convergence. However, there are an increasing number of scenarios where the use of off-policy data is unavoidable for example, in asynchronous training frameworks. Thus, it is also important to study entropy control mechanisms under off-policy settings. We begin by examining entropy regularization, straightforward approach that attempts to prevent entropy collapse by directly adding an entropy loss term. Our preliminary experiments, presented in Section 4.2, show that applying entropy regularization with an appropriately chosen coefficient can mitigate entropy collapse and improve test performance. However, we later observed that the effectiveness of entropy regularization is highly sensitive to both the choice of coefficient and the characteristics of the training data, making it difficult to select an optimal coefficient in advance. This motivates dynamic adjustment of the entropy loss coefficient. In addition, we consider the clip-higher trick proposed in [34] as another means of entropy control. In the following, we present our detailed findings. Entropy Loss Is Sensitive to the Coefficient αk. To demonstrate the sensitivity of entropy loss to the choice of αk, we conduct the following ablation study. Ablation Experiments 8: Entropy Loss with Different Coefficients αk We conducted ablation studies on wide range of constant coefficients αk based on Skywork-OR1Math-7B-stage1 (not the baseline experiment in Section 4.1 ). We select αk=1e-4, 5e-4, 1e-3, 5e-3, 1e-2. The other hyperparameters are reported in Table 6. The results are presented in Figure 19. From the results in Figure 19, we find that: For αk = 5e-4, 1e-3, 5e-3, and 1e-2, the entropy eventually rises sharply, leading to model collapse. The larger the αk, the more rapidly the entropy increases. 24 Batch Size Mini-batch Size Group Size 64 16 Stage II 16K Temperature τ 1.0 KL Loss No Table 6: Shared Hyperparameters in Ablation Experiments 8 Based on Skywork-OR1-Math-7B-stage Figure 19: The results of Ablation Experiments 8. Left: The entropy of generated responses during RL training. Right: The AIME24 avg@8 performance at temperature 1 during RL training. For αk = 1e-4, while entropy does not exhibit continuous rise, it still collapses, persistently decreasing toward zero. Entropy Loss Is Sensitive to Training Data. From our two preliminary experiments, we observe that the entropy loss is highly sensitive to variations in training data. We conducted two experiments under identical configurations, both using an entropy loss coefficient of 1e-3. The only difference between the two setups was the training dataset used (both datasets belong to the math domain). The results, shown in Figure 20, reveal striking difference in entropy dynamics: while the original dataset exhibited steady decline in entropy throughout training, the new dataset resulted in consistent upward trend in entropy. This finding highlights the data-dependent nature of tuning the entropy loss coefficient. Adjusting the Coefficient of Entropy Loss Adaptively. Based on our findings regarding the sensitivity of entropy loss, we propose method called adaptive entropy control (see Section 3.2.5 for details), which dynamically adjusts the entropy loss coefficient during training. As shown in Figure 10, the entropy of Skywork-OR1-Math-7B remains lower-bounded by the target entropy throughout the RL training process. To further validate the effectiveness of adaptive entropy control, we conduct the following ablation experiments. Ablation Experiments 9: Effectiveness of Adaptive Entropy Control Consider the off-policy experiment in Ablation Experiments 6 with (NSGD, DR, DT , Nreuse) = (4, 64, 16, 1), which exhibits fast entropy collapse and bad test performance. Note that there is no entropy loss in this experiment. We ran an experiment based on its configuration with adaptive entropy control (using target entropy of 0.2) enabled. We report the results in Figure 21. As previously analyzed, increasing NSGD accelerates policy convergence and leads to degraded test performance. As shown in Figure 21, applying adaptive entropy control successfully prevents entropy collapse and results in higher test performance. However, it is worth noting that, although the coefficient is adjusted adaptively, entropy remains unstable when NSGD is large. We speculate that this is due to the entropy loss being computed over the entire vocabulary, which may increase the probability of many unintended tokens. Therefore, we do 25 Figure 20: Preliminary experiments investigating how training data affects the entropy during RL training. Both experiments used the same hyperparameter configurations with αk=1e-3 but differed in the training data. Both datasets are in math domain. simply switching the dataset resulted in dramatically different entropy evolution patterns not recommend using adaptive entropy control in scenarios where NSGD is large. Nonetheless, we find that when NSGD = 1 or 2, entropy dynamics remain acceptably stable under adaptive entropy control. Based on these findings, we adopt adaptive entropy control in the training of our Skywork-OR1 models. Figure 21: The results of Ablation Experiments 9. Applying adaptive entropy control prevents the entropy collapse, leading to better test performance. Left: Entropy of generated responses during RL training. Right: Test performance during RL training. The Impact of the Clip-Higher Trick. We tested popular trick called clip-higher [34] used in PPO-style policy loss to prevent the entropy collapse when NSGD > 1. We conduct the following ablation experiments. 26 Ablation Experiments 10: The Impact of Different Higher-clip Ratios the off-policy experiment Consider 6 with the quadruple (NSGD, DR, DT , Nreuse) = (4, 64, 16, 1), which exhibits fast entropy collapse and poor test performance. Note that the clip ratio ϵ = 0.2 was applied in this experiment. We raised the higher-clip ratio from 0.20 to 0.25, 0.265, and 0.28 while keeping the lower-clip ratio fixed at 0.2. We report the results in Figure 22. in Ablation Experiments Our results, shown in Figure 22, indicate that using properly chosen higher-clip ratio e.g., 0.25 or 0.265 can prevent premature entropy collapse and lead to better test performance. However, it is worth noting that when the higher-clip ratio is set to 0.28, as suggested in [34], entropy increases sharply, resulting in poor test performance. This suggests that the optimal higher-clip ratio is task-dependent. Figure 22: The results of Ablation Experiments 10. Increasing the higher-clip ratio to an adequate value (e.g., 0.25 and 0.265) yields slower convergence and better test performance. However, we find that when the higher-clip ratio is set to 0.28 as recommended in [34], then entropy rises sharply and test performance is not improved. Left: Entropy of generated responses during RL training. Right: Test performance during RL training."
        },
        {
            "title": "5 Empirical Studies on Training Resource Allocation",
            "content": "During the RL training process, our goal is to select hyperparameters that make training both efficient and effective. This objective gives rise to two practical questions: Given fixed computational resources, how can we improve training efficiency? Given additional computational resources, how should we allocate them to achieve better test performance or improved training efficiency? In this section, we address these questions in the context of long CoT scenarios, using results from exhaustive ablation experiments as supporting evidence. The training process of online RL algorithms can generally be divided into two distinct phases: data rollout and policy update (which includes both forward and backward passes). Let tR, tT , and tO denote the time spent on rollout, policy update, and other operations (e.g., reward computation, experience generation), respectively. The total time consumption under synchronous training framework is: ttotal = tR + tT + tO. 27 Given fixed context length, the rollout time tR is primarily influenced by the rollout batch size DR and the group size (gs). As analyzed in Section 4.4, the policy update time tT depends on the number of SGD steps NSGD, which is determined by the number of mini-batches DR and the data reuse factor Nreuse. In the DT following subsections, we investigate how these factors impact both training efficiency and final performance. 5."
        },
        {
            "title": "Improving Training Efficiency with Fixed Computational Resources",
            "content": "In this section, we aim to answer the first question: Given fixed computational resources, how can training efficiency be improved? Figure 23: Overview of empirical studies on improving training efficiency given fixed computational resources. Grey blocks: Potential approaches to enhance training efficiency and their underlying principles. Yellow blocks: Experimental variables in the empirical studies Rollout Time tR Dominates the Total Training Time ttotal. fundamental observation regarding long CoT models (e.g. Deepseek-R1-Distill model series) is that the total training time is primarily determined by the rollout time. Table 7 presents the values of ttotal, tR, tT and tO of Skywork-OR1-32B over 1000 training steps. Clearly, tR dominates ttotal. Time Usage Hours total ttotal 309 rollout tR 223 policy update tT 27 others tO 59 tR/ttotal tT /ttotal 72.1% 8.7% Table 7: Analysis of training time usage of Skywork-OR1-32B for 1000 training steps. Since the primary bottleneck for ttotal in long CoT training is tR, it is reasonable to expect that appropriately increasing the number of SGD steps per training step, i.e., NSGD, will have minimal impact on ttotal while improving training efficiency. Therefore, in the following, we investigate the impact of the number of mini- ) and the data reuse times (Nreuse) on both the total training time ttotal and test performance. batches ( DR DT The overall idea of our study is illustrated in Figure 23. More SGD Steps, More Training Efficiency but Worse Performance. We have already examined the impact of increasing NSGD on entropy dynamics, as discussed in Ablation Experiments 6 (Section 4.4). Consider the configuration tuple (NSGD, DR, DT , Nreuse). We report the detailed time usage for the configurations (1, 64, 64, 1), (2, 64, 32, 1), and (4, 64, 16, 1) in Table 8. It is evident that increasing NSGD leads to higher tT . However, the impact on the overall training time ttotal remains minor, provided that DR is fixed. Thus, the configurations with NSGD {2, 4} perform multiple SGD steps within comparable training time, improving training efficiency. That said, the experimental results in Section 4.4 show that accelerating training via rollout batch decomposition or data reuse leads to faster entropy collapse and poorer test performance. Therefore, we do not recommend increasing NSGD solely for the purpose of improving training efficiency unless appropriate mechanisms are in place to mitigate entropy collapse, particularly those caused by off-policy updates as doing so may result in degraded generalization performance. Experiment (NSGD, DR, DT , Nreuse) (1,64,64,1) (2,64,32,1) (4,64,16,1) total ttotal 116 114 118 rollout tR 90 87 policy update tT 8 10 12 others tO 18 17 16 tR/ttotal tT /ttotal 77.6% 76.3% 76.3% 6.9% 8.7% 10.2% Table 8: Detailed time usage for three experiments from Ablation Experiments 6 over 1000 training steps. All the experiments utilized the same training resources (i.e., 32 H800 GPUs). 5."
        },
        {
            "title": "Improving Test Performance with More Computational Resources",
            "content": "In this section, we address the second question: given more computational resources, how should training resources be allocated to achieve higher test performance or better training efficiency? Regarding training efficiency, two approaches may be considered. On the one hand, increasing the number of SGD steps previously discussed may seem promising. However, experimental findings do not support the effectiveness of this approach (see Section 5.1). On the other hand, under fixed rollout budget (i.e., the number of samples to be rolled out), one might expect significant reduction in rollout time tR as training resources are scaled up. In practice, however, this expectation is not fully realized. Table 9 shows the rollout time tR for The number of H800 Rollout time tR (reduction) 32 375 128 256 270 (-105) 225 (-45) 205 (-20) Table 9: Rollout time tR (seconds) for generating 1024 responses in one training step. The data shows that as computational resources increase, the incremental reduction in tR diminishes. 1024 samples under varying training resources. Notably, as training resources increase, the reduction in tR diminishes. This is because tR is primarily determined by the batch size and the time required to generate the longest response. Once sufficient resources are available, further scaling does not significantly reduce the processing time dominated by the generation of the longest sample. Therefore, when additional training resources are available, more effective strategy is to increase the rollout budget appropriately, such that the rollout time tR remains roughly constant or increases only marginally. By leveraging larger rollout buffer, more accurate gradient estimates can be obtained, which may improve training efficiency and enhance test performance. In the following, we focus on how the rollout budget determined by the rollout batch size and group size affects RL performance. The overall idea of these studies are illustrated in Figure 24 Larger Batch Size, Better Test Performance. To investigate how the rollout batch size DR affects the training dynamics, we conducted the following ablation experiments. 29 Figure 24: Overview of empirical studies on the effect of an increased rollout budget when more training resources are available. Grey and green blocks: The motivation of the empirical studies. Yellow blocks: The experimental variables in the empirical studies. Ablation Experiments 11: The Impact of Rollout Batch Size DR Consider the quadruple (NSGD, DR, DT , Nreuse). We consider the baseline experiment with the quadruple (1,64,64,1) in Section 4.1 and two on-policy experiments in Ablation Experiments 7 with the quadruples (1,32,32,1) and (1,16,16,1) respectively. These three experiments were conducted using 64,32 and 16 H800 respectively. We present the experimental results in Figure 25. The results in Figure 25 indicate that increasing the rollout batch size DR in accordance with available training resources can lead to better test performance with similar training time consumption. Larger Group Size, Better Test Performance. To investigate how the group size affects the training dynamics, we conducted the following ablation experiments. Ablation Experiments 12: The Impact of Group Size (gs) Consider the baseline experiment with group size 16 in Section 4.1. We ran two additional on-policy experiments with gs = 8, 4 respectively. These three experiments were conducted using 64,32 and 16 H800 respectively. The experimental results are presented in Figure 26. It can be observed from Figure 26, given more training resources, increasing rollout budget by increasing the group size can lead to better test performance with similar total training hours."
        },
        {
            "title": "6 Dataset Preparation",
            "content": "In this section, we introduce the processing pipeline for our RL training data. 30 Figure 25: Results of Ablation Experiments 11. Given more training resources, increasing the rollout budget by increasing DR achieves better test performance with similar total training hours. Figure 26: Results of Ablation Experiments 12. Given more training resources, increasing rollout budget by increasing the group size can achieve better test performance with similar total training hours."
        },
        {
            "title": "6.1 Data Source Selection and Preprocessing",
            "content": "For the math domain, we primarily focus on NuminaMath-1.5 [13], comprehensive dataset containing 896K math problems drawn from widely used sources and advanced mathematical topics. Although the dataset is sufficiently large, its quality requires careful examination prior to use. For the code domain, we find that data source options are more limited, and the overall difficulty of available datasets is generally low relative to the capabilities of current models. In our pilot studies, we experimented with several popular datasets including CODE-RL [12], TACO [14], and the Eurus-RL collection [2] in their original mixtures, but obtained unsatisfactory results. Selection Criteria To select and curate high-quality data for RL, we adhere to the following general criteria for both data domains: 1. Verifiable: We exclude problems that cannot be verified, such as proof-based problems and code problems lacking test cases. 2. Correct: We filter out math problems with invalid or incorrect answers, as well as code problems without comprehensive test cases. 3. Challenging: We pre-filter problems for which all generations from the base model are either entirely correct or entirely incorrect. Following these criteria, we incorporate challenging problems from NuminaMath-1.5 and other sources to enhance problem difficulty and diversity in our data mixture: 1) NuminaMath-1.5 subsets: amc aime, olympiads, olympiads ref, aops forum, cn contest, inequalities, and number theory. 2) DeepScaleR. 3) STILL3-Preview-RL-Data. 4) Omni-MATH. 5) AIME problems prior to 2024. For the code data mixture, we primarily consider problems from the following two sources, which offer sufficiently challenging coding questions: 1) LeetCode problems [30]. 2) TACO [15]. Preprocessing Pipeline For both math and coding problems, we first perform in-dataset deduplication to eliminate redundancy. For all collected math problems: We use Math-Verify [11] to re-extract answers from the provided textual solutions and retain only those problems where the extracted answer matches the corresponding answer in the dataset. We remove all instances that contain external URLs or potential figures in the problem statement. We then perform cross-dataset deduplication to eliminate potentially duplicated problems from similar sources and decontaminate against AIME24 and AIME25 problems, following DeepScaleRs deduplication scheme. This process yields approximately 105K math problems. For coding problems, we apply more rigorous filtering process as follows: We discard samples with empty, incomplete, or corrupted original unit test cases. We programmatically verify all test cases using the provided original solutions. sample is marked as valid only if the solution passes all corresponding test cases perfectly. We conduct extensive deduplication based on embedding similarity across the collected coding problems, as many share the same problem with only slight variations in instructions. This results in total of 13.7K coding questions (2.7K from LeetCode and 11K from TACO) in the final dataset."
        },
        {
            "title": "6.2 Model-Aware Difficulty Estimation",
            "content": "Due to the zero-advantage in GRPO when all sampled responses are either entirely correct or entirely incorrect within group, we conduct an initial offline difficulty estimation for each problem relative to the models being trained. Specifically, for each problem, we perform N=16 rollouts for math problems and N=8 for coding questions using temperature of 1.0 and maximum token length of 32K, and use the percentage of correct solutions as proxy for problem difficulty with respect to given model. After verifying the correctness of the sampled solutions, we exclude problems with 0/N (all incorrect) or N/N (all correct) rollouts. We report the percentage statistics of discarded and retained math/code problems for both the 7B and 32B models as follows: 0 Correct (math/code) Correct (math/code) Remaining (math/code) Deepseek-R1-Distill-Qwen-7B 21.4% / 28% 32.4% / 24% 46.2% / 48% Deepseek-R1-Distill-Qwen-32B 20.7% / 17.1% 42.0% / 45.4% 37.3% / 37.6%"
        },
        {
            "title": "6.3 Quality Assessment via Human and LLM-as-a-Judge",
            "content": "During the data processing stage, we identified that many problems in the math portion were either incomplete or poorly formatted. Consequently, we conducted an additional round of strict human-LLMcombined inspection to ensure data quality. We sampled few hundred questions from the remaining pool and asked human evaluators to assess whether each problem met the following criteria: 1. Clear Wording: Is the problem stated in way that is easy to understand? 2. Complete Information: Does the problem provide all necessary details? 3. Good Formatting: Are the numbers, symbols, and equations clear and appropriately formatted? 4. No Distractions: Is the problem free of irrelevant information? We provide below examples of original problem statements that human evaluators identified as problematic: Incomplete Problems: 6. Five spherical surfaces can divide space int Which of the following numbers is equal to 33 million? (STILL-3-Preview-RL-Data) Which number is greater than 0.7 (STILL-3-Preview-RL-Data) Example 27 Find σ2(28) = ? (NuminaMath-1.5, Number Theory) parts. (NuminaMath-1.5, Olympiads) Irrelevant Information: 250. = ln (cid:0)x3 1(cid:1).nn 250. = ln (cid:0)x3 1(cid:1).nn The above text has been translated into English, retaining the original texts line breaks and format. However, since the original text is mathematical expression, the translation is identical to the original as mathematical expressions are generally universal and do not change between languages. (NuminaMath-1.5, Olympiads) 1. (12 points) The figure is composed of 5 identical squares. The number of triangles that can be formed using the 12 points in the figure as vertices is.10. (12 points) The figure is composed of 5 identical squares. The number of triangles that can be formed using the 12 points in the figure as vertices is. (NuminaMath-1.5, Olympiads) Interestingly, these problems passed the difficulty estimation procedure (i.e., model can produce correct answer even when the problem is invalid or incomplete). This indicates that the models answered these problems correctly at least once during the 16 rollouts, suggesting they may have been trained on similar examples or that the answers were trivially guessable. To efficiently curate the entire dataset, we employed Llama-3.3-70B-Instruct and Qwen2.5-72B-Instruct to automatically filter out low-quality problems. Each model was prompted to evaluate given math problem based on clarity, completeness, formatting, and relevance, and to identify reasons problem might be considered low quality, ultimately providing binary rating. This process mimics human assessment while being significantly more efficient. For each problem and each LLM judge, we collected 16 evaluations, resulting in total of 32 votes per problem. We retained problems that received at least 9 valid votes and removed approximately 1K-2K math questions in total."
        },
        {
            "title": "7.1 Math Verifiers",
            "content": "During the initial stage of all experiments on math reasoning, we conducted several preliminary analyses of the rule-based math verifiers available at the time. These verifiers included: The original MATH verifier (verl version) PRIME verifier Qwen2.5 verifier DeepScaleRs verifier Math-Verify We first sampled small set of problems along with their associated solutions and answers, and manually examined the quality of their parsers and verifiers. We found that the Qwen2.5 verifier tends to lose information during the parsing process (e.g., when parsing boxed{a^2}} $, it fails to retain ^2). We also observed that the PRIME verifier can occasionally stall during execution. As result, we excluded these two verifiers from further analysis. We then used rollout data from the difficulty estimation procedure and applied the remaining verifiers to evaluate the generated solutions. We plotted the number of problems at each difficulty level (08) in Figure 27: Figure 27: Distributions of the number of correct rollouts from DeepSeek-R1-Distill-Qwen-7B, obtained using four different verifiers on subset of NuminaMath-1.5 problems. The numbers 08 indicate difficulty levels. The size of each sector represents the number of problems at specific difficulty level. Based on combination of verifier results and human judgments, we observed the following: Both the original MATH verifier (verl version) and DeepScaleRs verifier produced higher rates of false positives and false negatives. For Math-Verify, some implementation details changed as we explored different versions. Therefore, we include both version 0.5.2 and the default version (0.6.0), which we extensively used in model development, noting only trivial differences between them. Note that Math-Verify may still yield incorrect results for solutions with non-standard formatting or mathematical expressions it does not support (e.g., problems with multiple answers). In our final implementation of the reward function, we verify whether the answer in text solution is correct using the following steps: 34 Extract the answer that appears after the reasoning process. Use Math-Verifys parser to parse the answer and obtain its string representation. If the string representation directly matches the gold answer, return True; otherwise, fall back to Math-Verifys verify function. Wrap the gold answer in boxed{} and run the verification to obtain the final result. We find that wrapping the gold answer with boxed{} is crucial step. Parsing the gold answer directly can alter the mathematical expression."
        },
        {
            "title": "7.2 Code Sandboxes",
            "content": "For unit test execution, we constructed highly efficient and secure local code sandbox based on LiveCodeBenchs implementation, leveraging subprocess processing. This sandbox supports various testing methods, including standard input-output testing, solution function unit testing, and assertion-based tests. To further enhance its security and robustness, we implemented the following measures: Syntax validation: We first validate submitted solutions using Abstract Syntax Trees (AST). If syntax errors are detected, the sandbox immediately terminates the test and returns False. Memory monitoring: During training, we identified potential memory leak risks in some generated solutions. To mitigate this, we integrated memory monitoring mechanism for each test process. If process exceeds 50GB of memory usage, the sandbox proactively terminates the test and returns False, effectively preventing resource exhaustion. Parallel stability optimization: Initially, we used asynchronous testing combined with process pools for parallel execution. However, we later discovered that the sandbox could crash under this setup, leading to incorrect test results. To resolve this, we revised our approach to rely solely on multiprocessing, ensuring stable and efficient parallel execution. Additionally, we conducted performance comparison between our sandbox and the PRIME sandbox. The results demonstrate the superior effectiveness of our implementation on specific datasets. Notably, the PRIME sandbox occasionally misclassified correct solutions as failures, whereas our sandbox more accurately evaluated solution correctness. It is also important to note limitation of our sandbox identified during practical usage: it does not currently handle cases where the same input can yield multiple valid outputs. Such cases are common in real-world code testing scenarios involving non-deterministic or open-ended problems."
        },
        {
            "title": "8 Experiments",
            "content": "In this section, we present the experimental results of our three models: Skywork-OR1-Math-7B, SkyworkOR1-7B, and Skywork-OR1-32B. We begin with the details of the training configurations, followed by an analysis of the training results. Finally, we discuss the evaluation outcomes."
        },
        {
            "title": "8.1 Training and Evaluation Details",
            "content": "Training Configurations Below, we describe the training configurations of our Skywork models. The 7B and 32B models are fine-tuned based on DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Qwen-32B, respectively. We collect math and code problems from various sources and apply comprehensive preprocessing, difficulty filtering, and quality control. This ensures problem mixture that is verifiable, valid, and challenging. See Section 6 for details. Based on this curated mixture, all three models are fine-tuned by optimizing the policy loss (3.1) with constant learning rate of 1e-6, clip ratio of 0.2, target entropy of 0.2, sampling temperature of 1.0, and rejection sampling. Notably, we do not apply any KL loss in our training process, as 35 discussed in Section 3.2.6. Please refer to Section 3.1 for more details on the policy update procedure. All experiments use multi-stage training. We report the detailed configuration for each training stage in Table 10, Table 11, and Table 12. The released checkpoints correspond to step 2160 for Skywork-OR1-Math-7B, step 1320 for Skywork-OR1-7B, and step 1000 for Skywork-OR1-32B. Stage Steps 1 2 3 3. 0-740 740-1740 1740-2080 2080-2160 Context Length 8K 16K 32K 32K Batch Size Mini-batch Size Group Size 256 256 256 128 128 128 128 16 16 16 64 Table 10: Training configurations of Skywork-OR1-Math-7B. Stage Steps 1 2 0-660 660Context Length 16K 32K Batch Size Mini-batch Size Group size 256 160 256 160 16 Table 11: Training configurations of Skywork-OR1-7B. Stage Steps 1 2 0-760 760-1130 Context Length 16K 24K Batch Size Mini-batch Size Group Size 256 160 256 160 16 32 Table 12: Training configurations of Skywork-OR1-32B. Benchmarks & Baselines We evaluate our models on challenging benchmarks. For math capabilities, we assess performance on the American Invitational Mathematics Examination (AIME) 2024 and 2025. For coding capabilities, we use LiveCodeBench [10] (from 2024-08 to 2025-02). We compare against several strong baselines, including DeepSeek-R1 [3], Qwen3-32B [32], QwQ-32B [25], Light-R1-32B [29], TinyR1-32B-Preview [27], and several 7B RL models based on DeepSeek-R1-Distill-Qwen-7B, such as AceReason-Nemotron-7B [1], AReaL-boba-RL-7B [18], and Light-R1-7B-DS [29]. Evaluation Setup We set the maximum generation length to 32,768 tokens for all models. For AIME24/25, we report avg@32 performance; for LiveCodeBench (2024-08 to 2025-02), we report avg@4 performance. Responses are generated using temperature of 1 and top-p of 1. The avg@n metric is defined as avg@n = 1 (cid:88) i= {(x, yi) is correct}, where is the evaluation question and yi is the i-th response."
        },
        {
            "title": "8.2 Evaluation Results of Skywork-OR1 models",
            "content": "As shown in Table 13, Skywork-OR1 models achieve significant improvements over their base SFT models (e.g., the DeepSeek-R1-Distill series). Specifically, Skywork-OR1-32B achieves scores of 82.2 on AIME24, 73.3 on AIME25, and 63.0 on LiveCodeBench, outperforming strong contemporary models such as DeepSeek-R1 and Qwen3-32B on key math benchmarks, setting new SOTA records at the time of release. SkyworkOR1-7B scores 70.2 on AIME24, 54.6 on AIME25, and 47.6 on LiveCodeBench, demonstrating competitive"
        },
        {
            "title": "Model",
            "content": "AIME 24 avg@32 AIME 25 avg@32 LiveCodeBench (2024-08 - 2025-02) avg@4 DeepSeek-R1-Distill-Qwen-7B Light-R1-7B-DS AReaL-boba-RL-7B AceReason-Nemotron-7B Skywork-OR1-Math-7B Skywork-OR1-7B 7B Models 55.5 59.1 61.9 69.0 69.8 70. 32B Models DeepSeek-R1-Distill-Qwen-32B TinyR1-32B-Preview Light-R1-32B QwQ-32B Qwen3-32B DeepSeek-R1 Skywork-OR1-32B 72.9 78.1 76.6 79.5 81.4 79.8 82.2 39.2 44.3 48.3 53.6 52.3 54.6 59.0 65.3 64.6 65.3 72.9 70.0 73.3 37.6 39.5 - 51.8 43.6 47. 57.2 61.6 - 61.6 65.7 65.9 63.0 Table 13: Comparison of Skywork-OR1 models and other models on reasoning-related benchmarks. performance relative to similarly sized models across both math and coding tasks. Our earlier released model, Skywork-OR1-Math-7B, also delivers competitive results among models of similar size, scoring 69.8 on AIME24, 52.3 on AIME25, and 43.6 on LiveCodeBench. These SOTA results are especially noteworthy given that they are obtained through fine-tuning the DeepSeek-R1-Distill series SFT base models with relatively modest initial performance clearly demonstrating the substantial impact of our pipeline."
        },
        {
            "title": "9 Conclusion",
            "content": "In this work, we present Skywork-OR1, an effective and scalable reinforcement learning (RL) implementation for enhancing the reasoning capabilities of long CoT models. Building upon the DeepSeek-R1-Distill model series, our RL approach achieves significant performance improvements on various mathematical and coding benchmarks. The Skywork-OR1-32B model outperforms both DeepSeek-R1 and Qwen3-32B on AIME24 and AIME25, while delivering comparable results on LiveCodeBench. Additionally, the Skywork-OR1-7B and Skywork-OR1-Math-7B models demonstrate competitive reasoning performance among similarly sized models. Our comprehensive ablation studies validate the effectiveness of the core components in our training pipeline, including data mixture and filtration, multi-stage training without advantage masking, high-temperature sampling, exclusion of KL loss, and adaptive entropy control. We conduct extensive investigations into entropy collapse phenomena, identifying key factors that influence entropy dynamics. Our findings show that preventing premature entropy collapse is critical for achieving optimal test performance, offering valuable insights for future research and development. Furthermore, we explore how different training resource allocations affect both training efficiency and final model performance."
        },
        {
            "title": "References",
            "content": "[1] Yang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee, Mohammad Shoeybi Peng Xu, and Wei Ping Bryan Catanzaro. Acereason-nemotron: Advancing math and code reasoning through reinforcement learnin, 2025. 37 [2] Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, Jiarui Yuan, Huayu Chen, Kaiyan Zhang, Xingtai Lv, Shuo Wang, Yuan Yao, Xu Han, Hao Peng, Yu Cheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou, and Ning Ding. Process reinforcement through implicit rewards. CoRR, abs/2502.01456, 2025. [3] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [4] Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. Omni-math: universal olympiad level mathematic benchmark for large language models, 2024. [5] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [6] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [7] Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Bo An, Yang Liu, and Yahui Zhou. Skywork open reasoner series. https://capricious-hydrogen-41c.notion.site/S kywork-Open-Reaonser-Series-1d0bc9ae823a80459b46c149e4f51680, 2025. Notion Blog. [8] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Openreasoner-zero: An open source approach to scaling up reinforcement learning on the base model, 2025. [9] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [10] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint, 2024. [11] Hynek Kydlíček. Math-verify: robust mathematical expression evaluation system. https://github.c om/huggingface/Math-Verify, 2025. Version 0.6.1. [12] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu-Hong Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. [13] Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Numinamath. [https://huggingface.co/AI-MO/NuminaMath-1.5](ht tps://github.com/project-numina/aimo-progress-prize/blob/main/report/numina_dataset.p df), 2024. [14] Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and Ge Li. TACO: topics in algorithmic code generation dataset. CoRR, abs/2312.14852, 2023. [15] Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and Ge Li. Taco: Topics in algorithmic code generation dataset. arXiv preprint arXiv:2312.14852, 2023. [16] Michael Luo, Sijun Tan, Roy Huang, Ameen Patel, Alpay Ariyak, Qingyang Wu, Xiaoxiang Shi, Rachel Xin, Colin Cai, Maurice Weber, Ce Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepcoder: 38 fully open-source 14b coder at o3-mini level. https://pretty-radio-b75.notion.site/DeepCoder -A-Fully-Open-Source-14B-Coder-at-O3-mini-Level-1cf81902c14680b3bee5eb349a512a51, 2025. Notion Blog. [17] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl. https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-P review-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2, 2025. Notion Blog. [18] Ant Research RL Lab. Areal: Ant reasoning rl. https://github.com/inclusionAI/AReaL, 2025. [19] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015. [20] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [21] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [22] Richard S. Sutton and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2nd edition, 2018. [23] Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems, pages 10571063, 1999. [24] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [25] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. [26] RUCAIBox STILL Team. Still-3-1.5b-preview: Enhancing slow thinking abilities of small models through reinforcement learning. 2025. [27] TinyR1 Team. Superdistillation achieves near-r1 performance with just 5 [28] Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, et al. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond. arXiv preprint arXiv:2503.10460, 2025. [29] Liang Wen, Fenrui Xiao, Xin He, Yunke Cai, Zhenyu Duan Qi An, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, and Xiangzheng Zhang. Light-r1: Surpassing r1-distill from scratch with $1000 through curriculum sft & dpo, 2025. [30] Yunhui Xia, Wei Shen, Yan Wang, Jason Klein Liu, Huifeng Sun, Siyue Wu, Jian Hu, and Xiaolong Xu. Leetcodedataset: temporal dataset for robust evaluation and efficient training of code llms. arXiv preprint arXiv:2504.14655, 2025. [31] Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning, 2025. [32] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi 39 Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [33] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [34] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [35] Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, et al. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025. [36] Yuxin Zuo, Kaiyan Zhang, Shang Qu, Li Sheng, Xuekai Zhu, Biqing Qi, Youbang Sun, Ganqu Cui, Ning Ding, and Bowen Zhou. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084, 2025."
        }
    ],
    "affiliations": [
        "Skywork AI, Kunlun Inc"
    ]
}