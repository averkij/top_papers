{
    "paper_title": "Medical large language models are easily distracted",
    "authors": [
        "Krithik Vishwanath",
        "Anton Alyakin",
        "Daniel Alexander Alber",
        "Jin Vivian Lee",
        "Douglas Kondziolka",
        "Eric Karl Oermann"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have the potential to transform medicine, but real-world clinical scenarios contain extraneous information that can hinder performance. The rise of assistive technologies like ambient dictation, which automatically generates draft notes from live patient encounters, has the potential to introduce additional noise making it crucial to assess the ability of LLM's to filter relevant data. To investigate this, we developed MedDistractQA, a benchmark using USMLE-style questions embedded with simulated real-world distractions. Our findings show that distracting statements (polysemous words with clinical meanings used in a non-clinical context or references to unrelated health conditions) can reduce LLM accuracy by up to 17.9%. Commonly proposed solutions to improve model performance such as retrieval-augmented generation (RAG) and medical fine-tuning did not change this effect and in some cases introduced their own confounders and further degraded performance. Our findings suggest that LLMs natively lack the logical mechanisms necessary to distinguish relevant from irrelevant clinical information, posing challenges for real-world applications. MedDistractQA and our results highlights the need for robust mitigation strategies to enhance LLM resilience to extraneous information."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 1 0 2 1 0 . 4 0 5 2 : r a"
        },
        {
            "title": "Medical large language models are easily distracted",
            "content": "Krithik Vishwanath1,3,4, Anton Alyakin1,5, Daniel Alexander Alber1, Jin Vivian Lee5, Douglas Kondziolka1, Eric Karl Oermann1,2,6 1Department of Neurological Surgery, 2Department of Radiology, NYU Langone Medical Center, New York, New York, 10016 3Department of Aerospace Engineering and Engineering Mechanics, 4Department of Mathematics The University of Texas at Austin, Austin, Texas, 78712 5Department of Neurosurgery, Washington University School of Medicine in St. Louis, St. Louis, Missouri, 63110 6Center for Data Science, New York University, New York, New York, 10016 Send correspondence to: krithik.vish@utexas.edu, eric.oermann@nyulangone.org Abstract Large language models (LLMs) have the potential to transform medicine, but real-world clinical scenarios contain extraneous information that can hinder performance. The rise of assistive technologies like ambient dictation, which automatically generates draft notes from live patient encounters, has the potential to introduce additional noise making it crucial to assess the ability of LLMs to filter relevant data. To investigate this, we developed MedDistractQA, benchmark using USMLE-style questions embedded with simulated real-world distractions. Our findings show that distracting statements (polysemous words with clinical meanings used in non-clinical context or references to unrelated health conditions) can reduce LLM accuracy by up to 17.9%. Commonly proposed solutions to improve model performance such as retrieval-augmented generation (RAG) and medical fine-tuning did not change this effect and in some cases introduced their own confounders and further degraded performance. Our findings suggest that LLMs natively lack the logical mechanisms necessary to distinguish relevant from irrelevant clinical information, posing challenges for real-world applications. MedDistractQA and our results highlights the need for robust mitigation strategies to enhance LLM resilience to extraneous information. Keywords: Data contamination, USMLE, distractions, medical Q&A, MedDistractQA Preprint. Under review."
        },
        {
            "title": "Main Text",
            "content": "Clinical history is central to medical diagnosis, requiring physicians to distinguish critical facts from irrelevant details. However, patient histories often include extraneous information, making effective filtering essential for clinical AI. In recent years, the rise of ambient dictation further complicates this process by introducing unsupervised and often irrelevant content into clinical notes [1]. Medical imaging studies have highlighted the vulnerability of AI models to confounding, where they mistakenly learn spurious correlations between irrelevant data and clinical outcomes [2]. Large language models (LLMs) synthesize medical knowledge with superhuman speed and perform well on board-style exams [3]. Proprietary language models like GPT-4o [4, 5] and Claude Sonnet [5], as well as open-source, generalist models such as Llama [6] and Gemma [5, 7] possess capabilities rivaling those of medical professionals on standard benchmarks. Medically fine-tuned language models, including UltraMedical (Llama 3-3) [6], Meditron (Llama 2) [8], MedPaLM (PaLM) [3], and MedMobile (Phi-3) [9], further refine these capabilities. However, LLMs remain susceptible to distraction: irrelevant information, ambiguous prompts, or subtle (but semantically neutral) changes in input structure degrade benchmark performance [10]. Unlike humans who rely on common sense to filter noise, LLMs lack similar mechanisms, leaving them potentially susceptible to noise when parsing complex medical narratives [10, 11]. Prior studies have shown that simple alterations, such as changing the order of clinical details presented, can reduce diagnostic accuracy by up to 18% [10]. Similar vulnerabilities have been reported in non-medical domains. For example, LLMs in the Grade-School Math with Irrelevant Context (GSM-IC) dataset showed declines in accuracy of up to 35% when arithmetic problems were interspersed with irrelevant text [11]. Another study showed that GSM8K models experienced performance declines of up to 65% when datasets were injected with non-operational statements [12]. To systematically evaluate LLMs sensitivity to irrelevant information in medical scenarios, we develop MedDistractQA (Fig. 1a) as an extension of the MedQA benchmark [13]. MedQA, based on United States Medical Licensing Exam (USMLE) questions, is routinely used to assess clinical LLMs [3, 4, 5, 6, 7, 8, 9]. MedDistractQA injects confounding statements into the MedQA dataset to quantitatively measure LLM robustness against distractions that could appear in real-world clinical settings. We further hypothesized that if introducing confounding tokens could degrade performance in our distraction benchmarks, then retrieval augmented generation (RAG) with poor retrieval could behave similarly and might not be as helpful as it is often portrayed. In second set of experiments, we further demonstrate that RAG with poor retrieval can introduce extraneous information and negatively impact model accuracy as well. 2 Figure 1: Overview of our study. To create the MedDistractQA datasets, we combine MedQA question with confounding statement generated by GPT-4o. GPT-4o is prompted to utilize clinical terminology from randomly selected incorrect answer, and ensure that the statement bears no clinical value. The confounding statement is embedded within the question itself, and is comically irrelevant to the diagnosis. For the MedDistractQA benchmarks, each question contains its own unique distracting statements. b, Example MedDistractQA-Nonliteral and MedDistractQA-Bystander questions, respectively, and GPT-4o incorrect responses. 3 We used two types of distractions in our benchmark: (i) nonliteral use of medical terms in non-clinical contexts (MedDistractQA-Nonliteral), and (ii) extraneous medical details attributed to third parties (MedDistractQA-Bystander), such as family member or pet (MedDistractQA-Bystander) which may be included in patients social history. For MedDistractQA-Nonliteral, LLM accuracy declined by 2.2% to 17.8% across all models (Fig. 2a, Fig. 2c). For MedDistractQA-Nonliteral, LLM accuracy dropped by 2.2% to 17.8% across all models (Fig. 2a, Fig. 2c). Notably, open-source models were more adversely affected by distractions than proprietary models. Specifically, general open-source models experienced 10.9% decline compared to only 3.8% decline in proprietary models (Extended Data Fig. 1, = 5.38 108), and medically fine-tuned open-source models saw 10.0% decline versus 3.8% for proprietary models (p = 0.0375). Similarly, for MedDistractQA-Bystander, LLM accuracy declined by 1.3% to 17.9%. General open-source models dropped by 10.6% compared to 3.7% for proprietary models (p = 2.11 106), and medically fine-tuned open-source models declined by 9.0% versus 3.7% for proprietary models (p = 0.0691). Higher baseline MedQA performance correlated with greater robustness (r2 = 0.578, = 1.10 106 for MedicalDistractQA-Nonliteral; r2 = 0.486, = 1.87 106 for MedDistractQA-Bystander, Extended Data Fig. 2). Reasoning-focused proprietary models, such as Claude 3.7 Sonnet and o3, were the most robust of our tested model families. Fine-tuning on medical data alone did not significantly alter robustness to distractions across all models (p = 0.683 for MedDistractQA-Nonliteral; = 0.550 for MedDistractQA-Bystander). Within the Llama-3-8B model series, the UltraMedical and Meerkat models - despite sharing the same base model - exhibited different levels of robustness. Meerkat showed significantly greater resilience compared to the base model (pMedDistractQA-Nonliteral = 0.0438; pMedDistractQA-Bystander = 0.00218), whereas UltraMedical appeared to non-significantly hurt model resilience (pMedDistractQA-Nonliteral = 0.157; pMedDistractQA-Bystander = 0.139). Furthermore, distilled reasoning training worsened both baseline MedQA accuracy (p = 5.47 106) and resilience to distractions (pMedDistractQA-Nonliteral = 0.0585; pMedDistractQA-Bystander = 0.00262). Explicitly instructing models to ignore irrelevant information had no significant effect on performance (p = 0.341, Extended Data Fig. 3). We also analyzed the effect of distractions on LLMs across physician competencies derived from the USMLE framework, USMLE Physician Tasks/Competencies [14] (Extended Data Fig. 4). Accuracy dropped most significantly in the \"Patient Care: Diagnosis\" (-10.1%) and \"Medical Knowledge/Scientific Facts\" (-10.1%) categories. The least affected competency was \"Systems-based Practice, Including Patient Safety\" (+1.4%). Among human systems categories, the \"Respiratory System\" suffered the most (-11.0%), while \"Legal/Ethical Issues\" were least impacted (+2.2%)(Extended Data Fig. 5). To investigate the impact of incorporating new information via RAG and its potential as distractor to LLMs, we evaluated the performance of LLMs in the presence of relevant text excerpts from Harrisons Principles of Internal Medicine, 21st Edition [15]. Similar to the effects observed with MedDistractQA distractions, RAG produced significant performance degradations, with declines ranging from 10.3% to increases of +1.9% (Fig. 2e). However, this degradation was slightly less than that observed with MedDistractQA-Nonliteral (p = 3.02 109) and with MedDistractQA-Bystander (p = 1.16 108). Moreover, performance degradation in MedQA+RAG correlated with performance degradation in MedDistractQA-Nonliteral (r2 = 0.17, = 0.0260) and with MedDistractQA-Bystander (r2 = 0.18, = 0.0207), suggesting that RAG introduces similar risks of confounding. The cosine-similarity rank of retrieved text had no significant impact on model accuracy (Extended Data Fig. 6). Figure 2: MedDistractQA experimental results between proprietary, general open-source, and medical open-source models. a, shows accuracy drop on the MedQA for leading models on the MedDistractQA-Nonliteral. c, shows accuracy drop on the MedQA for leading models on the MedDistractQA-Bystander. displays the loss of model accuracy with the introduction of high-quality context (RAG) from Harrisons Internal Medicine 21e [15]. Error bars show the 95% SE. 5 Medical LLMs have been claimed to approach or exceed human clinicians on diagnostic tasks [3, 16]. Although these models excel on standardized multiple-choice exams, our findings reveal critical weakness: LLMs struggle with irrelevant and distracting information commonly encountered in clinical practice. This vulnerability poses significant risks for deploying medical AI models in real-world settings, where clinicians must routinely filter extraneous details. Our study characterizes this gap through three key contributions: quantitative assessment of how different types of distractions impact model accuracy; an exploratory analysis showing that RAG - despite its touted benefits - can introduce similar confounding effects; and the establishment of benchmarks incorporating curated distracting statements to support future research in generative AI solutions. Our results show that state-of-the-art models exhibit significant performance declines when distractions are introduced in medical Q&A, suggesting that LLMs lack the intrinsic ability of human clinicians to filter irrelevant information. The capacity to convert conversations from live patient encounters into clinically relevant outputs - key requirement for AI diagnostic pilots - remains limited. Transformerbased models allocate attention through learned weights rather than clinical hierarchies, making them prone to \"recency bias\" where they overweight later inputs regardless of medical relevance [11]. This limitation complicates their integration into standardized care pathways, where reliability is critical. Other studies corroborate our findings. LLMs parsing discharge summaries with mixed critical and incidental findings often misprioritize information, leading to diagnostic errors [10]. When tested on 2,400 real patient cases, LLMs demonstrated 1625% lower diagnostic accuracy compared to physicians, with performance variability directly tied to input structure [10]. Notably, presenting the same clinical data in different order caused diagnostic accuracy to fluctuate by up to 18%, suggesting that model conclusions depend more on input sequence than medical relevance [10]. We also observed strong correlation between model baseline accuracy and resilience to distractions (r0.70, Extended Data Fig. 2). Larger, more capable models - such as o3, Claude, and Llama 370B - were less easily misled, likely due to more robust understanding of medical reasoning. Similar trends have been reported in the literature. 70B model reduced errors on complex medical reasoning tasks nearly twice as effectively as 10B model [17]. Likewise, an improved model like Med-PaLM 2, which improved MedQA accuracy from 67% to 86.5% compared to its predecessor, showed significant gains on adversarial challenge questions [16]. Recent work confirms this pattern: when exposed to distractions in adversarial questioning, state-of-the-art models such as GPT-4 and Anthropic Claude retained near-human-level performance on USMLE-style questions, while less capable models suffered greater accuracy drops with each additional distractor [18]. This robustness likely stems from broader training exposure, as stronger models can recognize illogical information while weaker ones are easily misled. Proprietary models consistently outperformed open-source models in handling distractions. However, this advantage may stem from superior baseline performance rather than targeted robustness mechanisms. We acknowledge that lack of transparency regarding proprietary architectures and training data precludes any definitive, mechanistic conclusions. Interestingly, our results challenge the assumption that medical fine-tuning improves robustness. While some initial research suggests that medically fine-tuned models are more vulnerable to distractions [19], we found no significant differences between medical and general open-source models overall. However, within the series of Llama 3-8B models - where architecture and size are constant - fine-tuned medical models (e.g., Llama-3-8B-Meerkat and Llama-3-8B-UltraMedical) exhibit higher baseline performance but not necessarily greater resilience to distractions. This pattern suggests that fine-tuning may overfit models to specific tasks, rendering 6 them more susceptible to irrelevant details. Finally, our study reveals RAG as an unexpected source of distraction. While RAG is commonly proposed as solution to enhance medical LLM accuracy, our findings indicate it can function as distractor as opposed to mitigator. Recent studies show that RAG systems introducing extraneous or conflicting information degrade model coherence and increase hallucinations [20, 21, 22, 23]. Likewise, we found that adding high-quality retrieved context (e.g., from Harrisons Internal Medicine 21e) decreased accuracy of most tested models, mirroring the effects of MedDistractQA. Altogether, these results suggest that indiscriminate implementation of RAG may introduce information overload, serving to impair rather than improve decision-making. These findings emphasize the necessity of fine-tuning RAG systems for beneficial deployment. This study has several limitations. First, we focused on the MedQA dataset, which although widely used, does not necessarily encompass the full range of clinical reasoning tasks. The exclusive use of multiple-choice questions limits generalizability and may not extend to other formats such as openended problem-solving. Additionally, our distracting statements were generated algorithmically using specific LLM. Thus, although they were designed to mimic realistic distractions, they may not fully capture the complexity or breadth of real-life clinical discourse. Finally, while we observed strong correlation between baseline accuracy and resilience to distractions, further controlled experiments are needed to establish causality. Future research should expand these distraction evaluations beyond MedQA to encompass diagnostic reasoning, treatment planning, and patient triage. Developing targeted mitigation strategies - from improved prompt engineering to context-aware filtering and architectural modifications - will be essential for clinical deployment. Particular attention should be paid to emerging technologies like ambient dictation systems, which may introduce substantial extraneous information into downstream AI processes. Additionally, evaluating model performance across structured data and multimodal inputs could reveal new failure modes. Understanding how model architecture, training scale, and data quality influence distraction susceptibility will be crucial for building robust clinical AI systems. Our findings indicate that LLMs, even frontier models (Claude Sonnet, o3), are vulnerable to obvious distractions in medical narratives. Fine-tuning open-source LLMs with medical data provides only limited protection against these vulnerabilities. More concerning, RAG - common strategy for enhancing LLMs accuracy and robustness - can inadvertently introduce confounding information that degrades model performance. These results underscore how seemingly simple, common-sense scenarios can expose critical limitations in medical LLMs. As deployment of these systems accelerates, our findings emphasize the urgent need for robust evaluation frameworks that better reflect real-world clinical complexity."
        },
        {
            "title": "MedQA Benchmark",
            "content": "To determine an LLMs ability in the medical domain, we evaluate the model on the MedQA, USMLE-style question bank [13]. We choose to evaluate on this dataset due to the expert level of medical reasoning and knowledge required for USMLE-style questions, and to test the models ability against the range of critical clinical tasks such as differential diagnosis."
        },
        {
            "title": "MedDistractQA Benchmark Curation",
            "content": "To generate confounding statements, we first identify medical terms from the incorrect answer choices of each MedQA question. By focusing on these distractor terms rather than the correct ones, we ensure that the added statements do not directly hint at the true solution and instead create non-operational content. We parse the list of wrong answer choices and extract clinically relevant conceptssuch as diseases, conditions, or proceduresthat appear within them. For instance, if an incorrect choice in particular question includes heart attack, we flag that term for potential use in distracting statement. This selection process capitalizes on the inherent diversity of erroneous answer choices, which frequently contain common medical conditions that can be repurposed in nonclinical contexts. After extracting these terms, we prompt large language model (GPT-4o) to generate short, coherent sentences in which each medical term is used in nonclinical or socially oriented manner. The model is instructed to produce statements that sound natural yet are clinically irrelevantexamples might include The patients zodiac sign is Cancer (where Cancer is the zodiac rather than the disease) or My aunts fish had heart attack (imputing clinical symptom on distant bystander, in this case, fish of the patients aunt). These statements are then embedded into the original MedQA questions, creating augmented versions intended to distract or confuse the model. By systematically introducing such confounding statements, we can evaluate whether the presence of irrelevant medical language degrades the models ability to identify the correct clinical answer. This method of benchmark curation is visually depicted in Fig. 1a."
        },
        {
            "title": "Model Evaluation",
            "content": "To calculate accuracy of model on the MedQA, we use string-based matching on model output Inference is computed at temperature of 0 without ensemble. We note that chain-of-thought. some models do not allow for temperature control (e.g., OpenAIs reasoning models), and are left at their default. MedMobile is ran using the PyTorch and Transformers library on A100 GPUs during evaluation. vLLM is utilized for all other open-source and medically fine-tuned models on A100 GPUs. Proprietary models inference is generated via their respective official API provider. Model names are unedited, and are directly labeled as present in HuggingFace Hub or the corresponding proprietary providers API console."
        },
        {
            "title": "Prompting",
            "content": "Prompts utilized are available in within the GitHub repo. We keep an identical prompting template between evaluations of MedQA, MedDistractQA-Nonliteral, MedDistractQA-Bystander, and MedQA+RAG. Retrieval-Augmented Generation To conduct RAG based on vector embeddings, we compute cosine similarity based on MedCPT [24] vectors generation between the question and paragraphs in the textbook. RAG selects the paragraph with the highest cosine-similarity score for particular question. The source of information for these evaluations is from Harrisons Principles of Internal Medicine, 21e [15]. After selecting relevant paragraph from the corpora, we insert it into the prompt directly during inference."
        },
        {
            "title": "Categorization of Data",
            "content": "For each MedQA and MedDistractQA question, we utilize OpenAIs o3-mini to classify the category that the question best fits under. The categories topics are derived directly from the USMLE website [14]."
        },
        {
            "title": "Uncertainty Quantification and Significance Testing",
            "content": "The statistical analysis was performed on paired accuracy measurements from baseline and modified (MedDistractQA or MedQA+RAG) evaluations for each model. For each model, the fraction of correctly answered questions was computed under both conditions, and the difference in accuracy (multiplied by 100 to express percentage points) was calculated. To quantify variability, the standard error of the difference was estimated using formula derived from binomial variance components. Specifically, if p1 and p2 denote the baseline and MedDistractQA accuracies respectively, and p12 represents the joint accuracy (the proportion of questions correctly answered in both conditions), then the standard error was computed as SEdiff = p1(1 p1) + p2(1 p2) 2 (p12 p1p2) 100, where is the number of paired observations. This approach accounts for the covariance between the two conditions, ensuring more accurate estimate of the uncertainty in the observed differences. Further statistical analyses included group comparisons in which models were classified into three categories (general open-source, medical open-source, or proprietary). Grouped models are compared using Welchs t-test to account for differences in sample size and variance. The t-test produces two-tailed p-value which is then converted into one-tailed p-values to test directional hypotheses about which group exhibits greater degradation. Pairwise comparisons were conducted using two-sample t-tests (with both one-tailed and two-tailed tests) to assess the significance of observed differences in accuracy loss. Linear regression analyses were also performed to evaluate relationships between model size, baseline performance, and accuracy 9 loss, and performance degradation. Additionally, paired t-tests and correlation analyses were applied to compare the degradation in performance across different evaluation settings (MedDistractQA and RAG_medqa). All statistical tests were implemented using standard Python libraries such as numpy, scipy.stats, and pandas to ensure reproducibility."
        },
        {
            "title": "Acknowledgements",
            "content": "E.K.O. is supported by the National Cancer Institutes Early Surgeon Scientist Program (3P30CA01608741S1) and the W.M. Keck Foundation. We would like to acknowledge Nader Mherabi and Dafna Bar-Sagi, Ph.D., for their continued support of medical AI research at NYU. We thank Michael Constantino, Kevin Yie, and the NYU Langone High-Performance Computing (HPC) Team for supporting computing resources fundamental to our work."
        },
        {
            "title": "Author Contributions",
            "content": "E.K.O. and A. A. conceptualized and supervised the study. KV designed, implemented, and developed the LLM evaluation pipeline and the MedDistractQA benchmarks. KV wrote the initial draft of the manuscript. All authors revised and approved the manuscript."
        },
        {
            "title": "Competing Interests",
            "content": "Disclosures: EKO reports consulting income with Sofinnova Partners. EKO reports equity in Eikon Therapeutics, Artisight Incorporate. The other authors have no personal, financial, or institutional interest pertinent to this article."
        },
        {
            "title": "Data Availability",
            "content": "The datasets generated or analyzed during the current study are available in the nyuolab/clinical_confounders repository, https://github.com/nyuolab/MedDistractQA. The benchmarks developed (i.e., MedDistractQA-Nonliteral and MedDistractQA-Bystander) are available on HuggingFace dataset hub upon publication of this work."
        },
        {
            "title": "Code Availability",
            "content": "Our code is shared publicly on GitHub upon publication of this work and can be found at https://github.com/nyuolab/MedDistractQA."
        },
        {
            "title": "References",
            "content": "[1] Suzanne Blackley, Valerie Schubert, Foster Goss, Wasim Al Assad, Pamela Garabedian, and Li Zhou. Physician use of speech recognition versus typing in clinical documentation: controlled observational study. International Journal of Medical Informatics, 141:104178, 2020. [2] John Zech, Marcus Badgeley, Manway Liu, Anthony Costa, Joseph Titano, and Eric Karl Oermann. Variable generalization performance of deep learning model to detect pneumonia in chest radiographs: cross-sectional study. PLoS medicine, 15(11):e1002683, 2018. [3] Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. Nature, 620(7972):172180, 2023. [4] Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, et al. Can generalist foundation models outcompete special-purpose tuning? case study in medicine. arXiv preprint arXiv:2311.16452, 2023. [5] Asma Ben Abacha, Wen-wai Yim, Yujuan Fu, Zhaoyi Sun, Meliha Yetisgen, Fei Xia, and Thomas Lin. Medec: benchmark for medical error detection and correction in clinical notes. arXiv preprint arXiv:2412.19260, 2024. [6] Kaiyan Zhang, Sihang Zeng, Ermo Hua, Ning Ding, Zhang-Ren Chen, Zhiyuan Ma, Haoxin Li, Ganqu Cui, Biqing Qi, Xuekai Zhu, et al. Ultramedical: Building specialized generalists in biomedicine. Advances in Neural Information Processing Systems, 37:2604526081, 2025. [7] Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, Tim Strother, Chunjong Park, Elahe Vedadi, et al. Capabilities of gemini models in medicine. arXiv preprint arXiv:2404.18416, 2024. [8] Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, et al. Meditron-70b: Scaling medical pretraining for large language models. arXiv preprint arXiv:2311.16079, 2023. [9] Krithik Vishwanath, Jaden Stryker, Anton Alyakin, Daniel Alexander Alber, and Eric Karl Oermann. Medmobile: mobile-sized language model with expert-level clinical capabilities. arXiv preprint arXiv:2410.09019, 2024. [10] Paul Hager, Friederike Jungmann, Robbie Holland, Kunal Bhagat, Inga Hubrecht, Manuel Knauer, Jakob Vielhauer, Marcus Makowski, Rickmer Braren, Georgios Kaissis, et al. Evaluation and mitigation of the limitations of large language models in clinical decision-making. Nature medicine, 30(9):26132622, 2024. [11] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Schärli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. In International Conference on Machine Learning, pages 3121031227. PMLR, 2023. [12] Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. arXiv preprint arXiv:2410.05229, 2024. 11 [13] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421, 2021. [14] United States Medical Licensing Examination. Usmle physician tasks/competencies, 2024. [15] Silverman, Crapo, Make, Jameson, Fauci, Kasper, Hauser, Longo, and Loscalzo. Harrisons principles of internal medicine 21e, 2022. [16] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Mohamed Amin, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, et al. Toward expert-level medical question answering with large language models. Nature Medicine, pages 18, 2025. [17] Yuxuan Zhou, Xien Liu, Chen Ning, Xiao Zhang, Chenwei Yan, Xiangling Fu, and Ji Wu. Revisiting the scaling effects of LLMs on medical reasoning capabilities, 2025. [18] Robert Osazuwa Ness, Katie Matton, Hayden Helm, Sheng Zhang, Junaid Bajwa, Carey Priebe, and Eric Horvitz. Medfuzz: Exploring the robustness of large language models in medical question answering. arXiv preprint arXiv:2406.06573, 2024. [19] Divyanshu Kumar, Anurakt Kumar, Sahil Agarwal, and Prashanth Harshangi. Increased llm vulnerabilities from fine-tuning and quantization. arXiv e-prints, pages arXiv2404, 2024. [20] Rongwu Xu, Zehan Qi, Zhijiang Guo, Cunxiang Wang, Hongru Wang, Yue Zhang, and Wei Xu. Knowledge conflicts for llms: survey. arXiv preprint arXiv:2403.08319, 2024. [21] Seong-Il Park and Jay-Yoon Lee. Toward robust ralms: Revealing the impact of imperfect retrieval on retrieval-augmented language models. Transactions of the Association for Computational Linguistics, 12:16861702, 2024. [22] Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173, 2024. [23] Kevin Wu, Eric Wu, and James Zou. Clasheval: Quantifying the tug-of-war between an llms In The Thirty-eight Conference on Neural Information internal prior and external evidence. Processing Systems Datasets and Benchmarks Track, 2024. [24] Qiao Jin, Won Kim, Qingyu Chen, Donald Comeau, Lana Yeganova, John Wilbur, and Zhiyong Lu. Medcpt: Contrastive pre-trained transformers with large-scale pubmed search logs for zero-shot biomedical information retrieval. Bioinformatics, 39(11):btad651, 2023."
        },
        {
            "title": "Extended Data",
            "content": "Extended Data Figure 1. Group-wise comparison of performance degradation due to distractions, with models split up as Open-source, general (n=18), Open-source, medical (n=3), or Proprietary (n=8). Results for MedDistractQA-Nonliteral, results for MedDistractQA-Bystander 13 Extended Data Figure 2. Relationship between baseline performance on MedQA and the resulting accuracy loss when tested on two MedDistractQA variants: Nonliteral and Bystander. Each marker corresponds to different class of large language model (circle: open-source, general; square: open-source, medical; triangle: proprietary). The horizontal axis shows each models baseline accuracy on MedQA (%), while the vertical axis shows the loss in accuracy (%) incurred under the distractor conditions. The dashed trend lines illustrate positive correlations between baseline accuracy and accuracy loss, with = 0.76 for MedDistractQA-Nonliteral and = 0.70 for MedDistractQA-Bystander. 14 Extended Data Figure 3. Trying to reduce distractor (MedDistractQA-Nonliteral) impact via explicit prompting. Loss in model accuracy is relative to the MedDistractQA-Nonliteral, and represents accuracy of MedDistractQA-Nonliteral with new prompting style minus the original prompting style on MedDistractQA-Nonliteral. Positive score indicates new prompting method helped over original. Extended Data Figure 4. Average performance change due to MedDistractQA-Nonliteral and MedDistractQA-Bystander by medical competency categorization of question. positive number indicates that performance was improved after distraction was added, while negative number indicates that performance degraded with the addition of distraction. 16 Extended Data Figure 5. Average performance change due to MedDistractQA-Nonliteral and MedDistractQA-Bystander by medical system categorization of question. positive number indicates that performance was improved after distraction was added, while negative number indicates that performance degraded with the addition of distraction. 17 Extended Data Figure 6. Comparison of MedQA accuracy with and without retrieval-augmented generation (RAG) across six different language models. Each subplot shows accuracy (y-axis) versus the retrieval rank (x-axis), with the red horizontal line indicating the models baseline accuracy (no RAG) and the blue points showing accuracy under RAG at varying ranks."
        },
        {
            "title": "Supplementary Materials",
            "content": "Supplemental Figure 1. MedDistractQA-Nonliteral and performance on individual sentences with nonliteral clinical terms. For these ablation studies, we utilize one singular distracting sentence curated for the entire dataset, rather than for each individual question. 19 Supplemental Figure 2. MedDistractQA-Bystander and performance on individual sentences with socially-applied clinical terms. For these ablation studies, we utilize one singular distracting sentence curated for the entire dataset, rather than for each individual question."
        }
    ],
    "affiliations": [
        "Center for Data Science, New York University, New York, New York, 10016",
        "Department of Aerospace Engineering and Engineering Mechanics, The University of Texas at Austin, Austin, Texas, 78712",
        "Department of Mathematics, The University of Texas at Austin, Austin, Texas, 78712",
        "Department of Neurological Surgery, NYU Langone Medical Center, New York, New York, 10016",
        "Department of Neurosurgery, Washington University School of Medicine in St. Louis, St. Louis, Missouri, 63110",
        "Department of Radiology, NYU Langone Medical Center, New York, New York, 10016"
    ]
}