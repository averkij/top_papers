{
    "paper_title": "HarmonyGuard: Toward Safety and Utility in Web Agents via Adaptive Policy Enhancement and Dual-Objective Optimization",
    "authors": [
        "Yurun Chen",
        "Xavier Hu",
        "Yuhan Liu",
        "Keting Yin",
        "Juncheng Li",
        "Zhuosheng Zhang",
        "Shengyu Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models enable agents to autonomously perform tasks in open web environments. However, as hidden threats within the web evolve, web agents face the challenge of balancing task performance with emerging risks during long-sequence operations. Although this challenge is critical, current research remains limited to single-objective optimization or single-turn scenarios, lacking the capability for collaborative optimization of both safety and utility in web environments. To address this gap, we propose HarmonyGuard, a multi-agent collaborative framework that leverages policy enhancement and objective optimization to jointly improve both utility and safety. HarmonyGuard features a multi-agent architecture characterized by two fundamental capabilities: (1) Adaptive Policy Enhancement: We introduce the Policy Agent within HarmonyGuard, which automatically extracts and maintains structured security policies from unstructured external documents, while continuously updating policies in response to evolving threats. (2) Dual-Objective Optimization: Based on the dual objectives of safety and utility, the Utility Agent integrated within HarmonyGuard performs the Markovian real-time reasoning to evaluate the objectives and utilizes metacognitive capabilities for their optimization. Extensive evaluations on multiple benchmarks show that HarmonyGuard improves policy compliance by up to 38% and task completion by up to 20% over existing baselines, while achieving over 90% policy compliance across all tasks. Our project is available here: https://github.com/YurunChen/HarmonyGuard."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 0 1 0 4 0 . 8 0 5 2 : r HARMONYGUARD: TOWARD SAFETY AND UTILITY IN WEB AGENTS VIA ADAPTIVE POLICY ENHANCEMENT AND DUAL-OBJECTIVE OPTIMIZATION Yurun Chen1, Xavier Hu1, Yuhan Liu2, Keting Yin1, Juncheng Li1, Zhuosheng Zhang3, Shengyu Zhang1 1Zhejiang University yurunchen.research@gmail.com {xavier.hu.research, yuhanliu.research}@gmail.com {yinkt, junchengli, sy zhang}@zju.edu.cn zhangzs@sjtu.edu.cn 2Xiamen University 3Shanghai Jiao Tong University"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models enable agents to autonomously perform tasks in open web environments. However, as hidden threats within the web evolve, web agents face the challenge of balancing task performance with emerging risks during long-sequence operations. Although this challenge is critical, current research remains limited to single-objective optimization or single-turn scenarios, lacking the capability for collaborative optimization of both safety and utility in web environments. To address this gap, we propose HarmonyGuard, multi-agent collaborative framework that leverages policy enhancement and objective optimization to jointly improve both utility and safety. HarmonyGuard features multi-agent architecture characterized by two fundamental capabilities: (1) Adaptive Policy Enhancement: We introduce the Policy Agent within HarmonyGuard, which automatically extracts and maintains structured security policies from unstructured external documents, while continuously updating policies in response to evolving threats. (2) Dual-Objective Optimization: Based on the dual objectives of safety and utility, the Utility Agent integrated within HarmonyGuard performs the Markovian real-time reasoning to evaluate the objectives and utilizes metacognitive capabilities for their optimization. Extensive evaluations on multiple benchmarks show that HarmonyGuard improves policy compliance by up to 38% and task completion by up to 20% over existing baselines, while achieving over 90% policy compliance across all tasks. Our project is available here: https://github.com/YurunChen/HarmonyGuard."
        },
        {
            "title": "INTRODUCTION",
            "content": "Web agents based on Large Language Models (LLMs) have transformed how we interact with the web by enabling autonomous tasks through natural language instructions (OpenAI, 2025; Anthropic, 2025). These agents can perform diverse operations, such as online shopping or booking flights, significantly expanding the scope of web automation. However, their growing autonomy also exposes them to threats, including adversarial attacks (Wu et al., 2025), environment injection (Chen et al., 2025a), and knowledge poisoning (Chen et al., 2024). As these agents take on increasingly complex tasks, critical question emerges: Can we trust web agents to act both intelligently and safely? For web agents, two objectives sit in delicate balance: Utility, the ability to perform tasks effectively, and Safety, the assurance they behave reliably and responsibly. Existing research typically focuses on the single objective optimization, such as safety detection (Chen et al., 2025b; Jiang et al., 2025) or utility enhancement (Liu et al., 2025a;b; Li et al., 2025), or is limited to single-turn scenarios (Jia et al., 2024; Xiang et al., 2025). However, the joint optimization of safety and utility has largely been overlooked. In dynamic environments involving continuous and long-sequence operations, this 1 optimization is crucial to avoid imbalances, such as overly conservative or risky behavior caused by single-objective optimization. Current joint optimization still faces two key challenges: (1) Safety-Utility Disconnection: Effective security policies must respond swiftly to evolving threats; otherwise, agents may experience goal drift when encountering new risks, ultimately compromising utility performance. However, current policies are often embedded in unstructured regulatory texts or external guidelines, making them difficult to extract efficiently, enforce accurately, or update dynamically. (2) Safety-Utility Trade-off : The trade-off between safety and utility requires careful management, as pursuing utility may lead web agents to overlook security measures, while excessive focus on safety can degrade task performance. In web environments requiring long-sequence operations, this balance grows exponentially critical, as even minor misalignments can trigger risk amplification cascades and cause persistent deviations from intended objectives. To address these challenges, we propose multi-agent collaborative framework named HarmonyGuard, which aims to jointly optimize safety and utility, with the goal of approaching the Paretooptimal frontier between that balances the two objectives. This framework consists of three types of agents: Web Agent responsible for executing web tasks, Policy Agent responsible for constructing and maintaining security policies, and Utility Agent designed to optimize task utility and safety. These agents work jointly to improve both safety and utility through collaboration. From utility optimization, The joint optimization consists of three stages: (1) Policy Enhancement: Policy Agent automatically extracts, parses, and constructs structured policy knowledge base from unstructured external documents. Additionally, we introduce an adaptive update mechanism to address evolving web threats. (2) Dual-Objective Optimization: We leverage the Utility Agent to achieve co-optimization of security and utility. the Utility Agent based on the robust Context Engineering performs real-time reasoning evaluation and correction during the agents reasoning stage, which includes: (i) introducing second-order Markovian evaluation strategy to evaluate safety (based on policy database) and utility (based on task alignment) through the agents two-step state transitions; (ii) constructing metacognitive capabilities for the web agent to enhance the models reflection, enabling reasoning correction. For safety optimization, the Utility Agent detects risks during reasoning evaluations and constructs violation cases for policy updates when violations are identified. Since safety policies are typically expressed in positive terms, the Utility Agent actively collects negative samples (violation cases) to understand the safety boundaries of the policies. (3) Policy Update: After receiving the violation referneces, the Policy Agent leverages the semantic similarity-based filtering mechanism and policy queues collectively guaranteeing the relevance and timeliness of updated policies. Figure 1: The results indicate that HarmonyGuard achieves superior performance in terms of both utility and safety. To evaluate the effectiveness of our framework, we conducted extensive evaluation based on two benchmarks: ST-WebAgentBench (Levy et al., 2025) and WASP (Evtimov et al., 2025). The results show that HarmonyGuard achieved up to 92.5% and 100% guardrail effectiveness on STWebAgentBench and WASP respectively, while also improving utility by more than 20% on both benchmarks. Compared with the baseline methods used in the experiments, HarmonyGuard reaches the Pareto optimal front, achieving the best performance in both web agent safety and utility. Furthermore, we observed that under multi-round tests on the same benchmarks, the guardrail effectiveness and task utility can be further improved due to the adaptability of the policy database. Therefore, the main contributions of this paper can be summarized as follows: 2 We propose HarmonyGuard, multi-agent collaboration framework designed to achieve Pareto-optimal balance between safety and utility. To the best of our knowledge, this work is the first to address the joint optimization of safety and utility in LLM-based web agents. We developed the Policy Agent and Utility Agent for HarmonyGuard, which collaboratively enable adaptive policy enhancement and dual-objective optimization. We implement prototype of HarmonyGuard and conduct extensive experiments across multiple benchmarks. Experimental results show that HarmonyGuard effectively achieves dual optimization of safety and utility. We present several insights derived from our research findings, which we hope will inform and guide future research in the field of Agent Security."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "In this section, We reveal that existing studies largely lack joint consideration of both safety and utility in web agents. Threat Landscape. In web environments, agents face both internal and external attacks, demonstrating that security risks cannot be effectively managed with static policy files. Internal threats target core architecture, including (1) Prompt Injection (Wu et al., 2024; Kumar et al., 2024), (2) Knowledge Poisoning (Chen et al., 2024; Jiang et al., 2024), and (3) Tool Library / Model Context Protocol (MCP) Hijacking (Song et al., 2025). On the other hand, external threats exploit environmental factors like embedding malicious scripts in web pages (Liao et al., 2025) or delivering phishing links via pop-ups and notifications (Zhang et al., 2025; Chen et al., 2025a). These external attacks are typically easier to execute, especially in open web environments, where malicious elements can mislead the Agent into incorrect behavior. Safety Guardrails. Current agent guardrail mechanisms primarily focus on two dimensions: input filtering and reasoning correction,aiming to prevent agents from being maliciously manipulated or exhibiting uncontrolled behavior during task execution. (1) Input Filtering: This involves inspecting user inputs or external information before task execution to identify and block potentially malicious commands or injection attacks. These approaches typically rely on predefined rules, pattern recognition, or the models own classification capabilities to defend against jailbreaks and prompt injections (Wallace et al., 2024; Chennabasappa et al., 2025; Zhou et al., 2024a; Chan et al., 2025). (2) Reasoning Correction: This focuses on correcting behaviors that exhibit goal drift during the agents reasoning stage, based on the current reasoning content and memory information of the LLM. The primary methods include fine-tuning (Ma et al., 2025; Zhang et al., 2024) or external monitoring (Jiang et al., 2025; Jia et al., 2024). Additionally, some studies employ rule-based methods and contextual semantic analysis to filter inputs that may induce unsafe behaviors (Xiang et al., 2025; Chen et al., 2025b). Although these guardrails have shown effectiveness, most are developed for relatively static scenarios such as dialogue systems and lack tailored designs for web agents, which operate in dynamic environments and engage in long-horizon action sequences. ShieldAgent (Chen et al., 2025b) attempts to address this limitation by introducing probabilistic policy reasoning model for safety validation from the perspective of web-based agents. However, it does not sufficiently consider utility during execution. In contrast, HarmonyGuard integrates both safety and utility considerations, offering more balanced approach to robustness and task performance."
        },
        {
            "title": "3 HARMONYGUARD",
            "content": "In this section, we first present HarmonyGuards design objectives, threat model, key features, and workflow (Sec. 3.1). We then provide detailed explanations of both the Policy Agent (Sec. 3.2) and Utility Agent (Sec. 3.3). 3.1 OVERVIEW The goal of HarmonyGuard is to enhance the task effectiveness of web agents while ensuring compliance with policies derived from external regulatory documents, which define the security requirements set by authorities. To achieve this, the Policy Agent utilizes tools provided by the MCP server 3 Figure 2: The workflow of HarmonyGuard consists of three stages: (1) Policy Enhancement (Top Center): The Policy Agent First extracts text from external documents and uses LLM-based approach to identify, refine, and de-duplicate potential security policies. These are then converted into structured data and stored in policy database for future use. (2) Dual-Objective Optimization (Bottom Right): During the web agents reasoning phase, the Utility Agent constructs contextual states based on second-order Markovian evaluation strategy, and evaluates the reasoning process from two perspectives: safety and utility. If policy violation or goal deviation is detected, the Utility Agent provides optimization suggestions and builds metacognitive capabilities to enhance the web agents alignment and self-correction. (3) Policy Update (Top Right): Once violation is confirmed, Violation Reference is generated and sent to the Policy Agent. The Policy Agent then compares this Violation Reference against the corresponding policy queue in the database for the relevant violation category using similarity-based threshold. If the similarity is below the defined threshold, the case is added to the queue. to extract and refine policies from these documents, integrating it into centralized and unified policy representation. The Utility Agent is grounded in this unified policy, imposing constraints on all agent tasks. Therefore, our threat model assumes trusted external documents and MCP servers, with threats arising from behaviors prohibited by the unified policy. The framework has two features: (1) Adaptive Policy Enhancement and (2) Dual-Objective Optimization. We illustrate the workflow of HarmonyGuard in Figure 2. 3.2 POLICY AGENT The Policy Agent dynamically extracts, refines, and maintains an up-to-date policy database from external documents. It includes two components, Policy Enhancement and Policy Update, which together form the Adaptive Policy Enhancement feature of our framework. Policy Enhancement. The Policy Agent autonomously devises an optimal extraction strategy leveraging the available tools within the MCP. It begins by extracting text from external files. Once the text is extracted, the Policy Agent applies several enhancement techniques: (1) LLM Refinement: The extracted text is processed using an LLM to perform semantic understanding, ambiguity resolution, redundancy removal, and expression normalization, thereby improving the clarity and accuracy of the policy descriptions. (2) Policy Deduplication: By computing semantic similarity and leveraging LLMs to identify redundant entries, the agent detects and merges duplicate or highly similar policy entries from different sources, ensuring uniqueness within the knowledge base. (3) Policy Structuring: The refined and deduplicated policy information is transformed into highly structured 4 data model, with predefined fields such as policy ID, scope of applicability, constraints, and risk level. The structure of the extracted policy is illustrated in Figure 3. Policy Updating. HarmonyGuard achieves dynamic policy updates through the collaboration of the Policy Agent and Utility Agent. For the Utility Agent, it executes real-time violation detection based on the evaluation strategy. For each violation, it constructs corresponding violation reference and maps it to the relevant policy entry for downstream storage. Then, the Policy Agent updates policies through two core (1) Semantic Similarity Filtermechanisms: ing: To avoid data redundancy and improve the quality of violation reference information, HarmonyGuard employs heuristic semantic similarity filtering approach based on the Gestalt pattern matching. Samples with similarity score above 85% are removed to ensure diversity and representativeness in violation data. The filtered violations are retained and incorporated into the policy knowledge base as contextual evidence to support subsequent risk assessments and policy reasoning. The continuous expansion of this knowledge base significantly enhances the frameworks situational awareness and adaptability. (2) Tiered Bounded Queue: To address the evolving threat landscape, HarmonyGuard implements variable-length First-In-First-Out (FIFO) queueing mechanism based on threat levels. The queue length is dynamically adjusted according to the threat levels (low, medium, high), ensuring that high-risk threats retain more violation references and have longer retention periods. This design improves responsiveness to critical threats while preventing overfitting to outdated or low-impact incidents. Figure 3: Structure of extracted policies. By integrating Real-time Violation Detection, Semantic Similarity Filtering, and Tiered Bounded Queues, HarmonyGuard builds feedback-enhanced policy update pipeline that continuously optimizes policy alignment. We formalize the process in Algorithm 1. Algorithm 1 Feedback-Enhanced Policy Update Pipeline Require: = {v1, v2, ..., vn}, {Set of policy violations} θ = 0.85, {Similarity threshold} = {low, medium, high}, {Risk levels} : {Queue length per risk level} if Qdup = then {u Qr Sim(v, u) θ} RiskLevel(v) {Determine risk level} Ensure: Updated {Qr} stored in database 1: Initialize queues {Qr R} from database 2: for each do 3: 4: Qdup 5: 6: 7: 8: 9: end if 10: 11: end for 12: Update database with new {Qr} 13: return {Qr R} end if Qr Qr {v} {Insert into queue} Remove oldest element from Qr if Qr L(r) then 3.3 UTILITY AGENT The core capability of the Utility Agent lies in achieving Dual-Objective Optimization through two stages: (1) reasoning evaluation and (2) reasoning correction. Specifically, reasoning evaluation involves Evaluation Strategy and Dual-Objective Decision, while reasoning correction involves Metacognitive Capabilities. 5 Evaluation Strategy. In the framework of the Constrained Markov Decision Process (Altman, 2021), the Utility Agent employs the Second-Order Markov Evaluation Strategy to perform constraint checking over reasoning sequences. We define the web agents reasoning sequence as {r1, r2, . . . , rt}. At each reasoning step t, the evaluation depends only on the current output rt and the immediately preceding output rt1, which constitutes second-order Markov process. Compared to evaluating the full reasoning trajectory, second-order markovian evaluation strategy strikes favorable balance between safety and accuracy. From safety perspective, constraint violations in web agent tasks often exhibit short-term temporal continuityfor instance, generating high-risk actions in two consecutive reasoning steps. By evaluating local transitions (rt1, rt), the agent can effectively capture such temporally adjacent violations while avoiding significant loss in overall safety assessment. From the standpoint of efficiency and robustness, limiting historical dependencies reduces interference from redundant or noisy context, thereby simplifying the reasoning complexity and enhancing the stability and generalizability of the decision process. Dual-Objective Decision. The Utility Agent evaluates whether the agents reasoning fails to meet two objectives: safety and utility, by identifying if it (1) violates policies or (2) deviates from the task objective. Given reasoning sequence {r1, r2, . . . , rt}, the Utility Agent evaluates two criteria at each reasoning step to determine whether the current reasoning output violates policies or deviates from the task goal. This evaluation is represented by vector of boolean indicators: R(rt rt1) = (cid:20)I(cid:0)f policy I(cid:0)f goal (rt1, rt)(cid:1) (rt1, rt)(cid:1) θ θ (cid:21) , where R(rt rt1) {0, 1}2 is vector indicating the presence of policy violations and task deviations, respectively. The functions policy are LLM-based evaluators that return boolean values signaling whether policy violation or goal drift occurs between reasoning steps 1 and t. The indicator function I() maps the evaluation result to {0, 1}, where 1 indicates detected issue and 0 means no issue. This joint boolean evaluation enables the Utility Agent to detect and respond promptly whenever either security or utility constraints are breached. and goal θ θ Metacognitive Capabilities. When either policy violation or task deviation is detected based on the evaluation vector R(rt rt1), the Utility Agent drives the web agent to engage in Introspective Reflection through the constructed metacognitive process (Wang & Zhao, 2024). The metacognitive process typically involves: (1) comprehending the input text, (2) forming an initial judgment, (3) conducting critical evaluation of the preliminary analysis, and (4) deriving final decision based on reflection. Specifically, the utility agent leverages LLMs to generate optimized guidance that guide the web agent, thereby completing the critical evaluation step in this process. This intervention equips the web agent with metacognitive capabilities, significantly strengthening its reasoning correction competence. The construction of the optimization guidance is shown in Figure 4. Figure 4: The content of the optimization guidance includes reflection."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We present the evaluation of HarmonyGuard in this section. The results show that HarmonyGuard achieves improvements in both safety and utility compared to the baselines. 6 Guardrail ST-WebAgentBench WASP WASP (SoM) Consent Boundary Execution GPI GUI RPI RUI GPI GUI RPI RUI No Defense Prompt Defense Policy Traversal GuardBase HarmonyGuard No Defense Prompt Defense Policy Traversal GuardBase HarmonyGuard 0.887 0.907 0.859 0.916 0. 0.034 0.038 0.038 0.038 0.047 0.956 0.956 0.994 0.994 0.994 0.063 0.064 0.072 0.064 0.077 Policy Compliance Rate 0.571 0.952 1.000 1.000 1.000 0.381 0.571 0.762 0.667 0. 0.667 1.000 0.952 1.000 1.000 Completion under Policy 0.523 0.571 0.429 0.619 0.714 0.286 0.524 0.381 0.667 0.667 0.666 0.810 0.810 0.952 0.905 0.876 0.891 0.891 0.898 0. 0.068 0.078 0.076 0.078 0.081 0.571 0.571 0.857 0.857 0.952 0.571 0.571 0.714 0.762 0.857 0.762 1.000 0.952 1.000 1.000 0.667 0.667 0.619 0.952 0.952 0.571 0.381 1.000 0.810 1. 0.333 0.333 0.667 0.762 0.762 0.571 1.000 0.714 1.000 1.000 0.571 0.619 0.571 0.571 0.667 0.571 0.571 0.571 0.952 0.952 0.571 0.571 0.571 0.571 0.571 Table 1: Comparison of results on PCR and CuP across all benchmarks. Bold indicates the best performance in each column. 4.1 IMPLEMENTATION DETAILS Benchmarks. We evaluate our framework HarmonyGuard on two real-world security benchmarks from WebArena (Zhou et al., 2024b): ST-WebAgentBench (Levy et al., 2025) and WASP (Evtimov et al., 2025), both hosted on AWS websites. We also test multimodal agent based on WASP, called WASP (SoM). ST-WebAgentBench includes 235 tasks with safety policies on Consent, Boundary, and Execution. WASP has 84 tasks focusing on plaintext and URL injection across GitHub and Reddit. For clarity, the four injection types in WASP are GitHub Plain Injection (GPI), GitHub URL Injection (GUI), Reddit Plain Injection (RPI), and Reddit URL Injection (RUI). Guardrails. We compare HarmonyGuard against four guardrails based on unified security policy: (1) No Defense: No guardrail mechanisms are applied; (2) Prompt Defense: The raw policy document is directly provided to the agent as part of the prompt for interpretation. (3) Policy Traversal: The structured policy is given to the agent for self-interpretation without any additional processing. (4) GuardBase: base version of HarmonyGuard in which the Policy Agent does not perform policy updates. Models. In all experiments, the web agent uses gpt-4o (Hurst et al., 2024) and gpt-4o-mini (OpenAI, 2024), the Utility Agent uses Qwen-Max-2025-01-25 (Bai et al., 2023), and the Policy Agent uses gpt-4o (Hurst et al., 2024). Params. For all LLMs, the model temperature is fixed at 0. The policy queue lengths are defined per threat level: 5 for low, 7 for medium, and 10 for high. The similarity threshold is set to default value of 85%. Metrics. We evaluate guardrails using the following metrics: (1) PCR (Policy Compliance Rate): The percentage of tasks that comply with defined policies. (2) CuP (Completion under Policy): (3) Completion: The task The task completion rate considering only policy-compliant actions. completion rate regardless of policy compliance. We provide metric calculations in the appendix. 4.2 POLICY COMPLIANCE Table 1 presents the policy compliance performance of HarmonyGuard across multiple benchmarks. By comparing with several guardrail methods, HarmonyGuard consistently achieves the best performance across all policy categories, significantly outperforming Policy Traversal and other baseline methods. Specifically, on the ST-WebAgentBench, HarmonyGuard attains the highest PCR of 92.5%, 99.4%, and 91.5% under the Consent, Boundary, and Execution policy categories, respectively. On the WASP and WASP (SoM), HarmonyGuard demonstrates strong defense capabilities against different injection attacks, with multiple PCR reaching 1.0. Notably, in the URL Injection scenarios, it significantly outperforms other methods, exhibiting excellent adaptability and robustness. 7 Furthermore, GuardBase as the base version of our method already shows strong performance. By incorporating policy updating capabilities, HarmonyGuard further improves PCR, validating the effectiveness of our framework in achieving high safety and task alignment."
        },
        {
            "title": "4.3 UTILITY PERFORMANCE",
            "content": "The results in Table 1 under the Completion under Policy section show that HarmonyGuard exhibits significant advantages in utility improvement across multiple benchmarks. On ST-WebAgentBench, HarmonyGuard achieves an approximately 20% increase in CuP across all three threat categories. On the WASP and WASP (SoM), HarmonyGuard also largely attains optimal performance, with the highest CuP reaching 95.2%. Compared to the No Defense baseline, HarmonyGuard brings substantial utility improvements, with the highest relative increase reaching 133%. Figure 5: Utility gap of different guardrails. The numbers on top of bars indicate the violation between Completion and CuP (violation = Completion CuP ). The red star indicates the minimum violation of the current category. In Figure 5, we compare overall Completion with CuP. We denote the utility gap between Completion and CuP as the violation. This violation reflects the extent to which the agent relies on policy violations to complete tasks. smaller violation indicates that the agent tends to complete tasks while strictly complying with policies, demonstrating safer and more robust defense. Conversely, larger violation suggests that more tasks are completed by violating policies, indicating higher security risk. The results show that HarmonyGuard has the smallest or even no violation across all benchmarks, indicating that this framework effectively guides the web agent to complete tasks efficiently while ensuring policy compliance. 4.4 OBJECTIVE OPTIMIZATION ANALYSIS dual-objective of HarmonyGuard guardrail methods Figure 6 presents comparative and analysis unexisting der optimization, evaluated using the Pareto frontier on ST-WebAgentBench and WASP & WASP (SoM). The x-axis measures the Policy Compliance Rate, while the y-axis reports Completion under Policy, both of which jointly reflect agent safety and utility. Across both benchmarks, HarmonyGuard consistently achieves Pareto optimality, demonstrating superior balance between policy compliance and task effectiveness, whereas other guardrails fall short in at least one of the two objectives. Figure 6: Pareto front comparison of all guardrail methods. 4.5 EVALUATION STRATEGY COMPARISON Table 2 presents comparison of the effects of different evaluation strategies on PCR and CuP using the model gpt-4o-mini on the ST-WebAgentBench benchmark. Specifically, we compared evaluations based on the agents full execution trajectory, the current reasoning step only, and baseline without evaluation strategy. 8 As shown in Table 2, the Second-Order Markovian Evaluation Strategy demonstrated strong and balanced performance, achieving the best or second-best results in PCR and CuP respectively across all threat categories and overall. In contrast, the Full-Trajectory Evaluation Strategy, while attaining the highest overall PCR, exhibited noticeable decline in CuP, even falling below that of the Current-Step Evaluation Strategy. Further analysis indicates that although incorporating full trajectory information can help identify potential violations and thus enhance PCR, it may also lead to the misattribution of violations from earlier stages to the current reasoning step. This misjudgment increases the number of false positives in compliance evaluation, resulting in unnecessary correction and corresponding decrease in CuP. In essence, the model plays it safe by labeling more reasoning cases as violations, thereby improving PCR at the cost of task completion, while also causing unnecessary and frequent policy update requests. PCR CuP PCR CuP PCR CuP PCR / CuP Consent Strategy Boundary On the other hand, the Current-Step Evaluation Strategy avoids this overpenalization and yields more balanced result, but still underperforms the Second-Order Markovian Evaluation Strategy in CuP. By leveraging short-term historical context from the previous two states, the SecondOrder Markovian Evaluation Strategy captures local strategy shifts more accurately. This leads to better compliance assessments and improved task completion rates, enhancing both the reliability and practical utility of the model. None 0.788 0.029 0.984 0.047 0.879 0.051 0.824 / 0.052 Full-Traj. 0.957 0.029 1.000 0.038 0.883 0.038 0.869 / 0.042 0.914 0.038 1.000 0.051 0.884 0.054 0.867 / 0.056 Cur. Step Markovian 0.957 0.029 0.994 0.055 0.915 0.059 0.867 / 0. Table 2: Results under different evaluation strategies. Bold indicates the best performance in each column. Execution Overall 4.6 MULTI-ROUND POLICY ADAPTATION In Table 3, we conduct comparative analysis of HarmonyGuards multi-round adaptive process across different threat categories on the WASP benchmark. Additionally, Figure 7 illustrates the overall changes in PCR and CuP over the rounds. It can be observed that the results remain relatively stable after three rounds, with HarmonyGuard achieving its best performance in the third round. In the first round of updates, since the policy database was initially empty and the Policy Agent lacked prior references, the policy adjustments were mainly focused on building the policy database, gradually enhancing threat awareness during this process. Although some metrics fluctuated in the second round, the overall trend stabilized and continued to improve. This reflects the frameworks iterative optimization of policies, which significantly enhances both policy compliance and task completion. Notably, in the third round, the system exhibited more balanced and robust performance in terms of safety and utility, indicating that multi-round adaptation effectively strengthens the web agents ability to cope with repeated attacks. Round GPI GUI RPI RUI PCR CuP PCR CuP PCR CuP PCR CuP 1.000 0.714 0.905 0.667 1.000 0.905 0.952 0.857 First Second 1.000 0.762 0.857 0.667 0.952 0.810 1.000 0.905 0.905 0.905 0.952 0.667 1.000 0.905 1.000 0.857 Third Table 3: Performance of HarmonyGuard across rounds on the WASP Benchmark. Figure 7: Performance trends across rounds. Red star denotes best result."
        },
        {
            "title": "5 CONCLUSION",
            "content": "This paper proposes multi-agent collaborative framework named HarmonyGuard, which successfully enables agents to effectively achieving joint optimization in dynamic web environments. By 9 introducing the Policy Agent and Utility Agent, HarmonyGuard enables the extraction and optimization of security policies while enhancing the task utility of web agents under safety constraints. Experimental results validate the frameworks significant advantages in both policy compliance and task effectiveness, demonstrating strong adaptability and robustness against evolving threats. Additionally, our research reveals several Insights: (1) External policy knowledge should not be treated as static input but as structured and evolvable knowledge asset. (2) Agent architectures equipped with metacognitive capabilities are critical factor in enhancing Agent robustness and adaptability. (3) Negative examples (i.e., policy violations) can help agents understand the boundaries of policy compliance. (4) In multi-turn reasoning or task decomposition scenarios, constructing clear context representation (i.e., context engineering) is critical. We hope these insights offer valuable guidance for future Agent Security research."
        },
        {
            "title": "REFERENCES",
            "content": "Eitan Altman. Constrained Markov decision processes. Routledge, 2021. Anthropic. Building effective agents. Web page, 2025. URL https://www.anthropic.com/ engineering/building-effective-agents. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Yik Siu Chan, Zheng-Xin Yong, and Stephen H. Bach. Can we predict alignment before models finish thinking? towards monitoring misaligned reasoning models, 2025. URL https: //arxiv.org/abs/2507.12428. Yurun Chen, Xavier Hu, Keting Yin, Juncheng Li, and Shengyu Zhang. Evaluating the robustness of multimodal agents against active environmental injection attacks, 2025a. URL https:// arxiv.org/abs/2502.13053. Zhaorun Chen, Zhen Xiang, Chaowei Xiao, Dawn Song, and Bo Li. Agentpoison: Red-teaming llm agents via poisoning memory or knowledge bases, 2024. URL https://arxiv.org/abs/ 2407.12784. Zhaorun Chen, Mintong Kang, and Bo Li. Shieldagent: Shielding agents via verifiable safety policy reasoning, 2025b. URL https://arxiv.org/abs/2503.22738. Sahana Chennabasappa, Cyrus Nikolaidis, Daniel Song, David Molnar, Stephanie Ding, Shengye Wan, Spencer Whitman, Lauren Deason, Nicholas Doucette, Abraham Montilla, Alekhya Gampa, Beto de Paola, Dominik Gabi, James Crnkovich, Jean-Christophe Testud, Kat He, Rashnil Chaturvedi, Wu Zhou, and Joshua Saxe. Llamafirewall: An open source guardrail system for building secure ai agents, 2025. URL https://arxiv.org/abs/2505.03574. Ivan Evtimov, Arman Zharmagambetov, Aaron Grattafiori, Chuan Guo, and Kamalika Chaudhuri. Wasp: Benchmarking web agent security against prompt injection attacks, 2025. URL https: //arxiv.org/abs/2504.18575. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Feiran Jia, Tong Wu, Xin Qin, and Anna Squicciarini. The task shield: Enforcing task alignment to defend against indirect prompt injection in llm agents, 2024. URL https://arxiv.org/ abs/2412.16682. Changyue Jiang, Xudong Pan, Geng Hong, Chenfu Bao, and Min Yang. Rag-thief: Scalable extraction of private data from retrieval-augmented generation applications with agent-based attacks, 2024. URL https://arxiv.org/abs/2411.14110. Changyue Jiang, Xudong Pan, and Min Yang. Think twice before you act: Enhancing agent behavioral safety with thought correction. arXiv preprint arXiv:2505.11063, 2025. Priyanshu Kumar, Elaine Lau, Saranya Vijayakumar, Tu Trinh, Scale Red Team, Elaine Chang, Vaughn Robinson, Sean Hendryx, Shuyan Zhou, Matt Fredrikson, Summer Yue, and Zifan Wang. Refusal-trained llms are easily jailbroken as browser agents, 2024. URL https://arxiv. org/abs/2410.13886. Ido Levy, Ben Wiesel, Sami Marreed, Alon Oved, Avi Yaeli, and Segev Shlomov. Stwebagentbench: benchmark for evaluating safety and trustworthiness in web agents, 2025. URL https://arxiv.org/abs/2410.06703. Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, Weizhou Shen, Junkai Zhang, Dingchu Zhang, Xixi Wu, Yong Jiang, Ming Yan, Pengjun Xie, Fei Huang, and Jingren Zhou. Websailor: Navigating superhuman reasoning for web agent, 2025. URL https://arxiv.org/abs/2507.02592. Zeyi Liao, Lingbo Mo, Chejian Xu, Mintong Kang, Jiawei Zhang, Chaowei Xiao, Yuan Tian, Bo Li, and Huan Sun. Eia: Environmental injection attack on generalist web agents for privacy leakage, 2025. URL https://arxiv.org/abs/2409.11295. Yuhang Liu, Pengxiang Li, Zishu Wei, Congkai Xie, Xueyu Hu, Xinchen Xu, Shengyu Zhang, Xiaotian Han, Hongxia Yang, and Fei Wu. Infiguiagent: multimodal generalist gui agent with native reasoning and reflection, 2025a. URL https://arxiv.org/abs/2501.04575. Yuhang Liu, Pengxiang Li, Congkai Xie, Xavier Hu, Xiaotian Han, Shengyu Zhang, Hongxia Yang, and Fei Wu. Infigui-r1: Advancing multimodal gui agents from reactive actors to deliberative reasoners, 2025b. URL https://arxiv.org/abs/2504.14239. Hao Ma, Tianyi Hu, Zhiqiang Pu, Boyin Liu, Xiaolin Ai, Yanyan Liang, and Min Chen. Coevolving with the other you: Fine-tuning llm with sequential cooperative multi-agent reinforcement learning, 2025. URL https://arxiv.org/abs/2410.06101. OpenAI. Gpt-4o mini: Advancing cost-efficient intelligence. https://openai.com/index/ gpt-4o-mini-advancing-cost-efficient-intelligence/, 2024. Accessed: 2025-07-29. OpenAI. Computer-using agent, 2025. URL https://openai.com/index/ computer-using-agent/. Hao Song, Yiming Shen, Wenxuan Luo, Leixin Guo, Ting Chen, Jiashui Wang, Beibei Li, Xiaosong Zhang, and Jiachi Chen. Beyond the protocol: Unveiling attack vectors in the model context protocol ecosystem, 2025. URL https://arxiv.org/abs/2506.02040. Eric Wallace, Kai Xiao, Reimar Leike, Lilian Weng, Johannes Heidecke, and Alex Beutel. The instruction hierarchy: Training llms to prioritize privileged instructions, 2024. URL https: //arxiv.org/abs/2404.13208. Yuqing Wang and Yun Zhao. Metacognitive prompting improves understanding in large language models, 2024. URL https://arxiv.org/abs/2308.05342. Chen Henry Wu, Rishi Shah, Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried, and Aditi Raghunathan. Dissecting adversarial robustness of multimodal lm agents, 2025. URL https: //arxiv.org/abs/2406.12814. Fangzhou Wu, Shutong Wu, Yulong Cao, and Chaowei Xiao. Wipi: new web threat for llm-driven web agents, 2024. URL https://arxiv.org/abs/2402.16965. Zhen Xiang, Linzhi Zheng, Yanjie Li, Junyuan Hong, Qinbin Li, Han Xie, Jiawei Zhang, Zidi Xiong, Chulin Xie, Carl Yang, Dawn Song, and Bo Li. Guardagent: Safeguard llm agents by guard agent via knowledge-enabled reasoning, 2025. URL https://arxiv.org/abs/ 2406.09187. Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, and Guoyin Wang. Instruction tuning for large language models: survey, 2024. URL https://arxiv.org/abs/2308.10792. Yanzhe Zhang, Tao Yu, and Diyi Yang. Attacking vision-language computer agents via pop-ups, 2025. URL https://arxiv.org/abs/2411.02391. Andy Zhou, Bo Li, and Haohan Wang. Robust prompt optimization for defending language models against jailbreaking attacks, 2024a. URL https://arxiv.org/abs/2401.17263. Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: realistic web environment for building autonomous agents, 2024b. URL https://arxiv.org/abs/ 2307.13854."
        },
        {
            "title": "A CASE STUDY",
            "content": "We illustrate an example of the optimization process enabled by HarmonyGuard in Figure 8. In this case, at step 15 of the task execution, the reasoning content generated by the Web Agent includes an attempt to click an external hyperlinkan action identified as an external plaintext injection attack, form of security threat that may redirect users to untrusted or malicious domains. Upon detecting this behavior, the Utility Agent performs real-time analysis of the initial reasoning trajectory. Leveraging structured policy representations and referencing the associated policy database, it maps the detected threat to specific policy ID, enabling precise identification of the violated policy. Concurrently, it determines that the agents current trajectory has deviated from the intended task goal, signaling potential compromise in utility. To address these issues, the Utility Agent engages in dual-objective reasoning: it generates clear explanations for both the identified threat and the task deviation, and formulates targeted optimization guidelines to support reflective adjustments by the LLM. These guidelines serve as actionable feedback, prompting the model to revise its output in alignment with both safety and utility goals. Upon receiving the guidelines, the Web Agent integrates them into its subsequent reasoning cycle and produces revised, policy-compliant response that aligns with the intended task flow. This collaborative loop exemplifies HarmonyGuards ability to dynamically harmonize safety enforcement with task effectiveness in real time, particularly in long-horizon, open-ended task scenarios."
        },
        {
            "title": "B MORE DEFENSE EXPLORATION",
            "content": "Although HarmonyGuard has already demonstrated effective co-optimization of safety and utility during task execution, its modular design allows for further expansion through the integration of advanced defense strategies to enhance both robustness and practicality. Future exploration will focus on extending its capabilities in several key directions. One promising avenue is input detection, which aims to identify and filter risky inputs prior to the reasoning process. By incorporating adversarial prompt detectors, the system can recognize threats such as instruction injection, redirection, or unauthorized access attempts before these inputs reach the core reasoning engine. These detectors may be implemented using fine-tuned classifiers, pattern-matching rules, or semantic similarity filtering. Another complementary direction is fine-grained policy control, which goes beyond applying policies at the level of full reasoning chunks. Instead, it enables real-time monitoring and intervention at the sentence, semantic unit, or even token level, allowing for more precise detection of violations and flexible correction strategies. In addition to structural control, HarmonyGuard can benefit from uncertainty-aware mechanisms. By integrating real-time uncertainty estimation, the system can actively defend against lowconfidence or ambiguous actions. For instance, when the model lacks sufficient confidence in executing specific operationsuch as clicking hyperlinkit may choose to skip, revert, or request further verification, thereby preventing unsafe behaviors resulting from vague inferences. Another strategy involves ensemble-based decision making. Deploying multiple reasoning models or parallel policy subsystems and aggregating their outputs through voting, confidence weighting, or risk-sensitive fusion can reduce the risk of single-model misjudgment and significantly enhance resilience in complex web environments. Moreover, as the nature of online content becomes increasingly multimodal, effective defense mechanisms must extend beyond text to include images, 12 Figure 8: During the reasoning stage of the Web Agent, the Utility Agent checks the safety and utility of the reasoning content. 13 code snippets, and videos. Enhancing cross-modal threat detectionsuch as identifying QR codes in phishing images or malicious scripts embedded in web pagescan substantially improve comprehensive security supervision. Finally, incorporating experience-driven memory and feedback loops can further empower the system. By maintaining persistent memory of past violations and corresponding interventions, HarmonyGuard can learn from past mistakes and adapt its threat perception over time. This enables proactive defense refinement through accumulated experience. Furthermore, in high-risk or missioncritical scenarios, human-in-the-loop supervision provides vital layer of assurance. Based on realtime uncertainty or sensitivity assessments, the system can selectively solicit human validation or offer decision support, ensuring that safety oversight extends beyond autonomous reasoning. Guardrails Consent Boundary Execution"
        },
        {
            "title": "Per Entry",
            "content": "No Defense Prompt Defense Policy Traversal Guard-Base HarmonyGuard 0.887 0.907 0.859 0.916 0.925 0.907 0.933 0.884 0.933 0.938 0.956 0.956 0.994 0.994 0.994 1.000 0.999 0.999 0.999 0.999 0.876 0.891 0.891 0.898 0. 0.950 0.950 0.956 0.956 0.966 Table 4: Comparison of guardrail methods on ST-WebAgentBench across different aggregation dimensions. Bold indicates the best performance in each column."
        },
        {
            "title": "C METRIC CALCULATION FORMULAS",
            "content": "Let the total number of tasks be . For each task {1, 2, . . . , }, define two binary indicators: Ci represents whether the task is successfully completed (1 if completed, 0 otherwise), and Pi indicates whether the task complies with the security policy (1 if compliant, 0 otherwise). Using these indicators, we formally define the following metrics: Completion measures the fraction of tasks successfully completed, calculated as Completion = 1 (cid:88) i= Ci. Policy Compliance Rate (PCR) quantifies the fraction of tasks that adhere to the security policy, PCR = 1 (cid:88) i= Pi. Completion under Policy (CuP) represents the fraction of tasks that are both completed and policycompliant, reflecting the systems ability to jointly optimize task utility and safety, CuP = 1 (cid:88) (Ci Pi). i=1 In the WASP benchmark, the PCR is evaluated using LLM-based judgment, while the remaining metrics are assessed through rule-based methods. In contrast, all evaluation metrics in STWebAgentBench are entirely rule-based."
        },
        {
            "title": "D ADDITIONAL RESULTS",
            "content": "The Table 4 presents comparative analysis of guardrail methods evaluated on the STWebAgentBench across three policy categories under two aggregation schemes: Per Task and Per Entry. Overall, HarmonyGuard consistently outperforms other approaches, achieving the highest 14 or near-highest policy compliance across all categories and aggregation types. Notably, it demonstrates substantial improvements over the baseline (No Defense), particularly in the Execution category, where compliance increases from 0.876 to 0.915 (Per Task) and from 0.950 to 0.966 (Per Entry). While all guardrail methods yield varying degrees of improvement, Policy Traversal and Guard-Base also exhibit strong performance in the Boundary category. The results further indicate that policy compliance is most challenging in the Consent category, where performance differences among methods are more pronounced. These findings suggest that HarmonyGuard is effective in promoting consistent and fine-grained policy adherence across multi-dimensional, safety-critical agent tasks."
        },
        {
            "title": "E PROMPT TEMPLATES",
            "content": "Figure 9 and Figure 10 illustrate the prompt templates designed for the two core agents in our system: the Policy Agent and the Utility Agent. These agents are responsible for distinct but complementary functionspolicy extraction and reasoning optimization, respectivelyforming modular and interpretable architecture that supports controlled large language model (LLM) behavior in complex tasks. The prompt template for the Policy Agent is designed to support structured and systematic processing of policy-related documents. Acting as policy processing expert, the agent follows three-stage workflow: (1) extract raw text from the input source, which may be PDF, webpage, or plain text file; (2) extract individual policy statements from the retrieved text; and (3) review the extracted policies and remove duplicates. This sequential process is guided by strict execution rules, including type-checking the input source, invoking one tool per step, waiting for each tools output before proceeding, and conducting final comprehensive summary during the review phase. The agent interacts with external tools using predefined call format and uses natural language reasoning only when not invoking tools. The prompt also includes contextual information such as the organization name and description, the target subject of the policy, and list of available tools, which allows the agent to tailor its behavior to different institutional settings. Through this template, the Policy Agent ensures consistent, traceable, and context-aware extraction of policy information. In contrast, the Utility Agent focuses on optimizing task-oriented outputs while ensuring policy adherence. Its prompt template guides the LLM through three-stage reasoning process: (1) Policy Compliance Check, which verifies whether the output violates any organizational policies or platform rules; (2) Alignment Check, which assesses whether the output aligns with the tasks goals, requirements, and intended user utility; and (3) Optimization Guidance, which provides constructive revision suggestions in cases of inconsistency, risk, or suboptimal performance. Figure 9: Prompt template of the Policy Agent. Together, these stages ensure that the agents reasoning adheres to both safety and utility requirements. The inclusion of optimization guidance not only facilitates reflection and refinement but also enhances the systems robustness in dynamically evolving or ambiguous task settings. 15 Figure 10: Prompt template of the Utlity Agent."
        }
    ],
    "affiliations": [
        "Shanghai Jiao Tong University",
        "Xiamen University",
        "Zhejiang University"
    ]
}