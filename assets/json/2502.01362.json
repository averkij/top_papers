{
    "paper_title": "Inverse Bridge Matching Distillation",
    "authors": [
        "Nikita Gushchin",
        "David Li",
        "Daniil Selikhanovych",
        "Evgeny Burnaev",
        "Dmitry Baranchuk",
        "Alexander Korotin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Learning diffusion bridge models is easy; making them fast and practical is an art. Diffusion bridge models (DBMs) are a promising extension of diffusion models for applications in image-to-image translation. However, like many modern diffusion and flow models, DBMs suffer from the problem of slow inference. To address it, we propose a novel distillation technique based on the inverse bridge matching formulation and derive the tractable objective to solve it in practice. Unlike previously developed DBM distillation techniques, the proposed method can distill both conditional and unconditional types of DBMs, distill models in a one-step generator, and use only the corrupted images for training. We evaluate our approach for both conditional and unconditional types of bridge matching on a wide set of setups, including super-resolution, JPEG restoration, sketch-to-image, and other tasks, and show that our distillation technique allows us to accelerate the inference of DBMs from 4x to 100x and even provide better generation quality than used teacher model depending on particular setup."
        },
        {
            "title": "Start",
            "content": "Nikita Gushchin 1 David Li * 1 Daniil Selikhanovych * 1 2 3 Evgeny Burnaev 1 4 Dmitry Baranchuk 2 Alexander Korotin 1 4 5 2 0 2 3 ] . [ 1 2 6 3 1 0 . 2 0 5 2 : r Abstract Learning diffusion bridge models is easy; making them fast and practical is an art. Diffusion bridge models (DBMs) are promising extension of diffusion models for applications in image-toimage translation. However, like many modern diffusion and flow models, DBMs suffer from the problem of slow inference. To address it, we propose novel distillation technique based on the inverse bridge matching formulation and derive the tractable objective to solve it in practice. Unlike previously developed DBM distillation techniques, the proposed method can distill both conditional and unconditional types of DBMs, distill models in one-step generator, and use only the corrupted images for training. We evaluate our approach for both conditional and unconditional types of bridge matching on wide set of setups, including super-resolution, JPEG restoration, sketch-to-image, and other tasks, and show that our distillation technique allows us to accelerate the inference of DBMs from 4x to 100x and even provide better generation quality than used teacher model depending on particular setup. 1. Introduction Diffusion Bridge Models (DBMs) represent specialized class of diffusion models designed for data-to-data tasks, such as image-to-image translation. Unlike standard diffusion models, which operate by mapping noise to data (Ho et al., 2020; Sohl-Dickstein et al., 2015), DBMs construct diffusion processes directly between two data distributions (Peluchetti, 2023a; Liu et al., 2022b; Somnath et al., 2023; Zhou et al., 2024a; Yue et al., 2024; Shi et al., 2023; De Bortoli et al., 2023). This approach allows DBMs to modify only the necessary components of the data, starting from an input sample rather than generating it entirely from Gaus- *Equal contribution 1Skolkovo Institute of Science and Technology 2Yandex Research 3HSE University 4Artificial Intelligence Research Institute. Correspondence to: Nikita Gushchin <n.gushchin@skoltech.ru>, Alexander Korotin <a.korotin@skoltech.ru>. t s - u i o r P i a e I - - r g - - e"
        },
        {
            "title": "Input",
            "content": "IBMD (Ours)"
        },
        {
            "title": "Teacher",
            "content": "Figure 1. Outputs of DBMs models distilled by our Inverse Bridge Matching Distillation (IBMD) approach on various image-to5). Teachers use NFE 500 image translation tasks and datasets ( steps, while IBMD distilled models use NFE 4. (cid:77) sian noise. As result, DBMs have demonstrated impressive performance in image-to-image translation problems. The rapid development of DBMs has led to two dominant approaches, usually considered separately. The first branch of Inverse Bridge Matching Distillation approaches (Peluchetti, 2023a; Liu et al., 2022b; 2023a; Shi et al., 2023; Somnath et al., 2023) considered the construction of diffusion between two arbitrary data distributions performing Unconditional Bridge Matching (also called the Markovian projection) of process given by mixture of diffusion bridges. The application of this branch includes different data like images (Liu et al., 2023a; Li et al., 2023), audio (Kong et al., 2025) and biological tasks (Somnath et al., 2023; Tong et al., 2024) not only in paired but also in unpaired setups using its relation to the Schrodinger Bridge problem (Shi et al., 2023; Gushchin et al., 2024). The second direction follows framework closer to classical diffusion models, using forward diffusion to gradually map to the point of different distibution rather than mapping distribution to distribution as in previous case (Zhou et al., 2024a; Yue et al., 2024). While these directions differ in theoretical formulation, their practical implementations are closely related; for instance, models based on forward diffusion can be seen as performing Conditional Bridge Matching with additional drift conditions (De Bortoli et al., 2023). Similar to classical DMs, DBMs also exhibit multistep sequential inference, limiting their adoption in practice. Despite the impressive quality shown by DBMs in the practical tasks, only few approaches were developed for their acceleration, including more advanced sampling schemes (Zheng et al., 2024; Wang et al., 2024) and consistency distillation (He et al., 2024), adapted for bridge models. While these approaches significantly improve the efficiency of DBMs, some unsolved issues remain. The first one is that the mentioned acceleration approaches are directly applicable only for DBMs based on the Conditional Bridge Matching, i.e., no universal method can accelerate any DBMs. Also, due to some specific theoretical aspects of DBMs, consistency distillation cannot be used to obtain the single-step model (He et al., 2024, Section 3.4). Contributions. To address the above-mentioned issues of DBMs acceleration, we propose new distillation technique based on the inverse bridge matching problem, which has several advantages compared to existing methods: 1. Universal Distillation. Our distillation technique is applicable to DBMs trained with both conditional and unconditional regimes, making it the first distillation approach introduced for unconditional DBMs. 2. Single-Step and Multistep Distillation. Our distillation is capable of distilling DBMs into generators with any specified number of steps, including the distillation of DBMs into one-step generators. for conditional and unconditional DBMs in both one and multi-step regimes. It demonstrates improvements compared to the previous acceleration approaches including DBIM(Zheng et al., 2024) and CDBM (He et al., 2024). 2. Background In this paper, we propose universal distillation framework for both conditional and unconditional DBMs. To not repeat fully analogical results for both cases, we denote by this color the additional conditioning on xT used for the conditional models, i.e. for the unconditional case this conditioning is not used. 2.1. Bridge Matching We start by recalling the bridge matching method (Peluchetti, 2023b;a; Liu et al., 2022b; Shi et al., 2023). Consider two probability distributions p(x0) and p(xT ) on RD dimensional space, which represent target and source domains, respectively. For example, in an image inverse problem, p(x0) represents the distribution of clean images and p(xT ) the distribution of corrupted images. Also consider coupling p(x0, xT ) of these two distributions, which is probability distribution on RD RD. Coupling p(x0, xT ) can be provided by paired data or constructed synthetically, i.e., just using the independent distribution p(x0, xT ) = p(x0)p(xT ). Bridge Matching aims to construct the diffusion that transforms source distribution p(xT ) to target distribution p(x0) based on given coupling p(x0, xT ) and specified diffusion bridge. Diffusion bridges. Consider forward-time diffusion called Prior on time horizon [0, ] represented by the stochastic differential equation (SDE): Prior : dxt = (xt, t)dt + g(t)dwt, (1) (xt, t) : RD [0, ] RD, g(t) : [0, ] RD, where (xt, t) is drift function, g(t) is the noise schedule function and dwt is the differential of the standard Wiener process. By q(xtxs), we denote the transition probability density of prior process from time to time t. Diffusion bridge is conditional process Qx0,xT , which is obtained by pinning down starting and ending points x0 and xT . This diffusion bridge can be derived from prior process using the Doob-h transform (Doob & Doob, 1984): Diffusion Bridge Qx0,xT : x0, xT are fixed, (2) dxt = {f (xt, t)dt + g2(t)xt log q(xT xt)}dt + g(t)dwt, 3. Target data-free distillation. Our method does not require the target data domain to perform distillation. For this diffusion bridge we denote the distribution at time of the diffusion bridge Qx0,xT by q(xtx0, xT ). 4. Better quality of distilled models. Our distillation technique is tested on wide set of image-to-image problems Mixture of bridges. Bridge Matching procedure starts with creating mixture of bridges process Π. This process is 2 Inverse Bridge Matching Distillation Figure 2. Overview of (Conditional) Bridge Matching with (cid:98)x0 reparameterization. The process begins by sampling pair (x0, xT ) from the data coupling p(x0, xT ). An intermediate sample xt is then drawn from the diffusion bridge q(xtx0, xT ) at random time [0, ]. The model (cid:98)x0 is trained with an MSE loss to reconstruct x0 from xt. In the conditional setting (dashed red path), (cid:98)x0 is also conditioned on xT as an additional input, leveraging information about the terminal state to improve reconstruction. represented as follows: Mixture of Bridges Π : (cid:90) Qx0,xT ()p(x0, xT )dx0dxT . Π() = (3) Practically speaking, the definition (3) means that to sample from mixture of bridges Π, one first samples the pair (x0, xT ) p(x0, xT ) from data coupling and then samples trajectory from the bridge Qx0,xT (). Bridge Matching problem. The mixture of bridges Π cannot be used for data-to-data translation since it requires first to sample pair of data and then just inserts the trajectory. In turn, we are interested in constructing diffusion, which can start from any sample xT p(xT ) and gradually transform it to x0 p(x0). This can be done by solving the Bridge Matching problem (Shi et al., 2023, Proposition 2) Bridge Matching problem: (4) BM(Π) def= arg min KL(ΠM ), where is the set of Markovian processes associated with some SDE and KL(ΠM ) is the KL-divergence between constructed mixture of bridges Π and diffusion . It is known that the solution of Bridge Matching is the reversedtime SDE (Shi et al., 2023, Proposition 9): The SDE of Bridge Matching solution : dxt = {ft(xt) g2(t)v(xt, t)}dt + g(t)d wt, xT pT (xT ), (5) where is standard Wiener process when time flows backward from = to = 0, and dt is an infinitesimal negative timestep. The drift function is obtained solving the following problem (Shi et al., 2023; Liu et al., 2023a): Bridge Matching problem with tractable objective: (6) Ex0,t,xt (cid:2)vϕ(xt, t) xt log q(xtx0)2(cid:3), min ϕ (x0, xT ) p(x0, xT ), ([0, ]), xt q(xtx0, xT ). Time moment here is sampled according to the uniform distribution on the interval [0, ]. Relation Between Flow and Bridge Matching. The Flow Matching (Liu et al., 2023b; Lipman et al., 2023) can be seen as the limiting case σ 0 of the Bridge Matching for particular example see (Shi et al., 2023, Appendix A.1). 2.2. Augmented (Conditional) Bridge Matching and Denoising Diffusion Bridge Models (DDBM) For given coupling p(x0, xT ) = p(x0xT )p(xT ), one can use an alternative approach to build data-to-data diffusion. Consider set of Bridge Matching problems indexed by xT between p0 = p(x0xT ) and p(xT ) = δxT (x) (delta measure centered at xT ). This approach is called Augmented Bridge Matching (De Bortoli et al., 2023). The key difference of this version in practice is that it introduces the condition of the drift function v(xt, t, xT ) on the starting point xT in the reverse time diffusion (5): dxt = {ft(xt) g2(t)v(xt, t, xT )}dt + g(t)d wt. The drift function can be recovered almost in the same way just by the addition of this condition on xT : Augmented (Conditional) Bridge Matching Problem. (cid:2)vϕ(xt, t, xT ) xt log q(xtx0)2(cid:3), Ex0,t,xt,xT min ϕ (x0, xT ) p(x0, xT ), and xt q(xtx0, xT ). Since the difference is the addition of conditioning on xT , we call this approach Conditional Bridge Matching. Relation to DDBM. As was shown in the Augmented Bridge Matching (De Bortoli et al., 2023), the conditional Bridge Matching is equivalent to the Denoising Diffusion Bridge Model (DDBM) proposed in (Zhou et al., 2024a). The difference is that in DDBM, the authors learn the score function of s(xt, xT , t) conditioned on xT of process for which x0 p(x0xT ) and q(xt) q(xtx0, xT ): Then, it is combined with the drift of forward Doob-h transform (5) to get the reverse SDE drift v(xt, t, xT ): v(xt, t, xT ) = s(xt, xT , t) xt log q(xT xt), dxt = {f (xt, t)dt g2(t)v(xt, t, xT )}dt + g(t)d wt, or reverse probability flow ODE drift: vODE(xt, t, xT ) = 1 2 s(xt, xT , t) xt log q(xT xt), Inverse Bridge Matching Distillation dxt = {f (xt, t)dt g2(t)vODE(xt, t, xT )}dt, which is used for consistency distillation in (He et al., 2024). 2.3. Practical aspects of Bridge Matching Priors used in practice. In practice (He et al., 2024; Zhou et al., 2023; Zheng et al., 2024), the drift of the prior process is usually set to be (xt, t) = (t)xt, i.e, it depends linearly on xt. For this process the transitional distribution q(xtx0) = (xtαtx0, σ2 I) is Gaussian, where: (t) = log αt dt , g2(t) = dσ2 dt 2 log αt dt σ2 . The bridge process distribution is also Gaussian q(xtx0, xT ) = (xT atxT + btx0, c2 I) with coefficients: (cid:19) (cid:18) at = αt αT"
        },
        {
            "title": "SNRT\nSNRt",
            "content": ", bt = αt"
        },
        {
            "title": "SNRT\nSNRt",
            "content": ", = σ2 c2 (cid:18) 1 (cid:19) , SNRT SNRt where SNRt = α2 σ2 is the signal-to-noise ratio at time t. Data prediction reparameterization. The regression target of the loss function (6) for the priors with the drift v(xt, t) is given by xt log q(xtx0) = xtαtx0 . Hence, one can use the parametrization v(xt, t, xT ) = xtαt (cid:98)x0(xt,t,xT ) and solve the equivalent problem: σ2 σ2 Reparametrized (Conditional) Bridge Matching problem: min ϕ Ex0,t,xt,xT (cid:2)λ(t)(cid:98)xϕ 0 (xt, t, xT ) x02(cid:3), (7) (x0, xT ) p(x0, xT ), ([0, ]), xt q(xtx0, xT ), where λ(t) is any positive weighting function. Note that xT is used only for the Conditional Bridge Matching model. 2.4. Difference Between Acceleration of Unconditional and Conditional DBMs Since both conditional and unconditional approaches learn drifts of SDEs, they share the same problems of long inference. However, these models significantly differ in the approaches that can accelerate them. The source of this difference is that Conditional Bridge Matching considers the set of problems of reversing diffusion, which gradually transforms distribution p(x0xT ) to the fixed point xT . Furthermore, the forward diffusion has simple analytical drift and Gaussian transitional kernels. Thanks to it, for each xT to sample, one can use the probability flow ODE and ODEsolvers or hybrid solvers to accelerate sampling (Zhou et al., 2024a) or use consistency distillation of bridge models (He et al., 2024). Another beneficial property is that one can consider non-Markovian forward process to develop more efficient sampling scheme proposed in DBIM (Zheng et al., 2024) similar to Denoising Diffusion Implicit Models (Song et al., 2021). However, in the Unconditional Bridge Matching problem, the forward diffusion process, which maps p(x0) to p(xT ) without conditioning on specific point xT , is unknown. Hence, the abovementioned methods cannot be used to accelerate this model type. 3. IBMD: Inverse Bridge Matching Distillation This section describes our proposed universal approach to distill the both Unconditional and (Conditional) Bridge Matching models (called the teacher model) into fewstep generator using only the corrupted data pT (xT ). The key idea of our method is to consider the inverse problem of finding the mixture of bridges Πθ, for which Bridge Matching provides the solution vθ with the same drift as the given teacher model v. We formulate this task as the optimization problem ( 3.1). However, gradient methods cannot solve this optimization problem directly due to the absence of tractable gradient estimation. To avoid this problem, we prove theorem that allows us to reformulate the inverse problem in the tractable objective for gradient optimiza3.2). Then, we present the fully analogical results tion ( 3.3). Next, for the Conditional Bridge Matching case in ( we present the multistep version of distillation ( 3.5) and (cid:77) the final algorithm ( 3.4). We provide the proofs for all considered theorems and propositions in Appendix A. (cid:77) (cid:77) (cid:77) (cid:77) 3.1. Bridge Matching Distillation as Inverse Problem In this section, we focus on the derivation of our distillation method for the case of Unconditional Bridge Matching. Consider the fitted teacher model v(xt, t), which is an SDE drift of some process = BM(Π), where Π constructed using some data coupling p(x0, xT ) = p(x0xT )p(xT ). We parametrize pθ(x0, xT ) = pθ(x0xT )p(xT ) and aim to find such Πθ build on that BM(Π) = BM(Πθ). pθ(x0, xT ), In practice, we parametrize pθ(x0xT ) by the stochastic generator Gθ(xT , z), (0, I), which generates samples based on input xT p(xT ) and the gaussian noise z. Now, we formulate the inverse problem as follows: KL(BM(Πθ)M ). min θ (8) Note, that since the objective (8) is the KL-divergence between BM(Πθ) and , it is equal to 0 if and only if BM(Πθ) and coincide. Furthermore, using the disintegration and Girsanov theorem (Vargas et al., 2021; Pavon & Wakolbinger, 1991), we have the following result: Proposition 3.1 (Inverse Bridge Matching problem). The inverse problem (8) is equivalent to Ext,t min θ = arg min (cid:2)λ(t)v(xt, t) v(xt, t)2(cid:3), Ext,t,x0 (9) (cid:2)v(xt, t) xt log q(xtx0)2(cid:3), s.t. (x0, xT ) pθ(x0, xT ), ([0, ]), xt q(xtx0, xT ), Inverse Bridge Matching Distillation Figure 3. Overview of our method Inverse Bridge Matching Distillation (IBMD). The goal is to distill trained (Conditional) Bridge Matching model into generator Gθ(z, xT ), which learns to produce samples using the corrupted data p(xT ). Generator Gθ(z, xT ) defines the coupling pθ(x0, xT ) = pθ(x0xT )p(xT ) and we aim to learn the generator in such way that Bridge Matching with pθ(x0, xT ) produces the same (Conditional) Bridge Matching model (cid:98)xϕ 0 = (cid:98)xθ 0 using coupling pθ in the same way as the teacher model was learned. Then, we use our novel objective given in Theorem 3.2 to update the generator model Gθ. 0. To do so, we learn bridge model (cid:98)xϕ where λ(t) is any positive weighting function. (x0, xT ) pθ(x0, xT ), ([0, ]), xt q(xtx0, xT ). Thus, this is the constrained problem, where the drift is the result of Bridge Matching for coupling pθ(x0, xT ) parametrized by the generator Gθ. Unfortunately, there is no clear way to use this objective efficiently for optimizing generator Gθ since it would require gradient backpropagation through the argmin of the Bridge Matching problem. 3.2. Tractable objective for the inverse problem In this section, we introduce our new unconstrained reformulation for the inverse problem (9), which admits direct optimization using gradient methods: Theorem 3.2 (Tractable inverse problem reformulation). The constrained inverse problem (9) w.r.t θ is equivalent to the unconstrained optimization problem: (cid:104) min θ Ext,t,x0 Ext,t,x0 min ϕ (cid:2)λ(t)v(xt, t) xt log q(xtx0)2(cid:3) (cid:2)λ(t)vϕ(xt, t) xt log q(xtx0)2(cid:3)(cid:105) , (x0, xT ) pθ(x0, xT ), ([0, ]), xt q(xtx0, xT ), Where the constraint in the original inverse problem (9) is relaxed by introducing the inner bridge matching problem. This is the general result that can applied with any diffusion bridge. For the priors with with drift (xt, t) = (t)xt, we present its reparameterized version. Proposition 3.3 (Reparameterized tractable inverse problem). Using the reparameterization ( 2.3) for the prior with the linear drift (xt, t) = (t)xt, the inverse problem in Theorem 3.2 is equivalent to: (cid:77) (cid:104) min θ Ext,t,x0 Ext,t,x0 min ϕ (cid:2)λ(t)(cid:98)x (cid:2)λ(t)(cid:98)xϕ 0(xt, t) x02(cid:3) (cid:105) 0 (xt, t) x02(cid:3)dt , The key difference of the reformulated problem is that it admits clear gradients of generator Gθ, which can be calculated automatically by using the autograd techniques. Thanks to the unconstrained reformulation of an inverse problem given by Theorem 3.2, it can now be solved directly by parameterizing (cid:98)x0(xt, t) by neural network. 3.3. Distillation of conditional Bridge Matching models Since Conditional Bridge Matching is, in essence, set of Unconditional Bridge Matching problems for each xT 2.2), the analogical results hold just by adding the condi- ( tioning on xT for v, i.e., using v(xt, t, xT ) or (cid:98)x0, i.e. using (cid:77) (cid:98)x0(xt, t, xT ). Here, we provide the final reparametrized formulation, which we use in our experiments: Theorem 3.4 (Reparameterized tractable inverse problem for conditional bridge matching). (cid:104) min θ Ext,t,x0,xT Ext,t,x0,xT min ϕ (cid:2)λ(t)(cid:98)x (cid:2)λ(t)(cid:98)xϕ 0(xt, t, xT ) x02(cid:3) (10) 0 (xt, t, xT ) x02(cid:3)(cid:105) , (x0, xT ) pθ(x0, xT ), ([0, ]), xt q(xtx0, xT ). where λ(t) is some positive weight function. To use it in practice, we parameterize (cid:98)x0(xt, t, xT ) by neural network with an additional condition on xT . 3.4. Algorithm We provide one-step Algorithm 1 that solves the inverse Bridge Matching problem in the reformulated version that we use in our experiments. We provide visual abstract of it in Figure 3. Note that teacher in the velocity parameterization v(xt, t) can be easily reparameterized ( 2.3) in v(xt,t)+xt x0-prediction model using (cid:98)x(xt, t) = σ2 (cid:77) . αt 5 Inverse Bridge Matching Distillation 3.5. Mulitistep distillation We also present multistep modification of our distillation technique if one-step generator struggles to distill the models, e.g., in inpainting setups, where the corrupted image xT contains less information. Our multistep technique is inspired by similar approaches used in diffusion distillation methods (Yin et al., 2024a, DMD) and aims to avoid training/inference distribution mismatch. We choose timesteps {0 < t1 < t2 < ... < tN = } and add additional time input to our generator Gθ(xt, z, t). For the conditional Bridge Matching case, we also add conditions on xT and use Gθ(xt, z, t, xT ). To perform inference, we alternate between getting prediction from the generator (cid:101)x0 = Gθ(xt, z, t) and using posterior sampling q(xtn1(cid:101)x0, xtn ) given by the diffusion bridge. To train the generator in the multistep regime, we use the same procedure as in one step except that to get input xt for intermediate times tn < tN , we first perform inference of our generator to get x0 and then use bridge q(xt(cid:101)x0, xT ). 4. Related work Diffusion Bridge Models (DBMs) acceleration. Unlike wide scope of acceleration methods developed for classical diffusion and flow models, only few approaches were developed for DBM acceleration. For the conditional DBMs, acceleration methods include more advanced samplers (Zheng et al., 2024; Wang et al., 2024) based on reformulated forward diffusion process as non-markovian process inspired by Denoising Diffusion Implicit Models (Song et al., 2021). Also, there is distillation method based on the distilling probability-flow ODE into few steps using consistency models (He et al., 2024). However, for theoretical reasons (He et al., 2024, Section 3.4), consistency models for Diffusion Bridges cannot be distilled into onestep generators. Unlike these existing works, our method is applicable to both conditional and unconditional types of DBMs and can distill models into the one-step generator. Related diffusion and flow models distillation techniques. Among the methods developed for the distillation of classical diffusion and flow models, the most related to our work are methods based on simultaneous training of few-step generators and auxiliary fake model, that predict score or drift function for the generator (Yin et al., 2024b;a; Zhou et al., 2024b; Huang et al., 2024). Unlike these approaches, we consider the distillation of Diffusion Bridge Models - the generalization of flow and diffusion models. 5. Experiments This section highlights the applicability of our IBMD distillation method in both unconditional and conditional settings. To demonstrate this, we conducted experiments utilizing pretrained unconditional models used in I2SB paper (Liu et al., 2023a). Then we evaluated IBMD in conditional Algorithm 1 Inverse Bridge Matching Distillation (IBMD) 0 : RD [0, ] RD RD; Input :Teacher network (cid:98)x Bridge q(xtx0, xT ) used for training x; Generator network Gθ : RD RD RD; Bridge network (cid:98)xϕ 0 : RD [0, ] RD RD; Input distribution p(xT ) accessible by samples; Weights function λ(t) : [0, ] R+; Batch size ; Number of student iterations K; Number of bridge iterations L. Output :Learned generator Gθ of coupling pθ(x0, xT ) for which Bridge Matching outputs drift v. // Conditioning on xT is used only for distillation of Conditional Bridge Matching models. for = 1 to do for = 1 to do 0 (xt, t, xT ) x02(cid:3) n= (cid:80)N (cid:2)λ(t)(cid:98)xϕ Sample batch xT p(xT ) Sample batch of noise (0, I) x0 Gθ(xT , z) Sample time batch [0, ] Sample batch xt q(xtx0, xT ) (cid:98)Lϕ 1 Update ϕ by using (cid:98)Lϕ ϕ Sample batch xT p(xT ) Sample batch of noise (0, I) x0 Gθ(xT , z) Sample time batch [0, ] Sample batch xt q(xtx0, xT ) (cid:98)Lθ 1 λ(t)(cid:98)xϕ Update θ by using (cid:98)Lθ θ (cid:2)λ(t)(cid:98)x (cid:80)N 0 (xt, t, xT ) x02(cid:3) n=1 0(xt, t, xT ) x02 5.2). settings using DDBM (Zhou et al., 2024a) setup ( For clarity, we denote our models as IBMD-DDBM and IBMD-I2SB, indicating that the teacher model is derived from DDBM or I2SB framework, respectively. We provide all the technical details in Appendix B. (cid:77) 5.1. Distillation of I2SB (5 setups) Since known distillation and acceleration techniques are designed for the conditional models, there is no clear baseline for comparison. Thus, this section aims to demonstrate that our distillation technique significantly decreases NFE required to obtain the same quality of generation. Experimental Setup. To test our approach for unconditional models, we consider models trained and published in I2SB paper (Liu et al., 2023a), specifically (a) two models for the 4x super-resolution with bicubic and pool kernels, (b) two models for JPEG restoration using quality factor QF= 5 and QF= 10, and (c) model for center-inpainting with center mask of size 128 128 all of which were trained on ImageNet 256 256 dataset (Deng et al., 2009). Inverse Bridge Matching Distillation Table 1. Results on the image super-resolution task. Baseline results are taken from I2SB (Liu et al., 2023a). Table 3. Results on the image super-resolution task. Baseline results are taken from I2SB (Liu et al., 2023a). 4 super-resolution (bicubic) ImageNet (256 256) 4 super-resolution (pool) ImageNet (256 256) NFE FID CA NFE FID CA DDRM (Kawar et al., 2022) DDNM (Wang et al., 2023) ΠGDM (Song et al., 2023) ADM (Dhariwal & Nichol, 2021) CDSB (Shi et al., 2022) I2SB (Liu et al., 2023a) IBMD-I2SB (Ours) 20 100 100 1000 50 1000 21.3 13.6 3.6 14.8 13.6 2.8 2.5 63.2 65.5 72.1 66.7 61.0 70.7 72.4 DDRM (Kawar et al., 2022) DDNM (Wang et al., 2023) ΠGDM (Song et al., 2023) ADM (Dhariwal & Nichol, 2021) CDSB (Shi et al., 2022) I2SB (Liu et al., 2023a) IBMD-I2SB (Ours) 20 100 100 1000 50 1 14.8 9.9 3.8 3.1 13.0 2.7 2.6 64.6 67.1 72.3 73.4 61.3 71.0 72.7 Table 2. Results on the image JPEG restoration task with QF=5. Baseline results are taken from I2SB (Liu et al., 2023a). Table 4. Results on the image JPEG restoration task with QF=10. Baseline results are taken from I2SB (Liu et al., 2023a). JPEG restoration, QF= 5. ImageNet (256 256) JPEG restoration, QF= 10. ImageNet (256 256) NFE FID CA NFE FID CA DDRM (Kawar et al., 2022) ΠGDM (Song et al., 2023) Palette (Saharia et al., 2022) CDSB (Shi et al., 2022) I2SB (Liu et al., 2023a) I2SB (Liu et al., 2023a) IBMD-I2SB (Ours) 20 100 1000 50 1000 100 1 28.2 8.6 8.3 38.7 4.6 5.4 5. 53.9 64.1 64.2 45.7 67.9 67.5 67.2 DDRM (Kawar et al., 2022) ΠGDM (Song et al., 2023) Palette (Saharia et al., 2022) CDSB (Shi et al., 2022) I2SB (Liu et al., 2023a) I2SB (Liu et al., 2023a) IBMD-I2SB (Ours) 20 100 1000 50 1000 100 1 16.7 6.0 5.4 18.6 3.6 4. 3.8 64.7 71.0 70.7 60.0 72.1 71.6 72.4 For all the setups we use the same train part of ImageNet dataset, which was used to train the used models. For the evaluation we follow the same protocol used in the I2SB paper, i.e. use the full validation subset of ImageNet for super-resolution task and the 10000 subset of validation for other tasks. We report the same FID (Heusel et al., 2017) and Classifier Accuracy (CA) using pre-trained ResNet50 model metrics used in the I2SB paper. We present our results in Table 1, Table 3, Table 2, Table 4 and Table 6. We provide the uncurated samples for all setups in Appendix C. Results. For both super-resolution tasks (see Table 1, Table 3), our 1-step distilled model outperformed teacher model inference using all 1000 steps used in the training. Note that our model does not use the clean training target data p(x0), only the corrupted p(xT ), hence this improvement is not due to additional training using paired data. We hypothesize that it is because the teacher model introduces approximation error during many steps of sampling, which may accumulate. For both JPEG restoration (see Table 2, Table 4), our 1-step distilled generator provides the quality of generation close to the teacher model and achieves around 100x time acceleration. For the inpainting problem (see Table 6), we present the results for 1, 2 and 4 steps distilled generator. Our 2 and 4-step generators provide quality similar to the teacher I2SB model, in turn, there is still some gap for the 1-step model. These models provide around 5x time acceleration. We hypothesize that this setup is harder for our model since it is required to generate the entire center fragment from scratch, while in other tasks, there is already some good approximation given by corrupted images. 5.2. Distillation of DDBM (3 setups) This section addresses two primary objectives: (1) demonstrating the feasibility of conditional model distillation within our framework and (2) comparing with the CDBM (He et al., 2024) - leading approach in Conditional Bridge Matching distillation, presented into different models: CBD (consistency distillation) and CBT (consistency training). Experimental Setup. For evaluation, we use the same setups used in competing methods (He et al., 2024; Zheng et al., 2024). For the image-to-image translation task, we utilize the EdgesHandbags dataset (Isola et al., 2017) with resolution of 64 64 pixels and the DIODE-Outdoor dataset (Vasiljevic et al., 2019) with resolution of 256256 pixels. For these tasks, we report FID and Inception Scores (IS) (Barratt & Sharma, 2018). For the image inpainting task, we use the same setup of center-inpainting as before. Results. We utilized the same teacher model checkpoints and as in CDBM. We present the quantitative and qualitative results of IBMD on the image-to-image translation task in Table 5 and in Figures 12, 10 respectively. The competing methods, DBIM (Zhou et al., 2024a, Section 4.1) and CDBM (He et al., 2024, Section 3.4), cannot use single-step inference due to the singularity at the starting point xT . 7 Inverse Bridge Matching Distillation Table 5. Results on the Image-to-Image Translation Task (Training Sets). Methods are grouped by NFE (> 2, 2, 1), with the best metrics bolded in each group. Baselines results are taken from CDBM. NFE 40 DDIB (Su et al., 2022) 40 SDEdit (Meng et al., 2021) Rectified Flow (Liu et al., 2022a) 40 I2SB (Liu et al., 2023a) 40 50 DBIM (Zheng et al., 2024) DBIM (Zheng et al., 2024) 100 CBD (He et al., 2024) CBT (He et al., 2024) IBMD-DDBM (Ours) Pix2Pix (Isola et al., 2017) IBMD-DDBM (Ours) 2 1 Edges Handbags (64 64) DIODE-Outdoor (256 256) FID 186.84 26.5 25.3 7.43 1.14 0.89 1.30 0.80 0.67 74.8 1.26 FID 242.3 31.14 77.18 9.34 3.20 2.57 3.66 2.93 3.12 82.4 4. IS 4.22 5.70 5.87 5.77 6.08 6.06 6.02 6.06 5.92 4.22 5.89 IS 2.04 3.58 2.80 3.40 3.62 3.62 3.62 3.65 3.69 4.24 3.66 Table 6. Results on the Image Inpainting Task. Methods are grouped by NFE (> 4, 4, 2, 1), with the best metrics bolded in each group. Baselines results are taken from CDBM. Inpainting, Center (128 128) DDRM (Kawar et al., 2022) ΠGDM (Song et al., 2023) DDNM (Wang et al., 2022) Palette (Saharia et al., 2022) I2SB (Liu et al., 2023a) DBIM (Zheng et al., 2024) DBIM (Zheng et al., 2024) CBD (He et al., 2024) CBT (He et al., 2024) IBMD-I2SB (Ours) IBMD-DDBM (Ours) CBD (He et al., 2024) CBT (He et al., 2024) IBMD-I2SB (Ours) IBMD-DDBM (Ours) IBMD-I2SB (Ours) IBMD-DDBM (Ours) ImageNet (256 256) NFE FID CA 62.1 24.4 20 72.6 7.3 100 55.9 15.1 100 63.0 6.1 1000 65.97 5.4 10 72.4 3.92 50 72.6 3.88 100 69.6 5.34 70.3 4.77 70.3 5.1 72.2 4.03 69.6 5.65 69.8 5.34 65.7 5.3 72.3 4.23 65.0 6.7 70.6 5. 4 2 1 We trained our IBMD with 1 and 2 NFEs on the EdgesHandbags dataset. We surpass CDBM at 2 NFE, outperform the teacher at 100 NFE, and achieve performance comparable to the teacher at 50 NFE with 1 NFE, resulting in 50 acceleration. For the DIODE-Outdoor setup, we trained IBMD with 1 and 2 NFEs. We surpassed CBD in FID at 2 NFE, achieving results comparable to CBT with slight drop in performance and maintaining strong performance at 1 NFE with minor quality reductions. For image inpainting, Table 6 and Figure 9 show the quantitative and qualitative results of IBMD. We train IBMD with 4 NFE for image inpainting. It outperforms CBD and CBT at 4 NFE with significant gap, surpassing both at 2 NFE and maintaining strong performance at 1 NFE while achieving teacher-level results at 50 NFE with 12.5 speedup. Concerns regarding the evaluation protocol used in prior works. For Edges-Handbags and DIODE-Outdoor setups, we follow the evaluation protocol originally introduced in DDBM (Zhou et al., 2024a) and later used in works on acceleration of DDBM (Zheng et al., 2024; He et al., 2024). For some reason, this protocol implies evaluation of the train set. Furthermore, test sets of these datasets consist of tiny fraction of images (around several hundred), making the usage of standard metrics like FID challenging due to high statistical bias or variance of their estimation. Still, to assess the quality of the distilled model on the test sets, we provide the uncurated samples produced by our distill model and teacher model on these sets in Figures 13 and 11 in Appendix C. We also provide the uncurated samples on the train part in Figures 12 and 10 to compare models behavior on train and test sets. From these results, we see that the teacher model exhibits overfitting on both setups, e.g., it produces exactly the same images as corresponding reference images. In turn, on the test sets, teacher models work well for the handbag setups, while on the test set of DIODE images, it exhibits mode collapse and produces gray images. Nevertheless, our distilled model shows exactly the same behavior in both sets, i.e., our IBMD approach precisely distills the teacher model as expected. 6. Discussion Potential impact. DBMs are used for data-to-data translation in different domains, including images, audio, and biological data. Our distillation technique provides universal and efficient way to address the long inference of DBMs, making them more affordable in practice. Limitations. Our method alternates between learning an additional bridge model and updating the student, which may be computationally expensive. Moreover, the student optimization requires backpropagation through the teacher, additional bridge, and the generator network, making it 3x time more memory expensive than training the teacher. Inverse Bridge Matching Distillation"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Barratt, S. and Sharma, R. note on the inception score. arXiv preprint arXiv:1801.01973, 2018. De Bortoli, V., Liu, G.-H., Chen, T., Theodorou, E. A., and Nie, W. Augmented bridge matching. arXiv preprint arXiv:2311.06978, 2023. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. Dhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. Doob, J. L. and Doob, J. Classical potential theory and its probabilistic counterpart, volume 262. Springer, 1984. Gushchin, N., Selikhanovych, D., Kholkin, S., Burnaev, E., and Korotin, A. Adversarial schr odinger bridge matching. arXiv preprint arXiv:2405.14449, 2024. He, G., Zheng, K., Chen, J., Bao, F., and Zhu, J. arXiv preprint Consistency diffusion bridge models. arXiv:2410.22637, 2024. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. Huang, Z., Geng, Z., Luo, W., and Qi, G.-j. Flow generator matching. arXiv preprint arXiv:2410.19310, 2024. Isola, P., Zhu, J.-Y., Zhou, T., and Efros, A. A. Image-toimage translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 11251134, 2017. Kawar, B., Elad, M., Ermon, S., and Song, J. Denoising diffusion restoration models. Advances in Neural Information Processing Systems, 35:23593 23606, 2022. Kong, Z., Shih, K. J., Nie, W., Vahdat, A., Lee, S.- g., Santos, J. F., Jukic, A., Valle, R., and Catanzaro, B. A2sb: Audio-to-audio schrodinger bridges. arXiv preprint arXiv:2501.11311, 2025. Li, B., Xue, K., Liu, B., and Lai, Y.-K. Bbdm: Image-toimage translation with brownian bridge diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern Recognition, pp. 19521961, 2023. Lipman, Y., Chen, R. T. Q., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=PqvMRDCJT9t. Liu, G.-H., Vahdat, A., Huang, D.-A., Theodorou, E. A., I2sb: Image-to-image Nie, W., and Anandkumar, A. schr odinger bridge. arXiv preprint arXiv:2302.05872, 2023a. Liu, X., Gong, C., et al. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2022a. Liu, X., Wu, L., Ye, M., and qiang liu. Let us build bridges: Understanding and extending diffusion generative models. In NeurIPS 2022 Workshop on Score-Based Methods, 2022b. URL https://openreview.net/forum? id=0ef0CRKC9uZ. Liu, X., Gong, C., and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2023b. URL https: //openreview.net/forum?id=XVjTT1nw5z. Meng, C., Song, Y., Song, J., Wu, J., Zhu, J.-Y., and Ermon, S. Sdedit: Image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. Pavon, M. and Wakolbinger, A. On free energy, stochastic control, and schrodinger processes. In Modeling, Estimation and Control of Systems with Uncertainty: Proceedings of Conference held in Sopron, Hungary, September 1990, pp. 334348. Springer, 1991. Peluchetti, S. Diffusion bridge mixture transports, schrodinger bridge problems and generative modeling. Journal of Machine Learning Research, 24(374):151, 2023a. Peluchetti, S. Non-denoising forward-time diffusions. arXiv preprint arXiv:2312.14589, 2023b. 9 Inverse Bridge Matching Distillation Wang, Y., Yu, J., and Zhang, J. image restoration using denoising diffusion null-space model. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=mRieQgMtNTQ. Zero-shot Wang, Y., Yoon, S., Jin, P., Tivnan, M., Song, S., Chen, Z., Hu, R., Zhang, L., Chen, Z., Wu, D., et al. Implicit image-to-image schrodinger bridge for image restoration. Zhiqiang and Wu, Dufan, Implicit Image-to-Image Schrodinger Bridge for Image Restoration, 2024. Yin, T., Gharbi, M., Park, T., Zhang, R., Shechtman, E., Durand, F., and Freeman, W. T. Improved distribution matching distillation for fast image synthesis. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024a. URL https: //openreview.net/forum?id=tQukGCDaNT. Yin, T., Gharbi, M., Zhang, R., Shechtman, E., Durand, F., Freeman, W. T., and Park, T. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 66136623, 2024b. Yue, Z., Wang, J., and Loy, C. C. Resshift: Efficient diffusion model for image super-resolution by residual shifting. Advances in Neural Information Processing Systems, 36, 2024. Zheng, K., He, G., Chen, J., Bao, F., and Zhu, J. Diffusion bridge implicit models. arXiv preprint arXiv:2405.15885, 2024. Zhou, L., Lou, A., Khanna, S., and Ermon, S. Denoising diffusion bridge models. arXiv preprint arXiv:2309.16948, 2023. Zhou, L., Lou, A., Khanna, S., and Ermon, S. Denoising diffusion bridge models. In The Twelfth International Conference on Learning Representations, 2024a. URL https://openreview.net/forum? id=FKksTayvGo. Zhou, M., Zheng, H., Wang, Z., Yin, M., and Huang, H. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In Forty-first International Conference on Machine Learning, 2024b. Saharia, C., Chan, W., Chang, H., Lee, C., Ho, J., Salimans, T., Fleet, D., and Norouzi, M. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 conference proceedings, pp. 110, 2022. Shi, Y., De Bortoli, V., Deligiannidis, G., and Doucet, A. Conditional simulation using diffusion schrodinger bridges. In Uncertainty in Artificial Intelligence, pp. 17921802. PMLR, 2022. Shi, Y., Bortoli, V. D., Campbell, A., and Doucet, A. Diffusion schrodinger bridge matching. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum? id=qy07OHsJT5. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265. PMLR, 2015. Somnath, V. R., Pariset, M., Hsieh, Y.-P., Martinez, M. R., Krause, A., and Bunne, C. Aligned diffusion schrodinger In Uncertainty in Artificial Intelligence, pp. bridges. 19851995. PMLR, 2023. Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. URL https://openreview. net/forum?id=St1giarCHLP. Song, J., Vahdat, A., Mardani, M., and Kautz, J. inverse In International Conference on Learning Pseudoinverse-guided diffusion models for problems. Representations, 2023. Su, X., Song, J., Meng, C., and Ermon, S. Dual diffusion implicit bridges for image-to-image translation. arXiv preprint arXiv:2203.08382, 2022. Tong, A. Y., Malkin, N., Fatras, K., Atanackovic, L., Zhang, Y., Huguet, G., Wolf, G., and Bengio, Y. Simulationfree schrodinger bridges via score and flow matching. In International Conference on Artificial Intelligence and Statistics, pp. 12791287. PMLR, 2024. Vargas, F., Thodoroff, P., Lamacraft, A., and Lawrence, N. Solving schrodinger bridges via maximum likelihood. Entropy, 23(9):1134, 2021. Vasiljevic, I., Kolkin, N., Zhang, S., Luo, R., Wang, H., Dai, F. Z., Daniele, A. F., Mostajabi, M., Basart, S., Walter, M. R., et al. Diode: dense indoor and outdoor depth dataset. arXiv preprint arXiv:1908.00463, 2019. Wang, Y., Yu, J., and Zhang, J. Zero-shot image restoration using denoising diffusion null-space model. arXiv preprint arXiv:2212.00490, 2022. 10 A. Proofs Inverse Bridge Matching Distillation Since all our theorems, propositions and proofs for the inverse Bridge Matching problems which is formulated for the already trained teacher model using some diffusion bridge, we assume all corresponding assumptions used in Bridge Matching. Extensive overview of them can be found in (Shi et al., 2023, Appendix C). Proof of Proposition 3.1. Since both BM(Πθ) and given by reverse-time SDE and the same distribution pT (xT ) the KL-divergence expressed in the tractable form using the disintegration and Girsanov theorem (Vargas et al., 2021; Pavon & Wakolbinger, 1991): (cid:2)g2(t)v(xt, t) v(xt, t)2(cid:3), KL(BM(Πθ)M ) = Ext,t (x0, xT ) pθ(x0, xT ), ([0, ]), xt q(xtx0, xT ). The expectation is taken over the marginal distribution p(xt) of Πθ since it is the same as for BM(Πθ) (Shi et al., 2023, Proposition 2). In turn, the drift v(xt, t) is the drift of Bridge Matching using Πθ, i.e. BM(Πθ): = arg min Ext,t,x0 (cid:2)v(xt, t) xt log q(xtx0)2(cid:3), (x0, xT ) pθ(x0, xT ), ([0, ]), xt q(xtx0, xT ). Combining this, the inverse problem can be expressed in more tractable form: (cid:2)g2(t)v(xt, t) v(xt, t)2(cid:3), Ext,t s.t. (11) min θ = arg min Ext,t,x0 (cid:2)v(xt, t) xt log q(xtx0)2(cid:3)dt, We can add positive valued weighting function λ(t) for the constraint: (x0, xT ) pθ(x0, xT ), ([0, ]), xt q(xtx0, xT ). = arg min Ext,t,x0 (cid:2)λ(t)v(xt, t) xt log q(xtx0)2(cid:3)dt, since it is the MSE regression and its solution is conditional expectation for any weights given by: v(xt, t) = Ex0xt,t (cid:2)xt log q(xtx0)]. We can add positive valued weighting function λ(t) for the main functional: Ext,t (cid:2)λ(t)v(xt, t) v(xt, t)2(cid:3), since it does not change the optimum value (which is equal to 0) and optimal solution, which is the mixture of bridges with the same drift as the teacher model. Proof of Theorem 3.2. Consider inverse bridge matching optimization problem: (cid:2)λ(t)v(xt, t) v(xt, t)2(cid:3), Ext,t,x0 min θ = arg min (cid:2)v(xt, t) xt log q(xtx0)2(cid:3), Ext,t s.t. (12) (x0, xT ) pθ(x0, xT ), ([0, ]), xt q(xtx0, xT ). First, note that since = arg minv Ext,t,x0 by conditional expectation as: (cid:2)v(xt, t) xt log q(xtx0)2(cid:3), i.e. minimizer of MSE functional it is given v(xt, t) = Ex0xt,t (cid:2)xt log q(xtx0)xt, t(cid:3). (13) Then note that: Ext,t,x (cid:2)λ(t)v(xt, t) xt log q(xtx0)2(cid:3) = min 11 Inverse Bridge Matching Distillation Ext,t,x0 (cid:124) Ext,t (cid:2)λ(t)v(xt, t)2(cid:3) (cid:125) (cid:2)λ(t)v(xt,t)2(cid:3) (cid:123)(cid:122) 2Ext,t,x0 Ext,t,x0 (cid:2)λ(t)v(xt, t) xt log q(xtx0)2(cid:3) = (cid:2)λ(t)v(xt, t), xt log q(xtx0)(cid:3) + Ext,t,x0 (cid:2)λ(t)xt log q(xtx0)2(cid:3) = Ext,t (cid:2)λ(t)v(xt, t)2(cid:3) 2Ext,t (cid:42) (cid:104) λ(t) v(xt, t), Ex0xt,t (cid:124) (cid:2)xt log q(xtx0)(cid:3) (cid:125) (cid:123)(cid:122) =v(xt,t) (cid:2)λ(t)v(xt, t)2(cid:3) + Ext,t,x0 (cid:2)λ(t)xt log q(xtx0)2(cid:3) = + Ext,t,x0 (cid:2)λ(t)xt log q(xtx0)2(cid:3) = (cid:2)λ(t)v(xt, t)2(cid:3) + Ext,t,x0 (cid:2)λ(t)xt log q(xtx0)2(cid:3). (14) Ext,t (cid:2)λ(t)v(xt, t)2(cid:3) 2Ext,t Ext,t (cid:43) (cid:105) Hence, we derive that Ext,t (cid:2)λ(t)v(xt, t)2(cid:3) = Ext,t,x0 (cid:2)λ(t)xt log q(xtx0)2(cid:3) min Ext,t,x0 (cid:2)λ(t)v(xt, t) xt log q(xtx0)2(cid:3). Now we use it to reformulate the initial objective: Ext,t (cid:2)λ(t)v(xt, t)2(cid:3) 2Ext,t (cid:2)λ(t)xt log q(xtx0)2] min (cid:2)λ(t)v(xt, t) v(xt, t)2(cid:3) = (cid:2)λ(t) v(xt, t), v(xt, t) (cid:3) + Ext,t (cid:2)λ(t)v(xt, t)2(cid:3) = (cid:2)λ(t)v(xt, t) xt log q(xtx0)2(cid:3) (cid:125) Ext,t,x0 (cid:123)(cid:122) Ext,t Ext,t,x0 (cid:124) (cid:2)λ(t)v(xt,t)2(cid:3) =Ext,t (cid:2)λ(t) v(xt, t), v(xt, t) (cid:3) + Ext,t 2Ext,t (cid:2)λ(t)v(xt, t)2(cid:3) = Ext,t,x0 (cid:2)λ(t)xt log q(xtx0)2(cid:3) 2Ext,t (cid:2)λ(t) v(xt, t), v(xt, t) (cid:3) + Ext,t (cid:2)λ(t)v(xt, t)2(cid:3) (cid:125) (cid:2)λ(t)v(xt,t)2(cid:3) (cid:123)(cid:122) (cid:124) Ext,t,x0 Ext,t,x0 (cid:2)λ(t)v(xt, t) xt log q(xtx0)2(cid:3) min Therefore, we get: Ext,t,x0 (cid:2)λ(t)xt log q(xtx0)2(cid:3) 2Ext,t (cid:2)λ(t) v(xt, t), v(xt, t) (cid:3) + Ext,t,x0 (cid:2)λ(t)v(xt, t)2(cid:3) Ext,t (cid:2)λ(t)v(xt, t) v(xt, t)2(cid:3) = Ext,t,x0 (cid:2)λ(t)v(xt, t) xt log q(xtx0)2(cid:3) min To complete the proof, we use the relation v(xt, t) = Ex0xt,t components, we arrive at the final result: (cid:2)xt log q(xtx0)xt, t(cid:3) from Equation 13. Integrating these Ext,t (cid:2)λ(t)v(xt, t) v(xt, t)2(cid:3) = Ext,t,x0 (cid:2)λ(t)xt log q(xtx0)2(cid:3) 2Ext,t (cid:2)λ(t) (cid:10)Ex0xt,t (cid:2)xt log q(xtx0)xt, t(cid:3), v(xt, t)(cid:11) (cid:3) + Ext,t,x (cid:2)λ(t)v(xt, t)2(cid:3) Ext,t,x0 (cid:2)λ(t)xt log q(xtx0)2(cid:3) 2Ext,t,x0 (cid:2)λ(t) xt log q(xtx0), v(xt, t) (cid:3) + Ext,t,x0 (cid:2)λ(t)v(xt, t)2(cid:3) Ext,t,x (cid:2)λ(t)v(xt, t) xt log q(xtx0)2(cid:3) = min Ext,t,x0 (cid:2)λ(t)v(xt, t) xt log q(xtx0)2(cid:3) min (cid:2)λ(t)v(xt, t) xt log q(xtx0)2(cid:3) = Ext,t,x0 min Ext,t,x (cid:2)λ(t)v(xt, t) xt log q(xtx0)2(cid:3). Proof of Proposition 3.3. Consider the problem from Proposition 3.2: (cid:104) Ext,t,x0 min θ (cid:2)λ(t)v(xt, t) xt log q(xtx0)2(cid:3) min ϕ Ext,t,x0 (cid:2)λ(t)vϕ(xt, t) xt log q(xtx0)2(cid:3)(cid:105) , Inverse Bridge Matching Distillation For the priors with the drift (t)x the regression target is xt log q(xtx0) = xtαtx0 tion v(xt, t) = xtαt (cid:98)x0(xt,t) We use reparameterization of both and vϕ given by: σ2 σ2 . Hence one can use the parametrizav(xt, t) = 0(xt, t) xt αt(cid:98)x σ2 , vϕ(xt, t) = xt αt(cid:98)xϕ σ2 0 (xt, t) and get: (cid:104) Ext,t,x0 min θ (cid:2)λ(t)v(xt, t) xt log q(xtx0)2(cid:3) min ϕ (cid:104) Ext,t,x0 min θ 0(xt, t) x02(cid:3) min ϕ Ext,t,x0 Ext,t,x (cid:2)λ(t)vϕ(xt, t) xt log q(xtx0)2(cid:3)(cid:105) 0 (xt, t) x02(cid:3)(cid:105) (cid:2) λ(t) = (cid:98)xϕ = (cid:2) λ(t) (cid:98)x α2 σ4 (cid:124) (cid:123)(cid:122) (cid:125) def=λ(t) (cid:2)λ(t)(cid:98)x α2 σ4 (cid:124) (cid:123)(cid:122) (cid:125) def=λ(t) (cid:2)λ(t)(cid:98)xϕ (cid:104) min θ Ext,t,x 0(xt, t) x02(cid:3) min Ext,t,x0 ϕ 0 (xt, t) x02(cid:3)(cid:105) , where λ(t) is just another positive weighting function. Proof of Theorem 3.4. In fully analogical way, as for the unconditional case we consider the set of the Inverse Bridge Matching problems indexes by xT : (cid:8) min θ (cid:2)KL(BM(ΠθxT )M xT )(cid:3)(cid:9) , xT where using bridge q(xtx0, xT ) and coupling pθ(x0xT )δxT (x). xT is result of Bridge Matching conditioned on xT and ΠθxT is Mixture of Bridges for each xT constructed By employing the same reasoning as in the proof of Proposition 3.1, the inverse problem can be reformulated as follows: Ext,t,xT min θ = arg min (cid:2)g2(t)v(xt, t, xT ) v(xt, t, xT )2(cid:3), s.t. Ext,t,x0,xT (cid:2)v(xt, t, xT ) xt log q(xtx0)2(cid:3)dt, Following the proof of Theorem 3.2, we obtain tractable formulation incorporating weighting function: (x0, xT ) pθ(x0, xT ), ([0, ]), xt q(xtx0, xT ). (cid:104) Ext,t,x0,xT min θ Ext,t,x0,xT min ϕ (cid:2)λ(t)v(xt, t, xT ) xt log q(xtx0)2(cid:3) (cid:2)λ(t)vϕ(xt, t, xT ) xt log q(xtx0)2(cid:3)(cid:105) . Utilizing the reparameterization under additional conditions ( v(xt, t, xT ) = xt αt(cid:98)x 0(xt, t, xT ) σ2 , 2.3), we obtain the following representations: xt αt(cid:98)xϕ 0 (xt, t, xT ) σ2 (cid:77) vϕ(xt, t, xT ) = . Consequently, applying the proof technique from Proposition 3.3, we derive the final expression: 0(xt, t, xT ) x02(cid:3) min (cid:2)λ(t)(cid:98)x (x0, xT ) pθ(x0, xT ), ([0, ]), xt q(xtx0, xT ). (cid:2)λ(t)(cid:98)xϕ Ext,t,x0 ϕ 0 (xt, t, xT ) x02(cid:3)(cid:105) , (cid:104) Ext,t,x min θ B. Experimental details All hyperparameters are listed in Table 7. We used batch size 256 and ema decay 0.99 for setups. For each setup, we started the student and bridge networks using checkpoints from the teacher models. In setups where the model adapts to noise: (1) We added extra layers for noise inputs (set to zero initially), (2) Noise was concatenated with input data before input it to the network. Datasets, code sources, and licenses are included in Table 8. 13 Inverse Bridge Matching Distillation Task 4 super-resolution (bicubic) 4 super-resolution (pool) JPEG restoration, QF = 5 JPEG restoration, QF = 10 Center-inpainting (128 128) Sketch to Image Sketch to Image Normal to Image Normal to Image Center-inpainting (128 128)"
        },
        {
            "title": "Dataset\nImageNet\nImageNet\nImageNet\nImageNet\nImageNet",
            "content": "I2SB I2SB I2SB I2SB I2SB Edges Handbags DDBM Edges Handbags DDBM DDBM DDBM DDBM Teacher NFE L/K ratio 1 1 1 1 4 2 1 2 1 4 DIODE-Outdoor DIODE-Outdoor ImageNet 5:1 5:1 5:1 5:1 5:1 5:1 5:1 5:1 5:1 1:1 LR 5e-5 5e-5 5e-5 5e-5 5e-5 1e-5 1e-5 1e-5 1e-5 3e-"
        },
        {
            "title": "Grad Updates Noise",
            "content": "3000 3000 2000 3000 2000 300 14000 500 3700 3000 3.5); L/K represents bridge/student Table 7. Table entries specify experimental configurations: NFE indicates multistep training (Sec. gradient iteration ratios (Alg. 3.4); Grad Updates shows student gradient steps; Noise notes stochastic pipeline incorporation. (cid:77) (cid:77) Table 8. The used datasets, codes and their licenses. URL Name EdgesHandbags GitHub Link Dataset Link DIODE-Outdoor Website Link ImageNet GitHub Link Guided-Diffusion I2SB GitHub Link GitHub Link DDBM GitHub Link DBIM License BSD MIT Citation (Isola et al., 2017) (Vasiljevic et al., 2019) (Deng et al., 2009) (Dhariwal & Nichol, 2021) MIT (Liu et al., 2023a) (Zhou et al., 2023) (Zheng et al., 2024) CC-BY-NC-SA-4.0 B.1. Distillation of I2SB models. We extended the I2SB repository (see Table 8), integrating our distillation framework. The following sections outline the setups, adapted following the I2SB. Multistep implementation In this setup, we use the student models full inference process during multistep training (Section 3.5). This means that x0 is generated with inferenced of the model Gθ through all timesteps (T = tN , . . . , t1 = 0) in the multistep sequence. The generated x0 is subsequently utilized in the computation of the bridge (cid:98)Lϕ or student (cid:98)Lθ objective functions, as formalized in Algorithm 1. 4 super-resolution. Our implementation of the degradation operators aligns with the filters implementation proposed in DDRM (Kawar et al., 2022). Firstly, we synthesize images at 64 64 resolution, then upsample them to 256 256 to ensure dimensional consistency between clean and degraded inputs. For evaluation, we follow established benchmarks (Saharia et al., 2022; Song et al., 2023) by computing the FID on reconstructions from the full ImageNet validation set, with comparisons drawn against the training set statistics. JPEG restoration. Our JPEG degradation implementation, employing two distinct quality factors (QF=5, QF=10), follows (Kawar et al., 2022). FID is evaluated on 10, 000-image ImageNet validation subset against the full validation sets statistics, following baselines (Saharia et al., 2022; Song et al., 2023). Inpainting. For the image inpainting task on ImageNet at 256 256 resolution, we utilize fixed 128 128 centrally positioned mask, aligning with the methodologies of DBIM (Zheng et al., 2024) and CDBM (He et al., 2024). During training, the model is trained only on the masked regions, while during generation, the unmasked areas are deterministically retained from the initial corrupted image xT to preserve structural fidelity of unmasked part of images. We trained the model with 4 NFEs via the multistep method (Section 3.5) and tested it with 1, 2, and 4 NFEs. B.2. Distillation of DDBM models. We extended the DDBM repository  (Table 8)  by integrating our distillation framework. Subsequent sections outline the experimental setups, adapted from the DDBM (Zheng et al., 2024). Multistep implementation In this setup, the multistep training (Section 3.5) adopts the methodology of DMD (Yin et al., 2024a), wherein timestep is uniformly sampled from the predefined sequence (t1, . . . , tN ). The model Gθ then generates 14 Inverse Bridge Matching Distillation x0 by iteratively reversing the process from the terminal timestep tN = to the sampled intermediate timestep t. This generated x0 is subsequently used to compute the bridge networks loss (cid:98)Lϕ or the student networks loss (cid:98)Lθ, as detailed in Algorithm 1. Edges Handbags The model was trained utilizing the EdgesHandbags image-to-image translation task (Isola et al., 2017), with the 64 64 resolution images. Two versions were trained under the multistep regime (Section 3.5), with 2 and 1 NFEs during training. Both models were evaluated using the same NFE to match training settings. DIODE-Outdoor Following prior work (Zhou et al., 2023; Zheng et al., 2024; He et al., 2024), we used the DIODE outdoor dataset, preprocessed via the DBIM repositorys script for training/test sets  (Table 8)  . Two versions were trained under the multistep regime (Section 3.5), with 2 and 1 NFEs during training. Both models were evaluated using the same NFE to match training settings. Inpainting All setups matched those in Section B.1 inpainting, except we use CBDM checkpoint (Zheng et al., 2024). This checkpoint is adjusted by the authors to: (1) condition on xT and (2) ImageNet class labels as input to guide the model. Also this is the same checkpoint used in both CDBM (He et al., 2024) and DBIM (Zheng et al., 2024) works. C. Additional results 15 Inverse Bridge Matching Distillation Figure 4. Uncurated samples for IBMD-I2SB distillation of 4x-super-resolution with bicubic kernel on ImageNet 256 256 images. 16 Inverse Bridge Matching Distillation Figure 5. Uncurated samples for IBMD-I2SB distillation of 4x-super-resolution with pool kernel on ImageNet 256 256 images. 17 Inverse Bridge Matching Distillation Figure 6. Uncurated samples for IBMD-I2SB distillation of Jpeg restoration with QF=5 on ImageNet 256 256 images. 18 Inverse Bridge Matching Distillation Figure 7. Uncurated samples for IBMD-I2SB distillation of Jpeg restoration with QF=10 on ImageNet 256 256 images. 19 Inverse Bridge Matching Distillation Figure 8. Uncurated samples for IBMD-I2SB distillation trained for inpaiting with NFE= 4 and inferenced with different inference NFE on ImageNet 256 256 images. 20 Inverse Bridge Matching Distillation Figure 9. Uncurated samples for IBMD-DDBM distillation trained for inpaiting with NFE= 4 and inferenced with different inference NFE on ImageNet 256 256 images. 21 Inverse Bridge Matching Distillation Figure 10. Uncurated samples from IBMD-DDBM distillation trained on the DIODE-Outdoor dataset (256 256) with NFE= 2 and NFE= 1, inferred using the corresponding NFEs on the training set.22 Inverse Bridge Matching Distillation Figure 11. Uncurated samples from IBMD-DDBM distillation trained on the DIODE-Outdoor dataset (256 256) with NFE= 2 and NFE= 1, inferred using the corresponding NFEs on the test set. 23 Inverse Bridge Matching Distillation Figure 12. Uncurated samples from IBMD-DDBM distillation trained on the Edges Handbags dataset (64 64) with NFE= 2 and NFE= 1, inferred using the corresponding NFEs on the training set.24 Inverse Bridge Matching Distillation Figure 13. Uncurated samples from IBMD-DDBM distillation trained on the Edges Handbags dataset (64 64) with NFE= 2 and NFE= 1, inferred using the corresponding NFEs on the test set."
        }
    ],
    "affiliations": [
        "Skolkovo Institute of Science and Technology",
        "Yandex Research"
    ]
}