{
    "paper_title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language Models",
    "authors": [
        "Keda Tao",
        "Haoxuan You",
        "Yang Sui",
        "Can Qin",
        "Huan Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video large language models (VideoLLMs) have demonstrated the capability to process longer video inputs and enable complex reasoning and analysis. However, due to the thousands of visual tokens from the video frames, key-value (KV) cache can significantly increase memory requirements, becoming a bottleneck for inference speed and memory usage. KV cache quantization is a widely used approach to address this problem. In this paper, we find that 2-bit KV quantization of VideoLLMs can hardly hurt the model performance, while the limit of KV cache quantization in even lower bits has not been investigated. To bridge this gap, we introduce VidKV, a plug-and-play KV cache quantization method to compress the KV cache to lower than 2 bits. Specifically, (1) for key, we propose a mixed-precision quantization strategy in the channel dimension, where we perform 2-bit quantization for anomalous channels and 1-bit quantization combined with FFT for normal channels; (2) for value, we implement 1.58-bit quantization while selectively filtering semantically salient visual tokens for targeted preservation, for a better trade-off between precision and model performance. Importantly, our findings suggest that the value cache of VideoLLMs should be quantized in a per-channel fashion instead of the per-token fashion proposed by prior KV cache quantization works for LLMs. Empirically, extensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show that VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit precision with almost no performance drop compared to the FP16 counterparts."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 7 5 2 6 1 . 3 0 5 2 : r Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language Models Keda Tao1,2, Haoxuan You3, Yang Sui4, Can Qin5, Huan Wang1,* Westlake University1, Xidian University2, Columbia University3 Rice University4, Salesforce AI Research5 https://github.com/KD-TAO/VidKV"
        },
        {
            "title": "Abstract",
            "content": "Video large language models (VideoLLMs) have demonstrated the capability to process longer video inputs and enable complex reasoning and analysis. However, due to the thousands of visual tokens from the video frames, key-value (KV) cache can significantly increase memory requirements, becoming bottleneck for inference speed and memory usage. KV cache quantization is widely used approach to address this problem. In this paper, we find that 2-bit KV quantization of VideoLLMs can hardly hurt the model performance, while the limit of KV cache quantization in even lower bits has not been investigated. To bridge this gap, we introduce VidKV, plug-and-play KV cache quantization method to compress the KV cache to lower than 2 bits. Specifically, (1) for key, we propose mixed-precision quantization strategy in the channel dimension, where we perform 2-bit quantization for anomalous channels and 1-bit quantization combined with FFT for normal channels; (2) for value, we implement 1.58-bit quantization while selectively filtering semantically salient visual tokens for targeted preservation, for better trade-off between precision and model performance. Importantly, our findings suggest that the value cache of VideoLLMs should be quantized in perchannel fashion instead of the per-token fashion proposed by prior KV cache quantization works for LLMs. Empirically, extensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show that VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit precision with almost no performance drop compared to the FP16 counterparts. 1. Introduction Video large language models (VideoLLMs) have demonstrated strong performance in understanding diverse video contexts [24, 13, 16, 17, 19, 20, 40, 41, 44, 46]. In long video inference scenarios, the key-value (KV) cache stores attention keys and values to avoid redundant computations. However, as the number of video input frames and batch size grows, the substantial memory consumption of the KV cache has emerged as significant bottleneck in the inference of VideoLLMs, incurring prohibitively large memory usage and slow speed. For instance, in the LLaVA-OV-7B [13], with batch size of 256 and 1,000 input frames, the KV cache required for visual tokens can reach 720 GB1 by estimation, significantly exceeding the models own size. Therefore, compressing the KV cache in VideoLLMs is imperative. In previous works on KV cache compression, most existing approaches focus on removing or merging less critical tokens from the cache to optimize memory usage [18, 23, 31, 32, 34, 37, 39, 50]. However, such methods may compromise performance as fewer tokens are used. promising alternative has focused on the quantization of KV cache, technique that reduces memory usage by converting high-bit floating-point KV caches into lower-bit forms [6, 8, 25, 36, 45]. This group of methods has effectively reduced memory requirements while preserving model performance. However, existing studies have mostly explored in the context of LLMs. Its applicability to VideoLLMs remains unexplored, to our best knowledge. On VideoLLMs, our preliminary results (as shown in Tab. 1) indicate that, due to the high redundancy of video tokens, the basic group-wise 2-bit KV cache quantization has already achieved promising performance, comparable to the original 16-bit KV cache. This finding suggests the possibility of exploring even lower-bit quantization for KV cache in VideoLLMs. To the best of our knowledge, no prior study has thoroughly analyzed the unique element distribution of KV caches of VideoLLMs in the context of low-bit (1.x bits) quantization. To bridge this gap, we analyze the distribution of KV caches in VideoLLMs. Our analyses suggest that: For the key cache, consistent with previous findings [22, 25, 43], certain channels exhibit significantly large magnitudes and substantial variations. These anomalous channels introduce considerable errors in low-bit quantization, leading to model collapse. For the value cache, our findings are distinct from prior KV cache quantization methods for LLMs, e.g., [25], which reCorresponding author: wanghuan@westlake.edu.cn 1(4 28 128 1000 196 256) bytes. 1 (a) Layer 5 Key (b) Layer 5 Value (c) Layer 24 Key (d) Layer 24 Value (e) Layer 5 Key (f) Layer 5 Value (g) Layer 22 Key (h) Layer 22 Value Figure 1. Magnitude of KV cache for LLaVA-OV-7B and Qwen2.5-VL-7B. (1) In the key cache, certain channels exhibit significantly large magnitudes, while others display abnormal variations across the channel dimension, making them challenging to quantize. (2) In the value cache, channels exhibit variations in range. ports substantial per-channel magnitude variations, while we find the per-token magnitude variations are more obvious (see Fig. 1). This new outlier pattern motivates us to reduce quantization errors in VideoLLMs by adopting per-channel quantization approach for the value cache. Based on the above analyses, we propose VidKV, lower-bit KV cache quantization method that operates without requiring fine-tuning for VideoLLMs. At its core, we design 1.x-bit mixed-precision quantization schemes for the key and value caches, respectively. Specifically, (1) for the key cache, we employ straightforward yet effective range-based channel evaluation method to perform 2-bit quantization on anomalous channels and 1-bit quantization on normal channels. Our findings indicate that transforming the key cache to the frequency domain via the Fast Fourier Transform (FFT) not only stabilizes the distribution of elements across channels but also mitigates the impact of outliers, thereby enhancing quantization accuracy and reducing the complexity of the quantization process. Consequently, we convert the key cache from the time domain to the frequency domain before performing 1-bit quantization and subsequently restore it to the time domain using the inverse FFT (IFFT). (2) For the value cache, we implement 1.58-bit quantization, mapping the values to the set {1, 0, 1}. The matrix multiplication between the value and attention weight can be reformulated as an addition operation, thereby reducing computational energy consumption. In addition, as an option to better tradeoff precision and model performance, we introduce token protection mechanism to identify small set of critical tokens based on their semantic relevance; tokens in this subset are preserved at 2-bit precision during value cache quantization. By doing so, the performance can be significantly preserved. Our contributions in this work are summarized as follows: We introduce training-free plug-and-play 1.x-bit KV cache quantization framework tailored for video LLMs, for the first time. Leveraging distribution characteristics, the method features mixed-precision quantization schemes designed separately for key and value caches. For key cache, we propose simple yet effective rangebased way to split the channels into anomalous and normal ones and then quantize the anomalous channels to 2 bits, and normal channels to 1 bit in the frequency domain. For value cache, we propose 1.58-bit quantization scheme while selecting few semantically salient tokens for protection, offering an option to better tradeoff performance with precision. Importantly, we find that in contrast to previous LLM studies, the value cache of VideoLLMs is more suitable for per-channel quantization. Experimental results on several video benchmarks demonstrate that VidKV effectively compresses the KV cache to 2 Figure 2. Overview of our proposed method VidKV. We implement 1.x-bit mixed-precision quantization for the key cache and 1.58-bit quantization for the value cache. In addition, as shown in the figure, to balance precision and model performance, we protect important visual tokens in the value cache. It is noteworthy that we perform mixed-precision quantization on the key cache along the channel dimension, whereas on the value cache, we apply mixed-precision quantization along the token dimension. All key-value caches are quantized in per-channel fashion, different from prior KV cache quantization methods for LLMs such as KIVI [25]. 1.5-bit and 1.58-bit precision, with almost no performance drop compared to the FP16 counterparts. 2. Related Work 2.1. Video Large Language Models With the rapid blooming of large language models (LLMs) and multimodal large language models (MLLMs), many works have explored incorporating video encoders and LLMs (termed as VideoLLMs) for the video understanding and reasoning tasks [1, 10, 11, 14, 14, 20, 27, 29, 40]. Regardless of good performance, the efficiency of VideoLLMs is usually limited due to large amount of frames in the videos. Improving efficiency has been focus in recent VideoLLM works. For example, VideoLLaMA [46] utilized Q-Former module [15] to pool the video tokens. XgenMM-Vid [33] learns compact video representation with only 32 tokens. MovieChat [35] introduced memory module to merge and store the video tokens. Although the potential of VideoLLMs for video understanding and inference is increasingly recognized, the tens of thousands of visual tags required for long videos significantly increase the KV cache size, thereby affecting inference time and memory requirements. Consequently, different from previous works [9, 37], we explore the lower-bit quantization for VideoLLMs KV caches for the first time. LLaVA-OV-7B Bit (K/V)"
        },
        {
            "title": "MovieChat",
            "content": "Baseline - C, - - C, - - C, - - C, - C"
        },
        {
            "title": "Variance\nRange\nOutlier",
            "content": "16 2 / 2 2 / 2 1.5 / 1.58 1.5 / 1.58 1.5 / 2 1.5 / 2 1.5 / 2 3.01 3.03 3.00 2.79 2.21 2.71 2.95 2. 47.87 47.68 43.63 47.08 13.76 45.11 48.28 32.87 Table 1. Results of simulated KV cache group-wise quantization under various configurations on LLaVA-OV-7B. The group size is fixed at 32. denotes per-channel quantization, while represents per-token quantization. The quantization range for 1.58-bit quantization is {1, 0, 1}. Range, Variance, and Outlier are the metrics employed for channel selection in the mixed-precision quantization of the key cache, where Range is defined as max min. INT2 while retaining the most recent window in FP16. CQ [48] proposes to couple multiple key and value channels together for quantization to exploit their dependency. Unlike existing works that primarily focus on LLMs, we aim to analyze and explore the unique characteristics of the KV cache in VideoLLMs, which contain both temporal and spatial features from the video modality. 3. Preliminaries 2.2. KV Cache Quantization 3.1. Background on Video LLM Inference KV cache quantization optimizes the storage of precomputed keys and values, alleviating the memory bottleneck by reducing memory consumption and accelerating generation [68, 25, 36, 45, 48]. KVQuant [8] introduces sensitivity-based and dense-and-sparse quantization techniques for the KV cache, aiming to minimize quantization errors. KIVI [25] analyzes the distribution differences between keys and values in the Multi-Head Attention module. Based on the observations, they quantize keys per-channel and values per-token using group-wise quantization into Video LLM inference typically comprises two stages: prefilling and decoding. (1) Prefilling Stage. During the prefilling phase, the model processes the token sequence generated from the prompt and produces the initial output token, while each attention layer computes and stores KV pairs. Let Xs Rlsd, Xv Rlvd, and Xt Rltd denote the system token, visual token, and text token, respectively, where ls, lv, and lt represent their corresponding input token sequence lengths, and is the hidden dimension of the model. In each atten3 tion layer, the KV cache is derived as follows: = Wk, = Wv, Kcache K, Vcache V, (1) where = concat[Xs, Xv, Xt] and Wk, Wv Rdd are the weight matrices for the key and value, respectively. (2) Decoding Stage. In the decoding phase, owing to the KV cache, the model takes single token R1d as input. Subsequently, the attention output can be calculated as Qx = Wq, Kx = Wk, Vx = Wv, [Kcache, KX ], = Softmax [Vcache, Vx], (cid:19) (cid:18) Qx(K) V. (2) (3) (4) Quantization Error 2.39 Quantization Error 1.14 Figure 3. Analysis of the normal channel in the key cache and its distribution following FFT transformation reveals that the frequency-domain distribution is smoother after using FFT transformation, resulting in reduced quantization error. 3.2. KV Cache Quantization The n-bit integer KV cache quantization and dequantization process is formulated as follows: Q(X) = clamp (cid:18)(cid:22) zX (cid:25) sX (cid:19) , 0, 2n 1 , (5) = (X) sX + zX, (6) where sX = max(X)min(X) 2n1 min(X), and indicates round operation. is the scaling factor, zX = Notably, for video LLMs, the basic group-wise 2-bit KV cache quantization is sufficient to preserve model performance due to the high redundancy of video inputs. This observation motivated us to investigate the KV cache quantization of lower-bit or even 1-bit for video LLMs. 4. Methodology In Sec. 4.1, we are about to analyze the distribution characteristics of KV caches in video LLMs and evaluate the performance of 2-bit quantization. Our observations indicate that 2-bit quantization can hardly hurt the model performance due to the significant visual token redundancy, leading us to explore even lower-bit quantization. Based on these findings, we shall present VidKV, our 1.x-bit KV cache quantization method for video LLMs, as detailed in Secs. 4.2 and 4.3. 4.1. KV Cache Distribution of video LLMs Numerous prior studies have analyzed the KV cache distribution in LLMs. However, these findings have not been thoroughly validated for video LLMs. To better understand the KV cache, Fig. 1 demonstrate the KV cache distributions. It is observed that the key cache matrix frequently contains distinct outlier channels, with an average amplitude greater than other channels. This agrees with prior observations [8, 25]. Drastic variations in the magnitude of 4 abnormal channels significantly increase the complexity of the quantization process. Therefore, we propose applying low-bit (1-bit) quantization to the more stable channels while preserving high-bit (2-bit) quantization for the anomalous channels to minimize quantization errors. Furthermore, we observe that the magnitude of the value cache exhibits smoother variations within channel than that of the key cache. Overall, the value cache remains relatively stable. However, the magnitude of the value cache exhibits significant variability along the token dimension; therefore, the per-channel quantization is preferable to per-token quantization for value cache, which contradicts previous findings in LLMs [25]. As shown in Tab. 1, per-channel quantization provides significant accuracy advantage over per-token quantization for the value cache. Despite 2-bit quantization, per-channel quantization for the value cache may still yield slight improvements, as the impact remains nearly lossless. 4.2. Mixed-precision Quantization for Key Cache (1) Channel Selection. As previously analyzed, the key cache contains certain anomalous channels that pose challenges for lower-bit quantization. To address this, we explore mixed-precision quantization approach. Specifically, we first assess the quantization difficulty of each channel. Channels that are easier to quantize (normal) undergo 1-bit quantization, while more abnormal channels are assigned with 2-bit quantization to minimize quantization error. Thus, properly splitting the channels into abnormal and normal groups is critical problem here. It is known that quantization becomes increasingly challenging to assess when magnitude distributions exhibit drastic fluctuations and contain numerous outliers. Therefore, we explored several evaluation methods along the channel dimension, including Variance σ2, Range = max(K) min(K), and the number of Outliers Noutliers = (cid:80)l I(Ki > K), where i=1 is the number of tokens, I() is an indicator function that returns 1 if the condition is met, otherwise 0 and is predefined threshold. Tab. 1 (marked in light blue background) presents the results of an average 1.5-bit quantization (where 50% of the channels undergo 1-bit quantization) for the key cache using different evaluation methods. For all configurations, we set the group size to 32, = 3, and maintain the value cache at fixed 2-bit quantization. Specifically, we observe that evaluating anomalous channels using the Range achieves near-lossless quantization accuracy, whereas the other two methods exhibit certain degree of performance degradation. As shown in Fig. 2, VidKV selects k% abnormal channels in the key cache for 2-bit quantization by evaluating the range in each channel, while assigning the remaining normal channels to 1-bit quantization. (2) FFT-based 1-Bit Quantization. As analyzed in Sec. 4.1, the key cache contains numerous abnormal channels, and the distribution of each channel in the time domain exhibits sharp fluctuations, which not only complicates 1-bit quantization but also results in the uneven accumulation of quantization errors across different channels. To address this, we propose to apply Fast Fourier Transform (FFT) to transform data from the time domain to the frequency domain and mitigate large oscillations in the channel dimension by leveraging frequency domain properties (such as increased stability and energy concentration), as illustrated in Fig. 3. Due to the significant reduction in quantization error, 1-bit quantization not only substantially decreases storage overhead but also mitigates the loss of effective information. Thus, the specific quantization and dequantization process can be written as follows, Q(Xfft) = sign(cid:0)FFT(X)) + 1 {0, 1}, (7) = IFFT[Q (Xfft) sfft + zfft ], where the scale sfft = Mean(FFT(X)) and the zero offset zfft = 0. (8) 4.3. 1.x-Bit Quantization for Value Cache (1) 1.58-Bit Quantization. For the value cache, we propose to employ promising lower-bit quantization approach: 1.58-bit quantization as the base scheme. While 1.58-bit quantization has previously demonstrated its effectiveness for LLM weight quantization [28], we explore its applicability to KV cache quantization for the first time. The 1.58-bit means ternary quantization, i.e., mapping value to {1, 0, 1}. Concretely, in the prefilling stage, we compute the average value as the threshold and subsequently constrain the values to -1, 0, or +1: Q(V )1.58 = 1, > α, 1, < α, (9) 0, otherwise, Figure 4. Illustration of our 1.58-bit quantization for the value cache during the decoding stage. where α = γ meanV , and γ is hyperparameter. Notably, significant advantage of the 1.58-bit quantization is its potential in faster and cheaper computing. As shown in Fig. 4, the matrix multiplication between value and attention weights can be replaced with addition and subtraction, significantly reducing the computational energy consumption. Although 1-bit quantization of the value cache still poses challenges, the 1.58-bit scheme retains its advantages, especially its computational efficiency. (2) Semantic Token Protection (STP). Additionally, in video LLMs, certain visual tokens play more crucial role in the inference process due to their strong correlation with the input text, inspiring us to apply higher protection to these critical visual tokens through 2-bit quantization, thereby minimizing quantization errors for these essential tokens. Furthermore, this approach ensures that lower-bit quantization of other tokens does not adversely impact the accuracy of critical tokens. As illustrated in Fig. 2, the selection mechanism relies on cross-modal attention scores between each vision token and the text query: = Xv(i) Xt(j). (10) Selective application of 2-bit quantization to the top visual tokens preserves the semantic integrity of the most informative visual features, where = lv and is the percentage of tokens protection. Meanwhile, the remaining tokens undergo 1.58-bit quantization, maintaining resource efficiency while preserving essential semantic information. 5. Experimental Results 5.1. Experiment Settings Models. We select two of the most widely used video large language model families to evaluate our VidKV: LLaLA5 Method Baseline KIVI VidKV VidKV VidKV (p = 0.2) VidKV (p = 0.2) Baseline KIVI VidKV VidKV VidKV (p = 0.2) Settings VideoDC TempCompass VideoChat-GPT Moviechat WorldQA K-(Bit) V-(Bit) GPT Sco. Avg. CI DO CU TU CO Avg. GPT Score Acc. GPT Sco. 16-Bit 2-Bit (K-C V-T) 2-Bit (K-C V-C) 1.50 1.50 1.25 1.50 1.75 2.00 1.58 1. 1.66 1.66 16-Bit 2-Bit (K-C V-T) 2-Bit (K-C V-C) 1.50 1.25 2.00 1.50 1.50 2.00 2.00 1.58 1.58 1. 3.01 3.00 3.03 2.95 2.79 2.53 2.89 2.92 2.93 2.93 2. 2.88 2.54 3.01 2.68 2.87 LLaVA-OV-7B 3.47 3.48 3.48 3.49 3.32 3. 3.35 3.38 2.97 2.95 2.95 2.94 2.77 2.66 2.79 2.83 Qwen2.5-VL-7B 3.20 3.30 3.31 3.31 3.20 3.15 3.08 3.15 2.91 2.97 2. 2.90 2.76 2.81 2.78 2.85 49.05 49.70 50.69 50.45 47.35 45.21 47.55 48. 56.53 55.63 55.39 54.24 50.49 52.03 49.10 49.20 3.71 3.68 3. 3.63 3.57 3.59 3.60 3.61 3.36 3.54 3.57 3.56 3.43 3.46 3.41 3. 2.74 2.72 2.72 2.70 2.58 2.47 2.66 2.61 2.71 2.71 2. 2.67 2.56 2.58 2.51 2.63 3.49 3.35 3.55 3.38 3.10 3.06 3.11 3. 3.31 3.32 3.38 3.35 3.10 3.16 3.12 3.38 3.27 3.24 3. 3.23 3.06 3.01 3.10 3.13 3.10 3.17 3.18 3.15 3.01 3.03 3.00 3. 3.09 3.05 3.08 3.12 3.11 3.06 3.11 3.12 2.95 2.85 2. 2.92 2.90 2.92 2.91 2.92 47.87 46.63 47.68 48.28 47.08 47.21 47.25 47. 44.23 43.28 45.01 44.56 44.93 42.99 43.36 44.17 0.328 0.326 0. 0.322 0.313 0.309 0.319 0.312 0.334 0.330 0.332 0.311 0.286 0.309 0.310 0. Table 2. Results of different methods and quantization settings. For all values, higher is better. The best result of each metric in each model is in bold, and the second best is underlined. VideoChat-GPT comprises five subtasks: CI stands for correctness of information, DO stands for detail orientation, CU stands for contextual understanding, TU stands for temporal understanding, and CO stands for consistency. 1.66-bit means 20% tokens for 2-bit and 80% tokens for 1.58-bit Settings VATEX K-(Bit) V-(Bit) FFT BLEU-4 Meteor Rouge-L CIDEr Method Baseline KIVI VidKV VidKV VidKV (p = 0.2) LLaVA-OV-7B 16-Bit 2-Bit (K-C V-T) 2-Bit (K-C V-C) 1.5-Bit 1.5-Bit 1.5-Bit 1.5-Bit 1.75-Bit 2-Bit 2-Bit 1.58-Bit 1.66-Bit 1.66-Bit - - 14.88 14.33 15.24 14.06 14.96 14.06 15.31 15.20 Qwen2.5-VL-7B Baseline KIVI VidKV VidKV 16-Bit 2-Bit (K-C V-T) 2-Bit (K-C V-C) 1.5-Bit 2-Bit 1.5-Bit 2-Bit 1.58-Bit 1.58-Bit VidKV (p = 0.2) 1.5-Bit 1.66-Bit - - 19.17 19.20 19.97 19.46 13.61 13.09 14.63 19.85 19.55 19. 18.91 19.47 16.43 17.99 18.34 20.43 21.26 21.26 21.02 17.62 17.44 17. 39.25 38.67 39.45 38.11 39.01 35.28 37.24 38.44 40.99 41.66 42. 42.04 36.90 35.88 37.75 27.42 26.27 27.57 23.38 25.91 20.09 23.06 24. 41.87 41.98 43.15 41.86 28.86 29.76 31.72 Table 3. Comparison of different quantization settings on VATEX benchmarks. For all values, higher is better. The best result of each metric in each model is in bold, and the second best is underlined. FFT is exclusively applied alongside 1-bit quantization within mixed-precision quantization. OneVision [13] and Qwen2.5-VL [3]. We utilize the Hugging Face Transformers codebase and implement our VidKV algorithm on top of it. Specifically, we evaluate LLaLAOneVision-7B on 8 RTX 4090 GPUs, supporting up to 32 video input frames, and Qwen2.5-VL-7B on 8 A6000 GPUs, supporting up to 16 input frames. Tasks. For the evaluation of VLLMs, we do not select common video question-answering (QA) tasks where only single word is generated. Instead, we adopt the VideoDC [26], 6 VideoChat-GPT [29], MovieChat [35], TempCompass [24], VATEX [42], and WorldQA [49] benchmarks to evaluate long text generation performance on short videos. VideoDC is benchmark for single-video description. VideoChat-GPT evaluates performance across five key metrics: Correctness of Information, Detailed Orientation, Contextual Understanding, Temporal Understanding, and Consistency. These metrics are assessed using an LLM-generated prediction score ranging from 0 to 5 (GPT Score). MovieChat assesses models comprehension ability to long videos, evaluated through combination of GPT Score and accuracy. TempCompass evaluates five key aspects: action, speed, direction, attribute change, and event order. For KV cache evaluation, we use the caption branch task on TempCompass and only test the text generation task on WorldQA. Finally, VATEX is specialized video caption generation benchmark, and its accuracy is assessed using four metrics: BLEU [30], METEOR [5], ROUGE-L [21], and CIDEr [38]. Implementation Details. In group-wise quantization, we set residual length inspired by KIVI [25] to store the parts that are not divisible. We set the quantized group size to 32 and the residual key-value cache length to 128 in all experiments. The hyperparameter for threshold calculation in 1.58-bit quantization γ is set to 0.7 for 1.58-bit quantization. The key cache is quantized at mixed precisions, ranging from 1-bit to 2-bit. Both models use 128 cache channels. Owing to FFT computations, FFT-based 1-bit quantization is applied exclusively to key-1.5-bit (k = 0.5) and key-1.75bit (k = 0.75), while standard 1-bit quantization is used otherwise. 5.2. Main Results and Analyses This section presents the primary results of cache quantization for 1.x-bit KV representations. In the analyses, the key cache is tested within quantization range of 1.25 to 2 bits, while the value cache is evaluated with 1.58-bit and 1.66-bit (20% tokens for 2-bit and 80% tokens for 1.58-bit) quantization, both employing per-channel quantization. All benchmarks utilize the LMMs-Eval [12, 47] framework for evaluation and all evaluated code remains consistent with the official implementation. (1) Video Understanding. We evaluate VidKV across multiple video understanding benchmarks. Results in Tab. 2 show that the LLaVA-OV-7B and Qwen2.5-VL-7B models after 2-bit KV cache quantization can achieve comparable or slightly better performance vs. their FP16 counterparts. As is aforementioned, this motivated us to explore lower-bit quantization at the beginning. Additionally, VidKV is compared with KIVI [25] under 2bit quantization. Notably, the key distinction between VidKV and KIVI lies in the application of per-channel quantization in the value cache. Results indicate that VidKV outperforms KIVI across multiple benchmarks, validating the necessity of our KV cache distribution analyses for video LLMs. For 1.x-bit quantization, both models indicate that when the key cache is quantized to 1.5 bits, accuracy remains nearly unchanged, demonstrating the effectiveness of our proposed mixed-precision quantization and the FFT-based 1-bit quantization strategy for the key cache. For the LLaVAOV-7B model, reducing the KV cache precision from 16 bits to 1.5 bits (or even 1.25 bits) and 1.58 bits (for value cache) results in only minimal accuracy degradation. The Qwen2.5-VL-7B model employs highly compressed vision token representation relative to other video LLMs, wherein each video frame corresponds to 90 tokens. Consequently, slight degradation in accuracy is observed in the Qwen2.5VL-7B modelparticularly in the TempCompass, VideoDC, and WorldQA benchmarksalthough the performance remains within an acceptable range. Furthermore, enabling semantic token protection (STP) for the value cache increased the average quantized bit from 1.58 to 1.66, resulting in improved accuracy across multiple benchmarks (marked in yellow background), with notable improvement on VideoChat-GPT, as shown in Tab. 2. Spending less than 0.1 bit in both models allows them to attain accuracy comparable to that of the FP16 configuration. (2) Video Caption. For the video captioning task, VATEX is used for evaluation. As shown in Tab. 3, once again, no accuracy loss is observed for the quantization of the 2-bit KV cache. However, for 1.x-bit quantization (K-1.5, V-1.58), decline is observed across the four evaluation metrics, though (a) (b) Figure 5. Additional results on KV cache 1.x-bit quantization of VidKV. (a) Results indicate that the key cache undergoes sharp performance drop when quantized to 1.1-bit. Notably, FFT-based 1-bit quantization is applied only when the key cache is quantized to 1.5-bit or 1.75-bit, due to the computational constraints of FFT. (b) For value cache, as increases from 0 to 1, the average number of bits increases from 1.58 to 2. some accuracy recovery is achieved through STP. Considering the distinction between VATEX and other datasets that utilize GPT-based scoring, the four evaluation metrics used by VATEX are hard indicators, which are more sensitive to variations in the generated text and exhibit lower flexibility. Under this strict evaluation environment, our proposed VidKV continues to demonstrate acceptable performance. 5.3. Ablation Study Lower-Bit Key Cache Quantization. As shown in Fig. 5 (a), this study further investigates the principles and potential of lower-bit quantization for the key cache, building upon the findings of the previous section. While the value cache maintains 2-bit and 1.58-bit quantization, the key cache can be quantized from 1.75-bit to 1.2-bit with only minor reduction in accuracy. However, significant performance loss is observed when the key cache is quantized below 1.2 bits. These observations indicate that certain abnormal channels in the key cache induce significant quantization errors when subject to 1-bit quantization, implying that effective 1-bit quantization for the key cache remains challenging. STP of Value Cache. Preserving subset of tokens at higher precision consistently improves final accuracy. Thus, the proposed STP is compared against the random selection of an equivalent proportion of tokens, as demonstrated in Tab. 4, where STP outperforms random selection. Additionally, as shown in Fig. 5 (b), we investigate the trade-off between precision and model performance in the value cache using the STP method. Our results indicate that as increases, the average bit number of the value cache initially improves, resulting in gradual enhancement of model performance. However, 1-bit quantization of the value cache results in unacceptable performance degradation. FFT-based 1-Bit Quantization. Tab. 4 (marked in green) 7 Settings VideoDC MovieChat TempCompass VideoChat-GPT Bit FFT STP RTP 16-Bit K-1.5 / - 2 K-1.5 / - 2 K-1.5 / - 1.66 K-1.5 / - 1.66 16-Bit K-1.5 / - 2 K-1.5 / - 2 - - - - - - - 0.0 0.0 0.2 0. - 0.0 0.0 GPT Sco. GPT Sco. Acc. Avgerage CI DO CU TU CO Avg. 3.01 2.92 2.95 2.89 2.89 2.93 2.81 2.88 LLaVA-OV-7B 3.09 3.06 3.12 3.11 3.11 47.87 47.49 48.28 47.01 47. Qwen2.5-VL-7B 2.95 2.92 2.94 44.23 44.27 44.89 49. 48.98 50.45 46.36 47.55 56.53 52.32 54.24 3.47 3.47 3. 3.26 3.35 3.20 3.30 3.34 2.97 2.87 2.94 2.77 2. 2.91 2.86 2.95 3.71 3.60 3.63 3.54 3.65 3. 3.55 3.58 2.74 2.67 2.70 2.63 2.66 2.71 2.72 2. 3.49 3.33 3.38 3.10 3.11 3.31 3.29 3.31 3. 3.18 3.23 3.06 3.12 3.10 3.14 3.21 Table 4. Results of the ablation study of our method. In each pair of comparison results, the superior result is shown in bold. STP employs the proposed semantic-based token filtering protection strategy, while RTP protects randomly screened tokens. FFT is exclusively applied alongside 1-bit quantization within mixed-precision quantization. Settings VideoDC MovieChat TempCompass WorldQA Method Bit GPT Sco. GPT Sco. Acc. Baseline KIVI Ours 16-Bit 1-Bit 1-Bit Task Method Bit Baseline KIVI Ours 16-Bit 1-Bit 1-Bit 3.01 0.99 1.25 CI 3.47 0.66 1. 3.09 0.53 2.51 DO 2.97 1.07 1.40 47.87 0.910 31.15 VideoChat-GPT CU 3.71 0.94 1.53 Avg. 49.05 2.45 12.8 TU 2.74 0.95 1.22 GPT Sco. 0.33 - 0.15 CO 3.49 1.22 1.33 Table 5. Results of 1-Bit Quantization for KV Cache. The - symbol indicates complete model failure. shows the performance differences between using and not using the proposed FFT-based 1-bit quantization within the mixed-precision quantization for the key cache. The results indicate that the application of FFT enhances model performance across all benchmarks. Furthermore, as shown in Tab. 3 (marked in green), applying FFT to the video caption task leads to significant improvement in model quantization accuracy. Combined with FFT, similar trend is also observed in Fig. 5 (a). 5.4. Further Discussion on 1-bit Quantization This study provides an initial investigation into low-bit KV cache quantization (1.x-bit) for video LLMs. Empirical results across multiple benchmark programs indicate that maintaining 1.5-bit quantization for the key cache and 1.58bit quantization for the value cache results in negligible accuracy degradation. Nonetheless, extreme 1-bit quantization remains highly challenging and frequently results in model collapse. Tab. 5 presents comparative evaluation of the proposed VidKV and KIVI [25] under 1-bit quantization. Although VidKV experiences substantial performance degradation under 1-bit quantization, it still outperforms KIVI significantly. Future research will focus on advancing low-bit KV cache quantization to minimize bit-width while approaching the theoretical lower limit. Different Quantization Strategies of & V. Unlike previous studies, we introduce two distinct quantization strategies for key and value cache, respectively. Our findings indicate that the distribution characteristics of the two caches differ, making it challenging to directly apply the key caches mixed-precision quantization strategy to the value cache. Thus, more efficient and suitable approach1.58-bit quantizationis selected for the value cache. This approach retains almost all the advantages of 1-bit quantization and yields strong results. An attempt was also made to apply 1.58-bit quantization to the key cache, but it proved ineffective due to significant variations in the channel dimension. Accordingly, the proposed two different strategies for KV caching are based on their unique distribution characteristics, with extensive experiments confirming their effectiveness. 6. Conclusion This paper presents VidKV, the first KV cache quantization method for video LLMs. At its core, VidKV employs mixed-precision strategy to quantize key and value caches separately with specialized schemes: (1) key cache is quantized to 2 bits and 1 bit, where novel FFT-based quantization scheme is introduced for the 1-bit quantization, which effectively mitigates the performance drop; (2) value cache is quantized to 2-bits and 1.58 bits (+1/-1/0), where we importantly find the value cache should also be quantized in per-channel fashion, instead of the per-token fashion as argued by prior counterpart methods for LLMs (KIVI [25]), implying KV cache quantization for video LLMs is different from that for LLMs. Extensive experiments on six standard benchmarks show that we achieve 1.5-bit and 1.58-bit KV cache quantization without significant performance loss. Notably, the method is training-free and plug-and-play."
        },
        {
            "title": "References",
            "content": "[1] Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Deyao Zhu, Jian Ding, and Mohamed Elhoseiny. Minigpt4-video: Advancing multimodal llms for video understanding with interleaved visual-textual tokens. arXiv preprint arXiv:2404.03413, 2024. 3 [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. 1 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5vl technical report. arXiv preprint arXiv:2502.13923, 2025. 6 [4] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 1 [5] Michael Denkowski and Alon Lavie. Meteor universal: Language specific translation evaluation for any target language. In Proceedings of the ninth workshop on statistical machine translation, 2014. 6 [6] Haojie Duanmu, Zhihang Yuan, Xiuhong Li, Jiangfei Duan, Xingcheng Zhang, and Dahua Lin. Skvq: Sliding-window key and value cache quantization for large language models. arXiv preprint arXiv:2405.06219, 2024. 1, [7] Yefei He, Luoming Zhang, Weijia Wu, Jing Liu, Hong Zhou, and Bohan Zhuang. Zipcache: Accurate and efficient kv cache quantization with salient token identification. In NeurIPS, 2024. [8] Coleman Richard Charles Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael Mahoney, Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization. In NeurIPS, 2024. 1, 3, 4, 2 [9] Xiaohu Huang, Hao Zhou, and Kai Han. Prunevid: Visual token pruning for efficient video large language models. arXiv preprint arXiv:2412.16117, 2024. 3 [10] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. In CVPR, 2024. 3 [11] Yang Jin, Zhicheng Sun, Kun Xu, Liwei Chen, Hao Jiang, Quzhe Huang, Chengru Song, Yuliang Liu, Di Zhang, Yang Song, et al. Video-lavit: Unified video-language pre-training with decoupled visual-motional tokenization. arXiv preprint arXiv:2402.03161, 2024. 3 [12] Bo Li, Peiyuan Zhang, Kaichen Zhang, Fanyi Pu, Xinrun Du, Yuhao Dong, Haotian Liu, Yuanhan Zhang, Ge Zhang, Chunyuan Li, and Ziwei Liu. Lmms-eval: Accelerating the development of large multimodal models, 2024. [13] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 1, 6 [14] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 3 [15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023. 3 [16] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 1 [17] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In CVPR, 2024. 1 [18] Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: Llm knows what you are looking for before generation. In NeurIPS, 2024. [19] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. In ECCV, 2024. 1 [20] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. 1, 3 [21] Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, 2004. 6 [22] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. In MLSys, 2024. 1 [23] Akide Liu, Jing Liu, Zizheng Pan, Yefei He, Reza Haffari, and Bohan Zhuang. Minicache: Kv cache compression in depth dimension for large language models. In NeurIPS, 2024. 1 [24] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv preprint arXiv:2403.00476, 2024. 6 [25] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: tuning-free asymmetric 2bit quantization for kv cache. In ICML, 2024. 1, 3, 4, 6, 7, 8, [26] LMMs-Lab. Video detail caption, 2024. Accessed: 2024-11. 6 [27] Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Da Li, Pengcheng Lu, Tao Wang, Linmei Hu, Minghui Qiu, and Zhongyu Wei. Valley: Video assistant with large language model enhanced ability. arXiv preprint arXiv:2306.07207, 2023. 3 9 the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 1 [42] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. Vatex: large-scale, highquality multilingual dataset for video-and-language research. In CVPR, 2019. [43] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In ICML, 2023. 1 [44] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava: Parameter-free llava extension from images to videos for video dense captioning. arXiv preprint arXiv:2404.16994, 2024. 1 [45] Yuxuan Yue, Zhihang Yuan, Haojie Duanmu, Sifan Zhou, Jianlong Wu, and Liqiang Nie. Wkvquant: Quantizing weight and key/value cache for large language models gains more. arXiv preprint arXiv:2402.12065, 2024. 1, 3 [46] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 1, 3 [47] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmmseval: Reality check on the evaluation of large multimodal models, 2024. 7 [48] Tianyi Zhang, Jonah Yi, Zhaozhuo Xu, and Anshumali Shrivastava. Kv cache is 1 bit per channel: Efficient large language model inference with coupled quantization. In NeurIPS, 2024. [49] Yuanhan Zhang, Kaichen Zhang, Bo Li, Fanyi Pu, Christopher Arif Setiadharma, Jingkang Yang, and Ziwei Liu. Worldqa: Multimodal world knowledge in videos through long-chain reasoning. arXiv preprint arXiv:2405.03272, 2024. 6 [50] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. In NeurIPS, 2023. 1 [28] Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Lifeng Dong, Ruiping Wang, Jilong Xue, and Furu Wei. The era of 1-bit llms: All arXiv preprint large language models are in 1.58 bits. arXiv:2402.17764, 2024. 5 [29] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. 3, 6 [30] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In ACL, 2002. 6 [31] Xiaohuan Pei, Tao Huang, and Chang Xu. Cross-self kv cache pruning for efficient vision-language inference. arXiv preprint arXiv:2412.04652, 2024. [32] Siyu Ren and Kenny Zhu. On the efficacy of eviction policy for key-value constrained generative language model inference. arXiv preprint arXiv:2402.06262, 2024. 1 [33] Michael Ryoo, Honglu Zhou, Shrikant Kendre, Can Qin, Le Xue, Manli Shu, Silvio Savarese, Ran Xu, Caiming Xiong, and Juan Carlos Niebles. xgen-mm-vid (blip-3-video): You only need 32 tokens to represent video even in vlms. arXiv preprint arXiv:2410.16267, 2024. 3 [34] Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, et al. Longvu: Spatiotemporal adaptive compression for long video-language understanding. arXiv preprint arXiv:2410.17434, 2024. 1 [35] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In CVPR, 2024. 3, 6 [36] Zunhai Su, Zhe Chen, Wang Shen, Hanyu Wei, Linge Li, Huangqi Yu, and Kehong Yuan. Rotatekv: Accurate and robust 2-bit kv cache quantization for llms via outlier-aware adaptive rotations. arXiv preprint arXiv:2501.16383, 2025. 1, 3 [37] Keda Tao, Can Qin, Haoxuan You, Yang Sui, and Huan Wang. Dycoke: Dynamic compression of tokens for fast video large language models. In CVPR, 2025. 1, 3 [38] Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In CVPR, 2015. 6 [39] Zhongwei Wan, Ziang Wu, Che Liu, Jinfa Huang, Zhihong Zhu, Peng Jin, Longyue Wang, and Li Yuan. Look-m: Lookonce optimization in kv cache for efficient multimodal longcontext inference. arXiv preprint arXiv:2406.18139, 2024. [40] Jiawei Wang, Liping Yuan, Yuchen Zhang, and Haomiao Sun. Tarsier: Recipes for training and evaluating large video description models. arXiv preprint arXiv:2407.00634, 2024. 1, 3 [41] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of 10 Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language Models"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Detailed Implementations In this section, we present the algorithm for VidKV as discussed in Sec. 4 (Algorithms 1 to 3). Algorithm 1 details the computation process of VidKV during both the prefilling and decoding phases, while Algorithms 2 and 3 respectively present the custom functions employed. Algorithm 1: Algorithm of VidKV parameter: Group size G, residual length R, hyperparameters p, procedure Prefill: Input: Rld XK = XWK, XV = XWV = l%G XVq = XV [: r], XVr = XV [l :] XKq = XK[: r], XKr = XK[l :] if > 0 then Q(XVq ) STPQuant(XVq ) end else Q(XVq ) 1.58Quant(XVq ) end Q(XKg ) MixQuant(XKq , k) KV cache Q(XKq ), XKr , Q(XVq ), XVr return XK, XV end procedure Decoding: Input: KV cache, R1d Qx = xWQ, Kx = xWK, Vx = xWV Q(XKq ), XKr , Q(XVq ), XVr KV cache XKr Concat([XKr , xK], dim=token) XVr Concat([XVr , xV ], dim=token) XK Concat([X Aw Softmax(xQX xO AwQ(XVq ) + AwXVr if len(XVr ) = then , XKr ], dim=token) K), dim=token) DeQuant(Q(XKq )) Kq Kq Q(XVr ) 1.58Quant(XVr ) Q(XVq ) Concat([Q(XVq ), Q(XVr )]) XVr empty tensor. Q(XKq ) MixQuant(XK) XKr empty tensor. end KV cache Q(XKq ), XKr , Q(XVq ), XVr return xO end Algorithm 2: Function of 1.58-Bit Quantization parameter: Group size G, residual length R, important token index mask E, hyperparameters p, k, γ function 1.58Quant(XVq ): Mean(XVq , dim=channel) α γs xv > α, Q(XVq ) = 1, 1, xv < α, 0, otherwise return Q(XVq ) end function STPQuant(XVq ): XVq [E] XVq [ E] XV 1 XV 2 Q(XV 1 Q(XV 2 return [Q(XV q ) GQuant(XV 1 ) 1.58Quant(XV 2 ), Q(XV 2 )] q ) , d=channel, bit=2) end Algorithm 3: Function of Key Mix-Quantization parameter: Group size G, residual length function MixQuant(XKq , k): xk flatten(XKq ) range Max(xk) Min(xk) D-mask TopK(range, k) N-mask D-mask Xnom XKq [N-mask] Q(XK1 Xabn XKq [D-mask] Q(XK2 return [Q(XK1 ), Q(XK2 q ) 1BitQuant(Xnom, k) ) GQuant(Xabn, d=channel, bit=2) )] end function 1BitQuant(X, k): if in [0.5, 0.75] then Xfft FFT(X) Reshape(Xfft, [l, 2]) end Q(Xq) GQuant(X, d=channel, bit=1) return Q(Xq) end B. Ablation Study about the weight γ Fig. 7 presents visual comparison of the impact of different γ settings on 1.58-bit quantization accuracy. The results indi-"
        },
        {
            "title": "Video Understanding",
            "content": "User: 16-Bit: Describe what happens in this video. The video features small orange kitten and yellow chick interacting in cozy setting. The kitten is lying on its back on beige surface, with its paws stretched out and eyes closed, appearing to be asleep or very relaxed. The chick stands next to the kitten, occasionally pecking at the kittens face and pawing at its nose. The background includes brown pillow and beige textured surface, creating warm and comfortable atmosphere. Throughout the video, the text Teddy Kittens appears in the bottom right corner, indicating the name of the channel or creator. The interaction between the kitten and the chick is gentle and playful, with the chick moving around and occasionally standing on the kittens belly. 1.5 & 1.58 Bit: The video features close-up of small, fluffy kitten with light brown and white fur lying on its back. yellow chick stands on the kittens stomach, pecking gently at its face. The kitten remains still and relaxed throughout the clip, creating heartwarming scene of interspecies friendship. The background is textured beige surface, possibly cushion or blanket, adding to the cozy atmosphere. The text Teddy Kittens appears in the bottom right corner, indicating the source or creator of the video. The interaction between the kitten and the chick is tender and peaceful, emphasizing their bond. Figure 6. Showcases of our VidKV with LLaVA-OV-7B on long video understanding. The red mark indicates the reasoning results after KV cache quantization remain consistent with the 16-bit, showing that there are no abnormal errors in the output after KV cache quantization. and periodicityparticularly within the value cachewhich contrasts with findings from previous studies on models such as Llama [8, 25]. We attribute this phenomenon to the reliance of most current VideoLLMs on the sequential concatenation of video tokens. Within tokens corresponding to video frame, tokens occupying identical positions frequently convey similar information and exhibit uniform distribution patterns, resulting in distinctive regularity that may inform strategies such as token screening or reordering. This observation will represent major direction for our future research. Additionally, we recognize the high redundancy inherent in visual tokens, and we will focus on strategies such as token pruning and merging in future work. Figure 7. Ablation study of γ. CI stands for correctness of information, DO stands for detail orientation, CU stands for contextual understanding, TU stands for temporal understanding, and CO stands for consistency. cate that, contrary to conventional assumptions, the optimal performance is not attained when γ = 1. Instead, the highest benchmark test performance is observed when γ = 0.7. significantly lower γ value adversely impacts model performance, suggesting that the chosen 1.58-bit quantization threshold hyperparameter is both reasonable and effective. C. More Observations and Future Work In Sec. 1 and Sec. 4.1, we analyzed the distribution characteristics of the KV cache in VideoLLMs. However, the distinct temporal characteristics of video data warrant further analysis. As illustrated in Fig. 1, the distribution of the KV cache across each channel in VideoLLMs exhibits regularity"
        }
    ],
    "affiliations": [
        "Columbia University",
        "Rice University",
        "Salesforce AI Research",
        "Westlake University",
        "Xidian University"
    ]
}