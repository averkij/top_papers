{
    "paper_title": "Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned and Addressed for XR Research",
    "authors": [
        "Ke Li",
        "Mana Masuda",
        "Susanne Schmidt",
        "Shohei Mori"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The development of radiance fields (RF), such as 3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF), has revolutionized interactive photorealistic view synthesis and presents enormous opportunities for XR research and applications. However, despite the exponential growth of RF research, RF-related contributions to the XR community remain sparse. To better understand this research gap, we performed a systematic survey of current RF literature to analyze (i) how RF is envisioned for XR applications, (ii) how they have already been implemented, and (iii) the remaining research gaps. We collected 365 RF contributions related to XR from computer vision, computer graphics, robotics, multimedia, human-computer interaction, and XR communities, seeking to answer the above research questions. Among the 365 papers, we performed an analysis of 66 papers that already addressed a detailed aspect of RF research for XR. With this survey, we extended and positioned XR-specific RF research topics in the broader RF research field and provide a helpful resource for the XR community to navigate within the rapid development of RF research."
        },
        {
            "title": "Start",
            "content": "Radiance Fields in XR: Survey on How Radiance Fields are Envisioned and Addressed for XR Research *"
        },
        {
            "title": "Ke Li",
            "content": ", Mana Masuda , Susanne Schmidt , Shohei Mori 5 2 0 2 7 ] . [ 2 6 2 3 4 0 . 8 0 5 2 : r Fig. 1: We observe gap between the anticipated interests in RF-related technologies expected to advance new XR applications (left) and the actual interests that are considered the main factors in XR (right). This survey paper investigates and discusses how RF technologies, including NeRF and 3DGS, are envisioned for XR (i.e., (cid:17) XR-Envisioned) and how they are actually implemented for XR (i.e., (cid:192) XR-Addressed) in scientific papers from XR, HCI, CV, CG, Robotics, and MM communities via systematic review. Abstract The development of radiance fields (RF), such as 3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF), has revolutionized interactive photorealistic view synthesis and presents enormous opportunities for XR research and applications. However, despite the exponential growth of RF research, RF-related contributions to the XR community remain sparse. To better understand this research gap, we performed systematic survey of current RF literature to analyze (i) how RF is envisioned for XR applications, (ii) how they have already been implemented, and (iii) the remaining research gaps. We collected 365 RF contributions related to XR from computer vision, computer graphics, robotics, multimedia, human-computer interaction, and XR communities, seeking to answer the above research questions. Among the 365 papers, we performed an analysis of 66 papers that already addressed detailed aspect of RF research for XR. With this survey, we extended and positioned XR-specific RF research topics in the broader RF research field and provide helpful resource for the XR community to navigate within the rapid development of RF research. Index TermsXR, Neural Radiance Fields, 3D Gaussian Splatting, Survey, Systematic Review."
        },
        {
            "title": "1 INTRODUCTION",
            "content": "Machine learning revolutionized how we optimize scene representations for given images. Radiance fields (RF) [20, 38] are noticeable examples that have enabled efficient encoding and rendering of complex scene appearance since 2020. This innovation introduced significant potential for XR applications such as remote collaboration [52], teleoperation [26], and telepresence [50] and could potentially replace the traditional meshes, point clouds, and voxels. Advances in efficient training and rendering of RF [20,41] further expanded the opportunities in XR applications that require high-resolution and real-time rendering. RF is considered to provide novel technical foundation for enhancing immersive video see-through head-mounted displays [79], creating * 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. Ke Li is with University of Hamburg, Germany. E-mail: ke.li@uni-hamburg.de Mana Masuda is with Keio University, Japan. E-mail: mana.smile@keio.jp Susanne Schmidt is with University of Canterbury, New Zealand. E-mail: susanne.schmidt@canterbury.ac.nz Shohei Mori is with University of Stuttgart, Germany. E-mail: s.mori.jp@ieee.org Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication xx xxx. 201x; date of current version xx xxx. 201x. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org. Digital Object Identifier: xx.xxxx/TVCG.201x.xxxxxxx dynamic photorealistic avatars for the metaverse [50], and low-cost generation of digital twins for real-world XR applications [26]. Due to its significance, there has been an exponential growth of RF research in computer vision (CV), computer graphics (CG), robotics, and multimedia (MM) communities since the first introduction of the neural radiance field (NeRF) paper [38]. While many of these technical papers envisioned enormous impacts of RF technology for XR, the actual implementation of RF for XR-focused research and applications remains comparably sparse. For instance, Fig. 1 shows that 203 RFrelated papers were published at the Conference on Computer Vision and Pattern Recognition (CVPR) in 2024, with 68 explicitly mentioning XR as target application domain. However, in the proceedings of leading XR conferences IEEE VR 2024 and IEEE ISMAR 2024 only 11 contributions (including 4 conference papers and 7 extended abstracts) were related to RF. Among these, just 5 directly addressed an XR-related RF research question through technical benchmarks or user studies conducted in actual XR settings. This indicates significant research gap between the communitys initial vision for RF in XR and its current integration into XR research and applications. To better understand this research gap, this paper presents systematic survey on how radiance fields are envisioned for XR and how they have been implemented. While numerous surveys on radiance fields exist across various domains including robotics [74] and 3D computer vision [10] to the best of our knowledge, no dedicated survey has been conducted on radiance fields for XR. Moreover, other existing general radiance field surveys typically only provide broad overview of current research [8], offering limited insights into crucial XR-specific topics such as foveated rendering, haptic rendering, telepresence, 3D interaction, and perception. As result, the XR community currently has limited resources in evaluating the advancement of radiance field technology for XR and assessing its applicability in real-world immerreceiving the results from the renderer in XR middleware (e.g., Unity). 3DGS, as an explicit data representation, offers greater potential for tighter integration by leveraging shader codes for optimal performance. When depth rendering is available, on-screen composition with other frame buffers, such as those used for meshes, is possible. For detailed comparison of 3DGS, NeRF, and textured meshes for XR applications, we refer readers to the supplementary materials. Neural Radiance Fields: NeRF represents continuous 3D scene as parameters of multi-layer perceptron (MLP). The MLP is trained using set of multi-viewpoint images and their camera poses typically obtained from structured from motion (SfM) [57] algorithm  (Fig. 2)  . For any given 3D point and 2D viewing direction, NeRF model infers the corresponding color and density values [38]. Through volume rendering, color and density samples are accumulated per ray to represent complex scene geometry and photometric appearance. While the original NeRF model does have limitations in performance, recent advances in efficient neural network architecture and sampling strategies have significantly improved the performance [41]. We refer NeRF variants to NeRF with different data structures, such as k-planes [9] and instant neural primitives [41], as they share general ideas (i.e., neural implicit scene representation). For further technical details, we refer to previous survey focusing technical aspect of NeRF [10]. 3D Gaussian Splatting: 3DGS [20] is an explicit 3D scene representation that models appearances using set of anisotropic 3D Gaussians. Similarly to NeRF, it uses calibrated camera parameters and also sparse point cloud from SfM to initialize 3D Gaussians  (Fig. 2)  . The method employs rasterization approach by projecting the 3D Gaussians onto 2D, resulting in GPU-friendly differential rendering technique that achieves faster rendering speeds compared to typical NeRF-based implicit 3D scene representations. For further technical details, we refer to previous survey focusing on 3DGS and its optimization [8]."
        },
        {
            "title": "3.1 Data Acquisition: Collecting RF-Mentioned Corpus\nTo ensure coverage of a wide range of RF-related research conferences,\njournals, and venues, we started our search process using one of the\nlargest computer science online databases ACM Digital Library (ACM\nDL) and the IEEE Xplore library (Search date: February 26, 2025).\nThe year range starts from March 2020, when the first NeRF paper is\npublished, and terminates on the search date. Our search queries cover\ntypical terms and definitions of RF-related concepts, their variants, and\nabbreviations in all metadata of the paper entry in the database: “neural\nradiance field” OR “radiance field” OR “3D gaussian splatting” OR\n“gaussian splatting” OR “3DGS” OR “NeRF”. The initial search results\nin a total of 2, 259 items, with 889 from ACM and 1,676 from IEEE.\nIn addition, despite it’s recency and due to its high relevance, we also\ninclude the conference proceedings of IEEE VR 2025, which contains\nan additional 17 contributions.",
            "content": "To ensure the inclusion of only high-quality contributions, we selected core conferences and journals from key RF-related research communities. These include core XR venues: ACM Virtual Reality Software and Technology (VRST) (n = 4), IEEE VR (n = 23), IEEE ISMAR (n = 8), IEEE Transactions on Visualization and Computer Graphics (TVCG) (n = 48),; core CG venues and journals: ACM Transactions on Graphics (TOG) (n = 168), ACM SIGGRAPH and SIGGRAPH Asia (n = 243); core HCI venues: ACM SIGCHI (n = 9) and ACM UIST (n = 6); core robotics venues: the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (n = 59), the IEEE International Conference on Robotics and Automation (ICRA) (n = 43); core CV venues: IEEE/CVF International Conference on Computer Vision (ICCV) (n = 143), IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (n = 419); and multimedia venue: the ACM Multimedia Conference (MM) (n = 132). All selected conferences and journals hold CORE ranking of or A*. Fig. 2: RF pipeline for XR. Through analysis by synthesis, scene model (i.e., NeRF, 3DGS, or their variants) is optimized to match rendered and input views. XR applications utilize the rendering technology for new Interactive Experiences, User-centric Rendering for high-performance stereo rendering, Benchmarking XR systems, extending the concept to Multi-modal I/O, plausible Content Generation, interactive Content Editing, Spatial Registration such as SLAM, extending architecture for Dynamic Capture, or Optimization Techniques for demanding XR scenarios. Different color bars indicate ranges of individual topics over the RF pipeline. sive XR applications. This survey paper aims to bridge such research gaps by investigating the following research questions: RQ1: How are radiance fields envisioned for XR? RQ2: What aspects of radiance fields have been implemented in the current literature, specifically, through technical benchmarks or user studies in actual immersive XR settings? RQ3: What are the remaining research gaps for widespread adaptation of radiance fields in XR? To address these research questions, we conduct systematic survey of key RF-related literature across the CV, CG, robotics, MM, humancomputer interaction (HCI), and XR communities, drawing from the Association for Computing Machinery (ACM) and Institute of Electrical and Electronics Engineers (IEEE) databases. From the initial screening of 1304 papers, we identify 365 as XR-Mentioned papers those that mention XR as potential application among which we labeled 66 as XR-Addressed papers those that contribute to RF research through technical benchmarks or user experiments in immersive XR settings. To answer RQ1, we perform thematic analysis of the XR-Mentioned corpus, identifying nine key themes that reveal how RF is envisioned for XR. For RQ2, we conduct an in-depth literature review of the XR-Addressed papers, analyzing state-of-the-art RF research for XR topics, including foveated rendering, haptic rendering, remote collaboration, and 3D interaction. To address RQ3, we compare the findings from our thematic analysis and literature review to identify key challenges and future research opportunities for RF in XR."
        },
        {
            "title": "2 BACKGROUND\nXR: Many definitions exist to cover different ranges of reality technolo-\ngies, including Virtual Reality (VR) [63], Augmented Reality (AR) [1],\nMixed Reality (MR) [39, 62], eXtended Reality (xR) [35], Diminished\nReality (DR) [40], and Mediated Reality [34]. Each term discusses\nwhich types of realities should be included or excluded, thereby defin-\ning a new domain. To avoid any conflicts and to be inclusive, we use\nthe term XR, where X can represent any kind of reality.\nRadiance Fields: RF is a class of photorealistic scene representations\nthat model the spatial and directional distribution of light in a 3D en-\nvironment. These representations encode the color and density of a\nscene in various forms of data structures, such as neural networks or\n3D Gaussians, enabling realistic view synthesis and high-fidelity 3D\nreconstructions. NeRF and 3DGS are two prominent approaches within\nthis category, differing primarily in their underlying data structures and\nrendering techniques. A common integration strategy involves render-\ning RF into a frame buffer for XR displays (Fig. 2). This keeps the\noriginal renderer untouched by feeding view frustum information to and",
            "content": "For the core XR and core HCI venues, we also included the extended abstract proceedings (poster and research demo) to ensure that emerging research topics in the XR community that might still be in the stage of preliminary work are included. Further, we removed 37 items which are either duplicate results from the ACM DL, or irrelevant results ( for example, using the physical NeRF gun for user studies rather than radiance fields). This results in total of 1, 305 RF-Mentioned papers for further screening and selection."
        },
        {
            "title": "3.2 Screening and Inclusion Criteria: Determine XR-",
            "content": "Mentioned and XR-Addressed Our main goal in the literature screening step is to identify all XRMentioned contributions within the RF-Mentioned corpus. For the initial screening, we employed an automated screening technique using Python script. If paper contains any of the following filtering keywords in the main text (excluding references), the paper is labeled as XR-Mentioned: XR filtering keyword list: virtual reality, augmented reality, metaverse, extended reality, diminished reality, augmented virtuality, mixed reality, immersive, telepresence, holoportation. This initial screening process excluded 846 items, with the remaining 459 papers eligible for further manual screening and selection by the authors of this paper to determine which contributions have specifically addressed an aspect of XR. The main criteria for inclusion in the XR-Addressed categories are whether the contribution has an actual technical benchmark in an actual XR setting (e.g. VR head mounted display (HMD) or interactive AR/MR applications on conventional 2D display setups), whether the contribution shows clear potentials to transfer their results into XR applications, or if user-centric studies were performed involving testing with human subjects in XR. Borderline contributions are highlighted (e.g., contributions that developed 360 rendering techniques but didnt evaluate it using an HMD) and separately discussed by all the authors altogether to determine if they should be included in the XR-Addressed category. This screening step results in the following categorizations: (cid:192) XR-Addressed (n = 66): RF contributions addressing XR-specific aspects of it. They are included for the final in-depth literature review. (cid:17) XR-Envisioned (n = 299): RF contributions mentioned XR keywords only few times, but no validations in an actual XR setting (e.g., rendering in XR settings or performing user-centric evaluations). We extracted the text from each paper that included XR keywords. We included these texts in further thematic analysis to identify the key themes/taxonomy categories in how the community has envisioned RF for XR research and applications. Others (n = 94): Items are further manually filtered and excluded as they only mention RF-related keywords but contain no major contributions to RF (e.g., papers focusing on signed distance field (SDF) rather than NeRF/3DGS, or papers that mentioned immersive referring to video games rather than XR settings). We further categorized (cid:192) papers into the following: XR-Study (n=24): contributions that include an in-depth evaluation with users via XR setups and scenarios (e.g., performance evaluation using an XR headset) or contributions that include technical benchmark using XR-specific metrics whose results are directly transferrable to XR applications (e.g., performance tests of foveated rendering algorithm using pre-collected user eye-gaze data). ﬁ XR-Showcase (n=42): contributions that mostly showcase XR applications in the form of preliminary prototypes or demonstrations. Some of these do not have specific user-involved benchmarking or evaluations in the firm context of XR, but we value these researches, whose findings could easily transfer to actual immersive XR settings."
        },
        {
            "title": "3.3 Iterative Taxonomy Development Process\nBased on the identified XR-envisioned (n=299) and XR-addressed\ncontributions (n=66), a total of 365 contributions were eligible for\nfull-text screening and iterative taxonomy development.",
            "content": "Characteristics and dimension identification: In the first step of the taxonomy development process, each author was assigned set of papers for full-text screening to identify the main contributions. After screening, the authors annotated each paper with 2-3 keywords summarizing the key contributions. Based on these keywords, we grouped the most frequently occurring ones into several main themes while keeping in mind of their relevance to XR while developing the main themes. After several iterative discussions, one of the authors compiled the remaining keywords and themes to formulate the final categorizations and taxonomy for XR. Fig. 2 presents the final taxonomy with different categories of RF research captured within nine main research themes: optimization techniques, dynamic capture, spatial registration, content generation, content editing, multi-modal, benchmarking, user-centric rendering, interactive experiences. The taxonomy is structured so that the themes become increasingly relevant to interactivity and human factors in XR experiences from left to right. Tagging: Using the developed taxonomy, we assigned each of the 365 contributions to one of the nine key RF research themes. For the 299 XR-mentioned papers, each paper was allocated to one of the co-authors for tagging. In cases where contribution could fit into multiple categories, we identified the primary contribution and assigned the paper to the most appropriate category. Edge cases and borderline papers were flagged and discussed collectively among all co-authors in meetings, where tagging decisions were made through voting. For the 66 XR-addressed contributions, two authors independently tagged each paper to ensure coherence and accuracy. In instances where the two authors disagreed, the paper was tabled for discussion among the entire group of co-authors to reach consensus."
        },
        {
            "title": "4 GAPS IN TRENDS",
            "content": "Understanding which research communities contribute to XR-relevant RF development and how their efforts differ between envisioning ((cid:17)) and implementing XR use cases ((cid:192)) is key to identifying gaps, guiding collaborations, and accelerating progress in the field. In this section, we analyze publication trends across communities and reflect on their implications for XR research. Communities and Interests: growing interest in XR is observed in RF research (Fig. 3a). Notably, in 2024, the XR community published more XR-Addressed papers than other communities. The XR and HCI communities are the main contributors to XR-Addressed ((cid:192)), whereas CV, CG, Robotics, and MM communities typically mention XR only as benchmarking or motivation. We observe varied balances in XR-Envisioned ((cid:17)) and XRAddressed ((cid:192)) papers across communities: XR ((cid:17): 26, (cid:192): 32), HCI ((cid:17): 0, (cid:192): 7), CG ((cid:17): 81, (cid:192): 16), CV ((cid:17): 131, (cid:192): 8), Robotics ((cid:17): 22, (cid:192): 2), and MM ((cid:17): 39, (cid:192): 1). While the XR community published marginal number of papers (n = 57), it contributed the most (cid:192)papers, surpassing both CV (n = 139) and CG (n = 97). In contrast, CG, CV, Robotics, and MM communities contribute primarily to (cid:17)papers, which often discuss XR as future application area or aspirational use case. The HCI community publishes only (cid:192)papers, though in limited numbers. These distinctions underscore the need to examine more closely why certain communities contribute fewer system-level or human-in-the-loop XR works, despite frequent mentions of XR potential. Importantly, we caution against interpreting XR mentions as superficial or misleading. Rather, they often reflect future aspirations or alignment with long-term XR goals. Many of these papers propose technically advanced methods and explicitly mention XR as target domain. In this light, non-XR communities may play key role by envisioning how their technical advances can support XR systems despite not tackling full-system integration or evaluation. Fig. 3b shows the overall interest of each community in applying RF to XR across nine topic areas. The XR, CG, and CV communities show interest in all nine topics, whereas HCI, Robotics, and MM focus on fewer areas (three, six, and six topics, respectively). However, only the XR community addresses all nine topics in implementation. CG and CV address six and three topics, respectively, while the HCI community focuses implementation efforts on three. Notably, the Robotics community demonstrates interest in Spatial Registration (n = holder repositories. Among XR-Addressed papers, 34.8% (n = 23 out of 66) released OSS (XR: 25.0% = 8/32, HCI: 14.3% = 1/7, CG: 50.0% = 8/16, CV: 50.0% = 4/8, Robotics: 100.0% = 2/2, MM: 0.0% = 0/1). Among XR-Envisioned papers, 62.5% (n = 112 out of 187) released OSS (XR: 46.2% = 12/26, HCI: no XR-Envisioned papers, CG: 61.7% = 50/81, CV: 76.3% = 100/131, Robotics: 45% = 10/22, MM: 38, 5% = 15/39). These OSS release rates appear to reflect disciplinary norms, institutional IP policies, and authors strategic choices. However, for XR-Addressed research, the relatively low availability of OSS can hinder reproducibility and slow community-wide validation and adoption. Improving OSS availability, particularly for system-level implementations, will be crucial for strengthening the credibility, usability, and future impact of RF research in XR."
        },
        {
            "title": "5.1 Interactive Experiences\nThe interactive experiences theme covers a diverse range of topics\nrelated to interactive XR experiences using RF representations, such\nas collaborative systems, photorealistic 3D virtual humans, and RF-\nenabled industrial and medical applications.",
            "content": "Collaborative systems: Three of the considered papers explore the potential of RF for asymmetric XR collaboration, where local user captures their real-world environment, e.g. with 360 camera [14], mobile phone LiDAR scanner ﬁ [52], or head-mounted RGBD camera [55], and sends the model to remote user. The remote user can then inspect the virtual replica of the local users environment using immersive technology [14] or desktop PC [52, 55]. Both Huang et al. [14] and Reynolds et al. [52] additionally provide the remote user with virtual annotation tools, with the results being embedded in the local users view using AR technology. While the Thing2Reality platform by Hu et al. is also presented in telepresence context, it mainly focuses on the transition of virtual objects between shared 2D and 3D space ﬁ [12]. Using conditioned diffusion models, multiple views of 2D object are generated and used as input to 3DGS to create 3D representation of the pictured object. The RealityGit system by Li et al. highlights version control as particular aspect of collaborative work ﬁ [27]. RealityGit allows remote users to asynchronously visualize and edit the current and historical states of the local NeRF environment and to suggest changes to the local user. Avatars and agents: The largest portion of papers addressing XR for Interactive Experiences investigates the use of RF to create body representations of virtual characters, controlled either by the user (i.e. avatars) or by the computer (i.e. agents). Researched representations range from the full body ﬁ [2, 33, 49] to the upper body ﬁ [69] to the head ﬁ [66, 68] and hands ﬁ [42]. While five out of six papers aim at realistic representation of the human user, Luo et al. propose system for rendering and animating interactive virtual animals ﬁ [33]. At its core, their approach uses differentiable neural representation tailored to model dynamic animals, including their fur, in real time. Industrial and medical applications. Six of the 19 Interactive Experience contributions demonstrate the potential of RF for specific XR application fields. These include robot teleoperation [26, 45], medical visualization ﬁ [89], [23], industrial facility inspection ﬁ [30], and helicopter rescue crew training ﬁ [37]. XR interaction mechanisms: RF-scene and RF-object manipulations in an immersive environment can be performed via proxies. Li et al. implemented set of interactive features for NeRF models in bounded volume, including exocentric manipulation, tunneling effects, and scene appearance editing ﬁ [29]. Jiang et al. achieved physicsbased deformations and dynamics of 3DGS objects by integrating mesh-based physics simulation into 3DGS ﬁ [18]. XR integration considerations: Integration of RF representations into seamless interactive XR experiences faces various challenges, such Fig. 3: Trends in XR-related RF papers. (a) Growth of XR-Addressed papers over the years in the communities (2124). (b) Interest populations in XR-Mentioned papers (2021until IEEE VR 25 conference). The smaller bars indicate XR-Addressed papers. (c) Interest populations over the years (2124). 12), but actual implementations appear only in Interactive Experiences (n = 2). Similarly, the MM community has high number of envisioned papers in Optimization Techniques for XR (n = 39), but only one addresses XR in practice. This asymmetry reflects broader challenge: XR researchers must often rely on external technical developments without direct evidence of those technologies suitability in XR contexts. This reliance complicates the selection and adaptation of RF methods for XR systems. Conversely, implementing and evaluating RF methods within XR contexts can uncover new, system-level insights that are not evident from standalone evaluations. Envisioned vs. Addressed Topics: XR has been envisioned ((cid:17)) across all nine topics: Multi-modal (n = 2), Benchmarking (n = 5), User-centric Rendering (n = 9), Spatial Registration (n = 22), Content Generation (n = 24), Dynamic Capture (n = 43), Interactive Experiences (n = 47), Content Editing (n = 53), and Optimization Techniques (n = 94). XR has also been addressed ((cid:192)) in all nine topics: Content Generation (n = 2), Spatial Registration (n = 2), Multi-modal (n = 3), Dynamic Capture (n = 5), User-centric Rendering (n = 8), Optimization Techniques (n = 8), Benchmarking (n = 8), Content Editing (n = 10), and Interactive Experiences (n = 20). XR-Mentioned and XR-Addressed papers populate different topics  (Fig. 1)  . Envisioned papers frequently frame XR relevance through singlesentence mentions, often placed in the introduction (e.g., Novel view synthesis and real-time rendering from monocular video are critical not only for 3D vision tasks but also for practical applications such as virtual reality and video games, [58]) or conclusion (e.g., This application can be adapted and used in many circumstances of virtual reality and augmented reality, e.g., telepresence and sports broadcasting, [75]). Many use XR as motivating lens to justify further technical performance improvements. Over the last four years, such envisioning has appeared consistently in Optimization Techniques, Interactive Experience, Content Editing, and Dynamic Capture (Fig. 3c). However, actual implementations have emerged in more scattered and less predictable fashion. These differences between envisioned potential and realized implementation raise questions about the bottlenecks researchers face when transitioning from algorithm design to working XR systems. Open Source: To accelerate progress and mitigate replication crises in XR research, open-source software (OSS) is essential [64]. We surveyed available OSS projects associated with XR-Addressed and XR-Envisioned papers as of March 31, 2025, excluding empty or placeTable 1: 24 RF research contributing to XR-Study. Table 2: 42 RF research contributing to ﬁ XR-Showcase. Thematic topic Paper Sub-category Venue OSS Thematic topic Paper Sub-category Venue OSS Interactive experiences User-centric rendering Benchmarking Multi-modal Content editing Content gen. Spatial regist. [55] [14] [45] [26] [23] [4] [54] [60] [78] [7] [11] [70] [21] [48] [87] [51] [24] [56] [3] [85] Collaborative systems Collaborative systems Robot teleoperation Robot teleoperation Medical applications Foveated rendering Foveated rendering Foveated rendering Foveated rendering Foveated rendering Framework Framework User study Pilot testing CHI 24 UIST 24 IROS 24 IROS 24 TVCG 25 TVCG 22 ISMAR 23 TVCG 24 TVCG 24 TVCG VRST 22 SIGA-P 24 ISMAR 24 VRW 25 Haptics rendering UIST 23 Interactive color editing Interactive color editing Scene segmentation ISMAR 24 TVCG 25 VR 25 Context-aware text-to-3D TVCG 25 Localization TVCG 25 Dynamic cap. na na Optimization techniques [22] [86] [82] [71] Compression Compression Compression Rendering na VRST 24 VR 25 MM 24 CVPR 24 y y n n y n y as the interoperability of RF with the conventional render pipeline [28], compatibility with physics-based systems [18], and maintaining real-time performance during dynamic scene manipulation [29]. It is noteworthy that many systems currently only support RF rendering or AR annotation overlays on screen space with synthesized depth images on XR displays for efficiency [45, 52, 55, 89]. Collisions and occlusions are also handled in screen space with rendered depth images [55] or other modalities, such as scene mesh reconstruction [52]. An RF rendering pipeline is typically integrated to XR systems either by (i) converting NeRF to meshes using marching cubes [14], (ii) integrating native render plugins that bridge game engines like Unity with high-performance rendering backends (e.g., instant-ngp [2730], the original CUDA-based 3DGS renderer [26], and custom CUDAbased 3DGS physics simulator [18]) that directly stream rendered outputs to the frame buffers of XR displays, or (iii) using an opensource integration of 3DGS in the Unity game engine with optimized parallel sorting capabilities for fast XR rendering [23]. Applying superresolution algorithms or deep learning super sampling (DLSS) is popular technique for maintaining visual quality and high frame rates [27, 29, 30, 42]. In the supplementary material, we provide detailed summary of XR integration techniques, interaction techniques, realtime performance, and hardware requirements of the surveyed papers in this Interactive Experience category for supporting XR practitioners in designing, implementing, and evaluating interactive RF systems. Hardware and performance: We observe positive trend that RF rendering is becoming increasingly accessible, offering improved realtime performance and resolution with reduced computational demands. For example, Tran et al. demonstrated NeRF-based VR telepresence system that achieved 30 fps stereoscopic rendering, albeit at 512 512 pixel resolution, and required two NVIDIA RTX 6000 GPUs for per-eye rendering [67]. Afterwards, Tu et al. presented 3DGS telepresence 2,048 pixels on an autostereoscopic system that runs at 30 fps at 2,048 display with an NVIDIA RTX 4090 GPU [69]. Cao et al. demonstrated 3DGS immersive telepresence system of holoported patients at 400 fps at 1080p with an NVIDIA RTX 6000 GPU [2]. Challenges: i) In interactable RF, the relationship between scene objects might be dynamically changed by the user, which creates challenges regarding real-time lighting effects [2,33,42] as well as exposure Interactive experiences Collaborative systems [27] Collaborative systems [52] Collaborative systems [12] [2] Avatars and agents [49] Avatars and agents [33] Avatars and agents [68] Avatars and agents [69] Avatars and agents [66] Avatars and agents [42] Avatars and agents [30] [37] [89] Medical applications [29] XR interaction mechanisms [18] XR interaction mechanisms Industrial applications Industrial applications User-centric rendering [31] Omnidireaction [13] Omnidireaction [84] Immersive light field Benchmarking [28] [80] [61] [46] Toolkit Framework Pilot testing Pilot testing Multi-modal [83] Audio [19] Haptics rendering ISMAR-A 23 CHI-EA 24 UIST-A 24 TVCG 25 SIG 22 TOG 22 TOG 23 SIG 24 TOG 24 CVPR 23 VRW 24 SIG-P 24 ISMAR 24 CHI-EA 23 SIG VRW 22 SIGA-TC 22 TVCG 23 VRW 23 SIGA 23 VRW 24 VRW 25 VRW 25 SIG-IP 24 Content editing [81] Mesh insertion [47] Mesh insertion [73] Diminished reality [59] [36] [77] [67] TVCG 24 ICCV 23 VRW 25 Interactive editing techniques VRW 25 VRW 25 Scene segmentation CVPR 23 Scene disentanglement CVPR 24 Scene disentanglement Content gen. [72] Speech-to-3D UIST-A 24 Spatial regist. [44] Camera params. refinement TOG 23 Dynamic cap. Optimization techniques [25] [88] [5] [16] [32] [6] [53] [17] [76] Performance capture Performance capture Performance capture Performance capture Facial capture Capture improvement Rendering Compression Compression VRW 25 TOG 22 TOG 23 TOG 24 TOG 21 VRW 25 CVPR 23 CVPR 24 CVPR 24 y y y y n n n y y y y of formerly occluded regions [2, 33, 55]. ii) Several of the considered interactive experiences rely on manual pre-processing steps, such as pre-defining skeletal rig and capturing human motions [33, 49] or setting object parameters [18]. iii) Interactive modifications of RF, e.g., through reenactment or ad-hoc object duplication, are computationally expensive, therefore limiting the use for standalone XR [23, 66] or creating tradeoff between rendering speed and visual fidelity [14, 55]. Opportunities: i) Design and implement practical interactions that go beyond basic functionalities, such as changing the point of view or moving the scene [23, 33, 37, 55] (e.g., further investigations of simulating efficient physics-based interactions). ii) Use of machine learning (e.g., large vision models) to monitor the plausibility of the RF scene after interactive changes (e.g., generated dynamics) [18]. iii) Despite being the most directly applicable to XR, large proportion of research in this category remains limited to XR-showcase, lacking both robust implementations and in-depth user studies. This gap presents significant opportunities for future work. iv) Integrate multi-modality in interactions (e.g. haptic and textile feedback), as described in Section 5.4. v) Further investigations of applying hybrid representations (meshes + RFs) for supporting physics-based interactions, dynamic lighting, and real-time collision handling [18]."
        },
        {
            "title": "5.2 User-centric Rendering",
            "content": "Contributions within user-centric rendering primarily focus on foveated rendering [4, 7, 54, 60, 78] and the generation of omnidirectional images or videos [13, 24, 31], aiming to render scenes based on human visual attention and perceptual relevance. Sampling regulation leveraging human visual acuity: key challenge in RF rendering, especially for scene representations requiring intensive neural network queries, is achieving high rendering speed for stereoscopic, high-resolution, and wide field-of-view displays on highend HMD [4]. As result, early RF research within the XR community has focused on developing algorithms that leverage human visual perception characteristics to accelerate rendering, typically, via regulating sampling rates while maintaining comparable perceived rendering quality [4, 54, 60, 78]. Deng et al., first applied foveated rendering to NeRF in immersive VR settings. Their FoV-NeRF framework represents radiance fields using concentric spherical coordinates and employs multi-resolution image synthesis to exploit the fovea-peripheral characteristics of human vision [60]. To optimize stereoscopic rendering, they introduced method that applies disparity shift to peripheral images rather than generating separate images for both eyes. The introduction of instant-ngp [41] marked significant advancement in terms of NeRF rendering speed by utilizing hash encoding data structure to parallelize and accelerate neural network queries during ray casting [41]. Building on this, Rolff et al. extended instant-ngp [41] by incorporating variable rate shading (VRS) techniques, which further accelerated NeRF rendering by merging pixel blocks based on foveaperipheral characteristics and saliency. In similar line of work, Shi et al. designed multi-ellipsoidal neural scene representation that retrains the network based on depth-based saliency to enhance rendering efficiency [60], extending the FoV-NeRF framework. Likewise, Wang et al. proposed VPRF, NeRF variant that regulates the sampling rate along each ray through sampling weight-constrained training process, utilizing contrast-based saliency maps [78]. Fan et al. integrated foveated rendering into dynamic 3GDS pipeline by using Gaussian forest representation, which separates dynamic and static components and selectively activates, deforms, and renders Gaussians based on visual acuity [7]. Bringing large-scale RF scenes to XR with outward-facing camera setup: The original NeRF is typically reconstructed using perspective cameras with an inward-facing setup, which is optimized for accurately rendering well-centered object. However, many immersive XR applications require building 3D environments with large-scale, unbounded scenes. In such cases, the traditional NeRF training pipeline often results in low-quality rendering and could cause unpleasant viewing experiences [84]. To address this limitation, the CG and XR communities have begun investigating extensions of NeRF for omnidirectional rendering [13, 24, 31] and immersive light fields ﬁ [84]. For example, Li et al. introduced an approach to address uneven ray sampling in omnidirectional positional encoding by utilizing Fibonacci sphere model which results in improved rendering quality ﬁ [31]. Huang et al. proposed method to reconstruct omnidirectional RF from 360degree image sequences using geometry-aware RF network, enabling real-time 360-degree rendering of large indoor scenes ﬁ [13]. For reconstructing large-scale, unbounded outdoor scenes, Yu et al. proposed hybrid approach that combines background and foreground RF with different spatial mapping strategies for immersive light fields rendering based on RF representations ﬁ [84]. Challenges: i) One major challenge in foveated rendering for NeRF lies in the use of multi-resolution image synthesis, which requires retraining multiple networks and inefficient storage consumption, further complicating the RF generation process [4]. ii) Saliency-based usercentric rendering faces difficulties in generating accurate saliency maps in real time, particularly with complex or dynamic scenes [7, 54, 78, 78]. iii) The development of omnidirectional NeRF is still in its early stages, requiring more efficient representations and rendering methods to manage large-scale, full-sphere visual data [13, 31]. iv) All user-centric rendering methods currently encounter aliasing effects and rendering artifacts that could negatively impact user experiences [84]. Opportunities: i) Investigate more storage-efficient data structures and representations for training and RF generation, particularly when incorporating multi-modal saliency maps or synthesizing multi-image outputs. ii) Extend user-centric rendering methods to dynamic scenes, for instance, by predicting video saliency to accommodate motion and changing viewpoints. iii) Move beyond psychophysical experiments or simple demonstrations (e.g., 360 images) to evaluate user-centric rendering methods in real-world interactive XR environments."
        },
        {
            "title": "5.3 Benchmarking",
            "content": "The benchmarking theme category refers to contributions that extend existing methods with known techniques to achieve novel functionalities or insights into the RF representations, for example, by developing related toolkits to support accessibility [11, 28, 70, 80] or performing user centric evaluation or pilot testing of XR integrations [21,46,48,61]. Frameworks and toolkits: One important aspect of the benchmarking category includes contributions that extend existing RF methods by creating accessible toolkits or frameworks to facilitate easier usage of the RF techniques in XR. These contributions focus on building systems for RF capturing [80], post-processing [70], remote streaming [11], and efficient multi-GPU rendering [70, 80], utilizing known optimization techniques. Hiroi et al. introduced NeARPortation, system that enables real-time remote stereoscopic rendering of NeRF on an AR display. The system achieves full HD rendering at 35-40 frames per second (fps) by leveraging bidirectional client-server communication [11]. Li et al. developed immersive-ngp, an open-source Unity toolkit that tackled the interoperability challenges of neural network-based RF models. It allows for rendering and visualizing instant-ngp [41] models in immersive VR environments by implementing native C++ and CUDA plugin ﬁ [28]. Xu et al. presented VR-NeRF, an open-source framework designed for capturing and rendering high-fidelity NeRF environments specifically for immersive VR experiences. VR-NeRF utilizes perceptual color space for accurate HDR appearance and an efficient mip-mapping mechanism for level-ofdetail rendering with anti-aliasing. It also further extends and enhances instant-ngp, achieving dual 2K HDR rendering at 36 Hz by utilizing multi-GPU rendering setup ﬁ [80]. Recently, Tu et al. introduced fast and robust 3DGS rendering pipeline optimized for VR. This system employs mini-splatting techniques, incorporates stop-the-pop (STP) solution, introduces optimal projection to mitigate ghosting effects, and integrates foveated rendering to enhance rendering speed [70]. The resulting pipeline delivers frame rates comparable to the original 3DGS implementation [20] with improved visual quality and user experiences [70]. User evaluation and pilot testing: Another line of work within the benchmarking category involves contributions that perform human-inthe-loop evaluations of photorealistic RF representations in XR. Kim et al. conducted user study to investigate how different 3D visualization methods, including 3DGS, image-to-3D, and video playback, affect recognition memory and user experience. The study demonstrated that, in VR, 3DGS enables better object recognition memory [21]. Qiu et al. introduced pilot usability evaluation using the System Usability Scale (SUS) to compare the usability of mesh, 3DGS, and panoramic images for VR object creation. Their results revealed that 3DGS achieved superior SUS score compared to meshed reconstruction and paranomic 3D scenes [48]. Additionally, there are several initial efforts focused on pilot testing the basic features of RFbased photorealistic scene representations for VR content creation [48] ﬁ [61], as well as exploring artistic applications such as creating stop-motion animation in VR ﬁ [46]. Metrics: Benchmarking RF in XR experiences often aims to strike balance between frame rates (in frames per seconds, fps), latency (in milliseconds, ms), resolution (in the number of pixels, px, or pixel per degree (PPD) [4, 30]), and render quality (e.g., Structural Similarity Index Measure, SSIM, Peak Signal-to-Noise Ratio, PSNR, and Learned Perceptual Image Patch Similarity, LPIPS). For remote streaming, it is also crucial to minimize round trip latency (RTL) to below 20 ms [11]. Evaluating foveated rendering algorithms for RF also employs psychophysical experiments to statistically test participants subjective awareness of the peripheral degradation [4, 7, 54]. As rendering artifacts could be amplified in immersive settings, causing discomfort or motion sickness, human-centric metrics such as the Simulator Sickness Questionnaire (SSQ) and SUS are used to holistically evaluate user experience [48]. Challenges: i) There is lack of modular frameworks for benchmarking and exploring different variants of RF techniques in XR similar to Nerfstudio for the XR community to catch up with the rapid development in this research topic [65]. ii) Neural network-based RF models still face compatibility challenges with traditional graphics pipelines, resulting in limited interactive and interoperation capabilities [28]. iii) For many RF variants, achieving high quality rendering still requires complex hardware setups, such as multi-GPU configurations for rendering [11, 80] or camera rigs for high quality capturing, making existing iv) RTL below 20 framework less accessible for wider adaptation. ms is desirable to minimize perceived negative effects from network latency when using remote neural rendering, which could be difficult to achieve depending on network situations [11]. v) Existing human centric benchmarking efforts is still preliminary, focusing on simple exploratory testing and initial usability evaluation [21, 46, 48]. Opportunities: i) Develop standardized frameworks to evaluate different variants of RF methods, similar to existing tools like NeRFStudio [65], but focused on integrating popular XR platforms such as Unity and OpenXR for better accessibility for the XR community. ii) Standardize neural rendering models for game engines by compressing them into efficient formats (e.g., an ONNX model). iii) Enhance single-shot capturing techniques and optimize RF rendering on mobile and low-resource devices for better accessibility in RF generation. iv) Implement head and gaze motion prediction for remote neural rendering frameworks to mitigate perceived latency. v) Conduct more in-depth user-centric benchmarks to provide deeper design insights for XR, including detailed comparison of textured mesh, NeRF, and 3DGS in XR and measuring perceived rendering quality, sense of presence, perceived cybersickness, etc."
        },
        {
            "title": "5.4 Multi-modal",
            "content": "Multimodality in RF for XR refers to extending the rendering capabilities to additional sensory modalities, such as haptics [19, 87] and audio [83]. Current RF research primarily focus on visual realism, often neglecting other sensory dimensions. To enable truly immersive XR experiences, it is essential to develop 3D scene representations that not only achieve high visual fidelity but also provide congruence with other types of sensory feedback. As revealed in Tables 1 and 2, research in this category remains sparse. However, some pioneering contributions have begun to explore multi-modal RF. For instance, Zhang et al. developed the first 3 degrees of freedom (3DOF) haptic rendering method for NeRF, leveraging stochastic haptic rendering technique to approximate and smoothen the underlying geometric information [87]. This approach demonstrates promising performance in mitigating the noisy collision response from implicit 3D representations [87]. Additionally, Jiao et al. demonstrated an early prototype enabling interaction with free-viewpoint video augmented by haptic feedback, based on hybrid NeRF variant that integrates explicit geometric structures with implicit visual appearance representations ﬁ [19]. Interestingly, Yoshida et.al recently demonstrated the feasibility of extending 3DGS to novel-view acoustic synthesis by treating each pixel in the spectrogram as point, and, similar to 3DGS, use spherical harmonics to represent view-dependent audio magnitude for continuous spatial audio representations ﬁ [83]. Challenges: i) The lack of precise geometric information in NeRF, especially in regions reconstructed from sparse viewpoints or network hallucinations, can lead to inaccurate or very noisy haptics collision responses [87]. ii) Haptic devices require high refresh rates to deliver realistic feedback [87], placing additional computational demands on the already expensive NeRF-based rendering process. iii) Integrating multi-modal datasuch as visual, audio, and haptic informationinto unified RF representation could introduce challenges in terms of storage efficiency, synchronization, and optimization workflows. Opportunities: i) Further extending RF rendering to additional multi-sensory modalities such as olfaction, thermal, and gustatory feedback, potentially by leveraging the efficiency of positional encoding to create continuous, high-fidelity representations from sparse data. ii) Incorporate material and textile-level feedback to enable more realistic haptics experiences. iii) Investigate haptics rendering with 3GDS, an area currently still unexplored, for example, by converting point based splats to distance fields [87] or develop hybrid representations."
        },
        {
            "title": "5.5 Content Editing\nContent editing in RF for XR includes contributions that modify RF\nrepresentations, including appearance adjustments (e.g., recoloring,\nstyle transfer, and relighting) and scene manipulations (e.g., object\ninsertion/removal for augmented/diminished reality).",
            "content": "Object insertion and removal: Aiming to combine the strengths of RF and polygonal surface meshes, both ﬁQiao et al. [47] and ﬁYe et al. [81] propose hybrid approaches that embed meshes into NeRF. Both approaches estimate light sources in the NeRF scene and simulate realistic light transport from the NeRF to the embedded mesh and vice versa. Qiao et al. additionally present physics simulator that utilizes the SDF of the NeRF to detect and resolve collisions between the NeRF and meshes, including cloth, rigid and soft bodies [47]. Waldow et al. pursues the opposite goal of removing objects from an RF in real time, i.e. implementing diminished reality ﬁ [73]. The approach relies on Gaussian splats that are generated on prior state of the scene and have to be manually aligned with the current real-world footage. Segmentation and decomposition of RF scenes: Another line of research aims at building RF scene understanding and can precede further editing steps such as object insertion or color adjustment. Schieber et al. developed and empirically evaluated method for semantics-controlled Gaussian splatting that segments the scene into classes [56]. Mao et al. prompt an LLM to analyze the physical properties of each scene object, thereby supporting subsequent physics-based interactions ﬁ [36]. Focusing on user interaction, Shen et al. showcase GaussianShopVR, platform that leverages VR interface for 3DGS editing ﬁ [59]. Instead of segmenting an RF scene into different objects, Wang et al. disentangle the geometry and materials of NeRF from lighting effects ﬁ [77]. This approach supports relighting, e.g. when meshes are inserted. Tran et al. propose technique for head reenactment that uses the appearance from source image and the facial expressions from driver video ﬁ [67]. Their proposed technique converts both the source and driver data into shared 3D volumetric representation based on tri-planes. Interactive color editing: Recently, Kou et al. extended omnidirectional NeRF to dynamic 360-degree videos, focusing on interactive content editing capabilities, such as recoloring [24]. The method of Ren et al. also supports interactive color adjustment, both at the object and scene level [51]. They propose three modes of interaction, namely manual, text-driven and image-driven, the latter being an example of style transfer to RF. Challenges: i) Current techniques for embedding mesh-based objects into RF scenes are limited in their ability to incorporate advanced lighting effects, such as indirect lighting, and to render shadows without artifacts [47, 81]. ii) Two of the considered papers on scene segmentation and decomposition explicitly point out that they are currently limited to static scenes [56, 77]. iii) Manual preparatory steps are required for several of the discussed approaches, such as pre-recording the scene for live object removal [73], pre-defining labels for scene segmentation into classes [56], and designing priors for geometry-lighting disentanglement [77]. Opportunities: i) Although being explicitly developed for XR, only four of the ten Content Editing papers demonstrate their work on an immersive platform, such as VR HMD [24, 36, 56, 59]. New interaction patterns for RF editing could emerge from the targeted use of such platform, for example, through novel multimodal inputs in XR (e.g.combining hand gestures, gaze, and speech)."
        },
        {
            "title": "5.6 Content Generation\nContent generation for RF in XR includes topics such as 3D scene\nauthoring using generative AI models and is popular among the CG\nand CV community. As introduced in Section 4, while there are 28",
            "content": "contributions explicitly envisioned the applications of RF-based content generation to XR, only 2 have applied their methods in actual XR settings, indicating significant research gap. Nonetheless, exsiting work already started to investigate deploying RF-based text-to-3D AI models in VR [72] and context aware generation of NeRF in VR environment with consistent appearance [3]. Vachha et.al presented Dreamcrafter, framework that integrates generative AI algorithms for RF-based speech-to-3D content generation ﬁ [72]. They also introduced utilizing proxy models (e.g. mesh) to compensate for the large generation latency [72]. Recently, Dai et.al introduced Go-NeRF, novel framework for generating NeRF within an already established VR scenes based on text, user defined region, and the visual context of the scenes surrounding. Go-NeRF distills 2D diffusion prior to 3D, thereby, creating content that could adhere to scene conditions (e.g. reflection and shawdow) [3]. Challenges: i) Consistency between generated content and existing environments is essential for maintaining users sense of immersion and presence in XR. This requires advances in realistic relighting, context-aware rendering, and geometry-consistent scene integration. ii) Current RF-based text-to-3D generation methods have high latency, limiting their applicability in many real-time interactive XR scenarios. iii) Existing implementations have yet to achieve the level of physical realism needed for convincing blending for many MR experiences [3]. Opportunities: i). Further investigation in techniques for realistic inpainting and blending of heterogeneous 3D representations (e.g. seamless blending of photorealitisc RF model with meshes with abstract visualization in augmented virtuality experiences). ii). Develop intuitive authoring interfaces for RF-based content creation that minimize cognitive load, which requires further investigation in methods for reducing response latency and improving accuracy. iii) Conduct usability studies comparing the cognitive demands of different 3D content authoring methods (e.g. RF-based generation vs. traditional modeling methods). iv) Expand beyond controlled test datasets [3] to real-world, immersive MR environments to investigate the practical applicability of existing methods."
        },
        {
            "title": "5.7 Spatial Registration",
            "content": "Spatial registration in RF is crucial for XR applications, as it enables real-time camera pose localization and the optimization of camera parameters. CamP by Park et al. [44] improves initial camera parameters (e.g., from mobile device) by preconditioning the optimization in the NeRF training pipeline. SplatLoc by Zhai et al. ﬁ [85] leverages 3D Gaussian primitives to compress 3D scene models and introduces salient 3D landmark selection algorithm to select suitable subset of primitives for efficient camera localization. Challenges & Opportunities: Similar to conventional discussions, key challenges remain in achieving robust, accurate, and energyefficient registration in XR settings. This is especially true for mobile and head-worn systems operating in dynamic environments with limited compute budgets. Efficient scene representation, robust initialization and relocalization, and tight integration with real-time rendering pipelines are central to meeting the latency and visual coherence requirements of immersive applications."
        },
        {
            "title": "5.8 Dynamic Capture",
            "content": "Capturing dynamic objects or humans is indispensable for representing users as photorealistic volumetric avatars in telepresence systems or rendering playback videos in XR applications. The key to achieving real-time rendering of dynamic human motion is an efficient RF representation, such as compression. Lombardi et al. ﬁ [32] combine the advantages of volumetric and primitive-based approaches while minimizing computation in empty regions of space. Zhao et al. ﬁ [88] achieve neural surface reconstruction in minutes by bridging the traditional animated mesh workflow with RF rendering techniques. Jiang et al. ﬁ [16] propose DualGS, which can compress human motion to integrate RF into XR environments seamlessly. Krause et al. ﬁ [25] develop data storage and loading methods to achieve playback rates suitable for interactive VR experiences. For another stream of research, Dong et al. ﬁ [5] present generalizable method to handle unseen performers without fine-tuning, using very sparse RGBD live streams. Challenges & Opportunities: While many RF-based methods for dynamic capture have been proposed in the CG community, their direct applications evaluated in XR settings remain limited. In fact, all contributions are categorized as ﬁ XR-showcases. This is due to the high computational cost associated with avatar rendering. Moreover, existing approaches have not yet achieved the level of real-time high-fidelity rendering needed for compelling XR experiences. Future work should focus on human-involved studies to determine whether reconstructed faces, bodies, and expressions in motion still fall within the uncanny valley for XR users."
        },
        {
            "title": "5.9 Optimization Techniques",
            "content": "Unlike the benchmarking category, which focuses on building toolkits or framework to extend existing RF methods using known optimization techniques, substantial amount of RF research in the CG, CV, MM, and robotics communities explores novel approaches to RF optimization. Optimization provides the foundation to RFs practicality in XR applications, and, as result, this area encapsulate wide range of research topics, including handling sparse input data, reconstructing large-scale scenes, improving capture quality [6], accelerating rendering speed [53, 76, 86], and developing compression techniques [17, 22, 86]. However, despite total of 94 XR-envisioned contributions in this category, only 8 are XR-addressed, highlighting significant gap between the envisioned application and real-world implementation. Nonetheless, exsting work already investigated in topics such as rendering on resource constrained devices (standalone XR headsets) [53, 76], compression for photorealistic performance streaming and remote rendering [17, 22, 86], and capturing improvement [6]. Rendering on resource constrained devices: Rojas et al. introduced Re-ReND, real-time NeRF rendering framework for VR headsets that achieves 72 FPS by converting NeRF representations into hybrid format: meshes, which are compatible with traditional graphics pipelines, and light fields, which could output pixel colors through single-query ﬁ [53]. In another line of work, Wang et al. proposed VideoRF, which enables 4D human performance rendering on VR headsets via remote streaming. Their method is based on serialized 2D feature image stream, demonstrating the feasibility of real-time 4D NeRF playback in immersive settings ﬁ [76]. Compression: Compression is major challenge for 3DGS due to its high parameter storage requirements, especially for applications like 4D performance capturing and storage. To tackle this challenge, Jiang et al. proposed HiFi4G, compact 4DGS representation for real-time human performance rendering ﬁ [17]. Their method introduces dual-graph mechanismutilizing both fine-grain and coarse graphs to initialize Gaussians more efficiently [17]. Kim et al. presented superpixel-based initialization method for 3DGS, which extracts superpixels from keyframe images for sparse sampling, reducing the number of Gaussians [22]. They also conducted extensive benchmarks on memory consumption and evaluated the performance on VR devices. More recently, Zhang et al. introduced SRBF-Gaussian, viewportdependent color encoding technique using Spherical Radial Basis Functions (SRBFs) to enable selective transmission of viewport relevant color-data for more efficient 3DGS streaming and remote rendering [86]. Compression is also crucial for NeRF-based volumetric video streaming in telepresence and holoportation systems [41]. Yin et al. proposed adaptive feature grids, extending instant-ngp [41], to enable level-of-detail (LoD) streaming via dynamic feature selection during raymarching. They evaluated its applicability for efficient volumetric video playback on VR devices [82]. Capture Improvement: Dongye et al. demonstrated the feasibility of human-in-the-loop 3DGS capture by enabling users to teleoperate robotic arm in VR, such that viewpoints with limited viusal quality could be manually inspected and updated on demand ﬁ [6]. This approach also demonstrates the potential for continuous and adaptive 3DGS updates, which could further enhance 3DGSs application in remote collaboration systems [27, 55]. Challenges: i) Optimization and training speed remain bottlenecks for real-time applications such as dynamic SLAM, spatial registration, and live capture and reconstruction [17]. ii) 3D/4DGS still relies on fast sorting operations, which limits scalability as the number of Gaussian iii) Approaches based on representation conversion increases [17]. (e.g., NeRF to mesh) could depend on accurate geometric information, presenting barriers in rendering fine-grained geometry [53]. iv) There is speed-memory trade-off in optimization; faster rendering typically requires storing precomputed priors, increasing memory demands [76]. v) Accurate user behavior prediction for remote rendering remains an open challenge. Opportunities: i) Continue exploring novel representations to further extend the possibilities of optimal photorealistic scene representations. For example, 3DGS could be enhanced by incorporating scene-aware/semantic-aware reconstruction. ii) The current optimization framework primarily focuses on accelerating rendering speed and compression. However, real-world XR implementations may require further investigation into optimizing performance during complex immersive interactions, such as physics-based interactions [18] and haptic rendering [87]. iii) Many optimization techniques had already been developed prior to the milestones of 3DGS and instant-ngp ( e.g. hybridNeRF [71]), making them valuable resources for inspiring future optimization strategies in other RF variants. iv) Transfer and adapt the vast amount of available optimization techniques developed within other communities (CG, CV, and robotics) into actual XR setting. This could be achieved by encouraging more open source system & toolkit contributions for RF in XR as mentioned in Section 5.3. v) Continuous investigation in effective approaches for porting RF on mobile devices in terms of rendering speed and memory consumption."
        },
        {
            "title": "6 DISCUSSION\nLimitations: Our survey has several limitations. i) We focused on\nICCV and CVPR to capture the current developments and research\ntrends within the CV community. We could not include all major CV\nconferences, such as the European Conference on Computer Vision\n(ECCV), due to the large volume of papers. Moreover, we did not in-\nclude poster and workshop proceedings from CVPR and ICCV, as they\nare highly diverse and challenging to cover comprehensively within the\nscope of this survey. As a result, some emerging research trends from\nthe CV community may not be fully represented. ii) Given the vast\nnumber of papers, we focused on contributions related to XR directly\nor closely. As a result, some promising works had to be excluded,\nsuch as 360-degree scene representation contributions which had not\nextensively envisioned XR as core applications. Moreover, to main-\ntain a structured and consistent presentation, we classified the papers\naccording to the taxonomy and theme we developed. However, this\ncategorization required assigning each paper to a single primary cat-\negory based on its main contributions, even when some works could\nreasonably belong to multiple categories. iii) We did not pursue a deep\nanalysis of author identities. For example, we did not examine the over-\nlap of authors between XR-Addressed and XR-Envisioned papers to\ninvestigate whether the same researchers contributed to both categories.\nAn analysis in this direction could yield valuable insights.",
            "content": "Ethical considerations: The ability of RF to synthesize photorealistic scenes, approaching level of realism nearly indistinguishable from the real world, raises issues related to misinformation, identity misuse, and intellectual property (IP) violations. In immersive AR/MR environments, RF-generated content poses unique risks: users may struggle to distinguish between real and virtual elements. At the same time, there is limited work on mechanisms like watermarking to protect creators IP within RF-based representations. Among all the XR-related papers we screened in full text, only one has focused on ethical-related topics in detail in embedding watermarks in NeRF [15]. This highlights significant research gap in the intersection of RF and XR, one that demands urgent attention given the rapid pace of RF development and its integration into immersive systems."
        },
        {
            "title": "7 CONCLUSION\nIn conclusion, we introduced the first survey paper dedicated to explor-\ning the intersection of RF techniques and XR. Through a systematic",
            "content": "review, we identified key research gaps between how RF has been envisioned and how it has been implemented, particularly in areas such as content generation, spatial registration, optimization, and emerging topics like content editing and multimodal interaction (e.g. haptics). We also point out the limited focus on ethical considerations despite the increasing realism and accessibility of RF-based content. We believe this work will serve as an important resource for the XR community in understanding, navigating, and advancing the integration of RF technologies in XR. ACKNOWLEDGMENTS This work was supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germanys Excellence Strategy EXC 2120/1 390831618, the European Unions Horizon Europe research and innovation program under grant agreement No 101135025, PRESENCE project, and the MSCA Staff Exchanges project PLACES (grant agreement No 101086206). Early drafts of this submission were rephrased for clarity and grammatical correctness using OpenAIs GPT-4o model and Microsoft Copilot. REFERENCES [1] R. T. Azuma. survey of augmented reality. Presence: Teleoperators & Virtual Environments, 6(4):355385, 1997. 2 [2] S. Cao, J. Zhao, F. Hu, and Y. Gan. Real-time, free-viewpoint holographic patient rendering for telerehabilitation via single camera: data-driven approach with 3D Gaussian splatting for real-world adaptation. IEEE Trans. on Visualization and Computer Graphics (TVCG), 2025. 4, 5 [3] P. Dai, F. Tan, X. Yu, Y. Peng, Y. Zhang, and X. Qi. GO-NeRF: Generating objects in neural radiance fields for virtual reality content creation. IEEE Trans. on Visualization and Computer Graphics (TVCG), 2025. 5, 8 [4] N. Deng, Z. He, J. Ye, B. Duinkharjav, P. Chakravarthula, X. Yang, and Q. Sun. FoV-NeRF: Foveated neural radiance fields for virtual reality. IEEE Trans. on Visualization and Computer Graphics (TVCG), 28:3854 3864, 2021. 5, 6 [5] Z. Dong, K. Xu, Y. Gao, Q. Sun, H. Bao, W. Xu, and R. W. Lau. SAILOR: Synergizing radiance and occupancy fields for live human performance capture. ACM Trans. on Graphics (TOG), 42(6):2051, 2023. 5, 8 [6] X. Dongye, H. Guo, Y. Bao, H. Jiang, and D. Weng. Human-in-the-loop Gaussian model enhancement with mobile robotic re-capture. In Proc. IEEE Conf. on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW), pp. 000000, 2025. 5, [7] R. Fan, J. Wu, X. Shi, L. Zhao, Q. Ma, and L. Wang. Fov-gs: Foveated 3d gaussian splatting for dynamic scenes. IEEE Transactions on Visualization and Computer Graphics, 31:29752985, 2025. 5, 6 [8] B. Fei, J. Xu, R. Zhang, Q. Zhou, W. Yang, and Y. He. 3D Gaussian splatting as new era: survey. IEEE Trans. on Visualization and Computer Graphics (TVCG), pp. 120, 2024. Early Access. 1, 2 [9] S. Fridovich-Keil, G. Meanti, F. Warburg, B. Recht, and A. Kanazawa. K-Planes: Explicit radiance fields in space, time, and appearance. In Proc. IEEE/CVF Computer Vision and Pattern Recognition (CVPR), pp. 1247912488, 2023. 2 [10] K. Gao, Y. Gao, H. He, D. Lu, L. Xu, and J. Li. NeRF: Neural radiance field in 3D vision, comprehensive review, 2022. 1, 2 [11] Y. Hiroi, Y. Itoh, and J. Rekimoto. NeARportation: remote real-time In Proc. ACM Symp. on Virtual Reality neural rendering framework. Software and Technology (VRST), 2022. 5, 6, 7 [12] E. Hu, M. Li, X. Qian, A. Olwal, D. Kim, S. Heo, and R. Du. Experiencing Thing2Reality: Transforming 2D content into conditioned multiviews and 3D Gaussian objects for XR communication. In Adjunct Proc. ACM Symp. on User Interface Software and Technology (UIST-Adjunct(), pp. 13, 2024. 4, 5 [13] H. Huang, Y. Chen, T. Zhang, and S.-K. Yeung. Real-time omnidirectional roaming in large scale indoor scenes. In ACM SIGGRAPH Asia Tech. Comm., article no. 20, 5 pages, 2022. 5, 6 [14] X. Huang, M. Yin, Z. Xia, and R. Xiao. VirtualNexus: Enhancing 360degree video AR/VR collaboration with environment cutouts and virtual replicas. In Proc. ACM Symp. on Virtual Reality Software and Technology (VRST), pp. 112, 2024. 4, 5 [15] Y. Jang, D. I. Lee, M. Jang, J. W. Kim, F. Yang, and S. Kim. Waterf: Robust watermarks in radiance fields for protection of copyrights. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1208712097, 2024. 9 [16] Y. Jiang, Z. Shen, Y. Hong, C. Guo, Y. Wu, Y. Zhang, J. Yu, and L. Xu. Robust dual Gaussian splatting for immersive human-centric volumetric videos. ACM Trans. on Graphics (TOG), 43(6):115, 2024. 5, [17] Y. Jiang, Z. Shen, P. Wang, Z. Su, Y. Hong, Y. Zhang, J. Yu, and L. Xu. HiFi4G: High-fidelity human performance rendering via compact Gaussian splatting. In Proc. IEEE/CVF Computer Vision and Pattern Recognition (CVPR), pp. 1973419745, 2024. 5, 8, 9 [18] Y. Jiang, C. Yu, T. Xie, X. Li, Y. Feng, H. Wang, M. Li, H. Lau, F. Gao, Y. Yang, et al. VR-GS: physical dynamics-aware interactive Gaussian splatting system in virtual reality. In Proc. ACM SIGGRAPH Conference Papers, number 78, p. 1, 2024. 4, 5, 9 [19] S. Jiao, D. Zhao, Y. Chen, S. Wang, C. Xie, B. Zhu, A. Israr, and L. Zhang. Intractable live free-viewpoint video with haptic feedback. In ACM SIGGRAPH Immersive Pavilion, 2024. 5, 7 [20] B. Kerbl, G. Kopanas, T. Leimkuehler, and G. Drettakis. 3D Gaussian splatting for real-time radiance field rendering. ACM Trans. on Graphics (TOG), 42:1 14, 2023. 1, 2, 6 [21] H. Kim and I.-K. Lee. Is 3DGS useful?: Comparing the effectiveness of recent reconstruction methods in VR. In Proc. IEEE Int. Symp. on Mixed and Augmented Reality (ISMAR), pp. 7180, 2024. 5, 6, 7 [22] M. G. Kim, S. Jeong, S. Park, and J. Han. Superpixel-guided sampling for compact 3D Gaussian splatting. In Proc. ACM Symp. on Virtual Reality Software and Technology (VRST), pp. 115, 2024. 5, [23] C. Kleinbeck, H. Schieber, K. Engel, R. Gutjahr, and D. Roth. Multi-layer Gaussian splatting for immersive anatomy visualization. IEEE Trans. on Visualization and Computer Graphics (TVCG), 2025. 4, 5 [24] S. Kou, F.-L. Zhang, J. Nazarenus, R. Koch, and N. A. Dodgson. OmniPlane: recolorable representation for dynamic scenes in omnidirectional videos. IEEE Trans. on Visualization and Computer Graphics (TVCG), pp. 114, 2025. Early Access. 5, 6, 7 [25] K. L. Krause, W. Morgenstern, A. Hilsmann, and P. Eisert. Realtimerendering of dynamic scenes with neural radiance fields. In Proc. IEEE Conf. on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW), 2025. 5, 8 [26] K. Li, R. Bacher, S. Schmidt, W. Leemans, and F. Steinicke. Reality Fusion: Robust real-time immersive mobile robot teleoperation with volumetric visual data fusion. In Proc. IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), pp. 89828989, 2024. 1, 4, 5 [27] K. Li, T. Rolff, R. Bacher, and F. Steinicke. RealityGit: Cross reality version control of r&d optical workbench. In Proc. Int. Symp. on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct), pp. 807808, 2023. 4, 5, 8 [28] K. Li, T. Rolff, S. Schmidt, R. Bacher, S. Frintrop, W. Leemans, and F. Steinicke. Bringing instant neural graphics primitives to immersive In Proc. IEEE Conf. on Virtual Reality and 3D User virtual reality. Interfaces Abstracts and Workshops (VRW), pp. 739740, 2023. 5, 6, [29] K. Li, T. Rolff, S. Schmidt, R. Bacher, W. Leemans, and F. Steinicke. Interacting with neural radiance fields in immersive virtual reality. In Extended Abstracts of CHI Conf. on Human Factors in Computing Systems (CHI-EA), pp. 14, 2023. 4, 5 [30] K. Li, S. Schmidt, T. Rolff, R. Bacher, W. Leemans, and F. Steinicke. Interactive data fusion of neural radiance fields for industrial facility inspection in virtual reality. In Proc. IEEE Conf. on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW), pp. 815816, 2024. 4, 5, 6 [31] Q. Li, I. Ueda, C. Xie, H. Shishido, and I. Kitahara. Omnidirectional neural radiance field for immersive experience. In Proc. IEEE Conf. on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW), pp. 740741, 2022. 5, 6 [32] S. Lombardi, T. Simon, G. Schwartz, M. Zollhoefer, Y. Sheikh, and J. Saragih. Mixture of volumetric primitives for efficient neural rendering. ACM Trans. on Graphics (TOG), 40(4):113, 2021. 5, 8 [33] H. Luo, T. Xu, Y. Jiang, C. Zhou, Q. Qiu, Y. Zhang, W. Yang, L. Xu, and J. Yu. Artemis: Articulated neural pets with appearance and motion synthesis. ACM Trans. on Graphics (TOG), 41(4):119, 2022. 4, 5 [34] S. Mann. Mediated reality. Technical Report TR 260, M.I.T. Media Lab, Perceptual Computing Section, Cambridge, MA, 1994. [35] S. Mann, T. Furness, Y. Yuan, J. Iorio, and Z. Wang. All reality: Virtual, augmented, mixed (x), mediated (x, y), and multimediated reality, 2018. arXiv preprint arXiv:1804.08386. 2 [36] H. Mao, Z. Xu, S. Wei, Y. Quan, N. Deng, and X. Yang. LIVE-GS: LLM powers interactive VR by enhancing Gaussian splatting. In Proc. IEEE Conf. on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW), 2025. 5, 7 [37] J. McGhee, C. Bourke, R. Lawther, H. Zhou, and R. Petersen. 3DCrewCap: Applying 3D volumetric video capture for XR helicopter rescue crew training and simulation. In ACM SIGGRAPH Posters, pp. 12, 2024. 4, 5 [38] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In Proc. Eur. Conf. on Computer Vision (ECCV), 2020. 1, 2 [39] P. Milgram and F. Kishino. taxonomy of mixed reality visual displays. IEICE Trans. on Information and Systems, 77(12):13211329, 1994. [40] S. Mori, S. Ikeda, and H. Saito. survey of diminished reality: Techniques for visually concealing, eliminating, and seeing through real objects. IPSJ Trans. on Computer Vision and Applications, 9:114, 2017. 2 [41] T. Müller, A. Evans, C. Schied, and A. Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM Trans. on Graphics (TOG), 41:1 15, 2022. 1, 2, 6, 8 [42] A. Mundra, J. Wang, M. Habermann, C. Theobalt, M. Elgharib, et al. LiveHand: Real-time and photorealistic neural hand rendering. In Proc. Int. Conf. on Computer Vision (ICCV), pp. 1803518045, 2023. 4, 5 [43] M. J. Page, J. E. McKenzie, P. M. M. Bossuyt, I. Boutron, T. C. Hoffmann, C. D. Mulrow, L. Shamseer, J. M. Tetzlaff, E. A. Akl, S. E. Brennan, R. Chou, J. M. Glanville, J. M. Grimshaw, A. Hrõbjartsson, M. M. Lalu, T. Li, E. W. Loder, E. Mayo-Wilson, S. McDonald, L. A. McGuinness, L. A. Stewart, J. Thomas, A. C. Tricco, V. A. Welch, P. F. Whiting, and D. Moher. The PRISMA 2020 statement: An updated guideline for reporting systematic reviews. Systematic Reviews, 10, 2020. 2 [44] K. Park, P. Henzler, B. Mildenhall, J. T. Barron, and R. Martin-Brualla. CamP: Camera preconditioning for neural radiance fields. ACM Trans. on Graphics (TOG), 42(6):111, 2023. 5, 8 [45] V. Patil and M. Hutter. Radiance fields for robotic teleoperation. In Proc. IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), pp. 1386113868. IEEE, 2024. 4, [46] F. Proksa, J. Salhofer, V. Ohner, P. Wenninger, S. Nebel, and M. Husinsky. Artistic exploration of pixilated choreography in virtual reality: Exploring 3D Gaussian splatting for stop-motion animation. In Proc. IEEE Conf. on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW), 2025. 5, 6, 7 [47] Y.-L. Qiao, A. Gao, Y. Xu, Y. Feng, J.-B. Huang, and M. C. Lin. Dynamic mesh-aware radiance fields. In Proc. Int. Conf. on Computer Vision (ICCV), pp. 385396, 2023. 5, 7 [48] S. Qiu, B. Xie, Q. Liu, and P.-A. Heng. Creating virtual environments with 3D Gaussian splatting: comparative study. In Proc. IEEE Conf. on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW), 2025. 5, 6, 7 [49] E. Remelli, T. Bagautdinov, S. Saito, C. Wu, T. Simon, S.-E. Wei, K. Guo, Z. Cao, F. Prada, J. Saragih, et al. Drivable volumetric avatars using texel-aligned features. In Proc. ACM SIGGRAPH Conference Papers, pp. 19, 2022. 4, 5 [50] E. Remelli, T. Bagautdinov, S. Saito, C. Wu, T. Simon, S.-E. Wei, K. Guo, Z. Cao, F. Prada, J. Saragih, and Y. Sheikh. Drivable volumetric avatars using texel-aligned features. In Proc. ACM SIGGRAPH Conference Papers, article no. 56, 9 pages, 2022. 1 [51] C. Ren, H. Qiu, Y. Shao, Z. Qiu, and K. Song. PaletteGaussian: 3D photorealistic color editing with Gaussian splatting. In Proc. IEEE Int. Symp. on Mixed and Augmented Reality (ISMAR), pp. 12061215, 2024. 5, [52] B. Reynolds, J. Hobin, Y. Cao, M. Klaudiny, D. A. Dangond, K. Xie, K. Theodosiou, R. De Leeuw, S. Kashalkar, and V. Heun. The pop-up metaverse: multi-user, multi-tasking spatial computing environment for collaborative spatial problem-solving. In Extended Abstracts of CHI Conf. on Human Factors in Computing Systems (CHI-EA), pp. 15, 2024. 1, 4, 5 [53] S. Rojas, J. Zarzar, J. C. Pérez, A. Sanakoyeu, A. Thabet, A. Pumarola, and B. Ghanem. Re-ReND: Real-time rendering of nerfs across devices. In Proc. Int. Conf. on Computer Vision (ICCV), pp. 36323641, 2023. 5, 8, 9 [54] T. Rolff, S. Schmidt, K. Li, F. Steinicke, and S. Frintrop. VRS-NeRF: Accelerating neural radiance field rendering with variable rate shading. In Proc. IEEE Int. Symp. on Mixed and Augmented Reality (ISMAR), pp. 243252, 2023. 5, 6 [55] M. Sakashita, B. Thoravi Kumaravel, N. Marquardt, and A. D. Wilson. SharedNeRF: Leveraging photorealistic and view-dependent rendering for real-time and remote collaboration. In Proc. ACM CHI Conf. on Human Factors in Computing Systems (CHI), pp. 114, 2024. 4, 5, 8 [56] H. Schieber, J. Young, T. Langlotz, S. Zollmann, and D. Roth. Semanticscontrolled Gaussian splatting for outdoor scene reconstruction and rendering in virtual reality. In Proc. IEEE Conf. on Virtual Reality and 3D User Interfaces (VR), pp. 318328, 2025. 5, 7 [57] J. L. Schönberger and J.-M. Frahm. Structure-from-motion revisited. In Proc. IEEE/CVF Computer Vision and Pattern Recognition (CVPR), pp. 41044113, 2016. W. Chen, and S. Fidler. Neural fields meet explicit geometric representations for inverse rendering of urban scenes. In Proc. IEEE/CVF Computer Vision and Pattern Recognition (CVPR), pp. 83708380, 2023. 5, 7 [78] Z. Wang, J. Wu, R. Fan, W. Ke, and L. Wang. VPRF: Visual perceptual radiance fields for foveated image synthesis. IEEE Trans. on Visualization and Computer Graphics (TVCG), 30:71837192, 2024. 5, 6 [58] M. Shao, Y. Qiao, K. Zhang, and L. Meng. Frequency-aware uncertainty Gaussian splatting for dynamic scene reconstruction. IEEE Trans. on Visualization and Computer Graphics (TVCG), 2025. 4 [79] L. Xiao, S. Nouri, J. Hegland, A. G. Garcia, and D. Lanman. NeuralPassthrough: Learned real-time view synthesis for VR. In Proc. ACM SIGGRAPH Conference Papers, article no. 40, 9 pages, 2022. 1 [59] Y. Shen, B. Li, J. Huang, and Z. Wang. GaussianShopVR: Facilitating immersive 3D authoring using Gaussian splatting in VR. In Proc. IEEE Conf. on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW), 2025. 5, 7 [80] L. Xu, V. Agrawal, W. Laney, T. Garcia, A. Bansal, C. Kim, S. R. Bulò, L. Porzi, P. Kontschieder, A. Bozic, D. Lin, M. Zollhöfer, and C. Richardt. VR-NeRF: High-fidelity virtualized walkable spaces. In Proc. ACM SIGGRAPH Asia Conference Papers, 2023. 5, 6, 7 [60] X. Shi, L. Wang, X. Liu, J. Wu, and Z. Shao. Scene-aware foveated neural radiance fields. IEEE Trans. on Visualization and Computer Graphics (TVCG), 2024. Early Access. 5, [81] K. Ye, H. Wu, X. Tong, and K. Zhou. real-time method for inserting virtual objects into neural radiance fields. IEEE Trans. on Visualization and Computer Graphics (TVCG), pp. 112, 2024. 5, 7 [82] D. Yin, J. Shi, M. Zhang, Z. Huang, J. Liu, and F. Dong. FSVFG: Towards immersive full-scene volumetric video streaming with adaptive feature grid. In Proc. ACM International Conference on Multimedia, pp. 11089 11098, 2024. 5, 8 [83] M. Yoshida, R. Togo, T. Ogawa, and M. Haseyama. Extending Gaussian splatting to audio: Optimizing audio points for novel-view acoustic synthesis. In Proc. IEEE Conf. on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW), 2025. 5, 7 [84] X. Yu, H. Wang, Y. Han, L. Yang, T. Yu, and Q. Dai. ImmersiveNeRF: Hybrid radiance fields for unbounded immersive light field reconstruction. IEEE Trans. on Visualization and Computer Graphics (TVCG), PP, 2023. 5, 6 [85] H. Zhai, X. Zhang, B. Zhao, H. Li, Y. He, Z. Cui, H. Bao, and G. Zhang. SplatLoc: 3D Gaussian splatting-based visual localization for augmented reality. IEEE Trans. on Visualization and Computer Graphics (TVCG), 2025. 5, 8 [86] D. Zhang, Z. Liang, Z. Cao, D. Wang, and F. Wang. SRBF-Gaussian: In Proc. IEEE Conf. on Streaming-optimized 3D Gaussian splatting. Virtual Reality and 3D User Interfaces (VR), pp. 461471, 2025. 5, 8 [87] H. Zhang, L. Zhu, Y. Xiang, J. Zheng, and A. Song. Haptic rendering of neural radiance fields. In Proc. ACM Symp. on User Interface Software and Technology (UIST), 2023. 5, 7, 9 [88] F. Zhao, Y. Jiang, K. Yao, J. Zhang, L. Wang, H. Dai, Y. Zhong, Y. Zhang, M. Wu, L. Xu, et al. Human performance modeling and rendering via neural animated mesh. ACM Trans. on Graphics (TOG), 41(6):117, 2022. 5, 8 [89] X. Zou, Z. Zhang, A. Schwarz, M. Armand, and A. Martin-Gomez. ARthroNeRF: Field of view enhancement of arthroscopic surgeries using augmented reality and neural radiance fields. In Proc. IEEE Int. Symp. on Mixed and Augmented Reality (ISMAR), pp. 11971205, 2024. 4, 5 [61] A. Sijan and P. Willemsen. Exploring radiance field content generation for virtual reality. In Proc. IEEE Conf. on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW), pp. 697698, 2024. 5, 6 [62] R. Skarbez, M. Smith, and M. C. Whitton. Revisiting milgram and kishinos reality-virtuality continuum. Frontiers in Virtual Reality, 2:647997, 2021. 2 [63] I. E. Sutherland et al. The ultimate display. In Proc. the IFIP Congress, vol. 2, pp. 506508. New York, 1965. [64] J. E. Swan II. The replication crisis, reproducibility, and the reproducibility project in psychology, 2023. ISMAR Workshop on Replication in Extended Reality (WoRXR) - Tutorial. 4 [65] M. Tancik, E. Weber, E. Ng, R. Li, B. Yi, J. Kerr, T. Wang, A. Kristoffersen, J. Austin, K. Salahi, A. Ahuja, D. McAllister, and A. Kanazawa. Nerfstudio: modular framework for neural radiance field development. In Proc. ACM SIGGRAPH Conference Papers, 2023. 7 [66] P. Tran, E. Zakharov, L.-N. Ho, L. Hu, A. Karmanov, A. Agarwal, M. Goldwhite, A. B. Venegas, A. T. Tran, and H. Li. VOODOO XP: Expressive one-shot head reenactment for VR telepresence. ACM Trans. on Graphics (TOG), 43(6):126, 2024. 4, 5 [67] P. Tran, E. Zakharov, L.-N. Ho, A. T. Tran, L. Hu, and H. Li. VOODOO 3D: Volumetric portrait disentanglement for one-shot 3D head reenactment. In Proc. IEEE/CVF Computer Vision and Pattern Recognition (CVPR), pp. 1033610348, 2024. 5, 7 [68] A. Trevithick, M. Chan, M. Stengel, E. Chan, C. Liu, Z. Yu, S. Khamis, M. Chandraker, R. Ramamoorthi, and K. Nagano. Real-time radiance fields for single-image portrait view synthesis. ACM Trans. on Graphics (TOG), 42(4):115, 2023. 4, 5 [69] H. Tu, R. Shao, X. Dong, S. Zheng, H. Zhang, L. Chen, M. Wang, W. Li, S. Ma, S. Zhang, et al. Tele-Aloha: telepresence system with lowbudget and high-authenticity using sparse RGB cameras. In Proc. ACM SIGGRAPH Conference Papers, pp. 112, 2024. 4, [70] X. Tu, B. Kerbl, and F. D. la Torre. Fast and robust 3D Gaussian splatting for virtual reality. In ACM SIGGRAPH Asia Posters, 2024. 5, 6 [71] H. Turki, V. Agrawal, S. R. Bulò, L. Porzi, P. Kontschieder, D. Ramanan, M. Zollhöfer, and C. Richardt. HybridNeRF: Efficient neural rendering via adaptive volumetric surfaces. In Proc. IEEE/CVF Computer Vision and Pattern Recognition (CVPR), pp. 1964719656, 2024. 5, 9 [72] C. Vachha, Y. Kang, Z. Dive, A. Chidambaram, A. Gupta, E. Jun, and B. Hartmann. Dreamcrafter: Immersive editing of 3D radiance fields through flexible, generative inputs and outputs. In Adjunct Proc. ACM Symp. on User Interface Software and Technology (UIST-Adjunct(), pp. 13, 2024. 5, 8 [73] K. Waldow, J. Scholz, and A. Fuhrmann. DimSplat: real-time diminished reality system for revisiting environments using Gaussian splats in mobile WebXR. In Proc. IEEE Conf. on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW), 2025. 5, 7 [74] G. Wang, L. Pan, S. Peng, S. Liu, C. Xu, Y. Miao, W. Zhan, M. Tomizuka, M. Pollefeys, and H. Wang. NeRF in robotics: survey, 2024. 1 [75] K. Wang, S. Peng, X. Zhou, J. Yang, and G. Zhang. Nerfcap: Human performance capture with dynamic neural radiance fields. IEEE Trans. on Visualization and Computer Graphics (TVCG), 29(12):50975110, 2022. [76] L. Wang, K. Yao, C. Guo, Z. Zhang, Q. Hu, J. Yu, L. Xu, and M. Wu. VideoRF: Rendering dynamic radiance fields as 2D feature video streams. In Proc. IEEE/CVF Computer Vision and Pattern Recognition (CVPR), pp. 470481, 2024. 5, 8, 9 [77] Z. Wang, T. Shen, J. Gao, S. Huang, J. Munkberg, J. Hasselgren, Z. Gojcic,"
        }
    ],
    "affiliations": [
        "Keio University, Japan",
        "University of Canterbury, New Zealand",
        "University of Hamburg, Germany",
        "University of Stuttgart, Germany"
    ]
}