{
    "paper_title": "BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning",
    "authors": [
        "Beichen Zhang",
        "Yuhong Liu",
        "Xiaoyi Dong",
        "Yuhang Zang",
        "Pan Zhang",
        "Haodong Duan",
        "Yuhang Cao",
        "Dahua Lin",
        "Jiaqi Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Cutting-edge large language models (LLMs) demonstrate promising performance in solving complex math problems with a divide-and-conquer pipeline and the assistance of in-context learning (ICL) examples. However, their potential for improvement is limited by two critical problems within their ICL examples: granularity-mismatch and the ensuing negative-effect noise problem. Specifically, the LLMs are capable of the dividing process yet mostly failed by inaccurate reasoning within a few conquer steps, while the ICL examples retrieved in question-grained sometimes lack relevant steps for a specific challenging reasoning step. Further, this disconnect may hinder the correct reasoning due to its irrelevance. To this end, we focus on improving the reasoning quality within each step and present BoostStep. BoostStep aligns the granularity between the retrieving and reasoning on step grained, and provides highly related ICL examples for each reasoning step with a novel `first-try' strategy. BoostStep provides more relevant examples than the coarse question-grained strategy, enhancing the model reasoning quality within each step steadily. BoostStep is a general and robust reasoning-enhancing method that not only improves standalone reasoning performance but also integrates seamlessly with Monte Carlo Tree Search methods (MCTS) to refine both candidate generation and decision-making. Quantitatively, it improves GPT-4o and Qwen2.5-Math-72B by 3.6\\% and 2.0\\% respectively on various mathematical benchmarks, and 7.5\\% gain combined with MCTS."
        },
        {
            "title": "Start",
            "content": "BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning Beichen Zhang1,2, Yuhong Liu1,2, Xiaoyi Dong1,3, Yuhang Zang1, Pan Zhang1, Haodong Duan1, Yuhang Cao1, Dahua Lin1,3, Jiaqi Wang1 1Shanghai AI Laboratory 2Shanghai Jiao Tong University 3The Chinese University of Hong Kong https://github.com/beichenzbc/BoostStep 5 2 0 J 6 ] . [ 1 6 2 2 3 0 . 1 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Cutting-edge large language models (LLMs) demonstrate promising performance in solving complex math problems with divideand-conquer pipeline and the assistance of incontext learning (ICL) examples. However, their potential for improvement is limited by two critical problems within their ICL examples: granularity-mismatch and the ensuing negative-effect noise problem. Specifically, the LLMs are capable of the dividing process yet mostly failed by inaccurate reasoning within few conquer steps, while the ICL examples retrieved in question-grained sometimes lack relevant steps for specific challenging reasoning step. Further, this disconnect may hinder the correct reasoning due to its irrelevance. To this end, we focus on improving the reasoning quality within each step and present BoostStep. BoostStep aligns the granularity between the retrieving and reasoning on step grained, and provides highly related ICL examples for each reasoning step with novel first-try strategy. BoostStep provides more relevant examples than the coarse question-grained strategy, enhancing the model reasoning quality within each step steadily. BoostStep is general and robust reasoning-enhancing method that not only improves standalone reasoning performance but also integrates seamlessly with Monte Carlo Tree Search methods (MCTS) to refine both candidate generation and decisionmaking. Quantitatively, it improves GPT-4o and Qwen2.5-Math-72B by 3.6% and 2.0% respectively on various mathematical benchmarks, and 7.5% gain combined with MCTS."
        },
        {
            "title": "Introduction",
            "content": "Mathematical reasoning is crucial and challenging task in the development of artificial intelligence. It serves as an indicator of models ability to perform complex reasoning and has wide range * indicates equal contribution indicates corresponding author 1 Figure 1: Our step-level in-context learning outperforms traditional problem-level few-shot learning for about 4% across in-domain, out-domain and cross-modality mathematical benchmark. Moreover, on benchmarks with lower similarity with example problem set (i.e. OlympiadBench and multi-modal benchmarks), where problem-level few-shot learning may have negative impact, while our strategy still provides valuable guidance. of applications, such as problem solving, theorem proving and scientific discovery. When solving complex mathematical problems, two capabilities are necessary, which are divide and conquer. The former decomposes the complex problem into several simpler steps to simplify the task, while the latter guarantees the correctness of the one-step reasoning. Through the analysis of error cases, we found that the current SOTA models have been relatively clear in the divide part, that is, the model can basically know exactly what tasks should be completed in each step. However, there will still be lot of mistakes in the conquer section, such as wrong formula use, wrong calculation, insufficient enumeration, etc. To quantitatively substantiate this conclusion, we provide GPT-4o-mini with groundtruth reasoning process to determine whether the error in another response were due to an overarching flawed reasoning approach or deviation within particular step. In less advanced models like LLaMA-3.1-8B-Instruct (Dubey et al., 2024), 8.7% of errors originates from an overall flawed reasoning approach. While in more advanced models like GPT-4o, only 0.8% of errors are ascribable to the overall approach, with the majority resulting from inaccuracies at individual steps of the reasoning process. Therefore, it can be concluded that the correctness of one-step reasoning is the bottleneck of reasoning capability. Various approaches have been employed to improve reasoning correctness, such as producing chains of thought through prompt engineering (Kojima et al., 2022; Wei et al., 2022), fine-tuning with mathematical data (Shao et al., 2024; Yang et al., 2024; Ying et al., 2024), or generating multiple candidate reasoning paths using Monte Carlo Tree Search (MCTS) (Zhang et al., 2024b,a). Among those techniques, in-context learning is particularly important one, which offers similar examples to provide detailed guidance. However, the examples retrieved by traditional problem-level in-context learning are listed before the reasoning process ever begins, thereby lacking fine-grained guidance during the reasoning process. Moreover, since the example problem cant be exactly identical to the new one, the irrelevant steps in those examples may even become distraction from the current reasoning, thus even negatively affecting the conquer capability for some specific steps. To this end, we refine in-context learning from problem-level to step-level granularity to offer similar example steps during an on-going reasoning process for fine-grained step-level guidance, as well as ensuring that the introduced example is still highly relevant at the step level to avoid distractions. Firstly, we have constructed an example problem bank with step-level granularity based on reasoning content, instead of commonly adopted grammatical separation. This ensures the steps in the question bank are consistent with the actual reasoning steps, thereby providing more appropriate guidance. Building on the step-level granularity within the example problem bank, we propose an approach that incorporates in-context learning through \"first-try\" format during an on-going reasoning process. Specifically, for given problem to be solved, we break down the solving process into step-bystep reasoning paths. During the reasoning of single step, we first allow the model to attempt \"first try\" to comprehend what the model currently needs to reason about. Based on this initial attempt, we search the question bank to find similar steps that can guide the model to accurately output the current step. This first-try strategy helps ensure higher similarity between the retrieved examples and the current step. Therefore, the distraction from irrelevant steps can be avoided and and the guidance effect is improved. Compared with traditional in-context learning in problem-level granularity, our method is capable of providing examples during the reasoning process directly based on the steps to be solved, thereby offering more relevant guidance. Our approach demonstrates significant improvements over traditional few-shot learning across various open-source benchmarks, with an average increase of 4%. Moreover, our method also reduces the dependency on the similarity between the example dataset and the target problem since two entirely different problems can still share similar key steps, such as utilizing prime factorization for both calculating the least common multiple and determining divisibility. Consequently, dissimilar problems can still offer effective guidance. For example, on multi-modal benchmarks with lower similarity to the example problem bank like MathVision (Wang et al., 2024), traditional few-shot learning can even have negative effects. In contrast, our approach continues to achieve significant improvements. Our method significantly enhances single-step reasoning accuracy through step-level in-context learning, making it highly compatible with various current reasoning strategies that employ Monte Carlo Tree Search (MCTS). Typically, an MCTS method requires reasoning model to generate multiple step-wise candidate reasoning paths and critic model to evaluate the correctness of these candidates. Our approach can be integrated into both aspects in plug-and-play manner. Specifically, when generating new candidate reasoning nodes, our method can introduce similar examples in the aforementioned first-try manner to improve the accuracy of candidates. Additionally, it can aid the critic model by incorporating similar correct reasoning steps into the evaluation of candidate reasoning processes. Experimental results indicate that both applications contribute positively to the models reasoning capabilities during MCTS process. Detailed illustration will be shown in Sec. 3."
        },
        {
            "title": "2 Related Works",
            "content": "Mathematical Reasoning. Mathematical reasoning has long been highly challenging task in the field of artificial intelligence. In the early days of artificial intelligence, constrained by lack of general capabilities, early methods (Feigenbaum et al., 1963; Fletcher, 1985) primarily attempted to perform simple mathematical reasoning through rulebased methods. With the advent of large language models with enhanced reasoning capabilities, contemporary approaches typically focus on enhancing performance during both the training and inference phases. The first category improves mathematical capability by fine-tuning with more high-quality mathematical data (Shao et al., 2024; Yang et al., 2024; Lewkowycz et al., 2022; Yue et al., 2023; Xu et al., 2024). This strategy can fundamentally improve the base models mathematical capabilities. However, it demands substantial high-quality mathematical data and computational resources. Consequently, more efforts have been put in exploring various techniques during inference to enhance mathematical reasoning performance. Some work (Wei et al., 2022; Kojima et al., 2022) involves prompt engineering to enable models to generate comprehensive chains of thought. Other studies (Madaan et al., 2024; Gou et al., 2023; Ke et al., 2024) involve using self-refinement techniques to revise the initial reasoning outputs. Stepwise Mathematical Reasoning. Recently, to further enhance mathematical reasoning capabilities, many studies have shifted the granularity of mathematical reasoning from the problem level to the step level. This approach involves addressing each next step individually, completing small segments of reasoning within the overall task. These works often employ strategies such as Tree of Thoughts (ToT) (Yao et al., 2024) or Monte Carlo Tree Search (Zhang et al., 2024b,a; Chen et al., 2024; Feng et al., 2023; Zhu et al., 2022), extending multiple steps to optimize step answers and ultimately obtain the optimal solution. Additionally, Process Supervision Models (PRMs) (Lightman et al., 2023; Luo et al., 2024) are frequently used to verify the correctness of new candidate nodes in real-time and prune reasoning paths, thereby improving the accuracy of the final answer. This more detailed auxiliary strategy demonstrates greater potential. In-context Learning in Mathematical Reasoning. In-context learning can provide low-cost guidance to models through similar examples, thereby enhancing the quality of model outputs and their ability to follow prompts. Consequently, it has been widely adopted. However, research on incontext learning within mathematical reasoning tasks remains insufficient. Typically, this approach involves providing the model with similar problems and their ground truth solutions to offer general strategy for solving new problems (Hendrycks et al., 2021; Wei et al., 2022). Some efforts have been made to improve the relevance of retrieved examples by designing better retrieval mechanisms and incorporating appropriate reference rejection techniques (Liu et al., 2024b). Others try to provide high-level context instead to improve the generalizability (Wu et al., 2024). However, all these methods share common limitation: the lack of fine-grained real-time guidance at the step level."
        },
        {
            "title": "Conditional Probability",
            "content": "Current models often employ next-token prediction for training and inference, where the conditional probability is central to the models generation of the next token. Given problem q, models reasoning process can be represented by rpredict = arg max Pmodel(r q), where we train the model to get better conditional probability Pmodel so that rpredict can be closer to the groundtruth answer rgt = arg max Pgt(r q). In-context learning provides the model with conditional probabilities similar to the groundtruth answer for imitation without changing the probability model Pmodel. Specifically, an example problem and its corresponding correct solution is provided and it can be posited that the conditional probability (r q) is similar to the probability of the grountruth answer of the target problem (rgt q). Consequently, the model will imitate this similar example and Pmodel(r q, q, r) will be closer to rgt comparing to rpredict. However, given that the actual reasoning process can be highly complex, the complete reasoning process is often divided into multiple steps s1, s2, . . .. Step-level reasoning iteratively guides the model to generate the next step s0shot = Pmodel(s q, s1, s2, . . . , si). arg max predict = arg max i+1 At the step granularity, examples retrieved based on the problem are evidently insufficient for 3 Figure 2: Our strategy refines in-context learning from problem-level granularity (fig.a) to step-level granularity(fig.b) to provide more real-time fine-grained guidance. Moreover, our strategy can guide the reasoning and verifying process in Monte Carlo Tree Search (MCTS) strategies by introducing examples. providing appropriate guidance. Similar problem may not necessarily contain the corresponding steps to guide the reasoning for the new problem q. Moreover, irrelevant steps may provide dissimilar conditional probabilities, thereby distracting the models reasoning process. To this end, we propose step-level in-context learning and first-try strategy to provide detailed and relevant example step when in step-level reasoning. Specifically, when generating new steps si+1 based on previous reasoning steps si, si1, . . . , s1 and question q, we first utilize first-try strategy to obtain an approximate estimate of sf irst i+1 . Then, we use this sf irst to retrieve similar step n+1 along with the corresponding q, 2, . . . , n. Since these two steps are similar, very reasonable assumption is that (s n) closely approximates (sgti+1 Therefore, the generated step si+1 = arg max Pmodel(s 1, q) will be more q, s1, . . . , si, n+1, closed to sgti+1 comparing to s0shot . Details about our step-level in-context learning and first-try 1, . . . , q, s1, . . . , si). n+1 q, n, . . . , 1, i+1 i+1 Figure 3: An example of different problems may contain similar steps. Problem-level in-context learning will ignore this example due to low problem similarity, while our step-level in-context learning can introduce the core skills by step-level retrieval and guidance. strategy will be explained in Sec. 3.3 3.2 Step-Level Example Problem Bank Due to the need for further improvement in mathematical capabilities, currently open-source mathematical data no longer consist solely of problems and their final answers to determine whether the fi4 nal answer obtained is correct or not. Instead, they also provide detailed solution processes to provide more fine-grained measurement. However, most current open-source mathematical data still do not break down the solution processes to the step level. Some approaches (Lightman et al., 2023) proposed using the period . as clear semantic delimiter to segment steps for training PRM. This strategy allows for the quick decomposition of each step from complete process without any additional assistance. However, this simple decomposition mode is obviously unreliable. Essentially, single reasoning step should have consistent target and encompass complete thought process, making it the atomic granularity of reasoning. Using period . as delimiter may disrupt this atomicity. For example, it may split complete enumerations for the same objective into multiple steps. Therefore, we suggest that the most appropriate method for step segmentation is to allow the reasoning model itself to autonomously decompose the process. This approach ensures that the granularity of the decomposed steps in example problem bank aligns with that of the real-time reasoning steps. Specifically, we define the concept of step through prompts, which encapsulate complete and simple inference. This guides GPT-4o in decomposing the answer at the step level. major advantage of decomposing the question example bank into individual steps is that it facilitates step-level retrieval and guidance, which is of significant importance. As illustrated in Fig. 3, two distinctly different problems may contain similar key steps. Traditional problem-level in-context learning often overlooks such examples, whereas step-level in-context learning can effectively recall these steps, thereby providing fine-grained guidance to the ongoing reasoning process. The proposed step-level example problem bank is available at https://github.com/ beichenzbc/BoostStep 3.3 Step-Level ICL with First-try Strategy The core challenge of in-context learning lies in how to effectively retrieve relevant problems or steps for effective guidance. This is contingent upon both the similarity between the problem database and the target problem, as well as the specific retrieval strategy employed. Traditional problem-level in-context learning involves retrieving similar problems based solely on the problem statement. This approach is relatively straightforward but effective, as similar problems typically encompass similar reasoning processes. At the more granular step level, however, the situation becomes much more complex. simple strategy is to perform retrieval using the given problem and all preceding reasoning steps si1, si2, . . . , s1, q. The clear drawback of this method is the excessive length of the retrieval content, which diminishes the emphasis on the uniqueness of the current step. Another strategy is to use the previous step si1 to retrieve j1 from steplevel database, thereby guiding the reasoning of si through the correct resolution of j. However, this approach is rather crude, as it models stepwise reasoning as Markov process, which is evidently unreasonable. Similar steps can be applicable to different reasoning tasks, and therefore similarity in the previous step does not necessarily indicate that the retrieved subsequent step will provide valuable guidance for the reasoning in the current step. To this end, we propose straightforward and effective \"first-try\" strategy to enhance the similarity of search steps. Our premise is that the most accurate way to estimate the next step is to actually allow the model to attempt the reasoning for the next step. Specifically, given problem and all preceding reasoning steps si1, si2, . . . , s1, we first instruct the model to attempt continuing the reasoning process to arrive at tentative step stry without the aid of any examples. Subsequently, we use stry to retrieve similar steps along with their corresponding problem and preceding steps 1, . . . , j1 from step-level database. Finally, we feed the retrieved similar steps back to the model, enabling it to deduce the final step si. Besides, we add widely-accepted strategy reference rejection. Specifically, If the similarity of the retrieved most similar example remains below certain threshold, we consider that there are no sufficiently similar examples available for reference. Consequently, we do not provide any examples in order to avoid the negative effects associated with incoherent in-context learning. This \"try-retrievereason\" strategy significantly enhances retrieval relevance, thereby improving reasoning effectiveness. Experiments in Sec. 4.4 compare our method with several other retrieval strategies, demonstrating the superiority of our approach. 3.4 Step-Level Example Guidance in MCTS The core aspect of our method is that our steplevel in-context learning can significantly enhance 5 the models single-step reasoning capability, which makes it easily integrable into common stepreasoning strategies, such as Monte Carlo Tree Search (MCTS). Generally, MCTS methods necessitate two key components: reasoning model that generates steplevel reasoning and Process-Supervision Reward Model (PRM) that continuously evaluates the current reasoning step in real time. Our method is beneficial for both of these components. It enhances the step-level reasoning performed by the reasoning model and improves the effectiveness of the PRM in evaluating current reasoning steps. For the reasoning model, MCTS inherently requires step-by-step reasoning expansion. When expanding at node si, we simply apply the previously mentioned strategy: the model performs first tries to obtain example steps. For each example, the model completes the reasoning to generate child nodes s1 i+1. Our strategy improves the accuracy of individual child nodes sj i+1, thereby enhancing the overall effectiveness of the MCTS strategy. i+1, . . . , sn i+1, s2 Evidently, judgment ability is closely related to reasoning ability. Therefore, since our strategy can enhance the accuracy of single-step reasoning, reasonable assumption is that introducing appropriate example steps can similarly improve the PRMs ability to assess the correctness In particular, of the current reasoning process. when evaluating the correctness of an inference step candidate sj , we retrieve similar steps along with their corresponding preceding steps k1, . . . , 1 and question from the step-level exs ample bank. Similarly, the probability distributions 1, q) and (sgtisi1, . . . , s1, q) (s exhibit similarities. This resemblance aids in assessing the discrepancy between sj and sgti, thereby enhancing the accuracy of the critic models evaluations. k1, . . . , ks Detailed ablation experiments in Sec. 4.5 demonstrate that both strategies contribute positively to the overall reasoning quality of Monte Carlo Tree Search (MCTS) methods."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experiment setting Reasoning Model. Our primary reasoning model is GPT-4o (Hurst et al., 2024). To demonstrate the generality of our approach, we also conducted tests on Qwen2.5-Math-72B-Instruct (Yang et al., 2024), 6 which is the state-of-the-art open-source model in mathematical reasoning capabilities. Evaluation Benchmark. We tested our approach on several challenging open-source mathematical benchmarks, including MATH500 (Hendrycks et al., 2021), AQuA (Ling et al., 2017), OlympiadBench-TO (He et al., 2024) and MATHBench (Liu et al., 2024a) College-level and Highlevel tasks. In addition, we manually collected selection of problems from the AMC-10 and AMC-12 competitions to serve as even more challenging benchmarks. This is available at https: //github.com/beichenzbc/BoostStep . To simulate benchmarks with lower similarity to the example problem bank, we also conducted tests on MathVision (Wang et al., 2024) and MathVerse (Zhang et al., 2025), highly challenging multi-modal math benchmarks Retriever. We utilized the classic TF-IDF encoding method combined with cosine similarity as the retriever for all for both problem-level and steplevel in-context learning methods. The TF-IDF weight matrix is derived from the example problem bank. This is because the impact of the newly generated step on both TF and IDF is negligible, and real-time calculation of TF-IDF would require significant amount of time. Hyper-Parameters. The temperature value is 0 in all the experiments except for Monte Carlo Tree Search, which needs some random sampling to generate different reasoning candidates and the temperature value for MCTS is set 0.3. The reference rejection threshold is 0.7. Prompt. Apart from some necessary guidance like step-level reasoning, we ensured that the prompts for each method were as similar as possible to make the comparison fairer. The specific prompts is listed in the supplementary materials. 4.2 Comparing to Problem-Level ICL We conduct rigorous comparison of the proposed step-level in-context learning approach and traditional problem-level few-shot learning across multiple benchmarks and base models. Specifically, for traditional problem-level few-shot learning, we set the shot num to 4, which is common setting for evaluation. The results are presented in Tab. 1. Our step-level In Context-Learning achieves improvements across various mathematical benchmarks, with particularly notable enhancements observed on the relatively more challenging AMC problems. Sinece the sensitivity to example problem banks Table 1: comparison of different in-context learning strategies on different benchmarks on GPT-4o and Qwen2.5Math-72B-Instruct. The example problem bank is constructed from PRM800K, so MATH500 is an in-domain benchmark while others are all out-domain benchmarks. Best results are in bold. Model Method in-domain out-domain MATH AMC12 AMC10 AQUA MathBench(C) MathBench(H) OlympiadBench-TO GPT-4o Qwen 0-shot few-shot Ours 0-shot few-shot Ours 73.4 73.8 76.4 83.0 83.8 85. 53.6 56.5 63.0 67.4 67.4 69.2 55.8 56.7 60.4 67.7 66.8 69.6 81.1 83.9 85.4 84.6 85.0 86. 80.0 80.7 82.0 80.6 81.3 82.7 77.3 79.3 84.0 82.0 82.7 84.7 40.6 39.3 43.3 49.7 49.9 52. Table 2: Comparison of different strategies in multimodal mathematical benchmarks with lower similarity with our problem bank. Base models are all GPT-4o. Table 4: Comparison of different step-level example problem Bank construction methods. Method MathVision-Mini MathVerse-Mini 0-shot few-shot Ours 30.6 28.7 35.2 53.2 53.2 54. Table 3: Experiments on the sensitivity of the similarity between the question and the example problem bank. R_t indicates that the examples are the t_th similar for different method without any rejection strategy. Given less similar example, our method suffers an 2.26% performance loss, which is much lower comparing to few-shot learning(4.4%). Method Math-level5 AMC12 AMC10 0-shot 50.7 53.6 55.8 few-shot R_1 few-shot R_4 52.2 46.3 (-5.9) 56.5 52.2(-4.3) 56.7 53.7 (-3.0) Ours R_1 Ours R_4 56 52.2 (-3.8) 62.3 61.6 (-0.7) 60.4 58.1 (-2.3) Strategy AMC12 AMC10 MATH Grammatical Separation Reasoning Content 56.5 63.0 58.1 60.4 74.8 76."
        },
        {
            "title": "These experiments suggest",
            "content": "selecting the t_th similar example during reasoning. The result is shown in Tab. 3. We can observe that traditional problem-level in-context learning suffers from severe decrease and is even worse than 0-shot learning when is larger than 4. In contrast, our method doesnt not show significant decline. traditional problem-level few-shot learning requires high similarity within the example problem bank, which limits its generalizability. Our approach, in contrast, refines the granularity to the step level. Since different problems can still contain similar steps, our method consistently achieves improvements even the examples are dissimilar, demonstrating better general applicability. that is critical factor limiting the generalizability of in-context learning, we have designed an additional experiment to assess the sensitivity of both step-level and problem-level in-context learning approaches. Specifically, we also test different incontext learning methods on multi-modal mathematical benchmarks including MathVision (Wang et al., 2024) and MathVerse (Zhang et al., 2025), which has much lower similarity with our example problem bank. The results are shown in Tab. 2. Problem-level few-shot learning not only fails to enhance reasoning performance but can also negatively impact reasoning quality, while ours continues to achieve appreciable improvements. Moreover, we also manually decrease the similarity between the examples and the problems by"
        },
        {
            "title": "4.3 Construction of Example Problem Bank",
            "content": "To better align with the steps in reasoning, we propose constructing step-level question bank based on the reasoning content rather than grammatical divisions. To prove our assumption, we compare our approach with commonly used strategy that constructs steps based on grammatical segmentation, using periods . as the delimiter, on the same dataset PRM800K and under identical conditions. Results are presented in tab. 4. Our method largely outperforms those using periods as delimiter."
        },
        {
            "title": "4.4 Comparison of Retrieving Strategies",
            "content": "The key factor in determining the effectiveness of in-context learning lies in the relevance of the retrieved examples. At the finer-grained step level, 7 Table 5: Comparison on different retrieval strategies in step-level in-context learning. The base model is GPT4o and all the prompts are the same. Path represents retrieving by the reasoning path including all previous step si1, si2, . . . , s1 and question q, while Pre-Step represents retrieving by only the immediately preceding step si1. Best results are in bold. Strategy AMC12 AMC10 MATH MathVision Path Pre-Step First-try 56.5 57.2 63. 58.1 56.7 60.4 73.8 74.0 76.4 31.7 31.0 35.2 Table 6: detailed ablation on incorporating retrieving similar steps to provide fine-grained guidance during the reasoning and verifying phases of Monte Carlo Tree Search (MCTS) methods. Base models are GPT-4o and prompts are the same. Best results are in bold. Reason Verify AMC12 AMC10 MATH w/o MCTS (cid:37) (cid:33) (cid:37) (cid:33) (cid:37) (cid:37) (cid:33) (cid:33) 53.6 58.7 64.4 61.6 65.2 55.8 59.0 62.2 60.4 63. 73.4 77.8 79.2 78.2 79.4 designing an appropriate retrieval strategy becomes even more crucial and challenging. Therefore, we propose the first-try strategy, which involves understanding what the model currently needs to reason about using first attempt, and then searching the problem set for similar steps to guide the model in fully outputting the current step. To validate the effectiveness of this method, we compare it with several other strategies mentioned in Sec.3.3, retrieving by the entire reasoning path si1, si2, . . . , s1, or only by the immediately preceding step si1. Tab. 5 presents the detailed result. Our method significantly outperforms the other two retrieving strategies, better anticipating the content that needs to be inferred in the current step. 4.5 Example-guided Monte Carlo Tree Search As discussed above, the reasoning capability of generating model and the verifying capability of critic model are two core factors of Monte Carlo Tree Search methods, and our strategy can enhance the reasoning quality of MCTS in both ways. On one hand, it can improve the accuracy of generating candidate nodes using the previously mentioned first-try strategy when reasoning nodes are generFigure 4: specific example of adjusting reasoning during real-time inference through step-level in-context learning. The first-try uses wrong equation while the retrieving example step guides the model to use the correct equation and get the correct conclusion. ated. On the other hand, it can increase the accuracy of evaluation by introducing similar examples during critic model assessments and therefore ensures that the correct reasoning nodes are more likely to be preserved. These two strategies can be decoupled, allowing us to demonstrate the effectiveness of each component through ablation studies. In the setup for MCTS, we utilize GPT-4o as reasoning model and GPT-4o-mini as Processsupervised Reward Model. Moreover, we adopted the Pairwise Preference Reward Model (PPRM) configuration (Zhang et al., 2024b)which reduces variability and ensures more robust evaluation. Detailed setting will be listed in appendix. Tab. 6 presents the results of integrating incontext learning into the reasoning and evaluation phases of the Monte Carlo Tree Search methods. The results indicate that introducing examples enhances the verifying and reasoning capabilities of MCTS methods, therefore both approaches contribute to the improvement of overall reasoning performance through providing similar groundtruth steps. Specifically, introducing example steps in reasoning phases will bring more benefits to the overall reasoning quality. 4.6 Case Study Here we demonstrate specific example of how our step-level in-context learning boosts step-level reasoning. Given the question, we first let the model to have first-try on step one. Unfortunately, because the model is unfamiliar with trigonometric functions, it makes an error on tangent sum for8 mula, therefore leading to wrong step. However, we can get rough idea on what the model wants to calculate at this step according to the first try. Then, we find similar step which correctly leverage tangent sum formula in the step-level example problem bank. Therefore, with the guidance provided, the model correctly applied the tangent sum formula during the second reasoning attempt and arrived at the correct answer."
        },
        {
            "title": "5 Conclusion",
            "content": "We propose step-level in-context learning, which provides real-time, fine-grained guidance during the reasoning process by searching for similar steps according to the first-try reasoning attempt. This approach improves the models reasoning capabilities and reduces the dependency on the similarity of example problem set, thereby increasing the generalizability of in-context learning. Moreover, our method can also enhance the reasoning and evaluation capability of Monte Carlo Tree Search (MCTS) by introducing similar steps in reasoning and verifying phases respectively, thereby improving the overall reasoning correctness. Potential Limitations. Currently, our example problem bank is entirely sourced from PRM800k, resulting in relatively homogeneous distribution of example problems and example steps. Furthermore, the TF-IDF retriever is based on modeling language term frequency directly and lacks an understanding of mathematical content, which limits its retrieval capabilities on math problems."
        },
        {
            "title": "Acknowledgments",
            "content": "This project is funded in part by Shanghai Artificial lntelligence Laboratory, the National Key R&D Program of China (2022ZD0160201), the Centre for Perceptual and Interactive Intelligence (CPII) Ltd under the Innovation and Technology Commission (ITC)s InnoHK. Dahua Lin is PI of CPII under the InnoHK."
        },
        {
            "title": "References",
            "content": "Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. 2024. Step-level value preference optimization for mathematical reasoning. arXiv preprint arXiv:2406.10858. OpenCompass Contributors. 2023. Opencompass: universal evaluation platform for foundation https://github.com/open-compass/ models. opencompass. Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. 2024. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM international conference on multimedia, pages 1119811201. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Edward Feigenbaum, Julian Feldman, et al. 1963. Computers and thought, volume 37. New York McGraw-Hill. Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. 2023. Alphazero-like tree-search can guide large language model decoding and training. arXiv preprint arXiv:2309.17179. Charles Fletcher. 1985. Understanding and solving arithmetic word problems: computer simulation. Behavior Research Methods, Instruments, & Computers, 17(5):565571. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. 2023. Critic: Large language models can self-correct arXiv preprint with tool-interactive critiquing. arXiv:2305.11738. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. 2024. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Pei Ke, Bosi Wen, Andrew Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, et al. 2024. Critiquellm: Towards an informative critique generation model for evaluation of large language model generation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1303413054. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in 9 neural information processing systems, 35:22199 22213. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. 2022. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843 3857. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets verify step by step. arXiv preprint arXiv:2305.20050. Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146. Hongwei Liu, Zilong Zheng, Yuxuan Qiao, Haodong Duan, Zhiwei Fei, Fengzhe Zhou, Wenwei Zhang, Songyang Zhang, Dahua Lin, and Kai Chen. 2024a. Mathbench: Evaluating the theory and application proficiency of llms with hierarchical mathematics benchmark. arXiv preprint arXiv:2405.12209. Jiayu Liu, Zhenya Huang, Chaokun Wang, Xunpeng Huang, Chengxiang Zhai, and Enhong Chen. 2024b. What makes in-context learning effective for mathematical reasoning: theoretical analysis. arXiv preprint arXiv:2412.12157. Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, et al. 2024. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2024. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. 2024. Measuring multimodal mathematical reasoning with math-vision dataset. arXiv preprint arXiv:2402.14804. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. 10 Jinyang Wu, Mingkuan Feng, Shuai Zhang, Feihu Che, Zengqi Wen, and Jianhua Tao. 2024. Beyond examples: High-level automated reasoning paradigm arXiv preprint in in-context learning via mcts. arXiv:2411.18478. Yifan Xu, Xiao Liu, Xinghan Liu, Zhenyu Hou, Yueyan Li, Xiaohan Zhang, Zihan Wang, Aohan Zeng, Zhengxiao Du, Wenyi Zhao, et al. 2024. Chatglmmath: Improving math problem-solving in large language models with self-critique pipeline. arXiv preprint arXiv:2404.02893. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. 2024. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36. Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, et al. 2024. Internlm-math: Open math large language models toward verifiable reasoning. arXiv preprint arXiv:2402.06332. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653. Di Zhang, Xiaoshui Huang, Dongzhan Zhou, Yuqiang Li, and Wanli Ouyang. 2024a. Accessing gpt-4 level mathematical olympiad solutions via monte carlo tree self-refine with llama-3 8b. arXiv preprint arXiv:2406.07394. Di Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong Li, Tong Xie, Xiaoshui Huang, Shufei Zhang, Marco Pavone, Yuqiang Li, et al. 2024b. Llama-berry: Pairwise optimization for o1-like olympiad-level mathematical reasoning. arXiv preprint arXiv:2410.02884. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. 2025. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer. Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Ruyi Gan, Jiaxing Zhang, and Yujiu Yang. 2022. Solving math word problems via cooperative reasoning induced language models. arXiv preprint arXiv:2210.16257. the solution is directly judged as inconsistent with ground truth. A.3 Benchmarks We tested our approach on several challenging open-source mathematical benchmarks, including MATH500 (Hendrycks et al., 2021), AQuA (Ling et al., 2017), OlympiadBench-TO (He et al., 2024) and MATHBench (Liu et al., 2024a). Specifically, we use the Olympiad-TO (text-only) subset of OlympiadBench, and the application problems in college-level and high-level difficulty. For multi-modal math benchmarks, we use MathVision-Mini (Wang et al., 2024) and visiondominant version of problems in MathVerse-Mini (Zhang et al., 2025)."
        },
        {
            "title": "B Detailed Setup for MCTS",
            "content": "In the setup for MCTS, we utilize GPT-4o as the reasoning model and employ GPT-4o-mini as the Process-supervised Reward Model (PRM). For the PRM, we adopted the Pairwise Preference Reward Model (PPRM) configuration (Zhang et al., 2024b). Specifically, PPRM transforms the absolute rewards calculation into preference predictions between solutions to calculate rewards. This approach reduces the variability associated with scoring characteristics and thus leads to more robust and consistent evaluation of different solutions. The complete reasoning process of MCTS in our experiment is as follows: we start with the target problem as the root node and obtain two initial solution steps through sampling to serve as the two initial parent nodes. In each step-level reasoning phase, we expand these two parent nodes through sampling, generating four candidate child nodes. Using the PPRM, we select the two child nodes with higher confidence to become the parent nodes for the next step of reasoning. This process continues until both candidate nodes have completed their reasoning paths, resulting in the final answers. Finally, PPRM is used to select the ultimate answer from these two reasoning paths."
        },
        {
            "title": "A Detailed Experiment Setting",
            "content": "A.1 Prompt Prompt for 0-shot COT: You are professional math problem solver. Solve the problem step by step and output the final answer within boxed{}. Prompt for problem-level few-shot learning: You are professional math problem solver. Solve the problem step by step and output the final answer within boxed{}. In case you dont know how to solve it, will give you example problems with their full solutions which you can refer to. Example i: Problem: xxx Solution: xxx Prompt for first-try in step-level COT: You are professional math problem solver. will give you math problem and part of its solution. And you need to only output the next step of the solution, starting with Step i:, where is the step number. If you think that the final step is derived, put the answer within boxed{} Prompt for step-level few-shot learning: You are professional math problem solver. will give you math problem and part of its solution. And you need to only output the next step of the solution, starting with Step i:, where is the step number. In case you dont know how to derive the correct content, an example with Key Step will be given. You need to learn how Key Step is derived, and implement similar strategy in your derivation procedure. If you think that the final step is derived, put the answer within boxed{}. Example Problem: xxx Example Solution: Step1: xxx, Step2: xxx, ..., Stepi(Key Step): xxx. A.2 Details of Grading and Metrics We follow the setting of Opencompass (Contributors, 2023) and VLMEvalKit (Duan et al., 2024). Specifically, we first require the model to put the final answer within boxed{}. Then, we use GPT4o-mini as judge model to compare the final answer with the groundtruth answer. Comparing to string matching, this approach can eliminate some false negative evaluations because the same mathematical expression can be expressed in many forms. If the model fails to follow the expected format in the prompt and the rule-based extraction fails,"
        }
    ],
    "affiliations": [
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong"
    ]
}