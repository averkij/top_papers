{
    "paper_title": "PIG: Physics-Informed Gaussians as Adaptive Parametric Mesh Representations",
    "authors": [
        "Namgyu Kang",
        "Jaemin Oh",
        "Youngjoon Hong",
        "Eunbyung Park"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The approximation of Partial Differential Equations (PDEs) using neural networks has seen significant advancements through Physics-Informed Neural Networks (PINNs). Despite their straightforward optimization framework and flexibility in implementing various PDEs, PINNs often suffer from limited accuracy due to the spectral bias of Multi-Layer Perceptrons (MLPs), which struggle to effectively learn high-frequency and non-linear components. Recently, parametric mesh representations in combination with neural networks have been investigated as a promising approach to eliminate the inductive biases of neural networks. However, they usually require very high-resolution grids and a large number of collocation points to achieve high accuracy while avoiding overfitting issues. In addition, the fixed positions of the mesh parameters restrict their flexibility, making it challenging to accurately approximate complex PDEs. To overcome these limitations, we propose Physics-Informed Gaussians (PIGs), which combine feature embeddings using Gaussian functions with a lightweight neural network. Our approach uses trainable parameters for the mean and variance of each Gaussian, allowing for dynamic adjustment of their positions and shapes during training. This adaptability enables our model to optimally approximate PDE solutions, unlike models with fixed parameter positions. Furthermore, the proposed approach maintains the same optimization framework used in PINNs, allowing us to benefit from their excellent properties. Experimental results show the competitive performance of our model across various PDEs, demonstrating its potential as a robust tool for solving complex PDEs. Our project page is available at https://namgyukang.github.io/Physics-Informed-Gaussians/"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 ] . [ 1 4 9 9 5 0 . 2 1 4 2 : r PIG: PHYSICS-INFORMED GAUSSIANS AS ADAPTIVE PARAMETRIC MESH REPRESENTATIONS Namgyu Kang1, Jaemin Oh2, Youngjoon Hong2, Eunbyung Park1,3 1Department of Artificial Intelligence, Sungkyunkwan University 2Department of Mathematical Sciences, KAIST 3Department of Electrical and Computer Engineering, Sungkyunkwan University https://namgyukang.github.io/Physics-Informed-Gaussians/"
        },
        {
            "title": "ABSTRACT",
            "content": "The approximation of Partial Differential Equations (PDEs) using neural networks has seen significant advancements through Physics-Informed Neural Networks (PINNs). Despite their straightforward optimization framework and flexibility in implementing various PDEs, PINNs often suffer from limited accuracy due to the spectral bias of Multi-Layer Perceptrons (MLPs), which struggle to effectively learn high-frequency and non-linear components. Recently, parametric mesh representations in combination with neural networks have been investigated as promising approach to eliminate the inductive biases of neural networks. However, they usually require very high-resolution grids and large number of collocation points to achieve high accuracy while avoiding overfitting issues. In addition, the fixed positions of the mesh parameters restrict their flexibility, making it challenging to accurately approximate complex PDEs. To overcome these limitations, we propose Physics-Informed Gaussians (PIGs), which combine feature embeddings using Gaussian functions with lightweight neural network. Our approach uses trainable parameters for the mean and variance of each Gaussian, allowing for dynamic adjustment of their positions and shapes during training. This adaptability enables our model to optimally approximate PDE solutions, unlike models with fixed parameter positions. Furthermore, the proposed approach maintains the same optimization framework used in PINNs, allowing us to benefit from their excellent properties. Experimental results show the competitive performance of our model across various PDEs, demonstrating its potential as robust tool for solving complex PDEs."
        },
        {
            "title": "INTRODUCTION",
            "content": "Machine learning techniques have become promising tools for approximating solutions to Partial Differential Equations (PDEs) (Raissi et al., 2017; Yu et al., 2018; Karniadakis et al., 2021; Finzi et al., 2023; Gaby et al., 2024). notable example is the Physics-Informed Neural Network (PINN) (Raissi et al., 2019), which leverages deep neural networks and gradient-based optimization algorithms. This approach circumvents the need for the time-intensive mesh design prevalent in numerical methods and allows us to solve both forward and inverse problems within the same optimization framework. With the increased computational power and the development of easy-to-use automatic differentiation software libraries (Abadi et al., 2015; Bradbury et al., 2018; Innes, 2018; Paszke et al., 2019), PINNs have successfully tackled broad range of challenging PDEs Hu et al. (2024c); Li et al. (2024); Oh et al. (2024). Although the mesh-free neural network approach shows significant promise in solving PDEs, it has several limitations. Training PINNs typically requires numerous iterations to converge (Saarinen et al., 1993; Wang et al., 2021; De Ryck et al., 2023). Despite recent techniques aimed at reducing Equal contribution Corresponding authors 1 Figure 1: Training visualization of the Allen-Cahn equation (200, 600, 1000, 2000 training iterations): Each Gaussian is displayed as the ellipsoids, exhibiting different positions and shapes according to the Gaussian parameters, mean and covariance. Since we adopt causal loss (Wang et al., 2024c), the solution is gradually approximated from = 0 to = 1. Note that the Gaussians are densely aligned in the locations where the solution changes abruptly. computational costs, multiple forward and backward passes of neural networks are still necessary to compute the PDE residual losses. Furthermore, obtaining more accurate approximations demands the use of wider and deeper neural networks, which enhances their expressiveness but significantly increases computational costs (Cybenko, 1989; Baydin et al., 2018; Kidger & Lyons, 2020). In addition, the inductive bias inherent in neural networks often hinders the accuracy of solution approximations. well-known example is the spectral bias, which favors learning low-frequency components of solution functions and struggles to capture high-frequency or singular behaviors (Rahaman et al., 2019). Although some solutions to this issue have been proposed (Tancik et al., 2020; Sitzmann et al., 2020), eliminating inductive biases from neural networks remains challenge. To address these issues, recent studies have explored combining classical grid-based representations with lightweight neural networks (Hui et al., 2018; Cao et al., 2023). In this approach, the parametric grids map input coordinates to intermediate features, which are then processed by neural networks to produce the final solutions. By relying on high-resolution parametric grids for representational capacity, this method reduces the impact of neural networks inductive biases. Additionally, using lightweight neural networks significantly reduces computational demands, leading to faster training speeds compared to traditional neural network-only methods. While promising, existing methods that combine parametric grids with neural networks face fundamental challenge. The positions of the parameters (the locations of vertices) are predetermined by the grid resolutions and remain fixed during training. Since the optimal allocation of representational capacity (determining where to place more vertices) is unknown, these methods typically use high-resolution grids that uniformly distribute many vertices across the entire input domain to achieve more accurate solutions. This approach results in using large set of learnable parameters, which often leads to overfitting issues, i.e., low PDE residual losses but inaccurate solutions. To mitigate this problem, large number of collocation points are sometimes used during training at the expense of the increased computational costs. In this work, we introduce novel representation for approximating solutions to PDEs. Drawing inspiration from adaptive mesh-based numerical methods (Berger & Oliger, 1984; Seol et al., 2016) and the recent parametric grid representations (Li & Lee, 2021; Jang et al., 2023), we propose the Physics-Informed Gaussian (PIG) that learns feature embeddings of input coordinates, using mixture of Gaussian functions. For given input coordinate, PIG extracts feature vector as the weighted sum of the feature embeddings held by Gaussians with their learnable parameters (positions and shapes). They are adjusted during the training process, and underlying PDEs govern this dynamic adjustment. To update the parameters of all Gaussians, we leverage the well-established PINNs training framework, which employs numerous collocation points to compute PDE residuals and uses gradient-based optimization algorithms. The proposed approach offers several advantages over existing parametric grid methods. PIG dynamically adjusts the computational mesh structure and the basis functions (Gaussians) to learn the feature embeddings. By following the gradient descent directions, the Gaussians move towards regions with high residual losses or singularities, and this adaptive strategy allows for more efficient and precise solutions than the static uniform grid structures. In addition, Gaussian functions 2 are infinitely differentiable everywhere, allowing for the convenient computation of higher-order gradients for PDE residuals, and they can be seamlessly integrated into deep-learning computation pipelines. The final architecture of the proposed approach, presented in 2-(c), that combines the learnable Gaussian feature embedding and the lightweight neural network is new learning-based PDE solver that can provide more efficient and accurate solution approximations. We have tested the proposed method on an extensive set of challenging PDEs (Krishnapriyan et al., 2021; Wang et al., 2024c; Cho et al., 2024). The experimental results show that the proposed PIG achieved competitive accuracy compared to the existing methods that use large MLPs or highresolution parametric grids. When the number of Gaussians in our PIG model is comparable to the number of vertices in previous parametric grids, our method significantly outperformed existing approaches, demonstrating its superior efficiency. Furthermore, the proposed PIG shows significantly faster convergence speed than PINNs using large neural networks, demonstrating its effectiveness as promising learning-based PDE solver. Our contributions are summarized as follows. We introduce Physics-Informed Gaussians, an efficient and accurate PDE solver that utilizes learnable Gaussian feature embeddings and lightweight neural network. We propose dynamically adaptive parametric mesh representation that effectively addresses the challenges encountered in previous static parametric grid approaches. We demonstrate that our PIG model achieves competitive accuracy and faster convergence with fewer parameters compared to state-of-the-art methods, establishing its efficacy and paving the way for new research avenues."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 PHYSICS-INFORMED NEURAL NETWORKS PINNs are class of machine learning algorithms designed to solve PDEs by integrating physical laws into the learning process. Introduced by Raissi et al. (2019), PINNs leverage neural networks to approximate the solutions of PDEs while ensuring that the learned solutions respect the underlying physics. This is achieved by incorporating the PDE residuals directly into the loss function, allowing the model to be trained using standard gradient-based optimization methods. PINNs have gained significant attention for their ability to handle high-dimensional (Wang et al., 2022b; Hu et al., 2024b;a) and complex problems (Yang et al., 2021; Pensoneault & Zhu, 2024) that are challenging for traditional numerical methods. They are particularly effective in scenarios where data is sparse or expensive to obtain, as they can incorporate prior knowledge about the physical system. Applications of PINNs span various domains, including fluid dynamics, solid mechanics, and electromagnetics, demonstrating their versatility and effectiveness in solving real-world problems (Cai et al., 2021; Khan & Lowther, 2022; Bastek & Kochmann, 2023). Key advantages of PINNs include their mesh-free nature, the ability to easily incorporate boundary and initial conditions, and their flexibility in handling various types of PDEs. However, they also face challenges, such as the need for extensive computational resources and the difficulty in training deep networks to achieve accurate solutions. For example, Wang et al. (2024b) typically uses around 9 hidden layers with 256 hidden units (sometimes up to 18 layers) to achieve high accuracy. This requires massive computations to run the neural network, which involves multiple forward and backward passes to compute the gradients for PDE residual loss. Furthermore, it slows down the convergence speed due to the large number of model parameters. 2.2 PHYSICS-INFORMED PARAMETRIC GRID REPRESENTATIONS Physics-informed parametric grid representations combine traditional grid-based methods with neural networks to solve PDEs (Kang et al., 2023; Huang & Alkhalifah, 2024; Wang et al., 2024a; Shishehbor et al., 2024a). These representations have also been extensively explored in image, video, and 3D scene representations (Liu et al., 2020; Yu et al., 2021; Fridovich-Keil et al., 2022; Muller et al., 2022; Chen et al., 2022; Sun et al., 2022; Fridovich-Keil et al., 2023) by training the models as supervised regression problems. By discretizing the solution domain into grid and associating each grid point with trainable parameters, these methods leverage the structured nature of 3 grids to capture spatial variations effectively. This hybrid approach maintains high accuracy and reduces computational costs compared to purely neural network-based methods. Key benefits include the ability to handle high-resolution representations and integrate boundary conditions efficiently. However, the fixed grid structure can lead to suboptimal allocation of representational capacity during training. Despite this limitation, physics-informed parametric grid representations are promising for achieving accurate solutions in complex scenarios."
        },
        {
            "title": "2.3 ADAPTIVE MESH-BASED METHODS",
            "content": "Adaptive mesh-based methods dynamically adjust the computational mesh to minimize the error between approximated and true solutions. This process involves posteriori error analysis, which estimates errors after solving, allowing for targeted mesh refinement. Such adaptivity is crucial in the numerical analysis as it ensures efficient allocation of computational resources, focusing on regions with high errors and thus improving overall accuracy and efficiency (Ainsworth & Oden, 1993; 1997). There are also some studies on non-uniform adaptive sampling methods in the context of PINNs. Lu et al. proposed residual-based adaptive refinement method in their work with DeepXDE, aiming to enhance the training efficiency of PINNs (Lu et al., 2021; Wu et al., 2023). More recently, Yang et al. (2023b) introduced Dynamic Mesh-based Importance Sampling (DMIS), novel approach that constructs dynamic triangular mesh to efficiently estimate sample weights, significantly improving both convergence speed and accuracy. Similarly, Yang et al. (2023a) developed an end-to-end adaptive sampling framework called MMPDE-Net, which adapts sampling points by solving the moving mesh PDE. When combined with PINNs to form MS-PINN, MMPDE-Net demonstrated notable performance improvements. While these adaptive methods offer significant benefits, they also introduce additional complexity into the PINN framework. 2.4 POINT-BASED REPRESENTATIONS Irregular point-based representations have long been considered promising approaches for representing, reconstructing, and processing data (Qi et al., 2017; Xu et al., 2022; Zhang et al., 2022). recent study in 3D scene representation utilized Gaussians as graphical primitive and showed remarkable performance in image rendering quality and training speed (Kerbl et al., 2023). The combination of Gaussian representation and neural networks has recently been explored in regressing images or 3D signed distance functions, showing its great expressibility (Chen et al., 2023). While those studies share some architectural similarities with our method, they all primarily focus on supervised regression problems to reconstruct the visual signals. We developed the architecture suitable for effective PDE solvers and first showed that the Gaussian features and neural networks can be trained in an unsupervised manner guided by the physical laws."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "3.1 PRELIMINARY: PHYSICS-INFORMED NEURAL NETWORKS Consider an abstract underlying equation, D[u](x) = (x), Ω Rd, (1) (2) where is differential operator, and is boundary operator which could contain the initial condition. The physics-informed neural network methods try to find an approximate solution by minimizing B[u](x) = g(x), Ω, L(θ) = (cid:90) Ω D[uθ](x) (x)2dx + λ (cid:90) Ω B[uθ](x) g(x)2dσ(x) (3) where uθ is neural network with the set of network parameters θ, λ is positive real number, and σ is surface measure. In practice, integrals are usually estimated via Monte Carlo integration. PINNs typically utilize automatic differentiation to compute the PDE residuals and θL(θ). For more details, please refer to the original paper (Raissi et al., 2019). Figure 2: (a) PINN directly takes input coordinates (four collocation points) as inputs and produces outputs. (b) Parametric grids first map input coordinates to output feature vectors. Each vertex in the grids holds learnable parameters, and output features are extracted through interpolation schemes. (c) The proposed PIG consists of numerous Gaussians moving around within the input domain, and their shapes change dynamically during training. Each Gaussian has learnable parameters, and feature vector for an input coordinate is the weighted sum of the learnable parameters based on the distance to the Gaussians. 3.2 PHYSICS-INFORMED GAUSSIANS In this section, we present the proposed Physics-Informed Gaussian representation (PIG) for approximating solutions to PDEs. It comprises two stages: Gaussian feature embeddings (3.2.1) and solution approximation based on these features (3.2.2). 3.2.1 LEARNABLE GAUSSIAN FEATURE EMBEDDING Let ϕ = {(µi, Σi, fi) : = 1, . . . , } be the set of Gaussian model parameters, where µi Rd is position of Gaussian, and Σi Sd ++ is covariance matrix and each Gaussian has learnable feature embedding fi Rk. Given an input coordinate Rd, the learnable Gaussian feature embedding FEϕ : Rd Rk is extracted as follows. FEϕ(x) = (cid:88) i=1 fiGi(x), Gi(x) = 1 2 (xµi)Σ1 (xµi), (4) where is the number of Gaussians and Gi represents the i-th Gaussian function. FEϕ maps an input coordinate to feature embedding by weighted sum of the individual features fi of each Gaussian. To enhance the expressive capability, we can use different Gaussians for each feature dimension. Further details are provided in Appendix A.1. Gaussian features distant from the input coordinates do not contribute to the final feature embedding, while only neighboring Gaussian features remain significant. Similar to the previous parametric grid methods, which obtain feature embeddings by interpolating only neighboring vertices, this locality encourages the model to capture high-frequency details by effectively alleviating spectral bias. All Gaussian parameters ϕ are learnable and iteratively updated throughout the training process. This dynamic adjustment, akin to adaptive mesh-based numerical methods, optimizes the structure of the underlying Gaussian functions to accurately approximate the solution functions. For example, the regions with high-frequency or singular behaviors require more computational parameters, and Gaussians, updated based on the gradients , will migrate to these regions to reduce the loss µi (see Figure 1). Compared to the existing parametric grid approaches, which achieve this goal by uniformly increasing grid resolution, the proposed method can build more parameter-efficient and optimal mesh structure. 3.2.2 SOLUTION APPROXIMATION WITH LEARNABLE GAUSSIANS FOLLOWED BY LIGHTWEIGHT NEURAL NETWORK Once the features are extracted, neural network processes the feature to produce the solution outputs. uϕ,θ(x) = NNθ(FEϕ(x)), (5) where NNθ is small and lightweight MLP with the parameter θ. We employed single hidden layer MLP with limited number of hidden units, resulting in negligible additional computational costs. 5 Feature extraction plays primary role in producing the final solution, while the MLP functions as feature refinement mechanism. Even though Gaussian features are already universal approximators (see 3.3), using small MLP at the end improved the solution accuracy by large margin compared to the method without the MLP, generating the solution directly from Gaussian representations, i.e., uϕ(x) = FEϕ(x)."
        },
        {
            "title": "3.2.3 PIG AS A NEURAL NETWORK",
            "content": "The proposed Gaussian feature embedding admits form of radial basis function network. Figure 3 depicts the overall PIG architecture as neural network. The first layer contains (the number of Gaussians) RBF units, and an input coordinate passes through all RBF units, Gi(x), resulting in -dimensional vector. single fully connected layer processes this vector to produce k-dimensional feature vector. The weight matrix RkN in this layer corresponds to the feature vectors held by each Gaussian, i.e., W:,i Rk equals fi Rk. The extracted feature vector is further processed by single hidden layer MLP (we used the tanh activation function) to produce the final output, as depicted in Figure 3. Overall, the proposed PIG architecture can be interpreted as an MLP with one input layer with RBF units and two hidden layers (no activation for the first hidden layer, and tanh for the second hidden layer). Figure 3: PIG as neural network. related study by Bai et al. (2023) has explored solving various PDEs using radial basis function networks (Park & Sandberg, 1991; Buhmann, 2000) within the framework of physics-informed machine learning. However, their approach differs from ours in that the positions of the basis functions are fixed. In contrast, our method allows the positions of the Gaussians to adjust dynamically, moving in directions that minimize the loss function. In addition, we extract the feature vectors from neighboring Gaussians and further process them using shallow neural networks while they directly predict the solution output from the Gaussians. 3.3 UNIVERSAL APPROXIMATION THEOREM FOR PIGS Here, we present the Universal Approximation Theorem (UAT) for PIGs. PIG consists of two functions: FEϕ and NNθ (see equation 5). We will prove the UAT only for FEϕ, as the UAT for PIGs follows directly from the standard UAT for MLPs. Given our earlier discussion on the relationship between PIGs and radial basis function networks, we begin with the following UAT specific to radial basis function networks. Theorem 1 ( Park & Sandberg (1991)) Let : Rd be an integrable bounded function such that is continuous and (cid:90) K(x) dx = 0. Then the family SK, defined as linear combinations of translations of K, Rd SK = (cid:40) (cid:88) i=1 (cid:12) (cid:12) fiK(x µi) (cid:12) (cid:12) fi R, µi Rd, , (cid:41) is dense in C(Rd). (6) (7) However, Theorem 1 does not apply to PIGs, as the feature embedding FEϕ in PIGs takes slightly different form: FEϕ(x) = fiK (x µi; Σi) , (8) (cid:88) i=1 6 Methods PINN LRA PIXEL SPINN JAX-PI PirateNet PIG (Ours) 1std best Allen-Cahn - - 8.86e-3 - 5.37e-5 2.24e-5 1.04e-4 Helmholtz 4.02e-1 3.69e-3 8.63e-4 - - - 4.13e4.12e-5, 2.59e-05, 5.93e-5 2.22e-5 Nonlinear Diffusion 9.50e-3 - - 4.47e-2 - - 2.69e-3 6.55e-4, 1.44e-3 Flow Mixing Klein Gordon - - - 2.90e-3 - - 4.51e-4 1.74e-4, 2.67e3.43e-2 - - 1.93e-2 - - 2.76e-3 4.27e-4, 2.36e-3 Table 1: Comparison of relative L2 errors across different methods. Three experiments were conducted using seeds 100, 200, and 300, with the mean and standard deviation presented in the table. The methods compared include PINN (Raissi et al., 2019), Learning Rate Annealing (LRA) (Wang et al., 2021), PIXEL (Kang et al., 2023), SPINN (Cho et al., 2024), JAX-PI (Wang et al., 2023), and Pirate-Net (Wang et al., 2024b). For fair comparisons, we included the reported values from the respective references and omitted results that were not provided in the original papers. where the key difference lies in the presence of Σi. Notably, the set = (cid:40) (cid:88) i=1 fiK(x µi; Σi) (cid:12) (cid:12) fi R, µi Rd, Σi Sd (cid:12) (cid:12) ++, (cid:41) , (9) with S++ denoting the set of positive definite matrices, contains SK. Therefore, C(Rd). We summarize this in the following corollary: is dense in Corollary 1 The scalar-valued, d-dimensional PIGs {NNθ FEϕ(θ, ϕ) Rp1+p2} are dense in C(Rd)."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETUP To validate the effectiveness of our proposed method, We conducted extensive numerical experiments on various challenging PDEs, including Allen-Cahn, Helmholtz, Nonlinear Diffusion, Flow Mixing, and Klein-Gordon equations (For more experiments, please refer to the Appendix). We used the Adam optimizer (Kingma & Ba, 2014) for all equations except for the Helmholtz equation, in which the L-BFGS optimizer (Liu & Nocedal, 1989) was applied for fair comparison to the baseline method PIXEL. For computational efficiency, we considered diagonal covariance matrix Σ = diag(σ1, . . . , σd) and we will discuss more in Section 4.3.3. 4.2 EXPERIMENTAL RESULTS 4.2.1 (1+1)D ALLEN-CAHN EQUATION We compared our method against one of the state-of-the-art PINN methods on the Allen-Cahn equation, JAX-PI (Wang et al., 2023). For the detailed description, please refer to Appendix A.2.1. As shown in Figure 4, our method converges significantly faster and achieves competitive final accuracy (see Table 1). JAX-PI used modified MLP architecture and 4 hidden layers with 256 hidden neurons. Thus, the number of parameters in JAX-PI is more than 250K, while ours used only around 20K parameters ((N, d, k) = (4000, 2, 1)). Also, note that the L2 error curve in Figure 4 is displayed per iteration, and computational costs per iteration of ours are significantly lower than JAX-PI, which requires multiple forward and backward passes of the wide and deep neural network. 4.2.2 2D HELMHOLTZ EQUATION Figure 5 illustrates the numerical performance of our proposed PIG method for the 2D Helmholtz equation, comparing it to PIXEL (Kang et al., 2023), one of the state-of-the-art methods within the 7 Figure 4: Allen-Cahn Equation. Reference solution and absolute error maps of PIG and one of the state-of-the-art methods (JAX-PI) to Allen-Cahn Equation (x-axis: t, y-axis: x). The rightmost depicts relative L2 error curve during the training process (x-axis: iterations, y-axis: L2 error). The experiment was conducted with three different seeds, and the best relative L2 error of PIG is 5.93 105. Figure 5: 2D Helmholtz Equation. Reference solution and absolute error maps of PIG and one of the state-of-the-art methods (PIXEL) to 2D Helmholtz Equation. The rightmost depicts relative L2 error curve during the training process and the best relative L2 error of PIG is 2.22 105. PINN family that uses parametric grid representations. more detailed description of the experimental setup is available in Appendix A.2.2. The experiments were conducted using three different seeds, with PIG achieving the best relative L2 error of 2.22 105 when employing the L-BFGS optimizer, and relative L2 error of 2.50 104 with the Adam optimizer (For fair comparison, we reported the result using L-BFGS since PIXEL used L-BFGS). Notably, the results show that PIGs error is four times lower than that of PIXEL, highlighting the efficiency and accuracy of our method. We did not compare against other state-of-the-art methods, such as JAX-PI or Pirate-Net, as they did not conduct experiments in this setting. While we could have used their codes, the sensitivity of PINN variants to hyperparameters complicates fair comparisons. 4.2.3 (2+1)D KLEIN-GORDON EQUATION Figure 6 presents the predicted solution profile for the Klein-Gordon equation, comparing our results with SPINN. The best relative L2 error achieved is 2.36 103, which outperforms SPINN by an order of magnitude. For further details, please refer to Appendix A.2.3. 4.2.4 (2+1)D NONLINEAR DIFFUSION EQUATION We evaluated the performance of PIGs on the (2+1) dimensional nonlinear diffusion equation, with visualizations presented in Figure 18. The relative L2 error achieved is 1.44 103. For details on the experimental setup, please refer to Appendix A.2.5. 4.2.5 (2+1)D FLOW-MIXING PROBLEM Figure 7 displays the numerical solutions and absolute errors for the (2+1) flow mixing problem. Our solutions closely match the reference, with PIG achieving maximum absolute error of 5.03 103, compared to 2.63 101 for SPINN, underlining the superior performance of PIG. Figure 19 presents solution profiles up to = 4. Additional details can be found in Appendix A.2.4. 8 Figure 6: Klein-Gordon Equation. Reference solution and absolute error maps of PIG and one of the state-of-the-art methods (SPINN) to Klein-Gordon Equation. The rightmost depicts relative L2 error curve during the training process and the best relative L2 error of PIG is 2.36 103. Figure 7: Flow mixing problem. The best relative L2 error of PIG is 2.67104, while its maximum absolute error is 5.03 103. In comparison, one of the state-of-the-art methods, SPINN achieved 1.93 102 L2 error and showed maximum absolute error of 2.63 101. 4.3 HYPERPARAMETER ANALYSIS AND ABLATION STUDY In this section, we present the experimental results to show the effects of each component of the proposed PIG (Using MLP, learnable Gaussian positions, and dense covariance matrices), In addition, We study the effect of the number of Gaussians, the size of MLP and input dimensions. 4.3.1 THE NUMBER OF GAUSSIANS In numerical analysis, there is general trend that the quality of the solution improves as the mesh is refined. Given our approach of using Gaussians as mesh points, we expect that the accuracy of PIGs will improve with an increased number of Gaussians. Table 2 illustrates the accuracy improvements of PIGs to the number of Gaussians. Overall, we observe positive correlation between the number of Gaussians and improved accuracy. It is important to note that achieving this trend can be challenging for other PINN-type methods. # Gaussians 200 400 600 800 1000 Flow-Mixing Nonliner-Diffusion Allen-cahn 1.83e-02 2.33e-03 2.93e-03 2.22e-03 2.75e-03 2.23e-03 1.22e-03 1.95e-03 4.81e-04 7.33e-03 3.98e-04 3.96e-03 6.07e-03 3.13e-03 1.50e-03 1.44e-03 1.31e-03 1.03e-03 Table 2: The number of Gaussians and approximation accuracy (Flow-Mixing, Nonlinear Diffusion, and Allen-Cahn). The results indicate that increasing the number of Gaussians typically leads to decrease in relative L2 error. 4.3.2 MLP IMPACT AND ADAPTIVE GAUSSIAN POSITIONS While FEϕ serves as universal approximator, we found that adding small MLP NNθ significantly enhances performance. Additionally, our ablation study explores the effectiveness of allowing adaptive Gaussian positions (learnable µ vs. fixed µ). The results in Table 3 illustrate that varying Gaussian positions µ improve accuracy, particularly when combined with the MLP. We also evaluate the sensitivity of PIGs to the width and input dimensions of the MLP, as summarized in Table 4. Notably, no clear trend emerges, highlighting the robustness of PIGs to MLP variations. (MLP, µ) (X, Fixed) (O, Fixed) (X, Learn) (O, Learn) Allen-Cahn Helmholtz Nonlinear Diffusion Flow-Mixing Klein-Gordon 4.72e-03 1.82e-03 7.29e-05 7.27e-05 3.97e-04 2.12e-04 1.86e-04 2.22e6.32e-03 2.10e-03 5.26e-03 1.44e-03 4.33e-03 1.09e-03 7.93e-04 4.51e-04 6.44e-02 2.69e-02 8.51e-03 2.76e-03 Table 3: Ablation study results on MLP and µ across various equations. MLP input dim (=k) # Hidden units 4 8 16 32 64 128 1 7.77e-03 8.55e-03 8.24e-03 7.14e-03 6.33e-03 6.38e-03 5.21e-03 2 9.60e-03 6.44e-03 1.06e-02 8.06e-03 7.50e-03 6.88e-03 6.60e-03 3 7.68e-03 1.06e-02 1.21e-02 1.22-02 1.09e-02 8.48e-03 5.22e-03 4 9.60e-03 8.54e-03 6.90e-03 6.87e-03 9.48e-03 7.47e-03 5.40e-03 Table 4: The performance of different MLP configurations for the Helmholtz equation, displaying L2 relative errors at iteration 1,000 across various configurations of hidden units and MLP input dimensions. Overall, the results highlight the robustness to the size of MLP, showing minimal variation in errors across different settings. 4.3.3 COVARIANCE MATRICES Dense covariance matrices can represent the most general form of Gaussians, but they are more computationally expensive than diagonal covariance matrices. We compared these two types of covariance matrices across several equations: the 2D Helmholtz equation, the Klein-Gordon equation, the Flow-Mixing equation, and the Nonlinear-Diffusion equation. Despite the increased number of network parameters and generality associated with dense matrices, both dense and diagonal covariance matrices yielded similar error levels, as summarized in Table 5. Note that the number of Gaussians used in the dense covariance matrices is significantly lower than that in the diagonal matrices, with 50 Gaussians for the dense case compared to 4,000 for the diagonal case, as seen in the Nonlinear-Diffusion equation. We believe that the advanced training techniques and engineering would improve the performance of PIG with dense covariance matrices and leave it to future works. Dense Diagonal Helmholtz Klein-Gordon 5.17e-05 2.22e-05 1.81e-03 2.76e-03 Flow-Mixing Nonlinear Diffusion 3.48e-04 4.51e3.86e-03 1.44e-03 Table 5: Comparison of error levels between dense and diagonal covariance matrices in PIGs. For dense covariance matrix experiments, we first trained PIG using diagonal covariance matrix and then fine-tuned full covariance matrix parameters initialized from the trained diagonal elements."
        },
        {
            "title": "5 CONCLUSION AND LIMITATIONS",
            "content": "In this work, we introduced PIGs as novel method for approximating solutions to PDEs. By leveraging explicit Gaussian functions combined with deep learning optimization, PIGs address the limitations of traditional PINNs that rely on MLPs. Our approach dynamically adjusts the positions and shapes of the Gaussians during training, overcoming the fixed parameter constraints of previous methods and enabling more accurate and efficient approximations of complex PDEs. Experimental 10 results demonstrated the superior performance of PIGs across various PDE benchmarks, showcasing their potential as robust tool for solving high-dimensional and nonlinear PDEs. Despite the promising results, our approach has certain limitations that warrant further investigation. Firstly, the dynamic adjustment of Gaussian parameters introduces additional computational overhead. While this improves accuracy, it may also lead to increased training times, particularly for very large-scale problems. However, by leveraging the locality of Gaussians, we can limit the evaluations to nearby Gaussians, which reduces the necessary computations and saves GPU memory. Secondly, the number of Gaussians is fixed at the beginning of training. Ideally, additional Gaussians should be allocated to regions requiring more computational resources to capture more complex solution functions. We believe it is promising research direction and leave it to future work. Finally, complete convergence analysis of the proposed method is not yet available. While empirical results show improved accuracy and efficiency, theoretical understanding of the convergence properties would provide deeper insights and guide further enhancements."
        },
        {
            "title": "REPRODUCIBILITY",
            "content": "We are committed to ensuring the reproducibility of our research. All experimental procedures, data sources, and algorithms used in this study are clearly documented in the paper. We already submitted the codes and command lines to reproduce the part of the results in Table 1 as supplementary materials. The code and datasets will be made publicly available upon publication, allowing others to validate our findings and build upon our work."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "This research adheres to the ethical standards required for scientific inquiry. We have considered the potential societal impacts of our work and have found no clear negative implications. All experiments were conducted in compliance with relevant laws and ethical guidelines, ensuring the integrity of our findings. We are committed to transparency and reproducibility in our research processes."
        },
        {
            "title": "REFERENCES",
            "content": "Martın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available from tensorflow.org. Mark Ainsworth and Tinsley Oden. unified approach to posteriori error estimation using element residual methods. Numerische Mathematik, 65:2350, 1993. Mark Ainsworth and Tinsley Oden. posteriori error estimation in finite element analysis. Computer methods in applied mechanics and engineering, 142(1-2):188, 1997. Jinshuai Bai, Gui-Rong Liu, Ashish Gupta, Laith Alzubaidi, Xi-Qiao Feng, and YuanTong Gu. Physics-informed radial basis network (pirbn): local approximating neural network for solving nonlinear partial differential equations. Computer Methods in Applied Mechanics and Engineering, 415:116290, 2023. Jan-Hendrik Bastek and Dennis Kochmann. Physics-informed neural networks for shell structures. European Journal of Mechanics-A/Solids, 97:104849, 2023. Atilim Gunes Baydin, Barak Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic differentiation in machine learning: survey. Journal of machine learning research, 18(153):143, 2018. 11 Marsha Berger and Joseph Oliger. Adaptive mesh refinement for hyperbolic partial differential equations. Journal of computational Physics, 53(3):484512, 1984. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao JAX: composable transformations of Python+NumPy programs, 2018. URL http: Zhang. //github.com/google/jax. Martin Dietrich Buhmann. Radial basis functions. Acta numerica, 9:138, 2000. Shengze Cai, Zhiping Mao, Zhicheng Wang, Minglang Yin, and George Em Karniadakis. Physicsinformed neural networks (pinns) for fluid mechanics: review. Acta Mechanica Sinica, 37(12): 17271738, 2021. Yadi Cao, Menglei Chai, Minchen Li, and Chenfanfu Jiang. Efficient learning of mesh-based physical simulation with bi-stride multi-scale graph neural network. In International Conference on Machine Learning, pp. 35413558. PMLR, 2023. Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In Computer VisionECCV 2022: 17th European Conference, Tel Aviv, Israel, October 2327, 2022, Proceedings, Part XXXII, pp. 333350. Springer, 2022. Zhang Chen, Zhong Li, Liangchen Song, Lele Chen, Jingyi Yu, Junsong Yuan, and Yi Xu. Neurbf: neural fields representation with adaptive radial basis functions. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 41824194, October 2023. Junwoo Cho, Seungtae Nam, Hyunmo Yang, Seok-Bae Yun, Youngjoon Hong, and Eunbyung Park. Separable physics-informed neural networks. Advances in Neural Information Processing Systems, 36, 2024. George Cybenko. Approximation by superpositions of sigmoidal function. Mathematics of control, signals and systems, 2(4):303314, 1989. Tim De Ryck, Florent Bonnet, Siddhartha Mishra, and Emmanuel de Bezenac. An operator preconditioning perspective on training in physics-informed machine learning. arXiv preprint arXiv:2310.05801, 2023. Tobin Driscoll, Nicholas Hale, and Lloyd Trefethen. Chebfun guide, 2014. Marc Finzi, Andres Potapczynski, Matthew Choptuik, and Andrew Gordon Wilson. stable and scalable method for solving initial value pdes with neural networks. arXiv preprint arXiv:2304.14994, 2023. Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 55015510, 2022. Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, and Angjoo Kanazawa. K-planes: Explicit radiance fields in space, time, and appearance. In CVPR, 2023. Nathan Gaby, Xiaojing Ye, and Haomin Zhou. Neural control of parametric solutions for highdimensional evolution pdes. SIAM Journal on Scientific Computing, 46(2):C155C185, 2024. Zheyuan Hu, Zekun Shi, George Em Karniadakis, and Kenji Kawaguchi. Hutchinson trace estimation for high-dimensional and high-order physics-informed neural networks. Computer Methods in Applied Mechanics and Engineering, 424:116883, 2024a. Zheyuan Hu, Khemraj Shukla, George Em Karniadakis, and Kenji Kawaguchi. Tackling the curse of dimensionality with physics-informed neural networks. Neural Networks, pp. 106369, 2024b. Zheyuan Hu, Zhongqiang Zhang, George Em Karniadakis, and Kenji Kawaguchi. Score-based physics-informed neural networks for high-dimensional fokker-planck equations. arXiv preprint arXiv:2402.07465, 2024c. 12 Xinquan Huang and Tariq Alkhalifah. Efficient physics-informed neural networks using hash encoding. Journal of Computational Physics, 501:112760, 2024. ISSN 0021-9991. doi: https://doi. org/10.1016/j.jcp.2024.112760. URL https://www.sciencedirect.com/science/ article/pii/S0021999124000093. Tak-Wai Hui, Xiaoou Tang, and Chen Change Loy. Liteflownet: lightweight convolutional neural network for optical flow estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 89818989, 2018. Michael Innes. Dont unroll adjoint: Differentiating ssa-form programs. CoRR, abs/1810.07951, 2018. URL http://arxiv.org/abs/1810.07951. Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018. Hojun Jang, Minkwan Kim, Jinseok Bae, and Young Min Kim. Dynamic mesh recovery from partial point cloud sequence. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1507415084, 2023. Namgyu Kang, Byeonghyeon Lee, Youngjoon Hong, Seok-Bae Yun, and Eunbyung Park. Pixel: In Proceedings of the Physics-informed cell representations for fast and accurate pde solvers. AAAI Conference on Artificial Intelligence, volume 37, pp. 81868194, 2023. George Em Karniadakis, Ioannis Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang. Physics-informed machine learning. Nature Reviews Physics, 3(6):422440, 2021. Aly-Khan Kassam and Lloyd Trefethen. Fourth-order time-stepping for stiff pdes. SIAM Journal on Scientific Computing, 26(4):12141233, 2005. Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4), 2023. Arbaaz Khan and David Lowther. Physics informed neural networks for electromagnetic analysis. IEEE Transactions on Magnetics, 58(9):14, 2022. Patrick Kidger and Terry Lyons. Universal approximation with deep narrow networks. In Conference on learning theory, pp. 23062327. PMLR, 2020. Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Aditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert Kirby, and Michael Mahoney. Characterizing possible failure modes in physics-informed neural networks. Advances in Neural Information Processing Systems, 34:2654826560, 2021. Chen Li and Gim Hee Lee. Coarse-to-fine animal pose and shape estimation. Advances in Neural Information Processing Systems, 34:1175711768, 2021. Zhengyi Li, Yanli Wang, Hongsheng Liu, Zidong Wang, and Bin Dong. Solving the boltzmann equation with neural sparse representation. SIAM Journal on Scientific Computing, 46(2):C186 C215, 2024. Dong Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization. Mathematical programming, 45(1):503528, 1989. Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields. Advances in Neural Information Processing Systems, 33:1565115663, 2020. Lu Lu, Xuhui Meng, Zhiping Mao, and George Em Karniadakis. Deepxde: deep learning library for solving differential equations. SIAM review, 63(1):208228, 2021. Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM Transactions on Graphics (ToG), 41(4):115, 2022. 13 Jaemin Oh, Seung Yeon Cho, Seok-Bae Yun, Eunbyung Park, and Youngjoon Hong. Separable physics-informed neural networks for solving the bgk model of the boltzmann equation. arXiv preprint arXiv:2403.06342, 2024. Jooyoung Park and Irwin Sandberg. Universal approximation using radial-basis-function networks. Neural computation, 3(2):246257, 1991. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019. Andrew Pensoneault and Xueyu Zhu. Efficient bayesian physics informed neural networks for inverse problems via ensemble kalman inversion. Journal of Computational Physics, 508:113006, 2024. Charles Qi, Hao Su, Kaichun Mo, and Leonidas Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. 2017. Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua Bengio, and Aaron Courville. On the spectral bias of neural networks. In International conference on machine learning, pp. 53015310. PMLR, 2019. Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Machine learning of linear differential equations using gaussian processes. Journal of Computational Physics, 348:683693, 2017. Maziar Raissi, Paris Perdikaris, and George Karniadakis. Physics-informed neural networks: deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational physics, 378:686707, 2019. Max Rensen, Michael Weinmann, Benno Buschmann, and Elmar Eisemann. Physics-informed gaussian splatting. Masters thesis, 2024. Sirpa Saarinen, Randall Bramley, and George Cybenko. Ill-conditioning in neural network training problems. SIAM Journal on Scientific Computing, 14(3):693714, 1993. Yunchang Seol, Wei-Fan Hu, Yongsam Kim, and Ming-Chih Lai. An immersed boundary method for simulating vesicle dynamics in three dimensions. Journal of Computational Physics, 322: 125141, 2016. Mehdi Shishehbor, Shirin Hosseinmardi, and Ramin Bostanabad. Parametric encoding with attention and convolution mitigate spectral bias of neural partial differential equation solvers. arXiv preprint arXiv:2403.15652, 2024a. Mehdi Shishehbor, Shirin Hosseinmardi, and Ramin Bostanabad. Parametric encoding with attention and convolution mitigate spectral bias of neural partial differential equation solvers. Structural and Multidisciplinary Optimization, 67(7):128, 2024b. Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. Advances in neural information processing systems, 33:74627473, 2020. Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast conIn Proceedings of the IEEE/CVF Conference on vergence for radiance fields reconstruction. Computer Vision and Pattern Recognition, pp. 54595469, 2022. Panos Tamamidis and Dennis Assanis. Evaluation of various high-order-accuracy schemes with and without flux limiters. International Journal for Numerical Methods in Fluids, 16(10):931 948, 1993. Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. Advances in neural information processing systems, 33:75377547, 2020. 14 Haoxiang Wang, Tao Yu, Tianwei Yang, Hui Qiao, and Qionghai Dai. Neural physical simulation with multi-resolution hash grid encoding. Proceedings of the AAAI Conference on Artificial Intelligence, 38(6):54105418, Mar. 2024a. doi: 10.1609/aaai.v38i6.28349. URL https://ojs.aaai.org/index.php/AAAI/article/view/28349. Sifan Wang, Yujun Teng, and Paris Perdikaris. Understanding and mitigating gradient flow pathologies in physics-informed neural networks. SIAM Journal on Scientific Computing, 43(5):A3055 A3081, 2021. Sifan Wang, Xinling Yu, and Paris Perdikaris. When and why pinns fail to train: neural tangent kernel perspective. Journal of Computational Physics, 449:110768, 2022a. Sifan Wang, Shyam Sankaran, Hanwen Wang, and Paris Perdikaris. An experts guide to training physics-informed neural networks. arXiv preprint arXiv:2308.08468, 2023. Sifan Wang, Bowen Li, Yuhan Chen, and Paris Perdikaris. Piratenets: Physics-informed deep learning with residual adaptive networks. arXiv preprint arXiv:2402.00326, 2024b. Sifan Wang, Shyam Sankaran, and Paris Perdikaris. Respecting causality for training physicsinformed neural networks. Computer Methods in Applied Mechanics and Engineering, 421: 116813, 2024c. Yifan Wang, Pengzhan Jin, and Hehu Xie. Tensor neural network and its numerical integration. arXiv preprint arXiv:2207.02754, 2022b. Chenxi Wu, Min Zhu, Qinyang Tan, Yadhu Kartha, and Lu Lu. comprehensive study of nonadaptive and residual-based adaptive sampling for physics-informed neural networks. Computer Methods in Applied Mechanics and Engineering, 403:115671, 2023. Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf: Point-based neural radiance fields. In CVPR, pp. 54385448, 2022. Liu Yang, Xuhui Meng, and George Em Karniadakis. B-pinns: Bayesian physics-informed neural networks for forward and inverse pde problems with noisy data. Journal of Computational Physics, 425:109913, 2021. Yu Yang, Qihong Yang, Yangtao Deng, and Qiaolin He. Mmpde-net and moving sampling physicsinformed neural networks based on moving mesh method. arXiv preprint arXiv:2311.16167, 2023a. Zijiang Yang, Zhongwei Qiu, and Dongmei Fu. Dmis: Dynamic mesh-based importance sampling for training physics-informed neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 53755383, 2023b. Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. Plenoctrees for In Proceedings of the IEEE/CVF International real-time rendering of neural radiance fields. Conference on Computer Vision, pp. 57525761, 2021. Bing Yu et al. The deep ritz method: deep learning-based numerical algorithm for solving variational problems. Communications in Mathematics and Statistics, 6(1):112, 2018. Qiang Zhang, Seung-Hwan Baek, Szymon Rusinkiewicz, and Felix Heide. Differentiable pointbased radiance fields for efficient view synthesis. arXiv preprint arXiv:2205.14330, 2022."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 ENHANCED GAUSSIAN FEATURE EMBEDDING To enhance the expressive capability, different Gaussians can be used for each feature dimension. The learnable Gaussian feature embedding FE(x; ϕ) : Rd Rk and the set of Gaussian model parameters ϕ = {(µi, Σi, fi) : = 1, . . . , } are defined as previously described. Then, we have 15 different Gaussians for each feature dimension, where µi Rkd is the Gaussian position parameters, µi,j Rd denotes the position parameter for j-th feature dimension, and fi,j represents j-th feature value. Similarly, Σi,j Sd ++ is covariance matrix for j-th feature dimension. Given an input coordinate Rd, j-th element of the learnable Gaussian feature embedding FEj(x; ϕ) is defined as follows, FEj(x; ϕ) = (cid:88) i=1 fi,jGi,j(x), Gi,j(x) = 1 2 (xµi,j )Σ1 i,j (xµi,j ), (10) where Gi,j is the Gaussian function using Gaussians parameters for j-th feature dimension. A.2 DETAILED DESCRIPTION OF EXPERIMENTS A.2.1 (1+1)D ALLEN-CAHN EQUATION The Allen-Cahn equation is one-dimensional time-dependent reaction-diffusion equation that describes the evolutionary process of phase separation, which reads ut 0.0001uxx + 5u3 5u = 0, (x, t) [1, 1] [0, 1], with the periodic boundary condition u(1, t) = u(1, t), ux(1, t) = ux(1, t). (11) (12) The initial condition for the experiment was u(x, 0) = x2cos(πx). We used the NTK-based loss balancing scheme (Wang et al., 2022a) to mitigate the ill-conditioned spectrum of the neural tangent kernel (Jacot et al., 2018). We used = 4000 Gaussians for training and diagonal covariance matrix for parameter efficiency, where the diagonal elements of the initial Σ were set to constant value of 0.025. The µi was uniformly initialized following Uniform[0, 2]2. We used shallow MLP with one hidden layer with 16 hidden units, and the dimension of the Gaussian feature was = 1. Reference solution was generated by Chebfun (Driscoll et al., 2014), which utilizes the Fourier collocation method with = 4096 Fourier modes with ETDRK4 time stepping (Kassam & Trefethen, 2005) with fixed time step = 1/200. A.2.2 2D HELMHOLTZ EQUATION The Helmholtz equation is the eigenvalue problem of the Laplace operator = 2. We consider the manufactured solution u(x, y) = sin(a1πx) sin(a2πy), (a1, a2) = (4, 1), (13) to the two-dimensional Helmholtz equation with the homogeneous Dirichlet boundary condition given by + k2u = q, (x, y) [1, 1]2, = 1, (14) where q(x, y) = k2 sin (a1πx) sin (a2πy) (a1π)2 sin (a1πx) sin (a2πy) (a2π)2 sin (a1πx) sin (a2πy) (15) can be extracted from the solution u. We used = 3000 Gaussians in this experiment. The weights and scales of Gaussians were initialized following Uniform[1, 1] and 0.1, respectively. The feature size of Gaussians was fixed at 4. The shallow MLP has 16 hidden nodes, and its network parameters were initialized by Glorot normal. The inputs for the Gaussians were rescaled into [0, 1]2, therefore the positions were initialized following Uniform[0, 1]2. A.2.3 (2+1)D KLEIN-GORDON EQUATION The Klein-Gordon equation is relativistic wave equation, which predicts the behavior of particle at high energies. We consider the manufactured solution u(x, y, t) = (x + y) cos(2t) + xy sin(2t) (16) 16 to the (2+1) dimensional inhomogeneous Klein-Gordon equation utt + u2 = f, (x, y, t) [1, 1]2 [0, 10], (17) where the forcing , initial condition, and Dirichlet boundary condition are extracted from the manufactured solution u. In this experiment, we employed = 100 Gaussians and shallow MLP whose input dimension is 4 and hidden layer size is 16. The network parameters for the shallow MLP were initialized by Glorot Normal. Every weight of Gaussian was initialized following Normal(0, 0.012). The scale parameter σis were initialized with constant value of 0.5. Instead of direct usage of the computational domain [1, 1]2 [0, 10], we used linearly rescaled values [0, 2]3 for the inputs of Gaussians. Accordingly, position parameters of Gaussians were initialized following Uniform[0, 2]3. A.2.4 (2+1)D FLOW-MIXING PROBLEM mixing procedure of two fluids in two-dimensional spatial domain could be described in the following equation a(x, y) = (x, y, t) [4, 4]2 [0, 4], ut + aux + buy = 0, vt vt,max vt = sech2(r)tanh(r), x2 + y2, (21) The analytic solution is u(x, y, t) = tanh (cid:0) 2 sin(wt)(cid:1), where = vt/(rvt,max); see e.g., Tamamidis & Assanis (1993). The initial condition can be extracted from the analytic solution. 2 cos(wt) vt,max = 0.385. vt vt,max b(x, y) = (18) (20) (19) = (cid:112) , , To predict the solution to the PDE, we used = 4000 Gaussians. The weights and scales were initialized to Normal(0, 0.012) and 0.1, respectively. The size of Gaussian features was fixed at 4. MLP had 16 hidden nodes, and its parameters were initialized by Glorot normal. Inputs for the Gaussians were rescaled to [0, 2]3, hence the positions of Gaussians were initialized following Uniform[0, 2]3. A.2.5 (2+1)D NONLINEAR DIFFUSION EQUATION The diffusion equation is parabolic PDE describing the diffusion process of physical quantity, such as heat. We consider nonlinear diffusion equation for our benchmark, which reads ut = 0.05 (cid:0)u2 + uu(cid:1) , (cid:19) (cid:18) (cid:18) (x, y, t) [1, 1]2 [0, 1], (cid:18) (cid:19) 1 10 1 u0(x) = 0.25g x; 0.2, 0.3, + 0.4g x; 0.1, 0.5, + 0.3g x; 0.5, 0, (22) (cid:19) , 1 20 where = (x, y) and g(x, y; a, b, σ) = (xa)2+(yb) σ2 . There are three peaks at the initial time and the peaks spread out as time goes on. We employed = 4000 Gaussians. The weights and scales of Gaussians were initialized to Normal(0, 0.012) and 0.1, respectively. The size of Gaussian features was 4. The hyperbolic tangent MLP had only single hidden layer with 16 nodes, and its parameters were initialized by Glorot normal. The inputs for the Gaussians were rescaled into [0, 1]3. Correspondingly, the positions of Gaussians were initialized following Uniform[0, 1]3. A.3 ADDITIONAL EXPERIMENTS Here, we compare PIGs to PIRBNs (Bai et al., 2023). Two equations in the PIRBN paper are chosen as benchmarks. Equation (15) in Bai et al. (2023): 2 x2 u(x 100) 4µ2π2 sin(2µπ(x 100)) = 0, (23) 17 and u(100) = u(101) = 0. The exact solution is u(x) = sin(2µπ(x 100)). We considered µ = 4. Equation (30) in Bai et al. (2023): 2 x2 u(x) = 2π(22 x) cos(2πx) + 0.5 sin(2πx) π2(22 x)2 sin(2πx) + 16π(x 20) cos(16πx) + 0.5 sin(16πx) 64π2(x 20)2 sin(16πx), (24) and u(20) = u(22) = 0. The exact solution is u(x) = (cid:0) 22x Referring to the numbers in 6, PIGs achieved error levels by two orders of magnitude smaller than PIRBNs. This improvement could be attributed to the introduction of tiny MLP and letting positions move during training. sin(2πx) + (cid:0) x20 sin(16πx). (cid:1)2 (cid:1)2"
        },
        {
            "title": "PIRBNs\nPIGs",
            "content": "Equation 23 6.87e-03 3.70e-04 1.79e-05 3.80e-06 Equation 24 1.47e-02 9.16e-03 1.14e-04 1.19e-05 Table 6: Results of the comparison study between PIGs and PIRBNs for Equations 23 and 24. PIGs achieve lower errors than PIRBNs, highlighting their superior performance in both equations. A.4 SEPARABLE PIGS Separable PINNs have shown excellent performance across various PDEs (Cho et al., 2024; Oh et al., 2024). When mesh points are tensor products of 1D grids, the number of network forward passes of SPINNs scale linearly O(N d), in contrast to the exponential scaling O(N d) of traditional PINNs, which adopt single MLP. Here, we provide proof-of-concept for combining SPINNs and PIGs. Separable PIGs (SPIGs) might have the following form: u(x1, . . . , xd) (cid:88) (cid:89) PIGr(xi; θi) (25) i=1 where PIGr is the r-th component of the output vector. r=1 Figure 8: Klein-Gordon equationA.2.3. The relative L2 error of SPIG is 3.68 104. 2D L-shaped Poisson equation the two-dimensional Poisson equation defined on an L-shaped domain. Despite the non-tensor-product nature of the computational domain, SPINNs can deal with such complex domains by masking outputs. Please refer to (Cho et al., 2024) for the description of this benchmark problem. SPIG achieved 1.89 102 relative L2 error for this problem, while SPINN solution was 3.22 102. (2+1)D Klein-Gordon equation SPIG achieved 3.68 104 relative L2 error. PIGs best relative L2 error was 2.36 103. Please refer to A.2.3 for description of PDE. SPIG used modified MLP with 2 layer and 16 hidden features. The weights and scales were initialized to Normal(0, 0.012) 18 and 0.1, respectively. position parameters of Gaussians were initialized following Uniform[0, 2]3. 2500 Gaussians are used. (3+1)D Klein-Gordon equation SPIG achieved 2.88 104 relative L2 error. SPINNs relative L2 error was 1.20 103. Please refer to (Cho et al., 2024) for the description of this benchmark problem. SPIG used modified MLP with 2 layer and 16 hidden features. The weights and scales were initialized to Normal(0, 0.012) and 0.25, respectively. position parameters of Gaussians were initialized following Uniform[0, 2]3. 2500 Gaussians are used. Figure 9: 3D Helmholtz equation A.4. The relative L2 error of SPIG is 1.50 103. 3D Helmholtz equation SPIG achieved 1.50 103 relative L2 error. SPINNs relative L2 error was 3.00 102. Please refer to (Cho et al., 2024) for the description of this benchmark problem. SPIG used modified MLP with 2 layers and 16 hidden features. The weights and scales were initialized to Normal(0, 0.012) and 0.05, respectively. position parameters of Gaussians were initialized following Uniform[0, 2]3. 2500 Gaussians are used. A.5 INVERSE PROBLEM With observation data, the PINN framework can estimate unknown equation parameters by letting them be learnable. Here we consider (1+1)D Allen-Cahn equation ut 104uxx + λu3 5u = 0, with an unknown coefficient λ. Other conditions are the same with Section A.2.1. We estimated λ using reference solution as observation data. Figure 10 presents estimated λ over iterations, clearly showing PIGs faster convergence. Figure 10: Allen-Cahn Inverse problem. The experiment was conducted on five different seeds (100, 200, 300, 400, 500). PIG showed better performance than PINN. 19 A.6 HIGH-DIMENSIONAL EQUATIONS Hu et al. introduced stochasticity in the dimension during the gradient descent (SDGD) to efficiently handle high-dimensional PDEs within the PINN framework Hu et al. (2024b). PIGs can utilize SDGD to tackle extremely high dimensional PDEs, e.g., 100D Allen-Cahn, and Poisson equation. Specifically, let = 100 and Bd = {x Rd : x2 1} be the domain. We consider function uexact = (cid:0)1 x2 (cid:1) (cid:32)d1 (cid:88) i=1 ci sin (xi + cos(xi+1) + xi+1 cos(xi)) , (cid:33) as our exact solution, where ci Normal(0, 12). Our benchmark problems are the Poisson equation and the Allen-Cahn equation, which read = (Poisson) and + u3 = (Allen-Cahn) where is induced from the exact solution. Figure 11 presents relative L2 error curves over iterations. Note that global polynomial-based methods cannot handle such high dimensional equations due to the curse of dimensionality. Figure 11: Relative L2 error curves for two high dimensional PDEs. Left: 100D Allen-Cahn equation. Right: 100D Poisson equation. PIGs achieved 8.88 103, and 8.42 103, respectively. A.7 LID-DRIVEN CAVITY To further illustrate the effectiveness of PIGs over traditional parametric mesh methods, we chose PGCAN Shishehbor et al. (2024b) as our baseline and considered the lid-driven cavity problem presented in the paper. The domain is [0, 1]2. The homogeneous Dirichlet boundary condition is imposed except for the lid {(x, 1) : [0, 1]}. The governing equation is 2D stationary incompressible Naiver-Stokes equation, = 0 ρ(u )u = + µ2u where the boundary conditions are given as follows: u(0, y) = u(1, y) = (0, 0), u(x, 0) = (0, 0), u(x, 1) = (A sin(πx), 0), = 1, p(0, 0) = 0. We used 2000 Gaussians. Covariance matrices were diagonal and initialized at 0.1 and positions were initialized following Uniform[0, 2]. Figure 12 depicts numerical results. PIG shows excellent agreement with the reference solution. Figure 13 illustrates faster convergence of PIGs compared to the baseline method PGCAN. 20 Figure 12: Lid-driven cavity flow problem. PIG achieved 4.04 104 relative L2 error whereas the baseline parametric grid method PGCAN resulted in 1.22 103. A.8 EXAMPLE FOR SPECTRAL BIAS Figure 14 illustrates PIGs ability to approximate high-frequency functions. We considered 2D Helmholtz equation (see Section A.2.2) with high wavenumber (a1, a2) = (10, 10) for benchmark problem. A.9 THE HISTOGRAM OF VARIANCES AND DISTANCES OF GAUSSIANS Figure 15 shows the histograms of the Gaussian parameters for the two benchmark problems discussed in Section 4.2.5 and Section 4.2.3. Readers may observe that the Gaussians in the right panels are more global and, therefore, more sparsely distributed compared to those in the left panels. A.10 COMPARISON BETWEEN PIG AND SIREN In this section, we compare the performance of PIG with SIREN Sitzmann et al. (2020). PIG is composed of feature embedding FEϕ and lightweight neural network NNθ. Here, we investigate the effectiveness of SIREN when used either as feature embedding or as lightweight neural network. When used as FEϕ, SIREN is implemented as an MLP with 4 layers, each containing 256 units, and sin(3x) as the activation function. As NNθ, SIREN is shallow MLP with 16 units and sin(30x) 21 Figure 13: Relative L2 error curve of the lid-driven cavity problem. PIG achieved 4.04 104 and PGCAN which used the parametric grid method achieved 1.22 103. Figure 14: 2D Helmholtz equation with high wavenumber (a1, a2) = (10, 10). PIG achieved relative L2 error of 7.09 103, while the parametric fixed grid method PIXEL reached relative L2 error of 7.47 102. PINN failed to converge. activation function. It is worth noting that using sin(30x) as the activation function for the feature embedding FEϕ did not yield effective results. FEϕ + NNθ Helmholtz 1.68e-03 2.02e-03 SIREN + Id 1.31e-03 8.26e-04 SIREN + tanh PIG + SIREN 1.37e-05 1.64e-06 4.13e-05 2.59e-05 PIG + tanh Flow-Mixing 1.22e-02 4.17e-03 2.80e-02 2.50e-02 1.28e-03 1.09e-04 4.51e-04 1.74e-04 Klein-Gordon 1.18e-01 4.88e-02 1.04e-01 8.61e-02 2.37e-02 4.62e-03 2.76e-03 4.27e-04 Table 7: Comparison of PIG and SIREN performance. For all cases except the Helmholtz equation, the original PIG + tanh formulation outperformed other methods. The improved performance of PIG + SIREN on the Helmholtz equation may be attributed to the functional form of its exact solution. The results, summarized in Table 7, indicate that SIREN as FEϕ did not perform notably well. However, when SIREN was employed as NNθ, it demonstrated excellent performance in solving the Helmholtz equation discussed in Section 4.2.2. This improvement is likely due to the structural similarity between the SIREN activation and the functional form of the exact solution (equation 13). 22 Figure 15: Histograms of the Gaussian parameters for the flow-mixing equation and the KleinGordon equation. The upper panels display histograms of the minimum distances between the Gaussian centers, where distances > 0 indicate the absence of mode collapse. The lower panels show histograms of the Gaussian variances, highlighting the non-degeneracy of the Gaussians. A.11 COMPARISON WITH CONCURRENT WORK We conducted several experiments to compare PIG with the Physics-Informed Gaussian Splatting (PI-GS) method proposed by Rensen et al. (2024). Table 8 summarizes the relative L2 errors and computation times per iteration (shown in parentheses). Across the three benchmark problems, PIG consistently outperforms PI-GS in both accuracy and efficiency. PIG PI-GS Burgers equation (1) 7.68 104 (0.28s/it) 1.62 101 (1.5s/it) Burgers equation (2) 1.08 103 (0.29s/it) 2.61 101 (1.68s/it) Table 8: Performance comparison of PIG and PI-GS across 3 benchmark problems. Results include relative L2 errors and computation times per iteration (s/it). Benchmarks are conducted on two variations of the (2+1)D Burgers equation. Across all experiments, We considered [2.5, 2.5]2 as spatial domain with zero Dirichlet boundary condition. (2+1)D Burgers Equation For ν = 1/(10π), the (2+1)D Burgers equation is given by: t + x = ν (cid:18) 2u x2 + (cid:19) , 2u y2 where is scalar-valued function. 23 In the first example, the initial condition is set as the probability density function (PDF) of standard two-dimensional Normal distribution. In the second example, the initial condition is the PDF of mixture of two Gaussians. Figures 16 and 17 illustrate the reference solutions and the corresponding PIG solutions. Figure 16: Prediction results of PIG for the first example of the (2+1)D Burgers equation. PIG achieved relative L2 error of 7.68 104, with computation time of 0.28 seconds per iteration. In contrast, PI-GS attained relative L2 error of 1.62 101, requiring 1.50 seconds per iteration. A.12 ADDITIONAL FIGURES 24 Figure 17: Prediction results of PIG for the second example of the (2+1)D Burgers equation. PIG achieved relative L2 error of 1.08103, with computation time of 0.29 seconds per iteration. In comparison, PI-GS attained relative L2 error of 2.61 101, requiring 1.68 seconds per iteration. Figure 18: Non-linear diffusion equation 4.2.4. The experiment was conducted on three different seeds (100, 200, 300). The best relative L2 error is 1.44 103. 25 Figure 19: Flow mixing equation 4.2.5. The experiment was conducted on three different seeds (100, 200, 300). The best relative L2 error is 2.67 104."
        }
    ],
    "affiliations": [
        "Department of Artificial Intelligence, Sungkyunkwan University",
        "Department of Electrical and Computer Engineering, Sungkyunkwan University",
        "Department of Mathematical Sciences, KAIST"
    ]
}