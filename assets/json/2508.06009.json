{
    "paper_title": "MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models",
    "authors": [
        "Jun Feng",
        "Zixin Wang",
        "Zhentao Zhang",
        "Yue Guo",
        "Zhihan Zhou",
        "Xiuyi Chen",
        "Zhenyang Li",
        "Dawei Yin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in visual mathematical reasoning across various existing benchmarks. However, these benchmarks are predominantly based on clean or processed multimodal inputs, without incorporating the images provided by real-world Kindergarten through 12th grade (K-12) educational users. To address this gap, we introduce MathReal, a meticulously curated dataset comprising 2,000 mathematical questions with images captured by handheld mobile devices in authentic scenarios. Each question is an image, containing the question text and visual element. We systematically classify the real images into three primary categories: image quality degradation, perspective variation, and irrelevant content interference, which are further delineated into 14 subcategories. Additionally, MathReal spans five core knowledge and ability categories, which encompass three question types and are divided into three difficulty levels. To comprehensively evaluate the multimodal mathematical reasoning abilities of state-of-the-art MLLMs in real-world scenarios, we design six experimental settings that enable a systematic analysis of their performance. Through extensive experimentation, we find that the problem-solving abilities of existing MLLMs are significantly challenged in realistic educational contexts. Based on this, we conduct a thorough analysis of their performance and error patterns, providing insights into their recognition, comprehension, and reasoning capabilities, and outlining directions for future improvements. Data and code: https://github.com/junfeng0288/MathReal."
        },
        {
            "title": "Start",
            "content": "MATHREAL: We Keep It Real! Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models Jun Feng*1, Zixin Wang*2, Zhentao Zhang3, Yue Guo4, Zhihan Zhou5, Xiuyi Chen1, Zhenyang Li1, Dawei Yin1 1Baidu Inc., Beijing, China 2Nanyang Technological University, Singapore 3Xiaopeng Motors, China 4Gaoling School of Artificial Intelligence, Renmin University of China 5Beihang University, Beijing, China junfeng0288@gmail.com, zixin006@e.ntu.edu.sg, chenxiuyi2017@gmail.com 5 2 0 2 8 ] . [ 1 9 0 0 6 0 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in visual mathematical reasoning across various existing benchmarks. However, these benchmarks are predominantly based on clean or processed multimodal inputs, without incorporating the images provided by real-world Kindergarten through 12th grade (K12) educational users. To address this gap, we introduce MATHREAL, meticulously curated dataset comprising 2,000 mathematical questions with images captured by handheld mobile devices in authentic scenarios. Each question is an image, containing the question text and visual element. We systematically classify the real images into three primary categories: image quality degradation, perspective variation, and irrelevant content interference, which are further delineated into 14 subcategories. Additionally, MATHREAL spans five core knowledge and ability categories, which encompass three question types and are divided into three difficulty levels. To comprehensively evaluate the multimodal mathematical reasoning abilities of state-of-the-art MLLMs in real-world scenarios, we design six experimental settings that enable systematic analysis of their performance. Through extensive experimentation, we find that the problem-solving abilities of existing MLLMs are significantly challenged in realistic educational contexts. Based on this, we conduct thorough analysis of their performance and error patterns, providing insights into their recognition, comprehension, and reasoning capabilities, and outlining directions for future improvements. Data and code: https://github.com/junfeng0288/MathReal. Introduction Recent advances in Large Language Models (LLMs) have catalyzed the development of MLLMs, which are capable of jointly interpreting visual and textual information. This evolution has substantially enhanced model performance across broad range of multimodal understanding tasks, including visual question answering, diagram interpretation, document analysis, and mathematical reasoning. As MLLMs become increasingly adept at bridging text and vision, their reasoning capabilities, particularly in domains requiring pre- *These authors contributed equally. Corresponding author. cise symbol processing and structured logic, have drawn significant attention from the research community. With the rapid development of reasoning models, an increasing number of mathematical reasoning benchmarks have been proposed, including both pure-text benchmarks and multimodal benchmarks. Pure-text mathematical reasoning benchmarks, such as AIME24 (Ankner et al. 2024), AIME25 (Jaech et al. 2024), OlympiadBench (He et al. 2024), and Polymath (Wang et al. 2025e), primarily focus on evaluating reasoning ability from textual question statements. More recently, multimodal benchmarks have been introduced to incorporate visual contexts, such as MathVista (Lu et al. 2023), MathVerse (Zhang et al. 2024b), TrustGeoGen (Fu et al. 2025), MM-MATH (Sun et al. 2024), MathVision (Awais et al. 2024), LogicVista (Xiao et al. 2024), DynaMath (Zou et al. 2024), VisOnlyQA (Kamoi et al. 2024), MathGlance (Sun et al. 2025), VisioMath (Li et al. 2025), MV-MATH (Wang et al. 2025b), GeoEval (Zhang et al. 2024a), and We-Math (Qiao et al. 2024). These benchmarks provide diverse evaluation settings that test not only pure symbolic reasoning but also multimodal perception and reasoning, thereby driving progress in the development of more general and robust MLLMs. Despite these advancements, the majority of existing multimodal math benchmarks consist of clean or post-processed images, which rarely account for cases encountered by realworld users, making it difficult to assess how multimodal models perform in real environments. For instance, K12 users often capture textbook pages or homework questions using handheld mobile devices to ask models for help. Realworld scenarios are often more challenging than traditional clean image inputs and the entire question text is embedded within the image, unlike conventional benchmarks that frequently rely on textual inputs. Additionally, mathematical question images captured by real-world users often reflect distribution that differs substantially from both prior multimodal math benchmarks and the training data of existing models, as they are embedded in authentic educational contexts and aligned with real user needs, thereby posing joint challenges for both perception and reasoning. To bridge this gap, we introduce MATHREAL, novel benchmark designed to assess the performance of MLLMs Figure 1: Sampled MATHREAL examples from each question type. Each question contains real image and annotated information. on real-world, visually grounded K12 mathematical questions. To support this, we develop comprehensive data construction pipeline tailored to real-world multimodal math questions, addressing the challenges of collection, annotation, and validation under realistic conditions. MATHREAL comprises 2,000 high-quality questions sourced from authentic educational contexts, each captured via mobile photography as an image containing figure, requiring models to first perceive visual content before performing reasoning. We define three primary challenges commonly encountered in real-world K12 educational scenarios: image quality degradation, perspective variation, and irrelevant content interference, which are further divided into 14 finegrained subcategories, such as blur, rotation, handwritten answers, etc. To evaluate the multimodal mathematical reasoning abilities of MLLMs under real-world conditions, we construct MATHREAL with carefully designed annotations. Every question image spans five core knowledge and ability categories, three question types, and three difficulty levels. The dataset includes three question types and is systematically categorized across three difficulty levels and five knowledge domains, such as geometry, algebra, statistics, logical reasoning, and function graphs. To ensure high-quality and consistent annotations, each question is independently verified by at least two expert annotators, and is enriched with precise ground-truth metadata, including the ground-truth question text, detailed descriptions of visual elements, and correct answers. We conduct extensive evaluations on MATHREAL across 4 LLMs and 40 multimodal models. Even in relatively simple K12 scenarios, the best-performing model Doubao-1.5thinking-vision-pro attains only 53.9% accuracy, in sharp contrast to the near-human or competition-level performance often reported on established mathematical benchmarks, underscoring the substantial gap to real-world applicability and the necessity of MATHREAL grounded in authentic educational scenarios. In conclusion, the contributions of this paper are summarized as follows: We propose MATHREAL, the first real-world benchmark of 2,000 K12 multimodal math questions photographed in natural settings, covering 3 systematic characterizations of real-world scenarios, 5 knowledge and ability categories, 3 question types, and 3 difficulty levels. We evaluate 40 MLLMs under 6 experimental settings to assess their reasoning abilities under real-world conditions. Our results demonstrate notable performance gap between real and clean images, indicating that existing MLLMs remain far from reliable when applied in realworld educational scenarios. Through controlled experiments, we demonstrate that visual conditions commonly encountered in real-world scenarios, such as blur, rotation, and handwritten answers, significantly impair the reasoning performance of current MLLMs. In contrast, these models achieve notably higher accuracy when provided with clean textual or visual inputs, indicating that their visual perception components remain fragile when exposed to realistic distortions. MATHREAL While MLLMs have shown strong performance on existing visual math benchmarks, these benchmarks predominantly feature clean inputs and rarely reflect usage in real-world educational scenarios. This is particularly relevant because MLLMs have the potential to explain solutions and evaluate answer correctness in real educational settings. To bridge this gap, we present MATHREAL, benchmark grounded in naturally captured images and designed to evaluate MLLMs under realistic visual conditions. Real Visual Math Dataset Dataset Overview MATHREAL comprises 2,000 math question instances, each represented as noisy image captured via handheld mobile devices under real conditions. All Statistic Number Total questions - Multiple-Choice Questions - Fill-in-the-Blank Questions - Constructed-Response Questions Questions in the testmini set Elementary-level Questions Middle School-level Questions High School-level Questions Questions with only real images Questions with real images and clean images Questions with single figure Questions with multiple figures Questions with single sub-question Questions with multiple sub-questions Minimum question length Maximum question length Average question length Average answer length 2000 104 475 1421 480 779 883 338 745 1255 1296 704 829 1171 7 451 122.03 27.25 Table 1: Key Statistics of MATHREAL.The unit of question length is words. images are sourced from authentic K12 educational materials, including textbooks, exam papers, and printed exercises. The photographs reflect wide range of real-world acquisition scenarios, encompassing three major categories of noise: image quality degradation, image perspective variation, and handwriting interference. These three categories are further divided into total of 14 fine-grained subtypes, providing rich taxonomy of real-world imperfections. This collection process intentionally preserves the complexity and imperfection inherent to mobile-based image capture in practical settings. Each sample in MATHREAL is an image that contains complete math question, with both the question text and the associated figures embedded within the image rather than provided as separate clean inputs. The dataset includes 1,296 questions with single figure and 704 questions with multiple figures. It also includes 829 questions with single subquestion and 1,171 with multiple sub-questions, providing diverse reasoning structures. All questions are manually annotated with three supplementary elements: the ground-truth question text (QG), an exact visual description of the figure present in the image (DG), and the correct reference answer. The purpose of these annotations is to enable systematic analysis of models multimodal perception and reasoning abilities in real-world scenarios. The dataset includes three types of questions: multiplechoice, fill-in-the-blank, and constructed-response. In terms of academic stage, questions are distributed across three educational stages: primary school, middle school, and high school, ensuring coverage of content across the K12 spectrum. Additionally, 745 questions are accompanied only by real images, while 1,255 are paired with both real images Figure 2: Performance comparison of six MLLMs on five categories and overall average accuracy. The radar chart shows results under two evaluation standards: strict accuracy (Accstr) and loose accuracy (Acc), symmetrically arranged across 12 axes. and clean images, which exclude real-world artifacts. The dataset also includes testmini subset of 480 questions. Detailed statistics on question types and visual content categories are summarized in Table 1. Data Collection Process We construct the dataset by sampling 1.5 million photographed math questions from largescale user-uploaded repository. two-stage filtering process is applied to ensure quality and relevance. First, domain-specific classifier selects math-related samples containing figures. Then, GPT-4o, Doubao-1.5-vision-pro-32k, and Qwen2.5-VL-Instruct-72B independently evaluate each image to determine whether it contains single, complete question and whether the figure is essential. Samples with irrelevant visuals or dialogue-style formats are excluded. Only those approved by all three models are retained, resulting in high-quality dataset for evaluating the visual reasoning capabilities of MLLMs. Data Annotation Process We build Gradio-based platform and organize the annotation into three fully manual stages. In Stage One, we filter out samples that do not meet benchmark criteria, such as incomplete questions, multiplequestion images, or irrelevant figures. In Stage Two, we annotate image conditions according to predefined taxonomy covering three major real-world scenario types. In Stage Three, we annotate question-level metadata, including question content, type, educational stage, knowledge category, figure descriptions, and ground truth answers. In the end, we conduct fully human-verified process to ensure that the final dataset reflects diverse real-world conditions while maintaining high semantic and structural quality for evaluating multimodal models. Data Characteristics In contrast to other MLLMs math reasoning datasets, the unique characteristics of MATHREAL are summarized as Figure 4: Acc of five models under different input settings. handwritten process for constructed-response questions. Detailed annotations are provided for each subtype."
        },
        {
            "title": "Experiments",
            "content": "Experimental Setup Data Preparation and Subset Division The MATHREAL dataset contains 2,000 questions. To enable faster evaluation and model development validation, we divide the dataset into two subsets: testmini and test. The testmini subset includes 480 questions and serves as validation set for model development or for users with limited computational resources. The test subset consists of the remaining 1,520 questions and functions as the standard evaluation set. We use stratified random sampling strategy across different categories, ensuring that the sample sizes within each stratum are proportional to those in the full dataset, thus maintaining statistical representativeness. In the experiments that follow, all quantitative results are reported using the testmini subset of MATHREAL. Experimental Settings To evaluate the reasoning capability of MLLMs in real-world, image-based mathematical questions, we design six experimental settings that progressively disentangle visual perception and reasoning. Each question is an image containing both textual content (the question) and visual elements (the figure, which can be represented by textual description). Based on this, three primary input modalities are defined: image only (I), which serves as the primary evaluation; image with humanannotated question text (I+QG); and image with humanannotated question text and figure description (I+QG+DG). Two reasoning paradigms are considered: one-stage approach, where the model performs question recognition and reasoning jointly from the raw image (IUER), and twostage approach, where the model first generates intermediate representationsmodel-generated question text (QM) and figure description (DM)followed by reasoning (I+QM and I+QM+DM). This framework enables systematic analysis of perception and reasoning under realistic conditions. Figure 3: The flowchart of data construction, including data filtering and manual annotation. vision-only input and in-the-wild challenges. These two features better align with the data distribution in real educational scenarios and pose distinct challenges to the perception and reasoning capabilities of MLLMs. Vision-Only Input In real educational scenarios, all information necessary for solving mathematical questions, including the question statement, figures, or diagrams, is typically contained within single image. This requires models to first perceive and extract key information from the image before proceeding to reason and solve the question. Correspondingly, MATHREAL uses single raw image as the sole input. However, to decouple perception and reasoning, the dataset provides QG and DG as supplementary annotations , facilitating fine-grained evaluation of MLLMs capabilities. In-the-Wild Challenges In real educational scenarios, raw images often contain substantial noise due to unconstrained capture conditions. This challenges models to robustly perceive critical content while ignoring non-essential artifacts. To reflect this realism, MATHREAL categorizes noise into three major categories, encompassing 14 finegrained subtypes. Specifically, image quality degradation includes blur, underexposure/overexposure, shadow coverage, and glare; image perspective variation includes rotation, inplane tilt, non-planar capture, and background distortion; irrelevant content interference includes handwritten questions, reverse side content, question marking, figure marking, handwritten answer for multiple-choice questions, and Model Accstr Acc PG SG LR FG SC Avg PG SG LR FG SC Avg LLMs (Question Text + Figure Description, CoT with 0-shot) Qwen3-235B-A22B-thinking DeepSeek-V3 Qwen3-235B-A22B-instruct DeepSeek-R1 29.1 27.5 34.0 42. 30.6 31.5 33.3 36.9 41.3 34.8 37.0 41.3 20.9 27.9 39.5 30.2 48.5 57.6 45.5 57.6 31.2 31.2 35.4 41.2 Grok-4 Claude-sonnet-4 Claude-sonnet-4-thinking GPT-4.1 GPT-4o Qwen-VL-Max o4-mini o3 Doubao-1.5-vision-pro-32k Doubao-seed-1.6-thinking Gemini-2.5-flash-thinking Gemini-2.5-pro-thinking Doubao-seed-1.6 Doubao-1.5-thinking-vision-pro Gemma-3-4b-it Gemma-3n-E4B Gemma-3-27b-it Kimi-VL-A3B-Instruct Qwen2.5-VL-7B-Instruct InternVL3-8B InternVL3-14B Llama-4-Maverick InternVL3-78B Qwen2.5-VL-32B-Instruct InternVL3-38B GLM-4.1v-thinking-flashx Qwen2.5-VL-72B ERNIE-4.5-Turbo-VL-Preview Keye-VL-8B-Preview OVR Revisual-R1 Skywork-R1V3-38B OpenVLThinker ThinkLite-VL VLAA-Thinker-Qwen2.5VL-7B WeThink MMR1-Math-v0-7B MM-Eureka MiMo-VL-7B-RL VL-Rethinker-7B Closed Models (Image-only, CoT with 0-shot) 5.7 7.3 10.9 12.1 13.4 10.5 26.3 27.1 27.5 36.8 42.9 40.1 40.9 43.3 2.7 7.2 7.2 14.4 14.4 13.5 23.4 29.7 27.9 27.0 36.9 41.4 37.8 43.2 0.0 8.7 10.9 13.0 13.0 10.9 21.7 15.2 19.6 17.4 21.7 39.1 32.6 26. 0.0 4.7 9.3 9.3 11.6 16.3 18.6 14.0 20.9 39.5 41.9 39.5 37.2 32.6 0.0 15.2 15.2 30.3 15.2 30.3 27.3 36.4 27.3 30.3 48.5 48.5 48.5 48.5 3.5 7.7 10.2 13.8 13.5 13.1 24.6 26.0 26.2 32.5 39.8 40.8 39.6 41.0 Open-source MLLMs (Image-only, CoT with 0-shot) 1.2 2.4 4.5 3.6 4.0 8.5 7.7 11.3 7.7 8.9 10.1 14.2 12.6 18.2 3.2 2.8 6.1 7.3 5.3 6.1 5.7 6.9 8.9 6.1 15.4 10. 1.8 2.7 4.5 10.8 9.0 10.8 14.4 10.8 15.3 13.5 16.2 12.6 17.1 13.5 2.2 4.3 2.2 0.0 13.0 4.3 8.7 13.0 15.2 13.0 8.7 8.7 10.9 13.0 0.0 7.0 2.3 9.3 4.7 9.3 4.7 9.3 11.6 18.6 11.6 9.3 16.3 16.3 0.0 6.1 6.1 0.0 6.1 12.1 21.2 6.1 15.2 30.3 24.2 18.2 18.2 27.3 1.2 3.3 4.2 5.2 6.2 9.0 10.0 10.8 11.0 12.7 12.5 13.1 14.2 17.1 Reasoner (Image-only, CoT with 0-shot) 4.5 5.4 6.3 10.8 9.0 9.9 10.8 9.9 11.7 16.2 12.6 15.3 0.0 4.3 4.3 8.7 6.5 8.7 8.7 13.0 4.3 8.7 4.3 13.0 4.7 7.0 4.7 9.3 9.3 4.7 7.0 11.6 9.3 4.7 9.3 14.0 6.1 15.2 12.1 6.1 12.1 12.1 9.1 9.1 12.1 15.2 21.2 18.2 3.5 4.8 6.2 8.3 7.1 7.5 7.5 8.8 9.4 9.2 13.5 12.7 35.2 42.4 46.0 56. 7.7 14.3 19.1 21.0 23.2 21.4 37.3 37.3 41.2 48.4 54.2 51.3 53.0 56.2 4.2 8.1 10.0 11.1 15.0 16.0 15.6 19.8 17.3 18.4 19.5 27.1 26.5 32.5 4.7 6.8 11.9 12.8 14.8 16.7 16.0 17.5 19.8 18.6 23.7 21.6 36.4 36.5 40.9 44.5 3.9 9.5 9.0 18.9 20.0 19.9 29.4 36.1 36.7 33.7 43.1 48.1 45.0 52.1 2.4 6.6 6.0 14.5 14.7 16.5 18.7 13.9 19.1 18.4 19.7 20.5 24.1 21. 4.8 7.2 7.5 13.8 14.4 15.3 17.6 18.1 17.9 21.5 19.1 21.6 48.8 46.9 50.8 51.7 2.9 14.5 15.2 24.2 24.4 20.3 30.6 26.1 30.5 30.9 36.2 50.0 49.5 41.0 2.9 11.0 7.6 9.4 23.2 11.4 20.0 21.7 24.3 19.9 15.9 15.9 20.2 24.6 0.7 9.4 8.7 19.2 13.0 15.9 18.2 27.1 14.3 19.0 10.1 23.0 27.5 41.3 52.3 47. 0.0 20.4 16.3 23.7 24.4 3.4 35.5 25.2 39.5 49.6 51.6 49.8 49.8 49.8 0.0 11.0 9.5 17.8 18.2 15.5 14.3 18.6 25.6 31.8 26.2 22.7 34.9 32.7 4.7 14.9 9.3 16.1 20.9 20.2 22.9 24.0 24.4 19.0 18.0 29.4 61.4 69.9 60.5 77.0 3.3 27.5 23.5 43.2 27.5 38.4 41.7 44.2 42.6 55.8 64.4 62.6 65.3 66.7 1.0 15.4 13.1 9.3 21.7 30.2 35.4 22.5 34.5 41.4 42.2 32.5 42.2 50. 13.4 19.6 26.0 21.5 24.7 32.5 34.0 33.2 35.1 38.9 37.6 35.3 37.9 43.3 46.8 53.8 5.4 14.7 16.5 22.6 23.0 23.0 35.0 35.4 39.1 43.9 50.4 51.1 51.4 53.9 3.1 8.8 9.0 12.2 16.5 16.6 18.0 18.7 20.3 21.3 21.4 24.5 27.2 30.4 4.9 8.7 11.3 14.5 15.8 17.7 18.5 20.2 20.3 20.7 21.8 23.4 Table 2: Comparison of model performances across five categories. PG: Plane Geometry, SG: Solid Geometry, LR: Logical Reasoning, FG: Function Graphs, SC: Statistical Charts. Accstr is strict accuracy, Acc is loose accuracy. The first and second highest accuracy of LLMs are bolded and underlined, respectively. Evaluation Protocol Strict Accuracy (Accstr) Accstr requires that all subanswers within question be correct for the model to receive credit. If any sub-answer is incorrect, the entire question is marked wrong. Loose Accuracy (Acc) Acc allows partial correctness and is computed as the proportion of correctly answered subquestions within each question. For both metrics, an automated scoring pipeline based on GPT-4.1-nano compares model answers against reference answers, enforcing strict rules for mathematical equivalence, numerical tolerance, unit consistency, and symbolic structure to ensure scalable and reliable evaluation in real-world tasks."
        },
        {
            "title": "Main Results",
            "content": "Robustness Challenge Under Real-world Visual Noise MATHREAL presents math questions photographed in realistic settings, introducing three key types of visual degradation: image quality deterioration, viewpoint shifts, and handwritten annotations. These factors pose substantial challenges to visual understanding and reasoning for MLLMs. Evaluation reveals sharp performance disparities under these conditions. Under the Acc, the top-performing models are Doubao-1.5-thinking-vision-pro (53.9%) and Doubao-seed1.6 (51.4%), while GPT-4o and Claude-sonnet-4 reach only 23.0% and 14.7%, respectively. At the other end of the spectrum, the weakest model, Gemma-3-4b-it, achieves just 3.1%. These results highlight the difficulty current MLLMs face in handling perceptual degradation. Performance drops are substantial even for frontier models, underscoring the limitations of current vision-language alignment and error tolerance. MATHREAL thus offers more realistic and discriminative benchmark for evaluating robustness under imperfect, real-world inputs. Performance Gap Between Closed and Open Models Results on the MATHREAL benchmark show that closedsource models significantly outperform their open-source counterparts across all evaluation metrics and task types, with performance gaps further amplified under noisy visual inputs. Under the strict accuracy metric (Accstr), Doubao1.5-thinking-vision-pro achieves the highest average accuracy of 41.0%. In contrast, the best open-source model, ERNIE-4.5-Turbo-VL-Preview, reaches only 17.1%, resulting in gap of over 20%. Reasoners also lag behind, with the strongest performer, MiMo-VL-7B-RL, reaching only 13.5% under Accstr. Most others fall below 10%, highlighting the difficulty of integrating reasoning pipelines with robust visual perception under degraded inputs. This further emphasizes the advantage of end-to-end, well-aligned architectures in closed models when handling real-world visual challenges. Performance Divergence Across Categories MATHREAL reveals substantial performance divergences across the five categories, reflecting distinct cognitive demands and multimodal challenges. Statistical charts (SC) yield the highest accuracies under both strict and loose metrics; for example, Doubao-1.5-thinking-vision-pro achieves 48.5% Accstr, and Doubao-seed-1.6 reaches 48.5%. These tasks benefit from structured layouts and low geometric ambiguity, enabling extraction from bar charts, tables, and plots. In contrast, logical reasoning (LR) and function graphs (FG) Figure 5: Acc comparison of models on real images vs. clean images across selected 175 samples in MATHREAL testmini. are the most challenging. LR involves abstract symbolic inference, with top models like Gemini-2.5-pro-thinking at 39.1% Accstr and Doubao-seed-1.6 at 32.6%. FG requires precise spatial alignment between visual features and expressions; even the best models, such as Gemini-2.5-flashthinking, attain only 41.9%. Overall, models perform best when visual input is structured and symbolic reasoning is limited. Tasks requiring spatial abstraction, continuous alignment, or geometric complexityparticularly under visual noiseremain key limitations for current MLLMs. Model Gaps in OCR and Description Handling Across the six settings defined above, model performance reveals limitations in handling OCR and structured figure descriptions. Claude-sonnet-4-thinking shows weaknesses in both aspects: accuracy falls from 16.5 under to 15.6 under I+QM, and further to 13.5 under I+QM+DM; clear improvement appears only with ground-truth inputs I+QG and I+QG+DG, indicating weak visualtext extraction. Grok-4 also struggles with image understanding, starting at 5.4 under I, gaining little with I+QM, yet jumping to 57.7 under I+QG and I+QG+DG, which suggests weak perception but strong text-based reasoning once accurate inputs are provided. In contrast, Gemini-2.5-pro-thinking improves steadily from 51.1 under to 59.3 under I+QM and to 61.9 under I+QM+DM; gains with I+QG and I+QG+DG are incremental, consistent with stronger internal perception. Overall, while some models reason well over clean text, most still lack robust extraction and structuring directly from real visual inputs. More detailed results are provided in the Appendix. Real Image vs. Clean Image To assess model robustness to image quality, we select 175 questions from the testmini set and retrieve higher-quality clean versions of those images. We then evaluate models on both real and clean inputs, computing = AccClean AccReal and aggregating these deltas across the fourteen interference categories with both coarse-grained (binary presence/absence) Figure 6: Error distribution over 100 annotated cases from Doubao-1.5-thinking-vision-pro (left) and Gemini-2.5-prothinking (right) error cases. and fine-grained groupings. Most models exhibit substantial gains on clean images. Llama-4-Maverick improves by +12.0% and Claude-sonnet-4-thinking by +11.8%indicating that visual noise significantly constrains their realimage performance. Blur attenuates the high-frequency details essential for OCR-based text extraction and finegrained visual feature recognition, while rotation disrupts spatial alignment and forces reliance on implicit geometric transforms, causing the strict accuracy of Claude-sonnet4-thinking and Doubao-seed-1.6 to drop by approximately 0.25 and 0.20, respectively; in contrast, models pretrained with extensive rotational augmentation, such as Gemini2.5-pro-thinking and Qwen2.5VL-72B, remain largely unaffected. Figure marking and handwritten answer interference often highlight key regions or provide solution cues, yielding modest benefits to Doubao-1.5-thinking-vision-pro and Gemini-2.5-pro-thinking; by contrast, InternVL-3-78B and Claude-sonnet-4-thinking, which exhibit weaker visualsaliency integration, suffer slight declines. Notably, Doubao1.5-thinking-vision-pro achieves remarkable +0.21 increase in strict accuracy (Accstr) on non-blurred real images versus clean versionslikely due to its vision backbone being thoroughly trained on authentic mobile-captured data, enabling it to exploit real-world lighting, shading, and texture cues. Error Analysis We conduct detailed error analysis by randomly sampling 100 failed cases (Acc = 0) from each of Doubao-1.5thinking-vision-pro and Gemini-2.5-pro-thinking. The errors are categorized into six types: OCR error,figure perception error, calculation error,reasoning error, hallucination, and reject error. The distribution is shown in Figure 6. We observe broadly consistent trend across both models. Reasoning errors account for the largest proportion (over one-third), indicating that even when perception is mostly accurate, models often fail to construct valid logical chains or apply correct mathematical principles. Visual understanding remains another major source of failure. Specifically, figure perception errors and OCR errors together account for 4050% of the failures, reflecting the strong dependence of Figure 7: basic figure perception error, with the error highlighted in red. More examples can be found in the appendix. multimodal math tasks on accurate visual decoding. In particular, noisy charts, distorted symbols, and handwritten notations frequently lead to misread digits or misinterpreted geometric structures. These perception issues are critical, as they compromise the models input before any reasoning occurs. Calculation errors, hallucinations, and reject errors occur less frequently but still contribute to overall performance degradation. Notably, hallucinations often arise when models fabricate nonexistent quantities or assumptions, while reject errors reflect failure to produce meaningful answers under uncertainty. Overall, the findings highlight two primary challenges: robust visual understanding under imperfect inputs, and consistent multi-step reasoning over noisy or ambiguous content. Addressing either alone is insufficientfuture progress in MLLMs will require tightly integrated improvements across perception, parsing, and reasoning components. Conclusion MATHREAL introduces new benchmark for evaluating MLLMs on real-world, noisy images of K12 math questions, addressing the limitations of existing benchmarks that rely on clean images. The dataset includes diverse math questions with various types of visual noise, such as blur, perspective distortions, and handwritten interference. By evaluating several open-source and closed-source models, we establish benchmark that highlights the limitations of current MLLMs in multi-visual mathematical reasoning, emphasizing the impact of image quality, input methods, and question types on performance. Our analysis reveals that most models struggle with noisy images, pointing to the need for more robust visual encoders in MLLMs. This work sets the stage for future improvements in multimodal reasoning, especially in real-world educational settings."
        },
        {
            "title": "References",
            "content": "AI, M. 2025a. Llama 4. https://www.llama.com/. AI, Z. 2025b. GLM-4.1v-thinking-flashx Model Announcement. https://www.zhipuai.cn/. Amini, A.; Gabriel, S.; Lin, S.; Koncel-Kedziorski, R.; Choi, Y.; and Hajishirzi, H. 2019. MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based ForIn Proceedings of the 2019 Conference of the malisms. North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 23572367. Ankner, Z.; Paul, M.; Cui, B.; Chang, J. D.; and Ammanabrolu, P. 2024. Critique-out-Loud Reward Models. In Pluralistic Alignment Workshop at NeurIPS 2024. Anthropic. 2025. Claude Sonnet 4. https://www.anthropic. com/claude/sonnet. Awais, M.; Ahmed, T.; Aslam, M.; Rehman, A.; Alamri, F. S.; Bahaj, S. A.; and Saba, T. 2024. Mathvision: An accessible intelligent agent for visually impaired people to understand mathematical equations. IEEE Access. Bai, J.; Bai, S.; Yang, S.; Wang, S.; Tan, S.; Wang, P.; Lin, J.; Zhou, C.; and Zhou, J. 2023. Qwen-VL: Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. arXiv:2308.12966. Bai, S.; Chen, K.; Liu, X.; Wang, J.; Ge, W.; Song, S.; Dang, K.; Wang, P.; Wang, S.; Tang, J.; et al. 2025. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923. Baidu. 2025. ERNIE Technical Report. https://yiyan.baidu. com/blog/publication/ERNIE Technical Report.pdf. ByteDance. 2025a. Doubao-1.5-thinking-vision-pro. https: //www.volcengine.com/docs/82379/1554521. ByteDance. 2025b. Doubao-1.5-vision-pro. https://www. volcengine.com/docs/82379/1553586. ByteDance. 2025c. bytedance.com/en/seed1 6. ByteDance. 2025d. Doubao-seed-1.6-thinking. https://seed. bytedance.com/en/seed1 6. Chen, H.; Tu, H.; Wang, F.; Liu, H.; Tang, X.; Du, X.; Zhou, Y.; and Xie, C. 2025a. Sft or rl? an early investigation into training r1-like reasoning large vision-language models. arXiv preprint arXiv:2504.11468. Chen, S.; Guo, Y.; Su, Z.; Li, Y.; Wu, Y.; Chen, J.; Chen, J.; Wang, W.; Qu, X.; and Cheng, Y. 2025b. Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning. arXiv preprint arXiv:2506.04207. Cobbe, K.; Kosaraju, V.; Bavarian, M.; Chen, M.; Jun, H.; Kaiser, L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R.; et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Comanici, G.; Bieber, E.; Schaekermann, M.; Pasupat, I.; Sachdeva, N.; Dhillon, I.; Blistein, M.; Ram, O.; Zhang, D.; Rosen, E.; et al. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, arXiv preprint and next generation agentic capabilities. arXiv:2507.06261. Deng, Y.; Bansal, H.; Yin, F.; Peng, N.; Wang, W.; and Chang, K.-W. 2025. Openvlthinker: An early exploration Doubao-seed-1.6. https://seed. to complex vision-language reasoning via iterative selfimprovement. arXiv preprint arXiv:2503.17352. Fu, D.; Chen, Z.; Xia, R.; Liu, Q.; Feng, Y.; Zhou, H.; Zhang, R.; Feng, S.; Gao, P.; Yan, J.; et al. 2025. Trustgeogen: Scalable and formal-verified data engine for trustworthy arXiv preprint multi-modal geometric problem solving. arXiv:2504.15780. Fu, L.; Kuang, Z.; Song, J.; Huang, M.; Yang, B.; Li, Y.; Zhu, L.; Luo, Q.; Wang, X.; Lu, H.; et al. 2024. Ocrbench v2: An improved benchmark for evaluating large multimodal models on visual text localization and reasoning. arXiv preprint arXiv:2501.00321. Guo, D.; Yang, D.; Zhang, H.; Song, J.; Zhang, R.; Xu, R.; Zhu, Q.; Ma, S.; Wang, P.; Bi, X.; et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. He, C.; Luo, R.; Bai, Y.; Hu, S.; Thai, Z. L.; Shen, J.; Hu, J.; Han, X.; Huang, Y.; Zhang, Y.; et al. 2024. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008. Hendrycks, D.; Burns, C.; Kadavath, S.; Arora, A.; Basart, S.; Tang, E.; Song, D.; and Steinhardt, J. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874. Hong, Z.; Wu, H.; Dong, S.; Dong, J.; Xiao, Y.; Zhang, Y.; Wang, Z.; Huang, F.; Li, L.; Yang, H.; et al. 2025. Benchmarking large language models via random variables. arXiv preprint arXiv:2501.11790. Huang, M.; Shi, Y.; Peng, D.; Lai, S.; Xie, Z.; and Jin, L. 2025. Ocr-reasoning benchmark: Unveiling the true capabilities of mllms in complex text-rich image reasoning. arXiv preprint arXiv:2505.17163. Jaech, A.; Kalai, A.; Lerer, A.; Richardson, A.; El-Kishky, A.; Low, A.; Helyar, A.; Madry, A.; Beutel, A.; Carney, A.; et al. 2024. OpenAI o1 System Card. CoRR. Kamoi, R.; Zhang, Y.; Das, S. S. S.; Zhang, R. H.; and Zhang, R. 2024. Visonlyqa: Large vision language models still struggle with visual perception of geometric information. arXiv preprint arXiv:2412.00947. Leng, S.; Wang, J.; Li, J.; Zhang, H.; Hu, Z.; Zhang, B.; Zhang, H.; Jiang, Y.; Li, X.; Zhao, D.; et al. 2025. Mmr1: Advancing the frontiers of multimodal reasoning. Li, B.; Ge, Y.; Chen, Y.; Ge, Y.; Zhang, R.; and Shan, Y. 2024a. Seed-bench-2-plus: Benchmarking multimodal large language models with text-rich visual comprehension. arXiv preprint arXiv:2404.16790. Li, C.; Zhang, T.; Wang, M.; and Huang, H. 2025. VisioMath: Benchmarking Figure-based Mathematical Reasoning in LMMs. arXiv preprint arXiv:2506.06727. Li, Z.-Z.; Zhang, M.-L.; Yin, F.; Ji, Z.-L.; Bai, J.-F.; Pan, Z.-R.; Zeng, F.-H.; Xu, J.; Zhang, J.-X.; and Liu, C.-L. 2024b. Cmmath: chinese multi-modal math skill evaluation benchmark for foundation models. arXiv preprint arXiv:2407.12023. Liu, A.; Feng, B.; Xue, B.; Wang, B.; Wu, B.; Lu, C.; Zhao, C.; Deng, C.; Zhang, C.; Ruan, C.; et al. 2024a. Deepseekv3 technical report. arXiv preprint arXiv:2412.19437. Liu, C.; Wei, H.; Chen, J.; Kong, L.; Ge, Z.; Zhu, Z.; Zhao, L.; Sun, J.; Han, C.; and Zhang, X. 2024b. Focus anywhere for fine-grained multi-page document understanding. arXiv preprint arXiv:2405.14295. Liu, H.; Zheng, Z.; Qiao, Y.; Duan, H.; Fei, Z.; Zhou, F.; Zhang, W.; Zhang, S.; Lin, D.; and Chen, K. 2024c. MathBench: Evaluating the Theory and Application Proficiency of LLMs with Hierarchical Mathematics Benchmark. In Findings of the Association for Computational Linguistics ACL 2024, 68846915. Lu, P.; Bansal, H.; Xia, T.; Liu, J.; Li, C.; Hajishirzi, H.; Cheng, H.; Chang, K.-W.; Galley, M.; and Gao, J. 2023. MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts. arXiv e-prints, arXiv2310. Masry, A.; Long, D. X.; Tan, J. Q.; Joty, S.; and Hoque, E. 2022. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244. Mathew, M.; Karatzas, D.; and Jawahar, C. 2021. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, 22002209. Meng, F.; Du, L.; Liu, Z.; Zhou, Z.; Lu, Q.; Fu, D.; Shi, B.; Wang, W.; He, J.; Zhang, K.; et al. 2025. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. CoRR. OpenAI. 2024. GPT-4o. https://platform.openai.com/docs/ models/gpt-4o. OpenAI. 2025a. GPT-4.1. https://openai.com/index/gpt-41/. OpenAI. 2025b. o3-and-o4-mini. https://openai.com/index/ introducing-o3-and-o4-mini/. Qiao, R.; Tan, Q.; Dong, G.; Wu, M.; Sun, C.; Song, X.; Gongque, Z.; Lei, S.; Wei, Z.; Zhang, M.; et al. 2024. WeMath: Does Your Large Multimodal Model Achieve Humanlike Mathematical Reasoning? CoRR. Shen, W.; Pei, J.; Peng, Y.; Song, X.; Liu, Y.; Peng, J.; Sun, H.; Hao, Y.; Wang, P.; and Zhou, Y. 2025. Skywork-R1V3 Technical Report. arXiv preprint arXiv:2507.06167. Shi, F.; Suzgun, M.; Freitag, M.; Wang, X.; Srivats, S.; Vosoughi, S.; Chung, H. W.; Tay, Y.; Ruder, S.; Zhou, D.; et al. 2022. Language models are multilingual chain-ofthought reasoners. In The Eleventh International Conference on Learning Representations. Sun, K.; Bai, Y.; Qi, J.; Hou, L.; and Li, J. 2024. MMMATH: Advancing Multimodal Math Evaluation with Process Evaluation and Fine-grained Classification. In Findings of the Association for Computational Linguistics: EMNLP 2024, 13581375. Sun, Y.; Zhang, S.; Tang, W.; Chen, A.; Koniusz, P.; Zou, K.; Xue, Y.; and van den Hengel, A. 2025. MATHGLANCE: Multimodal Large Language Models Do Not Know Where to Look in Mathematical Diagrams. CoRR. Team, C.; Yue, Z.; Lin, Z.; Song, Y.; Wang, W.; Ren, S.; Gu, S.; Li, S.; Li, P.; Zhao, L.; Li, L.; Bao, K.; Tian, H.; Zhang, H.; Wang, G.; Zhu, D.; Cici; He, C.; Ye, B.; Shen, B.; Zhang, Z.; Jiang, Z.; Zheng, Z.; Song, Z.; Luo, Z.; Yu, Y.; Wang, Y.; Tian, Y.; Tu, Y.; Yan, Y.; Huang, Y.; Wang, X.; Xu, X.; Song, X.; Zhang, X.; Yong, X.; Zhang, X.; Deng, X.; Yang, W.; Ma, W.; Lv, W.; Zhuang, W.; Liu, W.; Deng, S.; Liu, S.; Chen, S.; Yu, S.; Liu, S.; Wang, S.; Ma, R.; Wang, Q.; Wang, P.; Chen, N.; Zhu, M.; Zhou, K.; Zhou, K.; Fang, K.; Shi, J.; Dong, J.; Xiao, J.; Xu, J.; Liu, H.; Xu, H.; Qu, H.; Zhao, H.; Lv, H.; Wang, G.; Zhang, D.; Zhang, D.; Zhang, D.; Ma, C.; Liu, C.; Cai, C.; and Xia, B. 2025a. MiMo-VL Technical Report. arXiv:2506.03569. Team, G.; Kamath, A.; Ferret, J.; Pathak, S.; Vieillard, N.; Merhej, R.; Perrin, S.; Matejovicova, T.; Rame, A.; Rivi`ere, M.; et al. 2025b. Gemma 3 technical report. arXiv preprint arXiv:2503.19786. Team, K.; Du, A.; Yin, B.; Xing, B.; Qu, B.; Wang, B.; Chen, C.; Zhang, C.; Du, C.; Wei, C.; et al. 2025c. Kimi-vl technical report. arXiv preprint arXiv:2504.07491. Team, K. K.; Yang, B.; Wen, B.; Liu, C.; Chu, C.; Song, C.; Rao, C.; Yi, C.; Li, D.; Zang, D.; et al. 2025d. Kwai KeyeVL Technical Report. arXiv preprint arXiv:2507.01949. Wang, H.; Qu, C.; Huang, Z.; Chu, W.; Lin, F.; and Chen, W. 2025a. Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837. Wang, K.; Pan, J.; Shi, W.; Lu, Z.; Ren, H.; Zhou, A.; Zhan, M.; and Li, H. 2024. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37: 9509595169. Wang, P.; Li, Z.-Z.; Yin, F.; Ran, D.; and Liu, C.-L. 2025b. Mv-math: Evaluating multimodal math reasoning in multivisual contexts. In Proceedings of the Computer Vision and Pattern Recognition Conference, 1954119551. Wang, P.; Li, Z.-Z.; Yin, F.; Ran, D.; and Liu, C.-L. 2025c. Mv-math: Evaluating multimodal math reasoning in multivisual contexts. In Proceedings of the Computer Vision and Pattern Recognition Conference, 1954119551. Wang, X.; Yang, Z.; Feng, C.; Lu, H.; Li, L.; Lin, C.-C.; Lin, K.; Huang, F.; and Wang, L. 2025d. Sota with less: Mctsguided sample selection for data-efficient visual reasoning self-improvement. arXiv preprint arXiv:2504.07934. Wang, Y.; Zhang, P.; Tang, J.; Wei, H.; Yang, B.; Wang, R.; Sun, C.; Sun, F.; Zhang, J.; Wu, J.; et al. 2025e. Polymath: Evaluating mathematical reasoning in multilingual contexts. arXiv preprint arXiv:2504.18428. Wang, Z.; Sun, J.; Zhang, W.; Hu, Z.; Li, X.; Wang, F.; and Zhao, D. 2025f. Benchmarking Multimodal Mathematical Reasoning with Explicit Visual Dependency. arXiv preprint arXiv:2504.18589. Wei, Y.; Zhao, L.; Sun, J.; Lin, K.; Yin, J.; Hu, J.; Zhang, Y.; Yu, E.; Lv, H.; Weng, Z.; et al. 2025. Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning. arXiv preprint arXiv:2507.05255. xAI. 2025. Grok. https://x.ai/grok. Xiao, Y.; Sun, E.; Liu, T.; and Wang, W. 2024. Logicvista: Multimodal llm logical reasoning benchmark in visual contexts. arXiv preprint arXiv:2407.04973. Xu, L.; Xue, H.; Zhu, L.; and Zhao, K. 2024. Supercluemath6: Graded multi-step math reasoning benchmark for llms in chinese. arXiv preprint arXiv:2401.11819. Yang, A.; Li, A.; Yang, B.; Zhang, B.; Hui, B.; Zheng, B.; Yu, B.; Gao, C.; Huang, C.; Lv, C.; et al. 2025a. Qwen3 technical report. arXiv preprint arXiv:2505.09388. Yang, J.; Ma, F.; Wang, Z.; Yin, D.; Rong, K.; Rao, F.; and Zhang, R. 2025b. WeThink: Toward General-purpose Vision-Language Reasoning via Reinforcement Learning. arXiv preprint arXiv:2506.07905. Yang, Z.; Tang, J.; Li, Z.; Wang, P.; Wan, J.; Zhong, H.; Liu, X.; Yang, M.; Wang, P.; Bai, S.; et al. 2024. Cc-ocr: comprehensive and challenging ocr benchmark for evaluating large multimodal models in literacy. arXiv preprint arXiv:2412.02210. Yu, T.; Jing, Y.; Zhang, X.; Jiang, W.; Wu, W.; Wang, Y.; Hu, W.; Du, B.; and Tao, D. 2025. Benchmarking reasoning robustness in large language models. arXiv preprint arXiv:2503.04550. Zhang, J.; Li, Z.-Z.; Zhang, M.-L.; Yin, F.; Liu, C.-L.; and Moshfeghi, Y. 2024a. GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on Geometry ProblemSolving. In Findings of the Association for Computational Linguistics ACL 2024, 12581276. Zhang, R.; Jiang, D.; Zhang, Y.; Lin, H.; Guo, Z.; Qiu, P.; Zhou, A.; Lu, P.; Chang, K.-W.; Qiao, Y.; et al. 2024b. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, 169186. Springer. Zheng, M.; Feng, X.; Si, Q.; She, Q.; Lin, Z.; Jiang, W.; and Wang, W. 2024. Multimodal table understanding. arXiv preprint arXiv:2406.08100. Zhu, J.; Wang, W.; Chen, Z.; Liu, Z.; Ye, S.; Gu, L.; Tian, H.; Duan, Y.; Su, W.; Shao, J.; et al. 2025. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479. Zou, C.; Guo, X.; Yang, R.; Zhang, J.; Hu, B.; and Zhang, H. 2024. Dynamath: dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models. arXiv preprint arXiv:2411.00836. Appendix The appendix includes related work, dataset details, experimental details, and additional results analysis."
        },
        {
            "title": "Related Work",
            "content": "Benchmark Mathematical Reasoning Plain Text Benchmarks MathQA (Amini et al. 2019) is large-scale benchmark consisting of math word problems designed to evaluate problem-solving in arithmetic and algebra through natural language. GSM8K (Cobbe et al. 2021) contains 8,500 elementary-level math problems that test multi-step reasoning. In contrast, MATH (Hendrycks et al. 2021) provides 12,500 challenging highschool competition-level questions. SuperCLUE-Math (Xu et al. 2024) specializes in Chinese mathematical reasoning tasks. MathBench (Liu et al. 2024c) covers wide range of difficulties, from basic arithmetic to university-level mathematics. RV-Bench (Hong et al. 2025) evaluates structural understanding by programmatically replacing numerical values in problems. Math-RoB (Yu et al. 2025) introduces controlled perturbations to assess model stability under variations. Existing multilingual benchmarks like MGSM (Shi et al. 2022) rely on translated problems, which often lack sufficient difficulty or consistency. PolyMath (Wang et al. 2025e) addresses this by providing high-quality, largescale multilingual evaluation set. Multimodal Benchmarks With the development of multimodal large models, many benchmarks focused on multimodal math problems have also emerged. MathVista (Lu et al. 2023) establishes the first comprehensive multimodal math evaluation through 6,141 visual tasks across diverse mathematical reasoning scenarios. MathVerse (Zhang et al. 2024b) advances visual understanding assessment through 15,000 diagram-based samples, specifically designed to quantify diagram utilization in math problem-solving. TrustGeoGen (Fu et al. 2025) ensures trustworthy geometric problem solving through formally-verified data engine generating the GeoTrust-200K dataset with guaranteed modality integrity. MM-MATH (Sun et al. 2024) enables automatic solution step analysis and error categorization through 5,929 open-ended middle school problems with process evaluation methodology. MATH-Vision (Wang et al. 2024) elevates evaluation standards with 3,040 competition-grade problems, creating rigorous testbed for advanced mathematical reasoning. LogicVista (Xiao et al. 2024) assesses integrated logical reasoning capabilities of MLLMs through 448 multi-choice questions across five logical reasoning tasks. DynaMath (Zou et al. 2024) evaluates mathematical reasoning robustness and generalization ability through 501 Python-programmed seed problems generating diverse visual and textual variations. VisOnlyQA (Kamoi et al. 2024) reveals fundamental limitations in geometric perception through 12 tasks demonstrating that even SOTA models struggle with basic visual perception. MathGlance (Sun et al. 2025) isolates mathematical perception evaluation through 1,200 images and 1,600 questions spanning core perceptual tasks. VisioMath (Li et al. 2025) addresses figure-based mathematical reasoning through 8,070 images and 1,800 questions requiring fine-grained distinctions among visual answer options. MV-MATH (Wang et al. 2025c) challenges the multivisual reasoning by developing 2,009 multi-image problems mirroring real-world mathematical contexts. GeoEval (Zhang et al. 2024a) emphasizes unseen dataset evaluation importance through 2,000 geometry problems with specialized subsets for comprehensive assessment. We-Math (Qiao et al. 2024) introduces fourdimensional evaluation metrics for knowledge acquisition and generalization assessment through 6,500 visual problems spanning 67 hierarchical concepts. CMMath (Li et al. 2024b) delivers the first native Chinese mathematical benchmark with 23,000 curriculum-aligned questions, filling the critical gap in K-12 educational assessment. VCBENCH (Wang et al. 2025f) also introduces 1,720 multi-image mathematical reasoning problems to evaluate visual dependency integration in MLLMs."
        },
        {
            "title": "Benchmark for Perception and OCR as the\nFoundation of Reasoning",
            "content": "DocVQA (Mathew, Karatzas, and Jawahar 2021) introduces 28,000 real document QA pairs, establishing the first visual question answering evaluation framework for structured documents like contracts and reports. ChartQA (Masry et al. 2022) develops 3,200 chart QA samples, pioneering the joint reasoning evaluation mechanism between axis text and visual elements. SEED-Bench-2-Plus (Li et al. 2024a) expands to 15,672 test samples covering three richtext environments, enabling fine-grained evaluation across 63 data types. Fox (Liu et al. 2024b) introduces 9 specialized sub-tasks including region-level OCR and color-guided text recognition, establishing the first benchmark for finegrained document understanding across multi-page layouts. MMTab (Zheng et al. 2024) releases 5,000+ tax/medical form test sets with specialized metrics for complex table reasoning like merged cells and cross-column references. CC-OCR (Yang et al. 2024) collects 15,000 cross-language text images, supporting complex document parsing validation across LaTeX, HTML and SMILES formats. OCRReasoning (Huang et al. 2025) creates 1,069 advanced reasoning questions with only 2.3% directly extractable answers, specifically testing deep reasoning capabilities like spatial relationships and numerical calculations. OCRBench v2 (Fu et al. 2024) upgrades to 10,000 human-verified QA pairs across 31 scenarios and 23 tasks, first integrating eight core capability assessments including text localization and logical reasoning."
        },
        {
            "title": "Data Annotation Process",
            "content": "To facilitate annotation, we develop Gradio-based data annotation platform and organize the process into three fully manual stages: e-screening of basic image content, annotation of image conditions, annotation of question-level metadata. This structured workflow ensures high semantic and structural quality while reflecting the complexity and diversity of real-world educational scenarios. Figure 8: Gradio annotation page of stage two. Figure 9: Gradio annotation page of stage three. Stage One Re-screening. We manually verify whether each sample satisfies the three conditions established during data collection: Single Question Only: the image contains exactly one complete question, with possible interference from other incomplete or partial questions. Complete Question: the question text and figure are fully visible, with no missing text or critical contents. Figure Relevant to Solution: the diagram or figure is essential for understanding or solving the problem, not merely decorative or incidental. Samples that fail to meet any of these criteria are discarded. This step ensures that only valid, solvable, and diagramdependent math questions proceed to the next stage. Stage Two Real-world scenario categories and levels. We annotate each image according to fine-grained taxonomy of real-world scenario categories and levels. This taxonomy comprises three primary categories with fourteen subcategories: Image Quality Degradation: Blur: The degree to which the images text and figures are visually out of focus, ranging from completely clear and legible to entirely unrecognizable. (03) Underexposure/Overexposure: The extent of excessive darkness or brightness in the image that may obscure content, from no exposure issues to fully black or white images. (03) Shadow Coverage: The proportion of the question area obscured by shadows, from none to more than 60% coverage. (03) Glare: The presence of reflected light spots on the image, ranging from none to severe glare that renders the content unreadable. (03) Image Perspective Variation: Rotation: The orientation of the image compared to correctly aligned version. (Upright, clockwise 90, counterclockwise 90, or 180) In-plane Tilt: The tilt angle of the image within the xy-plane, from no tilt to tilt angle greater than 30. (03) Non-planar Capture: Perspective distortion caused by capturing the image from non-perpendicular angle, resulting in trapezoidal or irregular shapes. (03) Background Distortion: Physical bending or warping of the background or paper, from flat to severely deformed shapes affecting content recognition. (03) Question Marking: The presence of underlining, circling, or other markings on the question text, from none to heavily marked. (03) Figure Marking: Markings drawn on figures, from none to extensive markings obscuring geometric shapes. (03) Handwritten Answers for Multiple-choice or Fill-inthe-blank Questions: The presence of handwritten answers in answer blanks or options. (01) Handwritten Process for Constructed-response Questions: The amount of handwritten solution steps shown in the image, from none to four or more lines. (03) We provide detailed annotations for each subtype to support fine-grained analysis of model robustness under diverse real-world conditions.The gradio page of this stage is in Figure 8. Stage Three Question Metadata Annotation. We annotate eight key attributes: Ground-truth Question: The printed question text exactly as it appears in the image. Presence of Tables: Whether the question contains any tabular data (0 for no, 1 for yes). Educational Level: The intended education stage, categorized as primary, middle, or high school. Question Type: The answer format, including multiplechoice, fill-in-the-blank, or constructed-response. Category: The primary domain of the question, including plane geometry (PG), solid geometry (SG), logical reasoning (LR), function graphs (FG), and statistical charts (SC). Ground-truth Answer: The correct answer verified by annotators. Figure Description: detailed natural-language description of the figure, excluding any question text. Clean Image: standardized and clean version of the image retrieved via web search when available. The gradio page of this stage is in Figure 9. Finally, we conduct fully human-verified review to ensure consistency and accuracy across all stages. Through this three-stage pipeline, we construct MATHREAL, highquality dataset of real-world, diagram-based math questions that provides rigorous benchmark for evaluating visual perception and reasoning under authentic conditions. Irrelevant Content Interference:"
        },
        {
            "title": "Question Distribution",
            "content": "Handwritten Questions: The extent to which the question text is handwritten, from neatly written to extremely illegible. (03) Reverse-side Content: Visual interference from text or images on the reverse side of the paper, from none to severe bleed-through. (03) All questions in the dataset are presented in Chinese. The longest question contains 451 characters, while the shortest has only 7 characters, with an average length of 122.03 characters. Figure 10 further illustrates the distribution of question lengths, revealing diverse range from very short prompts to extended, detailed questions. reasoning, to strictly formatted final answer, ensuring that all information in the image is effectively utilized. The IUER Setting Prompt is tailored for the unified endto-end reasoning scenario, where the model performs OCR, figure understanding, and solution derivation within single interaction. The workflow in this prompt is explicitly divided into OCR extraction, detailed figure analysis, reasoning, and final answer formatting. By combining perception and reasoning within unified instruction set, this prompt facilitates systematic assessment of models ability to integrate multimodal information in one-pass pipeline under real-world conditions."
        },
        {
            "title": "Prompt for Extract and Evaluate Answers",
            "content": "To ensure consistent and objective measurement of model performance across all six experimental settings, we design two-stage evaluation pipeline comprising an Answer Extraction step followed by an Answer Evaluation step. In the extraction stage, we first apply direct string matching to capture any content enclosed in boxed{} from the model output. If no such match is found, we invoke dedicated answer extraction prompt to identify the final answer based on explicit keyword matching or, failing that, from the concluding part of the output. In the evaluation stage, the extracted answer is compared against the reference answer using mathematical answer evaluation prompt, which enforces strict equivalence rules on numerical values, algebraic expressions, units, and multiple-part answers, while supporting proportional partial credit for partially correct responses. This design enables scalable, fine-grained, and reproducible accuracy assessment under realistic educational conditions."
        },
        {
            "title": "Evaluation Protocol",
            "content": "OCR Accuracy Evaluation In real-world multimodal settings, OCR quality is often compromised by noise, handwriting, or layout distortions. To assess the reliability of model-generated OCR outputs, we adopt hybrid metric that combines five components: numeric accuracy, keyword accuracy, semantic similarity, format and structure accuracy, and lexical term based on normalized Levenshtein distance. The final score is computed as: AccOCR = 0.2 Accnum + 0.2 Acckeyword + 0.2 Simsem + 0.2 Accformat + 0.2 (1 Levnorm) Here, Accnum measures exact agreement on all numbers and units, Acckeyword evaluates proper nouns and other key entities, Simsem reflects sentence-level meaning consistency, and Accformat assesses structural fidelity (tables, paragraphs, lists). Levnorm is the normalized Levenshtein distance between the OCR output and the ground-truth question text. The first four scores are in [0, 1] following the rubric above (with semantic decisions based on GPT-4.1nano judgments), and the lexical component contributes via (1 Levnorm). Figure 10: QuestionCN Length Distribution."
        },
        {
            "title": "Propmt for OCR and Figure Understanding\nGeneration",
            "content": "This prompt is designed to separately guide multimodal large language models in performing OCR-based question text extraction and detailed figure understanding for realworld, image-based mathematical problems. The OCR Task section specifies strict recognition rules, focusing solely on printed question stems while excluding handwritten content, metadata, and irrelevant figure text. It enforces format preservation, standardized handling of blanks, and precise processing of tables, ensuring faithful reproduction of textual content without interpretation or solution attempts. The Figure Understanding Task section instructs the model to analyze only the mathematical figuressuch as geometric diagrams, function plots, and statistical chartspresent in the image. It requires comprehensive, standalone description that details the figures structure, key elements, and mathematical properties, without solving the problem or performing OCR. Together, these prompts enable clear separation between textual content extraction and visual element analysis, supporting controlled evaluations of perception and reasoning."
        },
        {
            "title": "Prompt for Answer Generation",
            "content": "In our study, we design six experimental settings (I, IUER, I+QM, I+QM+DM, I+QG, and I+QG+DG) to progressively disentangle visual perception and reasoning, enabling systematic evaluation of MLLMs perception and reasoning abilities under realistic educational scenarios. To operationalize these settings and ensure consistency across experiments, we develop task-specific prompts that guide the models in processing visual and textual information in controlled manner. The Main Setting Prompt is used for the primary evaluation setting (I), where the model receives only the raw image and is required to jointly perform visual perception and mathematical reasoning. The instructions are structured to guide the model from problem analysis, through detailed"
        },
        {
            "title": "Prompt for OCR Task",
            "content": "You are professional OCR text recognition expert. Please strictly follow the instructions below: 1. Recognition Scope: Recognize only the printed question stem in the image. Ignore any handwritten content. Include only the question stem, excluding the problem number, year, region, and score. 2. Output Format: Output text according to the original layout in the image, preserving paragraphs and line breaks. Do not merge or split paragraphs arbitrarily. 3. Multiple-Choice Options: If the option content consists only of text or numbers, fully recognize and output the options and their corresponding content. If any option contains image elements, do not recognize or output any option content. 4. Fill-in-the-Blank Questions: If blanks are present, represent them uniformly as If blanks are parentheses that need to be filled, represent them uniformly as ( (four underscores). ) (two parentheses and four spaces). 5. Math Questions with Figures: If text in the figure consists only of numbers, letters, or labels (e.g., AB, 30), do not recognize or output it. Ignore all text embedded in abstract graphics (e.g., geometric figures, statistical charts, function plots); do not include it in the question stem. 6. Figure Captions: Ignore all figure captions; do not recognize or output them. 7. Table Processing: Recognize text in the table row-by-row according to its original order. Use single space as the delimiter between columns (e.g., No. Name 1 Zhang San 2 Li Si). Important Notes!! Only return the actual recognized text content. Do not add any explanations, analysis, hints, or extra notes. Do not solve the problem or return the answer. No image analysis is required; directly return the OCR results only."
        },
        {
            "title": "Prompt for Figure Understanding Task",
            "content": "You are professional mathematical figure analysis expert. Please analyze the mathematical figure in the image and provide detailed description. Requirements: 1. Analyze only the mathematical figures in the image, including geometric figures, function plots, and statistical charts. 2. Describe in detail the basic features, key elements, and mathematical properties of the figure. 3. Your answer should contain only one part: description. 4. The description must clearly and thoroughly describe the elements, structures, geometric shapes, or chart contents in the figure. 5. Do not solve the problem or perform OCR recognition; only analyze what is present in the figure itself. Directly output the description without adding any extra content, explanations, or hints. Table 3: Prompt for OCR Task and Figure Understanding Task Please solve the problem in the image by following these steps, and do not refuse to answer: 1. Problem Analysis: Clearly identify the problem requirements, known conditions, and the objective to be solved from the image. 2. Solution Process: Main Setting Prompt for Response Generation (1) Fully utilize the information provided in the image. (2) Present the reasoning and calculation process in detail. (3) Explain the principles behind each key step. (4) Perform verification or validation when necessary. 3. Final Answer: (1) Place the answer inside boxed{}. (2) If there are multiple answers, place each one inside separate boxed{}. (3) Strictly follow the required format for numerical values, units, etc., as stated in the problem. IUER Setting Prompt for Response Generation Please answer the following math problem and strictly follow the steps below. Do not refuse to answer. 1. OCR of the Question Text: Scope Recognize only the printed question stem in the image. Ignore any handwritten content. Exclude problem number, year, region, and score. Output Format Preserve the original layout, paragraphs, and line breaks. Do not merge or split paragraphs arbitrarily. Use English punctuation only. Multiple-Choice Questions If options are text or numbers, recognize and output them completely. If options contain image elements, do not output any options. Fill-in-the-Blank Questions Represent blanks with Represent to-be-filled parentheses with ( (four underscores). ) (two parentheses with four spaces). Questions with Figures Ignore pure digits, letters, and labels inside the figure. Do not OCR text embedded in abstract graphics such as geometric figures, statistical charts, or function plots. Special Handling Figure captions: ignore completely. Dialogue-style context images: recognize only the question stem and ignore dialogues in the image. Tables: recognize row by row in the original order; separate columns with single space. Notes Recognize text content only. Do not add any explanations, analyses, or hints. 2. Figure Understanding: Analyze only the mathematical graphics in the image, including geometric figures, function plots, and statistical charts. Describe the basic characteristics, key elements, and mathematical properties of the figure in detail. Your output should contain single section named description. Description must detail the elements, structures, geometric shapes, or chart content present in the figure. Do not solve the problem and do not perform OCR here; only analyze the figure content. 3. Solution Process: (1) Fully utilize information from the image, the OCR step, and the figure understanding step. (2) Present the reasoning and calculation steps in detail. (3) Explain the principles behind each key step. (4) Perform verification or validation when necessary. 4. Final Answer: (1) Place the answer inside boxed{}. (2) If there are multiple answers, place each one inside separate boxed{}. (3) Strictly follow the required format for numbers, units, and other specifications as stated in the problem. Table 4: Prompt for Response Generation You are professional answer extraction expert. Please extract the final answer from the following text as accurately as possible, strictly following the priority strategy below:"
        },
        {
            "title": "Prompt for Answer Extraction Task",
            "content": "Priority 1: Look for explicit answer keywords - Search for the following keywords: * final answer, answer, result * the answer is, the result is * Summary words such as therefore, so, in conclusion followed by the answer content - Extract the content that immediately follows these keywords Priority 2: Extract from the end of the text - If no explicit answer is found in the previous step, try to extract the most likely answer from the last paragraph or last sentence of the text Important Requirements: 1. Multiple answers should be separated by semicolons (;) 2. Return only the answer content itself, without extra explanations or formatting 3. If the answer cannot be determined, return null Strictly follow the above priority order for extraction. Table 5: Prompt for Answer Extraction Task Answer Accuracy Evaluation Accstr requires that all subanswers within question be correct for the model to receive credit. If any component is incorrect, the entire question is marked as wrong. This metric emphasizes the completeness and consistency of chain-of-thought reasoning and aligns with the standard pedagogical principle of full marks only if fully correct. It is formally defined as: Accstr ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) (cid:104) i=1 {1, . . . , Ki}, apred i,j agt i,j (cid:105) Here, denotes the total number of questions, Ki is the number of answer blanks in the i-th question, apred i,j and agt i,j denote the model-predicted and ground truth answers for the j-th blank, respectively. The indicator function I[] returns 1 if the condition is satisfied, and denotes mathematical equivalence. Acc permits partial correctness and is calculated based on the proportion of correctly predicted sub-answers within each question. This metric captures the models partial understanding and reasoning ability under imperfect outputs: Acc ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i="
        },
        {
            "title": "1\nKi",
            "content": "Ki(cid:88) j=1 (cid:104) i,j agt apred i,j (cid:105)"
        },
        {
            "title": "Evaluation Models",
            "content": "We evaluate the performance of diverse set of models on the MathReal benchmark, categorized into four groups: (a) Large Language Models (LLMs), serving as text-only baselines, including Deepseek-v3 (Liu et al. 2024a), Deepseek-r1 (Guo et al. 2025), Qwen3 (Yang et al. 2025a) and Qwen3-thinking(Yang et al. 2025a); (b) Closed-source Multimodal Large Language Models (MLLMs), including Grok-4 (xAI 2025), Claude-sonnet-4 (Anthropic 2025), (ByteDance Claude-sonnet-4-thinking (Anthropic 2025), GPT-4.1 (OpenAI 2025a), GPT-4o (OpenAI 2024), o3 (OpenAI 2025b), o4-mini (OpenAI 2025b), Qwen-VL-Max(Bai et al. 2023), Gemini-2.5-flash-thinking(Comanici et al. 2025), Gemini2.5-pro-thinking(Comanici et al. 2025), Doubao-1.5-visionpro 2025b), Doubao-1.5-thinking-visionpro (ByteDance 2025a), Doubao-seed-1.6 (ByteDance 2025c), Doubao-seed-1.6-thinking (ByteDance 2025d); (c) Open-source MLLMs, including Gemma-3-4b-it (Team et al. 2025b), Gemma-3-27b-it (Team et al. 2025b), Gemma3n-e4b (Team et al. 2025b), Qwen2.5VL-7B(Bai et al. 2025), Qwen2.5VL-32B(Bai et al. 2025), Qwen2.5VL72B(Bai et al. 2025), InternVL-3-8B(Zhu et al. 2025), InternVL-3-14B(Zhu et al. 2025), InternVL-3-38B(Zhu et al. 2025), InternVL-3-78B(Zhu et al. 2025), Kimi-VLA3B-Instruct(Team et al. 2025c), Llama-4-Maverick (AI 2025a), GLM-4.1v-thinking-flashx (AI 2025b), and ERNIE4.5-VL-28B-A3B-PT (Baidu 2025); and (d) Multimodal Reasoning Models, including Keye-VL (Team et al. 2025d), OVR (Wei et al. 2025), Revisual-R1 (Chen et al. 2025b), Skywork-R1V3 (Shen et al. 2025), OpenVLThinker (Deng et al. 2025), ThinkLite-VL (Wang et al. 2025d), VLAAThinker (Chen et al. 2025a), WeThink (Yang et al. 2025b), MMR1-Math-v0 (Leng et al. 2025), MM-Eureka (Meng et al. 2025), MiMo-VL-7B-RL (Team et al. 2025a), and VL-Rethinker (Wang et al. 2025a)."
        },
        {
            "title": "Results Analysis",
            "content": "Results by Question Types Table 911 compare model performances across three question types using the loose accuracy (Acc) average (Avg) as the primary metric. The analysis here focuses on multimodal closed-source, open-source, and reasoning-oriented models. Multiple-choice. Overall accuracy is relatively low, with the best-performing model Doubao-seed-1.6 achieving an Avg of 42.3. The second-best closed-source model, GeminiPrompt for Mathematical Answer Evaluation Task You are top-tier mathematics evaluation expert, tasked with rigorously and precisely determining the correctness of model-generated answers. Core Task Determine whether the Model Answer below is mathematically and option-wise completely equivalent to the Reference Answer, and assign partial credit score based on the proportion of correct components. Evaluation Principles 1. Numerical Core Priority: - Focus solely on the final numerical values, expressions, options, or conclusions. - Ignore solution processes, explanatory text (e.g., the answer is:, therefore the result is:), variable names (e.g., D, E, Q1), and irrelevant descriptions. - Only retain mathematical content that directly corresponds to the reference answer for comparison. 2. Mathematical Equivalence (Strict Judgment): - Fractions and decimals: 1/2 is equivalent to 0.5; 1/2 is equivalent to 5/10. - Numerical formats: 10 is equivalent to 10.0; 1,887,800 is equivalent to 1887800 (ignore thousand separators). - Special symbols: π is equivalent to 3.14 (only when the problem explicitly allows approximation). - Algebraic expressions: x2 + is equivalent to + x2; however, 18+6sqrt{3} and 18-6sqrt{3} are not equivalent. - Formatting: ( 3/2 + 3/2. - Range notation: [0, 1] is equivalent to 0 1. - Operator Sensitivity: +, , , , (power), etc., must be strictly consistent; any symbol error renders the expressions non3 + 3)/2 is equivalent to equivalent. - Coordinate Points: (x, y) values must be numerically identical. Treat and as two sub-components. If one is correct and the other wrong, assign 0.5 for that point. - Whitespace-induced formatting differences: y=2x+3 and = 2 + 3 are equivalent; ignore the impact of spaces within expressions. 3. Unit Handling: - Reference answer has no unit: if the model answer includes correct and reasonable unit (e.g., 15 vs 15m), it is considered correct. - Reference answer has unit: incorrect units are considered wrong (e.g., 15m vs 15cm); if the model answer lacks unit but the numerical value is correct, it is considered correct. - Ignore unit formatting differences: 180 { dm}2 and 180dm2 are equivalent; correctly extract the content. 4. Handling Multi-Part Answers (Critical!): - You must split the reference answer into all sub-answers (blanks) based on its structure. - Each newline n, semicolon ;, or major section (1), (2) indicates separate blank. - For each blank, further decompose it if it contains multiple components: Or-connected answers: e.g., 5 or -75 two valid solutions. If model answers only 5, give 0.5 for that blank. Coordinate pairs: e.g., (5, 0) treat as two values. If model says (5, 1), give 0.5. Multiple points: e.g., (1, 0), (9, 8), (1, 9) three points. Each correct point gives 1/3. - Total score = sum of all correct sub-components / total number of sub-components. - Always allow proportional partial credit unless explicitly stated otherwise. 5. Special Rules for Multiple-Choice Questions: - If the reference answer is single option (e.g., B), then as long as the model answer contains that option letter (e.g., B, B., Option B, B. (x0) > g(x0)) and no other options, it is considered correct 1.0. - If multiple options appear or an incorrect option is selected, it is considered wrong 0.0. 6. Semantic Equivalence: - Even if the phrasing differs, as long as the mathematical meaning is the same, it is considered correct. 7. Proof or Graphing Questions: - If the question type is proof or graphing question, treat the model answer as acceptable by default; do not score it, and directly return <score>1.0</score>. Scoring Criteria - 1.0: All components are correct. - 0.01.0: Assign partial credit proportionally based on the number of correct sub-components. - 0.0: No component is correct. - Round to two decimal places (e.g., 0.83, 0.67, 0.50). Output Format You must strictly return only the XML tag containing the score, with no additional text or explanation. <score>score</score> Table 6: Prompt for Mathematical Answer Evaluation Task Model AccOCR GLM-4.1v-thinking-flashx Qwen-VL-Max ERNIE-4.5-turbo-vl Llama-4-Maverick GPT-4o GPT-4.1 Claude-sonnet-4 Claude-sonnet-4-thinking Doubao-1.5-vision-pro o4-mini Grok-4 Gemini-2.5-flash-thinking o3 Doubao-seed-1.6-thinking Doubao-1.5-thinking-vision-pro Doubao-seed-1.6 Gemini-2.5-pro-thinking 81.8 87.0 89.8 71.0 78.6 79.2 54.0 53.9 87.8 81.9 35.6 89.8 78.4 87.9 89.8 89.7 94.0 24.5 23.0 30.4 18.7 23.0 22.6 14.7 16.5 39.1 35.0 5.4 50.4 35.4 43.9 53.9 51.4 51. IUER 19.6 23.0 30.5 20.5 22.4 22.9 13.8 13.7 39.2 24.4 7.7 51.5 32.0 46.2 56.9 43.8 57.4 I+QM I+QG I+QM+DM I+QG+DG 24.9 26.0 28.4 18.8 22.7 21.5 15.0 15.6 35.8 34.5 9.7 51.4 33.0 45.8 52.6 52.5 59.3 32.5 28.1 32.7 32.2 32.2 37.7 36.5 40.5 44.1 48.6 45.8 54.0 47.8 59.5 61.7 59.5 62.0 22.1 24.8 27.8 18.0 24.5 19.1 15.2 13.5 36.7 30.9 9.3 49.2 34.2 46.9 53.3 48.3 61. 34.9 35.1 36.6 38.2 38.7 40.8 45.1 46.9 51.8 55.8 57.7 58.3 58.5 63.2 64.1 64.2 66.0 Table 7: The Acc of the OCR and the six experimental settings of models. Model Real Clean Grok-4 Qwen2.5VL-7b InternVL3-14b InternVL3-8b InternVL3-38b Claude-sonnet-4 InternVL3-78b GPT-4.1 GPT-4o Claude-sonnet-4-thinking Llama-4-Maverick Qwen-VL-Max Qwen2.5VL-72b Qwen2.5VL-32b ERNIE-4.5-turbo-vl GLM-4.1v-thinking-flashx Doubao-1.5-vision-pro o4-mini Gemini-2.5-flash-thinking o3 Gemini-2.5-pro-thinking Doubao-seed-1.6-thinking Doubao-1.5-thinking-vision-pro Doubao-seed-1. 5.6 18.2 21.2 18.6 20.6 15.8 23.1 22.9 24.1 20.1 18.5 22.2 31.7 21.9 32.2 24.6 42.0 41.4 54.5 40.7 56.3 47.8 62.9 56.2 12.7 20.0 21.8 23.3 25.1 26.7 29.0 29.7 31.0 31.8 31.8 32.1 32.6 32.8 33.0 36.0 49.6 50.8 51.1 53.1 56.3 57.1 59.9 63.6 +7.1 +1.8 +0.6 +4.7 +4.5 +10.9 +5.9 +6.8 +6.9 +11.7 +13.3 +9.9 +0.9 +10.9 +0.8 +11.4 +7.6 +9.4 -3.4 +12.4 +0.0 +9.3 -3.0 +7.4 Table 8: Acc Comparison: Clean vs. Real, where = AccClean AccReal 2.5-pro-thinking, reaches 34.6, while the best open-source model, InternVL3-8B, also achieves 34.6. Reasoningoriented models lag behind, with the top performer VLRethinker-7B reaching 30.8. These results indicate that multiple-choice questions are more vision-centric, favoring strong visual encoders capable of distinguishing among distractors rather than relying heavily on long-chain reasoning. Fill-in-the-blank. This type yields the highest overall scores, with Doubao-1.5-thinking-vision-pro achieving 67. and Doubao-seed-1.6 close behind at 63.8. The best opensource model, ERNIE-4.5-Turbo-VL-Preview, reaches 34.5, and the top reasoning model, WeThink, achieves 30.9. Compared with multiple-choice, fill-in-the-blank questions reward coherent step-by-step reasoning and numerical computation, allowing models with strong symbolic reasoning capabilities to narrow the gap with top vision models. Accuracy in this category could be further improved through better normalization of numeric outputs, unit handling, and formatting. Constructed-response. Performance is moderate, with the top closed-source vision model Doubao-1.5-thinkingvision-pro achieving 51.8, and the best open-source model ERNIE-4.5-Turbo-VL-Preview reaching 29.9. The strongest reasoning-oriented model, MiMo-VL-7B-RL, scores 21.7. Constructed-response questions require multi-step reasoning and coherent explanations, favoring models that can maintain complete reasoning chains and produce structured final answers. Further improvements could be achieved by explicitly presenting intermediate variables and incorporating step verification to reduce omissions. Cross-type comparison. Considering Acc Avg across the three types, the achievable performance ceiling follows the order: Fill-in-the-blank (approximately 68%) Constructedresponse (approximately 53%) Multiple-choice (approximately 42%). Multiple-choice questions are more dependent on visual recognition, while fill-in-the-blank and constructed-response formats rely more heavily on symbolic reasoning and structured output. Open-source and reasoning-oriented models consistently trail behind the top closed-source models, highlighting gaps in both robust visual encoding and end-to-end reasoning consistency. Intra-family Performance Patterns The Doubao family demonstrates strong geometric and structured reasoning capabilities. Doubao-1.5-thinkingvision-pro achieves the highest strict accuracy in PG Model Acc PG SG LR FG SC Avg LLMs (Question Text + Figure Description, CoT with 0-shot) Qwen3-235B-A22B-thinking DeepSeek-V3 Qwen3-235B-A22B-instruct DeepSeek-R1 12.5 12.5 12.5 25.0 60.0 40.0 33.4 60.0 66.7 66.7 33.3 66.7 Closed Models (Image-only, CoT with 0-shot) Grok-4 Claude-sonnet-4 Claude-sonnet-4-thinking GPT-4.1 GPT-4o Qwen-VL-Max o4-mini o3 Doubao-1.5-vision-pro-32k Doubao-seed-1.6-thinking Gemini-2.5-flash-thinking Gemini-2.5-pro-thinking Doubao-seed-1.6 Doubao-1.5-thinking-vision-pro 0.0 0.0 0.0 0.0 12.5 0.0 0.0 12.5 12.5 25.0 25.0 25.0 37.5 25.0 0.0 20.0 0.0 20.0 0.0 0.0 0.0 20.0 0.0 20.0 0.0 20.0 40.0 40.0 0.0 0.0 0.0 33.3 0.0 0.0 0.0 0.0 0.0 33.3 33.3 100.0 66.7 0.0 14.3 14.3 28.6 14.3 0.0 28.6 14.3 28.6 28.6 28.6 0.0 14.3 14.3 42.9 42.9 28.6 28.6 14. Open-source MLLMs (Image-only, CoT with 0-shot) Gemma-3-4b-it Gemma-3n-E4B Gemma-3-27b-it Kimi-VL-A3B-Instruct Qwen2.5-VL-7B-Instruct InternVL3-8B InternVL3-14B Llama-4-Maverick InternVL3-78B Qwen2.5-VL-32B-Instruct InternVL3-38B GLM-4.1v-thinking-flashx Qwen2.5-VL-72B ERNIE-4.5-Turbo-VL-Preview 0.0 0.0 12.5 0.0 0.0 37.5 25.0 0.0 12.5 12.5 12.5 0.0 12.5 25.0 0.0 0.0 0.0 0.0 20.0 20.0 0.0 0.0 0.0 0.0 20.0 0.0 0.0 0.0 0.0 33.3 33.3 0.0 0.0 0.0 33.3 0.0 0.0 33.3 0.0 33.3 0.0 0.0 Reasoner (Image-only, CoT with 0-shot) Keye-VL-8B-Preview OVR Revisual-R1 Skywork-R1V3-38B OpenVLThinker ThinkLite-VL VLAA-Thinker-Qwen2.5VL-7B WeThink MMR1-Math-v0-7B MM-Eureka MiMo-VL-7B-RL VL-Rethinker-7B 0.0 0.0 0.0 25.0 0.0 25.0 12.5 12.5 12.5 12.5 0.0 25.0 0.0 0.0 0.0 0.0 0.0 0.0 20.0 0.0 20.0 20.0 0.0 40.0 0.0 0.0 0.0 16.7 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 42.9 14.3 28.6 14.3 42.9 14.3 28.6 14.3 28.6 42.9 28.6 42.9 28.6 14.3 0.0 14.3 14.3 28.6 14.3 14.3 14.3 14.3 14.3 0.0 28. 66.7 66.7 33.3 66.7 0.0 33.3 66.7 33.3 33.3 33.3 33.3 33.3 33.3 33.3 0.0 33.3 66.7 22.3 0.0 0.0 0.0 0.0 0.0 66.7 100.0 33.3 0.0 33.3 66.7 33.3 33.3 33.3 33.3 66.7 66.7 39.0 0.0 33.3 0.0 33.3 33.3 55.7 33.3 66.7 34.6 30.8 25.7 38.5 0.0 15.4 11.5 19.2 15.4 11.5 3.8 15.4 11.5 30.8 23.1 34.6 42.3 21. 0.0 15.4 11.5 7.7 7.7 34.6 26.9 11.5 7.7 19.2 26.9 15.4 19.2 19.2 7.7 7.7 11.5 18.0 7.7 15.4 11.5 11.5 15.4 18.0 3.8 30.8 Table 9: Comparison of model performances across five categories on multiple-choice questions. PG: Plane Geometry, SG: Solid Geometry, LR: Logical Reasoning, FG: Function Graphs, SC: Statistical Charts. Accstr is strict accuracy, Acc is loose accuracy. The first and second highest accuracy of LLMs are bolded and underlined, respectively. Model Accstr Acc PG SG LR FG SC Avg PG SG LR FG SC Avg LLMs (Question Text + Figure Description, CoT with 0-shot) Qwen3-235B-A22B-thinking DeepSeek-V3 Qwen3-235B-A22B-instruct DeepSeek-R1 41.5 37.7 47.2 49.1 7.1 35.7 21.4 50.0 57.1 38.1 38.1 38.1 23.1 30.8 30.8 23. 58.3 50.0 50.0 50.0 39.8 38.1 40.7 44.2 Grok-4 Claude-sonnet-4 Claude-sonnet-4-thinking GPT-4.1 GPT-4o Qwen-VL-Max o4-mini o3 Doubao-1.5-vision-pro-32k Doubao-seed-1.6-thinking Gemini-2.5-flash-thinking Gemini-2.5-pro-thinking Doubao-seed-1.6 Doubao-1.5-thinking-vision-pro Gemma-3-4b-it Gemma-3n-E4B Gemma-3-27b-it Kimi-VL-A3B-Instruct Qwen2.5-VL-7B-Instruct InternVL3-8B InternVL3-14B Llama-4-Maverick InternVL3-78B Qwen2.5-VL-32B-Instruct InternVL3-38B GLM-4.1v-thinking-flashx Qwen2.5-VL-72B ERNIE-4.5-Turbo-VL-Preview Keye-VL-8B-Preview OVR Revisual-R1 Skywork-R1V3-38B OpenVLThinker ThinkLite-VL VLAA-Thinker-Qwen2.5VL-7B WeThink MMR1-Math-v0-7B MM-Eureka MiMo-VL-7B-RL VL-Rethinker-7B Closed Models (Image-only, CoT with 0-shot) 11.3 11.3 18.9 17.0 18.9 17.0 30.2 43.4 28.3 47.2 50.9 45.3 50.9 58.5 7.1 7.1 7.1 14.3 14.3 35.7 28.6 42.9 35.7 50.0 57.1 42.9 57.1 42.9 0.0 14.3 19.0 14.3 14.3 14.3 33.3 23.8 23.8 23.8 28.6 47.6 52.4 38.1 0.0 0.0 0.0 15.4 7.7 30.8 30.8 38.5 7.7 30.8 38.5 30.8 30.8 30.8 0.0 8.3 8.3 25.0 0.0 41.7 16.7 25.0 16.7 25.0 41.7 50.0 33.3 58.3 6.2 9.7 14.2 16.8 14.2 23.0 29.2 37.2 24.8 38.9 45.1 44.2 47.8 49. Open-source MLLMs (Image-only, CoT with 0-shot) 3.8 7.5 3.8 7.5 3.8 9.4 7.5 17.0 9.4 9.4 17.0 13.2 13.2 20.8 3.8 1.9 5.7 9.4 13.2 9.4 5.7 7.5 5.7 7.5 18.9 11.3 0.0 7.1 0.0 14.3 28.6 14.3 21.4 14.3 35.7 35.7 28.6 0.0 21.4 21.4 0.0 0.0 0.0 0.0 19.0 0.0 9.5 19.0 14.3 14.3 4.8 9.5 14.3 9.5 0.0 0.0 0.0 7.7 7.7 0.0 7.7 7.7 23.1 30.8 7.7 15.4 7.7 15. 0.0 0.0 0.0 0.0 16.7 0.0 8.3 0.0 16.7 33.3 16.7 8.3 8.3 25.0 1.8 4.4 1.8 6.2 11.5 6.2 9.7 14.2 15.9 18.6 15.0 10.6 13.3 18.6 Reasoner (Image-only, CoT with 0-shot) 7.1 7.1 14.3 14.3 21.4 28.6 14.3 21.4 14.3 28.6 14.3 21.4 0.0 4.8 4.8 4.8 4.8 14.3 4.8 19.0 4.8 9.5 9.5 14.3 0.0 7.7 0.0 7.7 15.4 7.7 15.4 23.1 15.4 7.7 7.7 15. 0.0 8.3 0.0 8.3 16.7 8.3 0.0 8.3 8.3 0.0 16.7 8.3 2.7 4.4 5.3 8.8 13.3 12.4 7.1 13.3 8.0 9.7 15.0 13.3 49.4 47.2 60.0 60.3 16.8 19.2 30.2 23.7 26.5 24.6 41.2 60.3 40.7 60.0 62.2 57.5 60.1 71.2 6.5 13.5 9.6 16.9 17.5 18.0 16.8 26.2 18.8 16.8 25.4 27.5 25.2 30.7 4.9 5.0 14.8 14.6 19.0 17.4 16.7 20.8 20.2 24.2 28.0 26. 20.9 44.0 36.1 55.9 9.5 14.2 16.6 14.3 22.0 41.4 35.7 52.4 40.4 59.5 61.9 63.5 66.7 65.9 2.4 17.9 7.1 16.6 36.3 22.6 28.6 22.6 38.1 38.1 33.4 20.6 42.6 38.6 7.1 7.1 14.3 26.1 38.6 38.7 26.8 37.1 16.6 33.4 21.4 33.9 68.9 51.5 60.6 50.6 6.3 19.0 20.6 28.5 31.3 23.8 46.0 36.6 41.3 40.8 46.0 58.7 73.2 56. 0.0 9.8 8.7 17.4 26.1 9.0 23.1 25.3 24.6 24.5 17.5 22.2 30.0 23.8 1.6 9.5 9.5 16.0 18.9 25.3 16.0 36.8 20.1 20.3 15.9 29.7 26.9 53.9 46.8 56.5 0.0 20.6 20.5 28.2 25.6 43.6 53.2 55.2 38.4 53.8 52.5 42.9 51.2 59.6 0.0 8.3 12.8 18.0 29.5 15.4 24.3 15.4 46.1 53.9 27.6 31.4 38.4 37.8 0.0 20.5 5.2 22.5 33.3 26.2 42.3 38.4 34.5 33.3 23.7 35. 67.3 60.4 62.4 74.3 6.2 27.8 25.7 45.2 28.5 58.4 34.1 43.8 53.4 58.2 70.8 74.3 74.0 82.0 2.8 16.7 13.8 8.2 31.2 25.3 29.9 20.8 41.4 50.0 37.5 34.0 54.2 61.6 17.3 15.0 17.4 24.3 29.8 38.2 39.4 49.8 45.1 38.9 44.4 41.0 48.8 49.8 55.9 59.0 10.9 19.6 25.2 26.2 27.0 32.3 42.1 52.6 41.9 55.5 59.0 58.6 63.8 67. 3.6 13.1 9.9 16.2 24.3 17.4 21.7 23.8 27.8 28.7 26.4 26.8 32.8 34.5 5.3 9.0 12.9 18.2 24.2 24.8 23.2 30.9 24.1 27.2 26.2 30.4 Table 10: Comparison of model performances across five categories on fill-in-the-blank questions. PG: Plane Geometry, SG: Solid Geometry, LR: Logical Reasoning, FG: Function Graphs, SC: Statistical Charts. Accstr is strict accuracy, Acc is loose accuracy. The first and second highest accuracy of LLMs are bolded and underlined, respectively. Model Accstr Acc PG SG LR FG SC Avg PG SG LR FG SC Avg LLMs (Question Text + Figure Description, CoT with 0-shot) Qwen3-235B-A22B-thinking DeepSeek-V3 Qwen3-235B-A22B-instruct DeepSeek-R1 26.3 25.3 31.2 41.9 32.6 30.4 35.9 33.7 22.7 27.3 36.4 40.9 21.7 30.4 47.8 39. 38.9 61.1 44.4 61.1 28.2 29.0 34.6 40.5 Grok-4 Claude-sonnet-4 Claude-sonnet-4-thinking GPT-4.1 GPT-4o Qwen-VL-Max o4-mini o3 Doubao-1.5-vision-pro-32k Doubao-seed-1.6-thinking Gemini-2.5-flash-thinking Gemini-2.5-pro-thinking Doubao-seed-1.6 Doubao-1.5-thinking-vision-pro Gemma-3-4b-it Gemma-3n-E4B Gemma-3-27b-it Kimi-VL-A3B-Instruct Qwen2.5-VL-7B-Instruct InternVL3-8B InternVL3-14B Llama-4-Maverick InternVL3-78B Qwen2.5-VL-32B-Instruct InternVL3-38B GLM-4.1v-thinking-flashx Qwen2.5-VL-72B ERNIE-4.5-Turbo-VL-Preview Keye-VL-8B-Preview OVR Revisual-R1 Skywork-R1V3-38B OpenVLThinker ThinkLite-VL VLAA-Thinker-Qwen2.5VL-7B WeThink MMR1-Math-v0-7B MM-Eureka MiMo-VL-7B-RL VL-Rethinker-7B Closed Models (Image-only, CoT with 0-shot) 4.3 6.5 9.1 11.3 11.8 9.1 26.3 23.1 28.0 34.4 41.4 39.2 38.2 39.8 2.2 6.5 7.6 14.1 15.2 10.9 23.9 28.3 28.3 23.9 35.9 42.4 34.8 43.5 0.0 4.5 4.5 9.1 13.6 9.1 13.6 9.1 18.2 9.1 13.6 22.7 9.1 18.2 0.0 0.0 13.0 0.0 8.7 4.3 17.4 0.0 30.4 43.5 43.5 47.8 43.5 39.1 0.0 16.7 11.1 33.3 22.2 22.2 33.3 44.4 33.3 33.3 61.1 50.0 55.6 50.0 2.9 6.5 8.8 12.3 13.2 10.0 24.6 23.2 27.9 30.5 39.3 40.2 36.7 39. Open-source MLLMs (Image-only, CoT with 0-shot) 0.5 1.1 4.3 2.7 4.3 7.0 7.0 10.2 7.0 8.6 8.1 15.1 12.4 17.2 3.2 3.2 6.5 5.9 3.2 4.3 5.4 6.5 9.7 5.4 15.1 9.7 2.2 2.2 5.4 10.9 5.4 9.8 14.1 10.9 13.0 10.9 14.1 15.2 17.4 13.0 4.5 4.5 0.0 0.0 9.1 9.1 4.5 9.1 18.2 9.1 13.6 4.5 9.1 18.2 0.0 0.0 0.0 4.3 0.0 4.3 0.0 4.3 4.3 8.7 4.3 0.0 13.0 13. 0.0 11.1 11.1 0.0 0.0 11.1 16.7 5.6 16.7 27.8 22.2 22.2 22.2 27.8 1.2 2.1 4.4 4.7 4.4 7.9 8.8 9.7 9.7 10.3 10.6 13.8 14.1 16.4 Reasoner (Image-only, CoT with 0-shot) 4.3 5.4 5.4 10.9 7.6 7.6 9.8 8.7 10.9 14.1 13.0 13.0 0.0 4.5 4.5 13.6 9.1 4.5 13.6 9.1 4.5 9.1 0.0 13.6 4.3 8.7 4.3 8.7 0.0 0.0 0.0 4.3 4.3 0.0 13.0 8. 5.6 11.1 11.1 5.6 11.1 11.1 16.7 5.6 11.1 22.2 22.2 16.7 3.5 4.7 6.2 7.9 5.0 5.3 7.3 7.0 9.4 8.5 13.8 11.1 32.1 42.4 43.4 56.6 5.5 13.6 16.8 21.1 22.7 21.4 37.8 31.9 42.6 46.1 53.2 50.7 51.7 53.2 3.7 6.8 10.0 10.0 14.9 14.5 14.8 18.9 17.1 19.1 18.1 28.2 27.5 33.3 4.8 7.6 11.6 11.7 14.2 16.1 16.0 16.8 20.1 17.3 23.4 20. 37.5 35.1 42.0 42.0 3.3 8.2 8.3 19.5 20.8 17.8 30.0 34.5 38.2 30.5 42.6 47.3 41.9 50.6 2.5 5.3 6.2 14.9 11.1 15.4 18.2 13.3 17.2 16.4 17.6 21.7 22.6 20.1 4.7 7.6 6.9 12.7 11.5 12.5 16.1 16.2 18.0 19.8 19.8 18.7 27.3 39.8 44.0 50.7 0.0 12.1 12.1 18.9 21.2 19.7 20.1 19.7 24.3 21.2 27.3 34.9 24.6 31. 6.0 9.1 3.0 3.0 23.5 15.1 15.1 21.2 27.3 13.6 16.6 7.6 13.6 28.8 0.0 10.6 9.1 22.7 9.1 9.1 22.7 21.6 10.6 20.5 6.0 19.7 31.9 42.4 62.7 52.9 0.0 17.8 14.5 19.6 22.5 25.4 36.3 11.7 47.9 49.3 53.7 60.2 55.4 55.1 0.0 2.9 6.1 14.5 13.1 7.2 8.7 17.4 17.4 20.3 20.3 16.0 30.4 31.2 4.3 16.3 10.1 13.0 11.6 18.5 14.5 18.9 21.8 12.3 20.3 26. 56.5 76.8 63.8 80.6 1.8 26.4 14.8 43.5 25.9 25.9 48.2 46.3 37.0 57.8 70.8 59.7 59.2 63.9 0.0 17.1 14.8 11.6 19.0 27.3 28.2 21.8 35.6 37.0 41.2 31.4 35.7 45.3 7.4 14.8 25.0 16.7 25.4 28.7 36.1 22.2 28.7 36.1 33.8 26.3 34.5 42.1 45.4 53.3 4.0 13.0 14.0 21.6 22.2 20.8 35.0 31.2 40.3 41.1 49.6 49.8 48.0 51. 3.1 6.8 8.5 11.3 14.5 15.0 16.1 17.6 18.8 19.0 19.2 24.4 25.9 29.9 4.6 8.7 10.8 13.0 13.6 15.5 17.4 17.4 19.5 18.8 21.7 20.5 Table 11: Comparison of model performances across five categories on constructed-response questions. PG: Plane Geometry, SG: Solid Geometry, LR: Logical Reasoning, FG: Function Graphs, SC: Statistical Charts. Accstr is strict accuracy, Acc is loose accuracy. The first and second highest accuracy of LLMs are bolded and underlined, respectively. (43.3%), SG (43.2%), and SC (48.5%), indicating superior performance in tasks requiring spatial understanding and formal visual parsing. Within the family, Doubao-seed1.6 outperforms its thinking variant on more abstract reasoning tasks. In LR, the non-thinking version leads with 32.6%, while the thinking model drops to 17.4%, suggesting that longer reasoning chains may hinder performance under noisy visuals. The Gemini family also shows consistently strong and balanced performance. Gemini-2.5-pro-thinking ranks among the top across tasks, with 48.5% in SC and over 40% in PG and SG. Even in the most challenging LR category, it reaches 39.1%, indicating stable multimodal reasoning. InternVL models show reversed scaling pattern. The InternVL-3-78B model achieves the best LR score among open models (15.2%), but underperforms the InternVL-338B model in SC, possibly due to overfitting or degraded visual generalization at scale. The Qwen2.5VL family excels at structured visual tasks. The 32B model leads in FG (18.6%) and SC (30.3%), showing strength in visual-text alignment. However, scaling to 72B yields only marginal gains, especially in complex reasoning. Overall, different model families show strengths in specific task typessome favor spatial or symbolic inference, others visual parsing. No model excels across all categories, underscoring the current limitations in developing truly general-purpose MLLMs capable of handling diverse visual reasoning tasks. Strict Evaluation Reveals Instability in Multi-step Reasoning While many models perform decently under Acc, real-world applications often demand fully correct multi-step solutions. Our evaluation reveals clear gaps between Accstr and Acc, exposing weaknesses in reasoning stability and compositional understanding. For example, Gemini-2.5-pro-thinking scores 48.1% Acc but drops to 42.9% under strict evaluation, reflecting small reasoning failures or incomplete logic. More noticeably, InternVL-3-14B achieves 19.0% Acc but only 10.9% Accstr, gap of over 8 points, highlighting its difficulty with full-task consistency. Strict metrics thus better reflect whether models can fully solve multi-step problems. They uncover bottlenecks in long-form reasoning and align more closely with educational standards. Reporting both scores is essential for clearer picture of true problemsolving ability. Analysis of OCR Accuracy and Answer Accuracy Overall Performance and Ranking Based on Table 8, in the Clean setting the overall accuracy shows clear gap between the top performers and the rest. Doubao-seed-1.6 ranks first (63.6), followed by Doubao-1.5-thinking-visionpro (59.9), Gemini-2.5-pro-thinking (56.3), o3 (53.1), Gemini-2.5-flash-thinking (51.1), and o4-mini (50.8). In the Real setting, the best-performing model changes to Doubao1.5-thinking-vision-pro (62.9), followed by Gemini-2.5-prothinking (56.3), Doubao-seed-1.6 (56.2), and Gemini-2.5flash-thinking (54.5). This indicates that the Doubao family consistently dominates in both conditions, Gemini-2.5pro-thinking maintains balanced performance across domains, while models like o3 and o4-mini have stronger upFigure 11: Scatter plot of the relationship between OCR accuracy and accuracy in the I+QG setting, where the size of each circle represents the difference in accuracy between the I+QG setting and the I+QM setting. per bounds in the Clean setting but drop in ranking for Real, showing higher sensitivity to input cleanliness. Robustness and Analysis From the perspective of = AccClean AccReal, smaller absolute value indicates greater robustness across domains. The most stable model is Gemini-2.5-pro-thinking ( = 0.0), followed by ERNIE-4.5-turbo-vl (+0.8), InternVL3-14b (+0.6), and Qwen2.5VL-72b (+0.9), suggesting minimal dependence on input cleaning. Most mainstream models gain between 5 and 10 percentage points in Clean compared to Real, such as GPT-4o (+6.9), GPT-4.1 (+6.8), o4-mini (+9.4), Doubao-1.5-vision-pro (+7.6), and Qwen-VL-Max (+9.9), indicating that standardization and denoising benefit wide range of systems. Notably, two atypical patterns emerge: first, models with negative , including Gemini-2.5-flashthinking (-3.4) and Doubao-1.5-thinking-vision-pro (-3.0), perform better in Real than in Clean, possibly due to stronger adaptation to realistic noise and layout variations; second, models with very large , such as Llama4-Maverick (+13.3), o3 (+12.4), Claude-sonnet-4-thinking (+11.7), GLM-4.1v-thinking-flashx (+11.4), Qwen2.5VL32b (+10.9), and Claude-sonnet-4 (+10.9), show substantial benefits from cleaner inputs, implying higher vulnerability to noise and complex formatting. Family and Model-Type Comparison Within the Doubao series, Doubao-1.5-thinking-vision-pro leads in Real accuracy (62.9) but slightly drops in Clean (negative ), making it well-suited for raw, noisy data. Doubaoseed-1.6 achieves the highest Clean score (63.6) while remaining competitive in Real (56.2), representing the strongest all-around performer. The Gemini family presents contrast: Gemini-2.5-pro-thinking achieves perfect robustness ( = 0) and high scores in both domains, while Gemini-2.5-flash-thinking is notably stronger in Real than Clean. OpenAIs o3 and o4-mini benefit greatly from cleaner inputs (large positive ), making them excellent candidates for pipelines with strong preprocessing. Other major model families, such as GPT-4o/4.1, Claude, Qwen, and InternVL, generally follow the trend of significantly higher accuracy in Clean, reinforcing the importance of preprocessing for optimal performance. Figure 12: Samples of Plane Geometry. Figure 13: Samples of Solid Geometry. Figure 14: Samples of Logical Reasoning. Figure 15: Samples of Function Graphs. Figure 16: Samples of Statistical Charts."
        }
    ],
    "affiliations": [
        "Baidu Inc., Beijing, China",
        "Beihang University, Beijing, China",
        "Gaoling School of Artificial Intelligence, Renmin University of China",
        "Nanyang Technological University, Singapore",
        "Xiaopeng Motors, China"
    ]
}