{
    "paper_title": "Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation",
    "authors": [
        "Isabela Albuquerque",
        "Ira Ktena",
        "Olivia Wiles",
        "Ivana KajiÄ‡",
        "Amal Rannen-Triki",
        "Cristina Vasconcelos",
        "Aida Nematzadeh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite advances in generation quality, current text-to-image (T2I) models often lack diversity, generating homogeneous outputs. This work introduces a framework to address the need for robust diversity evaluation in T2I models. Our framework systematically assesses diversity by evaluating individual concepts and their relevant factors of variation. Key contributions include: (1) a novel human evaluation template for nuanced diversity assessment; (2) a curated prompt set covering diverse concepts with their identified factors of variation (e.g. prompt: An image of an apple, factor of variation: color); and (3) a methodology for comparing models in terms of human annotations via binomial tests. Furthermore, we rigorously compare various image embeddings for diversity measurement. Notably, our principled approach enables ranking of T2I models by diversity, identifying categories where they particularly struggle. This research offers a robust methodology and insights, paving the way for improvements in T2I model diversity and metric development."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 7 4 5 0 1 . 1 1 5 2 : r https://arxiv.org/abs/ 2025-11-04 Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation Isabela Albuquerque1, Ira Ktena2, Olivia Wiles1, Ivana KajiÄ‡1, Amal Rannen-Triki1, Cristina Vasconcelos1 and Aida Nematzadeh1 1Google DeepMind, 2Ellison Institute of Technology, work done while at Google DeepMind. Despite advances in generation quality, current text-to-image (T2I) models often lack diversity, generating homogeneous outputs. This work introduces framework to address the need for robust diversity evaluation in T2I models. Our framework systematically assesses diversity by evaluating individual concepts and their relevant factors of variation. Key contributions include: (1) novel human evaluation template for nuanced diversity assessment; (2) curated prompt set covering diverse concepts with their identified factors of variation (e.g. prompt: An image of an apple, factor of variation: color); and (3) methodology for comparing models in terms of human annotations via binomial tests. Furthermore, we rigorously compare various image embeddings for diversity measurement. Notably, our principled approach enables ranking of T2I models by diversity, identifying categories where they particularly struggle. This research offers robust methodology and insights, paving the way for improvements in T2I model diversity and metric development. Keywords: Diversity evaluation, Text-to-image models, Human evaluation, Benchmark 1. Measuring diversity in text-to-image models Figure 1 Evaluating diversity requires specifying both the concept being assessed and the factor of variation to reduce ambiguity in the annotation process. Output diversity is widely considered desirable for text-to-image (T2I) generation models aiming to accurately represent the natural variability of entities in the real world. This is crucial not only technically, for serving as faithful world models, but also for downstream applications like supporting creative processes and ensuring broad conceptual representation across contexts. For example, diverse model generating an image of house should produce variations in architectural style and background. However, current diversity metrics often conflate it with other properties like fidelity (e.g., FrÃ©chet Inception Distance (FID) (Heusel et al., 2017)). While progress has been made by Corresponding author(s): isabelaa@google.com 2025 Google. All rights reserved Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation developing dedicated metrics (e.g., Vendi Score (Friedman and Dieng, 2022)), the conditions for measuring diversity remain poorly defined and lack standardization, highlighting the need for principled framework. In particular, previous work often measures the variability of generated images in scenarios that do not explicitly account for diversity. For instance, images may be generated using prompt set that neither requires nor controls for output variations (e.g., Astolfi et al., 2024; Sadat et al., 2024), or models may be compared using generic human evaluation template that does not specifically probe for diversity (e.g., Betker et al., 2023). This can result in measures of diversity that are ambiguous or inconclusive (see Fig. 1). To address this challenge, we propose framework to measure diversity without conflating constructs (Jalali et al., 2024; Mironov and Prokhorenkova, 2024; Vrijenhoek et al., 2024; Zhao et al., 2024a,b): we operate under the premise that systematically evaluating diversity requires specifying both the concept being assessed and the attribute of interest, as illustrated in Fig.1. We empirically validate this by demonstrating that human accuracy in evaluating diversity is at chance level when the attribute is not defined. Building on this observation, we introduce novel evaluation framework designed to measure the per-attribute intrinsic diversity of T2I models. This framework includes synthetically generated prompt set spanning common concepts and their variations, as well as human evaluation template. The template, informed by empirical findings on golden set, improves human accuracy by dividing the evaluation into two subtasks: counting and counts comparison. Considering the high cost of human evaluations for model ranking, developing automated metrics that accurately reflect human judgment is crucial for advancing T2I models. While various diversity metrics have been proposed (Friedman and Dieng, 2022; Jalali et al., 2024), their alignment with human perceptions of diversity often remains unevaluated. To address this, we use our proposed human evaluation template and prompt set to examine the reliability of autoevaluation metrics. Specifically, we investigate the Vendi Score (Friedman and Dieng, 2022), widely adopted diversity metric (Hemmat et al., 2024; Kannen et al., 2024b) whose correlation with human-perceived diversity has not yet been thoroughly established. Our analysis reveals that the Vendi Score, when optimized for the appropriate representation space, can achieve approximately 65% accuracy in capturing human diversity judgments. We also find that the accuracy improves to 80% when the model pairs are more different, highlighting the need for more discriminant representations. Furthermore, we apply our framework to compare five recent generative models: Imagen 3 (Baldridge et al., 2024), Imagen 2.5 (Vasconcelos et al., 2024), Muse 2.2 (Chang et al., 2023), DALLE3 (Betker et al., 2023), and Flux 1.1 (Labs, 2024). This comparison identifies Imagen 3 and Flux 1.1 as the top-performing models regarding attribute diversity. We believe our framework provides robust foundation for future work in developing more human-aligned evaluation metrics and improving T2I model diversity. This research makes three key contributions: We formalize the problem of quantifying diversity in T2I models and introduce practical evaluation framework based on pre-defined factors of variation. We introduces an evaluation framework consisting of the first human evaluation template tailored for diversity, prompt set covering 86 concept-factor variation pairs, and statistical hypothesis test to compare models. We use the proposed framework to collect comprehensive dataset of 24591 human annotations comparing 5 prominent T2I models and use this data to rank automatic evaluation metrics. Prompts are available in the Supplementary Material and the full benchmark (annotations, images, and prompts) will be released upon publication. Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation 2. The three ingredients for diversity evaluation To evaluate diversity, our framework is based on three components: definition of what specific diversity is being measured, prompt set to elicit relevant outputs, and human evaluation template for reliably comparing models. These are described below. 2.1. clearly specified problem: Diversity per attribute Prelude: formalizing diversity. Consider set of images ğ‘‹ = {ğ‘¥1, ğ‘¥2, . . . , ğ‘¥ğ‘›}, where each image ğ‘¥ğ‘– belongs to space â„ğ·. We posit that the visual appearance of each image ğ‘¥ğ‘– is primarily determined by set of ğ¾ underlying independent generative factors ğ‘“ğ‘– = { ğ‘“ 1 ğ‘– }. potential generative model could be formulated as: ğ‘– , . . . , ğ‘“ ğ¾ ğ‘(ğ‘¥ğ‘–) = ğ¾ (cid:214) ğ‘˜=1 ğ‘(ğ‘¥ğ‘– ğ‘“ ğ‘˜ ğ‘– ) ğ‘( ğ‘“ ğ‘˜ ğ‘– ). (1) We focus on scenarios where images represent scenes containing instances from well-defined concepts (e.g., bottle, forest). Given concept, we can often map these abstract generative factors to concrete, observable attributes. For instance, an image ğ‘¥ğ‘– depicting bottle can be described by attributes such as: ğ‘“ material {glass, plastic, metal}, ğ‘“ shape {cylindrical, square}, and ğ‘“ state {open, closed}. Let ğ¶ = {ğ‘1, . . . , ğ‘ğ½ } be the set of concepts, ğ´ ğ‘— = {ğ‘ ğ‘—,1, . . . , ğ‘ ğ‘—,ğ¾ } the relevant attributes for given concept ğ‘ ğ‘—, and ğ‘‰ ğ‘—,ğ‘˜ the finite set of possible values for attribute ğ‘ ğ‘—,ğ‘˜. Each image ğ‘¥ğ‘– depicting concept is associated with specific value ğ‘£ ğ‘—,ğ‘˜ ğ‘– ğ‘‰ ğ‘—,ğ‘˜ for each attribute ğ‘ ğ‘—,ğ‘˜. We define sample of images ğ‘‹ ğ‘— (for the same concept ğ‘ ğ‘—) as perfectly diverse if it comprehensively covers all attribute variations. More precisely, for every attribute ğ‘ ğ‘—,ğ‘˜ ğ´ ğ‘— and every possible value ğ‘£ ğ‘‰ ğ‘—,ğ‘˜ there must exist at least one image ğ‘¥ ğ‘— ğ‘– ğ‘‹ ğ‘— such that the attribute ğ‘ ğ‘—,ğ‘˜ for image ğ‘¥ ğ‘— takes the value ğ‘£. ğ‘– tractable notion of diversity. Measuring diversity across the complete set of generative factors underlying natural data is significantly challenging. Firstly, the sheer number of potential factors (ğ¾) is often immense. Secondly, as highlighted by Tsirigotis et al. (2024), the combination of their possible values grows exponentially, leading to curse of generative dimensionality where no realistic finite sample can cover all possible combinations. Thirdly, many factors may inherently possess continuous value ranges, making exhaustive coverage impossible even for single factor. Given these challenges, and since achieving the perfect diversity (as defined earlier) is intractable with finite sample, we instead propose to measure tractable diversity. This approach focuses on carefully selected subset of the most salient and practically relevant generative factors (ğ¾) for specific concept. Identifying which factors are practically relevant is non-trivial and must be tailored for given use case. In this work, to identify these factors, we focus on commonly observed concepts reflective of T2I model training data. To effectively sample from the distribution of generative factors within these concepts, we leverage the knowledge encoded by Large Language Models (LLMs) (Rassin et al., 2024). Specifically, we prompt an LLM (Gemini 1.5 (Team et al., 2024)) to identify relevant aspects of variation for evaluating the diversity of given concept. The full system instruction is given in the Appendix. 2.2. systematically generated prompt set Our goal is to rigorously evaluate generative models and diversity metrics, specifically focusing on their ability to represent variation within distinct attributes of concepts. To effectively rank these models and metrics, our framework must accommodate both precisely controlled scenarios and 3 Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation complex, real-world use cases. We deliberately select concepts that are ubiquitous in everyday life and common image datasets, such as ImageNet (Deng et al., 2009) (e.g., fruit, car, snake), thereby anchoring our evaluation in practical utility. However, simple concepts alone are insufficient. They must also possess inherent complexity and variability, presenting genuine challenge to the models and metrics. The chosen concepts and their attributes need to be sufficiently nuanced to allow our methodology to clearly reveal performance differences and track improvements over time or across different systems. To structure this process, we classify concepts into three widely applicable categories: Food and Drink (e.g.coffee cup, cake), Nature (elements e.g.river, butterfly), and Human-made Objects (e.g.bridge, laptop). We leverage the generative capabilities of Large Language Models (LLMs) to systematically produce wide range of concepts within these categories, producing concrete, ImageNet-like concepts, which are typically visualizable nouns, similar in scope to those in large-scale image datasets. For each generated concept, the LLM is used to identify semantically relevant aspect of variation (attribute) that is intrinsic or commonly associated with that concept. This yields conceptattribute pairs (ğ‘ ğ‘—, ğ‘ ğ‘—,ğ‘˜) such as: (apple, color), (tree, species), (coffee cup, material), (chair, style). This LLM-driven process allows us to systematically build prompt set specifically designed to probe and evaluate diversity along meaningful, contextually relevant dimensions for broad range of common concepts. Finally, the authors manually verified all concept-attribute pairs and removed 5 where the attribute was potentially difficult / ambiguous to categorize (e.g. (food, cuisine)). The specific prompt used can be found in Appendix D.1. Additionally, in Appendix D.2 we discuss the sufficiency of our prompt set to discriminate models. Figure 2 Each slice represents concept, grouped and color-coded by its overall category. 2.3. validated, bespoke human evaluation template Prior work has shown that developing an appropriate human evaluation template is an essential component in the process of measuring desired capability of generative model (Clark et al., 2021; Wiles et al., 2024). To that end, we develop human evaluation template that: (a) allows annotators to understand the task well, (b) captures their judgment faithfully, and (c) yields meaningful ground truth annotations for per-attribute diversity, subsequently used to validate automated evaluation metrics. The annotators are provided with 4 options for the side-by-side comparison: (i) Left more diverse, (ii) Right more diverse, (iii) Equally diverse, (iv) Unable to answer. Visualizations of the template can be seen in Appendix B.2. template to measure per-attribute diversity. Our template for measuring per-attribute diversity 4 Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation employs comparative, side-by-side approach due to the difficulty of evaluating diversity within single set. Many existing diversity metrics also require reference set. We considered the following design choices for our human evaluation template to ensure meaningful assessment (1) Set size: Balancing the perception of diversity with minimizing annotation fatigue and enabling robust computation for metrics requiring larger sets (e.g., Vendi score). (2) Attribute specification: Explicitly stating the attribute for evaluation versus allowing open-ended diversity assessment. (3) Anchoring task: Incorporating an intermediate task to guide annotators to focus on the intended attribute. Validating the template with golden set. To evaluate the quality of the evaluation template, we curate golden set of 10 <concept, aspect> pairs, where concept corresponds to concept that should be considered common across images in set and aspect describes the associated aspect of variation that we want to measure diversity against. The full list of concepts and aspects of variation can be found in Appendix B.1. We validate the evaluation template by comparing cases where (i) the concept remains constant across images in the set while the aspect varies (ii) the concept varies across images while the aspect remains the same, and (iii) both the concept and the aspect vary across images within the set. We expect images in set (i) to be considered more diverse than images in set (ii), and similarly images in set (iii) to be considered more diverse than images in set (ii). Finally, we expect that images in sets (ii) and (iii) are considered equally diverse as we want to focus on the aspect as axis of variation. In Fig. 3, we present the annotation accuracy of human experts using our template under various conditions, considering the aforementioned definitions as ground truth. The different templates are shown in Fig. 9. The accuracy for the w/o aspect task is 30.0% for comparisons of sets of size 4 and 26.7% for sets of size 8. In contrast, the template that includes the aspect shows significant increase in accuracy (82.5% for set size 4 and 53.3% for set size 8), indicating that explicitly mentioning the desired aspect of variation improves accuracy. This improvement likely stems from preventing annotators from unintentionally conflating the concept and the aspect when not guided to focus on specific axis. Furthermore, we observe that adding the count anchoring question enhances accuracy, especially for the set size of 8, reaching 77.9%. Figure 3 Match with the golden set depending on different set sizes. For the count task, we found strong (ğœŒ = 0.88) and statistically significant (ğ‘ < .001) correlation between the annotators final diversity comparison and the comparison inferred from their individual subset counts (where higher count on one side implies more diverse final response for that side, and equal counts imply equal diversity). This confirms that the anchoring count question effectively guides annotators. To further validate our setup, we analyzed instances where annotators responses deviated from the ground truth in our golden set. We examined the distributions of attribute counts for two image subsets: (1) those labelled diverse in the ground truth, where we expected count mode of 8 and (2) those labelled non-diverse, where we expected mode of 1. The results of this analysis are presented in Fig. 4. While generally, annotator responses aligned with the golden set labels, we observed few exceptions. For instance, in one case labelled as diverse set of chairs, all annotators counted only 3 or 4 distinct chair types, indicating lower diversity than expected. Upon closer inspection, these chairs appeared visually similar despite potentially different underlying material prompts (e.g., metal, iron, aluminum). 5 Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation (a) The diverse golden set. (b) The non-diverse golden set. Figure 4 The distribution of counts for sets of images labelled as diverse or non-diverse in the golden set for the pilot study. 3. Our framework in practice We demonstrate our frameworks practical application by: (i) collecting comprehensive human annotations with our template to compare models, (ii) using these annotations as ground truth to evaluate diversity metrics, and (iii) comparing model rankings from human versus automatic evaluations to highlight the gap between human-perceived diversity and current metric capabilities. 3.1. Ranking models via human evaluation With the proposed prompt set from Sec. 2.2 and the human evaluation template introduced in Sec. 2.3, we evaluate the attribute-based diversity of five generative models, namely: Muse 2.2 (Chang et al., 2023), Imagen 2.5 (Vasconcelos et al., 2024), Imagen 3 (Baldridge et al., 2024), DALLE3 (Betker et al., 2023), and Flux 1.1 (Labs, 2024). For each model, we generate 20 distinct samples for each prompt, randomly combine them in 10 different sets of 8 images, and run side-by-side evaluations for all 10 combinations of 2 models. For each side-by-side comparison, evaluations from 5 different raters were collected. Raters had access to slide deck with instructions to perform the task and were compensated for the time invested in the data collection. In total, 24591 annotations were collected in our study from 20 different annotators, including the pilot runs. The average time to complete the task with the final template was 32 seconds More details can be found in the Appendix (Sec.A). Before comparing each model pair in terms of diversity, we evaluate the overall annotations quality by computing the inter-annotator agreement via Krippendorff alpha reliability (ğ›¼) (Hayes and Krippendorff, 2007). In Fig. 5a, we observe that for all cases ğ›¼ > 0.8, indicating high-degree of agreement across annotators (Marzi et al., 2024). Ratings aggregation. Given the high levels of inter-annotator agreement for all runs of the human evaluation, we aggregate annotations for each side-by-side comparison across raters by taking the mode of the ratings. We then follow this step with second aggregation, this time at the level of all side-by-side comparisons for each concept. For instance, when comparing given model pair, there are 10 side-by-side comparisons for the concept apple (each side-by-side comparison here corresponds to the evaluation of two sets of 8 images). At the end of this process, for the considered models pair, we obtain single human evaluation result for each concept in the prompt set. 6 Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation 1 1 . F 3 a Flux 1. Imagen 3 DALLE3 Muse 2.2 Imagen 2.5 = < < < = = < < 3 A > = = < 2 2 . M > > = = . 5 2 a I > > > = (a) Krippendorff ğ›¼-reliability. (b) Binomial test results at 95% confidence level. Figure 5 Human evaluation results. (a) Inter-annotator agreement results in terms of Krippendorff ğ›¼-reliability. (b) We compare model rankings in terms of significance in the number of wins with twosided Binomial tests under 95% confidence level. Each entry in the grid represents comparison between two models. The sign indicates the model in the row is better (>), worse (<), or not significantly different (=) than the model in the column. Model ranking. Using the results from the ratings aggregation, we propose to use Binomial tests to verify the following hypothesis: there is significant difference between the outcomes of given pair of models. To do so, we count the number of categories for which each model was deemed best and perform two-sided Binomial test under the null-hypothesis that the rate for which each model is the best for concept is equal to 50% (i.e. both models have equal win rates). Results considering 95% confidence level for all tests are shown is Fig. 5b. Imagen 3 and Flux 1.1 are significantly better or not worse than all other models. Imagen 2.5 and Muse 2.2 are not significantly better than any contender, showing that our benchmark is able to capture an overall progress in diversity when comparing newer and older models. DALLE3 is significantly better than Imagen 2.5, but does not significantly surpass the performance of the other models considered for comparison. 3.2. Comparing autoevaluation metrics While human evaluation is often considered gold standard, it can be impractical to rely solely on human annotation. We then leverage the collected human annotations to perform an extensive study of the role of embeddings for the Vendi Score 1. Autoraters based on the Vendi Score. Given set of images ğ‘‹ ğ‘—,ğ‘˜ = {ğ‘¥ ğ‘—,ğ‘˜ ğ‘– } (corresponding to given model, concept ğ‘ ğ‘— and attribute ğ‘ ğ‘—,ğ‘˜ ğ´ ğ‘—), we extract embeddings â„Î (ğ‘¥ ğ‘—,ğ‘˜ ) for each image. â„Î is ğ‘– pretrained feature extractor that can be dependent on set of conditions Î = {ğœ‰ğ‘™} (ğ¶ ğ´) {ğœ‰0} where ğœ‰0 is condition unrelated to the considered categories and attributes that can be added to test the impact of conditioning. The different feature extractors and conditions we used are detailed in the following paragraph, but here are few generic examples to clarify the notation: (i) â„Î takes only images as input. In this case, Î = . (ii) â„Î is vision and language model. In this case, embeddings can be conditioned on text data that depends on the concept only (i.e., Î = {ğ‘ ğ‘—}), attribute only (i.e., Î = {ğ‘ ğ‘—,ğ‘˜}), or both concept and attribute (i.e., Î = {ğ‘ ğ‘—, ğ‘ ğ‘—,ğ‘˜}). To test the impact of conditioning on text, we can instead choose an unrelated prompt (i.e., using Î = {ğœ‰0}). Finally, we aggregate the embeddings using diversity metric to obtain score for the set. As we do not have access to reliable reference in our setting, we use the Vendi Score (Friedman and Dieng, 2022), reference-free 1Results with other autoraters can be found in the Appendix Sec.E. 7 Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation and widely adopted metric (Hemmat et al., 2024; Jalali et al., 2024; Kannen et al., 2024a; Pasarkar and Dieng, 2023). The Vendi Score is defined as follows: Definition 1 (Adapted from (Friedman and Dieng, 2022), Definition 3.1). Given concept ğ‘ ğ‘—, an attribute ğ‘ ğ‘—,ğ‘˜ and set of conditions Î, let {ğ‘¥ ğ‘—,ğ‘˜ , . . . , ğ‘¥ ğ‘—,ğ‘˜ ğ‘› } denote set of images representing given 1 concept and attribute. Let ğ‘˜ : ğ‘‹ ğ‘‹ â„ be the cosine similarity between the embeddings of two images, ğ¾Î â„ğ‘›ğ‘› be the kernel matrix, with ğ¾Î ğ‘› be the eigenvalues of ğ¾Î/ğ‘›. , . . . , ğ‘¥ ğ‘—,ğ‘˜ The Vendi Score for the set {ğ‘¥ ğ‘—,ğ‘˜ ğ‘™ğ‘š = ğ‘˜Î (ğ‘¥ ğ‘—,ğ‘˜ ğ‘› } is defined as: ğ‘š ), and let ğœ† Î 1, . . . , ğœ† Î , ğ‘¥ ğ‘—,ğ‘˜ ğ‘™ ğ‘ Î (ğ‘¥ ğ‘—,ğ‘˜ , . . . , ğ‘¥ ğ‘—,ğ‘˜ ğ‘› ) = exp( ğ‘› ğ‘–=1 ğ‘– log ğœ† Î ğœ† Î ğ‘– ). (2) Experimental setup. We compare three different types of embeddings. First, we compare embeddings obtained using only the image input. Here we consider two models trained for ImageNet classification the ImageNet Inception model introduced in Szegedy et al. (2015) and an ImageNet ViTB/16 model trained on ImageNet21K as described in Steiner et al. (2022). We also consider one self-supervised model, DINOv2 (Oquab et al., 2023). Second, we consider embeddings conditioned on both the image and textual attribute. We use PALI embeddings Beyer et al. (2024) at various points after fusing the text and visual input, and CLIP (Radford et al., 2021) combined text and image embedding. We use these embedding models to obtain an embedding for each image in set. We then use the Vendi Score in order to aggregate embeddings and obtain diversity prediction for the set. Finally, we consider the first word output by the PALI model as discrete token. We aggregate these outputs by counting the number of unique words generated for set to get an estimate for diversity. 1 ) > ğ‘ Î (ğ‘‹ ğ‘—,ğ‘˜ For each pair of image sets, we analyze the agreement between diversity assessment based on our autoraters, and the assessment resulting from the human annotations, not taking into account pairs where the annotators found the sets to be equally diverse. If the autoraters and the human evaluations both indicate the same set as being the most diverse (i.e., ğ‘ Î (ğ‘‹ ğ‘—,ğ‘˜ 2 ) and annotators rated generated with model 1 based on concept ğ‘ ğ‘— and attribute ğ‘ ğ‘—,ğ‘˜ as more diverse than ğ‘‹ ğ‘—,ğ‘˜ the set ğ‘‹ ğ‘—,ğ‘˜ 2 1 generated with model 2 based on the same concept and attribute), we say that for that pair of sets, the autorater is correct, else it is incorrect. We then report accuracy by aggregating the number of pairs for which the autoraters are correct. Results are reported in Figs. 6a-6c. We can see that, on the diverse golden set, the ViT model does the best, and then the tokens of PALI. This is perhaps surprising, as the ViT model is not specifically trained to focus on the aspects we are considering for diversity but to be able to discriminate between broad classes. However, we see minimal difference in results if we consider the model data. All approaches perform similarly and lead to accuracies that are not significantly different. We hypothesize that the reason for the observed small difference in results was that the models were similar to each other. As result, we looked at ratings where the annotators perceived larger gap between models by using the counts as proxy. We consider subset of the data where the difference in counts between the two sets is greater than 4, keeping about 24% of the data. We find that now, on the model data we see bigger difference in results. First, all autoraters are more accurate. Second, we can see that again the image based approaches (e.g., the Inception model, the DINO model and ViT model) perform best. In Figs. 7 and 15 we visualize examples for four side-by-side comparisons where the corresponding autoraters indicate that group of images have highest or lowest diversity. We can see that results are reasonable and that in general, images with low diversity arise due to mode collapse, i.e. the model generates very similar image for the same concept. This could explain why the Inception model performs poorly on the pilot data but well on the model comparison data. Inception features are Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation (a) The diverse golden set. (b) Side-by-side model comparisons. (c) Side-by-side model comparisons with diversity gap > 4. Figure 6 Autoevaluation results: the performance of the Vendi Score given different embeddings across three settings: (a) the golden set; (b) all the annotations gathered; (c) the easy subset of the annotations where raters identified diversity gap of > 4 for pair. On the golden set, ViT performs best but this does not transfer to side-by-side comparisons. The performance is generally better on the easy split of the data, showing that the embeddings perform considerably worse when the difference between the generated sets of images is more subtlemodels are more similar. effective for identifying these issues but no effective for identifying diversity in the case of confounding aspects (e.g., the background is changing while the animal is staying the same). Model 2 diverse sets 2 non-diverse sets ViT Clothes (Mat/Type/Style/Tex) Zebra (Pose) Mountain (Height) Moon (Phase) Figure 7 Qualitative results for different autoraters on the T2I annotated dataset, showing two very diverse and two non diverse sets as determined by the ViT-based autorater. 3.3. Ranking models with autoevaluation approaches Ranking is achieved by counting the frequency at which the left model on the left achieves higher score than the model on the top, i.e. for \"model 1\" on the left axis and \"model 2\" on the top, we count how many times ğ‘ Î (ğ‘‹ ğ‘—,ğ‘˜ generated with 1 model 2, and subtracting 0.5. The win rate matrices with all models and the score distributions for Imagen 3 and Flux 1.1, the two models that were preferred by human annotators, are shown in Sec. E.5 in the Appendix. In order to test the significance, we aggregate the scores per concept and perform Wilcoxon signed-rank test under 95% confidence level. On the left panel, we consider generated with model 1, and ğ‘‹ ğ‘—,ğ‘˜ 2 ), with ğ‘‹ ğ‘—,ğ‘˜ 1 ) > ğ‘ Î (ğ‘‹ ğ‘—,ğ‘˜ 9 Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation 1 . 1 F > = < < 3 a I < < < < 3 A = > < < 2 . 2 M > > > = 5 . 2 a > > > = Flux 1.1 Imagen 3 DALLE3 Muse 2. Imagen 2.5 1 . 1 F = = < < 3 a = = < < 3 A = = < < 2 . 2 M > > > < 5 . 2 a > > > > Flux 1. Imagen 3 DALLE3 Muse 2.2 Imagen 2.5 1 . 1 F > = < < 3 a I < < < < 3 A = > < < 2 . 2 M > > > < 5 . 2 a > > > > Flux 1.1 Imagen 3 DALLE3 Muse 2. Imagen 2.5 (a) Inception embeddings. (b) PALI(emb1) embeddings - conditioned on attribute. (c) PALI(emb1) embeddings - conditioned on object and attribute. Figure 8 Ranking by autoevaluation. We compare model pairs given the Vendi Score based on (a) Inception, (b) PALI(emb1) conditioned on the attribute, and (c) PALI(emb1) conditioned on object and attribute. Each entry in grid represents comparison between two models. Significance is tested via the Wilcoxon signed-rank under 95% confidence level. The sign indicates the model in the row is better (>), worse (<), or not significantly different (=) than the model in the column. the ImageNet Inception embeddings, as they yielded the highest accuracy on the model data. In the middle and the right panels, we consider text-conditioned embeddings, as they are closest to our human evaluation procedure. We show the results using PALI(emb1), as they show marginal advantage on model data. On the middle panel, we show the results corresponding to conditioning the embedding model on the attribute only, while on the right panel, conditioning takes into account both attribute and object. Results with other embeddings can be found in the Appendix (Sec E.5). Through the autoevaluation model ranking, we find that independently of the chosen embedding, Imagen 3 is not worse than all other models, and Flux 1.1, Imagen 3 and DALLE3 are better than Imagen 2.5 and Muse 2.2. We also observe that using the ImageNet Inception embeddings and the PALI(emb1) with conditioning on object and attribute captures more differences across the 3 top models, and that using both types of the PALI(emb1) embeddings captures more differences between Imagen 2.5 and Muse 2.2. By adopting the model comparison results obtained with the human annotations as shown Fig. 5b as ground-truth, we find that all used embeddings are of similar quality in terms of closeness to human perception of diversity. They all did not flip conclusions, but the autoevaluation approach seems more sensitive to certain variations depending on the choice of embedding model and conditioning. Text conditioning, while closest to the human evaluation procedure, did not show significant advantage with the current choice of embedding models and conditioning. However, we observe in Fig. 8 the influence of the conditioning. The additional results in the Appendix (Sec. E.5) show the influence of the choice of embedding models. It is possible that better choices of models and conditioning prompts can lead to better results, but we leave this question open for future investigation. 4. Related work The primary method for evaluating text-to-image models involves gathering human judgments on specific benchmark (i.e., set of prompts). Previous research highlights that the composition of this benchmark significantly influences the resulting model rankings. This has led to the development of benchmarks with broader skill coverage, e.g., text rendering and spatial reasoning (Cho et al., 2023; Li et al., 2024; Wiles et al., 2024), as well as benchmarks targeting specific skills like numerical reasoning (KajiÄ‡ et al., 2024). Although human evaluation remains the gold standard, numerous automatic metrics have been proposed to potentially replace human judgments, at least for certain 10 Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation applications (e.g., Hessel et al., 2021; Huang et al., 2023; Lin et al., 2024; Senthilkumar et al., 2024; Wiles et al., 2024). Rigorous validation of these metrics is crucial across diverse conditions, including different prompt sets, human evaluation templates, and models (Wiles et al., 2024). An important facet of evaluating text-to-image models involves measuring the diversity of their output (Dombrowski et al., 2024; Vice et al., 2024). This has resulted in different metrics, both reference-based (Heusel et al., 2017; Sajjadi et al., 2018; Salimans et al., 2016) and reference-free (Friedman and Dieng, 2022; Limbeck et al., 2024; Mironov and Prokhorenkova, 2024; Ospanov et al., 2025; Rassin et al., 2024). The advantage of reference-free metrics is their independence from ground-truth set, which permits the evaluation of diversity in broader contexts. One such recent metric, the Vendi score (Friedman and Dieng, 2022), has influenced subsequent research (Hemmat et al., 2024; Jalali et al., 2024; Kannen et al., 2024a). Despite these developments, none of the proposed metrics have undergone thorough evaluation, frequently being tested only on generic prompts or in simplified settings. Moreover, surprisingly, the majority of previous studies lack human evaluation to demonstrate the validity of these metrics. To address this gap, we introduce prompt set designed for evaluating diversity across particular attributes and propose and validate human evaluation template to gather ground-truth diversity judgments. Finally, we compare existing metrics and models under various conditions. 5. Discussion Ensuring diversity in text-to-image (T2I) model outputs is essential, serving as measure of their ability to express real-world variety. However, rigorous evaluation of this diversity, particularly for specific attributes, remains challenging. This paper introduces novel framework for attribute-specific T2I diversity evaluation. It comprises systematic prompt set and human evaluation template, which has been validated to significantly improve the accuracy of human judgments by explicitly defining the attribute of interest. This framework provides crucial ground truth for understanding and measuring diversity beyond general impressions. Applying this framework, we ranked prominent T2I models based on their attribute-specific diversity, identifying Imagen 3 and Flux 1.1 as strong performers. Furthermore, we leveraged our human data to evaluate automated evaluation approaches based on the Vendi Score. Our results demonstrate that the choice of embedding space, upon which autoevaluation metrics operate, is crucial for achieving results that broadly align with human judgments. Notably, our findings indicate that Vendi Score-based autoevaluation approaches can capture human-perceived diversity with approximately 80% accuracy and correctly yield similar results for pairwise model comparisons when comparable statistical analysis methodology is employed. The proposed framework and our collected data are intended to encourage future work on both T2I model improvement and the development of more reliable evaluation metrics. The broad impact of this work lies in its potential to improve T2I model quality in terms of diversity by providing an evaluation framework grounded in human perception. Moreover, unlike the previous work that often relies on attribute classifiers (e.g., gender), our evaluation methodology can be employed to measure demographic diversity in classification-free manner in future research. 6. Ethics Statement This work involved data collection from human annotators. Each one of the 20 different participants has been compensated for the time invested in the experiment according to the minimum wage in their geographical location. Before completing the annotation task, annotators were given comprehensive set of instructions and could take as much time as necessary to complete the task. 11 Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation"
        },
        {
            "title": "References",
            "content": "P. Astolfi, M. Careil, M. Hall, O. MaÃ±as, M. Muckley, J. Verbeek, A. R. Soriano, and M. Drozdzal. Consistency-diversity-realism pareto fronts of conditional image generative models. arXiv preprint arXiv:2406.10429, 2024. J. Baldridge, J. Bauer, M. Bhutani, N. Brichtova, A. Bunner, L. Castrejon, K. Chan, Y. Chen, S. Dieleman, Y. Du, et al. Imagen 3. arXiv preprint arXiv:2408.07009, 2024. J. Betker, G. Goh, L. Jing, T. Brooks, J. Wang, L. Li, L. Ouyang, J. Zhuang, J. Lee, Y. Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. L. Beyer, A. Steiner, A. S. Pinto, A. Kolesnikov, X. Wang, D. Salz, M. Neumann, I. Alabdulmohsin, M. Tschannen, E. Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. H. Chang, H. Zhang, J. Barber, A. Maschinot, J. Lezama, L. Jiang, M.-H. Yang, K. Murphy, W. T. Freeman, M. Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. J. Cho, Y. Hu, R. Garg, P. Anderson, R. Krishna, J. Baldridge, M. Bansal, J. Pont-Tuset, and S. Wang. Davidsonian scene graph: Improving reliability in fine-grained evaluation for text-image generation. arXiv preprint arXiv:2310.18235, 2023. E. Clark, T. August, S. Serrano, N. Haduong, S. Gururangan, and N. A. Smith. All thats human is not gold: Evaluating human evaluation of generated text. In C. Zong, F. Xia, W. Li, and R. Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2021. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. M. Dombrowski, W. Zhang, S. Cechnicka, H. Reynaud, and B. Kainz. Image generation diversity issues and how to tame them. arXiv preprint arXiv:2411.16171, 2024. D. Friedman and A. B. Dieng. The vendi score: diversity evaluation metric for machine learning. arXiv preprint arXiv:2210.02410, 2022. A. F. Hayes and K. Krippendorff. Answering the call for standard reliability measure for coding data. Communication methods and measures, 1(1):7789, 2007. R. A. Hemmat, M. Hall, A. Sun, C. Ross, M. Drozdzal, and A. Romero-Soriano. Improving geo-diversity of generated images with contextualized vendi score guidance. arXiv preprint arXiv:2406.04551, 2024. J. Hessel, A. Holtzman, M. Forbes, R. L. Bras, and Y. Choi. CLIPScore: reference-free evaluation metric for image captioning. In EMNLP, 2021. M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 12 Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation K. Huang, K. Sun, E. Xie, Z. Li, and X. Liu. T2i-compbench: comprehensive benchmark for openworld compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:7872378747, 2023. M. Jalali, A. Ospanov, A. Gohari, and F. Farnia. Conditional vendi score: An information-theoretic approach to diversity evaluation of prompt-based generative models. arXiv preprint arXiv:2411.02817, 2024. I. KajiÄ‡, O. Wiles, I. Albuquerque, M. Bauer, S. Wang, J. Pont-Tuset, and A. Nematzadeh. Evaluating numerical reasoning in text-to-image models. Advances in Neural Information Processing Systems, 37:4221142224, 2024. N. Kannen, A. Ahmad, M. Andreetto, V. Prabhakaran, U. Prabhu, A. B. Dieng, P. Bhattacharyya, and S. Dave. Beyond aesthetics: Cultural competence in text-to-image models. arXiv preprint arXiv:2407.06863, 2024a. N. Kannen, A. Ahmad, V. Prabhakaran, U. Prabhu, A. B. Dieng, P. Bhattacharyya, S. Dave, et al. Beyond aesthetics: Cultural competence in text-to-image models. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024b. B. F. Labs. Flux. https://github.com/black-forest-labs/flux, 2024. B. Li, Z. Lin, D. Pathak, J. Li, Y. Fei, K. Wu, T. Ling, X. Xia, P. Zhang, G. Neubig, and D. Ramanan. Genai-bench: Evaluating and improving compositional text-to-visual generation, 2024. URL https://arxiv.org/abs/2406.13743. K. Limbeck, R. Andreeva, R. Sarkar, and B. Rieck. Metric space magnitude for evaluating the diversity of latent representations. Advances in Neural Information Processing Systems, 37:123911123953, 2024. Z. Lin, D. Pathak, B. Li, J. Li, X. Xia, G. Neubig, P. Zhang, and D. Ramanan. Evaluating text-to-visual In European Conference on Computer Vision, pages generation with image-to-text generation. 366384. Springer, 2024. G. Marzi, M. Balzano, and D. Marchiori. K-alpha calculatorkrippendorff alpha calculator: userfriendly tool for computing krippendorff alpha inter-rater reliability coefficient. MethodsX, 12: 102545, 2024. M. Mironov and L. Prokhorenkova. Measuring diversity: Axioms and challenges. arXiv preprint arXiv:2410.14556, 2024. M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. A. Ospanov, J. Zhang, M. Jalali, X. Cao, A. Bogdanov, and F. Farnia. Towards scalable referencefree evaluation of generative models. Advances in Neural Information Processing Systems, 37: 120892120927, 2025. A. P. Pasarkar and A. B. Dieng. Cousins of the vendi score: family of similarity-based diversity metrics for science and machine learning. arXiv preprint arXiv:2310.12952, 2023. A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 13 Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation R. Rassin, A. Slobodkin, S. Ravfogel, Y. Elazar, and Y. Goldberg. Grade: Quantifying sample diversity in text-to-image models. arXiv preprint arXiv:2410.22592, 2024. S. Sadat, J. Buhmann, D. Bradley, O. Hilliges, and R. M. Weber. Cads: Unleashing the diversity of diffusion models through condition-annealed sampling. In The Twelfth International Conference on Learning Representations, 2024. M. S. M. Sajjadi, O. Bachem, M. Lucic, O. Bousquet, and S. Gelly. Assessing generative models via precision and recall, 2018. URL https://arxiv.org/abs/1806.00035. T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. N. K. Senthilkumar, A. Ahmad, M. Andreetto, V. Prabhakaran, U. Prabhu, A. B. Dieng, P. Bhattacharyya, and S. Dave. Beyond aesthetics: Cultural competence in text-to-image models. Advances in Neural Information Processing Systems, 37:1371613747, 2024. A. P. Steiner, A. Kolesnikov, X. Zhai, R. Wightman, J. Uszkoreit, and L. Beyer. How to train your vit? data, augmentation, and regularization in vision transformers. Transactions on Machine Learning Research, 2022. C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 19, 2015. G. Team, P. Georgiev, V. I. Lei, R. Burnell, L. Bai, A. Gulati, G. Tanzer, D. Vincent, Z. Pan, S. Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. C. Tsirigotis, J. Monteiro, P. Rodriguez, D. Vazquez, and A. C. Courville. Group robust classification without any group information. Advances in Neural Information Processing Systems, 36, 2024. C. N. Vasconcelos, A. Rashwan, A. Waters, T. Walker, K. Xu, J. Yan, R. Qian, Y. Li, S. LUO, Y. Onoe, et al. Greedy growing enables high-resolution pixel-based diffusion models. Transactions on Machine Learning Research, 2024. J. Vice, N. Akhtar, R. Hartley, and A. Mian. On the fairness, diversity and reliability of text-to-image generative models. arXiv preprint arXiv:2411.13981, 2024. S. Vrijenhoek, S. Daniil, J. Sandel, and L. Hollink. Diversity of what? on the different conceptualizations of diversity in recommender systems. In The 2024 ACM Conference on Fairness, Accountability, and Transparency, pages 573584, 2024. O. Wiles, C. Zhang, I. Albuquerque, I. KajiÄ‡, S. Wang, E. Bugliarello, Y. Onoe, C. Knutsen, C. Rashtchian, J. Pont-Tuset, et al. Revisiting text-to-image evaluation with gecko: On metrics, prompts, and human ratings. arXiv preprint arXiv:2404.16820, 2024. D. Zhao, J. T. Andrews, O. Papakyriakopoulos, and A. Xiang. Position: Measure dataset diversity, dont just claim it. arXiv preprint arXiv:2407.08188, 2024a. D. Zhao, J. T. Andrews, A. Sony, T. O. Papakyriakopoulos, and A. Xiang. Measuring diversity in datasets. In International Conference on Learning Representations, volume 1, page 36, 2024b. 14 Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation"
        },
        {
            "title": "Appendix",
            "content": "A. Human evaluation task details A.1. Instructions Before completing the annotation task, annotators were given comprehensive set of instructions including the following guidelines: The goal of the task is to compare the how diverse two sets of images are with respect to given attribute; For the given two sets of images, answer the question about how diverse the concept is with respect to the specific attribute highlighted in the prompt; You should count how many different instances of particular attribute they observe on the left and right sets of images, separately; For example, if the attribute is background and the prompt is animal, raters should count how many different backgrounds appear in each set of images and finally judge how diversity of the two sets compares to each other with respect to this attribute; Finally, based on the counts, pick one of the following options: (1) Left is more diverse; (2) Right is more diverse; (3) Equally diverse; (4) Unable to answer. Along with the written instructions, annotators were also given examples corresponding to options 1, 2, and 3. A.2. Additional information In total, 24591 annotations were collected in our study, including the pilot runs. The average time to complete the task with the final template was 32 seconds. B. Human evaluation template B.1. Golden set concept-attribute pairs We considered the following categories and aspects of variation for the golden set: <color, flower>, <material, container>, <color, language), <background, animal>, <material, chair>, <side dish, cookie shape>, <pattern, clothing>, <style, building>, <weather, biome>, <color, vehicle>. We validate the evaluation template by comparing cases where (i) the concept remains constant across images in the set while the aspect varies: images of the same flower (rose) in all considered colors (8 images per concept); (ii) the concept varies across images while the aspect remains the same: images of all considered flowers types in the red color (8 images per concept); and (iii) both the concept and the aspect vary across images within the set: images of all flowers, each one in one of the different colors (8 images per concept). For each concept we then generate 24 different images, yielding total of 240 images for the full golden set. In the table below we present all considered concepts and aspects of variations values. The specific values for each concept and attribute are presented in Table 1. For each case, images were generated using Imagen 3 with the following prompt: photorealistic image of aspect of variation value concept value. For example, photorealistic image of yellow begonia. As images were synthetically generated following carefully crafted protocol, we could 15 Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation compare the performance of human annotators as well as autoraters based on multimodal language models such as Gemini in the task of evaluating for specific aspects of variation."
        },
        {
            "title": "Vehicle",
            "content": "Begonia, Carnation Geranium, Hibiscus Lily, Poppy Rose, Tulip Beer, champagne cognac, cup doublewalled, mug shot glass, water Bonjour, hello hei, oi sawubona, hola buna, ciao Capybara, monkey dog, snake cat, lion tree, elephant Dinning, armchair office, rocking lounge, folding barstool, recliner Round, square crescent, start heart, diamond ghost, bat Tshirt, dress pants, skirt jacket, gloves sweater, scarf"
        },
        {
            "title": "Pattern",
            "content": "Skyscraper, residential industrial, commercial church, theater train station, school"
        },
        {
            "title": "Style",
            "content": "Desert, rainforest grassland, tundra swamp, coastal jungle, mountain Car, truck motorcycle, bus airplane, boat train, helicopter"
        },
        {
            "title": "Color",
            "content": "Yellow, light purple white, blue green, orange red, purple Porcelain, metal stainless steal, ceramic glass, gold copper, plastic Blue, green orange, pink purple, red white, yellow Beach, jungle park, rock room, savannah tree, water Wood, upholstered mesh, wicker leather, metal plastic, microfiber Milk, coffee tea, hot chocolate soda, fruits ice cream, walnuts Solid color blue, striped polka dot, floral plaid, checkered animal print, camouflage Modern, gothic victorian, art deco baroque, romanesque brutalist, traditional japanese Sunny, cloudy rainy, snowy foggy, stormy sunset, overcast Red, blue green, yellow white, black orange, gray Table 1 Golden set generation: concepts and respective aspects of variation. Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation B.2. User interface screenshots Figure 9 Examples of human evaluation templates used in the pilot study. In the template variant w/o aspect, only the category is provided. In the variant with count, an additional question is included for each set, prompting annotators to specify the number of distinct values observed for the target attribute within the corresponding image set. For exact examples see Figs. 10-12. 17 Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation Figure 10 screenshot of the user interface for one annotation example for the condition \"No aspect\". Figure 11 screenshot of the user interface for one annotation example for the condition \"Aspect\". Figure 12 screenshot of the user interface for one annotation example for the condition Count. 18 Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation C. Additional human evaluation results In Fig. 13 we show the histogram of counts averaged across the 5 raters each set in all side-by-side comparisons. Figure 13 Distribution of all counts annotated by human raters. D. deep dive on our curated Prompt set generation details D.1. Prompt set generation We used the following prompt to generate the concept-factor pairs: Your task is to generate dataset with prompts for evaluating text-to-image models. These prompts will be used to generate realistic images and assess the diversity of the corresponding generative model with respect to specific aspect. All prompts should correspond to realistic images. Write on the side the main object of the prompt and the aspect diversity will be measured with respect to. Here are few examples: Apple. An image of an apple. Color. Book. photograph of book. Thickness. Bowl of soup. An image of bowl of soup. Ingredients. Bridge. photograph of bridge. Shape. Building. An image of building. Style. Cake. photograph of cake. Flavour. Car. photograph of car. Type. Omit any other text. Generate at least 95 cases. Do not include categories that involve people. 19 Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation D.2. On the sufficiency of the prompt set for discriminating models In order to further show that our results are significant with the current set, we ran new versions of the model comparison with the human annotations presented in Sec. 3 with versions of our prompt set that have smaller number of concepts. More specifically, we repeated the Binomial tests (at the same significance level) after randomly removing an increasing amount of concepts, which resulted in prompt sets of size 74, 64, 54, and 24 concepts. Overall, we find that decreasing the prompt set size to 74 concepts doesnt affect any of the results. As the prompt set size further decreases, we start to see the results changing as the number of significant pairwise comparisons decreases. We observe that drastically decreasing the prompt set size makes the data no longer able to capture significant differences between models such as Imagen 3 and Imagen 2.5, as expected. In the Table 2, we show the results of the Binomial tests for the 5 different sizes of prompt set, including the full set, from left to right (i.e. the first symbol represents the result with the full set as in Fig. 5b, the second symbol the result with 74, then 64, 54, and 24 concepts). Notice that even with the smallest set we dont see contradiction in the ordering. Flux 1.1 Imagen 3 DALLE3 Muse 2.2 Imagen 2. Flux 1.1 Imagen 3 DALLE3 Muse 2.2 Imagen 2.5 =, =, >, =, < >, >, =, =, = >, >, >, >, > >, >, >, =, > =, =, <, =, > =, =, =, =, = >, >, >, >, = <, <, =, =, = =, =, =, =, = <, <, <, <, < Table 2 Repeating model comparisons across smaller versions of our prompt set. Decreasing the prompt set size to 74 concepts doesnt affect any of the results. E. Additional autoevaluation results E.1. Compute Usage We used accelerators for running automatic evaluation metrics and generating the images. We run all metrics on TPU V3 hardware2. The image generation pipeline ran on 4 TPUs. E.2. Performance for detecting equally diverse image sets We evaluate how good embeddings are at detecting equally diverse image sets. To not have threshold-dependent metric, we use the area-under-the-ROC curve (AUC). We construct the true binary label as whether the image sets are labelled as equally diverse or not. We construct the scores as the absolute difference between the metric scores. We then plot the AUC. good metric would have an AUC close to one, indicating that when the differences are small, the image sets are more likely to have been labelled as the same by the human annotators. We plot results in Figure 14, and find that no metric performs particularly well (AUC < 0.6 in all cases). However, the ImageNet Inception one performs best, presumably as it is trained to be invariant to small differences and so, as we can see in Figures 7-15, as lack of diversity usually arises when images are very similar, the embedding performs well. However, we hypothesise that in the face of confounders (e.g. we want to 2https://cloud.google.com/tpu/docs/v3 20 Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation measure diversity of the color of an object but not the type of object), we would not expect such an embedding to do well. Figure 14 AUC to measure metrics ability to identify sets of equal diversity. It is clear that no metric is particularly effective at differentiating visually similar versus not sets of images. E.3. Additional qualitative results In Fig. 15 we visualize examples for four side-by-side comparisons where the corresponding autoraters indicate that group of images have highest or lowest diversity. E.4. Impact of the prompt for the multimodal embeddings We explore how the choice of prompt impacts results for the multimodal embeddings. We explore four different prompts which differ in their specificity and relatedness to the attributes under question. [attribute] and [object] are placeholders and filled in based on the object / attribute under test. The templates we consider are as follows: 1. object_attribute: What is the [attribute] of the [object]? 2. attribute: What is the [attribute]? 3. object: What is the [object]? 4. eiffel: Where is the Eiffel Tower? We would expect the first two questions to be most effective as they directly ask about the property for which we are measuring diversity. The object may be related but can be confounder and the Eiffel Tower question is unrelated. Results are shown in Figure 16. Surprisingly, we find that we do not see consistent benefit from the two most related prompts (object_attribute, attribute), implying that the embeddings are mostly vision based. more controllable multimodal embedding we hypothesise would be more effective in this setting. E.5. Model ranking with autoevaluation approaches In this section, we include more results for model ranking based on our auto-evaluation approaches: Figures 17, 18 and 19 show the results of compare model rankings in terms of significance in the number of wins with Wilcoxon signed-rank tests under 95% confidence level using additional models to compute embeddings. This figure completes Figure 8 in Sec. 3.3. In theses figures, we can see: Model ranking based on other embeddings. We observe that similarly to the observations in Sec. 3.3, for all embeddings except ImageNet ViT, Imagen3 is not worse than all other 21 Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation Model 2 diverse sets 2 non-diverse sets CLIP Train (Type) Bridge (Shape) Tiger (Age) Sun (Time of day) PALI (tokens) Animal (Species) Necklace (Material) Tree (Species) Whale (Species) Figure 15 Qualitative results for different models, showing two very diverse and two non diverse sets. models. We also observe that independently of the choice of embedding, Flux1.1, Imagen3 and DALLE3 are not worse than Muse2.2 and Imagen2.5. The differences between the models in the top group and the bottom group are more or less detected depending on the embeddings. As mentioned in the main text, we also see the differences between multimodal models. These results highlight how the influence of the choice of embedding models and of conditioning on the model ranking results. Figures 20, 21 and 22 show the win rates corresponding to the results shown in Figure 8 in Sec. 3.3 and the additional results described above on the left panels, and compare the distributions of the two best and closest models in terms of behavior according to human evaluation, Imagen3 and Flux1.1, on the right panels. These figures correspond respectively to image models, multimodal model conditioned on attributes, and multimodal models conditioned on objects and attributes. Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation (a) Results on the diverse golden set. (b) Results on the annotation set, where annotators see count differences > 4. Figure 16 Additional auto-eval results that show how results vary based on the textual prompt for the multimodal embeddings. We can see that we do not see consistently better results with more related prompts (What is the [attribute] of the [object]?, What is the [attribute]?), implying the textual input is being ignored. 1 . 1 F < = < < 3 a I > > = < 3 A = < < < 2 . 2 M > = > < 5 . 2 a > > > > Flux 1.1 Imagen 3 DALLE3 Muse 2. Imagen 2.5 1 . 1 F 3 a Flux 1.1 Imagen DALLE3 Muse 2.2 Imagen 2.5 > < < < < < < < 3 A > > < < 2 . 2 M > > > < 5 . 2 a > > > > (a) ViT embeddings. (b) DINO embeddings. Figure 17 Model ranking using auto evaluation approaches with additional image models. We compare model rankings in terms of significance in the number of wins with Wilcoxon signed-rank tests under 95% confidence level. Each entry in the each of the grids represents comparison between two models. The > sign indicates the model in the row is better, worse (<), or not significantly different (=) than the model in the column. The win rates in each of the grids are computed using the scores based on (a) ImageNet ViT embeddings and (b) DINO embeddings. Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation 1 . 1 F = < < < 3 a = < = < 3 A > > = = 2 . 2 M > = = = 5 . 2 a > > = = Flux 1. Imagen 3 DALLE3 Muse 2.2 Imagen 2.5 1 . 1 F = = < < 3 a I = = < < 3 A = = < < 2 . 2 M > > > < 5 . 2 a > > > > Flux 1.1 Imagen 3 DALLE3 Muse 2. Imagen 2.5 1 . 1 F > = < Flux 1.1 Imagen 3 DALLE3 Muse 2.2 Imagen 2.5 = 3 a < < < < 3 A = > < < 2 . 2 M > > > = 5 . 2 a = > > = (a) CLIP embeddings. (b) PALI(emb2) embeddings. (c) PALI(tokens) embeddings. Figure 18 Model ranking using auto evaluation approaches with additional vision and language models conditioned on attributes. We compare model rankings in terms of significance in the number of wins with Wilcoxon signed-rank tests under 95% confidence level. Each entry in the each of the grids represents comparison between two models. The > sign indicates the model in the row is better, worse (<), or not significantly different (=) than the model in the column. The win rates in each of the grids are computed using the scores based on (a) CLIP embeddings, (b) PALI(emb2) embeddings, and (c) PALI(tokens) embeddings. All models are conditioned on attributes. 1 . 1 F > < < < 3 a < < < < 3 A > > = = 2 . 2 M > > = = 5 . 2 a > > = = Flux 1. Imagen 3 DALLE3 Muse 2.2 Imagen 2.5 1 . 1 F > = < < 3 a I < < < < 3 A = > < < 2 . 2 M > > > < 5 . 2 a > > > > Flux 1.1 Imagen 3 DALLE3 Muse 2. Imagen 2.5 1 . 1 F = = < < 3 a = = = < 3 A = = = < 2 . 2 M > = = = 5 . 2 a > > > = Flux 1. Imagen 3 DALLE3 Muse 2.2 Imagen 2.5 (a) CLIP embeddings. (b) PALI(emb2) embeddings. (c) PALI(tokens) embeddings. Figure 19 Model ranking using auto evaluation approaches with additional vision and language models conditioned on objects and attributes. We compare model rankings in terms of significance in the number of wins with Wilcoxon signed-rank tests under 95% confidence level. Each entry in the each of the grids represents comparison between two models. The > sign indicates the model in the row is better, worse (<), or not significantly different (=) than the model in the column. The win rates in each of the grids are computed using the scores based on (a) CLIP embeddings, (b) PALI(emb2) embeddings, and (c) PALI(tokens) embeddings. All models are conditioned on objects and attributes. 24 Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation Figure 20 Model ranking using auto evaluation approaches. Win rate matrices and score distributions for Flux1.1 and Imagen3 using image models to compute embeddings. Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation Figure 21 Model ranking using auto evaluation approaches. Win rate matrices and score distribu26 tions for Flux1.1 and Imagen3 using text-conditioned multimodal models to compute embeddings, conditioned on attributes. Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation Figure 22 Model ranking using auto evaluation approaches. Win rate matrices and score distribu27 tions for Flux1.1 and Imagen3 using text-conditioned multimodal models to compute embeddings, conditioned on objects and attributes. Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation E.6. Evaluating diversity using foundation models Besides the investigating multiple embeddings with the Vendi Score for evaluating diversity as presented in Sec. 3, we also propose to use the Gemini model family (Team et al., 2024) for comparing T2I models in terms of attribute-based diversity. For that, we use the following instruction: \"I am currently comparing two models with the prompt [prompt] and would like to know which model generates more diverse images with respect to the attribute [attribute], while disregarding any other attribute in the images. In the following image show [number of images] images generated by one model in the left, which is [model in the left side] and [number of images] images generated by another model in the right, which is [model in the right side]. You must count the number of different instances of [attribute] in both sets and use this information to decide which set is the most diverse. If there is set of images which is more diverse than the other with respect to [attribute], can you tell me which one is the most diverse set and explain why? Any other aspects in the images besides [attribute] must not be taken into account. You can also respond that both sets are equally diverse.\" In addition to the instruction, similarly to the human evaluation, two sets of images are given to the model as input. In Fig. 23 we show the results of three different Gemini models on the task by showing the accuracy in the golden set described in Sec. 2.3. The most recent version of Gemini, v2.5 Flash, achieves the best performance, even surpassing the human raters in this task. These results indicate that such approaches are promising strategies for evaluating diversity which are: (i) able to capture cases where diversity is equally represented in both sets and (ii) do not rely on extracting embeddings. Figure 23 Accuracy of autoraters based on the Gemini model family on the task of comparing diversity of side-by-side sets of 8 images from the golden set. Most recent versions of Gemini perform better in the task, with the v2.5 Flash model surpassing the accuracy of human evaluators. We highlight that both human and auto raters perform similarly in almost all the cases, with the mismatches corresponding to the evaluating of diversity for the pair <building, style>. We hypothesize judging diversity of architectural styles is complex task that heavily depends on the cultural background of annotators, thereby being more accurately performed by powerful visionlanguage model like Gemini. F. Absence of Diversity-Fidelity Trade-off Evaluating the diversity of generative models presents unique challenge: model can trivially achieve high diversity by producing random noisethe generated noisy images are always different in high dimensional space. Therefore, any meaningful assessment of diversity must be predicated on 28 Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation the assumption that the models in question are capable of generating images of sufficient quality. This quality criterion implies that the generated images must not only be visually coherent and free from significant artifacts but also effectively capture the salient aspects and core intent of the given prompt. Without this foundational understanding of quality and adherence to prompt specifications, high diversity score would be misleading, indicating lack of control and semantic understanding rather than beneficial range of outputs. To illustrate this for some of the strong models we considered in our work, we compute the state-of-the-art text-to-image alignment metric Gecko (Wiles et al., 2024) for the same images used in our study in Table 3. Results show that models achieve the same average Gecko score (higher is better, 1 is the maximum) indicating they not only have strong performance in terms of text-to-image alignment, but are not statistically different in terms of this evaluation aspect. Notably, our diversity evaluation in Sec. 3 and showed that Imagen 3 is significantly better than both Imagen 2.5 and Muse 2.2."
        },
        {
            "title": "Gecko",
            "content": "95% CI lowerbound 95% CI upperbound"
        },
        {
            "title": "0.9591\nMuse 2.2\nImagen 3\n0.9591\nImagen 2.5 0.9591",
            "content": "0.9530 0.9527 0.9527 0.9646 0.9647 0.9645 Table 3 Alignment results for models with different diversity. G. LLM use disclosure An LLM was used for polish writing of some paragraphs of the manuscript and improving the phrasing of few sentences. No LLM was used to write extended parts of the paper, or for writing sentences from scratch, retrieval, discovery and research ideation."
        }
    ],
    "affiliations": [
        "Ellison Institute of Technology",
        "Google DeepMind"
    ]
}