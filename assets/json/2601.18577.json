{
    "paper_title": "Self-Refining Video Sampling",
    "authors": [
        "Sangwon Jang",
        "Taekyung Ki",
        "Jaehyeong Jo",
        "Saining Xie",
        "Jaehong Yoon",
        "Sung Ju Hwang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modern video generators still struggle with complex physical dynamics, often falling short of physical realism. Existing approaches address this using external verifiers or additional training on augmented data, which is computationally expensive and still limited in capturing fine-grained motion. In this work, we present self-refining video sampling, a simple method that uses a pre-trained video generator trained on large-scale datasets as its own self-refiner. By interpreting the generator as a denoising autoencoder, we enable iterative inner-loop refinement at inference time without any external verifier or additional training. We further introduce an uncertainty-aware refinement strategy that selectively refines regions based on self-consistency, which prevents artifacts caused by over-refinement. Experiments on state-of-the-art video generators demonstrate significant improvements in motion coherence and physics alignment, achieving over 70\\% human preference compared to the default sampler and guidance-based sampler."
        },
        {
            "title": "Start",
            "content": "Self-Refining Video Sampling Sangwon Jang * 1 Taekyung Ki * 1 Jaehyeong Jo * 1 Saining Xie 2 Jaehong Yoon 3 Sung Ju Hwang 1 4 1KAIST 2NYU 3NTU Singapore 4DeepAuto.ai Project Page: https://agwmon.github.io/self-refine-video/ 6 2 0 2 6 2 ] . [ 1 7 7 5 8 1 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Modern video generators still struggle with complex physical dynamics, often falling short of physical realism. Existing approaches address this using external verifiers or additional training on augmented data, which is computationally expensive and still limited in capturing fine-grained motion. In this work, we present self-refining video sampling, simple method that uses pre-trained video generator trained on large-scale datasets as its own self-refiner. By interpreting the generator as denoising autoencoder, we enable iterative inner-loop refinement at inference time without any external verifier or additional training. We further introduce an uncertainty-aware refinement strategy that selectively refines regions based on self-consistency, which prevents artifacts caused by over-refinement. Experiments on state-of-the-art video generators demonstrate significant improvements in motion coherence and physics alignment, achieving over 70% human preference compared to the default sampler and guidance-based sampler. 1. Introduction The rapid advancement of diffusion and flow matching models (Song et al., 2020; 2021; Lipman et al., 2022) has led to powerful video generators, which are increasingly viewed as early-stage world models (Brooks et al., 2024; Ball et al., 2025; Ali et al., 2025) that capture physical dynamics and causal structures of future states. Despite the impressive results, current video generators still struggle to model complex physical dynamics (Kang et al., 2025; Li et al., 2025a), and remain far from reliable physical simulators. The inconsistencies and implausible outputs undermine real-world applications, such as robot manipulation (Qi et al., 2025; Bharadhwaj et al., 2025; Chen et al., 2025), where small visual errors, such as shape deformations of objects, can * Equal contribution. Equal advising. 1 Figure 1. Concept of the self-refining video sampling. Within the same noise level, the video latent zt is refined as the predicted endpoint ˆz1 is pulled toward the data manifold. lead to incorrect actions. Recent works attempt to address these limitations by either incorporating external models or additional training. One line of work employs external verifiers to improve physical plausibility via rejection sampling (Azzolini et al., 2025; Liu et al., 2025a), repeatedly generating new videos until success. Yet, low acceptance rates necessitate numerous proposals, making it highly inefficient. Moreover, these verifiers are often domain-specific (Chi et al., 2025) and are ill-suited for evaluating temporal coherence and physical plausibility (Bansal et al., 2025). Another line of work adopts post-training strategies (Liu et al., 2025b; Li et al., 2025a; Ali et al., 2025), for example, generating synthetic data and fine-tuning on the augmented dataset (Cai et al., 2025a). However, these methods typically require highquality, domain-specific external data or substantial computation. Furthermore, accurately capturing fine-grained motion dynamics via reward models remains challenging, which in turn limits the applicability to real-world tasks. To overcome these limitations, we propose using the video generator as self-refiner at inference time, without external models or additional training. Modern video generators (Yang et al., 2025b; Wang et al., 2025a) that are trained on large-scale datasets already encode rich priors over realistic motion and structure (Yuan et al., 2025; Mi et al., 2025). We aim to leverage these learned priors by iteraSelf-Refining Video Sampling tively refining samples during inference. The key question is how to realize self-refinement for video generators. Unlike LLMs, which can directly re-ingest their output tokens and revise, video generators lack an explicit internal feedback signal for critique and correction, especially given the high-dimensionality and temporal coupling of videos. To this end, we introduce Predict-and-Perturb (P&P), training-free sampling method that uses flow matching video generator as its own self-refiner. We reinterpret the flow matching objective as time-conditioned denoising autoencoder (DAE) (Vincent et al., 2008; Bengio et al., 2013) training, and reuse this property at inference time. Our main idea is to refine the video latents during sampling, by iteratively noising and denoising at fixed noise level as illustrated in Fig. 1. Mirroring the corruptreconstruct structure of DAE, the model first predicts clean endpoint video latent and then perturbs it back to the same noise level. This simple inner-loop refinement pulls the latent toward higherdensity regions of learned video distribution, corresponding to temporally coherent and physically plausible videos. We further propose Uncertainty-aware P&P, an extension that retains the benefit of refined sampling while mitigating artifacts caused by over-refinement. While repeated P&P iterations progressively improve video quality, naively applying them may lead to over-saturation from repeated classifier-free guidance (CFG) (Ho & Salimans, 2022; Sadat et al., 2024), particularly in static regions. We extend P&P by selectively refining only spatio-temporal regions where the model exhibits uncertainty, while leaving stable regions largely unchanged. We leverage self-consistency measure from the model predictions within the P&P process and use it to gate refinement at no extra computation cost. As result, it retains the benefits of P&P while mitigating over-refinement artifacts and preserving visual quality. We validate our approach with extensive experiments on state-of-the-art video generative models, including Wan2.1, Wan2.2 (Wang et al., 2025a), and Cosmos-2.5 (Ali et al., 2025). Across all models, we significantly improve physical realism, such as motion coherence, physical plausibility, and spatial consistency. Notably, on Wan2.2, which already produces strong human motion, our method further improves motion quality, yielding more than 73% preference in human evaluation compared with the default sampler. 2. Related Works Self-Refining in Generative Models In this paper, we refer to self-refinement as an inference-time paradigm in which generative model improves its outputs using only its internal signal without any external evaluator, teacher, verifier, or additional training. In language modeling, SelfRefine (Madaan et al., 2023) proposes an iterative loop in which the model critiques and revises its own outputs. Reasoning with Sampling (Karan & Du, 2025) introduces MCMC-based sampling scheme that uses only the base language model to elicit strong reasoning performance without reinforcement learning. In diffusion models, ZigzagDiffusion (Bai et al., 2025a) proposes self-reflective sampling method that alternates between guided denoising and inversion during inference. Improving Physical Realism in Video Generation Previous works explored improving motion coherence in video generative models (Shi et al., 2024; Wu et al., 2024; Chefer et al., 2025; Shaulov et al., 2025). VideoJAM (Chefer et al., 2025) introduces joint training approach with an additional optical flow denoising objective. FlowMo (Shaulov et al., 2025) proposes training-free guidance method to reduce temporal variance. However, these methods require substantial computational cost in training or inference, and still struggle with complex motions. Recent work aims to improve physical fidelity in world simulation (Brooks et al., 2024; Ball et al., 2025; Wiedemer et al., 2025). One line of research trains models on curated physics datasets (Zhang et al., 2025; Wang et al., 2025c; Li et al., 2025a) or domain-specific datasets (Gosselin et al., 2025; Gillman et al., 2025). For example, WISA (Wang et al., 2025c) uses physics-focused MoE, and Zhao et al. (2025) trains model with synthetic computer-generated imagery (CGI) data. While effective, these approaches necessitate extensive data curation and additional training. Another line of work bypasses large-scale training by employing external physics-aware modules at inference time (Lv et al., 2023; Yang et al., 2025a; Savant Aira et al., 2024; Liu et al., 2024; Wang et al., 2025b). GPT4Motion (Lv et al., 2023), PhysGen (Liu et al., 2024), and VLIPP (Yang et al., 2025a) leverage LLMs as high-level physics planner, but dependence on external modules can limit generalization. 3. Preliminaries: Flow Matching in Video"
        },
        {
            "title": "Diffusion Models",
            "content": "Recent video generative models (Polyak et al., 2025; Wang et al., 2025a; Kong et al., 2024; HaCohen et al., 2024; Jin et al., 2025) adopt flow matching (Lipman et al., 2022) in VAE latent space. Specifically, an RGB video = RF HW 3 is first encoded by video VAE into compressed latent representation = Rf hwc, where (f, h, w) denote the downsampled spatio-temporal resolution and is the latent channel dimension. This latent space significantly reduces computational cost while preserving the essential spatio-temporal structure of the input video. On this latent space, flow matching learns time-dependent vector field model uθ : [0, 1] that transforms samples from prior distribution p0 = (0, I) to the target 2 Self-Refining Video Sampling data distribution p1 via an ordinary differential equation (ODE) dzt dt = uθ(zt, t). Samples are generated by solving the ODE over discretized timesteps 0 = t0 < < tT = 1: zti+1 = zti + (ti+1 ti) uθ(zti, ti), (1) where uθ is the learned vector field and zt0 is an initial point sampled from the prior distribution p0. common training strategy constructs straight path zt = (1 t)z0 + tz1 between paired samples z0 p0 and z1 p1, with the target vector field vt = z1 z0. The vector field model uθ is trained to approximate the vector field vt: LFM(θ) = Et,z0,z1 (cid:2)uθ(zt, t) (z1 z0)2 2 (cid:3). (2) 4. Self-Refining Video Sampling 4.1. Flow Matching as Denoising Autoencoder To enable self-refinement for flow matching-based video models, we revisit the connection between diffusion models and denoising autoencoders (DAEs) (Vincent, 2011; Song & Ermon, 2019), and extend the link to interpret flow matching as DAE from training objective perspective. The flow matching objective (Eq. (2)) can be rewritten as: LFM(θ) = Et,z0,z (cid:20) 1 (1 t)2 (cid:13) (cid:13)ˆzθ 1 z1 (cid:13) 2 (cid:13) 2 (cid:21) , (3) where ˆzθ 1 := zt + (1 t) uθ(zt, t) represents the model prediction of the clean data z1. Notably, Eq. (3) corresponds to the weighted version of the generalized DAE objective (Bengio et al., 2013): LDAE(θ) = Et,z0,z1 (cid:2)(cid:13) (cid:13)ˆzθ 1 (cid:13) 2 (cid:13) 2 (cid:3), (4) for which the model learns to denoise the corrupted input zt back to the clean sample z1. Therefore, the flow matching objective can be interpreted as training time-conditioned DAE across all noise levels. At inference time, for any fixed t, the denoising via the flow matching model acts as DAE reconstruction at that noise level. We leverage the pseudo-Gibbs Markov chain of generalized DAE (Bengio et al., 2013), alternating the corruption and reconstruction at each discretized inference timestep to steer predictions toward the data manifold. Building on this, we introduce novel sampling method based on the iterative refinement of zt for each timestep t. 4.2. Predict-and-Perturb (P&P) In the DAE perspective, we first define the reconstruction and corruption operators for the flow matching model. At timestep t, the reconstruction from state zt corresponds to the denoiser Dθ(, t): Predict: Dθ(zt, t) := zt + (1 t) uθ(zt, t), (5) Figure 2. Sampling comparison on 2D synthetic dataset. (ab) P&P generates samples closer to the data manifold than the Euler solver. (c-d) With fixed timestep, iterative P&P pulls the prediction ˆz1 closer to the data manifold. where uθ is the trained vector field model, for which Dθ maps the noisy state zt to prediction of the clean sample ˆz1. Moreover, the corruption of state at timestep corresponds to the linear interpolation with the noise ϵ (0, I): Perturb: Rϵ(z, t) := tz + (1 t)ϵ, (6) where Rϵ adds noise ϵ to the sample with noise level t. With Predict and Perturb operators, we iteratively refine the state zt at fixed noise level t, producing sequence {z(k) } via pseudo-Gibbs sampling, similar to the generalized DAE (Bengio et al., 2013). Each iteration consists of reconstruction step (Predict) followed by corruption step (Perturb) as follows: ˆz(k) 1 := Dθ (cid:0)z(k) , t(cid:1), z(k+1) := Rϵk (cid:0)ˆz(k) 1 , t(cid:1), (7) with initial state z(0) = zt and ϵk (0, I). Conceptually, each Predict-Perturb cycle steers the reconstruction ˆz1 toward regions of higher-density (Bengio et al., 2013), yielding refined state zt. We define single refinement iteration, termed Predict-and-Perturb (P&P), as: , t(cid:1) := Rϵk (cid:0)Dθ(z(k) = P&Pϵk , t), t(cid:1), z(k+1) (cid:0)z(k) (8) which forms self-refinement loop using only the generators signal, without any external model or verifier. In this self-refine loop, Predict corresponds to the correction of the noisy state via the denoiser, while Perturb performs local resampling at the same noise level t. In particular, local resampling allows larger exploratory moves at early timesteps, thereby mitigating early lock-in in video generation, where temporal dynamics such as motion and physics are largely determined in the first few steps (Chefer et al., 2025; Shaulov et al., 2025; Jang et al., 2025). We empirically find that only 2-3 updates of zt are sufficient to improve temporal coherence and physical plausibility of the prediction ˆz1, even for high-dimensional video latents. 3 Self-Refining Video Sampling i=1, P&P interval rate α, confiAlgorithm 1 Self-Refining Video Sampling Require: Timesteps (ti)T dence threshold τ , number of P&P iterations Kf . 1: Sample Noise zt0 (0, I) 2: for = 0 to 1 do 3: 4: 1 Dθ(zti, ti) z(0) ti+1 zti + (ti+1 ti) uθ(zti, ti) Base NFE Motion stage if αT then Predict ˆz(0) Eq. (5) for = 1 to Kf do Perturb z(k) Predict ˆz(k) ti 1(cid:0)U(z(k1) (k) ti+1 z(k) z(k) ti+1 (k) z(k) end for zti+1 z(K) ti+ Eq. (6) ti ) Eq. (5), +1 NFE ti ) > τ (cid:1) Eq. (10) , z(k) ti + (ti+1 ti) uθ(z(k) , ti) ti+1 + (1 (k) ti z(k) ti Rϵ(ˆz(k1) 1 Dθ(z(k) ti ) z(k1) Refined latent ti+1 ti ti ) 1 5: 6: 7: 8: 9: 10: zti+1 z(0) ti+1 Base ODE step else 11: 12: 13: 14: 15: end if 16: 17: end for Output: ztT tifies low-confidence regions, where 1 indicates uncertain regions to be refined and 0 marks confident regions to be preserved. Specifically, we construct an uncertainty map at the k-th refinement step by comparing the reconstructed predictions ˆz(k) from the Predict step: 1 U(z(k1) ti , z(k) ti ) := Dθ(z(k1) ti , ti) Dθ(z(k) ti , ti)1, and ˆz(k1) 1 1 where denotes the latent channel dimension, and the norm is computed per spatio-temporal location by averaging over channels. The uncertainty mask is then obtained by thresholding the uncertainty map with confidence threshold τ : U(z(k1) ti (cid:16) := ti ) > τ (k) ti , z(k) (10) (cid:17) , where 1() is the indicator function. As visualized in Fig. 3, uncertain regions align with moving objects (e.g., human motion) while certain regions correspond to the static background, demonstrating that the model-inherent self-consistency signal identifies regions for refinement. In practice, fixed threshold τ = 0.25 robustly separates the regions. To use the uncertainty mask without additional NFE, we introduce simple technique that performs denoising and mask creation simultaneously. In the Predict step (Eq. (5)), we compute the next timestep latent z(k) ti using already computed z(k1) from the previous P&P iteration. We reformulate the ODE solver in Eq. (9) without explicitly computing the refined : ti+1 + (1 (k) ti z(k) ti+1 (k) z(k) ti+1 from z(k) ti ) z(k1) (11) ti+1 ti+1 , Figure 3. Visualization of uncertainty maps, showing higher values in motion-related regions. Maps are computed at = 0.1T . Bottom row overlays the corresponding binary masks (τ = 0.25) on videos generated by Wan2.2-A14B T2V (Wang et al., 2025a). Notably, the proposed P&P can be integrated into existing ODE solvers in plug-and-play manner, by simply replacing zt with the refined zti+1 = ti := z(Kf ) + uθ(z ti , t), = ti+1 ti with Kf 3: (9) In particular, since coarse motion and structure are largely determined in the first few steps, we experimentally demonstrate that applying P&P only at early noise levels (i.e., for timesteps < 0.2) suffices to produce refined samples. Toy experiment We validate our method with toy experiment on simple 2D sine dataset. As shown in Fig. 2 (ab), samples generated with P&P capture the data manifold more faithfully than those from the Euler solver. In addition, Fig. 2 (cd) shows that applying P&P steps within the same timestep pulls ˆz1 toward the data manifold. 4.3. Uncertainty-aware P&P While P&P enables iterative self-refinement, we observe that applying multiple P&P updates (Kf > 3) with classifierfree guidance (CFG) (Ho & Salimans, 2022), can cause over-saturation (Sadat et al., 2024) or simplification in static regions such as the background, as shown in Fig. 9(b). The issue arises from repeated CFG updates with an amplified scale (i.e., 1t instead of t) during denoising. Regions that are significantly altered by P&P are less affected by this amplified CFG, as the guidance impact is reset after each P&P step. In contrast, static regions that remain largely unchanged after P&P are repeatedly influenced by the guidance, causing the guidance to accumulate and leading to over-saturation. To address this issue, we propose Uncertainty-aware P&P, an extension of P&P that selectively refines only the locally uncertain regions. Specifically, we leverage the model confidence of the prediction, applying the P&P steps only on video regions with low reconstruction confidence. For each P&P step, we create an uncertainty mask that iden4 Self-Refining Video Sampling where denotes element-wise multiplication. Uncertain regions where the mask is set to one are refined via P&P, correcting physical inconsistencies or jitter artifacts, while certain regions are retained, preventing artifacts from overrefinement. In Algorithm 1, we summarize the overall procedure of Uncertainty-aware P&P with an example code implementation provided in Algorithm 2. Notably, Lines 5 and 10 in Algorithm 1 do not incur additional NFEs, as they reuse predictions computed in earlier steps. Human Eval VBench Method Motion (%) Text (%) Motion Const. NFE Time Wan2.2 T2V + NFE2 + CFG-Zero + FlowMo + Ours 73.57 74.05 81.53 70.57 - 57.64 57.55 65.71 61.71 - 98.01 98.03 98.27 97.68 98.41 90.68 90.66 91.16 90.95 91.33 40 2.0 80 1.0 40 40* 3.9 1.5 Table 1. Dynamic-bench results measuring motion coherence for challenging motions using Wan2.2-A14B T2V. Human evaluation shows the percentage of votes favoring ours. Additional inference time (*) of FlowMo is introduced by gradient computation. 5. Experiments 5.1. Motion Coherence for Challenging Motions Benchmarks We use two benchmarks to evaluate motion coherence. First, we introduce Dynamic-bench, constructed to assess state-of-the-art video generators such as Wan2.2A14B (Wang et al., 2025a) under challenging motion scenarios, including multi-object interactions, complex human motions, and physics-driven dynamics. Dynamic-bench consists of 120 prompts (40 per category) generated using Gemini 3, with details provided in Appendix D. We also evaluate on VideoJAM-bench (Chefer et al., 2025). For both benchmarks, we generate single video per prompt and evaluate them using VBench (Huang et al., 2024). To fully assess the fine-grained motion quality of videos that automated evaluation cannot capture, we additionally conduct human evaluation comparing our method with baselines. Motion quality and text alignment are evaluated on 30 challenging videos using win-tie-lose criteria. An example of the human evaluation is provided in Fig. 26, with further details in Appendix A.3. Baselines We use Wan2.1 and Wan2.2 T2V as the base video generators and compare our approach against four inference-time sampling methods: the default ODE solver UniPC (Zhao et al., 2023), the same solver with doubled function evaluations (NFE2), CFG-Zero (Fan et al., 2025), an improved classifier-free guidance variant for flow matching models, and FlowMo (Shaulov et al., 2025), gradientbased training-free guidance method for coherent motion. Qualitative Results As shown in Fig. 4, our method produces videos with significantly enhanced motions even for complex dynamics. For instance, the first row of Fig. 4 shows failed gymnastic motion generated by the ODE sampler even with doubled NFE, exhibiting duplicated arms highlighted in red boxes and physically implausible poses. In contrast, our method (second row of Fig. 4) produces successful motion, including realistic poses and plausible interactions between the hands and the pommel. We provide additional frames in Fig. 24 of the Appendix. Quantitative Comparison Tab. 1 left shows the human evaluation results from 20 evaluators, reporting the tieadjusted win rate of our method, where each tie is counted as half win. The motion quality of our videos is strongly preferred over all other methods, with 73% favoring ours over the default sampler and 70% favoring ours over the training-free guidance method FlowMo. We provide full human evaluation results in Fig. 10. In Tab. 1 right, we present the automated evaluation results on the Dynamic-bench, where our method achieves the strongest performance on VBench metrics, including motion and consistency. We further provide the VideoJam-bench results in Tab. 7, where our method achieves the best scores. 5.2. Physical Realism in Robotics Videos Benchmarks We evaluate our method on PAI-Bench (Zhou et al., 2025) using its predefined VQA questions and VBench quality scores. We generate videos for 174 Robotdomain prompts with three random seeds each, and assess them with Qwen2.5-VL-72B-Instruct (Bai et al., 2025b). To assess detailed physical coherence, we additionally report grasp success rates for videos generated from 155 grasprelated prompts, focusing on contact and object manipulation. These are evaluated by Gemini 3 Flash (Google, 2025a), which supports higher resolution video inputs. We provide further details in Appendix A.4. Baselines We use post-trained Cosmos-Predict2.5-2B (Ali et al., 2025) and Wan2.2-A14B I2V as the base video generators, and compare our approach against inference-time sampling methods. We additionally compare with verifierbased rejection sampler using Cosmos-Reason1 7B (Azzolini et al., 2025) as the video critic. It generates four samples per video and selects the sample with the highest non-anomalous score (best-of-4). Qualitative Results As shown in Fig. 5, our method generates videos that are aligned with the text prompt and exhibit realistic physical interactions with reduced artifacts. As visualized in the top row of Fig. 5, samples from the base ODE solver often show noticeable grasping artifacts (red box), and fail to move the bowl onto the blue cloth as specified in the prompt. In contrast, samples from our method closely follow the instructions and achieve accurate grasping. 5 Self-Refining Video Sampling Figure 4. Qualitative comparison on challenging motion generation. Figure 5. Qualitative comparison on I2V generation in robotics domain. Figure 6. Qualitative comparison on physics-aligned video generation. Figure 7. Qualitative comparison on spatially consistent video generation. 6 Self-Refining Video Sampling Method Grasp Robot-QA Quality NFE Time Method L2 CD IoU Cosmos-Predict-2.5 + NFE2 + Verifier (best-of-4) + Ours Wan2.2-I2V-A14B + NFE2 + Verifier (best-of-4) + Ours 79.2 78.6 84.4 89.6 77.3 83.1 80.5 85.7 71.7 72.6 72.3 76.3 77.4 76.7 78.1 80. 75.1 75.1 75.3 75.1 75.3 75.5 75.3 75.5 35 70 140 57 40 80 144 60 2.0 4.0 1.6 2.0 4.0 1.5 Wan2.2 0.132 0.348 0.069 + Ours 0.128 0.338 0.074 (a) Full real dataset Method L2 CD IoU Wan2.2 0.186 0.489 0.057 + Ours 0.184 0.482 0.060 Table 2. PAI-Bench-G evaluation results on robotics I2V generation. Grasp is measured by Gemini 3 Flash, and Robot-QA is measured by Qwen2.5-VL-72B. VideoPhy Human Eval Gemini3-F PhyWorldBench Gemini3-F Method PC (%) SA (%) PC SA PC SA Both Wan2.2 T2V 84.29 + NFE2 74.76 78.10 + CFG-Zero - + Ours 65.24 64.29 59.76 - 54.5 53.1 50.6 55.6 66.1 61.7 67.0 66.2 29.3 31.4 29.3 40.0 78.1 81.4 80.1 78.6 28.6 31.4 29.3 37. Table 3. Videophy2 and PhyWorldBench evaluation results using Wan2.2-A14B T2V. Human evaluation shows the percentage of votes favoring ours. Quantitative Results Tab. 2 shows that our method outperforms all baselines on both video generators while incurring only moderate computational overhead. Compared to the base ODE sampler, ours significantly improves the grasp success rate by +11.0% on Cosmos and +8.4% on Wan. Ours also outperforms verifier-based rejection sampling (best-of-4), which requires additional inference cost and depends on an external verifier. Moreover, our method achieves the highest Robot-QA accuracy, indicating improved prompt alignment. The quality score, averaged over the VBench, shows negligible variation as all methods perform I2V generation using the same generator. 5.3. Physics Alignment in the Wild Benchmarks We first evaluate on VideoPhy2 (Bansal et al., 2025), which consists of action-centric, physics-related prompts. We generate 360 videos using upsampled captions from the hard and easy subsets, with 180 videos from each. We additionally conduct human evaluation for complementary assessment. We further evaluate on PhyWorldBench (Gu et al., 2025) using 70 prompts from the kinematics and interaction dynamics domain, generating two samples per prompt. For both benchmarks, we assess physical commonsense (PC) and semantic alignment (SA) using Gemini 3 Flash, which supports higher frame rates. To demonstrate the improved consistency of our method, we use PisaBench (Li et al., 2025a), benchmark designed to assess free-fall I2V generation. We use the full real dataset for evaluation and additionally generate 32 videos for each of the three selected scenarios with clearly visible objects to analyze failure cases. (b) Multiple generations on three samples (see right) Wan2.2-I2V + Ours Table 4. PisaBench evaluation. (Left) Quantitative results on the full real dataset. (Right) Visualization of 32 generated free-fall trajectories. Physically implausible falls are shown in red. Method SSIM Wan2.2 T2V + Ours 0.401 0.485 L1 37.26 30.16 PSNR (dB) NFE 14.96 17. 40 60 Table 5. Spatial consistency evaluation results using Wan2.2A14B T2V. We measure distances between frame pairs at revisited viewpoints after camera-pose-based warping. Qualitative Results As visualized in Fig. 6, ours generates videos following the physical law with fewer visual hallucinations. For example, in the top row of Fig. 6, the base model often exhibits non-physical behavior in which sand abruptly appears in the childrens hands without any causal interaction (red boxes). In contrast, our method follows the physical constraints and causal consistency. We further visualize 32 free-fall trajectories of free fall in Tab. 4 right. While the default ODE solver produces physically implausible falls (red trajectories), our method consistently generates realistic videos of the falling object. Quantitative Comparison Tab. 3 shows the human evaluation results from 20 evaluators, indicating that the physics alignment of our videos is strongly preferred over all other methods. In particular, 84% favor ours over the default sampler, and 74% favor ours over the doubled-NFE baseline. We provide full human evaluation results in Fig. 11. Moreover, automated evaluation in Tab. 3 shows that our method outperforms all baselines in physics commonsense (PC) metric on both benchmarks, with larger gains on the motion-centric PhyWorldBench. Results on PisaBench in Tab. 4 further validate that our method generates more accurate trajectories in the free-fall experiments. 5.4. Improvement in Spatial Consistency Moreover, we observe that our self-refinement can improve the spatial consistency of the generated videos. We assess this capability with simple experiments that evaluate videos in which camera revisits previously seen viewpoint, for example, after rotations exceeding 360. 7 Self-Refining Video Sampling Figure 8. Examples of self-refinement applied to visual reasoning tasks: (Top) graph traversal and (Bottom) maze solving from Wiedemer et al. (2025). We use Wan2.2-A14B I2V as the base model. For graph traversal, self-refinement yields dramatic improvement in the success rate from 0.1 to 0.8. For maze solving, self-refinement does not yield meaningful gain, with success remaining near zero. Benchmarks We generate videos from 20 prompts generated by Gemini that involve large camera motions. We then estimate per-frame camera parameters with MegaSaM (Li et al., 2025b) and measure visual distances between frame pairs corresponding to revisited viewpoints with similar estimated camera poses. Specifically, we warp one frame into the other using the estimated depth and camera poses, and measure visual similarity on visible pixels using SSIM, L1, and PSNR. We provide evaluation details in Appendix A.6. However, tasks whose success depends on discrete or semantic correctness show little or no improvement. For example, the maze solving problem at the bottom of Fig. 8 shows no meaningful gain, with success remaining near zero. We speculate that in cases where the video generator fails almost entirely, it lacks the knowledge needed to correct the underlying errors, and late-stage refinement of the maze trajectory becomes insufficient. In these cases, external verifiers are likely required. We provide more details in Appendix B.3. Results Fig. 7 demonstrate that our method generates spatially consistent videos that preserve previously observed scene content even under large camera motions. The top row of Fig. 7 shows that the default ODE solver often produces inconsistent backgrounds that differ from earlier frames when the camera movement is large. In contrast, ours maintains much stronger consistency with earlier viewpoints. As shown in Tab. 5, our method achieves significantly improved spatial consistency compared to the default ODE solver. 5.5. Application to Visual Reasoning We conduct extensive analysis of whether our method can improve the emergent visual reasoning capabilities of recent video generators (Wiedemer et al., 2025; Cai et al., 2025b). First, we find that tasks that can be partially refined through motion or temporal consistency show noticeable improvements with our self-refining video sampling. For example, the graph traversal problem visualized at the top of Fig. 8 shows dramatic increase in success rate, from 0.1 to 0.8, after applying self-refinement. Qualitatively, refinement reduces visual artifacts and improves temporal coherence, which leads to correct reasoning trajectories. 5.6. Ablation Studies Importance of Uncertainty-Aware Refinement As shown in Fig. 9(b), excessive P&P iterations (e.g., Kf = 5) without our uncertainty-aware strategy lead to over-saturation and simplification. This causes shifts in color tone and contrast as well as exaggerated reflections on the water surface, which is similar to the effect of increasing the CFG scale. The issue can be mitigated by using the uncertaintyaware strategy, which selectively refines the motion-related regions, as visualized in Fig. 9(c). Hyperparameters of P&P We conduct ablation studies on the key hyperparameters of P&P: number of P&P iterations Kf , confidence threshold τ , and P&P interval rate α. We observe that these hyperparameters remain robust across wide range of settings. In Fig. 16 of Appendix, we show that increasing Kf strengthens refinement at the cost of additional NFEs, while τ regulates background appearance. In Fig. 17, we show that applying P&P at earlier inference stages is more effective for correcting motion errors, with later stages contributing marginally. We provide further details in Appendix A.7. 8 Self-Refining Video Sampling 6. Discussion 6.1. Cross-Frame Consistency of Video Here, we discuss unique property of videos and how it affects our design of self-refinement sampling. Videos are notably more robust to perturbations during generation compared to images, due to cross-frame consistency, where neighboring frames share strongly correlated layouts and motion trajectories. We illustrate this in Fig. 13(a), which applies SDEdit (Meng et al., 2022) with changed prompt for the image and video. While the image exhibits clear semantic transition, the video largely preserves its content. Due to the cross-frame consistency, multiple P&P updates during video generation produce controlled changes in temporal structures like motion. In contrast, images can shift substantially after single P&P update, even when applied at later timesteps of generation. We visualize this in Fig. 13(b), where repeated P&P iterations lead to large deviations for images, but only minimal changes in the video. Consequently, iterative P&P updates for videos act as local search that refines the latents, rather than global resampling that resets them and induces large semantic transitions. 6.2. Mode-Seeking Behavior of Iterative P&P We observe that iterative P&P exhibits mode-seeking behavior in which samples concentrate in high-density, stable modes of the data distribution. We visualize this in toy example  (Fig. 21)  using 2D Gaussian mixture, where repeated P&P yield samples concentrated in the high-density regions. Similarly, applying multiple P&P (Kf = 8) in image generation reduces output diversity and concentrates outputs toward small number of classes, as shown in Fig. 22. In video generation, this mode-seeking behavior manifests differently. Rather than collapsing to identical content, refined videos show reduced temporal variance, removing temporal artifacts such as jittering and flickering. We hypothesize this difference is due to cross-frame consistency as temporally inconsistent videos lie in low-density regions. Consequently, iterative P&P appears as temporal modeseeking, which leads to physically plausible videos. 6.3. Connection to Prior Works Annealed Langevin Dynamics (ALD) (Song & Ermon, 2019) is an MCMC sampler that alternates with Gaussian noise injection and score-guided Langevin updates, resembling our iterative perturb-and-predict refinement. However, ALD samples through sequence of annealed noise scales, whereas our method performs stochastic perturbations and corrections at fixed noise level within each refinement loop. Moreover, ALD is intended to approximate the target distribution, while our self-refinement is not strict MCMC sampler and exhibits mode-seeking behavior. 9 Figure 9. Ablation on uncertainty-aware strategy. Multiple P&P updates without uncertainty-aware strategy cause over-saturation. Red arrow indicates motion misaligned with the prompt. Restart (Xu et al., 2023) alternates between forward noising restart steps and deterministic backward ODE integration, using stochasticity to reduce error accumulation. At high level, it resembles our approach in that noise injection is followed by deterministic update. However, our method differs in when and how stochasticity is applied. While Restart adds noise by jumping forward in time and integrating back along the ODE, we perform local resampling at the same noise level via P&P. Furthermore, Restart applies macro forward-backward cycle to reduce accumulated errors that are often positioned late in the trajectory, whereas we perform fine-grained refinement in the same noise-level, typically applied in the early steps. FreeInit (Wu et al., 2024) is training-free inference method that improves video temporal consistency by iteratively refining the initial noise, using only the generator. The key difference with our method is where the refining happens. FreeInit refines only the initial noise and re-runs the full denoising process, whereas we iteratively refine the intermediate latents within the same sampling trajectory. Our approach is both more effective and significantly more compute-efficient than repeating the full denoising process. 7. Conclusion In this work, we present self-refining video sampling method that reuses pre-trained video generator as selfrefiner. We revisit the flow matching objective as generalized denoising autoencoder and leverage it to refine latents at each timestep during inference. We further propose an uncertainty-aware strategy that selectively refines uncertain regions using self-consistency signals from the model itself. Extensive experiments demonstrate that P&P consistently improves motion coherence, physical plausibility, and overall quality across diverse video generation tasks. We believe this work provides practical and broadly applicable approach for more effective use of existing pre-trained video generators. We discuss the limitations in Appendix C."
        },
        {
            "title": "References",
            "content": "Ali, A., Bai, J., Bala, M., Balaji, Y., Blakeman, A., Cai, T., Cao, J., Cao, T., Cha, E., Chao, Y.-W., et al. World simulation with video foundation models for physical ai. arXiv preprint arXiv:2511.00062, 2025. 1, 2, 5, 14 Azzolini, A., Bai, J., Brandon, H., Cao, J., Chattopadhyay, P., Chen, H., Chu, J., Cui, Y., Diamond, J., Ding, Y., et al. Cosmos-reason1: From physical common sense to embodied reasoning. arXiv preprint arXiv:2503.15558, 2025. 1, 5, 15 Bai, L., Shao, S., zhou, z., Qi, Z., Xu, Z., Xiong, H., and Xie, Z. Zigzag diffusion sampling: Diffusion models can selfimprove via self-reflection. In International Conference on Learning Representations, 2025a. 2 Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025b. 5, 15 Ball, P. J., Bauer, J., Belletti, F., Brownfield, B., Ephrat, A., Fruchter, S., Gupta, A., Holsheimer, K., Holynski, A., Hron, J., Kaplanis, C., Limont, M., McGill, M., Oliveira, Y., Parker-Holder, J., Perbet, F., Scully, G., Shar, J., Spencer, S., Tov, O., Villegas, R., Wang, E., Yung, J., Baetu, C., Berbel, J., Bridson, D., Bruce, J., Buttimore, G., Chakera, S., Chandra, B., Collins, P., Cullum, A., Damoc, B., Dasagi, V., Gazeau, M., Gbadamosi, C., Han, W., Hirst, E., Kachra, A., Kerley, L., Kjems, K., Knoepfel, E., Koriakin, V., Lo, J., Lu, C., Mehring, Z., Moufarek, A., Nandwani, H., Oliveira, V., Pardo, F., Park, J., Pierson, A., Poole, B., Ran, H., Salimans, T., Sanchez, M., Saprykin, I., Shen, A., Sidhwani, S., Smith, D., Stanton, J., Tomlinson, H., Vijaykumar, D., Wang, L., Wingfield, P., Wong, N., Xu, K., Yew, C., Young, N., Zubov, V., Eck, D., Erhan, D., Kavukcuoglu, K., Hassabis, D., Gharamani, Z., Hadsell, R., van den Oord, A., Mosseri, I., Bolton, A., Singh, S., and Rocktäschel, T. Genie 3: new frontier for world models, 2025. URL https://deepmind.google/blog/ genie-3-a-new-frontier-for-world-models/. 1, 2 Bansal, H., Peng, C., Bitton, Y., Goldenberg, R., Grover, A., and Chang, K.-W. Videophy-2: challenging actioncentric physical commonsense evaluation in video generation. arXiv preprint arXiv:2503.06800, 2025. 1, 7, Bengio, Y., Yao, L., Alain, G., and Vincent, P. Generalized denoising auto-encoders as generative models. In Advances in Neural Information Processing Systems, 2013. 2, 3 Self-Refining Video Sampling Bharadhwaj, H., Dwibedi, D., Gupta, A., Tulsiani, S., Doersch, C., Xiao, T., Shah, D., Xia, F., Sadigh, D., and Kirmani, S. Gen2act: Human video generation in novel scenarios enables generalizable robot manipulation. In Conference on Robot Learning, 2025. 1 Black-Forest-Labs. Flux. https://github.com/ black-forest-labs/flux/, 2024. URL https: //github.com/black-forest-labs/flux/. 17 Brooks, T., Peebles, B., Holmes, C., DePue, W., Guo, Y., Jing, L., Schnurr, D., Taylor, J., Luhman, T., Luhman, E., Ng, C., Wang, R., and Ramesh, A. Video generation models as world simulators, 2024. URL https://openai.com/research/ video-generation-models-as-world-simulators. 1, 2 Cai, Y., Li, K., Jia, M., Wang, J., Sun, J., Liang, F., Chen, W., Juefei-Xu, F., Wang, C., Thabet, A., et al. Phygdpo: Physics-aware groupwise direct preference optimization for physically consistent text-to-video generation. arXiv preprint arXiv:2512.24551, 2025a. Cai, Z., Qiu, H., Ma, T., Zhao, H., Zhou, G., Huang, K.- H., Kordjamshidi, P., Zhang, M., Wen, X., Gu, J., et al. Mmgr: Multi-modal generative reasoning. arXiv preprint arXiv:2512.14691, 2025b. 8, 17 Chefer, H., Singer, U., Zohar, A., Kirstain, Y., Polyak, A., Taigman, Y., Wolf, L., and Sheynin, S. Videojam: Joint appearance-motion representations for enhanced motion generation in video models. In International Conference on Machine Learning, 2025. 2, 3, 5 Chen, B., Zhang, T., Geng, H., Song, K., Zhang, C., Li, P., Freeman, W. T., Malik, J., Abbeel, P., Tedrake, R., et al. Large video planner enables generalizable robot control. arXiv preprint arXiv:2512.15840, 2025. 1 Chen, T., Xu, B., Zhang, C., and Guestrin, C. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016. 14 Chi, X., Fan, C.-K., Zhang, H., Qi, X., Zhang, R., Chen, A., Chan, C.-M., Xue, W., Liu, Q., Zhang, S., and Guo, Y. Empowering world models with reflection for embodied video prediction. In International Conference on Machine Learning, 2025. 1 De Vita, M. and Belagiannis, V. Diffusion model guided sampling with pixel-wise aleatoric uncertainty estimation. In Winter Conference on Applications of Computer Vision, 2025. Fan, W., Zheng, A. Y., Yeh, R. A., and Liu, Z. Cfg-zero*: Improved classifier-free guidance for flow matching models. arXiv preprint arXiv:2503.18886, 2025. 5, 14 10 Self-Refining Video Sampling Gillman, N., Herrmann, C., Freeman, M., Aggarwal, D., Luo, E., Sun, D., and Sun, C. Force prompting: Video generation models can learn and generalize physics-based control signals. arXiv preprint arXiv:2505.19386, 2025. 2 Google. Gemini 3, 2025a. URL https://blog. google/products/gemini/gemini-3/. 5, 15 Google. Veo3.1, 2025b. URL https://deepmind. google/models/veo/. 24 Gosselin, A., Luo, G. Y., Lara, L., Golemo, F., Nowrouzezahrai, D., Paull, L., Jolicoeur-Martineau, A., and Pal, C. Ctrl-crash: Controllable diffusion for realistic car crashes. arXiv preprint arXiv:2506.00227, 2025. 2 Gu, J., Liu, X., Zeng, Y., Nagarajan, A., Zhu, F., Hong, \" D., Fan, Y., Yan, Q., Zhou, K., Liu, M.-Y., et al. phyworldbench\": comprehensive evaluation of physical realism in text-to-video models. arXiv preprint arXiv:2507.13428, 2025. 7, 15 HaCohen, Y., Chiprut, N., Brazowski, B., Shalem, D., Moshe, D., Richardson, E., Levin, E., Shiran, G., Zabari, N., Gordon, O., et al. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. 2 Ho, J. and Salimans, T. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 2, Huang, Z., He, Y., Yu, J., Zhang, F., Si, C., Jiang, Y., Zhang, Y., Wu, T., Jin, Q., Chanpaisit, N., et al. Vbench: Comprehensive benchmark suite for video generative models. In Conference on Computer Vision and Pattern Recognition, 2024. 5 Karan, A. and Du, Y. Reasoning with sampling: Your base model is smarter than you think. arXiv preprint arXiv:2510.14901, 2025. 2 Kong, W., Tian, Q., Zhang, Z., Min, R., Dai, Z., Zhou, J., Xiong, J., Li, X., Wu, B., Zhang, J., et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2 Kou, S., Gan, L., Wang, D., Li, C., and Deng, Z. Bayesdiff: Estimating pixel-wise uncertainty in diffusion via bayesian inference. In International Conference on Learning Representations, 2024. 20 Kuaishou. Kling, 2025. URL https://klingai.com/ global/. Li, C., Michel, O., Pan, X., Liu, S., Roberts, M., and Xie, S. PISA experiments: Exploring physics post-training for video diffusion models by watching stuff drop. In International Conference on Machine Learning, 2025a. 1, 2, 7, 15 Li, Z., Tucker, R., Cole, F., Wang, Q., Jin, L., Ye, V., Kanazawa, A., Holynski, A., and Snavely, N. Megasam: Accurate, fast and robust structure and motion from casual dynamic videos. In Computer Vision and Pattern Recognition Conference, 2025b. 8 Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 1, 2 Liu, F., Wang, H., Cai, Y., Zhang, K., Zhan, X., and Duan, Y. Video-t1: Test-time scaling for video generation. In International Conference on Computer Vision, pp. 18671 18681, 2025a. 1 Jang, S., Jo, J., Lee, K., and Hwang, S. J. Identity decoupling for multi-subject personalization of text-to-image models. Advances in Neural Information Processing Systems, 37:100895100937, 2024. 21 Liu, J., Liu, G., Liang, J., Yuan, Z., Liu, X., Zheng, M., Wu, X., Wang, Q., Xia, M., Wang, X., et al. Improving video generation with human feedback. arXiv preprint arXiv:2501.13918, 2025b. Jang, S., Ki, T., Jo, J., Yoon, J., Kim, S. Y., Lin, Z., and Hwang, S. J. Frame guidance: Training-free guidance for frame-level control in video diffusion models. arXiv preprint arXiv:2506.07177, 2025. 3 Jin, Y., Sun, Z., Li, N., Xu, K., Xu, K., Jiang, H., Zhuang, N., Huang, Q., Song, Y., MU, Y., and Lin, Z. Pyramidal flow matching for efficient video generative modeling. In International Conference on Learning Representations, 2025. 2 Kang, B., Yue, Y., Lu, R., Lin, Z., Zhao, Y., Wang, K., Huang, G., and Feng, J. How far is video generation from world model: physical law perspective. In International Conference on Machine Learning, 2025. 1 Liu, S., Ren, Z., Gupta, S., and Wang, S. Physgen: Rigidbody physics-grounded image-to-video generation. In European Conference on Computer Vision, 2024. 2 Lv, J., Huang, Y., Yan, M., Huang, J., Liu, J., Liu, Y., Wen, Y., Chen, X., and Chen, S. Gpt4motion: Scripting physical motions in text-to-video generation via blenderoriented gpt planning. In Conference on Computer Vision and Pattern Recognition Workshops, 2023. 2 Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., et al. Self-refine: Iterative refinement with selffeedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. 11 Self-Refining Video Sampling Meng, C., He, Y., Song, Y., Song, J., Wu, J., Zhu, J.-Y., and Ermon, S. SDEdit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2022. 9 Mi, X., Yu, W., Lian, J., Jie, S., Zhong, R., Liu, Z., Zhang, G., Zhou, Z., Xu, Z., Zhou, Y., et al. Video generation models are good latent reward models. arXiv preprint arXiv:2511.21541, 2025. 1 NVIDIA. Using cosmos-reason1 for rejection sampling, 2025. URL https://docs.nvidia.com/ cosmos/latest/reason1/video_critic. html. Accessed: 2026-01-25. Polyak, A., Zohar, A., Brown, A., Tjandra, A., Sinha, A., Lee, A., Vyas, A., Shi, B., Ma, C.-Y., Chuang, C.-Y., Yan, D., Choudhary, D., Wang, D., Sethi, G., Pang, G., Ma, H., Misra, I., Hou, J., Wang, J., Jagadeesh, K., Li, K., Zhang, L., Singh, M., Williamson, M., Le, M., Yu, M., Singh, M. K., Zhang, P., Vajda, P., Duval, Q., Girdhar, R., Sumbaly, R., Rambhatla, S. S., Tsai, S., Azadi, S., Datta, S., Chen, S., Bell, S., Ramaswamy, S., Sheynin, S., Bhattacharya, S., Motwani, S., Xu, T., Li, T., Hou, T., Hsu, W.-N., Yin, X., Dai, X., Taigman, Y., Luo, Y., Liu, Y.-C., Wu, Y.-C., Zhao, Y., Kirstain, Y., He, Z., He, Z., Pumarola, A., Thabet, A., Sanakoyeu, A., Mallya, A., Guo, B., Araya, B., Kerr, B., Wood, C., Liu, C., Peng, C., Vengertsev, D., Schonfeld, E., Blanchard, E., Juefei-Xu, F., Nord, F., Liang, J., Hoffman, J., Kohler, J., Fire, K., Sivakumar, K., Chen, L., Yu, L., Gao, L., Georgopoulos, M., Moritz, R., Sampson, S. K., Li, S., Parmeggiani, S., Fine, S., Fowler, T., Petrovic, V., and Du, Y. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2025. 2 Qi, H., Yin, H., Zhu, A., Du, Y., and Yang, H. Strengthening generative robot policies through predictive world modeling. arXiv preprint arXiv:2502.00622, 2025. 1 Sadat, S., Hilliges, O., and Weber, R. M. Eliminating oversaturation and artifacts of high guidance scales in diffusion models. In International Conference on Learning Representations, 2024. 2, 4 Savant Aira, L., Montanaro, A., Aiello, E., Valsesia, D., and Magli, E. Motioncraft: Physics-based zero-shot video generation. In Advances in Neural Information Processing Systems, 2024. 2 Shaulov, A., Hazan, I., Wolf, L., and Chefer, H. Flowmo: Variance-based flow guidance for coherent motion in video generation. arXiv preprint arXiv:2506.01144, 2025. 2, 3, 5, 14, 15 Motion-i2v: Consistent and controllable image-to-video generation with explicit motion modeling. In ACM SIGGRAPH, 2024. Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. 1 Song, Y. and Ermon, S. Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 32, 2019. 3, 9 Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021. 1 Vincent, P. connection between score matching and denoising autoencoders. Neural computation, 23(7):1661 1674, 2011. 3 Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. Extracting and composing robust features with denoising autoencoders. In International Conference on Machine Learning, 2008. 2 von Platen, P., Patil, S., Lozhkov, A., Cuenca, P., Lambert, N., Rasul, K., Davaadorj, M., Nair, D., Paul, S., Berman, W., Xu, Y., Liu, S., and Wolf, T. Diffusers: State-of-the-art diffusion models. https://github. com/huggingface/diffusers, 2022. Wang, A., Ai, B., Wen, B., Mao, C., Xie, C.-W., Chen, D., Yu, F., Zhao, H., Yang, J., Zeng, J., et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025a. 1, 2, 4, 5, 14 Wang, C., Chen, C., Huang, Y., Dou, Z., Liu, Y., Gu, J., and Liu, L. Physctrl: Generative physics for controllable and physics-grounded video generation. In Advances in Neural Information Processing Systems, 2025b. 2 Wang, J., Ma, A., Cao, K., Zheng, J., Zhang, Z., Feng, J., Liu, S., Ma, Y., Cheng, B., Leng, D., Yin, Y., and Liang, X. Wisa: World simulator assistant for physics-aware text-to-video generation. arXiv:2502.08153, 2025c. 2 Wiedemer, T., Li, Y., Vicol, P., Gu, S. S., Matarese, N., Swersky, K., Kim, B., Jaini, P., and Geirhos, R. Video models are zero-shot learners and reasoners. arXiv preprint arXiv:2509.20328, 2025. 2, 8, 17, 19 Wu, T., Si, C., Jiang, Y., Huang, Z., and Liu, Z. Freeinit: Bridging initialization gap in video diffusion models. In European Conference on Computer Vision, 2024. 2, 9 Shi, X., Huang, Z., Wang, F.-Y., Bian, W., Li, D., Zhang, Y., Zhang, M., Cheung, K. C., See, S., Qin, H., et al. Xu, Y., Deng, M., Cheng, X., Tian, Y., Liu, Z., and Jaakkola, T. Restart sampling for improving generative processes. 12 Self-Refining Video Sampling Advances in Neural Information Processing Systems, 2023. 9 Yang, X., Li, B., Zhang, Y., Yin, Z., Bai, L., Ma, L., Wang, Z., Cai, J., Wong, T.-T., Lu, H., et al. Vlipp: Towards physically plausible video generation with vision and language informed physical prior. arXiv:2503.23368, 2025a. 2 Yang, Z., Teng, J., Zheng, W., Ding, M., Huang, S., Xu, J., Yang, Y., Hong, W., Zhang, X., Feng, G., Yin, D., Yuxuan.Zhang, Wang, W., Cheng, Y., Xu, B., Gu, X., Dong, Y., and Tang, J. Cogvideox: Text-to-video diffusion models with an expert transformer. In International Conference on Learning Representations, 2025b. 1, Yuan, J., Pizzati, F., Pinto, F., Kunze, L., Laptev, I., Newman, P., Torr, P., and De Martini, D. Likephys: Evaluating intuitive physics understanding in video diffusion models via likelihood preference. arXiv preprint arXiv:2510.11512, 2025. 1 Zhang, K., Xiao, C., Xu, J., Mei, Y., and Patel, V. M. Think before you diffuse: Llms-guided physics-aware video generation. arXiv preprint arXiv:2505.21653, 2025. 2 Zhao, Q., Ni, X., Wang, Z., Cheng, F., Yang, Z., Jiang, L., and Wang, B. Synthetic video enhances physical fidelity in video synthesis. arXiv preprint arXiv:2503.20822, 2025. 2 Zhao, W., Bai, L., Rao, Y., Zhou, J., and Lu, J. Unipc: unified predictor-corrector framework for fast sampling of diffusion models. Advances in Neural Information Processing Systems, 36:4984249869, 2023. 5 Zhou, F., Huang, J., Li, J., Ramanan, D., and Shi, H. Pai-bench: comprehensive benchmark for physical ai. arXiv preprint arXiv:2512.01989, 2025. 5, 15 Self-Refining Video Sampling"
        },
        {
            "title": "Appendix",
            "content": "Organization The Appendix is organized as follows: We provide experimental details in Sec. and further discussion in Sec. B. Lastly, in Sec. C, we discuss the limitations and future directions of our work. A. Experimental Details A.1. Base models Wan (Wang et al., 2025a) Wan2.1 and Wan2.2 are opensource, flow matching-based video generation models, released in text-to-video (T2V) and image-to-video (I2V) variants. The I2V model is trained with first-frame condition, ensuring that the generated video exactly reproduces the input image as the first frame without additional inference techniques. This strong first-frame constraint provides stable context during sampling, preventing oversaturation even under high classifier-free guidance (CFG). Consequently, multiple P&P iterations can be applied without uncertainty-aware strategy. For our experiments with Wan2.2 I2V, we therefore disable uncertainty-aware sampling (i.e., τ = 0), allowing unconstrained P&P refinement. Wan2.2 improves upon Wan2.1 by incorporating two expert transformer models that are activated based on the flow matching timestep. In addition, Wan2.2 adopts an exponential sampling schedule, allocating more NFEs at high-noise timesteps. This design enhances motion synthesis in early sampling stages. Accordingly, when applying P&P with the time interval αT in Algorithm 1, the motion stage is longer than in Wan2.1. To ensure fair comparison, we use smaller α for Wan2.2 so that the total NFEs do not exceed 1.5 that of the base sampler. Cosmos-Predict-2.5 (Ali et al., 2025) We use CosmosPredict-2.5-2B post-trained for I2V generation. Empirically, we observe that this model is more prone to over-saturation under high CFG scale compared to Wan I2V. Accordingly, we set the CFG scale to 4 for both the base sampler and P&P, which we find to be stable with minimal saturation artifacts. To further mitigate over-saturation, we apply uncertaintyaware P&P with τ = 0.5. More details of hyperparameters are in Sec. A.2. A.2. Implementation Details All experiments are conducted on single NVIDIA H100 80GB GPU. Notably, while our method increases the NFE, its memory usage remains identical to that of the base sampler. For the Wan series, including the CFG-zero (Fan et al., 2025) baseline, we primarily use our own implementation built upon the Diffusers (von Platen et al., 2022) Wan pipeline. FlowMo (Shaulov et al., 2025) experiments Algorithm 2 single uncertainty-aware P&P step (code) 1 # buffer (previous step): [pred_z1, pred_z_next, m_unc] 2 3 noise = randn_like(buffer[0]) 4 z_t_pnp = * buffer[0] + (1-t) * noise # Perturb 5 6 flow_pred = model(z_t_pnp, t, **kwargs) # w/ CFG 7 pred_z1 = z_t_pnp + (1-t) * flow_pred 8 pred_z_next = z_t_pnp + delta_t * flow_pred 9 10 uncertainty = L1_distance(buffer[0], pred_z_1) 11 m_unc = (uncertainty > tau) buffer[2] 12 13 pred_z1 = m_unc * pred_z1 + (1-m_unc) * buffer[0] 14 pred_z_next = m_unc * pred_z_next + (1-m_unc) * buffer [1] 15 16 # in last P&P iteration, return pred_z_next 17 buffer = [pred_z1, pred_z_next, m_unc] follow the official implementation, with additional engineering modifications to improve efficiency. Specifically, we incorporate gradient checkpointing (Chen et al., 2016), reducing the required hardware from two GPUs to single GPU while also improving runtime performance. These modifications enable FlowMo to scale to larger models such as Wan2.2. We follow the official setting and used learning rate of η = 0.005 for all FlowMo experiments. For Cosmos, we use the official implementation and the classifier-free guidance scale to = 4 across all experiments. The output video resolution is set to 480p for all Wan-series models and 720p for Cosmos. Detailed Algorithm We provide detailed code-level implementation in Algorithm 2. In practice, the uncertainty mask is accumulated across P&P iterations (line 11), ensuring that regions identified as certain are frozen and no longer refined in later iterations. Hyperparameters In our implementation, the refinement strength is controlled by specifying how many P&P iterations Kti are applied at each inference step ti within the motion stage, rather than using single global value of Kf and α. Concretely, we define P&P plan as mapping from inference step ranges to the number of P&P iterations applied at each step. For example, plan {2-5 : 2, 6-10 : 1} applies Kf = 2 at steps 25 and Kf = 1 at steps 610. The specific plan is adjusted slightly depending on the task and model. For motion-enhanced video generation, we do not apply P&P at the earliest steps in order to allow coarse spatial layout (e.g., camera movement) to be determined. Specifically, we use {3-6 : 3, 7-14 : 1}, which results in an additional 20 NFEs in total. In later steps, we apply only single P&P iteration to lightly refine less critical regions while maintaining computational efficiency. All taskand model-specific hyperparameters are summarized in Table 6. Self-Refining Video Sampling Task Setting P&P plan τ Cosmos2. Physical AI (I2V) Wan2.2-I2V {3-6 : 3, 7-14 : 1} 0. {3-4 : 5, 5-15 : 1} 0.5 Physical AI (I2V) Physics Video (T2V) Wan2.2-T2V {3-6 : 3, 7-14 : 1} 0.25 Wan2.2-T2V {3-6 : 3, 7-14 : 1} 0.25 Motion-enhanced Wan2.1-T2V {3-7 : 3, 7-16 : 1} 0.50 Motion-enhanced Wan2.2-T2V {3-6 : 3, 7-14 : 1} 0.25 Spatial Table 6. Taskand model-specific hyperparameters used for P&P. VBench Figure 10. Full human evaluation results on Dynamic-Bench, including ties. Method Motion Dynamic Const. Quality NFE Time Wan2.1-14B + NFE2 + FlowMo + CFG-Zero + Ours 98.10 98.01 97.49 98.00 98.37 Wan2.1-1.3B 98.21 + NFE2 98.23 97.89 + FlowMo 98.01 + CFG-Zero 98.84 + Ours 77.34 77.34 79.17 78.13 77.34 75.00 77.34 75.00 85.16 73.31 94.22 94.32 93.40 94.20 94. 94.05 94.23 93.77 93.71 94.95 61.92 61.95 60.89 61.63 63.08 61.10 61.60 59.88 60.71 61.43 50 100 2.0 50* 3.3 1.0 50 1.5 74 50 100 2.0 50* 3.3 1.0 50 1.5 74 Table 7. VideoJAM-bench results measuring motion coherence. Additional inference time (*) of FlowMo is introduced by gradient computation. A.3. Motion Enhanced Video Generation Baselines We reimplement FlowMo (Shaulov et al., 2025) with gradient checkpointing, enabling it to run on single GPU. For Wan2.1, we follow the official implementation. For Wan2.2, since FlowMo requires additional GPU memory due to gradient computation, we employ CPU offloading, and set the length of the FlowMo refinement steps to match that of the P&P steps in our method. We report VideoJam-bench results using Wan2.1 in Tab. 7. Our method achieves the strongest performance on VBench metrics. We note that these automated metrics are largely saturated, which may limit their sensitivity to fine-grained motion quality. Human Evaluation We provide an example of the human evaluation interface in Fig. 26 left. For each prompt, we display pair of videos generated with the same random seed, one from our method and one from baseline, and ask evaluators to assess motion quality and text alignment. Each evaluator views only single video pair per prompt, with baseline methods randomly shuffled to avoid bias. The evaluation includes tie option. In Tab. 1, we report the tie-adjusted win rate (counting each tie as half win), while the complete results including ties are shown in Fig. 10. A.4. Image-to-Video Generation (Robotics) Benchmark We use all 174 robot-domain datasets from PAI-Bench-G (Zhou et al., 2025) as image-prompt pairs. For Robot-QA, we use Qwen2.5-VL-72B-Instruct (Bai et al., Figure 11. Full human evaluation results on VideoPhy2 (Bansal et al., 2025) hard subset, including ties. 2025b), which provides sufficiently strong performance on robot-domain evaluation. We provide the grasp success rate evaluated using Gemini 3 Flash (Google, 2025a) in Fig. 12. To accurately assess grasp motion, videos are evaluated at high input resolution with frame rate of 4 fps, and samples with scores of 4 or 5 are treated as successful grasps. A.5. Physics-aligned Video Generation Benchmark We follow the original evaluation prompts of VideoPhy2 (Bansal et al., 2025) and PhyWorldBench (Gu et al., 2025), but perform all automatic evaluations using Gemini 3 Flash, which supports higher input frame rates. For PhyWorldBench, we evaluate only the two categories most closely related to motion, Object Motion and Kinematics and Interaction Dynamics. For PisaBench (Li et al., 2025a), since the evaluation requires square inputs, all videos are generated in resolution of 512 512. Baselines Regarding rejection sampling (best-of-4), we follow the official documentation (NVIDIA, 2025). Specifically, we use Cosmos-Reason1 7B (Azzolini et al., 2025) to repeatedly query whether generated video contains anomalies or artifacts, and compute the score by averaging the number of responses indicating the absence of anomalies. Human Evaluation We provide an example of the human evaluation interface in Fig. 26 right. For each prompt, we display pair of videos generated with the same random seed, one from our method and one from baseline, and ask evaluators to assess physical commonsense (PC) and text alignment (semantic alignment; SA). Since the video generation prompts in the benchmark are relatively long, we highlight key phrases using colored blocks. All other evaluation details follow those used for motion-enhanced video generation. We provide the complete human evaluation 15 Self-Refining Video Sampling Task Description: Evaluate whether the robot in the video successfully performs grasp action with proper physical contact with the target object. Evaluation Criteria: 1. Contact Detection: Does the robots gripper/end-effector make actual physical contact with the object? 2. Grasp Validity: Is the grasp physically plausible? (No floating objects, no penetration artifacts) 3. Object Manipulation: After grasping, is the object properly held/moved by the robot? 4. Visual Artifacts: Are there any visual artifacts such as objects floating without contact, gripper passing through objects, or impossible physical interactions? Instructions for Scoring: - 1 (Fail): No grasp attempt, or severe artifacts (object floats, no contact, gripper passes through object) - 2 (Poor): Grasp attempted but clear physical violations (partial penetration, unrealistic contact) - 3 (Moderate): Grasp occurs but with minor artifacts or questionable contact quality - 4 (Good): Clear grasp with proper contact, minor visual imperfections acceptable - 5 (Excellent): Perfect grasp with realistic physical contact and natural object manipulation. Special Cases: - If there is NO grasp action in the video (robot doesnt attempt to grasp anything), score as 1 (Fail) - If the object appears to stick to the gripper without proper contact, score as 1Please provide the output in the format: Score: [1-5] Grasp Detected: [Yes/No] Explanation: [Your detailed reasoning about the grasp quality and any artifacts observed] Figure 12. Gemini prompt for evaluating grasp success rate in Tab. 2. We treat scores of 4 or 5 as successful grasps. results including ties are provided in Fig. 11. A.6. Improving Spatial Consistency Benchmark The prompts used in this evaluation all include first-person view. Additionally, since models frequently produce misaligned videos under such settings, including limited camera rotation or unstable camera trajectories, we generate multiple videos per prompt and filter them based on camera viewpoints. Specifically, we retain videos with sufficiently large yaw coverage and stable camera trajectories, while discarding cases dominated by in-place rotation or exhibiting unreliable viewpoint estimates. In total, we conduct our evaluation on 20 videos. A.7. Ablation Studies on Hyperparameters We provide ablation studies on the key hyperparameters of our method, the number of P&P iterations Kf and the confidence threshold τ , in Fig. 16. Increasing Kf strengthens the refinement effect of P&P, but also incurs additional NFEs and results in larger deviations from the base ODE samples. In contrast, small value such as Kf = 1 is insufficient to adequately refine large motion in the generated videos. The confidence threshold τ primarily controls how well background appearance and overall color tone from the base ODE sampling are preserved. As τ increases, refinement becomes more conservative, slightly reducing refinement strength while better preserving the original background structure and color tone. As discussed in Sec. 4.3, when Kf becomes large, saturation artifacts may still appear even with uncertainty-aware strategy, such as an overall brightening of the video. In such cases, jointly increasing τ effectively mitigates these artifacts by restricting refinement to more uncertain regions. Based on these observations, we use Kf = 3 and τ = 0.25 by default, which provides favorable trade-off between sample quality and computational cost. As shown in Fig. 17, we conduct an ablation study on α, which determines the temporal extent of the motion stage where P&P refinement is applied. When α exceeds certain threshold, all configurations in Fig. 17(bd) produce stable and coherent motion. However, comparing (c) and (d) reveals that later inference steps contribute less to motion dynamics. From an efficiency perspective, reducing Kf or disabling P&P at later steps is more effective. Similarly, applying P&P only at late stages, as in Fig. 17(e), reduces visual artifacts since the motion has already been largely determined, but fails to fully correct motion errors due to strong cross-frame consistency. B. More Discussions B.1. Cross-Frame Consistency of Video As discussed in Sec. 6.1, strong cross-frame consistency makes videos robust to perturbations. As shown in Fig. 13, 16 Self-Refining Video Sampling (a) Comparison of SDEdit results on an image and video while changing the prompt from orange cat to brown dog. Figure 14. Accumulated effect of iterative P&P at an early inference step. We plot the L2 distance between the intermediate refined latent ˆz(k) and the final refined latent ˆz 1 at fixed inference step = 0.009T . Results are obtained using Wan2.2-A14B T2V. Image generation with P&P Video generation with P&P (b) Comparison of applying P&P on an image and video. We apply 3 P&P iterations at the inference step indicated by the topleft number of each sample. Figure 13. Cross-frame consistency in videos. Due to strong temporal correlations across frames, video is more robust to perturbation than image. videos are less responsive to both SDEdit and multiple P&P iterations than images, making late-stage one-shot perturbations less effective. This indicates that effective video refinement, especially for motion, requires larger early-stage perturbations and iterative refinement. Despite this difficulty, such robustness allows refinement effects to accumulate stably across iterations. When we measure the L2 distance between the final refined prediction 1 and intermediate predictions ˆz(k) ˆz during P&P iterations at an early timestep, we observe near-linear decrease for videos, as shown in Fig. 14. In contrast, images exhibit oscillatory behavior, indicating inconsistent refinement directions. This behavior indicates that refinement effects accumulate consistently across iterations for videos, motivating an early-stage, iterative refinement strategy. B.2. Other domains Proposed P&P is applicable to general flow matching generators. We further examine the effectiveness of this framework on the image generation using FLUX-1.dev (BlackForest-Labs, 2024). As shown in Fig. 15, P&P can reduce text-related artifacts, leading to clearer and more coherent text rendering compared to the base ODE sampling. We Figure 15. Image generation with P&P using FLUX.1-dev. With only two additional NFEs (4%), our method effectively reduces text-related artifacts, resulting in clearer and more coherent text. generate four samples with different random seeds using the prompt cat holding sign that says Predict-and-Perturb: Self-Refining Video Sampling. For image generation, P&P is applied only twice at the 10th inference step out of 50 total steps in FLUX, resulting in only 4% increase in NFEs. Unlike the video domain, where cross-frame consistency makes refinement more robust but often requires multiple iterations, image generation typically benefits from only few P&P iterations at fixed noise level to achieve noticeable improvements. Notably, the uncertainty estimation in the image domain is primarily concentrated on challenging regions such as text rendering, leading P&P to selectively refine the text while leaving the rest of the image unchanged. B.3. Application to Visual Reasoning As discussed in Sec. 5.5, we evaluate whether P&P also improves visual reasoning capabilities (Cai et al., 2025b; Wiedemer et al., 2025) using Wan2.2-A14B I2V. We first consider the graph traversal task introduced by Wiedemer et al. (2025). In this task, graph is visualized as connected nodes and edges, and the model is required to simulate traversal process that progressively propagates from designated source node to neighboring nodes over time. qualitative example is provided in Fig. 18. With frame-byframe human verification over 10 runs with different random seeds, the base Wan2.2-A14B I2V model achieves success rate of 0.1, whereas our method improves this to 0.8. 17 Self-Refining Video Sampling Figure 16. Ablation studies on the hyperparameters Kf and τ . Figure 17. Ablation studies on the hyperparameter α. Gray blocks indicate Euler method and orange blocks indicate P&P. P&P significantly improves motion coherence when applied in earlier steps (b-c), while providing only marginal gains at later steps (d-e). Self-Refining Video Sampling Figure 18. Graph traversal task in Wiedemer et al. (2025). We use Wan2.2-A14B I2V with an upsampled prompt: Starting from the blue well, blue water begins to flow slowly through the connected channel system. The water gradually fills the nearest nodes first.... The success rate increases from 0.1 to 0.8 with P&P method. Figure 19. Maze solving task in Wiedemer et al. (2025). We use Wan2.2-A14B I2V with base prompt: The red square slides smoothly along the white path, stopping perfectly on the green square. Both the base model and P&P method achieve near-zero success rates. Figure 20. Visualization of uncertainty maps across inference timesteps. Overall uncertainty gradually decreases as inference progresses. Even at an early timestep (t = 0.0037T ), higher uncertainty values are observed for objects exhibiting motion. 19 Self-Refining Video Sampling Figure 21. Toy experiment on 2D Gaussian mixture. Repeated P&P iterations (i.e., Kf = 32) yield samples concentrated in the modes. Figure 22. Mode-seeking behavior induced by excessive P&P iterations in image generation. We use Wan2.2-A14B T2V with single frame and apply P&P with Kf = 8, τ = 0 at steps 1620 of the 40 step flow matching inference. However, as shown in Fig. 19, when evaluated on mazesolving tasks, the improvement remains limited. In particular, the model frequently generates invalid paths that cross walls, and the moving block often fails to stop precisely at the target green cell. This contrast highlights an inherent limitation of our approach: due to its local search nature, P&P is effective for reasoning tasks that can be partially refined through motion correction or temporal consistency, such as graph traversal, where errors can be progressively corrected during sampling. In contrast, tasks whose success is determined by discrete or semantic correctness, such as maze solving, require global planning and path-level decisions, which are not easily corrected by local refinement. In such cases, incorporating external verifiers or global search mechanisms is likely necessary. B.4. Uncertainty Map Instead of variance-based methods (De Vita & Belagiannis, 2025; Kou et al., 2024), for example, those that estimate uncertainty by computing the variance of multiple score predictions from repeated stochastic forward passes (typically = 5 evaluations per step), we adopt simpler and more efficient formulation. Our uncertainty estimate is obtained directly within the base P&P iteration, introducing no additional sampling or computational overhead., We provide more visual examples of uncertainty estimation in Fig. 20. As inference progresses, the magnitude of perturbations decreases, leading to gradual reduction in uncertainty. While this observation suggests that an adaptive threshold, such as time-dependent τt, could be considered instead of fixed τ , we leave the investigation of such adaptive schemes for future work. B.5. Mode-Seeking Behavior of P&P Toy experiments on 2D Gaussian mixture show that excessive P&P concentrates samples in high-density regions, exhibiting clear mode-seeking behavior, as illustrated in Fig. 21. similar effect is observed in image generation. As shown in Fig. 22, increasing the number of P&P iterations (Kf = 8) significantly reduces sample diversity, with the prompt an animal producing nearly identical white goats. In the video domain, however, the effect is different. Due to cross-frame consistency and uncertainty-aware sampling, the method does not collapse semantic diversity. Instead, it primarily refines motion while preserving the original content, reducing undesired temporal variance such as motion 20 Self-Refining Video Sampling Figure 23. P&P is also applicable to diffusion-based video generation models (e.g., CogVideoX (Yang et al., 2025b)), where it corrects video artifacts, such as truncated lightsaber and distortions around the teddy bears mouth. (Image credit: MuDI (Jang et al., 2024)) finding good initial noise may be more effective than iterative refinement. Combining refinement with global search strategies or external verifiers is possible direction for future research. Refinement Model Choice Although we use the same model for self-refinement, this is not strict requirement. Future work may explore using different generative models or fine-tuned model specialized for refinement. Stochasticity in Refinement More refinement iterations increase the chance of improvement but still rely on stochastic noise. Developing more effective ways to control or utilize this stochasticity is left for future work. artifacts or flickering. From this perspective, our method can be viewed as an intended temporal mode-seeking for improving output consistency. B.6. P&P with Diffusion Models In this paper, we primarily focus on flow matchingbased models, which are widely adopted in recent video and image generators. Our framework is also applicable to diffusion models, such as CogVideoX (Yang et al., 2025b), since diffusion models are trained with similar objectives, allowing our method to be applied in the same manner at inference time. As shown in Fig. 23, P&P corrects artifacts such as truncated lightsaber and distortions around the teddy bears mouth. C. Limitations and Future Work In this section, we discuss the limitations of our approach and outline directions for future research. Risk of Over-Refinement Hyperparameters such as the uncertainty threshold τ help prevent over-refinement and loss of semantic diversity. However, conservative settings can weaken refinement or require larger number of P&P iterations Kf . Finding better balance between refinement strength and diversity remains future work. Local-Search Behavior Our method can be viewed as local search process. For tasks such as maze solving, Self-Refining Video Sampling Figure 24. Additional visual examples of complex motion generation using Wan2.2-A14B T2V. 22 Self-Refining Video Sampling Figure 25. Additional visual examples of physics-aligned generation using Wan2.2-A14B T2V. Our method also captures realistic physical interactions and fine-grained visual details. Figure 26. screenshot of the human evaluation questionnaires used for (left) motion-enhanced video generation on Dynamic-Bench and (right) physics-aligned video generation on the VideoPhy2 hard subset. 23 Self-Refining Video Sampling Figure 27. Qualitative comparison with commercial closed models, Veo 3.1 (Google, 2025b) and Kling 2.6 (Kuaishou, 2025). While the commercial models produce more aesthetic visual quality, our method demonstrates competitive performance on complex motion scenarios. Prompt: parkour athlete runs up vertical wall, grabs the ledge, and muscles up to stand on the roof in one fluid motion. and gymnast on pommel horse swings their legs in wide circles (flares), supporting their entire weight on alternating hands. 24 Self-Refining Video Sampling D. Dynamic Bench 1-40: Multi-object interactions, 41-80: Complex human motions, 81-120: Physics-driven dynamics. 1. bowling ball rolls down polished lane and strikes perfect strike, sending all ten pins flying in different trajectories. 2. chef tosses pizza dough high into the air, catching it on their knuckles and spinning it to expand its size. 3. playful Golden Retriever catches frisbee in mid-air, causing the dog to twist its body and land on its hind legs. 4. robot arm on an assembly line picks up car door and precisely welds it onto chassis, creating sparks upon contact. 5. gust of wind blows stack of papers off an outdoor table, causing person to scramble and catch them before they fly away. 6. sword fighter parries heavy blow from an opponents axe, causing the axe to slide down the blade and spark against the crossguard. 7. child builds tower of wooden blocks, then pulls bottom block out, causing the structure to wobble and collapse chaotically. 8. pool player executes jump shot; the cue ball hops over blocking ball to sink the 8-ball in the corner pocket. 9. sweeping broom pushes pile of dust and small debris into dustpan, with some dust particles escaping into the air. 10. drone flies into hanging wind chime, tangling its propellers in the strings and causing the chimes to swing violently. 11. basketball hits the rim, bounces straight up, hits the backboard, and finally falls through the net. 12. wrecking ball smashes through brick wall, sending debris and dust clouding into the interior of the building. 13. person pours hot milk into cup of coffee, creating swirling mixture of brown and white liquids. 14. Two bumper cars collide head-on at carnival, causing both drivers to jolt forward while the cars recoil backward. 15. tennis ball is served at high speed, deforming against the racket strings before launching across the net. 16. bartender shakes cocktail mixer vigorously, with ice cubes audibly clinking and condensation forming on the metal exterior. 17. cat paws at dangling yarn ball, causing it to swing in pendulum motion while the cat tries to grab it again. 18. heavy book falls from shelf onto beanbag chair, causing the chair to depress deeply and then slowly regain some shape. 19. person opens shaken soda can, causing foam to spray out and coat their hand and the table. 20. skateboarder grinds along metal rail, sparks flying from the trucks before they land on the concrete. 21. knife slices through ripe tomato, separating slice that falls flat onto the cutting board while juice spreads. 22. person types rapidly on mechanical keyboard, with each key depressing and springing back up individually. 23. wrecking crew uses grapple to pull down rusted metal tower, which twists and buckles before hitting the ground. 24. soccer goalkeeper punches high ball, changing its trajectory from toward the net to over the crossbar. 25. magnet is brought close to pile of iron filings, causing them to leap up and attach to the magnet in spiky pattern. 26. domino chain reaction begins, with the dominoes splitting into two separate paths that eventually trigger small flag to raise. 27. person struggles to close an overfilled suitcase, sitting on it to compress the clothes inside before zipping it shut. 25 Self-Refining Video Sampling 28. hammer strikes nail, driving it partially into the wood, but the second strike bends the nail sideways. 29. bird lands on thin tree branch, causing the branch to bow significantly under the weight and bounce as the bird stabilizes. 30. figure skater lifts their partner overhead, rotating while the partner holds pose, their costumes flowing together. 31. person uses wrench to tighten leaking pipe; as the nut turns, the water spray reduces to drip. 32. coin is spun on table, wobbling faster and faster until it settles flat with distinctive rattle. 33. car drives through large puddle, splashing water high onto the sidewalk and drenching nearby fire hydrant. 34. robotic vacuum bumps into sleeping dog, causing the dog to lift its head and the vacuum to rotate and move away. 35. majestic eagle swoops down to the water surface, snatching fish with its talons and creating splash pattern. 36. person playing Jenga carefully pushes block from the center, the tower swaying slightly but remaining upright. 37. grandiose chandelier falls from the ceiling, crashing onto banquet table and shattering plates and glasses. 38. baker kneads heavy dough, pushing their palms into it, causing it to stretch and fold back over itself. 39. bicyclist hits curb, the front tire compressing and the rider jerking the handlebars to maintain balance. 40. Newtons Cradle is set in motion; one ball hits the stack, and the ball on the opposite end swings out, demonstrating momentum transfer. 41. breakdancer performs headspin, transitioning smoothly into freeze pose with legs crossed in the air. 42. parkour athlete runs up vertical wall, grabs the ledge, and muscles up to stand on the roof in one fluid motion. 43. ballerina performs series of rapid fouetté turns en pointe, maintaining fixed spotting point with her head. 44. martial artist executes flying spinning hook kick, landing in crouched combat stance. 45. gymnast on the uneven bars swings from the high bar, releases, performs double backflip, and re-catches the bar. 46. figure skater executes triple axel, taking off forward and rotating three and half times before landing backward on one foot. 47. capoeira practitioner performs ginga movement followed immediately by low sweeping leg kick (meia lua de compasso). 48. high jumper performs the Fosbury Flop, arching their back severely over the bar and kicking their legs up at the last second. 49. yoga instructor flows from downward dog into scorpion handstand, balancing on their forearms with legs arched over their head. 50. sprinter explodes out of the starting blocks, body at 45-degree angle, transitioning into an upright running posture. 51. rock climber performs dynamic \"dyno\" move, leaping from one hold to distant hold, catching it with one hand and swinging. 52. rhythmic gymnast throws hoop high into the air, performs cartwheel, and catches the hoop with her foot. 53. snowboarder rides up halfpipe, performs McTwist (inverted 540 degree spin), and lands cleanly on the transition. 54. professional wrestler performs suplex on dummy, arching their back to throw the weight over their head. 55. salsa dancer spins their partner rapidly, then dips them low to the ground, pausing for beat before pulling them back up. 26 Self-Refining Video Sampling 56. pole vaulter plants the pole, the pole bends dramatically, launching the athlete feet-first over the bar. 57. surfer performs sharp cutback on wave, twisting their torso and shifting weight to spray water off the tail of the board. 58. contortionist slowly bends backward from standing position until they grab their own ankles. 59. hip-hop dancer performs \"the worm,\" rippling their body along the floor from chest to feet. 60. soccer player performs bicycle kick, leaping back-first into the air and scissoring legs to strike the ball. 61. diver performs reverse 2.5 somersault from the 10-meter platform, entering the water with minimal splash. 62. fencer lunges deeply with foil, extending their arm fully while their back leg remains straight and grounded. 63. heavy metal drummer plays rapid blast beat, arms and legs moving in blur of independent rhythms. 64. traditional Indian dancer (Bharatanatyam) stomps rhythmically while performing complex mudras (hand gestures) and eye movements. 65. cheerleader is thrown into the air, performs twist, and is caught in cradle position by her teammates. 66. skateboarder performs tre-flip (360 pop shove-it plus kickflip) down set of stairs. 67. stunt performer is \"shot,\" jerking backward violently and falling over railing, flailing arms. 68. tai chi master performs \"Parting the Wild Horses Mane,\" moving with extreme slowness and fluid weight transfer. 69. basketball player performs crossover dribble, fake-drives left, spins right, and performs slam dunk. 70. swimmer performs tumble turn underwater, tucking tightly and pushing off the wall to glide in streamline. 71. trapeze artist releases their bar, performs triple somersault in mid-air, and is caught by the catcher on the opposing bar. 72. person slips on banana peel (cartoon style), feet flying up above their head before they land flat on their back. 73. cricket bowler runs up and delivers the ball with straight-arm action, following through with their body momentum. 74. baton twirler spins the baton around their body, under their legs, and over their neck without using their hands. 75. synchronized swimming team emerges from the water in pyramid formation, holding the pose before sinking back down. 76. BMX rider performs backflip tailwhip over dirt jump, kicking the bike frame around while upside down. 77. slackliner walks across loose line, arms flailing to maintain balance as the line shakes violently. 78. An ice hockey goalie drops into butterfly position to block shot, then quickly scrambles back to standing position. 79. conductor leads an orchestra with vigorous arm movements, hair flying as they signal crescendo. 80. gymnast on pommel horse swings their legs in wide circles (flares), supporting their entire weight on alternating hands. 81. glass of red wine shatters on marble floor, the liquid splashing outward in slow motion while shards glide across the surface. 82. Thick, golden honey is poured from jar onto stack of pancakes, folding over itself and slowly dripping down the sides. 83. silk scarf blows in violent gale storm, rippling rapidly and snapping in the wind without tearing. 27 Self-Refining Video Sampling 84. water balloon hits persons face in slow motion, the rubber expanding around their features before bursting and spraying water. 85. large soap bubble floats through the air, wobbling and reflecting an iridescent rainbow before popping into tiny droplets. 86. campfire crackles in the night, with sparks rising in spiral pattern and smoke shifting direction with the breeze. 87. car drives through thick fog, its headlights creating volumetric beams that illuminate the swirling mist particles. 88. block of dry ice is dropped into warm water, instantly generating thick, heavy white fog that spills over the containers edge. 89. handful of glitter is thrown into the air, catching the light and twinkling as it drifts slowly to the ground. 90. large wave crashes against cliffside, the water atomizing into fine mist and white foam running down the rocks. 91. cannonball is fired into sand dune, displacing massive crater of sand that sprays outward and slides back into the hole. 92. heavy velvet curtain is pulled back, bunching up in thick, heavy folds that sway heavily with the movement. 93. distinct drop of ink falls into glass of clear water, blooming into abstract, smoke-like tendrils as it diffuses. 94. pristine snowbank collapses, triggering small avalanche where clumps of snow break apart into powder as they slide. 95. jellyfish swims in the deep ocean, its translucent bell pulsing rhythmically and its long tentacles trailing fluidly behind. 96. person with long hair stands in front of high-powered fan, the hair whipping chaotically and obscuring their face. 97. Molten lava flows slowly down volcano, the surface cooling into black crust while red-hot magma breaks through the cracks. 98. rubber ball bounces on trampoline, depressing the surface deeply and launching higher with every bounce. 99. stack of newspapers is left in the rain; the paper darkens, sags, and begins to disintegrate into pulp. 100. tornado touches down in field, pulling up dirt, grass, and debris into rotating funnel cloud. 101. high-speed bullet passes through an apple, causing the exit side to explode outward in cone of pulp and juice. 102. candle flame flickers in drafty room, the wax melting and dripping down the side of the candle unevenly. 103. bowl of Jell-O is nudged, wobbling vigorously with gelatinous, elastic motion that slowly dampens. 104. heavy metal chain is dropped onto metal floor, coiling and uncoiling as the links settle with metallic weight. 105. Dust motes dance in shaft of sunlight in an old attic, moving with Brownian motion. 106. wet dog shakes itself dry in slow motion, the loose skin rippling and water droplets forming halo around the animal. 107. porcelain vase is glued back together, but when filled with water, it slowly leaks from the cracks, forming beads on the surface. 108. huge flag waves in slow motion, showcasing the heavy fabric rolling and snapping, creating shadows within the folds. 109. Oil and vinegar are shaken in bottle, forming temporary emulsions of small bubbles that slowly separate back into layers. 110. meteor enters the atmosphere, burning up with fiery tail and shedding glowing debris before disintegrating. 111. feather falls in vacuum chamber (straight down) versus feather falling in air (drifting side to side). 28 Self-Refining Video Sampling 112. mesmerizing ferrofluid spikes and dances in response to moving magnetic field, the black liquid looking alien and sharp. 113. Raindrops hit puddle, creating concentric ripples that interfere with one another in complex geometric pattern. 114. marshmallow is roasted over fire, the outer skin bubbling, browning, and eventually catching small blue flame. 115. piece of paper burns from the center, the edges curling and turning to black ash that flakes away. 116. slime toy is stretched between two hands, becoming thin and translucent before snapping back into glob. 117. Heavy rain falls on car windshield, the wipers pushing the water aside in sheets that immediately reform. 118. wrecking ball hits building made of glass, causing cascade of shattering panes that reflect the sky as they fall. 119. Steam rises from hot geyser, billowing rapidly and dissipating into the cold air above. 120. hand touches plasma globe, causing the purple arcs of electricity to concentrate and follow the fingers across the glass."
        }
    ],
    "affiliations": [
        "DeepAuto.ai",
        "KAIST",
        "NTU Singapore",
        "NYU"
    ]
}