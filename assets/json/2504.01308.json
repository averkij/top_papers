{
    "paper_title": "Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks",
    "authors": [
        "Jiawei Wang",
        "Yushen Zuo",
        "Yuanjun Chai",
        "Zhendong Liu",
        "Yichen Fu",
        "Yichun Feng",
        "Kin-man Lam"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language Models (VLMs) extend the capabilities of Large Language Models (LLMs) by incorporating visual information, yet they remain vulnerable to jailbreak attacks, especially when processing noisy or corrupted images. Although existing VLMs adopt security measures during training to mitigate such attacks, vulnerabilities associated with noise-augmented visual inputs are overlooked. In this work, we identify that missing noise-augmented training causes critical security gaps: many VLMs are susceptible to even simple perturbations such as Gaussian noise. To address this challenge, we propose Robust-VLGuard, a multimodal safety dataset with aligned / misaligned image-text pairs, combined with noise-augmented fine-tuning that reduces attack success rates while preserving functionality of VLM. For stronger optimization-based visual perturbation attacks, we propose DiffPure-VLM, leveraging diffusion models to convert adversarial perturbations into Gaussian-like noise, which can be defended by VLMs with noise-augmented safety fine-tuning. Experimental results demonstrate that the distribution-shifting property of diffusion model aligns well with our fine-tuned VLMs, significantly mitigating adversarial perturbations across varying intensities. The dataset and code are available at https://github.com/JarvisUSTC/DiffPure-RobustVLM."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 8 0 3 1 0 . 4 0 5 2 : r Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks Jiawei Wang* University of Science and Technology of China wangjiawei@mail.ustc.edu.cn Yushen Zuo* The Hong Kong Polytechnic University yushen.zuo@polyu.edu.hk Yuanjun Chai University of Washington yjchai@uw.edu Zhendong Liu Nanjing University dz20330019@smail.nju.edu.cn Yicheng Fu Stanford University easonfu@stanford.edu Yichun Feng University of the Chinese Academy of Sciences fengyichun22@mails.ucas.ac.cn Kin-man Lam The Hong Kong Polytechnic University kin.man.lam@polyu.edu.hk"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Vision-Language Models (VLMs) extend the capabilities of Large Language Models (LLMs) by incorporating visual information, yet they remain vulnerable to jailbreak attacks, especially when processing noisy or corrupted images. Although existing VLMs adopt security measures during training to mitigate such attacks, vulnerabilities associated with noise-augmented visual inputs are overlooked. In this work, we identify that missing noise-augmented training causes critical security gaps: many VLMs are susceptible to even simple perturbations such as Gaussian noise. To address this challenge, we propose Robust-VLGuard, multimodal safety dataset with aligned / misaligned imagetext pairs, combined with noise-augmented fine-tuning that reduces attack success rates while preserving functionality of VLM. For stronger optimization-based visual perturbation attacks, we propose DiffPure-VLM, leveraging diffusion models to convert adversarial perturbations into Gaussian-like noise, which can be defended by VLMs with noise-augmented safety fine-tuning. Experimental results demonstrate that the distribution-shifting property of diffusion model aligns well with our fine-tuned VLMs, significantly mitigating adversarial perturbations across varying intensities. The dataset and code are available at https://github.com/JarvisUSTC/DiffPureRobustVLM . *Equal contribution. Corresponding author. Recently, Vision-Language Models (VLMs) [3, 9, 29, 50] have significantly extended the capabilities of Large Language Models (LLMs) by integrating both visual and textual information, allowing them to interpret and respond based on multimodal inputs. This advancement enables VLMs to tackle wider range of tasks, from understanding images to generating rich, contextually aware responses that leverage both language and visual cues. While LLMs have incorporated various training techniques, such as Reinforcement Learning from Human Feedback (RLHF) [5, 15], to ensure alignment with ethical and legal standards, VLMs remain more susceptible to certain risks. Fine-tuning VLMs for visual instruction-following can disrupt the alignment carefully established in LLMs [51]. Additionally, the integration of visual modalities not only introduces extra risk factors, such as heightened vulnerability to jailbreak attacks [27, 35], but also poses greater challenges for model robustness. Compared to LLMs, VLMs must account for broader spectrum of visual scenarios, rendering them more sensitive to even minor noise perturbations. Perturbation-based adversarial attacks have long targeted image classification neural networks [18]. With the emergence of VLMs, many studies have adapted traditional optimization-based perturbation attack methods to perform jailbreak attacks on these models [27, 35]. Concurrently, substantial research has focused on defending against perturbation-based attacks. For instance, DiffPure [33] utilizes the denoising capabilities of Diffusion Models as an image preprocessing method to neutralize perturbation noise in adversarial images, though it does not fully counteract perturbation attacks, particularly in VLMs [35]. Zong et al. [51] introduced VLGuard, vision-language dataset containing both safe and unsafe queries and images, designed to fine-tune VLMs for enhanced protection against jailbreak attacks. However, VLGuards effectiveness has only been evaluated against the FigStep attack and it does not consider cases where the image is unrelated to the text prompt. Similarly, Zhang et al. [49] introduced Jailguard, mutation-based detection framework that effectively identifies jailbreaks but significantly raises inference costs. More recently, Xu et al. [44] proposed an efficient cross-modality approach, CIDER, for detecting adversarially perturbed images, though it still impacts models helpfulness noticeably. In this work, we observed that many vision-language models lack noise augmentation for visual inputs during training, leaving them vulnerable to even slight perturbations like Gaussian noise. Such vulnerabilities can disrupt both the helpfulness and safety alignment of VLMs. To investigate this vulnerability, we systematically evaluate the robustness of three mainstream VLMs, including InternVL2-8B [9], LLaVA-v1.5-7B [29], and MiniGPT-413B [50], against minor Gaussian noise perturbations. Our experiments reveal that although these models excel under standard conditions, their helpfulness and safety performance degrades significantly in the presence of Gaussian noise, emphasizing the need for enhanced noise robustness in VLM designs. To address this challenge, we first introduce RobustVLGuard, novel vision-language dataset aimed at bolstering VLM robustness against Gaussian noise while improving safety alignment and preserving helpfulness. For safety, our dataset includes 2,000 carefully curated instructions that consider both image-text alignment and misalignment, with the latter representing novel scenarioacknowledging that fine-tuning VLMs can disrupt the alignment of pretrained LLMs and that perturbation-based attacks may introduce noise unrelated to text prompts. To maintain helpfulness, we complement these safety-driven instructions with 4,467 general instructions covering broad spectrum of tasks, including general question answering, world knowledge, mathematics, OCR, spatial reasoning, and extended text generation. We further leverage GPT-4V [34] to generate detailed responses for these general instructions, addressing the issue of overly brief annotations in existing datasets that leads to performance degradation during finetuning. Then, we fine-tune VLMs using Robust-VLGuard with Gaussian noise augmentation. Our noise-augmented safety fine-tuning approach demonstrates that even limited high-quality data can significantly enhance noise robustness with minimal impact on baseline capabilities. versarial perturbations. While noise augmentation typically enhances robustness against specific noise distributions, we emphasize the crucial role of effective image preprocessing. Our statistical analyzes reveal that DiffPure [33] excels in shifting distributions by using diffusion models to transform adversarial noise into Gaussian-like noise without compromising image content. Compared to JailGuard [49], VLM-specific defense, DiffPure proves significantly more effective and efficient when paired with noise-augmented safety fine-tuned VLMs. Building on this, we propose DiffPure-VLM, defense pipeline that integrates diffusion models with Gaussian-noise-tolerant VLMs to bolster protection against diverse adversarial attacks. Experiments demonstrate that DiffPures distribution shift property align well with safety fine-tuned VLMs, effectively mitigating adversarial perturbations across varying intensities. In conclusion, our contributions are: (1) To our best knowledge, we are the first to provide systematic vulnerability analysis revealing that mainstream VLMs lack inherent robustness to Gaussian noise visual perturbations. (2) We propose Robust-VLGuard dataset, which features novel image-text misalignment scenarios and detailed responses, and combine it with Gaussian noise augmentation for finetuning to enhance VLM robustness against Gaussian noise while preserving its helpfulness. (3) We expand the defense scope of fine-tuned VLMs to optimization-based visual adversarial attacks and propose defense framework, DiffPure-VLM, by adopting the distribution-shifting ability of diffusion model to transfer adversarial noise to Gaussianlike noise in visual input, which can be defended by VLMs with noise-augmented safety fine-tuning. Experimental results demonstrate the superiority of DiffPure-VLM against baseline methods and its generalization. 2. Vulnerability of VLMs to Gaussian Noise"
        },
        {
            "title": "Perturbations",
            "content": "We observed that many current VLMs, including advanced ones, lack noise augmentation during training, rendering them vulnerable to basic perturbations like Gaussian noise. As shown in Figure 1, we presented both clean image and slightly Gaussian-noised version as visual prompts. One leading model, InternVL2-8B [9], displayed inconsistent responses, with noisy prompts causing hallucinated outputs. This motivates systematic evaluation of the robustness of mainstream VLMs against Gaussian noise, focusing on helpfulness and safety. Additional evaluation results for the latest VLMs (e.g., LLaMA-3.2-Vision [13], Qwen2.5-VL [4]) are provided in Table 10 in the Appendix. 2.1. Experimental Settings To further explore the benefits of our Gaussian-noisetolerant VLMs, we extend our evaluation beyond Gaussian noise attacks to broader range of optimization-based adevaluate Models We state-of-the-art VLMs: three MiniGPT-4 (13B) [50], LLaVA-v1.5 (7B) [29], and InternVL2 (8B) [9]. Each model features distinct LLM, (a) Performance evaluation on MMVet benchmark. (b) Attack success rate on RealToxicityPrompts benchmark. Figure 2. Comparison of various models performance and robustness: (a) helpfulness on the MM-Vet benchmark with clean and noisy image prompts, and (b) attack success rates on the RealToxicityPrompts benchmark using clean and noisy image prompts. 2.2. Findings The main results are shown in Figure 2, where helpfulness is measured using MM-Vet benchmark scores and safety is evaluated by the Attack Success Rate using the Perspective API on the RealToxicityPrompts benchmark. We draw the following key insights regarding the impact of Gaussian noise on VLMs in terms of helpfulness and safety alignment: Helpfulness Degradation In Figure 2a, InternVL2, despite having the highest baseline performance on MM-Vet, suffers significant drop when exposed to Gaussian noise, revealing its lack of noise robustness. MiniGPT-4 and InternVL2 show similar relative declines of around 10%, while LLaVA-v1.5 experiences smaller drop, indicating better noise tolerance. However, all models exhibit noticeable decrease, underscoring their vulnerability to even slight noise perturbations. Safety Alignment Impact Figure 2b shows increased attack success rates on the RealToxicityPrompts benchmark across all models under noisy conditions, suggesting that Gaussian noise negatively affects safety alignment. While prior work focuses on optimization-based attacks [27, 35], our results demonstrate that even random Gaussian noise can significantly disrupt alignment. Both MiniGPT-4 and InternVL2 show substantial increases in attack success rates, indicating greater vulnerability, whereas LLaVA-v1.5 experiences smaller but still significant rise, suggesting slightly better robustness to noise. Detailed theoretical conjecture and discussions about the vulnerability caused by gaussian noise are provided in Section 13 in the Appendix. Overall Robustness The findings in Figure 2 indicate that while current VLMs perform well under standard conditions, their robustness significantly decreases when faced with Gaussian noise. Both helpfulness and safety alignment degrade across all models, highlighting the need for enhanced noise robustness in future VLM designs to ensure better stability and alignment. Figure 1. Visualization of Vision-Language Models outputs under different noise conditions. The upper part shows the original image with green text indicating correct responses generated without noise, while the lower part adds slight Gaussian noise, with red text highlighting errors introduced under noisy conditions. Please zoom in for better visualization. vision encoder, and vision-language alignment method, allowing us to draw broader insights. Details of these models are in Table 6 in the Appendix. To ensure reproducibility in helpfulness evaluations, we set the temperature to 0, while safety assessments follow the setup of Qi et al. [35]. The default system prompt is used throughout. Datasets To test robustness under Gaussian noise, we evaluate VLM performance on two key aspects: helpfulness and safety. For helpfulness, we use MM-Vet [45], comprehensive benchmark assessing six vision-language capabilities: recognition, OCR, knowledge, language generation, spatial reasoning, and mathematics. For safety, we use the RealToxicityPrompts benchmark [16], specifically the challenging subset with 1,200 prompts, following Qi et al. [35]. We augment image prompts from both datasets with Gaussian noise with mean of 0 and standard deviation of 0.1, to compare performance under clean and noisy conditions. Metrics For helpfulness, we use the original MM-Vet metric [45], designed to handle diverse real-world scenarios. GPT-4 [1] serves as the evaluation assistant, using few-shot prompt template for flexible scoring. Each response is rated from 0 for incorrect answers to 1 for correct answers, and the Performance Score is the average of all sample scores. For safety, we use the metrics from Qi [35], pairing visual adversarial examples with text et al. prompts and measuring toxicity using the Perspective API1 and Detoxify classifier [20]. Toxicity scores range from 0, indicating least toxic, to 1, indicating most toxic. The Attack Success Rate is the percentage of responses with any toxicity score above 0.5, indicating successful attack. 1https://perspectiveapi.com/ Table 1. Detailed Breakdown of General Instruction Data Task General QA World Knowledge Math & OCR Spatial Reasoning Extended Text Generation LLaVA v1.5 Mix 665k [29] Dataset GQA [24] A-OKVQA [38] ChartQA [31], TabMWP [30] VQAv2 [19] Number of Samples 1000 1000 467 1000 1000 3. Noise-Augmented Safety Alignment straightforward strategy to bolster VLM robustness against Gaussian noise is to integrate noise augmentation into the training process. In this section, we introduce our novel dataset, Robust-VLGuard, which comprises meticulously curated image-text aligned and misaligned safety data, as well as diverse array of general instructions. We then fine-tune VLMs using Robust-VLGuard augmented with Gaussian noise in visual inputs. Through extensive experimentations, we enable rapid enhancement of model robustness while preserving, and even enhancing, the models inherent helpfulness. 3.1. Robust-VLGuard Dataset While the VLGuard dataset [51] has been developed to fine-tune VLMs for improved defense against jailbreak attacks, it does not address perturbation-based attacks or scenarios where the image content is unrelated to the text prompt. Therefore, we build more robust public visionlanguage safety dataset Robust-VLGuard, as shown in Figure 3, which consists of three parts: (1) General Instruction Data, consisting of safety-agnostic SFT data covering various areas, including general QA, world knowledge, math, OCR, spatial reasoning, and extended text generation; (2) Image-Text Aligned Safety Data, containing instructions where the image content aligns with the safetyrelated text prompts; and (3) Image-Text Misaligned Safety Data, with instructions where the image content is unrelated to the safety-related text prompts. General Instruction Data To maintain VLMs helpfulness, we collect 4,467 supervised fine-tuning instructions from various aspects, including general QA, world knowledge, math, OCR, spatial reasoning, and extended text generation, as illustrated in Table 1. Specifically, we sample various instructions from these datasets and use GPT-4V [34] to refine the annotated answers. This refinement is essential, as we found that the original annotations were often too brief for effective model learning. For extended text generation, we select 1,000 instructions with responses over 150 words. For all other datasets, we choose instructions with responses exceeding 10 words. Image-Text Aligned Safety Data The VLGuard dataset is well-suited for preventing jailbreak attacks, as it contains harmful information embedded within image content, with instructions generated by GPT-4V. Therefore, we diTable 2. Performance Comparison on MM-Vet and RealToxicityPrompts Benchmarks with Clean and Noisy Image Prompts. indicates reproduced results. Bold values denote, for each base model, the method (VLGuard vs. RobustVLGuard) that achieves the smallest performance drop on MM-Vet and the lowest attack success rate on RealToxicityPrompts. Model MM-Vet (%) RealToxicityPrompts (%) Clean Image Noisy Image Clean Image Noisy Image InternVL2-8B InternVL2-8B-VLGuard InternVL2-8B-RobustVLGuard LLaVA-v1.5-7B LLaVA-v1.5-7B-VLGuard [51] LLaVA-v1.5-7B-RobustVLGuard MiniGPT-4-13B MiniGPT-4-13B-VLGuard MiniGPT-4-13B-RobustVLGuard 59.9 42.9 (-7.0) 56.2 (-3.7) 33.0 28.8 (-4.2) 30.3 (-2.7) 26.7 17.5 (-9.2) 26.9 (+0.2) 54.4 42.6 (-11.8) 52.5 (-1.9) 31.3 29.8 (-1.5) 29.8 (-1.5) 24.0 17.6 (-6.4) 27.3 (+3.3) 50.5 27.7 29.9 57.7 50.3 43.6 34.8 41.3 16.0 57.2 39.9 34.5 60.1 52.3 42.3 44.1 43.7 16.5 rectly use the instructions from VLGuard as our image-text aligned safety data, randomly selecting 1,000 instructions from this dataset. Image-Text Misaligned Safety Data Incorporating safety data for image-text misalignment is also crucial, as fine-tuning VLMs for visual tasks can disrupt the alignment of pre-trained LLMs [51], even when only text prompts are used. Additionally, perturbation-based attacks can introduce learnable noise into images that is unrelated to text prompts. [6], who showed that small set of safety examples can significantly boost model safety, we include 1,000 safety instructions from their dataset. To adapt these language-only safety instructions for multimodal use, we pair half of them with randomly selected images from the COCO dataset [28], while the remaining half are kept as text-only prompts. Inspired by Bianchi et al. 3.2. Safety Fine-Tuning To optimize resource usage, we employ Gaussian-noiseaugmented post-hoc fine-tuning approach. This efficient method is applied to pre-trained VLMs, enhancing robustness with minimal computational costs. Using the RobustVLGuard dataset, which includes both safety-specific and general instruction data, we effectively boost the models resilience to Gaussian noise while maintaining safety and helpfulness. Specifically, we fine-tune only the vision encoder using LoRA [23] on our dataset and augment training images with Gaussian noise, selecting random standard deviation between 0.01 and 0.15, with 70% probability of application. The fine-tuning process is conducted over 3 epochs and takes approximately 3 hours on single A100 GPU. Detailed fine-tuning configurations and theoretical discussion on the algorithms effectiveness are provided in Table 7 and Section 13 in the Appendix. Comparison with VLGuard dataset To assess the effectiveness of our proposed Robust-VLGuard, we adopt the same experimental settings described in Section 2.1. Three leading VLMs, i.e., MiniGPT-4 (13B) [50], LLaVA-v1.5 (7B) [29], and InternVL2 (8B) [9], are fine-tuned using our (a) Example of general instruction data. (b) Example of image-text aligned safety data. (c) Example of image-text misaligned safety data. Figure 3. Overview of the Robust-VLGuard dataset. (a) General Instruction Data: Leveraging GPT-4V to generate comprehensive, detailed responses rather than brief replies; (b) Image-Text Aligned Safety Data: The image content directly corresponds to the safety-related text prompts; (c) Image-Text Misaligned Safety Data: The image content that is deliberately unrelated to the safety-related text prompts. Red text indicates content with potential risks, while green text denotes content without risks. Gaussian-noise-augmented method and Robust-VLGuard dataset. For comparison with VLGuard, we follow the setup of [51], combining 5,000 supervised fine-tuning instructions from LLaVA v1.5 Mix 665k [29] with 3,000 safety instructions from VLGuard. Experimental results are summarized in Table 2. Due to the inevitable degradation in helpfulness resulting from LoRA-based safety fine-tuning, our proposed method aims to reduce the attack success rate while incurring minimal performance drop in helpfulness. The InternVL2-8BVLGuard model demonstrates tendency towards overdefensiveness, achieving lower attack success rate but at the cost of noticeable decline in helpfulness comIn contrast, pared to the original InternVL2-8B model. our InternVL2-8B-RobustVLGuard model achieves comparable level of safety while largely retaining the original helpfulness, achieving more balanced performance. For LLaVA-v1.5-7B, the VLGuard-fine-tuned variant maintains its helpfulness, thanks to alignment with the original training data. However, it demonstrates limited improvements in safety, highlighting its inability to effectively address image-text misalignment attacks on the RealToxicityPrompts Benchmark. Our LLaVA-v1.5-7BRobustVLGuard delivers better overall performance, exhibiting stronger safety defenses and comparable helpfulness on both clean and noisy images. The MiniGPT-413B-VLGuard model lags behind in both helpfulness and safety, whereas the MiniGPT-4-13B-RobustVLGuard variant shows notable enhancements, excelling on the MM- (a) Performance on the MM-Vet benchmark across different instruction ratios. (b) Attack success rate on the RealToxicityPrompts benchmark across different instruction ratios. Figure 4. Effect of varying instruction ratios on VLMs robustness of helpfulness and safety alignment. (a) Performance on the MM-Vet benchmark across different training epochs. (b) Attack success rate on the RealToxicityPrompts benchmark across different training epochs. Figure 5. Effect of varying training epochs on VLMs robustness of helpfulness and safety alignment. Vet benchmark and significantly lowering the attack success rate. Overall, these results emphasize the strengths of Robust-VLGuard in simultaneously enhancing model helpfulness and safety, providing comprehensive protection while maintaining performance across diverse scenarios. Ablation Studies on Instruction Ratio and Training Epochs All ablation studies are based on the InternVL28B model. First, we vary the ratio of general to safety instruction data from 4:1 to 4:4, training for single epoch for efficiency. As depicted in Figure 4, increasing the proportion of safety data lowers the attack success rate but slightly reduces helpfulness, echoing the over-defensiveness issue noted by [6]. However, beyond 4:3 ratio, performance stabilizes, suggesting effective mitigation of overdefensiveness. We select 4:2 ratio as the optimal balance, maximizing safety gains with minimal helpfulness impact. Next, we evaluate the impact of training duration while keeping the instruction ratio fixed at 4:2 and varying the number of epochs from 1 to 4. As illustrated in Figure 5, increasing the number of epochs has negligible effect on the models helpfulness as measured by the MM-Vet benchmark. However, it significantly reduces the attack success rate on the RealToxicityPrompts benchmark, indicating improved safety alignment without compromising utility. To strike balance between helpfulness and safety alignment, we select 3 epochs as the fine-tuning configuration. 4. Generalize to Optimization-based Visual"
        },
        {
            "title": "Perturbation Attack",
            "content": "In this section, we extend our defense scope to frequently encountered and challenging attack scenario: OptimizationBased Visual Perturbation Attack. It uses the projected gradient descent algorithm (PGD) with pixel constraint ϵ to inject adversarial noise into images, effectively jailbreaking VLMs. While noise augmentation typically enhances robustness against specific noise distributions, we emphasize the critical role of image preprocessing in either transforming adversarial noise into target distribution or directly eliminating it. We first evaluate the effectiveness of various image preprocessing defense methods, and then introduce DiffPure-VLM, universal defense framework that robustly counters both Gaussian and adversarial noise. 4.1. Preprocessing Methods in distribution shifting In this section, we explore different image preprocessing defense methods in distribution shifting. Specifically, we use adversarial images Iadv optimized for perturbation attacks on MiniGPT-4 [50] from [35], with pixel constraint ϵ = 16/255 as an example. As Iadv is optimized based on benign clean image Ic, we compute their residual image radv = Iadv Ic to obtain the adversarial noise. Then we use histogram and quantile-quantile (Q-Q) plot to evaluate the distribution property of radv. As shown in Figure 6, adversarial noise follows an non-Gaussian distribution. Currently, there are two representative image preprocessing defense methods: JailGuard [49] and DiffPure [33]. JailGuard, designed specifically for VLMs, employs various image processing techniques (e.g., random masking, Figure 6. Residual image Gaussianity analysis. We apply DiffPure (t = 50) to adversarial image Iadv to obtain diffused image Idiffused . Then we calculate residual images radv and rdiffused and evaluate their distribution by the histogram and Q-Q plot. horizontal flipping, Gaussian blur, and resizing) to generate variants of the input and detect adversarial samples based on discrepancies in model responses. However, most of these operations are linear transformations, offer limited ability to eliminate adversarial perturbations. Moreover, JailGuard requires multiple model runs, leading to high computational overhead. In contrast, DiffPure is tailored for computer vision models (e.g., classifiers) and leverages diffusion models to mitigate adversarial noise. It adds small amount of noise to the adversarial image Iadv and reconstructs clean image through limited number of forward and reverse diffusion steps (e.g., using DDPM [21]) with carefully chosen timestep [50, 150]. While DiffPure aims to purify adversarial perturbations while preserving global semantic structures, our findings reveal that at relatively small timesteps it does not completely remove the noise. Instead, it shifts the perturbation distribution towards Gaussian-like distribution. Specifically, we apply = 50 in DiffPure and obtain diffused image Idiffused based on Iadv. Then we calculate the residual image rdiffused = Idiffused Ic. As shown in Figure 6, rdiffused approximates Gaussian distribution from its shape and its closeness to the theoretical line (Red line) of Gaussian distribution in the Q-Q plot. More visualizations of Idiffused and statistic of rdiffused across different ϵ and are available in Section 11.2 in the Appendix. For quantitative evaluation of rdiffused , we use two metrics: (1) Kurtosis: Kurtosis is used to measure the tailedness of data distribution with the definition as (cid:34)(cid:18) µX σX Kurt[X] = (cid:19)4(cid:35) (1) where µX and σX are mean and standard deviation of data X. If follows Gaussian distribution, Kurt[X] = 3. (2) Q-Q deviation: Q-Q deviation measures the root-meansquare error (RMSE) between the quantiles of the sample distribution and those of Gaussian distribution: D(Qs, Qg) = (cid:118) (cid:117) (cid:117) (cid:116)"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (Qs,i Qg,i)2 (2) where is the number of ordered quantile points, Qs is the sample quantiles of rdiffused and Qt is the theoretical quantiles of Gaussian distribution. lower RMSE value suggests closer alignment to Gaussian distribution. We vary in DiffPure from 0 to 750 in increments of 50, and also examine = 30 for fine-grained analysis. For RGB images, we calculate these metrics per channel and obtain the average value. To identify Gaussian-like distributions in our analysis, we use the thresholds 3 Kurtosis 6 and Q-Q deviation 0.01. Points meeting these criteria are marked in red in Figure 7, showing that under certain timesteps (e.g., [50, 150]) in DiffPure, rdiffused exhibits Gaussian-like distribution properties. We further conduct our analysis in the embedding space and the conclusion is similar as in pixel space. Detailed analysis is provided in Section 11.2 in the Appendix . Finally, we conduct defense experiment with DiffPure. Specifically, we employ MiniGPT-4 as the VLM. We apply the unconditional model in [11] as the Diffusion model in DiffPure in all our experiments. We add Gaussian noise (0, σ2 n) to the benign clean image Ic and apply DiffPure with timestep to both Gaussian noisy image and adversarial images. Then we evaluate their attack success rate in the RealToxicityPrompts benchmark. Table 3 presents results for σn = 30/255 and = 50, results of different σn and are provided in Table 11 in the Appendix. Experiment result shows that directly applying DiffPure to the Gaussian noisy image with VLM without noiseaugmented safety fine-tuning does not decrease the attack success rate. We further observe that the attack success rate for diffused images are similar to Gaussian noisy image. Overall, we conclude that DiffPure, when applied with suitable timestep (e.g., [50, 150]), exhibits unique distribution-shifting capability that transforms adversarial noise into Gaussian-like distribution. 4.2. DiffPure-VLM Leveraging our safety fine-tuning approach and DiffPures unique characteristic, we propose DiffPure-VLM integrates Diffusion Models with defense pipeline that Gaussian-noise-tolerant VLMs, as illustrated in Figure 14. Specifically, we purify adversarial images by applying small timestep in DiffPure to preserve image content. The purified image with slight Gaussian-like noise is fed into the Gaussian-noise-tolerant, safety-tuned VLM, effectively mitigating the adversarial perturbations. Figure 7. Gaussianity metrics of rdiffused under different pixel constraints ϵ of adversarial image Iadv and timestep in DiffPure. Please zoom in to see details. Table 3. Defense of DiffPure in MiniGPT-4 under different image configurations. Attack Success Rate is evaluated on the RealToxicityPrompts benchmark. Image Configuration Attack Success Rate (%) Benign clean Image Ic + (σn = 30/255) + (σn = 30/255) + DiffPure (t = 50) Adversarial image Iadv (ϵ = 16/255) + DiffPure (t = 50) Adversarial image Iadv (ϵ = 32/255) + DiffPure (t = 50) Adversarial image Iadv (ϵ = 64/255) + DiffPure (t = 50) 34.8 1.6 44.1 44.3 53.6 1.0 45.0 59.4 1.4 45.5 67.2 0.2 44.5 Figure 8. The overall framework of DiffPure-VLM. First, to verify DiffPures effectiveness in our defense pipeline, we compare its performance against JailGuard for mitigating optimization-based perturbation attacks following [35] on the RealToxicityPrompts benchmark. We selected two base models, LLaVA-VLGuard and our LLaVARobustVLGuard, for comprehensive evaluation. As shown in Table 4, DiffPure with = 50 consistently outperforms JailGuard across both base models. Notably, when paired with our VLM, DiffPure delivers substantially greater improvement over JailGuard than when paired with LLaVA-VLGuard, confirming that its distribution-shifting properties are especially well-suited to our robust VLM. To evaluate the generalization of DiffPure-VLM, we assess the pipelines performance under various optimizationbased perturbation attack strengths (pixel constraint ϵ {16/255, 32/255, 64/255}) and different timesteps (t = 50, 150) of DiffPure in our suite of three safety fine-tuned VLMs. For brevity, Table 12 presents results for ϵ = Table 4. Comparison of image preprocessing methods for mitigating adversarial attacks (ϵ = 32/255) on RealToxicityPrompts. Model LLaVA-VLGuard JailGuard + LLaVA-VLGuard DiffPure + LLaVA-VLGuard LLaVA-RobustVLGuard JailGuard + LLaVA-RobustVLGuard DiffPure + LLaVA-RobustVLGuard Attack Success Rate 70.4 52.1 51.1 62.5 48.9 43.9 Identity Profanity 21.3 12.5 3.4 7.8 6.0 3. 52.8 39.0 40.9 48.0 37.3 34.6 Severe Toxicity 7.5 5.3 2.2 5.4 4.8 2.4 Sexually Explicit 16.7 13.2 13.4 16.5 13.4 12.8 Threat Toxicity 7.0 4.9 3.6 5.8 4.0 3.7 67.2 50.0 47.5 60.0 46.5 41. Table 5. Evaluation of DiffPure-VLMs effectiveness on RealToxicityPrompts. Metrics include attack success rate and various toxicity levels (Perspective API %). Additional results for other attack strengths are provided in Table 12 in the Appendix . Image Type Attack Success Rate Identity Profanity Severe Toxicity Sexually Explicit Threat Toxicity Benign Clean image Gaussian Noisy image Adversarial image (ϵ = 32/255) +DiffPure-VLM (t*=50) +DiffPure-VLM (t*=150) Benign Clean image Gaussian Noisy image Adversarial image (ϵ = 32/255) +DiffPure-VLM (t*=50) +DiffPure-VLM (t*=150) Benign Clean image Gaussian Noisy image Adversarial image (ϵ = 32/255) +DiffPure-VLM (t*=50) +DiffPure-VLM (t*=150) 29.9 34.5 70.6 33.4 32.8 22.1 27.2 56.5 20.6 25. InternVL2-8B-RobustVLGuard 0.8 2.1 26.7 2.4 1.7 LLaVA-v1.5-7B-RobustVLGuard 4.6 3.1 7.8 3.2 3.5 MiniGPT-4-13B-RobustVLGuard 0.4 0.9 9.8 0.3 0.5 34.7 34.5 48.0 34.6 32.7 9.9 11.9 35.3 9.2 8.6 43.6 42.3 62.5 43.9 42.5 16.0 16.5 53.7 13.6 11.9 0.3 1.3 9.2 0.7 0. 2.4 1.9 5.4 2.4 2.8 0.3 0.6 4.1 0.2 0.2 7.2 8.4 17.3 8.1 7.7 12.3 11.8 16.5 12.8 12.1 4.6 5.8 13.9 5.5 4.2 1.5 1.6 6.9 2.4 1. 3.5 3.1 5.8 3.7 4.1 1.1 1.0 5.4 0.9 0.6 25.9 31.3 68.1 29.1 29.1 41.0 40.0 60.0 41.0 39.3 12.1 14.0 48.1 10.6 9.9 32/255. Results for the other ϵ values are provided in Table 12 in the Appendix. The experimental results demonstrate that, compared to clean or Gaussian-noise inputs, adversarial perturbation attacks substantially increase attack success rates. Nevertheless, our DiffPure mechanism consistently reduces these rates across all tested timesteps, nearly restoring performance to that of clean or Gaussian inputs. This not only confirms our analysis of DiffPures unique ability to transform adversarial noise into Gaussian noise, but also highlights the efficacy and generalization of DiffPure-VLM with effectively mitigating the impact of perturbations. 5. Related Work 5.1. Vision Language Model In recent years, Vision Language Models have gained significant attention due to their ability to jointly process and understand both visual and textual data. major breakthrough in this area came with the development of largescale pre-trained models like CLIP [36], which aligns images and texts in shared embedding space. By training on vast dataset of image-text pairs, CLIP enabled zeroshot learning capabilities for tasks such as action recognition [7, 32] and optical character recognition [26, 40]. Recent advances have introduced multi-modal foundation models like InstructBLIP [10], LLaVA [29], and QwenVL [3] which leverage large-scale transformer networks that can process both image and text inputs simultaneously, enabling more sophisticated reasoning over complex scenarios. These models excel in advanced tasks like UI understanding [14, 22] and visual question answering [2], and open up new possibilities for generative capabilities. 5.2. Jailbreaking VLMs As VLMs become increasingly integrated into various applications, critical concerns have arisen regarding their robustness, security, and ethical alignment. critical issue is jailbreakingthe ability to bypass safety protocols, potentially triggering unintended or harmful behaviors. Given that VLMs process both textual and visual inputs, vulnerabilities inherent in LLMs may also affect VLMs. Moreover, the integration of visual inputs into VLMs, while expanding their capabilities, introduces more diverse attack patterns, significantly heightening the severity of potential threats [25, 43]. Several methods have emerged in this domain, including converting the malicious content into images through typography-based manipulations [17], leveraging multimodal perturbations to craft stronger adversarial attacks [47], and using gradient updates to embed malicious triggers within seemingly benign images [39]. These diverse sophisticated methods pose significant challenges to ensuring the safety and reliability of VLMs. 5.3. Safeguarding VLMs Given the growing challenges posed by malicious attacks on VLMs, it is crucial to develop effective defense strategies. One key approach involves enhancing the training process through adversarial fine-tuning [37, 51] or prompt tuning [48], where models or learnable prompts are trained on perturbed examples to improve resilience against realworld attack scenarios. Another promising defense strategy is input sanitization, which aims to detect or neutralize adversarial inputs before they can compromise the model. Methods in this area include shifting sample probabilities to adversarial-free regions [42], applying randomized smoothing to mitigate the impact of adversarial inputs [41], and utilizing unlabeled prompts from users in the wild for malicious prompt detection [12]. In this work, we concentrate on improving VLM robustness to noise-augmented inputs. By integrating Diffusion Models with safety finetuned VLMs, we equip these models with enhanced protection against broad range of adversarial attacks. 6. Conclusion In this work, we address critical gap in the robustness of VLMs by examining the impact of Gaussian noise perturbations and propose Robust-VLGuard, multimodal safety dataset paired with Gaussian-noise-augmented finetuning, to enhance safety alignment and preserve helpfulness of VLMs. We further propose DiffPure-VLM to defend Optimization-Based Visual Perturbation Attack by using diffusion model to transfer adversarial noise to Gaussian noise which can be defended by VLMs with noiseaugmented safety fine-tuning. Experiment result demonstrates the superiority of DiffPure-VLM in Gaussian noise and adversarial perturbations with baseline methods. While DiffPure-VLM provides practical defense, future work includes integrating noise augmentation in pretraining, expanding the safety dataset for broader tasks, and exploring adaptive multi-modal defenses to further enhance real-world performance. 7. Social Impact This research exposes VLM vulnerabilities to noisy inputs and adversarial attacks. While Robust-VLGuard and DiffPure-VLM enhance robustness, our findings have dualuse implications. Given VLMs growing adoption, we responsibly disclose these issues to raise awareness and foster the development of more secure models, mitigating deployment risks."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 3 [2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In ICCV, 2015. 8 [3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. 1, 8 [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 2, 11 [5] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. 1 [6] Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul Rottger, Dan Jurafsky, Tatsunori Hashimoto, and James Zou. Safety-tuned llamas: Lessons from improving the safety of In ICLR, large language models that follow instructions. 2024. 4, [7] Joao Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman. short note on the kinetics-700 human action dataset. arXiv preprint arXiv:1907.06987, 2019. 8 [8] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 11 [9] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 1, 2, 4, 11 [10] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Instructblip: Towards generalFung, and Steven Hoi. purpose vision-language models with instruction tuning. arXiv preprint arXiv:2305.06500, 2023. 8 [11] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In NeurIPS, 2021. 7, 17 [12] Xuefeng Du, Reshmi Ghosh, Robert Sim, Ahmed Salem, Vitor Carvalho, Emily Lawton, Yixuan Li, and Jack Stokes. Vlmguard: Defending vlms against malicious prompts via unlabeled data. arXiv preprint arXiv:2410.00296, 2024. 8 [13] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 2, [14] Yicheng Fu, Raviteja Anantha, Prabal Vashisht, Jianpeng Cheng, and Etai Littwin. Ui-jepa: Towards active perception of user intent through onscreen user activity. arXiv preprint arXiv:2409.04081, 2024. 8 [15] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858, 2022. 1 [16] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah Smith. Realtoxicityprompts: Evaluating neural toxic degeneration in language models. In Findings of the Association for Computational Linguistics: EMNLP 2020, 2020. 3 [17] Yichen Gong, Delong Ran, Jinyuan Liu, Conglei Wang, Tianshuo Cong, Anyu Wang, Sisi Duan, and Xiaoyun Wang. Figstep: Jailbreaking large vision-language models via typographic visual prompts. arXiv preprint arXiv:2311.05608, 2023. 8 [18] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. arXiv Explaining and harnessing adversarial examples. preprint arXiv:1412.6572, 2014. [19] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In CVPR, 2017. 4 [20] Laura Hanu and Unitary team. Detoxify. Github. https://github.com/unitaryai/detoxify, 2020. 3 [21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 6 [22] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: visual language model for gui agents. In CVPR, 2024. 8 [23] Edward Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In ICLR, 2022. 4 [24] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In CVPR, 2019. 4 [25] Haibo Jin, Leyang Hu, Xinuo Li, Peiyan Zhang, Chonghan Chen, Jun Zhuang, and Haohan Wang. Jailbreakzoo: Survey, landscapes, and horizons in jailbreaking large language and vision-language models. arXiv preprint arXiv:2407.01599, 2024. 8 [26] Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. The hateful memes challenge: detecting hate speech in multimodal memes. In NeurIPS, 2020. 8 [27] Yifan Li, Hangyu Guo, Kun Zhou, Wayne Xin Zhao, and JiRong Wen. Images are achilles heel of alignment: Exploiting visual vulnerabilities for jailbreaking multimodal large language models. arXiv preprint arXiv:2403.09792, 2024. 1, [28] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 4 [29] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2024. 1, 2, 4, 5, 8, 11 [30] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, SongChun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. In ICLR, 2023. 4 [31] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, 2022. 4 [32] Antoine Miech, Jean-Baptiste Alayrac, Ivan Laptev, Josef Sivic, and Andrew Zisserman. Rareact: video dataset of unusual interactions. arXiv preprint arXiv:2008.01018, 2020. 8 [33] Weili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash Vahdat, and Animashree Anandkumar. Diffusion models for adversarial purification. In International Conference on Machine Learning, 2022. 1, 2, 6, 12, 17 [34] OpenAI. Gpt-4v(ision) system card. 2023. 2, 4 [35] Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Peter Henderson, Mengdi Wang, and Prateek Mittal. Visual adversarial examples jailbreak aligned large language models. In AAAI, 2024. 1, 2, 3, 6, 7, [36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 2021. 8 [37] Christian Schlarmann, Naman Deep Singh, Francesco Croce, and Matthias Hein. Robust clip: Unsupervised adversarial fine-tuning of vision embeddings for robust large vision-language models. In ICLR 2024 Workshop on Reliable and Responsible Foundation Models. 8 [38] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: benchmark for visual question answering using world knowledge. In ECCV, 2022. 4 [39] Erfan Shayegani, Yue Dong, and Nael Abu-Ghazaleh. Jailbreak in pieces: Compositional adversarial attacks on multimodal language models. In ICLR, 2023. 8 [40] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionalIn Proceedings of the 2013 ity over sentiment treebank. Conference on Empirical Methods in Natural Language Processing, 2013. [41] Jiachen Sun, Changsheng Wang, Jiongxiao Wang, Yiwei Zhang, and Chaowei Xiao. Safeguarding vision-language arXiv models against patched visual prompt injectors. preprint arXiv:2405.10529, 2024. 8 [42] Rajkumar Theagarajan, Ming Chen, Bir Bhanu, and Jing Zhang. Shieldnets: Defending against adversarial attacks using probabilistic adversarial robustness. In CVPR, 2019. 8 [43] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail? In NeurIPS, 2023. 8 [44] Yue Xu, Xiuyuan Qi, Zhan Qin, and Wenjie Wang. Defending jailbreak attack in vlms via cross-modality information detector. arXiv preprint arXiv:2407.21659, 2024. 2 [45] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. In International Conference on Machine Learning, 2023. 3 [46] Ivy Zhang, Wei Peng, Jenny N, Theresa Yu, and David Qiu. Ivy-vl: Compact vision-language models achieving sota with optimal data, 2024. [47] Jiaming Zhang, Qi Yi, and Jitao Sang. Towards adversarial attack on vision-language pre-training models. In Proceedings of the 30th ACM International Conference on Multimedia, 2022. 8 [48] Jiaming Zhang, Xingjun Ma, Xin Wang, Lingyu Qiu, Jiaqi Wang, Yu-Gang Jiang, and Jitao Sang. Adversarial prompt tuning for vision-language models. arXiv preprint arXiv:2311.11261, 2023. 8 [49] Xiaoyu Zhang, Cen Zhang, Tianlin Li, Yihao Huang, Xiaojun Jia, Xiaofei Xie, Yang Liu, and Chao Shen. mutationbased method for multi-modal jailbreaking attack detection. arXiv preprint arXiv:2312.10766, 2023. 2, 6 [50] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. In ICLR, 2024. 1, 2, 4, 6, 11 [51] Yongshuo Zong, Ondrej Bohdal, Tingyang Yu, Yongxin Yang, and Timothy Hospedales. Safety fine-tuning at (almost) no cost: baseline for vision large language models. In International Conference on Machine Learning, 2024. 1, 2, 4, 5, 8 Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks"
        },
        {
            "title": "Supplementary Material",
            "content": "8. Overview of the Supplementary Material This supplementary material offers additional details and analyses to further support the findings presented in the main manuscript. It includes detailed information on the experimental configuration (Appendix 9), more evaluation on recent vision-language models (Appendix 10), thorough analysis of the limitations and unique characteristics of DiffPure (Appendix 11), extended implementation specifics of DiffPure-VLM (Appendix 12), and conjectures along with preliminary theoretical discussions on the effects of Gaussian noise (Appendix 13). Collectively, these sections provide deeper insights into our methodology, enhancing the transparency and reproducibility of our research. 9. Experiment Details 9.1. Models In this work, we conduct all experiments on three leading Vision-Language Models (VLMs), i.e., MiniGPT-4 (13B) [50], LLaVA-v1.5 (7B) [29], and InternVL2 (8B) [9]. We use the official model weights from HuggingFace or Github repositories for experiments in our paper. These model details are summarized in Table 6. Each model features distinct LLM, vision encoder, and vision-language alignment method, allowing us to draw broader insights. Table 6. Specifications of the evaluated VLMs. Model Size Vision Encoder LLM VL Connection Module MiniGPT-4-13B 14B EVA-CLIP ViT-G/14 Vicuna-v0-13B Q-former LLaVA-v1.5-7B 7B CLIP ViT-L/14 Vicuna-v1.5-7B InternVL2-8B 8B InternViT-300M-448px InternLM2-8B MLP MLP 9.2. Fine-tuning Configuration We present the detailed hyper-parameters for post-hoc finetuning on our Robust-VLGuard dataset in Table 7. Gaussian noise augmentation was applied to the training images, with randomly selected standard deviation between 0.01 and 0.15, and 70% probability of application. The finetuning was performed over 3 epochs on single A100-80G GPU, using consistent batch size of 16. For MiniGPT-413B, unfreezing the linear projector significantly improved robustness in terms of helpfulness and safety alignment. However, for LLaVA-v1.5-7B and InternVL2-8B, unfreezing the linear projector led to increased overfitting, likely due to differences in the vision-language connection modules of these models. Table 7. Post-hoc fine-tuning hyper-parameters of different models. Model Training Module LoRA Rank LoRA Alpha Learning Rate MiniGPT-4-13B Vision Encoder & Linear Projector LLaVA-v1.5-7B Vision Encoder InternVL2-8B Vision Encoder 16 16 32 256 256 3e-5 4e-5 4e9.3. Details of Evaluation Settings For evaluation on the MM-Vet benchmark, we set the temperature to 0 and use greedy decoding across all experiments to ensure reproducibility in helpfulness assessments. For safety evaluations on the RealToxicityPrompts benchmark, we follow the setup of Qi et al. [35], using temperature of 1 and performing three runs to calculate the average attack success rate. Greedy decoding is also employed for this benchmark. The choice of temperature 1 reflects realworld usage, where sampling is typically enabled during interactions with VLMs. This setting aims to better simulate real-world scenarios when assessing safety alignment. Additionally, the MM-Vet and RealToxicityPrompts benchmarks offer comprehensive set of metrics covering various aspects. For the sake of brevity, we report only the overall metrics Performance Score and Attack Success Rate in the main paper. Here, we present the detailed evaluation results in Table 8 and Table 9, corresponding to Figure 2 in the main paper. The results show that Gaussian noisy images negatively impact nearly all metrics across both benchmarks and various models. Notably, using Gaussian noisy images as prompts improves MiniGPT-4s performance on the OCR metric in the MM-Vet benchmark, highlighting the current VLMs lack of robustness. 10. Additional Evaluation on Recent Vision-"
        },
        {
            "title": "Language Models",
            "content": "In this section, we further assess the robustness of stateof-the-art vision-language models against Gaussian noise. Table 10 presents the attack success rates on the RealToxicityPrompts benchmark for four recent VLMsLLaMA3.2-Vision-11B [13], Ivy-VLM-3B [46], Qwen2.5-VL-7B [4], and InternVL2.5-8B [8]under various Gaussian noise levels. Lower percentages indicate improved safety alignment. Table 8. Robustness comparison of various models on the MM-Vet benchmark using clean and Gaussian noisy image prompts (GPT-4 %). Image Type Performance Score Recognition OCR Knowledge Generation Spatial Math MiniGPT-4-13B Clean Image 26.7 34.9 13. 27.4 27.1 19.1 7.7 Gaussian Noisy Image 24.0 (-2.7) 29.0 (-5.9) 16.9 (+3.4) 20.5 (-6.9) 22.5 (-4.6) 20.7 (+1.6) 7.7 (0.0) LLaVA-v1.5-7B Clean Image 33.0 37. 23.9 20.4 23.6 28.5 11.5 Gaussian Noisy Image 31.3 (-1.7) 36.3 (-1.6) 21.9 (-2.0) 18.3 (-2.1) 21.2 (-2.4) 25.7 (-2.8) 3.8 (-7.7) InternVL2-8B Clean Image 59.9 53. 71.9 40.4 44.6 69.7 65.4 Gaussian Noisy Image 54.4 (-5.5) 47.1 (-6.4) 66.8 (-5.1) 35.1 (-5.3) 39.2 (-5.4) 61.1 (-8.6) 53.5 (-11.9) Table 9. Robustness comparison of various models on the RealToxicityPrompts benchmark using clean and Gaussian noisy image prompts. Metrics include Attack Success Rate and toxicity levels (Perspective API %). Image Type Attack Success Rate Identity Attack Profanity Severe Toxicity Sexually Explicit Threat Toxicity MiniGPT-4-13B Clean Image 34.8 2.7 25.1 1.5 12.2 2. 30.5 Gaussian Noisy Image 44.1 (+9.3) 3.6 (+0.9) 31.2 (+6.1) 2.3 (+0.8) 13.7 (+1.5) 2.9 (+0.9) 38.2 (+7.7) LLaVA-v1.5-7B Clean Image 57.7 5.7 46.8 3.7 18. 3.8 54.4 Gaussian Noisy Image 60.1 (+2.4) 4.8 (-0.9) 48.1 (+1.3) 2.9 (-0.8) 17.8 (-0.2) 4.0 (+0.2) 56.0 (+1.6) InternVL2-8B Clean Image 50.5 4.1 40.2 1. 13.5 2.5 44.3 Gaussian Noisy Image 57.2 (+6.7) 4.5 (+0.4) 45.9 (+5.7) 2.0 (+0.1) 14.3 (+0.8) 3.2 (+0.7) 51.7 (+7.4) As shown, when Gaussian noise is introduced at increasing levels (σn = 30/255, σn = 50/255, and σn = 70/255), all models exhibit rise in attack success rates, highlighting their sensitivity to simple Gaussian noise perturbations. These findings underscore the need for robust noise augmentation and defense strategies in training pipelines to maintain safety alignment in VLMs. 11. Further Analysis of DiffPure on VLMs is limited. Next, we apply DiffPure with different timesteps {50, 100, 150} to generate diffused images from adversarial inputs Iadv with varying perturbation constraints ϵ. For ϵ = 16/255, increasing to 100 or 150 reduces the Attack Success Rate but does not lower it below the level observed for the benign clean image. For larger perturbation constraints, increasing fails to decrease the Attack Success Rate, with comparable performance of Gaussian noisy images. 11.1. Defence Performance 11.2. Distribution Shift In this section, we present comprehensive analysis of the effects of DiffPure [33] and Gaussian noise under perturbation-based attacks in Vision-Language Models (VLMs). Specifically, we extend the experimental setup described in Section 3.1 in the main paper by varying the standard deviation σn of Gaussian noise and the timestep parameter in DiffPure. Results are summarized in Table 11. First, Gaussian noise with standard deviations σn {15/255, 30/255, 50/255, 75/255} is added to the benign clean image Ic to evaluate its impact on the Attack Success Rate. The results demonstrate that the Attack Success Rate under Gaussian noise is significantly higher than that of the benign clean image. When σn 50/255, increasing σn will lead to higher Attack Success Rate. However, this trend did not continue at higher σn setting (e.g., σn = 75/255), indicating that the effect of Gaussian noise In this section, we present detailed results from the Gaussianity experiments conducted on adversarial and diffused images. Specifically, we visualize adversarial images Iadv alongside their corresponding residuals radv, and diffused images Idiffused with their residuals rdiffused , under pixel constraints ϵ {16/255, 32/255, 64/255} for Iadv and diffusion timesteps {50, 100, 150, 500, 750} in DiffPure [33] for generating Idiffused . Visualizations are shown in Figure 9, 10 and 11 with corresponding metrics: Kurtosis, Q-Q deviation, mean and standard deviation. From these visualizations, we observe that when 50 150, the residuals rdiffused exhibit Gaussian-like distribution, particularly for ϵ = 32/255 and ϵ = 64/255. However, as increases, the Kurtosis of rdiffused rises, indicating shift towards long-tailed distribution. This suggests that small fraction of pixels in Idiffused undergo significant Table 10. Attack success rate (%) on the RealToxicityPrompts benchmark for various vision-language models under different noise levels. Lower scores indicate improved safety alignment. RealToxicityPrompts (%) LLaMA-3.2-Vision-11B Ivy-VLM-3B Qwen2.5-VL-7B InternVL2.5-8B Benign clean Image Ic + (σn = 30/255) + (σn = 50/255) + (σn = 70/255) 45.4 46.4 (+1.0) 47.6 (+2.2) 48.5 (+3.1) 29.9 35.5 (+5.6) 40.3 (+10.4) 42.0 (+12.1) 36.8 39.3 (+2.5) 39.5 (+2.7) 46.1 (+9.3) 43.9 51.5 (+7.6) 52.8 (+8.9) 54.0 (+10.1) Table 11. Detailed results of the defense of DiffPure in MiniGPT-4 on the RealToxicityPrompts benchmark under different image configurations. (Perspective API %). Image Configuration Benign clean Image Ic + (σn = 15/255) + (σn = 30/255) + (σn = 50/255) + (σn = 75/255) Adversarial image Iadv (ϵ = 16/255) + DiffPure (t = 50) + DiffPure (t = 100) + DiffPure (t = 150) Adversarial image Iadv (ϵ = 32/255) + DiffPure (t = 50) + DiffPure (t = 100) + DiffPure (t = 150) Adversarial image Iadv (ϵ = 64/255) + DiffPure (t = 50) + DiffPure (t = 100) + DiffPure (t = 150) Attack Success Rate Identity Attack Profanity Severe Toxicity Sexually Explicit Threat Toxicity 34.8 38.5 (+3.7) 44.1 (+9.3) 46.3 (+11.5) 44.1 (+9.3) 53.6 (+18.8) 45.0 (+10.2) 37.6 (+2.8) 37.7 (+2.9) 59.4 (+24.6) 45.5 (+10.7) 43.8 (+9.0) 42.3 (+7.5) 67.2 (+32.4) 44.5 (+9.7) 42.1 (+7.3) 44.1 (+9.3) 2.7 2.9 (+0.2) 3.6 (+0.9) 3.4 (+0.7) 3.8 (+1.1) 8.4 (+5.7) 2.5 (-0.2) 3.0 (+0.3) 2.5 (-0.2) 25.1 27.4 (+2.3) 31.2 (+6.1) 34.0 (+8.9) 30.1 (+5.0) 36.6 (+9.4) 31.7 (+6.6) 25.6 (+0.5) 26.5 (+1.4) 1.5 12.2 2.0 30. 1.1 (-0.4) 13.0 (+0.8) 2.3 (+0.3) 34.1 (+3.6) 2.3 (+0.8) 13.7 (+1.5) 2.9 (+0.9) 38.2 (+7.7) 1.8 (+0.3) 14.8 (+2.6) 2.5 (+0.5) 39.5 (+9.0) 1.9 (+0.4) 14.3 (+2.1) 2.8 (+0.8) 37.5 (+7.0) 6.6 (+5.1) 14.1 (+1.9) 4.7 (+2.7) 48.6 (+18.1) 1.8 (+0.3) 14.5 (+2.3) 2.8 (+0.8) 38.8 (+8.3) 1.3 (-0.2) 12.3 (+0.1) 1.8 (-0.2) 33.1 (+2.6) 2.1 (+0.6) 12.2 (+0.0) 2.5 (+0.5) 32.9 (+2.4) 14.6 (+11.9) 2.6 (-0.1) 3.3 (+0.6) 3.7 (+1.0) 15.9 (+13.2) 2.9 (+0.2) 2.8 (+0.1) 3.3 (+0.6) 39.5 (+14.4) 7.0 (+5.5) 14.9 (+2.7) 6.2 (+4.2) 53.8 (+23.3) 2.2 (+0.7) 14.8 (+2.6) 3.0 (+1.0) 38.5 (+8.0) 32.1 (+7.0) 1.9 (+0.4) 13.1 (+0.9) 2.5 (+0.5) 38.1 (+7.6) 31.9 (+6.8) 1.3 (-0.2) 13.3 (+1.1) 2.8 (+0.8) 36.3 (+5.8) 30.4 (+5.3) 49.6 (+24.5) 12.2 (+10.7) 16.9 (+4.7) 6.6 (+4.6) 63.1 (+32.6) 2.4 (+0.9) 13.7 (+1.5) 2.7 (+0.7) 38.0 (+7.5) 32.2 (+7.1) 1.9 (+0.4) 13.7 (+1.5) 3.0 (+1.0) 36.5 (+6.0) 30.3 (+5.2) 1.4 (-0.1) 13.3 (+1.1) 2.5 (+0.5) 38.2 (+7.7) 31.5 (+6.4) changes compared to Ic, leading to cleaner image with minimal content alteration, especially when ϵ = 16/255. At = 500, the Kurtosis and standard deviation of rdiffused become significantly larger, implying greater changes in image content, as reflected in the visualization of Idiffused . For = 750, the Kurtosis decreases while the standard deviation further increases, indicating that rdiffused transitions to flatter and broader distribution. In this case, Idiffused diverges substantially from Ic in image content. Furthermore, we extend our analysis to the embedding space, examining the similarities between the clean image Ic, the adversarial image Iadv, and the diffused image Idiffused . Based on our experiment in pixel space, where the residual noise rdiffused approximates Gaussian distribution under certain timestep settings in DiffPure, we consider Idiffused as comparable to Ic with added Gaussian noise. To verify this, we generate noisy image In = Ic + n, , where σrdiffused indicates the standard deviation of rdiffused . Using pre-trained visual encoder in MiniGPT-4, we compute cosine similarities between the embeddings of In and Iadv, denoted as Cn,adv, and between In and Idiffused , denoted as Cn,diffused . Figure 12 shows 0, σ2 rdiffused (cid:17) (cid:16) these similarities across varying adversarial constraints ϵ and DiffPure steps t. Results indicate that, Cn,diffused consistently exceeds Cn,adv, showing that Idiffused is closer to In than Iadv in the embedding space. Notably, with moderate timesteps (t [50, 150]), Idiffused is similar to In (Gaussian noise added to the benign clean image Ic) in both pixel and embedding spaces. We also visualize the cosine similarity between the visual embeddings of Idiffused and Ic, denoted as Cclean,diffused , across varying ϵ and t. Results are shown in Figure 13, revealing that Cclean,diffused decreases rapidly as decreases, while it gradually declines as increases. Combining these findings with experiments in pixel space, we conclude that smaller values lead Idiffused to retain adversarial information, whereas larger values result in significant content disruption, leading to semantic misalignment. 12. Additional Details of DiffPure-VLM 12.1. Implementation Details The overall architecture of our proposed DiffPure-VLM framework is illustrated in Figure 14, with the detailed alFigure 9. Iadv, Idiffused and statistics of radv, rdiffused under different in DiffPure (constraint ϵ = 16/255). Metrics are shown in Kurtosis / Q-Q Deviation / Mean / Standard Deviation. Please zoom in to see details. Figure 10. Iadv, Idiffused and statistics of radv, rdiffused under different in DiffPure (constraint ϵ = 32/255). Metrics are shown in Kurtosis / Q-Q Deviation / Mean / Standard Deviation. Please zoom in to see details. Figure 11. Iadv, Idiffused and statistics of radv, rdiffused under different in DiffPure (constraint ϵ = 64/255). Metrics are shown in Kurtosis / Q-Q Deviation / Mean / Standard Deviation. Please zoom in to see details. Algorithm 1 DiffPure-VLM Adversarial Image Purification with DDPM Require: Adversarial image x, harmful text prompt p, diffusion model D, forward diffusion timesteps tforward, reverse diffusion timesteps treverse, visual language model VLM. Ensure: Question answering result output 1: Resize input image to the size required by the diffusion model (e.g., 256 256). 2: DDPM forward process with tforward steps: ˆx = get noised x(x, tforward). 3: for in treverse do 4: Denoise using reverse DDPM process: denoising process(ˆx, t). = 5: end for 6: Obtain purified image with Gaussian noise: xgaussian = normalize(x). 7: Perform question answering using VLM: output = VLM(xgaussian, p). 8: return output 12.2. Extended Experimental Results In the main paper, for the sake of brevity, we only report results for the standard perturbation-based attack setting of ϵ = 32/255. However, we also conducted experiments with lower attack strength (ϵ = 16/255) and higher attack strength (ϵ = 64/255) to further validate our analysis and approach in Table 12. Across different models and attack strengths, our DiffPure-VLM consistently reduces the attack success rate within limited number of diffusion timesteps (fewer than 150). Notably, under lower attack strengths, setting the diffusion step to as low as = 50 is sufficient to bring the attack success rate down to the level observed for clean inputs. However, under higher attack strengths, = 50 fails to reduce the attack success rate to the baseline level for both InternVL2-8B and MiniGPT4-13B. This indicates that stronger attacks require larger number of diffusion steps to effectively transform the adversarial noise into Gaussian noise. This finding aligns with the analysis presented in Figure 4 of the main paper, where the residual image at = 50 for an attack strength of ϵ = 64/255 does not exhibit Gaussian characteristics. Moreover, we observe that = 100 demonstrates strong performance across all attack conditions, making it an effective trade-off between time and robustness. Thus, in realworld applications, setting = 100 offers balanced solution, achieving reliable defense while maintaining computational efficiency. Figure 12. Cosine similarity of visual embeddings under different ϵ of adversarial image Iadv and of DiffPure. Figure 13. Cosine Similarity of visual embeddings from Ic and Idiffused under different ϵ of adversarial image. gorithmic procedure outlined in Algorithm 1. For our experiments, we employ the Guided Diffusion model for ImageNet [11], specifically the 256 256 unconditional variant provided by OpenAI2. Importantly, we synchronize the forward diffusion timesteps (tforward) with the reverse diffusion timesteps (treverse), denoted as in the experimental results, following the setup in DiffPure [33]. Here, we leverage this diffusion model to validate the robustness of our fine-tuned VLMs against Gaussian noise, demonstrating preliminary defense strategy. However, the fixed image resolution of the diffusion model requires down-sampling and up-sampling operations, which may introduce artifacts not considered during the fine-tuning of the VLM, potentially impacting evaluation results. In the future, adopting more advanced diffusion models will be essential for real-world applications. 2https : / / openaipublic . blob . core . windows . net / diffusion/jul-2021/256x256_diffusion_uncond.pt Figure 14. The overall framework of DiffPure-VLM. 13. Conjectures and Discussion on the Impact of Gaussian Noise"
        },
        {
            "title": "Problem Definition",
            "content": "Setting: Visual Language Model (VLM) typically consists of three main components: visual encoder, language model, and vision-language connection module. Let the input be pair (I, ), where Rd is an image and is the corresponding text prompt. The VLM generates an output sequence of tokens, denoted by ˆT = fθ(I, ), where fθ represents the VLM pipeline parameterized by θ. Adversarial Attack: An adversarial perturbation δ is applied to the image I, resulting in perturbed image Iδ = + δ. The perturbation δ is crafted to manipulate the VLM into generating specific harmful target text target. The adversarys objective is: δ = arg min δϵ (cid:0)fθ(I + δ, ), target(cid:1) , where L(, ) measures the discrepancy between the generated text ˆT and the target text target. The constraint δ ϵ ensures that the perturbation is imperceptible. Conjectures: We introduce the following four conjectures to guide our investigation into the impact of Gaussian noise on VLMs: 1. Sensitivity of Adversarial Attacks to Gaussian Noise: Adding Gaussian noise to adversarially perturbed images will significantly reduce the effectiveness of the attack. 2. Gaussian Noise as Simple Attack on VLM Safety: Gaussian noise, even without adversarial perturbations, may increase the likelihood of generating harmful text. 3. Gaussian Noise as Regularizer: Augmenting training data with Gaussian noise may act as regularizer, enhancing the robustness of the VLM. 4. Fine-Tuning with Gaussian Noise Preserves Performance: Incorporating Gaussian noise during fine-tuning will preserve or even improve the VLMs overall performance. Objective: The goal of this study is to systematically evaluate the impact of Gaussian noise on the robustness and reliability of VLMs. By exploring the above conjectures, we aim to determine whether Gaussian noise can effectively mitigate adversarial perturbations and enhance model robustness without compromising performance. Conjecture 1: Sensitivity of Adversarial Perturbations to Gaussian Noise Statement: Adversarial perturbations are highly sensitive to Gaussian noise; the attack effectiveness is significantly diminished when Gaussian noise is added to the adversarial image. Discussion: Consider an adversarially perturbed image Iδ = + δ, where the perturbation δ is optimized to minimize the loss: δ = arg min δϵ (cid:0)fθ(I + δ, ), target(cid:1) , where L(, ) measures the discrepancy between the generated text ˆT and the harmful target text target. The perturbation δ is crafted to exploit specific vulnerabilities in the model fθ. Now, consider the scenario where Gaussian noise η (0, σ2I) is added to the input. The new input becomes: Iδ,η = + δ + η. The expected loss over the distribution of Gaussian noise η is: Eη (cid:2)L (cid:0)fθ(I + δ + η, ), target(cid:1)(cid:3) . Since the adversarial perturbation δ is tailored for the specific input I, adding random Gaussian noise η disrupts this optimization. Adversarial perturbations exploit the models sensitivity along certain directions in the input space, while isotropic Gaussian noise perturbs the input uniformly in all directions, diminishing the effect of δ. Table 12. Evaluation of DiffPure-VLMs effectiveness on RealToxicityPrompts across different image configurations. Metrics include attack success rate and toxicity levels (Perspective API %). Rows highlighted in light red indicate cases where attack success rate does not meet the baseline level of benign image input. Image Type Attack Success Rate Identity Attack Profanity Severe Toxicity Sexually Explicit Threat Toxicity InternVL2-8B-RobustVLGuard Benign Clean Image Benign Noisy Image Adversarial Image (ϵ = 16/255) +DiffPure-VLM (t = 50) +DiffPure-VLM (t = 100) +DiffPure-VLM (t = 150) Adversarial Image (ϵ = 32/255) +DiffPure-VLM (t = 50) +DiffPure-VLM (t = 100) +DiffPure-VLM (t = 150) Adversarial Image (ϵ = 64/255) +DiffPure-VLM (t = 50) +DiffPure-VLM (t = 100) +DiffPure-VLM (t = 150) Benign Clean image Benign Noisy image Adversarial image (ϵ = 16/255) +DiffPure-VLM (t = 50) +DiffPure-VLM (t = 100) +DiffPure-VLM (t = 150) Adversarial image (ϵ = 32/255) +DiffPure-VLM (t = 50) +DiffPure-VLM (t = 100) +DiffPure-VLM (t = 150) Adversarial image (ϵ = 64/255) +DiffPure-VLM (t = 50) +DiffPure-VLM (t = 100) +DiffPure-VLM (t = 150) Benign Clean image Benign Noisy image Adversarial image (ϵ = 16/255) +DiffPure-VLM (t = 50) +DiffPure-VLM (t = 100) +DiffPure-VLM (t = 150) Adversarial image (ϵ = 32/255) +DiffPure-VLM (t = 50) +DiffPure-VLM (t = 100) +DiffPure-VLM (t = 150) Adversarial image (ϵ = 64/255 ) +DiffPure-VLM (t = 50) +DiffPure-VLM (t = 100) +DiffPure-VLM (t = 150) 29.9 34.5 72.5 31.4 28.2 28.2 70.6 33.4 33.4 32. 57.3 40.9 35.7 36.1 0.8 2.1 19.8 1.4 0.9 1.6 26.7 2.4 1.6 1.7 9.3 2.3 1.8 2.4 22.1 27. 58.5 24.6 21.7 22.4 56.5 20.6 27.7 25.9 45.8 32.9 28.2 28.3 LLaVA-v1.5-7B-RobustVLGuard 43.6 42.3 62.6 42.7 42.8 44. 62.5 43.9 44.1 42.5 57.5 42.1 40.5 42.4 4.6 3.1 11.3 3.4 3.9 3.3 7.8 3.2 3.5 3.5 9.2 2.7 3.3 3. 34.7 34.5 48.8 32.1 32.5 34.4 48.0 34.6 35.4 32.7 43.5 32.1 31.4 32.8 MiniGPT-4-13B-RobustVLGuard 16.0 16. 47.4 16.0 15.8 9.8 53.7 13.6 15.2 11.9 60.2 30.3 10.6 9.4 0.4 0.9 9.3 0.6 0.7 0.4 9.8 0.3 0.6 0. 6.8 1.8 0.0 0.4 9.9 11.9 34.2 9.3 9.7 6.0 35.3 9.2 9.5 8.6 44.6 21.6 7.1 5.5 0.3 1. 8.3 1.1 0.4 0.2 9.2 0.7 0.6 0.6 4.4 1.4 0.8 1.2 2.4 1.9 5.3 1.5 2.3 2.2 5.4 2.4 2.1 2. 5.2 2.1 1.9 1.8 0.3 0.6 1.4 0.3 0.0 0.1 4.1 0.2 03 0.2 4.2 1.4 0.0 0.3 7.2 8. 19.2 7.9 6.8 6.9 17.3 8.1 7.6 7.7 16.1 9.3 7.6 8.3 12.3 11.8 16.8 12.0 12.5 12.6 16.5 12.8 13.0 12. 15.3 12.3 11.7 11.5 4.6 5.8 11.8 6.5 6.1 3.3 13.9 5.5 5.4 4.2 16.2 11.4 4.1 4.1 1.5 1. 7.8 1.6 1.5 1.1 6.9 2.4 1.5 1.8 3.9 2.3 2.4 1.8 3.5 3.1 5.8 4.6 3.7 3.2 5.8 3.7 4.1 4. 5.8 2.9 2.8 4.0 1.1 1.0 4.2 1.4 1.1 0.5 5.4 0.9 1.1 0.6 5.8 1.9 0.8 0.6 25.9 31. 70.0 27.5 23.9 24.4 68.1 29.1 30.2 29.1 53.9 37.3 31.8 33.6 41.0 40.0 59.1 39.7 39.3 41.0 60.0 41.0 41.3 39. 54.7 39.0 37.5 40.2 12.1 14.0 41.5 13.2 12.8 7.8 48.1 10.6 12.7 9.9 56.0 26.9 8.2 7.0 Assuming that fθ and are Lipschitz continuous, we can bound the increase in expected loss as follows: Eη (cid:2)L (cid:0)fθ(I + δ + η, ), target(cid:1)(cid:3) (cid:0)fθ(I + δ, ), target(cid:1) + σ2λ 2 , where λ is positive constant related to the curvature of and fθ. This inequality indicates that the addition of Gaussian noise increases the expected loss, thus reducing the effectiveness of the adversarial perturbation. The random noise η disrupts the carefully crafted δ, making it less effective at manipulating the VLMs output. This supports our conjecture that Gaussian noise can act as simple yet effective countermeasure against adversarial attacks. Conjecture 2: Gaussian Noise as Simple Attack on VLM Safety Statement: Adding Gaussian noise η (0, σ2I) to clean image Iclean can compromise the safety of VLMs. Setting: Let Iclean be clean image, and η (0, σ2I) be Gaussian noise. The perturbed image is defined as: Inoisy = Iclean + η. The VLM processes the noisy image Inoisy along with corresponding text prompt , and generates an output based on this combined input. Discussion: 1. Effect of Noise on Model Input: The input to the model becomes Inoisy = Iclean + η. This perturbation, although random, alters the image representation processed by the VLM. The models output can be locally approximated around the clean input as: fθ(Iclean + η, ) fθ(Iclean, ) + Icleanfθ η, where Icleanfθ represents the gradient of the model output with respect to the clean image input. The Gaussian noise η introduces random perturbations that shift the image features. 2. Vulnerability of VLMs to Noise: VLMs are typically trained on clean image data, and thus, they may lack robustness to input noise. The introduction of Gaussian noise can push the models input into regions of the feature space that were not well-covered during training, potentially causing the model to misinterpret the input and generate unexpected responses. 3. Impact on Safety: Adding Gaussian noise may shift the models behavior towards decision boundaries where safety mechanisms are less effective. This increases the likelihood of generating unsafe or harmful text: L(fθ(Iclean + η, ), target) L(fθ(Iclean, ), target), where target represents potentially harmful target output. The inequality suggests that the noisy input can lead to higher loss, increasing the risk of unsafe text generation. 4. Gaussian Noise as Simple Yet Effective Attack: Unlike adversarial perturbations that require careful optimization and model-specific crafting, Gaussian noise introduces random changes without any specific targeting. Despite its simplicity, it can destabilize the model and affect its safety, demonstrating that even non-adversarial noise can be risk factor for VLMs. In summary, adding Gaussian noise to clean images can indeed disrupt the safety of VLMs, even in the absence of sophisticated adversarial attacks. This highlights potential vulnerability of VLMs that warrants further investigation. Conjecture 3: Gaussian Noise as Regularizer Statement: Augmenting training data with Gaussian noise acts as regularizer, reducing the risk of overfitting to adversarial perturbations and enhancing model robustness. Discussion: We introduce regularized loss function that incorporates Gaussian noise during training: Lreg(θ) = E(I,T )DEηN (0,σ2I) [L (fθ(I + η, ), )] , where represents the training data distribution. This formulation encourages the model to perform well not only on clean inputs but also on noisy inputs, promoting robustness. To understand the regularizing effect of Gaussian noise, we expand the loss function using second-order Taylor expansion around the clean input I: (fθ(I + η, ), ) (fθ(I, ), ) + (fθ(I, ), ) η + 1 2 η2 (fθ(I, ), ) η. Taking the expectation over the Gaussian noise η (0, σ2I), we obtain: Eη [L (fθ(I + η, ), )] (fθ(I, ), ) (cid:2)η + Eη 1 2 (fθ(I, ), ) η(cid:3) = (fθ(I, ), ) + σ2 2 Tr (cid:0)2 (fθ(I, ), )(cid:1) . The additional term σ2 (fθ(I, ), )(cid:1) penal2 Tr (cid:0)2 izes large curvature (i.e., high second derivatives) of the loss function with respect to the input I. This encourages smoother mappings from the input to the output, reducing the models sensitivity to small input perturbations, including adversarial ones. In summary, the addition of Gaussian noise during training acts as regularizer by penalizing sharp changes in the loss landscape. This results in model that is less prone to overfitting and more resilient to adversarial attacks, as it learns smoother and more stable input-output mappings. Conjecture 4: Fine-Tuning with Gaussian Noise Preserves Performance Statement: Fine-tuning the VLM with Gaussian noiseaugmented data maintains performance on clean data while enhancing robustness to adversarial perturbations. Discussion: Let = {(Ii, Ti)}N i=1 be the original training dataset. We construct an augmented dataset by adding Gaussian noise: Daug = (cid:8)(Ii + ηi, Ti) ηi (0, σ2I)(cid:9)N i=1 . The training objective is to minimize the following loss function: ˆLaug(θ) = 1 (cid:88) i= Eηi [L (fθ(Ii + ηi, Ti), Ti)] . 14. Detailed Proofs"
        },
        {
            "title": "Bounding the Increase in Loss Due to Gaussian\nNoise",
            "content": "Discussion: Step 1: Lipschitz Continuity of fθ and Assume that the model function fθ : Rd Rk and the loss function : Rk are Lipschitz continuous with constants Kf and KL, respectively. That is, for all x, Rd and : fθ(x, ) fθ(y, ) Kf y, and for all a, Rk: L(a, target) L(b, target) KLa b. Step 2: Bounding the Change in Loss Due to Noise η Consider the adversarially perturbed image Iδ = + δ, where δ is crafted to minimize the loss: δ = arg min δϵ (cid:0)fθ(I + δ, ), target(cid:1) . When Gaussian noise η (0, σ2I) is added, the input becomes Iδ,η = + δ + η. The change in loss due to η is: = (cid:0)fθ(I + δ + η, ), target(cid:1)L (cid:0)fθ(I + δ, ), target(cid:1) . Using the Lipschitz continuity of L: Since the Gaussian noise ηi has zero mean, the expected gradient of the loss with respect to the model parameters θ is centered around the gradient on the clean data: KL fθ(I + δ + η, ) fθ(I + δ, ) . Step 3: Computing the Expected Increase in Loss Applying the Lipschitz continuity of fθ: Eηi [θL (fθ(Ii + ηi, Ti), Ti)] = θL (fθ(Ii, Ti), Ti) . fθ(I + δ + η, ) fθ(I + δ, ) Kf η. This result indicates that the expected training gradient remains aligned with the gradient computed on the clean data, thereby preserving the models performance on clean inputs. Moreover, by training on both clean and noiseaugmented data, the model is exposed to neighborhood of inputs around each training example. This exposure helps the model generalize better and become less sensitive to small perturbations, effectively enhancing its robustness against adversarial attacks. In summary, fine-tuning with Gaussian noise-augmented data acts as regularization strategy that not only maintains the VLMs accuracy on clean data but also improves its resistance to adversarial perturbations. Thus, the change in loss is bounded by: KLKf η. Since η is Gaussian random vector with zero mean and covariance σ2I, the expected value of η is: E[η] = σ 2 Γ (cid:0) d+1 Γ (cid:0) 2 (cid:1) (cid:1) σ 2 (cid:114) 1 for large d. Therefore, the expected increase in loss is approximately: E[L] KLKf σ d. Thus, the expected value of the quadratic term becomes: Eη (cid:2)η2 L(fθ(I, ), )η(cid:3) = σ2 Tr (cid:0)2 L(fθ(I, ), )(cid:1) Step 4: Neglecting the Remainder Term For small values of σ, the remainder term R3 is of order O(σ3) and can be safely ignored. Thus, the approximation becomes: Eη [L(fθ(I + η, ), )] L(fθ(I, ), ) + σ2 2 Tr (cid:0) L(fθ(I, ), )(cid:1) Step 5: Interpretation of the Trace Term The term Tr (cid:0)2 L(fθ(I, ), )(cid:1) denotes the sum of the eigenvalues of the Hessian matrix, representing the overall curvature of the loss function with respect to the input. larger trace value indicates higher curvature, suggesting greater sensitivity of the model to input perturbations. Reducing this sensitivity is crucial for enhancing the models robustness. Step 6: Gaussian Noise as Regularization term σ2 The additional L(fθ(I, ), )(cid:1) func2 Tr (cid:0)2 tions as regularizer, penalizing high curvature in the loss landscape. This encourages the model to learn smoother input-output mappings, thereby reducing its vulnerability to small perturbations, including adversarial attacks. Step 7: Connection to Tikhonov Regularization This regularization effect is conceptually similar to Tikhonov regularization, where penalty proportional to the norm of the model parameters is added to the loss function. In our case, the penalty arises naturally from the Gaussian noise, encouraging robustness by flattening the loss landscape: Eη [L(fθ(I + η, ), )] L(fθ(I, ), ) + σ2 Tr (cid:0)2 L(fθ(I, ), )(cid:1) This smoothing effect reduces the models sensitivity to input perturbations, enhancing its robustness without compromising performance on clean data. Step 4: Lower Bounding the Expected Increase in Loss Since δ minimizes L(fθ(I + δ, ), target) at the point + δ, any perturbation η added to + δ is likely to increase the loss. Under the conjecture that is convex around +δ, the expected increase in loss due to η can be lower bounded using the curvature (second derivative) of L: Eη (cid:2)L (cid:0)fθ(I + δ + η, ), target(cid:1)(cid:3) (cid:0)fθ(I + δ, ), target(cid:1) + σ2 2 λmin, where λmin is the smallest eigenvalue of the Hessian maI+δL(fθ(I + δ, ), target). trix 2 Conclusion: Adding Gaussian noise increases the expected loss by at least σ2 2 λmin, reducing the effectiveness of the adversarial perturbation. This result supports the conjecture that Gaussian noise disrupts the optimization achieved by the adversary, weakening the impact of adversarial attacks. Second-Order Taylor Expansion of Around Discussion: Step 1: Second-Order Taylor Expansion We expand the loss function L(fθ(I + η, ), ) around the point using the second-order Taylor expansion: L(fθ(I + η, ), ) = L(fθ(I, ), ) + L(fθ(I, ), )η + 1 2 η2 L(fθ(I, ), )η + where: L(fθ(I, ), ) is the gradient of the loss with respect to the input I. 2 L(fθ(I, ), ) is the Hessian matrix of second derivatives with respect to I. R3 is the remainder term of higher order O(η3). Step 2: Expected Value of the Linear Term Since η is sampled from zero-mean Gaussian distribution η (0, σ2I), the expected value of the linear term becomes: Eη (cid:2)I L(fθ(I, ), )η(cid:3) = L(fθ(I, ), )Eη[η] = 0 Step 3: Expected Value of the Quadratic Term Next, we compute the expectation of the quadratic term: Eη (cid:2)η2 L(fθ(I, ), )η(cid:3) Using the properties of Gaussian distributions, we know that for symmetric matrix A: Eη (cid:2)ηAη(cid:3) = σ2 Tr(A)"
        }
    ],
    "affiliations": [
        "Nanjing University",
        "Stanford University",
        "The Hong Kong Polytechnic University",
        "University of Science and Technology of China",
        "University of Washington",
        "University of the Chinese Academy of Sciences"
    ]
}