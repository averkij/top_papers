{
    "paper_title": "Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation",
    "authors": [
        "Hai Zhang",
        "Siqi Liang",
        "Li Chen",
        "Yuxian Li",
        "Yukuan Xu",
        "Yichao Zhong",
        "Fu Zhang",
        "Hongyang Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Why must vision-language navigation be bound to detailed and verbose language instructions? While such details ease decision-making, they fundamentally contradict the goal for navigation in the real-world. Ideally, agents should possess the autonomy to navigate in unknown environments guided solely by simple and high-level intents. Realizing this ambition introduces a formidable challenge: Beyond-the-View Navigation (BVN), where agents must locate distant, unseen targets without dense and step-by-step guidance. Existing large language model (LLM)-based methods, though adept at following dense instructions, often suffer from short-sighted behaviors due to their reliance on short-horimzon supervision. Simply extending the supervision horizon, however, destabilizes LLM training. In this work, we identify that video generation models inherently benefit from long-horizon supervision to align with language instructions, rendering them uniquely suitable for BVN tasks. Capitalizing on this insight, we propose introducing the video generation model into this field for the first time. Yet, the prohibitive latency for generating videos spanning tens of seconds makes real-world deployment impractical. To bridge this gap, we propose SparseVideoNav, achieving sub-second trajectory inference guided by a generated sparse future spanning a 20-second horizon. This yields a remarkable 27x speed-up compared to the unoptimized counterpart. Extensive real-world zero-shot experiments demonstrate that SparseVideoNav achieves 2.5x the success rate of state-of-the-art LLM baselines on BVN tasks and marks the first realization of such capability in challenging night scenes."
        },
        {
            "title": "Start",
            "content": "Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation Hai Zhang* Siqi Liang* Li Chen Yuxian Li Yukuan Xu Yichao Zhong Fu Zhang Hongyang Li The University of Hong Kong https://github.com/OpenDriveLab/SparseVideoNav 6 2 0 2 5 ] . [ 1 7 2 8 5 0 . 2 0 6 2 : r Fig. 1: In this work, we investigate the beyond-the-view navigation task in the real world, where agents must locate distant, unseen targets without step-by-step guidance. Traditional large language model-based methods suffer from short-horizon supervision, leading to short-sighted behaviors, e.g., unexpected turning and dead-end trapping. We address this challenge from new perspective, by introducing the video generation model to this field for the first time. The whole training pipeline is sparsified further for the sake of extended prediction horizon and computational efficiency. AbstractWhy must vision-language navigation be bound to detailed and verbose language instructions? While such details ease decision-making, they fundamentally contradict the goal for navigation in the real-world. Ideally, agents should possess the autonomy to navigate in unknown environments guided solely by simple and high-level intents. Realizing this ambition introduces formidable challenge: Beyond-the-View Navigation (BVN), where agents must locate distant, unseen targets without dense and stepby-step guidance. Existing large language model (LLM)-based methods, though adept at following dense instructions, often suffer from short-sighted behaviors due to their reliance on shorthorizon supervision. Simply extending the supervision horizon, however, destabilizes LLM training. In this work, we identify that video generation models inherently benefit from long-horizon supervision to align with language instructions, rendering them uniquely suitable for BVN tasks. Capitalizing on this insight, we propose introducing the video generation model into this field for the first time. Yet, the prohibitive latency for generating videos spanning tens of seconds makes real-world deployment Equal contribution. impractical. To bridge this gap, we propose SparseVideoNav, achieving sub-second trajectory inference guided by generated sparse future spanning 20-second horizon. This yields remarkable 27 speed-up compared to the unoptimized counterpart. Extensive real-world zero-shot experiments demonstrate that SparseVideoNav achieves 2.5 the success rate of state-of-theart LLM baselines on BVN tasks and marks the first realization of such capability in challenging night scenes. I. INTRODUCTION Vision-language navigation (VLN) empowers agents to perform complex tasks by grounding natural language instructions into sequential actions based on visual observations [18]. Recently, the advent of large-language models (LLMs) has catalyzed significant breakthroughs in this field [46, 47, 40]. However, fundamental tension remains: while real-world interactions typically demand navigation based on simple and high-level intents, current agents often require dense and step-by-step instructions [7, 39], paradigm typically termed Instruction-Following Navigation (IFN). This clear discrepancy highlights formidable but less-explored frontier: Beyond-the-View Navigation (BVN), where agents must autonomously locate distant, unseen targets in the absence of such granular and intermediate guidance. The primary bottleneck in BVN for existing LLM-based methods is their inherent short sight. These models are typically supervised by short-horizon action sequences (e.g., 4 to 8 steps) during training [39, 46, 33]. Consequently, they struggle to infer correct navigational intents, typically causing two failure modes during deployment. First, the inability to observe the target over long distance induces severe uncertainty, resulting in unexpected turning behaviors or spinning in place. Second, upon entering dead end, the agent mistakenly assumes the path end, leading to dead-end trapping. While extending the supervision horizon seems like logical fix, it often destabilizes the training process of LLM [38], rendering it non-viable solution. In this work, we pivot toward video generation models (VGMs) based on key observation: unlike LLMs, VGMs are inherently pre-trained to capture long-horizon future aligned with the language instruction [36, 41]. Recognizing this strength, we identify VGMs as well-suited interface to provide the long-horizon foresight that BVN demands. Nevertheless, whether we should strictly adhere to the standard VGM paradigm of predicting continuous videos [14, 33] remains question worthy of scrutiny. We contend that the dense and high-frequency temporal information required by continuity is redundant for guiding navigation. This insight drives us to explore sparse video generation paradigm. By doing so, we aim to achieve the dual advantage of extending the prediction horizon while simultaneously reducing both training and inference overhead. To this end, we reformulate the training objective by employing sparse video frames corresponding to strategically selected timesteps as the direct supervision signal for VGM. Translating this sparse foresight into comprehensive navigation system, yet, presents two primary challenges. Primarily, the computational overhead of injecting the whole history, coupled with the extensive denoising steps required to generate dynamic scenes of navigation tasks, still leads to significant inference latency that makes real-world deployment impractical. In addition, unlike LLM-based methods that can bridge the sim-to-real gap by co-training with heterogeneous realworld data [46, 39], VGM lacks straightforward mechanism to leverage such data sources. To bridge this gap, we introduce SparseVideoNav, Sparse Video generation-based system for vision-language Navigation. For the efficiency bottleneck, we instantiate SparseVideoNav via structured four-stage training pipeline. For the data challenge, we curate large-scale navigation dataset spanning 140 hours across diverse real-world environments. Through zero-shot evaluations in six real-world scenes, we demonstrate that SparseVideoNav achieves state-of-theart (SOTA) performance on BVN tasks, marking the first realization of such capability in challenging night scenes. Contributions: 1) We investigate beyond-the-view navigation tasks in the real world from new perspective by introducing video generation model to this field for the first time. 2) We advance beyond the conventional constraint of continuous video generation in existing methods by pioneering paradigm shift towards sparse video generation. This reformulation liberates the capability to reason over substantially longer prediction horizon while achieving 1.4 training speedup and 1.7 inference speed-up. 3) SparseVideoNav achieves sub-second trajectory inference guided by generated sparse future spanning 20-second horizon. This yields remarkable 27 speed-up compared to the unoptimized counterpart. Extensive real-world zeroshot experiments demonstrate SparseVideoNav adeptly handles beyond-the-view tasks even in complex terrains like dead ends, narrow accessible ramp and hillside with high inclination angles, achieving 2.5 the success rate of SOTA LLM baselines in beyond-the-view scenarios, marking the first realization of such capability in challenging night scenes with 17.5% success rate. II. RELATED WORK A. Foundation Models for Vision-Language Navigation To better optimize spatio-temporal The integration of foundation models into VLN predominantly transitions from modular, training-free approaches to end-to-end fine-tuning approaches. Training-free approaches leverage the zero-shot reasoning capabilities of off-the-shelf LLMs to perform specific sub-tasks, such as task reasoning [22], stage scheduling [5], frontier detection [11], and object validation [45]. While these modular approaches maintain interpretability, they inherently suffer from cascading error propagation [45] or limited generalization capability [44, 6]. information throughout the training pipeline, fine-tuning approaches have gained inputs increasing traction, which aim to map multimodal directly to actions in an end-to-end manner [46, 48]. Recent SOTA methods have demonstrated strong generalization through large-scale training. Notably, Zhang et al. [46] propose Uni-Navid to establish competitive baseline by unifying diverse navigation tasks with single versatile policy. StreamVLN [39] introduces streaming framework that manages continuous video inputs via hybrid slow-fast context modeling to accelerate the inference. Despite their promise, these methods predominantly require dense and step-by-step instructions, dependency that contradicts with practical realworld demands. When facing beyond-the-view tasks guided they are prone to falter intents, by simple and high-level due to the vulnerability induced by short-horizon supervision. Consequently, there is pressing need for novel paradigm designed to bridge this gap. B. Video Generation Models for Embodied Agents Distinguished from text-centric pretraining, VGM implicitly encodes dynamic variation between frames, leading to smaller domain gap towards downstream actions. This unique Fig. 2: Architecture and four-stage training pipeline of SparseVideoNav. (Top) denotes our whole training architecture. Current observation, historical observations, and the language instruction are fed into the video generation model (VGM) backbone to generate future sparse video latents. DiT-based action head then predicts continuous actions conditioned on generated sparse future and the language instruction. (Bottom) denotes our four-stage training pipeline, with Stage 1 (Sec. III-C) adapting T2V to I2V, Stage 2 (Sec. III-D) injecting history into I2V backbone; Stage 3 (Sec. III-E) distilling the backbone to reduce denoising steps; Stage 4 (Sec. III-F) learning actions based on generated sparse future. Components not utilized in specific stage are indicated by gray blocks. property has driven an explosion of interest across wide spectrum of embodied tasks from robotic manipulation [14, 9, 2] to autonomous driving [10, 43, 49]. Within the specialized scope of navigation, InternVLA-N1 [33] demonstrates that pretraining with video generation objectives can improve downstream policy performance. Nevertheless, these endeavors adopt default prerequsite to generate continuous videos, ignoring the immense potential of sparse video generation for extending the prediction horizon. into the training of VGM to enable fixed-interval sparse video generation. As shown in Fig. 3, setting the interval to 3 strikes the best balance between the prediction horizon and the visual fidelity. To further ensure action prediction accuracy, we maintain continuous generation for the first two observation chunks, covering 8 timesteps. This design leads to the sparse generation timesteps at [T + 1, + 2, + 5, + 8, + 11, + 14, + 17, + 20], covering 20s at 4 FPS, where denotes the current timestep. III. METHODOLOGY The design of SparseVideoNav is guided by two fundamental pillars: training paradigm-level innovation through sparsification (Sec. III-A), and system-level innovation via synergistic orchestration. To achieve this, we develop data curation pipeline (Sec. III-B) alongside structured four-stage training pipeline. The linchpin of the training pipeline is the sparse future video, which serves as the supervision across stages 1, 2, and 3, and the conditional input of stage 4. The whole architecture and training pipeline are shown in Fig. 2, where Former block denotes Q-Former [19] and VideoFormer [14] used to compress history. A. Sparsification To extend the prediction horizon and cover the distant beyond-the-view target, we bring in sparse video supervision B. Data Curation Pipeline Different from methods built upon video-based LLMs [46, 47], which can mitigate the sim-to-real gap by co-training on pure simulation navigation data [26] mixed with real-world VQA datasets, we cannot effectively leverage this strategy. Relying exclusively on simulation data typically leads to mode collapse [32, 34] due to the substantial domain gap. Furthermore, existing real-world navigation datasets are often characterized by severe fisheye distortion [27, 13] and limited scale [12], making them unsuitable for fine-tuning VGM. To address these challenges, we develop data curation pipeline involving human operators equipped with handheld camera to capture diverse videos. To minimize human jitter that blocks VGM to learn consistent dynamics, we explicitly utilize DJI Osmo Action 4 with RockSteady+ stabilization. Wan for fine-tuning. During training, given chunk latent at an arbitrary timestep cT , the following sparse chunk latents x1 = [cT +1, cT +2, cT +5, cT +8, ..., cT +20], random noise x0 (0, I), and timestep [0, 1] sampled from logitnormal distribution, an intermediate latent xt is obtained as the training input. xt is defined as linear interpolation between x0 and x1: xt = tx1 + (1 t)x0. The ground-truth velocity vt is defined as: vt = dxt dt = x1 x0. (1) (2) The loss function is formulated as the mean square error between the model output and the velocity vt: Lstage1 = Ex0,x1,l,cT ,tu(xt, l, cT , t; θ) vt2, (3) where is the language embedding of umT5 [8], θ is the model weights, and u(xt, l, cT , t; θ) denotes the predicted velocity. D. Stage 2: History Injection critical distinction between navigation foundation models and previous vision-language-action models lies in the necessity of incorporating the entire history of observations [16, 4, 15]. Unlike LLMs that can directly absorb long sequence of image tokens, VGMs lack this capability to process this input [36, 41]. We draw inspiration from CDiT [1] architecture to inject the history information with efficient training and inference computational overhead. Specifically, we introduce an additional cross-attention block within each transformer block of Wan backbone to explicitly inject history information. To preserve the generative priors of the fine-tuned I2V model, we initialize the final linear layer of these newly added crossattention blocks with zeros. Nevertheless, the whole history input presents another challenge due to its excessive length and high dimensionality. To efficiently extract spatio-temporal features, we employ twostep strategy that utilizes Q-Former [19] to process features along the temporal dimension and subsequently applies Video-Former [14] for features along the spatial dimension. We denote the history embedding processed by Q-Former and Video-Former at an arbitrary timestep as hT , the training objective is formulated as: Lstage2 = Ex0,x1,l,cT ,hT ,tu(xt, l, cT , hT , t; θ) vt2. (4) E. Stage 3: Diffusion Distillation Unlike manipulation tasks, which involve limited visual changes [42, 16, 28] and allow high-fidelity reconstruction with few denoising steps [14, 3], navigation tasks involve highly dynamic scene transitions [46, 47]. Consequently, generating high-fidelity future frames in navigation scenarios via few-step denoising is inherently difficult, posing significant challenges for real-world deployment. While video generationbased methods have been explored in autonomous driving [10, 49], they still suffer from high inference latency, requiring tens of seconds to minutes for generation. Fig. 3: Qualitative comparison of different sparse intervals. With the sparse interval of 3, the model successfully imagines path towards beyond-the-view target, while maintaining visual fidelity. We collect 140 hours of real-world navigation videos. These videos are processed into approximately 13,000 trajectories via uniform temporal sampling, with an average length of 140 frames at 4 FPS. Subsequently, we estimate camera poses using Depth Anything 3 (DA3) [20] to extract continuous action labels. Detailed visualizations concerning action label extraction are deferred in Sec. B. Finally, language instructions are manually annotated by human experts. This pipeline establishes the largest real-world VLN dataset to date. We will release this dataset to benefit the community. C. Stage 1: T2V I2V To achieve balance between computational overhead and video generation fidelity, we adopt Wan2.1-1.3B T2V [36] model as our backbone. Wan utilizes 3D causal VAE [17] structure to compress the spatio-temporal dimensions. Specifically, given an input video R(1+T )HW 3, WanVAE encodes it into latent chunks R1+T /4,H/8,W/8,16, resulting in (1+T /4) chunks where the shape of each chunk is [H/8, W/8, 16]. By default, all subsequent training processes are performed at the chunk level. Note that the T2V model is designed to generate the future mainly based on language instructions rather than visual inputs. Hence, the first stage involves adapting the backbone from T2V to image-to-video (I2V) to ensure the consistency of the generated future with the initial observation. We retain the original flow matching [21] objective of To address this efficiency bottleneck, we adapt PCM [37] to flow-matching paradigm for distilling history-injected I2V model. In this setup, we designate the history-injected I2V model as the teacher model and initialize an architecturally identical student model with the same weights. We then partition the noise schedule into 4 phases, where the student model learns to predict the solution point of each phase on the probability flow ODE [30, 31] trajectory of the teacher model [37]. By minimizing the consistency loss between adjacent timesteps, we progressively distill the inference steps from = 50 to = 4. F. Stage 4: Action Learning Drawing inspiration from VPP [14], we freeze the distilled I2V model and employ an inverse dynamics paradigm to predict continuous actions. Specifically, the generated sparse future and the language instruction are injected into the DiT-based action head via cross-attention. Nevertheless, we observe clear visual discrepancy between the generated and the original ground-truth future frames, which leads to misalignment between the synthesized dynamics and the original action labels. To combat this inconsistency, we employ DA3 to relabel the generated future frames, thereby ensuring the action supervision is precisely aligned. (cid:113) We adopt DDIM [29] to reconstruct the relabeled actions a0 from noised action ak = 1 βkϵ, where ϵ denotes βka0+ white noise and βk is the noisy coefficient at timestep k. The training process can be formulated as learning denoised Dψ to approximate the noise: (cid:113) Laction(ψ; A) = Ea0,ϵ,kDψ(ak, l, ) a02, (5) where denotes the generated sparse future observation. IV. EVALUATIONS To validate the efficacy of SparseVideoNav, we design our experiments to investigate: 1) Instruction Following Competency. Can SparseVideoNav, as framework grounded in video generation, exhibit foundational competency in following language instructions? (See Sec. IV-B) 2) Efficacy in Beyond-the-View Navigation. Does the sparse foresight successfully translate to superior performance in challenging beyond-the-view scenarios? (See Sec. IV-B and Sec. IV-C) 3) Efficiency and Trade-offs. Does our architectural design strikes good balance between efficiency and performance? (See Sec. IV-C) 4) Scalability, Adaptability and Robustness. Can SparseVideoNav demonstrate clear scalability with increased data, adaptability to dynamic obstacles, and robustness to camera height variations? (See Sec. IV-D) A. Experimental Setup Evaluation Protocols. To assess the navigation capabilities in real-world environments, we select six diverse unseen scenes across three categories: Indoors (Room, Lab Building), Outdoors (Yard, Park), and Night (Square, Mountain) to test the zero-shot generalization capability. For each scene, we design four distinct navigation tasks, consisting of two standard instruction-following navigation (IFN) tasks and two challenging beyond-the-view navigation (BVN) tasks. Detailed specifications for all the tasks are deferred in Sec. D. To ensure statistical reliability, each model is tested 10 times per task, resulting in total of 240 trials per model. We report the Success Rate averaged across task types for each scene. To demonstrate the superiority of the VGM paradigm, we select three competitive LLM-based models for comparison: UniNavid [46] is video-based LLM model for unifying diverse vision-language navigation tasks. StreamVLN [39] is designed to accelerate the inference process through KV Cache. InternVLA-N1 [33] is the first dual-system LLM model for vision-language navigation. We observe that LLM-based baselines often stop with lateral orientation towards the target, whereas our model typically stops facing the target directly. To ensure fair comparison, we define success solely based on proximity, where trial is considered successful if the agent stops within 1.5 meters of the target. To minimize the influence of time and weather, we conduct comparative tests for all models within the same time window to ensure consistent environmental conditions. Hardware Deployment. We perform real-world experiments based on Unitree Go2 robotic dog. The robot is equipped with an upward facing camera DJI Osmo Action 4 for stabilized RGB observations. However, given that InternVLA-N1 [33] requires depth input, we adhere to its original configuration by employing an Intel RealSense D455 camera, mounted with 15 downward pitch to acquire RGB-D observations. The camera is secured to the back of Go2 using custom 3Dprinted bracket, positioned at height of approximately 1m above the ground. This mounting height is standardized across all evaluated methods. We deploy SparseVideoNav and all the baselines on remote workstation with an RTX 4090 GPU. The Go2 robot continuously sends visual observations to the server. Upon receiving the images, the server processes them and generates corresponding navigation action commands, which are then sent back to Go2 for execution. B. Main Results As shown in Table I, SparseVideoNav achieves unanimously SOTA zero-shot performance across all real-world scenes on both IFN and BVN tasks. Compared to the strongest baseline StreamVLN, our method achieves significant performance improvement, with the average success rate by +15.0% on IFN and +15.0% on challenging BVN. This substantial gap underscores the effectiveness of our video generation paradigm in handling real-world navigation demands. Notably, the diminished visibility in challenging night environments exacerbates the short-sighted limitation, precipitating systematic failure across all established baselines on BVN tasks. In contrast, by leveraging robust long-horizon guidance, SparseVideoNav TABLE I: Quantitative results of zero-shot performance on diverse real-world scenes. SparseVideoNav achieves unanimously SOTA performance across all real-world scenes on both instruction-following navigation (IFN) and beyondthe-view navigation (BVN) tasks. The highest success rate across SparseVideoNav and all baselines is highlighted in bold. (Numbers represent success rate in %) Method Baselines Uni-NaVid [41] StreamVLN [34] InternVLA-N1 [28] SparseVideoNav Ablation Study (a) distilled 4 steps cont. 2 (b) distilled 4 steps cont. 10 (c) w/o distilled 50 steps cont. 20 (d) SparseVideoNav w/o Former Indoors Outdoors Night Average Room Lab Bldg Yard Park Square Mountain Total IFN BVN IFN BVN IFN BVN IFN BVN IFN BVN IFN BVN IFN BVN 20 45 25 55 25 50 70 40 5 10 5 0 10 35 15 10 40 15 55 20 40 65 60 0 15 0 0 5 45 25 10 30 35 50 10 35 55 40 0 5 5 0 5 30 20 20 50 30 65 35 45 75 60 10 30 40 15 40 60 45 0 20 0 50 5 30 75 45 0 0 0 0 5 20 20 0 25 0 25 0 20 35 25 0 0 0 0 5 25 10 10.0 35.0 17.5 50.0 15.8 36.7 62.5 45.0 2.5 10.0 8.3 25. 2.5 11.7 35.8 22.5 Fig. 4: Qualitative results of zero-shot beyond-the-view navigation in challenging, unstructured environments. SparseVideoNav successfully navigates through challenging scenarios, including dead ends, narrow accessible ramp, and hillside with high inclination angles. stands as the sole method capable of navigating to these distant goals in such extreme environments. Qualitative Results. To further showcase the superiority of SparseVideoNav in BVN, we visualize several trajectories in Fig. 4. SparseVideoNav can successfully navigate through extremely challenging scenarios, including dead ends, narrow accessible ramp, and hillside with high inclination angles. Analysis. To better understand why SparseVideoNav excels in BVN tasks, we present model prediction results (See Fig. 5) during deployment and several representative failure cases (See Fig. 1). As LLM-based baselines rely on shorthorizon supervision, they suffer from inherent short-sighted limitations, leading to unexpected turning under long-range uncertainty and premature trapping in dead ends. Conversely, by integrating this guidance with closed-loop feedback, SparseVideoNav can effectively mitigate this limitation. Fig. 5: Analysis of video generation results of SparseVideoNav during zero-shot deployment in beyond-the-view navigation. Fig. 6: Ablation study on a) data scalibility and computational overhead comparison over b) sparse design, c) distillation, and d) history compression. C. Ablation Study Data Scalability. We conduct the whole training process except Stage 4 on varying data scales: 8h, 50h, and 140h, to with additional 3h of unseen data as validation set compute FVD [35]. As shown in Fig. 6(a), the consistent downward trend of the FVD curve highlights the scalability of SparseVideoNav, demonstrating the capability to absorb largescale real-world navigation data. Sparse Video Generation. To demonstrate the advantage of our proposed sparse design, we devise the following variants for comparative analysis: Fig. 7: Ablation study on the effectiveness of diffusion distillation (Stage 3). Our distillation strategy enables the model to achieve visual fidelity with only 4 denoising steps (top row) comparable to the original model using 50 steps (bottom row). a) distilled 4 steps cont. 2 denotes distilled 4-step variant for generating 2 continuous chunks. Variant (a) is specifically designed to emulate the effects of short-sighted limitations typically observed in LLM baselines. b) distilled 4 steps cont. 10 denotes distilled 4-step variant for generating 10 continuous chunks. c) w/o distilled 50 steps cont. 20 denotes undistilled 50-step variant for generating 20 continuous chunks, where variant (c) can be seen as an oracle compared to SparseVideoNav. d) SparseVideoNav w/o Former denotes our method without Q-Former and Video-Former. As shown in the Ablation Study of Table I, constrained by the same short-horizon supervision issue as LLM baselines, variant (a) yields suboptimal performance. While extending the horizon in variant (b) offers partial mitigation, distinct performance disparity remains compared to SparseVideoNav. In further comparison with variant (c), SparseVideoNav demonstrates substantial efficiency gains, specifically 1.7 increase in inference speed (See Fig. 6(b)) and 1.4 reduction in the cumulative convergence time across training stages 1 and 2 (See Fig. 1). Despite slight performance compromise, these results represent favorable trade-off between efficiency and effectiveness. Diffusion Distillation. As illustrated in Fig. 6(c), the inclusion of distillation yields substantial computational advantage, accelerating inference by approximately 10 with slight performance compromise (See Ablation Study of Table variant (c)). Crucially, as evidenced by Fig. 7, distillation enables the model to match the visual fidelity of the original 50-step inference with only 4 denoising steps. History Compression Strategy. Fig. 6(d) presents the inference latency across varying history lengths. Former structure decouples latency from history length, ensuring stable inference latency. In contrast to the variant without Former, which incurs +54.9% latency penalty at = 45, SparseVideoNav achieves superior efficiency and slightly better performance (See Ablation Study of Table variant (d)). Pretraining from T2V to I2V. To validate the necessity of Stage 1, we benchmark the training time required for convergence under two variants: a) our proposed progressive adaptation; b) training Stage 2 directly from scratch. Experiments are conducted on cluster of 32 NVIDIA H200 GPUs. The results show that variant (b) requires 64 hours to reach convergence, whereas our progressive adaptation strategy converges in 32 hours. This yields 2 training speedup with similar performance, demonstrating that Stage 1 serves as an efficient initialization to significantly reduce the overall computational overhead. D. Further Discussion Dynamic Pedestrians Avoidance. Given that DA3 [20] struggles with reliable action estimation in the presence of frontal dynamic pedestrians, we filter out such trajectories to preserve the accuracy of the actions. Remarkably, despite this exclusion, SparseVideoNav exhibits an emergent capability to dynamically avoid pedestrians during deployment, underscoring its strong adaptability and generalization, as shown in Fig. 8. Camera Height Insensitivity. We further observe that while LLM-based paradigm is notably susceptible to camera height shifts, video generation-based paradigm maintains robustness Fig. 8: Discussion of SparseVideoNav under dynamic pedestrian interference. SparseVideoNav successfully avoids the oncoming pedestrian and reaches the door. Fig. 9: Discussion of SparseVideoNav with the camera fixed at 50cm. Images in green boxes demonstrate the predictions generated by SparseVideoNav. SparseVideoNav exhibits strong robustness against camera height variations. against these discrepancies. Despite being trained on data collected at an approximate height of 1m, SparseVideoNav successfully performs navigation with fixed camera height of 50cm, as shown in Fig. 9. V. CONCLUSION AND LIMITATION In this work, we address the formidable challenge of realworld beyond-the-view navigation by introducing SparseVideoNav, the first sparse video generation-based navigation system. We shift the paradigm from LLM-based short-horizon action sequences to long-horizon sparse foresight, effectively overcoming short-sightedness in current methods. By strategically supervising the model with sparse future, SparseVideoNav achieves significantly extended prediction horizon while intensively reducing computational overhead. Extensive zero-shot evaluations in diverse and challenging real-world environments demonstrate that SparseVideoNav outperforms SOTA LLM baselines by large margin. Our work paves new way for leveraging video generation models to achieve efficient and robust embodied intelligence. Notwithstanding the promising capabilities exhibited by the video generation paradigm, we discuss certain limitations of our current approach: 1) Our curated 140-hour data scale is not yet exhaustive compared to the web-scale [1, 24]. We view scaling up data as critical avenue for further improvement. 2) Regarding inference latency, despite our extensive optimizations to enable real-world deployment, our speed still lags slightly behind existing LLM-based navigation paradigms [39]. We believe that exploring accelerated distillation and quantization techniques for VGM represents promising direction for future research."
        },
        {
            "title": "ACKNOWLEDGMENT",
            "content": "This study is supported by National Natural Science Foundation of China (62206172). This work is in part supported by the JC STEM Lab of Autonomous Intelligent Systems funded by The Hong Kong Jockey Club Charities Trust. We are grateful to Jiazhi Yang and Di Zhang for their valuable discussions, Haoguang Mai, Longyang Wu, Hongchen Li, Shuo Diao, Haolin Ou, Ronghao Li, and members from OpenDriveLab for their assistance throughout this project. We aslo acknowledges DJI for funding support during the project."
        },
        {
            "title": "REFERENCES",
            "content": "[1] Amir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, and Yann LeCun. Navigation World Models. In CVPR, 2025. 4, 8 [2] Qingwen Bu, Jia Zeng, Li Chen, Yanchao Yang, Guyue Zhou, Junchi Yan, Ping Luo, Heming Cui, Yi Ma, and Hongyang Li. Closed-Loop Visuomotor Control with Generative Expectation for Robotic Manipulation. In NeurIPS, 2024. 3 [3] Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Xindong He, Xu Huang, et al. AgiBot World Colosseo: Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems. In IROS, 2025. 4 [4] Qingwen Bu, Yanting Yang, Jisong Cai, Shenyuan Gao, Guanghui Ren, Maoqing Yao, Ping Luo, and Hongyang Li. UniVLA: Learning to Act Anywhere with Taskcentric Latent Actions. In RSS, 2025. 4 [5] Yihan Cao, Jiazhao Zhang, Zhinan Yu, Shuzhen Liu, Zheng Qin, Qin Zou, Bo Du, and Kai Xu. CogNav: Cognitive Process Modeling for Object Goal Navigation with LLMs. In ICCV, 2025. 2 [6] Li Chen, Chonghao Sima, Kashyap Chitta, Antonio Loquercio, Ping Luo, Yi Ma, and Hongyang Li. Intelligent Robot Manipulation Requires Self-Directed Learning. Authorea Preprints, 2025. [7] An-Chieh Cheng, Yandong Ji, Zhaojing Yang, Xueyan Zou, Jan Kautz, Erdem Biyik, Hongxu Yin, Sifei Liu, and Xiaolong Wang. NaVILA: Legged Robot VisionLanguage-Action Model for Navigation. In RSS, 2025. 1 [8] Hyung Won Chung, Xavier Garcia, Adam Roberts, Yi Tay, Orhan Firat, Sharan Narang, and Noah Constant. UniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining. In ICLR, 2023. 4 [9] Xiao Fu, Xintao Wang, Xian Liu, Jianhong Bai, Runsen Xu, Pengfei Wan, Di Zhang, and Dahua Lin. Learning Video Generation for Robotic Manipulation with Collaborative Trajectory Control. arXiv preprint arXiv:2506.01943, 2025. 3 [10] Ruiyuan Gao, Kai Chen, Bo Xiao, Lanqing Hong, Zhenguo Li, and Qiang Xu. MagicDriveDiT: High-resolution Long Video Generation for Autonomous Driving with Adaptive Control. 2024. 3, 4 arXiv preprint arXiv:2411.13807, [11] Zeying Gong, Rong Li, Tianshuai Hu, Ronghe Qiu, Lingdong Kong, Lingfeng Zhang, Yiyi Ding, Leying Zhang, and Junwei Liang. Stairway to Success: Zero-Shot FloorAware Object-Goal Navigation via LLM-Driven Coarseto-Fine Exploration. arXiv preprint arXiv:2505.23019, 2025. 2 [12] Noriaki Hirose, Amir Sadeghian, Marynel Vazquez, Patrick Goebel, and Silvio Savarese. GoNet: SemiSupervised Deep Learning Approach for Traversability Estimation. In IROS, 2018. 3 [13] Noriaki Hirose, Dhruv Shah, Ajay Sridhar, and Sergey Levine. Sacson: Scalable Autonomous Control for Social Navigation. IEEE RA-L, 2023. 3 [14] Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, and Jianyu Chen. Video Prediction Policy: Generalist Robot Policy with Predictive Visual Representations. In ICML, 2025. 2, 3, 4, 5, 12 [15] Haoran Jiang, Jin Chen, Qingwen Bu, Li Chen, Modi Shi, Yanjie Zhang, Delong Li, Chuanzhe Suo, Chuang Wang, Zhihui Peng, and Hongyang Li. WholeBodyVLA: Towards Unified Latent VLA for Whole-Body LocoManipulation Control. In ICLR, 2026. 4 [16] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. OpenVLA: An Open-Source Vision-Language-Action Model. In CoRL, 2024. [17] Diederik Kingma and Max Welling. Auto-Encoding Variational Bayes. In ICLR, 2014. 4 [18] Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, and Jason Baldridge. Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense Spatiotemporal Grounding. In EMNLP, 2020. 1 [19] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping Language-Image Pretraining with Frozen Image Encoders and Large Language Models. In ICML, 2023. 3, 4, 12 [20] Haotong Lin, Sili Chen, Jun Hao Liew, Donny Y. Chen, Zhenyu Li, Guang Shi, Jiashi Feng, and Bingyi Kang. Depth Anything 3: Recovering the Visual Space from Any Views. arXiv preprint arXiv:2511.10647, 2025. 4, 8, 11 [21] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow Matching for Generative Modeling. In ICLR, 2023. [22] Yuxing Long, Wenzhe Cai, Hongcheng Wang, Guanqi Zhan, and Hao Dong. InstructNav: Zero-shot System for Generic Instruction Navigation in Unexplored Environment. In CoRL, 2024. 2 [23] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. In ICLR, 2019. 11 [24] Yanghong Mei, Yirong Yang, Longteng Guo, Qunbo Wang, Ming-Ming Yu, Xingjian He, Wenjun Wu, and Jing Liu. UrbanNav: Learning Language-Guided Urban In Navigation from Web-Scale Human Trajectories. AAAI, 2026. 8 [25] William Peebles and Saining Xie. Scalable Diffusion Models with Transformers. In ICCV, 2023. 12 [26] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: Platform for Embodied AI Research. In ICCV, 2019. 3 [27] Dhruv Shah, Benjamin Eysenbach, Nicholas Rhinehart, and Sergey Levine. Rapid Exploration for Open-World Navigation with Latent Goal Models. In CoRL, 2021. 3 [28] Modi Shi, Li Chen, Jin Chen, Yuxiang Lu, Chiming Liu, Guanghui Ren, Ping Luo, Di Huang, Maoqing Yao, and Hongyang Li. Is Diversity All You Need for Scalable Robotic Manipulation? arXiv preprint arXiv:2507.06219, 2025. 4 [29] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising Diffusion Implicit Models. In ICLR, 2021. 5 [30] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. ScoreBased Generative Modeling through Stochastic Differential Equations. In ICLR, 2021. 5 [31] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency Models. In ICML, 2023. 5 [32] Akash Srivastava, Lazar Valkov, Chris Russell, Michael Gutmann, and Charles Sutton. VEEGAN: Reducing Mode Collapse in Gans Using Implicit Variational Learning. In NeurIPS, 2017. 3 [33] InternVLA-N1 Team. InternVLA-N1: An Open DualSystem Navigation Foundation Model with Learned Latent Plans, 2025. URL https://internrobotics.github.io/ internvla-n1.github.io/static/pdfs/InternVLA N1.pdf. 2, 3, 5, 11 [34] Hoang Thanh-Tung and Truyen Tran. Catastrophic Forgetting and Mode Collapse in GANs. In IJCNN, 2020. 3 [35] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. FVD: New Metric for Video Generation, 2019. URL https://openreview.net/forum?id=rylgEULtdN. 7 [36] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and Advanced Large-Scale Video Generative Models. arXiv preprint arXiv:2503.20314, 2025. 2, 4, 12 [37] Fu-Yun Wang, Zhaoyang Huang, Alexander Bergman, Dazhong Shen, Peng Gao, Michael Lingelbach, Keqiang Sun, Weikang Bian, Guanglu Song, Yu Liu, et al. Phased Consistency Models. In NeurIPS, 2024. 5 [38] Shaoan Wang, Jiazhao Zhang, Minghan Li, Jiahang Liu, Anqi Li, Kui Wu, Fangwei Zhong, Junzhi Yu, Zhizheng Zhang, and He Wang. TrackVLA: Embodied Visual Tracking in the Wild. arXiv preprint arXiv:2505.23189, 2025. [39] Meng Wei, Chenyang Wan, Xiqian Yu, Tai Wang, Yuqiang Yang, Xiaohan Mao, Chenming Zhu, Wenzhe Cai, Hanqing Wang, Yilun Chen, et al. StreamVLN: Streaming Vision-and-Language Navigation via Slowfast arXiv preprint arXiv:2507.05240, Context Modeling. 2025. 1, 2, 5, 8 [40] Meng Wei, Chenyang Wan, Jiaqi Peng, Xiqian Yu, Yuqiang Yang, Delin Feng, Wenzhe Cai, Chenming Zhu, Tai Wang, Jiangmiao Pang, et al. Ground Slow, Move Fast: Dual-System Foundation Model for GeneralizIn ICLR, 2026. able Vision-and-Language Navigation. 1 [41] Bing Wu, Chang Zou, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Jack Peng, Jianbing Wu, Jiangfeng Xiong, Jie Jiang, et al. Hunyuanvideo 1.5 Technical Report. arXiv preprint arXiv:2511.18870, 2025. 2, 4 [42] Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, and Tao Kong. Unleashing Large-Scale Video Generative In ICLR, Pre-training for Visual Robot Manipulation. 2024. 4 [43] Jiazhi Yang, Kashyap Chitta, Shenyuan Gao, Long Chen, Yuqian Shao, Xiaosong Jia, Hongyang Li, Andreas Geiger, Xiangyu Yue, and Li Chen. ReSim: Reliable World Simulation for Autonomous Driving. In NeurIPS, 2025. 3 [44] Naoki Yokoyama, Sehoon Ha, Dhruv Batra, Jiuguang Wang, and Bernadette Bucher. VLFM: Vision-Language Frontier Maps for Zero-Shot Semantic Navigation. In ICRA, 2024. 2 [45] Bangguo Yu, Yuzhen Liu, Lei Han, Hamidreza Kasaei, Tingguang Li, and Ming Cao. VLN-Game: VisionLanguage Equilibrium Search for Zero-Shot Semantic Navigation. arXiv preprint arXiv:2411.11609, 2024. [46] Jiazhao Zhang, Kunyu Wang, Shaoan Wang, Minghan Li, Haoran Liu, Songlin Wei, Zhongyuan Wang, Zhizheng Zhang, and He Wang. Uni-NaVid: Video-based Vision-Language-Action Model for Unifying Embodied Navigation Tasks. In RSS, 2025. 1, 2, 3, 4, 5 [47] Jiazhao Zhang, Anqi Li, Yunpeng Qi, Minghan Li, Jiahang Liu, Shaoan Wang, Haoran Liu, Gengze Zhou, Yuze Wu, Xingxing Li, et al. Embodied Navigation Foundation Model. In ICLR, 2026. 1, 3, 4 [48] Zhang, Hao, Xu, Zhang, Zhang, Wang, Zhang, Wang, Zhang, and MapNav Xu. Novel Memory Representation via Annotated Semantic Maps for VLM-Based Vision-and-Language Navigation. In ACL, 2025. 2 [49] Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Xinze Chen, Guan Huang, Xiaoyi Bao, and Xingang Wang. DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation. In AAAI, 2025. 3,"
        },
        {
            "title": "APPENDIX",
            "content": "In the Appendix, we first compile motivating questions in Sec. A. Task specifications details in Sec. are presented to supplement Sec. IV-A in the main paper. We then provide additional elaborations on data curation details and implementation details in Sec. and Sec. C, respectively. The Appendix concludes with Sec. E, which details the ethics statement and licenses. A. Motivating Questions Q1: Compared to the pixel-goal guidance in InternVLA-N1 [33], what is the advantage of sparse video guidance? The point-goal generated by language models are fundamentally confined to the current view. Consequently, it often fails to assist the agent in escaping when trapped in dead ends. In contrast, video generation models can leverage long-horizon video guidance to imagine backtracking trajectory to exit dead ends, thereby mitigating this perspective limitation and leading to more robust solutions for BVN tasks. Q2: Despite the SOTA results achieved by SparseVideoNav, what factors currently constrain the success rate? Similar to prevalent video generation methods, we observe that SparseVideoNav would exhibits mode collapse in highly challenging scenarios, leading to navigation failures (See Fig. A-1). Our empirical findings indicate that scaling up the training data correlates with lower FVD (See Sec. IV-C Data Scalability). Given that our current 140-hour real-world dataset is much smaller than web-scale, we envision combining large-scale data from diverse sources, such as YouTube videos and simulation trajectories in the future. Fig. A-1: Potential mode collapse of SparseVideoNav in challenging scenarios. B. Data Curation Details As illustrated in Figure A-2, our pipeline efficiently transforms raw video streams into high-quality training image-action pairs. The process begins with temporal sampling, where raw videos are decimated to 4 FPS to reduce redundancy. We then utilize Depth Anything 3 (DA3) [20] to estimate the 6-DoF camera extrinsics for each frame, reconstructing the 3D trajectory of the agent. Finally, action extraction is performed by calculating the relative pose transformations between frames. These 3D motions are projected onto the local XY plane to derive continuous action labels (x, y, θ), while static segments and views with extreme pitch angles are filtered out to ensure high-quality selection. Fig. A-2: Data Curation Pipeline. The workflow consists of three stages: (1) temporal sampling from raw video; (2) pose estimation via Depth Anything 3; and (3) extrinsic-based action extraction. C. Implementation Details Training Configurations. Our model is trained using the AdamW optimizer [23] on cluster of 32 NVIDIA H200 GPUs, with the full four-stage pipeline requiring approximately 64 hours to complete. All training hyper-parameters of each stage is presented in Table A-I. TABLE A-I: Hyperparameter configurations for the four-stage training pipeline. Stage Stage 1 Stage 2 Stage 3 Stage 4 Batch Learning Size Rate LR Scheduler Trainable Params Sampling Strategy Discrete Steps 32 32 32 256 1 105 1 105 5 106 5"
        },
        {
            "title": "Constant\nConstant\nConstant\nCosine",
            "content": "1.3B 1.8B 1.7B 23.4M FM (Uniform) FM (Uniform) PCM (Distill) DDIM 1000 1000 50 100 TABLE A-II: Hyper-parameters of Q-Former and Video-Former used for history compression. Q-Former Configuration Video-Former Configuration Hidden Dim Layers Heads Num. Latents 512 4 8 10240 Hidden Dim Layers Heads Num. Latents 512 6 8 2560 TABLE A-III: Hyper-parameters of the inverse dynamics-based action head, including the Action Video-Former and the Diffusion Transformer. Action Video-Former Diffusion Transformer Hidden Dim Layers Heads Num. Latents 256 8 8 640 Hidden Dim Layers Heads Num. Actions 256 12 8 8 Network Architecture Configurations. Our system integrates three core components: video generation backbone, history compression module, and an action prediction head. For the video backbone (Stage 1), we directly adopt the Wan2.1 T2V-1.3B [36] architecture. To handle high-dimensional historical context, we implement two-stage compression strategy: Q-Former [19] first reduces temporal redundancy, followed by Video-Former [14] that performs 4 spatial downsampling. This reduces the token burden to 2,560 while preserving essential context. We present the detailed hyperparameters of Q-Former and Video-Former in Table A-II. For navigation control, we design an inverse dynamics-based action head that couples feature-aggregating Video-Former with Diffusion Transformer (DiT) [25]. Video-Former encodes spatiotemporal features from 640 sparse future latent tokens. These features then serve as the cross-attention context for the DiT to predict continuous 8-step action trajectories. Detailed parameters for these modules are summarized in Table A-III. D. Task Specifications and Details Fig. A-3 and Fig. A-4 provide the complete specifications for all 24 zero-shot evaluation tasks across six real-world scenes. Each scene contains 2 instruction-following navigation (IFN) tasks and 2 beyond-the-view navigation (BVN) tasks. E. Data Ethics and License Privacy Protection and De-identification. To strictly protect individual privacy, we implement robust and irreversible deidentification processing pipeline. All captured video frames are processed by human expert to identify all sensitive information. All sensitive information, including human faces and vehicle identifiers, is permanently blurred before being used for model training or stored in our servers. We ensure that no identifiable personal information is present in the final dataset. Data Collection Ethics. Our data collection involves human operators recording in diverse public environments. All operators are briefed on ethical guidelines, ensuring that they follow local regulations and avoid recording in restricted or private areas without explicit authorization. Data collection is performed using handheld DJI Osmo Action 4 in real-world environments. This manual approach ensures data diversity and procedural transparency throughout the recording process. License. We build upon Wan2.1 [36], which is under Apache License 2.0. The code and dataset for SparseVideoNav would be open-sourced under CC BY-NC-SA 4.0 License. Fig. A-3: Task specifications. We present the language instructions, the initial positions of robot dog and the task settings for each scene. When the precise locations can not be clearly seen, we utilize green arrow icon to denote the initial positions and location pin to mark the destination for visualization clarity. Fig. A-4: Task specifications. We present the language instructions, the initial positions of robot dog and the task settings for each scene."
        }
    ],
    "affiliations": [
        "The University of Hong Kong"
    ]
}