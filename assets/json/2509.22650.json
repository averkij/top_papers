{
    "paper_title": "RefAM: Attention Magnets for Zero-Shot Referral Segmentation",
    "authors": [
        "Anna Kukleva",
        "Enis Simsar",
        "Alessio Tonioni",
        "Muhammad Ferjad Naeem",
        "Federico Tombari",
        "Jan Eric Lenssen",
        "Bernt Schiele"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Most existing approaches to referring segmentation achieve strong performance only through fine-tuning or by composing multiple pre-trained models, often at the cost of additional training and architectural modifications. Meanwhile, large-scale generative diffusion models encode rich semantic information, making them attractive as general-purpose feature extractors. In this work, we introduce a new method that directly exploits features, attention scores, from diffusion transformers for downstream tasks, requiring neither architectural modifications nor additional training. To systematically evaluate these features, we extend benchmarks with vision-language grounding tasks spanning both images and videos. Our key insight is that stop words act as attention magnets: they accumulate surplus attention and can be filtered to reduce noise. Moreover, we identify global attention sinks (GAS) emerging in deeper layers and show that they can be safely suppressed or redirected onto auxiliary tokens, leading to sharper and more accurate grounding maps. We further propose an attention redistribution strategy, where appended stop words partition background activations into smaller clusters, yielding sharper and more localized heatmaps. Building on these findings, we develop RefAM, a simple training-free grounding framework that combines cross-attention maps, GAS handling, and redistribution. Across zero-shot referring image and video segmentation benchmarks, our approach consistently outperforms prior methods, establishing a new state of the art without fine-tuning or additional components."
        },
        {
            "title": "Start",
            "content": "REFAM: ATTENTION MAGNETS FOR ZERO-SHOT REFERRAL SEGMENTATION Anna Kukleva1, Enis Simsar2, Alessio Tonioni3, Muhammad Ferjad Naeem3, Federico Tombari3,4, Jan Eric Lenssen1, Bernt Schiele1 1Max Planck Institute for Informatics, SIC 2ETH Zurich https://refam-diffusion.github.io/ 4TU Munich 3Google 5 2 0 2 6 ] . [ 1 0 5 6 2 2 . 9 0 5 2 : r Figure 1: Global Attention Sinks (GAS) in DiT. We highlight tokens (here, tokens #1 and #16) that act as GAS in late layers. These tokens allocate disproportionately high and nearly uniform attention across all text and image tokens simultaneously. GAS are absent in early layers, emerge consistently in deeper blocks, and serve as indicators of semantic structure. While uninformative themselves, they can suppress useful signals when they occur on meaningful tokens."
        },
        {
            "title": "ABSTRACT",
            "content": "Most existing approaches to referring segmentation achieve strong performance only through fine-tuning or by composing multiple pre-trained models, often at the cost of additional training and architectural modifications. Meanwhile, largescale generative diffusion models encode rich semantic information, making them attractive as general-purpose feature extractors. In this work, we introduce new method that directly exploits features, attention scores, from diffusion transformers for downstream tasks, requiring neither architectural modifications nor additional training. To systematically evaluate these features, we extend benchmarks with vision-language grounding tasks spanning both images and videos. Our key insight is that stop words act as attention magnets: they accumulate surplus attention and can be filtered to reduce noise. Moreover, we identify global attention sinks (GAS) emerging in deeper layers and show that they can be safely suppressed or redirected onto auxiliary tokens, leading to sharper and more accurate grounding maps. We further propose an attention redistribution strategy, where appended stop words partition background activations into smaller clusters, yielding sharper and more localized heatmaps. Building on these findings, we develop REFAM, simple training-free grounding framework that combines cross-attention maps, GAS handling, and redistribution. Across zero-shot referring image and video segmentation benchmarks, our approach consistently outperforms prior methods, establishing new state of the art without fine-tuning or additional components. Equal contribution 1 Figure 2: Pipeline overview. We first extract cross-attention maps for the referring expression with attention magnets. Next, we filter out stop words and attention magnets, aggregate the remaining maps, identify the argmax location, and apply SAM to generate the final segmentation mask."
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion transformers (DiTs) have rapidly advanced generative modeling and, more recently, been adopted as powerful feature extractors for downstream visionlanguage tasks such referring object segmentation (Ni et al., 2023). Their cross-attention maps encode rich spatial and semantic information without task-specific training, making them attractive for training-free and zero-shot applications. However, attention in transformers is also known to exhibit emergent behaviors that are not always semantically meaningful. In large language models, for instance, certain tokens, often first tokens, attract disproportionately high attention while carrying little to no semantic content, phenomenon referred to as attention sinks or massive activations (Xiao et al., 2024; Yona et al., 2025; Jin et al., 2025; Sun et al., 2024a; Barbero et al., 2025). In this paper, we extend this observation to generative diffusion transformers and show that they exhibit similar attention sink behaviors when applied to visionlanguage grounding tasks. Specifically, we uncover languagevision attention sinks, where stop words emerge as high-attention tokens despite lacking semantic value. We find two distinct patterns. First, small set of stop words consistently act as global attention sinks (GASs) in the later layers of DiTs: they attend almost uniformly across text and image tokens, and filtering their channels does not harm downstream performance. Second, other stop words behave as local background attractors, drawing attention toward irrelevant regions. Surprisingly, appending additional stop words introduces more such attractors, which redistributes background attention and yields cleaner heatmaps. We further show that replacing stop words with random vectors also improves results, but real stop words are consistently more effective, likely due to their repeated presence during pretraining. These findings suggest that stop words can serve as simple yet effective tool for attention redistribution. Building on this, we propose REFAM, training-free grounding method, that augments referring expressions with stop words, filters their attention maps, and aggregates the remaining cross-attention for grounding. This approach requires no modifications to the diffusion model, no additional supervision, and generalizes to both image and video tasks. In summary, our contributions are threefold: We identify and analyze global attention sinks (GASs) with respect to both language and visual tokens in DiTs, linking their emergence to semantic structure and showing that they carry no useful signal for grounding. We introduce stop-word based attention redistribution strategy, where auxiliary stop words act as magnets that absorb surplus background attention, yielding cleaner and more localized cross-attention maps. We achieve state-of-the-art zero-shot referring segmentation on both image and video benchmarks using REFAM, features from diffusion transformers, outperforming prior training-free methods without fine-tuning or auxiliary components."
        },
        {
            "title": "2 RELATED WORK",
            "content": "High-Norm Tokens Across Transformer Architectures. Recent research has identified tokens exhibiting high-norm activations across various domains, including language models (Xiao et al., 2024; Yona et al., 2025; Jin et al., 2025; Sun et al., 2024a; Barbero et al., 2025), vision models (Kang et al., 2025; Darcet et al., 2024; Jiang et al., 2025; Wang et al., 2024), and vision-language models (An et al., 2025; Woo et al., 2025). In language models, these tokens are referred to as attention sinks (Xiao et al., 2024; Yona et al., 2025; Barbero et al., 2025) or massive activations (Jin et al., 2025; Sun et al., 2024a). In vision models, similar phenomena are termed registers (Darcet et al., 2024; Jiang et al., 2025), visual attention sinks (Kang et al., 2025), or defective path tokens (Wang et al., 2024). In vision-language models, this phenomenon has been described as attention deficiency (An et al., 2025) or blind tokens (Woo et al., 2025), reporting individual visual tokens that consistently receive disproportionately high attention. These studies consistently show that small fraction of tokens absorb disproportionately high attention, often without semantic relevance. In our work, we identify global attention sinks (GAS), tokens that span both language and visual streams under the same query and systematically suppress useful signals in both modalities, simultaneously. common explanation attributes this behavior to the softmax normalization constraint: attention weights must sum to one, even when the query lacks strong contextual match (Xiao et al., 2024). In such cases, attention is distributed toward tokens that act as sinks. Other studies offer complementary views: Jin et al.(Jin et al., 2025) point to positional encodings; Sun et al.(Sun et al., 2024a) implicate learned bias terms; and Wang et al. (Wang et al., 2024) connect the effect to the power method, where repeated matrix multiplications amplify dominant directions in feature space. While many approaches aim to mitigate this behavior, we instead draw on it: our method introduces attention magnets, tokens, e.g., stop words, that deliberately absorb surplus attention to help redistribute focus more effectively in generative models. Referring object segmentation. Referring object segmentation aims to localize region in an image or video based on natural language expression. We address both static and temporal settings. (1) Referring Image Object Segmentation (RIOS). Traditional methods are supervised (Kazemzadeh et al., 2014; Ding et al., 2020; Feng et al., 2021; Li et al., 2018), but recent zero-shot approaches leverage pre-trained models. Global-Local (Yu et al., 2023) extracts CLIP-based features from mask proposals, and Ref-Diff (Ni et al., 2023) uses diffusion priors. HybridGL (Liu & Li, 2025) fuses global-local context with spatial cues. These methods first generate multiple object proposals and then score masks by their similarity to text embeddings. (2) Referring Video Object Segmentation (RVOS). Temporal methods like LoSh (Yuan et al., 2024) and WRVOS (Zhao et al., 2023) require supervision. In contrast, recent zero-shot methods such AL-Ref-SAM (Ren et al., 2024) use pre-trained image grounding GroundedSAM (Ren et al., 2024) and SAM2 (Ravi et al., 2025) models for frame-wise grounding with minimal adaptation. Unlike prior approaches, our method unifies both tasks under single framework using diffusion-derived features, operating in training-free, zero-shot setting."
        },
        {
            "title": "3 SEMANTIC FEATURES FROM DIFFUSION TRANSFORMERS",
            "content": "We describe how semantic features are extracted from rectified-flow diffusion transformers (DiTs) and how stop words, acting as attention magnets, enable robust referral segmentation. Our method consists of three components: (i) extracting cross-attention maps from DiTs, (ii) identifying and filtering attention sinks, and (iii) redistributing surplus attention through additional tokens (Figure 2). 3.1 FEATURE EXTRACTION FROM DITS Rectified flow models (Labs, 2024; Team, 2024) combine an image-to-latent encoderdecoder with DiT (Peebles & Xie, 2023) backbone. The encoder compresses inputs into latent space, while the DiT performs denoising through sequence of transformer blocks. Architectures may interleave double-stream blocks, which process text and visual tokens separately before merging in attention, and single-stream blocks, which operate on concatenated tokens with shared weights. Given clean latent X0, the rectified flow forward process perturbs it as Xt = (1 σ)X0 + σϵ, ϵ (0, I). 3 Figure 3: Emergence of semantic information in DiT. Top: text-to-text attention across layers. Early layers (019) are diffuse and uniform, while middle and late layers (2047) develop blockdiagonal structure, indicating meaningful linguistic grouping. Bottom: text-to-image attention for the patches token. Early layers spread attention broadly over the scene, whereas middle layers begin to localize, and late layers sharpen around the target object. These dynamics illustrate how semantic alignment emerges progressively with depth. While the DiT is trained to predict the noise ϵ, its intermediate activations capture rich semantic information. In particular, cross-attention maps between text and image tokens provide spatial grounding signals. For the denoising process itself, the model uses either the source prompt (if available) or an empty prompt. In parallel, we collect features from separate text branch that encodes the referring expression, similarly as in (Helbling et al., 2025). This branch is used exclusively for feature extraction and has no effect on the denoising trajectory. Unlike prior work that primarily relies on U-Net features (Tang et al., 2023a; Zhang et al., 2023), we exploit these transformer attention maps directly, which we find more effective for referring segmentation. 3.2 REFERRAL OBJECT SEGMENTATION The goal of referring object segmentation is to localize target region in an image or video given natural language expression. Formally, for an input (I, e), where is an image or video frame and is referring expression, the task is to predict segmentation mask that highlights the region described by e. Cross-attention features. Following Concept Attention (CA) (Helbling et al., 2025), we use crossattention maps as grounding signals. Unlike CA, which assumes access to all relevant concepts in the image, our setting is more realistic: only the referring expression is provided. For each token tk e, we extract cross-attention maps (k) from multiple layers and heads of the DiT, then aggregate them into consolidated heatmap He. The referred location is obtained as pref = arg max He. Stop-word augmentation and filtering. During attention computation, stop words frequently attract disproportionately high attention (see Fig. 1), which degrades localization precision. We turn this phenomenon into an advantage through two-step procedure. First, we augment the expression by appending additional stop words (e.g., ., a, with), producing an expanded expression ˆe. Second, we filter out attention maps corresponding to stop words when aggregating token-level maps. Formally, He = mean{M (k) tk ˆe, tk / Sstop}, where Sstop is predefined set of stop words, extended with tokenizer-specific symbols (., , and ). Appended stop words act as attention magnets, absorbing surplus background activations; discarding them yields sharper, less cluttered heatmaps. Segmentation. Our method is model-agnostic and applies to both images (FLUX (Labs, 2024)) and videos (Mochi (Team, 2024)). For images, we convert the attention heatmap into segmentation mask using foundation model such as SAM or SAM2 (Kirillov et al., 2023; Ravi et al., 2025). For videos, we extract the query point from the first frame and propagate the segmentation across the sequence with SAM2. In both cases, the pipeline is entirely training-free and operates in zero-shot setting. 4 Filtered Blocks &F 54.3 56.3 57.6 57.6 80 % 70 % 60 % 0 % 51.2 53.2 54.6 54.5 57.5 59.4 60.6 60.6 Table 1: RVOS performance dependence on % of filtered transformer blocks. Performance starts degrading only after filtering more than 60% of blocks. Figure 4: Entropy across transformer blocks. Blocks 0-25 contain no specific information."
        },
        {
            "title": "3.3 EMERGENCE OF SEMANTIC INFORMATION IN DIT",
            "content": "We next examine how semantic structure arises across transformer blocks in diffusion transformers (DiTs). As shown in Fig. 3, text-to-text and text-to-image attention evolve from diffuse to semantically structured with depth. Early layers (diffuse attention). In the initial blocks (016), both text and image tokens attend broadly and diffusely. Attention maps are uniform, producing little usable alignment for grounding (Fig. 3, blocks 0-25). In particular, we show in Fig. 4 that 60% of the transformer blocks contain no structured information as the average and minimal entropy of the blocks remains high. Moreover, in Tab. 1 we demonstrate that filtering these blocks does not change the performance. Middle layers (clustering and alignment). From mid-level blocks onward, structure begins to emerge: image tokens form clusters corresponding to coarse regions, while text tokens specialize toward different spatial areas. For example, the patches token in Fig. 3 gradually concentrates on the brown patches on the animals body. This stage marks the onset of meaningful cross-modal alignment. Late layers (emergence of GAS). In later layers, semantic alignment sharpens, but we also consistently observe Global Attention Sinks (GAS). These are tokens, most often stop words, that allocate unusually high and nearly uniform attention across both text and image tokens  (Fig. 1)  . We identify GAS by computing per-token text-to-text activations and marking tokens whose average mass is 10 higher than the mean across all layers and all tokens. In Fig. 1, we visualize layer-averaged text-to-text and text-to-visual attention for each token. On the left, tokens #1 ( a) and #16 (</s>) exhibit uniformly high attention across both textual and visual tokens, characteristic of global attention sinks. Typically, 13 GAS tokens appear per sequence. 3.4 INTERPRETATION OF GLOBAL ATTENTION SINKS Next, we analyze the role of Global Attention Sinks (GAS) and their impact on referral segmentation. Uninformative role. While GAS tokens serve as indicators of emerging semantic structure (see Fig. 3), they do not encode meaningful content. Removing them has no negative effect on performance; when suppressed during inference, their surplus activations are naturally redistributed to non-sink tokens, confirming that their contribution is noise-like rather than semantically useful. Indicators of semantic structure. GAS consistently emerge only after meaningful structure is established in the middle layers. Their appearance therefore marks the onset of semantically organized representations, even if the GAS tokens themselves are uninformative. Potentially harmful role. While the majority of GAS tokens (77%) correspond to stop words, about 10% fall on color tokens and another 10% on other content words. In these cases, GAS behavior can suppress discriminative cues (e.g., color specificity), suggesting untapped headroom if such suppression were prevented. 3.5 REDISTRIBUTION STRATEGY WITH ATTENTION MAGNETS The distribution of semantic information across tokens in later layers raises two challenges for referral segmentation: (i) GAS tokens that suppress meaningful content when they fall on discriminative 5 Figure 5: Influence of attention magnets on RVOS. Examples demonstrating attention magnets filtering impact. tokens, and (ii) background activations that contaminate attention maps. We address both through redistribution with attention magnetsappended tokens that attract surplus attention and are later filtered out. (i) Redistributing GAS. When GAS fall on stop words, they are harmless. However, when they occur on meaningful tokens such as colors, they erase discriminative distinctions. By appending auxiliary magnets (extra stop words and color words), we redirect uniform attention away from these tokens. Empirically, in 89% of cases, color-GAS tokens reassign their mass to the magnets, allowing the original tokens (e.g., red, white) to recover specificity. (ii) Redistributing background attention. Even in the absence of GAS, stop words act as local magnets that absorb surplus attention from irrelevant regions such as sky, ground, or background objects. single or small set of stop words often clusters large areas into one diffuse blob, which still contaminates the averaged heatmap. By appending additional stop words with diverse embeddings, we increase the number of available magnets. This partitions the background into multiple smaller clusters, each absorbed by different magnet. After filtering these tokens, the residual heatmaps are sharper and contain less clutter, see Fig. 5. Practical effect. The combined mechanism, (i) redirecting global sinks into magnets and (ii) partitioning background noise across multiple attractors, consistently improves grounding. Foreground maps become sharper and more concentrated, while meaningful tokens preserve their semantic roles. Crucially, the approach is entirely training-free: it leverages inductive behavior already learned during pretraining (e.g., frequent exposure to stop words) rather than introducing new parameters."
        },
        {
            "title": "4 RESULTS",
            "content": "We evaluate our proposed method on referring image object segmentation (RIOS), and referring video object segmentation (RVOS). For each task, we compare against state-of-the-art baselines under training-free settings. Datasets. We evaluate our method on the standard benchmarks for referring image and video segmentation tasks. For referring image segmentation (RIOS), we use RefCOCO/+/g (Kazemzadeh et al., 2014), containing referring expressions for objects in COCO images (Lin et al., 2014). For referring video segmentation (RVOS), we use Ref-DAVIS17 (Khoreva et al., 2019), Ref-YouTubeVOS (Seo et al., 2020) and MeViS (Ding et al., 2023), which provides video object masks and expressions for sequences. MeViS is newly established dataset that is targeted at motion information analysis and its test set consists of 50 videos and 793 annotations. The Ref-YouTube-VOS stands 6 Metric Method Vision Backbone Pre-trained Model RefCOCO testA val testB val RefCOCO+ testA testB RefCOCOg test val oIoU mIoU zero-shot methods w/ additional training Pseudo-RIS (Yu et al., 2024) VLM-VG (Wang et al., 2025) zero-shot methods w/o additional training Grad-CAM (Selvaraju et al., 2017a) MaskCLIP (Zhou et al., 2022) Global-Local (Yu et al., 2023) Global-Local (Yu et al., 2023) Global-Local (Yu et al., 2023) Ref-Diff (Ni et al., 2023) TAS (Suo et al., 2023) HybridGL (Liu & Li, 2025) REFAM (ours) weakly-supervised methods CLRL (Lee et al., 2023) PPT (Dai & Yang, 2024) zero-shot methods w/ additional training Pseudo-RIS (Yu et al., 2024) VLM-VG (Wang et al., 2025) ViT-B R50 R50 R50 R50 ViT-B ViT-B ViT-B ViT-B DiT ViT-B ViT-B ViT-B R101 SAM, CoCa, CLIP COCO, VLM-VG 37.33 45.40 43.43 48. 31.90 41.40 40.19 37.00 46.43 40.70 33.63 30.50 41.63 42.80 43.52 44. SAM, CLIP SAM, CLIP FreeSOLO, CLIP SAM, CLIP SAM, CLIP SAM, SD, CLIP SAM, BLIP2, CLIP SAM,CLIP SAM, FLUX 23.44 20.18 24.58 24.55 21.71 35.16 29.53 41.81 46.91 23.91 20.52 23.38 26.00 24.48 37.44 30.26 44.52 52.30 21.60 21.30 24.35 21.03 20.51 34.50 28.24 38.50 43.88 26.67 22.06 25.87 26.62 23.70 35.56 33.21 35.74 38.57 27.20 22.43 24.61 29.99 28.12 38.66 38.77 41.43 42. 24.84 24.61 25.61 22.23 21.86 31.40 28.01 30.90 34.90 23.00 23.05 30.07 28.92 26.57 38.62 35.84 42.47 45.53 23.91 23.41 29.83 30.48 28.21 37.50 36.16 42.97 44.45 - SAM 31.06 46.76 32.30 45. 30.11 46.28 31.28 45.34 32.11 45.84 30.13 44.77 32.88 42.97 - - SAM, CoCa, CLIP COCO, VLM-VG 41.05 49.90 48.19 53.10 33.48 46.70 44.33 42.70 51.42 47. 35.08 36.20 45.99 48.00 46.67 48.50 zero-shot methods w/o additional training Grad-CAM (Selvaraju et al., 2017a) MaskCLIP (Zhou et al., 2022) Global-Local (Yu et al., 2023) Global-Local (Yu et al., 2023) Global-Local (Yu et al., 2023) CaR (Sun et al., 2024b) Ref-Diff (Ni et al., 2023) TAS (Suo et al., 2023) HybridGL (Liu & Li, 2025) REFAM (ours) R50 R50 R50 R50 ViT-B ViT-B and ViT-L ViT-B ViT-B ViT-B DiT SAM, CLIP SAM, CLIP FreeSOLO, CLIP SAM, CLIP SAM, CLIP CLIP SAM, SD, CLIP SAM, BLIP2, CLIP SAM, CLIP SAM, FLUX 30.22 25.62 26.70 31.83 33.12 33.57 37.21 39.84 49.48 57.24 31.90 26.66 24.99 32.93 36.52 35.36 38.40 41.08 53.37 59.78 27.17 25.17 26.48 28.64 29.58 30.51 37.19 36.24 45.19 53.32 33.96 27.49 28.22 34.97 35.29 34.22 37.29 43.63 43.40 43.59 25.66 28.49 26.54 37.11 39.58 36.03 40.51 49.13 49.13 47.28 32.29 30.47 27.86 30.61 31.89 31.02 33.01 36.54 37.17 38. 33.05 30.13 33.02 40.66 40.08 36.67 44.02 46.62 51.25 47.11 32.50 30.15 33.12 40.94 40.74 36.57 44.51 46.80 51.59 48.35 Table 2: Comparison with state-of-the-art zero-shot methods on RefCOCO, RefCOCO+, and RefCOCOg. The top two results in each setting (without additional training) are marked in bold and underlined, respectively. * denotes use of extra training data beyond the task-specific set. out as the most extensive RVOS dataset, comprising 202 videos and 834 annotations. Ref-DAVIS17 builds upon DAVIS17 (Khoreva et al., 2019) and contains 30 videos with 244 annotations. Implementation Details. As attention magnets, we append , with, to, and stop words and pink as and auxiliary color that redistributes some of the meaningful GAS tokens from the referral expression to ours attention magnets. We filter out stop words used not only as attention magnets, but also stop words within the referring expressions, and the end-of-sequence (/s) token. We use the same preprocessing strategy as HybridGL (Liu & Li, 2025) to extract noun phrases (NP) and spatial bias (SB) from the referring expression using spacy package, ensuring fair comparison. For referring image object segmentation (RIOS), we use the FLUX model (Labs, 2024) and collect features from timestep 750. For referring video object segmentation (RVOS), we use the Mochi model (Team, 2024) and collect features from timestep 990. To produce final attention map, we aggregate attention maps across all transformer blocks if not stated differently. Since the COCO dataset already provides captions with its annotations, we use these directly to guide feature extraction. We use chatGPT4o to generate captions for DAVIS, Ref-YouTube-VOS and MeViS test videos. 4.1 SOTA COMPARISON We evaluate REFAM on referral image object segmentation (RIOS) on RefCOCO, RefCOCO+, and RefCOCOg datasets using oIoU and mIoU metrics in Tab. 2 and on referral video object segmentation (RVOS) Ref-DAVIS17, Ref-YouTube-VOS and MeViS datasets using standard &F metrics in Tab. 3. Our method significantly outperforms prior training-free approaches on both RIOS and RVOS, establishing new state-of-the-art results across all datasets. Notable RIOS baselines include Ref-Diff (Ni et al., 2023), MaskCLIP (Zhou et al., 2022), Global-Local (Yu et al., 2023), and the recent HybridGL (Liu & Li, 2025), which rely on complex modeling of spatial or relational cues. In contrast, our approach is simple and leverages the semantic structure learned by pretrained generative models. In particular, compared to HybridGLthe strongest prior zero-shot methodREFAM achieves an absolute gain of +2.5 mIoU on RefCOCOg test and +1.8 mIoU on RefCOCO+ testB. On RefCOCO+ testA, REFAM improves mIoU by more than 9 points over Ref-Diff and by over 12 points over Global-Local. Despite relying only on frozen FLUX features and SAM segmentation method, our approach achieves performance competitive with, and in some cases exceeding, methods that incorporate additional task-specific training or fine-tuning. In Tab. 3, REFAM outperforms all prior training-free baselines and narrowing the gap to recent methods such as Grounded-SAM (Kirillov 7 Method Ref-DAVIS17 &F Ref-YouTube-VOS &F MeViS &F Training-Free with Grounded-SAM Grounded-SAM (Ren et al., 2024) Grounded-SAM2 (Ren et al., 2024) AL-Ref-SAM2 (Huang et al., 2025) G-L + SAM2 (Yu et al., 2023) G-L (SAM) + SAM2 (Yu et al., 2023) REFAM + SAM2 (ours) 65.2 66.2 74.2 40.6 46.9 57.6 62.3 62.6 70.4 68.0 69.7 78.0 Training-Free 43.6 49.7 60.6 37.6 44.0 54.5 62.3 64.8 67. 27.0 33.6 42.7 61.0 62.5 65.9 24.3 29.9 37.6 63.6 67.0 69.9 29.7 37.3 47.8 - 38.9 42. 23.7 26.6 30.6 - 35.7 39.5 20.4 22.7 24.7 - 42.1 46.2 30.0 30.5 36. Table 3: Comparison with state-of-the-art zero-shot methods Ref-DAVIS17, Ref-YouTube-VOS and MeViS. Results are from Al-Ref (Huang et al., 2025). SB AM NP - - - - - - - - - Ref-DAVIS17 54.5 50.9 52.2 49.5 51.5 46.8 60.6 57. 58.0 56.7 56.9 53.2 &F 57.6 54.4 55.1 53.1 54.2 50.0 PA 68.9 59. 67.2 60.2 59.0 52.5 Table 4: Influence of the components. AM denotes appending attention magnets with the following filtering, NP is filtering of everything but noun phrase, SB is spatial bias. PA is predicted point accuracy. AM stop words + color stop words random stop words (5x) random vectors (5x) none &F 57.6 56.8 57.5 56.2 54. Ref-DAVIS17 54.5 53.7 54.3 53.1 50.9 60.6 59.9 60.5 59.4 57.6 PA 68.9 67.2 68.5 65.5 59.8 Table 5: Influence of different AM. AM denotes appending attention magnets to the referral expression. PA is predicted point accuracy. Figure 6: Qualitative examples. Referring image object segmentation results. Each triplet shows the input image with the referring expression, the cross-attention heatmap with the detected argmax point (star), and the final segmentation mask produced by SAM. et al., 2023), Grounded-SAM2 (Ren et al., 2024), and AL-Ref-SAM (Huang et al., 2025), which are pretrained with image grounding datasets. These results demonstrate that carefully leveraging diffusion features, without retraining, is sufficient to close the gap with supervised and weakly supervised methods, while maintaining the simplicity and generality of fully training-free pipeline. 8 AM NP - - - - - SB val 46.91 33.89 37.60 - - 32.99 - - 35.54 29.14 RefCOCO RefCOCO+ testA testB val testA testB RefCOCOg test val 52.30 44. 41.81 35.22 39.80 31.49 43.88 34.14 34.22 31.98 32.86 29.21 38.57 35. 38.53 34.47 37.12 31.61 42.66 37.69 42.60 37.05 40.89 34.29 34.90 33. 35.66 33.62 34.32 30.65 45.53 42.93 42.59 41.83 38.66 34.09 44.45 42. 42.75 41.00 40.90 35.81 Table 6: Ablation on spatial bias and noun phrase encoding. Both components contribute to performance, with spatial bias providing the largest gains, while combining both yields the best results across RefCOCO, RefCOCO+, and RefCOCOg."
        },
        {
            "title": "4.2 ABLATIONS",
            "content": "In Tabs. 4 and 5, we decouple &F mask evaluation from our predicted points by introducing the point accuracy (PA) metric, which considers point correct if it falls within the ground-truth mask. Influence of Attention Magnets. Including stop words in attention map aggregation results in overly diffuse localization. As shown in Tabs. 4 and 6, introducing and then filtering our attention magnets (AM) out from the referring expressions improves predicted point accuracy from 59.9 to 68.9 and raises the &F metric by 3.2 points on RVOS. Moreover, we observe consistent gains across settings when attention magnets are appended. Fig. 5 further illustrates how redistributing background activations followed by filtering, produces sharper and more focused attention maps. Variants of Attention Magnets. In Tab. 5, we evaluate the role of including color tokens as attention magnets. As discussed above, they help redistribute GAS away from meaningful tokens in the referring expressions, yielding an improvement of roughly 1% across metrics. We then examine whether the specific choice of stop words matters. Sampling five different stop-word sets produces consistent results. However, replacing stop words with random vectors (re-normalized to match token distributions) leads to slightly worse performance. This suggests that background redistribution is crucial for capturing semantics in generative models, and that real stop words, which are frequently encountered during training, are particularly effective at absorbing meaningless background activations. Noun Prase and Spatial Bias. We conduct an ablation study to disentangle the contributions of spatial bias and noun phrase encoding, as shown in Tab. 6 and Tab. 4. We use the same preprocessing strategy as HybridGL (Liu & Li, 2025) to extract noun phrases and spatial relations from the referring expression, ensuring fair comparison. When combined, the two components with our attention magnets yield the best performance across all benchmarks, confirming their complementary roles in grounding referring expressions. See Sec. for more details. Qualitative Examples. Figs 6 and 5 present qualitative examples of RIOS and RVOS. Each example shows the input image, the corresponding cross-attention map with the predicted argmax location indicated by star, and the final segmentation mask produced by SAM when seeded with this location. Fig. 5 additionally shows aggregated attention maps with and without our attention magnets. We observe that REFAM accurately grounds diverse referring expressions including various attributes. While the attention maps often highlight multiple candidate regions when objects are visually similar, the predicted argmax location reliably falls on the correct instance, enabling accurate segmentation."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We introduce REFAM, training-free framework for zero-shot referring segmentation that exploits cross-attention features from flow-matching DiTs. By identifying stop words as attention magnets and uncovering global attention sinks (GAS), we proposed simple redistribution mechanism that sharpens localization without retraining or architectural changes. REFAM sets new state of the art among training-free methods: on RefCOCO, RefCOCO+, and RefCOCOg it outperforms previous zero-shot approaches, including gains of up to +2.5 mIoU over HybridGL, and on Ref-DAVIS17, RefYouTube-VOS, and MeViS it achieves the best reported results for video. These findings highlight diffusion attention as powerful, general foundation for grounding referring expressions in both images and videos."
        },
        {
            "title": "REFERENCES",
            "content": "Samira Abnar and Willem Zuidema. Quantifying attention flow in transformers. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 41904197, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.385. URL https://aclanthology.org/2020.acl-main.385/. Wenbin An, Feng Tian, Sicong Leng, Jiahao Nie, Haonan Lin, QianYing Wang, Ping Chen, Xiaoqin Zhang, and Shijian Lu. Mitigating object hallucinations in large vision-language models with In Proceedings of the Computer Vision and Pattern assembly of global and local attention. Recognition Conference, 2025. Federico Barbero, Alvaro Arroyo, Xiangming Gu, Christos Perivolaropoulos, Petar Veliˇckovic, Razvan Pascanu, and Michael M. Bronstein. Why do LLMs attend to the first token? In Second Conference on Language Modeling, 2025. URL https://openreview.net/forum?id= tu4dFUsW5z. Emily Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pp. 610623, 2021. Alexander Binder, Gregoire Montavon, Sebastian Lapuschkin, Klaus-Robert Muller, and Wojciech Samek. Layer-wise relevance propagation for neural networks with local renormalization layers. In International conference on artificial neural networks, pp. 6371. Springer, 2016. Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency, pp. 7791. PMLR, 2018. Aylin Caliskan, Joanna Bryson, and Arvind Narayanan. Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334):183186, 2017. Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the International Conference on Computer Vision (ICCV), 2021. Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 782791, 2021. Qiyuan Dai and Sibei Yang. Curriculum point prompting for weakly-supervised referring image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1371113722, June 2024. Timothee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. In The Twelfth International Conference on Learning Representations, 2024. Terrance De Vries, Ishan Misra, Changhan Wang, and Laurens Van der Maaten. Does object recognition work for everyone? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pp. 5259, 2019. Henghui Ding, Scott Cohen, Brian Price, and Xudong Jiang. Phraseclick: toward achieving flexible interactive segmentation by phrase and click. In European Conference on Computer Vision, pp. 417435. Springer, 2020. Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, and Chen Change Loy. Mevis: large-scale benchmark for video segmentation with motion expressions. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 26942703, 2023. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL https://openreview. net/forum?id=YicbFdNTTy. 10 M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual object classes challenge: retrospective. International Journal of Computer Vision, 111(1):98136, January 2015. Guang Feng, Zhiwei Hu, Lihe Zhang, and Huchuan Lu. Encoder fusion network with co-attention embedding for referring image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1550615515, 2021. Yossi Gandelsman, Alexei Efros, and Jacob Steinhardt. Interpreting CLIPs image representation via text-based decomposition. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=5Ca9sSzuDp. Matthieu Guillaumin, Daniel Kuttel, and Vittorio Ferrari. Imagenet auto-annotation with segmentation propagation. International Journal of Computer Vision, 110:328348, 2014. Alec Helbling, Tuna Han Salih Meral, Benjamin Hoover, Pinar Yanardag, and Duen Horng Chau. Conceptattention: Diffusion transformers learn highly interpretable features. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/ forum?id=Rc7y9HFC34. Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-or. PromptIn The Eleventh International Conferto-prompt image editing with cross-attention control. ence on Learning Representations, 2023. URL https://openreview.net/forum?id= _CDixzkzeyb. Shaofei Huang, Rui Ling, Hongyu Li, Tianrui Hui, Zongheng Tang, Xiaoming Wei, Jizhong Han, and Si Liu. Unleashing the temporal-spatial reasoning capacity of gpt for training-free audio and language referenced video object segmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 37153723, 2025. Nick Jiang, Amil Dravid, Alexei Efros, and Yossi Gandelsman. Vision transformers dont need trained registers. arXiv preprint arXiv:2506.08010, 2025. Mingyu Jin, Kai Mei, Wujiang Xu, Mingjie Sun, Ruixiang Tang, Mengnan Du, Zirui Liu, and Yongfeng Zhang. Massive values in self-attention modules are the key to contextual knowledge understanding. In Forty-second International Conference on Machine Learning, 2025. Seil Kang, Jinyeong Kim, Junhyeok Kim, and Seong Jae Hwang. See what you are told: Visual attention sink in large multimodal models. In The Thirteenth International Conference on Learning Representations, 2025. Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 787798, 2014. Anna Khoreva, Anna Rohrbach, and Bernt Schiele. Video object segmentation with language referring expressions. In Computer VisionACCV 2018: 14th Asian Conference on Computer Vision, Perth, Australia, December 26, 2018, Revised Selected Papers, Part IV 14, pp. 123141. Springer, 2019. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 39924003. IEEE, 2023. Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. Jungbeom Lee, Sungjin Lee, Jinseok Nam, Seunghak Yu, Jaeyoung Do, and Tara Taghavi. Weakly supervised referring image segmentation with intra-chunk and inter-chunk consistency. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2187021881, 2023. Ruiyu Li, Kaican Li, Yi-Chun Kuo, Michelle Shu, Xiaojuan Qi, Xiaoyong Shen, and Jiaya Jia. Referring image segmentation via recurrent refinement networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 57455753, 2018. 11 Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer vision ECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pp. 740755. Springer, 2014. Ting Liu and Siyuan Li. Hybrid global-local representation with augmented spatial guidance for zero-shot referring image segmentation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2963429643, 2025. Minheng Ni, Yabo Zhang, Kailai Feng, Xiaoming Li, Yiwen Guo, and Wangmeng Zuo. Ref-diff: Zero-shot referring image segmentation with generative models. arXiv preprint arXiv:2308.16777, 2023. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollar, and Christoph FeichtIn The Thirteenth International enhofer. SAM 2: Segment anything in images and videos. Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=Ha6RTeWMd0. Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. Litu Rout, Yujia Chen, Nataniel Ruiz, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng Chu. Semantic image inversion and editing using rectified stochastic differential equations. In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=Hu0FSOSEyS. Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 618626, 2017a. doi: 10.1109/ICCV.2017.74. Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), Oct 2017b. Seonguk Seo, Joon-Young Lee, and Bohyung Han. Urvos: Unified referring video object segmentation network with large-scale benchmark. In European conference on computer vision, pp. 208223. Springer, 2020. Shreya Shankar, Yoni Halpern, Eric Breck, James Atwood, Jimbo Wilson, and D. Sculley. No classification without representation: Assessing geodiversity issues in open data sets for the developing world. In NIPS 2017 workshop: Machine Learning for the Developing World, 2017. Mingjie Sun, Xinlei Chen, Zico Kolter, and Zhuang Liu. Massive activations in large language models. In First Conference on Language Modeling, 2024a. Shuyang Sun, Runjia Li, Philip Torr, Xiuye Gu, and Siyang Li. Clip as rnn: Segment countless visual concepts without training endeavor. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1317113182, 2024b. Yucheng Suo, Linchao Zhu, and Yi Yang. Text augmented spatial aware zero-shot referring image segmentation. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https://openreview.net/forum?id=xhqICRykZk. 12 Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent correspondence from image diffusion. Advances in Neural Information Processing Systems, 36: 13631389, 2023a. Raphael Tang, Linqing Liu, Akshat Pandey, Zhiying Jiang, Gefei Yang, Karun Kumar, Pontus Stenetorp, Jimmy Lin, and Ferhan Ture. What the DAAM: Interpreting stable diffusion using cross attention. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023b. URL https://aclanthology.org/2023. acl-long.310. Genmo Team. Mochi 1. https://github.com/genmoai/models, 2024. Antonio Torralba and Alexei Efros. Unbiased look at dataset bias. In CVPR 2011, pp. 15211528. IEEE, 2011. Haoqi Wang, Tong Zhang, and Mathieu Salzmann. Sinder: Repairing the singular defects of dinov2. In European Conference on Computer Vision, 2024. Shijie Wang, Dahun Kim, Ali Taalimi, Chen Sun, and Weicheng Kuo. Learning visual grounding from generative vision and language model. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pp. 80578067. IEEE, 2025. Sangmin Woo, Donguk Kim, Jaehyuk Jang, Yubin Choi, and Changick Kim. Dont miss the forest for the trees: Attentional vision calibration for large vision language models. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Findings of the Association for Computational Linguistics: ACL 2025, pp. 19271951, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-256-5. doi: 10.18653/v1/2025.findings-acl.99. URL https://aclanthology.org/2025.findings-acl.99/. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming In The Twelfth International Conference on Learning language models with attention sinks. Representations, 2024. Itay Yona, Ilia Shumailov, Jamie Hayes, Federico Barbero, and Yossi Gandelsman. Interpreting the repeated token phenomenon in large language models. In Forty-second International Conference on Machine Learning, 2025. Seonghoon Yu, Paul Hongsuck Seo, and Jeany Son. Zero-shot referring image segmentation with global-local context features. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1945619465, 2023. Seonghoon Yu, Paul Hongsuck Seo, and Jeany Son. Pseudo-ris: Distinctive pseudo-supervision generation for referring image segmentation. In Proceedings of the European Conference on Computer Vision, 2024. Linfeng Yuan, Miaojing Shi, Zijie Yue, and Qijun Chen. Losh: Long-short text joint prediction network for referring video object segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1400114010, 2024. Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Polania Cabrera, Varun Jampani, Deqing Sun, and Ming-Hsuan Yang. tale of two features: Stable diffusion complements dino for zero-shot semantic correspondence. Advances in Neural Information Processing Systems, 36:4553345547, 2023. Wangbo Zhao, Kepan Nan, Songyang Zhang, Kai Chen, Dahua Lin, and Yang You. Learning referring video object segmentation from weak annotation. arXiv preprint arXiv:2308.02162, 2023. Chong Zhou, Chen Change Loy, and Bo Dai. Extract free dense labels from clip. In European Conference on Computer Vision, pp. 696712. Springer, 2022."
        },
        {
            "title": "APPENDIX CONTENTS",
            "content": "A Additional Experiments A.1 Feature Collection with Inversion . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Referral Video Object Segmentation . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Comparison to ConceptAttention on Image Segmentation Task . . . . . . . . . . . Stop Word Filtering & Additional Stop Words Details of Noun Phrase and Spatial Bias Extraction Limitations Societal Impact Qualitative Examples 15 15 17 18 19 19 20 14 USE OF LARGE LANGUAGE MODELS (LLMS) We used an LLM as an auxiliary tool in two limited capacities. For dataset preparation, the model generated captions for DAVIS, Ref-YouTube-VOS, and MeViS test videos, as well as semantically related prompt variations. All generated outputs were carefully screened and curated by the authors before use. For writing, the authors drafted all sections of the paper, and the LLM was employed only for copy-editing and stylistic refinement. The model was never used to introduce technical content, conduct experiments, or contribute to the research methodology."
        },
        {
            "title": "A ADDITIONAL EXPERIMENTS",
            "content": "A.1 FEATURE COLLECTION WITH INVERSION In the main paper, we extract diffusion features without applying inversion. For completeness, we also report results obtained when performing inversion prior to feature collection. Setup. All experiments were conducted on single NVIDIA A100 GPU. The peak memory usage was approximately 37GB for both Flux and Mochi models. For REFAM + inversion, the inversion step requires 69 seconds per input, while subsequent denoising (28 steps) requires 1620 seconds. Video segmentation with 50 frames using SAM2 adds 34 seconds. We use batch size of 1 for the visual input, but our framework supports multiple referring expressions per image or video. In practice, we process 58 expressions in parallel without hard limit. Overall, feature extraction with REFAM + inversion is roughly 60% faster than the commonly used SD+DINO pipeline. Feature collection via inversion. Generative diffusion models are trained to synthesize images from pure noise, and directly perturbing real image with Gaussian noise does not guarantee that the resulting latent follows trajectory observed during generation. This mismatch often causes reconstructions that diverge from the input in content or style (Hertz et al., 2023; Rout et al., 2025). To address this, we adopt rectified flow inversion (Rout et al., 2025), which introduces controlled backward process ensuring trajectory consistency. The latent dynamics are defined as dYt dt = v1t(Yt) + γ(cid:0)ut(Yt y1) + v1t(Yt)(cid:1), where ut() is the forward vector field, v1t() its reverse, and γ [0, 1] correction factor. This inversion yields structured noise latents XT that, when denoised, reconstruct the original input X0 while preserving semantic fidelity. Procedure. Our feature collection consists of two stages: (i) invert the clean input X0 into structured noise latent XT via rectified flow inversion; (ii) apply standard rectified flow denoising to XT , collecting cross-attention maps from intermediate DiT blocks. To further improve semantic quality, we generate caption for each input and condition both inversion and denoising on this caption. This ensures that the extracted features encode meaningful semantics aligned with downstream referring segmentation tasks. Results and trade-off. We compare feature extraction with and without inversion on standard referring expression benchmarks in Table 7. While both variants establish new state-of-the-art zero-shot results, inversion consistently improves performance across RefCOCO, RefCOCO+, and RefCOCOg, particularly in terms of mIoU. However, these gains come at the cost of additional latency: inversion adds 69 seconds per input before denoising. Thus, practitioners may choose between the non-inversion variant for efficiency, or the inversion-based variant for maximal accuracy, depending on application requirements. A.2 REFERRAL VIDEO OBJECT SEGMENTATION Representation Space. In Tab. 8, we compare cross-attention maps with output representations of attention. The output space is used in Concept Attention (CA) (Helbling et al., 2025), which has been shown to perform better for single-object segmentation tasks. However, key limitation of CA is its reliance on predefined set of simple, one-word concepts to represent the entire scene. For example, to segment an image of dragon sitting on stone, concepts like dragon, rock, sun, and clouds must all be explicitly defined. In contrast, our approach detects references without Metric Method Vision Backbone Pre-trained Model RefCOCO RefCOCO+ val testA testB val testA testB oIoU mIoU REFAM (ours) REFAM + Inversion (ours) REFAM (ours) REFAM + Inversion (ours) DiT DiT DiT DiT SAM, FLUX SAM, FLUX SAM, FLUX SAM, FLUX 46.91 47.99 57.24 59.12 52.30 52.22 59.78 60.21 43.88 43.99 53.32 55. 38.57 40.29 43.59 46.86 42.66 46.05 47.28 51.13 34.90 36.71 38.77 42. RefCOCOg test val 45.53 47.08 47.11 49.42 44.45 47.38 48.35 50.55 Table 7: Zero-shot referring segmentation results on RefCOCO, RefCOCO+, and RefCOCOg. We compare our method with and without inversion. Both variants achieve state-of-the-art performance, and inversion further improves consistency and semantic alignment, particularly in mIoU. Best and second-best results are shown in bold and underlined, respectively. requiring detailed scene decomposition and instead relies solely on multi-word, complex concepts defined by the referring expression. We observe that cross attention representation space shows better results than the proposed attention output in CA (Helbling et al., 2025). Space Attention Output Cross Attention &F 55.6 57.6 Ref-DAVIS17 J 52.3 54.5 58.8 60.6 PA 64.8 68.9 Table 8: Ablations of representation space. PA is predicted point accuracy. Figure 7: Visualization of different representaiton spaces. REFAM features with cross attention representations or output representations of attention. For referral tasks, REFAM use cross attention. Ablation on Text Conditioning. We investigate how textual prompting affects performance by varying the use of captions and empty prompts during the reconstruction stages. As shown in Tab. 9, using captions achieves better performance across all metrics. Removing captions results in noticeable performance drops. These results demonstrate that textual prompts are beneficial for the feature extraction from the diffusion models. text condition empty caption &F 56.6 57. Ref-DAVIS17 53.5 54.5 59.8 60.6 PA 65.5 68. Table 9: Ablations of text conditioning. PA is predicted point accuracy. SAM2 Variant. Finally, we evaluate the effect of using the smaller variant of SAM2. Replacing SAM2-H with SAM2-S leads to performance decrease across all scores, including sharp drop in &F from 57.6 to 51.8. This suggests that higher model capacity is important for capturing fine-grained spatial details in referring video segmentation. 16 size of SAM small huge &F 51.8 57. Ref-DAVIS17 48.4 54.5 55.3 60.6 PA 68.9 68.9 Table 10: Influence of size of SAM on RVOS. PA is predicted point accuracy. A.3 COMPARISON TO CONCEPTATTENTION ON IMAGE SEGMENTATION TASK We compare our method with ConceptAttention (CA) (Helbling et al., 2025) on direct image segmentation using Pascal VOC (Everingham et al., 2015) and ImageNet Segmentation (Guillaumin et al., 2014). While CA supports multi-object segmentation, it requires that all relevant concepts in the scene be explicitly specified in advance. This reliance on predefined set of simple, often one-word concepts makes it less flexible in open-world or complex scenes, where full concept enumeration is impractical or ambiguous. In contrast, our method bypasses this requirement by leveraging extra stop words (see Sec. B) that serve as background-attention magnets within cross-attention maps. Additionally, we condition feature extraction on general caption of the input image, which improves detection performance. This enables segmentation from expressive, natural language descriptions without concept-by-concept supervision. As shown in Tab. 11, our training-free approach performs competitively with CA, and qualitative results in Fig. 7 highlight improved object coverage. Whereas CA often attends to isolated, salient object parts, our method tends to capture the full spatial extent of the described object. It is also worth noting that CA was originally introduced as an interpretability method for analyzing attention in diffusion transformers, rather than as practical segmentation technique. In CA, features are extracted from the attention layers of multi-modal DiTs without modifying the denoising trajectory: the model is conditioned on either the source prompt or an empty prompt, and additional concept tokens are introduced only for interpretability. These tokens participate in attention to produce contextualized representations, but do not influence the visual stream or alter the generated image. ConceptAttention saliency maps are then constructed by projecting image patch outputs onto concept embeddings across multiple layers. By contrast, our approach uses cross-attention features linked to referring expressions and augmented stop words explicitly for segmentation guidance. Thus, while CA provides insight into model internals, our method turns attention mechanisms into practical tool for zero-shot segmentation via semantic grounding. Method PascalVOC (Single Class) Architecture Acc mIoU mAP Acc mIoU mAP ImageNet-Segmentation LRP (Binder et al., 2016) Partial-LRP (Binder et al., 2016) Rollout (Abnar & Zuidema, 2020) ViT Attention (Dosovitskiy et al., 2021) GradCAM (Selvaraju et al., 2017b) TextSpan (Gandelsman et al., 2024) TransInterp (Chefer et al., 2021) DINO Attention (Caron et al., 2021) DAAM (Tang et al., 2023b) DAAM (Tang et al., 2023b) Flux Cross Attention (Helbling et al., 2025) ConceptAttention (Helbling et al., 2025) REFAM (ours) REFAM (ours) + Inversion CLIP ViT CLIP ViT CLIP ViT CLIP ViT CLIP ViT CLIP ViT CLIP ViT DINO ViT SDXL UNet SD2 UNet Flux DiT Flux DiT Flux DiT Flux DiT 51.09 76.31 73.54 67.84 64.44 75.21 79.70 81.97 78.47 64.52 74.92 83.07 85.61 87.28 32.89 57.94 55.42 46.37 40.82 54.50 61.95 69.44 64.56 47.62 59.90 71.04 71.37 73.33 55.68 84.67 84.76 80.24 71.60 81.61 86.03 86.12 88.79 78.01 87.23 90.45 87.94 90. 48.77 71.52 69.81 68.51 70.44 75.00 76.90 80.71 72.76 64.28 80.37 87.85 89.14 89.82 31.44 51.39 51.26 44.81 44.90 56.24 57.08 64.33 55.95 45.01 54.77 76.45 78.57 80.07 52.89 84.86 85.34 83.63 76.80 84.79 86.74 88.90 88.34 83.04 89.08 90.19 90.09 90.70 Table 11: Our method, REFAM with inversion, consistently outperforms range of interpretability techniques based on Diffusion, DINO, CLIP ViT, and Flux DiT on both ImageNet-Segmentation and PascalVOC (Single Class). The performance numbers for the other methods are taken directly from ConceptAttention, and we follow the same evaluation procedure to ensure fair comparison. 17 STOP WORD FILTERING & ADDITIONAL STOP WORDS In this section, we discuss the rationale behind filtering stop words from the attention maps and describe the method we employ to accomplish this. Stop Word Filtering. Given referral expression tokenized into tokens {tk}K k=1, and an input image or video, we compute cross-attention maps between each text token tk and all the visual tokens in the image or video frames. Consequently, for each token tk, there exists corresponding cross-attention map Hk. To normalize these attention maps, we apply softmax function across all tokens: ˆHk = softmaxk(Hk). This normalization implies that for each visual patch we define probability distribution that associate it with the token having the highest softmax score relative to that patch. Given that the referral expression corresponds specifically to particular region or element within the visual input, it follows that visual areas not directly associated with the referral expression must be attributed to other tokens. We observe that words with minimal semantic significance, such as stop words, often represent the broader context or background elements of the scene relative to the specific referral expression. Observing this behavior, we propose to filter out attention maps corresponding to stop words before averaging attention maps, resulting in more focused and precise attention representations of the referral expression. See Fig. 11, Fig. 12, Fig. 13, and Fig. 14 for qualitative examples illustrating attention maps per token associated with stop words. Extra Stop Words. We observe that the given referral expression usually contains limited number of stop words, insufficient to effectively capture all background details of the visual input. To allow finer granularity in attention-to-token associations, we introduce additional stop words that act as magnets for background attention during the softmax computation. Similarly to the existing stop words in the referral expression, we filter out these attention maps associated with additional stop words after computing the softmax and before averaging the attention maps. See Fig. 11, Fig. 12, Fig. 13, and Fig. 14 for comparisons of attention maps calculated with and without extra stop words. List of Stop Words. Below we list stop words that we filter during attention computation semicolon separated. The stop words are taken from NLTK library and extended by symbols , ,, . to account for the special symbols from the tokenization. i; me; my; myself; we; our; ours; ourselves; you; your; yours; yourself; yourselves; he; him; his; himself; she; her; hers; herself; it; its; itself; they; them; their; theirs; themselves; what; which; who; whom; this; that; these; those; am; is; are; was; were; be; been; being; have; has; had; having; do; does; did; doing; a; an; the; and; but; if; or; because; as; until; while; of; at; by; for; with; about; against; between; into; through; during; before; after; above; below; to; from; up; down; in; out; on; off; over; under; again; further; then; once; here; there; when; where; why; how; all; any; both; each; few; more; most; other; some; such; no; nor; not; only; own; same; so; than; too; very; s; t; can; will; just; don; should; now; ,; .; ;"
        },
        {
            "title": "C DETAILS OF NOUN PHRASE AND SPATIAL BIAS EXTRACTION",
            "content": "We adopt the same preprocessing strategy as HybridGL (Liu & Li, 2025) to extract noun phrases and spatial cues from referring expressions. Following their approach, we parse each input sentence into object-centric noun phrases (e.g., the man, red car) and spatial relations (e.g., left of, behind, top). The noun phrases are then encoded with the text encoder and compared against diffusion-derived visual features, guiding attention toward semantically relevant regions. Spatial cues are incorporated as lightweight spatial priors. Relative relations (left of the dog) are modeled by comparing bounding box centroids of candidate regions, while absolute terms (top left, bottom right) are mapped to normalized positional masks over the image grid. This procedure mirrors the spatial relationship guidance in HybridGL but operates directly on cross-attention features extracted from diffusion transformers. As shown in our ablation study  (Table 6)  , both components contribute complementary gains. Spatial bias alone improves localization accuracy, while noun phrase extraction enhances semantic alignment. When combined with our attention magnet mechanism, the two yield the strongest results across benchmarks. We further confirm this effect in video segmentation benchmarks  (Table 4)  , where the same preprocessing consistently benefits temporal grounding."
        },
        {
            "title": "D LIMITATIONS",
            "content": "While our approach demonstrates strong performance across referring object segmentation tasks, there are few aspects that warrant further consideration. The method benefits from high-quality captions, which better guide semantic alignment; when unavailable, we rely on LLM-generated descriptions. Although this introduces soft dependence on LLMs, performance does not degrade substantially with empty prompts. Moreover, in video referring object segmentation (VROS), we currently ignore temporal aspects of the expression and always localize in the first frame. Future improvements will require detecting the frame in which the referred object actually appears. Additionally, we use SAM2 (Ravi et al., 2025) to generate segmentation mask of an object. Generally, SAM2 takes as an input prompt point, multiple points, or bounding box. In the context of referring image and video segmentation, we use single point per referring expression. This approach can lead to undersegmentation, as illustrated in Fig. 8. For instance, even animals may be only partially segmented, only one ear of camel was segmented in one of the examples. This limitation could be addressed by employing more sophisticated strategy for sampling points from the output attention maps. Figure 8: Visualization of SAM2 failure under-segmentations."
        },
        {
            "title": "E SOCIETAL IMPACT",
            "content": "Our work presents training-free framework for referral image and video object segmentation using cross-attention features from large diffusion models. By avoiding task-specific fine-tuning and leveraging existing pre-trained models, our method reduces the need for supervised datasets and extensive retraining. However, it still depends on powerful foundation models, such as FLUX, Mochie and SAM2, trained on large-scale image-text and video-text datasets, the exact composition of which is not always publicly disclosed. As prior studies have shown, large-scale training datasets can contain cultural, racial, or gender biases that may propagate into downstream tasks (Buolamwini & Gebru, 2018; Shankar et al., 2017; Torralba & Efros, 2011). Even in segmentation or correspondence, these biases may lead to varying performance across different demographic groups or underrepresented visual domains (De Vries et al., 2019). Additionally, our reliance on natural language prompts or LLM-generated captions introduces soft dependence on language models that may encode their own textual biases (Bender et al., 2021; Caliskan et al., 2017). We encourage future work toward training diffusion models on more transparent and carefully curated datasets. However, the considerable computational cost of such efforts continues to pose challenges, especially in academic settings. Our method is intended for research applications such as content-based retrieval, visual understanding, and open-set image analysis, and is not designed for high-risk or sensitive decision-making domains such as surveillance or biometric identification."
        },
        {
            "title": "F QUALITATIVE EXAMPLES",
            "content": "We present qualitative results to illustrate the effectiveness of our method across various referring image and video object segmentation scenarios. These examples highlight how our method, REFAM, captures semantically meaningful regions aligned between object and with the referring expression, and how segmentation quality benefits from attention-based guidance. We also visualize the effect of our stop word filtering strategy, showing improved focus on target objects and reduced attention to irrelevant regions. The following figures show qualitative examples and comparisons across different settings, as shown in Figs. 9 to 14. Figure 9: Additional qualitative examples for referring image object segmentation. Each panel shows the input image with its corresponding referring expression and the predicted segmentation mask. These examples complement the results in the main paper and illustrate the diversity of object categories and spatial references handled by our method. 20 Figure 10: Qualitative examples. VROS task, evaluated on Ref-DAVIS17. From left to right: first frame of the video with the corresponding ref.expression on the top, avg. attention map from our REFAM + inversion features, segmentation outputs with SAM2. 21 Figure 11: Qualitative examples. Qualitative comparison of attention maps obtained with and without additional stop words. The top row shows the first frame of the video along with the corresponding referring expression. The first row includes the average attention map, where the star indicates the argmax point with indication if it was correctly detected. 22 Figure 12: Qualitative examples. Qualitative comparison of attention maps obtained with and without additional stop words. The top row shows the first frame of the video along with the corresponding referring expression. The first row includes the average attention map, where the star indicates the argmax point with indication if it was correctly detected. 23 Figure 13: Qualitative examples.Qualitative comparison of attention maps obtained with and without additional stop words. The top row shows the first frame of the video along with the corresponding referring expression. The first row includes the average attention map, where the star indicates the argmax point with indication if it was correctly detected. 24 Figure 14: Qualitative examples. Qualitative comparison of attention maps obtained with and without additional stop words. The top row shows the first frame of the video along with the corresponding referring expression. The first row includes the average attention map, where the star indicates the argmax point with indication if it was correctly detected."
        }
    ],
    "affiliations": [
        "ETH Zurich",
        "Google",
        "Max Planck Institute for Informatics",
        "TU Munich"
    ]
}