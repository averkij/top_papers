{
    "paper_title": "KV-Edit: Training-Free Image Editing for Precise Background Preservation",
    "authors": [
        "Tianrui Zhu",
        "Shiyi Zhang",
        "Jiawei Shao",
        "Yansong Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Background consistency remains a significant challenge in image editing tasks. Despite extensive developments, existing works still face a trade-off between maintaining similarity to the original image and generating content that aligns with the target. Here, we propose KV-Edit, a training-free approach that uses KV cache in DiTs to maintain background consistency, where background tokens are preserved rather than regenerated, eliminating the need for complex mechanisms or expensive training, ultimately generating new content that seamlessly integrates with the background within user-provided regions. We further explore the memory consumption of the KV cache during editing and optimize the space complexity to $O(1)$ using an inversion-free method. Our approach is compatible with any DiT-based generative model without additional training. Experiments demonstrate that KV-Edit significantly outperforms existing approaches in terms of both background and image quality, even surpassing training-based methods. Project webpage is available at https://xilluill.github.io/projectpages/KV-Edit"
        },
        {
            "title": "Start",
            "content": "KV-Edit: Training-Free Image Editing for Precise Background Preservation Tianrui Zhu1*, Shiyi Zhang1*, Jiawei Shao2, Yansong Tang1 1Shenzhen International Graduate School, Tsinghua University 2Institute of Artificial Intelligence (TeleAI), China Telecom xilluill070513@gmail.com,sy-zhang23@mails.tsinghua.edu.cn shaojw2@chinatelecom.cn,tang.yansong@sz.tsinghua.edu.cn https://xilluill.github.io/projectpages/KV-Edit/ 5 2 0 2 5 ] . [ 2 3 6 3 7 1 . 2 0 5 2 : r Figure 1. We propose KV-Edit to address the challenge of background preservation in image editing, thereby enhancing the practicality of AI editing. Rather than designing complex mechanisms, we achieve impressive results by simply preserving the key-value pairs of the background. Our method effectively handles common semantic editing operations, including adding, removing, and changing objects."
        },
        {
            "title": "Abstract",
            "content": "Background consistency remains significant challenge in image editing tasks. Despite extensive developments, existing works still face trade-off between maintaining similar- *Equal contribution. Corresponding author. ity to the original image and generating content that aligns with the target. Here, we propose KV-Edit, training-free approach that uses KV cache in DiTs to maintain background consistency, where background tokens are preserved rather than regenerated, eliminating the need for complex mechanisms or expensive training, ultimately generating new content that seamlessly integrates with the background within user-provided regions. We further explore the memory consumption of the KV cache during editing and optimize the space complexity to O(1) using an inversion-free method. Our approach is compatible with any DiT-based generative model without additional training. Experiments demonstrate that KV-Edit significantly outperforms existing approaches in terms of both background and image quality, even surpassing training-based methods. 1. Introduction Recent advances in text-to-image (T2I) generation have witnessed significant shift from UNet [43] to DiT [39] architectures, and from diffusion models (DMs) [11, 48, 52] to flow models (FMs) [1, 23, 64]. Flow-based models, such as Flux [1], construct straight probability flow from noise to image, enabling faster generation with fewer sampling steps and reduced training resources. DiTs [39], with their pure attention architecture, have demonstrated superior generation quality and enhanced scalability compared to UNetbased models. These T2I models [1, 14, 42] can also facilitate image editing, where target images are generated based on source images and modified text prompts. In the field of image editing, early works [12, 16, 36, 50] proposed the inversion-denoising paradigm to generate edited images, but they struggle to maintain background consistency during editing. One popular approach is attention modification, such as HeadRouter [57] modifying attention maps and PnP [50] injecting original features during the denoising process, aiming to increase similarity with the source image. However, there remains significant gap between improved similarity and perfect consistency, as it is challenging to control networks behavior as intended. Another common approach is designing new samplers [37, 38] to reduce errors during inversion. Nevertheless, errors can only be reduced but not completely eliminated and both training-free approaches above still require extensive hyperparameter tuning for different cases. Meanwhile, exciting training-based inpainting methods [26, 65] can maintain background consistency but suffer from expensive training costs and potential degradation of quality. To overcome all the above limitations, we propose new training-free method that preserves background consistency during editing. Instead of relying on regular attention modification or new inversion samplers for similar results, we implement KV cache in DiTs [39] to preserve the key-value pairs of background tokens during inversion and selectively reconstruct only the editing region. Our approach first employs mask to distinguish between background and foreground regions and then inverts the image into noise space while caching KV values of background tokens at each timestep and attention layer. During the subsequent denoising process, only foreground tokens are processed, while their keys and values are concatenated with the cached background information. Effectively, we guide the generative model to maintain new content continuity with the background and keep the background content identical to the input. We call this approach KV-Edit. To further enhance the practical utility of our approach, we conduct an analysis of the removal scenario. This challenge arises from the residual information in surrounding tokens and the object itself which sometimes conflict with the editing instruction. To address this issue, we introduce mask-guided inversion and reinitialization strategies as two enhancement techniques for inversion and denoising separately. These methods further disrupt the information stored in surrounding tokens and self tokens respectively, enabling better alignment with the text prompt. In addition, we apply KV-Edit to the inversion-free method [23, 56], which no longer caches key-value pairs for all timesteps, but uses KV immediately after one step, significantly reducing the memory consumption of the KV cache. In summary, our key contributions include: 1) new training-free editing method that implements KV cache in DiTs, ensuring complete background consistency during editing with minimal hyperparameter tuning. 2) Maskguided inversion and reinitialization strategies that extend the methods applicability across various editing tasks, offering flexible choices for different user needs. 3) Using the inversion-free method to optimize the memory overhead of our method and enhance its usefulness on PC. 4) Experimental validation demonstrating perfect background preservation while maintaining generation quality comparable to direct T2I synthesis. 2. Related Work 2.1. Text-guidanced Editing Image editing approaches can be broadly categorized into training-based and training-free methods. Training-based methods [1, 7, 20, 22, 26], have demonstrated impressive editing capabilities through fine-tuning pre-trained generative models on text-image pairs, achieving controlled modifications. Training-free methods have emerged as flexible alternative, with pioneering works [12, 16, 36, 50] establishing the two-stage inversion-denoising paradigm. Attention modification has become prevalent technique in these methods [4, 9, 25, 49, 57], specially Add-it [49] broadcast features from inversion to denoising process to maintain source image similarity during editing. Some other work [21, 27, 37, 38, 53] focused on better inversion sampler such as the RF-solver [53] designs second-order sampler. The methods most similar to ours [3, 10, 29, 49] attempt to preserve background elements by blending source and target images at specific timesteps using masks. common consensus is that accurate masks are crucial for better Figure 2. Overview of our proposed KV-Edit. Given an input image and mask, we separate the image into foreground and background. Here, and denote intermediate results in inversion and denoising processes respectively. Starting from x0, we first perform inversion to obtain predicted noise xN while caching KV pairs. Then, we choose the input zf 0 based on new prompt. Finally, we concatenate it with the original background xbg and generate edited foreground content zf 0 to obtain the edited image with preserved background. quality, where user-provided inputs [20, 26] and segmentation models [6, 18, 34, 35, 41, 58, 59] prove to be more effective choices compared to masks derived from attention layers in UNet [43]. However, the above methods frequently encounter failure cases and struggle to maintain perfect background consistency during editing, while trainingbased methods [1, 7, 20, 22, 26, 65] face the additional challenge of computational overhead. 2.2. KV cache in Attention Models KV cache is widely-adopted optimization technique in Large Language Models (LLMs) [5, 8, 30, 54] to improve the efficiency of autoregressive generation. In causal attention, since keys and values remain unchanged during generation, recomputing them leads to redundant resource consumption. KV cache addresses this by storing these intermediate results, allowing the model to reuse key-value pairs from previous tokens during inference. This technique has been successfully implemented in both LLMs [5, 8, 30, 54] and Vision Language Models (VLMs) [2, 17, 24, 31, 60 62]. However, it has not been explored in image generation and editing tasks, primarily because image tokens are typically assumed to require bidirectional attention [13, 15]. 3. Method In this section, we first analyze the reasons why the inversion-denoising paradigm [12, 16] faces challenges in background preservation. Then, we introduce the proposed KV-Edit method, which achieves strict preservation of background regions during the editing process according to the mask. Finally, we present two optional enhancement techniques and an inversion-free version to improve the usability of our method across diverse scenarios. 3.1. Preliminaries Deterministic diffusion models like DDIM [46] and flow matching [28] can be modeled using ODE [47] to describe the probability flow path from noise distribution to real distribution. The model learns to predict velocity vectors that transform Gaussian noise into meaningful images. During the denoising process, x1 represents noise, x0 is the final image, and xt represents intermediate results. (cid:18) dxt = (xt, t) 1 2 g2(t)xt log p(xt) (cid:19) dt, [0, 1]. (1) where sθ(x, t) = xt log p(xt) predicted by networks. Both DDIM [46] and flow matching [28] can be viewed as special cases of this ODE function. By setting (xt, t) = (cid:16) βt , and sθ(x, t) = ϵθ(x,t) xt , αt αt we obtain the discretized form of DDIM: dαt dt , g2(t) = 2αtβt dt βt (cid:17) xt1 = αt1 (cid:18) xt βtϵθ(xt, t) αt (cid:19) + βt1ϵθ(xt, t) (2) Both forward and reverse processes in ODE follow Eq. (1), describing reversible path from Gaussian distribution to real distribution. During image editing, this ODE establishes mapping between noise and real images, where noise can be viewed as an embedding of the image, carrying information about structure, semantics, and appearance. Recently, Rectified Flow [32, 33] constructs straight path between noise distribution and real distribution, training model to fit the velocity field vθ(x, t). This process can be simply described by the ODE: dxt = vθ(x, t)dt, [0, 1]. (3) The reconstruction error in the inversionFigure 3. reconstruction process. Starting from the original image xt0 , the inversion process proceeds to xtN . During inversion process, we use intermediate images xti to reconstruct the original image and calculate the MSE between the reconstructed image t0 and the original image xt0 . Due to the reversible nature of ODEs, flow-based models can also be used for image editing through inversion and denoising in less timesteps than DDIM [46]. 3.2. Rethinking the Inversion-Denoising Paradigm The inversion-denoising paradigm views image editing as an inherent capability of generative models without additional training, capable of producing semantically different but visually similar images. However, empirical observations show that this paradigm only achieves similarity rather than perfect consistency in content, leaving significant gap compared to users expectations.This section will analyze the reasons for this issue into three factors. Taking Rectified Flow [32, 33] as an example, based on Eq. (3), we can derive the discretized implementation of inversion and denoising. The model takes the original image xt0 and Gaussian noise xtN (0, I) as path endpoints. Given discrete timesteps = {tN , ..., t0}, the model predictions vθ(C, xti , ti), {N, , 1}, where xti and zti denote intermediate states in inversion and denoising respectively, as described by the following equations: xti = xti1 + (ti ti1)vθ(C, xti, ti) zti1 = zti + (ti1 ti)vθ(C, zti, ti) (4) (5) Ideally, zt0 should be identity with xt0 when directly reconstructed from xtN . However, due to discretization and causality in the inversion process, we can only estimate using vθ(C, Xtt1, tt1) vθ(C, Xti, ti), introducing cumulative errors. Fig. 3 shows that with fixed number of timesteps , error accumulation increases as inversion timesteps approach tN , preventing accurate reconstruction. In addition, consistency is affected by condition. We can divide the image into regions we wish to edit zf t0 and regions we want to preserve zbg t0 , where fg and bg represent foreground and background respectively. Based on Figure 4. Analysis of factors affecting background changes. The four images on the right demonstrate how foreground content and condition changes influence the final results. these definitions, the background denoising process is: vθ(C, zti, ti) = vθ(C, zf ti , zbg ti , ti) (6) (7) ti , ti) ti , zbg ti1 = zbg zbg ti + (ti1 ti)vθ(C, zf According to these formulas, when generating edited results, the background will be influenced by both the new condition and new foreground zf ti . Fig. 4 demonstrates that background regions change when only modifying the In summary, uncontrollable prompt or foreground noise. background changes can be attributed to three factors: error accumulation, new conditions, and new foreground content. In practice, any single element will trigger all three effects simultaneously. Therefore, this paper will present an elegant solution to address all these issues simultaneously. 3.3. Attention Decoupling Traditional inversion-denoising paradigms process background and foreground regions simultaneously during denoising, causing undesired background changes in response to foreground and condition modifications. Upon deeper analysis, we observe that in UNet [43] architectures, the extensive convolutional networks lead to the fusion of background and foreground information, making it impossible to separate them. However, in DiT [39], which primarily relies on attention blocks [51], allows us to use only foreground tokens as queries, generating foreground content separately and then combined with the background. Moreover, directly generating foreground tokens often results in discontinuous or incorrect content relative to the background. Therefore, we propose new attention mechanism where queries contain only foreground information, while keys and values incorporate both foreground and Algorithm 1 Simplified KV cache during inversion 1: Input: ti, image xti , -layer block {lj}M j=1, foreground region mask, KV cache 2: Output: Prediction vector Vθti, KV cache 3: for = 0 to do 4: Q, K, = WQ(xti), WK(xti), WV (xti) bg Append(K bg xti = xti + Attn(Q, K, ) ij , bg ij ) ij , bg 5: ij = K[1 mask > 0], [1 mask > 0] 6: 7: 8: end for 9: Vθti = MLP(xti, ti) 10: Return Vθti , Algorithm 2 Simplified KV cache during denosing 1: Input: ti, foreground zf ti , -layer block {lj}M j=1, KV cache ij = CK[i, j], CV [i, j] 5: ij , bg 2: Output: Prediction vector θti 3: for = 0 to do 4: Qf g, g, = WQ(zf bg K, = Concat(K bg ti = zf zf 7: 8: end for 9: θti 10: Return θti = MLP(zf ti , ti) 6: ti + Attn(Qf g, K, ) ti ), WK(zf ti ), WV (zf ti ) ij , g), Concat(V bg ij , g) in-depth analysis, we reveal that this issue stems from the residual information of the original object, which persists both in its own tokens and propagates to surrounding tokens through attention mechanisms, ultimately leading the model to reconstruct the original content. To address the challenge in removing objects, we introduce two enhancement techniques. First, after inversion, we replace ztN with fused noise = noisetN +ztN (1tN ) tN to disrupt the original content information. Second, we incorporate an attention mask during the inversion process, as illustrated in Fig. 2, to prevent foreground content from being incorporated into the KV values, further reducing the preservation of original content. These techniques serve as optional enhancements to improve editing capabilities and performances in different scenarios as shown in Fig. 1. 3.5. Memory-Efficient Implementation Inversion-based methods require storing key-value pairs for timesteps, which can pose significant memory constraints when working with large-scale generative models (e.g., 12B parameters [1]) on personal computers. Fortunately, inspired by [23, 56], we explore an inversion-free approach. The method performs denoising immediately after each inFigure 5. Demonstration of inversion-free KV-Edit. The right panel shows three comparative cases including failure case, while the left panel illustrates inversion-free approach Significantly optimizes the space complexity to O(1). background information. Excluding text tokens, the imagemodality self-attention computation can be expressed as: Att(Qf g, (Kf g, Kbg), (Vf g, Vbg)) = S( Qf gKT )V (8) where Qf represents queries containing only foreground tokens, (Kf g, Kbg) and (Vf g, Vbg) denote the concatenation of background and foreground keys and values in their proper order (equivalent to the complete images keys and values), and represents the softmax operation. Notably, compared to conventional attention computations, Eq. (8) only modifies the query component, which is equivalent to performing cropping at both input and output of the attention layer, ensuring seamless integration of the generated content with the background regions. 3.4. KV-Edit Building upon Eq. (8), achieving background-preserving foreground editing requires providing appropriate key-value pairs for the background. Our core insight is that background tokens keys and values reflect their deterministic path from image to noise. Therefore, we implement KV cache during the inversion process, as detailed in Algorithm 1. This approach records the keys and values at each timestep and block layer along the probability flow path, which are subsequently retrieved during denoising based on the corresponding timestep and layer index as shown in Algorithm 2. We term this complete pipeline KV-Edit, where KV represents the key-value caching mechanism employed within the DiT architecture, as illustrated in Fig. 2. Since the background tokens are preserved rather than regenerated, KV-Edit ensures perfect background consistency, effectively circumventing the three influencing factors discussed in Sec. 3.2. Previous works [9, 12, 16] often fail in object removal tasks when using image captions as guidance, as the original object still aligns with the target prompt. Through our Figure 6. Qualitative Results on PIE-Bench. Unlike existing methods, our method demonstrates superior performance by strictly maintaining background consistency and simultaneously following users text prompt. The comparison also showcases user-friendly workflow. version step, computing the vector difference between the two results to derive probability flow path in the t0 space. This approach allows immediate release of KV cache after use, reducing memory complexity from O(N ) to O(1). However, the inversion-free method may occasionally result in content retention artifacts as shown in Fig. 5 and FlowEdit [23]. Since our primary focus is investigating background preservation during editing, we leave further analysis of the inversion-free approach for future work. 4. Experiments 4.1. Experimental Setup Baselines. We compare our method against two cate- (1) Training-free methods includgories of approaches: ing P2P [16], MasaCtrl [9] based on DDIM [46], and RFEdit [53], RF-Inversion [44] based on Rectified Flow [33]; (2) Training-based methods including BrushEdit [26] and FLUX-Fill [1], which are based on DDIM and Rectified Flow respectively. In total, we evaluate against six prevalent image editing and inpainting approaches. Datasets. We evaluate our method and baselines on nine tasks from PIE-Bench [21], which comprises 620 images with corresponding masks and text prompts. Following [26, 56], we exclude style transfer tasks from PIE-Bench [21] as our primary focus is background preservation in semantic editing tasks such as object addition, removal, and change. Implementation Details. We implement our method based on FLUX.1-[dev] [1], following the same framework as other Rectified Flow-based methods [23, 44, 53]. We main-"
        },
        {
            "title": "Method",
            "content": "VAE P2P [16] MasaCtrl [9] RF Inv. [44] RF Edit [53] BrushEdit [26] FLUX Fill [1] Ours +NS+RI"
        },
        {
            "title": "Text Align",
            "content": "HPS 102 AS PSNR LPIPS 103 MSE 104 CLIP Sim IR10 24.93 25.40 23.46 27.99 27.60 25.81 25.76 27.21 28.05 6.37 6.27 5.91 6.74 6.56 6.17 6. 6.49 6.40 37.65 17.86 22.20 20.20 24.44 32.16 32.53 35.87 33.30 7.93 208.43 105.74 179.73 113.20 17.22 25. 9.92 14.80 3.86 219.22 86.15 139.85 56.26 8.46 8.55 4.69 7.45 19.69 22.24 20.83 21.71 22.08 22.44 22. 22.39 23.62 -3.65 0.017 -1.66 4.34 5.18 3.33 5.71 5.63 9.15 Table 1. Comparison with previous methods on PIE-Bench. VAE denotes the inherent reconstruction error through direct VAE reconstruction. P2P and MasaCtrl are DDIM-based methods, while RF Inversion and RF Edit are flow-based. BrushEdit and FLUX Fill represent training-based methods. NS indicates there is no skip step during inversion. RI indicates the addition of reinitialization strategy. Bold and underlined values denote the best and second-best results respectively. Method Image Quality Text Align HPS 102 AS CLIP Sim IR KV Edit (ours) +NS +NS+AM +NS+RI +NS+AM+RI 26.76 26.93 26.72 26.73 26.51 6.49 6.37 6.35 6.34 6.28 25.50 25.05 25.00 24.82 24. 10 6.87 3.17 2.55 0.22 0.90 Table 2. Ablation Study for object removal task. CLIP Sim and IR represent alignment between source prompt and new image through CLIP [40] and Image Reward [55] to evaluate whether remove particular object from image. NS indicates there is no skip step during inversion. RI indicates the addition of reinitialization strategy. AM indicates that using attention mask during inversion. tain consistent hyperparameters with FlowEdit [23], using 28 timesteps in total, skipping the last 4 timesteps (N = 24) to reduce cumulative errors, and setting guidance values to 1.5 and 5.5 for inversion and denoising processes respectively. NS in tables and charts represent no skip step (N = 28). Other baselines retain their default parameters or use previously published results. Unless otherwise specified, Ours in tables refers to the inversion-based KV-Edit without the two optional enhancement techniques proposed in Sec. 3.4. All experiments are conducted on two NVIDIA 3090 GPUs with 24GB memory. Metrics Following [20, 21, 26], we use seven metrics across three dimensions to evaluate our method. For image quality, we report HPSv2 [63] and aesthetic scores [45]. For background preservation, we measure PSNR [19], LPIPS [63], and MSE. For text-image alignment, we report CLIP score [40] and Image Reward [55]. Notably, while Image Reward was previously used for quality assessment, we found it particularly effective at measuring text-image alignment, providing negative scores for unedited images. Based on this observation, we also utilize Image Reward to evaluate the successful removal of objects. 4.2. Editing Results We conduct experiments on PIE-Bench [21], categorizing editing tasks into three major types: removing, adding, and changing objects. For practical applications, these tasks prioritize background preservation and text alignment, followed by overall image quality assessment. Quantitative Comparison. Sec. 4 presents quantitative results including baselines, our method, and our method with the reinitialization strategy. We exclude results with the attention mask strategy, as it shows improvements only in specific cases. Our method surpasses all others in Masked Region Preservation metrics. Notably, as shown in Fig. 6, methods with PSNR below 30 fail to maintain background consistency, producing results that merely resemble the original. RF-Inversion [44], despite obtaining high image quality scores, generates entirely different backgrounds. Our method achieves the third-best image quality, which has been higher than the original images, and perfectly preserving the background at the same time. With the reinitialization process, we achieve optimal text alignment scores, as the injected noise disrupts the original content, enabling more effective editing in certain cases (e.g., object removal and color change). Even compared to training-based inpainting methods [1, 26], our approach better preserves backgrounds while following user intentions. Qualitative Comparison. Fig. 6 demonstrates our methods performance against previous works across three different tasks. For removal tasks, the examples shown require both enhancement techniques proposed in Sec. 3.4. Previous training-free methods fail to preserve backgrounds, particularly Flow-Edit [23] which essentially genours vs. Random RF Inv. [44] RF Edit [53] BrushEdit [26] FLUX Fill [1] Quality Background Text Overall 50.0% 61.8% 54.5% 71.8% 60.0% 50.0% 94.8% 90.5% 66.7% 53.7% 50.0% 50.0% 79.6% 85.1% 75.0% 73.6% 68.7% 70.2% 58.6% 61.9% Table 3. User Study. We compared our method with four popular baselines. Participants were asked to choose their preferred option or indicate if both methods were equally good or not good based on four criteria. We report the win rates of our method compared to baseline excluding equally good or not good instances. Random denotes the win rate of random choices. versal solutions applicable to all scenarios. Fig. 7 visualizes the impact of these strategies. In the majority of cases, reinitialization alone suffices to achieve the desired results, while small subset of cases requires additional attention masking for enhanced performance. 4.4. User study We conduct an extensive user study to compare our method with four baselines, including the training-free methods RFEdit [53], RF-Inversion [44], and the training-based methods BrushEdit [26] and Flux-Fill [1]. We use 110 images from the random class in the PIE-Bench [21] (excluding style transfer task, images without backgrounds, and controversial content). More than 20 participants are asked to compare each pair of methods based on four criteria: image quality, background preservation, text alignment, and overall satisfaction. As shown in Tab. 3, our method significantly outperforms the previous methods, even surpassing Flux-Fill [1], which is the official inpainting model of FLUX [1]. Additionally, users feedback reveals that background preservation plays crucial role in their final choices, even if RF-Edit [53] achieves high image quality but finally fails in satisfaction comparison. 5. Conclusion In this paper, we introduce KV-Edit, new training-free approach that achieves perfect background preservation in image editing by caching and reusing background keyvalue pairs. Our method effectively decouples foreground editing from background preservation through attention mechanisms in DiT, while optional enhancement strategies and memory-efficient improve its practical utility. Extensive experiments demonstrate that our approach surpasses both training-free methods and training-based inpainting models in terms of both background preservation and image quality. Moreover, we hope that this straightforward yet effective mechanism could inspire broader applications, such as video editing, multi-concept personalization, and other scenarios. implementation further Figure 7. Ablation Study of different optional strategies on object removal task. From left to right, applying more strategies leads to stronger removal effect and the right is the best. erates new images despite high quality. Interestingly, training-based methods like BrushEdit [26] and FLUXFill [1] exhibit notable phenomena in certain cases (first and third rows in Fig. 6). BrushEdit [26], possibly limited by generative model capabilities, produces meaningless content. FLUX-Fill [1] sometimes misinterprets text prompts, generating unreasonable content like duplicate subjects. In contrast, our method demonstrates satisfactory results, successfully generating text-aligned content while preserving backgrounds, eliminating the traditional trade-off between background preservation and foreground editing. 4.3. Ablation Study We conduct ablation studies to illustrate the impact of two enhancement strategies proposed in Sec. 3.4 and the no-skip step on our methods object removal performance. Tab. 2 presents the results in terms of image quality and text alignment scores. Notably, for text alignment evaluation, we compute the similarity between the generated results and the original prompt using CLIP [40] and Image Reward [55] models. This metric proves more discriminative in removal tasks, as still presenting of specific objects in the final images significantly increases the similarity scores. As shown in Tab. 2, the combination of NS (No-skip) and RI (Reinitialization) achieves the optimal text alignment scores. However, we observe slight decrease in image quality metrics after incorporating these components. We attribute this phenomenon to the presence of too large masks in the benchmark, where no-skip, reinitialization, and attention mask collectively disrupt substantial information, leading to some discontinuities in the generated images. Consequently, these strategies should be viewed as optional enhancements for editing effects rather than uni-"
        },
        {
            "title": "References",
            "content": "[1] Flux. https : / / github . com / black - forest - labs/flux/. 2, 3, 5, 6, 7, 8 [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 3 [3] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended latent diffusion. TOG, 42(4):111, 2023. [4] Omri Avrahami, Or Patashnik, Ohad Fried, Egor Nemchinov, Kfir Aberman, Dani Lischinski, and Daniel CohenOr. Stable flow: Vital layers for training-free image editing. arXiv preprint arXiv:2411.14430, 2024. 2 [5] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei arXiv preprint Huang, et al. Qwen technical report. arXiv:2309.16609, 2023. 3 [6] Sule Bai, Yong Liu, Yifei Han, Haoji Zhang, and Yansong Tang. Self-calibrated clip for training-free open-vocabulary segmentation. arXiv preprint arXiv:2411.15869, 2024. 3 [7] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, pages 1839218402, 2023. 2, 3 [8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. NeurIPS, 33:1877 1901, 2020. 3 [9] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In ICCV, pages 2256022570, 2023. 2, 5, 6, 7 [10] Zhennan Chen, Yajie Li, Haofan Wang, Zhibo Chen, Zhengkai Jiang, Jun Li, Qian Wang, Jian Yang, and Ying Tai. Region-aware text-to-image generation via hard binding and soft refinement. arXiv preprint arXiv:2411.06558, 2024. 2 [11] Wenxun Dai, Ling-Hao Chen, Jingbo Wang, Jinpeng Liu, Bo Dai, and Yansong Tang. Motionlcm: Real-time controllable motion generation via latent consistency model. In ECCV, pages 390408, 2024. [12] Wenkai Dong, Song Xue, Xiaoyue Duan, and Shumin Han. Prompt tuning inversion for text-driven image editing using diffusion models. In ICCV, pages 74307440, 2023. 2, 3, 5 [13] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 3 [14] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. 2 [15] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, pages 1600016009, 2022. 3 [16] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt imarXiv preprint age editing with cross attention control. arXiv:2208.01626, 2022. 2, 3, 5, 6, 7 [17] Wenke Huang, Jian Liang, Zekun Shi, Didi Zhu, Guancheng Wan, He Li, Bo Du, Dacheng Tao, and Mang Ye. Learn from downstream and be yourself in multimodal large language model fine-tuning. arXiv preprint arXiv:2411.10928, 2024. 3 [18] Xiaoke Huang, Jianfeng Wang, Yansong Tang, Zheng Zhang, Han Hu, Jiwen Lu, Lijuan Wang, and Zicheng Liu. In CVPR, pages 13405 Segment and caption anything. 13417, 2024. [19] Quan Huynh-Thu and Mohammed Ghanbari. Scope of validity of psnr in image/video quality assessment. Electronics letters, 44(13):800801, 2008. 7 [20] Xuan Ju, Xian Liu, Xintao Wang, Yuxuan Bian, Ying Shan, and Qiang Xu. Brushnet: plug-and-play image inpainting In ECCV, model with decomposed dual-branch diffusion. pages 150168, 2024. 2, 3, 7 [21] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Pnp inversion: Boosting diffusion-based editing with 3 lines of code. In ICLR, 2024. 2, 6, 7, 8 [22] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In CVPR, pages 60076017, 2023. 2, 3 [23] Vladimir Kulikov, Matan Kleiner, Inbar HubermanSpiegelglas, and Tomer Michaeli. Flowedit: Inversion-free arXiv text-based editing using pre-trained flow models. preprint arXiv:2412.08629, 2024. 2, 5, 6, [24] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, pages 1973019742, 2023. 3 [25] Senmao Li, Joost van de Weijer, Taihang Hu, Fahad Shahbaz Khan, Qibin Hou, Yaxing Wang, and Jian Yang. Stylediffusion: Prompt-embedding inversion for text-based editing. arXiv preprint arXiv:2303.15649, 2023. 2 [26] Yaowei Li, Yuxuan Bian, Xuan Ju, Zhaoyang Zhang, Ying Shan, and Qiang Xu. Brushedit: All-in-one image inpainting and editing. arXiv preprint arXiv:2412.10316, 2024. 2, 3, 6, 7, 8 [27] Haonan Lin, Mengmeng Wang, Jiahao Wang, Wenbin An, Yan Chen, Yong Liu, Feng Tian, Guang Dai, Jingdong Wang, and Qianying Wang. Schedule your edit: simple yet effective diffusion noise schedule for image editing. arXiv preprint arXiv:2410.18756, 2024. 2 [28] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 3 [29] Aoyang Liu, Qingnan Fan, Shuai Qin, Hong Gu, and Yansong Tang. Lipe: Learning personalized identity prior for non-rigid image editing. arXiv preprint arXiv:2406.17236, 2024. [30] Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434, 2024. 3 [31] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 36, 2024. 3 [32] Qiang Liu. Rectified flow: marginal preserving approach to optimal transport. arXiv preprint arXiv:2209.14577, 2022. 3, 4 [33] Xingchao Liu, Chengyue Gong, et al. Flow straight and fast: Learning to generate and transfer data with rectified flow. In ICLR, 2022. 3, 4, [34] Yong Liu, Sule Bai, Guanbin Li, Yitong Wang, and Yansong Tang. Open-vocabulary segmentation with semantic-assisted calibration. In CVPR, pages 34913500, 2024. 3 [35] Yong Liu, Cairong Zhang, Yitong Wang, Jiahao Wang, Yujiu Yang, and Yansong Tang. Universal segmentation at arbitrary granularity with language instruction. In CVPR, pages 34593469, 2024. 3 [36] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In ICLR, 2022. 2 [37] Daiki Miyake, Akihiro Iohara, Yu Saito, and Toshiyuki Tanaka. Negative-prompt inversion: Fast image inversion for editing with text-guided diffusion models. arXiv preprint arXiv:2305.16807, 2023. 2 [38] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In CVPR, pages 60386047, 2023. 2 [39] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, pages 41954205, 2023. 2, 4 [40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 87488763, 2021. 7, 8 [41] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: arXiv preprint Segment anything in images and videos. arXiv:2408.00714, 2024. 3 [42] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10684 10695, 2022. 2 [43] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, pages 234241, 2015. 2, 3, 4 [44] Litu Rout, Yujia Chen, Nataniel Ruiz, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng Chu. Semantic image inversion and editing using rectified stochastic differential equations. arXiv preprint arXiv:2410.10792, 2024. 6, 7, Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. NeurIPS, 35:25278 25294, 2022. 7 [46] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 3, 4, 6 [47] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 3 [48] Siao Tang, Xin Wang, Hong Chen, Chaoyu Guan, Zewen Wu, Yansong Tang, and Wenwu Zhu. Post-training quantization with progressive calibration and activation relaxing for text-to-image diffusion models. In ECCV, pages 404420, 2024. 2 [49] Yoad Tewel, Rinon Gal, Dvir Samuel, Yuval Atzmon, Lior Wolf, and Gal Chechik. Add-it: Training-free object insertion in images with pretrained diffusion models. arXiv preprint arXiv:2411.07232, 2024. 2 [50] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Plug-and-play diffusion features for text-driven In CVPR, pages 19211930, Dekel. image-to-image translation. 2023. [51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. page 60006010, 2017. 4 [52] Changyuan Wang, Ziwei Wang, Xiuwei Xu, Yansong Tang, Jie Zhou, and Jiwen Lu. Towards accurate post-training quantization for diffusion models. In CVPR, pages 16026 16035, 2024. 2 [53] Jiangshan Wang, Junfu Pu, Zhongang Qi, Jiayi Guo, Yue Ma, Nisha Huang, Yuxin Chen, Xiu Li, and Ying Shan. Taming rectified flow for inversion and editing. arXiv preprint arXiv:2411.04746, 2024. 2, 6, 7, 8 [54] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. 3 [55] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for textto-image generation. NeurIPS, 36:1590315935, 2023. 7, 8 [56] Sihan Xu, Yidong Huang, Jiayi Pan, Ziqiao Ma, and Joyce Chai. Inversion-free image editing with language-guided diffusion models. In CVPR, pages 94529461, 2024. 2, 5, [57] Yu Xu, Fan Tang, Juan Cao, Yuxin Zhang, Xiaoyu Kong, Jintao Li, Oliver Deussen, and Tong-Yee Lee. Headrouter: training-free image editing framework for mmdits by adaptively routing attention heads. arXiv preprint arXiv:2411.15034, 2024. 2 [58] Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Hengshuang Zhao, and Philip HS Torr. Lavt: Language-aware vision transformer for referring image segmentation. In CVPR, pages 1815518165, 2022. 3 [45] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo [59] Zhao Yang, Jiaqi Wang, Xubing Ye, Yansong Tang, Kai Chen, Hengshuang Zhao, and Philip HS Torr. Languageaware vision transformer for referring segmentation. TPAMI, 2024. 3 [60] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong Tang. Atp-llava: Adaptive token pruning for large vision language models. arXiv preprint arXiv:2412.00447, 2024. [61] Xubing Ye, Yukang Gan, Xiaoke Huang, Yixiao Ge, Ying Shan, and Yansong Tang. Voco-llama: Towards vision compression with large language models. arXiv preprint arXiv:2406.12275, 2024. [62] Haoji Zhang, Yiqin Wang, Yansong Tang, Yong Liu, Jiashi Feng, Jifeng Dai, and Xiaojie Jin. Flash-vstream: Memorybased real-time understanding for long video streams. arXiv preprint arXiv:2406.08085, 2024. 3 [63] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep In CVPR, pages 586595, features as perceptual metric. 2018. 7 [64] Yixuan Zhu, Wenliang Zhao, Ao Li, Yansong Tang, Jie Zhou, and Jiwen Lu. Flowie: Efficient image enhancement via rectified flow. In CVPR, pages 1322, 2024. 2 [65] Junhao Zhuang, Yanhong Zeng, Wenran Liu, Chun Yuan, and Kai Chen. task is worth one word: Learning with task prompts for high-quality versatile image inpainting. In ECCV, pages 195211, 2024. 2,"
        }
    ],
    "affiliations": [
        "Institute of Artificial Intelligence (TeleAI), China Telecom",
        "Shenzhen International Graduate School, Tsinghua University"
    ]
}