{
    "paper_title": "A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models",
    "authors": [
        "Jinyi Han",
        "Xinyi Wang",
        "Haiquan Zhao",
        "Tingyun li",
        "Zishang Jiang",
        "Sihang Jiang",
        "Jiaqing Liang",
        "Xin Lin",
        "Weikang Zhou",
        "Zeye Sun",
        "Fei Yu",
        "Yanghua Xiao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in self-refinement have demonstrated significant potential for improving the outputs of large language models (LLMs) through iterative refinement. However, most existing self-refinement methods rely on a reactive process with a fixed number of iterations, making it difficult to determine the optimal timing and content of refinement based on the evolving generation context. Inspired by the way humans dynamically refine their thoughts during execution, we propose ProActive Self-Refinement (PASR), a novel method that enables LLMs to refine their outputs during the generation process. Unlike methods that regenerate entire responses, PASR proactively decides whether, when, and how to refine based on the model's internal state and evolving context. We conduct extensive experiments on a diverse set of 10 tasks to evaluate the effectiveness of PASR. Experimental results show that PASR significantly enhances problem-solving performance. In particular, on Qwen3-8B, PASR reduces average token consumption by 41.6 percent compared to standard generation, while also achieving an 8.2 percent improvement in accuracy. Our code and all baselines used in the paper are available in the GitHub."
        },
        {
            "title": "Start",
            "content": "A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models Jinyi Han, Xinyi Wang , Haiquan Zhao, Tingyun li, Zishang Jiang , Sihang Jiang , Jiaqing Liang , Xin Lin , Weikang Zhou , Zeye Sun , Fei Yu , Yanghua Xiao *, Shanghai Institute of Artificial Intelligence for Education, East China Normal University School of Data Science, Fudan University College of Computer Science and Artificial Intelligence, Fudan University Antgroup jinyihan099@gmail.com, xinywang24@m.fudan.edu.cn 5 2 0 2 8 1 ] . [ 1 3 0 9 2 1 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in self-refinement have demonstrated significant potential for improving the outputs of large language models (LLMs) through iterative refinement. However, most existing self-refinement methods rely on reactive process with fixed number of iterations, making it difficult to determine the optimal timing and content of refinement based on the evolving generation context. Inspired by the way humans dynamically refine their thoughts during execution, we propose ProActive Self-Refinement (PASR), novel method that enables LLMs to refine their outputs during the generation process. Unlike methods that regenerate entire responses, PASR proactively decides whether, when, and how to refine based on the models internal state and evolving context. We conduct extensive experiments on diverse set of 10 tasks to evaluate the effectiveness of PASR. Experimental results show that PASR significantly enhances problem-solving performance. In particular, on Qwen3-8B, PASR reduces average token consumption by 41.6% compared to standard generation, while also achieving an 8.2% improvement in accuracy. Our code and all baselines used in the paper are available in the GitHub 1."
        },
        {
            "title": "Introduction",
            "content": "Self-refinement, as fundamental cognitive capacity, is essential for effective problem-solving in both humans and models. Self-refinement is characterized by the active monitoring of ones thought processes, the identification and subsequent remediation of errors, and the iterative adjustment of responses and behaviors (Dewey, 1986)(Kuhl and Beckmann, 2012). Its significance in human intelligence highlights promising direction for developing more autonomous and robust AI agents (Tong * Corresponding authors 1https://github.com/JinyiHan99/Proactive-Self-Refine-inLLMs/ 1 Figure 1: Comparison between the post-hoc refinement method (left) and our proposed PASR (right). The post-hoc refinement method iteratively refines its initial answer. In contrast, PASR proactively refines its reasoning process during the generation. et al., 2024)(Xie et al., 2025)(An et al., 2024). Inspired by this powerful cognitive process, recent work has applied the self-refinement to Large Language Models (LLMs). Existing LLM self-refinement methods typically follow patch-after-failure (post-hoc) paradigm, where an initial response is generated and then iteratively improved based on feedback through multiple rounds of refinement iterations(Madaan et al., 2023)(Welleck et al., 2023)(Huang et al., 2024)(Ganguli et al., 2023a). Broadly, these methods fall into two categories. The first employs carefully crafted prompts to elicit self-refinement behaviors, often by explicitly instructing it to correct or refine its previous outputs (Ganguli et al., 2023b)(Olausson et al., 2024)(Olausson et al., 2023a). The second leverages supervised finetuning on synthetic datasets that pair suboptimal responses with improved versions, training the model to refine its outputs automatically (Havrilla et al., 2024)(Du et al., 2025). While these post-hoc self-refinement methods have demonstrated the performance gains across various tasks, they still lack the ability to proactively determine whether, when and how to perform refinement. (Whether:) these methods are often applied in blind, static manner after initial generation, regardless of whether refinement is necessary. This delayed intervention frequently requires multiple iterative steps to yield meaningful improvement, yet the optimal number of iterations is neither predefined nor easily inferred, often requiring extensive empirical tuning (Du et al., 2025)(Madaan et al., 2023). (When:) once an error or deviation is introduced during the initial generation and is not properly addressed, it can propagate throughout subsequent steps (Gan et al., 2025)(Bachmann and Nagarajan, 2024), making effective recovery significantly more challenging. (How:) these methods also rely heavily on external feedback mechanisms, such as tool-assisted evaluations and auxiliary models (Gou et al., 2024)(Xie et al., 2025)(Chen et al., 2024b), to identify and correct errors. (Huang et al., 2024) demonstrates that without appropriate external feedback, selfrefinement loops even lead to performance degradation. We argue that it is indispensable to enhance the capability of LLMs to perform proactive selfrefinement during the generation process, enabling models to autonomously determine the appropriate timing and content for refinement based on the evolving context. However, even advanced LLMs equipped with strong deep thinking capabilities, such as DeepSeek R1 (Guo et al., 2025) and O1 2, still struggle to achieve satisfactory proactive self-refinement. Although their reasoning process involve various meta-cognitive functions such as planning (Song et al., 2023)(Dagan et al., 2023) and evaluation (Gu et al., 2024)(Li et al., 2024), they lack dedicated mechanisms optimized for proactive self-refinement. As result, these models often bring superficial self-refinement (Liu et al., 2025) and fall into cognitive dilemmas, exhibiting patterns of overthinking (Shen et al., 2025)(Chen et al., 2024a) and underthinking (Wang et al., 2025b), as widely observed in recent studies. To equipping the model with such proactive selfimprovement capability, straightforward method is to perform supervised training on data that demonstrates adaptive refinement behavior. However, this method faces two significant challenges. First, constructing such demonstration data is nontrivial, as it is impractical to define criteria for the optimal timing for refinement during the generation. It is impractical to distill from advanced LLMs. Furthermore, simply imitating such data is insufficient for the model to truly acquire this 2https://openai.com/o1/ capability (Kumar et al., 2025)(Wang et al., 2025a). The model struggles to generalize adaptive selfrefinement behavior to unseen tasks, and in some instances, its performance even degrades. Therefore, we propose ProActive SelfRefinement (PASR), Reinforcement Learning (RL) method to train LLMs to refine their outputs adaptively during generation. The difference between pos-hoc refinement and PASR is show in Figure 1. PASR leverages on-policy rollouts from the learner model to explore whether, when and how to refine, conditioned on the task and generation state, rather than relying on predefined rules or manually designed refinement positions. Different from supervised learning, RL relies heavily on the reward signals to shape the models behavior (Lee et al., 2024)(Yuan et al., 2024). key challenge is defining what counts as an effective refinement. If the rewards are misaligned, the model may either miss important opportunities for refinement or make unnecessary refinements to already good outputs. Therefore, we introduce proxy evaluation strategy that compares the refinements relative to the standard outputs, encouraging timely, necessary, and contextually appropriate refinement. In summary, our main contributions are summarized as follows: (1) To the best of our knowledge, we are the first to propose enhancing proactive selfrefinement as formal task, aiming to equip LLMs with the ability to refine their outputs in an dynamic and self-directed manner during generation. (2) We propose PASR, method that enables proactive self-refinement throughout the generation process via reinforcement learning. (3) We design comparison-based reward strategy to assess the effectiveness of proactive self-refinement and guide model behavior during training. (4) We empirically demonstrate the effectiveness and efficiency of PASR across diverse set of tasks. In particular, on Qwen3-8B, PASR significantly reduces average token consumption by 41.6% compared to the standard generation method, while also achieving 8.2% improvement in accuracy."
        },
        {
            "title": "2.1 Task Formulation\nUnlike existing post-hoc refinement methods, our\ntask is that empowers the model to proactive self-\nrefine its generated content during the generation\nprocess. We formalize this in-process refinement\nbehavior as follows:",
            "content": "2 Error Correction. Fixing factual inaccuracies, logical fallacies, or computational mistakes introduced in earlier outputs. Information Complement. Filling in missing yet critical details to ensure completeness and correctness. Solution Improvement. Improving the effectiveness and efficiency of the proposed solution by introducing more advanced strategies or refined representations. Task Alignment. Re-aligning content with the task goal or user intent when divergence is detected. The model proactively decides whether, when and how to refine previously generated parts of its internal reasoning trace, integrating these updates into its ongoing generation process. This sequential decision-making problem is naturally formulated as Markov Decision Process (MDP) (Bellman, 1957). Formally, given an input query x, the goal is to generate final response . This is achieved through an iterative refinement process that constructs an intermediate generation trace = (z1, z2, . . . , zT ), where is the total number of generation tokens. At each timestep (from 1 to ), the model is in the state si, which is determined by the input and the trace generated z{1:i1} so far. It then takes an action ai chosen from an action space A, which consists of two main types of actions: Content Generation agen and Trace Refinement arefine. The Content Generation extends the current trace z{1:i1} by appending new token, segment, or reasoning step, resulting in z{1:i}. The Trace Refinement detects potential weaknesses in the current trace z{1:i1} and generates additional content to address them. The refinement content z{i1:i} is appended to the existing trace, forming the updated trace z{1:i1}. The sequence of states, actions, and resulting trace segments ((s1, a1, z1), . . . , (sT , aT , zT )) constitutes an observation. The final response is derived from the complete trace z. The training objective is to learn the optimal policy π that maximizes the expected reward of proactive refinement responses. The reward, denoted as Ry , reflects the quality of the response resulting from proactive trace refinement. The objective is formalized as:"
        },
        {
            "title": "2.2 PASR: ProActive Self-Refinement via RL",
            "content": "In this work, we employ Group Relative Policy Optimization (GRPO) algorithm, variant of Proximal Policy Optimization (PPO), specifically designed to stabilize learning through group-wise advantage normalization. By normalizing advantages within groups of responses generated from the same input, GRPO reduces variance in the policy gradient updates and promotes stable learning. Let πθ represent the current policy parameterized by θ. For each query x, we obtain set of candidate responses through policy rollout, forming group Gx = {(y ) i, Ry consists of sampled response and its corresponding reward score. )}. Each (y ), , (y n, Ry 1, Ry 1 To normalize the advantage within each group Gx, we compute the normalized advantage Ai(y ix) for each response as follows: Ai(y ix) = µx Ry σx + ξ , (2) where µx and σx are the mean and standard deviation of reward scores within group Gx, and ξ is small constant added for numerical stability to avoid division by zero. The GRPO objective function JGRP O(θ) is formulated to balance reward maximization and policy stability, which is defined as: JGRPO(θ) = ExDEaiπθ (x) (cid:20) 1 (cid:88) i= Ai(y ix) (cid:16) ri, clip(ri, 1 ϵ, 1 + ϵ) (cid:17) min βDKL(πθ(x)πref(x)) (cid:21) (3) x) x) where ri = πθ(y , πold is the policy before πold(y the update. ϵ is hyperparameter controlling the clipping range, and β is weight coefficient for the KL divergence penalty. The KL divergence term, x) DKL(πθπref ) = πref (y πθ(y x) enforces proximity to reference policy πref , thus preventing excessive policy shifts and mitigating the risk of over-optimization. πref (y πθ(y x) x) log 1, (cid:19) (cid:18) PASR Rollout. To enable the model to autonomously determine both whether, when and how to perform refinement during the generation process, we first design structured output format guided by system prompt. The prompt is shown in Figure 11. max π (cid:88) Eyπ(x) (cid:105) (cid:104) Ry (1) The system prompt explicitly instructs the model to format its output using three specialized tags: 3 Figure 2: Answer format used in PASR (Left). Reward design for generated answer reward is computed as the sum of the format score, accuracy score, and refinement score, as defined in Equation 7. during training (Right). The total <think>, <refine> and <answer>, which denote the reasoning trajectory, the refinement segments, and the final response, respectively. The <think> tag encapsulates the entire reasoning trajectory. Within this reasoning scope, the <refine> tag identifies specific segments where the model is expected to revise and improve previously generated content. Importantly, the <refine> tag required to be nested within the <think> tag, indicating that refinement is an integral part of the models reasoning process. After each <refine> segment, the model continues its reasoning based on the updated content, allowing refinements to directly influence subsequent inference steps. The model is encouraged to perform recursive refinement, allowing it to invoke the <refine> action multiple times within single generation when it deems such actions beneficial for improving output quality. The introduction of these special tags imposes semantically structured format on the generation process, guiding the model to focus on each phase of generation, including reasoning, refinement, and final response, with explicit functional roles. The refinement answer format of PASR is shown in Figure 2."
        },
        {
            "title": "2.3 Reward Design\nRule-based reward mechanisms have demonstrated\nstrong empirical performance and are widely\nadopted in RL settings (Dao and Vu, 2025)(Shao\net al., 2024). In our training framework, we employ\na hybrid reward scheme that incorporate both rule-\nbased and model evaluation mechanisms to guide\nthe model’s generation and refinement behavior.\nSpecifically, we define three types of rewards: a\nformat reward rformat, an accuracy reward racc and\na refinement reward rrefine.",
            "content": "Format Reward. This reward evaluates whether the generated output adheres to predefined structural constraints (as illustrated in Figure 2). The constraints are formally specified as follows: Constraint 1 (C1): the output must include both <think> and <answer> tag pairs; the <refine> tag is optional. Constraint 2 (C2): if the <refine> tag appears, it must be properly nested within the <think> tag. Constraint 3 (C3): the relative order of the three tags must be preserved and cannot be rearranged. Let Ci(y 0, 1) be Boolean function indicating whether condition Ci is satisfied for given output . The format reward rformat(y) is then defined as: rf ormat(y ) = 2(C1(y ) C2(y ) C3(y )) 1 (4) This formulation assigns reward of 1 if and only if all structural constraints are satisfied; otherwise, penalty of -1 is applied. This strict binary scheme ensures that only fully well-formed outputs are positively reinforced. Accuracy Reward It is designed to evaluate the quality and correctness of PASRs generated answers. As our training tasks are drawn from open-domain question, many of which are inherently ambiguous or under-specified. Consequently, the models outputs are often diverse and expressed in free-form language, making evaluation methods,such as rule-based checks or exact string matching, ineffective. To address this issue, we follow the method used in prior work (Zheng et al., 2023) and employ another advanced LLM as the judge model. The evaluation model is prompted with three components: the original question x, the generated answer and oracle answer ˆy. The judge model then outputs continuous score in the range [0, 1], reflecting 4 the semantic quality and task relevance of the generated response relative to the reference. Let denote the judgment function instantiated by the LLM evaluator, then the accuracy reward racc(y) is defined as: racc(y ) = (x, ˆy, ) (5) Refinement Reward. It is used to assess whether refinement actions of are beneficial and timely. Since directly measuring the effectiveness of adaptive self-refinement remains challenging, we instead employ proxy evaluation strategy that assesses refinement quality by comparing the refined response with set of standard responses without refinement. Given the stochastic nature of the models generation, we sample multiple standard responses to estimate the expected accuracy of the model, denoted as racc(y). The refinement reward is designed according to the follows principles: Reward effective refinements. positive reward is given when the refined response achieves significantly higher accuracy than the average of standard responses. Penalize harmful refinements. negative reward is assigned if the refinement results in lower accuracy than the baseline average. Discourage unnecessary refinements. When the refined response yields comparable accuracy to the average, small penalty is applied to discourage redundant changes. Specifically, the refinement reward is then defined as: rref ine(y ) = 1, racc(y) > racc(y) + ζ 1, racc(y) < racc(y) ζ 0.5, racc(y) racc(y) ζ (6) Here, ζ is the tolerance parameter that provides robustness against noise and minor fluctuations. This formulation encourages the model to refine its output only when the refinement yields measurable gain, while penalizing ineffective or unnecessary modifications. Overall Reward. The final reward for each response generated by πθ is computed as the sum of the three components. Ry = rf ormat(y ) + racc(y ) + rref ine(y ) (7) Unlike prior approaches that rely solely on binary reward signals, our fine-grained reward is designed to encourage meaningful and constructive refinement while explicitly discouraging both excessive and insufficient refinement."
        },
        {
            "title": "3 Experiments\n3.1 Setup\nBenchmarks and Metrics. We evaluate general-\nization of PASR across ten datasets covering di-\nverse tasks. For general knowledge evaluation, we\nuse MMLU (Hendrycks et al., 2021a). DROP (Dua\net al., 2019) is included to assess multi-hop and\ncomprehensive reasoning. Mathematical reasoning\nis evaluated using GSM8K (Cobbe et al., 2021),\nMATH (Hendrycks et al., 2021b), and AIME24 3.\nTo test complex reasoning abilities, we adapt ARC\n4 and GPQA 5. Winogrande (Wino) (Sakaguchi\net al., 2021) and CommonsenseQA (CSQA) (Tal-\nmor et al., 2019) are used for knowledge-based rea-\nsoning. For summarization, we use XSum dataset\n6. Accuracy is used as the evaluation metric for all\ndatasets except XSum, for which we report similar-\nity scores.",
            "content": "Baselines. We use Qwen2.5-7B (Qwen et al., 2025) and Qwen3-8B7 as the backbone models, and compare PASR against several existing methods designed to induce self-improvement or selfcorrection abilities in LLMs. The baselines include: (1) Self-refine (Shinn et al., 2023): Prompts base model to critique and iteratively revise its own responses in single-turn format. (2) Self-refine+ (with oracle) (Madaan et al., 2023): An enhanced version of Self-Refine, where the model leverages ground truth answers to identify and revise errors after generating an initial response. (3) PTR (Du et al., 2025): Constructs progressive selfrefinement dataset and applies instruction tuning to enable multi-turn, answer-level refinement. (4) SCoRe (Kumar et al., 2025): Employs multi-turn reinforcement learning framework to train LLMs to self-correct without relying on oracle feedback. (5) STaR (Zelikman et al., 2022): Uses few-shot prompting to generate rationales for multiple questions. If the answer is incorrect, the rationale is regenerated using the correct answer. The model is iteratively fine-tuned on rationales that lead to correct outcomes. (6) ISC (Han et al., 2024): Builds self-correction dataset and applies instruction tuning to train the models intrinsic self-correction (7) ability to detect and amend its own errors. RISE (Qu et al., 2024): Creates improvement trajectories showing how model can refine its own 3https://huggingface.co/datasets/math-ai/aime24 4https://huggingface.co/datasets/allenai/ai2_arc 5https://huggingface.co/datasets/Idavidrein/gpqa 6https://huggingface.co/datasets/EdinburghNLP/xsum 7https://huggingface.co/Qwen/Qwen3-8B Table 1: PASR vs. other baselines. Compared to the base model, PASR achieves an average performance improvement of 4.8% and 8.2% on the two models, respectively. The best results are highlighted in bold, and the second-best results are underlined. Vanilla and self-refine+ are excluded from the comparison. Methods Public GSM8K MATH AIME24 ARC GPQA Wino CSQA Drop MMLU Xsum Math Reasoning Knowledge Comp. Gene. Sum. Vanilla Self-Refine+(Madaan et al., 2023) NIPS23 - Self-Refine(Shinn et al., 2023) PTR(Du et al., 2025) SCoRe(Kumar et al., 2025) STaR(Zelikman et al., 2022) ISC(Han et al., 2024) RISE(Qu et al., 2024) PASR(+prompt) PASR(+IFT) PASR NIPS23 ICLR25 ICLR25 NIPS22 AAAI24 NIPS24 - - - Vanilla Self-Refine+(Madaan et al., 2023) NIPS23 - Self-Refine(Shinn et al., 2023) PTR(Du et al., 2025) SCoRe(Kumar et al., 2025) STaR(Zelikman et al., 2022) ISC(Han et al., 2024) RISE(Qu et al., 2024) PASR(+prompt) PASR(+IFT) PASR NIPS23 ICLR25 ICLR25 NIPS22 AAAI24 NIPS24 - - - 88.8 89.6 88.7 88.6 82.4 83.5 56.2 84. 79.0 89.2 88.8 91.3 94.8 90.5 88.7 91.4 72.7 23.6 92.5 60.3 91.7 94.9 Qwen2.5-7B 16.7 16.7 16.7 10.0 3.3 10.0 6.7 13. 6.7 3.3 10.0 Qwen3-8B 13.3 23.3 10.0 6.7 13.3 0.0 6.7 16.7 10.0 6.7 16.7 68.4 69.4 68.4 61.8 63.2 70.8 56.6 62. 54.4 70.8 73.6 80.2 84.4 73.0 72.0 81.2 55.2 57.2 77.4 67.8 74.6 81.4 85.3 89.0 85.3 91.0 67.2 88.3 67.6 82. 46.8 84.6 86.6 89.0 94.0 91.3 80.9 87.3 64.2 68.2 88.3 57.9 73.6 92.3 25.6 27.7 25.6 27.7 14.5 19.3 19.4 23. 22.5 23.6 29.3 25.0 43.7 29.1 32.3 36.7 26.0 29.2 33.3 29.4 35.1 24.5 64.7 73.8 64.1 59.0 48.1 53.7 56.3 60. 34.8 62.4 57.0 64.5 83.0 76.8 66.1 70.7 55.3 63.5 70.8 60.4 68.7 80.0 62.8 67.5 62.3 75.3 46.4 19.4 50.1 74. 30.3 65.4 67.0 66.3 83.5 75.8 46.4 63.9 28.8 28.3 37.2 74.3 29.3 79.6 78.6 80.2 78.6 75.7 65.8 72.2 57.8 73. 70.6 77.3 79.6 71.2 85.0 80.8 65.5 78.9 49.5 42.5 82.4 75.1 73.5 85.3 46.0 63.0 49.0 74.0 56.0 47.0 35.0 45. 34.0 51.0 75.0 72.0 85.0 73.0 53.0 72.0 22.0 28.0 44.0 52.0 36.0 83.0 31.6 56.2 36.0 50.4 35.0 32.9 31.5 56. 23.1 42.0 49.9 36.3 51.1 50.2 33.7 45.0 13.7 38.3 49.3 26.6 36.3 53.0 Avg 56.9 63. 57.5 61.6 48.2 49.7 43.7 57.7 40.2 57.0 61.7 60.9 72.8 65.0 54.5 64.0 38.7 38.6 59.2 51.4 52.6 69.1 responses under its own distribution, and fine-tunes the model on these recursive rollouts. Detailed descriptions of the prompts, important parameters and implementation settings for all baselines are shown in the Appendix A."
        },
        {
            "title": "3.2 Main Results\n3.2.1 Performance Analysis of PASR",
            "content": "Unlike prior approaches that perform refinement only after the generation is complete, PASR refines answers adaptively during the generation process. To evaluate its effectiveness, we conduct experiments across diverse set of tasks, with focus on generalization capability. For fair comparison, we re-implement representative baselines that are only trained on specific domains under the same training data. The results are shown in Table 1. PASR consistently outperforms baseline models, with particularly notable gains on more challenging tasks. For example, on the Qwen2.5-7B model evaluated with the MATH dataset, PASR yields 5.2% improvement in accuracy compared to the standard method. Similarly, on the Qwen38B model tested with the Drop dataset, PASR achieves 14.1% accuracy gain over the standard method. These results suggest that PASR, is capable of dynamically detecting and correcting reasoning errors, leading to effective and domain-agnostic performance gains. PASR achieves high performance without relying on external feedback or task-specific supervision. Our experiments show that Self-refine, without any oracle hint from the environment or human feedback, it leads to degradation in performance across all models. Only when oracle feedback is available to assist refinement, the selfrefine+ provides the performance boost. This highlights the limitation of the self-refine structure in effectively improving model performance without external guidance , which is also observed in (Kumar et al., 2025)(Qu et al., 2024). However, external supervision signals are often difficult to obtain and introduce additional costs. In contrast, PASR performs self-refinement autonomously, relying solely on intrinsic, self-adaptive decisions made during the generation process. PASR demonstrates strong generalization capabilities. PASR is trained on general tasks and evaluated on domain-specific datasets to assess its generalization ability. Despite this domain shift, PASR achieves the best average performance compared to other self-refinement methods. While PASR does not always outperform all baselines on every individual dataset. For instance, its performance on Qwen2.5-7B is slightly lower on certain Figure 3: Comparison of token usage across different methods on various tasks. Values represent the average output length of the model on each dataset. The left figure uses the Qwen3-8B backbone, while the right figure uses Qwen2.5-7B. domain-specific tasks. This outcome is expected and understandable. Domain-specific tasks often require specialized knowledge or exhibit distributional characteristics not present in the training data. Moreover, we observe that the effectiveness of PASR can also vary with the underlying model. Compared to the more advanced Qwen38B, Qwen2.5-7B appears to exhibit relatively weaker ability to leverage the learned proactive selfrefinement mechanism. This suggests that stronger base models provide are fundamental to proactive self-refinement capability."
        },
        {
            "title": "3.2.2 Efficiency Analysis of PASR",
            "content": "PASR optimizes the output quality with minimal additional token overhead. We compare token consumption across different baselines, as illustrated in Figure 3. Compared to standard decoding method, PASR achieves notable accuracy gains with only slight increase in token usage. This highlights its ability to enhance outputs through targeted, dynamic refinements rather than full rewrites, making it cost-efficient refinement method. Specifically, on the Qwen2.5-7B, PASR yields 4.8% absolute performance improvement with only an 8.4% increase in token consumption compared to standard generation. Additionally, while PASR and PTR achieve comparable performance on Qwen2.5-7B, PTR incurs significantly higher token costs. The performance gain of PTR mainly stems from the use of highquality, answer-level refinement data. However, the effectiveness of this data diminishes considerably on Qwen3-8B. However, PTR regenerates entire answers at each refinement step, resulting in substantial token overhead. 3.3 In-depth Analysis"
        },
        {
            "title": "3.3.1 Does PASR genuinely exhibit proactive\nrefinement capabilities during\ngeneration?",
            "content": "We investigate whether PASR performs proactive refinement during the generation process rather than passively correcting outputs after completion. To validate this, we conduct quantitative analysis from three complementary perspectives: (1) whether PASR performs refinement at appropriate moments; (2) whether the refinement behavior modifies earlier reasoning steps or simply regenerates content; (3) whether these refinements contribute causally to improving the final output quality. The prompts used in this subsection are shown in Figure 16 and 17. The results are shown in the Figure 4. PASR autonomously determine when to refine. We randomly sample 384 questions, among which 267 are initially answered incorrectly by the base model. PASR does not refine all answers indiscriminately; instead, it selectively triggers refinement. Among the 267 incorrect answers, 235 are revised and corrected by PASR. While many originally correct answers nearly remain unchanged. This indicates that PASR is able to identify and act upon potentially flawed generations when refinement is necessary. PASR shows high coherence between preand post-refinement outputs. We randomly sample 300 answers and employ an independent LLM, Qwen2.5-32B-Instruct, to evaluate their semantic consistency before and after refinement. Each sample is scored multiple times within in [0, 1]to ensure the reliability of the assessment. The results indicate that nearly 80% of samples received semantic consistency score exceeding 0.9. 7 Figure 4: From left to right, the pie charts show: (1) the proportion of answers changed by PASR refinement, (2) the distribution of coherence scores reflecting how well the self-refinement builds upon the initial generation, and, and (3) the distribution of alignment scores measuring the consistency between the refinement process and the final answer. For (2) and (3), each segment represents the proportion of examples falling within specific score range (e.g., [00.45), [0.450.85), [0.851.0]). PASRs proactive self-refinement process contributes to the answer correctness. We further analyze the 300 samples mentioned above to evaluate the alignment between the refinement process and the final answer. Over 85% of the samples achieved alignment score above 0.9, indicating that refinement leads to the quality of outputs. 3.3.2 What makes PASR effective? Reinforcement learning enables the model to perform proactive self-refinement. In contrast, prompt-based or supervised signals are insufficient to elicit proactive refinement capabilities. We explore whether proactive self-refinement can be induced via prompting. The results are shown in Table 1. When the model is explicitly instructed to self-refine during generation via prompt design (PASR+prompt), we observe consistent performance decline across all tasks, with an average decrease of 16.9% and 9.5% on two backbone models. It indicates that prompt-based guidance alone is insufficient to elicit the models adaptive selfrefinement capability. Similarly, we apply instruction-following finetuning (PASR+IFT) to inject this capability. However, the model shows limited generalization to unseen tasks. On the Qwen3-8B model, performance drops by 8.3% compared to the base version. These results suggest that proactive self-refinement is not an innate capability and cannot be effectively acquired through supervised fine-tuning. Comparison-based rewards setting help the model learn to perform effective refinements. We use Qwen2.5-7B as the backbone and evaluate two alternative reward strategies. The first is Single-reference comparison (w/o multi-answer), computes refinement rewards by comparing the refined output to single standard answer. The second is Refinement-triggered reward (w/o comparison), assigns coarse positive refinement reward whenever refinement action is taken, regardless of its necessity or effectiveness. The results are shown in Table 2. Although both alternative strategies show moderate performance, they consistently under perform compared to our proposed reward design. Our method computes the refinement reward by comparing the refined output to the average score across multiple standard answers, providing more stable and reliable evaluation. This reward strategy offers several key advantages. First, averaging over multiple standard answers reduces the variance introduced by the randomness of LLM outputs. It provides more robust and stable learning signal for guiding meaningful refinements during training. This strategy enables the model to better recognize when refinement yields genuine improvement. Moreover, coarse-grained reward signals are easily exploited by the model, leading to unnecessary refinement in pursuit of high reward (i.e., reward hacking). In contrast, our comparison-based signal avoids this by rewarding only measurable improvements, leading to more targeted and meaningful refinements."
        },
        {
            "title": "4 Related Work",
            "content": "prompt-based self-refinement. Prior work on selfrefinement typically follows two-stage paradigm. The model first generates an initial response and is then prompted to refine or improve it (Ganguli et al., 2023b). These methods have seen widespread use in complex reasoning tasks, including math (Weng et al., 2023)(Wang et al., 2024) and code generation (Olausson et al., 2023b)(Olausson et al., 2024)(Olausson et al., 2023a). However, 8 Table 2: PASR performance across datasets under different refinement reward signals. The comparison-based fine-grained reward better guides the model to learn adaptive and meaningful refinements. Dataset PASR w/o multi-answer w/o comparison MMLU Drop Xsum GSM8K MATH AIME24 ARC GPQA Wino CSQA 75.0 79.6 49.9 88.8 73.6 10.0 86.6 29.3 57.0 67. 71.0 (-4.0) 76.7 (-2.9) 44.3 (-5.6) 75.7 (-13.1) 62.2 (-11.4) 10.0 (+0.0) 83.9 (-2.7) 28.9 (-0.4) 53.4 (-3.6) 65.9 (-1.1) 53.0 (-22.0) 78.6 (-1.0) 31.9 (-18.0) 86.0 (-2.8) 62.2 (-11.4) 10.0 (+0.0) 82.9 (-3.7) 27.4 (-1.9) 65.3 (+8.3) 64.9 (-2.1)"
        },
        {
            "title": "AVG",
            "content": "61.7 57.2 (-4.5) 56.2 (-5.5) simply prompting model to refine its own output does not consistently yield better results, and there is little evidence that prompting alone is sufficient for reliable self-improvement(Huang et al., 2024)(Tyen et al., 2024). Success in these settings often relies on the availability of ground truth feedback or external supervision, such as explicit information about the error, its location, and an explanation of why it is wrong (Kim et al., 2023)(Shinn et al., 2023). Unfortunately, such fine-grained feedback is rarely accessible in practical applications (Gou et al., 2024)(Pan et al., 2024). Therefore, some studies utilize stronger models or train auxiliary teacher models to evaluate outputs and provide feedback (Xie et al., 2025)(Madaan et al., 2023)(Uesato et al., 2023)(Welleck et al., 2023). While effective, these approaches usually require task-specific annotations to train the feedback models, which significantly increases the cost and limits scalability across diverse tasks (Du et al., 2025). Fine-tuning for self-refinement. Another line of work focuses on supervised fine-tuning using synthetic self-refinement data. In these settings, initial answers are generated by one model, while refined answers are produced by stronger model or taken from oracle answers (Havrilla et al., 2024)(Du et al., 2025)(Han et al., 2024) (Xie et al., 2025). The resulting pairs of bad to good answers are used to train models to imitate the refinement process. However, such methods suffer from either distributional mismatch, where the errors in training data do not reflect the mistakes the model makes during inference (Kang et al., 2025), or behavioral collapse, where the model learns narrow correction pattern that fails to generalize across tasks or domains (Kumar et al., 2025)(Qu et al., 2024)."
        },
        {
            "title": "References",
            "content": "Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, and Weizhu Chen. 2024. Learning from mistakes makes llm better reasoner. Preprint, arXiv:2310.20689. Gregor Bachmann and Vaishnavh Nagarajan. 2024. The pitfalls of next-token prediction. Preprint, arXiv:2403.06963. Richard Bellman. 1957. markovian decision process. Journal of mathematics and mechanics, pages 679 684. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. 2024a. Do 9 not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187. for moral self-correction in large language models. Preprint, arXiv:2302.07459. Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2024b. Teaching large language models to self-debug. In The Twelfth International Conference on Learning Representations. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. Preprint, arXiv:2110.14168. Gautier Dagan, Frank Keller, and Alex Lascarides. 2023. Dynamic planning with llm. arXiv preprint arXiv:2308.06391. Alan Dao and Dinh Bach Vu. 2025. Alphamaze: Enhancing large language models spatial intelligence via grpo. arXiv preprint arXiv:2502.14669. John Dewey. 1986. Experience and education. In The educational forum, volume 50, pages 241252. Taylor & Francis. Chengyu Du, Jinyi Han, Yizhou Ying, Aili Chen, Qianyu He, Haokun Zhao, Sirui Xia, Haoran Guo, Jiaqing Liang, Zulong Chen, et al. 2025. Think thrice before you act: Progressive thought refinement in large language models. In The Twelfth International Conference on Learning Representations. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 23682378, Minneapolis, Minnesota. Association for Computational Linguistics. Zeyu Gan, Yun Liao, and Yong Liu. 2025. Rethinking external slow-thinking: From snowball errors to probability of correct reasoning. Preprint, arXiv:2501.15602. Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas I. Liao, Kamile Lukošiute, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, Dawn Drain, Dustin Li, Eli TranJohnson, Ethan Perez, Jackson Kernion, Jamie Kerr, Jared Mueller, Joshua Landau, Kamal Ndousse, Karina Nguyen, Liane Lovitt, Michael Sellitto, Nelson Elhage, Noemi Mercado, Nova DasSarma, Oliver Rausch, Robert Lasenby, Robin Larson, Sam Ringer, Sandipan Kundu, Saurav Kadavath, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, Christopher Olah, Jack Clark, Samuel R. Bowman, and Jared Kaplan. 2023a. The capacity Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas I. Liao, Kamile Lukošiute, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, Dawn Drain, Dustin Li, Eli TranJohnson, Ethan Perez, Jackson Kernion, Jamie Kerr, Jared Mueller, Joshua Landau, Kamal Ndousse, Karina Nguyen, Liane Lovitt, Michael Sellitto, Nelson Elhage, Noemi Mercado, Nova DasSarma, Oliver Rausch, Robert Lasenby, Robin Larson, Sam Ringer, Sandipan Kundu, Saurav Kadavath, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, Christopher Olah, Jack Clark, Samuel R. Bowman, and Jared Kaplan. 2023b. The capacity for moral self-correction in large language models. Preprint, arXiv:2302.07459. Zhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu Yang, Nan Duan, and Weizhu Chen. 2024. CRITIC: Large language models can self-correct with tool-interactive critiquing. In The Twelfth International Conference on Learning Representations. Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. 2024. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Haixia Han, Jiaqing Liang, Jie Shi, Qianyu He, and Yanghua Xiao. 2024. Small language model can self-correct. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 18162 18170. Alex Havrilla, Sharath Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravynski, Eric Hambro, and Roberta Raileanu. 2024. Glore: when, where, and how to improve llm reasoning via global and local refinements. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021a. Measuring massive multitask language understanding. In International Conference on Learning Representations. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021b. Measuring mathematical problem solving with the MATH dataset. In Thirtyfifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). 10 Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2024. Large language In The models cannot self-correct reasoning yet. Twelfth International Conference on Learning Representations. Katie Kang, Eric Wallace, Claire Tomlin, Aviral Kumar, and Sergey Levine. 2025. Unfamiliar finetuning examples control how language models hallucinate. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 36003612, Albuquerque, New Mexico. Association for Computational Linguistics. Geunwoo Kim, Pierre Baldi, and Stephen McAleer. 2023. Language models can solve computer tasks. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, Red Hook, NY, USA. Curran Associates Inc. Julius Kuhl and Jürgen Beckmann. 2012. Action control: From cognition to behavior. Springer Science & Business Media. Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. 2025. Training language models to self-correct via reinIn The Twelfth International forcement learning. Conference on Learning Representations. Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Ren Lu, Thomas Mesnard, Johan Ferret, Colton Bishop, Ethan Hall, Victor Carbune, and Abhinav Rastogi. 2024. RLAIF: Scaling reinforcement learning from human feedback with AI feedback. Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, et al. 2024. From generation to judgment: Opportunities and challenges of llm-as-a-judge. arXiv preprint arXiv:2411.16594. Zichen Liu, Changyu Chen, Wenjun Li, Tianyu Pang, Chao Du, and Min Lin. 2025. There may not be aha moment in r1-zero-like training pilot study. https://oatllm.notion.site/oat-zero. Notion Blog. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: iterative refinement with self-feedback. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, Red Hook, NY, USA. Curran Associates Inc. 2023a. Demystifying gpt self-repair for code generation. CoRR, abs/2306.09896. Theo X. Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando Solar-Lezama. 2023b. Demystifying gpt self-repair for code generation. CoRR, abs/2306.09896. Theo X. Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando Solar-Lezama. 2024. Is self-repair silver bullet for code generation? In The Twelfth International Conference on Learning Representations. Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. 2024. Automatically correcting large language models: Surveying the landscape of diverse automated correction strategies. Transactions of the Association for Computational Linguistics, 12:484506. Yuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. 2024. Recursive introspection: Teaching lanIn The guage model agents how to self-improve. Thirty-eighth Annual Conference on Neural Information Processing Systems. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: an adversarial winograd schema challenge at scale. Commun. ACM, 64(9):99106. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Preprint, arXiv:2402.03300. Yi Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing Zhang, Jiangze Yan, Ning Wang, Kai Wang, and Shiguo Lian. 2025. Dast: Difficulty-adaptive slowthinking for large reasoning models. arXiv preprint arXiv:2503.04472. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652. Theo X. Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando Solar-Lezama. Chan Hee Song, Jiaman Wu, Clayton Washington, Brian Sadler, Wei-Lun Chao, and Yu Su. 2023. Llm-planner: Few-shot grounded planning for embodied agents with large language models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 29983009. In Findings of the Associawith self-verification. tion for Computational Linguistics: EMNLP 2023, pages 25502575, Singapore. Association for Computational Linguistics. Zhihui Xie, Jie chen, Liyu Chen, Weichao Mao, Jingjing Xu, and Lingpeng Kong. 2025. Teaching language models to critique via reinforcement learning. In ICLR 2025 Third Workshop on Deep Learning for Code. Jiachen Yu, Shaoning Sun, Xiaohui Hu, Jiaxu Yan, Kaidong Yu, and Xuelong Li. 2025. Improve llmas-a-judge ability as general ability. Preprint, arXiv:2502.11689. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. 2024. Self-rewarding language models. In Forty-first International Conference on Machine Learning. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. STar: Bootstrapping reasoning with reasoning. In Advances in Neural Information Processing Systems. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Preprint, arXiv:2306.05685. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41494158, Minneapolis, Minnesota. Association for Computational Linguistics. Yongqi Tong, Dawei Li, Sizhe Wang, Yujia Wang, Fei Teng, and Jingbo Shang. 2024. Can LLMs learn from previous mistakes? investigating LLMs errors to boost for reasoning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3065 3080, Bangkok, Thailand. Association for Computational Linguistics. Gladys Tyen, Hassan Mansoor, Victor Carbune, Peter Chen, and Tony Mak. 2024. LLMs cannot find reasoning errors, but can correct them given the error location. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1389413908, Bangkok, Thailand. Association for Computational Linguistics. Jonathan Uesato, Nate Kushman, Ramana Kumar, H. Francis Song, Noah Yamamoto Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2023. Solving math word problems with processbased and outcome-based feedback. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. 2024. Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 94269439, Bangkok, Thailand. Yubo Wang, Xiang Yue, and Wenhu Chen. 2025a. Critique fine-tuning: Learning to critique is more Preprint, effective than learning to imitate. arXiv:2501.17703. Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian Yu, Juntao Li, Zhuosheng Zhang, et al. 2025b. Thoughts are all over the place: On the underthinking of o1-like llms. arXiv preprint arXiv:2501.18585. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. 2023. Generating sequences by learning to self-correct. In The Eleventh International Conference on Learning Representations. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao. 2023. Large language models are better reasoners"
        },
        {
            "title": "A Experimental Details",
            "content": "A."
        },
        {
            "title": "Implementation Details for PASR",
            "content": "Platform. All of our experiments are conducted on workstations equipped with eight NVIDIA A800 PCIe GPUs with 80GB memory, running Ubuntu 20.04.6 LTS and PyTorch 2.0.1. About the training cost, using Qwen2.5-7B as an example, we train PASR with the following setup: 2 GPUs for rollout generation, 1 GPU for policy updates, and 1 GPU for hosting the reference model server. Training for 3,000 steps takes approximately 8 hours in total. Training Data. Our training data is derived from the alpaca_evol_instruct_70k8 dataset, general instruction-following corpus. We performed thorough cleaning and filtering process based on the following criteria: (1) Removed questions with excessively long ground truth answers to maintain manageable response lengths. (2) Eliminated noise such as HTML tags, non-alphanumeric characters, and duplicate entries. (3) Applied frequency-based filtering to exclude rare or long-tail queries and lowfrequency phrases that are unlikely to contribute effectively to the models refinement capabilities. After these preprocessing steps, we obtained approximately 40,000 high-quality, open-domain query-answer pairs for training. We have release the training data in the GitHub. Important Parameters of PASR. The PASR is implemented based on the open-source GitHub repository 9. The KL divergence penalty coefficient β is set to 0.04 to balance policy improvement and deviation from the reference policy. The clipping parameter ϵ is set to 0.2. For each group, 8 answers are generated, and the training batch size is set to 2. Distributed training utilizes the DeepSpeed library with the AdamW optimizer and learning rate of 1e-6. Gradient accumulation occurs over 4 steps, and with per-GPU batch size of 2, the effective batch size is 8 NGPUs, where NGPUs denotes the number of GPUs. Mixed-precision training with BF16 is enabled. Memory optimization employs ZeRO Stage 2, with optimizer state offloading to CPU. Key ZeRO configurations include allgather partitions, an allgather bucket size of 2e8, reduce scatter, and reduce bucket size of 2e8. Contiguous gradients are en8https://huggingface.co/datasets/WizardLMTeam/ WizardLM_evol_instruct_70k/blob/main/alpaca_ evol_instruct_70k.json 9https://github.com/lsdefine/simple_GRPO Figure 5: The frequency distribution of the four refinement types in PASR. abled, communication overlap is disabled, and 16-bit weights are gathered during model saving. Training loss is logged every 5 steps. Details on the Judge Model. uring training, we employed Qwen2.5-32B-Instruct as the judge model, which has been widely adopted for assessing answer correctness (Yu et al., 2025). To ensure reliable and objective evaluation, our prompt design explicitly incorporated three elements: the question, the ground truth, and the model-generated answer. The judge model was instructed to ground its judgment on the provided ground truth rather than on subjective impressions, thereby avoiding inconsistent criteria and yielding more stable evaluations than direct answer-only comparisons. The full evaluation prompts used in both training and testing are shown in Figures 13 and 15. To verify the trustworthiness of the judge model, we randomly sampled 50 evaluation cases from the test set and performed manual verification. Each case was independently reviewed by two human annotators, who compared the generated answer against the ground truth. We observed 91% agreement rate between the judge models assessments and human judgments, confirming that the judge model provides consistent and reliable scoring. For deployment, the judge model runs on four A800 (80GB) GPUs with batch size of 8, achieving an evaluation speed of approximately 43.27 tokens per second (about 2 seconds per batch). A."
        },
        {
            "title": "Implementation Details for Baselines",
            "content": "We use the LLaMA-Factory framework10 to train all baseline methods. The key parameters are shown in the Table 4. 10https://github.com/hiyouga/LLaMA-Factory 13 Table 3: PASR vs. other baselines. Compared to the base model, PASR achieves an average performance improvement of 4.9% on Qwen2.5-14B. Methods Public GSM8K MATH AIME24 ARC GPQA Wino CSQA Drop MMLU Xsum Math Reasoning Knowledge Comp. Gene. Sum. Vanilla Self-Refine+(Madaan et al., 2023) NIPS - Self-Refine(Shinn et al., 2023) PTR(Du et al., 2025) SCoRe(Kumar et al., 2025) STaR(Zelikman et al., 2022) ISC(Han et al., 2024) NIPS23 ICLR25 ICLR25 NIPS22 AAAI24 PASR(+prompt) PASR(+IFT) PASR - - - 92.9 93. 92.3 87.6 93.3 87.0 88.1 88.7 75.0 93.6 Qwen2.5-14B 20.0 30.0 75.6 78.0 75.2 63.6 78.2 75.4 64.0 71.6 59.4 78. 20.0 10.0 10.0 6.7 23.3 26.7 23.3 30.0 89.0 92.3 89.0 86.6 86.3 87.0 77.9 78.9 86.0 88.8 38.4 46. 38.5 37.0 44.1 39.2 35.2 26.3 38.4 45.1 81.1 88.1 80.2 84.5 86.8 78.0 71.2 71.0 67.4 86.0 66.4 74. 65.7 75.3 70.5 70.2 62.9 68.0 69.0 78.3 87.5 92.3 86.9 83.7 84.6 89.5 83.7 88.5 78.9 89.9 57.0 73. 57.0 54.0 80.0 72.0 75.0 66.0 68.0 74.0 60.5 57.1 57.2 44.3 70.9 63.2 46.2 17.7 61.3 53.2 Avg 66.8 72.5 66.2 62.7 70.5 66.8 62.8 60.3 62.7 71."
        },
        {
            "title": "B Further Analysis",
            "content": "B.1 Further Performance Analysis of PASR As shown in Table 1, PASR achieves an average performance improvement of 4.8% and 8.2% on Qwen2.5-7B and Qwen3-8B, respectively, compared to standard generation across the 10 benchmarks. We further evaluate PASR on Qwen2.514B  (Table 3)  , where it consistently outperforms all baselines, achieving the highest overall accuracy with an average improvement of 4.9% over standard answers. Notably, PASR provides larger gains on models with stronger reasoning capabilities; for instance, on Qwen3-8B, it improves average accuracy by 8.2%. These results indicate that PASRs effectiveness is not merely function of model scale, but rather reflects its intrinsic ability to generalize across diverse tasks and model configurations. B.2 Refinement Behavior Analysis of PASR This experiment aims to investigate how PASR autonomously refines its outputs during generation, including the types of refinement behaviors it exhibits and the factors that limit its effectiveness. Specifically, we analyze both qualitative examples and quantitative statistics of refinement types, and examine failure cases to understand the models strengths and inherent constraints. Refinement behavior examples of PASR. In the Section 2, we define four intended refinement behaviors of PASR, including Error Correction, Information Complement, Solution Improvement, and Task Alignment. While these four categories guide the design of the system prompt during training, PASR is not explicitly instructed to follow specific type when solving tasks. Instead, the model autonomously decides the appropriate refinement behavior based on the task context. We provide concrete example for each of the four refinement types to clearly demonstrate how PASR operates. Examples are shown in Figure 6, Figure 7, Figure 8 and Figure 9. Statistical analysis of the four refinement types. We sample 2,678 refinement outputs from PASRs training process and used Qwen2.5-32BInstruct to classify the type of refinement performed. The prompt used is shown in Figure 10 and the results are shown in Figure 5. We find that PASR mainly performs Task Alignment and Information Complement. This pattern is related to the training data, which consists mostly of general instruction-tuning corpora. As result, the model tends to ensure task compliance and complete missing information during generation, rather than focus on structural changes or post-hoc error correction. Error Case Analysis. We conducted an analysis of PASRs failure cases to better understand its limitations. As discussed in Section 3.3. Among 267 questions initially answered incorrectly, PASR successfully corrected 235 through refinement, while 32 questions remained incorrect (Figure 4). Manual inspection of these 32 cases revealed two main reasons for failure. First, questions beyond knowledge boundaries. These involved the question outside the models existing knowledge, and selfrefinement cannot introduce new information, similar to the limitations of human self-correction. This represents an inherent limitation of current models rather than shortcoming of PASR, and identifying such cases can guide future targeted improvements. Second, limited metacognitive ability of existing LLMs. The model sometimes fails to accurately recognize or locate its own errors. This restricts the refinement process, causing it to only partially address or overlook core mistakes. 14 not yet established objective and reliable metrics to deeply and systematically analyze the refinement trajectory. In future work, we aim to design more rigorous and scientifically grounded metrics to better quantify the effectiveness of the refinement trajectory during the generation process. Summary. PASRs refinement behavior is inherently adaptive: the model chooses how and when to refine outputs based on the specific task, demonstrating the advantage of autonomous, contextaware refinement. However, self-refinement has intrinsic limitations. It cannot fully correct errors that stem from gaps in the models knowledge or limitations in its reasoning capabilities. B.3 Discussion on How PASR Mitigates Over-thinking. Over-thinking in LLMs often results from redundant or unfocused thought processes, leading to unnecessarily long outputs. PASR alleviates this issue by enabling the model to perform more targeted and effective self-refinement, which naturally produces shorter and more purposeful responses. Empirical results support this effect. As shown in Figure 3, PASR generates consistently shorter outputs compared to other self-refinement methods. This behavior is further encouraged by our reward function, which reinforces effective refinements, penalizes detrimental ones, and discourages unnecessary refinement actions. Notably, even without explicitly penalizing output length, PASR achieves more concise reasoning by focusing on meaningful refinements, demonstrating that enhancing the quality and efficiency of self-refinement can reduce over-thinking."
        },
        {
            "title": "C Detailed Prompts",
            "content": "This section presents the detailed prompts used in our experiments, including the PASR system prompt and the evaluation prompts for different datasets, among others, as illustrated in figs. 11 to 17."
        },
        {
            "title": "D Limitations",
            "content": "Despite the promising results achieved by PASR, there still are some limitation remain. Similar to existing self-refinement methods, the performance evaluation of PASR primarily focuses on the quality of the final answers. Although we analyze PASRs refinement trajectory from three aspects to demonstrate its ability to perform genuine refinement (as shown in Section 3.3), only the change in answer correctness before and after refinement serves as the objective metric. The coherence is evaluated with the assistance of large language model, introducing degree of subjectivity. However, given the free-form nature of text generation, we have 15 Figure 6: Example of the Error Correction behavior in PASR. Figure 7: Example of the Information Complement behavior in PASR. Figure 8: Example of the Solution Improvement behavior in PASR. Table 4: Important parameters for each baseline method"
        },
        {
            "title": "RISE",
            "content": "per_device_train_batch_size: 1 gradient_accumulation_steps: 2 learning_rate: 1.0 105 num_train_epochs: 2 lr_scheduler_type: cosine warmup_ratio: 0.1 bf16: true Dataset: Public GitHub per_device_train_batch_size: 1 gradient_accumulation_steps: 4 learning_rate: 1.0 105 num_train_epochs: 2.0 lr_scheduler_type: cosine warmup_ratio: 0.1 bf16: true Dataset: preference pairs form PTR experiment per_device_train_batch_size: 1 gradient_accumulation_steps: 2 learning_rate: 1.0 105 num_train_epochs: 2 lr_scheduler_type: cosine warmup_ratio: 0.1 bf16: true Dataset: alpaca_evol_instruct_70k(filtered generated pairs)) per_device_train_batch_size: 1 gradient_accumulation_steps: 2 learning_rate: 1.0 105 num_train_epochs: 2.0 lr_scheduler_type: cosine warmup_ratio: 0.1 bf16: true Dataset: alpaca_evol_instruct_70k per_device_train_batch_size: 1 gradient_accumulation_steps: 2 learning_rate: 1.0 105 num_train_epochs: 2.0 lr_scheduler_type: cosine warmup_ratio: 0.1 bf16: True Dataset: alpaca_evol_instruct_70k PASR(+IFT) per_device_train_batch_size: 1 gradient_accumulation_steps: 2 learning_rate: 1.0 105 num_train_epochs: 2.0 lr_scheduler_type: cosine warmup_ratio: 0.1 bf16: True Dataset: good refinement paths generated during PASR training 17 Figure 9: Example of the Task Alignment behavior in PASR. Figure 10: Prompt for identifying the refinement type performed by PASR. 18 Figure 11: Prompt template for PASR. We used it to guide the LLM to perform refinement during generation, and employed another LLM to evaluate the quality of generated outputs. Figure 12: Prompt template used for PASR evaluation during training. This prompt guides the judge model in evaluating the answers generated by the model during the rollout process. 19 Figure 13: Evaluation prompt template during the test stage. We design different prompts for different types (Summary, Multi-choice and Open question) of test datasets to ensure accurate evaluation. Figure 14: Prompt template for refinement method self-refine and self-refine+. 20 Figure 15: Evaluation prompt template during the test stage. We design different prompts for MMLU, Drop, Xsum, Math type, ARC, Wino, and CommensenseQA to ensure accurate evaluation. 21 Figure 16: Prompt for evaluating the reasonableness of the refinement trajectory in PASR. This prompt is used to assess whether the model-generated answers evolve in reasonable manner throughout the refinement process. 22 Figure 17: Prompt for evaluating the alignment between the refinement process and the final answer in PASR."
        }
    ],
    "affiliations": [
        "Antgroup",
        "College of Computer Science and Artificial Intelligence, Fudan University",
        "East China Normal University",
        "School of Data Science, Fudan University",
        "Shanghai Institute of Artificial Intelligence for Education"
    ]
}