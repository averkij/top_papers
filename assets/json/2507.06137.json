{
    "paper_title": "NeoBabel: A Multilingual Open Tower for Visual Generation",
    "authors": [
        "Mohammad Mahdi Derakhshani",
        "Dheeraj Varghese",
        "Marzieh Fadaee",
        "Cees G. M. Snoek"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-to-image generation advancements have been predominantly English-centric, creating barriers for non-English speakers and perpetuating digital inequities. While existing systems rely on translation pipelines, these introduce semantic drift, computational overhead, and cultural misalignment. We introduce NeoBabel, a novel multilingual image generation framework that sets a new Pareto frontier in performance, efficiency and inclusivity, supporting six languages: English, Chinese, Dutch, French, Hindi, and Persian. The model is trained using a combination of large-scale multilingual pretraining and high-resolution instruction tuning. To evaluate its capabilities, we expand two English-only benchmarks to multilingual equivalents: m-GenEval and m-DPG. NeoBabel achieves state-of-the-art multilingual performance while retaining strong English capability, scoring 0.75 on m-GenEval and 0.68 on m-DPG. Notably, it performs on par with leading models on English tasks while outperforming them by +0.11 and +0.09 on multilingual benchmarks, even though these models are built on multilingual base LLMs. This demonstrates the effectiveness of our targeted alignment training for preserving and extending crosslingual generalization. We further introduce two new metrics to rigorously assess multilingual alignment and robustness to code-mixed prompts. Notably, NeoBabel matches or exceeds English-only models while being 2-4x smaller. We release an open toolkit, including all code, model checkpoints, a curated dataset of 124M multilingual text-image pairs, and standardized multilingual evaluation protocols, to advance inclusive AI research. Our work demonstrates that multilingual capability is not a trade-off but a catalyst for improved robustness, efficiency, and cultural fidelity in generative AI."
        },
        {
            "title": "Start",
            "content": "NeoBabel: Multilingual Open Tower for Visual Generation 5 2 0 2 8 ] . [ 1 7 3 1 6 0 . 7 0 5 2 : r Mohammad Mahdi Derakhshani2, Dheeraj Varghese2, Marzieh Fadaee1, and Cees G. M. Snoek2 1Cohere Labs, 2University of Amsterdam Corresponding authors: m.m.derakhshani@uva.nl, marzieh@cohere.com Abstract Text-to-image generation advancements have been predominantly English-centric, creating barriers for non-English speakers and perpetuating digital inequities. While existing systems rely on translation pipelines, these introduce semantic drift, computational overhead, and cultural misalignment. We introduce NeoBabel, novel multilingual image generation framework that sets new Pareto frontier in performance, efficiency and inclusivity, supporting six languages: English, Chinese, Dutch, French, Hindi, and Persian. The model is trained using combination of large-scale multilingual pretraining and high-resolution instruction tuning. To evaluate its capabilities, we expand two English-only benchmarks to multilingual equivalents: m-GenEval and m-DPG. NeoBabel achieves state-of-the-art multilingual performance while retaining strong English capability, scoring 0.75 on m-GenEval and 0.68 on m-DPG. Notably, it performs on par with leading models on English tasks while outperforming them by +0.11 and +0.09 on multilingual benchmarks, even though these models are built on multilingual base LLMs. This demonstrates the effectiveness of our targeted alignment training for preserving and extending crosslingual generalization. We further introduce two new metrics to rigorously assess multilingual alignment and robustness to code-mixed prompts. Notably, NeoBabel matches or exceeds English-only models while being 24 smaller. We release an open toolkit, including all code, model checkpoints, curated dataset of 124M multilingual text-image pairs, and standardized multilingual evaluation protocols, to advance inclusive AI research. Our work demonstrates that multilingual capability is not trade-off but catalyst for improved robustness, efficiency, and cultural fidelity in generative AI. Website https://Neo-Babel.github.io Code https://github.com/mmderakhshani/NeoBabel Models https://hf.co/mderakhshani/NeoBabel Pretraining Data https://hf.co/datasets/mderakhshani/NeoBabel-Pretrain Instruction Data https://hf.co/datasets/mderakhshani/NeoBabel-Instruct Evaluation Data https://hf.co/datasets/mderakhshani/NeoBabel-Eval Principal senior advisors. Released as preprint on July 9, 2025 1 Figure 1: NeoBabel establishes new Pareto frontier in multilingual image generation performance, efficiency, and inclusivity. Left: GenEval English-only scores show that NeoBabel matches state-of-the-art models despite being 24 smaller. Right: On our multilingual benchmark extensions, m-GenEval and m-DPG, NeoBabel outperforms the second-best model, demonstrating strong multilingual generalization. NeoBabel is fully open (weights, code, data) and supports six languages with consistent cross-lingual performance."
        },
        {
            "title": "1 Introduction",
            "content": "Recent advances in diffusion models and large-scale vision-language pretraining have revolutionized text-to-image generation, enabling the creation of high-quality images from natural language descriptions [Rombach et al., 2022; Peebles & Xie, 2023; Bao et al., 2023; Chen et al., 2024a; Xie et al., 2023; Wu et al., 2023a; Lipman et al., 2022; Xie et al., 2025a; Qin et al., 2025; Zhang et al., 2023; Seawead et al., 2025]. Despite these remarkable capabilities, the field suffers from critical limitation: an overwhelming reliance on English as the primaryand often exclusiveinput language [Ramesh et al., 2022; Xie et al., 2025b; Team, 2024]. This monolingual bias creates substantial barriers for the billions of users who communicate in other languages, fundamentally restricting global access to state-of-the-art generative AI technologies [Bassignana et al., 2025; Peppin et al., 2025]. The consequences of this linguistic limitation extend far beyond mere inconvenience. As text-to-image systems become integral to education, creative industries, art, and journalism, the lack of native multilingual support perpetuates existing digital divides and cultural inequities [Liu et al., 2023; Rege et al., 2025]. Non-English speakers are forced to navigate through translation layers that not only introduce friction but also risk losing the nuanced meanings and cultural contexts that make their creative expressions unique [Kannen et al., 2024; Friedrich et al., 2024]. Building truly multilingual models, like we do in this paper, is therefore not merely technical challenge but an ethical imperative, one that ensures equitable access to generative AI while preserving linguistic diversity and cultural authenticity in the digital age. Existing approaches to multilingual image generation typically employ translation-first strategy, converting non-English prompts to English before processing. While this appears pragmatic, it introduces cascade of problems that fundamentally compromise the user experience [Kreutzer et al., 2 2025; Li et al., 2025b; Bafna et al., 2025]. The computational overhead of chaining translation and generation models effectively doubles inference time, creating prohibitive delays for real-time applications, thereby further disadvantaging non-English speakers. Most critically, this approach suffers from semantic driftthe systematic loss of culturally specific meanings and linguistic subtleties [Cohn-Gordon & Goodman, 2019; Vanmassenhove et al., 2019; Beinborn & Choenni, 2020]. For instance consider the Dutch term gezellig which encompasses complex blend of coziness, conviviality, and belonging and has no direct English equivalent. When forced through translation, such rich cultural concepts are inevitably flattened or distorted, resulting in generated images that fail to capture the intended meaning. The fundamental issue lies deeper than mere translation accuracy [Wein & Schneider, 2023; Singh et al., 2024; Salazar et al., 2025]. Current vision-language architectures treat multilingual support as an afterthought, forcing diverse linguistic communities to conform to English-centric models rather than developing systems that natively understand and respect linguistic diversity. This design philosophy not only limits accessibility but also wastes the potential benefits of multilingual training, which could enhance model robustness, cross-cultural understanding, and generalization capabilities across different linguistic and cultural contexts [Ji et al., 2024; Faisal & Anastasopoulos, 2024; Dash et al., 2025; Shimabucoro et al., 2025]. These challenges demand paradigm shift toward native multilingual understanding in text-to-image generation. The primary obstacle remains the scarcity of high-quality, culturally annotated visual-linguistic datasets for non-English languages. Even with adequate data, significant technical barriers persist: establishing robust cross-lingual concept alignment, modeling typological variations across language families, and preserving culture-specific semantics during generation. Overcoming these limitations is critical for transitioning from mere translation-based approaches to systems with genuine multilingual competence. This paper introduces NeoBabel, novel multilingual image generation framework that represents the first scalable solution for direct text-to-image synthesis across six languages. Through meticulous curation of high-quality multilingual vision-language datasets and end-to-end training, NeoBabel establishes direct cross-lingual mappings between textual descriptions and visual outputs across all supported languages. This approach not only removes translation dependencies but also maintains crucial cultural and linguistic specificity in the generated images. Our model demonstrates that multilingual capability isnt trade-off but rather catalyst for improved model performance. Our work addresses three key questions: 1) How can we train single model to handle multiple languages effectively? 2) Does multilingual training degrade performance in high-resource languages like English? and 3) Can unified model outperform language-specific or translation-based approaches? To answer these, we introduce progressive training pipeline that combines large-scale multilingual pretraining with high-resolution instruction tuning. We evaluate NeoBabel on m-GenEval and mDPG, our multilingual extensions of GenEval [Ghosh et al., 2023] and DPG-Bench [Hu et al., 2024], and introduce two new metrics, Cross-Lingual Consistency (CLC) and Code Switching Similarity (CSS), to quantify multilingual performance. As shown in Figure 1, NeoBabel matches the performance of state-of-the-art English-only models while being 24 smaller. Here, we report English-only results for fair comparison, as prior work evaluates only in English. Furthermore, NeoBabel maintains strong generation quality in all six supported languages. For instance, on the m-GenEval benchmark, it achieves new state-of-theart score of 0.75an improvement of 0.11 over the very recent BLIP3-o 8B model (0.64) [Chen et al., 2025a]. Similarly, on m-DPG, it scores 0.68, outperforming BLIP3-o 8B by 0.09. These 3 results demonstrate that strong multilingual generation is achievable without resorting to largescale models or sacrificing output quality. To summarize, we make the following key contributions: 1. novel multilingual training framework. We introduce novel multilingual training framework that establishes new state-of-the-art performance in cross-lingual image generation. Our approach achieves language-agnostic understanding by directly mapping prompts from any supported language to visual concepts without requiring translation, while maintaining performance parity that matches or exceeds English-only models across all languages. This unified architecture delivers significant operational efficiency gains by eliminating the need for separate translation infrastructure, enabling single-model deployment that reduces both computational overhead and system complexity. The unified architecture delivers significant efficiency improvements, processing multilingual prompts 2.8x faster than translation-thengeneration pipelines while using 59% less memory which is critical for real-world deployment scenarios. To train the unified model, we introduce data curation pipeline that prepares multilingual image-text pairs for both pretraining and instruction tuning. 2. Comprehensive multilingual benchmark and metrics. We introduce the first standardized framework for evaluating multilingual image generation, addressing critical gaps in existing benchmarks. Our protocol includes: (1) extended versions of GenEval [Ghosh et al., 2023] and DPG-Bench [Hu et al., 2024], referred to as m-GenEval and m-DPG, across six languages, enabling direct comparison between native multilingual and translation-based approaches; and (2) two novel metricsCross-Lingual Consistency (CLC) and Code-Switching Similarity (CSS), to quantify semantic alignment and robustness to mixed-language prompts (see Figure 8). CLC measures image equivalence across languages using EVA-CLIP [Sun et al., 2023b] and DINOv2 [Oquab et al., 2023] embeddings, while CSS evaluates real-world code-switching scenarios. NeoBabel achieves state-of-the-art multilingual performance while maintaining strong English capabilities. Notably, it matches the English results of leading multilingual models while outperforming them by +0.11 and +0.09 on multilingual benchmarksdespite those models being built on multilingual base LLMs. This positions NeoBabel as strong foundation for future research in equitable, culturally adaptive generative AI. 3. Open toolkit for inclusive research. We release comprehensive research toolkit comprising NeoBabel model checkpoints trained on six languages (English, Chinese, Dutch, French, Hindi, and Persian), systematically curated dataset of 124M multilingual text-image pairs with quality-controlled translations, and complete reproducibility package including training scripts, hyperparameter configurations, and standardized evaluation protocols. Our framework is designed to be easily extensible to additional languages, thanks to scalable training pipeline, with validation metrics and benchmarking guidelines that support systematic comparison of multilingual generation across research groups. In the following sections, we present the details of NeoBabel, including its architecture (Section 2), multilingual datasets (Section 3), progressive training stages (Section 4), and multilingual evaluation suite (Section 5). We then provide both quantitative and qualitative evaluations (Section 6), followed by ablation studies and analysis (Section 7). 4 Figure 2: NeoBabel: Multilingual Open Tower for Visual Generation. Regardless of modality, all input data is first tokenized and embedded into unified input sequence. NeoBabel then applies causal attention to text tokens and full attention within discrete denoising diffusion framework for image tokens, ultimately generating the desired image. This design enables NeoBabel to support wide range of tasks, including text-to-image generation, text-guided inpainting and extrapolation, as well as cross-lingual image generation."
        },
        {
            "title": "2 NeoBabel Architecture",
            "content": "We first outline the core architectural components of NeoBabel, including its multilingual transformer backbone (Section 2.1), training objectives (Section 2.2), and the multilingual model merging strategy (Section 2.3) designed to enhance generation quality across diverse linguistic settings. 2.1 Model Architecture Our architectures core components, multilingual tokenizer and transformer backbone, are specifically optimized for efficient, scalable cross-lingual image generation, supporting seamless processing across diverse languages and image types. Figure 2 provides an overview of the NeoBabel architecture. 2.1.1 Tokenizers Text Tokenization. For textual input, we adopt the tokenizer of the pretrained multilingual large language model Gemma-2 [Gemma Team et al., 2024] without any modifications. This approach maintains compatibility with multilingual inputs while utilizing proven tokenization methods from language modeling. Image Tokenization. For image input, we leverage the MAGVIT-v2 quantizer [Yu et al., 2023] retrained by Show-o [Xie et al., 2025b] on 25 million images. This lookup-free quantizer learns discrete codebook of size K=8,192 and encodes 256 256 resolution images into 16 16 grids of discrete tokens. The quantization approach supports efficient downstream training and generation while preserving fine-grained visual details. 5 2.1.2 Transformer Backbone As we build upon the pretrained multilingual large language model (LLM) Gemma-2 [Gemma Team et al., 2024], we maintain its overall transformer architecture, while introducing two key modifications: (1) integration of unified multimodal embedding space, and (2) modality-aware attention patterns for flexible generation. Additionally, we apply qk-norm [Henry et al., 2020] to each attention layer to enhance training stability and convergence. Unified Multimodal Embedding and Prompt Design. To enable seamless multimodal learning, we extend the LLMs embedding table with 8,192 new learnable embeddings for discrete image tokens, allowing the model to process image inputs natively without architectural changes. Both text and image tokens are embedded in shared space, enabling the model to learn cross-modal compositionality and semantic alignment. We represent all tasks including text-to-image generation as unified autoregressive sequences. Given tokenized image-text pair, text and image tokens are concatenated into single sequence. Special tokens such as [T2I], [SOT], [EOT], [SOI], and [EOI] explicitly mark task type and modality boundaries, enabling the model to disambiguate different modalities and tasks through prompting alone. This design simplifies the training pipeline by removing the need for modality-specific components or task-specific heads, allowing for flexible, scalable, and unified multimodal generation. Modality-Aware Attention Patterns. To accommodate the differing structural needs of text and image modalities, we employ hybrid attention mechanism. Text tokens are modeled with causal attention to preserve autoregressive language modeling capabilities. Image tokens, in contrast, are modeled using full bidirectional attention, allowing rich interactions that are critical for high-fidelity image synthesis. When both modalities are present, attention masks are dynamically configured so that image tokens can fully attend to text tokens and preceding image tokens, enabling coherent, contextually grounded generation. 2.2 Training Objective The model is trained on sequences composed of both textual and visual tokens, where text tokens act as prefix and visual tokens form the postfix. We do not apply any learning objective to the text tokens; the loss is computed solely over the visual tokens. Let = {t1, t2, . . . , tN } denote the text tokens and = {i1, i2, . . . , iM } denote the image tokens, forming full input sequence [t; i]. During training, we randomly select subset {1, . . . , } of image token indices to be masked. The corresponding masked sequence is denoted by i, where ij is replaced with special [MASK] token for all . The model is trained to reconstruct the original visual tokens at the masked positions by conditioning on the full input sequence of text tokens and (partially masked) image tokens. The objective is defined as: = (cid:88) jJ log pθ(ij t, i), (1) where pθ() is the models predicted distribution over image codebook entries, parameterized by θ. The loss is only applied to the masked image tokens in . We follow the masking strategy introduced by Xie et al. [2025b], randomly masking fixed ratio of visual tokens within each training sample. To further improve generation controllability, we incorporate classifier-free guidance [Ho & Salimans, 2022] by replacing the conditioning text with null string with some probability during training. 6 2.3 Multilingual Model Merging To enhance generalization and stability of multilingual image generation models, we adopt model merging techniques that combine multiple checkpoints from the training trajectory. Let {Mi}N i=1 their corresponding non-negative weights. denote sequence of model checkpoints and {wi}N i=1 The merged model (cid:99)M is defined as convex combination: (cid:99)M = (cid:88) i=1 αiMi where αi = wi j=1 wj (cid:80)N . (2) This formulation allows the merged model to interpolate within the solution space spanned by the selected checkpoints, potentially improving generalization on unseen prompts and enhancing robustness to overfitting. We consider three widely used weighting strategies for this purpose, each reflecting different assumptions about model evolution during training. The comparative results and analysis of these approaches are presented ablation studies section. Simple Moving Average (SMA) assigns equal weight to all checkpoints. It is defined as: Mavg = 1 (cid:88) i=1 Mi. (3) SMA is simple, stable, and particularly effective when applied in the later stages of training where model weights exhibit minimal drift. Prior work [Li et al., 2025c] found SMA to perform robustly due to this stabilization. Exponential Moving Average (EMA) emphasizes recent checkpoints by applying exponentially decaying weights. It is computed recursively as: (i) avg = αMi + (1 α)M (i1) avg , [2, ]. (4) The decay factor α (0, 1) controls the trade-off between recency and stability. EMA adapts more quickly to recent model dynamics but is sensitive to noise if weights are unstable. Weighted Moving Average (WMA) assigns custom, possibly increasing weights to later checkpoints. The merged model is computed using the normalized form: Mavg = (cid:88) i=1 wi wsum Mi, where wsum = (cid:88) i=1 wi. (5) This general formulation allows flexibility in how much importance is placed on each checkpoint. In our case, we use wi = to emphasize later-stage models."
        },
        {
            "title": "3 NeoBabel Multilingual Datasets",
            "content": "3.1 Data Curation Pipeline Multilingual multimodal data remains scarce, especially compared to the abundance of Englishcentric resources. This imbalance poses significant barrier to training and evaluating models that 7 Original English-Only Dataset NeoBabel Multilingual Expansion Dataset Image Source Caption Source Size Recaptioning Translation New Size Web Web Photography ImageNet 1K CC12M SA-1B LAION-Aesthetic Web JourneyDB BLIP3-o Instruct Web + Synthetic GPT-4o / human Class labels Alt-text (noisy) LLaVA Alt-text (noisy) GPT-3.5 Synthetic 1M 12M 10M 12M 4M 60K 39M 6M 12M 10M 72M 24M 360K 124M Table 1: NeoBabel multilingual datasets, detailing their English-only data source, image origin, caption format, and size. Our multilingual expansion covers model-generated recaptioning, translation into multiple languages, or both. Our expansions increase the total size from 39M to 124M imagecaption/label pairs. In the remainder of this paper, all modified datasets are prefixed with mto denote their expanded form. can understand grounded language across diverse linguistic contexts. To address this gap, we curate and augment several multilingual datasets by translating and recaptioning existing image-caption pairs into six target languages: English, Chinese1, Dutch, French, Hindi, and Persian. We summarize the datasets curated in Table 1. At the core of our approach is multilingual captioning pipeline designed to ensure both semantic richness and linguistic diversity. We begin by generating detailed English caption for each image using InternVL [Chen et al., 2024c], prompted with simple instruction: Describe this image in detail in English. This step guarantees comprehensive coverage of the visual content. To preserve quality and consistency across languages, we implement multi-step post-processing and filtering stage based on four strategies: Length filtering: Remove captions that are too short (e.g., fewer than 5 tokens) or excessively long (e.g., more than 500 tokens). Language validation: Detect and discard captions containing non-English phrases or corrupted outputs using language identification tools. We use the fastText language identification model trained on 176 languages [Joulin et al., 2016]. We discard any caption not classified as English with confidence score above 90%. Visual-text mismatch filtering: Discard captions that do not align with visual content, measured via auxiliary vision-language models (e.g., using VQAScore). Specifically, we leverage MolMo-72B [Deitke et al., 2025] deployed with vLLM [Kwon et al., 2023], formulating the task as binary structured prediction (yes/no) via vLLMs output interface. Toxicity and NSFW filtering: Discard samples using the LAION-5B NSFW classifier [Schuhmann et al., 2022] to ensure safe visual content before captioning, assuming high likelihood of appropriateness in the resulting captions. Once high-quality English captions are obtained, we translate them into five target languages using the NLLB model [Costa-Jussà et al., 2022] for the pretraining datasets, and the Gemini Experimental model (gemini-2.0-flash-lite) for the instruction tuning datasets. This separation ensures 1Throughout this work Chinese refers to Simplified Chinese. 8 high translation coverage at scale during pretraining, while leveraging higher-quality outputs for instruction-tuned data. Using English as pivot allows us to take advantage of strong captioning performance in high-resource settings while ensuring consistent semantic content across all languages. This approach not only amplifies the linguistic diversity of our dataset but also maintains alignment between captions, which is critical for multilingual training and evaluation. Ultimately, this step plays central role in constructing high-quality, language-balanced multimodal resource an essential step toward more inclusive and globally-relevant vision-language models. 3.2 NeoBabel Pretraining Data The previous section described the overall pipeline and transformation steps, and next we detail the data sources and multilingual adaptations used to train the model. We use diverse collection of image-text datasets to build strong multilingual visual-language alignment combining real-world and synthetic image sources. While the images are drawn from established, high-quality datasets, the accompanying captions have been significantly enriched through our recaptioning and multilingual translation pipelineresulting in more diverse, detailed, and valuable resource for future multilingual generative models. m-ImageNet-1K: The original English class labels are translated into five more languages to obtian total of six target languages, forming multilingual textual prompts for class-conditional image generation. m-SA-1B and m-CC12M: We incorporate 22 million image-caption pairs in English from SA-1B [Kirillov et al., 2023] and CC12M [Changpinyo et al., 2021]. These datasets provide rich natural image-caption pairs and enhance visual diversity. The texts are enhanced through our recaptioning pipeline described in Section 3.1. m-LAION-Aesthetic: subset of the LAION dataset including 12M image-text pairs2 is enhanced and translated, yielding approximately 72 million image-caption pairs for total of six languages. m-JourneyDB: This synthetic dataset consists of 4 million high-quality images generated by the Midjourney model [Sun et al., 2023a]. We apply the same recaptioning and translation pipeline to generate 24 million image-caption pairs for our six languages. Combining all sources, the final pretraining dataset contains approximately 124 million image-text pairs across six languages, covering diverse domains and visual aesthetics. 3.3 NeoBabel Instruction Tuning Data Here we describe our datasets and mixing strategies used for instruction tuning. This phase reuses two datasets introduced earlier and adds smaller but higher-quality dataset focused on multimodal instruction tuning: m-LAION-Aesthetic and m-JourneyDB: Our setup continues to use the LAION-Aesthetic and JourneyDB datasets, as extended in the pretraining data. 2https://huggingface.co/datasets/dclure/laion-aesthetics-12m-umap 9 m-BLIP3o-Instruct: An instruction-focused dataset introduced by Chen et al. [2025a], containing multimodal instruction samples, also translated into six languages for multilingual supervision. All images are resized to 512 512. While the images are drawn from established, high-quality sources, most accompanying texts have been significantly enriched or rewritten, resulting in more valuable and linguistically diverse dataset for instruction tuning and multilingual generation."
        },
        {
            "title": "4 NeoBabel Training Stages: Learning Progression",
            "content": "NeoBabel is trained using staged learning framework consisting of three progressive pretraining stages (Section 4.1) followed by two instruction tuning stages (Section 4.2). 4.1 Progressive Pretraining Our pretraining includes three stages, progressively scaling from basic visual understanding to advanced multilingual image generation: Stage 1 Pixel Dependency Learning: The model initially learns foundational visual representations using m-ImageNet-1K. Class-conditional image generation is guided by translated class labels, enabling the model to form robust image token embeddings and capture pixel-level dependencies for high-fidelity output. Stage 2 Scaling Alignment with Large-Scale Multilingual Data: Using weights from the first stage, the model is fine-tuned on 22 million English-only image-caption pairs (from m-SA-1B and m-CC12M) and 72 million translated samples from m-LAION-Aesthetic. This stage strengthens the models grounding in natural image-text alignment while developing multilingual capabilities through broad cross-lingual exposure. Stage 3 Refined Multilingual Pretraining: In the final stage, the model is trained on 96 million multilingual image-text pairs derived from m-LAION-Aesthetic and m-JourneyDB. The training balances high-quality real-world aesthetic data with diverse, synthetic images to improve generalization across languages, domains, and modalities. 4.2 Progressive Instruction Tuning Following pretraining, the model advances to instruction tuning, where the focus shifts from unsupervised representation learning to explicit task-guided adaptation, refining its ability to interpret and execute complex, multilingual instructions through our curated datasets and progressive exposure to prompt-driven generation in two stages: Stage 1 Initial Multilingual Instruction Alignment: To build robust multilingual instructionfollowing capabilities at high resolution, the model is first trained with diverse mixture of the three datasets described above. In this stage, training samples are drawn from m-LAION-Aesthetic, mJourneyDB, and m-BLIP3o-Instruct using mixing weights α1, α2, and α3, respectively, such that α1 + α2 + α3 = 100. higher α1 and moderate α2 prioritize real-world and aesthetic content, while smaller α3 introduces early exposure to instruction-rich samples. This balance helps the model 10 learn cross-lingual, cross-modal grounding without overwhelming it with complex prompts in the early stages. In the second stage, we adjust the mixing weights to Stage 2 Instruction Refinement: emphasize instruction-rich and synthetic supervision. Specifically, α2 and α3 are increased to draw more heavily from m-JourneyDB and m-BLIP3o-Instruct, while α1 is decreased to reduce reliance on LAION-based content. This curriculum-style shift enables the model to refine its instructionfollowing capabilities using complex multilingual prompts and high-quality synthetic images. The increased semantic richness improves the models generalization to both benchmark instruction tasks and open-ended generation scenarios. Each stage is trained for 500k steps (except the final stage of instruction tuning with 200k) using the AdamW optimizer and cosine learning rate decay. The learning rate is set to 1e4 during pretraining and adjusted during instruction tuning. We gradually increase prompt sequence length and resolution from 128 to 512 and from 256 256 to 512 512 respectively. The vocabulary and codebook sizes are fixed across all stages. Full hyperparameter settings for each pretraining and instruction tuning stage are summarized in the Appendix."
        },
        {
            "title": "5 Multilingual Evaluation of Image Generation",
            "content": "Existing image generation benchmarks are mostly English-centric, failing to capture cross-lingual performance. To resolve this limitation, we introduce multilingual evaluation suite that extends established (English-only) benchmarks to cover six diverse languages and introduces new evaluation metrics for assessing cross-lingual visual consistency. This section outlines our multilingual evaluation suite (Section 5.1) and multilingual evaluation metrics (Section 5.2). 5.1 Multilingual Evaluation Suite We assess the image generation capabilities of NeoBabel using two complementary benchmarks: GenEval [Ghosh et al., 2023] and DPG-Bench [Hu et al., 2024]. GenEval offers structured evaluation of prompt-to-image alignment across six compositional dimensions: single object, two objects, counting, colors, position, and color attribute. In contrast, DPG-Bench targets general-purpose generation with open-ended, diverse prompts that test broader semantic understanding. However, both benchmarks are English-only and fail to capture multilingual generative performance. As part of our multilingual evaluation suite, we introduce m-GenEval and m-DPG, multilingual extensions of the original benchmarks. All prompts are translated into five additional languages: Chinese, Dutch, French, Hindi, and Persian, using the Gemini Experimental model, followed by human verification and manual corrections to ensure semantic fidelity and linguistic fluency. Together with the paper, we publicly release m-GenEval and m-DPG to promote inclusive and realistic evaluation of multilingual text-to-image models and support broader community adoption. 5.2 Multilingual Evaluation Metrics To complement the multilingual benchmarks introduced above, we introduce two metrics that assess how well generative models preserve visual and semantic consistency across languages. Existing 11 evaluations focus on monolingual alignment, overlooking whether models produce consistent outputs across languages or under mixed-language inputs. To address this, we introduce two scores to assess cross-lingual consistency and robustness under intra-prompt language mixing. Together, these metrics provide more diagnostic view of multilingual generation performance. Cross-Linguistic Consistency (CLC). To evaluate whether multilingual models generate semantically consistent and faithful outputs across languages, we introduce the CLC score. Multilingual image generation models should produce visually similar outputs when given semantically equivalent prompts, regardless of the input language. Measuring this consistency is crucial for understanding how well the model aligns its multilingual text inputs with the corresponding visual outputs, which reflects the quality of its cross-lingual grounding. We evaluate in multilingual setting consisting of prompts, each paired with language variations, forming parallel dataset. For each prompt , we generate images (one per language), resulting in images. Let xi denote {pi}P an image and (xi) Rd its corresponding embedding obtained from vision encoder. i=1 To measure consistency, we treat the images generated from the English version of the prompt as the reference set Rp, and the remaining (L 1) images generated from other languages as the target set Tp. The core idea is that if the model is truly language-agnostic in its understanding, images generated from non-English prompts should be visually similar to those generated from the English prompt. The CLC Score for prompt is computed by averaging the cosine similarity between all reference and non-reference embeddings: CLCp = 1 Rp Tp (cid:88) (cid:88) xiRp xj Tp cos (f (xi), (xj)) . (6) Finally, the overall CLC score is obtained by averaging CLCp over all prompts . For evaluation, we use m-DPG prompts and compute embeddings with two strong vision encoders, EVA-CLIP [Sun et al., 2023b] and DINOv2 [Oquab et al., 2023], to ensure robustness across different feature representations. This metric provides quantitative measure of how well multilingual generation models maintain semantic and visual alignment across languages. Code-Switching Similarity (CSS). Real-world multilingual communication frequently involves code switching, i.e., interleaving of multiple languages within single utterance. Therefore, wellaligned multilingual model should demonstrate robustness not only to monolingual prompts but also to mixed-language inputs, capturing the inherent complexity and variability of natural language. Code switching often increases perplexity and degrades performance in language models; however, its impact on image generation remains largely unexplored. To evaluate this, we introduce the CSS Score, which quantifies visual consistency under intra-prompt language variation. Given set of reference prompts composed entirely in English, we construct two variants per prompt for each of the L1 non-English target languages: (1) English-First (EF): the first half of the prompt remains in English while the second half is translated into the target language, and (2) English-Second (ES): the first half is translated while the second half remains in English. , we generate single reference image xref from the original English For each prompt {pi}P prompt and 1 code-switched images: x(l) for each target language l. Each image is EF encoded into an embedding (x) Rd using vision encoder. The Code Switching Similarity (CSS) score for each prompt is computed by measuring the average cosine similarity between the reference and x(l) ES i= 12 Method (cid:140) Type Params. Single Object Two Object Counting Colors Position Color Attribute Overall LlamaGen LDM SDv1.5 PixArt-alpha SDv2.1 DALL-E 2 SDXL SD3 G G CoDI Chameleon LWM SEED-X Janus TokenFlow EMU3 Show-o Janus-Pro BLIP3-o BLIP3-o U&G U&G U&G U&G U&G U&G U&G U&G U&G U&G U&G 0.8B 1.4B 0.9B 0.6B 0.9B 6.5B 2.6B 2B - 7B 7B 17B 1.3B 14B 8B 1.3B 7B 4B 8B 0.71 0.92 0.97 0.98 0.98 0.98 0.98 0.98 0.89 - 0.93 0.97 - - - 0.98 - - - 0.34 0.29 0.38 0.50 0.51 0.66 0.74 0.74 0.16 - 0.41 0.58 - - - 0.80 - - - NeoBabel 2B 1.00 0. 0.21 0.23 0.35 0.44 0.44 0.49 0.39 0.63 0.16 - 0.46 0.26 - - - 0.66 - - - 0.62 0.58 0.70 0.76 0.80 0.85 0.77 0.85 0.67 0.65 - 0.79 0.80 - - - 0.84 - - - 0.07 0.02 0.04 0.08 0.07 0.10 0.15 0. 0.02 - 0.09 0.19 - - - 0.31 - - - 0.04 0.05 0.06 0.07 0.17 0.19 0.23 0.36 0.01 - 0.15 0.14 - - - 0.50 - - - 0.91 0.81 0. 0.32 0.37 0.43 0.48 0.50 0.52 0.55 0.62 0.31 0.39 0.47 0.49 0.61 0.63 0.66 0.68 0.80 0.81 0.83 0.83 Table 2: English-only GenEval benchmark comparison. NeoBabel achieves the highest overall score, outperforming larger models on tasks requiring compositional reasoning and fine-grained prompt-image alignment. Symbol legend: (cid:140) denotes multilingual generation capability, with indicates full multilingual capability, represents partial multilingual capability (i.e. bilingual or multilingual to limited extent), and denotes monolingual models. embedding (xref) and the embeddings from the EF and ES variants: CSSpEF = 1 1 L1 (cid:88) l=1 (cid:16) cos (xref), (x(l) EF) (cid:17) , CSSpES = 1 1 L1 (cid:88) l=1 (cid:16) cos (xref), (x(l) ES) (cid:17) . (7) The final CSS scores are obtained by averaging across all prompts: CSSEF = 1 (cid:88) p=1 CSSEF , CSSES = 1 (cid:88) p=1 CSSES . (8) To assess how well models preserve semantic consistency under intra-prompt code switching, we report both CSSEF and CSSES, using embeddings from EVA-CLIP [Sun et al., 2023b] and DINOv2 [Oquab et al., 2023] computed on m-DPG prompts."
        },
        {
            "title": "6 Results and Discussions",
            "content": "We evaluate NeoBabel on our multilingual extension of standard benchmarks, including mGenEval and m-DPG, to assess performance across languages both quantitatively and qualitatively. Baselines. We evaluate our model against diverse range of baselines, which we group into two categories: generative-only models (G) and unified models (U&G). The generative models are 13 Figure 3: m-GenEval benchmark comparison. Models such as Janus Pro and BLIP3-o rely on multilingual base LLMs but are trained solely on English image-generation data, leading to sharp performance drop in non-English languages. In contrast, NeoBabel maintains strong and consistent results across all six languages, demonstrating robust cross-lingual generalization. Here baseline models are ordered by parameter count. designed exclusively for text-to-image generation, without any visual understanding components. This category includes LlamaGen [Sun et al., 2024], LDM [Rombach et al., 2022], SDv1.5 and SDv2.1 [Rombach et al., 2022], SDXL [Podell et al., 2023], SD3 [Esser et al., 2024], DALL-E 2 [Ramesh et al., 2022], and PixArt-α [Chen et al., 2024a] models primarily optimized for high-quality and compositional image generation. In contrast, the unified models support both image generation and image understanding tasks such as captioning and visual question answering. This group includes CoDI [Tang et al., 2023], LWM [Liu et al., 2024a], SEED-X [Ge et al., 2024], Chameleon [Team, 2024], TokenFlow [Qu et al., 2025], EMU3 [Wang et al., 2024], Janus [Wu et al., 2025], Janus-Pro [Chen et al., 2025b], and BLIP3-o [Chen et al., 2025a]. Our comparison includes both small-scale and large-scale models, spanning from under 1B to over 17B parameters. 6.1 Multilingual Image Generation Performance m-GenEval Comparison. We begin by evaluating NeoBabel on the English prompts of the m-GenEval benchmark, with results reported in Table 2. The comparison includes both generative models (G), which focus solely on text-to-image generation, and unified models (U&G), which also support image understanding tasks such as captioning and visual question answering. Despite having only 2B parameters, NeoBabel outperforms or matches best-performing unified models such as Janus-Pro 7B (0.77) and BLIP3-o 8B (0.83), which are significantly larger in terms of parameters. It also surpasses SD3 2B (0.62), leading model in the generative category, achieving the highest overall score of 0.83. This performance reflects strong fine-grained and compositional prompt-image alignment particularly in challenging subcategories like color attributes and positional grounding. In Figure 3, we further evaluate NeoBabel across five more languages including Chinese, Dutch, French, Hindi, and Persian to assess its multilingual generalization capabilities beyond English. As can be seen, the performance gap between NeoBabel and the strongest baselines is small 14 Model Params. English Chinese Dutch French Hindi Persian Overall Show-o EMU3 TokenFlow-XL Janus Janus Pro BLIP3-o BLIP3-o NeoBabel 1.3B 8B 14B 1.3B 7B 4B 8B 2B 0.67 0.80 0.73 0.79 0.84 0.79 0.80 0.75 0.10 0.56 0.50 0.60 0. 0.22 0.42 0.61 0.58 0.59 0.32 0.53 0.68 0.59 0.61 0.04 0.17 0.12 0.47 0.50 0.04 0.13 0.12 0.49 0.53 0.23 - - 0.43 0.47 0.58 0.59 0. 0.69 0.70 0.63 0.65 0.68 Table 3: m-DPG benchmark comparison. Despite its small parameter count, NeoBabel achieves competitive results in English and consistently outperforms all baselines across five non-English languages, demonstrating strong cross-lingual prompt understanding and image generation. in Chinese (by 0.03). We attribute this to two factors: (i) the use of bilingual English-Chinese instruction-tuning data in models like Janus Pro, whose training setup is not publicly disclosed, and (ii) architectural choices such as BLIP3-os use of frozen LLM backbone with prompt learning instead of full model adaptation. In medium-resource languages like Dutch and French, the gap widens (0.06 and 0.04 respectively), and in low-resource languages such as Hindi and Persian, NeoBabel significantly outperforms all baselines by large margin (up to 0.3 improvement), despite having 4 fewer parameters than Janus Pro 7B and BLIP3-o 8B. These results underscore the cross-lingual robustness and data efficiency of our multilingual instruction-tuning strategy. m-DPG Comparison. Compared to m-GenEval, which emphasizes fine-grained attributes and atomic compositional reasoning, m-DPG focuses on models ability to follow natural, descriptive multilingual prompts. It tests whether the generated images are semantically accurate, detailed, and coherent, making it stronger indicator of real-world prompt-image alignment performance. We evaluate NeoBabel on m-DPG to assess prompt-image alignment across 6 languages in Table 3. NeoBabel achieves comparable performance in English (0.75), even though it uses only 2B parameters, which is far fewer than BLIP3-o (4B and 8B) and Janus Pro (7B). More importantly, NeoBabel outperforms all baselines in the non-English settings. Existing models show notable performance drops in several languages, especially in low-resource settings (Hindi and Persian) where models such as Janus, Janus Pro, and Show-o perform poorly. In contrast to the best-performing baseline (BLIP3-o 8B), NeoBabel consistently achieves the highest scores across all six target languages. As in m-GenEval, we observe similar trend in m-DPG, where the performance gap widens in medium-resource languages by 0.10 in Dutch and 0.09 in French and becomes even larger in low-resource settings, with gaps of 0.13 in Hindi and 0.12 in Persian. 6.2 Qualitative Evaluation To complement the quantitative findings, we present qualitative results from NeoBabel across diverse prompt categories, including compositional scenes, abstract concepts, and multilingual instructions, in Figures 4 and 5 (main paper) and Figure 12 (appendix). The results show that NeoBabel consistently generates semantically aligned and visually coherent images. Objects, layouts, and attributes are preserved across languages, demonstrating the models strong multilingual alignment and consistency in representing concepts. 15 Figure 4: Qualitative evaluation of NeoBabel. Each row is based on single concept expressed in six different languages. For clarity, we show only one of the prompts (in one language) and present six images generated from its translated prompts in the other five languages. Across all languages, NeoBabel delivers semantically accurate and visually cohesive outputs with reliable consistency. 6.2.1 Multilingual Image Inpainting and Extrapolation NeoBabel enables new collaborative applications, such as multilingual visual canvas where users can contribute prompts in their native languages to co-create coherent and expressive visual scenes. 16 Figure 5: Qualitative evaluation of NeoBabel. Each row is based on single concept expressed in six different languages. For clarity, we show only one of the prompts (in one language) and present six images generated from its translated prompts in the other five languages. No matter the language, NeoBabel consistently produces semantically aligned, visually coherent results. It supports text-guided image inpainting and extrapolation across multiple languages without requiring additional fine-tuning. As shown in Figures 6 and 7, NeoBabel can modify or extend an input image based on prompts in different languages, producing results that remain semantically faithful and visually consistent with the adjacent visual content. These examples demonstrate the models ability to maintain coherence across inpainted and extrapolated regions, highlighting its potential for interactive and multilingual visual editing. 6.2.2 Cross-lingual Image Generation more challenging evaluation of the models multilingual ability involves prompts that combine multiple languages within the same input. This requires the model to integrate information from different languages into coherent and semantically accurate image. To create these prompts, we split base prompt into three parts and translate each into different language. Figure 8 (in the Introduction) illustrates two such examples. The images generated by NeoBabel demonstrate its ability to follow complex multilingual instructions, producing visually coherent and semantically faithful outputs. These results highlight the models cross-lingual alignment, despite not being explicitly trained for this task."
        },
        {
            "title": "7 Ablations and Analyses",
            "content": "In the following sections, we conduct series of ablation studies and analyses to evaluate the effects of progressive pretraining (Section 7.1), instruction tuning (Section 7.2), and model merging (Section 7.3). We also perform additional multilingual evaluations, including cross-linguistic consistency (Section 7.4) and code switching similarity analyses (Section 7.5). 17 Figure 6: Multilingual image inpainting. NeoBabel supports multilingual text-guided image inpainting, highlighting its potential for interactive and language-inclusive visual editing across diverse user groups. 7.1 Effect of Progressive Pretraining We first analyze the impact of our progressive pretraining strategy across three stages at 256256 resolution. As shown in Figure 9, each stage leads to steady improvements in multilingual performance on m-GenEval and m-DPG. In the first stage, using only m-ImageNet 1K, the average scores are modest: 0.04 on m-GenEval and 0.14 on m-DPG, indicating weak multilingual alignment. In the second stage, the addition of large-scale but noisy datasets (m-SA-1B, m-CC12M, m-LAIONAesthetic) results in significant increase, reaching 0.17 on m-GenEval and 0.58 on m-DPG. The average gain in performance from stage one to stage two is substantially larger on m-DPG (0.44) than on m-GenEval (0.14), suggesting that this stage improves the models ability to handle natural, descriptive prompts in multiple languages. The third stage incorporates higher-quality datasets (m-LAION-Aesthetic and m-JourneyDB), leading to further gains: 0.02 on m-GenEval and 0.04 on m-DPG. These results demonstrate the cumulative benefits of progressively increasing both the diversity and quality of pretraining data. While large, noisy datasets drive early generalization, high-quality data is essential for refining multilingual alignment. This staged pretraining approach provides strong initialization for downstream instruction tuning. Figure 7: Multilingual image extrapolation. NeoBabel successfully performs text-guided image extrapolation using multilingual prompts. Given the middle image and two different multilingual prompts (for the left and right extensions), NeoBabel generates coherent visual completions on both sides, demonstrating extrapolation capability. Figure 8: Cross-Lingual Prompt Generation. Examples of code-switched prompts mixing three languages, along with images generated by NeoBabel. Top: English, Dutch and French. Bottom: Hindi, Persian and Chinese. English translations are shown below each prompt for reader convenience, they are not used as input. 19 Figure 9: Effect of Progressive Pretraining and Instruction Tuning. Performance on m-GenEval (top) and m-DPG (bottom) improves steadily across pretraining and instruction tuning stages. Pretraining at 256256 yields significant gainsespecially on m-DPGwhen scaling to large multilingual datasets (Stage 2), followed by refinement using higher-quality data (Stage 3). Instruction tuning at 512512 brings substantial boost (e.g., +0.52 on m-GenEval) and further improvement by increasing the share of curated, instruction-aligned samples. These results highlight how scale drives broad generalization, while data quality and resolution are key for performance refinement. 7.2 Effect of Progressive Instruction Tuning We analyze the effect of high-resolution instruction tuning using fixed dataset mixture: m-LAIONAesthetic, m-JourneyDB, and m-BLIP3o-Instruct, all at 512512 resolution. As shown in Figure 9, both tuning stages progressively refine multilingual alignment, with consistent gains across all six In the first stage, each training batch consists of 60% m-LAION-Aesthetic, 30% mlanguages. JourneyDB, and 10% m-BLIP3o-Instruct samples. This stage yields substantial multilingual gain of 0.52 on m-GenEval and 0.04 on m-DPG compared to the final stage of pretraining, indicating that high-resolution supervision provides strong improvements when combined with highly curated instruction tuning dataset. In the second stage, each training batch shifts emphasis toward higherquality and instruction-aligned data, consisting of 25% m-LAION-Aesthetic, 60% m-JourneyDB, and 15% m-BLIP3o-Instruct samples. This leads to further multilingual gain of 0.02 in m-GenEval and boost in m-DPG. These results show that beyond increasing the resolution, the relative weight of curated and instruction-focused datasets plays pivotal role in shaping multilingual capability. Prioritizing high-quality supervision at higher resolution proves effective for achieving competitive alignment ultimately enabling our 2B model to rival much larger models. 20 7.3 Effect of Model Merging on Generalization Method 0.73 0.75 0.75 0.75 0.81 0.82 0.83 0.83 m-GenEval m-DPG Last checkpoint EMA WMA SMA We investigate the impact of model merging on multilingual image generation performance by combining = 20 checkpoints sampled at 10,000-step intervals from the second instruction tuning stage (steps 0200K). Table 4 reports the results of three merging strategies: Simple Moving Average (SMA), Exponential Moving Average (EMA), and Weighted Moving Average (WMA) compared to the last checkpoint baseline. As reported, m-GenEval score on English prompt improves from 0.81 to 0.83 after model merging. Both WMA and SMA reach this upper bound, indicating that merging checkpoints along the optimization path enhances semantic alignment. Moreover, m-DPG score on English prompt remains stable or show modest gains, suggesting that merging preserves the models ability to accurately follow dense, attribute-rich prompts without sacrificing fine-grained multilingual grounding. Among the merging strategies, SMA performs best overall due to its uniform averaging over well-aligned checkpoints. EMA also improves results but remains more susceptible to short-term training noise. WMA offers compromise by emphasizing later checkpoints, trading off stability for adaptability. These findings underscore that checkpoint merging can meaningfully enhance both compositional understanding and multilingual robustness, with SMA offering simple yet effective strategy. Table 4: Effect of model merging on generalization. Without any fine-tuning, all merging strategies improve performance on m-GenEval and slightly enhance m-DPG, highlighting model merging as simple yet effective way to boost generalization. This ablation uses English prompts. 7.4 Cross-Lingual Consistency Analysis This ablation examines how different models preserve consistency in image generation across languages, evaluated using the CLC score introduced in Section 5.2 with both EVA-CLIP and DINOv2 vision encoders  (Table 5)  . Across both backbones, NeoBabel achieves the highest scores, 0.79 (EVA-CLIP) and 0.61 (DINOv2), outperforming larger models such as BLIP3-o 8B (0.77/0.45) and Janus Pro 7B (0.67/0.30). This indicates that training strategy and data alignment play more significant role than parameter count alone in achieving cross-lingual consistency. In the EVA-CLIP based CLC, which emphasizes high-level semantic similarity due to its contrastive training objective with text, high scores reflect strong cross-lingual consistency in scene-level concept. In contrast, DINOv2based CLC captures visual-structural coherence making it more sensitive to differModel Params EVA-CLIP DINOv2 Show-o Janus Janus Pro BLIP3-o BLIP3-o NeoBabel 1.3B 1.3B 7B 4B 8B 2B 0.47 0.67 0.67 0.76 0.77 0.79 0.16 0.26 0.30 0.44 0.45 0.61 Table 5: Cross-Lingual Consistency Analysis using CLC scores with EVA-CLIP and DINOv2 backbones. NeoBabel (2B) outperforms larger models, showing stronger crosslingual consistency in both semantic and visual domains. The larger gap in DINOv2 scores highlights its greater sensitivity to visual-structural variations, revealing inconsistencies that EVA-CLIPs semantic focus may overlook. 21 Figure 10: Cross-Lingual Consistency (CLC) Score Distributions across Models. We show the distribution of CLC scores computed using EVA-CLIP (left column) and DINOv2 (right column), where higher values reflect greater consistency across languages. EVA-CLIP captures semantic similarity, while DINOv2 is more sensitive to visual structure and layout. NeoBabel achieves the highest scores under both backbonesparticularly with DINOv2demonstrating strong alignment in both meaning and visual form. Box widths reflect model size, showing that bigger models arent always more consistent. ences in object composition, layout, or fine-grained visual patterns. Notably, the relative margin between NeoBabel and other models is more pronounced under DINOv2. For instance, BLIP3-os DINOv2 score drops significantly despite competitive EVA-CLIP CLC score suggesting its outputs vary more in visual structure across languages. ES ES EF Model Params EF EVA-CLIP DINOv2 Show-o Janus Janus Pro BLIP3-o BLIP3-o This amplifies the role of multilingual alignment not just in semantics, but in visual form dimension better captured by DINOv2. NeoBabel achieves high scores under both backbones, thereby demonstrating strong cross-lingual consistency in both scene semantics and visual structure. Figure 10 complements Table 5 by visualizing the distribution of CLC scores for each model using both EVA-CLIP and DINOv2 backbones. In terms of EVA-CLIP based CLC variation, NeoBabel performs on par with BLIP3-o 8B, the second-best model. However, under the DINOv2-based CLC variation, NeoBabel outperforms all baselines, exhibiting lower dispersion and higher consistency. We can also observe from the figure that simply increasing model size does not guarantee better cross-lingual consistency, as evidenced by the lower DINOv2 scores of Janus Pro 7B and BLIP3-o 8B compared to NeoBabel. Table 6: Code Switching Similarity (CSS) Analysis using EVA-CLIP and DINOv2 backbones. Scores are reported for two prompt variants: English First (EF) and English Second (ES). NeoBabel (2B) outperforms larger models, showing strong visual consistency and robustness to code-mixed input order. The larger DINOv2 gap reflects its higher sensitivity to visual-structural variation, while EVA-CLIP remains more stable due to its semantic focus. 1.3B 0.73 1.3B 0.75 7B 0.76 4B 0.75 8B 0.74 0.41 0.50 0.58 0.54 0.52 0.38 0.43 0.50 0.54 0.51 0.72 0.73 0.72 0.75 0.74 NeoBabel 2B 0. 0.64 0.81 0.67 7.5 Code Switching Similarity Analysis We evaluate model robustness to intra-prompt code switching using the proposed CSS score in Section 5.2 with EVA-CLIP and DINOv2 backbones. As shown in Table 6 and Figure 11, NeoBabel consistently outperforms larger models, demonstrating stronger visual consistency under both English-First (EF) and English-Second (ES) variations. Under EVA-CLIP, all models exhibit minimal difference between EF and ES, suggesting that the position of the English segment has limited effect on global semantic alignment. In contrast, DINOv2 scores are lower across the board, indicating greater difficulty in maintaining consistent visual structure when mixing languages. Importantly, desirable outcome is both high CSS scores (indicating alignment with the reference image) and minimal gap between EF and ES (indicating robustness to code-switch position). NeoBabel achieves this balance, with CSS scores of 0.82 (EF) and 0.81 (ES) for EVA-CLIP, and 0.67 (EF) and 0.64 (ES) for DINOv2. The box plots reveal that although larger models like BLIP3-o (8B) achieve competitive means, they show greater variability across prompts. NeoBabel demonstrates both higher median performance and lower dispersion, confirming its consistent handling of code-mixed inputs. These results further highlight that scaling model size does not necessarily improve code-mixed prompt robustness effective multilingual alignment plays larger role. Figure 11: Variation in Code Switching Similarity (CSS) Scores across Models. We report CSS scores for code-mixed prompts under two settings: English-first (left column) and English-second (right column), using EVA-CLIP (top row) and DINOv2 (bottom row) as backbones. Higher scores indicate stronger visual alignment with the reference image, while smaller EFES gaps suggest robustness to code-switch position. NeoBabel consistently achieves higher medians and lower variance than larger baselines, especially under DINOv2, highlighting its effective and stable handling of multilingual prompts."
        },
        {
            "title": "8 Related Works",
            "content": "Large Multimodal Models. Recent advances in large multimodal models (LMMs) [Liu et al., 2024b; Chen et al., 2024b; Li et al., 2024a; Bai et al., 2025; Dash et al., 2025] have extended large language models (LLMs) [Touvron et al., 2023; Yang et al., 2024] to support image understanding 23 tasks, including image captioning and visual question answering. These models typically rely on vision encoder to extract image features, which are then projected into the LLM embedding space for cross-modal alignment. More recent encoder-free models [Xie et al., 2025b; Diao et al., 2024; 2025] bypass the explicit image encoder and instead align raw visual tokens directly within the LLM space. Among early efforts to enable multilingual visual understanding, Maya [Alam et al., 2024; 2025], Aya-Vision [Dash et al., 2025] and Pangea [Yue et al., 2024] incorporate multilingual training corpus. However, they are limited to image understanding tasks. In contrast, our proposed NeoBabel architecture focuses exclusively on multilingual image generation, offering the first encoder-free model that aligns visual features in the LLM space while supporting crosslingual generation. Architecturally, NeoBabel is closely related to show-o [Xie et al., 2025b], sharing the same design goal of direct visual alignment in language space, but differs in its task focus and multilingual design. Visual Generative Models. Two dominant paradigms have emerged for image and video generation: diffusion-based [Rombach et al., 2022; Peebles & Xie, 2023; Bao et al., 2023; Chen et al., 2024a; Xie et al., 2023; Wu et al., 2023a; Lipman et al., 2022; Xie et al., 2025a; Qin et al., 2025; Zhang et al., 2023; Seawead et al., 2025] and autoregressive [Sun et al., 2024; Kondratyuk et al., 2023; Chen et al., 2020; Pang et al., 2024; Li et al., 2025a] models. Diffusion models typically combine pretrained text encoders with denoising networks to iteratively refine visual outputs, while autoregressive models adopt LLM-based architectures trained via next-token prediction. Recent hybrid approaches [Li et al., 2024b; Liu et al., 2024c; Fan et al., 2025] attempt to unify the strengths of both paradigms for more powerful generation. NeoBabel follows the diffusion-based paradigm but distinguishes itself by adopting an LLM-style architecture for visual token modeling. This removes the reliance on frozen text encoders and instead builds on top of strong multilingual decoder-based LLM, enabling tighter integration between language and vision. Unified Multimodal Models. Unified multimodal models aim to handle both image understanding and generation within single architecture, typically categorized into native and adapter-based approaches. Native approaches such as Chameleon [Team, 2024], Show-o [Xie et al., 2025b], and Transfusion [Zhou et al., 2025] adopt either autoregressive, diffusion, or hybrid modeling strategies to jointly process vision and language. Recent work [Wang et al., 2024; Wu et al., 2024; Ma et al., 2025; Jiao et al., 2025; Chen et al., 2025c; Song et al., 2025] has focused on improving tokenization and training efficiency to enhance cross-modal alignment. parallel direction [Tang et al., 2023; Lu et al., 2023; Dong et al., 2024; Ge et al., 2024; Tong et al., 2024; Pan et al., 2025; Chen et al., 2025a; Wu et al., 2023b] constructs unified multimodal models by connecting pretrained LMMs and generative models via adapters or learnable tokens. While modular and flexible, these systems often rely on frozen components and lack full cross-modal integration. Our model, NeoBabel, aligns more closely with native unified multimodal models by unifying visual and textual modeling within single decoder-based architecture, without relying on adapters or frozen backbones. Although NeoBabel supports multilingual multimodal understanding, this work focuses specifically on multilingual image generation."
        },
        {
            "title": "9 Limitations",
            "content": "While NeoBabel demonstrates strong multilingual image generation capabilities, several limitations remain. First, the model currently supports only six languages; extending to broader linguistic coverage would require further tokenizer adaptation and additional training. Second, although 24 NeoBabel adopts unified architecture, it does not yet support vision-language tasks such as visual question answering, due to the absence of task-specific fine-tuning. Third, the models performance is constrained by its parameter size and the diversity and quality of the training data. For instance, during instruction tuning, we used fixed mixture of m-LAION-Aesthetic, m-JourneyDB, and m-BLIP3o-Instruct, without performing an extensive sweep over mixture ratiosan area that could reveal further improvements. We leave these directions, including task expansion, larger-scale scaling, and wider language coverage, for future research."
        },
        {
            "title": "10 Conclusion",
            "content": "NeoBabel demonstrates that high-quality, efficient multilingual image generation is not only possible but also advantageous. Through strategic data curation and unified architecture, we set new Pareto frontier in performance, efficiency, and inclusivity. While currently focused on text-toimage generation, our model is structurally capable of broader multimodal tasks. Our results across m-GenEval and m-DPG benchmarks, paired with the introduction of new evaluation metrics (CLC and CSS), establish robust foundation for the next generation of multilingual generative models. Our work opens several promising avenues for future research. First, extending NeoBabel to encompass wider variety of languages, particularly those currently underrepresented in visionlanguage research, remains an important objective. The modularity of our framework and the accompanying open-source toolkit are designed to facilitate such extensions. Second, beyond linguistic diversity, integrating cultural grounding into multimodal models presents compelling research direction. By curating datasets annotated with region-specific concepts, aesthetic preferences, and social norms, future work could develop models that are not only multilingual but also culturally aware and adaptive. Finally, this work contributes to the broader goal of democratizing generative AI. By releasing all model weights, datasets, and evaluation protocols, we aim to encourage the research community to build upon this foundation, ultimately advancing toward generative models that better reflect and serve global linguistic and cultural diversity."
        },
        {
            "title": "Acknowledgment",
            "content": "We would like to thank the Cohere Labs team for their valuable feedback and for providing generous computing resources for conducting and analyzing our experiments. We further acknowledge the Dutch Research Council (NWO) in The Netherlands for awarding this project access to the LUMI supercomputer, owned by the EuroHPC Joint Undertaking, hosted by CSC (Finland) and the LUMI consortium through the Computing Time on National Computer Facilities call. We also acknowledge NWO for providing access to Snellius, hosted by SURF through the Computing Time on National Computer Facilities call for proposals. Cees G. M. Snoek is (partially) funded by the Horizon Europe project ELLIOT (GA No. 101214398)."
        },
        {
            "title": "References",
            "content": "Nahid Alam, Karthik Reddy Kanjula, Surya Guthikonda, Timothy Chung, Bala Krishna Vegesna, Abhipsha Das, Anthony Susevski, Ryan Sze-Yin Chan, SM Uddin, Shayekh Bin Islam, et al. Maya: 25 An instruction finetuned multilingual multimodal model. arXiv preprint arXiv:2412.07112, 2024. Nahid Alam, Karthik Reddy Kanjula, Surya Guthikonda, Timothy Chung, Bala Krishna Vegesna, Abhipsha Das, Anthony Susevski, Ryan Sze-Yin Chan, SM Uddin, Shayekh Bin Islam, et al. Behind maya: Building multilingual vision language model. arXiv preprint arXiv:2505.08910, 2025. Niyati Bafna, Tianjian Li, Kenton Murray, David Mortensen, David Yarowsky, Hale Sirin, and Daniel Khashabi. The translation barrier hypothesis: Multilingual generation with large language models suffers from implicit translation failure. arXiv preprint arXiv:2506.22724, 2025. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth In Conference on Computer Vision and Pattern words: vit backbone for diffusion models. Recognition, 2023. Elisa Bassignana, Amanda Cercas Curry, and Dirk Hovy. The ai gap: How socioeconomic status affects language technology interactions. arXiv preprint arXiv:2505.12158, 2025. Lisa Beinborn and Rochelle Choenni. Semantic drift in multilingual representations. Computational Linguistics, 2020. Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing webscale image-text pre-training to recognize long-tail visual concepts. In Conference on Computer Vision and Pattern Recognition, 2021. Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025a. Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Zhongdao Wang, James T. Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In International Conference on Learning Representations, 2024a. Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In International Conference on Machine Learning, 2020. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025b. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024b. 26 Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Conference on Computer Vision and Pattern Recognition, 2024c. Zisheng Chen, Chunwei Wang, Xiuwei Chen, Hang Xu, Jianhua Han, and Xiaodan Liang. Semhitok: unified image tokenizer via semantic-guided hierarchical codebook for multimodal understanding and generation. arXiv preprint arXiv:2503.06764, 2025c. Reuben Cohn-Gordon and Noah Goodman. Lost in machine translation: method to reduce meaning loss. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019. Marta Costa-Jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672, 2022. Saurabh Dash, Yiyang Nan, John Dang, Arash Ahmadian, Shivalika Singh, Madeline Smith, Bharat Venkitesh, Vlad Shmyhlo, Viraat Aryabumi, Walter Beller-Morales, Jeremy Pekmez, Jason Ozuzu, Pierre Richemond, Acyr Locatelli, Nick Frosst, Phil Blunsom, Aidan Gomez, Ivan Zhang, Marzieh Fadaee, Manoj Govindassamy, Sudip Roy, Matthias Gallé, Beyza Ermis, Ahmet Üstün, and Sara Hooker. Aya vision: Advancing the frontier of multilingual multimodality. arXiv preprint arXiv:2505.08751, 2025. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. In Conference on Computer Vision and Pattern Recognition, 2025. Haiwen Diao, Yufeng Cui, Xiaotong Li, Yueze Wang, Huchuan Lu, and Xinlong Wang. Unveiling encoder-free vision-language models. arXiv preprint arXiv:2406.11832, 2024. Haiwen Diao, Xiaotong Li, Yufeng Cui, Yueze Wang, Haoge Deng, Ting Pan, Wenxuan Wang, Huchuan Lu, and Xinlong Wang. Evev2: Improved baselines for encoder-free vision-language models. arXiv preprint arXiv:2502.06788, 2025. Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, and Li Yi. DreamLLM: Synergistic multimodal comprehension and creation. In International Conference on Learning Representations, 2024. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In International Conference on Machine Learning, 2024. Fahim Faisal and Antonios Anastasopoulos. An efficient approach for studying cross-lingual transfer in multilingual language models. In Jonne Sälevä and Abraham Owodunni (eds.), Proceedings of the Fourth Workshop on Multilingual Representation Learning, 2024. Lijie Fan, Luming Tang, Siyang Qin, Tianhong Li, Xuan Yang, Siyuan Qiao, Andreas Steiner, Chen Sun, Yuanzhen Li, Tao Zhu, et al. Unified autoregressive visual generation and understanding with continuous tokens. arXiv preprint arXiv:2503.13436, 2025. 27 Felix Friedrich, Katharina Hammerl, Patrick Schramowski, Manuel Brack, Jindrich Libovicky, Kristian Kersting, and Alexander Fraser. Multilingual text-to-image generation magnifies gender stereotypes and prompt engineering may not help you. arXiv preprint arXiv:2401.16092, 2024. Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances on Neural Information Processing Systems, 2023. Alex Henry, Prudhvi Raj Dachapally, Shubham Pawar, and Yuxuan Chen. Query-key normalization for transformers. arXiv preprint arXiv:2010.04245, 2020. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. Shaoxiong Ji, Timothee Mickus, Vincent Segonne, and Jörg Tiedemann. Can machine transarXiv preprint lation bridge multilingual pretraining and cross-lingual transfer learning? arXiv:2403.16777, 2024. Yang Jiao, Haibo Qiu, Zequn Jie, Shaoxiang Chen, Jingjing Chen, Lin Ma, and Yu-Gang Jiang. Unitoken: Harmonizing multimodal understanding and generation through unified visual encoding. arXiv preprint arXiv:2504.04423, 2025. Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hérve Jégou, and Tomas Mikolov. Fasttext.zip: Compressing text classification models. arXiv preprint arXiv:1612.03651, 2016. Nithish Kannen, Arif Ahmad, marco Andreetto, Vinodkumar Prabhakaran, Utsav Prabhu, Adji Bousso Dieng, Pushpak Bhattacharyya, and Shachi Dave. Beyond aesthetics: Cultural competence in text-to-image models, 2024. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In IEEE International Conference on Computer Vision, 2023. Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. Julia Kreutzer, Eleftheria Briakou, Sweta Agrawal, Marzieh Fadaee, and Kocmi Tom. Déjà vu: Multilingual llm evaluation through the lens of machine translation evaluation, 2025. 28 Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024a. Haopeng Li, Jinyue Yang, Guoqi Li, and Huan Wang. Autoregressive image generation with randomized parallel decoding. arXiv preprint arXiv:2503.10568, 2025a. Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024b. Yafu Li, Ronghao Zhang, Zhilin Wang, Huajian Zhang, Leyang Cui, Yongjing Yin, Tong Xiao, and Yue Zhang. Lost in literalism: How supervised training shapes translationese in llms, 2025b. Yunshui Li, Yiyuan Ma, Shen Yan, Chaoyi Zhang, Jing Liu, Jianqiao Lu, Ziwen Xu, Mengzhao Chen, Minrui Wang, Shiyi Zhan, et al. Model merging in pre-training of large language models. arXiv preprint arXiv:2505.12082, 2025c. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Bingshuai Liu, Longyue Wang, Chenyang Lyu, Yong Zhang, Jinsong Su, Shuming Shi, and Zhaopeng Tu. On the cultural gap in text-to-image generation, 2023. Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with blockwise ringattention. arXiv preprint arXiv:2402.08268, 2024a. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances on Neural Information Processing Systems, 2024b. Haozhe Liu, Shikun Liu, Zijian Zhou, Mengmeng Xu, Yanping Xie, Xiao Han, Juan C. Pérez, Ding Liu, Kumara Kahatapitiya, Menglin Jia, Jui-Chieh Wu, Sen He, Tao Xiang, Jürgen Schmidhuber, and Juan-Manuel Pérez-Rúa. Mardini: Masked autoregressive diffusion for video generation at scale. arXiv preprint arXiv:2410.20280, 2024c. Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision, language, audio, and action. arXiv preprint arXiv:2312.17172, 2023. Chuofan Ma, Yi Jiang, Junfeng Wu, Jihan Yang, Xin Yu, Zehuan Yuan, Bingyue Peng, and Xiaojuan Qi. Unitok: unified tokenizer for visual generation and understanding. arXiv preprint arXiv:2502.20321, 2025. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, Ji Hou, and Saining Xie. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. 29 Ziqi Pang, Tianyuan Zhang, Fujun Luan, Yunze Man, Hao Tan, Kai Zhang, William T. Freeman, and Yu-Xiong Wang. Randar: Decoder-only autoregressive visual generation in random orders. arXiv preprint arXiv:2412.01827, 2024. William Peebles and Saining Xie. Scalable diffusion models with transformers. In IEEE International Conference on Computer Vision, 2023. Aidan Peppin, Julia Kreutzer, Alice Schoenauer Sebag, Kelly Marchisio, Beyza Ermis, John Dang, Samuel Cahyawijaya, Shivalika Singh, Seraphina Goldfarb-Tarrant, Viraat Aryabumi, Aakanksha, Wei-Yin Ko, Ahmet Üstün, Matthias Gallé, Marzieh Fadaee, and Sara Hooker. The multilingual divide and its impact on global ai safety, 2025. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Qi Qin, Le Zhuo, Yi Xin, Ruoyi Du, Zhen Li, Bin Fu, Yiting Lu, Jiakang Yuan, Xinyue Li, Dongyang Liu, et al. Lumina-image 2.0: unified and efficient image generative framework. arXiv preprint arXiv:2503.21758, 2025. Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel Du, Zehuan Yuan, and Xinglong Wu. Tokenflow: Unified image tokenizer for multimodal understanding and generation. In Conference on Computer Vision and Pattern Recognition, 2025. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. Aniket Rege, Zinnia Nie, Mahesh Ramesh, Unmesh Raskar, Zhuoran Yu, Aditya Kusupati, Yong Jae Lee, and Ramya Korlakai Vinayak. Cure: Cultural gaps in the long tail of text-to-image systems, 2025. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Conference on Computer Vision and Pattern Recognition, 2022. Israfel Salazar, Manuel Fernández Burda, Shayekh Bin Islam, Arshia Soltani Moakhar, Shivalika Singh, Fabian Farestam, Angelika Romanou, Danylo Boiko, Dipika Khullar, Mike Zhang, et al. Kaleidoscope: In-language exams for massively multilingual vision evaluation. arXiv preprint arXiv:2504.07072, 2025. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances on Neural Information Processing Systems, 2022. Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu Qi, Sen Wang, et al. Seaweed-7b: Cost-effective training of video generation foundation model. arXiv preprint arXiv:2504.08685, 2025. Luisa Shimabucoro, Ahmet Ustun, Marzieh Fadaee, and Sebastian Ruder. post-trainers guide to multilingual training data: Uncovering cross-lingual transfer dynamics. arXiv preprint arXiv:2504.16677, 2025. 30 Shivalika Singh, Angelika Romanou, Clémentine Fourrier, David Adelani, Jian Gang Ngui, Daniel Vila-Suero, Peerat Limkonchotiwat, Kelly Marchisio, Wei Qi Leong, Yosephine Susanto, et al. Global mmlu: Understanding and addressing cultural and linguistic biases in multilingual evaluation. arXiv preprint arXiv:2412.03304, 2024. Wei Song, Yuran Wang, Zijia Song, Yadong Li, Haoze Sun, Weipeng Chen, Zenan Zhou, Jianhua Xu, Jiaqi Wang, and Kaicheng Yu. Dualtoken: Towards unifying visual understanding and generation with dual visual vocabularies. arXiv preprint arXiv:2503.14324, 2025. Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, et al. Journeydb: benchmark for generative image understanding. Advances on Neural Information Processing Systems, 2023a. Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. arXiv preprint Autoregressive model beats diffusion: Llama for scalable image generation. arXiv:2406.06525, 2024. Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023b. Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal. Any-to-any generation via composable diffusion. Advances on Neural Information Processing Systems, 2023. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. Computing Research Repository, 2023. Eva Vanmassenhove, Dimitar Shterionov, and Andy Way. Lost in translation: Loss and decay of linguistic richness in machine translation. In Proceedings of Machine Translation Summit XVII: Research Track, pp. 222232, Dublin, Ireland, 2019. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. Shira Wein and Nathan Schneider. Lost in translationese? reducing translation effect using abstract meaning representation. arXiv preprint arXiv:2304.11501, 2023. Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. In Conference on Computer Vision and Pattern Recognition, 2025. Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion 31 models for text-to-video generation. 2023a. In IEEE International Conference on Computer Vision, Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. arXiv preprint arXiv:2309.05519, 2023b. Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Chengyue Wu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, et al. Sana 1.5: Efficient scaling of training-time and inferencetime compute in linear diffusion transformer. arXiv preprint arXiv:2501.18427, 2025a. Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and Mike Zheng Shou. Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion. In IEEE International Conference on Computer Vision, 2023. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. International Conference on Learning Representations, 2025b. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. Lijun Yu, José Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusion tokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. Xiang Yue, Yueqi Song, Akari Asai, Seungone Kim, Jean de Dieu Nyandwi, Simran Khanuja, Anjali Kantharuban, Lintang Sutawika, Sathyanarayanan Ramamoorthy, and Graham Neubig. Pangea: fully open multilingual multimodal llm for 39 languages. In International Conference on Learning Representations, 2024. David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. arXiv preprint arXiv:2309.15818, 2023. Chunting Zhou, LILI YU, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. In International Conference on Learning Representations, 2025."
        },
        {
            "title": "Appendix A",
            "content": "This appendix provides additional training details and qualitative results to supplement the main paper. Table 7 outlines the key hyperparameters used across the three pretraining stages and two instruction tuning stages of NeoBabel. Figure 12 presents representative multilingual generation examples, demonstrating visual consistency across six languages."
        },
        {
            "title": "Hyperparameters",
            "content": "1st Stage 2nd Stage 3rd Stage 1st Stage 2nd Stage"
        },
        {
            "title": "Training Steps\nWarmup Steps\nLearning Rate\nLearning Rate Decay\nOptimizer\nImage Resolution\nLLM Sequence Length\nLLM Vocab Size\nCodebook Size",
            "content": "500k 5000 1e 4 cosine AdamW 256 256 128 256k 8192 500k 5000 1e 4 cosine AdamW 256 256 512 256k 8192 500k 5000 1e 4 cosine AdamW 256 256 512 256k 8192 500k 5000 2e 4 cosine AdamW 512 512 512 256k 8192 200k 2000 5e 05 cosine AdamW 512 512 512 256k 8192 Table 7: Hyperparameters across training progression. 33 Figure 12: Qualitative Evaluation of NeoBabel. Each row corresponds to single concept expressed in six different languages: English, Chinese, Dutch, French, Hindi, and Persian. Although prompts are not shown for readability, all images were generated using translated versions of the same underlying prompt in each language. NeoBabel consistently produces semantically aligned and visually coherent results across languages, highlighting its strong multilingual generation capabilities. We intentionally omit the prompts here due to their length, focusing instead on the visual consistency across languages."
        }
    ],
    "affiliations": [
        "Cohere Labs",
        "University of Amsterdam"
    ]
}