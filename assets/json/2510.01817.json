{
    "paper_title": "Sparse Query Attention (SQA): A Computationally Efficient Attention Mechanism with Query Heads Reduction",
    "authors": [
        "Adam Filipek"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The Transformer architecture, underpinned by the Multi-Head Attention (MHA) mechanism, has become the de facto standard for state-of-the-art models in artificial intelligence. However, the quadratic computational complexity of MHA with respect to sequence length presents a significant barrier to scaling, particularly for applications involving long contexts. Prevailing solutions, such as Multi-Query Attention (MQA) and Grouped-Query Attention (GQA), have effectively addressed the memory bandwidth bottleneck that dominates autoregressive inference latency by sharing Key and Value projections. While highly successful, these methods do not reduce the fundamental number of floating-point operations (FLOPs) required for the attention score computation, which remains a critical bottleneck for training and full-sequence processing. This paper introduces Sparse Query Attention (SQA), a novel attention architecture that pursues an alternative and complementary optimization path. Instead of reducing Key/Value heads, SQA reduces the number of Query heads. This architectural modification directly decreases the computational complexity of the attention mechanism by a factor proportional to the reduction in query heads, thereby lowering the overall FLOPs. This work presents the theoretical foundation of SQA, its mathematical formulation, and a family of architectural variants. Empirical benchmarks on long sequences (32k-200k tokens) demonstrate that SQA can achieve significant throughput improvements of up to 3x in computation-bound scenarios such as model pre-training, fine-tuning, and encoder-based tasks, with only a minimal impact on model quality in preliminary smallscale experiments. SQA was discovered serendipitously during the development of the upcoming Reactive Transformer architecture, suggesting its potential as a powerful tool for building more efficient and scalable models"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 7 1 8 1 0 . 0 1 5 2 : r Sparse Query Attention (SQA): Computationally Efficient Attention Mechanism with Query Heads Reduction Adam Filipek (adamfilipek@rxai.dev) Reactive AI (https://rxai.dev) September 2025 Abstract The Transformer architecture, underpinned by the Multi-Head Attention (MHA) mechanism, has become the de facto standard for state-of-the-art models in artificial intelligence. However, the quadratic computational complexity of MHA with respect to sequence length presents significant barrier to scaling, particularly for applications involving long contexts. Prevailing solutions, such as Multi-Query Attention (MQA) and Grouped-Query Attention (GQA), have effectively addressed the memory bandwidth bottleneck that dominates autoregressive inference latency by sharing Key and Value projections. While highly successful, these methods do not reduce the fundamental number of floating-point operations (FLOPs) required for the attention score computation, which remains critical bottleneck for training and full-sequence processing. This paper introduces Sparse Query Attention (SQA), novel attention architecture that pursues an alternative and complementary optimization path. Instead of reducing Key/Value heads, SQA reduces the number of Query heads. This architectural modification directly decreases the computational complexity of the attention mechanism by factor proportional to the reduction in query heads, thereby lowering the overall FLOPs. This work presents the theoretical foundation of SQA, its mathematical formulation, and family of architectural variants. Empirical benchmarks on long sequences (32k-200k tokens) demonstrate that SQA can achieve significant throughput improvements of up to 3x in computation-bound scenarios such as model pre-training, fine-tuning, and encoder-based tasks, with only minimal impact on model quality in preliminary smallscale experiments. SQA was discovered serendipitously during the development of the upcoming Reactive Transformer architecture, context in which its computational advantages are maximized, suggesting its potential as powerful tool for building more efficient and scalable models."
        },
        {
            "title": "1.1 The Computational Burden of Self-Attention",
            "content": "The introduction of the Transformer architecture by Vaswani et al. (2017) marked paradigm shift in sequence modeling and has since become the foundational building block for most modern large language models (LLMs) and other advanced AI systems. The source of the Transformers remarkable power lies in its self-attention mechanism, which allows the model to dynamically weigh the importance of all tokens in sequence relative to each other, capturing complex, long-range dependencies without the sequential constraints of recurrent networks. This representational power, however, comes at steep price. The core of the self-attention mechanism involves computing dot product between matrix of queries (Q) and the transpose of matrix of keys (K), an operation whose computational and memory requirements scale 1 quadratically with the sequence length, . The computational complexity of the standard Multi-Head Attention (MHA) is formally expressed as O(N 2 dmodel +N d2 model), where dmodel is the models hidden dimension. For the long sequences that are increasingly common in modern applications, the 2 term becomes the dominant factor, making self-attention formidable computational bottleneck. This quadratic scaling limits the context length that models can feasibly process, hindering progress in areas such as long-document understanding, extended dialogue, and high-resolution multimodal processing. Consequently, the development of more efficient attention mechanisms has become one of the most critical areas of research in deep learning."
        },
        {
            "title": "1.2 A Tale of Two Bottlenecks: Computation vs. Memory Bandwidth",
            "content": "To develop effective optimizations, it is essential to recognize that the performance of Transformer models is constrained by two distinct, though related, bottlenecks. The failure to differentiate between these two challenges can lead to solutions that are highly effective in one scenario but suboptimal in another. The first is the computational bottleneck, which refers to the sheer volume of floatingpoint operations (FLOPs) required to execute the attention algorithm. This is primarily dictated by the matrix multiplication QKT , which involves O(N 2 dmodel) operations. This bottleneck is most prominent in scenarios where computations can be heavily parallelized and the full sequence is processed at once. Such scenarios include: Model Pre-training and Fine-tuning: During training, forward and backward passes are performed on large batches of long sequences, where the primary limitation is the raw throughput of the compute hardware (e.g., Tensor Cores on GPUs). Encoder Architectures: Models like BERT (Devlin, J., et al., 2019) or the encoder component of sequence-to-sequence models process the entire input sequence in single, parallel step. Prompt Processing in Decoders: When decoder-only LLM is given long prompt, the initial processing of that prompt to generate the first token is parallel operation on the entire prompt sequence, which is compute-bound. The second is the memory bandwidth bottleneck. This issue is most acute during autoregressive decoding, the token-by-token generation process used by LLMs for inference. At each generation step, the model must compute attention between the query for the new token and the keys and values for all previous tokens in the sequence. These past keys and values are stored in Key-Value (KV) cache in high-bandwidth memory (HBM). The bottleneck arises from the need to load this entire, ever-growing KV cache from HBM into the much faster but smaller on-chip SRAM of the GPU for every single token that is generated. For long sequences, the size of the KV cache can reach several gigabytes, and the time spent on this data transfer can far exceed the time spent on actual computation, making inference latency memory-bound. The research communitys focus has disproportionately gravitated towards solving the memory bandwidth bottleneck, driven by the pressing need for low-latency inference in commercial LLM applications. This has led to groundbreaking innovations but has also created an environment where optimizations for the equally important computational bottleneck of training and encoding have been comparatively underexplored. This reveals systemic bias toward specific application profilethe interactive, decoder-only LLMpotentially leaving significant performance gains on the table for other critical use cases."
        },
        {
            "title": "1.3 Existing Paradigms: Optimizing for the Memory Bottleneck",
            "content": "The dominant approaches to creating more efficient attention mechanisms, Multi-Query Attention (MQA) (Shazeer, 2019) and Grouped-Query Attention (GQA) (Ainslie et al., 2023), are masterful solutions designed explicitly to alleviate the memory bandwidth bottleneck. MQA takes radical approach by having all query heads share single, common projection for keys and values, dramatically reducing the KV cache size. GQA provides more nuanced interpolation between the standard MHA and the aggressive MQA. It divides the query heads into several groups and assigns shared key/value projection to each group. This allows model architects to strike balance, achieving most of the speed benefits of MQA while mitigating the potential quality degradation that can arise from having only single key/value representation. More recently, Multi-head Latent Attention (MLA), introduced in models like DeepSeekV2 (DeepSeek-AI, 2024), represents further evolution in this direction. MLA compresses the Key and Value tensors into low-rank latent representation before they are cached. This technique achieves an even greater reduction in the KV cache size, pushing the boundaries of memory efficiency for autoregressive inference. The progression from MHA to MQA, GQA, and MLA illustrates clear research trajectory focused on minimizing data movement from HBM. However, it is crucial to recognize their underlying mechanism: they optimize performance by reducing the size of the data being transferred, not the amount of computation performed. In all these methods, the number of query heads remains unchanged. As result, the size of the query matrix and the dimensions of the resulting attention score matrix remain the same as in MHA. Consequently, the number of FLOPs required for the QKT operation is not reduced. While they are indispensable for fast inference, they do not accelerate the compute-bound tasks of training or full-sequence encoding."
        },
        {
            "title": "1.4 The Broader Landscape of Efficiency",
            "content": "Beyond memory-centric optimizations, two other major research directions have emerged to tackle the Transformers scaling challenges: approximating full attention and developing entirely new architectural paradigms. 1.4.1 Approximating Full Attention: Sliding Window Mechanisms widely adopted technique to move from quadratic to linear complexity is Sliding Window Instead of attending to all tokens in the sequence, Attention (SWA) (Beltagy et al., 2020). each token only attends to fixed-size local window of neighboring tokens. This reduces the computational complexity from O(N 2) to O(N k), where is the window size. While highly effective, SWAs primary limitation is its inability to capture dependencies between tokens that are farther apart than the window size. To mitigate this, architectures like Longformer (Beltagy et al., 2020) combine SWA with few designated global tokens that can attend to the entire sequence. SWA is complementary mechanism, often used in conjunction with other attention variants like GQA in models such as Gemma (Google, 2024) and Mistral, to manage long contexts efficiently. 1.4.2 Architectural Alternatives to the Transformer more radical approach involves replacing the attention mechanism entirely with sub-quadratic alternatives. State Space Models (SSMs), exemplified by Mamba (Gu & Dao, 2023), have emerged as powerful alternative. Inspired by classical state space models from control theory, SSMs are designed to operate with linear complexity in sequence length, making them highly efficient for very long sequences. Similarly, Retentive Networks (RetNet) (Sun et al., 2023) derive connection between recurrence and attention to achieve parallelizable training process with linear-time inference. Other frameworks, such as Hierarchical Memory Transformers (HMT) (He et al., 2025), augment existing models with external memory and recurrent mechanisms to process sequences in chunks. These architectures represent fundamental departure from the Transformer paradigm, offering significant performance benefits at the cost of moving away from the well-established attention-based ecosystem. However, these alternatives remain niche due to notable challenges in scalability and training stability. SSMs, while theoretically linear, suffer from an illusion of state - their expressive power is limited similarly to Transformers, struggling with true long-distance dependencies and exhibiting RNN-like gradient issues on massive datasets (Deletang et al., 2024). RetNets hybrid nature introduces locality biases that degrade performance in translation or reasoning tasks. HMT addresses long contexts via hierarchy but incurs compression overhead and hardware scalability bottlenecks, limiting adoption in production-scale LLMs (He et al., 2025). In contrast, optimizations like SQA evolve the attention layer itself, preserving Transformers parallelism, mature tooling like, FlashAttention (Dao, T., et al., 2022), and representational strengths for tasks like encoder-based processing, while delivering constant-factor FLOPs reductions."
        },
        {
            "title": "1.5 Our Contribution: Sparse Query Attention (SQA)",
            "content": "This paper introduces Sparse Query Attention (SQA), novel attention mechanism that directly addresses the computational bottleneck by reducing the number of FLOPs. The core idea is instead of further reduction of simple yet counter-intuitive in the context of existing work: the number of key and value heads, SQA reduces the number of query heads. This architectural change has direct and profound impact on the computational graph. The complexity of the attention score calculation is proportional to the number of query heads. By reducing the number of query heads from (the total number of heads in comparable MHA model) to Hq (where Hq < H), SQA reduces the computational cost of the attention layer by factor of H/Hq. This is not memory optimization; it is fundamental reduction in the amount of arithmetic required. This work makes the following contributions to the field of efficient Transformer architectures: It introduces Sparse Query Attention (SQA), new attention mechanism that reduces computational complexity by factor of H/Hq, where is the total number of heads in baseline MHA model and Hq is the number of query heads in the SQA model. It provides rigorous mathematical formulation of SQA and formal analysis of its computational and memory complexity profiles, contrasting them with MHA, MQA, and GQA. It presents family of SQA variants, including Symmetric SQA (sSQA) and Extreme SQA (xSQA), which allow for exploration of the trade-off space between computational efficiency and model capacity. It empirically demonstrates through performance benchmarks that SQA achieves significant throughput improvements of up to 3x on long sequences (32k-200k tokens) in compute-bound scenarios, such as training and encoding, where MQA and GQA offer no speed advantage. It shows through preliminary, small-scale experiments that these substantial performance gains are achievable with only minor impact on model quality, motivating the need for further large-scale research. 4 Figure 1: Legend for attention operations diagrams"
        },
        {
            "title": "2.1 The Foundation: Multi-Head Attention (MHA)",
            "content": "The original Multi-Head Attention mechanism, introduced as the cornerstone of the Transformer model, was designed to maximize representational power. The key idea is to allow the model to jointly attend to information from different representation subspaces at different positions. Instead of performing single attention function, MHA runs multiple scaled dot-product attention operations, or heads, in parallel and concatenates their results. The core operation for each head is the scaled dot-product attention, defined as: Attention(Q, K, ) = softmax (cid:18) QKT dk (cid:19) (1) Here, Q, K, and are the Query, Key, and Value matrices, respectively, and dk is the dimension of the keys. The scaling factor dk is used to prevent the dot products from growing too large, which could push the softmax function into regions with extremely small gradients. In MHA, the input representation is first linearly projected into queries, keys, and values for each of the heads. The output of each head is then concatenated and passed through final linear projection. The full operation is defined as: MultiHead(Q, K, ) = Concat(head1, . . . , headh)W where each head is computed as: headi = Attention(QW , KW , i ) (2) (3) The matrices , and are learnable parameter matrices. This multi-headed structure allows each head to specialize and capture different types of relationships within the data, such as syntactic dependencies or long-distance semantic links. , , The computational complexity of MHA is dominated by two main components: the matrix multiplication for the attention scores (QKT ) and the matrix multiplication for the output projection. For sequence of length and model dimension of dmodel, with heads each of dimension dk = dmodel/h, the complexity of the score calculation is O(h 2 dk) = O(N 2 dmodel). The complexity of the final projection is O(N d2 model). Thus, the total complexity is O(N 2dmodel+N d2 model). For long sequences where > dmodel, this is effectively O(N 2dmodel), establishing the quadratic dependency that motivates the search for more efficient alternatives."
        },
        {
            "title": "2.2 Multi-Query Attention (MQA): A Radical Solution for Memory Band-",
            "content": "width As Transformer models grew in size and were applied to longer sequences, the cost of autoregressive inference became critical operational challenge. The MHA design, conceived in an era of smaller models, proved to be inefficient for this specific task. The analysis by Shazeer (2019) 5 Figure 2: Multi-Head Attention (MHA) operations identified the memory bandwidth required to load the KV cache as the primary performance limiter on modern accelerators like GPUs and TPUs. Multi-Query Attention (MQA) was proposed as direct and aggressive solution to this problem. The architecture is identical to MHA with one crucial difference: single Key and Value head is shared across all Query heads. This means that while there are still independent query projections, there is only one key projection and one value projection for the entire layer. The primary benefit of this design is dramatic reduction in the size of the KV cache. In MHA, the KV cache for sequence of length has size of 2 dk = 2 dmodel. In MQA, this is reduced to 2 dk. The amount of data that needs to be loaded from HBM at each decoding step is therefore reduced by factor of h, the number of heads. This directly translates to significant speed-up in inference, with reported improvements of up to 12x in certain configurations. However, this efficiency comes with trade-off. By forcing all query heads to share the same key and value representations, MQA reduces the models capacity. This can lead to degradation in model quality and has been observed to sometimes cause training instability. MQA represents an extreme point on the efficiency-quality spectrum, prioritizing speed above all else."
        },
        {
            "title": "2.3 Grouped-Query Attention (GQA): The Balanced Interpolation",
            "content": "Grouped-Query Attention (GQA) was introduced by Ainslie et al. (2023) as way to capture the benefits of MQA while mitigating its drawbacks. GQA is generalization that elegantly interpolates between the full capacity of MHA and the radical efficiency of MQA. In GQA, the query heads are divided into groups, where 1 h. Each group of h/g query heads shares single Key and Value head. This architecture provides tunable parameter, g, that allows model designers to control the trade-off between performance and quality. When = h, each query head has its own key/value head, and GQA becomes equivalent to MHA. When = 1, all query heads share single key/value head, and GQA becomes equivalent to MQA. 6 Figure 3: Multi-Query Attention (MQA) operations Figure 4: Grouped-Query Attention (GQA) operations By choosing an intermediate value for (e.g., = 8 for model with = 32 query heads), GQA can achieve substantial reduction in KV cache size and memory bandwidth requirements, leading to inference speeds that are comparable to MQA, while maintaining model quality that is much closer to that of MHA. GQA has become standard in modern LLMs like Qwen2 (Bai et al., 2024), as it achieves most of the inference speed of MQA while maintaining quality much closer to that of MHA."
        },
        {
            "title": "2.4 Multi-head Latent Attention (MLA)",
            "content": "Multi-head Latent Attention (MLA) is more recent innovation, introduced in the DeepSeek model family (DeepSeek-AI, 2024), that further optimizes the memory bottleneck. While GQA reduces the KV cache by sharing K/V projection matrices, MLA goes step further by compressing the and tensors themselves. It uses learned low-rank projection matrices to map the full and tensors into smaller, latent space before they are stored in the KV cache. This approach can yield significantly smaller memory footprint than GQA for the same 7 number of query heads. The trade-off is slight increase in computation during the projection step. MLA represents the logical endpoint of the memory-optimization trajectory, where the primary goal is to minimize the size of the data transferred from HBM during autoregressive decoding. This reinforces the distinction between memory-centric optimizations and the compute-centric approach of SQA. The progression from MHA through MQA to GQA and MLA is clear example of hardwareaware algorithmic design. These architectures are not just abstract mathematical constructs; they are pragmatic engineering solutions tailored to the memory hierarchy of modern GPUs. They directly attack the memory wallthe growing gap between compute speed and memory access speedby minimizing data movement, which is often more energy-intensive and timeconsuming than the computation itself. This evolutionary path highlights critical principle: the most effective deep learning architectures are those that are co-designed with the underlying hardware in mind. SQA continues this trend, but by targeting different aspect of the hardware: the computational units themselves. While MQA/GQA/MLA optimize for the memory bus, SQA additionally optimizes for the Tensor Cores."
        },
        {
            "title": "2.5 Sliding Window Attention (SWA)",
            "content": "Sliding Window Attention (SWA) is popular method for approximating full attention to reduce its quadratic complexity (Beltagy et al., 2020). The core idea is to restrict each tokens attention computation to local neighborhood or window of fixed size, k. For token at position i, it only attends to tokens in the range [i-k/2,i+k/2]. This changes the complexity from O(N 2 dmodel) to much more manageable O(N dmodel). The primary drawback of this approach is the introduction of strong locality bias. The model cannot directly capture dependencies between tokens that are separated by more than the window size. This can be detrimental for tasks requiring long-range reasoning. Architectures like Longformer (Beltagy et al., 2020) address this by combining SWA with few global attention tokens, which are allowed to attend to the entire sequence, creating hybrid sparse attention pattern. SWA is complementary mechanism, often used in models that also employ GQA to manage both computational complexity and memory bandwidth."
        },
        {
            "title": "3 Sparse Query Attention (SQA)",
            "content": "Sparse Query Attention (SQA) introduces new axis of optimization for attention mechanisms. It primarily targets the computational complexity of the attention score calculation, complementary approach to the memory-centric optimizations of MQA and GQA. It achieves this by reducing the number of query heads. While this is its main innovation, SQA still leverages the reduction of key/value heads to maintain memory efficiency, offering spectrum of configurations. Some variants, like Symmetric SQA, may consciously increase the number of K/V heads relative to GQA baseline to improve quality, making them ideal for full sequence processing and cases where KV cache size is less critical. Other variants, like baseline SQA and Extreme SQA, can be configured to match the memory footprint of GQA, making them suitable for traditional LLMs."
        },
        {
            "title": "3.1 The Core Concept: Reversing the GQA Paradigm",
            "content": "The conceptual foundation of SQA can be understood by asking simple question: What if, instead of reducing the number of Key and Value heads as in MQA and GQA, we reduce the number of Query heads? This line of inquiry leads to fundamentally different performance profile. 8 The computational cost of the scaled dot-product attention is primarily driven by the matrix multiplication QKT . The dimensions of this operation are (N dq) (dk ), where dq and dk are the total dimensions of the query and key projections, respectively. In standard MHA model, dq = dk = dhead = dmodel. The resulting attention score matrix has dimensions (N ). This computation is performed for each of the heads (or in batched manner), leading to total complexity proportional to 2 dhead. MQA and GQA reduce the number of unique key/value projections, but they still require the keys and values to be broadcast or repeated to match the full number of query heads before the attention computation. Therefore, the dimensions of the and matrices entering the dot product remain effectively the same, and the number of FLOPs is not reduced. SQA takes the opposite approach. By reducing the number of query heads to Hq < H, it directly shrinks the dimension of the query matrix Q. This results in smaller number of attention score calculations. As illustrated in the provided architectural diagrams, reducing the number of query heads leads to proportionally smaller number of attention weight matrices being computed, which in turn reduces the number of value heads that need to be aggregated. This directly translates into reduction in the total number of floating-point operations."
        },
        {
            "title": "3.2 Mathematical Formulation",
            "content": "Let us formalize the SQA mechanism. We define the following parameters: H: The total number of heads in comparable MHA baseline model. Hq: The number of query heads in the SQA layer (1 Hq < H). Hkv: The number of key/value heads in the SQA layer (1 Hkv Hq). dmodel: The hidden dimension of the model. dhead: The dimension of each attention head, typically set to dmodel/H. Given an input sequence representation RN dmodel, SQA first projects it into Query, Key, and Value matrices using learned weight matrices WQ, WK, and WV : = XWQ, where WQ Rdmodel(Hqdhead) = XWK, where WK Rdmodel(Hkvdhead) = XWV , where WV Rdmodel(Hkvdhead) (4) (5) (6) The resulting matrices Q, K, and are then reshaped to separate the head dimension: RN Hqdhead RN Hkvdhead RN Hkvdhead To perform the attention computation, the number of key and value heads must match the number of query heads. This is achieved by repeating the and tensors. Let the repetition factor be = Hq/Hkv. The key and value heads are repeated times along the head dimension to create expanded tensors RN Hqdhead and RN Hqdhead. This operation is analogous to the one used in GQA to match key/value heads to query groups. The scaled dot-product attention is then computed in parallel for each of the Hq query heads using the corresponding (repeated) key and value heads: headi = Attention(Qi, i, ) for = 1, . . . , Hq (7) 9 i, and where Qi, are the tensors for the i-th head. Finally, the outputs of the Hq heads are concatenated and passed through final linear projection R(Hqdhead)dmodel to produce the final output: SQA(X) = Concat(head1, . . . , headHq )W Note that the output projection matrix maps from smaller dimension (Hq dhead) back to dmodel, compared to MHA where it maps from (H dhead). (8) 3.2.1 Complexity Analysis The computational complexity of SQA can be analyzed by focusing on the dominant matrix multiplication steps. 1. Score Calculation (QKT ): This operation is performed for Hq heads. For each head, the multiplication is between matrix of shape (N dhead) and matrix of shape (dhead ). The complexity per head is O(N 2 dhead). Across all Hq heads, the total complexity for score calculation is O(Hq 2 dhead). 2. Value Aggregation: The multiplication of the attention scores (shape ) with the value matrix (shape dhead) also has complexity of O(N 2 dhead) per head, for total of O(Hq 2 dhead). The total complexity of the attention operations in SQA is therefore proportional to Hq 2 dhead. Now, consider the baseline MHA model, where the number of query heads is H. Its complexity is proportional to 2 dhead. By comparing the two, we can see that the computational complexity of SQA is factor of Hq/H relative to MHA. This leads to theoretical computational speed-up of: Speed-upSQA = ComplexityMHA ComplexitySQA = 2 dhead Hq 2 dhead = Hq (9) This formal derivation provides the theoretical foundation for SQAs performance benefits. 50% reduction in query heads leads to 2x reduction in computational cost for the attention mechanism."
        },
        {
            "title": "3.3 Architectural Variants",
            "content": "The SQA framework allows for variety of configurations, enabling trade-off between computational efficiency and model capacity. Several key variants are proposed and explored in this work: Standard SQA: This is the most general form, where the number of query heads Hq and key/value heads Hkv can be chosen independently, with the constraints that 1 Hq < and 1 Hkv Hq. This flexibility allows for fine-grained control over the models architecture. Symmetric SQA (sSQA): This is specific and compelling configuration where the number of query heads and key/value heads are equal and set to half the total number of heads in the baseline MHA model: Hq = Hkv = H/2. This variant is designed to achieve clean 2x computational speed-up over MHA while maintaining symmetric and balanced capacity for queries and keys/values. It represents principled reduction in complexity. Extreme SQA (xSQA): This category includes configurations that push the limits of query head reduction, typically where Hq H/4. These variants are designed to 10 Figure 5: symmetric Sparse Query Attention (sSQA) operations maximize computational savings and are useful for exploring the lower bounds of required query capacity before model quality degrades significantly. For example, an xSQA variant with Hq = H/8 would offer theoretical 8x speed-up in the attention computation."
        },
        {
            "title": "3.4 Synergy and Composability with Other Mechanisms",
            "content": "A key advantage of SQA is its architectural simplicity and composability. It can function as direct, drop-in replacement for any standard attention layer, including MHA, MQA, or GQA, without requiring other changes to the model architecture. This makes it straightforward to integrate into existing models and training pipelines. Furthermore, SQA is not mutually exclusive with other efficiency mechanisms; it is complementary. Its synergy with Sliding Window Attention (SWA) is particularly noteworthy. model can be constructed with hybrid SW-SQA layers. In such layer, the attention pattern is first restricted to local window (the SWA component), and then the attention computation within that window is accelerated by using reduced number of query heads (the SQA component). This combines the linear complexity scaling of SWA with the constant-factor FLOP reduction of SQA, offering powerful tool for building highly efficient models for very long sequences. This combination is also allowing to use longer sliding windows with the same efficiency."
        },
        {
            "title": "3.5 Comparative Positioning",
            "content": "To clarify SQAs unique contribution, it is useful to position it relative to the alternatives: vs. GQA/MLA: SQA reduces FLOPs by shrinking the query matrix. GQA and MLA reduce memory bandwidth by shrinking the KV cache. They target different bottlenecks (computation vs. memory), while some SQA variants have the same influence on memory bottleneck. 11 Figure 6: extreme Sparse Query Attention (xSQA) operations vs. SWA: SQA reduces the cost of the attention computation. SWA reduces the scope of the attention computation (from global to local). SQA is based on structural sparsity, while SWA on spatial. The benefit of the SQA is the access to all tokens, but with partial information about them. They are complementary optimizations that can be used together. vs. SSMs/RetNet: SQA is an evolution of the Transformers attention block, designed to make it more efficient. SSMs and RetNets are replacement for the attention block, representing different architectural paradigm. vs. HMT: SQA is layer-level architectural modification. HMT is framework-level system for managing long contexts via recurrence and memory chunks. They operate at different levels of abstraction."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Two groups of small-scale models were trained to evaluate SQA against established attention mechanisms. Dense Models: set of models with 10-12M parameters were trained for single epoch on 50% subset of the wikimedia/wikipedia (English) dataset. These models used hidden dimension of 256, 8 layers, and baseline of 16 total heads. Mixture-of-Experts (MoE) Models: smaller set of MoE models with 8.5M parameters were trained for 5 epochs on the roneneldan/TinyStories dataset (Eldan, R., & Li, Y. (2023)). These models used hidden dimension of 128, 6 layers, and baseline of 8 total heads. 12 Hardware and Software: Model quality experiments were conducted on NVIDIA L40S and L4 GPUs. Performance benchmarks were run on single NVIDIA A100 40GB GPU. The implementation used our internal RxNN framework (v0.1.59), with PyTorch 2.6.0 and Flash Attention 2.7.4.post1 (Dao, T., et al., 2022), and is publicly available in RxNNAttention (https://github.com/RxAI-dev/rxnn-Attention) library."
        },
        {
            "title": "4.2 Model Quality Evaluation",
            "content": "These experiments provide preliminary assessment of the impact of reducing query heads on model learning capacity. Due to budget constraints, these evaluations were conducted at small scale, but they provide valuable evidence of SQAs viability. 4.2.1 Dense Models (10-12M parameters) Models were trained with context size of 1024. The configurations and results are summarized in Table 1. The SQA variants demonstrate clear trade-off between performance and quality. Notably, sSQA and SQA achieve validation loss very close to GQA while completing training significantly faster. The xSQA variant is the fastest, with performance still slightly better than MQA. Table 1: Quality and Training Performance of Dense Models Model Hq (of 16) Hkv (of 16) Val. Loss Perplexity Accuracy (%) Time (min) MHA GQA MQA SQA sSQA xSQA xSMQA 16 16 16 8 8 4 4 16 4 1 4 8 4 1 1.1976 1.2177 1.2497 1.2272 1.2201 1.2428 1.2815 3.3121 3.3794 3.4893 3.4117 3.3875 3.4653 3.6020 77.35 77.12 76.64 76.97 77.05 76.74 76. 269 258 261 241 243 235 235 The models trained in this experiment are available on HuggingFace Hub. 1 4.2.2 Micro Mixture-of-Experts Models (8.5M parameters) These smaller MoE models were trained with short context of 256 tokens. The results in Table 2 show that even with very short sequences, the computational benefits of SQA are noticeable (3-4% faster training time), while the differences in validation loss are minimal. The sSQA configuration is particularly noteworthy, achieving loss nearly identical to GQA (a 0.3% difference) while being 2% faster. Table 2: Quality and Training Performance of MoE Models Model Hq (of 8) Hkv (of 8) Val. Loss Perplexity Accuracy (%) Time (min) GQA MQA SQA sSQA xSQA 8 8 4 4 2 2 1 2 4 2 1.139 1.158 1.159 1.142 1.169 3.124 3.184 3.187 3.133 3.219 70.66 70.33 70.32 70.63 70.12 398 399 387 390 1https://huggingface.co/ReactiveAI/SQAT-m, https://huggingface.co/ReactiveAI/sSQAT-m, https:// huggingface.co/ReactiveAI/xSQAT-m, https://huggingface.co/ReactiveAI/xSMQAT-m 13 The models trained in this experiment are available on HuggingFace Hub."
        },
        {
            "title": "4.3 Computational Performance Benchmarks",
            "content": "This section presents the core empirical validation of SQAs primary claim: that it significantly improves performance in compute-bound scenarios. The benchmarks were run on the dense model architecture, measuring the time per step for forward pass across various sequence lengths. Table 3: Performance Benchmarks for Long Sequence Processing (Time per step in seconds) Seq. Length xSQA SQA sSQA SWA (128) MQA GQA MHA 1,024 4,096 32,768 131,072 200,000 0.0570 0.0642 0.0637 0.0750 0.1348 0.1991 0.3759 0.6308 0.8194 1.4116 0.0669 0.0793 0.2117 0.6630 1.4824 0.0759 0.0794 0.1871 0.7531 1.1871 0.0760 0.1001 0.3612 1.2530 2.8555 0.0785 0.1027 0.3637 1.2558 2. 0.0869 0.1114 0.3727 1.2648 2.8734 The results provide clear and compelling confirmation of the theoretical benefits of SQA. As predicted, the MQA and GQA models show no significant performance improvement over the MHA baseline in this compute-bound setting. Their throughput is virtually identical as sequence length grows. All SQA variants are significantly faster than MHA, MQA, and GQA, and this performance gap widens dramatically as sequence length increases. At sequence length of 200k, the standard SQA model is over 2x faster than GQA (1.41s vs 2.86s), and the xSQA model is over 3.4x faster (0.82s vs 2.86s). For Sliding Window Attention with 128 tokens windows, sliding loop overhead dominates attention calculations. These benchmarks unequivocally demonstrate that SQA is highly effective at reducing computational load and accelerating processing in scenarios dominated by FLOPs. The direct correlation between the reduction in query heads and the increase in throughput validates the core mechanism of SQA. The results from our small-scale experiments suggest an important scaling dynamic. As demonstrated in Table 3, the throughput advantage of SQA over GQA/MHA grows superlinearly with sequence length, as the quadratic computational cost becomes the dominant factor. Conversely, comparing the results across our dense and MoE models (Tables 1 and 2), the validation loss gap between SQA variants and the GQA baseline remains minimal. This leads to the compelling hypothesis that as models and datasets scale, the representational capacity lost by reducing query heads may be increasingly negligible, while the computational and financial savings become ever more significant. Validating this scaling hypothesis is critical direction for future work. 2https://huggingface.co/ReactiveAI/GQA-Ref-Micro, MQA-Ref-Micro, sSQAT-mm, https://huggingface.co/ReactiveAI/xSQAT-mm https://huggingface.co/ReactiveAI/SQAT-mm, https://huggingface.co/ReactiveAI/ https://huggingface.co/ReactiveAI/"
        },
        {
            "title": "5.1 The Performance Profile of SQA: When and Why it Excels",
            "content": "SQAs advantages are most pronounced in any task that involves parallel, full-sequence processing. Crucially, SQA is not an alternative to sparse patterns like Sliding Window Attention but complementary technique; SQA can be used to reduce the computational cost of the attention calculated within each local window, compounding the efficiency gains and enabling longer sliding windows. Pre-training and Supervised Fine-tuning: These processes are fundamentally limited by computational throughput. 2-3x speed-up in the most expensive component of the model, as demonstrated by sSQA and xSQA, translates directly into substantial reduction in the time and financial cost of training. For organizations training models from scratch, this is significant advantage. Encoder Architectures: Any model that relies on an encoder stack, such as for natural language understanding, information retrieval, or as part of larger system, will benefit directly from SQA. Since encoders process the entire input sequence in parallel, their performance is compute-bound, making SQA an ideal choice. Prompt Processing Phase in LLMs: For modern LLMs that handle very long contexts, the initial processing of the users prompt can be significant source of latency. This prompt phase is parallel, non-autoregressive computation over the entire input sequence. SQA can drastically accelerate this step. For an application with 100k token context window, speeding up the prompt processing by 2-3x can noticeably improve the users time to first token experience. Conversely, during the autoregressive generation phase, SQAs computational advantage is less impactful. This phase is typically memory-bandwidth-bound, as the model loads the KV cache for each new token. In this regime, the performance of an SQA model will be primarily determined by the size of its KV cache, which is function of its number of key/value heads (Hkv). variant like sSQA with more K/V heads (Hkv = 16) than comparable GQA model (Hkv = 8) would have larger KV cache and could be slower during token generation. However, this is deliberate design choice for quality. SQA configurations can be designed to match the K/V head count of GQA (e.g., an xSQA model with Hq = 8, Hkv = 8), thereby matching its memory footprint and performance in memory-bound scenarios. This nuanced behavior does not diminish SQAs value but rather clarifies its optimal application domain. This leads to more sophisticated view of model architecture. The traditional, monolithic application of single attention type throughout model may be suboptimal. more principled approach would consider the distinct computational profiles of different phases of operation. For instance, future LLM architecture could dynamically use an SQA-like mechanism for the compute-bound prompt processing phase and then switch to GQA-like mechanism for the memory-bound generation phase. This concept of dynamic attention profile represents promising direction for architectural innovation, moving beyond one-size-fits-all approach to more context-aware design."
        },
        {
            "title": "5.2 Trade-offs and Broader Implications for LLMs",
            "content": "When considering the application of SQA to standard, monolithic LLMs like Llama or GPT, careful analysis of the trade-offs is required. As discussed, deploying an sSQA variant (Hq = 16, Hkv = 16) in model that currently uses GQA with 8 KV heads would double the size of the KV cache. This would likely increase memory consumption during inference and could slow down autoregressive generation. 15 However, this does not preclude the use of SQA in such models. The variants like xSQA offer compelling alternative. Consider xSQA configuration with Hq = 8 and Hkv = 8. This model would have the exact same KV cache size as standard GQA model with 8 KV heads. Therefore, its performance during memory-bound autoregressive generation would be identical. Yet, it would still benefit from theoretical 4x computational speed-up (H/Hq = 32/8 = 4) during the compute-bound phases of training and prompt processing. This presents highly attractive configuration: one that matches the state-of-the-art in inference efficiency while offering substantial acceleration for training and long-prompt ingestion. This demonstrates that SQA is not just niche solution but flexible framework that can provide significant value even within the constraints of existing LLM architectures."
        },
        {
            "title": "6 Future Work and Extensions",
            "content": "The promising results from our small-scale experiments strongly motivate the need for validation at larger scale. The immediate next step for this research will be to apply SQA to pretrained, open-source LLM. Specifically, we plan to conduct fine-tuning experiments on model such as Qwen3-0.6B, where the original GQA layers are replaced with our sSQA and xSQA variants. This will allow for direct and robust evaluation of SQAs impact on state-ofthe-art architecture and provide clearer insights into the quality-performance trade-off at scale. Beyond this direct validation, several other promising avenues for extending the SQA framework exist: Light SQA (lSQA): The variants tested in this work focused on aggressive query reduction (50% or more). It would be valuable to explore light SQA configurations with more modest reduction, for example, setting Hq = 0.75 H. Such model might offer 25% computational speed-up while potentially outperforming GQA on quality metrics, thus finding new sweet spot on the Pareto frontier. Reverse SQA (rSQA): An intriguing, though likely less performant, corner of the design space is to have fewer query heads than key/value heads (Hq < Hkv). In this setup, the query heads would be repeated to match the number of key/value heads. The computational complexity would then scale with Hkv instead of Hq. While this may not offer direct performance benefit, exploring its properties could yield deeper insights into the respective roles of queries and keys in the attention mechanism. Flex-SQA: This direction proposes combining SQA with advanced sparse attention patterns, such as those found in Googles Flex Attention or Longformer. These methods typically combine local (sliding window) attention with few global attention tokens. Implementing these patterns efficiently, especially with optimized kernels like FlashAttention, can be complex with the asymmetric head configurations of GQA. symmetric SQA configuration (where Hq = Hkv) could simplify the implementation and improve the performance of such hybrid patterns. This could enable models with SQA to handle extremely long sequences (e.g., 1M+ tokens) with high efficiency. SW-SQA (Sliding-Window SQA): simpler variant of the above would be to apply standard sliding window attention mechanism on top of an SQA layer. This would combine the FLOPs reduction of SQA with the linear complexity of sliding window attention, potentially creating highly efficient attention layer for tasks where locality is strong prior. These potential extensions highlight that SQA is not an endpoint but rather new building block for designing the next generation of efficient and scalable Transformer models."
        },
        {
            "title": "7 Conclusion",
            "content": "This paper has introduced Sparse Query Attention (SQA), novel attention mechanism that offers new and effective strategy for mitigating the computational cost of the Transformer architecture. By challenging the prevailing focus on memory bandwidth optimization and instead targeting the fundamental computational complexity of the attention score calculation, SQA carves out distinct and valuable niche in the landscape of efficient deep learning. The core contribution of SQA is its simple yet powerful architectural modification: reducing the number of query heads. This directly reduces the number of floating-point operations required by the attention layer, leading to theoretical speed-up of H/Hq. This work has provided the mathematical formulation for SQA, derived its complexity, and empirically validated its performance benefits. Benchmarks on long sequences conclusively show that SQA can accelerate compute-bound tasks like model training, fine-tuning, and encoding by up to 3x, domain where existing methods like MQA and GQA provide no advantage. Preliminary experiments suggest that these significant performance gains can be achieved with only modest and graceful trade-off in model quality. SQA is particularly well-suited for architectures where computational throughput for fullsequence processing is prioritized over minimizing the autoregressive KV cache. Furthermore, variants like Extreme SQA (xSQA) present compelling option for standard LLMs, offering the potential for faster training and prompt processing while matching the inference memory footprint of state-of-the-art GQA models. Ultimately, SQA demonstrates the value of exploring the full design space of attention mechanisms. The optimal architecture is not universal but is instead function of the specific task, hardware, and performance objectives. By providing new tool optimized for computational throughput, SQA empowers researchers and practitioners to build more scalable, efficient, and cost-effective models. To facilitate further research, validation, and adoption by the community, the implementation of SQA and its variants has been made publicly available in the RxNN-Attention library (https://github.com/RxAI-dev/rxnn-attention), in the transformers.attention module. The experiments in this paper were performed in our internal RxNN library (that will be published after Reactive Transformer release), using version 0.1.59 with PyTorch 2.6.0 and Flash Attention 2.7.4.post1."
        },
        {
            "title": "References",
            "content": "[1] Vaswani, A., et al. (2017). Attention Is All You Need. Advances in Neural Information Processing Systems, 30. [2] Bhojanapalli, S., et al. (2023). Low-Rank Bottleneck in Multi-head Attention. Advances in Neural Information Processing Systems, 36. [3] Shazeer, N. (2019). Fast Transformer Decoding: One Write-Head is All You Need. arXiv preprint arXiv:1911.02150. [4] Ainslie, J., et al. (2023). GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. arXiv preprint arXiv:2305.13245. 17 [5] Radford, A., et al. (2018). Improving Language Understanding by Generative PreTraining. OpenAI Technical Report. [6] Brown, T., et al. (2020). Language Models are Few-Shot Learners. Advances in Neural Information Processing Systems, 33. [7] Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & Re, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. arXiv preprint arXiv:2205.14135. [8] Dao, T. (2023). FlashAttention-2: Faster Attention with Better Parallelism and Work preprint arXiv:2307.08691. Partitioning. arXiv [9] Pope, W., Douglas, C., & Tri Dao. (2022). Efficiently Scaling Transformer Inference. arXiv preprint arXiv:2211.05102. [10] Beltagy, I., Peters, M. E., & Cohan, A. (2020). Longformer: The LongDocument Transformer. arXiv preprint arXiv:2004.05150. [11] Fu, Z., et al. (2025). Sliding Window Attention Training for Efficient Large Language Models arXiv preprint arXiv:2502.18845. [12] Google. (2024). Gemma: Open Models Based on Gemini Research and Technology. g.co/gemini/gemma. [13] Bai, J., et al. (2024). Qwen2: The New Generation of Qwen Open-Source Large Language Models. arXiv preprint arXiv:2406.17853. [14] Yang, J., et al. (2025). Qwen3 Technical Report. arXiv preprint arXiv:2505.09388. [15] DeepSeek-AI. (2024). DeepSeek-V2: Strong, Economical, and Open-Source Mixture-of-Experts Language Model. arXiv preprint arXiv:2405.04434. [16] DeepSeek-AI. Technical arXiv:2412.19437. Report. (2024). DeepSeek-V3 preprint arXiv [17] DeepSeek-AI. DeepSeek-R1: (2025). Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv preprint arXiv:2501.12948. [18] Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). BERT: Pretraining of Deep Bidirectional Transformers for Language Understanding. arxiv preprint arXiv:1810.04805. [19] Raffel, C., et al. (2019). Exploring the Limits of Transfer Learning with Unified Text-to-Text Transformer. arXiv preprint arXiv:1910.10683. [20] Eldan, R., & Li, Y. (2023). TinyStories: How Small Can Language Models Be and Still Speak Coherent English? arXiv preprint arXiv:2305.07759. [21] Merity, S., et al. (2016). Pointer Sentinel Mixture Models. arXiv preprint arXiv:1609.07843. [22] Xu, C., et al. (2025). Comprehensive Survey of Mixture-of-Experts: Algorithms, Theory, and Applications arXiv preprint arXiv:2503.07137. [23] Wang, Y., et al. (2025). From S4 to Mamba: Comprehensive Survey on Structured State Space Models. arXiv preprint arXiv:2503.18970. [24] Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint arXiv:2312.00752. [25] He, Z., et al. (2024). HMT: Hierarchical Memory Transformer for Efficient Long Context Language Processing. arXiv preprint arXiv:2405.06067. [26] Sun, Y., et al. (2023). Retentive Network: Successor to Transformer for Large Language Models. arXiv preprint arXiv:2307.08621. [27] Deletang, G., Ruoss, A., Grau-Moya, J., Genewein, T., Wenliang, L. K., Catt, E., Cundy, C., Marcus, G., Yang, C., & Orseau, L. (2024). The Illusion of State in State-Space Models. arXiv preprint arXiv:2404.08819. [28] Chen, L., et al. (2023). Survey on Memory-Augmented Neural Networks: Challenges in Scalability and Compression. arXiv preprint arXiv:2312.06141."
        }
    ],
    "affiliations": [
        "Reactive AI"
    ]
}