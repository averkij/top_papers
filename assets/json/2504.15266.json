{
    "paper_title": "Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction",
    "authors": [
        "Vaishnavh Nagarajan",
        "Chen Henry Wu",
        "Charles Ding",
        "Aditi Raghunathan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We design a suite of minimal algorithmic tasks that are a loose abstraction of open-ended real-world tasks. This allows us to cleanly and controllably quantify the creative limits of the present-day language model. Much like real-world tasks that require a creative, far-sighted leap of thought, our tasks require an implicit, open-ended stochastic planning step that either (a) discovers new connections in an abstract knowledge graph (like in wordplay, drawing analogies, or research) or (b) constructs new patterns (like in designing math problems or new proteins). In these tasks, we empirically and conceptually argue how next-token learning is myopic and memorizes excessively; comparatively, multi-token approaches, namely teacherless training and diffusion models, excel in producing diverse and original output. Secondly, in our tasks, we find that to elicit randomness from the Transformer without hurting coherence, it is better to inject noise right at the input layer (via a method we dub hash-conditioning) rather than defer to temperature sampling from the output layer. Thus, our work offers a principled, minimal test-bed for analyzing open-ended creative skills, and offers new arguments for going beyond next-token learning and softmax-based sampling. We make part of the code available under https://github.com/chenwu98/algorithmic-creativity"
        },
        {
            "title": "Start",
            "content": "Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction Vaishnavh Nagarajan * 1 Chen Henry Wu * 2 Charles Ding 2 Aditi Raghunathan 2 5 2 0 2 1 2 ] . [ 1 6 6 2 5 1 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We design suite of minimal algorithmic tasks that are loose abstraction of open-ended realworld tasks. This allows us to cleanly and controllably quantify the creative limits of the presentday language model. Much like real-world tasks that require creative, far-sighted leap of thought, our tasks require an implicit, open-ended stochastic planning step that either (a) discovers new connections in an abstract knowledge graph (like in wordplay, drawing analogies, or research) or (b) constructs new patterns (like in designing math problems or new proteins). In these tasks, we empirically and conceptually argue how nexttoken learning is myopic and memorizes excessively; comparatively, multi-token approaches, namely teacherless training and diffusion models, excel in producing diverse and original output. Secondly, in our tasks, we find that to elicit randomness from the Transformer without hurting coherence, it is better to inject noise right at the input layer (via method we dub hashconditioning) rather than defer to temperature sampling from the output layer. Thus, our work offers principled, minimal test-bed for analyzing open-ended creative skills, and offers new arguments for going beyond next-token learning and softmax-based sampling. We make part of the code available under https://github.com/ chenwu98/algorithmic-creativity 1. INTRODUCTION Not all forms of intelligence are solely about being correct In open-ended tasks, what also matters is or wrong. finding creative ways to satisfy request, making surprising *Equal contribution 1Google Research, US 2Carnegie Mellon University, Pittsburgh, US. Correspondence to: Vaishnavh Nagarajan <vaishnavh@google.com>, Chen Henry Wu <chenwu2@cs.cmu.edu>. Preprint. and fresh connections never seen before. For instance, consider responding to highly under-specified prompts Generate challenging high-school word like problem involving the Pythagoras Theorem. or Suggest some candidate therapeutic antibodies targeting the HER2 antigen. or Provide vivid analogy to differentiate quantum and classical mechanics. Creativity in such tasks requires generating responses that are not just correct or coherent, but are also diverse across responses and are original compared to the training data. These currentlysidelined desiderata will rise to prominence as we explore LLMs for open-ended scientific discovery (Gruver et al., 2023; Romera-Paredes et al., 2024; Si et al., 2024; Lu et al., 2024a), for generating novel training data (Yu et al., 2024; Yang et al., 2024c; Wang et al., 2023), and as we scale up test-time compute approaches that benefit from diversity in exploration, such as best-of-N (Cobbe et al., 2021; Chow et al., 2024; Dang et al., 2025) and long chain-of-thought reasoning (OpenAI, 2024; DeepSeek-AI, 2025; Snell et al., 2024; Wu et al., 2024). Unlike simple open-ended tasks like generating names and basic sentences (Zhang et al., 2024b; Hopkins et al., 2023), many creative tasks (like designing clever Olympiad problem) are said to involve random flash of creative insight termed variously as leap of thought (Wang et al., 2024a; Talmor et al., 2020; Zhong et al., 2024), eureka moment (Bubeck et al., 2023), mental leap (Holyoak & Thagard, 1995; Callaway, 2013; Hofstadter, 1995) or an incubation step (Varshney et al., 2019). The thesis of this paper is that learning to solve such creative leap-of-thought tasks (defined shortly) is misaligned with the current language modeling paradigm (a) in terms of next-token learning, and (b) in how randomness is elicited. We articulate these two concerns by designing suite of algorithmic tasks inspired by such creative tasks. We then demonstrate how the creativity of language models suffers in these tasks, and how this can be alleviated (to an extent, within our tasks). Concretely, for the scope of this paper, creative leap-ofthought task refers to tasks that involve search-and-plan process; crucially, this process orchestrates multiple random decisions in advance before generating the output. Typically, GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION such leap of thought is highly implicit in the text to infer it, one has to deeply engage with the text and detect higher-order patterns in it. We think of tasks like designing satisfying math problem, generating worthwhile research ideas, or drawing surprising analogies as examples of such tasks. Ideally, one would directly study these real-world tasks to quantify the limits of language models. Indeed, flurry of recent works report that LLM-generated research ideas tend to be rephrased from existing ideas (Gupta & Pruthi, 2025; Beel et al., 2025) and that LLM outputs tend to be less creative than humans e.g., Chakrabarty et al. (2024); Lu et al. (2024b) (See J). While assessing real-world tasks is lofty goal, the evaluations are subjective (Wang et al., 2024b; Runco & Jaeger, 2012), and when the model has been exposed to all of the internet, originality is hard to ascertain. Thus, the conclusions will inevitably invite debate (such as Si et al. (2024) vs. Gupta & Pruthi (2025) or Lu et al. (2024a) vs. Beel et al. (2025)). In search of more definitive conclusions, we approach from different angle: we study minimal and controllable tasks that are loose abstractions of real-world tasks and yet allow one to rigorously quantify originality and diversity. This follows along the lines of recent works that have studied the diversity of models in graph path-finding (Khona et al., 2024) and generating challenging CFGs (Allen-Zhu & Li, 2023b). Broadly, we refer to such tasks as open-ended algorithmic tasks. Our aim is to design tasks more minimal than these prior tasks, and crucially, tease apart distinct computational skills required for creativity. This will allow us to systematically investigate issues in the current paradigm of model training and propose alternatives. As our first main contribution, we draw inspiration from cognitive science literature (Boden, 2003) (see also Franceschelli & Musolesi (2023)) to design algorithmic tasks isolating two distinct types of creative leaps of thought. The first class of tasks involves combinational creativity: drawing novel connections in knowledge, like in research, wordplay or drawing analogies (see Fig 1 for task description). The second class of tasks involves exploratory creativity: constructing fresh patterns subject to certain rules, like in designing problems and suspense (see Fig 2). In these tasks, we can precisely evaluate models for the fraction of generations that are coherent, unique and original (not present in training set). We term this metric algorithmic creativity to denote that it is solely evaluates the computational aspects of creativity. Within this framework, we articulate two creative limits of the current language modeling paradigm. First, we empirically find that next-token learning achieves lower algorithmic creativity (and higher memorization) compared to multi-token approaches, namely, teacherless training (Bach- (a) Sibling Discovery (b) Triangle Discovery Figure 1. Minimal tasks inspired by combinational creativity: Skills like research, humor and analogies often require identifying novel multi-hop connections from known pair-wise relationships in knowledge graph. For instance, creating the wordplay What kind of shoes do spies wear? Sneakers. requires searching over semantic graph, and carefully planning pair of words (shoes, spies) that lead to mutual neighbor (sneakers). Inspired by this, we define tasks where symbolic graph is stored in the model weights; the model is exposed to example node sequences that form specific multi-hop structure (like sibling or triangle) during training. The model must infer this structure from training; during inference, the model must implicitly recall-search-and-plan to generate novel and diverse node sequences obeying the same structure in the in-weights graph. Pictured are two example tasks with symbolic graph each, and corresponding example sequence obeying sibling (g, f, Y) or triangle structure (a, b, c). More details in 2.3 and Fig 9. (a) Circle Construction (b) Line Construction Figure 2. Minimal tasks inspired by exploratory creativity: Skills like designing problem sets, novel proteins and plots require devising patterns that can be resolved in novel ways through some general rules. Inspired by this, we design task where during training, we expose the model to adjacency lists that implicitly resolve into specific structure (a circle or line graph) under some permutation. The model must infer this higher-order structure; during inference, the model must generate adjacency lists resolving to the same structure, but under novel and diverse permutations. Pictured are example sequences and the corresponding implicit structure they would resolve to. See 2.4 and Fig10. mann & Nagarajan, 2024; Monea et al., 2023; Tschannen et al., 2023) and diffusion models (Hoogeboom et al., 2021; Austin et al., 2021; Lou et al., 2023) (see Fig 3 and Fig 4). Our argument is that in all our tasks, inferring the latent leap of thought requires observing global higher-order patterns rather than local next-token patterns in the sequence. Next, we turn to the de facto approach for randomization in Transformer: temperature sampling from the output softmax layer. We contrast this against an input-layer randomization approach we call hash-conditioning where we GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION This presents challenge to recent proposals that aim to fix next-token prediction via permutations (Pannatier et al., 2024; Thankaraj et al., 2025) or partial lookaheads (Bavarian et al., 2022; Fried et al., 2022; Kitouni et al., 2024; Nolte et al., 2024). As second direction of progress, we hope our work provides foundation to think about open-ended tasks which are extremely hard to quantify in the wild. This may spur more algorithmic explorations on improving diversity (such as our approach of hash-conditioning) and on curbing verbatim memorization in language models. Our contributions: 1. We create minimal, controlled and easy-to-quantify openended algorithmic tasks. These tasks isolate, and loosely capture two fundamental modes of creativity. 2. We find that multi-token prediction through teacherless training or diffusion, results in significantly increased algorithmic creativity and reduced memorization in our tasks compared to next-token prediction. 3. Our argument provides new support for multi-token prediction, going beyond (B&N24). We show gap in creativity in an open-ended task (rather than correctness in deterministic one), in much simpler 2-token-lookahead tasks, and in tasks where no token permutation is friendly to next-token-learning. 4. We find that hash-conditioning i.e., training with random hash prefixes, greatly improves diversity of algorithmic creativity in our tasks, compared to the standard paradigm of temperature sampling. 2. OPEN-ENDED ALGORITHMIC TASKS & TWO TYPES OF CREATIVITY We are interested in designing simple algorithmic tasks that are loosely inspired by endeavors such as generating scientific ideas, wordplay, narration, or problem-set design, where one needs to generate strings that are both interesting and never seen before. In all these tasks, before generating the output, one requires (creative) leap of thought, process that (a) is implicit i.e., is not spelled out in token space (or is even inherently hard to spell out), (b) involves discrete random choices (c) and together, those choices must be coherent in that they are carefully planned to satisfy various non-trivial, discrete constraints. These constraints fundamentally define the task and make it interesting e.g., word problem should be solvable by arithmetic rules, or pun must deliver surprising punchline. The goal in such open-ended tasks is not just coherence though, but also diversity and novelty generations must be as varied as possible and must not be regurgitated training data. Before we design tasks that capture the aforementioned leap of Figure 3. Multi-token teacherless finetuning improves algorithmic creativity (top; Eq 1) and reduces memorization (bottom; fraction of generations seen during training) on our four openended algorithmic tasks for Gemma v1 (2B) model. train models with random hash prefixes. We find that, in our tasks, not only does hash-conditioning induce non-trivial algorithmic creativity (even with deterministic, greedy decoding!), hash-conditioning is also competitive with or better than the conventional output-randomization (i.e., temperature sampling). Intuitively, maximizing diversity at the output-token-level is computationally burdensome: it requires simultaneously processing diverse set of leaps of thoughts to compute marginalized token distribution. It is easier to first sample single latent leap of thought, and then compute the token conditioned on that one leap. We conjecture that hash-conditioning enables this conditioned token generation. Overall, we hope our study advances the field in two directions. First, we provide new angle to advocate for multi-token approaches, orthogonal to the path-star example in Bachmann & Nagarajan (2024) (or B&N24 in short). Whereas, the path-star example portrays gap in correctness of reasoning, ours shows gap in diversity of open-ended thinking. We note though that B&N24 is an impossibility result where next-token learning breaks down spectacularly (unless there is exponential data or compute), while ours is data-inefficiency result (where next-token learning occurs but is mediocre). Next, the gap we show appears even in 2token-lookahead tasks as against the many-token-lookahead path-star task. Third, and perhaps most conceptually important is the fact that, while the path-star task is amenable to next-token prediction upon reversing the tokens, we identify tasks where no re-ordering is friendly towards next-token prediction the optimal thing to do is to globally learn higher-order patterns implicit in the whole future sequence. 3 GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION thought, we first clarify what tasks do not require such step. or chemistry. The leap of thought here requires searching over all possible sequences, constrained by set of rules. Open-ended tasks that do not require leap of thought. One simple open-ended task that may come to mind is generating uniformly-random known entities, like celebrity names (Zhang et al., 2024b). However, there is no opportunity to create novel string here. more interesting example may be generating grammatically coherent PCFG strings following subject verb object format e.g., the cat chased rat (Hopkins et al., 2023). While novel strings become possible here, no sophisticated leaps of thought are involved; each token can be generated on the fly, satisfying local next-token constraint to be coherent. In light of this, we can rephrase our goal as designing openended, creative tasks where coherence requires satisfying more interesting, global constraints. To build this systematically, we draw inspiration from literature in cognitive science (Boden, 2003). Boden (2003) argues that fundamentally, there are three forms of creativity in that order: combinational, exploratory and transformative. We elaborate on the first two (the last, we do not look at). 2.1. The fundamental types of creativity (Boden, 2003) Combinational creativity.1 Consider rudimentary wordplay of the form What musical genre do balloons enjoy? Pop music. or What kind of shoes do spies wear? Sneakers. There is global structure here: two unrelated entities (genre & balloons) are related eventually through punchline (pop); the punchline itself is mutual neighbor on semantic graph. More broadly, Boden (2003) argues that many tasks, like the above, involve making unfamiliar combinations of familiar ideas or the unexpected juxtaposition of [known] ideas. Other tasks include drawing analogies, or finding connections between disparate ideas in science.2 All these tasks involve leap of thought that in effect searches and plans over space of known facts and combines them. Exploratory creativity. Consider the act of developing mystery or designing logical puzzles. These endeavors are not as knowledge-heavy. What they crucially require is constructing fresh patterns that satisfy some highly nontrivial global constraint e.g., being resolvable as per some rules (e.g., logic). Such endeavors fall into second class of exploratory creativity in Boden (2003). This includes much grander forms of exploration e.g., exploring various forms of outputs within stylistic constraint, or exploring various corollaries within theoretical paradigm in physics 1Some call it combinatorial creativity. We use the term from Boden (2003), combinational. 2Even this very papers idea draws connection between the existing ideas of multi-token prediction, limits of next-token prediction and creative planning tasks. In the upcoming sections, we will attempt to capture some (not all) core computational aspects of rudimentary instances within the two classes of creative skills above. We emphasize that by no means does our minimal algorithmic setup intend to capture the human values that go into these endeavors; nor do they capture the rich array of creative acts that Boden (2003) discusses within these categories. (See limitations in 6) 2.2. The basic setting and notations In all our tasks, we assume the standard generative model setting: the model must learn an underlying distribution through training set of independent samples si D. The distribution is over space VL of L-length strings. The tasks are open-ended in that there is no one correct answer at test-time. The goal is to produce random string from D, much like responding to the query Design highschool word problem. Coherence: Each task is defined by boolean coherence function coh : VL (cid:55) {true, false} which is true only on the support i.e., supp(D) = {s VL coh(s)}. The exact form of coh will be defined in each algorithmic task but broadly, we are interested in scenarios where determining coherence requires global understanding of the whole string. This is inspired by the fact that wordplay must have preplanned punchline connecting what comes before, or word problem must be solvable. We can think of to be simple uniform distribution over all coherent strings. Algorithmic creativity: Upon witnessing finite set of examples, the model must learn to generate only strings that are (a) coherent, (b) original (not memorized) and (c) diverse (covers the whole support). An exact quantification of this is computationally expensive in our tasks. Instead, we approximate it by sampling set of many independent generations from the model and computing the fraction of that is original, coherent and unique. Let the boolean memS(s) denote whether an example is from the training set and let the integer function uniq(X) denote the number of unique examples in set X. (The exact definitions of these quantities vary by tasks, as we will see). Then, we define our (empirical) algorithmic creativity metric: ˆcrN (T ) = uniq({s memS(s) coh(s)}) . (1) Our setup models an in-distribution form of novelty as it offers rigorous and tractable way to study the problem. Admittedly though, this is far simpler form of novelty than what is expected in real-world tasks. Nevertheless, even this 4 GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION simple setting will help us appreciate the limits of current language models. next-token learning. More on this in 2.6. 2.3. Tasks inspired by combinational creativity Combinational creativity requires recall-and-search through entities from memory subject to the constraint that they relate to each other in an interesting way. We abstract this through tasks that discover structures from an in-weights graph i.e., graph stored in the model weights, not reveal in context. 2.3.1. SIBLING DISCOVERY This task involves an implicit, bipartite made of parent vertices = {A, B, C, . . .} each neighboring corresponding set of children nbr(A) = {a1, a2, . . . , }, nbr(B) = {b1, b2, . . .} and so on. We define coh(s) to hold true on sibling-parent triplets of the form = (γ, γ, Γ) such that γ, γ nbr(Γ). We then consider uniform distribution over all coherent strings (γ, γ, Γ) for fixed graph G. The model witnesses i.i.d samples from this distribution. During test-time, the model must maximize algorithmic creativity (Eq 1) by generating novel parent-sibling triplets based on its in-weights knowledge of G. Note that the model is not provided the graph in-context as this would sidestep core computational step in combinational creativity: recalling facts from large memory (see B.2). The hope is that the model infers and stores the pairwise adjacencies of in its weights (given sufficient data). Full dataset description is in and Fig 9. We view this task as an abstraction of the wordplay example. One can think of the parent Γ as the punchline that delivers connection between otherwise non-adjacent vertices, in the same way sneaker surprisingly connects the otherwise non-adjacent words, spies and shoes. note on the leap of thought. To concretely illustrate what we mean by leap of thought, we note that the above task can be designed with or without leap. Observe that the most natural order of generation is to generate the parent vertex (i.e., punchline) first, and pick the siblings after (conditioned on the parent). Thus, if the task demanded the ordering (Γ, γ, γ), it would involve no leap of thought: each next token can be learned and generated through simple rules conditioned on the past, without planning. However, the word play example involves non-sequential leap of thought in that even though the punchline (the parent) appears last, it must be planned ahead of time. Paralleling this leap-of-thought structure, we define our sibling discovery task to generate the triplets as = (γ, γ, Γ), where the siblings appear first. We hypothesize that this (siblingfirst) construction is adversarial towards next-token learning, while reversed (parent-first) dataset is friendlier towards 2.3.2. TRIANGLE DISCOVERY Next, we design task that requires more complex, higher-order planning: generating triangles from an appropriately-constructed knowledge graph = (V, E) (which contains many triangles; see C). Thus, in this task coh((v1, v2, v3)) = true iff all three edges between {v1, v2, v3} belong in G. Furthermore, we define uniq() and mem() such that various permutations of the same triangle are counted as one (see details in C, including the exact formatting of the string). Note that the leap of thought in this task is much harder to learn and execute as it requires co-ordinating three edges in parallel, from memory. This type of higher-order planning task can be thought of an abstraction of more complex wordplay (like antanaclasis, where word must repeat in two different senses in sentence, while still being coherently related to the rest of the sentence) or creating word games (like crosswords) or discovering contradictions or feedback loops in body of knowledge, an essential research skill see B.3. 2.4. Tasks inspired by exploratory creativity Recall that we are also interested in creativity that involves constructing new structures. For instance, this may be designing word problems that correspond to novel solutions. Below, we capture this through tasks that construct adjacency lists of structured graphs. Note that no knowledge graph is involved in these tasks. 2.4.1. CIRCLE CONSTRUCTION In this task, the generated strings must be randomized adjacency lists that can be rearranged to recover circle graphs of vertices. Let the generated list be = (vi1, vi2), (vi3, vi4), . . .. We define coh(s) = true iff there exists resolving permutation π such that π(s) = (vj1, vj2), (vj2, vj3), . . . (vjn , vj1 ) for distinct j1, j2, . . . jn. i.e., each edge leads to the next, and eventually circles back to the first vertex. We define uniq and mem such that different examples with the same resolving π are counted as the same, even if they have differing vertices. As always, the learner is then exposed to finite set of uniformly sampled coherent strings. Note that the latent leap of thought here requires constructing novel permutation π before generating the sequence. Loosely, we can think of the resolving permutation π as how conflict in story or word problem or puzzle is solved; the vertices as characters or mathematical objects; the rules of rearranging an adjacency list as rules of logic, math or story-building. The creative goal in this task is to create novel dynamics in the conflict, or equivalently, novel 5 GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION dynamics in how the conflict is resolved. Thus if only the entities differ, but the plot dynamics remain unaltered, we count them as duplicates. See details in C. 2.4.2. LINE CONSTRUCTION simple variant of the above task is one where the edge set corresponds to line graph. The resolving permutation π is such that π(s) = (vj1, vj2), (vj2 , vj3 ) . . . , (vjn1, vjn ) for distinct j1, j2, . . . jn. i.e., each edge leads to the next until dead-end. 2.5. Permutation-invariance of our tasks We emphasize key novel aspect in our last three tasks. Many algorithmic tasks in literature like addition (Lee et al., 2024), or the path-star task (B&N24) or Sibling Discovery have natural ordering in which the tokens can be learned and generated, even if it may not be left to right. However, the Triangle Discovery, Line Construction and Circle Construction tasks are permutationinvariant no token is more privileged than the other, and hence all tokens must be simultaneously learned to infer the underlying process. Intuitively, we view this as an abstraction of real-world tasks where the creative process is highly implicit, and not immediate from the text. These tasks offer test-bed even for non-next-token approaches that rely on re-permuting the tokens (Pannatier et al., 2024; Thankaraj et al., 2025) or predicting only part of the future (Kitouni et al., 2024; Nolte et al., 2024; Bavarian et al., 2022; Fried et al., 2022). 2.6. How next-token learning may suffer in our tasks Much like in sophisticated creative tasks, in our tasks, the most natural way to generate the string is by planning various random latent choices (say z) in advance and by producing plan-conditioned distribution p(sz) over coherent strings s. However, next-token prediction (NTP) or nexttoken learning to be precise we argue, is myopic and fails to learn such latent plan. Our argument extends that of B&N24 to our even simpler tasks. Consider learning the Sibling Discovery where we must generate sibling-parent triplets (γ, γ, Γ). Even if the parent must be emitted last, the most natural generative rule is to plan the parent first and decide the children last. We can think of this as learning latent plan := Γ. Then, learning the plan-conditioned generation p(γ, γ, Γz) factorizes to learning the distribution of children conditioned on parent as p(γz := Γ) and p(γz := Γ) (due to conditional independence), and the trivial p(Γz := Γ). This requires only as many parent-sibling edges as there are in the graph, i.e., O(m n) many points, if there are parents, each with children. This is optimal. Things proceed differently with NTP. We argue that NTPlearner would fail to learn the plan := Γ. The key intuition is that that an NTP-learner learns the parent Γ witnessing the siblings (γ, γ) as input. This is trivial to fit: the parent is simply the mutual neighbor of the two siblings revealed in the prefix! B&N24 term such shortcuts as Clever Hans cheats since the model witnesses and exploits part of the ground-truth it must generate (the siblings). Such cheats are simpler than even the true generative rule and are thus quickly picked up during learning. The model then loses any supervision to learn the latent plan, := Γ. After the Clever Hans cheat is learned, the NTP-learner learns the second sibling not through the plan-conditioned distribution p(γz := Γ) but through the next-tokenconditional, p(γγ). This is complex distribution: learning this would require witnessing every sibling-sibling pair totalling O(m n2) many training data larger by factor of than the data requirements of the more natural rule. More abstractly, in our tasks, it is most efficient to learn well-planned random latent p(z) and subsequent latentconditioned distribution p(sz). However, NTP factorizes this into pieces of the form p(sis<i, z). Consequently, the model learns uninformative latents from the later tokens, lured by Clever Hans cheats. Conversely, the earlier tokens are learned through complex rules bereft of latent plan. While this may not lead to complete breakdown of learning as in B&N24, it must lead to data-hungry learning. 3. TRAINING AND INFERENCE Transformers. For our next-token-trained (NTP) models, we use the standard teacher-forcing objective used in supervised finetuning. Given prompt and ground truth sequence s, the model is trained to predict the ith token si, given as input the prompt and all ground truth tokens up until that point, (p, s<i). We write the objective more explicitly in Eq 2. For the multi-token Transformer models, we use teacherless training (Monea et al., 2023; Bachmann & Nagarajan, 2024; Tschannen et al., 2023), where the model is trained to predict si simultaneously for all i, only given the prompt (and some dummy tokens in place of the that was once given as input). Since the exact details of this is irrelevant to our discussion, we describe this in Eq 2. To train our models, we use hybrid of this objective and the next-token objective. Diffusion models. Rather than sequentially predicting each token conditioned on previously generated tokens, discrete diffusion models (Hoogeboom et al., 2021; Austin et al., 2021) iteratively add noise to all tokens and then learn to denoise them in reverse. This strategy allows the model to capture global dependencies among tokens during training, making it an example of multi-token objective. In our 6 GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION experiments, we used the score entropy discrete diffusion model (SEDD, Lou et al., 2023), which starts generation from sequence of fully masked tokens. Over multiple steps, the model simultaneously predicts and unmasks these tokens, progressively refining the entire sequence. Inference. In all the above techniques, we extract each sample independently from the model (as against say, extracting them in continuous succession in the same context). For Transformers, during inference, we perform standard autoregression in both the nextand multi-token trained settings. We do this either with greedy decoding or with nucleus sampling (Finlayson et al., 2024). 3.1. Hash-conditioning for Transformers Unlike closed-ended tasks where prompt (i.e., prefix) maps to unique correct answer, our open-ended tasks are prompt-free. For prompt-free autoregressive Transformer to provide diverse outputs, we must use temperature sampling rather than greedy decoding. However, as we show later, training prompt-free Transformer model even on our simple tasks leads to poor creativity in some of our settings (while this was no problem for our Diffusion models!). As natural alternative to this, we tried was to prepend prompt of pause tokens (Goyal et al., 2024) to all datapoints both during training and during inference in order to allow extra computation to the model before it emits its outputs. Next, we tried an even more sophisticated alternative we call as hash-conditioning. Here, we use as prompt, random hash string unique to each training datapoint (rather than the same constant sequence of pause tokens); during test-time, we prompt with novel hash strings to extract the test data. We provide possible intuitions for why this may help in 5.1. 4. EXPERIMENTAL RESULTS Key details. Part of our experiments are performed for Gemma v1 (2B) pre-trained model (Gemma Team et al., 2024), averaged over 4 runs. For diffusion, we use 90M (non-embedding) parameters Score Entropy Discrete Diffusion model (SEDD; Lou et al., 2023). For fair comparison against NTP, we use 86M (non-embedding) parameters GPT-2 model (Radford et al., 2019). In all our experiments, we finetune the models until it is clear that algorithmic creativity (Eq. 1) has saturated. All values are reported from this checkpoint. Finally, since our best Transformer results were under hash-conditioning (for both nextand mult-token training), our main results are reported under that training setting; we provide various ablations without that as well. Please see for more experimental details, and for precise dataset details (e.g., how the graph is constructed, how sequences are formatted etc.,). 4.1. Observations Multi-token prediction improves algorithmic creativity significantly. In all our datasets, we observe from Fig 3 that the algorithmic creativity of the Gemma v1 (2B) model increases significantly under multi-token prediction, with nearly 5x factor for the discovery datasets. Note that for this, we have selected the learning rate favorable towards next-token prediction; tuning for multi-token yields further gains (Fig 15). In Fig 4, we report performance for the diffusion model against next-token & teacherless training of similar-sized Transformers. We see that diffusion models are consistently better than next-token training, achieving up to 5x higher algorithmic creativity. However, the gains are much smaller or absent with teacherless training. This echoes prior discussions suggesting that the teacherless objective is hard objective to optimize for smaller Transformers (B&N24); other multi-token approaches (Gloeckle et al., 2024) are even known to hurt small models of the order of 300M. Multi-token prediction reduces memorization significantly. Algorithmic creativity may suffer either because the model outputs incoherent garbage, or because it repeats the same original output, or because it simply parrots out the training data. In almost all settings, it is the last reason that dominates: across the board (in Fig 3, Fig 4 bottom), next-token prediction is significantly prone to memorizing the data, while multi-token methods are highly resistant. As foreshadowed in 2.6, we hypothesize that this is because NTP memorizes the earlier training tokens without global plan, having fit the later tokens via local coherence rules (because of Clever Hans cheats à la B&N24). Note that an exception to this is the smaller models (especially for diffusion) in our construction tasks, where memorization increases under the multi-token objectives; but this increase is mild and crucially, does not hurt algorithmic creativity. We point the reader to B.4 for further empirical evidence supporting our argument about NTP from 2.6, including experiments on token-reordering and experiments ruling out other hypotheses. Hash-conditioning improves algorithmic creativity for Transformers. Orthogonal to the effect of multi-token vs. next-token objectives, we point out three crucial effects that hash-conditioning has on Transformer. First, hashconditioning results in the highest algorithmic creativity in both the small models (Fig 6) and the larger models (Fig 5). In fact, in our larger models, the null and pause token prefix with temperature sampling (Fig 18) exhibit almost no algorithmic creativity (they are mode collapsed, see Fig 19, 20). In H.2, we find that this improved creativity from hashconditioning comes from aiding diversity, rather than by reducing memorization. Note that we do not see gains of GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION Figure 6. Hash-conditioning improves algorithmic creativity of the GPT-2 (86M) model (but not the diffusion model): The X-axis labels denote the training and decoding procedure, while the legend indicates the type of prefix used during both. conditioning as distinct knob for diversity with more potency than temperature-scaling. This is line with Peeperkorn et al. (2024); Chen & Ding (2023); Chung et al. (2023) who find that, in realistic tasks, temperature only has weak correlation with creativity, often inadvertently introducing incoherence. Robustness to hyperparameters. In and Fig 22, we do sensitivity analysis on all the datasets. We report how our above findings are robust to the choice of learning rate, batch-size, number of training steps, weight given to the multi-token objective, varying sampling conditions and reasonable changes to the complexity of the dataset and training set size (as per our argument in 2.6, we do expect the next-vs. multi-token gap to diminish for larger dataset size). 4.2. An initial exploration of real-world summarization For more realistic examination of our findings, we conduct preliminary investigation of GPT models finetuned with NTP and the multi-token teacherless objectives on summarization tasks (XSUM, CNN/DailyMail). We measure the diversity of model for any given prompt by generating 5 different completions and computing Self-Bleu metric (Zhu et al., 2018). Admittedly though, summarization task is not as openended as we would like: higher quality model (i.e., higher Rouge; Lin, 2004) necessarily means lower diversity. To account for this, we plot how diversity evolves over time as function of the quality of the model; we then find in Fig 7 that for given model quality, the larger multi-token models achieve higher diversity (albeit only by slight amount). This increase does not hold for smaller models and is not always noticeable for CNN/DailyMail (see I). Interestingly, teacherless training consistently shows an increase in summarization quality, measured by Rouge. 5. DISCUSSION 5.1. Intuition about hash-conditioning One could view the hash prefixes as simpler alternative to varying the wordings of prompt Li et al. (2023); Lau et al. Figure 4. Multi-token diffusion training improves algorithmic creativity (top; Eq 1) on our four open-ended algorithmic tasks, and it reduces memorization on discovery tasks but not construction tasks (bottom). Figure 5. Hash-conditioning significantly improves algorithmic creativity of both nextand multi-token prediction on Gemma v1 (2B) model. The labels in the X-axis denote the prefix (used during training and inference) and the temperature (used during inference). hash-conditioning when it comes to diffusion training (Fig 6). Second, surprisingly, with hash-conditioning, there is no need for temperature: even greedy decoding generates diverse outputs that are as good or even better than temperature in algorithmic creativity. Besides, for any fixed temperature, prefixing hash string only improves performance over null prefix (Fig 6, 18). Third, increasing the hash lengths consistently boosts algorithmic creativity for both next-token and multi-token approaches (see Fig 5, 23). Thus, for Transformers, we propose viewing hash8 GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION 6. LIMITATIONS We enumerate in detail the limitations of our work in terms of our experimental conclusions and in terms of our general approach to an abstraction of creativity. 6.1. Limitations of our experimental conclusions 1. There may be many ways to improve upon next-token prediction for minimal task. Unfortunately, success here does not necessarily guarantee success on more complex tasks. Conversely, minimal tasks are more valuable as failure base case: failure here guarantees failure in more complex tasks. 2. Our examples do not preclude the existence of tasks where next-token prediction will outperform multi-token prediction; multi-token prediction is simply more general-purpose objective suitable to lookahead tasks. 3. The teacherless multi-token prediction technique we explore as an alternative is generally harder to optimize than next-token prediction, especially for smaller models. 4. Even if multi-token approaches outperform next-token prediction relatively, in some of our simple tasks, all algorithms are far from delivering sufficiently diverse model. 5. Although our tasks are minimal, we note that there is certain range of hyperparameters (e.g., high degree or edge count) beyond which the models can struggle to learn them. We find that Triangle Discovery in particular is challenging task, especially for smaller models. We also note that the models are curiously sensitive to the way the edges are formatted (see F.3). 6.2. Our approach to creativity Below, we enumerate some important limitations of our approach towards building abstract and minimal models of creative tasks. 1. The skills we capture in our tasks are only (a subset of) computational skills necessary for creativity; these are far from being sufficient. 2. The type of algorithmic tasks we study capture only tiny subset of creative tasks that fall under the taxonomy in Boden (2003). There is yet another class called transformative creativity that we do not look at, and also other important taxonomies such as the Big-C/little-c creativity (Csikszentmihalyi, 1996). BigC Creativity corresponds breakthroughs and worldchanging ideas; what we focus on is adjacent to class of little-c creativity tasks. Relatedly, many real-world creative tasks appear to be out-of-distribution in nature, which we do not capture. Figure 7. Multi-token training improves diversity scores for XSUM summarization for large GPT-2 models: Here, we plot diversity and quality as measured over multiple checkpoints during finetuning, and observe differences in diversity for fixed quality. (2024); Naik et al. (2024) or tuning soft-prompt (Wang et al., 2024c), both of which are known to induce diversity. But why does this help? We speculatively put forth two arguments. First, is representational one. Fixing random seed upfront may help the model flesh out (i.e., compute the tokens of) one thought per sample, as against maintaining running set of multiple thoughts and computing distribution over all their tokens at each step. similar point is made in concurrent position paper (Jahrens & Martinetz, 2025). The second argument is specific to next-token prediction on open-ended planning tasks: fixing random seed upfront may help the model co-ordinate multiple interlocking random decisions in advance rather than deciding them on the fly. Finally, there are also optimization aspects of how hash-conditioning works that we do not understand (see B.1). Regardless, it remains to be seen whether hashconditioning is useful in tasks beyond the minimal ones we design. 5.2. Effects of reasoning-enhancing methods. Our argument is limited to learning open-ended tasks in supervised manner. While we do not comment on how well other approaches like RL (DeepSeek-AI, 2025), chainof-thought (CoT; Wei et al., 2022), and scaling test-time compute (OpenAI, 2024) would fare, we remark that these methods are designed to enhance the quality of single example. It is unclear how to design them to maximize originality against training set, and diversity over multiple responses. Furthermore, there is profound question as to whether merely spelling out models thought in token space can be an efficient way to search and maximize diversity. This may require enumerating all possible candidates by trial and error, an impossible feat when the search space is large. We present more discussions in B. GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION 3. Real-world creative tasks also apply over much larger context length and require drawing connections from significantly larger memory (literally, the set of all things human may know about). Our algorithmic tasks are tiny in comparison (although deliberately so). 4. Our empirical measure of creativity for algorithmic tasks is only computationally-efficient proxy. Achieving an absolute high algorithmic creativity score does not imply complete coverage of the space. 5. As stated earlier, we study abstract tasks that are inspired by the computations involved in creative tasks. Our study is not intended to capture the subjective, social, cultural and personal values integral to many creative tasks. 7. RELATED WORK Open-ended algorithmic tasks. Directly related to us are Khona et al. (2024); Allen-Zhu & Li (2023b) who study diversity of next-token-trained models on an open-ended algorithmic task. Khona et al. (2024) consider path-connectivity on knowledge graph. They observe that under temperaturescaling, diversity is at odds with accuracy. We show that this tradeoff can be greatly improved when we consider alternative training methods (multi-token, or hash-conditioning). Allen-Zhu & Li (2023b) empirically demonstrate that nexttoken predictors are able to learn synthetic, challenging CFG, in the infinite data regime ( 100m tokens). Our datasets are not CFGs, with the exception of Sibling Discovery, which can be thought of as simple PCFG. Our negative result does not contradict theirs since what we show is sub-optimality of NTP in smaller data regime. Our work also extends the above works by studying limitations in much more minimal tasks that require as little as 2-hop lookahead. There are other works that study Transformers on non-open-ended graph-algorithmic tasks, discussed in J. Diversity in generative models. Generative diversity has long been major goal, at least until the revolution in reasoning of language models, when accuracy took prominence over diversity. Much work has gone into concerns such as mode collapse (Che et al., 2017) or posterior collapse (Bowman et al., 2016) and memorization. In LLMs, regurgitation of training data has been serious concern (Carlini et al., 2020; 2023; Nasr et al., 2023). Our results on hashconditioning are also reminiscent of line of work on reinforcement learning (RL) showing that adding noises to the policy model parameters enables more efficient exploration than directly adding noises to the output space (Plappert et al., 2017; Fortunato et al., 2017). We defer discussion of theoretical studies of diversity and memorization J, along with empirical studies of creativity in natural language tasks. Going beyond next-token prediction (NTP). There has been recent emerging discussion surrounding the role of NTP as foundational piece in developing intelligent models. On the critical side, arguments have been made about the inference-time issues with auto-regression (Dziri et al., 2024; LeCun, 2024; Kääriäinen, 2006; Ross & Bagnell, 2010). Others have reported the planning and arithmetic limitations of next-token trained models (McCoy et al., 2023; Momennejad et al., 2023; Valmeekam et al., 2023a;b;c; Bachmann & Nagarajan, 2024) where the goal is accuracy, not diversity. As for diffusion as an alternative to NTP, our findings parallel that of Ye et al. (2024) who show that their variant of diffusion is able to solve the challenging path-star task of B&N24. We provide references to more lines of multi-token prediction work in There are also other Transformer failures such as the reversal curse (Allen-Zhu & Li, 2023a) or shortcut-learning (Dziri et al., 2024; Zhang et al., 2023; Liu et al., 2023; Young & You, 2022; Lai et al., 2021; Ranaldi & Zanzotto, 2023), however these are out-of-distribution failures; the sub-optimality we show is in-distribution, like in B&N24. Injecting noise into Transformer. Most related to hashconditioning is DeSalvo et al. (2024) who induce diversity by varying soft-prompt learned using reconstruction loss. Our approach requires no modification to the architecture or the loss; however, we train the whole model, which is more expensive than training only soft-prompt generator. concurrent position paper (Jahrens & Martinetz, 2025) conceptually suggests injecting noise with the same motivation as us. The benefits of hash-conditioning may also be related to the fact that varying the wording in prompt is known to induce diverse outputs (Li et al., 2023; Lau et al., 2024; Naik et al., 2024). Various works also inject noise into Transformer, in different form from ours (e.g., inducing Gaussian noise), and for different function such as quality, robustness (Hua et al., 2022; Jain et al., 2024) or efficiency (Wang et al., 2024c). 8. CONCLUSIONS This work provides minimal test-bed of tasks abstracting distinct modes of creativity. While these tasks are admittedly an extreme caricaturization of real-world tasks, they enable us to quantify otherwise elusive metrics like originality and diversity. They also enable us to control and investigate distinct parts of the current apparatus for language modeling (next-token learning and softmax-based temperature sampling) and advocate for alternatives (multitoken learning and hash-conditioning). The surprising effectiveness of hash-conditioning raises various open questions (B.1). There are also other profound questions as to whether reasoning-enhancing methods like RL and CoT are 10 GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION optimal for enhancing open-ended diversity and originality (5.2). Overall, we hope our work inspires discussion in the various directions of multi-token prediction, creativity and planning. 9. IMPACT STATEMENT This paper presents work whose goal is to advance the field of Machine Learning through the study of simple algorithmic tasks inspired by creativity. There are many potential societal consequences of our work especially if one applies AI to real-world creative endeavors none which we feel must be specifically highlighted in our focused algorithmic study. 10. ACKNOWLEDGEMENTS We wish to thank Gregor Bachmann, Jacob Springer, and Sachin Goyal for extensive feedback on draft of the paper. We also wish to thank Mike Mozer, Suhas Kotha, Clayton Sanford, Christina Baek, Yuxiao Qu, and Ziqian Zhong for valuable early discussions and pointers. The work was supported in part by Cisco, Apple, Google, OpenAI, NSF, Okawa foundation and Schmidt Sciences."
        },
        {
            "title": "REFERENCES",
            "content": "Alabdulmohsin, I., Tran, V. Q., and Dehghani, M. Fractal patterns may unravel the intelligence in next-token prediction, 2024. Allen-Zhu, Z. and Li, Y. Physics of language models: Part 3.2, knowledge manipulation. CoRR, abs/2309.14402, 2023a. doi: 10.48550/ARXIV.2309.14402. URL https: //doi.org/10.48550/arXiv.2309.14402. Allen-Zhu, Z. and Li, Y. Physics of language models: Part 1, context-free grammar. CoRR, abs/2305.13673, 2023b. doi: 10.48550/ARXIV.2305.13673. URL https://doi. org/10.48550/arXiv.2305.13673. Anderson, B. R., Shah, J. H., and Kreminski, M. Homogenization effects of large language models on human creative ideation. In Proceedings of the 16th Conference on Creativity & Cognition, Chicago, IL, USA, June 23-26, 2024, pp. 413425. ACM, 2024. URL https://doi.org/10.1145/3635636.3656204. Austin, J., Johnson, D. D., Ho, J., Tarlow, D., and van den Berg, R. Structured denoising diffusion models in discrete state-spaces. NeurIPS, 2021. Bachmann, G. and Nagarajan, V. The pitfalls of next-token prediction. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 22962318, 2024. Bavarian, M., Jun, H., Tezak, N. A., Schulman, J., McLeavey, C., Tworek, J., and Chen, M. Efficient training of language models to fill in the middle. ArXiv, abs/2207.14255, 2022. URL https://api. semanticscholar.org/CorpusID:251135268. Beel, J., Kan, M.-Y., and Baumgart, M. Evaluating sakanas ai scientist for autonomous research: Wishful thinking or an emerging reality towards artificial research intelligence (ari)?, 2025. Berglund, L., Tong, M., Kaufmann, M., Balesni, M., Stickland, A. C., Korbak, T., and Evans, O. The reversal curse: Llms trained on \"a is b\" fail to learn \"b is a\". In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview. net/forum?id=GPKTIktA0k. Boden, M. A. The Creative Mind - Myths and Mechanisms (2. ed.). Routledge, 2003. Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Józefowicz, R., and Bengio, S. Generating sentences from continuous space. In Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, CoNLL 2016,, pp. 1021. ACL, 2016. Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. Callaway, E. Cognitive science: Leap of thought. Nature, 502, 2013. Carlini, N., Tramèr, F., Wallace, E., Jagielski, M., HerbertVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D. X., Erlingsson, Ú., Oprea, A., and Raffel, C. Extracting training data from large language models. In USENIX Security Symposium, 2020. Carlini, N., Ippolito, D., Jagielski, M., Lee, K., Tramèr, F., and Zhang, C. Quantifying memorization across neural language models. ICLR, 2023. Chakrabarty, T., Laban, P., Agarwal, D., Muresan, S., and Wu, C. Art or artifice? large language models and the false promise of creativity. In Proceedings of the CHI Conference on Human Factors in Computing Systems, CHI 2024, Honolulu, HI, USA, May 11-16, 2024, pp. 30:130:34. ACM, 2024. Che, T., Li, Y., Jacob, A. P., Bengio, Y., and Li, W. Mode regularized generative adversarial networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. 11 GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION Chen, H. and Ding, N. Probing the \"creativity\" of large language models: Can models produce divergent semantic association? In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pp. 1288112888. Association for Computational Linguistics, 2023. Chow, Y., Tennenholtz, G., Gur, I., Zhuang, V., Dai, B., Thiagarajan, S., Boutilier, C., Agarwal, R., Kumar, A., and Faust, A. Inference-aware fine-tuning for best-of-n sampling in large language models. ArXiv, abs/2412.15287, 2024. URL https://api.semanticscholar.org/ CorpusID:274965054. Chung, J. J. Y., Kamar, E., and Amershi, S. Increasing diversity while maintaining accuracy: Text data generation with large language models and human interventions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL, pp. 575593. Association for Computational Linguistics, 2023. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Csikszentmihalyi, M. Creativity: Flow and the Psychology of Discovery and Invention. HarperCollins Publishers, New York, NY, first edition, 1996. Dang, X., Baek, C., Wen, K., Kolter, Z., and Raghunathan, A. Weight ensembling improves reasoning in language models. 2025. URL https://api.semanticscholar. org/CorpusID:277781120. Dawid, A. and LeCun, Y. Introduction to latent variable energy-based models: path towards autonomous machine intelligence. arXiv preprint arXiv:2306.02572, 2023. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. ArXiv, abs/2501.12948, 2025. DeepSeek-AI, Liu, A., Feng, B., Xue, B., Wang, B.-L., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Guo, D., Yang, D., Chen, D., Ji, D.-L., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Zhang, H., Ding, H., Xin, H., Gao, H., Li, H., Qu, H., Cai, J. L., Liang, J., Guo, J., Ni, J., Li, J., Wang, J., Chen, J., Chen, J., Yuan, J., Qiu, J., Li, J., Song, J.-M., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Xu, L., Xia, L., Zhao, L., Wang, L., Zhang, L., Li, M., Wang, M., Zhang, M., Zhang, M., Tang, M., Li, M., Tian, N., Huang, P., Wang, P., Zhang, P., Wang, Q., Zhu, Q., Chen, Q., Du, Q., Chen, R. J., Jin, R. L., Ge, R., Zhang, R., Pan, R., Wang, R., Xu, R., Zhang, R., Chen, R., Li, S. S., Lu, S., Zhou, S., Chen, S., Wu, S.-P., Ye, S., Ma, S., Wang, S., Zhou, S., Yu, S., Zhou, S., Pan, S., Wang, T., Yun, T., Pei, T., Sun, T., Xiao, W. L., Zeng, W., Zhao, W., An, W., Liu, W., Liang, W., Gao, W., Yu, W.-X., Zhang, W., Li, X. Q., Jin, X., Wang, X., Bi, X., Liu, X., Wang, X., Shen, X.-C., Chen, X., Zhang, X., Chen, X., Nie, X., Sun, X., Wang, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yu, X., Song, X., Shan, X., Zhou, X., Yang, X., Li, X., Su, X., Lin, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhu, Y. X., Zhang, Y., Xu, Y., Huang, Y., Li, Y., Zhao, Y., Sun, Y., Li, Y., Wang, Y., Yu, Y., Zheng, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Tang, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y.-B., Liu, Y., Guo, Y., Wu, Y., Ou, Y., Zhu, Y., Wang, Y., Gong, Y., Zou, Y., He, Y., Zha, Y., Xiong, Y., Ma, Y., Yan, Y., Luo, Y.-W., mei You, Y., Liu, Y., Zhou, Y., Wu, Z. F., Ren, Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Huang, Z., Zhang, Z., Xie, Z., guo Zhang, Z., Hao, Z., Gou, Z., Ma, Z., Yan, Z., Shao, Z., Xu, Z., Wu, Z., Zhang, Z., Li, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z.-A., Xie, Z., Song, Z., Gao, Z., and Pan, Z. Deepseek-v3 technical report. In ArXiv, 2024. DeSalvo, G., Kagy, J.-F., Karydas, L., Rostamizadeh, A., and Kumar, S. No more hard prompts: Softsrv prompting for synthetic data generation, 2024. URL https: //arxiv.org/abs/2410.16534. Du, L., Mei, H., and Eisner, J. Autoregressive modeling with lookahead attention. arXiv preprint arXiv:2305.12272, 2023. Dziri, N., Lu, X., Sclar, M., Li, X. L., Jiang, L., Lin, B. Y., Welleck, S., West, P., Bhagavatula, C., Le Bras, R., et al. Faith and fate: Limits of transformers on compositionality. Advances in Neural Information Processing Systems, 36, 2024. Feng, G., Zhang, B., Gu, Y., Ye, H., He, D., and Wang, L. Towards revealing the mystery behind chain of thought: theoretical perspective. Advances in Neural Information Processing Systems, 36, 2023. Finlayson, M., Hewitt, J., Koller, A., Swayamdipta, S., and Sabharwal, A. Closing the curious case of neural text degeneration. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. Fortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I., Graves, A., Mnih, V., Munos, R., Hassabis, D., Pietquin, O., Blundell, C., and Legg, S. Noisy networks for exploration. ArXiv, abs/1706.10295, 2017. URL https: //api.semanticscholar.org/CorpusID:5176587. 12 GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION Franceschelli, G. and Musolesi, M. On the creativity of large language models. CoRR, abs/2304.00008, 2023. Fried, D., Aghajanyan, A., Lin, J., Wang, S. I., Wallace, E., Shi, F., Zhong, R., tau Yih, W., Zettlemoyer, Incoder: generative model for L., and Lewis, M. code infilling and synthesis. ArXiv, abs/2204.05999, 2022. URL https://api.semanticscholar.org/ CorpusID:248157108. Gemma Team, T. M., Hardin, C., Dadashi, R., Bhupatiraju, S., Sifre, L., Rivière, M., Kale, M. S., Love, J., Tafti, P., Hussenot, L., and et al. Gemma. 2024. doi: 10.34740/ KAGGLE/M/3301. URL https://www.kaggle.com/ m/3301. Gloeckle, F., Idrissi, B. Y., Rozière, B., Lopez-Paz, D., and Synnaeve, G. Better & faster large language models via multi-token prediction. 2024. Gong, S., Li, M., Feng, J., Wu, Z., and Kong, L. Diffuseq: Sequence to sequence text generation with diffusion models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. C., and Bengio, Y. Generative adversarial networks. Commun. ACM, 63(11):139144, 2020. doi: 10.1145/3422622. URL https://doi.org/10.1145/3422622. Goyal, A., Sordoni, A., Côté, M.-A., Ke, N. R., and Bengio, Y. Z-forcing: Training stochastic recurrent networks. NeurIPS, 2017. Goyal, S., Ji, Z., Rawat, A. S., Menon, A. K., Kumar, S., and Nagarajan, V. Think before you speak: Training language models with pause tokens. The Twelfth International Conference on Learning Representations, ICLR 2024, 2024. Gruver, N., Stanton, S., Frey, N. C., Rudner, T. G. J., Hotzel, I., Lafrance-Vanasse, J., Rajpal, A., Cho, K., and Wilson, A. G. Protein design with guided discrete diffusion. ArXiv, abs/2305.20009, 2023. URL https://api. semanticscholar.org/CorpusID:258987335. Gu, J., Bradbury, J., Xiong, C., Li, V. O. K., and Socher, R. Non-autoregressive neural machine translation. In 6th International Conference on Learning Representations, ICLR 2018, Conference Track Proceedings. OpenReview.net, 2018. Hofstadter, D. review of mental leaps: Analogy in creative thought. AI Mag., 16(3):7580, 1995. doi: 10. 1609/AIMAG.V16I3.1154. URL https://doi.org/ 10.1609/aimag.v16i3.1154. Holyoak, K. J. and Thagard, P. Mental leaps: analogy in creative thought. MIT Press, Cambridge, MA, USA, 1995. ISBN 0262082330. Hoogeboom, E., Nielsen, D., Jaini, P., Forre, P., and Welling, M. Argmax flows and multinomial diffusion: Learning categorical distributions. In Neural Information Processing Systems, 2021. Hopkins, A. K., Renda, A., and Carbin, M. Can LLMs generate random numbers? evaluating LLM sampling in controlled domains. In ICML 2023 Workshop: Sampling and Optimization in Discrete Space, 2023. URL https: //openreview.net/forum?id=Vhh1K9LjVI. Hua, H., Li, X., Dou, D., Xu, C., and Luo, J. Fine-tuning pre-trained language models with noise stability regularization. CoRR, 2022. Jahrens, M. and Martinetz, T. Why llms cannot think and how to fix it, 2025. URL https://arxiv.org/abs/ 2503.09211. Jain, N., Chiang, P., Wen, Y., Kirchenbauer, J., Chu, H., Somepalli, G., Bartoldson, B. R., Kailkhura, B., Schwarzschild, A., Saha, A., Goldblum, M., Geiping, J., and Goldstein, T. Neftune: Noisy embeddings improve instruction finetuning. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. Jansen, P. A., Cote, M.-A., Khot, T., Bransom, E., Dalvi, B., Majumder, B. P., Tafjord, O., and Clark, P. Discoveryworld: virtual environment for developing and evaluating automated scientific discovery agents. NeurIPS, 2024. Kääriäinen, M. Lower bounds for reductions. In Atomic Learning Workshop, 2006. Kalai, A. T. and Vempala, S. S. Calibrated language models In Proceedings of the 56th Annual must hallucinate. ACM Symposium on Theory of Computing, STOC 2024, Vancouver, BC, Canada, June 24-28, 2024, pp. 160171. ACM, 2024. Kalavasis, A., Mehrotra, A., and Velegkas, G. On the limits of language generation: Trade-offs between hallucination and mode collapse. abs/2411.09642, 2024. Gupta, T. and Pruthi, D. All that glitters is not novel: Plagiarism in ai generated research, 2025. URL https: //arxiv.org/abs/2502.16487. Kamb, M. and Ganguli, S. An analytic theory of creativity in convolutional diffusion models, 2024. URL https: //arxiv.org/abs/2412.20292. 13 GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION Khona, M., Okawa, M., Hula, J., Ramesh, R., Nishi, K., Dick, R. P., Lubana, E. S., and Tanaka, H. Towards an understanding of stepwise inference in transformers: synthetic graph navigation model. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. Kingma, D. P. and Welling, M. Auto-encoding variational bayes. In Bengio, Y. and LeCun, Y. (eds.), 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. URL http://arxiv.org/ abs/1312.6114. Kitouni, O., Nolte, N., Bouchacourt, D., Williams, A., Rabbat, M., and Ibrahim, M. The factorization curse: Which tokens you predict underlie the reversal curse and more. CoRR, abs/2406.05183, 2024. doi: 10. 48550/ARXIV.2406.05183. URL https://doi.org/ 10.48550/arXiv.2406.05183. Kleinberg, J. M. and Mullainathan, S. Language generation in the limit. CoRR, abs/2404.06757, 2024. doi: 10. 48550/ARXIV.2404.06757. URL https://doi.org/ 10.48550/arXiv.2404.06757. Lai, Y., Zhang, C., Feng, Y., Huang, Q., and Zhao, D. Why machine reading comprehension models learn shortcuts? In Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, volume ACL/IJCNLP 2021 of Findings of ACL, pp. 9891002. Association for Computational Linguistics, 2021. Lau, G. K. R., Hu, W., Liu, D., Chen, J., Ng, S.-K., and Low, B. K. H. Dipper: Diversity in prompts for producing large language model ensembles in reasoning tasks, 2024. URL https://arxiv.org/abs/2412.15238. LeCun, Y. Do large language models need sensory grounding for meaning and understanding? University Lecture, 2024. Lee, N., Sreenivasan, K., Lee, J. D., Lee, K., and Papailiopoulos, D. Teaching arithmetic to small transformers. In The Twelfth International Conference on Learning Representations, ICLR 2024. OpenReview.net, 2024. Li, Y., Lin, Z., Zhang, S., Fu, Q., Chen, B., Lou, J.-G., and Chen, W. Making language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Toronto, Canada, July 2023. Association for Computational Linguistics. URL https: //aclanthology.org/2023.acl-long.291/. Lin, C.-Y. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 7481, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology. org/W04-1013/. Liu, B., Ash, J. T., Goel, S., Krishnamurthy, A., and Zhang, In The C. Transformers learn shortcuts to automata. Eleventh International Conference on Learning Representations, ICLR 2023, 2023. Lou, A., Meng, C., and Ermon, S. Discrete diffusion modeling by estimating the ratios of the data distribution. In International Conference on Machine Learning, 2023. Lu, C., Lu, C., Lange, R. T., Foerster, J., Clune, J., and Ha, D. The ai scientist: Towards fully automated open-ended scientific discovery, 2024a. Lu, X., Sclar, M., Hallinan, S., Mireshghallah, N., Liu, J., Han, S., Ettinger, A., Jiang, L., Chandu, K. R., Dziri, N., and Choi, Y. AI as humanitys salieri: Quantifying linguistic creativity of language models via systematic attribution of machine text against web text. abs/2410.04265, 2024b. Malach, E. Auto-regressive next-token predictors are universal learners. arXiv preprint arXiv:2309.06979, 2023. McCoy, R. T., Yao, S., Friedman, D., Hardy, M., and Griffiths, T. L. Embers of autoregression: Understanding large language models through the problem they are trained to solve. arXiv preprint arXiv:2309.13638, 2023. McLaughlin, A., Campbell, J., Uppuluri, A., and Yang, Y. Aidanbench: Stress-testing language model creativity on open-ended questions. In NeurIPS 2024 Workshop on Language Gamification, 2024. Merrill, W. and Sabharwal, A. The expressive power of transformers with chain of thought. In The Twelfth International Conference on Learning Representations, 2024. Mirowski, P. W., Love, J., Mathewson, K. W., and Mohamed, S. robot walks into bar: Can language models serve as creativity support tools for comedy? an evaluation of llms humour alignment with comedians. CoRR, abs/2405.20956, 2024. Momennejad, I., Hasanbeig, H., Frujeri, F. V., Sharma, H., Ness, R. O., Jojic, N., Palangi, H., and Larson, J. Evaluating cognitive maps and planning in large language models with cogeval. Advances in Neural Information Processing Systems, 36, 2023. Monea, G., Joulin, A., and Grave, E. Pass: Parallel speculative sampling. 3rd Workshop on Efficient Natural Language and Speech Processing (NeurIPS 2023), 2023. 14 GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION Nagarajan, V., Raffel, C., and Goodfellow, I. J. Theoretical insights into memorization in gans. In Neural Information Processing Systems Workshop, volume 1, pp. 3, 2018. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. 2019. Naik, R., Chandrasekaran, V., Yuksekgonul, M., Palangi, H., and Nushi, B. Diversity of thought improves reasoning abilities of llms, 2024. URL https://arxiv.org/abs/ 2310.07088. Nakkiran, P., Bradley, A., Zhou, H., and Advani, M. Stepby-step diffusion: An elementary tutorial, 2024. Nallapati, R., Zhou, B., dos santos, C. N., Gulcehre, C., and Xiang, B. Abstractive text summarization using sequenceto-sequence rnns and beyond, 2016. URL https:// arxiv.org/abs/1602.06023. Narayan, S., Cohen, S. B., and Lapata, M. Dont give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization, 2018. URL https://arxiv.org/abs/1808.08745. Nasr, M., Carlini, N., Hayase, J., Jagielski, M., Cooper, A. F., Ippolito, D., Choquette-Choo, C. A., Wallace, E., Tramèr, F., and Lee, K. Scalable extraction of training data from (production) language models. ArXiv, 2023. Nolte, N., Kitouni, O., Williams, A., Rabbat, M., and Ibrahim, M. Transformers can navigate mazes with multi-step prediction. CoRR, abs/2412.05117, 2024. doi: 10.48550/ARXIV.2412.05117. URL https://doi. org/10.48550/arXiv.2412.05117. OpenAI. Openai o1 system card. ArXiv, 2024. Padmakumar, V. and He, H. Does writing with language models reduce content diversity? In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id= Feiz5HtCD0. Pannatier, A., Courdier, E., and Fleuret, F. σ-gpts: new approach to autoregressive models. In Machine Learning and Knowledge Discovery in Databases. Research Track - European Conference, ECML PKDD 2024, Vilnius, Lithuania, September 9-13, 2024, Proceedings, Part VII, volume 14947 of Lecture Notes in Computer Science, pp. 143159. Springer, 2024. Ranaldi, L. and Zanzotto, F. M. Hans, are you clever? clever hans effect analysis of neural systems, 2023. Romera-Paredes, B., Barekatain, M., Novikov, A., Balog, M., Kumar, M. P., Dupont, E., Ruiz, F. J. R., Ellenberg, J. S., Wang, P., Fawzi, O., Kohli, P., and Fawzi, A. Mathematical discoveries from program search with large language models. Nat., 625(7995), 2024. Ross, S. and Bagnell, D. Efficient reductions for imitation learning. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, volume 9 of JMLR Proceedings, 2010. Runco, M. A. and Jaeger, G. J. The standard definition of creativity. Creativity Research Journal, 24(1):9296, 2012. Sanford, C., Fatemi, B., Hall, E., Tsitsulin, A., Kazemi, S. M., Halcrow, J., Perozzi, B., and Mirrokni, V. Understanding transformer reasoning capabilities via graph algorithms. abs/2405.18512, 2024. Saparov, A., Pawar, S., Pimpalgaonkar, S., Joshi, N., Pang, R. Y., Padmakumar, V., Kazemi, S. M., Kim, N., and He, H. Transformers struggle to learn to search, 2024. URL https://arxiv.org/abs/2412.04703. Schnitzler, J., Ho, X., Huang, J., Boudin, F., Sugawara, S., and Aizawa, A. Morehopqa: More than multi-hop reasoning. abs/2406.13397. doi: 10.48550/ARXIV.2406. 13397. Shannon, C. E. mathematical theory of communication. The Bell System Technical Journal, 27(3):379423, 1948. Shannon, C. E. Prediction and entropy of printed english. The Bell System Technical Journal, 30(1):5064, 1951. Shlegeris, B., Roger, F., Chan, L., and McLean, E. Language models are better than humans at next-token prediction. arXiv preprint arXiv:2212.11281, 2022. Peeperkorn, M., Kouwenhoven, T., Brown, D., and Jordanous, A. Is temperature the creativity parameter of large language models? abs/2405.00492, 2024. Si, C., Yang, D., and Hashimoto, T. Can llms generate novel research ideas? large-scale human study with 100+ NLP researchers. 2024. Plappert, M., Houthooft, R., Dhariwal, P., Sidor, S., Chen, R. Y., Chen, X., Asfour, T., Abbeel, P., and Andrychowicz, M. Parameter space noise for exploration. ArXiv, abs/1706.01905, 2017. URL https: //api.semanticscholar.org/CorpusID:2971655. Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm test-time compute optimally can be more effective than scaling model parameters. ArXiv, abs/2408.03314, 2024. URL https://api.semanticscholar.org/ CorpusID:271719990. 15 GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION Talmor, A., Tafjord, O., Clark, P., Goldberg, Y., and Berant, J. Leap-of-thought: Teaching pre-trained models to systematically reason over implicit knowledge. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. Thankaraj, A., Jiang, Y., Kolter, J. Z., and Bisk, Y. Looking beyond the next token, 2025. URL https://arxiv. org/abs/2504.11336. Tschannen, M., Kumar, M., Steiner, A., Zhai, X., Houlsby, N., and Beyer, L. Image captioners are scalable vision In Advances in Neural Information Prolearners too. cessing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. Valmeekam, K., Marquez, M., and Kambhampati, S. Can large language models really improve by self-critiquing arXiv preprint arXiv:2310.08118, their own plans? 2023a. Valmeekam, K., Marquez, M., Olmo, A., Sreedharan, S., and Kambhampati, S. Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change, 2023b. Valmeekam, K., Marquez, M., Sreedharan, S., and Kambhampati, S. On the planning abilities of large language models - critical investigation. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023c. Varshney, L. R., Pinel, F., Varshney, K. R., Bhattacharjya, D., Schörgendorfer, A., and Chee, Y. big data approach to computational creativity: The curious case of chef watson. IBM J. Res. Dev., 63(1):7:17:18, 2019. Walsh, M., Preus, A., and Gronski, E. Does chatgpt have poetic style? In Proceedings of the Computational Humanities Research Conference 2024, Aarhus, Denmark, December 4-6, 2024, volume 3834 of CEUR Workshop Proceedings, pp. 12011219. CEUR-WS.org. Wang, H., Zhao, Y., Li, D., Wang, X., Liu, G., Lan, X., and Wang, H. Innovative thinking, infinite humor: Humor research of large language models through structured thought leaps. abs/2410.10370, 2024a. Wang, H., Zou, J., Mozer, M., Goyal, A., Lamb, A., Zhang, L., Su, W. J., Deng, Z., Xie, M. Q., Brown, H., and Kawaguchi, K. Can ai be as creative as humans?, 2024b. URL https://arxiv.org/abs/2401.01623. Wang, J. X., King, M., Porcel, N., Kurth-Nelson, Z., Zhu, T., Deck, C., Choy, P., Cassin, M., Reynolds, M., Song, F., Buttimore, G., Reichert, D. P., Rabinowitz, N. C., Matthey, L., Hassabis, D., Lerchner, A., and Botvinick, M. M. Alchemy: benchmark and analysis toolkit In NeurIPS for meta-reinforcement learning agents. Datasets and Benchmarks, 2021. URL https://api. semanticscholar.org/CorpusID:239019925. Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 13484 13508. Association for Computational Linguistics, 2023. Wang, Y., Luo, X., Wei, F., Liu, Y., Zhu, Q., Zhang, X., Yang, Q., Xu, D., and Che, W. Make some noise: Unlocking language model parallel inference capability through noisy training. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pp. 1291412926. Association for Computational Linguistics, 2024c. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E. H., Le, Q. V., and Zhou, D. Chain-ofthought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, 2022. Wies, N., Levine, Y., and Shashua, A. Sub-task decomposition enables learning in sequence to sequence tasks. In The Eleventh International Conference on Learning Representations, ICLR 2023, 2023. Wu, Y., Sun, Z., Li, S., Welleck, S., and Yang, Y. Inference scaling laws: An empirical analysis of computeoptimal inference for problem-solving with language models. 2024. URL https://api.semanticscholar. org/CorpusID:271601023. Xu, M., Jiang, G., Zhang, C., Zhu, S.-C., and Zhu, Y. Interactive visual reasoning under uncertainty. In Neural Information Processing Systems, 2022. URL https:// api.semanticscholar.org/CorpusID:249889691. Yang, S., Gribovskaya, E., Kassner, N., Geva, M., and Riedel, S. Do large language models latently perform multi-hop reasoning? In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 1021010229. Association for Computational Linguistics, 2024a. GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION 2024. URL https://api.semanticscholar.org/ CorpusID:267094860. Zhong, S., Huang, Z., Gao, S., Wen, W., Lin, L., Zitnik, M., and Zhou, P. Lets think outside the box: Exploring leapof-thought in large language models with creative humor generation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, 2024. Zhu, Y., Lu, S., Zheng, L., Guo, J., Zhang, W., Wang, J., and Yu, Y. Texygen: benchmarking platform for text generation models. The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, 2018. Yang, S., Kassner, N., Gribovskaya, E., Riedel, S., and Geva, M. Do large language models perform latent multi-hop reasoning without exploiting shortcuts? abs/2411.16679, 2024b. doi: 10.48550/ARXIV.2411.16679. URL https: //doi.org/10.48550/arXiv.2411.16679. Yang, Z., Hu, Z., Salakhutdinov, R., and Berg-Kirkpatrick, T. Improved variational autoencoders for text modeling using dilated convolutions. ICML, 2017. Yang, Z., Band, N., Li, S., Candès, E. J., and Hashimoto, T. Synthetic continued pretraining. CoRR, abs/2409.07431, 2024c. doi: 10.48550/ARXIV.2409.07431. URL https: //doi.org/10.48550/arXiv.2409.07431. Ye, J., Gao, J., Gong, S., Zheng, L., Jiang, X., Li, Z., and Kong, L. Beyond autoregression: Discrete diffusion for complex reasoning and planning. 2024. doi: 10.48550/ ARXIV.2410.14157. Young, T. and You, Y. On the inconsistencies of conditionals learned by masked language models. arXiv preprint arXiv:2301.00068, 2022. Yu, L., Jiang, W., Shi, H., YU, J., Liu, Z., Zhang, Y., Kwok, J., Li, Z., Weller, A., and Liu, W. Metamath: Bootstrap your own mathematical questions for large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=N8N0hgNDRt. Zhang, H., Li, L. H., Meng, T., Chang, K., and den Broeck, G. V. On the paradox of learning to reason from data. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI 2023, 19th25th August 2023, Macao, SAR, China, pp. 33653373. ijcai.org, 2023. Zhang, J., Jain, L., Guo, Y., Chen, J., Zhou, K. L., Suresh, S., Wagenmaker, A., Sievert, S., Rogers, T. T., Jamieson, K., Mankoff, R., and Nowak, R. Humor in AI: massive scale crowd-sourced preferences and benchmarks for cartoon captioning. CoRR, abs/2406.10522, 2024a. Zhang, Y., Schwarzschild, A., Carlini, N., Kolter, Z., and Ippolito, D. Forcing diffuse distributions out of language models. abs/2404.10859, 2024b. Zhang, Y., Diddee, H., Holm, S., Liu, H., Liu, X., Samuel, V., Wang, B., and Ippolito, D. Noveltybench: Evaluating language models for humanlike diversity. 2025. URL https://api.semanticscholar.org/ CorpusID:277621515. Zhao, Y., Zhang, R., Li, W., Huang, D., Guo, J., Peng, S., Hao, Y., Wen, Y., Hu, X., Du, Z., Guo, Q., Li, L., and Chen, Y. Assessing and understanding creativity in large language models. ArXiv, abs/2401.12491, 17 GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION A. TRANSFORMER TRAINING OBJECTIVES Let LMθ be our language model, parameterized by θ, for which LMθ(ˆsi = si; s<i) is the probability it assigns to the ith output ˆsi being si, given as input sequence s<i. Let (p, r) be prefix-response pair. In standard next-token finetuning, we maximize the objective: Jnext-token(θ) = ED Lresp (cid:88) (cid:104) i=1 log LMθ (ˆri = ri; p, r<i) (cid:105) (2) In teacherless (multi-token) training (Monea et al., 2023; Bachmann & Nagarajan, 2024; Tschannen et al., 2023), we make use of an uninformative input string $ that simply corresponds to series of dummy tokens $. Jmulti-token(θ) = ED Lresp (cid:88) (cid:104) i=1 log LMθ (ˆri = ri; p, $<i) (cid:105) (3) B. FURTHER DISCUSSION B.1. Style of noise-injection Our technique of injecting noise into the model is somewhat different from how noise is introduced in traditional VAEs (Kingma & Welling, 2014) or GANs (Goodfellow et al., 2020), and this difference is worth noticing. In traditional approaches, although the model learns noise-output mapping, this mapping is enforced only at distribution level i.e., the distribution of noise vectors must map to distribution of real vectors. However, in our approach we arbitrarily enforce what noise vector goes to what real datapoint, at pointwise level. This raises the open questions of why hash-conditioning works in the first place surprisingly, without breaking optimization or generalization and whether there is way to enforce it at distribution-level, and whether that can provide even greater improvements. B.2. In-weights vs in-context graphs for combinational creativity Combinational creativity requires searching through known entities. In abstracting this, there is an interesting choice to be made as to whether the relevant search space is retrieved and spelled out in-context or whether it remains in-weights (like in Sibling Discovery and Triangle Discovery). We argue that the in-context version does not capture the creative skills required in many real-world tasks. For instance, discovering fresh and surprising analogy necessitates noticing similarities from sufficiently distinct parts of ones vast, rich space of memory. Thus, the core challenge here lies in retrieving from the entirety of ones memory. If one were to faithfully simulate this an in-context version of this in model, one would have to provide the entirety of the models pretraining data in context. B.3. Examples of Triangle Discovery Although we presented this task as more complex, higher-order counterpart to Sibling Discovery, we retrospectively identify some real-world examples that resemble the higher-order search skill involved in this task. 1. Discovering contradictions: Consider identifying non-trivial contradictions within (a large body) of knowledge (like legal system, or proof based on many lemmas, or the literature spanning many papers in certain field). This may require identifying two or more facts that together result in an implication that contradicts another fact. 2. Discovering feedback loops: Fields like biology, ecology, climate science and economics may involve discovering non-trivial feedback loops. Unlike feedback loops where two events encourage each other, non-trivial loop would be one where an Event encourages Event B, that in turn encourages Event that in turn encourages Event A. 3. Antanaclasis: An antanaclasis involves using word in two different senses in sentence, while still ensuring that each sense has coherent relationship with the rest of the sentence. Consider Benjamin Franklins quote, Your argument is sound, nothing but sound. Here, the two senses are sound1 as in logically correct, sound2 as in noise. This sentence encodes an pairwise relationship between three entities {argument, sound1,sound2} 18 GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION individually. While the last two entities (the two senses) themselves must be related to each other (through the common word, sound), for coherent sentence, both senses must also be appropriate descriptors for the first entity, argument. Thus, constructing this sentence requires searching through ones vocabulary to discover three words that satisfy these three relationships simultaneously. 4. Word games: Some word games require identifying set of words that simultaneously have pairwise relationships with each other. (a) For example, standard crosswords would require identifying sets of 4 or more words that have various simultaneous pairwise intersections in the letters used. (b) Devising & Lit. clues in cryptic crosswords are an altogether different, yet compelling example that require discovering satisfying triangular relationship. Consider the clue Some assassin in Japan whose answer is Ninja. Here the phrase Some assassin in Japan participates in two senses. First, is the direct semantic sense as definition of what Ninja is. indirect sense: the word Some indicates that the solution lies as some substring of the phrase, namely assassi(n in Ja)pan. Thus, constructing the clue requires identifying triangular relationship between {Ninja, (Some assassin in Japan)1, (Some assassin in Japan)2} just like in an antanaclasis. This is true generally of any & Lit. clues as these clues must perform double duty in pointing to the answer. there is second, But B.4. Further evidence of our argument in 2.6 Below we provide two more pieces of evidence affirming the failure mechanism of next-token prediction outlined in 2.6. Improved algorithmic creativity is not due to some form of capacity control. While 2.6 argues that multi-token prediction should help creativity by providing critical lookahead capabilities, it is also possible that it simply acts as form of capacity control that prevents memorization. We rule this out in Fig 8: even as memorization computed on unseen hash strings is controlled, the multi-token model perfectly reproduces the training data on seen hash strings. We term this hash-memorization. An exact equivalence of this phenomenon was noticed in GANs in Nagarajan et al. (2018), where the generator can be trained on specific latent vectors to memorize the mapping on those, and yet produce fresh samples outside of those latent vectors. Figure 8. Even if multi-token prediction reduces memorization (on unseen hash strings), it has enough capacity to memorize training data on the seen hash-strings (denoted by hash-memorization). Note that the best algorithmic creativity for NTP and MTP are achieved at step 10k and 40k, respectively, which are the checkpoints we used to report metrics in Fig 4. Effect of token reordering. The implication of our argument in 2.6 is that next-token learning would benefit from reversing the token ordering of the Sibling Discovery task (i.e., parent appears before siblings). Indeed, we find this to be the case in Fig 12 and Fig 22. Interestingly, we find that the reverse-trained NTP model is still far from the original multi-token teacherless model. More surprisingly, teacherless model trained on the reversed data, achieves even higher algorithmic creativity of all training methods here. Note that in all other datasets, no reordering of the tokens should make any change to the training. 19 GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION (a) Sibling Discovery (b) Triangle Discovery Figure 9. Minimal tasks inspired by combinational creativity: The in-weights graph represents the underlying knowledge graph used to generate the training data (not provided in-context). Based on our definition of algorithmic creativity in Eq. (1), generated samples that are incoherent or memorized, or duplicated are not counted as valid samples. Note that sequences that are permutations of each other are considered identical when computing duplicates and memorization. C. DESCRIPTION OF DATASETS C.1. Datasets inspired by combinational creativity Dataset 1: Sibling Discovery. This task is based off bipartite graph made of parent vertices = {A, B, C, . . .} each neighboring corresponding set of children nbr(A) = {a1, a2, . . . , }. We set the number of parent vertices to be small and the number of children for each parent vertex nbr(A) to be large. For example, = 5 and nbr(A) = 500. We define coh(s) to hold on sibling-parent triplets of the form = (γ, γ, Γ) such that γ, γ nbr(Γ). Next, we ensure that the training set is large enough for the model to infer all the edges in the graph. Let = and = nbr(Γ) (for all Γ V). This means = Ω(m n). At the same time, to keep the task non-trivial, the training set must be small enough to not cover all the coherent sibling-parent triplets. Thus, we ensure = o(m n2). For the default version of this dataset, we set = 5 and nbr(Γ) = 500 for all Γ V. Dataset 2: Triangle Discovery This task is based off an undirected graph = (V, E) which contains many triangles. Since triangle is symmetric structure, the problem remains the same even upon reordering the vertices. Thus, in this task coh((v1, v2, v3)) = true iff all three edges between {v1, v2, v3} belong in G. To make this task interesting (neither too trivial nor too non-trivial) for our models to learn, we enforce several constraints on the graph. First, we try to keep the degree deg of each vertex to be sufficiently small. On the one hand, this is so that no vertex requires too much computation to find triangle it is part of; on the other, we also do not want very dense graph where most random triplets are triangle. In addition to this degree requirement, we ensure that each vertex has minimum number of triangles. Thus to create graph that is neither too trivial nor too non-trivial, we define two-step graph generation procedure. In the first step, we iterate over the vertices, and add deg many edges from that vertex to other vertices in the set (where 20 GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION (a) Circle Construction (b) Line Construction Figure 10. Tasks inspired by exploratory creativity: The constructed graph visualizes the graph induced by the training or generated sample. Edge indices represent the order of edge appearing in the string. Based on our definition of algorithmic creativity in Eq. (1), generated samples that are incoherent, memorized, or duplicated are not counted as valid samples. Note that sequences that correspond to the same permutations but with different participating vertices are considered identical when computing duplicates and memorization deg is small, such as 3 or 10). To avoid creating high-degree vertices inadvertently, we only select neighbors with degree 1.2 deg. This alone may not ensure sufficient number triangles in each vertex; so we iterate over the vertices to explicitly create tri random triangles on each vertex (where tri is small, such as 6 or 10). We do this by selecting pairs of vertexs neighbors and drawing an edge between them. Next, we want training dataset such that (a) the model can infer all the edges from the graph and yet (b) not all triangles appear in the dataset. This necessitates training on dataset that consists not only of subset of the triangles, but also of edges from the graph. Our training data consists of two parts: (1) 1/3 are random triangles from the graph, (2) 2/3 are random edges from the graph. In the training set, the triangle and edge samples are distinguished by prefix triangle: or edge:. During test-time, we ensure that the model is prompted with triangle:. triangle (u, v, w) is tokenized as tri: (u, v), (v, w), (w, u) and an edge (u, v) as edge: (u, v), (v, u). We provide both the directions of edge to potentially avoid any issues with the reversal curse (Berglund et al., 2024; Allen-Zhu & Li, 2023a). For the default setting of the dataset, we set = 999, deg = 3, tri = 6. 21 GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION Table 1. Hyperparameter details for Gemma v1 (2B) model. Hyperparameter Sibling Discovery Triangle Discovery Circle Construction Line Construction Max. Learning Rate 5 104 5 104 5 10 5 105 Model Seq. Len. Training steps Training size Weight given to multi-token obj. 32 50k 0.5 32 10k 15k 0. 2048 15k 10k 0.75 2048 15k 10k 0.75 C.2. Datasets inspired by exploratory creativity Dataset 3: Circle Construction. In this task, the generated strings must be randomized adjacency lists that can be rearranged to recover circle graphs of vertices. The vertices come from fixed vocabulary of tokens. Specifically, let the generated list be = (vi1, vi2), (vi3, vi4), . . .. We define coh(s) = true iff there exists resolving permutation π such that π(s) = (vj1 , vj2 ), (vj2, vj3), . . . (vjn , vj1) for distinct j1, j2, . . . jn. i.e., each edge leads to the next, and eventually circles back to the first vertex. In our experiments, we set to be larger than . Our default experiments are reported for = 9, = 15. Dataset 4: Line Construction This task is minor variant of the above where the edge set corresponds to line graph. The details are same here except for coherence to hold, we need resolving permutation π such that π(s) = (vj1, vj2), (vj2, vj3) . . . , (vjn1 , vjn ) for distinct j1, j2, . . . jn. i.e., each edge leads to the next, stopping at dead-end. We use the same set of hyperparamters as Circle Construction. Our default experiments are reported for = 9, = 15. D. FURTHER EXPERIMENTAL DETAILS Details for Gemma v1 (2B) model. In Table 1, we provide the hyperparameter details for each of our datasets. We note some common details here. First, the batch size is 4, but each sequence is packed with multiple examples; thus the model sequence length (divided by the input length) can be treated as multiplicative factor that determines the effective batch size. The learning rates are chosen favorable to next-token prediction (not multi-token prediction). The training steps were chosen roughly based on point after which the model had saturated in algorithmic creativity (and exhibited decreasing creativity). We use learning rate with linear warm up for 100 steps, followed by cosine annealing upto factor 0.01 of the maximum learning rate. To measure creativity, we sample test dataset of 1024 datapoints. We represent the main tokens in our tasks with integers (ranging upwards of 0 to as many distinct integers as required). In the hash-conditioning setting, we use hash strings of default length 10, using randomly sampled uppercase characters from the English alphabet. In all datasets, we space-separate the vertices in string, and comma-separate the edges. Details for GPT-2 (86M) model. We use GPT-2 (small) with 86M non-embedding parameters when we are comparing Transformers with diffusion models. We train these models with learning rate of 104 and batch size of 64, to convergence in terms of the algorithmic creativity. We provide sensitivity analysis of learning rate in F. Details for SEDD (90M) model. We use SEDDs absorb variant, which begins denoising with fully masked sequence and iteratively refines tokens over 128 denoising steps. This variant achieves the best language modeling performance in the original paper. Same as GPT-2 (86M), we train these models with learning rate of 104 and batch size of 64, to convergence in terms of algorithmic creativity. 3 We provide sensitive analysis of learning rate in F. 3We use the codebase of Lou et al. (2023) at https://github.com/louaaron/Score-Entropy-Discrete-Diffusion. 22 GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION E. SENSITIVITY ANALYSES FOR Gemma v1 (2B) In this section, we report that our observations are robust to the choice of various hyper-parameters. First, we present series of plots for the Gemma v1 (2B) model; each group of plots reports varying one hyperparameter for all the datasets. Fig 11 for train set size, Fig 12 for task complexity, Fig 13 for the weight given to the multi-token objective (and Fig 14 correspondingly for memorization), Fig 15 for learning rates, Fig 16 for number of training steps and Fig 17 for batch size. In E.1, we report analyses for varying sampling conditions. It is worth noting that the occasional exceptions to our trends generally come from Line Construction, suggesting that this task is most friendly towards next-token prediction of the four we study. Note on task-complexity. In Fig 12, we report robustness of our results to variations in the task complexity (e.g., degree, path length etc.,). Note that the variations we have explored are within reasonable factors. If we vastly increase certain factors (e.g., increase the degree of the vertices), we expect learning to become either highly trivial or non-trivial (see for some reasoning). Besides, as discussed in the main paper, teacherless training is hard objective to optimize especially for smaller models; thus, we expect increasing the task complexity beyond point to hurt the teacherless model for fixed model size (crucially, for optimization reasons, not generalization reasons). Figure 11. Training size and algorithmic creativity for Gemma v1 (2B): Algorithmic creativity increases under multi-token prediction across various training set sizes. Note though that, in our examples, we except the gap to diminish eventually with sufficiently many training datapoints (this is unlike the failure of next-token prediction in B&N24). Figure 12. Task complexity and algorithmic creativity for Gemma v1 (2B): Algorithmic creativity increases under multi-token prediction across (reasonable) variations in the dataset parameters (as described in C). E.1. Varying sampling methods Fig 18, Fig 19, and Fig 20 report creativity, memorization and coherence (i.e., fraction of generated strings that are coherent) for various sampling methods (greedy decoding and nucleus sampling) with various prefix conditionings (namely, null, pause and hash). 23 GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION Figure 13. Weight given to multi-token objective and algorithmic creativity for Gemma v1 (2B): Algorithmic creativity increases under multi-token prediction across various weights given to the multi-token component of the objective, barring some deviations for Line Construction. Figure 14. Weight given to multi-token objective and memorization score for Gemma v1 (2B): Memorization reduces under multitoken prediction across various weights given to the multi-token component of the objective. Figure 15. Learning rate and algorithmic creativity for Gemma v1 (2B): Algorithmic creativity increases under multi-token prediction across various learning rates. 24 GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION Figure 16. Training steps and algorithmic creativity for Gemma v1 (2B): Algorithmic creativity under multi-token prediction across lengths of training. Figure 17. Batch size and algorithmic creativity for Gemma v1 (2B): Algorithmic creativity increases under multi-token prediction across various batch sizes. Note that here batch size is effectively proportional to the model sequence length, since we pack multiple finetuning examples into the sequence. 25 GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION Figure 18. Algorithmic creativity under various sampling conditions for Gemma v1 (2B): Across all conditions, and in almost all datasets (with few exceptions in Line Construction), multi-token prediction improves creativity. Furthermore, hash-conditioning achieves best algorithmic creativity, with longer hash helping more. 26 GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION Figure 19. Memorization under various sampling conditions for Gemma v1 (2B): Barring few conditions, the most prominent trend is that memorization reduces under multi-token prediction for various sampling conditions. Observe that the null and pause-conditioned models do produce some memorized output while their creativity was non-existent. 27 GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION Figure 20. Coherence under various sampling conditions for Gemma v1 (2B): Surprisingly, coherence of all models is high or at least noticeable, across various sampling conditions. This suggests that the low algorithmic creativity of the null-conditioned models in the previous plots arises from model collapsing to single original point. 28 GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION F. ADDITIONAL EXPERIMENTS IN SEDD (90M)VS. GPT-2 (86M) F.1. Ablation studies In this section, we first provide additional ablation studies for SEDD (90M)vs GPT-2 (86M)with different training and dataset settings (Fig 21 and Fig 22). Figure 21. Learning rates and algorithmic creativity for the SEDD (90M)model vs. GPT-2 (86M): MTP achieves higher algorithmic creativity than NTP when both are trained at their optimal learning rates. F.2. Effect of hash string length We provide an ablation study on the hash string length for NTP vs MTP on the Sibling Discovery task (Fig 23). We see that longer hash strings lead to higher algorithmic creativity. 29 GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION Figure 22. Task complexity and algorithmic creativity of SEDD (90M) model vs. GPT-2 (86M): MTP consistently outperforms NTP under varying task configurations, with some exceptions in the Line Constructionand Circle Constructiondatasets. F.3. Format sensitivity for Triangle Discovery Recall that our input format for Triangle Discovery follows the edge list representation of triangles (C, Fig. 10). For instance, triangle ABC is represented as AB, BC, CA. This format explicitly lists the edges of the triangle, making it easier for the model to attend to edge-level patterns during learning. We also experimented with an alternative node-based representation, where triangle ABC is represented more compactly as ABC, without making the edges explicit. We note in Fig 24 that the models are curiously sensitive to the way the triangles are 30 GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION Figure 23. GPT-2 (86M) Transformer achieves higher algorithmic creativity with longer hash strings. We report algorithmic creativity with hash strings of length 4 and 10, with both NTP and teacherless MTP. formatted. Models trained on the node-based format perform equally badly with all training objectives, while the diffusion model outperforms NTP by large margin with the edge list representation. Figure 24. Sensitivity to formatting of the sequence in Triangle Discovery: We find that all our small models perform equally poorly with node-wise representation of the input sequence, whereas there was stark difference in performance with the edge-wise representation. G. ADDITIONAL EXPERIMENTS WITH MEDIUM-SIZED TRANSFORMER AND SEDD We replicate our SEDD (90M) and GPT-2 (86M) experiments on larger model size (400M parameters). In Fig 25, we see similar trends to the smaller model sizes (Fig 4). H. DECOMPOSING CREATIVITY Through following experiments on the GPT-2 (86M) model in the Sibling Discovery task, we try to understand the dynamics between two important quantities that affect algorithmic creativity: diversity/duplication and originality/memorization H.1. Diversity score Equation (1) defines our algorithmic creativity by rewarding samples that are both unique and novel. higher score can be achieved either by enhancing diversity or by reducing memorization. In the following section, we examine this decomposition using the Sibling Discovery task. Formally, we define the diversity score as: ˆdvN (T ) = uniq({s coh(s)}) . (4) We first demonstrate that creativity and diversity are not necessarily correlated, and next, that MTP particularly improves creativity (while achieving lower diversity than NTP). To show this, we report the algorithmic creativity and diversity scores along training in Fig 26. We see that for NTP, the diversity score keeps increasing and stays high, while algorithmic creativity 31 GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION Figure 25. On medium-sized (400M) model, multi-token diffusion training improves algorithmic creativity from Eq 1 (top) on our four open-ended algorithmic tasks. increases in the first 10k steps and starts to decrease. For teacherless training, both scores increase throughout training. While the diversity of MTP is surprisingly lower than NTP throughout training, the creativity of MTP surpasses NTP at 20k steps. Figure 26. Algorithmic creativity and diversity are not necessarily correlated, exhibiting distinct dynamics: We find that NTP has high diversity score through training, even higher than MTP. However, its algorithmic creativity reaches only mediocre peak before descending, when MTP starts surpassing it. H.2. Decomposing algorithmic creativity as diversity and memorization Better creativity can be achieved either by enhancing diversity or by reducing memorization we try to disentangle these factors in this section. In Fig 27, we plot the algorithmic creativity, diversity, and memorization scores at the checkpoint of best algorithmic creativity. We see that hash-conditioning contributes to higher diversity but does little to bring down memorization; however, teacherless training contributes to higher diversity and also to reducing memorization. In Fig 26, we see that the best creativity and best diversity are not achieved at the same checkpoint. H.3. Data scaling for algorithmic creativity How does algorithmic creativity change as we increase the amount of training data? Intuitively, more training data helps the model learn the true distribution, but also makes it harder to generate unseen samples (since the uncovered space becomes rarer). To understand this, we plot how models perform relative to theoretically expected maximum algorithmic creativity. 32 GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION Figure 27. Decomposition of algorithmic creativity for GPT-2 (86M) in Sibling Discovery: We report algorithmic creativity, diversity and memorization at the checkpoint of best algorithmic creativity. We see that hash-conditioning contributes to higher diversity but does not help bring down memorization; teacherless training helps both diversity and in bringing down memorization. Figure 28. Data scaling curve for algorithmic creativity and diversity: As we increase the training data (for fixed underlying graph), the theoretically expected maximum algorithmic creativity decreases as expected, while the theoretically expected maximum diversity stays the same. NTP tails to achieve the theoretically expected algorithmic creativity, while MTP almost achieves the theoretically expected performance at scale. This is computed by assuming an oracle that samples generated set (in Eq. (1)) uniformly with replacement from the true underlying distribution, and then computing algorithmic creativity Eq. (1. In Fig 28, we see that as we increase the training data (for fixed underlying graph), the theoretically expected creativity decreases as expected, while the theoretically expected diversity stays the same (since this quantity does not care about being original with respect to the training set). Interestingly, as training data increases, MTP narrows the gap between NTP and the theoretically expected creativity and almost achieves the theoretically expected performance in the high data regime. 33 GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION I. EXPERIMENTS ON SUMMARIZATION Experimental Details. In Table 2, we provide the hyperparameter details for the GPT models finetuned on both XSUM (Narayan et al., 2018) and CNN/DailyMail (Nallapati et al., 2016) for one epoch. We use learning rate with linear warm up for 0.05 of the total steps, followed by linear decay to 0. To measure Rouge and Self-Bleu, we generate and average across 5 summarizations per document, on test dataset of 250 datapoints. We finetune our models with either the NTP objective (Eq 2) or the teacherless MTP objective (Eq 2), with equal weight to both. Table 2. Hyperparameter details for summarization experiments. Hyperparameter XSUM CNN/DailyMail Batch Size 32 32 Max. Learning Rate 5 105 3 106 Warmup Steps Training Steps 338 Training Size 248906 124 2486 79552 To measure quality, we compute the average of Rouge-1, Rouge-2, Rouge-L as Rouge. For measuring diversity, we generate five different summaries per test example, and compute Self-Bleu. This computes average pairwise sentence Bleu-2 scores with weights (0.5, 0.5, 0, 0) on 1and 2-tuples. I.1. Additional graphs for effect of multi-token training Fig 29 shows the diversity and quality graphs on the smaller-sized GPT-2 models on XSUM, and Fig 30 for CNN/DailyMail. While we consistently see improved quality from the multi-token model across the board, we dont see an increased diversity for fixed Rouge scores anymore. Figure 29. Multi-Token Objective has no effect on diversity for smaller GPT models on XSUM. I.2. Effect of hash-conditioning We also conducted hash-conditioning experiments as described in 3.1. The hash strings we use are 10 randomly sampled uppercase characters from the English alphabet. We report the quality-diversity plots in Fig 31 (for next-token prediction on XSUM) and Fig 32 (for multi-token prediction on XSUM). As such, we do not find any changes in diversity, perhaps because this is not sufficiently open-ended task. GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION Figure 30. Multi-Token Objective increases diversity for GPT-L and GPT-M but not for GPT-XL or GPT-S on CNN/DailyMail Figure 31. Hash-conditioning has no effect on diversity for GPT models on XSUM summarization with next-token prediction. J. MORE RELATED WORKS Empirical studies of creativity in LLMs. There is long line of recent works that measure novelty and creativity of LLMs and LLM-assisted users. (Chakrabarty et al., 2024; Lu et al., 2024b) quantitatively evaluate and report that models vastly underperform under expert human evaluation against human writers. Zhang et al. (2024a) argue that finetuning methods such as RLHF and DPO, are limited when applied to creative humor-generation tasks. Likewise models like GPT4 and Claude currently underperform top human contestants in generating humorous captions. In poetry, Walsh et al. argue that there are certain characterstic styles that ChatGPT restricts itself to. Even assisted-writing can reduce diversity GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION Figure 32. Hash-conditioning has no effect on diversity for GPT models on XSUM summarization with multi-token prediction. (Padmakumar & He, 2024) or produce bland writing (Mirowski et al., 2024). On the positive side, Si et al. (2024) report that LLMs surprisingly generate novel research ideas, although these are less feasible. Anderson et al. (2024) find that users tend to produce more divergent ideas when assisted by ChatGPT (although at group level, ideas tend to homogenize). Another line of works (Wang et al., 2024a; Talmor et al., 2020; Zhong et al., 2024) has proposed algorithmic improvements involve creative leaps-of-thought for real-world tasks. Other studies have proposed benchmarks for evaluating creativity. AidanBench (McLaughlin et al., 2024) and NoveltyBench (Zhang et al., 2025) evaluate LMs on their ability to produce diverse and coherent responses by penalizing repetition across generations. However, they do not measure originality relative to training data, leaving open whether outputs are genuinely novel or simply unseen paraphrases/recombinations. Zhao et al. (2024) evaluate LM creativity using the Torrance Tests of Creative Thinking, standard in human psychometrics. Another line of work such as Alchemy (Wang et al., 2021), IVRE (Xu et al., 2022), and DiscoveryWorld (Jansen et al., 2024) present simulations with hidden facts and rules, requiring LMs to explore, hypothesize, and test through interaction. While these simulations focus on pretrained models rather than examining how training shapes creative capabilities, they serve as valuable and realistic benchmarks for assessing the role of creativity in scientific discovery. Finally, we refer the reader to Franceschelli & Musolesi (2023) for rigorous treatment of philosophical questions surrounding creativity in LLMs. We also refer to Wang et al. (2024b) for theoretical treatment of how to formalize subjectivity in creativity. The next-token prediction debate. In support of next-token prediction, there are arguments (Shannon, 1948; 1951; Alabdulmohsin et al., 2024) that claim that language is captured by NTP with models even superceding humans (Shlegeris et al., 2022) at NTP. There are also theoretical results emphasizing the expressivity (Merrill & Sabharwal, 2024; Feng et al., 2023) and learnability (Malach, 2023; Wies et al., 2023) of autoregressive Transformers as long as there is sufficiently long chain of thought. Multi-token training. While these methods employ diverse strategies, common feature is their reliance on multi-token objectives that capture broader dependencies across entire sequences. Representative examples include teacherless training (Bachmann & Nagarajan, 2024; Monea et al., 2023; Tschannen et al., 2023) and independent output heads or modules (Gloeckle et al., 2024; DeepSeek-AI et al., 2024) or inserting lookahead attention (Du et al., 2023). Another line of research is discrete diffusion models (Hoogeboom et al., 2021; Austin et al., 2021; Gong et al., 2023; Lou et al., 2023), which avoid strict left-to-right factorization by iteratively refining an entire sequence at multiple positions. There are other 36 GOING BEYOND THE CREATIVE LIMITS OF NEXT-TOKEN PREDICTION models as well, such as energy-based models (Dawid & LeCun, 2023) and non-autoregressive models or (Gu et al., 2018). Transformers and graph algorithmic tasks. Graph tasks have been used to understand various limitations of Transformers in orthogonal settings. Bachmann & Nagarajan (2024); Saparov et al. (2024) report that Transformers are limited in terms of learning to search tasks on graphs, while Sanford et al. (2024) provide positive expressivity results for range of algorithmic tasks that process an graph. These works differ from our study of combinational creativity since their graphs are provided in-context and the tasks have unique answer. Other works (Schnitzler et al.; Yang et al., 2024a;b) study multi-hop question answering on knowledge graph; however, this does not require planning. Diversity of generative models. One line of work relevant to us in the history of generative models is RNN-based VAE for text data (Bowman et al., 2016). The motivation, like in our work, was to learn high-level semantic features rather than next-token features with the hope of producing more novel sentences. However, this suffered from posterior collapse, where the model ignores the latent variable altogether inspiring various solutions (Yang et al., 2017; Goyal et al., 2017). Our results on hash-conditioning are also reminiscent of line of work on exploration in reinforcement learning (RL), where it has been shown that adding noises to the policy model parameters enables more efficient exploration than directly adding noises to the output space (Plappert et al., 2017; Fortunato et al., 2017). Learning-theoretic studies of diversity in LLMs. Various theoretical works provide rigorous arguments for how preventing hallucination and maximizing the models coverage are at odds with each other in abstract settings (Kalai & Vempala, 2024; Kalavasis et al., 2024; Kleinberg & Mullainathan, 2024). We clarify that this tension does not apply in our concrete settings. In those abstract settings, the strings in the support can be arbitrary and adversarially chosen whereas, our strings are generated by simple rule (which can be learned). Another theoretical question underlying generative models is that the optimum of their objectives are attained at perfect memorization; yet they tend to produce novel examples e.g., this question has been posed for GANs in Nagarajan et al. (2018) and for diffusion in Nakkiran et al. (2024) (see remarks on generalization) or Kamb & Ganguli (2024). Of relevance to us is, Kamb & Ganguli (2024) who provide theoretical and empirical argument for how image diffusion models are able to generate combinatorially many creative outputs; theirs however do not require the type of planning our tasks do."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University, Pittsburgh, US",
        "Google Research, US"
    ]
}