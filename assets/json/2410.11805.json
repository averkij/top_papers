{
    "paper_title": "NesTools: A Dataset for Evaluating Nested Tool Learning Abilities of Large Language Models",
    "authors": [
        "Han Han",
        "Tong Zhu",
        "Xiang Zhang",
        "Mengsong Wu",
        "Hao Xiong",
        "Wenliang Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) combined with tool learning have gained impressive results in real-world applications. During tool learning, LLMs may call multiple tools in nested orders, where the latter tool call may take the former response as its input parameters. However, current research on the nested tool learning capabilities is still under-explored, since the existing benchmarks lack of relevant data instances. To address this problem, we introduce NesTools to bridge the current gap in comprehensive nested tool learning evaluations. NesTools comprises a novel automatic data generation method to construct large-scale nested tool calls with different nesting structures. With manual review and refinement, the dataset is in high quality and closely aligned with real-world scenarios. Therefore, NesTools can serve as a new benchmark to evaluate the nested tool learning abilities of LLMs. We conduct extensive experiments on 22 LLMs, and provide in-depth analyses with NesTools, which shows that current LLMs still suffer from the complex nested tool learning task."
        },
        {
            "title": "Start",
            "content": "NESTOOLS : Dataset for Evaluating Nested Tool Learning Abilities of Large Language Models Han Han, Tong Zhu, Xiang Zhang, Mengsong Wu, Hao Xiong, Wenliang Chen * Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, China {hhan,tzhu7,xzhangxzhang23,mswumsw,hxiongxionghao}@stu.suda.edu.cn wlchen@suda.edu.cn 4 2 0 2 5 1 ] . [ 1 5 0 8 1 1 . 0 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) combined with tool learning have gained impressive results in real-world applications. During tool learning, LLMs may call multiple tools in nested orders, where the latter tool call may take the former response as its input parameters. However, current research on the nested tool learning capabilities is still under-explored, since the existing benchmarks lack of relevant data instances. To address this problem, we introduce NESTOOLS to bridge the current gap in comprehensive nested tool learning evaluations. NESTOOLS comprises novel automatic data generation method to construct largescale nested tool calls with different nesting structures. With manual review and refinement, the dataset is in high quality and closely aligned with real-world scenarios. Therefore, NESTOOLS can serve as new benchmark to evaluate the nested tool learning abilities of LLMs. We conduct extensive experiments on 22 LLMs, and provide in-depth analyses with NESTOOLS , which shows that current LLMs still suffer from the complex nested tool learning task."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have shown powerful abilities in natural language understanding and reasoning (Achiam et al., 2023; Dubey et al., 2024; Yang et al., 2024; Jiang et al., 2024; Zhu et al., 2024). To extend such abilities into realworld systems, tool learning (Inaba et al., 2023) has become promising paradigm to solve complex problems and reduce hallucinations with external APIs, such as the calculator and the search engine (Patil et al., 2023; Schick et al., 2024). In realworld application, LLMs may interact with multiple tools (Song et al., 2023; Ye et al., 2024; Huang et al., 2023; Ma et al., 2024). As shown in Figure 1, the multi-tool calling process is nested, where the * Corresponding author 1 Figure 1: Example of nested tool calling. latter tool may take the former ones response as an input parameter. This scenario is prevalent in accomplishing complex tasks with many execution steps. However, the LLMs nested tool learning ability is still under-explored, and corresponding benchmarks are absent to provide comprehensive evaluations. Among current tool learning datasets, some datasets completely ignore the nested tool calls (Tang et al., 2023; Patil et al., 2023). Other benchmarks only have small quantity of nested tool calls (Li et al., 2023; Huang et al., 2024; Wu et al., 2024) or have low qualities with repetitive patterns (Shen et al., 2023) and coarse evaluations (Chen et al., 2024; Qin et al., 2023b). To this end, it is hard to comprehensively assess LLMs performance on the real-world nested tool learning scenario, and provide insights for further model development. To address the above challenges, we introduce NESTOOLS , high-quality nested tool learning benchmark to provide comprehensive evaluations. NESTOOLS offers an innovative automated data construction scheme, generating large-scale and NESTOOLS (Ours) T-Eval (Chen et al., 2024) API-Bank (Li et al., 2023) ToolBench (Qin et al., 2023b) UltraTools (Huang et al., 2024) BFCL (Patil et al., 2023) Tools Amount Avg. params (required) Instances Amount Multiple-tool callings Nested-tool callings Avg. tool call Multiple-tool scenario? Evaluation of nesting? 3,034 2.24 1,000 1,000 830 3.04 50 1.24 553 553 N/A 5.81 73 1.97 485 122 50 1.53 16,464 1.01 126,486 85,330 N/A unknown 436 4.22 1,000 867 227 2.38 1,618 2.11 2,000 490 0 unknown Table 1: Comparison of tool learning datasets for evaluation. The tool calling procedure is carried under multi-step setting and the exact number cannot be obtained. The statistics refer to Seal-Tools(Wu et al., 2024). more diverse examples of nested tool learning compared to existing datasets. The entire dataset construction process includes tool & instance generation, query generation, and data review & refinement. To ensure an accurate and consistent evaluation, we carefully select and cross-verify 1,000 data entries. For more comprehensive evaluations, we assess models on four deterministic dimensions: tool selection, tool calling order, parameter filling, and nested parameter filling. Compared to existing benchmarks in Table 1, our dataset focuses on the nested tool learning task and provides largescale tools with more nested calls. In addition, our fine-grained assessment dimensions on nesting tool calls could provide set of more comprehensive tool learning evaluations that are closely aligned with real-world scenarios. conduct on includNESTOOLS with 22 popular LLMs, ing API-based models and open-sourced models. We provide thorough analyses in terms of the nesting depth, nesting structure, scaled model sizes, and robustness effects. The results show that although models benefit from size scaling, they still suffer from the simple tool selection, and the performance would degrade heavily when tools are deeply nested. Our core contributions are as follows: experiments extensive We We propose novel automatic data construction pipeline to easily generate large-scale nested tool learning datasets. We introduce NESTOOLS , high-quality dataset with large-scale diverse examples for comprehensive nested tool learning evaluations. We conduct extensive experiments on 22 LLMs to verify their effectiveness and generalization abilities, providing detailed analyses 2 and insights for LLMs in the field of nested tool learning."
        },
        {
            "title": "2 Related Work",
            "content": "Tool learning Early works (Yao et al., 2022; Schick et al., 2024; Paranjape et al., 2023) have incorporated straightforward tools such as search engines and calculators to enhance LLMs access to up-to-date information and precise mathematical reasoning. Following this, ToolBench (Qin et al., 2023b) employs real-world APIs to construct datasets capable of addressing broader spectrum of real-world user queries. Although ToolBench provides large-scale dataset, it only evaluates the output results without assessing the process, lacking fine-grained comprehensive evaluations. Tool Evaluation Although tool learning is rapidly advancing, researchers tend to focus primarily on the general framework (Qin et al., 2023a) while fine-grained evaluations in tool learning still remain unexplored. Many benchmarks (Chen et al., 2024; Huang et al., 2024; Li et al., 2023; Basu et al., 2024) segment tool learning into multiple aspects, evaluating the performance of various LLMs across these dimensions. However, they focus more on the multi-tool calling instances, and nested tool call evaluations are ignored. Although these benchmarks include limited number of nested tool calling, researchers have not undertaken comprehensive analysis and evaluation of this phenomenon. Consequently, the performance of LLMs in handling nested tool calling remains under-explored. To address this gap, we introduce NESTOOLS , dataset specifically constructed to feature multitude of nested tool calling scenarios across various domains. The purpose of this dataset is to systematically evaluate the performance of LLMs in nested tool learning abilities. Figure 2: The dataset construction process of NESTOOLS ."
        },
        {
            "title": "3 Dataset Construction",
            "content": "To obtain NESTOOLS , we propose an automatic dataset generation pipeline with manual reviews. As shown in Figure 2, the whole process starts with tool and instance generation (3.1), followed by query generation (3.2), and data review & refinement (3.3). Lastly, we provide comparative analysis for NESTOOLS with other datasets (3.4)."
        },
        {
            "title": "3.1 Tool & Instance Generation",
            "content": "In real-world applications with limited number of tools, call chains containing nested calls are relatively restricted and homogenized, and there are not too many non-duplicated and diverse call chains. To get wide range of tools and instances, existing benchmarks mostly use conventional two-round generation scheme, including filtering or constructing portion of the tools first, and then filtering or generating some instances (Qu et al., 2024; Chen et al., 2024). However, the proportion of generated samples containing nested instances under this scheme is very small because LLMs cannot synthesize as many candidate tools as possible to obtain potential nested call chains under the limitations of the context. If relying on manual brainstorming, this is very costly and difficult to scale and ensure diversification. How to increase the frequency of nested calling instances? LLMs are relatively unfamiliar with nested tool calling. However, they are very familiar with nested functions in code because the pretraining process has already injected large amounts of code knowledge into LLMs (Dubey et al., 2024). The two types of nesting can be very similar to some extent, with the common code pattern being to define number of Python functions and then execute the calls in the main function. In this task, we ask GPT3.5 to generate fixed-format functions as tools with annotation for necessary tool information, and then generate call chain to execute the tools in the main function. To ensure that the automated generation tools can cover wider range of domains and minimize repetition rates as much as possible, we collect the domains from SealTools (Wu et al., 2024), including 146 domains and 5,860 sub-domains, serving as reference guide for the generation of the subsequent tools and instance. When generating tools, we adopt template with functions in Python code, including comments that describe the tool, required parameters, optional parameters, and return parameters. Initially, we attempt nested call pattern using func1(func2(...), *args). However, this style is insufficient for more complex nested examples, as it results in lengthy expressions and lacks clarity in referencing specific return parameters when dealing with multiple return values. The improved generation pattern is as follows: data0, ... = func1(...), data2, ... = func2(data0, ...). This approach accommodates more complex nesting and indicates specific return values, as illustrated in Figure 2. It is worth mentioning that the in-context examples are very important, there are different nested structures for different nested calls, to ensure the diversity and rationality of the generation, we carefully select portion of high-quality samples to form the sample pool, and rotate these examples during the iterative generation process. At the same time, to minimize homogeneous generation, we set the generation temperature to 0.95 to seek for more diversified generation. 3 3.2 Query Generation While the tool and call chain are generated, we reserve comment to initially generate some raw queries that refer to the logic and flow of the call chain, although they may have flaws such as missing necessary parameter values. We then provide the entire actual call chain and the raw query to allow GPT3.5 to generate the complete task, step that allows the big model to populate all necessary parameters. Finally, we let GPT-4 (Achiam et al., 2023) optimize and rewrite the query to make it more complete and closer to the real requirements. Corresponding prompts are presented in Table 6 and Table 7 in the Appendix. 3.3 Data Review & Refinement To ensure the data quality, we perform strict reviewing steps and make further refinements. We first perform automatic checks. Given that nested tool calling contains multiple tools where each tool contains multiple return values, it is necessary to ensure that each return value in the call chain has variable to receive it. As result, we conduct the format check to filter out abnormal output formats and irregularities in generation. It is worth noting that in addition to the format error, the automated screening includes an abstract syntax tree (AST) check to ensure the consistency of tools and call chains. This process could align the return values and parameters between the candidate tools and the call chains. After the automated check, we perform manual review for further quality control. For the call chains, we mainly check whether the values of the parameters are in accordance with the description information in the golden tools. Besides, the most critical aspect of this reviewing step is to determine whether the nested parameters are filled in correctly. It requires manual tracing back to the parent node tool that returns the parameter, and carefully comparing the parent nodes return value with the textual description of the parameter at this position. Potential errors include the parameter being non-nested parameter but being filled as nested one, or being nested parameter but being filled with an incorrect return value from previous tool. For the task query review, we mainly check whether the non-nested parameters in the call chain are mentioned in the query, whether the query description conforms to the logic of the call chain, and whether the low-quality samples are optimized to meet real-world requirements. We employ two annotators to tag NESTOOLS from two perspectives: consistency between the call chain and the tools, and whether the query meets the requirements. Besides, the annotators are required to refine the data instances if there are errors. If there is disagreement between the two annotators, an expert would help make final annotation. The overall annotation process shows high agreement of 0.96. 3.4 Dataset Summary As shown in Table 1, we compare NESTOOLS in detail with the currently available datasets, and with the necessary number of tools and instances guaranteed, NESTOOLS has clear advantage in terms of nested tool calling, which not only covers higher percentage of nesting and includes new framework of evaluation LLMs for nested tool calling."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "In order to evaluate the LLMs understanding of the whole call chain of nested tool calls, we use the pattern of prompting for evaluation. For the parameter of the nested tools, the model is constrained to generate placeholder like \"API_call_{number}\" to fill the parameter of nested position in the evaluation prompt. For each sample, in order to simulate the scenario when the LLM faces similar tools, we utilize the gte-large retriever 1 to retrieve 5 similar tools for each gold tool from the tool pool. To minimize the occurrence of ambiguous tools, we control for the fact that the name of each retrieved tool must meet certain Levenshtein distance requirement with the names of all gold tools. We conduct experiments on NESTOOLS with the latest API-based and open-sourced LLMs. For the API-based LLMs, we selected four representative LLMs: GPT-4o, GPT-4o-mini and GPT-3.5 from OpenAI and Claude3.5 from Anthropic2. For the open-sourced LLMs, we choose the representative ones: LLaMA3.1 (Dubey et al., 2024), Qwen1.5 (Bai et al., 2023), Qwen2 (Yang 1https://huggingface.co/Alibaba-NLP/ gte-large-en-v1.5 2The version for GPT-4o is gpt-4o-2024-08-06, for GPT-4o -mini is gpt-4o-mini-2024-07-18, for GPT-3.5 is gpt-3.5-turbo0125 and for Claude3.5 is claude-3-5-sonnet-20240620. 4 Model Selection Order Parameter Nested Param Avg. F1 F1 F1 F1 API-Based Models GPT-4o GPT-4o-mini GPT-3.5 Claude-3.5 85.1 86.2 85.7 72.8 74.2 73.5 75.4 78.5 76.9 73.4 69.2 71.3 76.8 77.4 72.6 74.9 60.7 56.9 58.7 65.3 64.5 64.9 58.9 48.1 53.0 62.9 69.1 71.3 70.2 48.0 50.4 49.2 56.8 62.0 59.3 46.5 41.8 44.0 55.7 79.6 82.8 81.2 64.2 68.1 66.1 70.4 75.8 73.0 68.2 65.4 66.7 71.7 Open-Sourced Models 0. 0.0 65.7 71.6 68.5 44.2 50.7 47.2 44.2 51.0 47.3 40.8 42.7 41.7 51.2 LLaMA3.1-8B LLaMA3.1-70B 82.2 81.9 82.0 68.8 68.7 68.8 57.6 60.2 58.9 67.4 60.9 64.0 68.4 LLaMA3.1-405B 87.9 87.2 87.5 77.3 77.4 77.3 66.3 68.4 67.3 78.8 72.7 75.7 77.0 49.5 44.0 46.6 22.0 21.7 21.8 35.1 32.9 34.0 21.9 14.0 17.1 29.9 Mistral-7B v0.2 62.4 64.3 63.4 38.0 41.3 39.6 46.2 49.8 48.0 35.6 35.1 35.3 46.6 Mixtral-8x7B 70.5 75.2 72.7 49.6 54.7 52.0 59.4 66.7 62.8 53.0 52.3 52.6 60.1 Mixtral-8x22B 0.0 0.0 0.0 Qwen2-0.5B 0.0 0.0 0.0 39.5 18.0 24.8 12.1 4.3 Qwen2-1.8B 12.5 28.2 14.1 18.8 0.0 59.3 58.9 59.1 34.8 35.4 35.1 41.0 43.0 42.0 29.6 14.2 19.1 38.8 Qwen2-7B 63.8 68.9 66.3 40.2 45.5 42.7 47.6 54.2 50.7 37.8 29.0 32.9 48.1 Qwen2-57B 78.4 79.2 78.8 62.8 64.2 63.5 66.9 69.8 68.3 57.9 58.7 58.3 67.2 Qwen2-72B 0.1 0.0 2.2 0.0 10.2 0.2 Qwen1.5-0.5B 21.3 3.4 Qwen1.5-1.8B 2.4 0.0 9.7 0.8 Qwen1.5-7B 26.3 54.9 35.5 43.1 27.6 18.6 22.2 37.9 27.1 31.6 21.2 5.2 61.8 57.0 59.3 36.9 35.3 36.1 46.4 45.8 46.1 34.2 21.4 26.3 42.0 Qwen1.5-14B 65.2 61.1 63.1 41.6 40.7 41.1 52.3 51.3 51.8 45.6 27.2 34.1 47.5 Qwen1.5-32B 66.0 59.7 62.7 43.1 39.9 41.5 49.4 47.2 48.3 31.4 21.4 25.5 44.5 Qwen1.5-72B 69.7 65.0 67.3 48.9 46.7 47.8 56.2 54.7 55.5 44.7 31.8 37.2 51.9 Qwen1.5-110B 0.0 0.0 8.4 0.0 6.3 0.0 0.0 0.0 0. 0.3 5.8 0.1 2.9 0.1 1.7 0.0 0.4 0.0 0.0 0.0 2. 0.0 0.0 Table 2: The main results of NESTOOLS . Avg. is the average F1 score of all evaluation metrics. Bold represents the best score among all models. et al., 2024), Mistral (Jiang et al., 2023) and Mixtral (Jiang et al., 2024). We perform detailed experiments on each of their sizes. The detailed evaluation prompt is shown in Appendix B.2."
        },
        {
            "title": "4.2 Evaluation Metrics",
            "content": "For NESTOOLS , to standardize the evaluation, we devise metrics for the following aspects: correctness of tool selection, correctness of the order of tool calls, correctness of parameter filling, and correctness of nested parameter filling. Each evaluation is deterministic P/R/F1 metric. Selection P/R/F1 measures the accuracy of LLMs tool selection, which is common evaluation dimension. Order P/R/F1 measures the accuracy of LLMs judgment of the previous and subsequent tools. Since nested tool calling includes the use of the return value of the previous tool, it is necessary to evaluate the order of the tools before and after. Parameter P/R/F1 measures the correctness of LLMs filling parameters, which is also common evaluation dimension. Nested Param P/R/F1 is specific to nested calls and evaluates the accuracy of parameters at nested locations in the call chain. Other metrics and details are listed in Appendix B.1."
        },
        {
            "title": "4.3 Main Results",
            "content": "In the field of tool learning, for long time, the GPT-4 series has occupied the position of the best performance, and open-source LLMs are once out of reach (Huang et al., 2024; Chen et al., 2024). However, as shown in Table 4, we are pleasantly surprised to find that LLaMA3.1-405B slightly outperforms GPT-4o on NESTOOLS , achieving the best performance among all evaluated LLMs. This signifies that open-sourced and API-based LLMs 5 are currently very competitive in tool learning. We observe that the LLaMA3.1 series exhibits significant advantages in Selection and Order. Notably, LLaMA3.1-8B outperforms LLMs like Mixtral-8x7B and Qwen2-57B, which are substantially larger in size. Qwen2-72B reaches the best performance in parameters among open-sourced LLMs. Mixtral-8x22B has the most stable JSON compliance among open-sourced LLMs, second only to GPT-4o and Claude3.5 in all LLMs. However, in terms of parameter filling, the LLaMA3.1 series shows an unusual disadvantage. Analysis of sample cases reveals that LLaMA3.1 is less sensitive to parameter types, often generating incorrect parameter types in JSON outputs, even though the extracted answer information may be correct. This issue is most pronounced in the LLaMA3.1 series. LLaMA3.1-70B is most severely affected, resulting in loss of nearly 10 percentage points in parameter metric. It is worth noting that such problems are significantly less prevalent in other LLMs, and they are entirely absent in Claude3.5. This suggests that compared to other LLMs, the LLaMA series may have undergone relatively less training focused on JSON format alignment. Nested tool calling presents certain level of difficulty, and the stronger the LLMs capabilities, the better it performs in resolving nested tool calling. Filling nested parameters requires the model to truly understand the call chain and identify which specific return value corresponds to each nested parameter. Judging from the metrics of parameters and nesting, LLMs performance in nesting is generally lower than that in parameters, but this gap is narrowing as the LLMs overall performance improves."
        },
        {
            "title": "5.1 Scaling Analysis",
            "content": "Scaling law is common in LLMs. As shown in Figure 3, we can conclude that most LLMs such as Qwen2, LLaMA3.1, Mistral, and GPT-4o series follow the scaling law in NESTOOLS . The larger the size, the stronger its performance in tool calling, which is consistent with the previous research conclusions. However, it is worth mentioning that the Qwen1.5 series does not fully follow the scaling law. Compared with the initially released Qwen1.5 series, Qwen team released Qwen1.5-32B two months later. We speculate that 6 Figure 3: Model scaling results on NESTOOLS . this process may include more high-quality data and more optimized iterative training. 5.2 Nesting Depth Analysis Different call chains have different call depths. We consider the entire call chains nesting depth to be 1 if the instance contains no nested calls. For nested tool calling instances, each tool is at specific nesting depth. Tools that do not use the return value of previous tools are at depth 1. If tool uses the return value of previous tool, its depth is the previous tools depth plus one, and so on. For an instance, its overall nesting depth matches the deepest level among the tools. We analyze the performance of LLMs on NESTOOLS with different nesting depths. As shown in Figure 4, although the performance of the different LLMs varies, each of them shows tendency that the deeper the nesting depth, the worse the performance of LLMs. This undoubtedly illustrates the difficulty of nested tool calling and its importance for multiple tool calling. This trend can be attributed to several factors. Firstly, as the nesting depth increases, the complexity of the task grows exponentially. Each additional layer of nesting introduces new dependencies and potential points of failure, making it more challenging for the LLM to maintain coherence and accuracy throughout the entire call chain. Secondly, deeper nesting requires the model to retain and process more contextual information over longer sequences, which can lead to issues with memory and attention span. This is particularly relevant for LLMs that have fixed-length context windows, where the ability to handle long-range dependencies diminishes as the sequence length increases."
        },
        {
            "title": "5.3 Nesting Structure Analysis",
            "content": "As shown in Figure 5, we observe similar phenomenon with the nesting depth experiment, where Setting Selection Order Parameter Nested Param Avg. F1 F1 F1 F1 base w/o type w/o conjunction 65.7 71.6 68.5 44.2 50.7 47.2 44.2 51.0 47.3 40.8 42.7 41.7 51.2 62.7 73.6 67.7 40.4 51.4 45.3 42.4 52.6 46.9 35.7 43.1 49.7 41.1 53.3 46.4 33.2 43.8 37.8 47.8 41 59.8 73.4 65.9 35.7 48.3 39 Table 3: Robustness analysis conducted on LLaMA3.1-8B, base represents the results of the original setting, w/o type represents masking the type of response parameters of each tool, and w/o conjunction represents removing the conjunctions of the query. Figure 4: The relation between nesting depth and Selection F1 among LLMs. shallow tool calls result in higher average performance. However, the averaged performance would be higher if there is leading tool call (12,13) that is followed by two separate calls (13,23). Besides the long nesting call structure (123), the additional nesting connection (1 3) at the most right column obtain the lowest performance among all the structures, which shows the adverse effect of structure complexity."
        },
        {
            "title": "5.4 Robustness Analysis",
            "content": "Although existing benchmarks have comprehensively evaluated the tool calling ability of LLMs, there is lack of robustness analysis for tool calling based on these benchmarks. We intend to analyze the robustness of tool calls from both tool and query perspectives on NESTOOLS . From the query perspective, we notice that some conjunctions that do not affect the meaning often appear in the query of tool calls, such as \"Then, After that, Please\", etc. We guess whether the model works well because of the existence of shortcut. Considering that the rule-based screening cannot completely cover the connectives in the query, we intend to remove these additional words and reFigure 5: The averaged performance of different nesting structures. The arrow between two numbers indicates the nesting shape. For example, the first structure type (12,13) denotes that the 1st tool calls response contributes the input parameters for both the 2nd and the 3rd tool calls. store an original direct query. Considering that the rule-based screening cannot completely cover the connectives in the query, we intend to ask LLMs to remove these additional words. The prompt can be found in Appendix B.3. To ensure that this operation does not affect the core information of the query, we check some samples and dont find biases caused by this process. From the tool perspective, we would like to explore the impact of tool integrity on the experimental results when the tool is missing small part of the information, in order to keep consistency with the original evaluation process, and to ensure that the core information of the tool is complete, we only mask out the type of the return value of the tool to explore the impact of tool completeness on the experimental results. As shown in Table 3, In the case of masking off the type of the tool return value, LLaMA3.1-8B shows slight decrease in all metrics. However, In the case of removing the connectives in the query, the performance drops more significantly, espe7 Figure 6: Parameter error analysis for Qwen2-7B and LLaMA3.1-8B cially the Order metric, which proves that the conjunction does help LLMs understand the order of the call chain. An interesting observation is that after applying additional factors, the R-value metrics generally increase, indicating that LLM tends to call more tools to cover the gold standard answers. In summary, it can be inferred that the robustness of current LLMs in the field of tool calling is still insufficient and worth exploring."
        },
        {
            "title": "5.5 Error Analysis",
            "content": "We analyze the errors from LLMs during the evaluation process, unfolding both the tool and parameter perspectives."
        },
        {
            "title": "5.5.1 Tool Error",
            "content": "We summarize the errors in the tool perspective and divide them into five categories: tool selection, tool omission, tool redundancy, hallucinations, and generation format. Since these types of errors are usually mixed, it is hard to provide quantitative analysis. To this end, we provide each error category with case study in Appendix B.4. Here are our overall findings: (1) Tool selection: We find that similar tools may confuse the model, leading to errors in tool selection. (2) Tool Omission: Although the golden tools corresponding to each query are included in the evaluation prompt, the model may still miss some key tools. (3) Tool Redundancy: LLMs sometimes call redundant tools that are not task-related. (4) Hallucinations: When faced with some specific tasks, the model may hallucinate and construct tools on its own that do not exist in the tool pool. (5) Generation Format: During the evaluation process, some LLMs may generate results in the wrong format, which cannot be parsed for evaluations. Corresponding results are presented in Table 8, where we find smaller models are pruned to generate contents in the wrong formats. 5.5.2 Parameter error For errors in the parameter section, we analyze the error types from two perspectives: (1)Nonnested parameter errors include parameter omissions (Omission), redundant parameters (Redundancy), the parameter value is correct but the type is incorrect (Type), the parameter value is wrongly extracted (Extraction), and other errors (Others). (2)Nested parameter errors include non-nested parameter mistaken for nested (Hallucination), nested parameter not involved (Omission), nested parameter recognized as non-nested parameter (Identification), nested parameter identified but take an incorrect value (Confusion). As shown in Figure 6, we count the proportion of different error categories of Qwen2-7B and LLaMA3.1-8B. For non-nested parameters, LLaMA3.1-8B tends to mispredict parameter types, while Qwen2-7B makes fewer mistakes. For nested parameters, the most common mistake made by Qwen2-7B is recognizing nested parameters as nonnested parameters. The errors made by LLaMA3.18B are relatively average, and they are more likely to fill in the wrong nested parameter values after identifying them. This shows that LLaMA3.1-8B is more capable of identifying nested tool calling than Qwen2-7B to some extent."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce NESTOOLS , largescale dataset of high-quality for evaluating the LLMs nested tool learning abilities. We propose novel automatic method to generate cases involving nested tool calls. Through multi-step process of manual review and refinement, we ensure the quality of NESTOOLS . Statistical analysis reveals that our dataset spans wide range of domains. Furthermore, experiments conducted on 22 LLMs demonstrate the significance of this task. Our findings indicate that LLMs still struggle with effectively handling nested tool calls. We hope that 8 NESTOOLS , combined with our detailed experiments, can provide valuable insights and inspire future in-depth exploration of practical applications in tool agents."
        },
        {
            "title": "Limitations",
            "content": "This study mainly builds complex nested tool calling instances, providing new perspective on nested tool calling. Although promising, in reality, there may be situations where the necessary preceding tools are missing, and the model needs to use its own capabilities to perform additional processing on the return value. Another limitation is that the tools involved in this study cannot be executed. Although these tools are not specific implementations, they can be used as simulation representations to accurately describe the functions of the tools, which can be used to evaluate the capabilities of LLMs in tool learning and provide guidance for the future development of actual tools. Later, we will try to design real execution interfaces for these tools to better simulate nested tool calls in real situations."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609. Kinjal Basu, Ibrahim Abdelaziz, Subhajit Chaudhury, Soham Dan, Maxwell Crouse, Asim Munawar, Sadhana Kumaravel, Vinod Muthusamy, Pavan Kapanipathi, and Luis Lastras. 2024. Api-blend: comprehensive corpora for training and benchmarking api llms. arXiv preprint arXiv:2402.15491. Zehui Chen, Weihua Du, Wenwei Zhang, Kuikun Liu, Jiangning Liu, Miao Zheng, Jingming Zhuo, Songyang Zhang, Dahua Lin, Kai Chen, et al. 2024. T-eval: Evaluating the tool utilization capability of large language models step by step. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 95109529. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Shijue Huang, Wanjun Zhong, Jianqiao Lu, Qi Zhu, Jiahui Gao, Weiwen Liu, Yutai Hou, Xingshan Zeng, Yasheng Wang, Lifeng Shang, et al. 2024. Planning, creation, usage: Benchmarking llms for comprehensive tool utilization in real-world complex scenarios. arXiv preprint arXiv:2401.17167. Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil Zhenqiang Gong, et al. 2023. Metatool benchmark for large language models: Deciding whether to use tools and which to use. arXiv preprint arXiv:2310.03128. Tatsuro Inaba, Hirokazu Kiyomaru, Fei Cheng, and Sadao Kurohashi. 2023. Multitool-cot: Gpt-3 can use multiple external tools with chain of thought prompting. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 15221532. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. arXiv preprint arXiv:2401.04088. Mixtral of experts. Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023. Api-bank: comprehensive benchmark for tool-augmented llms. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 31023116. Yubo Ma, Zhibin Gou, Junheng Hao, Ruochen Xu, Shuohang Wang, Liangming Pan, Yujiu Yang, Yixin Sciagent: ToolCao, and Aixin Sun. 2024. augmented language models for scientific reasoning. arXiv preprint arXiv:2402.11451. Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio Ribeiro. 2023. Art: Automatic multistep reasoning and tool-use for large language models. arXiv preprint arXiv:2303.09014. Shishir Patil, Tianjun Zhang, Xin Wang, and Joseph Gonzalez. 2023. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334. Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, 9 Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. 2023a. Tool learning with foundation models. Preprint, arXiv:2304.08354. Tong Zhu, Xiaoye Qu, Daize Dong, Jiacheng Ruan, Jingqi Tong, Conghui He, and Yu Cheng. 2024. Llama-moe: Building mixture-of-experts from llama with continual pre-training. arXiv preprint arXiv:2406.16554. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023b. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789. Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-Rong Wen. 2024. Tool learning with large language models: survey. arXiv preprint arXiv:2405.17935. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2024. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36. Yongliang Shen, Kaitao Song, Xu Tan, Wenqi Zhang, Kan Ren, Siyu Yuan, Weiming Lu, Dongsheng Li, and Yueting Zhuang. 2023. Taskbench: Benchmarking large language models for task automation. arXiv preprint arXiv:2311.18760. Yifan Song, Weimin Xiong, Dawei Zhu, Wenhao Wu, Han Qian, Mingbo Song, Hailiang Huang, Cheng Li, Ke Wang, Rong Yao, et al. 2023. Restgpt: Connecting large language models with real-world restful apis. arXiv preprint arXiv:2306.06624. Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, and Le Sun. 2023. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. arXiv preprint arXiv:2306.05301. Mengsong Wu, Tong Zhu, Han Han, Chuanyuan Tan, Xiang Zhang, and Wenliang Chen. 2024. Sealtools: Self-instruct tool learning dataset for agent tuning and detailed benchmark. arXiv preprint arXiv:2405.08355."
        },
        {
            "title": "A Dataset Construction details",
            "content": "A.1 Tool/Instance Generation The corresponding prompt is presented in Table 4. An example for the generation process can be found in Table 5. A.2 Query Normalization The corresponding prompt is presented in Table 6. A.3 Query Refinement The corresponding prompt is presented in Table 7."
        },
        {
            "title": "B Evaluation details",
            "content": "B.1 Other Evaluation Metrics In addition to the four metrics of Selection, Order, Parameter, and Nested Param, we also consider the following metrics: Format measures the accuracy of LLMs output format. Tree measures the pass rate of the entire call tree of LLMs, requiring absolute correctness in the four previous metrics of Selection, Order, Parameter, and Nested Param to be considered pass."
        },
        {
            "title": "We provide the format accuracy and Tree pass",
            "content": "rate of all evaluated LLMs in Table 8. B.2 Evaluation Prompt The corresponding prompt is presented in Table 9. B.3 Prompt of removing conjunctions The corresponding prompt is presented in Table 10. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671. B.4 Error Examples B.4.1 Tool Selection Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629. Junjie Ye, Guanyu Li, Songyang Gao, Caishuang Huang, Yilong Wu, Sixian Li, Xiaoran Fan, Shihan Dou, Qi Zhang, Tao Gui, et al. 2024. Tooleyes: Finegrained evaluation for tool learning capabilities of large language models in real-world scenarios. arXiv preprint arXiv:2401.00741. The error examples for Tool Selection are shown in Table 11. B.4.2 Tool Omission The error examples for Tool Omission are shown in Table 12. B.4.3 Tool Redundancy The error examples for Tool Redundancy are shown in Table 13. 10 Tool/Instance Generation Please generate several realistic functions with definitions in the given specific domain and generate complete function calling statements in main(), shaped like \"data0,data1 = func0(*args)\" , \"data2,data3,data4 = func1(*args)\",etc. Do not add any additional python code in main(). The most important thing is that the former functions return value should be latter ones input argument, in other words, data1, data2, etc. should work as the arguments of subsequent function calls.In this scenario, the subsequent function will use return value of preceding function as its parameter, you need to ensure that the inner meaning of this parameter in both functions should be consistent, with identical parameter type, while slight difference in parameter names and descriptions are acceptable. All required parameters must be filled in, by directly writing the actual values in order. Optional parameters can be filled in or not filled in, but remember to add \"parameter_name=\" before the value of the optional parameter if you are sure to fill in this optional parameter. You need to write the task in natural language as comment under the function calling statements in main() to describe the entire calling process. Please ensure that it includes all the function calling values. Dont use pythons intrinsic functions in your generation, such as lists and dictionaries (index access, slicing, adding, deleting, etc.). The generated API only needs to be related to the given domain and does not need to be completely subordinate to the given domain. Here are some examples. {example} Its your turn to generate. Domain: {} Answer: Table 4: Prompt of Tool/Instance Generation. B.4.4 Hallucinations The error examples for Hallucinations are shown in Table 14. B.4.5 Generation Format The error examples for Generation Format are shown in Table 15. An example for Tool/Instance Generation Domain: Food and Beverage/Food product development ```python def get_top_selling_wine(location: str, date: str): \"\"\"Retrieve the top-selling wine in specific location on specific date. required parameters: location (str): the geographical location (country, region, city) date (str): the date of the sales in 'YYYY-MM-DD' format Returns: wine_name (str): the name of the top-selling wine sales_value (float): the sales value of the wine in dollars \"\"\" ... def get_product_origin(product: str): \"\"\"Retrieve the origin of specific product. required parameters: product (str): the name or code of the product Returns: origin (str): the geographical origin of the product \"\"\" ... def get_export_quantity(location: str, date: str, product: str): \"\"\"Retrieve the export quantity of specific product from specific location on specific date. required parameters: location (str): the geographical location (country, region, city) date (str): the date of the export in 'YYYY-MM-DD' format product (str): the name or code of the product Returns: quantity (int): the export quantity of the product \"\"\" ... def main(): data0, data1 = get_top_selling_wine(\"California\", \"2023-10-08\") data2 = get_product_origin(data0) data3 = get_export_quantity(data2, \"2023-10-08\", data0) # would like to know which wine sold the best in California on October 8th, 2023. Could you provide me with the origin of this wine and inform me about the export data of this wine from the local area on that day? if __name__ == \"__main__\": main() ``` Table 5: An example for Tool/Instance generation process. 12 Query Normalization Below are some APIs, task, and API call result for this task. You need to check if any values of the API call result that are not present in the task, incorporate those values into the original task without mentioning the specific APIs, rewrite the original task to ensure that it includes all the values,which is very important.If the value is in the format of complex data type (such as dictionary, list, tuple, etc.), you need to incorporate each item from the value into the task in natural language, rather than directly adding the value in the raw format to the task. The instructions of the rewritten task do not need to be completely consistent with the information in the given API such as the name and despcription, and some uniqueness of the real-life scenario should be retained. You only need to generate the rewritten task without adding any additional explanations. Here is an example. {example} Its your turn to generate. APIs: {gold apis} Task: {task} API call result: {call chain with gold apis} rewritten task: Table 6: Prompt of query normalization. 13 Query Refinement You are an excellent task rewriter. Next, will give you task query, and ask you to rewrite it in high quality. Please follow the tips below. Tips: 1. Please make sure that the actual name of the API(e.g., get_article_headline) involved in the call does not appear in the rewritten task. 2. The API call result represents the golden tool calling process corresponding to the query. Parameter value in the \"API_call_\" format represents return value from previous tool. Refer to the provided APIs information to ensure that the rewritten task is consistent with the golden tool calling process. 3. Please make sure that all regular parameters in the gold tool calling process, which are not in the \"API_call_\" format, can be extracted or inferred from the rewritten task. 4. Without changing the core meaning and logic of the original task, and while adhering to the call chain, appropriately modify the form of expression, make the task more coherent and aligned with real-world scenarios. Heres an example: {example} Its your turn to generate. APIs: {gold apis} Task: {original task} API call result: {call chain with gold apis} rewritten task: Table 7: Prompt of query refinement. Model Format Tree GPT-4o GPT-4o-mini GPT-3.5 Claude-3.5 LLaMA3.1-8B LLaMA3.1-70B LLaMA3.1-405B Mistral-7B v0.2 Mixtral-8x7B Mixtral-8x22B Qwen2-0.5B Qwen2-1.8B Qwen2-7B Qwen2-57B Qwen2-72B Qwen1.5-0.5B Qwen1.5-1.8B Qwen1.5-7B Qwen1.5-14B Qwen1.5-32B Qwen1.5-72B Qwen1.5-110B API-Based Models 100.0 94.3 99.4 100.0 Open-Sourced Models 98.0 99.1 97.5 68.7 91.3 99.5 1.2 66.5 94.2 97.5 98.6 3.3 12.6 59.5 85.4 85.7 86.1 88.8 25.9 18.3 13.5 26.8 6.9 9.7 14.7 2.9 7.4 17.5 0.0 0.0 4.6 9.2 20.3 0.0 0.0 2.4 8.0 10.7 7.8 13.0 Table 8: Format accuracy and Tree pass rate of NESTOOLS . Bold represents the best score among all models. 15 Prompt of Evaluation You have access to list of APIs and the task description.You need to follow the given task description and determine which API to call in sequence according to the order required by the task description. API can be retrieved from the APIs list. Finally, you only need to return the API call result without any other content. The final result should be in the format of [{\"api_name\":__,\"api_id\":__,\"parameters\":{\"arg0\":\"value0\", \"arg1\":\"value1\",...},\"responses\":{\"arg0\":\"API_call_0\", ... ,\"argn\":\"API_call_n\"}},{\"api_name\":__, \"api_id\":__,\"parameters\":{\"arg0\":\"value0\",\"arg1\":\"value1\",...},\"responses\":{\"arg0\":\"API_call_{n+1}\", ...}}, ...]. You dont need to know the actual return value of the API call, just assign each return value as string \"API_call_{number}\" in \"responses\", such as \"API_call_0\",\"API_call_1\",\"API_call_2\" and so on. The \"number\" in \"API_call_{number}\" should increase by one from 0 globally. Please first determine which APIs to call in sequence based on the task, and then determine the parameter values of each API depending on the specific details of the task. If you decide to call the API, you need to fill in all this APIs required parameters which can be found in this APIs \"required\" list. If you think the task does not include the actual value of necessary parameter in APIs \"required\" list, you can assign the necessary parameter value of \"UNK\". The remaining parameters are optional parameters, determine whether to fill them in according to the task. If you think the parameter value to be filled in is the return value of previous API call, set it as \"API_call_x\", then the parameter value can be filled in with \"API_call_x\". Now it is your turn to generate the API call result based on the APIs and task description below. Remember that you only need to generate the API call result, not any additional explanations. APIs: {tools} Task description: {task description} API call result: Table 9: Prompt of evaluation."
        },
        {
            "title": "Remove Conjunctions",
            "content": "Now there is task description. hope you can remove potential conjunctions like \"Please\", \"then\", \"subsequently\", \"After that\" etc., while maintaining the core meaning. Ensure that the text remains in natural language without punctuation errors and adjust the capitalization accordingly. Here is an example: {example} Now its your turn: Task description: {} Result: Table 10: Prompt of removing conjunctions. 16 Model Qwen2-7B Query Please provide me with the area, perimeter, and diagonal of rectangle with side lengths 5.0 and 8.0. LLaMA3.1-8B Please share the title of piece in the politics division that incorporates the word which is elections. Following this, extract information concerning the author who penned the aforementioned piece. Lastly, suggest 5 articles that bear similarity about elections. {\"side1\": Response [{\"api_name\": \"calculate_rectangle_perimeter\", \"parameters\": {\"length\": 5.0, \"width\": 8.0}, \"responses\": {\"perimeter\": \"API_call_0\"}}, {\"api_name\": \"calculate_diagonal\", \"parameters\": 5.0, \"side2\": 8.0}, \"responses\": {\"diagonal_length\": \"API_call_1\"}}, {\"api_name\": \"calculate_area\", \"parameters\": {\"length\": 5.0, \"width\": 8.0}, \"responses\": {\"area\": \"API_call_2\"}}] [{\"api_name\": \"request_article\", \"parameters\": {\"article_title\": \"piece in the politics division that incorporates the word which is elections\", \"author\": \"UNK\", \"access_link\": \"UNK\"}, \"responses\": {\"access_granted\": \"API_call_0\", \"download_link\": \"API_call_1\"}}, {\"api_name\": \"get_author_info\", \"re- \"parameters\": sponses\": \"articles_written\": \"API_call_3\"}}, {\"api_name\": \"recommend_similar_articles\", \"parameters\": {\"keyword\": \"elections\", \"limit\": \"5\"}, \"responses\": {\"similar_articles\": \"API_call_4\"}}] \"API_call_1\"}, \"API_call_2\", {\"author\": {\"birthplace\": Table 11: Error examples for tool selection. Model Query LLaMA3.1-70B Scan the ISBN \"978-3-16-148410-0\" of book to extract comprehensive data and verify its presence in the library. Upon identifying the book situated, launch an augmented reality interaction correlating to the book. Qwen2-7B would like to know which company is responsible for producing the Smartphone X10, and am also interested in understanding the comprehensive development value of the industrial zone it belongs to. \"scan_isbn\", \"978-3-16-148410-0\"}, \"API_call_0\", Response [{\"api_name\": \"parameters\": {\"isbn\": \"responses\": {\"book_details\": \"availability\": \"locate_book\", {\"api_name\": \"API_call_1\"}}, \"parameters\": {...}, \"responses\": [...]},{\"api_name\": \"engage_ar_experience\", \"parameters\": {\"availability\": \"API_call_1\", \"exact_location\": \"API_call_0\"}, \"API_call_2\", {\"ar_message\": \"responses\": \"ar_duration\": \"API_call_3\"}}] [{\"api_name\": \"get_product_manufacturer\", \"parameters\": {\"product_name\": \"Smartphone X10\"}, \"responses\": {\"manufacturer_name\": \"API_call_0\"}}, {\"api_name\": \"get_factory_location\", \"parameters\": \"eval- {...}, uate_property\", {\"location\": \"parameters\": \"API_call_0\", \"type_of_land\": \"industrial\", \"area\": 100.0}, \"responses\": {\"land_value\": \"API_call_1\", \"development_potential\": \"API_call_2\"}}] [...]},{\"api_name\": \"responses\": Table 12: Error examples for tool omission. 17 Model Query LLaMA3.1-8B Im planning metro communication route that needs to span about 50 km, and were targeting bandwidth capacity of 100 Gbps. Could you please provide modulation format that will work well over that distance. Also, theres the challenge of compensating for line loss of around 3.5 dB, so Ill need to fine-tune the settings accordingly. Once got the right settings, Could you tell me how to set up stable connection between A1 and B1? Qwen2-7B Im working on project called \"Alien Planet\", which is VFX scene aim to create. It should last about 120 seconds and want it to be in 4K resolution to ensure high quality. Id like to include SFX featuring alien creatures and spacecraft in mysterious atmosphere. Once everything is put together, need the scene to be in MP4 format. Could you guide me with the whole process? \"metro\", \"line_loss\": \"API_call_0\", {\"route_type\": 50, \"bandwidth\": {\"receiver_status\": 3.5}, \"API_call_2\", \"setup_receiver\", \"API_call_3\", Response \"choose_modulation_format\", [{\"api_name\": \"dis- \"parameters\": 100}, \"responses\": tance\": \"sys- {\"modulation_scheme\": {\"api_name\": \"API_call_1\"}}, tem_settings\": \"adjust_amplification\", \"parameters\": {\"format\": \"responses\": \"API_call_0\", \"amplifier\": {\"optimized_gain\": \"API_call_4\"}}, \"API_call_3\", \"signal_quality\": \"parameters\": {\"api_name\": \"link_budget\": {\"trans_config\": \"single-mode\"}, \"API_call_2\", \"channel_type\": \"responses\": \"API_call_5\", \"API_call_6\"}}, {\"api_name\": \"signal_quality\": \"establish_connection\", {\"amplifier_type\": \"API_call_3\", \"end_points\": \"A1-B1\"}, \"responses\": {\"connection_status\": \"API_call_7\", \"connection_details\": \"API_call_8\"}}] [{\"api_name\": \"parame- \"create_vfx_scene\", ters\": {\"scene_name\": \"Alien Planet\", \"duration\": 120, {}}, \"4K\"}, {\"api_name\": \"add_special_effects\", \"parameters\": \"path_to_Alien_Planet_scene\", {\"scene_path\": \"spacecraft\"], \"effects_list\": \"mood\": {}}, \"parameters\": {\"api_name\": \"space- {\"scene_objects\": [3840, craft\"], \"quality\": 2160]}, \"responses\": \"ex- {\"scene_path\": port_final_scene\", \"parameters\": \"path_to_rendered_Alien_Planet_scene\", \"format_type\": \"MOV\"}, \"responses\": {}}] [\"alien_creatures\", \"high\", \"resolution\": {}}, {\"api_name\": [\"alien_creatures\", \"render_scene\", \"mysterious\"}, \"parameters\": \"resolution\": \"responses\": \"responses\": Table 13: Error examples for tool redundancy. 18 Model Query LLaMA3.1-8B Please assist in designing an optimal prosthesis for 35-year-old male, who has an unremarkable medical past and has suffered fracture impacting his upper extremity. Afterward, evaluate the patients steady condition and anticipate complete recovery within timeframe of 6 months. Finally, arrange an orthopedic surgical procedure for the patient. Qwen2-7B Assess the caliber of the Python code in the file found at \"path/to/file.py\". Subsequently, refine the code using the tool PyLint. Finally, deploy the optimized code as web application to the production environment. \"parameters\": \"unremarkable\"}\", Response [{\"api_name\": \"design_prosthesis\", \"parameters\": {\"patient_info\": \"{\"age\": 35, \"gender\": \"male\", \"medical_history\": \"injury\": \"upper extremity\"}, \"fracture\", \"affected_area\": \"responses\": {\"prosthesis_details\": \"API_call_0\", \"API_call_1\"}}, {\"api_name\": \"estimated_cost\": \"patient_assessment\", {\"patient_condition\": \"stable\", \"recovery_expectations\": \"complete recovery within 6 months\"}, \"responses\": {\"assessment_report\": \"API_call_2\"}}, {\"api_name\": \"schedule_orthopedic_surgery\", \"parameters\": {\"prosthesis\": \"API_call_0\", \"patient_info\": \"{äge: 35, gender: male, medical_history: ünremarkable}\", \"surgery_type\": \"surgery_date\": \"orthopedic\", \"2024-03-16\"}, \"responses\": {\"surgery_schedule\": \"API_call_3\"}}] [{\"api_name\": \"analyze_code_quality\", \"parameters\": {\"file_path\": \"path/to/file.py\", \"language\": \"Python\"}, \"responses\": {\"issues_found\": [\"issue1\", \"issue2\"], \"complexity_score\": 4.5, \"maintainability_score\": 8.2}}, {\"api_name\": \"optimize_code\", \"parameters\": \"issue2\"], \"optimization_tool\": \"PyLint\"}, \"responses\": {\"op- \"optimized_code_snippet\"}}, timized_code\": {\"api_name\": \"parameters\": \"deploy_app\", {\"app_id\": \"optimized_code_snippet\", \"server_type\": \"Global\"}, \"responses\": \"Production\", \"region\": \"deployed_url\": true, {\"deployment_status\": \"https://optimized-code.example.com\"}}] {\"issues\": [\"issue1\", Table 14: Error examples for hallucinations. 19 Model LLaMA3.1405B Query Please check the safety status of car with registration number ABC123, last inspected on May 12, 2022. Issue class driving license to John Doe aged 30. Then book vehicle inspection appointment for truck with registration number DEF456 on June 20, 2022. Qwen2-7B Load the VR environment named \"Fantasy Land\" for user ID 12345. Obtain information about the users avatar named \"Wizard\". Explore the VR world within the \"Fantasy Land\" environment, starting at coordinates (10, 5, 3) and interact with other users present. {\"license_number\": {\"applicant_name\": 30, \"license_type\": \"DEF456\", \"vehicle\": \"issue_driving_license\", Response [{\"api_name\": \"check_vehicle_safety\", \"parameters\": {\"vehicle_type\": \"car\", \"registration_number\": \"ABC123\", \"inspection_date\": \"May 12, 2022\"}, \"API_call_0\", {\"safety_status\": \"responses\": \"API_call_1\"}}, \"next_inspection_date\": {\"api_name\": \"parameters\": \"John Doe\", \"class C\"}}, \"re- \"age\": sponses\": \"API_call_2\", \"expiration_date\": \"API_call_3\"}}, {\"api_name\": \"book_vehicle_inspection\", \"parameters\": {\"reg- \"truck\", istration_num\": \"inspection_date\": \"June 20, 2022\"}, \"responses\": {\"appointment_confirmation\": \"API_call_4\"}}] \"load_vr_environment\", \"param- [{\"api_name\": \"Fantasy Land\", eters\": {\"environment_name\": \"user_id\": {\"environment_type\": \"Fantasy\", \"user_preferences\": \"adventure\", \"avatar_skin_color\": \"blue\"}}, {\"api_name\": {\"user_ID\": \"get_avatar_info\", 12345, \"avatar_name\": \"Wizard\"}, \"responses\": {\"avatar_height\": 1.8, \"avatar_accessories\": \"pointed hat, staff\", \"avatar_bio\": \"A powerful wizard who specializes in elemental magic.\"}, {\"api_name\": {\"environ- \"explore_vr_world\", ment_type\": \"Fantasy\", \"user_coordinates\": [10, 5, 3], \"interact_with_others\": true}, \"responses\": {\"discovered_elements\": [\"ancient ruins\", \"enchanted forest\", \"dragons lair\"], \"interactive_users\": 5}}] \"parameters\": \"parameters\": \"responses\": 12345}, Table 15: Error examples for generation format."
        }
    ],
    "affiliations": [
        "Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, China"
    ]
}