{
    "paper_title": "AIDE: AI-Driven Exploration in the Space of Code",
    "authors": [
        "Zhengyao Jiang",
        "Dominik Schmidt",
        "Dhruv Srikanth",
        "Dixing Xu",
        "Ian Kaplan",
        "Deniss Jacenko",
        "Yuxiang Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Machine learning, the foundation of modern artificial intelligence, has driven innovations that have fundamentally transformed the world. Yet, behind advancements lies a complex and often tedious process requiring labor and compute intensive iteration and experimentation. Engineers and scientists developing machine learning models spend much of their time on trial-and-error tasks instead of conceptualizing innovative solutions or research hypotheses. To address this challenge, we introduce AI-Driven Exploration (AIDE), a machine learning engineering agent powered by large language models (LLMs). AIDE frames machine learning engineering as a code optimization problem, and formulates trial-and-error as a tree search in the space of potential solutions. By strategically reusing and refining promising solutions, AIDE effectively trades computational resources for enhanced performance, achieving state-of-the-art results on multiple machine learning engineering benchmarks, including our Kaggle evaluations, OpenAI MLE-Bench and METRs RE-Bench."
        },
        {
            "title": "Start",
            "content": "AIDE: AI-Driven Exploration in the Space of Code Zhengyao Jiang Weco AI Dominik Schmidt Runway ML Dhruv Srikanth Weco AI Dixing Xu Weco AI Ian Kaplan Weco AI Deniss Jacenko Weco AI Yuxiang Wu Weco AI"
        },
        {
            "title": "Abstract",
            "content": "Machine learning, the foundation of modern artificial intelligence, has driven innovations that have fundamentally transformed the world. Yet, behind advancements lies complex and often tedious process requiring labor and compute intensive iteration and experimentation. Engineers and scientists developing machine learning models spend much of their time on trial-and-error tasks instead of conceptualizing innovative solutions or research hypotheses. To address this challenge, we introduce AI-Driven Exploration (AIDE), machine learning engineering agent powered by large language models (LLMs). AIDE frames machine learning engineering as code optimization problem, and formulates trial-anderror as tree search in the space of potential solutions. By strategically reusing and refining promising solutions, AIDE effectively trades computational resources for enhanced performance, achieving state-of-the-art results on multiple machine learning engineering benchmarks, including our Kaggle evaluations, OpenAIs MLE-Bench and METRs RE-Bench. The implementation of AIDE is publicly available at https://github.com/WecoAI/aideml."
        },
        {
            "title": "Introduction",
            "content": "Machine learning engineering supports many modern AI achievements, from basic regression on tabular data to the recent surge in large generative models. However, building high-performance machine learning model is always time consuming. Due to the inherent stochasticity of both the data and the optimization process, engineers and scientists rely heavily on trial-and-error. Researchers have long sought to automate these iterative processes, leading to advancements in fields like AutoML (Feurer et al., 2015, 2020; LeDell and Poirier, 2020a; Olson and Moore, 2016; Jin et al., 2023, 2019; Thornton et al., 2013a,b; Mueller and et al., 2024), Neural Architecture Search (Zoph and Le, 2017; Pham et al., 2018; Liu et al., 2019; Real et al., 2019; Elsken et al., 2019), and hyperparameter optimization (Falkner et al., 2018; Yang and Shami, 2020). These methods typically require predefined search space of configurations, such as hyperparameters and network architectures, within which the algorithm explores potential solutions (Elsken et al., 2019; Yang and Shami, 2020; White et al., 2023). Defining this space often requires significant domain expertise. Furthermore, search algorithms for hyperparameter tuning are often somewhat brute force compared to human experts, resulting in lower compute efficiency and risk of overfitting to the validation set. The emergence of advanced coding capabilities in large language models (LLMs) (OpenAI, 2023; Jimenez et al., 2024; Anthropic, 2024; Google, 2024; Jain et al., 2025; OpenAI, 2025a,b) has introduced an exciting new possibility: searching directly within the space of code rather than the space of predefined configurations. Code-space optimization offers greater flexibility and leverages *Equal contribution. Work done while at Weco AI. Corresponding to zhengyao@weco.ai and yuxiang@weco.ai. 5 2 0 2 8 1 ] A . [ 1 8 3 1 3 1 . 2 0 5 2 : r the extensive domain-specific knowledge inherent in LLMs, effectively narrowing the search to more promising solutions and thus boosting sample efficiency. This gives it the potential to address compute-bound tasks like deep learning or, presumably, even optimizing LLMs themselves. Here, we introduce AI-Driven Exploration (AIDE), LLM-powered agent* that automates the trialand-error process of machine learning engineering. Unlike the ReACT (Yao et al., 2023) style agent, which appends historical observations to the LLMs context and relies on the models capabilities to solve monolithic optimization problem, AIDE organizes all historical solutions in tree structure. It then asks the LLM to propose improvements based on individual tree nodes. hard-coded treesearch algorithm accumulates these incremental improvements, guided by automated evaluations. We benchmarked AIDE on set of Kaggle tasks focusing on tabular machine learning and released these initial results together in April 2024 (Weco AI, 2024). Subsequently, OpenAI released MLEBench (Chan et al., 2024), further showing that AIDE can be applied to even more challenging deep learning tasks from Kaggle while achieving state-of-the-art performance. Most notably, AIDE achieves twice the number of medals compared to follow-up agent (Wang et al., 2024) when both use GPT-4o; with o1-preview, the gap widens even further. In parallel, METR assessed AIDE on AI research tasks against human experts under time constraints, showing that AIDE can outperform expert-crafted solutions in limited time windows (METR, 2024). Moreover, for tasks with robust evaluation signal like Triton Kernel optimization, AIDEs final solution surpasses that of human experts, even when the latter had extended development time. The first half of this paper provides formal specification of AIDE for the research community. In the second half, we present and analyze empirical evaluations of AIDE, drawing on both our own experiments and independent benchmark results."
        },
        {
            "title": "2 Preliminaries",
            "content": "Many general-purpose LLM agents, including ReACT (Yao et al., 2023), frame their tasks as Partially Observable Markov Decision Processes (POMDPs) (Kaelbling et al., 1998), widely used framework in reinforcement learning. In POMDP, the agent tries to maximize cumulative reward by choosing actions based on all past observations, essentially treating the entire interaction history as the state. While this approach is flexible and unifies range of tasks, it lacks principled way to break down the problem when there is clear structure available. Moreover, for LLM-based agents, continually appending all historical data can lead to oversized prompts and limit scalability, because the models context window eventually fills up. In this work, we adopt an alternative framework for LLM-driven iterative problem solving by modeling the task as an optimization problem: Let be space of possible solutions (e.g., Python scripts), and let : be stateless objective function (for example, validation accuracy or loss). The goal is to find an optimal solution: = arg max sS h(s). (1) Each candidate solution can be evaluated independently via an objective function h(s). This perspective simplifies the problem considerably: rather than unrolling single, long-horizon decision process , we can directly evaluate and compare solutions. It also aligns naturally with existing optimization methods, like tree search, which depend on standalone evaluations of candidate solutions."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we introduce our approach to automating machine learning engineering with AIDE. By employing the tree search method, AIDE systematically explores solutions that optimize validation metrics, breaking down the monolithic optimization task into atomic improvement steps. We begin by outlining the high-level optimization algorithm. And then delve into key implementation details, such as the search policy and specialized prompts that drive the iterative generation and refinement of machine learning code. *In this paper, Agent may refer either to the algorithm built on top of LLMs or to the entire system with LLMs included, depending on the context. 2 Algorithm 1 AI-Driven Exploration (AIDE) 1: Initialize: solution tree T0 2: Initialize: base solution s0 3: for = 1, 2, ..., do sn (cid:0)s, Σ(Tn1)(cid:1) 4: vn h(sn) 5: Tn Tn1 {node (sn, vn), edge (s sn)} 6: π(Tn) 7: 8: end for 9: return argmaxs{s0,...,sN } h(s) Propose new solution Evaluate the solution Record node and its score Select the next base node Best solution found 3.1 AI-Driven Exploration in the Space of Solutions In AIDE, solution is the code to be optimized, with s0 denoting the empty root solution. An evaluator, : R, evaluates the code and provides scalar score. All discovered solutions are stored in solution tree, , whose nodes correspond to scripts and edges represent an improvement attempt (e.g., is an improvement of s). search policy, π(T ), selects which solution will serve as the base solution to be improved. To keep language model prompts concise while being aware of the historical attempts, summarization operator, Σ(T ), extracts relevant information from the tree, such as the high level idea of each improvement attempt and its corresponding performance metrics. Finally, coding operator, (cid:0)s, Σ(T )(cid:1), proposes new scripts by drafting an initial version from s0, fixing bugs, or refining promising solution based on the summarized context. With these components in place, AIDE can systematically explore the code solution space, as shown in Algorithm 1. 3.2 AIDE for Machine Learning Here we present more implementation details of AIDE for machine learning engineering, providing concrete instantiation of the core components from Section 3.1. In particular, we build upon the following design elements: Search Policy (π). In AIDE, the search policy π ( algorithm 1, line 7) follows simple hard-coded rule, determining whether to draft, debug, or improve based on an existing solution. Specifically, it selects: Drafting if we have not yet reached the desired number of initial solutions. Debugging if buggy node remains within certain debug depth. Improving otherwise, typically targeting the best (non-buggy) solution. This policy imposes practical heuristics, such as 1) first exploring set of diverse initial solutions and continuously improving the best one, and 2) constraining the number of debug attempts for broken solution. Coding Operator (f ). The coding operator has three main entry points, each with its own specialized prompts: Drafting, which is invoked when we need completely new solution from scratch. It prompts an LLM to outline brief plan for model (e.g., specifying particular network architecture or feature-engineering idea), then emits single-file Python program implementing that plan. Debugging, which focuses on repairing buggy solutions. By inspecting error logs and execution traces, it attempts to rectify issues in the code like broken imports, incorrect tensor dimensions, or other coding errors while preserving the overall approach. Improving, which is called when valid, non-buggy solution already exists but could benefit from data preprocessing, architectural or optimization modifications. Here, the LLM 3 s0 : draft : draft : draft s1 s2 : fix : fix : improve s4 s5 s6 s3 : fix s"
        },
        {
            "title": "Optimal Solution",
            "content": "Figure 1: sample solution tree for AIDE, where each node is Python script. Arrows represent transitions proposed by the coding operator . Some branches terminate in bug, while others lead to improved or optimal solutions. proposes exactly one atomic change, such as switching optimizers or adding regularization technique, so that its effect on performance is directly measurable. Combining these three operations keeps the solution tree structured and ensures that each new node arises from well-defined modification of parent node. Summarization Operator (Σ(T )). Despite the flexibility to generate arbitrarily large numbers of solutions, we avoid saturating the LLMs prompt by applying context summarization operator, Σ(T ). Instead of appending all historical logs, Σ(T ) selectively extracts: Performance metrics (e.g., accuracy, AUC-ROC, test set loss). Hyperparameter settings if solution involves hyperparameter sweep. Relevant hints for debugging (e.g., misaligned array shapes in tracebacks). concise summary is crucial to maintaining stateless perspective: each code revision stands on its own, but Σ(T ) uses prior information to guide subsequent proposals. This design offers much of the benefit of incremental reasoning without exploding the prompt size. In addition to dynamic updates from Σ(T ), AIDE for machine Data Preview in Coding Prompts. learning includes small static data preview in each prompt, giving the LLM basic knowledge of dataset size or feature layouts. In practice, we store relevant metadata (e.g., number of rows, column names, or data splits) in the workspace and insert it into the coding operators prompt. Although not complete EDA pipeline, this lightweight approach helps AIDE guide key code decisions. These decisions include selecting validation split or scaling hyperparameters, without repeatedly including extensive dataset context. Putting It All Together. Figure 1 illustrates how AIDEs instantiation for machine learning uses (i) search policy π to select which solution to refine next, (ii) coding operator for generating code by drafting, debugging, or improving solutions, and (iii) summarization operator Σ(T ) to keep the LLM prompts concise and targeted. By combining these components under stateless optimization framework, AIDE can systematically search within the space of possible code solutions for machine learning tasks, avoiding an ever-increasing prompt history while retaining the relevant knowledge needed to achieve high performance."
        },
        {
            "title": "4 Evaluation",
            "content": "In this section we report empirical evaluations of AIDE. We did our own evaluation on Kaggle competitions with focus on tabular machine learning tasks (Weco AI, 2024). On the other hand, after the open sourcing of the AIDE in April 2024, the community has done larger scale independent 4 evaluations showing promising results on deep learning (Chan et al., 2024) and AI R&D (METR, 2024) tasks. We therefore also aggregate relevant results here to provide better understanding of the AIDEs performance. Readers interested in the extended evaluations are encouraged to read and cite the papers from OpenAI (Chan et al., 2024) and METR (2024) respectively. 4.1 Weco Kaggle Benchmark We curated diverse set of Kaggle competitions to build Wecos internal Kaggle benchmark, called Weco-Kaggle, for evaluating AIDEs performance in machine learning. This set consists of 63 competitions of varied complexity and data size, spanning domains such as tabular machine learning, image classification, and time-series prediction. Some of these competitions require GPU to solve. Full details of the competitions in Weco-Kaggle are provided in Appendix C. From Weco-Kaggle, we selected subset of 16 tabular machine learning tasks with relatively lower complexity and primarily CPU-based runtime requirements. This subset, referred to as Weco-Kaggle Lite, is shown in Table 2. Evaluation Protocol. We evaluate the performance of AIDE by comparing its results to that of human competitors in each Kaggle competition, and averaging across competitions. We follow the evaluation protocol below to evaluate AIDEs and other frameworks performance: 1. Before running the agent on competition, we split the competitions training data into an agent train set and holdout test set. This split is defined manually for each competition following similar parameters as Kaggles official private test set (e.g. similar train-test percentages), but is not necessarily the same, since Kaggles test set is not released publicly for most competitions. Note that our holdout test set is also distinct from the train-validation split that AIDE itself generates as part of its internal node evaluation protocol. 2. During code generation, AIDE is given access to the holdout test inputs (but not labels) and prompted to evaluate its model on this data. In particular, we prompt AIDE to generate submission.csv file, analogously to how human competitors submit their competition results. 3. We define an Exceeds % of Human metric as 100(1 q), where is the quantile of AIDEs score on the official Kaggle leaderboard. This metric represents the percentage of human competitors whose performance AIDE surpasses. Whenever possible, we use Kaggles private leaderboard because it is less prone to overfitting by competitors; if private leaderboard is unavailable, we default to the public leaderboard. In addition, we report the Above Median metric, originally proposed by Chan et al. (2024), which indicates how frequently AIDE outperforms the median Kaggler performance across competitions. 4. This metric is then averaged across all competitions. We chose our evaluation protocol based on leaderboard-quantiles since, unlike each competitions included metric, these scores are similarly distributed between competitions, making it possible to simply average across competitions to obtain aggregated scores. Leaderboard quantiles are also more fine-grained, allowing us to evaluate, for example, the performance of single run on single task, unlike medal-counts (Chan et al., 2024) which collapse to binary metric in this case. Finally, our scores are interpretable and useful in assessing AIDEs performance relative to humans. Agent AIDE AutoML (H2O) AutoGPT (Langchain) GPT-4 Turbo Human with ChatGPT GPT-4 Turbo Model GPT-4 Turbo N/A Exceeds % of humans Above Median (%) 51.38 35.34 32.34 41.17 50.00 18.75 0.00 18.75 Table 1: Comparing AIDE to other agent frameworks on 16 tabular machine learning tasks from Kaggle. Exceeds % of humans indicates the percentage of human Kaggle participants being outperformed by the agents, averaged across the competitions. Above Median (%) is the fraction of competitions where the score was strictly above the median of human Kaggle participants. Baselines. To evaluate AIDEs effectiveness, we compare it against three baselines that illustrate different approaches to automated or assisted machine learning: 1. Conventional H2O AutoML. We select H2O, one of the leading AutoML platforms, to exemplify traditional AutoML tools. In each competition, the data is split into an 80%/20% train/validation set, and model selection is performed within 600-second search window. 2. AutoGPT. workflow automation framework that surged in popularity in early 2024. It generates plan and automatically executes the necessary steps to complete task. We adapt its task descriptor to produce solutions for our competitions. 3. Human Assisted with ChatGPT. An increasingly common scenario involves human engineers leveraging ChatGPT to assist with coding tasks. We adopt this baseline to understand how AIDE performs relative to human engineer directing ChatGPT to develop solutions. These baselines collectively provide robust comparative foundation for evaluating AIDE against both traditional AutoML workflows and modern LLM-assisted strategies. Further details about the baselines configuration can be found in Appendix A. Competition playground-series-s3e14 playground-series-s3e16 playground-series-s3e19 playground-series-s3e22 playground-series-s3e24 playground-series-s3e25 tabular-playground-series-aug-2022 tabular-playground-series-feb-2021 tabular-playground-series-feb-2022 tabular-playground-series-jan-2022 tabular-playground-series-jul-2021 tmdb-box-office-prediction bike-sharing-demand cat-in-the-dat house-prices-advanced-regression-techniques new-york-city-taxi-fare-prediction Average Total Teams AIDE Rank Exceeds % of Human Above Median 1877 1431 1174 1543 1910 1633 1889 1434 1257 1592 1294 1395 3243 1341 4978 1485 897 693 742 1142 655 948 392 559 708 886 1126 692 262 714 1357 819 52.21% 51.57% 36.80% 25.99% 65.71% 41.95% 79.25% 61.02% 43.68% 44.35% 12.98% 50.39% 91.92% 46.76% 72.74% 44.85% 51.38% False False True True False True False False True True True False False True False True 50.00% Table 2: AIDE vs. human performance comparison on Weco-Kaggle Lite. The submissions were made manually in February 2024. All rankings are actual rankings on the private/public Kaggle leaderboard, assessed in February 2024. AIDEs Results on Weco-Kaggle Lite. Table 1 compares AIDE against multiple baselines, including H2O AutoML, AutoGPT, and human competitor utilizing ChatGPT, averaged over the 16 tabular Kaggle tasks of Weco-Kaggle Lite. AIDE achieves an Exceeds % of humans score of 51.38%, outperforming half of the Kaggle participants on average, and surpasses the human median in 50% of these tasks. By contrast, H2O AutoML and LangChain AutoGPT attain lower Exceeds % of humans scores (35.34% and 32.34%, respectively). Table 2 offers detailed breakdown for each competition, indicating that AIDEs performance ranges from surpassing roughly 13% of human participants (for more challenging tasks) to nearly 92% (for tasks it handles more effectively). Across half of the competitions, AIDE ranks above the human median, underscoring its robustness in consistently delivering competitive results against diverse set of real-world machine learning challenges. AIDEs Results on Full Weco-Kaggle. Figure 2 illustrates AIDEs performance distribution across our extended set of Kaggle competitions, sorted by its Exceeds % of Humans value. Notably, AIDE achieves near-top-tier performance on several tasks, surpassing the vast majority of human participants, while on other tasks it exceeds only small fraction. Overall, the average Exceeds % of Humans rate is 48.23%, and AIDE outperforms the human median in 49.21% of the competitions. These results underscore that AIDE can be highly competitive in certain domains, yet there remains variability in its performance depending on the dataset and task requirements. 6 Figure 2: AIDEs performance distribution on full Weco-Kaggle benchmark. Exceeds % of Humans values are estimated from the leaderboard distribution. Model Agent o1-preview AIDE GPT-4o AIDE Llama 3.1 AIDE Claude 3.5 AIDE MLAB GPT-4o OpenHands GPT-4o Valid Subm. (%) Above Median (%) Gold (%) Any Medal (%) 82.8 1.1 54.9 1.0 27.3 2.6 51.1 3.3 44.3 2.6 52.0 3.3 29.4 1.3 14.4 0.7 6.7 1.4 12.9 2.2 1.9 0.7 7.1 1.7 9.4 0.8 5.0 0.4 1.7 0.7 4.4 1.4 0.8 0.5 2.7 1.1 16.9 1.1 8.7 0.5 3.0 1.0 7.6 1.8 0.8 0.5 4.4 1.4 Table 3: Full MLE-Bench results (pass@1) reported by (Chan et al., 2024) comparing AIDE to other agent frameworks. Valid Subm. (%) is the fraction of all competitions (not just those with submission) where the submission passed validity checks. Above Median (%) is the fraction of competitions where the score was strictly above the median of human Kaggle participants. Any Medal (%) is the fraction awarded bronze, silver, or gold medal (the primary success metric). Each experiment is repeated with 3 seeds, except for AIDE+o1-preview and AIDE+GPT-4o, which use 16 and 36 seeds respectively. Scores represent the mean one standard error of the mean. Potential Limitations. Despite the advantages discussed above, our protocol has some limitations. First, because our test set may differ from Kaggles private test set, scores may not always be directly comparable, which can result in variance in percentiles. Second, there is risk of contamination since some of the language models used in this work may have been trained on competition-related data. Although we found no significant correlation between agent performance and competition recency, the only way to fully ensure no data contamination would be to submit the agents solutions to live competitions. 4.2 AIDE in MLE-Bench MLE-Bench (Chan et al., 2024) is an offline evaluation framework comprising 75 real Kaggle competitions. Here, we present the results related to AIDE reported by Chan et al. (2024) and encourage readers to check and cite the original paper if they are interested in the results presented here. In these evaluations, AIDE emerged as the top-performing agent framework when paired with state-ofthe-art large language models. Other agent frameworks such as ResearchAgent from MLAB (Huang et al., 2024) and OpenHands (Wang et al., 2024) tended to terminate early or struggle with iterative refinement. AIDEs optimization-centric approach led to better scalability in terms of trial-and-error interactions, therefore higher valid-submission rates and ultimately more competition medals. Table 3 highlights key results of AIDE compared to other agents. The reported Any Medal (%) column shows the fraction of competitions on which the agent and model combination achieved medal (bronze, silver, or gold) in single pass (i.e. pass@1). AIDE with o1-preview earned medals in 16.9% of competitions, nearly four times that of the follow-up agent OpenHands. 7 Figure 3: Performance of o1-preview with and without AIDE on MLE-bench Lite (complexity=low) set. AIDEs Key Advantages. By explicitly implementing solution tree search strategy, AIDE keeps node-level code concise and focuses each language model call on localized problem (e.g. debugging only the most promising script). This design helps avoid oversized prompts, preserves clear performance record for each node, and repeatedly refines partial solutions over the entire 24-hour timeframe. Consequently, AIDE systematically addresses coding bugs and suboptimal hyperparameters rather than abandoning failed solutions. As shown in Table 3, these iterative improvements translate into higher medal acquisition rates in comparison to generic agents. Moreover, when given additional attempts per competition (increasing in pass@k), AIDE significantly increases its success rate; for instance, GPT-4o and o1-preview nearly double their medals from pass@1 to pass@6 (Chan et al., 2024). These observations underscore the specialized nature of AIDE, which often outperforms other agents through persistent, Kaggle-style iteration, highlighting the efficacy of competition-targeted design in real-world ML tasks. The impact of AIDE becomes particularly evident when comparing performance on the MLE-bench Lite subset, as shown in Figure 3. Using o1-preview with AIDE significantly improved performance across all metrics compared to using o1-preview alone. The valid submission rate increased from 63.6% 4.5% to 92.4% 2.6%, demonstrating AIDEs effectiveness in guiding the model through the submission process. More importantly, the fraction of solutions scoring above the median human performance increased dramatically from 13.6% to 59.1% 4.5%, and both medal-related metrics showed substantial improvements: the gold medal achievement rate more than tripled from 6.1% 2.6% to 21.2% 6.9%, while the overall medal achievement rate increased nearly fivefold from 7.6% 2.6% to 36.4% 7.9%. These improvements are statistically significant (p < 0.01 for all metrics, two-tailed t-test). The dramatic performance gains across all metrics demonstrate that AIDEs iterative optimization approach substantially enhances the models problem-solving capabilities, enabling more reliable and higher-quality solutions through systematic refinement. 4.3 AIDE in RE-Bench While AIDE is designed for building machine learning pipelines, METR applied it to much more challenging AI R&D tasks by formulating these tasks into optimization tasks. The tasks range from optimizing Triton Kernel to finetuning GPT-2 for QA. Surprisingly, AIDE performs quite well on these tasks, and is even comparable with the top human AI scientists from Google DeepMind, Google, Anthropic, OpenAI, FAR Labs, Redwood Research, University of California Berkeley, Carnegie Mellon University, Stanford University, or Massachusetts Institute of Technology (METR, 2024). 8 Figure 4: Average score achieved by AIDE+o1-preview and top human scientists on 7 AI R&D tasks, as report by METR (2024). AIDE managed to surpass human scientists within six hours by enabling faster experiment iterations. However, human scientists eventually caught up, as AIDE adopts simple greedy policy that may lead to local optima on challenging R&D tasks. Figure 4 illustrates AIDEs average performance over time across the seven RE-Bench environments. Since LLMs can implement solutions much faster, allowing for more iteration cycles, AIDE managed to outperform humans within the six-hour time limit. Notably, the agent exceeded human performance in Optimize Kernel, discovering custom Triton-based solution faster than any of the nine human experts did within 64 hours. However, AIDE fell short in environments that required handling larger codebases or where single improvement involved multiple steps of interaction. For example, in Agent for Rust CodeContests, AIDE was prone to repeating local patches instead of discovering new strategies."
        },
        {
            "title": "5 Related Work",
            "content": "5.1 LLM Agents Recent advances in large language models have spurred the development of agents that combine natural language reasoning with task execution. General-purpose agents such as ReAct (Yao et al., 2023) and HuggingGPT (Shen et al., 2023) interleave planning with action selection to perform tasks ranging from information retrieval to multi-modal processing. In contrast, specialized agents like Voyager (Wang et al., 2023) and AlphaCode (Li and et al., 2022) are tailored to specific domains such as embodied reasoning and competitive code generation. These systems integrate execution feedback into the LLMs reasoning process, enabling iterative refinement of candidate solutions. 5.2 Automated Machine Learning Automated Machine Learning (AutoML) aims to eliminate manual intervention in model selection, hyperparameter tuning, and pipeline configuration. Early frameworks such as Auto-WEKA (Thornton et al., 2013b) and TPOT (Olson and Moore, 2016) employed Bayesian optimization and genetic programming, respectively, to search over predefined model spaces. Later systems like AutoSklearn (Feurer et al., 2020) and AutoGluon (Mueller and et al., 2024) have leveraged meta-learning and ensemble techniques to further improve performance. Despite their success, many conventional AutoML methods rely on static search spaces and lack the dynamic adaptability required for more complex problem settings. 9 5.3 Neural Architecture Search Neural Architecture Search (NAS) focuses on automatically designing neural network topologies. Initial methods based on reinforcement learning (Zoph and Le, 2017) and evolutionary strategies (Real et al., 2019) demonstrated that automated search could yield competitive architectures. Differentiable approaches such as DARTS (Liu et al., 2019) have reduced the computational cost by enabling gradient-based optimization over relaxed search space. However, NAS still faces challenges in computational expense and search space design. AIDE, on the other hand, avoids such problems above with code space search and efficient design exploration powered by LLMs."
        },
        {
            "title": "6 Conclusion",
            "content": "In conclusion, we have presented AI-Driven Exploration (AIDE), an LLM Agent for machine learning engineering. By systematically drafting, debugging, and refining solutions, AIDE achieves superior performance on Kaggle tasks as well as on more research-oriented benchmarks. While developed for tabular machine learning tasks, third-party experiments show that this approach can generalize to challenges such as neural architecture search, Triton Kernel optimization, and other AI R&D tasks. We believe AIDE represents promising step toward the future of automated ML engineering, offering principled way to combine iterative LLM prompting with tree-based exploration of code solutions."
        },
        {
            "title": "References",
            "content": "Anthropic. Claude 3.5 sonnet model card addendum. Technical report, Anthropic, 2024. URL https://www-cdn.anthropic.com/fed9cc193a14b84131812372d8d5857f8f304c52/ Model_Card_Claude_3_Addendum.pdf. J. S. Chan, N. Chowdhury, O. Jaffe, J. Aung, D. Sherburn, E. Mays, G. Starace, K. Liu, L. Maksin, T. Patwardhan, L. Weng, and A. adry. MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering. arXiv preprint arXiv:2410.07095, 2024. T. Elsken, J. H. Metzen, and F. Hutter. Neural architecture search: survey. Journal of Machine Learning Research, 20(55):121, 2019. S. Falkner, A. Klein, and F. Hutter. BOHB: Robust and Efficient Hyperparameter Optimization at Scale. In Proc. of ICML, volume 35, 2018. M. Feurer, A. Klein, K. Eggensperger, J. T. Springenberg, M. Blum, and F. Hutter. Efficient and Robust Automated Machine Learning. In Proc. of NeurIPS, volume 28, 2015. M. Feurer, K. Eggensperger, E. Bergman, F. Pfisterer, B. Bischl, and F. Hutter. Auto-Sklearn 2.0: Hands-free AutoML via Meta-Learning. arXiv preprint arXiv:2007.04074, 2020. Google. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. Mlagentbench: Evaluating language agents on machine learning experimentation. In Forty-first International Conference on Machine Learning, 2024. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free In The Thirteenth International Conference on evaluation of large language models for code. Learning Representations, 2025. URL https://openreview.net/forum?id=chfJJYC3iL. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=VTF8yNQM66. H. Jin, Q. Song, and X. Hu. Auto-Keras: An Efficient Neural Architecture Search System. In Proc. of ACM SIGKDD (KDD), 2019. H. Jin, F. Chollet, Q. Song, and X. Hu. AutoKeras: An AutoML Library for Deep Learning. Journal of Machine Learning Research, 24(6):17, 2023. 10 Leslie Pack Kaelbling, Michael Littman, and Anthony Cassandra. Planning and acting in partially observable stochastic domains. Artificial Intelligence, 101(1-2):99134, 1998. E. LeDell and S. Poirier. H2O AutoML: Scalable Automatic Machine Learning. In Proc. of the AutoML Conference, pages 18, 2020a. Erin LeDell and Sebastien Poirier. H2O AutoML: Scalable automatic machine learning. 7th ICML Workshop on Automated Machine Learning (AutoML), July 2020b. URL https://www.automl. org/wp-content/uploads/2020/07/AutoML_2020_paper_61.pdf. Y. Li and et al. Competition-level Code Generation with AlphaCode. Science, 378:10921097, 2022. doi: 10.1126/science.abq1158. H. Liu, K. Simonyan, and Y. Yang. DARTS: Differentiable Architecture Search. In Proc. of ICLR, 2019. METR. Evaluating frontier AI R&D capabilities of language model agents against human experts. https://metr.org/blog/2024-11-22-evaluating-r-d-capabilities-of-llms/, 2024. Blog post (November 2024). J. Mueller and et al. AutoGluon: AutoML for Text, Image, and Tabular Data. Scientific Reports, 14 (1):72889, 2024. R. S. Olson and J. H. Moore. TPOT: Tree-based Pipeline Optimization Tool for Automating Machine Learning. In ICML AutoML Workshop, 2016. OpenAI. Gpt-4 technical report. Technical report, OpenAI, 2023. OpenAI. Openai o1 system card. Technical report, OpenAI, 2025a. URL https://cdn.openai. com/o1-system-card-20241205.pdf. OpenAI. Openai o3-mini system card. Technical report, OpenAI, 2025b. URL https://cdn. openai.com/o3-mini-system-card.pdf. H. Pham, M. Y. Guan, B. Zoph, Q. V. Le, and J. Dean. Efficient Neural Architecture Search via Parameter Sharing. In Proc. of ICML, volume 35, 2018. E. Real, A. Aggarwal, Y. Huang, and Q. V. Le. Regularized Evolution for Image Classifier Architecture Search. In Proc. of AAAI, 2019. Y. Shen, K. Song, X. Tan, D. Li, W. Lu, and Y. Zhuang. HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face. In Proc. of NeurIPS, 2023. C. Thornton, F. Hutter, H. H. Hoos, and K. Leyton-Brown. Auto-WEKA 2.0: Automatic Model Selection and Hyperparameter Optimization in WEKA. Journal of Machine Learning Research, 14(1):267281, 2013a. C. Thornton, F. Hutter, H. H. Hoos, and K. Leyton-Brown. Auto-WEKA: Combined Selection and Hyperparameter Optimization of Classification Algorithms. In Proc. of ACM SIGKDD (KDD), 2013b. G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and A. Anandkumar. Voyager: An Open-Ended Embodied Agent with Large Language Models. arXiv preprint arXiv:2305.16291, 2023. X. Wang, B. Li, Y. Song, F. F. Xu, X. Tang, M. Zhuge, J. Pan, Y. Song, B. Li, J. Singh, et al. Openhands: An open platform for AI software developers as generalist agents. arXiv preprint arXiv:2407.16741, 2024. Weco AI. AIDE: Data Science Automation Technical Report. https://www.weco.ai/blog/ technical-report, 2024. [Online; accessed February 3, 2025]. Colin White, Mahmoud Safari, Rhea Sukthanker, Binxin Ru, Thomas Elsken, Arber Zela, Debadeepta Dey, and Frank Hutter. Neural architecture search: Insights from 1000 papers. arXiv preprint arXiv:2301.08727, 2023. L. Yang and A. Shami. On hyperparameter optimization of machine learning algorithms: Theory and practice. Neurocomputing, 415:295316, 2020. S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. ReAct: Synergizing Reasoning and Acting in Language Models. In Proc. of ICLR, 2023. B. Zoph and Q. V. Le. Neural Architecture Search with Reinforcement Learning. In Proc. of ICLR, 2017."
        },
        {
            "title": "A Baseline Specifications",
            "content": "Table 4: Baseline hyperparameters. AutoGPT Parameter Value agent model seed max_runtime LangChain AutoGPT gpt-4-0125-preview 1 Human with ChatGPT Parameter Value model gpt-4-0125-preview A.1 H2O AutoML Baseline The machine learning algorithm selection process of H2O AutoML LeDell and Poirier (2020b) proceeds as follows. First, it searches over set of six algorithms: 1. Distributed Random Forest (DRF) and Extremely Randomized Trees (XRT) 2. Generalized Linear Model (GLM) with regularization 3. XGBoost 4. H2O Gradient Boosting Machines 5. Fully connected multi-layer artificial neural network (DeepLearning) 6. Stacked Ensembles (including an ensemble of all base models and ensembles using subsets of the base models) It then performs random search over predefined grid of hyperparameter combinations, avoiding the computational expense of an exhaustive grid search. After training individual models, H2O AutoML creates stacked ensembles by combining the predictions of the best-performing models from each algorithm. This ensemble method leverages the strengths of multiple models to improve overall performance. All trained models, including individual models and ensembles, are evaluated using cross-validation and ranked based on performance metrics such as accuracy, AUC, or RMSE, depending on the problem type. The configurations are shown in Table 4. A.2 AutoGPT Baseline We use the LangChain implementation of AutoGPT, which includes LangChain primitives such as PromptTemplates, VectorStores, and Embeddings. Inspired by Huang et al. (2024), we introduce task descriptor for AutoGPT in each competition to provide basic task planner and minimize human intervention. The task descriptor includes information retrieved from the Kaggle page, such as the dataset description, file details (train.csv, test.csv, sample_submission.csv), evaluation metrics, submission file format, and sample training script. An example task descriptor is shown in Figure 5. We also provide the agent with tools to read and write files, list directories, and run Python REPL evaluations. The agent reads the task descriptor with predefined goals, as shown below:"
        },
        {
            "title": "Goal Prompt",
            "content": "Go through task_descriptor . txt to understand the task and evaluation method . Iterate over different models or feature selections to get better performance based on evaluation method . You can use following steps as reference : 1. Select model and fill in the provided python snippet . 2. Train the model and Make predictions on data from test . csv and Prepare submission . csv by executing the script wiith python repl tool . 3. Save the script into local disk such as model_ { model_name }. py Here are some rules to follow : 1. Never try to change the train . csv and test . csv . 2. Never output graphs or figures . 3. Do Not change the capitalization of the column name 4. Do Not read train . csv and test . csv directly . A.3 ChatGPT with Human Assistance human operator is tasked with solving Kaggle competition using only the information provided in the overview and data tabs, which include the available dataset. The operator is permitted to utilize the ChatGPT web interface. The LLM is set to gpt-4-0125-preview in comparison with AutoGPT. Due to limitations in ChatGPTs capabilities, such as the potential for generating hallucinated results and occasionally using outdated packages, iterative interactions are required. The human operator will continue to issue instructions until valid submission is produced. Upon completion, the operator submits the results to Kaggle, where the submission is ranked against the competition leaderboard."
        },
        {
            "title": "B Analysis of AIDE",
            "content": "B.1 Code Complexity Growth In Figure 6, we observe that the aggregated code complexity (combining LOC, LLOC, Volume, N1, and MI) exhibits an overall increasing trend as the number of iterative steps grows. Initially, there is slight dip in complexity, but after the first step, the metrics begin generally steady rise. This suggests that as AIDE (GPT-4 Turbo) produces successive iterations of code, the solutions tend to become more elaborate, with additional lines of code and logical structures contributing to higher values for traditional software complexity measures. The progressive increase implies that, over multiple generation steps, the model accumulates more intricate functionalitypotentially reflecting deeper problem-solving processes or additional featuresleading to an increasingly complex codebase by the final iteration. B.2 Cost Analysis Figure 7 illustrates the per-task LLM inference cost for AIDE across the Weco-Kaggle benchmark, using GPT-4 Turbo (gpt-4-0125-preview) with pricing data from early 2024. Although certain tasks incur higher costs due to more extensive prompting (up to approximately $2.50 per task), the majority remain under $1.50, reflecting moderate token usage and minimal manual intervention. Overall, these expenditures are much lower than the investment required for human experts or conventional AutoML services, especially when considering the significant performance gains achieved by AIDEs fully automated design. Moreover, as language model costs continue to decline, AIDEs approach becomes increasingly competitive in terms of both performance and budget."
        },
        {
            "title": "C Weco Kaggle Benchmark",
            "content": "13 Figure 5: The task descriptor for bike-sharing-demand"
        },
        {
            "title": "Task Descriptor Prompt",
            "content": "Dataset Description See , fork , and run random forest benchmark model through Kaggle Scripts You are provided hourly rental data spanning two years . For this competition , the training set is comprised of the first 19 days of each month , while the test set is the 20 th to the end of the month . You must predict the total count of bikes rented during each hour covered by the test set , using only information available prior to the rental period . 1 = spring , 2 = summer , 3 = fall , 4 = winter Data Fields datetime - hourly date + timestamp season - holiday - whether the day is considered holiday workingday - whether the day is neither weekend nor holiday weather - 1: Clear , Few clouds , Partly cloudy , Partly cloudy 2: Mist + Cloudy , Mist + Broken clouds , Mist + Few clouds , Mist 3: Light Snow , Light Rain + Thunderstorm + Scattered clouds , Light Rain + Scattered clouds 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist , Snow + Fog temp - temperature in Celsius atemp - \" feels like \" temperature in Celsius humidity - relative humidity windspeed - wind speed casual - number of non - registered user rentals initiated registered - number of registered user rentals initiated count - number of total rentals Files train . csv - the training dat Columns : datetime , season , holiday , workingday , weather , temp , atemp , humidity , windspeed , casual , registered , count test . csv - the same format as train . csv , but without the target value ; your task is to predict the value for each of these targets . Columns : datetime , season , holiday , workingday , weather , temp , atemp , humidity , windspeed samp le_submission . csv - sample submission file in the correct format . Columns : datetime , count Evaluation Submissions are evaluated one the Root Mean Squared Logarithmic Error ( RMSLE ) . Submission Format Your submission file must have header and should be structured in the following format : datetime , count 2011 -01 -20 00:00:00 ,0 2011 -01 -20 01:00:00 ,0 2011 -01 -20 02:00:00 ,0 ... ... # Load and prepare the data train_data = pd . read_csv ( train . csv ) test_data = pd . read_csv ( test . csv ) # please continue with the python script for training 14 # Prepare submission file submission = pd . DataFrame ({ datetime : test_data [ datetime ] , count : test_predictions }) submission . to_csv ( submission . csv , index = False ) Figure 6: Aggregated code complexity (LOC, LLOC, Volume, N1, MI) of the code generated by AIDE (GPT-4 Turbo) with respect to number of steps. Figure 7: LLM cost of running AIDE per task on the Weco-Kaggle benchmark. We used GPT-4 Turbo (gpt-4-0125-preview) with pricing data from January to February 2024. Competition Local Eval Kaggle Submittable GPU Data Size bike-sharing-demand cat-in-the-dat godaddy-microbusiness-density-forecasting house-prices-advanced-regression-techniques icr-identify-age-related-conditions new-york-city-taxi-fare-prediction optiver-trading-at-the-close playground-series-s3e14 playground-series-s3e16 playground-series-s3e17 playground-series-s3e18 15 193.8 kB 22.3 MB 1.9 MB 203.8 kB 154.1 kB 1.7 GB 210.3 MB 649.5 kB 2.8 MB 3.7 MB 2.5 MB Competition has local eval kaggle submittable needs gpu compressed data size playground-series-s3e19 playground-series-s3e20 playground-series-s3e22 playground-series-s3e23 playground-series-s3e24 playground-series-s3e25 tabular-playground-series-aug-2022 tabular-playground-series-feb-2021 tabular-playground-series-feb-2022 tabular-playground-series-jan-2022 tabular-playground-series-jul-2021 tmdb-box-office-prediction career-con-2019 carvana-image-masking-challenge cat-in-the-dat-ii cifar-10 ciphertext-challenge-ii ciphertext-challenge-iii competitive-data-science-predict-future-sales digit-recognizer dog-breed-identification dont-overfit-ii facial-keypoints-detection forest-cover-type-prediction histopathologic-cancer-detection home-data-for-ml-course iwildcam-2019-fgvc6 jigsaw-toxic-comment-classification-challenge kuzushiji-recognition nlp-getting-started noaa-right-whale-recognition planttraits2024 playground-series-s3e1 playground-series-s3e11 playground-series-s3e13 playground-series-s3e15 playground-series-s3e26 playground-series-s3e3 playground-series-s3e5 playground-series-s3e7 playground-series-s3e9 playground-series-s4e1 playground-series-s4e2 predict-volcanic-eruptions-ingv-oe recognizing-faces-in-the-wild rsna-pneumonia-detection-challenge santa-2019-revenge-of-the-accountants scrabble-player-rating sentiment-analysis-on-movie-reviews spaceship-titanic tabular-playground-series-apr-2021 tabular-playground-series-apr-2022 16 1.2 MB 51.3 MB 61.2 kB 6.0 MB 7.1 MB 575.1 kB 2.4 MB 68.9 MB 279.7 MB 236.0 kB 270.4 kB 18.3 MB 36.5 MB 26.2 GB 43.3 MB 750.1 MB 95.6 MB 34.4 MB 15.8 MB 16.1 MB 724.5 MB 13.3 MB 80.0 MB 26.6 MB 6.8 GB 395.3 kB 46.6 GB 55.2 MB 4.5 GB 607.3 kB 9.8 GB 3.8 GB 2.4 MB 9.4 MB 21.0 kB 414.8 kB 358.8 kB 97.4 kB 69.6 kB 929.8 kB 110.9 kB 7.1 MB 939.5 kB 10.2 GB 399.8 MB 3.9 GB 95.7 kB 39.2 MB 2.0 MB 306.4 kB 4.6 MB 179.6 MB Competition has local eval kaggle submittable needs gpu compressed data size tabular-playground-series-aug-2021 tabular-playground-series-dec-2021 tabular-playground-series-jan-2021 tabular-playground-series-jul-2022 tabular-playground-series-jun-2021 tabular-playground-series-jun-2022 tabular-playground-series-mar-2021 tabular-playground-series-mar-2022 tabular-playground-series-may-2021 tabular-playground-series-may-2022 tabular-playground-series-nov-2021 tabular-playground-series-nov-2022 tabular-playground-series-oct-2021 tabular-playground-series-oct-2022 tabular-playground-series-sep-2021 tabular-playground-series-sep-2022 tgs-salt-identification-challenge 156.7 MB 131.8 MB 65.8 MB 20.6 MB 10.4 MB 245.9 MB 57.6 MB 4.9 MB 2.8 MB 269.9 MB 449.0 MB 1.1 GB 1.4 GB 3.7 GB 626.2 MB 630.1 kB 466.1 MB"
        }
    ],
    "affiliations": [
        "Runway ML",
        "Weco AI"
    ]
}