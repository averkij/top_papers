{
    "paper_title": "DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs for Knowledge-Intensive Visual Grounding",
    "authors": [
        "Xinyu Ma",
        "Ziyang Ding",
        "Zhicong Luo",
        "Chi Chen",
        "Zonghao Guo",
        "Derek F. Wong",
        "Xiaoyi Feng",
        "Maosong Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Human experts excel at fine-grained visual discrimination by leveraging domain knowledge to refine perceptual features, a capability that remains underdeveloped in current Multimodal Large Language Models (MLLMs). Despite possessing vast expert-level knowledge, MLLMs struggle to integrate reasoning into visual perception, often generating direct responses without deeper analysis. To bridge this gap, we introduce knowledge-intensive visual grounding (KVG), a novel visual grounding task that requires both fine-grained perception and domain-specific knowledge integration. To address the challenges of KVG, we propose DeepPerception, an MLLM enhanced with cognitive visual perception capabilities. Our approach consists of (1) an automated data synthesis pipeline that generates high-quality, knowledge-aligned training samples, and (2) a two-stage training framework combining supervised fine-tuning for cognitive reasoning scaffolding and reinforcement learning to optimize perception-cognition synergy. To benchmark performance, we introduce KVG-Bench a comprehensive dataset spanning 10 domains with 1.3K manually curated test cases. Experimental results demonstrate that DeepPerception significantly outperforms direct fine-tuning, achieving +8.08\\% accuracy improvements on KVG-Bench and exhibiting +4.60\\% superior cross-domain generalization over baseline approaches. Our findings highlight the importance of integrating cognitive processes into MLLMs for human-like visual perception and open new directions for multimodal reasoning research. The data, codes, and models are released at https://github.com/thunlp/DeepPerception."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 2 7 9 7 2 1 . 3 0 5 2 : r DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs for Knowledge-Intensive Visual Grounding Xinyu Ma1, Ziyang Ding4, Zhicong Luo3, Chi Chen2, , Zonghao Guo2, Derek F. Wong1, , Xiaoyi Feng3, Maosong Sun2 1University of Macau, 2Tsinghua University, 3Northwestern Polytechnical University, 4Shandong University chenchithu@gmail.com, derekfw@um.edu.mo Figure 1. (a) DeepPerception employs knowledge-driven reasoning to derive answers, while the baseline model directly outputs predictions without cognitive processing. (b) DeepPerception demonstrates superior cognitive visual perception capabilities that cannot be elicited in the foundation model through simplistic zero-shot CoT prompting."
        },
        {
            "title": "Abstract",
            "content": "Human experts excel at fine-grained visual discrimination by leveraging domain knowledge to refine perceptual features, capability that remains underdeveloped in current Multimodal Large Language Models (MLLMs). Despite possessing vast expert-level knowledge, MLLMs struggle to integrate reasoning into visual perception, often generating direct responses without deeper analysis. To bridge this gap, we introduce knowledge-intensive visual grounding (KVG), novel visual grounding task that requires both fine-grained perception and domain-specific knowledge integration. To address the challenges of KVG, we propose DeepPerception, an MLLM enhanced with cognitive visual perception capabilities. Our approach consists of (1) an automated data synthesis pipeline that generates high-quality, knowledge-aligned training samples, and (2) two-stage training framework combining supervised fine-tuning for cognitive reasoning scaffolding and reinforcement learning to optimize perception-cognition synergy. To benchmark performance, we introduce KVG-Bench, comprehensive dataset spanning 10 domains with 1.3K manually curated test cases. Experimental results demonstrate that DeepPerception significantly outperforms direct fine-tuning, achieving +8.08% accuracy improvements on KVG-Bench and exhibiting +4.60% superior cross-domain generalization over baseline approaches. Our findings highlight the importance of integrating cognitive processes into MLLMs for human-like visual perception and open new directions for multimodal reasoning research. The data, codes, and models are released at https://github.com/thunlp/ DeepPerception. Corresponding authors 1. Introduction Human experts demonstrate enhanced fine-grained discrimination of visual concepts by flexibly leveraging domain knowledge to refine discriminative features, resulting in superior domain-specific visual perception compared to nonexperts [9, 12, 33]. However, despite their possession of expert-level knowledge [47], current Multimodal Large Language Models (MLLMs) struggle to effectively utilize this knowledge for visual perception in the same way human experts do. This is primarily because current MLLMs often directly generate final answers to questions without deeper analysis [41, 43]. Consequently, when questions require knowledge and analytical reasoning, this directresponse mode frequently results in errors as illustrated in Fig. 1a. In this paper, we explore how to enhance MLLMs with cognitive visual perception capabilities, i.e., to integrate the inherent knowledge and reasoning capacities of MLLMs into the processing of visual perception tasks. Accordingly, we propose knowledge-intensive visual grounding (KVG), novel visual grounding task [13, 46] that advances conventional visual grounding by requiring both fine-grained visual perception and domain-specific knowledge integration. As shown in Fig. 1a, each query in KVG utilizes domain-specific terminology (Clumber Spaniel) rather than generic descriptions (the left dog). Moreover, the images contain distracting objects that require knowledgebased reasoning to differentiate and answer correctly. Unlike the mathematical and geometric tasks studied by previous multimodal reasoning LLMs [41, 43], KVG focuses more on the role of cognition in fundamental visual perception processes. To solve the KVG task, an MLLM must first engage in cognitive process, leveraging relevant domain knowledge, and conduct thorough visual analysis before providing final perceptual response. However, we find this task challenging for current MLLMs. For instance, the accuracy of Qwen2-VL-7B [36] on this task is only around 50% (Fig. 1b), while on traditional visual grounding dataset RefCOCO [46], it exceeds 90% [36]. Furthermore, when explicitly prompting the model to perform the cognitive process, the results drop significantly, which indicates that current MLLMs do not possess this capability. In order to advance MLLMs with cognitive visual perception, we introduce cognition-enhanced training framework comprising: (1) an automated data synthesis pipeline that alleviates the scarcity of high-quality training data by synthesizing knowledge-aligned samples from fine-grained visual categorization datasets, and (2) two-stage training framework where supervised fine-tuning establishes foundational cognitive capabilities through knowledgeintegrated reasoning chains, followed by reinforcement learning that optimizes perception-cognition synergy via perception-oriented reward signals. This integrated approach ensures systematic progression from perceptionoriented reasoning to precision-driven perceptual refinement while maintaining training stability, effectively bridging the gap between MLLMs inherent cognition capability and human-like visual perception. Through this framework, we develop DeepPerception, an MLLM that enhances visual perception through inherent cognition capability. To facilitate systematic evaluation on KVG, we introduce KVG-Bench, comprehensive benchmark encompassing 10 distinct domains with 1.3K manually curated test cases linked to 531 images and 882 entities. Our extensive experiments conducted across KVG and related tasks (1) Our DeepPerception model reveal two key findings: achieves substantial performance gains over direct finetuning (average +8.08% accuracy), thereby demonstrating its effectiveness in activating models cognitive visual perception capabilities; (2) Notably, DeepPerception exhibits superior cross-domain generalization on unseen domains compared to existing approaches (e.g., +4.60% over finetuning), demonstrating its ability to intrinsically enhance visual perception through intrinsic cognitive mechanisms rather than mere entity memorization. In summary, our contributions are threefold: We introduce the novel task of knowledge-intensive visual grounding (KVG) to explore the concept of cognitive visual perception for MLLMs, aiming to integrate their inherent knowledge and reasoning capabilities into visual perception processes. We propose DeepPerception, an MLLM with enhanced cognitive visual perception capabilities. To achieve this, we develop an automated dataset creation pipeline and novel two-stage framework integrating supervised cognitive capability enhancement with perception-oriented reinforcement learning. We introduce KVG-Bench, manually curated benchmark for the KVG task involving diverse knowledge domains and entities. Extensive experiments on KVGBench and other fine-grained visual recognition tasks consistently demonstrate DeepPerceptions exceptional cognitive visual perception capabilities and superior cross-domain generalization performance. 2. Related Work 2.1. Multimodal Large Language Models Recent years have witnessed rapid advancements in MLLMs [6, 19, 20, 28, 36, 44], with visual grounding [3, 17, 23, 29, 45] and reasoning [8, 32, 41, 48] emerging as their pivotal capabilities. For visual grounding, Shikra [3] introduces streamlined end-to-end architecture that allows the model to process spatial input and generate the corresponding visual references. Groma [23] Figure 2. (a) KVG-Bench images contain multiple subordinate-category entities (e.g., Boeing 777, 767, 757, 747, 737, 727, 717, 707 from top to bottom in the left image); (b) KVG-Bench exhibits high diversity across categories and entities. partitions images into multiple regions during tokenization, thereby improving the grasp of the visual information of the model. Regarding reasoning capabilities, MathLLaVA [32] and LLaVA-CoT [41] advances multimodal reasoning through fine-tuning on their synthesized datasets. Notably, LISA [17] uniquely bridges these capabilities by applying MLLM reasoning to visual grounding tasks. In contrast, our work aims to further empower models with expert-level perception-oriented reasoning capabilities to achieve cognitive visual perception. 2.2. Fine-grained Visual Perception Benchmarks Visual Grounding aims to locate objects or regions within an image based on textual query [13, 30, 46]. To enable more comprehensive evaluation of MLLMs visual grounding capabilities, researchers have recently proposed numerous novel and more challenging benchmarks [4, 5, 17, 21] in addition to the most widely adopted RefCOCO/+/g benchmarks [13, 25]. SK-VG [5] focuses on visual grounding with scene knowledge integration as additional input, whereas ReasonSeg [17] requires world knowledge utilization for object localization. Fine-Grained Visual Recognition (FGVR) aims to classify visually similar images to subcategories with super-category [14, 16, 24, 26, 38]. Our proposed KVG task synergistically combines visual grounding and FGVR, simultaneously demanding expertlevel knowledge for distinguishing subordinate categories and precise grounding capabilities. 3. KVG-Bench 3.1. Task Definition The task of knowledge-intensive visual grounding (KVG) is to predict bounding box = fθ(XI , XT ) through the joint understanding of visual input XI and textual query XT , requiring fine-grained alignment between multimodal representations. While sharing structural similarities with referring expression comprehension (REC), the KVG task significantly elevates the challenge beyond standard REC tasks. As exemplified in Fig. 2a, the queries of KVG involve fine-grained entity specifications (e.g., Boeing 747 and White-lipped snail) rather than generic categories such as aircraft and mollusk. Additionally, each image contains multiple objects from the same category of the target object (e.g., multiple aircraft in single image). This setup requires both expert-level knowledge and advanced perceptual and reasoning abilities to pinpoint the precise visual features that distinguish the target from similar objects. 3.2. Benchmark Construction KVG-Bench comprises 1,336 test instances spanning 10 categories with 882 distinct entities, as statistically visualized in Fig. 2b. The construction includes two key parts: image collection and data annotation. We designed meticulous collection process to ensure the diversity and complexity of the images. First, we carefully selected 10 categories from the field of fine-grained visual recognition (FGVR) [14, 16, 24, 26, 38] that are suitable for visual grounding, excluding categories which are challenging for object localization such as sports and scene. Second, an entity list for each category was systematically developed through initial extraction of finegrained labels from existing datasets, followed by comprehensive enrichment of entity names via ChatGPT-assisted expansion. We then retrieved web images using these entity names as search queries, enforcing strict quality criteria: each image must contain at least two entities from the same category with clear visual disparities. The annotation process prioritized quality control. Five annotators manually annotated each image with bounding Figure 3. Overview of the proposed data engine and two-stage training framework. boxes and entity labels by cross-referencing contextual information (e.g. caption, webpage metadata) with authoritative sources (e.g., Wikipedia entries) to verify entity identities. To ensure consistency, all the annotations underwent independent re-evaluation by annotators who did not participate in the initial labeling, with conflicting cases crossverified through multi-annotator reconciliation and persistently inconsistent instances eliminated to ensure annotation accuracy. By integrating comprehensive validation protocols and expert-aligned annotation workflows, KVG-Bench sets new standard for evaluating cognitive visual perception in multimodal systems. 3.3. Human Evaluation To assess the difficulty of KVG-Bench, we conducted human evaluations with 11 non-expert volunteers under two experimental settings: Closed-Book (no external resources) and Open-Book (allowing single Wikipedia query per case to simulate expert-level knowledge integration). Participants were randomly assigned several categories, with each category being evaluated by at least five evaluators to mitigate knowledge bias. The evaluation results, as shown in Tab. 1, reveal significant performance differences between settings. Notably, the Open-Book Setting demonstrated significant performance elevation (78.83% accuracy) compared to Closed-Book results (56.41%). This validates that KVG-Bench requires synergistic integration of expert-level knowledge and fine-grained visual comparison, thereby positioning it as meaningful testbed for advancing cognitive visual perception in MLLMs. 4. Method While current MLLMs demonstrate inherent grounding capabilities, as demonstrated in Fig. 1, their direct answer paradigm lacks reasoning mechanisms to synergize cognitive processing with visual perception, resulting in suboptimal performance for the KVG task. The absence of explicit reasoning pathways crucially prevents effective integration of domain knowledge with visual understanding, particularly in scenarios requiring multi-step cognitive operations. Drawing inspiration from the remarkable success of DeepSeek-R1 [10], we proposed two-stage cognitive training framework (See Fig. 3), which enables systematic development of cognitive visual perception capability while maintaining training stability. 4.1. Data Engine The training data images for KVG must meet the following criteria: (1) they should include expert-level entity and bounding box annotations, and (2) the images should contain multiple similar objects to ensure the difficulty of the task. To overcome the scarcity of such data, we developed an automated pipeline leveraging FGVR datasets as the data source. First, data from multiple FGVR datasets were categorized into ten categories (as shown in Fig. 2). The first five categories were used directly for training, while the last five were used solely as unseen categories, with their data not being used for training. Since there are no bounding box annotations in the original FGVR datasets, we employed Qwen2VL-7B [36] to generate bounding boxes through entity-specific prompts (e.g., Find and give the bounding box of Airbus A330). With entity labels preverified for presence, this automated method achieved over 95% accuracy, validated through manual checks. To overcome the limitation of single-object dominance in existing datasets, as illustrated in Fig. 3, we synthesized composite images containing at least 2 entities from the same category (e.g., multiple dog species in one image) using horizontal, vertical, grid, or random layouts, adjusting bounding box coordinates to preserve annotation consistency while enforcing entity uniqueness. This approach requires models to perform fine-grained visual comparisons through structured cognitive processes rather than relying on memorization. The final dataset, partitioned for two-stage training, balances real-world complexity with quality control, simulating scenarios demanding cognitive visual perception. 4.2. Chain-of-Thought Supervised Fine-Tuning The primary objective of the first stage training is to endow models with visual perception-oriented cognitive capabilities through synthesized CoT training data. This process fosters an initial cognitive-perceptual synergy by explicitly modeling reasoning trajectories in the KVG task while establishing fundamental knowledge association patterns for subsequent optimization. Specifically, we leveraged the powerful Qwen2-VL-72B [36] model to generate CoT reasoning data. As illustrated in Fig. 3, we generated CoT rationales by inputting images, ground-truth annotations (labels and bounding boxes), and CoT reasoning prompts into the model. This process synthesizes reasoning chains that combine the models domain knowledge with human-like cognitive processes (e.g., iterative hypothesis verification through joint domain knowledge and visual feature analysis). We then performed SFT on the Qwen2VL-7B model using these data, yielding the stage-1 model with foundational cognitive visual perception capabilities. This model demonstrates preliminary abilities to integrate domain knowledge and generate stepwise reasoning chains for visual grounding, which serves as stable foundation for the second-stage reinforcement learning training. This approach bridges the gap between raw perception and expertlevel cognition, enabling models to emulate human-like visual perception through knowledge-visual co-verification. 4.3. Perception-oriented Reinforcement Learning Following the initial CoT-SFT training, we conducted reinforcement learning using separate subset of training data to further enhance the models perception capabilities, building upon its acquired cognitive visual perception foundation. As illustrated in Fig. 3, we adopted GRPO following Guo et al. [10], which optimizes the policy model by sampling group of outputs to question and calculating group-relative advantage instead of critic model. To adapt GRPO for visual grounding tasks, we designed rule-based reward system and performed data filtering on the training data using the stage-1 model. Training Objective. Let denote the question and {o1, . . . , oG} denote the sampled outputs from the old policy model πθold, then the optimization objective of policy model πθ can be formally defined as: JGRP O(θ) = (cid:104) (Q), {oi}G (cid:105) i=1 πθold (O q)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 (cid:18) πθ (oi q) πθold (oi q) Ai βDKL (πθπref ) (cid:19) DKL (πθπref ) = πref (oi q) πθ (oi q) log πref (oi q) πθ (oi q) (2) where ε and β are hyper-parameters and Ai is the advantage computed based on group of rewards {r1, . . . , rG} (1) corresponding to the outputs of each group: Ai = ri mean({r1, . . . , rG}) std({r1, . . . , rG}) (3) Reward Modeling. We adopted rule-based reward system specifically tailored for visual grounding tasks, comprising two types of rewards: Intersection over Union (IoU) reward and Format reward. The IoU reward evaluates the spatial alignment between predicted bounding boxes and ground-truth annotations. Given the ground-truth bounding box and the predicted bounding box B, the IoU reward is formally defined as: RIoU = (cid:40) IoU(B, B), 0, if IoU(B, B) τ otherwise (4) where τ denotes the alignment threshold for valid spatial grounding. In contrast to the conventional binary accuracy reward (0/1) based solely on final answer correctness, our continuous IoU reward delivers finer-grained supervision signals proportional to spatial prediction quality. By introducing an alignment threshold τ , this design effectively prevents reward hacking through degenerate bounding boxes (e.g. [0, 0, 1000, 1000]) that exploit low-IoU predictions. The format reward employs regular expression-based pattern matching to enforce strict structural separation between reasoning processes and final answers, ensuring cognitive rationales are encapsulated within <think> and </think> tags while grounded predictions reside within <answer> and </answer>. In addition, we imposed constraints on the bounding box format to ensure correct output structure. Data Filtering. To enhance training efficiency, we implemented strategic data filtering using the stage-1 CoTSFT model. Specifically, we performed multiple sampling passes on the training data and filtered out cases where all samples were either fully correct (deemed too trivial for further training) or entirely incorrect (considered too challenging for performance improvement through repeated sampling). This approach ensures the model focuses on learnable instances where partial correctness provides actionable optimization signals, thereby maximizing training effectiveness while mitigating exposure to uninformative extremes. 5. Experiment 5.1. Implementation Details Datasets. We conducted experiments based on several FGVR datasets including FGVC-Aircraft [24], StanfordCars [16], iNaturalist2017 [35], Food101 [2], StanfordDogs [14], Flower-102 [26], and Google-Landmarksv2 [39]. Notably, as detailed in Sec. 4.1, only subset of 1reproduced using VLMEvalKit Models Seen Categories Unseen categories Avg. Air. Car Rep. Bird Food Avg. Dog Mol. Mam. Flwr. Ldmk. Avg. Human Evaluation Human Human + search 59.33 81.33 66.67 85.56 50.84 68.00 44.17 74.17 65.33 86.67 57.27 78. 48.33 78.89 45.33 74.00 51.67 74.17 64.45 84.44 68.00 86.67 55.56 79. 56.41 78.83 70B-Scale MLLMs InternVL2-76B [6] Qwen2-VL-72B [36] 62.50 63.16 74.04 75.96 60.00 59. 41.04 40.24 76.43 77.14 59.22 59.34 78.40 80.80 51.11 42.96 56.25 59. 43.82 65.17 55.42 66.27 57.90 62.32 58.68 60.55 Specialist Grounding Models YOLO-World [7] G-DINO-1.6-Pro [34] DINO-X [31] 41.45 39.47 43.42 28.85 41.35 49.04 8.28 48.97 42.76 14.74 23.11 28.29 30.71 24.29 41.43 23.36 33.59 38. 50.40 44.00 62.40 2.22 40.00 35.56 24.11 39.29 48.21 1.12 32.58 31.46 3.61 27.71 49.40 17.83 37.68 45. 21.11 35.25 41.69 7B-Scale MLLMs Shikra-7B [3] CogVLM-G [37] DeepSeek-VL2 [40] InternVL2-8B [6] Qwen2-VL-7B [36] Qwen2-VL-7B (SFT) DeepPerception 20.39 46.71 51.32 46.05 48.03 53.95 69. 25.96 64.42 60.57 52.88 74.04 79.81 86.54 15.17 49.66 53.10 40.69 51.30 53.10 64.14 16.33 34.26 29.08 26.29 33.07 31.87 41. 28.57 63.57 63.57 50.71 65.71 67.86 77.86 20.33 48.61 47.98 40.53 50.38 52.65 63.13 51.20 79.20 62.40 64.80 76.00 84.80 85. 19.26 31.11 35.56 34.81 33.33 34.81 40.74 25.00 54.46 50.89 45.54 54.46 54.46 58.04 16.85 56.18 44.94 41.57 57.30 40.45 59. 22.89 66.27 39.76 25.30 59.04 53.01 61.45 27.94 56.43 47.06 43.57 55.33 56.25 60.85 23.43 51.80 47.60 41.77 52.40 54.12 62. Table 1. KVG results of DeepPerception and baseline models. Models Dog Bird Air. Flwr. Pet Car Avg. LLaVA 1.5 [20] Phi-3-Vision [1] Idefics2 [18] Finedefics [11] Qwen2VL-7B [36] DeepPerception 38.96 39.80 57.96 72.86 71.39 78.29 35.24 37.63 47.17 57.61 65.31 67. 34.71 42.33 56.23 63.82 71.77 75.31 51.37 51.59 72.78 89.88 86.78 93.06 52.25 56.36 81.28 92.18 91.25 93.00 46.92 54.50 80.25 84.67 90.95 91.75 43.24 47.04 65.95 76.84 79.57 83.21 Table 2. FGVR results of DeepPerception and baseline models. Models MMBench-V1.1test MMMUval AI2D MathVision Qwen2VL-7B1 DeepPerception 82.0 81.1 50.0 49.8 79.4 80. 18.6 18.8 Table 3. Qwen2VL-7B on general multimodal benchmarks."
        },
        {
            "title": "Performance Comparison of DeepPerception and",
            "content": "the aforementioned datasets was utilized for KVG training, with 25K samples for stage-1 followed by 4K filtered instances for stage-2 training. Baseline Models. We built DeepPerception upon Qwen2VL-7B [36] for its top visual grounding capability and rich knowledge. Several MLLMs with strong visual grounding capabilitiy were selected for comparison. For 70B-scale models, we used InternVL2-Llama3-76B [6] For 7B-scale models, we and Qwen2VL-72B [36]. used Shikra [3], CogVLM-Grounding [37], DeepSeek-VL2 [40], InternVL2-8B [6], and Qwen2-VL-7B [36]. Additionally, we conducted comparisons with three specialist models: YOLO-World [7], GroundingDINO-1.6-Pro [34] and DINO-X [31]. Evaluation Settings. We evaluate all methods both on KVG-Bench and the FGVR task. For KVG-Bench, we adopt accuracy as the evaluation metric as in traditional REC [3, 36, 45]. Specifically, given predicted bounding box and ground-truth bounding box B, we compute their IoU and classify the prediction as correct if IoU 0.5. For FGVR, we follow the exact test settings with Finedefics [11] to frame it as multi-choice task. To calculate the average accuracy across multiple categories, we compute weighted average based on the number of instances per category. Training Details. We adopt 8 NVIDIA A100 GPUs with 80G of memory. All seeds are fixed during the training procedures. We use Adam optimizer [15] with learning rate as 5e 6, β1 = 0.9, β2 = 0.999, and ε = 1e 8 in the CoTSFT stage and AdamW optimizer [22] with learning rate as 1e 6, β1 = 0.9, β2 = 0.999, and ε = 1e 6 in the GRPO stage. The accumulated batch size is set to 16 in both stages. The GRPO stage employs maximum completion length of 512 tokens with 4 samples per input. 5.2. Main Results As demonstrated in Tab. 1, our DeepPerception model achieves significant performance improvements by inte-"
        },
        {
            "title": "In Domain Out of Domain Overall",
            "content": "1 2 SFT CoT-SFT CoT-SFT GRPO 52.65 56.82 56.94 63. 56.25 56.80 56.99 60.85 54.12 56.81 56.96 62.20 Table 4. Ablation study on the effectiveness of the proposed twostage cognitive training framework. that cognitive mechanisms central to biological vision can be effectively operationalized in multimodal AI systems. FGVR. To validate the generalizability of cognitive visual perception, we conducted experiments on FGVR datasets. Unlike the two-stage training paradigm used for KVG, only the reinforcement learning phase was applied here, as base models already exhibit strong baseline performance on FGVR. As shown in Tab. 2, our DeepPerception model achieves state-of-the-art results across all categories, demonstrating universal performance gains: DeepPerception achieves state-of-the-art results across all FGVR categories (83.21% average accuracy), outperforming Qwen2-VL-7B by +3.64%. This evidence strongly supports that cognitive visual perception, characterized by iterative knowledge-visual alignment, provides universal benefits for fine-grained perception, mirroring human experts ability to combine sensory input with conceptual understanding. General Capability. To comprehensively assess the models general capabilities across diverse multimodal scenarios, we conducted evaluations on established multimodal benchmarks. As shown in Tab. 3, DeepPerception maintains performance comparable to the base model, demonstrating preserved general capabilities without degradation. 5.3. Ablation Study As shown in Tab. 4, our ablation study systematically evaluates the contributions of the two-stage training: Stage-1. Replacing standard SFT with CoT-SFT elevates performance on seen categories from 52.65% to 56.82% while sustaining strong generalization on unseen categories (56.25%56.80%), demonstrating that cognitive process integration enhances perceptual capabilities beyond conventional training. Stage-2. The GRPO method achieves significant gains over CoT-SFT in stage-2, with in-domain accuracy rising to 63.13% (+6.19%) and out-of-domain accuracy reaching 60.85% (+3.86%). Combined with the KL Divergence analysis  (Fig. 5)  , this confirms GRPOs unique capability to Figure 4. Knowledge evaluation results. DeepPerception exhibits greater improvement on known entities across domains, evidencing cognitive visual perception with structured knowledge integration rather than superficial perceptual improvements. Figure 5. KL Divergence analysis between the probability distribution of response tokens from stage-2 models and the stage1 model reveals complementary specialization: CoT-SFT focuses on knowledge-guided reasoning process (higher CoT divergence) while GRPO optimizes perceptual precision (elevated answer divergence), synergistically bridging cognitive processing and perception refinement. grating cognitive visual perception capabilities, validating the hypothesis that human-inspired cognition-perception synergy enhances visual perception. On in-domain categories, the model attains 63.13% average accuracy, surpassing all 7B-scale baselines (e.g., +10.5% over SFT) and outperforming 70B-scale models such as InternVL2Llama3-76B (59.22%). For out-of-domain generalization, it achieves 60.85% accuracycomparable to 70B-scale modelswhile demonstrating near-human performance in categories where the model possesses richer knowledge, such as Dog (85.60% vs. human evaluators 78.89%). These results confirm that DeepPerceptions success stems from its ability to emulate human-like cognitive processes: iterative knowledge-guided reasoning refines perceptual hypotheses (e.g., verifying anatomical features stepwise), while reinforcement learning aligns these hypotheses with precise visual outputs. The consistent superiority over non-cognitive baselines, particularly in fine-grained categories like Car (86.54% vs. human 85.56%), proves that structured knowledge integrationnot mere visual memorizationdrives performance gains. By bridging knowledge reasoning with perception, DeepPerception establishes new state-of-the-art results on KVG-Bench, demonstrating translate refined reasoning into precise perceptual outputs (bounding box coordinates). The results reveal that structured cognition enhancement and perception refinement are complementary and critical for resolving fine-grained ambiguities. This validates our hypothesis that mimicking human-like cognitionperception interaction is essential for advancing visual perception of MLLMs. 5.4. Analysis To gain deeper insights into our models capabilities, we conducted the following analysis of its cognitive visual perception mechanisms: How knowledge influences perception. We systematically evaluated the base models knowledge by probing entity-specific discriminative features and validating factual alignment against Wikipedia references using Qwen2.514B [42]. Entities were then classified into four groups based on knowledge possession (Known/Unknown) and training data presence (in-domain/out-of-domain). As evidenced in Fig. 4, DeepPerception achieves significantly greater performance gains on known entities versus unknown entities regardless of domain boundaries, confirming that its improvements originate from knowledge-driven cognitive processing rather than superficial perceptual enhancements, thereby validating its capacity to harness domain knowledge for visual perception. This aligns with human experts reliance on domain expertise to resolve ambiguity, confirming our frameworks success in emulating cognitive visual perception. The roles of CoT-SFT and GRPO. As shown in Fig. 5, we quantified the impact of our two-stage training paradigm by measuring the KL Divergence between stage-2 models (trained with CoT-SFT or GRPO) and the stage-1 model, separately evaluating the CoT and answer components. For CoT-SFT, the significantly higher KL Divergence in the CoT components versus answer segments indicates that CoT-SFT primarily equips the model with perceptionoriented reasoning capabilities. In contrast, GRPO exhibited greater divergence in the answer components than in the CoT, demonstrating its focus on refining perceptual accuracy grounded in the reasoning process. By combining the two stages, the framework achieves synergistic enhancement of cognitive visual perception, effectively bridging knowledge-driven reasoning and sensory processing. Qualitative Results. As illustrated in Fig. 6, we conducted comparative case analysis between DeepPerception and Qwen2-VL-7B. The visual evidence demonstrates our models capability to generate accurate answers through Figure 6. Case study comparing DeepPerception and Qwen2-VL7B on KVG-Bench (top) and FGVR (bottom). reasoning process that systematically integrates domainspecific knowledge with visual observations, in contrast to the baseline models tendency to produce incorrect responses from superficial pattern recognition. 5.5. Discussion We discuss two key empirical findings of GRPO, with (1) Unlike detailed results provided in the appendix: DeepSeek-R1-Zeros [10] progressive increase in response length during training, GRPO training on both KVG and FGVR data exhibits stable completion lengths fluctuating within specific ranges, suggesting bounded reasoning complexity in visual perception tasks; (2) Notably, we observed instances where incorrect cot rationales led to correct answers, indicating that the presence of cognitive processes, rather than their length or even factual accuracy, is the primary determinant of performance improvement. Detailed results are provided in Appendix C.4. 6. Conclusion In summary, this work demonstrates that embedding cognitive visual perceptioncharacterized by perception enhancement through the integration of knowledge and reasoningsignificantly enhances fine-grained visual perception in MLLMs. Our DeepPerception model achieves stateof-the-art performance on both KVG and FGVR tasks, outperforming 7B-scale baselines and rivaling 70B-scale models. Extensive experiments and analyses reveal that the twostage training paradigm enables the model to effectively leverage cognitive mechanisms to refine perception, as evidenced by consistent improvements across domains. These findings establish cognitive enhancement as pivotal direction for advancing fine-grained perception and comprehension in MLLMs."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. 6 [2] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101mining discriminative components with random In Computer visionECCV 2014: 13th European forests. conference, zurich, Switzerland, September 6-12, 2014, proceedings, part VI 13, pages 446461. Springer, 2014. 5, 11 [3] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Shikra: Unleashing multiarXiv preprint Feng Zhu, and Rui Zhao. modal llms referential dialogue magic. arXiv:2306.15195, 2023. 2, 6, 12 [4] Zhenfang Chen, Peng Wang, Lin Ma, Kwan-Yee Wong, and Qi Wu. Cops-ref: new dataset and task on compositional referring expression comprehension. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1008610095, 2020. 3 [5] Zhihong Chen, Ruifei Zhang, Yibing Song, Xiang Wan, and Guanbin Li. Advancing visual grounding with scene In Proceedings of knowledge: Benchmark and method. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1503915049, 2023. 3 [6] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 2, 6 [7] Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, and Ying Shan. Yolo-world: Real-time openvocabulary object detection. In Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2024. 6, 12 [8] Linger Deng, Yuliang Liu, Bohan Li, Dongliang Luo, Liang Wu, Chengquan Zhang, Pengyuan Lyu, Ziyang Zhang, Gang Zhang, Errui Ding, et al. R-cot: Reverse chain-of-thought problem generation for geometric reasoning in large multimodal models. arXiv preprint arXiv:2410.17885, 2024. 2 [9] Bruce Goldstein. Cognitive psychology: Connecting mind, research and everyday experience. Wadsworth Publishing, 2007. [10] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 4, 5, 8, 13 [11] Hulingxiao He, Geng Li, Zijun Geng, Jinglin Xu, and Yuxin Peng. Analyzing and boosting the power of fine-grained visual recognition for multi-modal large language models. In The Thirteenth International Conference on Learning Representations, 2025. 6 [12] Jay Hegdé. Time course of visual perception: coarse-tofine processing and beyond. Progress in neurobiology, 84 (4):405439, 2008. 2 [13] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787798, 2014. 2, 3 [14] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Li Fei-Fei. Novel dataset for fine-grained image In First Workshop on Fine-Grained Visual categorization. Categorization, IEEE Conference on Computer Vision and Pattern Recognition, Colorado Springs, CO, 2011. 3, 5, [15] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. 6, 12 [16] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In Proceedings of the IEEE international conference on computer vision workshops, pages 554561, 2013. 3, 5, 11 [17] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95799589, 2024. 2, 3 [18] Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? Advances in Neural Information Processing Systems, 37:8787487907, 2025. 6 [19] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 2 [20] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2629626306, 2024. 2, 6 [21] Runtao Liu, Chenxi Liu, Yutong Bai, and Alan Yuille. Clevr-ref+: Diagnosing visual reasoning with referring expressions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 41854194, 2019. 3 [22] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. 6, [23] Chuofan Ma, Yi Jiang, Jiannan Wu, Zehuan Yuan, and Xiaojuan Qi. Groma: Localized visual tokenization for grounding multimodal large language models. In European Conference on Computer Vision, pages 417435. Springer, 2024. 2 [24] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013. 3, 5, 11 [25] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1120, 2016. 3 [26] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over large number of classes. In 2008 Sixth Indian conference on computer vision, graphics & image processing, pages 722729. IEEE, 2008. 3, 5, 11 [27] OpenAI. GPT4 technical report. arXiv preprint arXiv:2303.08774, 2023. 11 [28] OpenAI. Hello gpt-4, 2024. 2 [29] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. 2 [30] Yanyuan Qiao, Chaorui Deng, and Qi Wu. Referring expression comprehension: survey of methods and datasets. IEEE Transactions on Multimedia, 23:44264440, 2020. [31] Tianhe Ren, Yihao Chen, Qing Jiang, Zhaoyang Zeng, Yuda Xiong, Wenlong Liu, Zhengyu Ma, Junyi Shen, Yuan Gao, Xiaoke Jiang, Xingyu Chen, Zhuheng Song, Yuhong Zhang, Hongjie Huang, Han Gao, Shilong Liu, Hao Zhang, Feng Li, Kent Yu, and Lei Zhang. Dino-x: unified vision model for open-world object detection and understanding, 2024. 6, 12 [32] Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee. MathLLaVA: Bootstrapping mathematical reasoning for multimodal large language models. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 4663 4680, Miami, Florida, USA, 2024. Association for Computational Linguistics. 2, 3 [33] James Tanaka and Marjorie Taylor. Object categories and expertise: Is the basic level in the eye of the beholder? Cognitive psychology, 23(3):457482, 1991. 2 [34] Ren Tianhe, Huang Hongjie, and Jiang Xiaoke. Grounding dino 1.6, 2024. 6, 12 [35] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classification and detection dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 87698778, 2018. 5, 11 [36] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2, 4, 5, 6, 11, [37] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Song XiXuan, Jiazheng Xu, Keqin Chen, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. CogVLM: Visual expert In The Thirty-eighth Anfor pretrained language models. nual Conference on Neural Information Processing Systems, 2024. 6 [38] Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, and Pietro Perona. Caltech-ucsd birds 200. 2010. 3 [39] T. Weyand, A. Araujo, B. Cao, and J. Sim. Google Landmarks Dataset v2 - Large-Scale Benchmark for InstanceLevel Recognition and Retrieval. In Proc. CVPR, 2020. 5, 11 [40] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, et al. Deepseek-vl2: Mixture-ofexperts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024. 6, 12 [41] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision language models reason stepby-step. arXiv preprint arXiv:2411.10440, 2024. 2, 3 [42] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. 8, 12 [43] Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, et al. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319, 2024. [44] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. 2 [45] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. In The Twelfth International Conference on Learning Representations, 2024. 2, 6 [46] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expresIn Computer VisionECCV 2016: 14th European sions. Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 6985. Springer, 2016. 2, 3 [47] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for exIn Proceedings of the IEEE/CVF Conference pert agi. on Computer Vision and Pattern Recognition, pages 9556 9567, 2024. 2 [48] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186, 2024. 2 A. KVG-Bench This section details the data collection and annotation protocol for KVG-Bench. A.1. Data Collection Category Selection Ten categories were strategically curated from established fine-grained visual recognition (FVGR) datasets including FGVC-Aircraft [24], StanfordCars [16], iNaturalist2017 [35], Food101 [2], StanfordDogs [14], Flower-102 [26], and Google-Landmarksv2 [39]. Categories requiring ambiguous spatial localization (e.g., \"sports\" and \"scenes\") were systematically excluded. Entity List Curation We built the list of detailed entity names through step-by-step process. First, we started with existing names from aforementioned FGVR datasets to make sure they fit the right categories. Then, we used ChatGPT [27] to collect more entities in these categories by querying with category name and example entities. Finally, we checked all these entity names against Wikipedia to confirm their accuracy and avoid confusing or incorrect terms. Web Image Retrieval The image collection process employed diversified search strategies, systematically generating query variations (e.g., versus Y, compared to Y, differences between and Y) to retrieve visually discriminative instances across search engines. The image collection process focused on two core principles: diversity and challenge. To ensure diversity, images were collected to encompass variety of distinct entities within each category, achieved through systematically varied search query combinations. This approach guaranteed wide range of entity interactions and visual scenarios. For challenge, we specifically selected images containing multiple entities from the same category (requires fine-grained visual discrimination) or exhibited high visual similarity with subtle distinguishing features. A.2. Annotation The annotation process prioritized quality control. Five annotators manually annotated each image with bounding boxes and entity labels by cross-referencing contextual information (e.g. caption, webpage metadata) with authoritative sources (e.g., Wikipedia entries) to verify entity identities. To ensure consistency, the annotations underwent independent re-evaluation by annotators who did not participate in the initial labeling, with conflicting cases cross-verified through multi-annotator reconciliation and persistently inconsistent instances eliminated to ensure annotation accuracy. B. Method This section elaborates on the implementation details of the proposed method. B.1. Data Engine Our data processing pipeline comprises three core stages: categorization, fine-grained annotation, and image composition. sources. Categorization. We first categorize the data based on dataset For example, data from FGVCAircraft [24] is directly classified as aircraft . Second, for datasets covering multiple categories such as iNaturalist2017 [35], we subdivide entities into finer-grained categories (e.g., mammal , bird, etc.) based on their annotations. Finally, the data is divided into ten distinct categories: aircraft, car, reptilia, bird, food, dog, mollusca, mammal, flower, and landmark. The data from the first five categories are used for training, while the data from the last five categories are solely used to construct the entity list for testing purposes. Fine-grained Annotation. To acquire bounding box annotation, we employed the Qwen2-VL-7B [36] model for visual grounding. Given that each image already possessed verified ground truth labels where the corresponding entities were guaranteed to be present, we directly utilized these textual labels as prompts to generate precise bounding box annotations through the models visual-language alignment capabilities. Given an image and its entity label E, we input the structured query: Find and give the bounding box of {E} to generate bounding box coordinates. After the automatic annotation, we randomly sample the annotated data for manual verification. The results show that the accuracy of the automatic annotations exceeds 95%, allowing us to consider the resulting bounding boxes as approximations of the ground truth. Image Composition. For each category-specific dataset Dc, we implement an iterative sampling protocol without replacement, where the sample size is stochastically determined as Psel(2 6) per iteration, where Psel is pre-defined discrete distribution over range [2, 6]. This process strictly enforces entity exclusivity by guaranteeing (cid:84)k i=1 Ei = across all sampled instances, where Ei denotes the entity in the i-th sample. The sampled images are then composited using four layout strategies: horizontal tiling, vertical stacking, grid arrangement, and random placement. For each sampled batch, we randomly select one composition method and apply corresponding geometric transformations to adjust the original bounding box coordinates to their correct positions in the synthesized images. This spatial adaptation process preserves annotation consistency while creating multi-instance scenes. The final augmented dataset contains composite images with transformed bounding boxes and preserved entity labels, where each composite image integrates multiple object instances from the original samples. The dataset is then partitioned into two disjoint subsets for subsequent two-stage training."
        },
        {
            "title": "CoT Generation Prompt",
            "content": "<vision_start>Image.jpg<vision_end> This image shows [entity1] ([bbox1]), [entity2] ([bbox2]), , and [entityn] ([bboxn]). The bounding box of [target entity] is [target bbox]. Give the reasoning process that would identify it based on the image and your knowledge Note that you MUST pay attention to the differences from other objects of the same type in this image and make detailed comparison between them to find evidence that distinguishes this object from the others Note that you MUST first analyze the visual features that help you make judgment, and then compare the objects Note that when an object is [Unknown], you can still make comparison based on its visual features without knowing its name B.2. Chain-of-Thought Data Generation We employ Qwen2-VL-72B [36] to generate Chain-ofThought (CoT) rationales. Specifically, the model takes the image, ground-truth entity annotations (names and bounding boxes), and CoT generation instruction as input, producing detailed reasoning processes for training. Fig. 7 illustrates representative training instance. C. Experiment This section provides additional details about the experiments and analysis, along with discussions on empirical findings. C.1. Implementation Details Baseline Models. When evaluating certain baseline models, some adjustments were made to accommodate minor differences in their output formats.For DeepSeek-VL2 [40], the first output bounding box was selected as the models predicted answer for downstream evaluation. For the evaluation of GroundingDINO-1.6-Pro [34] and DINO-X [31], we used their official APIs (GroundingDINO API, DINO-X API) in deepdataspace . Images were converted to base64 format, with oversized images downsampled. Detection targets were fine-grained labels (e.g., Buick Enclave) instead of generic categories (e.g., car). API requests followed official configurations, and the highest-score bounding box was selected. YOLO-World [7] was evaluated locally under identical criteria. Regarding Shikra-7B [3], we have replicated the evaluation code based on the officially released model and code. However, the actual performance is significantly lower than that reported in the paper. Similar discrepancies have also been observed in replication attempts by other researchers. Training Details. We use Adam optimizer [15] with learning rate as 5e 6, β1 = 0.9, β2 = 0.999, and ε = 1e8 in the CoT-SFT stage and AdamW optimizer [22] with learning rate as 1e 6, β1 = 0.9, β2 = 0.999, and ε = 1e 6 in the GRPO stage. The accumulated batch size is set to 16 in both stages. The GRPO stage employs maximum completion length of 512 tokens with 4 samples per input. C.2. Analysis Knowledge Evaluation The knowledge evaluation operates through three sequential steps: (1) Knowledge Probing: For each entity in KVG-Bench, the model was prompted to generate discriminative visual features distinguishing it from same-category entities. (2) Reference Validation: Model outputs were evaluated against corresponding Wikipedia descriptions using Qwen2.5-14B [42] to measure factual consistency. (3) Entity Categorization: Entities were first categorized as Known (aligned with reference) or Unknown (divergent from reference), then further divided into four groups based on their presence in the training data. KL Divergence Analysis Revised as Continuous Prose: To ensure equitable comparison of CoT-SFT and GRPO enhancements, we trained stage-1 models on identical data volumes using each method, followed by systematic evaluation of resultant stage-2 models on KVG-Bench. KL Divergence between stage-2 and stage-1 models was computed and split via the special token (</think>) into CoT reasoning and answer components, with token-level averaging applied to both segments to mitigate the effect of length variation. C.3. Qualitative Results Figs. 10 and 11 present comparative analyses between DeepPerception and Qwen2-VL-7B on KVG-Bench and FGVR datasets, respectively. Extensive qualitative comparisons demonstrate DeepPerceptions substantial performance advantage in fine-grained visual perception tasks, attributable to its cognitive visual reasoning capabilities - critical competency absent in Qwen2-VL-7B. Fig. 12 illustrates atypical patterns including: (1) speculative reasoning (A/B hypotheses without definitive conFigure 7. Example of Chain-of-Thought data generated by Qwen2-VL-72B. C.4. Discussion Our analysis of response length dynamics during GRPO training on KVG  (Fig. 8)  and FGVR  (Fig. 9)  datasets reveals stable length throughout optimization, diverging from the progressively increasing trends observed in DeepSeekR1-Zero [10]. Notably, while an initial sharp decline occurred in training on KVG, which was primarily attributable to the 512 tokens completion limit imposing penalties on lengthy responses, length stabilization ultimately prevailed. This stability is attributed to constrained reasoning complexity requirements in our tasks: unlike DeepSeek-R1Zeros multi-faceted reasoning paradigms, our framework operates through streamlined cognitive pathways that inherently limit verbose rationalization needs. The mechanisms underlying cognitive-perceptual alignment, particularly how flawed reasoning enables accurate perception, present compelling avenue for future exploration into cognitive visual perception. Figure 8. The average response length of DeepPerception on the KVG training data during the RL process Figure 9. The average response length of DeepPerception on the FGVR training data during the RL process clusions) and (2) reasoning-output discrepancies (divergent rationalization vs final answers). These cases suggest that the presence of cognitive processes may hold greater significance than their factual accuracy in cognitive visual perception. Figure 10. Case study comparing DeepPerception and Qwen2-VL-7B on KVG-Bench Figure 11. Case study comparing DeepPerception and Qwen2-VL-7B on FGVR Figure 12. Illustrative Cases of atypical CoT Reasoning"
        }
    ],
    "affiliations": [
        "Northwestern Polytechnical University",
        "Shandong University",
        "Tsinghua University",
        "University of Macau"
    ]
}