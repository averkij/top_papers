{
    "paper_title": "DualTHOR: A Dual-Arm Humanoid Simulation Platform for Contingency-Aware Planning",
    "authors": [
        "Boyu Li",
        "Siyuan He",
        "Hang Xu",
        "Haoqi Yuan",
        "Yu Zang",
        "Liwei Hu",
        "Junpeng Yue",
        "Zhenxiong Jiang",
        "Pengbo Hu",
        "Börje F. Karlsson",
        "Yehui Tang",
        "Zongqing Lu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Developing embodied agents capable of performing complex interactive tasks in real-world scenarios remains a fundamental challenge in embodied AI. Although recent advances in simulation platforms have greatly enhanced task diversity to train embodied Vision Language Models (VLMs), most platforms rely on simplified robot morphologies and bypass the stochastic nature of low-level execution, which limits their transferability to real-world robots. To address these issues, we present a physics-based simulation platform DualTHOR for complex dual-arm humanoid robots, built upon an extended version of AI2-THOR. Our simulator includes real-world robot assets, a task suite for dual-arm collaboration, and inverse kinematics solvers for humanoid robots. We also introduce a contingency mechanism that incorporates potential failures through physics-based low-level execution, bridging the gap to real-world scenarios. Our simulator enables a more comprehensive evaluation of the robustness and generalization of VLMs in household environments. Extensive evaluations reveal that current VLMs struggle with dual-arm coordination and exhibit limited robustness in realistic environments with contingencies, highlighting the importance of using our simulator to develop more capable VLMs for embodied tasks. The code is available at https://github.com/ds199895/DualTHOR.git."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 2 1 0 6 1 . 6 0 5 2 : r DualTHOR: Dual-Arm Humanoid Simulation Platform for Contingency-Aware Planning Boyu Li1,2,3, Siyuan He4, Hang Xu4, Haoqi Yuan5,6, Yu Zang4, Liwei Hu4, Junpeng Yue5, Zhenxiong Jiang4, Pengbo Hu4, Börje F. Karlsson3, Yehui Tang4, Zongqing Lu5,6 1Institute of Automation, Chinese Academy of Sciences 2School of Artificial Intelligence, University of Chinese Academy of Sciences 3Beijing Academy of Artificial Intelligence, 4AgiBot 5School of Computer Science, Peking University, 6BeingBeyond"
        },
        {
            "title": "Abstract",
            "content": "Developing embodied agents capable of performing complex interactive tasks in real-world scenarios remains fundamental challenge in embodied AI. Although recent advances in simulation platforms have greatly enhanced task diversity to train embodied Vision-Language Models (VLMs), most platforms rely on simplified robot morphologies and bypass the stochastic nature of low-level execution, which limits their transferability to real-world robots. To address these issues, we present physics-based simulation platform DualTHOR for complex dual-arm humanoid robots, built upon an extended version of AI2-THOR. Our simulator includes real-world robot assets, task suite for dual-arm collaboration, and inverse kinematics solvers for humanoid robots. We also introduce contingency mechanism that incorporates potential failures through physics-based low-level execution, bridging the gap to real-world scenarios. Our simulator enables more comprehensive evaluation of the robustness and generalization of VLMs in household environments. Extensive evaluations reveal that current VLMs struggle with dual-arm coordination and exhibit limited robustness in realistic environments with contingencies, highlighting the importance of using our simulator to develop more capable VLMs for embodied tasks. The code is available at https://github.com/ds199895/DualTHOR.git."
        },
        {
            "title": "Introduction",
            "content": "The development of embodied intelligent agents [Song et al., 2023, Qian et al., 2025] capable of adapting to diverse environments [Zhao et al., 2024] and performing complex interactive tasks instead of humans [Chen et al., 2025, Yuan et al., 2025] remains central challenge in the current research landscape. In recent years, the enrichment of simulation data for embodied agents, particularly in terms of task diversity and interaction complexity [Deitke et al., 2022, Yang et al., 2024a], has significantly enhanced the foundational capabilities of Vision-Language Models (VLMs). As result, constructing sophisticated simulation environments and collecting diverse datasets have become prominent research directions. Platforms such as AI2-THOR [Kolve et al., 2022], Habitat [Puig et al., 2023], VirtualHome [Puig et al., 2018], and Isaac Gym [Makoviychuk et al., 2021a] have played crucial role in advancing the generation of embodied data, improving scene understanding and task performance for VLM-based agents [Yang et al., 2024b, Li et al., 2024a], and contributing to reducing the sim-to-real gap for real-world deployments [Black et al., 2024, Bu et al., 2025]. Project Lead. Correspondence to Zongqing Lu <zongqing.lu@pku.edu.cn>. Preprint. Under review. Figure 1: Overview of DualTHOR. Our simulator focuses on the H1 and X1 dual-arm humanoid robots, which serve as the primary embodied agents. We develop diverse suite of dual-arm tasks and introduce contingency mechanism during action execution to simulate real-world uncertainties. Building on these capabilities, our simulator provides more challenging and complete benchmark to evaluate high-level planning capabilities for embodied VLMs and systems. Existing simulators have mainly constructed physical simulations in household settings that emphasize the diversity of objects [Deitke et al., 2020], manipulation tasks [Ehsani et al., 2021], and observations [Zhou et al., 2024a]. However, the robotic platforms employed in these simulators are typically limited to wheeled robots and single-arm manipulators. In recent years, humanoid dual-arm robots have emerged as promising direction in robotics research, enabling advances in dual-arm task planning [Wong et al., 2021, Zhou et al., 2024b], whole-body motion control [Fu et al., 2024a, Cheng et al., 2024], and multi-robot collaboration [Mandi et al., 2024]. Compared to single-arm robots, dual-arm robots provide embodied agents with broader range of task capabilities [Sferrazza et al., 2024], facilitating more efficient, fine-grained, and realistic interactions. Including such robot morphologies is thus crucial for the development of more complex and generalizable embodied agents. Motivated by this, we aim to build household simulator for humanoid dual-arm robots. Implementing abundant low-level skills with high success rates, which is necessary to evaluate embodied VLMs in high-level planning [Smith et al., 2024, Fu et al., 2024b], is extremely difficult. Most prior simulation platforms [Kolve et al., 2022, Zhang et al., 2025] bypass this challenge by skipping more physically realistic process of low-level skill execution. For example, they usually employ instantaneous, flash-like transitions [Deitke et al., 2022, Nasiriany et al., 2024] to guarantee that robot and objects transition to success state of the skill. These approaches overlook the continuous process and potential failures of low-level execution and compromise the embodied agents ability to perceive skill progression and evaluate its true performance, which significantly undermines the transferability of VLM-based planning for sim-to-real deployments [Balazadeh et al., 2024, Li et al., 2024b]. To overcome such limitation, we propose to build simulation platform with probabilistic execution to simulate various real-world contingencies and unexpected situations. This maintains reproducibility while incorporating interaction-level contingencies, allowing for errors to occur during robot operations. Following this principle, we can provide more realistic simulator for evaluating and improving task comprehension and planning capabilities of VLMs. We introduce physics-based simulation platform, DualTHOR, to address the existing limitations. Firstly, we employ humanoid dual-arm robots as primary agents and design set of dual-arm tasks in which the two arms can either execute distinct actions in parallel or collaborate to accomplish single complex task, as shown in the right side of Figure 1. This setup enables the collection of more realistic and diverse embodied data. Secondly, we optimize the control logic and introduce dedicated humanoid inverse kinematics (IK) functions to ensure continuous and realistic observation variations throughout the interaction process, encompassing both robots states and environmental transitions. To further simulate real-world uncertainty, we incorporate contingency mechanism that mimics potential execution errors during action operation, as shown in the left side of Figure 1. Finally, we extend the AI2-THOR [Kolve et al., 2022] environment by integrating richer physical 2 simulation details, including object collisions and fluid dynamics (e.g., pouring and filling), thereby reducing the sim-to-real transfer challenges and enhancing scene understanding. Experimental results demonstrate that current VLMs still struggle with dual-arm collaborative tasks, and their ability to re-plan effectively in response to unexpected events remains limited. This highlights the importance of the DualTHOR simulator to develop more capable VLMs for embodied tasks in future research. In summary, our contributions encompass the following key advancements: 1. We propose humanoid dual-arm robot simulator DualTHOR (based on AI2-THOR) and introduce task suite for dual-arm planning. We create new benchmark tailored for household dual-arm tasks, providing standardized evaluation framework for future research. 2. By integrating humanoid IK functions and contingency mechanism, we enable the simulator to realize continuous transition and produce failure cases commonly encountered in real-world robotics, thereby increasing the realism of the simulation platform. 3. Our experimental results show that existing open-source and proprietary models have limited capabilities in planning for dual-arm embodied tasks and exhibit poor robustness, underscoring the potential for future research to develop more capable embodied agents."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Simulation Platforms for Interactive Learning Simulation environments are indispensable for advancing interactive learning in embodied AI, offering controllable settings for agent training and evaluation. AI2-THOR [Kolve et al., 2022] serves as foundational framework, providing high-quality visual environments and interactive object dynamics that support wide range of embodied AI tasks. Extensions such as RoboTHOR [Deitke et al., 2020], ManipulaTHOR [Ehsani et al., 2021], and ProcTHOR [Deitke et al., 2022] have enhanced the simulator by increasing task diversity, scene complexity, and object variety, yet the range of robot embodiments remains limited to single-arm and wheeled robots. HumanoidBench [Sferrazza et al., 2024] and Isaac Gym [Makoviychuk et al., 2021b] focus primarily on training low-level control policies for humanoid robots. Consequently, they do not support background rendering and are not suitable for assessing long-horizon planning capabilities for dual-arm tasks. To provide the necessary simulation capabilities along with wide range of task categories and extended robot morphologies, we introduce our DualTHOR simulator. 2.2 Asynchronous Control for Dual-Arm Humanoid Robot Existing simulation environments fall short in specifically addressing the nuanced motion control requirements of bimanual humanoid robots and often oversimplify the process of motion interaction [Kolve et al., 2022, Li et al., 2023, Zhang et al., 2025]. In robotics manipulation tasks, dual-arm systems are indispensable for executing complex operations that demand high stability and multi-point control. DA-VIL [Karim et al., 2024] employs reinforcement learning to address variable impedance control challenges, and LfDT [Kobayashi et al., 2023] introduces novel imitation learning framework to leverage human demonstrations. HumanPlus [Fu et al., 2024a] indicates that reinforcement learning and imitation learning methods exhibit limited generalization capabilities when training data is scarce. As result, we employ traditional control techniques and incorporate humanoid IK functions to ensure stable continuous action execution. Other techniques for building simulation environments and the differences to prior simulators are discussed in detail in Appendix C."
        },
        {
            "title": "3 DualTHOR",
            "content": "We introduce DualTHOR, simulator built upon AI2-THOR [Kolve et al., 2022] and extended to dual-arm humanoid robots. The environment integrates the Unity physics engine [Haas, 2014], humanoid-specific inverse kinematics (IK) functions [Habekost et al., 2024], and task-reversal capabilities. With these capabilities, we design novel set of tasks tailored specifically for dual-arm robots, significantly increasing the complexity of embodied interactions in household settings. key feature of our platform is its support for continuous interaction processes, as shown in Table 1, distinguishing it from existing simulation platforms that often rely on discrete or simplified transitions. Moreover, to model real-world uncertainty, the environment incorporates stochastic contingency 3 Table 1: systematic comparison of DualTHOR and existing household simulation platforms. Simulator Category Agents Transition Action Control Contingency Household Navigation Household Navigation ThreeDWorld [Gan et al., 2020] iGibson [Li et al., 2021] AI2-THOR [Kolve et al., 2022] RoboThor [Deitke et al., 2020] ManipulaThor [Ehsani et al., 2021] Manipulation Household ProcThor [Deitke et al., 2022] Multi-Domain Habitat [Puig et al., 2023] Multi-Domain OmniGibson [Li et al., 2023] MoMa-Kitchen [Zhang et al., 2025] Manipulation Wheel Wheel Wheel Wheel Single Arm Wheel Single Arm & Wheel Single Arm & Wheel Single Arm Discrete Discrete Discrete Discrete Discrete Discrete Discrete Discrete Discrete Discrete Discrete Discrete Discrete Discrete Discrete Discrete & Continuous Discrete Discrete DualTHOR (ours) Household Dual Arm Continuous Discrete & Continuous mechanism, enabling agents to develop and refine their re-planning abilities in response to execution errors. With these enhancements, DualTHOR provides highly interactive and flexible platform for advancing the development of robust dual-arm embodied systems. 3.1 Overview of DualTHOR Physics Engine. DualTHOR leverages the Unity engine to enable embodied simulation with several key advantages tailored for dual-arm humanoid robotics. First, Unity facilitates parallel execution of bimanual actions [Tabiszewski, 2024], allowing precise coordination to execute complex manipulation tasks. Second, its control framework supports interpolation-based motion execution [Lee, 2024], ensuring smooth and continuous transitions between action steps. This leads to visually coherent interactions, which are particularly important for perception-driven tasks. Moreover, Unity offers high extensibility in both object modeling and scene configuration, enabling the creation of diverse and increasingly complex task scenarios. This flexibility allows for the construction of more realistic household environments. Visual Perception. Similar to AI2-THOR [Kolve et al., 2022], DualTHOR is structured around multiple distinct rooms, as illustrated in Figure 7. Each room contains unique set of objects, enabling the agent to perform diverse range of tasks. Inspired by VirtualHome [Puig et al., 2018], our environment incorporates multi-camera system that provides both firstand third-person perspectives. The first-person cameras are mounted on the robots head and offer up to 360-degree panoramic view to simulate the robots visual perception. The third-person camera suite includes rear, frontal, left lateral views, and right lateral views, which are dynamically adjusted in real time to follow the robots movements. This configuration minimizes occlusion from the robots own limbs and provides enhanced situational awareness. Each camera supports omnidirectional rotation and features 160-degree horizontal field of view, enabling comprehensive, real-time environmental observation. (a) Bedroom (b) Kitchen (c) Living room 1 (d) Living room 2 Figure 2: Example scenes of different rooms in DualTHOR. The types and quantities of objects vary across rooms, and the humanoid robot is capable of interacting with all objects within each room. We re-render all objects based on the AI2-THOR simulator to ensure that object heights are compatible with humanoid robots enabled in the simulation. Humanoid Robots. DualTHOR mainly explores the long-horizon planning and adaptation capabilities for humanoid robots, including pre-defined Unitree H1 [Unitree, 2024a] and Agibot X1 [Agibot, 2024] robots. H1 excels in strength and stability, which is advantageous for tasks involving heavy lifting or high-force interactions, such as opening sealed containers or moving large furniture. 4 X1 is characterized by its high dexterity and precision, making it particularly suitable for tasks that require fine motor control, such as picking up small objects or operating delicate mechanisms. Both robots are equipped with dual-arm configurations, allowing for bimanual manipulation, which is essential for handling complex tasks that require coordinated arms. For manipulation tasks, X1 and H1 have different configurations: X1 uses grippers to interact with objects, while H1 uses dexterous hands. By integrating these two robots, DualTHOR is capable of addressing broad range of manipulation and interaction scenarios. Task Replay Mechanism. In data generation, exploring diverse task trajectories is crucial, particularly in scenarios where trajectories share common initial states but diverge at later stages. Traditional frameworks such as ALFWorld [Shridhar et al., 2020] require launching new simulator instance for each trajectory variation, leading to substantial inefficiencies in resource utilization. To address this limitation, we introduce task history management system featuring Undo and Redo functionalities, which enables efficient state tracking and restoration. This design allows for seamless exploration of alternative task trajectories without fully restarting the simulator, thus providing robust foundation for generating and collecting diverse trajectory data. By supporting rapid iteration and experimentation, our system significantly accelerates the data collection process, while greatly reducing computational resource consumption. 3.2 Task Categories DualTHOR supports comprehensive range of interactive actions, broadly categorized into objectindependent and object-dependent actions. Object-independent actions encompass navigational and perceptual behaviors such as MOVE (forward, backward, left, right), ROTATE (full rotation), OBSERVE (environmental monitoring with configurable gaze angles), and posture adjustments including CROUCH and STAND. These actions facilitate flexible navigation and situational awareness without engaging the robots manipulators. Object-dependent actions such as PICKUP, OPEN, FILL, and TOGGLE enable precise and context-aware object operation. This rich action space provides robust adaptability across wide spectrum of interactive environments and task complexities. With these actions, we categorize tasks into three types based on the number of robotic arms required: dual-arm essential tasks, dual-arm optional tasks, and single-arm tasks. Dual-arm essential tasks are those that necessarily require the involvement of both arms during interaction, such as lifting heavy objects or keeping container open to place an item inside it. Dual-arm optional tasks are those that can be accomplished with single arm, but benefit from dual-arm execution for improved realism and efficiency, for example, grasping multiple distinct items or toggling coffee machine to get cup of coffee. Single-arm tasks, primarily sourced from AI2-THOR, involve simpler interactions such as grasping single object. This categorization enables more nuanced evaluation of dual-arm embodied agents. 3.3 Low-Level Control In the DualTHOR simulation environment, control commands are executed through closed-loop system (enabled by bidirectional communication using JSON over TCP [Safeea and Neto, 2024] between the Python backend and the Unity engine). On the Python side, each invocation of the step function generates an action command that encodes parameters such as the action category, target object ID, and the selected robotic arm. These instructions are serialized into JSON format and transmitted to the Unity engine. Which, upon receipt, parses the data and utilizes the AgentMovement component to interface with the IK system [Habekost et al., 2024], enabling the execution of either singleor dual-arm motions, while simultaneously monitoring for real-time physical collisions. The results of the execution including motion outcome, collision feedback, and updates to the simulation scene are then encapsulated into JSON response and returned to the Python backend, thereby completing the closed-loop control cycle. The IK system in DualTHOR is implemented as cross-platform service accessible via HTTP [Javeed and Mehdi, 2024], ensuring both computational flexibility and modular architecture. When the Python backend sends request containing the target object ID and the specified action type, the Unity engine first computes the interaction point on the object and transforms its coordinates into the base frame of the robot. This information is then submitted to locally hosted IK server, which operates on 5 designated port and solves the IK function, returning the corresponding joint configurations required for task execution. To achieve smooth and continuous motion, Unity applies cubic spline interpolation to the joint angles, enabling the robotic end-effector to follow precise and fluid trajectories toward the intended target. We employ OmniManip [Pan et al., 2025] for pose computation and IK solving to generate accurate joint angles for each robotic arm. This computational framework facilitates the simulation of natural and precise grasping trajectories by also integrating dexterous hand models. The architecture of our simulator supports real-time visualization of complex manipulation tasks, including object grasping, manipulation, and release sequences, while ensuring adherence to physical plausibility and kinematic constraints. The implementation of IK functions varies between the X1 and H1 humanoid robots due to their distinct control architectures. The X1 robot adopts decoupled approach, solving the inverse kinematics independently for each arm using simplified pose matrix (a 3 3 rotation matrix with translation vector) relative to the base coordinate frame, and requires only the initial joint configuration for positioning. In contrast, the H1 robot utilizes whole-body coordination model, simultaneously solving for both arms using complete 4 4 homogeneous transformation matrix that incorporates full posture information. This model integrates current joint angles and velocities to perform dynamic optimization, with particular focus on maintaining balance constraints during dual-arm collaborative operations. 3.4 Contingency Mechanism Existing frameworks, such as AI2-THOR, typically assume deterministic state transitions following action execution, which fails to capture the inherent complexity and uncertainty of real-world interactions. This deterministic assumption introduces substantial distributional gap between simulated environments and the real-world, thereby constraining the robustness and adaptability of trained agents. In scenarios where actions fail or yield unintended consequences, agents often lack the capacity for dynamic re-planning or error recovery, ultimately resulting in task failure. To address this limitation, we propose probabilistic contingency mechanism designed to simulate the stochastic nature of physical environments, thereby enhancing agents robustness and adaptability under uncertainty. Figure 3: Example of picking up pourable\" cup of coffee. The possible results include success (80%), coffee spill (10%), and mug broken (10%). DualTHOR provides both visual observations and environmental feedback after the robot executes an action, enabling the evaluation of the effectiveness of the current plan and the acquisition of information necessary for VLM re-planning. DualTHOR maps actions to multiple potential outcomes based on the current state of objects within the environment. As illustrated in Figure 3, this probabilistic contingency mechanism introduces outcome variability to reflect real-world uncertainties. For example, when robot attempts to pick up \"pourable\" cup, there is an 80% probability of successful grasping it, 10% probability that the cup will break, and 10% probability that its contents will be spilled. These outcomes are modeled using categorical distributions, enabling discrete and mutually exclusive results. For actions involving repetition or multiple sequential steps, the system can be extended to multinomial distributions to capture more complex dependencies and temporal dynamics. This 6 probabilistic framework ensures that the simulation environment more accurately reflects the inherent uncertainties of real-world interactions, thereby providing richer and more informative training data for agents to learn how to recover from failure scenarios. The supported object categories (e.g., pourable) and this contingency mechanism implementation are described in detail in Appendix A. Moreover, actions in our system are constrained by the current state of the objects. For example, an ingredient labeled as Cooked cannot undergo an additional COOK action, as it has already reached its terminal state. However, it may still be classified as pickupable, allowing for continued manipulation even after its state has changed. This state-dependent execution model prevents unrealistic or logically inconsistent action sequences, while supporting multi-state representations that more accurately capture the complexity of real-world interactions. The proposed stochastic contingency mechanism enables agents to learn from unsuccessful attempts and develop effective recovery plans. When an action fails or yields an unintended outcome, the agent is required to reassess the current state and adapt its plan accordingly. Such adaptability is essential for long-horizon tasks, where early-stage errors can propagate and substantially hinder overall task completion."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup We conduct experiments on 356 tasks across 10 distinct room environments, involving 68 unique objects. Distributions of object categories, task categories, and the proportion of each task type are illustrated in Figure 4. Most dual-arm essential tasks and dual-arm optional tasks are composed of multiple single-arm tasks, while smaller portion is manually designed. More details of all tasks are described in Appendix B. (a) Distribution of interactive object types in DualTHOR. (b) Distribution of single-arm interaction tasks in DualTHOR. (c) Distribution of the three task categories in DualTHOR. Figure 4: Distribution of objects and tasks in our experiments. 4.2 Baselines We evaluate three distinct classes of baselines in DualTHOR: proprietary VLMs, open-source VLMs, and prompt-enhanced VLMs. The planning capabilities of these models for dual-arm tasks are assessed mainly by their success rates across all task categories. Proprietary VLMs refer to closed-source systems developed by commercial entities, typically optimized for performance through extensive pretraining on large-scale non-public datasets, and fine-tuned for specific downstream tasks. We select GPT-4o [Hurst et al., 2024] and Gemini 1.5 Pro [Team et al., 2023] as representatives to evaluate the performance of such large-scale models on dual-arm tasks within the DualTHOR environment. Open-source VLMs are publicly available systems, usually developed by the research community, and often trained on more transparent datasets. We select Qwen2.5-VL-7B [Bai et al., 2025] and InternVL2.5-8B [Chen et al., 2024] as representative lightweight models that demonstrate strong performance in embodied scenarios, evaluating their performance in handling dual-arm tasks. Prompt-enhanced VLMs incorporate structured or task-specific prompts to guide model reasoning, enabling improved performance in complex scenarios by aligning model outputs more closely with desired behaviors or contextual requirements. LLM Planner [Song et al., Table 2: Performance of baselines across different task configurations in DualTHOR. Each task includes results for both X1 and H1 robots. The evaluation metric is the success rate over 50 trials for each task. The robot is initialized in the same position within the environment for each trial to ensure the validity of the task-related trajectories for prompt-enhanced VLMs. Dual-Arm Essential Tasks Dual-Arm Optional Tasks Single-Arm Tasks X1 23.56% 25.31% 18.33% 9.71% 28.36% 27.63% 36.54% 27.34% 26.33% 17.54% 15.63% 31.73% 33.51% 41.53% X1 39.53% 37.31% 21.17% 23.51% 43.35% 45.37% 51.25% H1 41.37% 36.23% 19.56% 25.17% 45.69% 47.51% 52.31% H1 51.67% 55.39% 42.37% 43.64% 23.33% 25.31% 23.57% 31.62% 55.43% 55.73% 53.31% 54.26% 55.47% 57.86% GPT-4o Gemini-1.5-Pro Qwen2.5-VL-7B-Ins InternVL2.5-8B LLM-Planner RAP DAG-Plan 2023] and RAP [Kagaya et al., 2024] are representative frameworks that retrieve task-related trajectories for few-shot embodied planning in visually-perceived environments. DAG-Plan [Gao et al., 2024] is the state-of-the-art method for dual-arm planning that utilizes graph-based structure to represent dual-arm tasks. The base model used is GPT-4o. We adopt this approach to assess the difficulties imposed by DualTHOR in benchmarking VLM-based planning with dual-arm tasks. 4.3 Main Results for Dual-Arm Tasks We test all baselines in the three task categories described in Section 3.2. Each task is evaluated on both X1 and H1 robots with 50 trials to ensure statistical reliability. While initial positions vary across different rooms, they are kept consistent within the same room to ensure the reliability of historical trajectories used by prompt-enhanced VLMs. Different baselines are tested under identical experimental settings. For consistency, all models are configured with temperature of 0 and maximum token length of 2048 for response generation. Input images are scaled to resolution of 500 500 pixels. The maximum number of environment steps for high-level planning is set to 50. Further experimental details are provided in Appendix B. Can existing VLMs understand dual-arm tasks? Our experimental results, presented in Table 2, indicate that current methods struggle with long-horizon planning in dual-arm scenarios. Owing to the larger physical structure, the H1 robot possesses broader interaction range than X1, leading to higher average task success rate. Notably, performance on dual-arm essential tasks is consistently lower than performance on other task categories across all baselines, highlighting the limited ability of current VLMs to understand and perform coordinated bimanual control. These tasks require reasoning over the temporal sequencing of object interactions under dual-arm control, as well as effective decision-making regarding arm selection and target positioning. common failure case arises when an object is located in front of the robot, but is unreachable due to arm constraints. For example, sometimes the target object is to the left side of the robot, but the left hand is already occupied based on the VLMs prior plan, and the right hand cannot reach the object. This often necessitates re-planning for revised navigation and manipulation, significantly decreasing task completion within the limited timesteps. These results emphasize the importance of enhancing VLMs understanding of how to properly coordinate dual arms. Does the bimanual structure provide useful information for high-level planning? For dual-arm optional tasks in Table 2, although the overall success rates are lower compared to single-arm tasks, DAG-Plan demonstrates the smallest performance drop among the baselines. This indicates that leveraging dual-arm structural information can meaningfully improve high-level planning ability for VLMs. However, there remains significant space for improvement, as the success rate still lags behind that of single-arm tasks. DualTHOR can provide diverse and comprehensive suite of data and evaluation settings for dual-arm tasks, serving as valuable resource for advancing the development of more capable embodied agents. How effectively can VLMs handle contingency? To assess the re-planning capabilities of the baselines, each task is further divided into three difficulty levels Easy (100% action success rate), Medium (50%), and Hard (20%) with increasing skill failure rates. Experimental results shown in Table 3 reveal that even for single-arm tasks, existing baselines exhibit significant performance drop 8 Table 3: Performance of re-planning across different VLM baselines in DualTHOR. Tasks are categorized into three difficulty levels based on the success rate of low-level skills: Easy (100%), Medium (50%), and Hard (20%). To accomplish such tasks, agents must analyze failure scenarios and re-plan accordingly. For example, they should locate new cup when the original one is broken. Task Category Baseline X1 H1 Easy Medium Hard Easy Medium Hard Dual-Arm Essential Tasks Dual-Arm Optional Tasks Single-Arm Tasks GPT-4o InternVL2.5-8B DAG-Plan GPT-4o 23.56% 20.63% 11.54% 27.34% 21.51% 13.67% 9.71% 3.15% 2.67% 15.63% 36.54% 23.17% 11.69% 41.53% 20.33% 15.53% 6.31% 6.87% InternVL2.5-8B 23.51% 17.39% 39.53% 31.69% 23.34% 41.37% 35.87% 21.19% 7.94% 51.25% 43.13% 27.37% 52.31% 39.63% 23.34% 8.51% 25.17% 19.67% DAG-Plan GPT-4o 51.67% 44.69% 28.31% 55.39% 45.87% 26.69% InternVL2.5-8B 23.57% 20.39% 10.13% 31.62% 23.67% 11.33% 55.47% 52.67% 31.42% 57.86% 48.63% 36.53% DAG-Plan under both Medium and Hard conditions. This suggests that the high-level planning components of current VLMs lack robustness and rely on high success rates of low-level control policies. This issue is particularly critical in real-world deployment of embodied agents: high-level planners must be resilient to execution-level failures and capable of timely recovery to prevent the persistence of unsafe or ineffective behaviors. In our experiments, DAG-Plan responds to unexpected events by constructing new nodes and restarting the planning process from scratch, reflecting limited robustness. This limitation highlights the need for further research in handling dynamic and realistic scenarios. 4.4 Realistic Environment Transitions In addition to the contingency mechanism designed to mimic real-world environmental nondeterminism, DualTHOR further enhances realism by continuously updating the rendering of scenes. As illustrated in Figure 5, the simulator includes dynamic changes such as the gradual filling of sink after turning on the faucet. This feature provides more immersive and realistic environment, enabling VLMs to better comprehend scene transformations and assess whether planned actions are being or have been successfully executed. By incorporating such fine-grained physical interactions, DualTHOR not only improves the fidelity of the simulation, but also challenges VLMs to adapt their action plans in response to continuous environmental changes, thereby advancing their applicability in real-world scenarios. Figure 5: Rendered observations showing sink gradually filling up with water. DualTHOR uses improved physics rendering techniques to provide agents with detailed action effects."
        },
        {
            "title": "5 Conclusion and Limitations",
            "content": "We present DualTHOR, high-fidelity simulation environment designed to enable realistic bimanual humanoid robot interaction. DualTHOR provides novel capabilities like parallel dual-arm control, contingency supported by probabilistic action failures, and an advanced physical engine (including fluid dynamics and robust collision handling). We evaluate several representative embodied VLMs in 9 our simulator. Experimental results demonstrate that current methods struggle with both dual-arm planning and failure recovery. Our simulator surfaces these limitations, establishing challenging and extensible benchmark for the development of more capable embodied AI agents. Flexible asset generation tools and support for broader range of robots are under development, as the current scene diversity in DualTHOR remains limited. Further refinements to support non-gridbased positioning, more controllable failure modes, and multi-room environments are also planned. Additionally, future work should incorporate multi-agent cooperative evaluation."
        },
        {
            "title": "References",
            "content": "Chan Hee Song, Jiaman Wu, Clayton Washington, Brian Sadler, Wei-Lun Chao, and Yu Su. Llm-planner: Few-shot grounded planning for embodied agents with large language models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 29983009, 2023. Haofu Qian, Chenjia Bai, Jiatao Zhang, Fei Wu, Wei Song, and Xuelong Li. Discriminator-guided In The Thirteenth International Conference on Learning embodied planning for llm agent. Representations, 2025. Zhonghan Zhao, Wenhao Chai, Xuan Wang, Boyi Li, Shengyu Hao, Shidong Cao, Tian Ye, and Gaoang Wang. See and think: Embodied agent in virtual environment. In European Conference on Computer Vision, pages 187204. Springer, 2024. Yuhui Chen, Shuai Tian, Shugao Liu, Yingting Zhou, Haoran Li, and Dongbin Zhao. Conrft: reinforced fine-tuning method for vla models via consistency policy. arXiv preprint arXiv:2502.05450, 2025. Haoqi Yuan, Yu Bai, Yuhui Fu, Bohan Zhou, Yicheng Feng, Xinrun Xu, Yi Zhan, Börje F. Karlsson, and Zongqing Lu. Being-0: humanoid robotic agent with vision-language models and modular skills. arXiv preprint arXiv:2503.12533, 2025. Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Kiana Ehsani, Jordi Salvador, Winson Han, Eric Kolve, Aniruddha Kembhavi, and Roozbeh Mottaghi. Procthor: Large-scale embodied ai using procedural generation. Advances in Neural Information Processing Systems, 35:59825994, 2022. Yandan Yang, Baoxiong Jia, Peiyuan Zhi, and Siyuan Huang. Physcene: Physically interactable 3d scene synthesis for embodied ai. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1626216272, 2024a. Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, Aniruddha Kembhavi, Abhinav Gupta, and Ali Farhadi. Ai2-thor: An interactive 3d environment for visual ai, 2022. Xavi Puig, Eric Undersander, Andrew Szot, Mikael Dallaire Cote, Ruslan Partsey, Jimmy Yang, Ruta Desai, Alexander William Clegg, Michal Hlavac, Tiffany Min, Theo Gervet, Vladimir Vondrus, Vincent-Pierre Berges, John Turner, Oleksandr Maksymets, Zsolt Kira, Mrinal Kalakrishnan, Jitendra Malik, Devendra Singh Chaplot, Unnat Jain, Dhruv Batra, Akshara Rai, and Roozbeh Mottaghi. Habitat 3.0: co-habitat for humans, avatars and robots, 2023. Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. Virtualhome: Simulating household activities via programs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, et al. Isaac gym: High performance gpu-based physics simulation for robot learning. arXiv preprint arXiv:2108.10470, 2021a. Yijun Yang, Tianyi Zhou, Kanxue Li, Dapeng Tao, Lusong Li, Li Shen, Xiaodong He, Jing Jiang, and Yuhui Shi. Embodied multi-modal agent trained by an llm from parallel textworld. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2627526285, 2024b. 10 Boyu Li, Haobin Jiang, Ziluo Ding, Xinrun Xu, Haoran Li, Dongbin Zhao, and Zongqing Lu. Selu: Self-learning embodied mllms in unknown environments. arXiv preprint arXiv:2410.03303, 2024a. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: vision-language-action flow model for general robot control. URL https://arxiv. org/abs/2410.24164, 2024. Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xu Huang, Shu Jiang, et al. Agibot world colosseo: large-scale manipulation platform for scalable and intelligent embodied systems. arXiv preprint arXiv:2503.06669, 2025. Matt Deitke, Winson Han, Alvaro Herrasti, Aniruddha Kembhavi, Eric Kolve, Roozbeh Mottaghi, Jordi Salvador, Dustin Schwenk, Eli VanderBilt, Matthew Wallingford, Luca Weihs, Mark Yatskar, and Ali Farhadi. Robothor: An open simulation-to-real embodied ai platform. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 31613171, 2020. Kiana Ehsani, Winson Han, Alvaro Herrasti, Eli VanderBilt, Luca Weihs, Eric Kolve, Aniruddha Kembhavi, and Roozbeh Mottaghi. Manipulathor: framework for visual object manipulation. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4495 4504, 2021. Xian Zhou, Yiling Qiao, Zhenjia Xu, TH Wang, Chen, Zheng, Xiong, Wang, Zhang, Ma, et al. Genesis: generative and universal physics engine for robotics and beyond. Genesis, 2024a. Ching-Chang Wong, Shao-Yu Chien, Hsuan-Ming Feng, and Hisasuki Aoyama. Motion planning for dual-arm robot based on soft actor-critic. IEEE Access, 2021. Bohan Zhou, Haoqi Yuan, Yuhui Fu, and Zongqing Lu. Learning diverse bimanual dexterous manipulation skills from human demonstrations. arXiv preprint arXiv:2410.02477, 2024b. Zipeng Fu, Qingqing Zhao, Qi Wu, Gordon Wetzstein, and Chelsea Finn. Humanplus: Humanoid shadowing and imitation from humans. arXiv preprint arXiv:2406.10454, 2024a. Xuxin Cheng, Yandong Ji, Junming Chen, Ruihan Yang, Ge Yang, and Xiaolong Wang. Expressive whole-body control for humanoid robots. CoRR, 2024. Zhao Mandi, Shreeya Jain, and Shuran Song. Roco: Dialectic multi-robot collaboration with large language models. In 2024 IEEE International Conference on Robotics and Automation (ICRA), 2024. Carmelo Sferrazza, Dun-Ming Huang, Xingyu Lin, Youngwoon Lee, and Pieter Abbeel. Humanoidbench: Simulated humanoid benchmark for whole-body locomotion and manipulation. arXiv preprint arXiv:2403.10506, 2024. Laura Smith, Alex Irpan, Montserrat Gonzalez Arenas, Sean Kirmani, Dmitry Kalashnikov, Dhruv Shah, and Ted Xiao. Steer: Bridging vlms and low-level control for adaptable robotic manipulation. In CoRL 2024 Workshop on Mastering Robot Manipulation in World of Abundant Data, 2024. Xian Fu, Min Zhang, Peilong Han, Hao Zhang, Lei Shi, Hongyao Tang, et al. What can vlms do for zero-shot embodied task planning? In ICML 2024 Workshop on LLMs and Cognition, 2024b. Pingrui Zhang, Xianqiang Gao, Yuhan Wu, Kehui Liu, Dong Wang, Zhigang Wang, Bin Zhao, Yan Ding, and Xuelong Li. Moma-kitchen: 100k+ benchmark for affordance-grounded last-mile navigation in mobile manipulation, 2025. URL https://arxiv.org/abs/2503.11081. Soroush Nasiriany, Abhiram Maddukuri, Lance Zhang, Adeet Parikh, Aaron Lo, Abhishek Joshi, Ajay Mandlekar, and Yuke Zhu. Robocasa: Large-scale simulation of everyday tasks for generalist robots. arXiv preprint arXiv:2406.02523, 2024. Vahid Balazadeh, Mohammadmehdi Ataei, Hyunmin Cheong, Amir Hosein Khasahmadi, and Rahul Krishnan. Synthetic vision: Training vision-language models to understand physics. arXiv preprint arXiv:2412.08619, 2024. Heng Li, Minghan Li, Zhi-Qi Cheng, Yifei Dong, Yuxuan Zhou, Jun-Yan He, Qi Dai, Teruko Mitamura, and Alexander Hauptmann. Human-aware vision-and-language navigation: Bridging simulation to reality with dynamic human interactions. Advances in Neural Information Processing Systems, 37:119411119442, 2024b. Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, et al. Isaac gym: High performance gpu-based physics simulation for robot learning. arXiv preprint arXiv:2108.10470, 2021b. Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto MartínMartín, Chen Wang, Gabrael Levine, Michael Lingelbach, Jiankai Sun, et al. Behavior-1k: benchmark for embodied ai with 1,000 everyday activities and realistic simulation. In Conference on Robot Learning, pages 8093. PMLR, 2023. Md Faizal Karim, Shreya Bollimuntha, Mohammed Saad Hashmi, Autrio Das, Gaurav Singh, Srinath Sridhar, Arun Kumar Singh, Nagamanikandan Govindan, and Madhava Krishna. Da-vil: Adaptive dual-arm manipulation with reinforcement learning and variable impedance control, 2024. URL https://arxiv.org/abs/2410.19712. Masato Kobayashi, Yamada Jun, Masashi Hamaya, and Kazutoshi Tanaka. Lfdt: Learning dual-arm manipulation from demonstration translated from human and robotic arm. In The IEEE-RAS International Conference on Humanoid Robots (Humanoids), 2023. John Haas. history of the unity game engine. 2014. Jan-Gerrit Habekost, Connor Gäde, Philipp Allgeuer, and Stefan Wermter. Inverse kinematics for neuro-robotic grasping with humanoid embodied agents. In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 73157322. IEEE, 2024. Marek Tabiszewski. comparative analysis of transitions generated using the unity game development platform. Journal of Computer Sciences Institute, 30:4752, 2024. Taegyu Lee. Unity engine dissection: Improvement points in comparison between unity engine and open-source engines. Asia-pacific Journal of Convergent Research Interchange (APJCRI), pages 449458, 2024. Chuang Gan, Jeremy Schwartz, Seth Alter, Damian Mrowca, Martin Schrimpf, James Traer, Julian De Freitas, Jonas Kubilius, Abhishek Bhandwaldar, Nick Haber, et al. Threedworld: platform for interactive multi-modal physical simulation. arXiv preprint arXiv:2007.04954, 2020. Chengshu Li, Fei Xia, Roberto Martín-Martín, Michael Lingelbach, Sanjana Srivastava, Bokui Shen, Kent Vainio, Cem Gokmen, Gokul Dharan, Tanish Jain, et al. igibson 2.0: Object-centric simulation for robot learning of everyday household tasks. arXiv preprint arXiv:2108.03272, 2021. Unitree. H1 humanoid robot. https://www.unitree.com/h1, 2024a. Accessed: 2025-05-14. Agibot. X1 humanoid robot. https://www.agibot.com/products/X1, 2024. Accessed: 202505-14. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020. Safeea and Neto. Model-based hardware in the loop control of collaborative robots: Simulink and python based interfaces. International Journal of Computer Integrated Manufacturing, 37(9): 10431055, 2024. Yasir Javeed and Hafiz Muhammad Mehdi. Iort enabled gesture mimicking robotic hand based on computer vision. In 2024 International Conference on Emerging Trends in Smart Technologies (ICETST), pages 16. IEEE, 2024. Mingjie Pan, Jiyao Zhang, Tianshu Wu, Yinghao Zhao, Wenlong Gao, and Hao Dong. Omnimanip: Towards general robotic manipulation via object-centric interaction primitives as spatial constraints, 2025. URL https://arxiv.org/abs/2501.03841. 12 Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. Tomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou, Jayashree Karlekar, Sugiri Pranata, Akira Kinose, Koki Oguri, Felix Wick, and Yang You. Rap: Retrieval-augmented planning with contextual memory for multimodal llm agents. In NeurIPS Workshop on Open-World Agents, 2024. Zeyu Gao, Yao Mu, Jinye Qu, Mengkang Hu, Lingyue Guo, Ping Luo, and Yanfeng Lu. Dag-plan: Generating directed acyclic dependency graphs for dual-arm cooperative planning, 2024. URL https://arxiv.org/abs/2406.09953. Unitree. G1 humanoid robot. https://www.unitree.com/g1, 2024b. Accessed: 2025-05-14. Noah Snavely, Steven Seitz, and Richard Szeliski. Photo tourism: exploring photo collections in 3d. In ACM siggraph 2006 papers, pages 835846. 2006. Yasutaka Furukawa and Jean Ponce. Accurate, dense, and robust multiview stereopsis. IEEE transactions on pattern analysis and machine intelligence, 32(8):13621376, 2009. David Lowe. Distinctive image features from scale-invariant keypoints. International journal of computer vision, 60:91110, 2004. Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary Bradski. Orb: An efficient alternative to sift or surf. In 2011 International conference on computer vision, pages 25642571. Ieee, 2011. Shahram Izadi, David Kim, Otmar Hilliges, David Molyneaux, Richard Newcombe, Pushmeet Kohli, Jamie Shotton, Steve Hodges, Dustin Freeman, Andrew Davison, et al. Kinectfusion: real-time 3d reconstruction and interaction using moving depth camera. In Proceedings of the 24th annual ACM symposium on User interface software and technology, pages 559568, 2011. Stefan Leutenegger, Simon Lynen, Michael Bosse, Roland Siegwart, and Paul Furgale. Keyframebased visualinertial odometry using nonlinear optimization. The International Journal of Robotics Research, 34(3):314334, 2015. Michael Bloesch, Sammy Omari, Marco Hutter, and Roland Siegwart. Robust visual inertial odometry using direct ekf-based approach. In 2015 IEEE/RSJ international conference on intelligent robots and systems (IROS), pages 298304. IEEE, 2015. Carlos Campos, Richard Elvira, Juan Gómez Rodríguez, José MM Montiel, and Juan Tardós. Orb-slam3: An accurate open-source library for visual, visualinertial, and multimap slam. IEEE transactions on robotics, 37(6):18741890, 2021. Tristan Laidlow, Michael Bloesch, Wenbin Li, and Stefan Leutenegger. Dense rgb-d-inertial slam with map deformations. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 67416748. IEEE, 2017. Antoni Rosinol, Marcus Abate, Yun Chang, and Luca Carlone. Kimera: an open-source library for real-time metric-semantic localization and mapping. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 16891696. IEEE, 2020. Xiaohan Fei and Stefano Soatto. Visual-inertial object detection and mapping. In Proceedings of the European conference on computer vision (ECCV), pages 301317, 2018. 13 Yifei Ren, Binbin Xu, Christopher Choi, and Stefan Leutenegger. Visual-inertial multi-instance dynamic slam with object-level relocalisation. In 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1105511062. IEEE, 2022. Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis, 2020. URL https://arxiv.org/abs/2003.08934. Thomas Müller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM transactions on graphics (TOG), 41(4): 115, 2022. Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. Jad Abou-Chakra, Krishan Rana, Feras Dayoub, and Niko Sünderhauf. Physically embodied gaussian splatting: realtime correctable world model for robotics. arXiv preprint arXiv:2406.10788, 2024. Mazeyu Ji, Ri-Zhao Qiu, Xueyan Zou, and Xiaolong Wang. Graspsplats: Efficient manipulation with 3d feature splatting. arXiv preprint arXiv:2409.02084, 2024. Ivan Kapelyukh, Yifei Ren, Ignacio Alzugaray, and Edward Johns. Dream2Real: Zero-shot 3D object rearrangement with vision-language models. In IEEE International Conference on Robotics and Automation (ICRA), 2024. Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF international conference on computer vision, pages 92989309, 2023a. Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. arXiv preprint arXiv:2309.03453, 2023b. Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 99709980, 2024. Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and Andrea Vedaldi. Realfusion: 360deg reconstruction of any object from single image. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 84468455, 2023. Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110, 2023. Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. Sv3d: Novel multi-view synthesis and 3d generation from single image using latent video diffusion. In European Conference on Computer Vision, pages 439457. Springer, 2025. Xin Kong, Shikun Liu, Xiaoyang Lyu, Marwan Taher, Xiaojuan Qi, and Andrew Davison. Eschernet: generative model for scalable view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95039513, 2024. Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel Watson, Pratul Srinivasan, Dor Verbin, Jonathan Barron, Ben Poole, et al. Reconfusion: 3d reconstruction with diffusion priors. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2155121561, 2024. Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 45784587, 2021. 14 Zehao Zhu, Zhiwen Fan, Yifan Jiang, and Zhangyang Wang. Fsgs: Real-time few-shot view synthesis using gaussian splatting. In European conference on computer vision, pages 145163. Springer, 2024. ="
        },
        {
            "title": "A Simulator Details",
            "content": "This section details the different capability areas of DualTHOR and its implementation details. A.1 Visual Input DualTHOR is high-fidelity embodied AI simulation environment that supports advanced visual perception capabilities. It enables multi-view visual input by allowing agents to access and integrate observations from multiple camera perspectives within the same scene. This multi-view configuration facilitates comprehensive spatial understanding and enhances the agents ability to reason about occluded or partially observed regions. In addition to RGB input, DualTHOR provides depth images aligned with each visual frame, offering rich geometric information essential for tasks such as 3D scene reconstruction, object localization, and affordance estimation. The combination of multi-angle RGB views and depth sensing positions DualTHOR as versatile platform for developing and evaluating vision-based embodied AI models. (a) Front Camera Observation (b) Left Camera Observation (c) Right Camera Observation (d) Backwards Camera Observation Figure 6: Multi-view visual observations in the DualTHOR simulation environment. The agent captures RGB images from four distinct egocentric perspectives: (a) first person front-facing camera, (b) left-side camera, (c) right-side camera, and (d) backwards-facing camera. This multi-view configuration enables comprehensive scene understanding by providing complementary viewpoints that facilitate perception of object relationships, spatial context, and occluded regions. A.2 Humanoid Robots DualTHOR provides two humanoid robot embodiments, Unitree H1 [Unitree, 2024a] and Agibot X1 [Agibot, 2024]. Additional humanoid agents such as Unitree G1 [Unitree, 2024b] are currently under implementation, and DualTHOR is easily extendable to include additional robots. Both H1 and X1 are equipped with dual-arm configurations capable of coordinated manipulation. To support continuous control, we introduce dedicated inverse kinematics (IK) functions for each agent, along with optimized control logic, ensuring smooth action execution during the interaction process. 16 (a) H1 robot (b) X1 robot (c) Dual-arm lift action with H1 (d) Concurrent pick up actions with X1 Figure 7: The two humanoid robots initially supported in DualTHOR. (a) H1 robot, larger humanoid platform with wider interaction range, suitable for complex manipulation tasks. (b) X1 robot, more compact humanoid agent designed for efficient interaction and navigation in constrained environments. (c) (d) Both robots are equipped with dual-arm configurations to enable coordinated bimanual manipulation (e.g., lift coffee machine or pick up two objects at the same time). The H1 robot has larger overall structure compared to X1, and thus also possesses broader interaction range. This partly explains why H1 demonstrates superior performance in Tables 2 and 3, as object localization and navigation tasks can become relatively easier due to its extended reach and physical capabilities. A. Interactive Objects The objects initially supported in DualTHOR are listed in Table 4 along with their properties. These objects are primarily derived from the Unity assets provided by AI2-THOR (for matching environment characteristics and facilitate scenario comparisons). To accommodate the interaction ranges of H1 and X1, all objects are re-rendered to ensure physical plausibility. Additional objects can easily be added to DualTHOR. Table 4: Overview of object distribution in DualTHOR The same object type may appear multiple times within the same Scene. Actionable properties specify the set of permissible interactions for each object. State reflects an objects current situation in the environment, supporting the implementation of contingency mechanisms. Object Type AlarmClock Apple BaseballBat Book Bottle Bowl Bread Cabinet Candle Coat CoffeeMachine Scene(s) Bedroom Kitchen Bedroom Living Room Kitchen Kitchen Kitchen Kitchen/Bedroom Living Room Living Room Kitchen Actionable Properties Pickupable Pickupable, Sliceable Pickupable Pickupable, Openable Pickupable Pickupable Pickupable, Sliceable Openable Pickupable, Toggleable Pickupable Toggleable, Movable States IsPickedUp IsPickedUp, IsSliced IsPickedUp IsPickedUp, IsOpen IsPickedUp, IsFilled IsPickedUp, IsFilled IsPicked Up, IsSliced, IsCooked IsOpen IsPickedUp, IsToggledOn/Off IsPickedUp IsToggledOn/Off, IsLifted 17 Object Type Cup Curtains DeskLamp Drawer Egg Faucet Fork Fridge Knife Laptop Lettuce LightSwitch Microwave Mug Onion Orange Pan Pant Pen Pencil PepperShaker Picture Pillow Pizza Plant Plate Plunger Pot RemoteControl SaltShaker SoapBar SoapBottle Shirt StoveKnob Television Toaster ToiletPaper Tomato Towel Watch WineBottle Window Actionable Properties Pickupable Openable Openable Pickupable Toggleable Pickupable Openable Pickupable Pickupable, Toggleable Pickupable, Sliceable Scene(s) Kitchen Kitchen Bedroom/Living Room Toggleable Kitchen Kitchen Kitchen/Bathroom Kitchen Kitchen Kitchen Living Room Kitchen Bedroom/Living Room Toggleable Openable, Toggleable Kitchen Pickupable, Fillable Kitchen Pickupable, Sliceable Kitchen Pickupable, Sliceable Kitchen Pickupable Kitchen Living Room Pickupable Living Room/Bedroom Pickupable Living Room/Bedroom Pickupable Pickupable Kitchen Living Room/Bedroom Pickupable Bedroom/Living Room Pickupable Kitchen All Kitchen Bathroom Kitchen Living Room Kitchen Bathroom Bathroom/Kitchen Living Room Kitchen Living Room Kitchen Bathroom Kitchen Bathroom Bedroom Kitchen All Pickupable, Sliceable Pickupable Pickupable Pickupable Pickupable Pickupable Pickupable Pickupable Pickupable Pickupable Toggleable Toggleable, Movable Toggleable, Movable Pickupable Pickupable, Sliceable Pickupable Pickupable Pickupable Openable States IsPickedUp, IsFilled IsOpen IsToggledOn/Off IsOpen IsPickedUp, IsCooked IsToggledOn/Off IsPickedUp IsOpen IsPickedUp IsPickedUp, IsToggledOn/Off IsPickedUp, IsSliced IsToggledOn/Off IsOpen, IsToggledOn/Off IsPickedUp, IsFilled IsPickedUp, IsSliced IsPickedUp, IsSliced IsPickedUp IsPickedUp IsPickedUp IsPickedUp IsPickedUp IsPickedUp IsPickedUp IsPickedUp, IsSliced IsPickedUp IsPickedUp IsPickedUp IsPickedUp, IsFilled IsPickedUp IsPickedUp IsPickedUp IsPickedUp, IsFilled IsPickedUp IsToggledOn/Off IsToggledOn/Off, IsLifted IsToggledOn/Off, IsLifted IsPickedUp, IsUsedUp IsPickedUp, IsSliced, IsCooked IsPickedUp IsPickedUp IsPickedUp, IsFilled IsOpen A.4 Action Set With the optimization of control logic for both H1 and X1 in DualTHOR, alongside the integration of corresponding IK models, our simulator supports both continuous and discrete control for the humanoid robots. To facilitate the evaluation of VLMs high-level planning, we design set of language-based actions/skills for dual-arm humanoid agents, drawing inspiration from the AI2THOR framework. The specific details are provided in Table 5. 18 Table 5: DualTHORs Action Set. For object-dependent actions (pick, toggle, etc.), DualTHOR supports parallel execution of actions to demonstrate the advantages of dual-arm operation. Action Type Command Format Description and Parameters MoveAhead, MoveBack, MoveLeft, MoveRight RotateLeft, RotateRight Pick Lift Place (MoveXXX, Magnitude) (RotateXXX, Magnitude) (pick, arm, objectID) (lift, objectID) (place, arm, objectID, containerID) Toggle (toggle, arm, objectID) Open (open, arm, objectID) Teleport (tp, objectID) Undo / Redo (undo), (redo) LoadState loadstate A.5 DualTHOR Tasks Move the robot in given direction. Parameter Magnitude indicates the distance of movement. e.g., (MoveAhead, Magnitude=1) Rotate the robot by given angle. Parameter Magnitude = 1 corresponds to 90. e.g., (RotateRight Magnitude=1) Grab the object specified by objectID using the specified arm. e.g., (pick, arm=left, objectID=Kitchen_Cup_01). Lift the object specified by objectID using both arms. e.g., (lift, objectID=CoffeeMachine_01). Place the currently held object into the target container using the specified arm. e.g., (place, arm=right, objectID=Kitchen_Mug_01, containerID=Kitchen_CoffeeMachine_01). Toggle objects such as switches using the specified arm. e.g., (toggle, arm=left, objectID=Bedroom_LightSwitch_01). Open interactable objects such as cabinets using the specified arm. e.g., (open, arm=right, objectID=Bedroom_Cabinet_01). Teleport the robot near the specified object, preparing it for subsequent interaction. e.g., (tp, objectID=Living Room_Window_01). Undo reverts to the last state of the simulator, and Redo re-executes the last action again from the last state. e.g., (undo), (redo). Load the state feedback from the current environment, including robot states (action result, arm state, etc.) and object states (open, picked, etc.). e.g., (loadstate). The DualTHOR simulator introduces new task set in three categories: dual-arm essential tasks, dual-arm optional tasks, and single-arm tasks. The specific tasks are shown in Table 6. Distinctions between dual-arm optional and dual-arm essential tasks depend heavily on specific characteristics of the interactable objects being supported in the simulator. For example, when filling an object with water, the task category is primarily determined by the presence or absence of supporting platform where to place the object to be filled. In cases like operating faucet to fill an object, where no suitable surface is available to temporarily hold the object, it becomes necessary to concurrently execute the holding and toggling (named in the table as \"Hold objects filled with liquid\"). This simultaneous coordination inherently demands the use of both arms. Conversely, for tasks involving devices such as coffee machines, where the object to be filled can typically be placed in position beforehand, the toggling operation to be carried out independently. As result, such tasks are considered as dual-arm optional tasks, i.e., they could be executed separately in sequence with one arm, but are more naturally executed with two arms. 19 A.6 Contingency Mechanism Implementation Details Currently, DualTHOR introduces contingency exclusively for object-dependent actions, as discussed in Section 3.2. Object-independent actions are not subject to probabilistic outcomes at this stage. During action execution, DualTHOR utilizes probabilistic models to predict the outcome based on the current state of the interactive objects. When the robot arm approaches the object, inverse kinematics (IK) is computed as usual; once the distance to the object falls below predefined threshold, the simulator renders the predicted outcome. The types of contingencies are defined in accordance with the objects current state, as summarized in the Table 7. Their probabilities can be designed by users to satisfy any requirement for re-plan capability. This mechanism allows DualTHOR to simulate realistic outcome variability. Task Category Single-Arm Tasks Dual-Arm Optional Tasks Dual-Arm Essential Tasks Table 6: Tasks in DualTHOR. Task Name Pick objects Toggle objects Open objects Fill objects with liquid Use up objects Slice objects Cook objects Pick different objects and place in different containers Pick different objects and place in the same container Open general containers and pick/place objects Pick and slice the same object Pick objects filled with liquid Open affordance-specific containers and pick/place objects Lift objects Hold objects filled with liquid Task Counts 20 12 12 6 4 2 2 63 27 15 9 97 39 32 Table 7: Contignency setup for different states. When an object have multiple states, the contingency mechanism randomly selects an outcome based on the full set of possible contingencies. State IsPickedUp IsFilled IsSliced IsLifted IsOpen IsToggledOn/Off Contingency Outcomes success, broken, nothing happens success, liquid spill, nothing happens success, partial slice, nothing happens success, broken, nothing happens success, locked, half open, nothing happens success, stuck, nothing happens"
        },
        {
            "title": "B Experiment Details",
            "content": "B.1 Vision Language Models Table 8 lists the versions or full names of the models used in our experiments. We accessed proprietary models through API calls and open-source models via local deployment. 20 Table 8: Full names of VLMs used in our experiments. Model Name GPT-4o Gemini-1.5-Pro InternVL2.5-8B Qwen2-VL-7B-Ins Qwen Full Name Creator gpt-4o-2024-08-06 OpenAI Google gemini-1.5-pro OpenGVLab OpenGVLab/InternVL2.5-8B Qwen/Qwen2-VL-7B-Instruct B.2 Experiment Parameters As mentioned in Section 4.3, all models are configured with temperature of 0 and maximum token length of 2048 for response generation. Input images are scaled to resolution of 500 500 pixels. The maximum number of environment steps for high-level planning is set to 50. For prompt-enhanced VLMs, retrieval parameters follow the official implementation to ensure consistency. The number of retrieved trajectories is fixed at the minimum value of 3, as adopted in RAP [Kagaya et al., 2024], to facilitate fair comparison across all baselines. The list of parameters is provided in Table 9. Table 9: Parameter List for Experiments. Proprietary/Open-source VLMs Parameter Response generation temperature Maximum token length Input image resolution Maximum environment steps Value 0 2048 500 500 pixels 50 Prompt-enhanced VLMs Parameter Retrieve Buffer KNN Retrieves(LLM Planner [Song et al., 2023]) Multimodal Retrieves(RAP [Kagaya et al., 2024]) Retrieve Trajectories Input Number of Candidate Nodes [Gao et al., 2024] Value 20 9 5 3 6 (3 for each arm) B.3 Computer Resources We conduct development and experimental evaluation of the DualTHOR simulation environment on workstation equipped with an NVIDIA RTX 4090 GPU (40 GB). The environment supports seamless interaction between Windows and Linux systems, facilitating cross-platform compatibility for both development and deployment. B.4 Experiment Results Showcase Continuous Action Control Showcase. As in the experiments in Section 4.4, DualTHOR employs an optimized control logic and integrates humanoid inverse kinematics (IK) solvers to enable continuous transitions of robot actions, as shown in Figure 8. This is in contrast to the flash-like transitions commonly seen in previous simulation platforms, i.e., DualTHOR provides more detailed data for VLMs to understand robotic actions. Plan Failure Showcase. Failure cases, as illustrated in Figure 9, highlight several challenges that VLMs face in dual-arm task planning. Subfigures 16 depict the object search process, task extensively addressed in prior work and well-supported by existing simulators. Subfigures 7 and 8, however, reveal limitations in the VLMs ability to perform effective pre-planning for dual-arm robots. While subfigure 7 illustrates successful configuration in which both arms can simultaneously interact with two objects, subfigure 8 demonstrates failure, as such coordination is no longer possible. 21 Figure 8: Showcase of continuous action control in the X1 humanoid robot. In DualTHOR all action executions involve continuous transitions rather than flash-like changes, providing abundant data for VLMs to analyze scene and actions. Figure 9: Plan failure showcase by DAG-Plan [Gao et al., 2024]. This example clearly highlights the current limitations of existing VLMs in understanding bimanual configurations and the spatial relationships between objects. In the final subfigure, the plan fails to consider the interactive reachability of the left arm, underscoring the need for re-planning based on arm interaction ranges or the pre-assignment of objects to each arm. DualTHOR aims to emphasize these limitations for dual-arm embodied VLMs. DAG-Plan still generates plan to move from the configuration in subfigure 7 to that in subfigure 8. possible explanation is that existing embodied datasets primarily consist of sequentially executed task instructions. Consequently, VLMs trained on such data tend to overemphasize the current interaction target, often neglecting the spatial and temporal requirements of subsequent actions involving future objects. Subfigure 9 further underscores the VLMs limited understanding of dual-arm morphology. Despite the right arm already being occupied, the VLM continues to attempt interactions with it, indicating lack of structural reasoning in the planning process. This reflects broader limitation stemming from the insufficient availability of data that captures dual-arm configurations and constraints. Ideally, the model should either place the object held in the right hand before attempting to open the drawer 22 or reposition the robot (move right) to enable the left arm to perform the action. This failure case highlights the necessity of developing the DualTHOR simulator and constructing dual-arm task suite. Successful Re-Plan Showcase including Contingency. Figure 10 presents successful planning case under the influence of contingency, highlighting two key challenges for robot planning. The first challenge involves overcoming the effects of contingency mechanisms by enabling the VLM to re-plan in response to failed actions. As illustrated in subfigures 13, when contingency event such as failed pickup (nothing happens in Table 7) occurs, the VLM is expected to respond with an appropriate recovery action, such as reattempting the pickup action. While this represents relatively simple scenario, more complex contingenciessuch as object brokenrequire re-navigation and re-identification of an equivalent object. These cases demand significantly stronger re-planning capabilities, which current VLMs struggle to achieve within limited environment steps. The second challenge involves dynamically adjusting the robots interaction position based on environmental feedback and visual input. In subfigures 45, after an initial failure attempt to open drawer, the VLM infers from feedback that the robot is too far from the target and accordingly modifies the plan to move closer, ultimately completing the task. These challenges emphasize the need for robust planning capabilities in embodied VLMs for reducing the deployment difficulties in real-world applications. Figure 10: Showcase of successful plan by DAG-Plan [Gao et al., 2024] with contingency mechanism. Subfigures 13 show the necessity to recover from basic contingency scenario. Subfigures 46 illustrate the importance of adjusting the robots position to successfully complete the interaction. This example highlights key challenges in robust planning and adaptive behavior required for realworld deployment of embodied VLMs."
        },
        {
            "title": "C Additional Related Work",
            "content": "C.1 Simulation Platforms for Interactive Learning Simulation environments are indispensable for advancing interactive learning in embodied AI, offering controllable settings for agent training and evaluation. AI2-THOR [Kolve et al., 2022] serves as foundational framework, providing high-quality visual environments and interactive object dynamics that support wide range of embodied AI tasks. RoboTHOR [Deitke et al., 2020] extends AI2-THOR by introducing interconnected environments with topological floorplan structure, enabling research on long-horizon embodied navigation across multiple rooms. ManipulaTHOR [Ehsani et al., 2021] focuses on robotic manipulation tasks, particularly with articulated robotic arms, while ProcTHOR addresses scalability and diversity through procedurally generated environments, ensuring agents are trained on varied layouts, objects, and lighting conditions to improve generalization [Deitke et al., 2022]. These extensions have enhanced the simulator by increasing task diversity, scene complexity, and object variety, yet the range of robot embodiments remains limited to single-arm and wheeled robots. iGibson [Li et al., 2021] is another widely used physics-rich simulation environment 23 reconstructed from real-world scans and built on the PyBullet engine, but it is also limited to wheeled robot embodiments. Although HumanoidBench [Sferrazza et al., 2024] and Isaac Gym [Makoviychuk et al., 2021b] provide humanoid robot control frameworks, they only focus primarily on training low-level control policies for humanoid robots. Consequently, they do not support background rendering and are not suitable for assessing long-horizon planning capabilities for dual-arm tasks. To provide the necessary simulation capabilities along with wide range of task categories and extended robot morphologies, we introduce our DualTHOR simulator. C.2 Visual Rendering Technology in Embodied Simulators Visual environments span spectrum from simplistic, abstract representations to highly detailed, photorealistic 3D models. Achieving accurate 3D reconstruction for sim-to-real transfer remains persistent challenge, traditionally addressed through multi-view geometry techniques [Snavely et al., 2006, Furukawa and Ponce, 2009] that rely on feature matching [Lowe, 2004, Rublee et al., 2011]. While effective under optimal lighting and texture conditions, these methods struggle with low-texture surfaces, transparent materials, and dynamic scenes. To enhance robustness, approaches such as [Izadi et al., 2011, Leutenegger et al., 2015, Bloesch et al., 2015, Campos et al., 2021, Laidlow et al., 2017, Rosinol et al., 2020, Fei and Soatto, 2018, Ren et al., 2022] integrate depth sensors, inertial measurement units (IMUs), or object-level anchors. Recent advancements, including Neural Radiance Fields (NeRFs) [Mildenhall et al., 2020, Müller et al., 2022] and 3D Gaussian Splatting [Kerbl et al., 2023], have enabled photorealistic novel view synthesis and real-time rendering, significantly benefiting downstream robotic tasks [Abou-Chakra et al., 2024, Ji et al., 2024, Kapelyukh et al., 2024]. However, these methods require dense scene coverage, limiting their effectiveness with sparse views. Generative models [Liu et al., 2023a,b, Long et al., 2024, Melas-Kyriazi et al., 2023, Shi et al., 2023, Voleti et al., 2025, Kong et al., 2024] address this limitation by synthesizing plausible views from limited input, while scene-level generalization techniques [Wu et al., 2024, Yu et al., 2021, Zhu et al., 2024] leverage prior knowledge to accelerate reconstruction. Nevertheless, these algorithms still face challenges in generating complex objects with movable hinges and are prone to issues such as distortion and rough edges."
        }
    ],
    "affiliations": [
        "AgiBot",
        "Beijing Academy of Artificial Intelligence",
        "BeingBeyond",
        "Institute of Automation, Chinese Academy of Sciences",
        "School of Artificial Intelligence, University of Chinese Academy of Sciences",
        "School of Computer Science, Peking University"
    ]
}