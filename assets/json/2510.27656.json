{
    "paper_title": "RDMA Point-to-Point Communication for LLM Systems",
    "authors": [
        "Nandor Licker",
        "Kevin Hu",
        "Vladimir Zaytsev",
        "Lequn Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Emerging Large Language Model (LLM) system patterns, such as disaggregated inference, Mixture-of-Experts (MoE) routing, and asynchronous reinforcement fine-tuning, require flexible point-to-point communication beyond simple collectives. Existing implementations are locked to specific Network Interface Controllers (NICs), hindering integration into inference engines and portability across hardware providers. We present TransferEngine, which bridges the functionality of common NICs to expose a uniform interface. TransferEngine exposes one-sided WriteImm operations with a ImmCounter primitive for completion notification, without ordering assumptions of network transport, transparently managing multiple NICs per GPU. We demonstrate peak throughput of 400 Gbps on both NVIDIA ConnectX-7 and AWS Elastic Fabric Adapter (EFA). We showcase TransferEngine through three production systems: (1) KvCache transfer for disaggregated inference with dynamic scaling, (2) RL weight updates achieving 1.3 seconds for trillion-parameter models, and (3) MoE dispatch/combine implementation exceeding DeepEP decode latency on ConnectX-7, with the first viable latencies on EFA. We demonstrate that our portable point-to-point communication complements collectives while avoiding lock-in."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 3 ] . [ 1 6 5 6 7 2 . 0 1 5 2 : r RDMA POINT-TO-POINT COMMUNICATION FOR LLM SYSTEMS Nandor Licker * 1 Kevin Hu 1 Vladimir Zaytsev 1 Lequn Chen * 1 ABSTRACT Emerging Large Language Model (LLM) system patterns, such as disaggregated inference, Mixture-of-Experts (MoE) routing, and asynchronous reinforcement fine-tuning, require flexible point-to-point communication beyond simple collectives. Existing implementations are locked to specific Network Interface Controllers (NICs), hindering integration into inference engines and portability across hardware providers. We present TransferEngine, which bridges the functionality of common NICs to expose uniform interface. TransferEngine exposes one-sided WRITEIMM operations with IMMCOUNTER primitive for completion notification, without ordering assumptions of network transport, transparently managing multiple NICs per GPU. We demonstrate peak throughput of 400 Gbps on both NVIDIA ConnectX-7 and AWS Elastic Fabric Adapter (EFA). We showcase TransferEngine through three production systems: (1) KvCache transfer for disaggregated inference with dynamic scaling, (2) RL weight updates achieving 1.3 seconds for trillion-parameter models, and (3) MoE dispatch/combine implementation exceeding DeepEP decode latency on ConnectX-7, with the first viable latencies on EFA. We demonstrate that our portable point-to-point communication complements collectives while avoiding lock-in."
        },
        {
            "title": "INTRODUCTION",
            "content": "Mixture-of-Experts (MoE) architectures are becoming the dominant approach for scaling model capacity (Shazeer et al., 2017), while disaggregated inference is emerging as the standard for production serving. (Patel et al., 2024; Zhong et al., 2024; Qin et al., 2025) These new architectures rely on communication patterns that are fundamentally different from traditional collective-based parallelism. LLM frameworks overwhelmingly rely on collective communication, often through NCCL or torch.distributed. (NVIDIA, 2015; Li et al., 2020) While collectives excel at static patterns such as tensor or data parallelism (Shoeybi et al., 2020; Sergeev & Balso, 2018; Zhao et al., 2023), they impose constraints unsuitable for emerging workloads: fixed membership prevents dynamic scaling, synchronized initialization adds overhead, and uniform buffer sizes force dense communication even for sparse patterns. While these libraries offer SEND and RECV primitives for point-to-point communication, they often cannot be effectively composed to achieve viable latency. High-performance computing has long used primitives (SEND, RECV, WRITE) built on Remote Direct Memory Access (RDMA) for flexible low-latency high-bandwidth transfers (Kalia et al., 2016), yet such primitives are rarely available in LLM frameworks. The key barrier is hardware diversity without uniform abstraction. Cloud providers de- *Equal contribution 1Perplexity AI. Correspondence to: Nandor Licker <nandor@perplexity.ai>, Lequn Chen <lequn@perplexity.ai>. ploy different RDMA implementations: NVIDIA ConnectX NICs use traditional Reliable Connection (RC) transport with in-order delivery, while AWS Elastic Fabric Adapter (EFA) implements proprietary Scalable Reliable Datagram (SRD) protocol (Shalev et al., 2020) with out-of-order delivery. Existing solutions suffer from vendor lock-in: DeepEP (Zhao et al., 2025) requires GPU-initiated RDMA (IBGDA) (Agostini et al., 2018) provided exclusively by ConnectX, NVSHMEM (Langer et al., 2021) exhibits severe performance degradation on EFA and recent libraries like Mooncake (Qin et al., 2025) and NIXL (NVIDIA, 2025) lack EFA support or remain preliminary. Consequently, there are no viable cross-provider solutions for LLM inference. We address portability by leveraging the common functionality across heterogeneous RDMA hardware. Our key insight is that both ConnectX and EFA support reliable but unordered delivery: ConnectX RC can ignore ordering, while EFA SRD is inherently unordered. We introduce TransferEngine, portable RDMA communication library. It provides two-sided SEND/RECV and one-sided WRITEIMM operations with novel IMMCOUNTER primitive for completion notification that does not rely on message ordering. It exposes common interface to both ConnectX and EFA, avoiding vendor lock-in. It transparently manages multiple NICs per GPU, essential for EFA where four 100 Gbps NICs must be aggregated to reach 400 Gbps. We demonstrate the TransferEngine through production systems addressing the shortcomings of collectives: KvCache transfer (Section 4): Disaggregated inference RDMA Point-to-Point Communication for LLM Systems with unrestricted prefiller-decoder communication, enabling elastic scaling without synchronized initialization or fixed membership. Production tested on EFA with full CUDA Graph support and low-latency layer-by-layer transfers. RL weight update (Section 5): Point-to-point approach achieves 1.3-second updates for trillion-parameter models, over 100 faster than existing RL frameworks. (Huang et al., 2025; He et al., 2025) Utilizes full cluster bandwidth by one-sided RDMA WRITE directly from each training GPU to inference GPUs. Pipelined execution overlaps H2D memcpy, weight preparation, and RDMA transfer. MoE dispatch/combine (Section 6): State-of-the-art decode latency on ConnectX-7, competitive with specialized DeepEP kernels despite the use of host proxy thread. First viable implementation on EFA, relying on parallel token and route transfers to hide device-to-host and network latency. The systems we present span diverse communication patterns, ranging from paged writes to bulk transfers and coordinated scatter operations, all production-deployed on heterogeneous hardware. Our results demonstrate that point-topoint communication complements collectives for modern LLM workloads, and that portable abstractions can avoid vendor lock-in while retaining performance."
        },
        {
            "title": "2 BACKGROUND AND RELATED WORK",
            "content": "2.1 Network Technology RDMA Remote Direct Memory Access (RDMA) is the high-throughput, low-latency backbone of modern ML systems. Presently deployed NICs deliver 400 Gbps bandwidth at sub-µs latencies. RDMA achieves its performance through split design: control plane operations initialize the device and set up buffers with operating system involvement, while data plane operations (data transfer and completion polling) bypass the kernel, avoiding system call overheads. RDMA Operations RDMA provides both two-sided and one-sided operations. Two-sided SEND/RECV operations first post RECV with registered memory region, with the sender issueing SEND, notifying the receiver via completion queue. One-sided WRITE operations copy local memory to remote memory region without peer involvement, requiring the remote memory address and key (RKEY). WRITEIMM extends WRITE by delivering 32-bit immediate value that also notifies the receiver via the completion queue. While READ and atomic operations are available, their latencies are not suitable for our purposes. (Kalia et al., 2016; Reda et al., 2022) RDMA Transports The RDMA specification defines three transport protocols: Reliable Connection (RC), Unreliable Connection (UC), and Unreliable Datagram (UD). Table 1 summarizes their capabilities, highlighting our TransReliability Ordering Connection SEND/RECV WRITEIMM READ Atomic UD RC UC MTU SRD Ours Table 1. Comparison of RDMA transport types ferEngine as the common ground between them. Cloud RDMA Adapters Beyond the widely-deployed ConnectX series, major cloud providers are deploying their own solutions, such as AWS Elastic Fabric Adapter (EFA), Alibaba Cloud eRDMA and Google Falcon (Alibaba Cloud, 2025; Singhvi et al., 2025). While most are RC-compatible, EFA implements the proprietary SRD protocol, exposed via libfabric. (Shalev et al., 2020; OFIWG, 2014) SRD is connectionless and provides reliable but unordered delivery. 2.2 Programming Interface Collectives ML frameworks predominantly rely on collective communication libraries (e.g., torch.distributed, NCCL, MPI) for inter-GPU data exchange. (Li et al., 2020; NVIDIA, 2015; MPI Forum, 2025) While collective operations excel at structured data transfers, (Shoeybi et al., 2020; Sergeev & Balso, 2018; Zhao et al., 2023) their pointto-point capabilities are limited: (1) Fixed membership hinders dynamic scaling by requiring advance knowledge of all participants; (2) Synchronized initialization mandates global coordination to form worlds, blocking independent peer connections; (3) Operation ordering requires all participants to agree on operation sequence, necessitating extra synchronization even when NCCL supports concurrent receives; (Hu et al., 2025b) (4) Shape uniformity constrains transfer sizes across participants, even for point-to-point. GPUDirect GPUDirect RDMA enables RDMA NICs to directly access GPU memory over PCIe, eliminating intermediate CPU memory copies. (NVIDIA, 2012) Transfers can be initiated either by the host, or if GPUDirect Async (IBGDA) is available, from the GPU itself to bypass PCIe overheads (Agostini et al., 2018). Currently, GPUDirect Async is only supported on ConnectX NICs. Additionally, GPUDirect RDMA enables low-latency host-device memcpy via the GDRCopy library (Shi et al., 2014). 2.3 Related Work Disaggregated Inference Splitwise, DistServe, and Mooncake are among the earliest works demonstrating the benefits of disaggregated inference architectures by separating the prefill and decode stages of LLM inference to distinct devices. (Patel et al., 2024; Zhong et al., 2024; Qin et al., 2025) Our KvCache transfer use case implements this RDMA Point-to-Point Communication for LLM Systems paradigm by bridging the TransferEngine to an inference engine to connect prefill and decode clusters via RDMA. Point-to-Point Network Libraries NVSHMEM exposes both collective operations as well as flexible point-to-point communication. (Langer et al., 2021) It supports both GPUinitiated (IBGDA) and host-proxy communication (IBRC). However, it suffers from severe performance degradation on EFA. NVIDIA Inference Xfer Library (NIXL) targets P2P communication for LLM inference, built on UCX (NVIDIA, 2025; Shamis et al., 2015). Our production-deployed EFA implementation predates the preliminary EFA support in NIXL (v0.6.1, October 2025). Mooncake also provides an RDMA Transfer Engine, but without support for EFA. Other libraries, such as UCCL (Zhou et al., 2025) and MSCCL++ (Shah et al., 2025), focus on network-layer optimizations for collectives rather than point-to-point. Distributed KvCache Storage Mooncake Store and DeepSeek 3FS (DeepSeek AI, 2025) provide distributed storage for KV caches, albeit they currently lack EFA support. We complement these systems by providing portable RDMA primitives suitable for cloud deployments. Compute-Communication Overlapping Research on overlapping compute with collective communication for LLM kernels (Chang et al., 2024; Zhang et al., 2025; Zheng et al., 2025a;b) is orthogonal to our focus on RDMA primitives, although we do enable background transfers. LLM Frameworks Our TransferEngine and MoE kernels can be integrated into LLM inference frameworks and kernel libraries, such as vLLM, SGLang, TensorRT-LLM, FlashInfer. (Kwon et al., 2023; Zheng et al., 2024; NVIDIA, 2023; Ye et al., 2025) Our P2P weight update approach can be adopted by reinforcement learning frameworks, such as Slime, OpenRLHF, AReaL, veRL, LlamaRL, NVIDIA Nemo. (Wu et al., 2025; Zhu et al., 2025; Mei et al., 2025; Fu et al., 2025; Sheng et al., 2024; Hu et al., 2025a)"
        },
        {
            "title": "3 TRANSFERENGINE",
            "content": "TransferEngine is the foundational library that enables efficient RDMA-based point-to-point communication, abstracting heterogeneous hardware under simple protocol. It exposes SEND/RECV operations to implement RPC-like interfaces. For KvCache tranfers, it provides paged WRITEs for bulk writes. For RL weight transfers, it exposes low-latency high-throughput WRITE operations. For MoE routing, it specializes WRITEs targeting many peers to implement lowlatency SCATTER and BARRIER operations. There are no ordering guarantees across any of the operations. An immediate value can be optionally associated with WRITEs to increment counter on the receiver upon receipt. Figure 1. TransferEngine managing GPUs across NUMA nodes, each with multiple NICs. Commands are forwarded to workers, which respond back to the callback handler or IMMCOUNTER. 3.1 Overview and Design Goals The TransferEngine exposes minimal API that abstracts away the complexity of the underlying RDMA interfaces. It supports multiple interfaces, including EFA with its SRD protocol and multitude of NICs programmable via libibverbs, including ConnectX-7. Topologies are transparently detected to handle multiple NICs per GPU: while single ConnectX-7 NIC provides 400 Gbps bandwidth, achieving equivalent bandwidth on AWS p5 instances requires aggregating four 100 Gbps EFA NICs (or two 200 Gbps EFA NICs for p5en instances). single instance spans multiple threads to manage all GPUs and NICs within node. Low-latency operations are achieved through zerocopy interfaces and hardware-specific optimizations. To bridge the gap between the in-order guarantees of RDMA RC and the out-of-order delivery of EFA SRD, the TransferEngine relies solely on novel IMMCOUNTER. Instead of relying on ordering guarantees, all completion notifications are handled by polling completion queues and delivering notifications upon the bulk receipt of immediate values. Notifications are delivered through callbacks or atomic flags. 3.2 Architecture Figure 1 illustrates the architecture of the TransferEngine. The engine spawns worker per GPU, with each managing generic DOMAINGROUP that coordinates all the associated RDMA NICs, typically 14 NICs depending on the hardware platform. Within generic DOMAINGROUP, each DOMAIN is specialized to the hardware and responsible for single NIC, handling queue pair management, work submission, and completion polling. Each TransferEngine instance exposes single main address for identification and discovery to remote peers. We use NetAddr struct to capture and serialize network address of DOMAIN, exchanging the structure between peers that wish to communicate. As restriction, all peers must use the same number of NICs per GPU. Consequently, any transfer has full knowledge of the NICs between the source and RDMA Point-to-Point Communication for LLM Systems #[serde] struct NetAddr(Bytes); #[serde] struct MrDesc{ ptr: u64, rkeys: Vec<(NetAddr, u64)> } struct MrHandle(NonNull<c_void>); type Offset = u64; struct Pages{ indices: Vec<u32>, stride: u64, offset: Offset } struct PeerGroupHandle(u64); struct ScatterDst{ len: u64, src: Offset, dst: (MrDesc,Offset)} enum OnDone { Callback(fn () -> ()), Flag(Atomic<bool>) } trait TransferEngine { fn main_address() -> NetAddr; // Memory Region Management fn reg_mr(ptr, len, device) -> (MrHandle, MrDesc); // Two-sided Send/Recv fn submit_send(addr: NetAddr, msg: &[u8], cb: fn () -> ()); fn submit_recvs(len: u64, cnt: u64, cb: fn (&[u8]) -> ()); // One-sided Write fn expect_imm_count(imm: u32, count: u32, cb: fn () -> ()); fn submit_single_write(len: u64, imm: Option<u32> src: (MrHandle, Offset), dst: (MrDesc, Offset), OnDone); fn submit_paged_writes(page_len: u64, imm: Option<u32>, src: (MrHandle, Pages), dst: (MrDesc, Pages), OnDone); // One-sided Write to group of peers fn add_peer_group(addrs: Vec<NetAddr>) -> PeerGroupHandle; fn submit_scatter(h: Option<PeerGroupHandle>, OnDone, imm: Option<u32>, src: MrHandle, dst: Vec<ScatterDst>); fn submit_barrier(h: Option<PeerGroupHandle>, OnDone, imm: u32, dst: Vec<MrDesc>); // Watcher for CPU-GPU synchronization fn alloc_uvm_watcher(cb: fn(u64,u64) -> ()) -> NonNull<u64>; } Figure 2. TransferEngine API pseudo-code. Error handling, domain sharding, and resource releasing are omitted. destination domain, allowing the TransferEngine to shard or balance the request. This is crucial for EFA, which achieves the full 400Gbps bandwidth using multiple adapters. 3.3 API Design The API exposed by the TransferEngine is outlined in Figure 2, implementing the abstractions over RDMA: Memory Registration Memory regions must be registered with the engine, returning serializable MrDesc that can be exchanged with peers to submit WRITEs through and local MrHandle to be used as the source of transfers. The same API can register both host-side buffers and GPU memory. Behind the opaque types, the handles carry the addresses of all the NICs they are associated with, along with the domain-specific remote keys attached to them as list of (NetAddr, RKEY) pairs. Point-to-Point Transfer submit send and its remote counterpart submit recvs wrap the SEND/RECV operations to expose RPC-style communication, exchanging small payloads. Submission involves copy to allow the caller to reuse or release the buffer immediately. Receive posts rotating pool of buffers to receive data into. Upon each message, buffer is temporarily taken out of the pool to allow the callback to process it without copying. Upon callback completion, it is automatically re-posted. Sufficient buffers must be allocated to avoid rejecting messages. These operations utilize only the first NIC in domain group. submit single write and submit paged writes transfer data from source to destination buffer, writing contiguous or paged regions determined by indirect indices, strides and offsets, respectively. The engine translates these operations into possibly multiple zero-copy one-sided RDMA WRITE operations, sharding and rotating them along the available NICs. Each Transfer can optionally carry 32-bit immediate number to notify the receiver upon completion. Transfer completion notifications are delivered asynchronously to the caller. submit scatter send slice from source buffer to each peer in peer group at different offsets in their receive buffers, while submit barrier is an immediate-only operation for peer notification. These are optimized wrappers around WRITE, allowing applications to pre-register PeerGroup to target with low-latency bulk transfers. Transfers execute between two devices: it is up to the user to coordinate operations across multiple devices in system. UVM Watcher In order for the host to be able to initiate transfers upon GPU progress, alloc uvm watcher registers callback invoked upon changes to word in memory. It allocates unified virtual memory (UVM) location that can be updated by the devices, including kernels within CUDA graph, which is continuously polled by CPU thread using GDRCopy. Since not all changes are guaranteed to be observed immediately, the callback is invoked the old and the new values, allowing it to respond to GPU-side progress. Completion Notification Notifications are delivered for both send confirmation and receive completion, either through callbacks or atomic flags. The IMMCOUNTER is dedicated component that keeps track of per-immediate counters incremented on events retrieved from the completion queue of the underlying devices. Events are generated on the sender after transfer is completed and on the receiver once payload with an immediate attached to it has been fully delivered, guaranteeing atomicity. The counters are allocated in the same NUMA node as the domain worker. The counters can either be transparently synchronized with the GPU via GDRCopy, observed directly through polling or handed off to callback on separate dedicated thread within the engine, registered using expect imm count. All synchronization is implemented using such counters, as otherwise there are no other ordering guarantees. 3.4 Implementation Our TransferEngine, implemented in Rust, carefully optimizes allocations, threading and synchronization to minimize latency in order to achieve high throughput. The engine spawns one worker thread per DOMAINGROUP, each pinned to CPU core on the NUMA node to which the devices are attached to minimize both scheduling and memory access latency. Data structures specific to domain are allocated after pinning to ensure that memory is reserved in the correct NUMA node. One worker thread is responsible for handling up to 4 DOMAINs, each managing single RDMA Point-to-Point Communication for LLM Systems NIC, whereas another dedicated thread is responsible for polling the GPU to update UVM watchers. Cross-thread communication is done through lock-free queues. The API forwards requests to the DOMAINGROUP serving the appropriate device, with the first one also serving the host. In tight loop, the domain worker polls for new work, prioritizing the submission of new requests. Depending on the hardware and configuration, requests are sharded and load-balanced across the available DOMAINs. The first WRITE of composite request is immediately posted to the NICs send queue in the DOMAIN. Once new requests are exhausted, the worker proceeds to progress on pending, posting writes to fill up the hardware pipeline. Subsequently, completion queues are polled to query for finished transfers and immediate counter increments. Events are aggregated to deliver per-transfer notifications, handing the transfer over to dedicated callback thread shared by all groups. Sharding inside DOMAINGROUP is flexible. Transfers can target specific NICs by index. single WRITE can be split. Paged transfers, scatter and barrier operations, which all translate to multiple WRITEs, can shard across all NICs. 3.5 Hardware-Specific Optimizations The DOMAINs within the TransferEngine are specialized and optimized for the hardware under their control: AWS EFA We implement EFA support using libfabric, managing fabric domain per NIC within the DOMAINGROUP. Since EFA diverges from the RDMA spec which does not require valid target descriptor for immediate-only zero-sized writes, we enforce valid descriptors for all transfers. For bulk transfers and peer groups, we employ work request (WR) templating, pre-populating and retaining the common fields of libfabric descriptors before posting. NVIDIA ConnectX-7 We implement ConnectX support through libibverbs. For each peer, we use an UD queue pair to exchange RC hanshakes. We create 2 RC queue pairs per peer: one for two-sided SEND/RECV operations and another for one-sided WRITE and WRITEIMM operations. This separation is necessary because both RECV and WRITEIMM completions consume work requests in posting order. This allows us to provide high-level RECV semantics while supporting WRITEIMM without interference. In addition to WR templating, we employ WR chaining by linking up to 4 work requests through the next pointer of ibv send wr, reducing the number of doorbell rings to the NIC. Additionally, we enable IBV ACCESS RELAXED - ORDERING to permit out-of-order PCIe transactions between the NIC and GPU memory, reducing latency. Figure 3. KV transfer between prefillers and decoders"
        },
        {
            "title": "4 KVCACHE TRANSFER",
            "content": "In this section, we outline production-tested implementation of disaggrated inference relying on the TransferEngine. In disaggregated mode, prefiller node runs prefill on the input tokens, transferring the resulting KV pages and any additional context, such as last token hidden states and logits for speculative decoding, to the decoder node which proceeds to decode tokens one-by-one. Figure 3 illustrates our disaggregated setup. Once request is received, global scheduler selects prefiller node and decoder node to process it and forwards the request to the decoder. The decoder pre-allocates KV pages and storage for any context before dispatching the request to the designated prefiller using submit send, indicating the KV page indices where contents should be transferred to. During chunked prefill, we increment the UVM watcher value after the attention output projection of each layer, which is CUDA Graph compatible. Once TransferEngine detects the change in the UVM watcher value, the layers transfer is initiated, copying over the pages from the current chunk via submit paged writes. When the last chunk is complete and the context has been populated, it is copied over via submit single write. Prefillers await for commands using submit recvs. KV transfers must account for differences in the sharding or replication of KV caches between prefillers and decoders. MLA (Liu et al., 2024) replicates the compressed KV cache entries if tensor parallelism is used: under such scheme, prefiller ranks are randomly matched with decoder ranks to balance the transfers of replicas. Under GQA (Ainslie et al., 2023), we rely on page-wise offsets and strides to select slices from the source KV cache to copy into corresponding offsets in the destination KV cache. To minimize the number of writes and ensure that individual writes are sufficiently large, the KV caches are laid out with heads preceding the pages, ensuring continuity within consecutive heads. The prefiller does not issue an explicit completion message: the decoder knows in advance the number of transfers it RDMA Point-to-Point Communication for LLM Systems P2P approach each training GPU send weights directly to inference GPUs via one-sided RDMA WRITE, utilizing the full cluster bandwidth across all NICs. Figure 4 illustrates the difference between the two approaches. At initialization, the controller script performs three steps: First, it gathers parameter metadata from all training and inference GPUs, including weight name, shape, dtype, and DTensor sharding. Next, it computes static weight transfer schedule, mapping which training GPU sends which parameter to which inference GPU, and in what order. Finally, it broadcasts the schedule to all training GPUs. At each training step, the controller signals training GPUs to begin sending weights. The inference nodes remain unaware of the transfer, as it uses one-sided operations. 5.2 Pipelined Weight Transfer Execution Our training job shards model weights using FSDP (Zhao et al., 2023). Different parameter types (e.g., MoE vs nonMoE) require different FSDP sharding strategies. Each sharding strategy partitions the global DeviceMesh into disjoint sub-meshes. We call each sub-mesh MeshGroup. Parameters within MeshGroup are transferred in parallel, while MeshGroups are processed sequentially. We treat the transfer of each parameter tensor as task. To utilize different hardware resources simultaneously, we split each task into four pipeline stages that overlap in time, as illustrated in Figure 5: (1) Host-to-device memcpy if FSDP offloads weight to CPU. (2) Parameter preparation: Reconstruct full weight with full tensor(), apply projection fusion, quantize if needed. (3) RDMA transfer: Zero-copy WRITE to remote inference GPU memory. (4) Global barrier: After all full tensor() calls are done, synchronize across mesh groups using GLOO via Ethernet. full tensor() and other GPU operations introduce extra GPU memory usage. To avoid out-of-memory errors, we only start new task if the current in-flight tasks occupy less temporary GPU memory than configurable watermark."
        },
        {
            "title": "6 MOE DISPATCH/COMBINE",
            "content": "We present set of low-latency kernels for MoE dispatch and combine built around the TransferEngine, relying on host proxy thread to coordinate the GPUs and the NICs. Within node, we also utilize NVLink to reduce the load on the network. Despite the added latency of proxy thread, we achieve state-of-the-art decode performance while maintaining competitive performance for prefill without any tweaks. These kernels demonstrate the feasibility of proxy-based MoE dispatch with support for wider range of network cards, such as EFA. Consequently, we focus on decode performance (128 tokens per rank) as it is latency-bound, Figure 4. Weight transfer data path for different approaches. Figure 5. Pipelined weight transfer execution. expects and uses expect imm count to be notified by the TransferEngine of transfer completion and start decoding. The complexity of production-ready implementation of disaggregated decoding lies in the handling of errors and cancellation. Cancellation triggerred by decoder must be explicitly confirmed by the prefiller, as the KV pages cannot be reused as long as there is possibility of remote write clobbering them. We rely on heartbeat messages between prefillers and decoders to detect transport layer failures. If prefiller node is unresponsive, requests are cancelled on the decoder after timeout, as transfers can no longer reach it. per-request cancellation token can stop all future transfers whilst also waiting after all pending operations before sending the cancellation confirmation."
        },
        {
            "title": "5 RL ROLLOUT WEIGHT TRANSFER",
            "content": "In asynchronous reinforcement learning fine-tuning, training and inference run on separate GPUs. After each training step, new weights must be pushed to inference nodes, which can take tens to hundreds of seconds on trillion-parameter models using existing frameworks. Our solution achieves 1.3-second cross-machine parameter updates for models at the scale of Kimi-K2 (1T parameters), DeepSeek V3 (671B) and Qwen3 (235B) (Kimi Team et al., 2025; DeepSeek-AI et al., 2025; Yang et al., 2025), transferring weights from 256 training GPUs (bf16) to 128 inference GPUs (fp8). 5.1 Point-to-Point Weight Transfer Existing frameworks tend to form global collective communication world for all training and inference GPUs. Weights are gathered to training sub-group Rank0, then broadcast to Rank0 of each inference sub-group, bottlenecked by the NIC of training Rank0. In contrast, in our RDMA Point-to-Point Communication for LLM Systems Figure 6. Dispatch and Combine GPU-CPU-NIC coordination suffering more significantly from the added PCIe, driver and firmware overheads across the devices involved. 6.1 Architecture Routing is implemented using split kernels for both decode and combine, with sender half preparing the data into send buffers to be written to peers and receiver half shuffling tokens from receiver buffers into tensors used by other kernels. The individual kernels fully utilize all SMs and the memory bandwidth of the GPU to reduce latency, however their runtime is short enough to interleave other work between them, enabling overlapping and micro-batching. The host proxy uses GDRCopy to poll the GPU for progress, invoking the TransferEngine when source buffers are ready. Our design, illustrated in Figure 6, minimizes the proxy overhead by reducing the number of writes issued. To reduce the GPU memory required for send and receive buffers, all peers first exchange routing information, per-expert token counts, to determine unique range in contiguous receive buffer to write to. Since these payloads are small, their latency is hidden by speculatively dispatching small number of tokens into private per-source buffers. Combine issues single scatter as it re-uses the routing information. Routing information is always handled by the TransferEngine, but payloads can be copied via NVLink within the same node. Consequently, each rank issues up to 2 WRITEs for dispatch and 1 WRITE for combine for each inter-node peer. Receiver buffers must be sized to account for all tokens being sent to the current rank. Assuming there are ranks hosting experts, each dispatching tokens to experts, the upper bound is max(R, ). Senders pack writes into such contiguous buffer instead of relying on larger per-rank receiver buffers. To minimize overhead, the larger dispatch receive buffer can be re-used by combine send. 6.2 Dispatch The dispatch kernel receives the tokens and the indices of the experts to which they must each be routed, returning tensor which packs the tokens from all the ranks assigned to the local experts. The kernel first counts the number of tokens sent to each expert in shared memory and transfers the counts to the host via unified memory. It then signals the proxy, which uses the TransferEngine to initiate the scatter Figure 7. Dispatch into private and contiguous buffers of the routes to all peer ranks. The input tokens are then copied into the send buffers, creating contiguous source to be scattered to each peer, as shown in Figure 7. The proxy is notified to scatter tokens up to fixed limit into private buffers on each receiver peer. For decode-sized batches, the latency from the launch of the dispatch kernel to the first transfer is around 15 µs assuming EP=64. At this point, the transfers of routes and tokens saturate the NIC bandwidth. Once the routes are received and the position of tokens from each source rank can be determined on each destination rank, the remainder of the tokens are scattered to the peers, placing them contigously into shared receiver buffer. The host-side work to process routes and dispatch the second round of transfers takes tens of microseconds. The number of tokens in the initial transfer is chosen to hide this overhead. Although this latency is not on the critical path, reducing it can further reduce the size of the private buffers. Once all transfers are acknowledged as having completed and all incoming writes are accounted for, barrier synchronizes the proxies via the TransferEngine. Following the dispatch of transfers to inter-node peers over RDMA, the send kernel proceeds to issue writes over NVLink to transfer tokens within the same node, while RDMA transfers are pending in the background. NVLink peers synchronize themselves through their own set of barriers: before writing to peer, each rank must ensure that the data has been read and can be overwritten. This is done through flags written and read with relaxed semantics. After the payloads are written, ranks synchronize through release-acquire flags with each other. Write ordering in send kernels is latency-critical due to the lack of granularity in memory barriers. NVLink is exRDMA Point-to-Point Communication for LLM Systems posed via virtual memory, transparently translating reads and writes to peer devices mapped into the current address space into transactions on the interconnect. Loads are universally expensive, as they block the execution pipeline until they are satisfied. In contrast, stores are fire-and-forget, until memory barrier is encountered, which blocks until all prior stores within their scope complete. Since both the host system and NVLink peers are within the same scope, barrier ensuring ordering with the host might be slowed down by previously issued writes over NVLink. This is avoided by first signalling the host, then issuing NVLink writes after grid barrier. This strategy increases the total execution time of send kernels, but it reduces latency on the critical path to the first RDMA transfer. With NVLink, it is usually preferable to push data from source device to target, saving trip time. Additionally, after the stores are acknowledged on the current device, useful work can be executed while the transfers are in-flight to the remote. With dispatch, we only push the tokens to the private receive buffers, since at this stage the centralized routing information is not available to determine exactly where the rest of the tokens should be placed on the peer. The receiver half of the kernel kicks off by synchronizing on the barrier and reading the remaining tokens. These loads are likely to complete before the RDMA operations. After dispatch send, transfers run in the background. Once the receiver kernel is called, it waits for the TransferEngine to report the completion of all transfers via IMMCOUNTER and GDRCopy. Relying on indices computed based on the routing information, tokens are re-ordered with optional padding between experts following layout suitable for Grouped GEMM kernels. Since the receiver kernels must process around tokens while on the critical path, work is split across all available SMs and pipelining is maximised to fully utilize the available HBM bandwidth. 6.3 Combine Since routing information is centralized during the dispatch stage, combine transfers all payloads in single scatter. The command is prepared during the idle time between send and combine, during which the GPU would typically execute grouped GEMM. The sender, similarly to dispatchs, prepares send buffers and pushes tokens via NVLink to intra-node peers. The host proxy is signalled to send the SCATTER requests to the TransferEngine, after which the kernel finishes execution. The receiver caches all relevant offsets derived from routing information in shared memory before waiting for all tokens to be received. The weighted average of the tokens is then computed locally on each rank. The combine stage re-uses the same buffers as the dispatch stage, thus it requires both an NVLink and an RDMA barrier ensuring the completion of all prior operations before Figure 8. Point-to-Point communication performance overwriting the send buffers. At this point, the host proxy also waits for the TransferEngine to confirm that all writes have been sent out. Particularly for EFA, which also waits for receipt confirmation, it is important to maximise the interval between posting write and checking its status. 6.4 Comparison with DeepEP DeepEP offers state-of-the-art latency, however they are tied to ConnectX due to their reliance on IBGDA and mlx5 driver. Our kernels are more portable, relying on host proxy, supporting both EFA and ConnectX. Despite the added overhead, our latency exceeds DeepEP. DeepEP kernels rely on the strong ordering guarantees of RC QP. The tokens are balanced across the available SMs which transfer them over QP one-by-one. This ensures lower latency to the first transfer, however it also involves more work on per-token basis and results in more packets being sent over the network. Token counts and completion are signalled via ATOMICs. In contrast, our kernels spend more time before the first transfer is initiated due to the additional GPU-CPU-NIC communication overheads over PCIe. However, bulk transfers achieve better network utilization. For prefill, we scale our single-transfer strategy without tweaks. In contrast, DeepEP achieves better latencies by pre-accumulating tokens via NVLink on the sender node, reducing the amount of data transferred. Additionally, the DeepEP kernels use less buffer memory as subsets of tokens are transferred in batches. While this approach is faster, it also has implications on accuracy and determinism, as accumulation it not done entirely on fp32 in fixed order."
        },
        {
            "title": "7 EVALUATION",
            "content": "We evaluate the performance of the TransferEngine and the systems built on it on nodes with 8NVIDIA H200 GPUs connected via NVLink and paired each GPU with either single 400 Gbps ConnectX-7 or dual 200 Gbps EFA NIC, all connected to dual-socket Intel Sapphire Rapids CPUs. 7.1 Point-to-Point Communication The performance of KvCache transfers and RL rollouts is determined by the throughput of single and paged writes. We compare higher-level libraries (TransferEngine and NIXL v0.6.1) against hardware benchmarks for the underlying RDMA Point-to-Point Communication for LLM Systems n e P EFA 64 KiB 16 Gbps 256 KiB 54 Gbps 1 MiB 145 Gbps 32 MiB 336 Gbps CX-7 44 Gbps 116 Gbps 245 Gbps 378 Gbps 1 KiB 17 Gbps 2.11M op/s 91 Gbps 11.10M op/s 8 KiB 138 Gbps 2.10M op/s 320 Gbps 4.89M op/s 16 KiB 274 Gbps 2.08M op/s 367 Gbps 2.80M op/s 64 KiB 364 Gbps 0.69M op/s 370 Gbps 0.71M op/s Table 2. EFA and ConnectX-7 performance comparison NICs, measuring the fraction of bandwidth for range of message sizes. On ConnectX we compare against ib - write bw from rdma-core. On EFA, we use fi rma - bw from libfabric to measure peak single-NIC bandwidth, which we double to match the engine running on two NICs. Figure 8 shows the fraction of the peak bandwidth achieved by both libraries. To saturate with single WRITE, messages of at least 16MiB are required, while 32KiB and 64KiB are sufficient to saturate with paged WRITE on both TransferEngine and NIXL, respectively. Performance between our solution and NIXL is relatively close, with the TransferEngine being slightly faster. EFA requires larger messages to saturate, explaining the gap observed on MoE routing. Table 2 details absolute performance numbers. For 256 KiB single WRITE (typical for our MoE routing), our TransferEngine achieves 54 Gbps on EFA and 116 Gbps on ConnectX-7. At 64 KiB paged WRITE (typical size for KV Cache page), both EFA and ConnectX-7 are able to saturate the available bandwidth. The size of RL weight transfer messages is well beyond the saturation point. 7.2 MoE Dispatch/Combine We evaluate the performance of the MoE kernels for both decode and prefill across 8, 16, 32 and 64 GPUs, comparing them to DeepEP, which is the most performant open-source implementation, as well as the portable and open-source pplx-kernels (Licker et al., 2025) which are built around NVSHMEM v3.4.5, running on both EFA and ConnectX7. While there is overlap between our work and UCCLEP (Mao et al., 2025), we do not include detailed comparison as their published latencies are substantially higher. We show that on ConnectX-7 adapters our performance is the new state-of-the-art, despite the use of the host proxy. To compare with DeepEP, our benchmarks consider the settings of DeepSeek-V3 model. We dispatch 7168 fp8 tokens with 56 fp32 scaling factors and combine bf16 tensors of the same dimension. Decode is evaluated on batches of 128, prefill on chunks of 4096 tokens. Each token is dispatched to 8 random experts. We run 10,000 warmup iterations, followed by 10,000 benchmarked runs, aggregating timing across all ranks. We simulate overlapped work and clear caches by inserting large GEMMs. Figure 9. Impact of private buffer size on p50 decode latency Figure 10. Separate Send and Receive Latency for EP="
        },
        {
            "title": "7.2.1 Private Buffer Size",
            "content": "The private buffers were designed to hide the latency of routing information exchange. We compare the median latency of total decode dispatch times while varying the number of tokens with the peak latency achieved with buffer size that can accommodate all tokens in one burst. Figure 9 indicates the performance dropoff as the private buffer size decreases. The performance degradation in the absence of tokens justifies the use of the latency-reduction strategy. In the intra-node case, where tokens are transferred faster, at least around 32 tokens are necessary to hide the latency of route exchange across both NICs. In the internode case, ConnectX-7 NICs allow as few as 24 tokens to be used, while EFA NICs present performance dropoff under 32 tokens already, since route exchange is slower. 7.2.2 Send and Receive Latency Our kernels, similarly to DeepEP, are split into senders and receivers, with both NVLink and RDMA transfers executing in the background between them. Figure 10 shows these latencies, which were measured by inserting long artificial delay simulating shared experts or overlapped work before the receive kernels, allowing all transfers to settle. Dispatch and combine send outperform DeepEP as ours only copy memory. Due to faster accumulation, combine receive is also faster. Dispatch receive is an outlier, since it pulls data using NVLink loads. Overall, the total execution time of these kernels is under 15% of the transfer times. More indepth profiling reveals the proxy commences RDMA work midway through the execution of the send kernels, with shuffling adding only about 15 µs of idle time. 7.2.3 Decode Latency Figure 11 shows the latencies of our MoE dispatch and combine kernels on decode-sized batches, comparing them to the specialized DeepEP and the portable pplx-kernels. We report results for pplx-kernels only for ConnectX-7, as RDMA Point-to-Point Communication for LLM Systems Figure 11. MoE Decode Latency. Bar height is mean. Error bars show p01, p25, p50, p75, p95, p99. Error bars for pplx indicate stddev. Figure 12. MoE Prefill Latency. Bar height is mean. Error bars show p01, p25, p50, p75, p95, p99. NVSHMEM on EFA is unusably slow. Our proxy-based implementation not only makes EFA viable, it is also significantly faster than the IBGDA-based kernels on ConnectX-7 and an order of magnitude faster than the NVSHMEM kernels using IBRC through the generic host proxy. In the intra-node setup with 8 ranks, our kernels are slower by about 2 µs than DeepEP, largely due to the use of NICs to exchange routing information. Whilst this data point is not critical for our primary inter-node use case, it does highlight that the NVLink transfers whose latencies are otherwise hidden by RDMA are highly efficient. pplx-kernels rely on fine-grained per-token NVLink synchronization under this configuration. The discrepancy is due to bulk transfers and per-block/per-grid synchronization over NVLink. Inter-node on 16 and 32 ranks our kernels outperform DeepEP on both dispatch and combine, largely due to the bulk transfers and efficient pipelining in the combine phase. The ordering of RDMA and NVLink writes also helps reduce the latency to the first RDMA transfer. When scaling to 64 ranks, combine still outperforms DeepEP, but the CPU overhead of the proxy thread becomes noticable and dispatch is slower due to the roughly microsecond overhead of enqueuing transfer for each of the 56 inter-node peers. Since the bandwidth is not saturated by decode, EFA latencies are trailing behind by only 30%, despite 256KiB writes achieving less than half of the ConnectX-7 throughput. 7.2.4 Prefill Latency Figure 12 shows the latency of MoE Prefill. We exclude pplx-kernels from the comparison as they are not effective at 4096 tokens. Due to the lack of chunking in transfers, the memory overhead of our decode-optimized kernels limits the set of models for which deployment is viable. On the dispatch side, DeepEP performs better with fewer ranks as they use RDMA to transfer only one replica of the token, moving copies of it to other ranks via NVLink. At EP64, where the chances of multiple replicas landing on the same node are smaller, the effect of this optimization tapers off. On combine, DeepEPs sender-side partial sum greatly reduces RDMA bytes, hence shows much lower latency, despite the reduced accumulation precision to bf16."
        },
        {
            "title": "8 CONCLUSION",
            "content": "Existing RDMA solutions for LLM systems suffer from vendor lock-in, with no viable implementations on custom cloud hardware such as AWS EFA. TransferEngine addresses this by identifying common functionality across heterogeneous RDMA hardware. By layering reliable abstraction without ordering guarantees over the underlying protocols, we transparently extended support to multiple RDMA NICs, with particular focus on EFA and ConnectX. We demonstrated this approach through three production systems: KvCache transfer for disaggregated inference, RL weight updates achieving 1.3 seconds for trillion-parameter models, and MoE dispatch/combine with state-of-the-art latency on ConnectX-7 and the first viable implementation compatible with AWS EFA. TransferEngine enables portable point-to-point communication for modern LLM architectures, avoiding vendor lock-in while complementing collective libraries for cloud-native deployments. RDMA Point-to-Point Communication for LLM Systems"
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "We thank NVIDIA for generously providing access to the ConnectX-7 hardware for our evaluations, alongside valuable insights towards maximizing performance on H200 GPUs. We thank AWS for their insights and advice towards improving performance on EFA."
        },
        {
            "title": "REFERENCES",
            "content": "Agostini, E., Rossetti, D., and Potluri, S. GPUDirect async: Exploring GPU synchronous communication techniques for InfiniBand clusters. J. Parallel Distributed Comput., 114:2845, 2018. doi: 10.1016/J.JPDC.2017.12.007. URL https://doi.org/10.1016/j.jpdc.2017.12. 007. Ainslie, J., Lee-Thorp, J., De Jong, M., Zemlyanskiy, Y., Lebron, F., and Sanghai, S. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. Alibaba Cloud. Elastic compute service: eRDMA, 2025. URL https://www.alibabacloud.com/help/ en/ecs/user-guide/elastic-rdma-erdma/. Chang, L.-W., Bao, W., Hou, Q., Jiang, C., Zheng, N., Zhong, Y., Zhang, X., Song, Z., Jiang, Z., Lin, H., Jin, X., and Liu, X. Flux: Fast software-based communication overlap on gpus through kernel fusion, 2024. DeepSeek AI. Fire-flyer file system (3fs). https: //github.com/deepseek-ai/3FS, 2025. Highperformance distributed file system for AI training and inference workloads. DeepSeek-AI, Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Guo, D., Yang, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Zhang, H., Ding, H., Xin, H., Gao, H., Li, H., Qu, H., Cai, J. L., Liang, J., Guo, J., Ni, J., Li, J., Wang, J., Chen, J., Chen, J., Yuan, J., Qiu, J., Li, J., Song, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Xu, L., Xia, L., Zhao, L., Wang, L., Zhang, L., Li, M., Wang, M., Zhang, M., Zhang, M., Tang, M., Li, M., Tian, N., Huang, P., Wang, P., Zhang, P., Wang, Q., Zhu, Q., Chen, Q., Du, Q., Chen, R. J., Jin, R. L., Ge, R., Zhang, R., Pan, R., Wang, R., Xu, R., Zhang, R., Chen, R., Li, S. S., Lu, S., Zhou, S., Chen, S., Wu, S., Ye, S., Ye, S., Ma, S., Wang, S., Zhou, S., Yu, S., Zhou, S., Pan, S., Wang, T., Yun, T., Pei, T., Sun, T., Xiao, W. L., Zeng, W., Zhao, W., An, W., Liu, W., Liang, W., Gao, W., Yu, W., Zhang, W., Li, X. Q., Jin, X., Wang, X., Bi, X., Liu, X., Wang, X., Shen, X., Chen, X., Zhang, X., Chen, X., Nie, X., Sun, X., Wang, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yu, X., Song, X., Shan, X., Zhou, X., Yang, X., Li, X., Su, X., Lin, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhu, Y. X., Zhang, Y., Xu, Y., Xu, Y., Huang, Y., Li, Y., Zhao, Y., Sun, Y., Li, Y., Wang, Y., Yu, Y., Zheng, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Tang, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Wu, Y., Ou, Y., Zhu, Y., Wang, Y., Gong, Y., Zou, Y., He, Y., Zha, Y., Xiong, Y., Ma, Y., Yan, Y., Luo, Y., You, Y., Liu, Y., Zhou, Y., Wu, Z. F., Ren, Z. Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Huang, Z., Zhang, Z., Xie, Z., Zhang, Z., Hao, Z., Gou, Z., Ma, Z., Yan, Z., Shao, Z., Xu, Z., Wu, Z., Zhang, Z., Li, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z., Xie, Z., Song, Z., Gao, Z., and Pan, Z. Deepseek-v3 technical report, 2025. URL https://arxiv.org/abs/2412.19437. Fu, W., Gao, J., Shen, X., Zhu, C., Mei, Z., He, C., Xu, S., Wei, G., Mei, J., Wang, J., Yang, T., Yuan, B., and Wu, Y. Areal: large-scale asynchronous reinforcement learning system for language reasoning, 2025. URL https:// arxiv.org/abs/2505.24298. He, B., Zhu, Z., and Li, J. Efficient rl training - optimizing weight sync in slime. https://hebiao064.github. io/rl-weight-sync, 2025. Accessed: 2025-10-14. Hu, J., Wu, X., Shen, W., Liu, J. K., Zhu, Z., Wang, W., Jiang, S., Wang, H., Chen, H., Chen, B., Fang, W., Xianyu, Cao, Y., Xu, H., and Liu, Y. Openrlhf: An easyto-use, scalable and high-performance rlhf framework, 2025a. URL https://arxiv.org/abs/2405.11143. Hu, Z., Shen, S., Bonato, T., Jeaugey, S., Alexander, C., Spada, E., Dinan, J., Hammond, J., and Hoefler, T. Demystifying NCCL: An in-depth analysis of GPU communication protocols and algorithms, 2025b. URL https://arxiv.org/abs/2507.04786. Huang, G., Chadha, P., Kong, T., Gao, W., and Li, Z. NeMoRL: Journey of optimizing weight transfer in large moe models by 10x. https://github.com/NVIDIA-NeMo/ RL/discussions/1189, 2025. Accessed: 2025-10-14. Kalia, A., Kaminsky, M., and Andersen, D. G. Design guidelines for high performance RDMA systems. In Gulati, A. and Weatherspoon, H. (eds.), Proceedings of the 2016 USENIX Annual Technical Conference, USENIX ATC 2016, Denver, CO, USA, June 22-24, 2016, pp. 437450. USENIX Association, 2016. URL https://www.usenix.org/conference/atc16/ technical-sessions/presentation/kalia. Kimi Team, Bai, Y., Bao, Y., Chen, G., Chen, J., Chen, N., Chen, R., Chen, Y., Chen, Y., Chen, Y., Chen, Z., Cui, J., Ding, H., Dong, M., Du, A., Du, C., Du, D., Du, Y., Fan, Y., Feng, Y., Fu, K., Gao, B., Gao, H., Gao, P., Gao, T., Gu, X., Guan, L., Guo, H., Guo, J., Hu, H., Hao, X., He, RDMA Point-to-Point Communication for LLM Systems T., He, W., He, W., Hong, C., Hu, Y., Hu, Z., Huang, W., Huang, Z., Huang, Z., Jiang, T., Jiang, Z., Jin, X., Kang, Y., Lai, G., Li, C., Li, F., Li, H., Li, M., Li, W., Li, Y., Li, Y., Li, Z., Li, Z., Lin, H., Lin, X., Lin, Z., Liu, C., Liu, C., Liu, H., Liu, J., Liu, J., Liu, L., Liu, S., Liu, T. Y., Liu, T., Liu, W., Liu, Y., Liu, Y., Liu, Y., Liu, Y., Liu, Z., Lu, E., Lu, L., Ma, S., Ma, X., Ma, Y., Mao, S., Mei, J., Men, X., Miao, Y., Pan, S., Peng, Y., Qin, R., Qu, B., Shang, Z., Shi, L., Shi, S., Song, F., Su, J., Su, Z., Sun, X., Sung, F., Tang, H., Tao, J., Teng, Q., Wang, C., Wang, D., Wang, F., Wang, H., Wang, J., Wang, J., Wang, J., Wang, S., Wang, S., Wang, Y., Wang, Y., Wang, Y., Wang, Y., Wang, Y., Wang, Z., Wang, Z., Wang, Z., Wei, C., Wei, Q., Wu, W., Wu, X., Wu, Y., Xiao, C., Xie, X., Xiong, W., Xu, B., Xu, J., Xu, J., Xu, L. H., Xu, L., Xu, S., Xu, W., Xu, X., Xu, Y., Xu, Z., Yan, J., Yan, Y., Yang, X., Yang, Y., Yang, Z., Yang, Z., Yang, Z., Yao, H., Yao, X., Ye, W., Ye, Z., Yin, B., Yu, L., Yuan, E., Yuan, H., Yuan, M., Zhan, H., Zhang, D., Zhang, H., Zhang, W., Zhang, X., Zhang, Y., Zhang, Y., Zhang, Y., Zhang, Y., Zhang, Y., Zhang, Y., Zhang, Z., Zhao, H., Zhao, Y., Zheng, H., Zheng, S., Zhou, J., Zhou, X., Zhou, Z., Zhu, Z., Zhuang, W., and Zu, X. Kimi k2: Open agentic intelligence, 2025. URL https://arxiv.org/abs/2507.20534. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient memory management for large language model serving with pagedattention. In Flinn, J., Seltzer, M. I., Druschel, P., Kaufmann, A., and Mace, J. (eds.), Proceedings of the 29th Symposium on Operating Systems Principles, SOSP 2023, Koblenz, Germany, October 23-26, 2023, pp. 611 626. ACM, 2023. doi: 10.1145/3600006.3613165. URL https://doi.org/10.1145/3600006.3613165. Langer, A., Howell, S., Potluri, S., Dinan, J., and Kraus, J. Dynamic symmetric heap allocation in NVSHMEM. In OpenSHMEM and Related Technologies. OpenSHMEM in the Era of Exascale and Smart Networks: 8th Workshop on OpenSHMEM and Related Technologies, OpenSHMEM 2021, Virtual Event, September 1416, 2021, Revised Selected Papers, pp. 187198, Berlin, Heidelberg, 2021. Springer-Verlag. ISBN 978-3-031-048876. doi: 10.1007/978-3-031-04888-3 12. URL https: //doi.org/10.1007/978-3-031-04888-3_12. Li, S., Zhao, Y., Varma, R., Salpekar, O., Noordhuis, P., Li, T., Paszke, A., Smith, J., Vaughan, B., Damania, P., and Chintala, S. PyTorch Distributed: Experiences on accelerating data parallel training. Proc. VLDB Endow., 13(12):30053018, 2020. doi: 10. 14778/3415478.3415530. URL http://www.vldb. org/pvldb/vol13/p3005-li.pdf. Licker, N., Hu, K., Zaytsev, V., and Chen, L. pplxkernels: Perplexity MoE kernels. https://github. com/perplexityai/pplx-kernels, 2025. Liu, A., Feng, B., Wang, B., Wang, B., Liu, B., Zhao, C., Dengr, C., Ruan, C., Dai, D., Guo, D., et al. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434, 2024. Mao, Z., Zhou, Y., Zhang, Y., Cui, C., Chen, Z., and Xu, Z. Previewing UCCL-EP: Flexible and efficient expert parallelism for cloud and beyond. https://uccl-project. github.io/posts/uccl-ep/, 2025. Mei, Z., Fu, W., Li, K., Wang, G., Zhang, H., and Wu, Y. Real: Efficient rlhf training of large language models with parameter reallocation. In Proceedings of the Eighth Conference on Machine Learning and Systems, MLSys 2025, Santa Clara, CA, USA, May 12-15, 2025. mlsys.org, 2025. MPI Forum. MPI: Message-Passing InterURL face Standard Version 5.0, https://www.mpi-forum.org/docs/mpi-5.0/ mpi50-report.pdf. June 2025. NVIDIA. GPUDirect RDMA, 2012. URL https://docs. nvidia.com/cuda/gpudirect-rdma/. NVIDIA. NVIDIA collective communication library (NCCL), 2015. URL https://developer.nvidia. com/nccl. NVIDIA. TensorRT-LLM. https://github.com/ ai-dynamo/nixl, 2023. NVIDIA. NIXL: NVIDIA inference xfer library. https: //github.com/ai-dynamo/nixl, 2025. OFIWG. libfabric: Open Fabrics Interfaces (OFI), 2014. URL https://github.com/ofiwg/libfabric. Patel, P., Choukse, E., Zhang, C., Shah, A., Goiri, I., Maleki, S., and Bianchini, R. Splitwise: Efficient genIn 51st erative LLM inference using phase splitting. ACM/IEEE Annual International Symposium on Computer Architecture, ISCA 2024, Buenos Aires, Argentina, June 29 - July 3, 2024, pp. 118132. IEEE, 2024. doi: 10.1109/ISCA59077.2024.00019. URL https://doi. org/10.1109/ISCA59077.2024.00019. Qin, R., Li, Z., He, W., Cui, J., Ren, F., Zhang, M., Wu, Y., Zheng, W., and Xu, X. Mooncake: Trading more storage for less computation KVCache-centric architecture for serving LLM chatbot. In 23rd USENIX Conference on File and Storage Technologies (FAST 25), pp. 155170, Santa Clara, CA, February 2025. USENIX Association. ISBN 978-1-939133-45-8. URL https://www.usenix. org/conference/fast25/presentation/qin. RDMA Point-to-Point Communication for LLM Systems Reda, W., Canini, M., Kostic, D., and Peter, S. RDMA is turing complete, we just did not know it yet! In Phanishayee, A. and Sekar, V. (eds.), 19th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2022, Renton, WA, USA, April 4-6, 2022, pp. 7185. USENIX Association, 2022. URL https://www.usenix.org/ conference/nsdi22/presentation/reda. Sergeev, A. and Balso, M. D. Horovod: fast and easy distributed deep learning in tensorflow, 2018. URL https://arxiv.org/abs/1802.05799. Shah, A., Jangda, A., Li, B., Rocha, C., Hwang, C., Jose, J., Musuvathi, M., Saarikivi, O., Cheng, P., Zhou, Q., Dathathri, R., Maleki, S., and Yang, Z. Msccl++: Rethinking gpu communication abstractions for cutting-edge ai applications, 2025. URL https://arxiv.org/abs/ 2504.09014. Shalev, L., Ayoub, H., Bshara, N., and Sabbag, E. cloudoptimized transport protocol for elastic and scalable HPC. IEEE Micro, 40(6):6773, 2020. doi: 10.1109/MM.2020. 3016891. URL https://doi.org/10.1109/MM.2020. 3016891. Shamis, P., Venkata, M. G., Lopez, M. G., Baker, M. B., Hernandez, O., Itigin, Y., Dubman, M., Shainer, G., Graham, R. L., Liss, L., et al. UCX: an open source framework for HPC network APIs and beyond. In 2015 IEEE 23rd Annual Symposium on High-Performance Interconnects, pp. 4043. IEEE, 2015. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q. V., Hinton, G. E., and Dean, J. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id= B1ckMDqlg. Sheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Shi, R., Potluri, S., Hamidouche, K., Perkins, J. L., Li, M., Rossetti, D., and Panda, D. K. Designing efficient small message transfer mechanism for inter-node MPI communication on InfiniBand GPU clusters. In 21st International Conference on High Performance Computing, HiPC 2014, Goa, India, December 17-20, 2014, pp. 110. IEEE Computer Society, 2014. doi: 10.1109/HIPC.2014.7116873. URL https://doi.org/ 10.1109/HiPC.2014.7116873. Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. Megatron-LM: Training multi-billion parameter language models using model parallelism, 2020. URL https://arxiv.org/abs/1909.08053. Singhvi, A., Dukkipati, N., Chandra, P., Wassel, H. M. G., Sharma, N. K., Rebello, A., Schuh, H., Kumar, P., Montazeri, B., Bansod, N., Thomas, S., Cho, I., Seibert, H. L., Wu, B., Yang, R., Li, Y., Huang, K., Yin, Q., Agarwal, A., Vaduvatha, S., Wang, W., Moshref, M., Ji, T., Wetherall, D., and Vahdat, A. Falcon: reliable, low latency hardware transport. In Curado, M., Rothenberg, C. E., Porter, G., and Kandula, S. (eds.), Proceedings of the ACM SIGCOMM 2025 Conference, SIGCOMM 2025, Sao Francisco Convent, Coimbra, Portugal, September 8-11, 2025, pp. 248263. ACM, 2025. doi: 10.1145/3718958.3754353. URL https: //doi.org/10.1145/3718958.3754353. Wu, B., Wang, S., Tang, Y., Ding, J., Helenowski, E., Tan, L., Xu, T., Gowda, T., Chen, Z., Zhu, C., Tang, X., Qian, Y., Zhu, B., and Hou, R. LlamaRL: distributed asynchronous reinforcement learning framework for efficient large-scale LLM training, 2025. URL https://arxiv.org/abs/2505.24034. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., Zheng, C., Liu, D., Zhou, F., Huang, F., Hu, F., Ge, H., Wei, H., Lin, H., Tang, J., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Zhou, J., Lin, J., Dang, K., Bao, K., Yang, K., Yu, L., Deng, L., Li, M., Xue, M., Li, M., Zhang, P., Wang, P., Zhu, Q., Men, R., Gao, R., Liu, S., Luo, S., Li, T., Tang, T., Yin, W., Ren, X., Wang, X., Zhang, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Zhang, Y., Wan, Y., Liu, Y., Wang, Z., Cui, Z., Zhang, Z., Zhou, Z., and Qiu, Z. Qwen3 technical report, 2025. URL https: //arxiv.org/abs/2505.09388. Ye, Z., Chen, L., Lai, R., Lin, W., Zhang, Y., Wang, S., Chen, T., Kasikci, B., Grover, V., Krishnamurthy, A., and Ceze, L. FlashInfer: Efficient and customizable attention engine for LLM inference serving, 2025. URL https://arxiv.org/abs/2501.01005. Zhang, S., Zheng, N., Lin, H., Jiang, Z., Bao, W., Jiang, C., Hou, Q., Cui, W., Zheng, S., Chang, L.-W., Chen, Q., and Liu, X. COMET: Fine-grained computationcommunication overlapping for mixture-of-experts. In Eighth Conference on Machine Learning and Systems, 2025. URL https://openreview.net/forum?id= fGgQS5VW09. Zhao, C., Zhou, S., Zhang, L., Deng, C., Xu, Z., Liu, Y., Yu, K., Li, J., and Zhao, L. DeepEP: an efficient expertparallel communication library. https://github.com/ deepseek-ai/DeepEP, 2025. RDMA Point-to-Point Communication for LLM Systems Zhao, Y., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M., Wright, L., Shojanazeri, H., Ott, M., Shleifer, S., Desmaison, A., Balioglu, C., Damania, P., Nguyen, B., Chauhan, G., Hao, Y., Mathews, A., and Li, S. PyTorch FSDP: Experiences on scaling fully sharded data parallel. Proc. VLDB Endow., 16(12):38483860, August 2023. ISSN 2150-8097. doi: 10.14778/3611540.3611569. URL https://doi.org/10.14778/3611540.3611569. Zheng, L., Yin, L., Xie, Z., Sun, C., Huang, J., Yu, C. H., Cao, S., Kozyrakis, C., Stoica, I., Gonzalez, J. E., Barrett, C. W., and Sheng, Y. Sglang: Efficient execution of structured language model programs. In Globersons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and Zhang, C. (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http:// papers.nips.cc/paper_files/paper/2024/hash/ 724be4472168f31ba1c9ac630f15dec8-Abstract-Conference. html. Zheng, S., Bao, W., Hou, Q., Zheng, X., Fang, J., Huang, C., Li, T., Duanmu, H., Chen, R., Xu, R., Guo, Y., Zheng, N., Jiang, Z., Di, X., Wang, D., Ye, J., Lin, H., Chang, L.-W., Lu, L., Liang, Y., Zhai, J., and Liu, X. Tritondistributed: Programming overlapping kernels on distributed ai systems with the triton compiler, 2025a. URL https://arxiv.org/abs/2504.19442. Zheng, S., Fang, J., Zheng, X., Hou, Q., Bao, W., Zheng, N., Jiang, Z., Wang, D., Ye, J., Lin, H., et al. Tilelink: Generating efficient compute-communication overlapping kernels using tile-centric primitives. arXiv preprint arXiv:2503.20313, 2025b. Zhong, Y., Liu, S., Chen, J., Hu, J., Zhu, Y., Liu, X., Jin, X., and Zhang, H. Distserve: Disaggregating prefill and decoding for goodput-optimized large language model In Gavrilovska, A. and Terry, D. B. (eds.), serving. 18th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2024, Santa Clara, CA, USA, July 10-12, 2024, pp. 193210. USENIX Association, 2024. URL https://www.usenix.org/conference/ osdi24/presentation/zhong-yinmin. Zhou, Y., Chen, Z., Mao, Z., Lao, C., Yang, S., Kannan, P. G., Gao, J., Zhao, Y., Wu, Y., You, K., Ren, F., Xu, Z., Raiciu, C., and Stoica, I. An extensible software transport layer for gpu networking, 2025. URL https: //arxiv.org/abs/2504.17307. Zhu, Z., Xie, C., Lv, X., and slime Contributors. slime: An llm post-training framework for rl scaling. https:// github.com/THUDM/slime, 2025. GitHub repository. Corresponding author: Xin Lv."
        }
    ],
    "affiliations": [
        "Perplexity AI"
    ]
}