{
    "paper_title": "DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers",
    "authors": [
        "Ionut-Vlad Modoranu",
        "Philip Zmushko",
        "Erik Schultheis",
        "Mher Safaryan",
        "Dan Alistarh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Shampoo is one of the leading approximate second-order optimizers: a variant of it has won the MLCommons AlgoPerf competition, and it has been shown to produce models with lower activation outliers that are easier to compress. Yet, applying Shampoo currently comes at the cost of significant computational slowdown, due to its expensive internal operations. In this paper, we take a significant step to address this shortcoming by proposing \\method (for \\textbf{D}istributed \\textbf{A}ccelerated \\textbf{SH}ampoo), a faster implementation of Distributed Shampoo based on two main new techniques: First, we show that preconditioner blocks can be stacked into 3D tensors to significantly improve GPU utilization; second, we introduce the Newton-DB iteration and the Chebyshev polynomial approximations as novel and faster approaches for computing the inverse matrix roots required by Shampoo. Along with these algorithmic contributions, we provide a first in-depth analysis of how matrix scaling critically affects Shampoo convergence. On the practical side, our GPU-aware implementation achieves up to $4.83\\times$ faster optimizer steps compared to the well-optimized Distributed Shampoo, while Newton-DB attains the lowest validation perplexity per iteration among all tested methods. Our code is available at https://github.com/IST-DASLab/DASH."
        },
        {
            "title": "Start",
            "content": "DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers Ionut-Vlad Modoranu 1 Philip Zmushko 1 Erik Schultheis 1 Mher Safaryan 2 Dan Alistarh"
        },
        {
            "title": "Abstract",
            "content": "Shampoo is one of the leading approximate second-order optimizers: variant of it has won the MLCommons AlgoPerf competition, and it has been shown to produce models with lower activation outliers that are easier to compress. Yet, applying Shampoo currently comes at the cost of significant computational slowdown, due to its expensive internal operations. In this paper, we take significant step to address this shortcoming by proposing DASH (for Distributed Accelerated SHampoo), faster implementation of Distributed Shampoo based on two main new techniques: First, we show that preconditioner blocks can be stacked into 3D tensors to significantly improve GPU utilization; second, we introduce the Newton-DB iteration and the Chebyshev polynomial approximations as novel and faster approaches for computing the inverse matrix roots required by Shampoo. Along with these algorithmic contributions, we provide first in-depth analysis of how matrix scaling critically affects Shampoo convergence. On the practical side, our GPU-aware implementation achieves up to 4.83 faster optimizer steps compared to the well-optimized Distributed Shampoo, while Newton-DB attains the lowest validation perplexity per iteration among all tested methods. Our code is available at https://github.com/IST-DASLab/DASH. 6 2 0 2 2 ] . [ 1 6 1 0 2 0 . 2 0 6 2 : r 1. Introduction The promise of faster adaptive gradient optimization methods has led to full spectrum of methods inspired by the full-matrix AdaGrad optimizer (Duchi et al., 2011). At one end, the full-matrix approach offers theoretical guarantees but is computationally prohibitive for large models due to 1Institute of Science and Technology Austria 2Lancaster University 3Red Hat AI. Correspondence to: Ionut-Vlad Modoranu <ionut-vlad.modoranu@ista.ac.at>. Preprint. February 3, 2026. 1 its O(m2n2) memory complexity for an layer. At the other end, diagonal approximations such as Adam (Kingma & Ba, 2014) and AdamW (Loshchilov & Hutter, 2017) reduce this complexity to O(mn), and have become standard for deep learning. Yet, diagonal methods fail to capture complex parameter correlations, leading to significant work on efficiently incorporating non-diagonal information into optimizers, without incurring the prohibitive costs of fullmatrix methods. One such instance is the Shampoo optimizer (Gupta et al., 2018; Anil et al., 2020), which captures layer-wise second-order information, while maintaining manageable memory complexity of O(m2 + n2), making higher-order optimization feasible for large-scale models. Historically, Shampoo has remained in the shadow of standard diagonal optimizers like AdamW. Recently, however, Shampoo has gained significant traction, highlighted by its leading performance in the AlgoPerf benchmarking competition (Dahl et al., 2023), where it proved to be the best in terms of wall-clock time required to reach target training performance. Further, recent studies suggest that converged solutions found by Shampoo possess desirable properties, such as improved generalization (Pascanu et al., 2025) and robustness to quantization (Vlassis et al., 2025). While Shampoos memory complexity is manageable, the optimizer still incurs substantial computational overhead per optimizer step, relative to AdamW. Specifically, the algorithm requires computing inverse matrix roots, an operation that typically scales as Θ(n3) for an preconditioner matrix. To amortize this cost, standard implementations update the preconditioners infrequently (e.g., every 10100 steps). However, this creates trade-off between runtime speed and optimization quality: for example, evidence from Vyas et al. (2024) (see Figure 1 therein) indicates that more frequent preconditioner updates directly lead to better performance. The Distributed Shampoo implementation (Shi et al., 2023) addresses some of these computational bottlenecks by splitting preconditioners into blocks of the size with < n, effectively reducing the complexity to O(Bn2). Yet, the underlying algorithms still have critical efficiency gaps: for instance, the default implementation still relies DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers on EVD1, an operation that is notoriously difficult to parallelize on GPUs. Although Distributed Shampoo introduced more GPU-friendly, matrix-multiplication-based alternative relying on the Coupled-Newton (CN) iteration, this is not enabled by default, likely due to concerns regarding numerical stability2. Motivated by this efficiency gap for Shampoo, in this paper we aim to significantly reduce the computational overhead of Shampoo while preserving its numerical precision. We focus on accelerating the inverse matrix root computation, which is the algorithms primary bottleneck, by making high-quality preconditioning practical for widespread use. To this end, we propose DASH (Distributed Accelerated SHampoo), high-performance implementation designed to fully leverage modern GPU architectures. By bridging the gap between theoretical efficiency and hardware support, DASH empowers researchers to investigate the optimizers properties in diverse settings without prohibitive runtimes. Our contributions are as follows: 1. We introduce DASH, novel GPU-efficient approach for block preconditioners. The sequential loops used in prior Distributed Shampoo implementation are replaced with independent blocks stacked into 3-D tensors and processed in parallel, to significantly increase GPU utilization. This architectural change, combined with halfprecision support (FP16), reduces the running time of the optimizer step by up to 5 compared to the standard Distributed Shampoo. 2. We investigate two new advanced linear algebraic approaches for computing matrix powers in the context of deep learning optimizers: the Newton-Denman-Beavers (Newton-DB) iteration (Higham, 2008) and Chebyshev polynomial approximation using Clenshaws algorithm (Cody, 1970; Boyd, 2001). Motivated by the design of recent optimizers like Muon (Jordan et al., 2024), we explicitly aim to minimize the number of iterations required for these methods without degrading model performance. We show that our implementation of Newton-DB achieves lower validation perplexity than Coupled-Newton and standard Eigen-Value Decomposition when implemented into both Distributed Shampoo and our DASH block-preconditioned approach. the distinct convergence behavior observed in CoupledNewton approaches compared to Newton-DB. 4. To address the Frobenius norm scaling limitations identified in our analysis, we introduce multi-PowerIteration, an efficient half-precision implementation of the Power Iteration algorithm. This method robustly estimates the spectral radius (avoiding local maxima) to provide optimal scaling for the preconditioner blocks. This enables the Newton procedures to satisfy convergence criteria rapidly, further reducing the computational cost. Our paper is structured as follows: Section 2 introduces notation, Shampoo optimizer and its features; Section 3 covers the inverse root methods, as well as the analysis of the iterative methods and multi-Power-Iteration; Section 4 describes the efficient blocking strategy; Section 5 describes experimental results and we finally conclude with related work and discussion in Section 6. We present the Chebyshev polynomial technique in Appendix A. 2. Preliminaries on Shampoo This section introduces the notation used throughout the paper, simplified version of the Shampoo optimizer and highlights the features (e.g. heuristics) used in Distributed Shampoo which our DASH implementation inherits, as well as new features we introduce in our work. 2.1. Notation Rmℓnℓ}NL Rmℓnℓ}NL Throughout, we use θt = {θℓ ℓ=1 for the set of model parameters at optimization step with NL layers and Gt = {Gℓ ℓ=1 for the associated gradient; fθt(xt) for the model output that takes as input some sample data xt with associated label yt; L(ˆyt, yt) for the loss function that requires as input the model prediction ˆyt and the target label yt; for the block size used to split the gradient and subsequent states of Shampoo into blocks. In addition, we will use some abbreviations for the approaches we detail in our work: EVD for Eigen-Value Decomposition, CN for Coupled-Newton, NDB for Newton-Denman-Beavers and CBSHV for Chebyshev polynomials. 2.2. The Shampoo Optimizer 3. We provide behavioral analysis of Newton-based iterations for matrix root computation. We demonstrate that the standard Frobenius norm scaling is suboptimal because it results in slower convergence, thereby requiring higher number of iterations to reach the desired precision. Additionally, we offer intuitive explanations for 1See the Official Distributed Shampoo repository on GitHub. 2See the discussion in Section 3.2. Given gradient matrix Gt Rmn, Shampoo computes left and right preconditioning matrices Lt Rmm and Rt Rnn, incorporating products GtG Gt respectively as an exponential moving average (EMA) parameterized by βLR. Algorithm 1 presents pseudocode for Shampoo for single matrix, where ϵI is regularization term for matrices Lt and Rt. DASH inherits features from Distributed Shampoo, as described in Section 2.3. and 2 DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers Algorithm 1 Simplified Outline of the Shampoo Optimizer Initialize L0 = 0mm, R0 = 0nn, βLR (0, 1) for = 1 to do Gt = θL(fθt(xt), yt) Lt = βLR Lt1 + (1 βLR) GtG Rt = βLR Rt1 + (1 βLR) Gt θt+1 = θt ηt (Lt + ϵIm)1/4 Gt (Rt + ϵIn)1/4 end for 2.3. Features Inherited from Distributed Shampoo In this section we describe the techniques from Distributed Shampoo which we also integrated in our DASH implementation. We followed the pseudocode described in Shi et al. (2023, Algorithms 2 and 3). Grafting. Grafting (Agarwal et al., 2020) is technique introduced to transfer the learning rate schedule from another model. Specifically, it consists in using the unit-length direction from the current optimizer (Shampoo) and re-scale it to the norm of the other optimizer (Adam), for which we already have tuned learning rate schedule. In short, this technique is called Adam grafting (applied to Shampoo). The preconditioners for both Shampoo and grafting method are updated based on the same sequence of iterates. Grafting is mandatory to have numerically stable implementation for Shampoo. To briefly explain grafting, let Ut = L1/4 Gt R1/4 be the Shampoo update and Pt = Gt/(ϵ + At) be the Adam-grafting direction (if EMA is enabled for Gt, one can use Mt = βGMt1 + (1 βG)Gt instead). Then, the model is updated as θt+1 = θtηtstUt, where st = PtF /UtF . For each layer, we implement block-wise grafting using Adam rule in our DASH as explained in Algorithm 2 in Distributed Shampoo. Load Balancing. We implement the load-balancing approach explained in Algorithm 3 in Distributed Shampoo, which decides which GPU will process one specific layer. This is greedy algorithm that sorts all layers by the total number of parameters in descending order and allocates each layer to the GPU that has the lowest load among all workers. The parameters are scattered on different workers to avoid redundant computations (according to the optimizer state partitioning strategy in ZeRO (Rajbhandari et al., 2020)). After all GPUs updated their own parameters assigned by the greedy procedure, we broadcast the updated parameters to synchronize the model across all workers. 2.4. New Features in DASH Lower-Precision Iterations. Distributed Shampoo implemented the CN approach in float32 (FP32) precision. We introduce CN for FP16, which reduces the runtime of optimizer step by around 10% compared to the FP32, with no degradation in validation perplexity. In the context of CBSHV (details in Appendix A), using FP16 improves the validation perplexity and reduces the running time, while for NDB it leads to numerical instabilities. We leave the investigation of FP16 for NDB for future work. Efficient Kernels. Dion (Ahn et al., 2025) introduced efficient triton kernels to compute , which we also employ in our optimizer to speed up computations for the CN approach. Memory Usage. Our strategy to stack blocks in conjunction with the load balancing algorithm achieves better memory utilization across workers than Distributed Shampoo. For example, in our experiments for the 953M parameters in setting with 8 GPUs, Distributed Shampoo uses 76GB memory per GPU, while our DASH uses 73 GB for higher rank workers and and 71 GB for lower rank workers. 3. Inverse Root Methods This section first details the default numerical methods used in Shampoo, and then describes our additional Newton-DB approach for computing inverse roots A1/2 and A1/4 for given matrix A, followed by discussion on the importance of matrix scaling for the iteration convergence and our improvement to the Power-Iteration method. An additional technique for inverse roots based on Chebyshev polynomials can be found in Appendix A. 3.1. EVD: Eigen-Value Decomposition Given symmetric matrix Rnn and R, standard approach to compute matrix powers Ap is to perform the EVD of as = QΛQ, where are the eigenvectors of and Λ are the eigenvalues of A, followed by Ap = QΛpQ. Despite its accuracy, EVD has the issue that its computation is hard to parallelize on GPUs, as the underlying algorithm is iterative, requiring building Krylov subspace, re-orthogonalization of iterates and tridiagonalization (Lanczos, 1950; Ojalvo & Newman, 1970). The procedure has runtime Θ(n3), becoming prohibitive for large matrices. Even though Distributed Shampoo (Shi et al., 2023) performs this operation in blocks of size as trade-off heuristic to reduce complexity from Θ(n3) to Θ(B3) (for each of resulting blocks), EVD still incurs large overhead for multiple such blocks because the inverse root is computed for each block sequentially. Therefore, iterative methods, such as Newton iterations based only on matrix-multiplications are fit for this scenario. While less accurate than EVD, they benefit from the high-performance primitives of optimized routines (matmul/gemm/bmm) powered by tensor-cores. In addition, our aim is to minimize the number of iterations for the Newton-like procedures to minimize the running time of DASH. DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers 3.2. CN: The Coupled-Newton Iteration The Coupled-Newton iteration (Higham, 2008, Equation 7.18) is implemented in Anil et al. (2020) as faster alternative to the EVD approach. Given an input matrix Rnn with eigenvalues λi(A) [0, (p + 1)cp] with the constant > 0, it computes A1/p iteratively, as described in Equations 1, 2 and 3. This requires defining two A1/p sequences of matrices Xk and Mk with Xk and Mk In. 1 (cid:18) 1 cp X0 = In, M0 = (cid:19) 1 + Ck = 1 Xk+1 = XkCk, Mk+1 = In Mk 1 Mk (1) (2) (3) The matrix Mk is introduced for numerical stability and its closeness to the identity matrix In within desired tolerance can be used as condition for early stopping. In Equation 2, the matrix Ck is linear combination of the current iterate Mk and identity In and it has to be raised to the p-th power to compute the next iterate Mk+1 in Equation 3. The overhead per iteration for the CN approach is one matmul for the Xk term, followed by one matmul for = 2 and 2 matmuls for = 4 to compute the , plus one additional matmul to finally compute Mk+1. In total, the method requires 3 matmuls for = 2 and 4 matmuls for = 4. 3.3. NDB: The Newton-Denman-Beavers Iteration The Newton-Denman-Beavers iteration (Higham, 2008, Equation 6.35) is an iterative procedure that computes both the square and inverse square roots of an input matrix Rnn, for which the condition A2 < 1 holds. The NDB iteration is shown in Equations 4, 5 and 6, where it requires two sequences of matrices Yk and Zk with A1/2. To compute inverse Yk fourth root, we need two calls to the NDB procedure: from the first call we keep the square root, then input it to the second NDB call, keeping only the inverse square root which is actually the inverse fourth root (e.g. (A1/2)1/2 = A1/4). A1/2 and Zk Y0 = A, 1 2 Ek1 = Z0 = In (3I Zk1Yk1) Yk = Yk1Ek1, Zk = Ek1Zk1 (4) (5) (6) The overhead per iteration for the NDB approach is one matmul for each of the terms Ek, Yk, Zk, resulting in 3 matmuls per iteration. However, by inspecting the first iteration we observe two of these three matmuls are redundant. Specifically, under the initialization in Equation 4, the first iteration yields E1 = 3 2 A, Y1 = E1 and Z1 = E1. The 2 4 standard formulation computes E1 and Z1 via explicit matrix multiplications, despite their closed-form expression. To eliminate this redundancy, we directly initialize E1, Y1 and Z1 to their values mentioned above and continue the iterative procedure from the second iteration onward, thus saving two matrix multiplications for the first iteration without altering the algorithmic result. Since NDB computes powers A1/2, chaining multiple calls can only compute powers A1/2k with N. However, this is not shortcoming in the context of Shampoo, since we only need A1/2 and A1/4. 3.4. Analyzing Matrix Scaling vs. Iteration Convergence When using iterative procedures to compute inverse roots, the input matrix Rnn requires some initial conditions to hold in order for the iteration to converge. We will consider the initial condition for the NDB and CN methods as an example, i.e. A2 < 1 for NDB and A2 < (p+1)cp for CN, where X2 is the operator norm of (i.e. the largest eigenvalue of the matrix X, denoted by λmax(X)). In order to have the same interval for the two methods and to simplify our discussion for the CN approach, we will use (p + 1)cp = 1, i.e. = (1 + p)1/p. In theory, one should scale the matrix by λmax(A) and the standard approach to compute λmax(A) (besides inefficient EVD) is Power-Iteration. This iterative procedure computes an estimation of the eigenvector that corresponds to the largest eigenvalue (with slight abuse of terms, we will call it the largest eigenvector), which is then plugged into the Rayleigh quotient, defined as RA(x) = (xAx) / (xx) to estimate the largest eigenvalue. In Distributed Shampoo, the matrix is scaled by the Frobenius norm AF , which is an upper bound on λmax(A) and is computationally cheaper than Power-Iteration in their implementation. In practice, the gap between these quantities is quite large, with the Frobenius norm being larger than λmax(A) by around 10 100. Our hypothesis is that these two choices of matrix scaling influence convergence, meaning that the iterative procedure will need more steps to converge to target error, specifically when scaling the matrix by the Frobenius norm, which pushes eigenvalues towards zero. To validate this hypothesis, we designed the following numerical experiment: we input eigenvalues with different magnitudes in the interval (0, 1) to NDB and CN, and record the number of iterations required to compute the inverse square root up to fixed precision. Optionally, we also get the square root from the NDB approach. Figure 1 confirms our hypothesis: smaller eigenvalues require more steps for NDB and CN to converge to the target values x1/2 and x1/2 compared to the values closer to 1. DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers Figure 1. Number of steps required for NDB and CN to compute the square and inverse square roots of scalar numbers between 0 and 1 (in log-scale) up to precision 1010 to emphasize the behavior for small eigenvalues. Figure 2. Number of steps required for NDB and CN to compute the square and inverse square roots of scalars between 0 and 1 (in linear scale) up to precision 1010. We added shift for NDB iterations to improve visibility on the y-axis. The required number of steps to achieve fixed precision is inversely proportional to the value of x. Since we use fixed number of steps (e.g., 10) for both approaches in Shampoo, the error approximation for each eigenvalue depends on its magnitude: if the eigenvalue is small, using only 10 steps would yield larger approximation error because in reality the iterative procedure requires more than 10 to achieve the same error. Concretely, suppose the Frobenius norm is larger than the largest eigenvalue by 50. Then, an eigenvalue λ = 1e-2 that would normally require 5 steps to converge with CN would become λ = 2e-4 after scaling it by frobenius norm, which would now require 15 steps with the same procedure. Interestingly, the CN approach exhibits significantly different behavior than NDB for the interval (0.3, 1). In Figure 2, we plot the x-axis on linear scale to emphasize the behavior of NDB and CN in this interval, where the values require more steps to converge for CN, with peak of number of iterations around 1. This scenario will be encountered in practice when we use an accurate approximation of λmax(A), regime where NDB requires fewer steps than CN. This supports the usage of our NDB iteration in Shampoo instead of CN. In Section 5 we show that NDB consistently yields models with lower validation perplexity. consequence of the above discussion is that good approximation of λmax(A) via Power-Iteration would have the effect of pushing the spectrum of towards 1 in regime where we require fewer iterations. Recall that, for real symmetric matrix Rnn and any vector Rn, the Rayleigh quotient is RA(x) = (xAx)/(xx), which is 5 upper-bounded by the largest eigenvalue λmax(A), achieved when is the largest eigenvector. For any other vectors, Rayleigh quotient returns λ(x) = RA(x) < λmax(A), which is not enough to satisfy the convergence condition. Let vPI be our estimation for the largest eigenvector obtained via Power-Iteration. Since vPI is just an estimation of v, the associated largest eigenvalue via Rayleigh quotient is λPI = RA(vPI) that satisfies the condition λPI < λmax(A). Therefore, we chose to divide the matrix by 2λPI instead of Frobenius norm AF. This way, we make sure the estimation for the eigenvalue returned by Power-Iteration satisfies the convergence condition of NDB and CN approaches. In Section 5 we show that scaling by the Frobenius norm leads to numerical instabilities for NDB for larger block sizes, while scaling by Power-Iteration is stable, which supports our claim from this section. 3.5. Multi-Power-Iteration In Distributed Shampoo, scaling the matrix by Frobenius norm is cheaper than the Power-Iteration. In contrast, our DASH implementation makes Power-Iteration computationally cheap specifically since we work with stacked blocks, which allows us to estimate all the largest eigenvectors at once. It is known that the Power-Iteration can converge to an eigenvector that is not the largest (e.g. it does not correspond to the largest eigenvalue, but to smaller one). To minimize the likelihood of this scenario, we can improve the estimation by using pool of up to 16 or 32 starting vectors to estimate the largest eigenvectors in parallel. In the end, we choose the eigenvector with the largest Rayleigh quotient. We call this approach multi-Power-Iteration. We emphasize that using multi-Power-Iteration instead of simple Power-Iteration does not increase the practical runtime. Matrix-vector multiplication is memory-bound operation, with the memory transfers dominated by the size of A, so multiplying more vectors at the same time has negligible effect on the transfer size. By choosing the number of vectors to be multiple of 16, we ensure the efficient use of tensor cores. 4. Blocking Strategy In this section we present the blocking strategy implemented in Distributed Shampoo and how we turn it into our efficient strategy that powers up DASH. In turn, this leads to reduced running time for the optimizer step. In practice, DASH reduces the running time by up to 4 5 compared to Distributed Shampoo. Suppose we have layer of shape (m, n), where both and are perfectly divisible by B. Let Nm = m/B and Nn = n/B be the number of blocks for the and ndimension respectively and = NmNn be the total number of blocks DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers of size in the gradient matrix G. Therefore, we can write the gradient in block-matrix form as: G11 G21 ... = G12 G22 ... GNm1 GNm2 G1Nn G2Nn . . . GNmNn ... , Gij RBB Our DASH implementation stacks all these blocks Gij into 3D matrix block(G) RN BB and applies batched operations for each inverse root procedure. This batching approach slightly improves the running time for EVD and it is especially fast for all other matrix-based approaches, such as the built-in CN, but also the NDB and CBSHV methods we introduce in this work. After creating block(G), the Shampoo optimizer is implemented using the block matrix structure. Therefore, the products GG are computed using block(G), where the transposition swaps the last two Bdimensions on the right. In order to understand how this leads to practical speedups, let us consider detailed example. First, we make the convention that RN BB can be written as (N, B, B) to improve the visibility of dimensions. Let us consider block size = 1024 and an embedding layer with vocabulary size = 32 000 and embedding size = 2048, leading to layer with shape (V, E) = (32 000, 2048), with number of blocks Nm = 31.25 = 31 and Nn = 2, with total of = 62 full blocks of shape (B, B). Since mod = 256, we have two smaller blocks of shape (256, 1024). In the end, the gradient is split into block matrices Gfull (62, 1024, 1024) and Grest (2, 256, 1024). Next, we obtain the corresponding blocks for and as follows: Lfull (62, 1024, 1024), Lrest (2, 256, 256), Rfull (62, 1024, 1024), Rrest (2, 1024, 1024). Their corresponding inverse root matrices will have exactly the same shapes, which are the results of block-wise (or batch) matrix multiplications GG and GG. Our key observation is that matrices Lfull, Rfull and Rrest can be stacked together because they have the same shape (B, B) = (1024, 1024). In contrast to Distributed Shampoo, which computes inverse root of each block matrix of shape (B, B) sequentially, we can apply exactly the same inverse root procedure on the stacked matrices by performing only one call to our chosen inverse root procedure. EVD already supports batched matrices; the other approaches, such as CN, NDB, CBSHV, have to be modified to support batched matrix multiplications (bmm). Concretely, we define the operator stack(X, Y, Z) that stacks the matrices (NX , B, B), (NY , B, B) and (NZ, B, B) into (NX + NY + NZ, B, B). Therefore, we obtain the stacked matrix Sfull = stack(Lfull, Rfull, Rrest) (126, 1024, 1024) and Srest = Lrest (2, 256, 256). The classification head will have the resulting stacked matrices will have the Lrest and Rrest blocks swapped. Normalization Layers. For model with normalization layers, each of shape E, and block size B, we stack all layers together into tensor of shape (N, E). Concretely, for = 2048 and = 1024, the gradient will have shape (2N, B, 1) and the matrix and its inverse root will have shape (2N, B, B). After computing the preconditioned gradient = L1/2 of shape (2N, B, 1), we convert back to shape (N, E) and then we update each of the normalization layers individually. full and S1/p rest In our DASH implementation, we store the stacked 3-D tensors Sfull, Srest, S1/p for {2, 4}. Instead, Distributed Shampoo stores two lists for L, R, L1/p that contain individual blocks. It is important to mention that we need to store the inverse root buffers because Distributed Shampoo has the option to recompute them once at steps to alleviate the overhead of expensive procedures, such as EVD. In between two calls to the inverse root procedures, we have to use the stored buffers. In Section 5 we show that our implementation is fast even with = 1. Running any inverse root procedure on batch of matrices in our DASH implementation is faster than running on each individual matrix because we can benefit from the high throughput of the tensor-cores. Stacking benefits. Stacking blocks of the L, matrices and for their corresponding inverse root avoids memory fragmentation, which occurs in Distributed Shampoo when individual blocks are stored in lists. Therefore, the matrix multiplications performed with stacked blocks (for non-EVD approaches) become more efficient. Since working with stacked matrices is faster, this allows experimenting with lower-precision formats for matmuls (e.g. float16) and computing more accurate estimations for the largest eigenvalue used to scale the input matrix for the iterative procedures (more details in Section 3.4). 5. Experimental Results We now show practical results for DASH compared to Distributed Shampoo (denoted by DIST) with respect to validation perplexity and running time of optimizer step. Setting. We pretrain Llama model with 953M parameters with embedding size = 2048, sequence length 1024 and 2M token batch size with Chinchilla-optimal (Hoffmann et al., 2022) token counts (20 tokens/parameter) from the C4 dataset (Raffel et al., 2020), which results in 9089 optimization steps. We run 3 seeds for each experiment and show the average validation perplexity and running times. Since the converged runs are extremely stable, we omit standard deviations in the reporting of results. 6 DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers Learning Rate. Since we use Adam grafting for both Shampoo implementations, we first run grid search for AdamW and choose the learning rate that achieved the lowest validation perplexity to be used for our Shampoo runs. We found that the learning rate η = 1e-3 performed the best across our grid η {1e-4, 2e-4, 4e-4, 1e-3, 2e-3, 4e-3}. The running time for the forward and backward passes will be the same for all models, and we only focus on measuring the running time of the optimizer step. Block Size. We use preconditioner block sizes {1024, 2048} to validate the observation in Distributed Shampoo that increasing the block size leads to closer approximation of full-matrix Shampoo. We posit that the block size should not be set to larger value than the embedding size E, otherwise the blocks will be guaranteed to have rank at most E, which in turn adds noise to the spectrum of preconditioners. We experiment with preconditioner update frequencies {1, 10} for EVD and = 1 for CN and NDB. Table 1. Results for Llama-953M for Distributed Shampoo (DIST) and DASH. Abbreviations: for block size, IR for inverse root method, FREQ for preconditioner update frequency, TIME for optimizer step in milliseconds, NORM matrix normalization (FRO for Frobenius norm and and PI for Power-Iteration) and PREC for floating point precision FP16/32 used for the inverse root method. The blue text shows the results of our contribution. IR FREQ VAL PPL () TIME () NORM/PREC 2K 1K EVD CN NDB EVD CN NDB 1 10 1 1 10 1 1 DIST DASH DIST DASH 11.72 11.73 2200 1747 209 11.83 11.85 253 - - 11.87 11.87 11.87 11.87 11.68 11.73 11. 675 243 355 221 169 279 284 FRO / FP32 FRO / FP16 FRO / FP32 PI / FP32 11.80 11.81 3080 2850 315 11.91 11. 355 - - 11.87 11.87 11.87 11.87 11.76 11.77 11.69 11.68 666 471 558 149 138 188 194 FRO / FP32 FRO / FP16 FRO / FP32 PI / FP32 Benchmarking. Our results are summarized in Table 1, where we benchmarked the existing DIST implementation and DASH. Since our purpose is to speed up the computations in Shampoo, we compare the running time of optimizer step and validation perplexity in our evaluation. We are also interested in how inverse root methods EVD, CN and NDB influence these two metrics and how they behave in our DASH implementation. Overall Trends. DASH matches DIST in almost all stable configurations with respect to validation perplexity, while substantially reducing the running time of optimizer step 7 by up to 4 in one-to-one comparisons for each inverse root method and up to 5 when comparing different inverse root methods. Interestingly, some setups achieved lower validation perplexity for NDB than EVD. EVD. For both block sizes, DASH achieves identical validation perplexity compared to DIST, with differences being 0.01, while consistently reducing the runtime. Concretely, for = 2048 and = 1, DASH reduces running time from 2200ms to 1747ms (1.26 lower) and with = 10 from 253ms to 209ms (1.21 lower). Similar trend holds for = 1024, where DASH improves runtime from 3080ms to 2850ms (1.08 lower) for = 1 and from 355ms to 315ms (1.13) for = 10. We would like to emphasize that the running time for = 10 is averaged across 10 runs. The running time incurred for one preconditioning step is identical to the one for = 1, but for the other 9 optimization steps in between two consecutive updates it takes roughly 35ms to perform the step, which includes updating preconditioners and applying the inverse root previously computed and cached. CN. For both FP32 and FP16 variants with Frobenius normalization, DASH exactly matches the validation perplexity 11.87 of DIST, while yielding the largest relative speedups. For = 2048, DASH reduces the running time from 675ms to 221ms (3.05 lower) for FP32 and from 243ms to 169ms for FP16. We can also cross-compare the running time of DIST-CN-FP32 of 675ms with our contribution DASHCN-FP16 of 169ms (4 lower). For = 1024, DASH reduces the running time from 666ms to 149ms (4.47 lower) for FP32 and from 471ms to 138ms (3.41 lower) for FP16. We can again cross-compare the running time of DIST-CN-FP32 of 666ms with DASH-CNFP16 of 138ms (4.83 lower), which is the highest relative improvement in running time in our evaluation. These results highlight that the block strategy and half-precision iterations provide substantial reduction in runtime while preserving model performance. In our experiments we omitted the Power-Iteration scaling for CN because it does not affect the results in any way. The high precision of FP16 format leads to no loss in validation perplexity and, as expected, it improves the running time. We also experimented with BF16 instead of FP16 and the CN method diverges. This is caused by the lower precision of the BF16 format, despite having the same range as FP32. NDB. For both Frobenius norm and Power-Iteration normalizations, NDB consistently achieves lower validation perplexity than CN (the built-in iterative inverse root method in DIST), while matching and even outperforming EVD across all preconditioner block sizes in both DIST and DASH implementations. DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers For = 2048 and Power-Iteration normalization, DASH reduces the running time from 355ms to 284ms (1.25 lower) and achieves the same validation perplexity as EVD with = 1. However, the runs using Frobenius normalization failed for DIST across all seeds and therefore we skipped the results. For = 1024 and Frobenius normalization, DASH reduces the running time from 558ms to 188ms (2.97 lower), while achieving lower validation perplexity than EVD. However, when using Power-Iteration normalization, the runtime of DIST increases to 740ms because the Power-Iteration is not efficient when preconditioning the blocks sequentially. In contrast, DASH with Power-Iteration normalization requires 194ms (3.81 lower). We would like to emphasize that scaling by Frobenius norm achieves 11.76 validation perplexity, while Power-Iteration achieves significantly lower value of 11.68. This is practical validation that Power-Iteration is more accurate normalization choice than Frobenius, as shown in Section 3.4. In our experiments we omitted the FP16/BF16 results because NDB did not converge for these formats. We leave further investigations for future work. Optimal DASH Configuration. Assume we wish to choose the best configuration of preconditioner block size B, inverse root method, precision and normalization to crosscompare the methods based on our results in Table 1. When optimizing for validation perplexity, one should definitely choose the NDB with Power-Iteration scaling approach we introduce, which is shown to achieve the lowest loss in our setting. For practitioners who care about minimizing the optimizer runtime at all cost and afford trading some validation perplexity for faster runtime, we suggest using CN with Frobenius normalization executed in FP16, which achieves the lowest runtime in our table. Regarding block size, we observed that increasing from 1024 to 2048 only implies increased running time without significant improvement in validation perplexity. In fact, both block sizes achieve similar performances, but at different running times. Reduction in Overall Iteration Time. In all our experiments we have the same runtime for the forward (FWP) and backward (BWP) passes and the only change is the inverse root method, which impacts the running time of optimizer step (OPT). We would like to express the reduction in OPT as total reduction in time for the entire training run and we take the results for the preconditioner block size = 1024 as an example. To keep the comparison simple, we will consider 9000 optimization steps instead of actual 9089. In our setup, each FWP requires 1000ms, and each BPW requires 3000ms and on top of that we add the optimizer time. We chose the DIST-EVD-10 that requires 355ms, resulting in 10h 53m runtime and CN-FP16 that requires 138ms, resulting in 10h 21m. Overall, this is reduction of roughly 30m (5%) for training 953M model. 6. Related Work and Discussion Second-order methods can yield faster convergence, but can incur prohibitive quadratic runtime costs. The first approach to mitigate this was K-FAC (Martens & Grosse, 2015), which was precursor to Shampoo (Gupta et al., 2018; Anil et al., 2020) and similar matrix adaptive optimizers (Agarwal et al., 2019). The optimization community aimed at reducing the overhead of Shampoo in different ways. For example, prior work quantizes to 4-bit the eigenvectors obtained from Eigen-Value Decomposition (Wang et al., 2024) or even the preconditioning matrices themselves (Li et al., 2025) via Cholesky decomposition. Yen et al. (2023) focused on reducing the running time by performing block low-rank decomposition using randomized SVD with shared basis in the context of AdaGrad by employing QR-decomposition to compute the dominant eigen-space, procedure that has the same cubic complexity as the Eigen-Value Decomposition. On similar note, SOAP (Vyas et al., 2024) computes Adafactor updates in the eigen-basis of Shampoo. Further, Xie et al. (2025) suggests using only the left preconditioner in Shampoo to reduce the overhead of inverse root methods, while Lin et al. (2025) recasts the Shampoo estimation as covariance estimation under KL-Divergence. Recently, Eschenhagen et al. (2025) performed an analytical deconstruction of Shampoo heuristics, suggesting that eigenvalue correction can replace grafting, which is compatible with our robust Power-Iteration approach. Discussion. We presented DASH, high-performance implementation of the Shampoo optimizer that bridges the gap between theoretical second-order guarantees and practical efficiency. By revisiting the algorithm from systems perspective, we identified that the primary bottleneck was not mathematical complexity, but fragmented execution and numerical instability. Our contributions come in two distinct areas: numerical analysis (convergence, different solvers, multi-Power-Iteration) and system design (block execution to leverage TensorCores). Our approach opens several avenues for future work. For instance, the superior performance of Newton-DB suggests that dynamic solver selection might be beneficial: one could estimate the condition number of block, and select the best/cheapest solver for it on the fly. second interesting challenge is to stabilize our proposed Newton-DB iteration in the case of lower-precision execution. This could work by combining stochastic or error-correction methods, that have been shown to be effective for different preconditioners (Modoranu et al., 2023). third direction is to validate our approach at even larger model and data scale, and leverage it in the context of tensor-parallel training. 8 DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of machine learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to thank the Scientific Computing Department at ISTA for providing access to computational resources to develop this work. MSs work was supported by Research England under the Expanding Excellence in England (E3) funding stream, which was awarded to MARS: Mathematics for AI in Real-world Systems in the School of Mathematical Sciences at Lancaster University."
        },
        {
            "title": "References",
            "content": "Agarwal, N., Bullins, B., Chen, X., Hazan, E., Singh, K., Zhang, C., and Zhang, Y. Efficient full-matrix adaptive regularization. In International Conference on Machine Learning, pp. 102110. PMLR, 2019. Agarwal, N., Anil, R., Hazan, E., Koren, T., and Zhang, C. Disentangling adaptive gradient methods from learning rates. arXiv preprint arXiv:2002.11803, 2020. Ahn, K., Xu, B., Abreu, N., and Langford, J. Dion: Distributed orthonormalized updates. arXiv preprint: 2504.05295, 2025. Anil, R., Gupta, V., Koren, T., Regan, K., and Singer, Y. Scalable second order optimization for deep learning. arXiv preprint arXiv:2002.09018, 2020. Boyd, J. P. Chebyshev and Fourier spectral methods. Courier Corporation, 2001. Clenshaw, C. W. note on the summation of chebyshev series. Mathematics of Computation, 9(51):118120, 1955. Cody, W. J. survey of practical rational and polynomial approximation of functions. SIAM Review, 12(3):400 423, 1970. doi: 10.1137/1012082. URL https:// doi.org/10.1137/1012082. Dahl, G. E., Schneider, F., Nado, Z., Agarwal, N., Sastry, C. S., Hennig, P., Medapati, S., Eschenhagen, R., Kasimbeg, P., Suo, D., et al. Benchmarking neural network training algorithms. arXiv preprint arXiv:2306.07179, 2023. Eschenhagen, R., Defazio, A., Lee, T.-H., Turner, R. E., and Shi, H.-J. M. Purifying shampoo: Investigating shampoos heuristics by decomposing its preconditioner. arXiv preprint arXiv:2506.03595, 2025. Gupta, V., Koren, T., and Singer, Y. Shampoo: Preconditioned stochastic tensor optimization, 2018. URL https://arxiv.org/abs/1802.09568. Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van Kerkwijk, M. H., Brett, M., Haldane, A., del Rıo, J. F., Wiebe, M., Peterson, P., Gerard-Marchant, P., Sheppard, K., Reddy, T., Weckesser, W., Abbasi, H., Gohlke, C., and Oliphant, T. E. Array programming with NumPy. Nature, 585(7825):357362, September 2020. doi: 10. 1038/s41586-020-2649-2. URL https://doi.org/ 10.1038/s41586-020-2649-2. Higham, N. J. Functions of matrices: theory and computation. SIAM, 2008. Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Jordan, K., Jin, Y., Boza, V., Jiacheng, Y., Cecista, F., Newhouse, L., and Bernstein, J. Muon: An optimizer for hidden layers in neural networks, 2024. URL https://kellerjordan.github.io/posts/muon, 6, 2024. Kingma, D. P. and Ba, J. Adam: method for stochastic optimization, 2014. URL https://arxiv.org/abs/ 1412.6980. Lanczos, C. An iteration method for the solution of the eigenvalue problem of linear differential and integral operators. Journal of research of the National Bureau of Standards, 45(4):255282, 1950. Li, J., Ding, K., Toh, K.-C., and Zhou, P. Memory-efficient 4-bit preconditioned stochastic optimization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2263322643, 2025. Lin, W., Lowe, S. C., Dangel, F., Eschenhagen, R., Xu, Z., and Grosse, R. B. Understanding and improving shampoo and soap via kullback-leibler minimization. arXiv preprint arXiv:2509.03378, 2025. Duchi, J., Hazan, E., and Singer, Y. Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research, 12(7), 2011. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization, 2017. URL https://arxiv.org/abs/ 1711.05101. 9 DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers optimization. Advances in Neural Information Processing Systems, 36:1740817419, 2023. Martens, J. and Grosse, R. Optimizing neural networks with kronecker-factored approximate curvature. In International conference on machine learning, pp. 24082417. PMLR, 2015. Modoranu, I.-V., Kalinov, A., Kurtic, E., Frantar, E., and Alistarh, D. Error feedback can accurately compress preconditioners. arXiv preprint arXiv:2306.06098, 2023. Ojalvo, I. and Newman, M. Vibration modes of large structures by an automatic matrix-reduction method. Aiaa Journal - AIAA J, 8:12341239, 07 1970. doi: 10.2514/3.5878. Pascanu, R., Lyle, C., Modoranu, I.-V., Borras, N. E., Alistarh, D., Velickovic, P., Chandar, S., De, S., and Martens, J. Optimizers qualitatively alter solutions and we should leverage this. arXiv preprint arXiv:2507.12224, 2025. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21 (140):167, 2020. Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 116. IEEE, 2020. Shi, H.-J. M., Lee, T.-H., Iwasaki, S., Gallego-Posada, J., Li, Z., Rangadurai, K., Mudigere, D., and Rabbat, M. distributed data-parallel pytorch implementation of the distributed shampoo optimizer for training neural networks at-scale, 2023. URL https://arxiv.org/ abs/2309.06497. Vlassis, G., Ashkboos, S., Volkova, A., Hoefler, T., and Alistarh, D. Beyond outliers: study of optimizers under quantization. arXiv preprint arXiv:2509.23500, 2025. Vyas, N., Morwani, D., Zhao, R., Kwun, M., Shapira, I., Brandfonbrener, D., Janson, L., and Kakade, S. Soap: Improving and stabilizing shampoo using adam. arXiv preprint arXiv:2409.11321, 2024. Wang, S., Zhou, P., Li, J., and Huang, H. 4-bit shampoo for memory-efficient network training. Advances in Neural Information Processing Systems, 37:126997127029, 2024. Xie, S., Wang, T., Reddi, S., Kumar, S., and Li, Z. Structured preconditioners in adaptive optimization: unified analysis. arXiv preprint arXiv:2503.10537, 2025. Yen, J.-N., Duvvuri, S. S., Dhillon, I., and Hsieh, C.-J. Block low-rank preconditioner with shared basis for stochastic 10 DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers"
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Preliminaries on Shampoo 2.1 Notation . . . . . . . . . 2.2 The Shampoo Optimizer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Features Inherited from Distributed Shampoo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4 New Features in DASH . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Inverse Root Methods 3.1 EVD: Eigen-Value Decomposition . . 3.2 CN: The Coupled-Newton Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 NDB: The Newton-Denman-Beavers Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4 Analyzing Matrix Scaling vs. Iteration Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.5 Multi-Power-Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Blocking Strategy 5 Experimental Results 6 Related Work and Discussion Chebyshev polynomials A.1 Chebyshev polynomials for real numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Chebyshev polynomials for matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Numerical Precision. A.4 Experiments. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Improvements to Distributed Shampoo B.1 Regularization (dampening) for EVD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Our Dampening Heuristics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 2 2 3 3 3 4 4 4 5 5 8 12 12 13 14 15 15 16 11 DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers A. Chebyshev polynomials In this section we explain how we use Chebyshev polynomials to approximate the inverse roots in Shampoo. Chebyshev polynomials is an infinite series of polynomials that represent an orthogonal basis to approximate functions. We will focus on Chebyshev polynomials of the first kind denoted by Tn(x), which are defined recurrently as: T0(x) = 1 T1(x) = Tn+1(x) = 2 Tn(x) Tn1(x) (7) (8) (9) To obtain the Chebyshev polynomials of the second kind, one should use T1(x) = 2x. In our work we choose the first kind because it has better accuracy at the endpoints of the interval. First, we will explain how this known result from linear algebra can be used to approximate real function and then we will extend it to matrices. A.1. Chebyshev polynomials for real numbers Given function (x), we want to approximate it using Chebyshev polynomials. To do this, we need to decide how many terms we want to use. Let be the number of terms, which we call the degree of the polynomial because the largest power of the resulting polynomial will be at most d. Therefore, we want to approximate the function (x) as linear combination of Chebyshev terms with coefficients ck as follows: (x) (cid:88) k=0 ck Tk(x) (10) The Chebyshev polynomials can be evaluated using Clenshaws algorithm (Clenshaw, 1955), explained step by step in Algorithm 3. One needs to fit the parameters ck only once, given the expression of (x), the degree and the number of points , then perform at most + 1 multiplications to evaluate the polynomial at each optimization step. Note the fitting procedure is cheap and it can be computed for large values of , for example 1000 or 10 000 using only numpy (Harris et al., 2020). According to (Cody, 1970), the Chebyshev series expansion for (x) is identical to the Fourier cosine series expansion for (cos(θ)). Because of that, all inputs = cos(θ) belong to the interval [1, 1], thus our values have to be mapped to this interval. Important. The Chebyshev polynomial ensures good approximation only for the numbers in the interval [a, b]. Given [a, b], we can compute x1/p by linear mapping of to the interval [1, 1], followed by calling the Clenshaws algorithm. Since the coefficients were fitted for the interval [a, b], they implicitly embed the mapping. Algorithm 2 Fitting Coefficients for Chebyshev Polynomial Input: function (x), degree d, number of points Output: Chebyshev coefficients Rd+1 [0, 1, , 2, 1] // points used to fit coefficients θ (2v + 1) π 2N // compute cosine frequencies cos(θ) 1 = 0d+1 // initialize + 1 entries with zero, that will store the coefficients ck from 0 to inclusively fx = (x) // compute (x) only once for = 0 to (inclusively) do 2 (b + a) // convert ti [1, 1] to xi [a, b] 2 (b a)t + 1 ck = 2 x cos(k θ) // scalar product between (x) = x1/p and the vector cos(k θ) end for c0 1 return Rd+1 2 c0 // make the cosine transform orthogonal 12 DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers Algorithm 3 Clenshaws Algorithm to Evaluate the Chebyshev Polynomial (scalar case) Input: input value [a, b], Chebyshev coefficients Rd+1 fitted for (x) and [a, b] Output: estimation x1/p via Chebyshev polynomial bd+2 0 bd+1 0 for = down to 0 (inclusively) do bk 2 bk+1 bk+2 + ck end for ˆx b0 b1 return ˆx (x) = x1/p A.2. Chebyshev polynomials for matrices Chebyshev polynomials can be extended from the real case to the space of matrices by doing few changes. Let Rnn and we want to compute (A) = A1/p, which we can do in few steps. Once we decide the degree of the polynomial, we need to specify the interval [a, b] to fit the coefficients. In the matrix case, we care about the eigenvalues of matrix and we need to make sure they lie in the interval [a, b]. To be in line with the Eigen-Value Decomposition approach, we choose [a, b] = [ϵ, 1 + ϵ] and we fit the Chebyshev coefficients for this interval. Next, given the input matrix with eigenvalues in [λmin(A), λmax(A)], we need to obtain matrix with eigenvalues in the interval [1, 1] before calling Clenshaws algorithm on to estimate A1/p. We scale the matrix by Frobenius norm (or Power-Iteration) to have the eigenvalues in the interval [0, 1] and then we map it to [1, 1] by multiplying by 2 and subtracting the identity matrix, as presented in Algorithm 4, where we choose to scale the matrix via Frobenius norm: Algorithm 4 Clenshaws Algorithm to Evaluate the Chebyshev Polynomial (matrix case) Inputs: - input matrix Rnn with eigenvalues in [λmin(A), λmax(A)] - coefficients Rd+1 fitted for (x) and [ϵ, 1 + ϵ] Output: estimation of A1/p via Chebyshev polynomial = 2 AF Bd+2 0n Rnn Bd+1 0n Rnn for = down to 0 (inclusively) do In // λi(S) [1, 1] Bk 2 Bk+1 Bk+2 + ck In end for ˆA B0 B1 return ˆA (A) = A1/p Note this algorithm requires + 2 matrix multiplications. By carefully inspecting the iteration, we can perform some optimizations. The first optimization consists in observing that for steps {d, 1} there are some redundant multiplications with zero matrices. Instead of spending FLOPs on multiplications with zero, one can manually compute the terms Bd and Bd1 and reduce the number of iterations (thus matmuls) by 2 in the for-loop. Concretely, for steps and 1, we obtain Bd = cd In and Bd1 = 2 cd + cd1 In. Therefore, we can now initialize Bd and Bd1 with these quantities and then iterate {d 2, 1, ..., 1, 0}. The second optimization consists in observing that the last step ˆA = B0 B1 is also redundant. By plugging in the recurrence formula B0 = 2 B1 B1 + c0 In into the expression of ˆA, we obtain: ˆA = B0 B1 = (2 B1 B2 + c0 In) B1 = B1 B2 + c0 In 13 (11) (12) (13) DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers In the final expression we observe that we do not explicitly need to compute all terms including B0, but we can stop at B1. Therefore, we can now iterate {d 2, 1, ..., 2, 1}. In the end, we perform 2 matrix multiplications in the for-loop and another one after the for-loop to compute the final approximation, resulting in total of 1 matrix multiplications for Chebyshev polynomial with degree-d. The advantage here is that we can achieve the complexity of matmuls and actually use polynomial of degree + 1. Conversely, if we choose degree d, we only pay for 1 matmuls. We present the final version in Algorithm 5. Algorithm 5 Optimized Clenshaws Algorithm to Evaluate the Chebyshev Polynomial (matrix case) Inputs: - input matrix Rnn with eigenvalues in [λmin(A), λmax(A)] - coefficients Rd+1 fitted for (x) and [ϵ, 1 + ϵ] Output: estimation of A1/p via Chebyshev polynomial = 2 Bd cd In Rnn Bd1 = 2 cd + cd1 In Rnn for = 2 down to 1 (inclusively) do Bk 2 Bk+1 Bk+2 + ck In In AF end for ˆA B1 B2 + c0 In return ˆA (A) = A1/p A.3. Numerical Precision. Clenshaws algorithm used to evaluate the Chebyshev polynomial requires high precision for the iterates Bk, meaning that float32 is required for storage. However, one can benefit from two times higher throughput from tensor-cores on Nvidia GPUs by performing the multiplications in half precision and do accumulation in float32. We found that applying this technique with float16 improves the results compared to float32. Concretely, we store Bk in float32 and in float16 and at each iteration {d 2, 1, ..., 2, 1} to compute Bk, we convert Bk+1 to float16 and perform the matmul Bk+1 in float16, with the important mention that the accumulation should be done in float32. This is very important part of this optimization, otherwise the matrix multiplication procedure would return float32 buffer. There are few more optimizations one can do here, such as avoiding repeated allocations in the bmm call and we let this for future work. A.4. Experiments. Below we provide limited set of results for the CBSHV polynomial that serve as preliminary evaluation for this method. We test our CBSHV method with degree = 60 for the Llama-373M model with embedding size = 1024 and block size = 1024 when trained with batch size of 2 million tokens in both Distributed Shampoo (DIST) and our DASH, where we update the normalization layers using Adam (DASH-A). DIST runs performed in float32 precision lead to much higher validation perplexity of 18.6, in contrast to only 16.15 for the float16 version for both Frobenius and Power-Iteration normalizations. Unfortunately we do not have rigorous explanation for why this happens, as the performed computations are similar for both Distributed Shampoo and DASH. It is important to mention that the running time of the float16 implementation is higher than for float32, which is against our hypothesis that we can benefit from the higher throughput of tensor-cores. The explanation for this matter is the sequential computation of inverse roots performed in DIST, which is inefficient even at this small model scale and it serves as the motivation of our work to reduce the running time. On the other side, DASH-A has more consistent validation perplexity. For float32, both Frobenius norm and PowerIteration normalization converge, with Power-Iteration achieving higher validation perplexity. However, the running time is much lower, under 90 milliseconds per optimizer step, which is an improvement of bit less than 2. For float16, scaling by Frobenius norm converges with running time of 76 milliseconds. This is an indication that our DASH implementation allows lower-precision improvements. On the other side, using Power-Iteration in this context did not converge for the three DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers seeds we used. Since both DIST and DASH-A have some drawbacks at this small scale, we decided not to add it to the main body of our work. However, we believe it is important to bring this technique to the communitys attention as it can pave the way for improvements and potentially other applications than Shampoo optimizer. Next, we are going to show some preliminary results for the larger model. Table 2. Results for CBSHV with degree = 60 for Llama-373M, where Normalization Layers are updated using Adam (DASH-A). We update the preconditioners at each optimization step (f = 1). Time is in milliseconds. IMPL DIST DASH-A VAL PPL TIME INFO 18.60 18.63 16.15 16. 16.14 16.22 16.15 168 176 201 193 80 84 76 FRO / FP32 / = 60 PI / FP32 / = 60 FRO / FP16 / = 60 PI / FP16 / = 60 FRO / FP32 / = 60 PI / FP32 / = 60 FRO / FP16 / = 60 PI / FP16 / = 60 Table 3. Results for CBSHV with degree {40, 60, 100} for Llama-953M, where Normalization Layers are updated using Adam. We update the preconditioners at each optimization step (f = 1). Time is in milliseconds. IMPL DIST 2048 1024 DASH-A 1024 VAL PPL TIME INFO 12.00 12. 11.88 11.87 12.10 12.10 11.99 11.98 12.11 580 538 1056 330 262 242 228 162 FRO / FP32 / = 100 FRO / FP16 / = 100 FRO / FP32 / = 100 FRO / FP16 / = 100 FRO / FP32 / = 60 FRO / FP16 / = FRO / FP32 / = 60 FRO / FP16 / = 60 FRO / FP16 / = 40 B. Additional Improvements to Distributed Shampoo In this section we provide few observations for the Distributed Shampoo implementation that arised during the preparation of our work. B.1. Regularization (dampening) for EVD In the context of using EVD for inverse roots, the preconditioner blocks have to be regularized in order for the Eigen-Value Decomposition procedure to converge. This is mandatory because absolutely all blocks have to converge, otherwise the entire training will fail. As explained in Section 3.2.1 from the original Distributed Implementation (Shi et al., 2023), the authors explain how they apply regularization to each preconditioner block, which we reproduce here: Symmetric Eigendecomposition Approach for Computing Root Inverse Given Rnn (or R), perturbation ϵ > 0, and desired exponent r. 1. Compute symmetric eigendecomposition λ, eigh(L) where λ Rn and Rnn. 2. Compute λmin mini λi. 3. Compute λnew λ min(λmin, 0)1 + ϵ1. 15 DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers 4. Form and return matrix root inverse Linv diag(λr new) QT . In the Distributed Shampoo implementation, the authors implement step 1 as λ, eigh(L + ϵIn) and then proceed with steps 2, 3 and 4. Our observation is that the eigenvalues λ already contain the regularization ϵ and we state it should be subtracted from λ after the first step, otherwise in step 3 it will be added again, which would result in an inconsistent regularization because some entries will be incrased by 2ϵ and others by less than ϵ. Lets consider numerical example for regularization value ϵ = 1010. Suppose the matrix has the smallest eigenvalue λmin(L) = 1012 before adding regularization in the eigh call. Step 1 will call λ, eigh(L+1010In) and therefore in step 2 we will get λmin = 1012 + 1010. In step 3, the value of λmin will be set to λnew min = λmin min(λmin, 0) + ϵ = λmin + ϵ = 1012 + 2 1010, which is equivalent to adding 2ϵ. When the matrix is low-rank, it is likely that λmin < 0. In this case, min(λmin, 0) = λmin < 0 and therefore λnew min = ϵ. The other eigenvalues will be increased by ϵ λmin. In summary, the Distributed Shampoo implementation does not increase all eigenvalues by ϵ, but by 2ϵ for λmin and by ϵ λmin for the other eigenvalues λi = λmin. B.2. Our Dampening Heuristics. To avoid the inconsistencies in applying the regularization in Distributed Shampoo, we propose three new heuristics. By default, we perform EVD for as λ, eigh(L + ϵIn) and then subtract ϵ from λ as λ λ ϵ, which we call corrected spectrum. Since the small magnitude eigenvalues will be amplified when we raise them to power 1/p, we believe it is important to filter them out. Shifted-ReLU heuristic. We apply ReLU to the corrected spectrum to zerorize the entries that are smaller than ϵ, which is equivalent to doing λ RELU(λ ϵ). This way, we filter out the noise in the corrected spectrum and therefore keep only the entries we consider significant (e.g. larger than ϵ). This is essentially the same as performing an implicit low-rank selection of eigenvectors in that is ϵ-dependent. Therefore, when computing λ1/p, we can raise to power 1/p only the entries in the final ReLU-shifted λ that are larger than zero and the multiplication Q(λ1/p)Q will actually obtain rank-rϵ inverse, where rϵ = (cid:80)n i=1 I(λi > 0) is the number of non-zero eigenvalues after applying shifted-ReLU. ABS-based heuristic. Instead of applying ReLU on the corrected spectrum to remove the negative eigenvalues, we can also apply absolute value function to make sure all eigenvalues are positive. Optionally, we can also add ϵ to make sure all eigenvalues have lower bound."
        }
    ],
    "affiliations": [
        "Institute of Science and Technology Austria",
        "Lancaster University",
        "Red Hat AI"
    ]
}