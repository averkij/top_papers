{
    "paper_title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence",
    "authors": [
        "Diankun Wu",
        "Fangfu Liu",
        "Yi-Hsin Hung",
        "Yueqi Duan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced performance on 2D visual tasks. However, improving their spatial intelligence remains a challenge. Existing 3D MLLMs always rely on additional 3D or 2.5D data to incorporate spatial awareness, restricting their utility in scenarios with only 2D inputs, such as images or videos. In this paper, we present Spatial-MLLM, a novel framework for visual-based spatial reasoning from purely 2D observations. Unlike conventional video MLLMs which rely on CLIP-based visual encoders optimized for semantic understanding, our key insight is to unleash the strong structure prior from the feed-forward visual geometry foundation model. Specifically, we propose a dual-encoder architecture: a pretrained 2D visual encoder to extract semantic features, and a spatial encoder-initialized from the backbone of the visual geometry model-to extract 3D structure features. A connector then integrates both features into unified visual tokens for enhanced spatial understanding. Furthermore, we propose a space-aware frame sampling strategy at inference time, which selects the spatially informative frames of a video sequence, ensuring that even under limited token length, the model focuses on frames critical for spatial reasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k dataset and train the model on it using supervised fine-tuning and GRPO. Extensive experiments on various real-world datasets demonstrate that our spatial-MLLM achieves state-of-the-art performance in a wide range of visual-based spatial understanding and reasoning tasks. Project page: https://diankun-wu.github.io/Spatial-MLLM/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 7 4 7 3 2 . 5 0 5 2 : r Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence Diankun Wu Tsinghua University Fangfu Liu Tsinghua University Yi-Hsin Hung Tsinghua University Yueqi Duan Tsinghua University Figure 1: We propose Spatial-MLLM, method that significantly enhances the visual-based spatial intelligence of existing video MLLMs. As shown, Spatial-MLLM is capable of understanding and reasoning about the underlying scene from video input, achieving state-of-the-art performance across wide range of tasks."
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced performance on 2D visual tasks. However, improving their spatial intelligence remains challenge. Existing 3D MLLMs always rely on additional 3D or 2.5D data to incorporate spatial awareness, restricting their utility in scenarios with only 2D inputs, such as images or videos. In this paper, we present Spatial-MLLM, novel framework for visual-based spatial reasoning from purely 2D observations. Unlike conventional video MLLMs which rely on CLIP-based visual encoders optimized for semantic understanding, our key insight is to unleash the strong structure prior from the feed-forward visual geometry foundation model. Specifically, we propose dual-encoder architecture: pretrained 2D visual encoder to extract semantic features, and spatial encoderinitialized from the backbone of the visual geometry modelto extract 3D structure features. connector then integrates both features into unified visual tokens for enhanced spatial understanding. Furthermore, we propose space-aware frame sampling strategy at inference time, which selects the spatially informative frames of *Equal Contribution. Corresponding Author. Preprint. Under review. video sequence, ensuring that even under limited token length, the model focuses on frames critical for spatial reasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k dataset and train the model on it using supervised fine-tuning and GRPO. Extensive experiments on various real-world datasets demonstrate that our spatial-MLLM achieves state-of-the-art performance in wide range of visual-based spatial understanding and reasoning tasks. Project page: https://diankun-wu.github.io/Spatial-MLLM/."
        },
        {
            "title": "Introduction",
            "content": "Multimodal Large Language Models (MLLMs) [1, 2, 3] have achieved significant progress in processing multimodal inputs to generate contextually aware and semantically coherent responses. While proprietary models such as Gemini [4] and GPT-4o [5] exhibit state-of-the-art performance, the open-source community continues to advance the field by improving these models ability to interpret diverse content modalities, including images [6, 7, 8], videos [9, 10, 11, 12, 13, 14], and audio [15, 16, 17]. Although these models excel at wide range of 2D tasks, their capacity to perceive, understand, and reason about 3D scenes, i.e., spatial intelligence, remains limited [18, 19]. The requirement of spatial understanding and reasoning typically arises in two scenarios. In the first scenario, the model has access to additional 3D or 2.5D data (e.g., point clouds, camera parameters, or depth maps ) alongside 2D visual inputs (e.g., images or videos). These supplementary modalities enhance the models spatial awareness, enabling more accurate spatial reasoning. However, this setup limits the models applicability in many real-world scenarios where only monocular video of the scene is available, which is the second scenario. The models ability to perform spatial understanding and reasoning under such conditions is referred to as visual-based spatial intelligence [18, 20]. major challenge in this setting is that each video frame provides only partial observation of the scene, and no global representation (e.g., the point clouds [21, 22, 23] or posed depth maps [24, 25]) is available as input. This requires the model to infer the global spatial layout from incomplete cues and internally integrate these partial observations into coherent and implicit global representation, which demands strong spatial awareness. However, most existing video MLLMs pretrain their visual encoders on image-text pairsprimarily image-caption data [13, 14, 26]following the CLIP [27] paradigm. This makes the visual encoder excel at capturing high-level semantic content but lack structure and spatial information when only 2D video inputs are available [28, 29, 30]. Consequently, current video MLLMs generally perform worse on spatial reasoning tasks than on other tasks, such as temporal understanding. Moreover, their performance still significantly lags behind human capabilities [18]. In this paper, we introduce Spatial-MLLM, method that significantly improves the visual-based spatial intelligence of existing video MLLMs. To address the limitations of visual encoders in general-purpose video MLLMs, our key insight is to unleash the strong structure prior provided by the feed-forward visual geometry foundation model [31, 32, 33]. These models, typically trained on pixel-point pairs, complement the general-purpose video MLLM visual encoders that are trained primarily on image-text data [14]. Based on this insight, we design simple dual-encoder architecture consisting of 2D encoderinitialized from the visual encoder of general-purpose video MLLMto extract 2D semantic information, and spatial encoderleveraging the VGGT feature extractor [32]to recover implicit 3D structural information from 2D video inputs. We then use lightweight connector to integrate features from both branches into unified visual tokens. The resulting integrated representation enables the Large Language Model (LLM) backbone to perform effective spatial reasoning without requiring explicit 3D data as input. Furthermore, we fully exploit the additional information provided by the introduced feed-forward visual geometry model [32], and propose space-aware frame sampling strategy at inference time, which selects the most spatially informative frames from the video sequence when the total number of input frames is limited (e.g., due to the VRAM limitation). Specifically, we first feed relatively large number of frames into the spatial encoder and decode the resulting 3D features into voxel grid. The frame selection task is then reformulated as maximum coverage problem over these voxels, which we solve using greedy algorithm. To train Spatial-MLLM, we construct visual-based spatial questionanswering dataset, Spatial-MLLM-120K, and perform supervised fine-tuning on it. We further apply simple cold-start [34] to help the model adapt to the correct reasoning format, and then train it using Group Relative Policy Optimization (GRPO) [35, 34] to enhance its long-chain-of-thought (longCoT) spatial reasoning capability [36]. We conduct extensive evaluations on the VSIBench [18], 2 ScanQA [37], and SQA3D [38] benchmarks and demonstrate that the proposed spatial-MLLM achieves state-of-the-art performance in wide range of visual-based spatial understanding and reasoning tasks. In summary, our main contributions are: We introduce Spatial-MLLM, method that significantly enhances the visual-based spatial intelligence of existing video MLLMs, demonstrating strong spatial understanding and reasoning capabilities without requiring any 3D or 2.5D data input. We design dual-encoder and connector that effectively integrates semantic information from standard 2D visual encoder with structural information extracted by spatial encoder, which is initialized using feed-forward visual geometry foundation model. We fully exploit the additional information provided by the feed-forward visual geometry model and design space-aware frame sampling strategy that selects spatially informative frames, thereby improving model performance under input length constraints. We train our model on the Spatial-MLLM-120k dataset using two-stage pipeline. Extensive experiments demonstrate that our method achieves state-of-the-art performance on wide range of visual-based spatial understanding and reasoning tasks."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 MLLMs for Video Understanding Multimodal Large Language Models have made significant progress in integrating vision and language. Early works such as BLIP-2 [2] and Flamingo [1] introduce token-level fusion (e.g., Q-Former) and feature-level fusion (e.g., cross-attention layers) to bridge modalities. Other approaches, including the LLaVA series [3, 39], MiniGPT-4 [40], and subsequent models [13, 41, 42], leverage MLPs to project visual features into the language space. Recent advancements in MLLMs have extended their capabilities from static images to videos, typically by introducing video-language alignment through large-scale pretraining [9, 43]. Later models, such as Qwen2.5-VL [14], enhance temporal reasoning via dynamic resolution and absolute time encoding. Although existing video MLLMs excel at capturing high-level semantics and temporal patterns, they struggle to interpret the underlying 3D scene from video input, which inspires our work to enhance their spatial understanding capabilities. 2.2 3D MLLMs for Scene Understanding Recent advances in MLLMs have sparked interest in extending their capabilities from 2D to 3D scene understanding [23, 44, 45, 46, 47, 48, 49, 24, 25, 50]. LL3DA [23] extracts scene-level features from 3D point clouds using Q-Former, while Grounded 3D-LLM [44] integrates 3D detectors to generate object proposals. Methods like Chat3D [45], LEO [46], and Chat-Scene [47] first segment 3D objects and encode object-centric features for LLM fusion. Alternatively, 3D-LLM [48] and Scene-LLM [49] aggregate CLIP features from pre-segmented multi-view object patches into 3D point representations, leveraging multi-view images and camera parameters. LLaVA-3D [24] projects 2D multi-view patch features into voxel space for 3D-aware aggregation, and GPT4Scene [50] enhances 3D reasoning by first reconstructing scenes and then using BEV images as input. While these methods advance 3D scene understanding, they all require additional 3D or 2.5D input data that is difficult to acquire in many real-world scenarios. In contrast, our approach only requires 2D videos as input. 2.3 Visual-based Spatial Intelligence Visual-based spatial intelligence focuses on enabling video MLLMs to perceive and reason about 3D spatial relationships directly from video inputs. While traditional MLLMs excel at 2D visual-text alignment, their extension to 3D tasks, such as 3D question answering [6, 51] and robotic manipulation [52] often lacks fine-grained spatial alignment due to limited geometric supervision. To address this gap, specialized benchmarks for video-based spatial reasoning have emerged. For example, VSI-Bench [18] introduced visual-spatial intelligence benchmark to evaluate comprehensive spatial understanding capabilities for MLLMs. STI-Bench [53] introduces physics-aware challenges like velocity estimation to quantify spatial and kinematic reasoning, while Ego-ST Bench [54] evaluates 3 Figure 2: Overview of Spatial-MLLM. Our model is composed of 2D visual encoder E2D, spatial encoder ESpatial, which is initialized from feed-forward visual geometry foundation model, connector, and large language model backbone. At inference time, we incorporate space-aware frame sampling strategy to select spatially informative frames when the number of input frames is limited due to GPU memory constraints. egocentric navigation logic in first-person videos. Meanwhile, VLM4D [55] emphasizes motion dynamics, such as trajectory prediction, to probe 4D spatiotemporal interactions. These benchmarks collectively highlight the shift toward holistic evaluation of visual-based spatial intelligence."
        },
        {
            "title": "3 Method",
            "content": "In this section, we introduce Spatial-MLLM. Given video of frames depicting scene, denoted as = {fi}N i=1, where fi RHW 3, Spatial-MLLM is designed to understand spatial relationships, perform spatial reasoning, and generate appropriate responses. We begin by describing the model architecture in Section 3.1, which comprises 2D visual encoder, spatial encoder, connector, and large language model backbone. Then we present the space-aware frame sampling strategy in Section 3.2, which selects Nk spatially informative frames {f i=1, where Nk . Finally, we introduce the Spatial-MLLM-120k dataset and our two-stage training pipeline in Section 3.3. }Nk 3.1 Spatial-MLLM Architecture In this section, we present the architecture of Spatial-MLLM, which is shown in Figure 2. We adopt Qwen2.5-VL-3B [14] as our base model and explore strategies to enhance its spatial understanding and reasoning capability. Before diving into the details, we first briefly introduce the key insights that motivate our design. What hinders visual-based spatial intelligence in existing video MLLMs? Existing video MLLMs [14, 13, 12] typically employ pre-trained 2D visual encoder E2D to extract 2D patch features e2D. These features are then projected into visual tokens through lightweight connection module. large language model backbone fθ subsequently generates the final response by conditioning on both visual and textual tokens. critical bottleneck in this process lies in the nature of the visual features extracted. The required type of information varies by task: high-level semantic representations are essential for 2D recognition and understanding, whereas fine-grained structural cues are crucial for spatial reasoning. However, the visual encoders used in current video MLLMs are primarily pre-trained on image-text datasets (mainly image-caption pairs) [14, 26] following the CLIP [27] paradigm. As result, these models predominantly capture semantic content and often lack spatial awareness when no additional 3D or 2.5D data are available [28, 29, 30]. To address this, our key insight is to unleash feed-forward visual geometry foundation models [32], which are trained on pixelpoint pairs and can recover rich 3D structural information from 2D inputs, which complements the semantic features extracted by the 2D visual encoder. We design simple dual-encoder architecture that exploits the strengths of both models and connector to fuse semantic and structural information into unified visual tokens. Below, we introduce the core components of our design. Dual-Encoder. The proposed dual-encoder consists of 2D encoder E2D and spatial encoder ESpatial. For the 2D encoder branch, we adopt the same design as the visual encoder of Qwen2.5-VL [14] to encode input frames into semantically rich features: e2D = E2D (cid:16) {fi}Nk i=1 (cid:17) , e2D RNk (cid:106) p2D (cid:107) (cid:106) p2D (cid:107) d2D, (1) where p2D and d2D denote the patch size and feature dimension of the 2D visual encoder, respectively. The two consecutive frames are grouped for video input, thus Nk = Nk/2. For the spatial encoder branch, we utilize the feature backbone of VGGT [32]. Specifically, given frames of the scene video, we first patchify the input and then extract 3D features with alternating frame-wise self-attention and global self-attention [56]. This process allows Espatial to aggregate spatial information across different frames to get the final 3D features: (cid:106) p3D (cid:106) p3D (cid:17) (cid:16) (cid:107) {fi}Nk i=1 , e3D RNk e3D, ec, erigister = Espatial (cid:107) d3D , (2) where e3D, ec, and eregister represent the dense 3D feature, the camera feature for each frame, and the registration tokens [57], respectively. We only use e3D in the feature fusion stage as it captures the dense structure information of the input frames. Connector. After obtaining the 2D and 3D features, we use simple connector to integrate the semantic and structural information from both branches. Specifically, we first align e3D with e2D in both spatial and temporal dimensions: 3D = Rearrange(e3D), 3D RNk (cid:106) p2D (cid:107) (cid:106) p2D (cid:107) 3D . (3) Here, the spatially and temporally adjacent information in e3D is aggregated into the feature channel dimension, enabling alignment with e2D. Next, we employ two lightweight MLPs to fuse the information to obtain the unified visual tokens: = MLP2D(e2D) + MLP3D(e 3D), (4) where RSdllm denotes the final visual tokens and = Nk is the sequence length. Although more complex feature fusion methods, e.g., cross-attention [56, 58], could be applied, our experiments demonstrate that this simple and lightweight approach is sufficient to enhance the models spatial understanding and reasoning capabilities. We leave the exploration of more advanced fusion strategies for future work. (cid:107) (cid:106) p2D (cid:107) (cid:106) p2D 3.2 Space-Aware Frame Sampling Due to GPU memory constraints, video MLLMs can process only limited subset of frames from scene video sequence. For example, in the VSI-Bench setup [18], only 8 to 32 frames are sampled as input to the video MLLM, while typical scene video in VSI-Bench contains over 2,000 frames. widely adopted solution is uniform frame sampling [14, 18, 13], which is effective for generalpurpose video understanding. However, as spatial videos represent 3D scenes, the sampling strategy for spatial understanding tasks should focus on capturing most information of the underlying scene, which uniform sampling fails to achieve. Benefiting from the feed-forward visual geometry foundation model, we design straightforward space-aware frame sampling strategy at inference time. Specifically, given scene video = {fi}N i=1, our objective is to select Nk frames, {f i=1 that have most coverage of the underlying scene. To achieve this, we first uniformly subsample Nm frames, {f i=1, where Nm satisfies Nk < Nm < , and is determined by the available GPU memory. Typically, we choose Nm = 128 while Nk = 16. We then leverage E3D to extract their corresponding 3D features em 3D and camera features em . Subsequently, we use the pretrained camera head fc and depth head fd of the VGGT model [32] to decode set of camera parameters and depth maps: }Nm }Nk {Em , Km }Nm i=1 = fc(ec), and {Dm }Nm i=1 = fd(e3D). (5) This allows us to calculate the voxels (f ) covered by each frame selection as maximum coverage problem [59], i.e., select Nk frames {f , and formulate frame }Nk i=1 {f i=1 such }Nm 5 (cid:12) (cid:12) i=1 (f ) (cid:12) is maximized. In practice, we apply that the total number of unique covered voxels greedy algorithm to accelerate computation [60, 25]. In practice, once the Nk frames are selected, we dont need to recompute their 3D features ek 3D and can directly reuse the corresponding features from the precomputed set em 3D. We provide the complete algorithm and detailed explanation in Section A.1. (cid:83)Nk (cid:12) (cid:12) (cid:12) 3.3 Training Training Data Construction. We first construct visual-based spatial question-answering dataset, i.e., Spatial-MLLM-120k. The dataset is collected from three sources: the training set of ScanQA [37], SQA3D [38], as well as additional self-created spatial QA data. All items in Spatial-MLLM-120k are derived from scenes in the ScanNet training set [61] and are each represented as quadruple Ii = Qi, Ai, Vi, Mi, denoting the question, answer, video (scene) ID, and meta-information (e.g., task type), respectively. For the self-curated QA data, we follow the data processing pipeline proposed in VSI-Bench [18]. Specifically, we first convert ScanNet scenes into continuous video clips at 24 FPS and 640 480 resolution. Then we generate spatial reasoning QA pairs leveraging the provided meta-annotations of Scannet. The generated QA pairs cover various spatial understanding and reasoning tasks, including object counting, object size, room size, absolute distance, appearance order, relative distance, and relative direction. Since the QA pair construction process is similar to that of VSI-Bench [18], we exclude the QA pair Ii if its scene video Vi is used in the evaluation set of VSI-Bench (312 scene videos in total) to prevent data leakage. Finally, we create approximately 70k QA pairs. We provide additional details on training data construction in the section A.2. Figure 3 shows summary of key statistics of Spatial-MLLM-120k. Supervised Fine-tuning. Leveraging the constructed Spatial-MLLM-120k dataset, we first perform supervised fine-tuning (SFT) on our model. Since both E2D and Espatial are pre-trained on large-scale image-text and pixel-point pairs, respectively, we freeze them to preserve their ability to extract rich semantic and structural information. We jointly train the connection module and the LLM backbone to enable the model to adaptively fuse 2D and 3D features and enhance its spatial understanding and reasoning capability. During this stage, we employ the standard cross-entropy loss Lce between the model-generated answers and the ground-truth annotations: Lce(θ) = (cid:88) log (o(i) o(1:i1), q, {fj}Nk j=1) (6) where {fj}Nk j=1 denotes input video frames, denotes the system prompt and question, o(i) represents the i-th token in the ground-truth answer, and o(1:i1) denotes the preceding answer tokens. Figure 3: Basic statistic of our constucted Spatial-MLLM-120K dataset. RL Training. Following the SFT stage, we first perform simple cold start [34] to help the model adapt to the correct reasoning format. Then we train the model using Group Relative Policy Optimization (GRPO) [35] to enhance its long-CoT [36] spatial reasoning capability. During training, we first sample set of output {o1, o2, . . . , oG} for each question from the policy model πθold. Then we optimize the policy model by maximizing the following objective: JGRPO(θ) = Eq,oi (cid:34) 1 (cid:88) i=1 min (cid:18) πθ(oi q) πθold (oi q) Ai, clip( πθ(oi q) πθold (oi q) (cid:19) (cid:35) , 1 ϵ)Ai β KL[πθπref] (7) where Ai = r1mean(r1,r2,...,rG) std(r1,r2,...,rG) is the advantage function computed using the group rewards. In GRPO, the design of the reward function is critical. In addition to formatting reward applied to all task types, we introduce task-dependent reward modelling to ensure that it accurately reflects the proximity between the predicted and ground-truth answers. Specifically, we categorize the data into three types based on answer format: numeric answer questions, multiple-choice questions, and 6 Table 1: Evaluation Results on VSI-Bench [18]. For Spatial-MLLM and Qwen2.5VL-series [14], we use 16 frames as input. For other open-source methods and GPT-4o [5], we follow the setting of VSI-Bench to set frame numbers (ranging from 8 to 32 frames). For Gemini-1.5 Pro [4], it samples video frames at 1 FPS. Bold and underline denote the best-performing and second-best-performing open-source models, respectively. Methods Proprietary Models GPT-4o [5] Gemini-1.5 Pro [4] Open-source Models InternVL2-40B [7] LongVILA-8B [63] VILA-1.5-40B [64] LongVA-7B [65] LLaVA-OneVision-72B [6] LLaVA-Video-72B [12] Qwen2.5VL-3B [14] Qwen2.5VL-7B [14] Qwen2.5VL-72B [14] Spatial-MLLM-4B Numerical Qusetion Multiple-Choice Question Obj. Cnt. Abs. Dist. Obj. Size Room Size Rel. Dist. Rel. Dir. Route Plan Appr. Order 46.2 56.2 34.9 29.1 22.4 38.0 43.5 48.9 24.3 40.9 25.1 65.3 5.3 30.9 26.9 9.1 24.8 16.6 23.9 22.8 24.7 14.8 29.3 34.8 43.8 64.1 46.5 16.7 48.7 38.9 57.6 57.4 31.7 43.4 54.5 63. 38.2 43.6 31.8 0.0 22.7 22.2 37.5 35.3 22.6 10.7 38.8 45.1 37.0 51.3 42.1 29.6 40.5 33.1 42.5 42.4 38.3 38.6 38.2 41.3 41.3 46.3 32.2 30.7 25.7 43.3 39.9 36.7 41.6 38.5 37.0 46. 31.5 36.0 34.0 32.5 31.5 25.4 32.5 35.0 26.3 33.0 34.0 33.5 28.5 34.6 39.6 25.5 32.9 15.7 44.6 48.6 21.2 29.8 28.9 46.3 Avg. Rank 34.0 45. 36.0 21.6 31.2 29.2 40.2 40.9 30.6 33.0 37.0 48.4 7 2 6 12 9 11 4 3 10 8 5 1 verbal answer questions. For numeric questions, we compute the mean relative accuracy [18]. For multiple-choice questions, we employ an exact match reward. For verbal answer questions, we use fuzzy matching based on Levenshtein distance. Further details on reward calculation are provided in Section A.4."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Implementation Details Training details. Spatial-MLLM is built on Qwen2.5-VL [8] and VGGT [32] and has 4B parameters in total. We use the visual encoder of Qwen2.5-VL [14] to initialize E2D, and the LLM backbone of it to initialize fθ. We then use the feature backbone of VGGT [32] to initialize Espatial. During training, we use 640 480 resolution and limit video frames to 16. In the SFT stage, we train the model using Adam optimizer [62] for one epoch. We set the global batch size to 16 and use linear learning-rate schedule, with peak value of 105. In the cold start stage, we first construct small CoT dataset. Specifically, we prompt Qwen2.5-VL-72B [14] to generate multiple thinking processes and answers according to the scene video and question. Then we use the GT answer to filter correct thinking-answer pair (more details are provided in Section A.3). We use similar setting as in the SFT stage to train the model for 200 steps. In the RL stage, we perform 8 rollouts per question and set the default sampling temperature to 1. The KL divergence coefficient, β, is set to 0.04. Due to computational resource limitations, we train the model for 1,000 steps with learning rate of 1e-6. We show the training curve of SFT Stage and RL Stage in Figure 4. Inference Details. During inference, we set Nm = 128 and Nk = 16 for space-aware frame sampling. Since spatial reasoning requires certain level of determinism, we set the temperature to 0.1 and the top-p to 0.001. During inference, we use 16 frames at 640 480 resolution from the scene video as input unless otherwise specified. 4.2 Comparisons on VSI-Bench Setup. VSI-Bench [18] contains more than 5,000 question-answer pairs derived from egocentric videos sourced from ScanNet [61], ScanNet++[66], and ARKitScenes[67]. The task types are divided into Multiple-Choice Answer (MCA) and Numerical Answer (NA). For the MCA tasks, we compute mean accuracy, and for the NA tasks, we calculate relative accuracy across confidence thresholds = {0.5, 0.55 . . . , 0.95}. We report the final average score and individual metrics on eight task types of VSI-Bench, including: (1) configurational reasoning (object counting, relative direction, absolute direction, and route planning), (2) measurement estimation (object size, room size, and absolute distance), and (3) spatiotemporal reasoning (appearance order). Baselines. We compare our model with broad range of video MLLMs. For proprietary model, we include GPT-4o [5] and Gemini-1.5 Pro [4]. For open-source MLLMs, we compare our model with 7 Table 2: Evaluation Results on ScanQA [37] and SQA3D [38]. We use the val set of ScanQA and test set of SQA3D for evaluation following common practice [22, 68, 25]. Bold and underline denote the best-performing and second-best-performing models in each model category, respectively. Methods Task-Specific Models ScanQA [37] SQA3D [38] 3D-Vista [69] 3D/2.5D-Input Models 3D-LLM [70] LL3DA [23] Chat-Scene [22] 3D-LLaVA [21] Video-3D LLM [25] Video-Input Models Qwen2.5-VL-3B [14] Qwen2.5-VL-7B [14] Qwen2.5-VL-72B [14] LLaVA-Video-7B [14] Oryx-34B [51] Spatial-MLLM-4B ScanQA (val) SQA3D (test) Video-Input Only BLEU-1 BLEU-4 METEOR ROUGE-L CIDEr EM-1 EM-R1 30.2 30.5 - 39.3 - 43.2 - 47.1 22.5 27.8 26.8 39.7 38.0 44.4 10.1 11.2 - 12.0 13.5 14.3 17.1 16.2 3.8 3.0 12.0 3.1 - 14.8 13.1 13.5 13. 14.5 15.9 18.0 18.4 19.8 9.7 11.4 13.0 17.7 15.0 18.4 33.3 34.5 35.7 35.7 37.3 41.6 43.1 49.0 25.4 29.3 35.2 44.6 37.3 45.0 64.9 - - 69.4 76.8 87.7 92.6 102.1 47.4 53.9 66.9 88.7 72.3 91.8 47.2 46.6 48.5 - - 54.6 54.5 58.6 43.4 46.5 47.0 48.5 - 55.9 - - - - - 57.5 56.6 - 45.9 49.8 50.9 - - 58.7 InternVL2-40B [7], LLaVA-NeXT-Video-72B [12], LLaVA-OneVision-72B [6], and the Qwen2.5VL [14] series. To validate the effectiveness of our model, we also train Qwen2.5-VL [14] using the same training setup as in Spatial-MLLM SFT training as additional baselines (in Table 3). Results. We present the quantitative results on VSI-Bench [18] in Table 1. Despite having 4B parameters, Spatial-MLLM significantly outperforms all proprietary and open-source MLLMs, including those with substantially larger parameter counts (e.g., 32B or 72B). Among the remaining models, the best-performing one is the proprietary Gemini-1.5 Pro [4]. Notably, Spatial-MLLM is provided with only 16 input frames per video, while Gemini-1.5 Pro [4] samples videos at 1 FPS (i.e., an average of 85 frames per video on VSI-Bench) according to its API instructions [18]. Despite the significantly lower number of input frames, Spatial-MLLM still achieves 3.0% higher average accuracy than Gemini-1.5 Pro [4]. 4.3 Comparison on ScanQA and SQA3D Setup. ScanQA [37] and SQA3D [38] are two 3D question-answering benchmarks built upon ScanNet [61]. Since the authors did not provide test set for ScanQA, we evaluate it using the validation set, which consists of 4,675 QA pairs focused on understanding spatial relationships such as object alignment and orientation, as well as the ability to accurately identify objects in 3D scenes based on textual questions. We follow standard practice by evaluating answer quality using the following metrics: CiDEr, BLEU-1, BLEU-4, METEOR, and ROUGE-L. For SQA3D, we evaluate the model on its test set, which contains 3,519 QA pairs. The task requires the model to first understand its position and orientation within the 3D scene, as described by text, then reason about its environment and answer question under those conditions. Since SQA3D contains definitive answers, we use exact match accuracy (EM) and its refined version (EM-R) as evaluation metrics. We provide the evaluation results using additional metrics for both benchmarks in Section B.3. Baselines. Since both the ScanQA [37] and SQA3D [38] benchmarks provide additional 3D annotations (e.g., point clouds and depth maps of the scene), we compare Spatial-MLLM with several other model types in addition to video-input MLLM. These includes task-specific models designed for 3D question-answering tasks, such as ScanQA [37], SQA3D [38], 3D-VisTA [69], and LLMs that require point clouds or depth maps as input, such as Chat-Scene [22], Video-3D LLM [25], and 3D-LLaVA [21]. Results. We present the quantitative results on the ScanQA [37] and SQA3D [38] benchmarks in Table 2. As shown, Spatial-MLLM significantly outperforms all video-input models across all metrics on both ScanQA and SQA3D. Our model also surpasses all task-specific models. Among models utilizing 3D or 2.5D input, only 3D-LLaVA [21] (on ScanQA) and Video-3D-LLM [25] Figure 4: Visualization of Training Curves in the SFT and RL Stages. For the SFT stage, we present the mean token accuracy and loss curves. For the RL stage, we show the dynamics of completion length and reward. (on ScanQA and SQA3D) achieve better performance than Spatial-MLLM. However, 3D-LLaVA requires additional point cloud input, and Video-3D-LLM depends on depth maps. Despite not relying on any additional 3D or 2.5D input, our model still outperforms other 3D-dependent models such as 3D-LLM [70], LL3DA [23], and Chat-Scene [22]. 4.4 Ablation Study and Analysis Effectiveness of RL Training. We evaluate the supervised fine-tuning version of Spatial-MLLM, denoted by Spatial-MLLM-SFT-16, and the final version of Spatial-MLLM, denoted by SpatialMLLM-16. As shown in Table 3, although we only perform small-scale GRPO training (i.e., 1,000 steps), Spatial-MLLM-16 still achieves performance gains, suggesting that long-CoT reasoning benefits the spatial reasoning capabilities required by VSI-Bench [18]. Effectiveness of the Spatial-MLLM Architecture. We fine-tune Qwen2.5-VL-3B and Qwen2.5-VL-7B [14] on the SpatialMLLM-120K dataset using the same process as for Spatial-MLLM. As shown in Table 3, both Qwen2.5-VL-3B-SFT-16 and Qwen2.5-VL-7B-SFT-16 show improvements after fine-tuning, indicating the effectiveness of our spatial dataset to enhance the models spatial reasoning capabilities. Furthermore, both models underperform compared to Spatial-MLLM-SFT16, which validates the effectiveness of the proposed architecture. Table 3: Ablation Study. We report evaluation results on VSI-Bench [18] in different settings. \"-SFT\" refers to the model fine-tuned on the Spatial-MLLM-120K dataset. \"- 8/16/32\" denotes the number of input frames during inference. \"-Uni\" denotes using uniform frame sampling. Methods Numerical Multiple-Choice Avg. Spatial-MLLM-8 Spatial-MLLM-16 Spatial-MLLM-32 Spatial-MLLM-SFT-16 Qwen-2.5VL-3B-SFT-16 Qwen-2.5VL-7B-SFT-16 50.8 52.7 53.1 51.5 47.1 48. 41.2 43.8 45.3 40.4 32.6 34.7 46.1 48.4 49.3 46.1 40.0 42.0 Spatial-MLLM-Uni-8 Spatial-MLLM-Uni-16 Spatial-MLLM-Uni-32 48.2 51.6 52. Effectiveness of Space-aware Frame Sampling. We evaluate different frame sampling configurations in Table 3, including 8, 16, and 32 frames using uniform sampling and our proposed space-aware frame sampling strategy. As shown, increasing the number of sampled frames improves performance for both spaceaware frame sampling and uniform sampling. Compared with uniform sampling, space-aware frame sampling consistently outperforms it when the number of input frames is the same. 39.2 42.3 44.2 43.8 47.1 48."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce Spatial-MLLM, method that enables effective spatial understanding and reasoning from purely 2D visual inputs. By combining semantic 2D encoder with structure-aware spatial encoder initialized from visual geometry foundation model, our dual-encoder design captures both semantic and spatial cues. Additionally, our proposed space-aware frame sampling strategy further enhances performance under limited input constraints. Trained on the Spatial-MLLM-120K dataset, our model achieves state-of-the-art results across multiple benchmarks. Limitations and Future Work. Although Spatial-MLLM demonstrates significant improvements over previous video MLLMs across wide range of visual-based spatial understanding and reasoning tasks, there remains room to scale Spatial-MLLM further in terms of model size and training data. Moreover, as this work primarily addresses visual-based spatial intelligence, we have trained and evaluated our model specifically on relevant datasets and benchmarks. An interesting direction for 9 future work would be to explore how integrating spatial structural information might further benefit general video understanding and reasoning tasks."
        },
        {
            "title": "A Additional Method Details",
            "content": "A.1 Details of Space-Aware Frame Sampling Our space-aware frame sampling algorithm consists of three stages: (1) Scene geometry preprocessing, (2) Voxelization and coverage calculation, and (3) Greedy maximum coverage selection. Beginning with the original video sequence = {fi}N i=1, we first perform uniform subsampling to obtain }Nm Nm = 128 candidate frames {f i=1. For each subsampled frame, we leverage the backbone and }Nm i=1 and {Dm , Km head of VGGT [32] to compute {Em i=1 as illustrated in the main paper. Then we reconstruct 3D point maps through depth reprojection: [uv1] E1 }Nm (8) , = Dm K1 where (u, v) denote pixel coordinates. In practice, we also obtain confidence value c(p) [0, 1] for each point from the depth head. Although VGGT [32] can also directly decode point maps from 3D dense features, we find that using depth and camera produces more accurate results. The voxelization and coverage calculation process first establishes 3D bounding box encompassing all valid scene points: Pvalid = Nm(cid:91) i=1 {p c(p) > 0.1 c(p) Percentile({c(p)}, 50%)}. (9) We then discretize the bounding box into voxels. To handle relative scales in VGGT [32] outputs, we use an adaptive way to set the voxel size to 1 λ of the minimum dimension of the scenes bounding box: = min(max(Pvalid) min(Pvalid)), (10) 1 λ where λ is hyperparameter and we set it to 20. Each frames voxel coverage (f calculated by discretizing its valid points: ) is then (f ) = (cid:26)(cid:22) min(Pvalid) (cid:23) (cid:12) (cid:12)p (cid:12) Pvalid (cid:27) . Finally, we can formulate frame selection as the typical maximum coverage problem [59]: max S{1,...,Nm} (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:91) iS (cid:12) (cid:12) (f (cid:12) ) (cid:12) (cid:12) s.t. = Nk, (11) (12) In practice, we set Nk = 16 and use greedy approach [60, 25] to iteratively select the frame that provides the maximum new coverage, which is illustrated in Algorithm 1. A.2 Details of Spatial-MLLM-120k Dataset Construction We follow similar approach to that used in [18] to construct the Spatial-MLLM-120k dataset. Specifically, the construction involves three main processes: video preprocessing, metadata computation, and QA pair generation. Video Preprocessing. In this stage, we extract frames from the raw ScanNet [61] scans and convert them into videos at 24 FPS with resolution of 640 480. Metadata Computation. In this stage, we extract spatial and semantic metadata from raw ScanNet scans and their associated semantic annotations. First, we align each raw scene mesh using the provided axis alignment matrices and convert it to the Open3D [71] point cloud. At the room level, we compute the room size using the alpha-shape algorithm and determine the center coordinates. i=1, target selection size Nk Algorithm 1 Greedy Maximum Coverage Sampling )}Nm Input Frame voxel sets {V (f Output Selected frame indices {1, ..., Nm} 1: 2: 3: {1, . . . , Nm} 4: for 1 to Nk do 5: 6: 7: 8: if = then break end if argmax (f ) iR ) = 0 then if (f break 9: 10: 11: 12: 13: 14: 15: end for 16: return end if {i} (f ) {i} Selected frames Covered voxels Remaining candidates No remaining candidates Max coverage gain No additional coverage Update selection Update covered voxels Remove from candidates At the object level, we generate oriented bounding boxes (OBBs) for each valid object instance and assign semantic labels from the annotations, excluding structural elements (e.g., walls, floors) and ambiguous categories (e.g., otherstructure). To ensure consistency across categories, we remap the original ScanNet semantic labels to new label set based on the NYU40 classes [72, 73]. In addition, we collect the projected 2D semantic annotation of each scene video for the appearance order task. The final metadata for each scene includes: (1) room size and center coordinates; (2) the projected 2D semantic annotation of the scene video; (3) object instances and their OBB parameters, including rotation matrices, extents, and centers; and (4) semantic labels for each object. QA Pair Generation. Finally, we generate QA pairs of different tasks, including object counting, object size, room size, absolute distance, appearance order, relative distance, and relative direction. Object counting (numerical): We first count how many times each object category appears in the scene, then randomly sample category that appears at least twice. Question template: How many <category>(s) are in this room? Object size (numerical): We randomly sample unique object in the scene and take the longest side of its oriented bounding-box (OBB) as the ground-truth length (in cm). Question template: What is the length of the longest dimension (length, width, or height) of the <category>, measured in centimeters? Room size (numerical): We use the pre-computed room size (in m2) as the ground-truth value. Question template: What is the size of this room (in square meters)? Absolute distance (numerical): For pair of objects, we uniformly sample points inside each OBB and take the minimum Euclidean distance between the two point clouds as the ground-truth (in m). Question template: Measuring from the closest point of each object, what is the direct distance between the <category_A> and the <category_B> (in meters)? Appearance Order (multiple choice): We calculate the first appearance timestamp of each category, which is the timestamp when its visible pixel count exceeds predefined threshold. Using these timestamps, we generate the correct order of appearance among the categories, along with other options. Question template: What will be the first-time appearance order of the following categories in the video: <category_A>, <category_B>, <category_C>, <category_D> Relative distance (multiple choice): We use an anchor object that is unique in the scene and then select four additional objects while enforcing 15-30cm separation thresholds between options. Question template: Which of these objects (<category_A>, <category_B>, <category_C>, <category_D>) is closest to the <anchor_category>? 11 Relative direction (multiple choice): For triple {position, facing, query} of unique categories, we position query. The compute the horizontal angle between the vectors angle is then discretized into directional classes (easy: left/right, medium: left/right/back, hard: front-left/front-right/back-left/back-right). Question template (easy example): If am standing by the <position-category> and facing the <facing-category>, is the <query-category> to the left or the right? position facing and A.3 Details of Cold Start To align the model with the desired reasoning format, we perform simple cold start for 200 steps before GRPO training. The key to this stage is the construction of spatial reasoning dataset with chain-of-thought (CoT) annotations. The construction process is as follows: Subset Sampling. We begin by sampling subset D0 = {Ii}Ns Spatial-MLLM-120k dataset. i=1 = {Qi, Ai, Vi, Mi}Ns i=1 from the Multi-path CoT Generation. For each item Ii D0, we utilize Qwen2.5-VL-72B [14] to generate independent reasoning processes ˆT (k) . We then compute reward = Reward( ˆA(k) r(k) , Ai) for each reasoning-answer pair, where Reward(, ) is the reward function described in Sec A.4. Consequently, we obtain set of outputs Oi = {( ˆT (k) k=1 for each Ii D0. and corresponding answers ˆA(k) , ˆA(k) , r(k) )}K i Adaptive Filtering. Since Qwen2.5-VL-72B [14] may generate incorrect reasoning processes and answers, we apply filtering process based on the computed rewards. While using global reward threshold is straightforward, it often results in an imbalance across question types in the selected subset. To mitigate this, we adopt an adaptive filtering strategy. Specifically, for each item Ii D0, we first keep the output with the highest reward to get ˆOi = {( ˆT (k) )} where = arg maxk r(k) denote the maximum reward. We then categorize all items based on their question type and compute question type-dependent threshold τt(i), where t(i) denotes the type of problem i. The item is added into the cold start set if and only if: . Let ˆri = r(k) , ˆA(k) , r(k) i and where the type-dependent threshold satisfies τt(i) := Quantile(cid:0){ˆrj t(j) = t(i)}, 0.5(cid:1). This rule preserves approximately the top 50% of generations per question type while discarding degenerate (zero-reward) outputs. In practice, we set Ns = 5000 and = 3, and finally we get 2459 items in the cold start set. We provide pseudocode for this process in Algorithm 2: ˆri τt(i) ˆri > 0, A.4 Details of SFT and GRPO Training Reward Calculation. Given predicted answer Apred and ground truth answer Agt, the reward function Reward(Apred, Agt) consist of format reward Rfmt and task-specific reward: Reward(Apred, Agt) = λ1Rformat + λ2 multiple-choice numerical RMC, RMRA, RVerbal, verbal (13) where λ1 and λ2 are hyperparameters, both of which are set to 1 in our implementation. For multiple-choice questions, we implement exact match criterion: RMC(Apred, Agt) = (ψ(Apred) = ψ(Agt)) (14) where ψ() performs answer normalization through whitespace stripping and I() denotes the indicator function. For numerical tasks, we compute mean relative accuracy (MRA) [18]: (cid:19) RMRA(Apred, Agt) = < τ (15) 1 (cid:88) τ (cid:18) α(Apred) α(Agt) α(Agt) + ϵ where α() normalizes numeric values, ϵ = 108 prevents division by zero, and = {0.50, 0.55, ..., 0.95} defines accuracy thresholds. For verbal answer questions, we compute normalized similarity score using the Levenshtein ratio: RVerbal(Apred, Agt) = 1 DLev(ϕ(Apred), ϕ(Agt)) ϕ(Apred) + ϕ(Agt) (16) 12 Algorithm 2 Cold Start Dataset Construction Input Original dataset 1: Qwen2.5-VL model 2: Reward function Reward(, ) 3: Sample size Ns, Paths per item Output Filtered dataset Dcold 4: Initialize D0 Sampling(D, Ns) 5: Dcold 6: for each item Ii = Qi, Ai, Vi, Mi D0 do Generate reasoning paths: { ˆT (k) 7: Compute rewards: r(k) Select best path: arg maxk r(k) Record ˆri r(k) , ˆOi ( ˆT (k) , ˆA(k) 8: 9: ) }K k=1 (Qi, Vi) Reward( ˆA(k) , Ai), i 10: 11: end for 12: Group items by type: {Gt} GroupByType({ˆri}) 13: for each question type do 14: 15: end for 16: for each item Ii D0 do 17: Compute threshold: τt Quantile({ˆrjj Gt}, 0.5) if ˆri τt(i) and ˆri > 0 then Dcold Dcold { ˆOi} end if 18: 19: 20: end for 21: return Dcold where DLev denotes the Levenshtein edit distance, and ϕ() represents the text normalization function. In practice, we use the implementation provided by the Levenshtein library. In addition to the format and task-specific rewards, we also incorporate reasoning length reward following Video-R1 [12], which encourages the model to perform more thinking before generating the final answer. Other Details. Figure 5 presents the prompts used in the SFT and GRPO stages. For both stages, we adopt the default system prompt of Qwen2.5-VL [14], namely, \"You are helpful assistant.\" In the SFT stage, the user prompt consists of question and type template. In the GRPO stage, the user prompt comprises question, question post string, and type template."
        },
        {
            "title": "B Additional Experiments",
            "content": "B.1 Visualization of Space-Aware Frame Sampling We provide visualization of our space-aware frame sampling in Fig. 6, which shows the point maps (predicted by the VGGT [32]) corresponding to the frames selected by different sampling strategies. As shown, the proposed space-aware frame sampling strategy consistently yields more spatial coverage than uniform sampling, which often overlooks transient regions that appear briefly in the video and tends to produce redundant viewpoints when the camera remains static. B.2 Qualitative Results We present qualitative examples of Spatial-MLLM on the VSI-Bench [18] dataset in Figures 7 to 10. As illustrated, Spatial-MLLM is capable of reasoning with visual information across different task types and producing final answers accordingly. Furthermore, it demonstrates strong abilities in self-verification and task decomposition during the reasoning process. 13 Figure 5: Illustration of the prompts used in the SFT and GRPO stages. We use the default system prompt of Qwen2.5-VL [14] (i.e., , \"You are helpful assistant\") for both stages. In the SFT stage, the user prompt consists of question and type template. In the GRPO stage, the user prompt includes question, question post string, and type template. Figure 6: Visualization of different frame sampling strategies. For clarity of visualization, we set Nm = 128 and Nk = 8 in this example. Uniform frame sampling often overlooks transient regions that appear briefly in the video. Furthermore, when the camera remains static for extended periods, this strategy tends to yield redundant viewpoints. In contrast, our proposed space-aware frame sampling strategy achieves more comprehensive spatial coverage. B.3 Additional Evaluation Results on ScanQA and SQA3D We present additional evaluation results on the ScanQA [37] and SQA3D [38] benchmarks in Table 4 and Table 5. As shown, our proposed method consistently outperforms all video-input models, including LLaVA-Video-7B [12] and Oryx-34B [51], both of which incorporate spatial reasoning datasets such as ScanQA [37] during training. Despite having only 4.2 billion parameters, Spatial-MLLM significantly surpasses Qwen2.5-VL72B [14] on the ScanQA benchmark, achieving substantial gains across multiple metricsfor instance, +2.3 EM-1, +17.6 BLEU-1, and +24.9 CIDEr. Similarly, on the SQA3D benchmark, Spatial-MLLM consistently outperforms Qwen2.5-VL-72B across all question types and overall 14 Table 4: Additional evaluation results on ScanQA [37] for task-specific models, 3D/2.5D input models, and video-input models. Reported metrics include EM-1, BLEU-1 to BLEU-4, ROUGE-L, METEOR, and CIDEr. Methods ScanQA (val) EM-1 BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE-L METEOR CIDEr Task-Specific Models ScanQA [37] 3D-Vista [69] 3D/2.5D-Input Models 3D-LLM [70] LL3DA [23] Chat-Scene [22] 3D-LLaVA [21] Video-3D LLM [25] 21.1 22. 20.5 21.6 - 30.1 Video-Input Models 15.4 Qwen2.5-VL-3B [14] Qwen2.5-VL-7B [14] 19.0 Qwen2.5-VL-72B [14] 24.0 LLaVA-Video-7B [12] Oryx-34B [51] Spatial-MLLM-4B 26.3 30.2 - 39.3 43.2 - 47.1 22.5 27.8 26.8 39.7 38.0 44. 20.4 - 25.2 29.1 - 31.7 13.1 13.6 17.8 26.6 24.6 28.8 15.1 - 18.4 20.6 - 22.8 8.1 6.3 14.6 9.3 21. 10.1 10.4 12.0 13.5 14.3 17.1 16.2 3.8 3.0 12.0 3.1 14.8 33.3 35.7 35.7 37.3 41.6 43.1 49.0 25.4 29.3 35.2 44.6 37.3 45. 13.1 13.9 14.5 15.9 18.0 18.4 19.8 9.7 11.4 13.0 17.7 15.0 18.4 64.9 69.6 69.4 76.8 87.7 92.6 102.1 47.4 53.9 66.9 88.7 72.3 91. Table 5: Additional evaluation results on SQA3D [38] for task-specific models, 3D/2.5D input models, and video-input models. In addition to the average EM-1 and EM-R1 across all questions, we also report the average EM-1 for different question types, including What, Is, How, Can, Which, and Others."
        },
        {
            "title": "What",
            "content": "Is How Can Which Others Avg. (EM-1) Avg. (EM-R1) SQA3D (test)"
        },
        {
            "title": "Methods",
            "content": "Task-Specific Models SQA3D [38] 3D-Vista [69] 3D/2.5D-Input Models Scene-LLM [74] Chat-Scene [22] Video-3D LLM [25] 31.6 63.8 46.0 69.5 34.8 63.3 45.4 69.8 43.9 47.2 40.9 69.1 45.0 70.8 45.4 67.0 52.0 69.5 51.1 72.4 55.5 69.8 Video-Input Models Qwen2.5-VL-3B [14] 34.8 52.1 39.8 52.7 39.7 56.6 41.1 55.9 Qwen2.5-VL-7B [14] Qwen2.5-VL-72B [14] 41.7 56.3 41.5 55.6 LLaVA-Video-7B [12] 42.7 56.3 47.5 55.3 Spatial-MLLM-4B 45.9 71.6 55.1 69. 45.3 48.1 52.3 55.0 56.0 47.0 47.2 48.0 47.2 53.0 46.6 48.5 54.2 54.6 58.6 43.4 46.5 47.0 48.5 55. - - - 57.5 - 45.9 49.8 50.9 - 58.7 47.2 49.9 51.3 45.6 47.6 44.5 50.1 52.0 performance, including improvements of +4.2 EM-1 and +7.8 EM-R1, with notable gains in the Is (+15.3) and Which (+13.9) categories. 15 Figure 7: Qualitative example on VSI-Bench [18]. Figure 8: Qualitative example on VSI-Bench [18]. 16 Figure 9: Qualitative example on VSI-Bench [18]. Figure 10: Qualitative example on VSI-Bench [18]."
        },
        {
            "title": "References",
            "content": "[1] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al., Flamingo: visual language model for few-shot learning, NeurIPS, 2022. [2] J. Li, D. Li, S. Savarese, and S. Hoi, Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, in ICML, 2023. [3] H. Liu, C. Li, Q. Wu, and Y. J. Lee, Visual instruction tuning, NeurIPS, 2024. [4] G. Team, P. Georgiev, V. I. Lei, R. Burnell, L. Bai, A. Gulati, G. Tanzer, D. Vincent, Z. Pan, S. Wang, et al., Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, arXiv preprint arXiv:2403.05530, 2024. [5] A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford, et al., Gpt-4o system card, arXiv preprint arXiv:2410.21276, 2024. [6] B. Li, Y. Zhang, D. Guo, R. Zhang, F. Li, H. Zhang, K. Zhang, Y. Li, Z. Liu, and C. Li, Llava-onevision: Easy visual task transfer, arXiv preprint arXiv:2408.03326, 2024. [7] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, M. Zhong, Q. Zhang, X. Zhu, L. Lu, et al., Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks, in CVPR, 2024. [8] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou, Qwen-vl: frontier large vision-language model with versatile abilities, arXiv preprint arXiv:2308.12966, 2023. [9] B. Lin, B. Zhu, Y. Ye, M. Ning, P. Jin, and L. Yuan, Video-llava: Learning united visual representation by alignment before projection, arXiv preprint arXiv:2311.10122, 2023. [10] Z. Cheng, S. Leng, H. Zhang, Y. Xin, X. Li, G. Chen, Y. Zhu, W. Zhang, Z. Luo, D. Zhao, et al., Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms, arXiv preprint arXiv:2406.07476, 2024. [11] R. Qian, X. Dong, P. Zhang, Y. Zang, S. Ding, D. Lin, and J. Wang, Streaming long video understanding with large language models, arXiv preprint arXiv:2405.16009, 2024. [12] Y. Zhang, J. Wu, W. Li, B. Li, Z. Ma, Z. Liu, and C. Li, Video instruction tuning with synthetic data, ArXiv, vol. abs/2410.02713, 2024. [13] P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K.-Y. Chen, X. Liu, J. Wang, W. Ge, Y. Fan, K. Dang, M. Du, X. Ren, R. Men, D. Liu, C. Zhou, J. Zhou, and J. Lin, Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, ArXiv, vol. abs/2409.12191, 2024. [14] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, H. Zhong, Y. Zhu, M. Yang, Z. Li, J. Wan, P. Wang, W. Ding, Z. Fu, Y. Xu, J. Ye, X. Zhang, T. Xie, Z. Cheng, H. Zhang, Z. Yang, H. Xu, and J. Lin, Qwen2.5-vl technical report, ArXiv, vol. abs/2502.13923, 2025. [15] R. Huang, M. Li, D. Yang, J. Shi, X. Chang, Z. Ye, Y. Wu, Z. Hong, J.-B. Huang, J. Liu, Y. Ren, Z. Zhao, and S. Watanabe, Audiogpt: Understanding and generating speech, music, sound, and talking head, ArXiv, vol. abs/2304.12995, 2023. [16] C. Tang, W. Yu, G. Sun, X. Chen, T. Tan, W. Li, L. Lu, Z. Ma, and C. Zhang, Salmonn: Towards generic hearing abilities for large language models, ArXiv, vol. abs/2310.13289, 2023. [17] Z. Liu, Y. Dong, J. Wang, Z. Liu, W. Hu, J. Lu, and Y. Rao, Ola: Pushing the frontiers of omni-modal language model with progressive modality alignment, ArXiv, vol. abs/2502.04328, 2025. [18] J. Yang, S. Yang, A. W. Gupta, R. Han, F.-F. Li, and S. Xie, Thinking in space: How multimodal large language models see, remember, and recall spaces, ArXiv, vol. abs/2412.14171, 2024. [19] B. Chen, Z. Xu, S. Kirmani, B. Ichter, D. Driess, P. Florence, D. Sadigh, L. J. Guibas, and F. Xia, Spatialvlm: Endowing vision-language models with spatial reasoning capabilities, 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1445514465, 2024. [20] Y. Li, Y. Zhang, T. Lin, X. Liu, W. Cai, Z. Liu, and B. Zhao, Sti-bench: Are mllms ready for precise spatial-temporal world understanding?, ArXiv, vol. abs/2503.23765, 2025. [21] J. Deng, T. He, L. Jiang, T. Wang, F. Dayoub, and I. Reid, 3d-llava: Towards generalist 3d lmms with omni superpoint transformer, ArXiv, vol. abs/2501.01163, 2025. [22] H. Huang, Z. Wang, R. Huang, L. Liu, X. Cheng, Y. Zhao, T. Jin, and Z. Zhao, Chat-scene: Bridging 3d scene and large language models with object identifiers, in Neural Information Processing Systems, 2023. [23] S. Chen, X. Chen, C. Zhang, M. Li, G. Yu, H. Fei, H. Zhu, J. Fan, and T. Chen, Ll3da: Visual interactive instruction tuning for omni-3d understanding reasoning and planning, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2642826438, 2024. [24] C. Zhu, T. Wang, W. Zhang, J. Pang, and X. Liu, Llava-3d: simple yet effective pathway to empowering lmms with 3d-awareness, arXiv preprint arXiv:2409.18125, 2024. 18 [25] D. Zheng, S. Huang, and L. Wang, Video-3d llm: Learning position-aware video representation for 3d scene understanding, ArXiv, vol. abs/2412.00493, 2024. [26] S. Y. Gadre, G. Ilharco, A. Fang, J. Hayase, G. Smyrnis, T. Nguyen, R. Marten, M. Wortsman, D. Ghosh, J. Zhang, E. Orgad, R. Entezari, G. Daras, S. Pratt, V. Ramanujan, Y. Bitton, K. Marathe, S. Mussmann, R. Vencu, M. Cherti, R. Krishna, P. W. Koh, O. Saukh, A. J. Ratner, S. Song, H. Hajishirzi, A. Farhadi, R. Beaumont, S. Oh, A. G. Dimakis, J. Jitsev, Y. Carmon, V. Shankar, and L. Schmidt, Datacomp: In search of the next generation of multimodal datasets, ArXiv, vol. abs/2304.14108, 2023. [27] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, Learning transferable visual models from natural language supervision, in International Conference on Machine Learning, 2021. [28] Z. Tang, L. Lian, S. Eisape, X. Wang, R. Herzig, A. Yala, A. Suhr, T. Darrell, and D. M. Chan, Tulip: Towards unified language-image pretraining, ArXiv, vol. abs/2503.15485, 2025. [29] J. Qi, J. Liu, H. Tang, and Z. Zhu, Beyond semantics: Rediscovering spatial awareness in vision-language models, ArXiv, vol. abs/2503.17349, 2025. [30] B. Zhang, P. Zhang, X. wen Dong, Y. Zang, and J. Wang, Long-clip: Unlocking the long-text capability of clip, in European Conference on Computer Vision, 2024. [31] S. Wang, V. Leroy, Y. Cabon, B. Chidlovskii, and J. Revaud, Dust3r: Geometric 3d vision made easy, 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2069720709, 2023. [32] J. Wang, M. Chen, N. Karaev, A. Vedaldi, C. Rupprecht, and D. Novotny, Vggt: Visual geometry grounded transformer, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [33] Z. Li, R. Tucker, F. Cole, Q. Wang, L. Jin, V. Ye, A. Kanazawa, A. Holynski, and N. Snavely, Megasam: Accurate, fast, and robust structure and motion from casual dynamic videos, ArXiv, vol. abs/2412.04463, 2024. [34] DeepSeek-AI, D. Guo, D. Yang, H. Zhang, J.-M. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, X. Zhang, X. Yu, Y. Wu, Z. F. Wu, Z. Gou, Z. Shao, Z. Li, Z. Gao, A. Liu, B. Xue, B.-L. Wang, B. Wu, B. Feng, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, D. Dai, D. Chen, D.-L. Ji, E. Li, F. Lin, F. Dai, F. Luo, G. Hao, G. Chen, G. Li, H. Zhang, H. Bao, H. Xu, H. Wang, H. Ding, H. Xin, H. Gao, H. Qu, H. Li, J. Guo, J. Li, J. Wang, J. Chen, J. Yuan, J. Qiu, J. Li, J. Cai, J. Ni, J. Liang, J. Chen, K. Dong, K. Hu, K. Gao, K. Guan, K. Huang, K. Yu, L. Wang, L. Zhang, L. Zhao, L. Wang, L. Zhang, L. Xu, L. Xia, M. Zhang, M. Zhang, M. Tang, M. Li, M. Wang, M. Li, N. Tian, P. Huang, P. Zhang, Q. Wang, Q. Chen, Q. Du, R. Ge, R. Zhang, R. Pan, R. Wang, R. J. Chen, R. Jin, R. Chen, S. Lu, S. Zhou, S. Chen, S. Ye, S. Wang, S. Yu, S. Zhou, S. Pan, S. S. Li, S. Zhou, S.-K. Wu, T. Yun, T. Pei, T. Sun, T. Wang, W. Zeng, W. Zhao, W. Liu, W. Liang, W. Gao, W.-X. Yu, W. Zhang, W. L. Xiao, W. An, X. Liu, X. Wang, X. Chen, X. Nie, X. Cheng, X. Liu, X. Xie, X. Liu, X. Yang, X. Li, X. Su, X. Lin, X. Q. Li, X. Jin, X.-C. Shen, X. Chen, X. Sun, X. Wang, X. Song, X. Zhou, X. Wang, X. Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. Zhang, Y. Xu, Y. Li, Y. Zhao, Y. Sun, Y. Wang, Y. Yu, Y. Zhang, Y. Shi, Y. Xiong, Y. He, Y. Piao, Y. Wang, Y. Tan, Y. Ma, Y. Liu, Y. Guo, Y. Ou, Y. Wang, Y. Gong, Y.-J. Zou, Y. He, Y. Xiong, Y.-W. Luo, Y. mei You, Y. Liu, Y. Zhou, Y. X. Zhu, Y. Huang, Y. Li, Y. Zheng, Y. Zhu, Y. Ma, Y. Tang, Y. Zha, Y. Yan, Z. Ren, Z. Ren, Z. Sha, Z. Fu, Z. Xu, Z. Xie, Z. guo Zhang, Z. Hao, Z. Ma, Z. Yan, Z. Wu, Z. Gu, Z. Zhu, Z. Liu, Z.-A. Li, Z. Xie, Z. Song, Z. Pan, Z. Huang, Z. Xu, Z. Zhang, and Z. Zhang, Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, ArXiv, vol. abs/2501.12948, 2025. [35] Z. Shao, P. Wang, Q. Zhu, R. Xu, J.-M. Song, M. Zhang, Y. K. Li, Y. Wu, and D. Guo, Deepseekmath: Pushing the limits of mathematical reasoning in open language models, ArXiv, vol. abs/2402.03300, 2024. [36] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. H. Chi, F. Xia, Q. Le, and D. Zhou, Chain of thought prompting elicits reasoning in large language models, ArXiv, vol. abs/2201.11903, 2022. [37] D. Azuma, T. Miyanishi, S. Kurita, and M. Kawanabe, Scanqa: 3d question answering for spatial scene understanding, 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1910719117, 2021. [38] X. Ma, S. Yong, Z. Zheng, Q. Li, Y. Liang, S.-C. Zhu, and S. Huang, Sqa3d: Situated question answering in 3d scenes, ArXiv, vol. abs/2210.07474, 2022. [39] H. Liu, C. Li, Y. Li, and Y. J. Lee, Improved baselines with visual instruction tuning, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2629626306, 2024. [40] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, Minigpt-4: Enhancing vision-language understanding with advanced large language models, arXiv preprint arXiv:2304.10592, 2023. [41] Y. Su, T. Lan, H. Li, J. Xu, Y. Wang, and D. Cai, Pandagpt: One model to instruction-follow them all, arXiv preprint arXiv:2305.16355, 2023. 19 [42] R. Pi, J. Gao, S. Diao, R. Pan, H. Dong, J. Zhang, L. Yao, J. Han, H. Xu, L. Kong, et al., Detgpt: Detect what you need via reasoning, arXiv preprint arXiv:2305.14167, 2023. [43] K. Li, Y. He, Y. Wang, Y. Li, W. Wang, P. Luo, Y. Wang, L. Wang, and Y. Qiao, Videochat: Chat-centric video understanding, arXiv preprint arXiv:2305.06355, 2023. [44] Y. Chen, S. Yang, H. Huang, T. Wang, R. Xu, R. Lyu, D. Lin, and J. Pang, Grounded 3d-llm with referent tokens, arXiv preprint arXiv:2405.10370, 2024. [45] Z. Wang, H. Huang, Y. Zhao, Z. Zhang, and Z. Zhao, Chat-3d: Data-efficiently tuning large language model for universal dialogue of 3d scenes, arXiv preprint arXiv:2308.08769, 2023. [46] J. Huang, S. Yong, X. Ma, X. Linghu, P. Li, Y. Wang, Q. Li, S.-C. Zhu, B. Jia, and S. Huang, An embodied generalist agent in 3d world, arXiv preprint arXiv:2311.12871, 2023. [47] H. Huang, Y. Chen, Z. Wang, R. Huang, R. Xu, T. Wang, L. Liu, X. Cheng, Y. Zhao, J. Pang, et al., Chat-scene: Bridging 3d scene and large language models with object identifiers, in The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [48] Y. Hong, H. Zhen, P. Chen, S. Zheng, Y. Du, Z. Chen, and C. Gan, 3d-llm: Injecting the 3d world into large language models, Advances in Neural Information Processing Systems, vol. 36, pp. 2048220494, 2023. [49] R. Fu, J. Liu, X. Chen, Y. Nie, and W. Xiong, Scene-llm: Extending language model for 3d visual understanding and reasoning, arXiv preprint arXiv:2403.11401, 2024. [50] Z. Qi, Z. Zhang, Y. Fang, J. Wang, and H. Zhao, Gpt4scene: Understand 3d scenes from videos with vision-language models, arXiv preprint arXiv:2501.01428, 2025. [51] Z. Liu, Y. Dong, Z. Liu, W. Hu, J. Lu, and Y. Rao, Oryx mllm: On-demand spatial-temporal understanding at arbitrary resolution, arXiv preprint arXiv:2409.12961, 2024. [52] X. Wang, Y. Zhang, O. Zohar, and S. Yeung-Levy, Videoagent: Long-form video understanding with large language model as agent, in European Conference on Computer Vision, pp. 5876, Springer, 2024. [53] Y. Li, Y. Zhang, T. Lin, X. Liu, W. Cai, Z. Liu, and B. Zhao, Sti-bench: Are mllms ready for precise spatial-temporal world understanding?, arXiv preprint arXiv:2503.23765, 2025. [54] P. Wu, Y. Liu, M. Liu, and J. Shen, St-think: How multimodal large language models reason about 4d worlds from ego-centric videos, arXiv preprint arXiv:2503.12542, 2025. [55] S. Zhou, A. Vilesov, X. He, Z. Wan, S. Zhang, A. N. D. C. D. Chen, and X. E. W. A. Kadambi, Vlm4d: Towards spatiotemporal awareness in vision language models, [56] A. Vaswani, N. M. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, Attention is all you need, in Neural Information Processing Systems, 2017. [57] T. Darcet, M. Oquab, J. Mairal, and P. Bojanowski, Vision transformers need registers, ArXiv, vol. abs/2309.16588, 2023. [58] X. Pan, S. N. Shukla, A. Singh, Z. Zhao, S. K. Mishra, J. Wang, Z. Xu, J. Chen, K. Li, F. Juefei-Xu, J. Hou, and S. Xie, Transfer between modalities with metaqueries, 2025. [59] G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher, An analysis of approximations for maximizing submodular set functionsi, Mathematical Programming, vol. 14, no. 1, pp. 265294, 1978. [60] D. S. Hochbaum, Approximating covering and packing problems: set cover, vertex cover, independent set, and related problems, p. 94143. USA: PWS Publishing Co., 1996. [61] A. Dai, A. X. Chang, M. Savva, M. Halber, T. A. Funkhouser, and M. Nießner, Scannet: Richly-annotated 3d reconstructions of indoor scenes, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 24322443, 2017. [62] D. P. Kingma and J. Ba, Adam: method for stochastic optimization, CoRR, vol. abs/1412.6980, 2014. [63] F. Xue, Y. Chen, D. Li, Q. Hu, L. Zhu, X. Li, Y. Fang, H. Tang, S. Yang, Z. Liu, E. He, H. Yin, P. Molchanov, J. Kautz, L. Fan, Y. Zhu, Y. Lu, and S. Han, Longvila: Scaling long-context visual language models for long videos, ArXiv, vol. abs/2408.10188, 2024. [64] J. Lin, H. Yin, W. Ping, Y. Lu, P. Molchanov, A. Tao, H. Mao, J. Kautz, M. Shoeybi, and S. Han, Vila: On pre-training for visual language models, 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2667926689, 2023. [65] P. Zhang, K. Zhang, B. Li, G. Zeng, J. Yang, Y. Zhang, Z. Wang, H. Tan, C. Li, and Z. Liu, Long context transfer from language to vision, ArXiv, vol. abs/2406.16852, 2024. [66] C. Yeshwanth, Y.-C. Liu, M. Nießner, and A. Dai, Scannet++: high-fidelity dataset of 3d indoor scenes, 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 1222, 2023. 20 [67] G. Baruch, Z. Chen, A. Dehghan, T. Dimry, Y. Feigin, P. Fu, T. Gebauer, B. Joffe, D. Kurz, A. Schwartz, and E. Shulman, ARKitscenes - diverse real-world dataset for 3d indoor scene understanding using mobile RGB-d data, in Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021. [68] J. Huang, S. Yong, X. Ma, X. Linghu, P. Li, Y. Wang, Q. Li, S.-C. Zhu, B. Jia, and S. Huang, An embodied generalist agent in 3d world, ArXiv, vol. abs/2311.12871, 2023. [69] Z. Zhu, X. Ma, Y. Chen, Z. Deng, S. Huang, and Q. Li, 3d-vista: Pre-trained transformer for 3d vision and text alignment, 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 28992909, 2023. [70] Y. Hong, H. Zhen, P. Chen, S. Zheng, Y. Du, Z. Chen, and C. Gan, 3d-llm: Injecting the 3d world into large language models, NeurIPS, 2023. [71] Q.-Y. Zhou, J. Park, and V. Koltun, Open3d: modern library for 3d data processing, ArXiv, vol. abs/1801.09847, 2018. [72] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, Indoor segmentation and support inference from rgbd images, in European Conference on Computer Vision, 2012. [73] S. Gupta, P. Arbeláez, and J. Malik, Perceptual organization and recognition of indoor scenes from rgb-d images, 2013 IEEE Conference on Computer Vision and Pattern Recognition, pp. 564571, 2013. [74] R. Fu, J. Liu, X. Chen, Y. Nie, and W. Xiong, Scene-llm: Extending language model for 3d visual understanding and reasoning, ArXiv, vol. abs/2403.11401, 2024."
        }
    ],
    "affiliations": [
        "Tsinghua University"
    ]
}