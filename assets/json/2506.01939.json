{
    "paper_title": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning",
    "authors": [
        "Shenzhi Wang",
        "Le Yu",
        "Chang Gao",
        "Chujie Zheng",
        "Shixuan Liu",
        "Rui Lu",
        "Kai Dang",
        "Xionghui Chen",
        "Jianxin Yang",
        "Zhenru Zhang",
        "Yuqiong Liu",
        "An Yang",
        "Andrew Zhao",
        "Yang Yue",
        "Shiji Song",
        "Bowen Yu",
        "Gao Huang",
        "Junyang Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful approach to enhancing the reasoning capabilities of Large Language Models (LLMs), while its mechanisms are not yet well understood. In this work, we undertake a pioneering exploration of RLVR through the novel perspective of token entropy patterns, comprehensively analyzing how different tokens influence reasoning performance. By examining token entropy patterns in Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of tokens exhibit high entropy, and these tokens act as critical forks that steer the model toward diverse reasoning pathways. Furthermore, studying how entropy patterns evolve during RLVR training reveals that RLVR largely adheres to the base model's entropy patterns, primarily adjusting the entropy of high-entropy tokens. These findings highlight the significance of high-entropy tokens (i.e., forking tokens) to RLVR. We ultimately improve RLVR by restricting policy gradient updates to forking tokens and uncover a finding even beyond the 80/20 rule: utilizing only 20% of the tokens while maintaining performance comparable to full-gradient updates on the Qwen3-8B base model and significantly surpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71 on AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models, highlighting a strong scaling trend. In contrast, training exclusively on the 80% lowest-entropy tokens leads to a marked decline in performance. These findings indicate that the efficacy of RLVR primarily arises from optimizing the high-entropy tokens that decide reasoning directions. Collectively, our results highlight the potential to understand RLVR through a token-entropy perspective and optimize RLVR by leveraging high-entropy minority tokens to further improve LLM reasoning."
        },
        {
            "title": "Start",
            "content": "2025-06-03 Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning Shenzhi Wang1,2, Le Yu1, Chang Gao1, Chujie Zheng1, Shixuan Liu1, Rui Lu2, Kai Dang1, Xionghui Chen1, Jianxin Yang1, Zhenru Zhang1, Yuqiong Liu1, An Yang1, Andrew Zhao2, Yang Yue2, Shiji Song2, Bowen Yu1,(cid:66),, Gao Huang2,(cid:66) , Junyang Lin1 1 Qwen Team, Alibaba Inc. 2 LeapLab, Tsinghua University Project Page: https://shenzhi-wang.github.io/high-entropy-minority-tokens-rlvr"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as powerful approach to enhancing the reasoning capabilities of Large Language Models (LLMs), while its mechanisms are not yet well understood. In this work, we undertake pioneering exploration of RLVR through the novel perspective of token entropy patterns, comprehensively analyzing how different tokens influence reasoning performance. By examining token entropy patterns in Chain-of-Thought (CoT) reasoning, we observe that only small fraction of tokens exhibit high entropy, and these tokens act as critical forks that steer the model toward diverse reasoning pathways. Furthermore, studying how entropy patterns evolve during RLVR training reveals that RLVR largely adheres to the base models entropy patterns, primarily adjusting the entropy of high-entropy tokens. These findings highlight the significance of high-entropy tokens (i.e., forking tokens) to RLVR. We ultimately improve RLVR by restricting policy gradient updates to forking tokens and uncover finding even beyond the 80/20 rule: utilizing only 20% of the tokens while maintaining performance comparable to full-gradient updates on the Qwen3-8B base model and significantly surpassing full-gradient updates on the Qwen332B (+11.04 on AIME25 and +7.71 on AIME24) and Qwen3-14B (+4.79 on AIME25 and +5.21 on AIME24) base models, highlighting strong scaling trend. In contrast, training exclusively on the 80% lowest-entropy tokens leads to marked decline in performance. These findings indicate that the efficacy of RLVR primarily arises from optimizing the high-entropy tokens that decide reasoning directions. Collectively, our results highlight the potential to understand RLVR through token-entropy perspective and optimize RLVR by leveraging high-entropy minority tokens to further improve LLM reasoning. 5 2 0 2 2 ] . [ 1 9 3 9 1 0 . 6 0 5 2 : r Figure 1: (a) In CoTs, only minority of tokens exhibit high entropy and act as \"forks\" in reasoning paths, while majority tokens are low-entropy. (b) RLVR using policy gradients of only forking tokens delivers significant performance gains that scale with model size. With 20k maximum response length, our 32B model sets new SoTA scores (63.5 on AIME24 and 56.7 on AIME25) for RLVR on base models under 600B. Extending the maximum response length to 29k further boosts the AIME24 score to 68.1. (cid:66): Corresponding authors Emails: : Project lead wangshenzhi99@gmail.com yubowen.ph@gmail.com gaohuang@tsinghua.edu.cn"
        },
        {
            "title": "Introduction",
            "content": "The reasoning capabilities of Large Language Models (LLMs) have advanced substantially in domains like mathematics and programming, propelled by test-time scaling methodologies employed in OpenAI o1 (OpenAI, 2024), Claude 3.7 (Anthropic, 2025), DeepSeek R1 (DeepSeek-AI et al., 2025), Kimi K1.5 (Team et al., 2025), and Qwen3 (Yang et al., 2025). pivotal technique driving these improvements is Reinforcement Learning with Verifiable Rewards (RLVR) (Lambert et al., 2025; DeepSeek-AI et al., 2025; Yang et al., 2025), where models optimize outputs through RL objectives tied to automated correctness verification. While recent RLVR advancements have stemmed from algorithmic innovations (Yu et al., 2025; Yue et al., 2025b; Guan et al., 2025), cross-domain applications (Xue et al., 2025; Liu et al., 2025; Pan et al., 2025), and counterintuitive empirical insights (Wang et al., 2025; Yue et al., 2025a; Zhao et al., 2025), existing implementations directly train over all tokens with limited understanding of which tokens actually facilitate reasoning. These approaches neglect the heterogeneous functional roles tokens play in reasoning processes, potentially hindering further performance gains by failing to prioritize critical decision points in sequential reasoning trajectories. In this paper, we analyze the underlying mechanisms of RLVR through an innovative lens of token entropy patterns, investigating how tokens with varying entropy impact reasoning performance. We first point out that in the Chain-of-Thought (CoT) processes of LLMs, the entropy distribution exhibits distinct pattern where the majority of tokens are generated with low entropy, while critical minority of tokens emerge with high entropy. Through comparing the textual meanings of these two parts of tokens, we observe that the tokens with lowest average entropy primarily complete the ongoing linguistic structures, while the tokens with highest average entropy function as pivotal decision points that determine the trajectory of reasoning among multiple potential pathways (referred to as forks), as depicted in Figure 1(a). In addition to qualitatively anslysis, we conduct controlled experiments by manually modulating the entropy of forking tokens during decoding. Quantitative results reveal that moderately increasing the entropy of these high-entropy forking tokens leads to measurable improvements in reasoning performance, while artificially reducing their entropy results in performance degradation, confirming the importance of maintaining high entropy and the role as \"forks\" for these high-entropy tokens. Furthermore, by analyzing the evolution of token entropy during RLVR training, we find that the reasoning model largely retains the entropy patterns of the base model, exhibiting only gradual and relatively minor changes as training progresses. Additionally, RLVR primarily changes the entropy of high-entropy tokens, while the entropy of low-entropy tokens varies only within small range. The above observations highlight the critical role high-entropy minority tokens may play in CoTs and RLVR training. Building upon the discovery of forking tokens, we further refine RLVR by retaining policy gradient updates for only 20% of tokens with the highest entropy and masking gradients of the remaining 80%. We observe that although solely utilizing 20% of tokens, our approach can still achieve competitive reasoning performance on Qwen3-8B base model compared to full-gradient updates. Moreover, its effectiveness increases with model size, yielding reasoning improvements of +11.04 on AIME25 and +7.71 on AIME24 for the Qwen3-32B base model, and +4.79 on AIME25 and +5.21 on AIME24 for the Qwen3-14B base model, as shown in Figure 1(b). Notably, the 32B model trained with only 20% high-entropy tokens attains scores of 63.5 on AIME24 and 56.7 on AIME25, setting new state-of-the-art (SoTA) for reasoning models trained directly from base models with fewer than 600B parameters. Extending the maximum response length from 20k to 29k further elevates our 32B models AIME24 score from 63.5 to 68.1. Conversely, training exclusively on the 80% lowest-entropy tokens results in severe performance degradation. These observations show that 20% of tokens achieve performance comparable to or exceeding 100%, even surpassing the 80/20 rule. The results demonstrate that the high-entropy minority tokens, functioning as critical decision points in reasoning trajectories, account for nearly all performance gains in RLVR. Finally, we explore why retaining small fraction of the highest-entropy tokens leads to strong performance in RLVR via series of ablation studies. We adjust the chosen fraction of forking tokens, either by decreasing it from 20% to 10% or increasing it to 50% or 100%, and report the corresponding reasoning metrics and overall entropy during the training process. Experimental results demonstrate that retaining approximately 20% of the highest-entropy tokens optimally balances exploration and performance, while deviating from 20% reduces the overall entropy with diminished exploration and incurs worse performance. This suggests that only critical subset of high-entropy tokens meaningfully contributes to the exploration during RL while others may be neutral or even detrimental. Reducing the proportion to 10% removes certain useful tokens, which weakens exploration. Increasing the proportion to 50% or 100% adds low-entropy tokens, which also reduces the effectiveness of exploration. Last but not least, retaining the top 20% of high-entropy tokens results in the largest performance gains for the 32B model, followed by the 14B model, and the smallest gains for the 8B model. This may be due to the insufficient capacity of the smaller model, which restricts its ability to benefit from increased exploration. These findings highlight the importance of preserving an appropriate proportion of high-entropy tokens in RLVR. As model size increases, the strategy of selecting high-entropy tokens appears to scale effectively. 2 In summary, our findings emphasize the pivotal role of high-entropy minority tokens in shaping the reasoning abilities of LLMs. We hope this inspires further analyses from the perspective of token entropy and informs more effective RLVR algorithms that strategically leverage these tokens to enhance reasoning performance. The key takeaways of our paper are as follows: In CoTs, the majority of tokens are generated with low entropy, while only small subset exhibits high entropy. These high-entropy minority tokens often act as \"forks\" in the reasoning process, guiding the model toward diverse reasoning paths. Maintaining high entropy at these critical forking tokens is beneficial for reasoning performance. (3) During RLVR training, the reasoning model largely preserves the base models entropy patterns, showing only gradual and minor changes. RLVR primarily adjusts the entropy of high-entropy tokens, while the entropy of low-entropy tokens fluctuates only within narrow range. (4) High-entropy minority tokens drive nearly all reasoning performance gains during RLVR, whereas lowentropy majority tokens contribute little or may even hinder performance. One possible explanation is that, prior to performance convergence, subset ( 20% in our experiments) of high-entropy tokens facilitates exploration, while low-entropy tokens offer minimal benefit or may even impede it. (5) Based on the insights above, we further discuss (i) high-entropy minority tokens as potential reason why supervised fine-tuning (SFT) memorizes but RL generalizes, (ii) how prior knowledge and readability requirements shape the different entropy patterns seen in LLM CoTs compared to traditional RL trajectories, and (iii) the advantage of clip-higher over entropy bonus for RLVR. (6)"
        },
        {
            "title": "2 Preliminaries",
            "content": "2.1 Token entropy calculation The token-level generation entropy (referred to as token entropy for brevity) for token is defined as Ht := j=1 pt,j log pt,j, where (pt,1, , pt,V) = pt = πθ( q, o<t) = Softmax (cid:17) . (cid:16) zt (1) Here, πθ denotes an LLM parameterized by θ, is the input query, and o<t = (o1, o2, , ot1) represents the previously generated tokens. is the vocabulary size, zt RV denotes the pre-softmax logits at time step t, pt RV is the corresponding probability distribution over the vocabulary, and is the decoding temperature. In off-policy settings, sequences are generated by rollout policy πϕ while the training policy is πθ, with ϕ = θ. The entropy is still calculated using πθ, as defined in Equation (1), to measure the uncertainty of the training policy in the given sequence. \"Token entropy\" corresponds to the token generation distribution, not specific token. Throughout our paper, we clarify that the token entropy Ht refers to the entropy at index t, which is determined by the token generation distribution pt rather than by any specific token ot sampled from pt. For brevity, when discussing the token ot sampled from the distribution pt, we describe its associated entropy as Ht and refer to Ht as the token entropy of ot. However, if there exists another index = such that ot = ot, the token entropy of ot is not necessarily equal to Ht. 2.2 RLVR Algorithms Proximal Policy Optimization (PPO) PPO (Schulman et al., 2017) is widely adopted policy gradient algorithm in RLVR. To stabilize training, PPO restricts policy updates to remain within proximal region of the old policy πθold using the following clipped surrogate to maximize the objective: (cid:1)(cid:3) , (cid:2)min (cid:0)rt(θ) ˆAt, clip(rt(θ), 1 ϵ, 1 + ϵ) ˆAt JPPO(θ) =E (q,a)D,oπθold with rt(θ) = (q) πθ(otq, o<t) (otq, o<t) πθold . (2) Here, is dataset of queries and corresponding ground-truth answers a, ϵ is hyperparameter typically set to 2.0, and ˆAt is the estimated advantage computed using value network. Group Relative Policy Optimization (GRPO) Building on the clipped objective in Equation (2), GRPO (Shao et al., 2024) discards the value network by estimating advantages using the average reward within group of sampled responses. Specifically, for each query and its ground-truth answer a, the 3 Figure 2: Entropy patterns in the chain of thoughts of LLMs. (a) Token entropy distribution. The Y-axis frequency is on log scale. minority of tokens exhibit high entropy, while the majority have low entropy, often approaching zero. (b) & (c) Word clouds of the top 100 tokens with the highest and lowest average entropy, respectively, selected from the set of frequently occurring tokens. larger font size indicates higher average token entropy. Tokens with the highest average entropy typically function as \"forks\" to determine reasoning directions, whereas tokens with the lowest average entropy tend to execute reasoning steps along the established path. rollout policy πθold generates group of responses {oi}G where is the group size. The estimated advantage ˆAi (cid:26)1.0 0.0 ri mean({Ri}G std({Ri}G i=1) , where Ri = i=1) = ˆAi i=1 with corresponding outcome rewards {Ri}G i=1, is then computed as: if is_equivalent(a, oi), otherwise. (3) In addition to this modified advantage estimation, GRPO adds KL penalty term to the clipped objective in Equation (2). Dynamic sAmpling Policy Optimization (DAPO) Building on GRPO, DAPO (Yu et al., 2025) removes the KL penalty, introduces clip-higher mechanism, incorporates dynamic sampling, applies tokenlevel policy gradient loss, and adopts overlong reward shaping, leading to the following maximization objective, where ri t(θ) is defined as in Equation (2), and ˆAi is computed as in Equation (3): JDAPO(θ) =E (q,a)D,{oi}G i= πθold (q) (cid:34) clip(cid:0)ri t(θ), 1 ϵlow, 1 + ϵhigh 1 i=1 oi (cid:35) (cid:17) (cid:1) ˆAi i=1 oi t=1 (cid:16) min t(θ) ˆAi ri t, , s.t. 0 < (cid:110) (cid:12) (cid:12) (cid:12) oi is_equivalent(a, oi) (cid:111)(cid:12) (cid:12) (cid:12) < G. (4) DAPO is one of the state-of-the-art RLVR algorithms without value network. In this work, we use DAPO as the baseline for our RLVR experiments."
        },
        {
            "title": "3 Analyzing Token Entropy in Chain-of-Thought Reasoning",
            "content": "Although prior works (Yang et al., 2025; Yu et al., 2025; Yue et al., 2025b) have highlighted the importance of generation entropy in chain-of-thought reasoning, they typically analyze the entropy of all tokens collectively. In this section, we take closer look at generation entropy in chain-of-thought reasoning by examining it at the token level. To this end, we use Qwen3-8B (Yang et al., 2025), one of the most recent and capable reasoning models within comparable parameter scale, to generate responses for queries from AIME24 and AIME25, using decoding temperature of = 1.0. We enforce the use of the thinking mode for every question and collect over 106 response tokens. For each token, the entropy is computed according to the formulation in Equation (1). The statistical analysis of the entropy values of these 106 tokens is presented in Figure 2. Furthermore, visualization of token entropy for an entire long CoT response is provided in Figures 12 to 17 in the Appendix. From these analyses, we identify the following entropy patterns: 4 Entropy Pattern 1 in CoTs: Typically, only minority of tokens are generated with high entropy, while majority of tokens are outputted with low entropy. We can observe in Figure 2(a) that the entropy of large amount of tokens are quite small, and only small amount of tokens have high entropy. Specifically, the entropy of over half the tokens (approximately 50.64%) is below 102, while only 20% of tokens have entropy greater than 0.672. Entropy Pattern 2 in CoTs: Tokens with the highest entropy typically serve to bridge the logical connection between two consecutive parts of reasoning, while tokens with the lowest entropy tend to complete the current part of sentence or finish constructing word. Other tokens combine these two functions to varying degrees. In Figure 2(b) and (c), we select the 100 tokens generated with the highest average entropy and the lowest average entropy from total of 106 tokens, respectively. To mitigate the impact of noise on the average entropy, we only consider tokens with frequencies above 100. High-entropy tokens often act as logical connectors within and across sentences, such as \"wait,\" \"however,\" and \"unless\" (indicating contrasts or shifts), \"thus\" and \"also\" (showing progression or addition), or \"since\" and \"because\" (expressing causality). Similarly, tokens like \"suppose,\" \"assume,\" \"given,\" and \"define\" frequently appear in mathematical derivations to introduce assumptions, known conditions, or definitions. Conversely, low-entropy tokens are often word suffixes, source code fragments, or mathematical expression components, all of which exhibit high determinism. Additionally, Figures 12 to 17 provide detailed visualization of token entropy in long CoT, showing that most tokens outside the highest-entropy or lowest-entropy groups blend bridging and continuation functions to varying degrees. High-entropy tokens as \"forks\" in chain-of-thoughts Based on the two observed patterns above, we refer to high-entropy tokens as \"forking tokens\", as they often lead to different potential branches with high uncertainty in the reasoning process. To further confirm the role of forking tokens in quantitative way, we assign different decoding temperatures to the forking tokens and the other tokens in the evaluation on AIME 2024 and AIME 2025. Specifically, to analyze the effects of varying combinations of these temperatures on their behavior, we adjust probability distribution RV for each token as follows: = Softmax (cid:19) , (cid:18) zt where = (cid:40) Thigh Tlow if Ht > hthreshold, otherwise. (5) Here, zt RV denotes the pre-softmax logits for token t, and represents the adjusted temperature for token t; hthreshold = 0.672 is the entropy threshold used to distinguish forking tokens from the other tokens, and is estimated by calculating the 80th percentile among the sampled 106 tokens above; Thigh and Tlow correspond to the temperatures for forking tokens and the other tokens, respectively. In contrast, The effects of varying Thigh and Tlow are presented in Figure 3. It can be seen that lowering Thigh significantly degrades performance compared to lowering Tlow. increasing Thigh results in substantially better performance than increasing Tlow, which can even cause LLMs generating nonsensical outputs. These results suggest that forking tokens benefit from being assigned relatively higher temperature compared to other tokens. Given that forking tokens naturally exhibit higher entropy than other tokens, this further supports the need for them to operate at an even higher entropy level. This observation aligns with their role as \"forks,\" where high entropy enables them to branch into diverse reasoning directions. Figure 3: Average scores of AIME 2024 and AIME 2025. Red curve: varying Thigh with Tlow = 1. Blue curve: varying Tlow with Thigh = 1."
        },
        {
            "title": "4 RLVR Preserves and Reinforces Base Model Entropy Patterns",
            "content": "In this section, building on the observations of entropy patterns in CoTs discussed in Section 3, we further investigate how these patterns evolve throughout RLVR training. RLVR primarily preserves the existing entropy patterns of the base models To analyze the evolution of entropy patterns during RLVR training, we apply DAPO (Yu et al., 2025) to the Qwen3-14B base model (details in Section 5). Using the reasoning model after RLVR, we generate 16 responses per question across the six benchmarks in Table 2. For each token in these responses, we compute logits using reasoning models from various RLVR stages and identify those in the top 20% entropy. We then calculate the overlap ratio (i.e., the fraction of shared top 20% high-entropy positions) between each intermediate model and both the base and final RLVR models. As shown in Table 1, although overlap with the base model gradually decreases and overlap with the final RLVR model increases, the base models overlap still remains above 86% at convergence (step 1360), suggesting that RLVR largely retains the base models entropy patterns regarding which tokens exhibit high or low uncertainty. Table 1: The progression of the overlap ratio in the positions of the top 20% high-entropy tokens, comparing the base model (i.e., step 0) with the model after RLVR training (i.e., step 1360). Compared w/ Step 0 Step 16 Step 112 Step 160 Step 480 Step 800 Step 864 Step 840 Step 1280 Step 1360 Base Model 100% 98.92% 98.70% 93.04% 93.02% 93.03% 87.45% 87.22% 87.09% 86.67% RLVR Model 86.67% 86.71% 86.83% 90.64% 90.65% 90.64% 96.61% 97.07% 97.34% 100% RLVR predominantly alters the entropy of high-entropy tokens, whereas the entropy of low-entropy tokens remains comparatively stable with minimal variations. Using the same setup as Table 1, we compute the average entropy change after RLVR for each 5% entropy percentile range of the base model. It is observed that tokens with higher initial entropy in the base model tend to exhibit larger increases in entropy after RLVR. This observation could also further reinforce that RLVR primarily preserves the entropy patterns of the base model. Moreover, Figure 5 illustrates the evolution of entropy percentiles during RLVR training using the Qwen3-14B base model. The figure reveals that as we move from the 0th to the 100th percentile, the range of fluctuations during the whole RLVR training steadily diminishes. Thus, these observations suggest that throughout the whole training process, RLVR primarily adjusts the entropy of high-entropy tokens, while the entropy of low-entropy tokens exhibits minor variation and remains relatively stable. Figure 4: Average entropy change after RLVR within each 5% entropy percentile range of the base model. x% percentile means that x% of the tokens in the dataset have entropy values less than or equal to this value. It is worth noting that the Y-axis is presented on log scale. Tokens with higher initial entropy tend to experience greater entropy increases after RLVR."
        },
        {
            "title": "5 High-Entropy Minority Tokens Drive Effective RLVR",
            "content": "Reinforcement learning with verifiable rewards (RLVR) has become one of the most widely used approaches for training reasoning models (Yang et al., 2025; DeepSeek-AI et al., 2025; Yu et al., 2025; Yue et al., 2025b). However, there is lack of research on which types of tokens contribute the most to the learning of reasoning models. As highlighted in Section 3 and Section 4, high-entropy minority tokens are particularly important. In this section, we investigate the contribution of these high-entropy minority tokens, also referred to as forking tokens, on the development of reasoning capabilities during RLVR. 6 Figure 5: The evolution of entropy percentiles during RLVR training. x-th percentile means that x% of the tokens in the dataset have entropy values less than or equal to this value. In other words, it represents the threshold below which the entropy values of the lowest x% of tokens fall, allowing us to track how different segments of the entropy distribution change throughout training. 5.1 Formulation of RLVR Using Only Policy Gradients of the Highest-Entropy Tokens Building on DAPOs objective in Equation (4), we discard the policy gradients of low-entropy tokens and train the model using only the policy gradients of high-entropy tokens. For each batch sampled from the dataset D, we calculate the maximum objective as: HighEnt(θ) =E BD,(q,a)B,{oi}G i=1 πθold (q) (cid:34) i= oi t=1 (cid:104) Hi τB ρ (cid:105) min (cid:16) t(θ) ˆAi ri t, 1 i=1 oi (cid:35) (cid:17) clip(cid:0)ri t(θ), 1 ϵlow, 1 + ϵhigh (cid:1) ˆAi , s.t. 0 < (cid:110) (cid:12) (cid:12) (cid:12) oi is_equivalent(a, oi) (cid:111)(cid:12) (cid:12) (cid:12) < G. (6) denotes the entropy of token in response i, I[] is the indicator function that evaluates to 1 if the Here, Hi condition inside holds and 0 otherwise, ρ (0, 1] is predefined ratio specifying the top proportion of high-entropy tokens to be selected within batch, and τB ρ is the corresponding entropy threshold within the batch such that only tokens with Hi ρ , comprising the top-ρ fraction of all tokens in the batch, are used to compute the gradient. τB Comparing Equation (6) with Equation (4), there are only two differences, as highlighted in red in Equation (6): (i) The advantage term is multiplied by whose τB corresponding entropy Hi ρ are involved in the policy gradient loss calculation; (ii) We filter out the top ρ of high-entropy tokens within each (micro-)batch sampled from the dataset D. , ensuring that only tokens oi τB ρ Hi (cid:104) (cid:105) 5.2 Experimental Setup Training details We adapt our training codebase from verl (Sheng et al., 2024) and follow the training recipe of DAPO (Yu et al., 2025), one of the state-of-the-art RL algorithms for LLMs. Both configurations, RLVR with full gradients (vanilla DAPO depicted in Equation (4)) and RLVR with only policy gradients on forking tokens (described in Equation (6)), employ techniques such as clip-higher, dynamic sampling, token-level policy gradient loss, and overlong reward shaping (Yu et al., 2025). For fair comparisons, we apply the same hyperparameters as recommended by DAPO: for clip-higher, ϵhigh = 0.28, ϵlow = 0.2; 7 Table 2: Comparison between vanilla DAPO using all tokens and DAPO using only the top 20% high-entropy tokens (i.e. forking tokens) in policy gradient loss, evaluated on the Qwen3-32B, Qwen3-14B and Qwen38B base models. \"Acc@16\" and \"Len@16\" denotes the average accuracy and response length over 16 evaluations per benchmark, respectively. Benchmark AIME24 AIME25 AMC23 MATH500 Minerva Olympiad Average AIME24 AIME25 AMC23 MATH500 Minerva Olympiad Average AIME24 AIME25 AMC23 MATH500 Minerva Olympiad Average 55.83 45.63 91.88 94.36 45.70 66.16 66.59 9644.15 9037.48 5285.03 2853.51 2675.28 5597.37 5848.80 DAPO w/ Forking Tokens DAPO w/ All Tokens Len@16 Len@16 Acc@16 Acc@16 RLVR from the Qwen3-32B Base Model 63.54 12197.54 56.67 11842.25 94.22 5896.47 94.88 3366.01 45.82 2759.88 69.02 7300.01 70.69 7227.03 RLVR from the Qwen3-14B Base Model 50.42 11814.36 7945.15 42.92 12060.48 7056.98 91.56 7095.13 4509.37 93.59 3970.10 2348.22 43.20 2959.32 2011.16 64.62 7871.25 4642.07 64.39 4752.16 7628.44 RLVR from the Qwen3-8B Base Model 34.58 26.25 77.19 89.70 40.26 57.43 54.23 6884.89 5915.91 3967.91 2059.00 1450.68 3853.55 4021. 9494.29 8120.20 5450.62 2672.91 2068.41 5241.54 5508.00 45.21 38.13 89.53 92.23 42.16 61.14 61.40 33.33 25.42 77.81 89.24 39.77 56.67 53.71 Improvement Acc@16 Len@ +7.71 +11.04 +2.34 +0.52 +0.12 +2.86 +4.10 +5.21 +4.79 +2.03 +1.37 +1.03 +3.48 +2.99 +1.25 +0.83 -0.625 +0.46 +0.48 +0.76 +0.53 +2553.39 +2804.77 +611.44 +512.5 +84.6 +1702.64 +1378.22 +3869.21 +5003.5 +2585.76 +1621.88 +948.16 +3229.18 +2876.28 +2609.40 +2204.29 +1482.71 +613.91 +617.73 +1387.99 +1486. for overlong reward shaping, the maximum response length is 20480 and the cache length is 4096. Furthermore, we use training batch size of 512 and mini-batch size of 32 in verls configuration, resulting in 16 gradient steps per training batch, with learning rate of 106 and no learning rate warmup or scheduling. Importantly, the training process excludes both KL divergence loss and entropy loss. To evaluate the scaling ability of these methods, we perform RLVR experiments across the Qwen332B base and Qwen3-8B base models, using DAPO-Math-17K (Yu et al., 2025) as the training dataset. For main results, we set ρ = 20% in Equation (6), meaning that the policy is updated using only the gradients of the top 20% highest-entropy tokens within each batch. The chat template we use for Qwen3 models is \"User:n[question]nPlease reason step by step, and put your final answer within boxed{}.nnAssistant:n\" with \"<endoftext>\" serving as the EOS token, where \"[question]\" should be replaced by the specific question. Evaluation We evaluate our models on 6 standard mathematical reasoning benchmarks commonly used for assessing reasoning capabilities: AIME24, AIME25, AMC23, MATH500 (Hendrycks et al., 2021), Minerva, and OlympiadBench (He et al., 2024). All evaluations are conducted in zero-shot setting. For each question, we generate 16 independent responses under decoding temperature = 1.0, and report the average accuracy and the average number of tokens per response. 5.3 Main Results High-entropy tokens drive reinforcement learning for LLM reasoning. Figure 6 and Table 2 compare vanilla DAPO which uses all tokens, and our approach that retains only the top 20% high-entropy tokens in the policy gradient loss. Surprisingly, discarding the bottom 80% low-entropy tokens does not degrade reasoning performance and can even lead to improvements across six benchmarks. On the Qwen3-32B base model, this approach delivers gains of 7.71 points on AIME24 and 11.04 points 8 Figure 6: comparison of vanilla DAPO with full tokens and DAPO with top 20% high-entropy (forking) tokens in policy gradient loss was conducted on Qwen3-32B, Qwen3-14B, and Qwen3-8B models. (a) & (b) Qwen3-32B: Dropping the bottom 80% low-entropy tokens stabilizes training and improves the AIME24 score by 7.73. (c) & (d) Qwen3-14B: Similarly, removing 80% low-entropy tokens yields 5.21 increase in the AIME24 score. (e) & (f) Qwen3-8B: Retaining only the top 20% forking tokens maintains performance. Additionally, using only the top 20% high-entropy tokens increases response length across all model sizes. 9 Figure 7: Comparison among DAPO using different range of tokens in policy gradient loss. Top x% means only using the x% of the tokens with highest entropy (x = 10, 20, 50), bottom 80% means only using the 80% of the tokens with lowest entropy, and 100% means using all tokens (i.e., vanilla DAPO). Furthermore, \"overall entropy\" refers to the average entropy over all tokens. 10 on AIME25. Similarly, the Qwen3-14B base model shows improvements of 5.21 points on AIME24 and 4.79 points on AIME25. For the Qwen3-8B base model, performance remains unaffected. These findings suggest that the gains in reasoning ability during RLVR are driven primarily by high-entropy tokens, while low-entropy tokens may have little effect on or could even hinder reasoning performance, particularly on the Qwen3-32B and Qwen3-14B base models. To conduct deeper analysis, we vary the proportion, denoted as ρ in Equation (6), for experiments, as shown in Figure 7(a). The results show that the performance of the Qwen3-8B base model remains relatively consistent across different proportions, such as 10%, 20%, and 50%. For the Qwen3-14B and Qwen3-32B base model, Figure 7(c) and (e) reveal that reducing ρ from 20% to 10% leads to slight drop in performance, while increasing it sharply to 100% results in notable decline. These observations indicate that within reasonable range, reasoning performance is largely insensitive to the exact value of ρ. More importantly, they suggest that focusing on high-entropy tokens, rather than using all tokens, generally preserves performance, and could even offer substantial gains in larger models. Low-entropy tokens contribute minimally to reasoning performance. As illustrated in Figure 7(a) and (b), retaining only the bottom 80% of tokens with low entropy during RLVR leads to substantial decline in performance, even though these tokens account for 80% of the total token count used in training. This finding indicates that low-entropy tokens contribute minimally to enhancing reasoning capabilities, highlighting the greater importance of high-entropy tokens for effective model training. The effectiveness of high-entropy tokens may lie in their ability to enhance exploration. Our analysis reveals that focusing on subset of high-entropy tokens, approximately 20% in our experiments, strikes an effective balance between exploration and training stability in RLVR. As illustrated in Figure 7(b), (d) and (f), adjusting the ratio ρ from 20% to either 10%, 50%, or 100% leads to persistently lower overall entropy starting from the early training phase and continuing up to the point where performance begins to converge. Moreover, training with the bottom 80% of low-entropy tokens results in significantly reduced overall entropy. These findings indicate that retaining certain proportion of high-entropy tokens may facilitate effective exploration. Tokens outside this range could be less helpful or possibly even detrimental to exploration, particularly during the critical phase before performance convergence. This might explain why, on the Qwen3-32B base model, DAPO using only the top 20% high-entropy tokens outperforms vanilla DAPO, as shown in Figure 6(a). However, on the Qwen3-8B base model, probably due to the models lower capacity, the benefits of enhanced exploration appear limited. Focusing on forking tokens in the policy gradient loss benefits larger reasoning models. We present the scaling trend when utilizing only forking tokens in Figure 8. On the AIME24 and AIME25 benchmarks, we observe that as the model size increases, the performance gain over vanilla DAPO becomes increasingly significant. This suggests promising conclusion: focusing solely on forking tokens in the policy gradient loss could offer greater advantages in larger reasoning models. Figure 8: Scaling trend of DAPO using only forking tokens (i.e., top 20% of high-entropy tokens) in policy gradient loss. These results suggest that concentrating exclusively on forking tokens in the policy gradient loss may yield greater benefits in larger reasoning models. 11 5.4 Analysis Generalization ability to other domains As outlined in Section 5.2, we used the DAPO-Math-17K dataset, which primarily consists of mathematical data, for our RLVR experiments. Here, we test whether DAPO, when trained on math dataset and using only small fraction of high-entropy tokens in the policy gradient loss, can still surpass vanilla DAPO on out-of-distribution test sets, such as LiveCodeBench (Jain et al., 2024). The results comparing DAPO with only top 10% or 20% tokens with highest entropy to vanilla DAPO (which uses 100% tokens) on the Qwen3-32B base are illustrated in Figure 9, using the same setup described in Section 5.2. From these results, we observe that even when retaining only top 10% or 20% tokens with highest entropy, DAPO still significantly outperforms vanilla DAPO on the out-of-distribution test dataset LiveCodeBench. This finding suggests that high-entropy tokens may be associated with the generalization capabilities of reasoning models. Retaining only small subset of tokens with the highest entropy could potentially enhance the generalization ability of reasoning models. Figure 9: Comparison among DAPO using different range of tokens in policy gradient loss trained from the Qwen3-32B base model on the out-of-distribution LiveCodeBench Benchmark (Jain et al., 2024) (v5, Aug. 2024 to Feb. 2025). Top x% means only using the x% of the tokens with highest entropy (x = 10, 20), and 100% means using all tokens (i.e. vanilla DAPO). Due to the high variance of the accuracy curves, we smooth the curves using window smoothing with window size of 10. Unlocking more potential of RLVR with only forking tokens In the experiments described in Section 5.3, we set maximum response length of 20480. As shown in Figure 10, we increased the maximum response length for DAPO w/ only forking tokens (depicted in Figure 6(a) and (b))trained from the Qwen3-32B base modelto 29696. This adjustment resulted in an improvement of the already SoTA performance on AIME24, increasing from 63.54 to 68.12. These findings suggest that the full potential of our approach may not yet be realized, and with longer context length or potentially more challenging training data, even greater performance gains could be achieved. Results on models other than Qwen We compare DAPO using only forking tokens (i.e., the top 20% tokens with the highest entropy) against vanilla DAPO on models other than the Qwen series, specifically the Llama-3.1-8B model. When DAPO is applied to the Llama-3.1-8B base model, we observe that it achieves very low accuracy (approximately 1%) on the training dataset (i.e., DAPO-MATH-17K (Yu et al., 2025)) and often generates responses with repetitive words early in the RL training process. To address this, we use the Qwen3-32B model (Yang et al., 2025) as teacher to generate responses for DAPO-MATH17K queries. From the generated queries, we randomly sample 10,000 with correct answers to serve as cold-start data and perform supervised fine-tuning (SFT) on the Llama-3.1-8B base model (Grattafiori et al., 2024). The remaining 7,398 queries are reserved for RL after the cold-start phase. The AIME24 score and response length during RL training are plotted in Figure 11. The results indicate that DAPO with only forking tokens still surpasses vanilla DAPO, while also producing longer responses on average. However, given the relatively low performance of both configurations on AIME24, we believe the results on Llama-3.1-8B are less convincing compared to those observed on the Qwen3 models. 12 Figure 10: By extending the maximum response length from 20,480 to 29,696 and continuing training from the SoTA 32B model shown in Figure 6(a) and (b), the AIME24 scores improve further from 63.54 to 68.12, alongside notable increase in response length. Figure 11: Comparison of DAPO using only forking tokens and vanilla DAPO, both trained from Llama3.1-8B after cold-start."
        },
        {
            "title": "6 Discussions",
            "content": "Discussion 1: High-entropy minority tokens (i.e., forking tokens) could play key role in explaining why RL generalizes while SFT memorizes. Chu et al. (2025) demonstrated empirically that RL, particularly with outcome-based rewards, exhibits strong generalization to unseen, rule-based tasks, whereas supervised fine-tuning (SFT) is prone to memorizing training data and struggles with generalization outside the training distribution. We hypothesize that one critical factor underlying the differing generalization capabilities of RL and SFT may be related to entropy in forking tokens. Our experiments (e.g., Figure 5 and Figure 7) suggest that RL tends to preserve or even increase the entropy of forking tokens, maintaining the flexibility of reasoning paths. In contrast, SFT pushes output logits towards one-hot distributions, leading to reduced entropy in forking tokens and, consequently, loss of reasoning path flexibility. This flexibility may be crucial determinant of reasoning models ability to generalize effectively to unseen tasks. Discussion 2: Unlike traditional RL, LLM reasoning integrates prior knowledge and must produce readable output. Consequently, LLM CoTs contain mix of low-entropy majority tokens and highentropy minority tokens, whereas traditional RL can assume uniform action entropy throughout 13 trajectory. As shown in Figure 2(a), most LLM CoT tokens have low entropy, with only small fraction exhibiting high entropy. In contrast, traditional RL typically formulates each action distribution as Gaussian with predefined standard deviation (Schulman et al., 2017; Raffin et al., 2021; Weng et al., 2022), resulting in uniform entropy across actions. We attribute this distinct entropy pattern in LLM CoTs to their pretraining on large-scale prior knowledge and the need for language fluency. This forces most tokens to align with memorized linguistic structures, yielding low entropy. Only small set of tokens that are inherently uncertain in the pretraining corpus allows for exploration, and thus exhibits high entropy. This deduction is consistent with our results in Table 1. Discussion 3: In RLVR, entropy bonus may be suboptimal, as it increases the entropy of low-entropy majority tokens. In contrast, clip-higher effectively promotes entropy in high-entropy minority tokens. In RL, entropy bonus is commonly added to the training loss to encourage exploration by increasing the entropy of actionsa well-established practice in traditional tasks (Schulman et al., 2017; Williams, 1992; Mnih et al., 2016), and recently applied to LLM reasoning (Sheng et al., 2024; Hu et al., 2024). However, as discussed above, unlike typical RL trajectories, LLM CoTs display distinct entropy patterns. Increasing entropy across all tokens can degrade performance by disrupting the low-entropy majority, while selectively increasing the entropy of high-entropy minority tokens improves performance (Figure 3). Thus, uniformly applied entropy bonuses are suboptimal for CoT reasoning. Instead, clip-higher (Yu et al., 2025), which moderately raises ϵhigh in Equation (4), better targets high-entropy tokens. Empirically, we observe that tokens with high importance ratios rt(θ) (as defined in Equation (2)) tend to have higher entropy. By including more of these tokens in training, clip-higher increases overall entropy without significantly affecting low-entropy tokens, as supported by Yu et al. (2025) and illustrated in Figure 5."
        },
        {
            "title": "7 Related Work",
            "content": "Reinforcement learning for LLM Before the advent of reasoning-capable models like OpenAIs o1 (OpenAI, 2024), reinforcement learning (RL) was widely used in reinforcement learning from human feedback (RLHF) to improve large language models (LLMs) instruction-following and alignment with human preferences (Ouyang et al., 2022). RLHF methods are broadly categorized into online and offline preference optimization. Online methods, such as PPO (Schulman et al., 2017), GRPO (Shao et al., 2024), and REINFORCE (Williams, 1992), generate responses during training and receive real-time feedback. Offline methods like DPO (Rafailov et al., 2023), SimPO (Meng et al., 2024), and KTO (Ethayarajh et al., 2024) optimize policies using pre-collected preferences, typically from human annotators or LLMs. While offline methods are more training-efficient, they often underperform compared to online approaches (Tang et al., 2024). Recently, RL with verifiable rewards (RLVR)(Lambert et al., 2025) has emerged as promising approach for enhancing reasoning in LLMs, particularly in domains like mathematics and programming(Shao et al., 2024; DeepSeek-AI et al., 2025; Yang et al., 2025; Lambert et al., 2025). OpenAI o1 (OpenAI, 2024) was the first to show that RL can effectively incentivize reasoning at scale. Building on o1, models such as DeepSeek R1 (DeepSeek-AI et al., 2025), QwQ (Team, 2025), Kimi k1.5 (Team et al., 2025), and Qwen3 (Yang et al., 2025) have aimed to match or exceed its performance. DeepSeek R1 stands out for showing that strong reasoning can emerge through outcome-based optimization using the online RL algorithm GRPO (Shao et al., 2024). It also introduced zero RL paradigm, where reasoning abilities are elicited from the base model without conventional RL fine-tuning. Inspired by these results, subsequent methods such as DAPO (Yu et al., 2025), VAPO (Yue et al., 2025b), SimpleRLZoo (Zeng et al., 2025), and Open-Reasoner-Zero (Hu et al., 2025) have further explored RL-based reasoning. In this work, we use DAPO as our baseline to investigate key aspects of RL applied to LLMs. Analysis on reinforcement learning with verifiable rewards Recently, Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as prevalent approach to enhance the reasoning capabilities of large language models (LLMs). Several studies have analyzed the characteristics of RLVR and its related concepts. Gandhi et al. (2025) find that the presence of reasoning behaviors, rather than the correctness of answers, is the key factor driving performance improvements in reinforcement learning. Similarly, Li et al. (2025) show that the structure of long chains of thought (CoT) is critical to the learning process, while the content of individual reasoning steps has minimal impact. Vassoyan et al. (2025) identify \"critical tokens\" in CoTs, which are decision points where models are prone to errors, and propose encouraging exploration around these tokens by modifying the KL penalty. Lin et al. (2024) also identify critical tokens that significantly influence incorrect outcomes and demonstrate that identifying and replacing these tokens can alter model behavior. Our finding that RLVR primarily focuses on forking tokens in reasoning paths may share some common ground with the observations of Gandhi et al. (2025) and Li et al. (2025), who suggest that RLVR primarily learns the format rather than the content. However, our analysis goes further by identifying the finding at the token level. Moreover, the concept of critical tokens in Vassoyan et al. (2025) and Lin et al. (2024) is closely related to the high-entropy minority tokens we introduce. In 14 contrast to prior work, which judges token importance based on correctness of the output, we propose token entropy as criterion that may more accurately reflect the underlying mechanisms of LLMs."
        },
        {
            "title": "8 Conclusion",
            "content": "In this work, we analyze RLVR through novel perspective of token entropy, providing fresh insights into the mechanisms of reasoning in LLMs. Our study of CoT reasoning shows that only small subset of tokens exhibit high entropy and serve as forks in reasoning paths that influence reasoning directions. Additionally, our analysis of entropy dynamics during RLVR training reveals that the reasoning model largely retains the base models entropy patterns, with RLVR mainly modifying the entropy of already high-entropy tokens. Building on these findings, which underscore the significance of high-entropy minority tokens, we restrict policy gradient updates in RLVR to the top 20% highest-entropy tokens. This approach achieves performance comparable to, or even surpassing, full-token RLVR training, while exhibiting strong scaling trend with model size. In contrast, directing optimization toward the lowentropy majority results in significant decline in performance. These findings indicate that RLVRs effectiveness stems primarily from optimizing this high-entropy subset, suggesting more focused and efficient strategies for improving LLM reasoning capabilities. Limitations We believe there is still room for improvement in our work. First, our experiments could be extended to models beyond the Qwen family. Although we attempted to evaluate our approach on LLaMA models, they struggled to achieve meaningful performance on the AIME benchmarks. Additionally, the scope of our dataset could be expanded to encompass domains beyond mathematics, such as programming or more complex tasks like ARC-AGI (Chollet et al., 2025; Chollet, 2019). Furthermore, our findings are based on specific experimental settings, and it is possible that the observations and conclusions presented in this paper may not generalize to all RLVR scenarios. For instance, in different RLVR setting, the effective proportion of 20% observed in our experiments may need to be adjusted to different value to achieve optimal results. Future Directions Future directions involve developing new RLVR algorithms to better leverage highentropy minority tokens and exploring how these insights can enhance not only RLVR but also other approaches, such as supervised fine-tuning (SFT), distillation, inference, and multi-modal training. 15 References Anthropic. Claude 3.7 Sonnet. https://www.anthropic.com/claude/sonnet, 2025. [Accessed 01-05-2025]. Francois Chollet, Mike Knoop, Gregory Kamradt, Bryan Landers, and Henry Pinkard. Arc-agi-2: new challenge for frontier ai reasoning systems. arXiv preprint arXiv: 2505.11831, 2025. François Chollet. On the measure of intelligence. arXiv preprint arXiv: 1911.01547, 2019. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024. Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307, 2025. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas 16 Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519, 2025. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. OlympiadBench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, 2024. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, 2021. Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2025. Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Eric Tang, Sumanth Hegde, Kourosh Hakhamaneshi, Shishir Patil, Matei Zaharia, et al. Llms can easily learn to reason from demonstrations structure, not content, is what matters! arXiv preprint arXiv:2502.07374, 2025. Zicheng Lin, Tian Liang, Jiahao Xu, Xing Wang, Ruilin Luo, Chufan Shi, Siheng Li, Yujiu Yang, and Zhaopeng Tu. Critical tokens matter: Token-level contrastive estimation enhence llms reasoning capability. arXiv preprint arXiv:2411.19943, 2024. Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025. Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with reference-free reward. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 124198124235. Curran Associates, Inc., 2024. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Proceedings of The 33rd International Conference on Machine Learning, 2016. OpenAI. Learning to Reason with LLMs. https://openai.com/index/learning-to-reason-with-llms/, 2024. [Accessed 01-05-2025]. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Jiazhen Pan, Che Liu, Junde Wu, Fenglin Liu, Jiayuan Zhu, Hongwei Bran Li, Chen Chen, Cheng Ouyang, and Daniel Rueckert. Medvlm-r1: Incentivizing medical reasoning capability of vision-language models (vlms) via reinforcement learning, 2025. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stablebaselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 2021. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Yunhao Tang, Daniel Zhaohan Guo, Zeyu Zheng, Daniele Calandriello, Yuan Cao, Eugene Tarassov, Rémi Munos, Bernardo Ávila Pires, Michal Valko, Yong Cheng, et al. Understanding the performance gap between online and offline alignment algorithms. arXiv preprint arXiv:2405.08448, 2024. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, and Zonghan Yang. Kimi k1.5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. 18 Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm.github. io/blog/qwq-32b/. Jean Vassoyan, Nathanaël Beau, and Roman Plaud. Ignore the kl penalty! boosting exploration on critical tokens to enhance rl fine-tuning. arXiv preprint arXiv:2502.06533, 2025. Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang Wang, Simon Shaolei Du, and Yelong Shen. Reinforcement learning for reasoning in large language models with one training example. arXiv preprint arXiv:2504.20571, 2025. Jiayi Weng, Huayu Chen, Dong Yan, Kaichao You, Alexis Duburcq, Minghao Zhang, Yi Su, Hang Su, and Jun Zhu. Tianshou: highly modularized deep reinforcement learning library. Journal of Machine Learning Research, 2022. Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 1992. Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, and Ping Luo. Dancegrpo: Unleashing grpo on visual generation. arXiv preprint arXiv:2505.07818, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Weinan Dai, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, and Gao Huang. Does arXiv preprint reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv:2504.13837, 2025a. Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, Xiangyu Yu, Gaohong Liu, Juncai Liu, Lingjun Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Ru Zhang, Xin Liu, Mingxuan Wang, Yonghui Wu, and Lin Yan. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025b. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Yang Yue, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, and Gao Huang. Absolute zero: Reinforced self-play reasoning with zero data. arXiv preprint arXiv:2505.03335, 2025. 19 Figure 12: Visualization of token entropy (part 1). Figure 13: Visualization of token entropy (part 2). 21 Figure 14: Visualization of token entropy (part 3). 22 Figure 15: Visualization of token entropy (Part 4). For brevity, we omit the CoT following Figure 14 and preceding Figure 15. Figure 16: Visualization of token entropy (part 5). 24 Figure 17: Visualization of token entropy (part 6)."
        }
    ],
    "affiliations": [
        "LeapLab, Tsinghua University",
        "Qwen Team, Alibaba Inc."
    ]
}