{
    "paper_title": "SemiEvol: Semi-supervised Fine-tuning for LLM Adaptation",
    "authors": [
        "Junyu Luo",
        "Xiao Luo",
        "Xiusi Chen",
        "Zhiping Xiao",
        "Wei Ju",
        "Ming Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Supervised fine-tuning (SFT) is crucial in adapting large language models (LLMs) to a specific domain or task. However, only a limited amount of labeled data is available in practical applications, which poses a severe challenge for SFT in yielding satisfactory results. Therefore, a data-efficient framework that can fully exploit labeled and unlabeled data for LLM fine-tuning is highly anticipated. Towards this end, we introduce a semi-supervised fine-tuning framework named SemiEvol for LLM adaptation from a propagate-and-select manner. For knowledge propagation, SemiEvol adopts a bi-level approach, propagating knowledge from labeled data to unlabeled data through both in-weight and in-context methods. For knowledge selection, SemiEvol incorporates a collaborative learning mechanism, selecting higher-quality pseudo-response samples. We conducted experiments using GPT-4o-mini and Llama-3.1 on seven general or domain-specific datasets, demonstrating significant improvements in model performance on target data. Furthermore, we compared SemiEvol with SFT and self-evolution methods, highlighting its practicality in hybrid data scenarios."
        },
        {
            "title": "Start",
            "content": "SEMIEVOL: Semi-supervised Fine-tuning for LLM Adaptation Junyu Luo, Xiao Luo, Xiusi Chen, Zhiping Xiao, Wei Ju, Ming Zhang Peking University University of California, Los Angeles University of Illinois Urbana-Champaign University of Washington luojunyu@stu.pku.edu.cn, xiaoluo@cs.ucla.edu, xchen@cs.ucla.edu patxiao@uw.edu, {juwei, mzhang_cs}@pku.edu.cn 4 2 0 2 7 1 ] . [ 1 5 4 7 4 1 . 0 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Supervised fine-tuning (SFT) is crucial in adapting large language models (LLMs) to specific domain or task. However, only limited amount of labeled data is available in practical applications, which poses severe challenge for SFT in yielding satisfactory results. Therefore, data-efficient framework that can fully exploit labeled and unlabeled data for LLM fine-tuning is highly anticipated. Towards this end, we introduce semi-supervised finetuning framework named SEMIEVOL for LLM adaptation from propagate-and-select manner. For knowledge propagation, SEMIEVOL adopts bi-level approach, propagating knowledge from labeled data to unlabeled data through both in-weight and in-context methods. For knowledge selection, SEMIEVOL incorporates collaborative learning mechanism, selecting higher-quality pseudo-response samples. We conducted experiments using GPT-4o-mini and Llama-3.1 on seven general or domain-specific datasets, demonstrating significant improvements in model performance on target data. Furthermore, we compared SEMIEVOL with SFT and self-evolution methods, highlighting its practicality in hybrid data scenarios."
        },
        {
            "title": "Introduction",
            "content": "Supervised fine-tuning (SFT) is crucial method for enhancing large language models (LLMs) performance on instructional or domain-specific tasks (Raffel et al., 2020; Chung et al., 2024), playing vital role in adapting LLMs for specific scenarios. However, SFT relies on substantial amount of annotated labeled data, which can be increasingly costly in real-world applications (Honovich et al., 2023; Kung et al., 2023). While existing LLMs often employ unsupervised pretraining methods (Devlin, 2018; Radford et al., 2019; 1GitHub repository: https://github.com/luo-junyu/ SemiEvol. Figure 1: Comparison of SEMIEVOL with previous SFT methods. SEMIEVOL enables interaction between diverse data types for superior performance evolution. Brown, 2020) to improve their capabilities, this approach typically requires vast datasets and substantial computational resources, making it impractical for scenarios with limited accessible samples. In practice, however, it often presents hybrid situation, where small amount of labeled data coexists with relatively larger volume of unlabeled data. On the one hand, when deploying LLMs to new target tasks, limited amount of task-specific annotations can be valuable without incurring excessive costs (Perlitz et al., 2023; Kung et al., 2023). On the other hand, during the continuous inference process of LLMs, substantial amount of unlabeled data accumulates (Tao et al., 2024; Honovich et al., 2023; Wang et al., 2023b). Effectively leveraging the labeled data to enhance model performance on unlabeled data, while simultaneously selecting high-quality unlabeled samples, can improve LLMs performance in target scenarios, offering substantial practical utility. Therefore, we aim to address the following question: Can LLMs evolve in real-world scenario of limited labeled data and abundant unlabeled data? Designing an evolution framework for hybrid-data scenarios is non-trivial due to the following reasons: First, semi-supervised learning (Kipf and Welling, 2016; Shi et al., 2023), which has been widely studied in machine learning, primarily focuses on classification tasks. When considering generative tasks, the previous techniques such as pseudo-labeling (Sohn et al., 2020) and contrastive learning (He et al., 2020), cannot be directly applied to LLM use cases, like reasoning and planning (Chen et al., 2022; Hendrycks et al., 2020). Second, previous SFT and unsupervised pretraining methods typically deal with single type of data (either labeled or unlabeled) (Zhang et al., 2023). Under hybrid-data circumstances, effectively maximizing their combined potential for model improvement becomes challenging. In this work, we introduce SEMIEVOL for improving LLM reasoning in hybrid-data scenarios, as illustrated in Figure 1. SEMIEVOL employs bi-level strategy for knowledge propagation-andselection. For knowledge propagation, SEMIEVOL enhances LLMs inference performance using labeled data through both in-weight and in-context scopes. During in-weight propagation, SEMIEVOL uses labeled data to adapt the model. During in-context propagation, SEMIEVOL employs knearest neighbor retrieval in latent space to assist prediction. Moreover, SEMIEVOL introduces bilevel approach for data selection and generating pseudo-responses. First, it introduces collaborative learning framework, utilizing multiple LLMs with different configurations for inference and selfjustification of responses, yielding more accurate predictions. Second, SEMIEVOL adaptively selects unlabeled data by confidence based on response entropy. By mining on unlabeled data leveraging labeled data, we obtain high-quality pseudoresponses. Using these pseudo-response data, the model enhances its performance on target tasks. We conducted tests on seven general or domainspecific datasets (e.g., MMLU, MMLU-Pro and ConvFinQA), covering tasks such as questionanswering, reasoning, and numerical computation. We compared SEMIEVOL with popular methods like retrieval augmented generation, self-evolution and SFT, demonstrating SEMIEVOLs consistent effectiveness across various scenarios. We summarize the contributions as follows: To the best of our knowledge, we are the first to study practical problem of semi-supervised fine-tuning, aiming to adapt LLMs into different domains data-efficiently. We introduce SEMIEVOL, unified framework for knowledge propagation-and-selection that effectively combines labeled and unlabeled data for model evolution. We demonstrate the consistent effectiveness of SEMIEVOL across seven widely used general or domain-specific generative tasks in comparison to extensive baseline models."
        },
        {
            "title": "2 Challenges for Real-world LLM",
            "content": "Fine-tuning"
        },
        {
            "title": "2.1 Supervised Fine-tuning",
            "content": "Supervised fine-tuning (SFT) aims to adapt Large Language Models (LLMs) to domain-specific scenarios. Given an LLM and dataset Dlabeled = {Ti, Yi}N i=1, where Ti represents the input task or context and Yi denotes the corresponding expected response. The model minimizes the loss function for each token of the anticipated output during the fine-tuning process . Challenge: Annotation Cost. Despite the effectiveness of supervised fine-tuning, it would require expensive labeling costs to access abundant labeled data. An economic solution is to utilize easily accessible unlabeled data without feedback as supplement for fine-tuning."
        },
        {
            "title": "2.2 Background and Problem Definition:",
            "content": "Semi-supervised Fine-tuning In real-world scenarios, it is more common to have access to both small amount of labeled data Dlabeled and larger volume of unlabeled data Dunlabeled = {Ti}M i=1. Labeled data offers higher confidence, while unlabeled data represents broader sample distribution. In this paper, we propose SEMIEVOL approach, which primarily focuses on how to leverage both types of data Dsemi = Dlabeled Dunlabeled to optimize the LLM M. Our SEMIEVOL not only improves model performance but also offers greater practical value. Challenge: Generative Task. In fact, developing semi-supervised fine-tuning framework is highly challenging. Tradition semi-supervised approaches usually focus on classification problems solved by pseudo-labeling while our problem is generative task, which requires us to generate expected responses instead."
        },
        {
            "title": "3.1 Overview",
            "content": "In this paper, we develop SEMIEVOL to integrate labeled and unlabeled data for improving LLM performance in reasoning. The core idea of SEMIEVOL is to leverage labeled data through Figure 2: Overview of SEMIEVOL. It maximizes the utility of labeled data through bi-level knowledge propagationand-selection framework, while leveraging collaborative learning among multiple LLMs to exploit unlabeled data, thereby unleashing the full data potential. bi-level propagation-and-select process. As illustrated in Figure 2, SEMIEVOL is featured by three key components: (1) Knowledge Propagation: We utilize labeled data to enhance model Ms performance on unlabeled data. This process focuses on two aspects, i.e., model weights and context. The propagation process involves model adaptation using labeled data and providing the most relevant references from the latent space to assist model inference. (2) Collaborative Learning: We employ multiple LLMs with different configurations as mutual teachers to infer unlabeled data. We pay particular attention to inconsistent responses, using the models to self-justify these discrepancies. (3) Knowledge Self-selection: We design the adaptive selection for unlabeled data and pseudo-responses. Using labeled data as guide, we identify the most valuable unlabeled data for learning. By optimizing LLMs on these selected data samples, the model achieves superior evolution performance. In summary, SEMIEVOL addresses the prevalent real-world scenario where both labeled and unlabeled data coexist. By leveraging the labeled data and the capabilities of LLMs themselves, we perform knowledge propagation, mining, and selection on unlabeled data. This strategy improves model performance in the target scenarios."
        },
        {
            "title": "3.2 Knowledge Propagation",
            "content": "Labeled data contain expected target responses, while unlabeled data represents broader task distribution. To leverage this, we aim to propagate knowledge from labeled to unlabeled data, enabling the model to effectively utilize and learn from unlabeled instances. We design bi-level knowledge propagation framework that operates simultaneously on two fronts: in-weight and in-context. For in-weight propagation, we initially warm up the base model Mbase on labeled data Dlabeled to enhance its predictive capabilities for the target task. Specifically, we fine-tune the model, leveraging task data and target responses to obtain preliminary adapted model (Mwarm). This process is formulated as: Mwarm = (Mbase, Dlabeled) , (1) where is the fine-tuning process. For in-context propagation, we first embed labeled dataset into latent space using an embedding function ϵ(): Elabeled = {ϵ (ti) (ti, yi) Dlabeled} . (2) During inference on unlabeled data, for each task tj Dunlabeled, we retrieve the nearest labeled instances in the embedding space: (tj) = (Elabeled, ϵ (tj) , k) , (3) where is set to 3, is the nearest neighbors search. We use (tj) as context to improve the inference on the unlabeled data. In summary, labeled data facilitates knowledge propagation to unlabeled data through both inweight and in-context manners. In practice, we first adapt the model to obtain the warm-up LLM Mwarm, then utilize labeled data as context to enhance inference on unlabeled instances."
        },
        {
            "title": "3.3 Collaborative Learning",
            "content": "To further exploit unlabeled data, we designed collaborative learning framework tailored for LLMs. This framework utilizes the inherent capabilities of LLMs for self-justify to obtain high-confidence pseudo-responses from unlabeled data. Some concurrent works also attempt to use LLMs for similar functionality (Wang et al., 2024a), while their focus differs from ours. Initially, we employ set of LLMs, denoted as M1, M2, , Mn to perform inference on the unlabeled dataset Dunlabeled, where is 4 by default and will be discussed in Section 4.3.2. Each model is configured with different inference contexts and settings, providing diverse perspectives and yielding more comprehensive results. For each unlabeled sample tj Dunlabeled, we obtain multiple predictions: (cid:8)ym (cid:9) = {Mm (tj)}n m=1 . (4) Subsequently, we implement self-justification process using LLMs. This step synthesizes the inferences from various models to select and summarize the most accurate response ˆyj : (cid:17) (cid:16)(cid:8)ym (cid:9)n m=1 . (5) yj = Self-Justify where the Self-Justify operator is implemented via prompting Mwarm by natural language instructions. In summary, our LLM-specific collaborative learning framework harnesses multiple differently configured LLMs for multi-perspective inference. By utilizing the LLMs inherent abilities to selfjustify, we effectively mine unlabeled data, and generate high-confident pseudo-responses."
        },
        {
            "title": "3.4 Knowledge Adaptive Selection",
            "content": "While the pseudo-responses yj generated through the collaborative learning framework enrich the training data, they may still contain noise or low-quality information that could misguide the models learning. To address this issue, we design an adaptive data selection approach within the SEMIEVOL framework. Specifically, we measure the confidence of the responses yj for the unlabeled data selection. We use the entropy of the LLMs responses to measure the models confidence in the answers. Since LLMs generate responses token by token, we calculate the per-token negative log-likelihood, which serves as an approximation of the entropy. For each data sample tj Dunlabeled, the entropy (yj) is computed on pseudo-response yj after Eq. 5 as: (yj) ="
        },
        {
            "title": "1\nLj",
            "content": "Lj (cid:88) k=1 (cid:16) log tj, r<k rk (cid:17) , (6) where Lj is the length of the response rj generated by Mwarm, rk is the k-th token in the response, (cid:110) (cid:111) r<k = r1 , r2 , , rk (cid:16) tj, r<k rk of yj, and probability of token rk are the preceding tokens (cid:17) is Mwarms predicted at position k. For the unlabeled data, we compute the entropy (yj) for each pseudo-response yj corresponding to task tj Dunlabeled. We then use the θ percentile of the entropy values from the labeled data to establish dynamic threshold τ : τ = Percentileθ (cid:16) {H (yj)}M j=1 (cid:17) , (7) where is the amount of unlabeled samples, and θ is default to 50% and will be investigated in Section 4.3.2. Using this dynamic threshold, we select confident samples from the unlabeled data. In formula, Dselected = {(tj, yj) (yj) τ } . (8) We filter the pseudo-responses obtained previously, resulting in the refined dataset Dselected. Finally, we use the high-quality pseudoresponses to fine-tune the model, which can enhance its performance and adaptability on the target task: Mevol = (Mwarm, Dselected) , (9) where Mwarm is the model obtained after initial fine-tuning in Eq. 1, and denotes the finetuning process. By focusing on these high-quality assessed data, we enhance the models performance and adaptability on the target task while reducing the influence of noisy or erroneous information."
        },
        {
            "title": "3.5 Summary",
            "content": "SEMIEVOL enhances the performance and adaptability of LLMs in target tasks through two-stage knowledge mining process, combining labeled and unlabeled data for model evolution. Firstly, we leverage small amount of labeled data to enhance knowledge propagation across unlabeled data. Secondly, we employ knowledge mining and adaptive selection. This strategy effectively integrates both labeled and unlabeled data, culminating in the evolved model Mevol."
        },
        {
            "title": "4.1.1 Datasets\nWe employed both general-purpose and domain-\nspecific evaluation datasets to provide a compre-\nhensive assessment. These datasets encompass a",
            "content": "Model and Strategy MMLU MMLU Pro ARC FPB USMLE PubMedQA ConvFinQA GPT-4o-mini Vanilla GPT-4o-mini SFT GPT-4o-mini SEMIEVOL Error Reduction Llama3.1-8B Vanilla Llama3.1-8B SFT AdaptLLM InstructPT MemoryLLM RAG (BM25) RAG (FAISS) Hermes-3 Reflection-Llama Llama3.1-8B SEMIEVOL Error Reduction 77.4 77.8 79. 57.8 58.8 60.8 91.5 93.4 90. 98.0 92.7 98.9 73.8 75.0 77. 77.5 77.5 79.5 63.9 88.8 89. 11.1% 7.11% 14.1% 83.3% 13.0% 8.89% 70.1% 66. 67.9 56.4 66.6 66.5 63.6 65.5 70.3 47.1 49.8 31.8 37.4 38.8 37.9 37. 54.3 81.1 81.7 81.8 56.3 80.8 81.3 74.9 82.2 83. 96.2 49.7 76.1 57.7 83.7 82.5 73.9 80.8 96.9 70.2 70.8 31.5 47.4 37.8 69.3 69.1 54.5 67. 71.6 73.5 75.0 27.6 44.5 55.5 69.0 71.5 68.5 77.5 76.0 51. 81.3 30.9 55.2 37.2 63.4 64.6 54.9 40.8 83.6 11.6% 13.6% 16.9% 81.4% 4.70% 9.43% 66.5% Table 1: Performance comparison across different models on various datasets. variety of tasks, including multiple-choice questions, reasoning, numerical computations, etc.. Specifically, our general evaluation datasets include MMLU (Hendrycks et al., 2020), MMLUPro (Wang et al., 2024c), and ARC (Clark et al., 2018), while domain-specific datasets comprise FPB (Malo et al., 2014), USMLE (Jin et al., 2021), PubMedQA (Jin et al., 2019), and ConvFinQA (Chen et al., 2022), covering various fields such as finance and healthcare. This diverse selection enables thorough evaluation of the models performance across different task types and knowledge domains."
        },
        {
            "title": "4.1.2 Backbones and Baselines",
            "content": "Base Models. To demonstrate the generalization capability of SEMIEVOL, we employed diverse range of leading models, encompassing both commercial and open-source and LLMs, including GPT-4o-mini and Llama-3.1-8B (Dubey et al., 2024). Baselines. We evaluated our method against (1) Vanilla, baselines from several categories: which involves testing solely through API calls or using the original model; (2) Supervised Finetuning (SFT) (Hu et al., 2021; Wei et al., 2021), which adapts the model to the target task using the labeled data; (3) Self-Evolution Methods (SelfEvol), which enhance LLM capabilities using additional unlabeled data. We compare with ReflectionLlama (Li et al., 2024)2 and Hermes-3 (Teknium et al., 2024)3, both of which evolve from the Llama3.1-8B model; (4) Domain Adaptation Methods, including AdaptLLM (Cheng et al., 2024b) and InstructPT (Cheng et al., 2024a), utilize domainspecific data (e.g., finance and medical). We select models adapted to corresponding domains for testing, all with comparable parameter counts of 8B; (5) Inference-time enhancement methods, such as Retrieval Augmented Generation (RAG) (Lewis et al., 2020), including BM25 (Jones et al., 2000) and FAISS (Douze et al., 2024) algorithms. We also compare with MemoryLLM (Wang et al., 2024b), with the nearest labeled sample as memory; This comprehensive comparison allows us to assess the effectiveness of our proposed method across various state-of-the-art approaches in LLM fine-tuning and adaptation. 4.1."
        },
        {
            "title": "Implementation Details",
            "content": "For the setting of semi-supervised fine-tuning of LLMs, we have Dlabeled, Dunlabeled and Dtest. The data proportion in our experiments is labeled : unlabeled : test = 2 : 6 : 2 and will be further discussed in Section 4.3.6. The answer information for Dunlabeled is inaccessible in our setting. We fine-tuned Llama-3.1-8B using Low-Rank Adaptation (LoRA) (Hu et al., 2021) and applied fine2https://huggingface.co/Solshine/reflection-llama-3.1-8B 3https://huggingface.co/NousResearch/Hermes-3-Llama3.1-8B tuning with the official API for GPT-4o-mini4. All fine-tuning processes take 2 epochs. is set to 4 and θ is set to 50%, with further investigation planned in subsequent experiments. Detailed training configurations are provided in the Appendix. We evaluated all methods using the test sets Dtest. Model inference followed default settings for each approach. Codes are available in our GitHub repository5."
        },
        {
            "title": "4.2 Main Result",
            "content": "We present the main results of SEMIEVOL in Table 1. We can draw the following insights. Firstly, the tasks are generally challenging. Off-the-shelf LLMs perform poorly on these tasks, highlighting the necessity of leveraging scenario data to enhance model performance. Secondly, SEMIEVOL consistently improves both commercial and opensource models. Notably, SEMIEVOL is one of the few approaches that demonstrably enhances state-of-the-art commercial models, underscoring its practical value. Thirdly, SFT yield modest improvements, demonstrating the effectiveness of labeled data. Given the high cost of data labeling, SEMIEVOL effectively utilizes unlabeled data to complement this approach. Fourthly, the selfevolution method fails to achieve consistent improvements, showing limited improvement or even adverse effects on most datasets. Fifthly, adaptive fine-tuning methods can enhance performance only on specific tasks (e.g., ConvFinQA). Also, these methods may compromise the models instruction-following ability, leading to significant performance drops in some tasks (e.g., USMLE and PubMedQA). Lastly, SEMIEVOL consistently outperforms SFT methods, which demonstrates the effectiveness of incorporating unsupervised data and leveraging labeled data to fully utilize unsupervised data. Even when base models perform poorly (e.g., MMLU-Pro and ConvFinQA), SEMIEVOL can still achieve substantial improvements in model performance."
        },
        {
            "title": "4.3.1 Ablation Study",
            "content": "To evaluate the effectiveness of different components, we conducted an ablation analysis on SEMIEVOL, with results presented in Table 2. The findings reveal several key insights: (1) The full 4https://platform.openai.com/finetune. 5https://github.com/luo-junyu/SemiEvol Variant MMLU MMLU-Pro ARC Llama3.1-8B SEMIEVOL w/o IWP w/o ICP w/o CL w/o AS 70.3 68.7 69.7 69.1 69. 54.3 52.1 53.2 53.0 53.5 83.4 82.4 83.0 82.4 82.1 Table 2: Ablation study via performance comparison of different variants on SEMIEVOL. Figure 3: Sensitivity analysis of SEMIEVOLs performance under different and θ on variant datasets. model consistently outperforms all other configurations across the three datasets, demonstrating its comprehensive effectiveness. (2) In terms of knowledge propagation, both In-weight Propagation (IWP) and In-context Propagation (ICP) contribute significantly to the transfer of knowledge from labeled to unlabeled data and subsequent model evolution. In-weight Propagation, in particular, shows more pronounced impact. (3) Removing Collaborative Learning (CL) negatively affects model performance. This suggests that Collaborative Learning effectively leverages predictions from multiple LLMs to autonomously identify more accurate answers, thereby enhancing the prediction quality on unlabeled data. (4) The absence of Adaptive Selection (AS) also leads to decreased model performance. This indicates that AS successfully selects more confident samples, thus improving the accuracy of unlabeled data and enhancing the models evolutionary process."
        },
        {
            "title": "4.3.2 Sensitivity Analysis",
            "content": "We analyze the number of collaborating models (n) and the data selection ratio (θ), with results illustrated in Figure 3. From the results, we have the following observations. (1) Our method demonstrates robust performance across various settings, indicating low sensitivity to these parameters. (2) Model accuracy generally increases with n, as Figure 4: Entropy distribution indicates SEMIEVOL can enhanced response confidence. Lower entropy values indicate more confident predictions. Figure 5: Stability analysis via mean performance and standard deviation across multiple inference prompts. more collaborating LLMs enhance prediction accuracy. However, this also introduces additional computational overhead. We chose = 4 as the default. (3) Accuracy initially increases with θ but subsequently decreases, suggesting that introducing excessively noisy data is detrimental to model evolution. Consequently, we empirically set θ = 50% as the default value. It is noteworthy that we did not conduct extensive hyperparameter searches, as our primary focus was on validating the overall frameworks effectiveness."
        },
        {
            "title": "4.3.3 Response Entropy Analysis",
            "content": "We present the entropy distribution of different methods on the test set, as illustrated in Figure 4. Lower entropy indicates more confident responses. Compared to the Vanilla and SFT model, SEMIEVOL demonstrates significant improvement in response confidence. This observation substantiates the effectiveness of SEMIEVOL in producing more decisive and assured outputs. This signifies that SEMIEVOL not only improves accuracy but also enhances the models ability to generate more confident and reliable responses."
        },
        {
            "title": "4.3.4 Category-wise Performance Analysis",
            "content": "We conducted an in-depth investigation into the differential impact of SEMIEVOL across various Figure 6: Category-wise performance of SEMIEVOL. categories in MMLU-Pro, as illustrated in Figure 6. We find that (1) SEMIEVOL demonstrates enhanced performance across the majority of domains compared to both SFT and Vanilla approaches. This broad-spectrum improvement underscores the methods versatility and effectiveness across diverse subject areas. (2) SEMIEVOL achieves substantial gains in specific fields such as Law, Engineering, and Philosophy. This notable improvement suggests that knowledge in these domains is underrepresented in common knowledge bases, highlighting the necessity for targeted adaptation."
        },
        {
            "title": "4.3.5 Stability Analysis",
            "content": "We evaluate the inference stability of different models by utilizing diverse prompts. Specifically, we employed GPT-4o to rephrase the instructions and conducted 5 tests on each model, reporting the average performance and standard deviation. As illustrated in Figure 5, changing the inference prompts had minimal impact on the various models. Notably, SEMIEVOL even demonstrated slight improvement in model stability."
        },
        {
            "title": "4.3.6 Discussion on Continuous Evolution",
            "content": "In real-world scenarios, unlabeled data often accumulates continuously, altering the ratio between labeled and unlabeled data. Table 3 illustrates the impact of various data proportions on SEMIEVOLs performance. As illustrated, model performance consistently improves with an increase in unsupervised data across different base models. This validates SEMIEVOLs effectiveness in addressing real-world scenarios, where model performance in specific domains can be progressively enhanced as more unsupervised data accumulates. Base Model MMLU (Dunlabeled / Dlabled) MMLU-Pro (Dunlabeled / Dlabled) 50% 100% 200% 300% 50% 100% 200% 300% GPT-4o mini Llama3.1-8B 78.2 68.3 78.6 69.5 79.3 69.7 79.9 70.3 58.9 50. 59.5 52.0 60.1 53.5 60.8 54.3 Table 3: Performance of continuous evolution with varying amounts of unlabeled data."
        },
        {
            "title": "5.2 Semi-supervised Learning",
            "content": "Semi-supervised learning aims to reduce the annotation cost during model training (Zhu, 2005; Tarvainen and Valpola, 2017), which has received increasing attention in various fields such as text classification (Duarte and Berton, 2023; Thangaraj and Sivakami, 2018; Linmei et al., 2019) and neural machine translation (Cheng et al., 2016; Pham et al., 2023). Current semi-supervised learning approaches can be mainly divided into two types, i.e., pseudo-labeling (Lee et al., 2013) and consistency regularization (Sohn et al., 2020; Berthelot et al., 2019). Pseudo-labeling approaches usually add extra unlabeled data into the labeled dataset by leveraging the labels predicted by the model. Recent studies attempt different techniques to enhance pseudo-labeling such as considering adaptive thresholds (Zhang et al., 2024; Rhee and Cho, 2019) and class imbalance (Wang et al., 2023a). In contrast, consistency regularization aims to encourage the consistency of predictions under different perturbations. However, these approaches focus on classification problems (Shi et al., 2023), which cannot be applied to LLM fine-tuning. To tackle this issue, we propose new framework SEMIEVOL in propagate-and-select manner for LLM adaptation."
        },
        {
            "title": "6 Conclusion",
            "content": "We for the first time investigate the practical challenge of utilizing hybird-data (i.e., both labeled and unlabeled data) to enhance LLMs performance in specific scenarios. We designed bi-level framework SEMIEVOL for knowledge propagation-andselection. This framework leverages in-weight and in-context knowledge propagation from labeled data, while employing collaborative learning and adaptive selection to generate high-quality pseudo-responses. We validated SEMIEVOLs efficacy on both general and domain-specific datasets, conducting detailed analysis of the improvements it yields. Furthermore, we demonstrated SEMIEVOLs capability for continuous iterative evolution, which plays crucial role in enhancing LLMs effectiveness in real-world applications. Figure 7: Iterative evolution performance, each iteration means perform round of SEMIEVOL."
        },
        {
            "title": "4.3.7 Discussion on Iterative Evolution",
            "content": "We verify the models iterative evolution capability, as illustrated in Figure 7. After applying SEMIEVOL, we utilized the labeled data and pseudo-response data as new labeled data, initiating fresh round of SEMIEVOL on the previously filtered unlabeled data. By the fourth iteration, we had utilized 94.75% of the unlabeled data, resulting in further performance improvements in the target scenario. The models performance on MMLU-Pro exceeded 55%. This iterative evolution capability further demonstrates the practicality of SEMIEVOL."
        },
        {
            "title": "5.1 Data Engineering for SFT",
            "content": "With the rapid advancement of Large Language Models (LLMs) (Zhao et al., 2023), researchers have discovered that employing suitable data for Supervised Fine-Tuning (SFT) can enhance model performance on downstream tasks (Taori et al., 2023; Longpre et al., 2023). Some researchers focus on data selection (Bhatt et al., 2024; Zhou et al., 2024; Xia et al., 2024; Bukharin and Zhao, 2023), aiming to improve data quality to boost model effectiveness within limited training budgets. Others concentrate on data synthesis (Xu et al., 2023; Mukherjee et al., 2023; Chung et al., 2024; Honovich et al., 2022; Cheng et al., 2023), attempting to enhance models instruction-following capabilities through synthesized instruction data. Complementary to these approaches, SEMIEVOL focuses on LLMs ability to continuously evolve in real-world scenarios, relying solely on their inherent capabilities. It effectively utilizes small amounts of labeled data to improve model evolution performance."
        },
        {
            "title": "Limitations",
            "content": "One limitation of our work is that due to the limit of computational resources, we do not evaluate our framework on more LLMs such as GPT-4o and Llama3.1 70B. In future work, we will attempt to incorporate our framework into these LLMs. Moreover, although our framework is evaluated on various benchmark datasets, we do not involve more complicated domains which require more scientific knowledge. To solve this, we will extend our framework to more advanced scientific domains such as genomics analysis."
        },
        {
            "title": "Ethics Statement",
            "content": "Our research adheres to the ACL Code of Ethics. All datasets and language models used in this study are publicly available. The code and related materials will be appropriately released to ensure transparency and reproducibility of our work."
        },
        {
            "title": "References",
            "content": "David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin Raffel. 2019. Mixmatch: holistic approach to semisupervised learning. Advances in neural information processing systems, 32. Gantavya Bhatt, Yifang Chen, Arnav Das, Jifan Zhang, Sang Truong, Stephen Mussmann, Yinglun Zhu, Jeffrey Bilmes, Simon Du, Kevin Jamieson, et al. 2024. An experimental design framework for label-efficient supervised finetuning of large language models. arXiv preprint arXiv:2401.06692. Tom Brown. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165. Alexander Bukharin and Tuo Zhao. 2023. Data diversity matters for robust instruction tuning. arXiv preprint arXiv:2311.14736. Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and William Yang Wang. 2022. Convfinqa: Exploring the chain of numerical reasoning in conversational finance question answering. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6279 6292. Daixuan Cheng, Yuxian Gu, Shaohan Huang, Junyu Bi, Minlie Huang, and Furu Wei. 2024a. Instruction pretraining: Language models are supervised multitask learners. arXiv preprint arXiv:2406.14491. Daixuan Cheng, Shaohan Huang, and Furu Wei. 2023. Adapting large language models via reading comprehension. In The Twelfth International Conference on Learning Representations. Daixuan Cheng, Shaohan Huang, and Furu Wei. 2024b. Adapting large language models via reading comprehension. In The Twelfth International Conference on Learning Representations. Yong Cheng, Wei Xu, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. 2016. Semi-supervised learning for neural machine translation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 19651974, Berlin, Germany. Association for Computational Linguistics. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457. Jacob Devlin. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, and Hervé arXiv preprint Jégou. 2024. The faiss library. arXiv:2401.08281. José Marcio Duarte and Lilian Berton. 2023. review of semi-supervised learning for text classification. Artificial intelligence review, 56(9):94019469. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 97299738. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300. Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 2022. Unnatural instructions: Tuning language models with (almost) no human labor. arXiv preprint arXiv:2212.09689. Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 2023. Unnatural instructions: Tuning language models with (almost) no human labor. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics, pages 14409 14428. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. 2021. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421. Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019. Pubmedqa: dataset for biomedical research question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 25672577. Sparck Jones, Steve Walker, and Stephen E. Robertson. 2000. probabilistic model of information retrieval: development and comparative experiments: Information processing & management, Part 2. 36(6):809840. Thomas Kipf and Max Welling. 2016. Semisupervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907. Po-Nien Kung, Fan Yin, Di Wu, Kai-Wei Chang, and Nanyun Peng. 2023. Active instruction tuning: Improving cross-task generalization by training on prompt sensitive tasks. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 18131829. Dong-Hyun Lee et al. 2013. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In Workshop on challenges in representation learning, ICML, volume 3, page 896. Atlanta. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:94599474. Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, Jiuxiang Gu, and Tianyi Zhou. 2024. Selective reflectiontuning: Student-selected data recycling for LLM instruction-tuning. In Findings of the Association for Computational Linguistics ACL 2024, pages 16189 16211, Bangkok, Thailand and virtual meeting. Association for Computational Linguistics. Hu Linmei, Tianchi Yang, Chuan Shi, Houye Ji, and Xiaoli Li. 2019. Heterogeneous graph attention networks for semi-supervised short text classification. In Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP), pages 48214830. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc Le, Barret Zoph, Jason Wei, et al. 2023. The flan collection: Designing data and methods for effective instruction tuning. In International Conference on Machine Learning, pages 2263122648. PMLR. Pekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wallenius, and Pyry Takala. 2014. Good debt or bad debt: Detecting semantic orientations in economic texts. Journal of the Association for Information Science and Technology, 65(4):782796. Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707. Yotam Perlitz, Ariel Gera, Michal Shmueli-Scheuer, Dafna Sheinwald, Noam Slonim, and Liat Ein Dor. 2023. Active learning for natural language generIn Proceedings of the 2023 Conference on ation. Empirical Methods in Natural Language Processing, pages 98629877. Viet Pham, Thang Pham, Giang Nguyen, Long Nguyen, and Dien Dinh. 2023. Semi-supervised neural machine translation with consistency regularization for low-resource languages. arXiv preprint arXiv:2304.00557. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. 2020. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167. Hochang Rhee and Nam Ik Cho. 2019. Efficient and robust pseudo-labeling for unsupervised domain adaptation. In 2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), pages 980985. IEEE. Zhengxiang Shi, Francesco Tonolini, Nikolaos Aletras, Emine Yilmaz, Gabriella Kazai, and Yunlong Jiao. 2023. Rethinking semi-supervised learning with language models. In Findings of the Association for Computational Linguistics: ACL 2023, pages 5614 5634. Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. 2020. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in neural information processing systems, 33:596608. Zhengwei Tao, Ting-En Lin, Xiancai Chen, Hangyu Li, Yuchuan Wu, Yongbin Li, Zhi Jin, Fei Huang, Dacheng Tao, and Jingren Zhou. 2024. survey Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244. Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al. 2023. Instruction tuning for large language models: survey. arXiv preprint arXiv:2308.10792. Xuerong Zhang, Li Huang, Jing Lv, and Ming Yang. 2024. Self adaptive threshold pseudo-labeling and unreliable sample contrastive loss for semiIn International supervised image classification. Conference on Artificial Neural Networks, pages 61 75. Springer. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. survey of large language models. arXiv preprint arXiv:2303.18223. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. 2024. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36. Xiaojin Jerry Zhu. 2005. Semi-supervised learning literature survey. on self-evolution of large language models. arXiv preprint arXiv:2404.14387. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. Antti Tarvainen and Harri Valpola. 2017. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems, 30. Ryan Teknium, Jeffrey Quesnelle, and Chen Guang. 2024. Hermes 3 technical report. Muthuraman Thangaraj and Muthusamy Sivakami. 2018. Text classification techniques: literature review. Interdisciplinary journal of information, knowledge, and management, 13:117. Haixin Wang, Jinan Sun, Xiang Wei, Shikun Zhang, Chong Chen, Xian-Sheng Hua, and Xiao Luo. 2023a. Dance: Learning domain adaptive framework for deep hashing. In Proceedings of the ACM Web Conference 2023, pages 33193330. Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu, Richard Yuanzhe Pang, Maryam Fazel-Zarandi, Jason Weston, and arXiv Xian Li. 2024a. preprint arXiv:2408.02666. Self-taught evaluators. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023b. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics, pages 1348413508. Yu Wang, Yifan Gao, Xiusi Chen, Haoming Jiang, Shiyang Li, Jingfeng Yang, Qingyu Yin, Zheng Li, Xian Li, Bing Yin, Jingbo Shang, and Julian McAuley. 2024b. Memoryllm: Towards selfupdatable large language models. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. 2024c. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652. Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. 2024. Less: Selecting influential data for targeted instruction tuning. arXiv preprint arXiv:2402.04333."
        }
    ],
    "affiliations": [
        "Peking University",
        "University of California, Los Angeles",
        "University of Illinois Urbana-Champaign",
        "University of Washington"
    ]
}