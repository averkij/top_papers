{
    "paper_title": "Multi-Token Attention",
    "authors": [
        "Olga Golovneva",
        "Tianlu Wang",
        "Jason Weston",
        "Sainbayar Sukhbaatar"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Soft attention is a critical mechanism powering LLMs to locate relevant parts within a given context. However, individual attention weights are determined by the similarity of only a single query and key token vector. This \"single token attention\" bottlenecks the amount of information used in distinguishing a relevant part from the rest of the context. To address this issue, we propose a new attention method, Multi-Token Attention (MTA), which allows LLMs to condition their attention weights on multiple query and key vectors simultaneously. This is achieved by applying convolution operations over queries, keys and heads, allowing nearby queries and keys to affect each other's attention weights for more precise attention. As a result, our method can locate relevant context using richer, more nuanced information that can exceed a single vector's capacity. Through extensive evaluations, we demonstrate that MTA achieves enhanced performance on a range of popular benchmarks. Notably, it outperforms Transformer baseline models on standard language modeling tasks, and on tasks that require searching for information within long contexts, where our method's ability to leverage richer information proves particularly beneficial."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 7 2 9 0 0 . 4 0 5 2 : r Multi-Token Attention Olga Golovneva Tianlu Wang Jason Weston Sainbayar Sukhbaatar FAIR at Meta"
        },
        {
            "title": "Abstract",
            "content": "Soft attention is critical mechanism powering LLMs to locate relevant parts within given context. However, individual attention weights are determined by the similarity of only single query and key token vector. This single token attention bottlenecks the amount of information used in distinguishing relevant part from the rest of the context. To address this issue, we propose new attention method, Multi-Token Attention (MTA), which allows LLMs to condition their attention weights on multiple query and key vectors simultaneously. This is achieved by applying convolution operations over queries, keys and heads, allowing nearby queries and keys to affect each others attention weights for more precise attention. As result, our method can locate relevant context using richer, more nuanced information that can exceed single vectors capacity. Through extensive evaluations, we demonstrate that MTA achieves enhanced performance on range of popular benchmarks. Notably, it outperforms Transformer baseline models on standard language modeling tasks, and on tasks that require searching for information within long contexts, where our methods ability to leverage richer information proves particularly beneficial."
        },
        {
            "title": "Introduction",
            "content": "The attention mechanism (Bahdanau et al., 2014; Vaswani et al., 2017) is critical component of Large Language Models (LLMs) that enables them to retrieve and combine information from different parts of the context. Attention is especially useful when the context contains large number of tokens, as focusing on the relevant part while disregarding distractions becomes crucial. However, numerous works have shown that standard attention can suffer from suboptimal performance in this setting (Kamradt, 2023; Kuratov et al., 2025). Standard multi-head attention works by comparing the similarity of the current query vector and the key vectors corresponding to the context tokens using their dot products. The keys similar to the query obtain higher attention weights, and subsequently their value vectors dominate the output vector. For example, query vector corresponding to token Alice is capable of locating all mentions of Alice in the context. However, each attention weight conditions only on single key and query vector (besides the normalization to sum to one). We argue that the dependency on single token vector similarity brings fundamental limitation to the attention mechanism. In many cases, the relevant part of the context cannot be identified by single token. For example, looking up sentence that mentions both Alice and rabbit requires the query vector to encode both tokens. Looking up Alice with one attention head and using another head for rabbit could find their mentions separately, but is not sufficient to pinpoint where both are mentioned together. While it is possible to encode multiple tokens into single vector via the layers of the Transformer, this requires increased dimension, and for the model to use lots of its capacity for this task. In this paper, we propose novel attention mechanism that goes beyond this single token bottleneck, which we call Multi-Token Attention (MTA). The high level goal is to make it possible to use the similarities of multiple vector pairs to determine where attention must focus. We achieve this by making straight-forward modifications to the existing attention mechanism. In particular, we design convolution operations over attention weights that 1 Figure 1: Multi-Token Attention (MTA) on the right, compared to standard attention on the left. In MTA, within each head we apply key-query convolution on the attention scores and, after softmax normalization, head convolution across groups of heads. Additionally, we apply group normalization with depth-dependent scaling before final concatenation. operate on three dimensions: keys, queries, and attention heads. This allows its attention weights to condition on neighboring keys, previous queries, and other heads. Intuitively, following our previous example, MTA can find mentions of Alice and rabbit separately first, and then combine those attentions together to focus only on where both exist. We first experiment with motivating toy task that reveals the shortcoming of standard attention and demonstrate that MTA can easily solve it. Next, we test our method at scale by pre-training 880M parameters models on 105B tokens on standard language modeling task. There we see MTA bring improvements in terms in validation perplexity as well as standard benchmark tasks, while only increasing the number of parameters by 0.001%. Further, we evaluate the resulting models on long-context tasks such as Needle-in-the-Haystack and BabiLong where MTA outperforms the baselines by significant margin."
        },
        {
            "title": "2 Background on multi-head attention",
            "content": "We first describe the standard multi-head attention (Vaswani et al., 2017) and define the notation we use. In decoder-only Transformer architectures, the model receives sequence of tokens [x1, ..., xT] of length as an input. These tokens then undergo series of transformations into hidden states = [h1, . . . hT] RTD through an embedding layer and repeated transformer layers. Each layer consists of multi-head attention submodule, feedforward network, and normalization operations. Multi-head attention includes heads with dimensions = D/M working in parallel. Each head uses key, value and query projections Wk, Wv, Wq RDd to construct key, value and query vectors: = HWk, = HWv, = HWq The attention logits ˆA and weights (i.e. probabilities) are then calculated as follows: ˆA = QK/ d, = Softmax(Mask( ˆA)), (1) where the softmax operates over the key dimension, and the mask function replaces values at (i, j) with for < to prevent information leaking from future tokens. Finally, the attention output AV RTd from all heads is then concatenated, and multiplied by the output projection Wo RDD which is then passed to the normalization and feedforward components. This standard approach is summarized in Figure 1 (left)."
        },
        {
            "title": "3 Multi-Token Attention",
            "content": "Each attention value in standard multi-head attention, see Equation 1, depends solely on single key and query vector. That means all the necessary information for finding and attending to relevant part of the context must be compressed into these single vectors. This 2 Figure 2: MTA applies key-query and head convolution over attention values. might not be ideal if we are looking for sentence containing multiple elements. Consider for example the sentence Where did Alice see the rabbit?. We could try to find instances of Alice and rabbit independently and then check if there is sentence that has both. Let qa and qr be query vectors encoding Alice and rabbit respectively (assuming word tokenizer), then their attention weights are computed as follows: d), ar = Softmax(qrK/ aa = Softmax(qaK/ (2) By doing normal attention with those queries, we can attend where Alice and rabbit are mentioned in the context. All we have to do then is to check if both attention weights aa and ar have higher probabilities at the same nearby locations, e.g. in the same sentence, which will indicate that the sentence mentions both Alice and rabbit. Unfortunately, normal attention lacks such interaction between attention maps, and instead only uses them to compute output values. Even if we use different attention heads to find Alice and rabbit, there is no mechanism to combine these attention weights. This motivates us to modify the attention mechanism to allow combining different attention maps from nearby locations (both in terms of query and key locations), or between different attention heads. d) As shown in Figure 1 (right), our proposed Multi-Token Attention consists of three important components built on top of multi-head attention: key-query convolution, head mixing convolution, and group normalization with depth scaling. We propose key-query convolution to combine multiple keys and queries within heads, and head convolution to share knowledge between heads and amplify important information. Finally, we apply group normalization with depth scaling (Ye et al., 2024) to push back against residual streams and improve gradient flow. In this section, we will describe how MTA works. 3.1 Key-query convolution Pre-softmax convolution MTA applies convolution operation on attention logits to combine information from multiple query and key tokens: = Softmax (cid:0)Conv2dθ( ˆA)(cid:1) , (3) where Conv2dθ is 2-dimensional convolution operation with kernel weights θ, and kernel sizes (cq, ck). Convolution is applied to the key and query length dimensions, while the batch and head dimensions remain independent. More precisely, the attention weight aij from query qi to key kj is computed as follows: aij = Softmax cq1 i= ck/21 j=ck/2 1ijj θi,j qii jj / (4) Given that the query position is i, any information from position > should not be used to prevent information leakage from the future. That is why only past queries are used in Equation 4. For keys, we use indicator function 1ijj to zero out future keys. However such masking is too complex to implement (it is necessary to modify the convolution cuda kernels), thus we propose simpler version that applies the existing causal masking twice: = Softmax (cid:0)Mask (cid:0)Conv2dθ (5) Here, the first masking uses 0 so that those values will not affect the output from the convolution. Although this version masks out little more than necessary, it is simpler to implement and prevents information leakage, so we use it as our default. (cid:0)Mask0( ˆA)(cid:1)(cid:1)(cid:1) . 3 Post-softmax convolution Similarly, we can apply the convolution on top of the attention weights instead of the logits (6) This makes interaction between attention weights additive instead of multiplicative. We will experiment with both versions, but will use the pre-softmax version by default. = Mask0 (cid:0)Softmax (cid:0)Mask( ˆA)(cid:1)(cid:1)(cid:1) . (cid:0)Conv2dθ Each attention head has separate θ parameters, so they can perform different convolution operations. The chosen kernel dimensions dictate how far away tokens can be combined together. In the above example, the question Where did Alice see the rabbit? will make qa and qr queries separated by two tokens (assuming word tokenizer), so we need cq = 4 to cover both queries. Similarly, the target sentence Alice saw the white rabbit under the tree for example produces keys ka and kr separated by three tokens, so ck = 5 is sufficient for combining them. Before applying the convolution, we pad the input with an appropriate amount of zeros so that each convolution operation has valid input. 3.2 Head mixing convolution Unlike the key-query convolution, which allows mixing of attention weights from different time steps, we further propose to use head convolution over groups of heads, so attention weights from different heads can be combined. In particular, for head convolution kernel of size ch, all heads are divided in M/ch groups. Within each group, we apply nonoverlapping convolution operation1. This way, MTA allows not only to condition attention weights on multiple query and key vectors within each head, but also shares attention information across heads. For example, consider splitting all heads into groups of two, such that the kernel size is ch = 2. Let us use superscript to denote head indices, so A1 and A2 are attention weights from two different heads. Then the new attention weights are: A1 new = w11 A1 + w12 A2, A2 (7) where w11, w12, w21, w22 are kernel weights. Here the mixing occurred after the softmax, but we can also mix logits before the softmax: ˆA new = w11 ˆA1 + w12 ˆA2, Like the key-query convolution, we experiment with both versions. new = w21 ˆA1 + w22 ˆA2, new = w21 A1 + w22 A2, ˆA2 3.3 Putting everything together In the previous sections, we introduced two different ways of mixing attention weights, one across key-query time steps and another across different heads. These two can be implemented together in single MTA module. Because each has pre and post-softmax versions, there are multiple different ways of combining them. If both mixing methods are pre-softmax, then they can be implemented by single 3dim convolution operation as shown in Figure 2. Two of these dimensions will span key and query dimensions as described in Section 3.1. The third dimension will be across attention heads, but on groups of ch heads. The same 3-dim convolution can be applied after softmax to perform both mixing methods post-softmax. The third possibility is to apply the key-query convolution before softmax, and then head mixing after softmax. This is also straightforward by applying Equation 7 on the attention weights obtained from Equation 5. Finally, we follow Ye et al. (2024) and apply normalization with layer-dependent scaling for each head independently to improve gradient flow. We also report results without this step."
        },
        {
            "title": "4 Experiments",
            "content": "We conduct experiments with the MTA architecture, comparing to several baselines on set of standard and long-range dependency tasks, starting with toy task. Unless otherwise specified, we apply key-query convolution pre-softmax and head mixing post-softmax. 1Since the kernel size ch and group size ch are the same, it can also be viewed as fully-connected. 4 Block size = 5 Block size = 8 Model All First Last All First Last Transformer MTA 51.6 43.1 0.1 0.1 78.2 1.5 0.0 0.0 1.3 0.4 0.0 0.0 31.2 50.3 0.1 0.0 28.9 24.9 0.0 0. 58.4 46.8 0.0 0.0 Table 1: Error rates (%) on the motivating toy task where the model needs to locate block of letters containing the given = 2 letters. The output should contain all, first, or last tokens of the target block. MTA perfectly solves this task while standard Transformer struggles to learn it. We report average error rate over 3 seeds and their standard deviations. 4.1 Motivating toy task We start with simple toy task to demonstrate the effectiveness of our method over standard multi-head attention. In this task, the model is given sequence of blocks where each block consists of random letters. This is followed by question letters (L < N). The objective is to find the block that contains all question letters in any order. Then the model must output all letters of the target block, or only its first or last token (as three separate task variants). Despite its simplicity, this task poses challenge to standard attention because it requires pieces of information (i.e. question letters) to identify the target block. To succeed, standard soft attention must encode all question letters into single query vector. In contrast, MTA can first find the locations of each question letter, then use its convolution operation to increase the attention to the locations where all letters are found together. The results are shown in Table 1. As expected, Transformer with standard multi-head attention struggles to solve this task, often completely failing to find the target blocks even though the questions have only = 2 letters. This highlights the inherent limitation of standard attention conditioned on single token vectors. For MTA, we set the query kernel size cq = 2 to match the number of question letters, and the key kernel ck = 2N 1 so it can cover whole block on both sides. No head convolution is used. This way, it is sufficient for each query vector to encode only single question letter while the convolution operation can aggregate their locations to find the target block. As result, MTA successfully solves all the versions of the task with near zero error rate. See Appendix for more training details. 4.2 Large language modeling For language modeling experiments, we perform pre-training of 880M-size models and compare the Transformer model (Vaswani et al., 2017), Differential Transformer (DIFF Transformer) (Ye et al., 2024), and Transformer with MTA. DIFF Transformer calculates attention scores as the difference between two separate softmax attention maps, thus being related to our head convolution approach, so we use it as baseline. All models are trained in the same setup on the SlimPajama (Soboleva et al., 2023) dataset for 105B tokens using the Lingua framework (Videau et al., 2024). Training details are provided in Appendix D. To improve training efficiency, we apply the key-query convolution on every 4th layer, while head convolution is applied on all layers. Kernel dimensions are fixed at cq = 6, ck = 11, and we mix groups of ch = 2 heads within each layer. We ablate these parameters in Section 4.6. For each model, we conduct training twice and report average validation perplexity in Table 2. We observe consistent improvements across all validation datasets for the model trained with MTA, even though key-query convolution was only applied to quarter of the layers, operating with less learnable parameters than DIFF Transformer (see Appendix Table 9). We also note that group normalization with layer scaling is an important component that drives superior performance for both the DIFF Transformer and MTA architectures. We further evaluate our models on set of popular benchmarks in zero-shot setup as shown in Table 3. The model trained with MTA outperforms the baselines on most of them and achieves higher average score, despite these not being long-context tasks. 5 Model Pretraining Transformer DIFF w/o group norm. DIFF transformer MTA w/o group norm. MTA Long context finetuning Transformer DIFF transformer MTA w/o group norm. MTA arxiv book cc github se wiki Avg PPL 4.65 4.63 4.62 4.63 4.60 4.32 4.28 4.30 4. 13.47 13.41 13.33 13.36 13.26 13.18 13.01 13.07 12.97 20.20 20.06 19.99 20.03 19.90 20.14 19.87 19.94 19.84 14.41 14.34 14.28 14.31 14.22 14.08 13.93 13.96 13. 4.28 4.26 4.25 4.25 4.22 3.96 3.90 3.92 3.88 10.13 10.06 10.04 10.05 9.96 9.84 9.72 9.74 9.65 11.64 11.59 11.54 11.52 11.44 11.63 11.49 11.50 11. 11.25 (0.00) 11.19 (0.00) 11.14 (0.02) 11.16 (0.01) 11.09 (0.00) 11.02 10.89 10.92 10.85 Table 2: Validation perplexity for 880M Transformer model on SlimPajama dataset after training for 105B tokens, and finetuning with 2048 4096 context extension for another 10.5B tokens. Pretraining perplexity was averaged across two runs. Model BoolQ PIQA SIQA HellaS WinoG ARCe ARCc OBQA MMLU Avg Transformer DIFF transformer MTA w/o group norm. MTA 56.2 59.6 60.5 61.0 70.2 70.5 70.8 70.9 39.9 39.7 40.0 39.9 38.5 38.9 38.9 39.0 56.4 56.4 56.0 57.1 57.9 57.7 57.3 58. 25.9 25.6 25.1 25.0 23.8 21.4 21.7 22.7 24.5 24.9 24.9 25.7 43.7 (0.3) 43.9 (0.5) 43.9 (0.0) 44.4 (0.0) Table 3: Pretrained models evaluation results on standard benchmarks. Results are averaged over two model training runs for each method. Model LAMBADA standard LAMBADA OpenAI Transformer DIFF transformer MTA w/o group norm. MTA 17.6 14.9 15.2 13.6 9.5 9.3 9.2 8.7 Table 4: Perplexity evaluations on the LAMBADA standard (Paperno et al., 2016) and LAMBADA OpenAI (Radford et al., 2019) datasets. 4.3 Long context finetuning We further finetune our models on the same mix of datasets for another 10.5B tokens, but increase the context length from 2048 to 4096. We increase RoPEs theta to 500000, change the weight decay to 0, and reduce warm up steps to 50. Other parameters remain the same as during pretraining (see Appendix D). The resulting Transformer model with MTA similarly outperforms the new baselines in perplexity evaluations as shown in Table 2. 4.4 Long-range dependency tasks It was previously shown that Transformers struggle to find relevant information especially in the middle of long context (Liu et al., 2024; 2025). To test MTA in this setting, we evaluate trained models on three tasks: LAMBADA (Paperno et al., 2016; Radford et al., 2019), NeedleIn-A-Haystack (Kamradt, 2023) and BabiLong (Kuratov et al., 2025). All these tasks require models to almost sharply attend to the long-range tokens buried in the context. LAMBADA is collection of texts that test the models ability to search long-range dependencies in the context. In particular, this dataset is designed such that humans can correctly predict the next word only if they have the whole text, but not if they only see the last sentence preceding the target word. We observe models trained with MTA are better at correctly guessing the next word  (Table 4)  , significantly outperforming the baseline Transformer model. Pretraining, 2k cont. Long context finetuning, 4k cont. Model Transformer DIFF transformer MTA w/o group norm. MTA N=2 82.1 96.0 95.3 92. N=4 56.4 62.5 92.6 65.1 N=6 44.7 55.6 77.4 63.0 N=2 90.5 95.7 94.4 97. N=4 41.1 73.8 96.3 77.7 N=6 31.9 60.0 79.0 67.0 Table 5: Multi-needle retrieval accuracy (%) when varying the number of needles (N) averaged over different needle insertion depths. Pretrained models are evaluated with 2K context, while finetuned models are evaluated with 4K context. Figure 3: Kernel pattern (left) and corresponding attention map (right), which has the highest attention scores on the targeted needle The magic number of San Francisco is 8. This kernel amplifies attention if query token sequence matches key sequence useful for searching for related sentences that match the current sentence. Needle-In-A-Haystack We probe our models by inserting 2, 4, and 6 needles in 2k and 4k context windows at varying depths. We additionally ensure that for each new sample, the needles are shuffled, thus removing bias the models might have toward extracting the needle that was inserted first or last. Each setup is evaluated on 500 samples. Accuracy evaluations are averaged across the depths of insertion. As reported in Table 5, we observe significant improvement in needle extraction abilities for models trained with MTA across all needle counts and varying context lengths. Breakdown by depth is reported in Appendix G. BabiLong This benchmark tries to to assess language models ability to reason across facts scattered in long documents. We focus on tasks QA1-5 (Weston et al., 2015) where correct response requires various numbers of facts or argument relations. Examples of input and target output can be found in Table 7. We present average accuracy in Figure 4(left) and per-task in Appendix Figure 5. We observe that the MTA modelperforms well compared to other models, especially when there is more distraction text (4K tokens) in the input. 4.5 Kernel patterns Key-query and head convolution kernels across different layers and heads are shown in Appendix H. We observe that number of of the key-query kernels are close to identity, with near-one weight learned for targeted query and key, and near-zero weights learned for all other positions. However, there are numerous kernels that are more intricate. One such kernel is shown in Figure 3(left), which has diagonal structure. This kernel will amplify attention if sequence of query tokens match sequence of keys. Such ability is useful in searching sentences that matches the current sentence. Indeed, we observe that this particular kernel focuses attention on the target needle (see Figure 3(right)) in the Needle-in-the-Haystack task, where we must locate the magic number of the matching city. Other kernels, for example, can be viewed as priming, amplifying if the same key was attended by previous queries (head 5 on Figure 8), or edge detecting, amplifying the first or last of multiple contiguous keys with high attention (heads 8 and 11 on Figure 10). We leave further exploration of key-query kernel maps and their meaning for future explorations. 7 Figure 4: (left) Average accuracy on QA1-5 tasks in BabiLong. The models are all pretrained with 2K context and then finetuned with 4K context. Distraction text length varies from 0K (no distraction) to 4K tokens. MTA consistently outperforms the baseline models. (right) Ablation on the number of MTA layers with key-query convolutions (head convolution is applied to all layers). We report average validation perplexity on SlimPajama. Head kernel patterns, on the contrary, are simpler due to their small size, as shown in Appendix Figure 13. Besides an identity with scaling, common pattern is contrasting: subtracting one attention weight from another. We also observe that the kernel scales increase with layers when there is no group normalization (see Appendix Figure 14). This is probably to compete with the residual stream, which gets larger with the models depth. However, this pattern is not present with group normalization because it will undo the effect of such scaling. 4.6 Ablations Key-query convolution To understand how key-query convolution affects the model performance, we run ablation studies on the number of layers where key-query convolution is added to the attention. The results shown in Figure 4(right) demonstrate that when only 2 layers are enhanced with MTA, the model can outperform strong baselines, while 6 layers with MTA strikes balance between performance and additional complexity (see Appendix for more about computational complexity). Kernel initialization Kernel initialization can affect the convergence rate and stability of training. We evaluated MTA models initialized with zero, constant (0.3), and identity kernels. The latter corresponds to initialization with regular Transformer without convolution in the attention. We found that identity initialization leads to better convergence and final performance, while models initialized with zero and 0.3 values reduce average validation perplexity by 0.02 and 0.08 correspondingly. Ablation on MTA components We further experiment with MTA kernels of different sizes, changing the order of convolution operations with respect to softmax, and different normalizations, such as layer-norm scaling (Sun et al., 2025). Results are summarized in Table 6. We found that different kernel sizes display similar kernel patterns, while resulting in slightly different evaluation results (middle rows). Group normalization and exponential depth scaling are both important factors each bringing improvement over vanilla MTA (top rows), while square root layer-norm scaling surprisingly underperforms. Finally, changing the order of convolutions wrt softmax operations increases perplexity only by 0.01-0.04 points (bottom rows)."
        },
        {
            "title": "5 Related work",
            "content": "Focusing attention There has been number of attempts to focus the soft attention mechanism on important context. For example, modifications have been proposed to make softmax sharper. Martins & Astudillo (2016) propose to replace softmax with sparsemax for 8 Key-query conv Head conv Group norm no scaling layer-norm scaling post-softmax post-softmax pre-softmax pre-softmax PPL 11.09 11.16 11.13 11.41 11.23 11.23 11.31 11.11 11.19 11.10 11.20 cq = 4, ck = 9 cq = 6, ck = 11 cq = 8, ck = 13 post-softmax post-softmax pre-softmax pre-softmax Table 6: Ablation on MTA components: validation perplexity over SlimPajama dataset. sparse activations. Veliˇckovic et al. (2024); Nakanishi (2025) propose Adaptive temperature and Scalable-Softmax that adjust exponential base in softmax attention. Irrelevant tokens can be altogether removed from memory (Sukhbaatar et al., 2021) to make attention easier. In Golovneva et al. (2024); Desrochers et al. (2024) the authors incorporate contextual information into position encodings to allow positions to be amplified by context. More recently, several noise canceling mechanisms have been proposed for attention. DIFF transformer (Ye et al., 2024) uses differential amplifiers to focus attention to the relevant context, which is related to our head mixing step. Cang et al. (2025) further extends it by introducing normalization steps. OpAmp adaptation (Wu et al., 2025) propose adapters, which are fine-tuned with noisy context to enhance an LLMs denoising capability. Xu et al. (2024) modified attention so that keys and values can shift by one time step. This can be viewed as special case of MTA where the convolution performs shifting in the key dimension. Convolution in Attention Convolution layers have been used with attention (Gehring et al., 2017; Wu et al., 2019), especially in vision Transformers, to decompose each image into sequence of tokens with fixed length, and then apply multiple standard Transformer layers (Xiao et al., 2021; Wu et al., 2021). There have been some attempts to include convolution in language modeling as well. For example, Liu et al. (2018) use convolution to compress the keys and values in the multi-headed attention by factor of 3 thus allowing to process sequences 3x in length. Liu & Lapata (2019) later augment this architecture with the ability to encode multiple documents in hierarchical manner, and further improve on long-context summarization tasks. Later Subramanian et al. (2020) propose three hierarchical models to learn different levels of abstraction in language by interchanging casual transformer layers with convolutions or pooling, or compressing representations for tokens further away in the past. Gulati et al. (2020) proposed Conformer architecture for speech recognition, where convolution module is applied to multi-head self-attention output. This architecture was later used to encode speech in the Llama-3 herd of models (Grattafiori et al., 2024). However, none of these methods apply convolution to the attention weights."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we focused on limitation of the standard soft attention mechanism that stems from conditioning on the similarity of single vector pairs. This makes it challenging for Transformers to precisely locate relevant information based on richer distinguishing information. As remedy, we proposed novel Multi-Token Attention mechanism that combines attention weights from multiple queries, keys and heads, making it possible to attend using more fine-grained information. From simple motivating toy task to largescale LLM experiments on variety of popular benchmarks we demonstrate that models equipped with MTA achieve enhanced performance, especially when tested on tasks that require the precise location of relevant information within long context."
        },
        {
            "title": "References",
            "content": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical In Proceedings of the AAAI conference on artificial commonsense in natural language. intelligence, volume 34, pp. 74327439, 2020. Yueyang Cang, Yuhang Liu, Xiaoteng Zhang, Erlu Zhao, and Li Shi. Dint transformer. arXiv preprint arXiv:2501.17486, 2025. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Sarah Desrochers, James Wilson, and Matthew Beauchesne. Reducing hallucinations in large language models through contextual position encoding, 2024. Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann Dauphin. Convolutional sequence to sequence learning. In International conference on machine learning, pp. 12431252. PMLR, 2017. Olga Golovneva, Tianlu Wang, Jason Weston, and Sainbayar Sukhbaatar. Contextual position encoding: Learning to count whats important. arXiv preprint arXiv:2405.18719, 2024. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al. Conformer: Convolution-augmented transformer for speech recognition. arXiv preprint arXiv:2005.08100, 2020. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. Gregory Kamradt. Needle in haystack - pressure testing llms. https://github.com/ gkamradt/LLMTest NeedleInAHaystack, 2023. Yury Kuratov, Aydar Bulatov, Petr Anokhin, Ivan Rodkin, Dmitry Sorokin, Artyom Sorokin, and Mikhail Burtsev. Babilong: Testing the limits of llms with long context reasoning-ina-haystack. Advances in Neural Information Processing Systems, 37:106519106554, 2025. Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173, 2024. Peter Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198, 2018. Xiaoran Liu, Ruixiao Li, Mianqiu Huang, Zhigeng Liu, Yuerong Song, Qipeng Guo, Siyang He, Qiqi Wang, Linlin Li, Qun Liu, et al. Thus spake long-context large language model. arXiv preprint arXiv:2502.17129, 2025. Yang Liu and Mirella Lapata. Hierarchical transformers for multi-document summarization. arXiv preprint arXiv:1905.13164, 2019. Andre Martins and Ramon Astudillo. From softmax to sparsemax: sparse model of attention and multi-label classification. In International conference on machine learning, pp. 16141623. PMLR, 2016. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018. Ken Nakanishi. Scalable-softmax is superior for attention. arXiv preprint arXiv:2501.19399, 2025. Denis Paperno, German Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The lambada dataset: Word prediction requiring broad discourse context. arXiv preprint arXiv:1606.06031, 2016. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019. Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Daria Soboleva, Faisal Al-Khateeb, Robert Myers, deduplicated and Nolan Dey. Hestness, and slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama, 2023. URL https://huggingface.co/datasets/cerebras/SlimPajama-627B. of RedPajama. SlimPajama: version Jacob Steeves, Joel 627B token cleaned https://cerebras.ai/blog/ June Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Sandeep Subramanian, Ronan Collobert, MarcAurelio Ranzato, and Y-Lan Boureau. Multiscale transformer language models. arXiv preprint arXiv:2005.00581, 2020. Sainbayar Sukhbaatar, Da Ju, Spencer Poff, Stephen Roller, Arthur Szlam, Jason Weston, and Angela Fan. Not all memories are created equal: Learning to forget by expiring. In International Conference on Machine Learning, 2021. URL https://api.semanticscholar. org/CorpusID:234681615. Wenfang Sun, Xinyuan Song, Pengxiang Li, Lu Yin, Yefeng Zheng, and Shiwei Liu. The curse of depth in large language models. arXiv preprint arXiv:2502.05795, 2025. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, URL https://proceedings.neurips.cc/paper files/paper/2017/file/ Inc., 2017. 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. Petar Veliˇckovic, Christos Perivolaropoulos, Federico Barbero, and Razvan Pascanu. softmax is not enough (for sharp out-of-distribution). arXiv preprint arXiv:2410.01104, 2024. 11 Mathurin Videau, Badr Youbi Idrissi, Daniel Haziza, Luca Wehrstedt, Jade Copet, Olivier Teytaud, and David Lopez-Paz. Meta Lingua: minimal PyTorch LLM training library, 2024. URL https://github.com/facebookresearch/lingua. Jason Weston, Antoine Bordes, Sumit Chopra, Alexander Rush, Bart Van Merrienboer, Armand Joulin, and Tomas Mikolov. Towards ai-complete question answering: set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698, 2015. Felix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, and Michael Auli. Pay less attention with lightweight and dynamic convolutions. arXiv preprint arXiv:1901.10430, 2019. Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing convolutions to vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 2231, 2021. Haoyuan Wu, Rui Ming, Haisheng Zheng, Zhuolun He, and Bei Yu. Efficient opamp adaptation for zoom attention to golden contexts. arXiv preprint arXiv:2502.12502, 2025. Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Dollar, and Ross Girshick. Early convolutions help transformers see better. Advances in neural information processing systems, 34:3039230400, 2021. Mingyu Xu, Wei Cheng, Bingning Wang, and Weipeng Chen. Kv shifting attention enhances language modeling. ArXiv, abs/2411.19574, 2024. URL https://api.semanticscholar. org/CorpusID:274422840. Tianzhu Ye, Li Dong, Yuqing Xia, Yutao Sun, Yi Zhu, Gao Huang, and Furu Wei. Differential transformer. arXiv preprint arXiv:2410.05258, 2024. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019."
        },
        {
            "title": "A Limitations",
            "content": "To the best of our knowledge, the Multi-Token Attention method is not currently compatible with the popular optimized attention kernels. Optimizing the runtime performance was not the goal of this work, thus we leave further optimization of the MTA implementation for future research."
        },
        {
            "title": "B MTA Architecture",
            "content": "Head convolution vs normal attention with higher rank Let us consider mixing of two heads after softmax. If W1 are the output projections corresponding to those heads, their output can written as and W2 = (A1 newV1)W1 + (A2 newV2)W2 = (w11 A1V1 + w12 A2V1)W1 + w21V2W2 = A1(w11V1W1 W1 = A1(w11 HW1 W1 = A1 H(w11W1 + w21 HW2 W2 + w21W2 + (w21 A1V2 + w22 A2V2)W2 + w22V2W2 ) + A2(w12V1W1 ) W2 + w22 HW2 W1 ) W2 + w22W2 ) W1 ) + A2(w12 HW1 W1 W2 ) + A2 H(w12W1 If we compare the first term to normal attention output A1 HW1 , we note that the only difference is that now we have two rank-d matrix additions instead of one. Actually we can rewrite w11W1 W1 + w21W2 W2 = ˆWv 1 ˆWo 12 RD2d and ˆW1 R2dD. Thus, post-softmax head convolution can be repliwhere ˆW1 cated by normal attention head with twice the rank. This may help in understanding why head mixing is useful, as it may increase the expressive power using higher rank. However, it is not truly identical to 2 rank because the parameters of the two heads are not independent. Let us see if the same is true for the pre-softmax version. The attention logits after mixing is + w12 HW2 W2 + w12W2 Again, this can be rewritten like normal attention logits ˆA1 = w11Q1K1 + w12Q2K2 W1 W1 = w11 HW1 = H(w11W1 W2 )H RD2d. As before, it can be replicated by normal attention with twice the ˆA1 = ˆW1 ˆW1 where ˆW1 rank in the key and query projections. , ˆW"
        },
        {
            "title": "C Toy task details",
            "content": "The blocks are built by randomly choosing {5, 8} lowercase alphabets and concatenating them. We join up to 50 blocks into single sequence separated by ., followed by # and = 2 question letters. Here is an example sequence: hjnvt.qfjgt.whftb.bjtpq. ...(many blocks)... .pxjvf.ulhik.qoiax#pb Here the 4th block bjtpq is the target because it contains all query letters pb. During data generation, we make sure there is only one target block in each sequence. We use 1M such sequences for training and test on held-out 1K sequences. We train small Transformer model with 4 layers, 2 heads and 256 hidden dimensions. The batch size is 64 and the training continues for total of 100K steps. We run each experiment three times with different random seeds and report the average performance."
        },
        {
            "title": "D Language model training details",
            "content": "Training setup follows LLaMa architecture (Touvron et al., 2023), and includes RMSNorm pre-normalization (Zhang & Sennrich, 2019), SwiGLU activation function (Shazeer, 2020), and Rotary Embeddings (Su et al., 2024). Table 8 shows detailed hyperparameter breakdown for 880M model pretraining. All models were trained in the same setup, except Diff Transformer, which had twice less heads to account for doubled dimension. We evaluate on the following tasks: BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021), ARC easy and challenge (Clark et al., 2018), OpenBookQA (Mihaylov et al., 2018). We also report 5-shot performance on the aggregated MMLU benchmark (Hendrycks et al., 2020)."
        },
        {
            "title": "E BabiLong details",
            "content": "We include examples of input and output from BabiLong benchmark in Table 7. The details of per-task evaluation results on QA1-5 are shown in Figure 5."
        },
        {
            "title": "F Complexity evaluation",
            "content": "Table 9 shows estimated additional number of parameters and their actual counts. We note that DIFF transformer (Ye et al., 2024) introduces four learnable vectors per layer 13 Task Name Input Target Question QA1 single supporting fact QA2 two supporting fact QA3 three supporting fact QA4 two arg relations QA5 three arg relations John travelled to the hallway. Mary journeyed to the bathroom. Daniel went back to the bathroom. John moved to the bedroom. Mary journeyed to the bathroom. ... Daniel grabbed the football there. Sandra grabbed the milk there. Daniel went to the kitchen. Daniel moved to the bathroom. ... Mary got the football. Mary went back to the kitchen. Mary journeyed to the garden. The hallway is east of the bathroom. The bedroom is west of the bathroom. Fred picked up the football there. Fred gave the football to Jeff. ... Jeff gave the football to Fred. Fred gave the football to Jeff. bathroom Where is Mary? kitchen Where is the football? kitchen Where was the football before the garden? bedroom What is the bathroom east of? Jeff Who did Fred give the football to? Table 7: Examples of QA1-5 tasks in BabiLong benchmark when there is no distraction context."
        },
        {
            "title": "Parameter",
            "content": "Dimension, Layers Heads, RoPE theta Batch size (tokens) Context length Learning rate Weight decay Warm up steps"
        },
        {
            "title": "Value",
            "content": "1536 24 16 100, 000 262, 144 2048 1.5e 4 0.05 375 Table 8: Hyperparameters for 880M Transformer training. (λq1, λk1, λq2, λk2), plus group normalization weights. In total, that results in more parameters than Multi-Token Attention with key-query convolution added to the quarter of layers, even though all kernel weights in MTA models are different between all heads and layers. 14 Figure 5: Accuracy (%) on QA1-5 tasks in BabiLong benchmark. Note the random performance on QA3 is 16.67%, thus all models perform poorly on QA3. On other tasks, MTA demonstrate strong performance compared to baselines especially when there is lengthy distraction text (4K). Model Additional parameters, estimates Actual number of parameters Transformer DIFF transformer MTA MTA w/o group norm. 4 + 2 cq ck/4 + ch + cq ck/4 + ch 876,553,728 876,567,552 876,563,136 876,560,832 Table 9: Estimation of the parameter counts for each model architecture. Model Memory, GB FLOPS, 1013 TPS, 103 Transformer DIFF transformer MTA MTA w/o group norm. 17.5 21.6 59.1 58. 25.0 8.4 5.0 5.2 54.3 17.6 11.5 11.5 Table 10: Average memory consumption and training speed for models trained on 32 NVIDIA H200 GPUs with our unoptimized code. Table 10 shows actual runtime and memory consumption recorded during 880M model training. Note that the baseline and DIFF transformer utilize scaled dot product attention2 function implemented in Torch, that calls optimized CUDA kernels for improved performance. In contrast, our MTA implementation does not take advantage of such efficient kernels, which is the major reason behind its lower FLOPS. Multi-needle evaluation results We report on the detailed evaluation results in Figure 6. We observe that models trained with MTA are better at finding needles hidden deeper in the context. 2https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled dot product attention.html 15 Figure 6: Needle-in-a-Haystack accuracy across different models. Models trained with MTA are better at predicting needles hidden deep in the context. 16 Figure 7: Key-query kernel patterns across heads at 3rd layer of the Transformer model with MTA. MTA kernel patterns. Key-query convolution kernels across different layers and heads are reported in Figure 7Figure 12. 17 Figure 8: Key-query kernel patterns across heads at 7th layer of the Transformer model with MTA. 18 Figure 9: Key-query kernel patterns across heads at 11th layer of the Transformer model with MTA. Figure 10: Key-query kernel patterns across heads at 15th layer of the Transformer model with MTA. 20 Figure 11: Key-query kernel patterns across heads at 19th layer of the Transformer model with MTA. 21 Figure 12: Key-query kernel patterns across heads at 23rd layer of the Transformer model with MTA. Figure 13: Head kernel patterns across first three layers of the Transformer model with MTA. 23 Figure 14: Average of the diagonal kernel values across head pairs for Multi-Token Attention models with (left) and without (right) group normalization. Values for layers with key-query convolution are colored in orange. Figure 15: Average of the ratio between non-diagonal and diagonal kernel values across head pairs for Multi-Token Attention models with (left) and without (right) group normalization. Values for layers with key-query convolution are colored in orange."
        }
    ],
    "affiliations": [
        "FAIR at Meta"
    ]
}