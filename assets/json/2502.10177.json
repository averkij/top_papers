{
    "paper_title": "STMA: A Spatio-Temporal Memory Agent for Long-Horizon Embodied Task Planning",
    "authors": [
        "Mingcong Lei",
        "Yiming Zhao",
        "Ge Wang",
        "Zhixin Mai",
        "Shuguang Cui",
        "Yatong Han",
        "Jinke Ren"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "A key objective of embodied intelligence is enabling agents to perform long-horizon tasks in dynamic environments while maintaining robust decision-making and adaptability. To achieve this goal, we propose the Spatio-Temporal Memory Agent (STMA), a novel framework designed to enhance task planning and execution by integrating spatio-temporal memory. STMA is built upon three critical components: (1) a spatio-temporal memory module that captures historical and environmental changes in real time, (2) a dynamic knowledge graph that facilitates adaptive spatial reasoning, and (3) a planner-critic mechanism that iteratively refines task strategies. We evaluate STMA in the TextWorld environment on 32 tasks, involving multi-step planning and exploration under varying levels of complexity. Experimental results demonstrate that STMA achieves a 31.25% improvement in success rate and a 24.7% increase in average score compared to the state-of-the-art model. The results highlight the effectiveness of spatio-temporal memory in advancing the memory capabilities of embodied agents."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 7 7 1 0 1 . 2 0 5 2 : r STMA: Spatio-Temporal Memory Agent for Long-Horizon Embodied Task Planning Mingcong Lei135, Yiming Zhao415, Ge Wang15, Zhixin Mai135 Shuguang Cui213, Yatong Han135, Jinke Ren123 1FNii-Shenzhen, 2SSE, and 3Guangdong Provincial Key Laboratory of Future Networks of Intelligence, The Chinese University of Hong Kong, Shenzhen, China 4 Harbin Engineering University, China 5Infused Synapse AI jinkeren@cuhk.edu.cn; hanyatong@cuhk.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "A key objective of embodied intelligence is enabling agents to perform long-horizon tasks in dynamic environments while maintaining robust decision-making and adaptability. To achieve this goal, we propose the Spatio-Temporal Memory Agent (STMA), novel framework designed to enhance task planning and execution by integrating spatio-temporal memory. STMA is built upon three critical components: (1) spatio-temporal memory module that captures historical and environmental changes in real time, (2) dynamic knowledge graph that facilitates adaptive spatial reasoning, and (3) planner-critic mechanism that iteratively refines task strategies. We evaluate STMA in the TextWorld environment on 32 tasks, involving multi-step planning and exploration under varying levels of complexity. Experimental results demonstrate that STMA achieves 31.25% improvement in success rate and 24.7% increase in average score compared to the state-of-the-art model. The results highlight the effectiveness of spatio-temporal memory in advancing the memory capabilities of embodied agents."
        },
        {
            "title": "Introduction",
            "content": "In embodied intelligence, the ability of agents to perform complex tasks in dynamic environments depends on their capabilities for long-term planning, reasoning, and adaptability. Despite recent advances in Artificial Intelligence (AI) systems powered by Large Language Models (LLMs), such as open-world games [1], dialogue systems [2], and personalized recommendation systems [3], there remain significant challenges in their applications to embodied intelligence [4, 5]. In particular, limited memory capacity prevents agents from effectively integrating historical information and adapting to evolving environments, resulting in reduced decision-making accuracy and poor task execution in long-horizon scenarios [6, 7]. In recent years, researchers have explored various approaches to address this issue, such as Reinforcement Learning (RL) with memory augmentation [8], dynamic memory networks [9], and task reasoning using Knowledge Graphs (KGs) [10]. While these methods have achieved notable successes, they often struggle to jointly model spatial and temporal information, which are essential for reasoning and planning in dynamic environments. Furthermore, existing approaches typically rely on fixed plans or limited context, which hinders adaptability and robustness in multi-task scenarios. Correspondence authors: Jinke Ren; Yatong Han. * These authors contributed equally to this work. Preprint. Under review. Figure 1: Comparative overview of ReAct and STMA. (a) ReAct uses simple history buffer to store action-feedback pairs and reasoning information, generating actions one step at time. This approach lacks structured spatio-temporal reasoning, limiting its adaptability in complex, longhorizon tasks. (b) STMA utilizes dedicated spatial memory and temporal memory, summarized into refined spatial belief and temporal belief using the large models capabilities. The planner-critic module enables closed-loop planning, dynamically validating and adjusting action sequences based on environmental feedback. To address these issues, we introduce Spatio-Temporal Memory Agent (STMA), new framework designed to enhance task planning and decision-making by integrating both temporal and spatial memory. This memory design reduces reliance on large models by enabling the agent to dynamically reflect on and correct suboptimal strategies, thus rapidly converging toward optimal solutions. Figure 1 shows the comparison of the conventional framework ReAct [11] and our proposed framework STMA. ReAct utilizes simple history buffer to record action-feedback pairs along with some reasoning information, from which the next action is derived. In contrast, STMA replaces the traditional history buffer with spatio-temporal memory, significantly improving the agents spatial reasoning capabilities. Furthermore, rather than directly using raw spatio-temporal memory, we capitalize on the summarization to distill the memory into refined belief, allowing for more accurate decision-making by the planner based on this enriched input. Finally, we integrate the planner-critic, closed-loop planning architecture, to enable the large model to generate sequence of actions simultaneously, optimizing its planning capabilities, while dynamic feedback from the critic further enhances the plans accuracy. We evaluate the performance of STMA in the Textworld environment [12], partially observable, multi-task setting that requires agents to complete cooking tasks with varying complexities. Experimental results demonstrate that STMA outperforms the state-of-the-art models across several key metrics. Notably, STMA achieves 31.25% improvement in task success rate relative to the best-performing baseline, and 24.7% increase in average score, underscoring its enhanced ability to plan and adapt in complex, dynamic environments. The main contributions of this paper can be summarized as follows. Dynamic KG-based Spatial Memory: We propose dynamic KG for spatial memory that updates in real-time to reflect environmental changes. This approach enhances task reasoning and adaptability in complex, dynamic environments, improving long-horizon task planning over static memory systems. Planner-Critic Closed-Loop Architecture: We present planner-critic framework that combines proactive planning with real-time feedback. The planner generates multi-step action plans based on refined beliefs, while the critic adjusts strategies before each action step. This closed-loop mechanism ensures robust decision-making, particularly in environments with frequent re-planning and task switching. Spatio-Temporal Memory with Open-Source Model Efficacy: Our agent integrates spatio-temporal memory within closed-loop system, enabling dynamic adaptation to complex tasks. Remarkably, the agent performs competitively using open-source models (e.g., Qwen2.5-72b) without fine-tuning. Unlike architectures that rely heavily on large model capabilities, our approach demonstrates that well-designed spatio-temporal memory system, combined with planner-critic framework, can leverage open-source models for high-performance task execution."
        },
        {
            "title": "2 Related Work",
            "content": "The construction of embodied agents can be categorized into three approaches, including rule-based and traditional planning methods [13], RL methods [14], and LLM-based methods [15]. Rule-based and Traditional Planning Methods: Rule-based and traditional planning methods rely on expert-designed rules or algorithms to solve tasks, making them suitable for simple tasks in static environments. For example, the A* algorithm [16] excels in path planning but suffers from lower efficiency and accuracy in dynamic or unstructured environments [17]. Model-based planning [18] and constraint-based planning [19] attempt to improve adaptability to complex environments, but challenges persist when handling uncertainty and dynamic changes. RL Methods: RL enables agents to learn task strategies through interaction with the environment and has achieved notable success in various domains, such as AlphaZero [20] in board games. However, RL often faces issues in slow convergence and policy instability in high-dimensional action spaces and sparse reward scenarios [21, 22]. Additionally, RL performs poorly in tasks requiring global planning and long-term memory due to its focus on short-term returns [23, 24]. To address these challenges, researchers have proposed model-based RL [25, 26] to improve sample efficiency by constructing environment models, but these methods still struggle with model complexity and generalization. Deep RL [27] utilizes neural networks to enhance state representation but faces computational and data efficiency challenges in multi-task and high-dimensional scenarios [28, 29]. Furthermore, hierarchical RL [30] and meta-RL [31, 32] offer new approaches to handle long-term planning and task transfer, demonstrating potential in dynamic and complex environments. Despite these advances, RL still struggles with stability, computational overhead, and sample efficiency, particularly in the context of complex embodied tasks. LLM-based Methods: The introduction of LLMs has significantly enhanced the reasoning and taskhandling capabilities of agents [3335]. LLMs like GPT [36] and Gato [37] leverage self-supervised learning to process multimodal data and excel in natural language understanding and open-world tasks [38]. However, existing LLM-driven agents exhibit limitations in long-term planning and dynamic task environments, manifesting two key issues(1) Memory limitations: LLMs rely on autoregressive generation models and are unable to track task context or effectively store historical information; (2) Spatio-temporal reasoning deficits: LLMs perform reasoning based on pattern matching, lacking the ability to model spatio-temporal relationships in dynamic environments. Recently, researchers have proposed several approaches to address these issues. For example, ReAct [11] enhances task planning by introducing reflective and multi-step reasoning. However, ReActs reasoning process relies on manually set few-shot samples, which limits its generalization. Reflexion [39], building upon ReAct, incorporates self-reflection mechanism that allows agents to accumulate experience through multi-step trial and error. However, in embodied environments, errors may not be recoverable, limiting the effectiveness of this trial-and-error learning. Swiftsage [40], inspired by human dual-process theory [41] and fast-slow thinking [42], combines these modules to handle complex tasks. However, its open-loop architecture fails to adequately support long-term memory and dynamic planning. AdaPlanner [43] proposes closed-loop architecture where an initial plan is refined based on environmental feedback. Nevertheless, it lacks memory system, limiting its adaptability to long-horizon planning tasks. Hippo RAG [44] mimics the human hippocampus [45] and introduces KGs as long-term memory indices [46], significantly enhancing knowledge retrieval. However, these methods are still confined to short-term reasoning and lack support for long-term planning in dynamic environments."
        },
        {
            "title": "3 Problem Formulation",
            "content": "We model the agent-environment interaction as Partially Observable Markov Decision Process (POMDP) [47] extended with spatio-temporal reasoning. The POMDP is defined by the tuple (S, A, Ω, , O), where is the state space, the action space, Ω the observation space, (ss, a) the transition dynamics, and O(os, a) the observation function. At time-step i, the agent receives observation oi O(si, ai1) and maintains belief bi (S), probability distribution over states conditioned on the interaction history: bi(s) = (si = so1:i, a1:i1). (1) 3 Figure 2: Overview of STMA. STMA consists of two components: spatio-temporal memory module and planner-critic module. The spatio-temporal memory module is divided into temporal memory submodule and spatial memory submodule, which provide temporal and spatial beliefs, respectively. These beliefs serve as the spatio-temporal context for the planner-critic module. The planner-critic module consists of planner and critic. The planner performs action planning based on the belief and generates multi-step plans in single pass. The critic evaluates the plan before each action step, verifying whether the plan is correct and aligns with the most current environmental conditions. Following the predictive processing theory [47], this belief serves as the agents internal approximation of the true environmental state. In our work, we decompose the belief into: bi = (bt i, bs ), (2) where bt captures temporal dependencies through historical action-observation sequences, and bs encodes spatial relationships as dynamic KG. The agent uses these factored beliefs to approximate both the temporal progression of states and the spatial configuration of objects, enabling joint reasoning about environmental dynamics. This formulation provides theoretical foundation for integrating memory-augmented LLM agents with spatio-temporal awareness."
        },
        {
            "title": "4 Robotic Spatio-Temporal Memory Agent",
            "content": "As illustrated in Figure 2, STMA is composed of three key components, including temporal memory, spatial memory, and planner-critic module. 4.1 Temporal Memory Temporal memory maintains sequential event records to support time-dependent reasoning under partial observability. It addresses two key challenges: (1) LLMs struggle with long, unstructured histories, and (2) raw observations contain redundant details irrelevant to current decisions. Our solution combines raw data preservation with adaptive compression through two components: History Buffer. first-in-first-out queue storing raw interaction tuples is defined as: = {hi}n (3) where hi = (oi, ai) preserves the complete interaction history. This ensures no information loss while providing temporal grounding for downstream processing. i=1, Summarizer. Directly feeding raw interaction sequences to LLMs risks overwhelming them with redundant details and disordered temporal information. Our summarizer mitigates this via three 4 key operations(1) Information Compression: Condenses raw observations h[1:i1] into concise summaries, preserving task-relevant details while eliminating clutter. (2) Temporal Structuring: Organizes historical data into standardized formats (e.g., completed actions, active goals, observations) to enhance prompt clarity. (3) Preliminary Reasoning: Infers implicit patterns like progress tracking (retrieved key from the drawer) and strategic suggestions (consider examining the locked cabinet next). This process converts raw histories into temporal belief states, which is given by: : h[1:i1] bt i, (4) Where bt represents the temporal belief that compresses the temporal information for STMA. By maintaining compressed yet information-rich belief states, the summarizer enables efficient reasoning while retaining critical historical dependenciesstriking balance between memory efficiency and decision-making fidelity. 4.2 Spatial Memory Spatial memory aims to provide the LLM with direct spatial information to address the models limitations in spatial reasoning. By incorporating spatial memory, LLM can process and reason about spatial relationships, thereby improving its performance in tasks requiring spatial awareness and decision-making. As shown in Figure 2, the spatial memory is composed of four modules, including relation retriever, KG, retrieve algorithm, and relation aggregator. Relation Retriever. Inspired by HippoRAG [44], the relation retriever module leverages LLMs to derive spatial relationships from the temporal belief bt i, which encapsulates compressed historical information. This module extracts structured representations of spatial interactions as set of semantic triples: = (cid:8)ri ri = (xs , xr , xo )(cid:9), (5) where xs retriever operates as mapping function: denotes the subject entity, xr the relationship type, and xo the object entity. Formally, the : bt G, (6) which enables the translation of temporally condensed beliefs into explicit spatial relational knowledge. This process bridges temporal reasoning with spatial awareness, which is critical for complex environment navigation. Dynamic KG. To enhance system scalability and maintain stable long-term spatial memory, we formalize spatial relationships as dynamic KG. The graph G(V, E) comprises vertices (entities) and edges (relationships), updated iteratively as the agent interacts with the environment. At each step, newly extracted relationships replace outdated edges in G, ensuring the graph reflects current spatial configurations. This dynamic update mechanism allows the agent to adapt to environmental changes while mitigating memory staleness. By decoupling transient observations from persistent relational structures, the KG provides robust foundation for spatial reasoning and plan refinement. Retrieve Algorithm. To extract task-relevant subgraphs from the KG while balancing semantic relevance and relational diversity, we propose two-step retrieval process(1) Semantic Filtering: Compute cosine similarity between the query embedding (derived from task/environment context) and entity embeddings, retaining the top-n entities. (2) Relational Expansion: Perform K-hop neighborhood search from the filtered entities to capture local relational structures, forming the final subgraph. This approach ensures the retrieved subgraph Gs preserves both semantic alignment with the current context and spatial relationships critical for planning. The full procedure is formalized in Algorithm 1. Relation Aggregator. Directly inserting the extracted list of triples as string into the LLM prompt may degrade model performance. To address this issue, we introduce relation aggregator to organize the list of triples into natural language format. Similar to the summarizer, this module not only performs formatting but also incorporates degree of spatial reasoning. For example, if is west of and is west of C, the module deduces that is west of C. This allows the relation aggregator to 5 Algorithm 1 Retrieve Algorithm Require: Query string q, hop count K, top-n threshold, entity embeddings {ev}vV . Ensure: Relevant subgraph Gs. 1: Compute query embedding: Embed(q); 2: Initialize similarity scores: {sim(q, ev)v }; 3: Select top-n entities: Vtop argtopn(S); 4: Initialize candidate set: Vcand Vtop; 5: for = 1 to do 6: 7: end for 8: Extract subgraph: Gs (Vcand, Ecand) where Ecand = {(v, u) Ev, Vcand}; 9: return Gs. Expand neighbors: Vcand Vcand {u(v, u) E, Vcand}; preprocess the extracted spatial memory, enabling reasoning over spatial relationships. The processed memory is then represented as the spatial belief bs . The relation aggregator is expressed as : Gs bs , (7) where Gs is the task-relevant subgraph extracted by the retrieve algorithm. 4.3 Planner-Critic Agent The planner-critic module integrates spatio-temporal memory with systematic reasoning to generate robust action plans. As shown in Figure 2, it operates via two cooperative components: Planner. At step i, the planner synthesizes the temporal belief bt i, spatial belief bs observation oi to generate subgoal gi and corresponding action sequence {ˆai:k}m , and current k=1. Formally: (bt i, bs , oi) (gi, {ˆai:k}m k=1). (8) This dual-belief integration enables coherent planning by contextualizing current observations within historical patterns and spatial constraints. The planner is implemented as single agent that employs Chain-of-Thought (CoT) prompting [48] to explicitly reason about the current environment state, task requirements, and memory-derived constraints before outputting structured plans. Moreover, we urge planner to think about the consequences of the actions it plans to do before it decides on the output. Critic. Before executing each action ˆaj at step j(j [i, k]), the critic evaluates its validity using: (1) Temporal consistency with bt j, (2) Spatial feasibility per bs j, (3) Alignment with oj, and (4) Adherence to safety constraints. The evaluation function is given by: C(ˆaj, bt j, bs j, oi) (pj, fj), (9) which outputs validity flag pj {true, false} and feedback fj. If pj = false, the planner regenerates {ˆaj:k} regarding fj, creating an iterative refinement loop. This closed-loop process mitigates hallucinations by grounding decisions in spatio-temporal reality. Similar to the planner, the critic is implemented by single CoT LLM agent. We ask the critic to think of the possible consequences of the proposed action, decide whether this action aligns with the subgoal planner provided in the current stage, and check whether this action is out-of-date as the environment changes. This CoT process will improve the robustness of STMA in dynamic environments. The tight integration of memory-augmented beliefs with plan-critique cycles enables robust decisionmaking in partially observable environments. In the appendix C, we formalize this procedure by Algorithm 2, which demonstrates how spatio-temporal reasoning and iterative verification enhance plan reliability."
        },
        {
            "title": "5 Evaluation",
            "content": "5.1 Experiment Setting Environments. We use Textworld [12] as our experimental environment. In Textworld, we design series of cooking tasks with four difficulty levels. In these tasks, the agent is randomly placed in 6 Figure 3: Interaction with the Textworld Environment. The interaction pattern between Textworld and our framework involves the environment providing the agent with the current observation, inventory, and list of possible actions. Based on the agents executed actions, the environment returns feedback. These pieces of information are recorded in STMAs spatio-temporal memory, serving as the necessary context for the planner-critic agent. Within this framework, the planner and critic collaborate to generate action plans and interact with the environment. an indoor environment consisting of multiple rooms. It must explore the environment to locate the kitchen and complete the assigned cooking task. Upon reaching the kitchen, the agent follows the recipes instructions to find the required ingredients and corresponding utensils within the environment. Note that the ingredients and utensils may not be located in the kitchen. Other evaluation details are provided in the appendix A. The task difficulty is controlled by adjusting the number of rooms, the quantity of ingredients the agent needs to find, and the complexity of the recipe steps. Due to the unknown environment and the long sequences of actions required, this setting demands strong spatio-temporal reasoning skills to locate items and robust planning capabilities to execute the recipe instructions in the correct order using the appropriate utensils. Additionally, as the room layout and the distribution of items are unknown, the environment also requires the agent to exhibit high robustness in handling such uncertainties. Baselines. To facilitate comparisons, we consider three baseline agent frameworks: (1) ReAct [11], which is an early framework designed for agent operations. It provides prompting method that allows large models to exhibit reasoning capabilities while interacting with the environment through actions; (2) Reflexion [39], which introduces long-term memory and self-reflection module. By employing trial-and-error approach, Reflexion uses the self-reflection module to summarize experiences as long-term memory, thereby enhancing the models capabilities; and (3) AdaPlanner [43], which proposes closed-loop agent architecture. This framework continuously refines its plan to handle unforeseen situations, enabling dynamic adjustments to its strategy. In our experiments, each agent framework is tested using two LLMs: GPT-4o [36] and Qwen2.572b-instruct [49]. For embedding tasks, we uniformly use the nomic-embed-text-v1.5 model [50]. GPT-4o is one of the most commonly used proprietary LLMs, while Qwen2.5-72b-instruct represents high-performing open-source alternative. Notably, Qwen2.5-72b-instruct demonstrates performance comparable to GPT-4o in several benchmark tasks [51]. 7 Table 1: Comparison of Success Rates (SR) and Average Scores (AS) across difficulty levels between STMA and baseline methods. Performance metrics are reported as AS σ, where σ denotes the standard deviation across different difficulty levels. Difficulty Level ReAct(GPT-4o) Reflexion(GPT-4o) AdaPlanner(GPT-4o) STMA(GPT-4o) ReAct(Qwen2.5-72b) Reflexion(Qwen2.5-72b) AdaPlanner(Qwen2.5-72b) STMA(Qwen2.5-72b) 1 AS 100.00.0 100.00.0 90.624.8 100.00.0 75.043.3 100.00.0 100.00.0 100.00.0 SR 62.5 62.5 37.5 75.0 37.5 37.5 37.5 100.0 2 AS 76.531.9 78.630.3 51.843.4 91.115.9 60.734.1 60.732.5 53.641.5 100.00. SR 50.0 50.0 0.0 75.0 12.5 25.0 0.0 62.5 3 AS 65.039.4 60.040.9 3.84.8 87.526. 37.531.9 61.228.0 3.84.8 81.227.6 4 AS 43.337.9 51.031.9 1.93.3 60.638.5 16.39.0 19.27.7 3.85.4 58.730.3 SR 25.0 25.0 0.0 37.5 0.0 0.0 0.0 25.0 SR 100.0 100.0 87.5 100.0 75.0 100.0 100.0 100.0 Figure 4: Average score vs. steps of different frameworks (powered by GPT-4o) Evaluation metrics. We define two evaluation metrics to assess the performance: (1) Success Rate (SR), which is the ratio of completed tasks to the total number of tasks in each difficulty level. This metric reflects the agents ability to complete tasks across randomly generated scenarios; (2) Average Score (AS), which is the ratio of intermediate scores achieved to the maximum possible score in each scenario. An AS of 100% indicates that the task is completed in the given scenario. 5.2 Performance Evaluation Table 1 shows the comparison of our agent framework with the three baseline frameworks across four difficulty levels. For each difficulty level, we report the AS across eight randomly generated scenarios, along with its standard deviation (represented as mean standard deviation). Additionally, the SR for each difficulty level is recorded. For all models, AS and SR show general downward trend as task difficulty increases. This reflects the effectiveness of our experimental design in creating gradient of increasing complexity. As the environment becomes more challenging and tasks require more steps to complete, the SRs of all frameworks decrease. However, our framework consistently outperforms the baselines across all difficulty levels, regardless of the underlying model. The result stems from the integration of the spatio-temporal memory module, which enhances planning and execution capabilities. Specifically, our framework achieves an average improvement in SR of 12.5% (GPT-4o powered) and 31.25% (Qwen2.5-72b-instruct powered) compared to the best baseline. Similarly, the AS improves by an average of 11.15% (GPT-4o powered) and 24.7% (Qwen2.5-72b-instruct powered). Notably, the improvement is more pronounced for the open-source Qwen2.5-72b-instruct, demonstrating the frameworks ability to enhance the performance of less powerful models. Compared to ReAct and Reflexion, STMA incorporates spatio-temporal memory module and planner-critic agent, which significantly enhance reasoning capabilities and improve the robustness of the agent through the critic mechanism. As result, STMA demonstrates notable improvements in long-horizon planning tasks over ReAct. Besides, Reflexions self-reflection mechanism proves effective in the Textworld cooking tasks, leading to slight performance improvement over ReAct. However, experimental results indicate that this enhancement is relatively modest. While AdaPlanner also employs critic-like mechanism, its reliance on Python code as the action space and poorly Table 2: Ablation studies of the STMA. Success Rate (SR) and Average Score (AS) (AS σ) across four difficulty levels for model variants with individual modules are removed. Difficulty Level STMA w/o Spatio-Temporal Memory STMA w/o Summarizer STMA w/o Summarizer for Spatial Memory STMA w/o Spatial Memory STMA w/o Relation Aggregator STMA w/o Critic STMA 1 AS 0.00.0 87.533.1 100.00.0 93.816.5 100.00.0 100.00.0 100.00. 2 AS 0.00.0 100.00.0 65.040.6 91.115.9 96.49.4 85.725.8 100.00.0 SR 0.0 25.0 12.5 50.0 62.5 50.0 62.5 AS 0.00.0 55.032.8 37.531.9 65.040.6 71.237.2 70.033.5 81.227.6 SR 0.0 12.5 12.5 12.5 25.0 0.0 25.0 4 AS 0.00.0 49.028.5 37.535.1 30.828.0 51.034.8 26.031.0 58.730.3 SR 0.0 100.0 50.0 75.0 87.5 75.0 100.0 SR 0.0 87.5 100.0 87.5 100.0 100.0 100.0 designed memory system limits its adaptability to the Textworld cooking tasks, which require handling limited observations and long-term planning. In practice, AdaPlanner exhibits weaker spatial exploration capabilities than STMA. Figure 4 illustrates the AS progression curves for different models powered by GPT-4o. It is evident that our STMA framework consistently achieves the highest scores at faster rate in most cases. This advantage stems from the collaboration between the planner and critic, which reduces suboptimal or erroneous actions generated by the planner. For Level 1 tasks, the score progression of STMA is slightly lower than ReAct and Reflexion. It is because, in relatively simple tasks, the planner-critic agent in STMA behaves more cautiously, ensuring the correctness of certain actions before executing them. Additionally, GPT-4o-powered STMA tends to explore the environment more extensively before completing tasks. As result, STMA exhibits minor disadvantage in tasks with minimal spatial exploration requirements. However, for Level 2, 3, and 4 tasks, the exploration-oriented nature of STMA, combined with the enhanced spatial understanding enabled by its robust spatio-temporal memory framework, delivers significantly better performance. These results highlight STMAs superior ability to handle more complex tasks requiring advanced planning and spatial reasoning. 5.3 Ablation Studies We conduct ablation studies on agents powered by Qwen2.5-72b-instruct. The results are presented in Table 2. Spatio-Temporal Memory. In our experiments, completely removing the spatio-temporal memory module makes the agent unable to complete any task. For tasks involving unknown environments and requiring long-term planning, memory is crucial. Without memory system, the large model is incapable of remembering the recipe,\" making it impossible to complete even the simplest cooking tasks. Since our experimental setup requires the agent to recall and execute the steps specified in the recipe, the absence of memory renders task completion unattainable. Consequently, STMA without memory system scores zero across all difficulty levels. This highlights the critical role of memory in POMDP-based systems, where constructing cognitive understanding of the world state heavily relies on robust memory framework. Summarizer. The information in the history buffer is passed directly to the model as temporal belief in chronological order, without summarization (other components remain unchanged, with spatial memory still being extracted from summarized information). Results indicate that agent performance decreases slightly for simpler tasks (Levels 1 and 2) but drops significantly for more complex tasks. This suggests that for long-horizon tasks, the growing length of the history leads to an increasingly long prompt, which degrades the agents performance. Additionally, the longer history dilutes the density of useful information, making the LLM more susceptible to irrelevant data. The summarizer mitigates this issue by condensing the temporal memory, reducing the adverse effects of long history as the number of steps increases. Therefore, the Summarizer is critical for STMAs efficacy in complex scenarios, ensuring efficient information flow and robust decision-making by balancing detail with concision. Summarizer for Spatial Memory. The information in the history buffer is passed directly to the spatial memory module in chronological order (while summarized information is still used as temporal belief to control variables). Results show that agent performance decreases as task 9 complexity increases, and the decline is more significant compared to removing the summarizer for temporal belief. This indicates that the process of generating spatial belief from spatial memory relies on the summarizer for initial preprocessing. This is due to the LLMs limited capability to extract relevant spatial information from an unprocessed lengthy history. Thus, we can observe that LLMs may not be good at implicitly summarizing temporal trajectories while completing other tasks based on the information that the trajectory contains. This limitation affects not only inference tasks such as planning but also information extract tasks such as relation retrieval. Therefore, the summarizer is essential for elevating the performance of downstream tasks. Spatial Memory. We remove the spatial memory and its corresponding spatial belief input to evaluate the agents reliance on temporal belief alone. Results show significant performance drop, particularly for tasks with higher spatial complexity. This decline highlights the critical role of spatial memory in providing agents with foundational sense of space,\" enabling them to build an internal mental map\" of the environment during exploration. Notably, when comparing this experiment to the one where the summarizer for spatial memory is removed, we observe that eliminating spatial memory yields better performance across all but the most challenging Level 4 tasks. This suggests that agents require highly accurate spatial beliefs for effective decision-making. Incorrect or inaccurate spatial beliefs may mislead the agent, performing worse than when no spatial belief is provided. This underscores one of the key challenges in designing effective spatial memory systems. Thus, the spatial memory provides STMA with sense of space,\" enabling it to make better decisions based on the spatial information. Relation Aggregator. We remove the relation aggregator and use the raw list of relation triples extracted by the retrieve algorithm module as input without summarization or conversion into natural language. Results show moderate performance decline compared to the baseline. While LLMs demonstrate some ability to interpret spatial relationships from list of relation triples, converting these triples into natural language facilitates better understanding of the spatial information encoded in the spatial belief. This indicates that while the relation aggregator is not strictly necessary, it significantly improves the LLMs ability to process and utilize spatial memory. Critic. We remove the critic module from STMA, leaving only the planner to generate plans, which are executed without validation. Results show that while performance on the simplest tasks remains relatively stable, agent performance is significantly weakened for Levels 2, 3, and 4. This is attributed to the complexities of the cooking environment, which involves numerous dynamic and unpredictable elements. Without the critics involvement, any errors or hallucinations in the planners decisions are directly executed, leading to more redundant or incorrect actions in more complex tasks. For instance, during environment exploration, the agent often failed to update its plan upon encountering new information, continuing to follow outdated plans instead. This behavior results in inefficient exploration of new environments. Similarly, when executing recipe steps, errors such as using the wrong utensils are uncorrected, leading to task failures. In addition, we observe that the LLMs performance as critic is generally stronger than its performance as planner. This may be because the critics role is classifierdetermining whether an action is correct or incorrect based on beliefs and current environment. The classification task seems simpler than the planners generative task of creating new plans. This is one of the reasons why the critic is able to detect the error action generated by the planner. Therefore, removing the critic leads to significant drop in performance, as the planners unchecked errors accumulate and negatively impact the agents overall effectiveness."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper proposes new STMA framework for long-horizon embodied tasks, demonstrating the great potential of spatio-temporal memory mechanisms in task planning. Experimental results show that STMA performs exceptionally well across various task difficulty levels. Notably, when tackling complex tasks, STMA outperforms agents relying on random exploration by providing more precise and efficient solutions through rapid strategy adjustments and higher task completion rates. It is noteworthy that even with open-source models like Qwen2.5-72b, STMA achieves performance comparable to proprietary models, validating the superiority of the spatio-temporal memory module. Future work may optimize the spatio-temporal memory module to enhance its adaptability in more complex tasks."
        },
        {
            "title": "References",
            "content": "[1] Ming Yan, Ruihao Li, Hao Zhang, Hao Wang, Zhilan Yang, and Ji Yan. Larp: Language-agent role play for open-world games. arXiv preprint arXiv:2312.17653, 2023. [2] Zihao Yi, Jiarui Ouyang, Yuwen Liu, Tianhao Liao, Zhe Xu, and Ying Shen. survey on recent advances in llm-based multi-turn dialogue systems. arXiv preprint arXiv:2402.18013, 2024. [3] Hanjia Lyu, Song Jiang, Hanqing Zeng, Yinglong Xia, Qifan Wang, Si Zhang, Ren Chen, Christopher Leung, Jiajie Tang, and Jiebo Luo. Llm-rec: Personalized recommendation via prompting large language models. arXiv preprint arXiv:2307.15780, 2023. [4] Jiafei Duan, Samson Yu, Hui Li Tan, Hongyuan Zhu, and Cheston Tan. survey of embodied ai: From simulators to research tasks. IEEE Transactions on Emerging Topics in Computational Intelligence, 6(2):230244, 2022. [5] Rolf Pfeifer and Fumiya Iida. Embodied artificial intelligence: Trends and challenges. Lecture notes in computer science, pages 126, 2004. [6] Kostas Hatalis, Despina Christou, Joshua Myers, Steven Jones, Keith Lambert, Adam AmosBinks, Zohreh Dannenhauer, and Dustin Dannenhauer. Memory matters: The need to improve long-term memory in llm-agents. In Proceedings of the AAAI Symposium Series, volume 2, pages 277280, 2023. [7] Kuan Fang, Alexander Toshev, Li Fei-Fei, and Silvio Savarese. Scene memory transformer for embodied agents in long-horizon tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 538547, 2019. [8] Emilio Parisotto and Ruslan Salakhutdinov. Neural map: Structured memory for deep reinforcement learning. arXiv preprint arXiv:1702.08360, 2017. [9] Nishant Kumar, Kirti Soni, and Ravinder Agarwal. Prediction of temporal atmospheric boundary layer height using long short-term memory network. Tellus A: Dynamic Meteorology and Oceanography, 73(1):114, 2021. [10] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. Graph neural networks: review of methods and applications. AI open, 1:5781, 2020. [11] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022. [12] Marc-Alexandre Côté, Akos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, et al. Textworld: learning environment for text-based games. In Computer Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on Artificial Intelligence, IJCAI 2018, Stockholm, Sweden, July 13, 2018, Revised Selected Papers 7, pages 4175. Springer, 2019. [13] Brian Blake. Rule-driven coordination agents: self-configurable agent architecture for distributed control. In Proceedings 5th International Symposium on Autonomous Decentralized Systems, pages 271277. IEEE, 2001. [14] Stuart Russell and Andrew Zimdars. Q-decomposition for reinforcement learning agents. In Proceedings of the 20th international conference on machine learning (ICML-03), pages 656663, 2003. [15] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. Expel: Llm agents are experiential learners. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1963219642, 2024. [16] Xiang Liu and Daoxiong Gong. comparative study of a-star algorithms for search and rescue in perfect maze. In 2011 international conference on electric information and control engineering, pages 2427. IEEE, 2011. 11 [17] MG Mohanan and Ambuja Salgoankar. survey of robotic motion planning in dynamic environments. Robotics and Autonomous Systems, 100:171185, 2018. [18] Malik Ghallab, Dana Nau, and Paolo Traverso. Automated Planning: theory and practice. Elsevier, 2004. [19] Philippe Baptiste, Philippe Laborie, Claude Le Pape, and Wim Nuijten. Constraint-based scheduling and planning. In Foundations of artificial intelligence, volume 2, pages 761799. Elsevier, 2006. [20] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):11401144, 2018. [21] Mel Vecerik, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin, Bilal Piot, Nicolas Heess, Thomas Rothörl, Thomas Lampe, and Martin Riedmiller. Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards. arXiv preprint arXiv:1707.08817, 2017. [22] Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado Van Hasselt, and David Silver. Distributed prioritized experience replay. arXiv preprint arXiv:1803.00933, 2018. [23] Thanh Thi Nguyen, Ngoc Duy Nguyen, and Saeid Nahavandi. Deep reinforcement learning for multiagent systems: review of challenges, solutions, and applications. IEEE transactions on cybernetics, 50(9):38263839, 2020. [24] Andrei Rusu, Neil Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016. [25] Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Modelbased reinforcement learning for atari. arXiv preprint arXiv:1903.00374, 2019. [26] Thomas Moerland, Joost Broekens, Aske Plaat, Catholijn Jonker, et al. Model-based reinforcement learning: survey. Foundations and Trends in Machine Learning, 16(1):1118, 2023. [27] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei Rusu, Joel Veness, Marc Bellemare, Alex Graves, Martin Riedmiller, Andreas Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529533, 2015. [28] You Li, Xin Hu, Yuan Zhuang, Zhouzheng Gao, Peng Zhang, and Naser El-Sheimy. Deep reinforcement learning (drl): Another perspective for unsupervised wireless localization. ieee internet of things journal, 7(7):62796287, 2019. [29] Jie Zhu, Fengge Wu, and Junsuo Zhao. An overview of the action space for deep reinforcement learning. In Proceedings of the 2021 4th International Conference on Algorithms, Computing and Artificial Intelligence, pages 110, 2021. [30] Shubham Pateria, Budhitama Subagdja, Ah-hwee Tan, and Chai Quek. Hierarchical reinforcement learning: comprehensive survey. ACM Computing Surveys (CSUR), 54(5):135, 2021. [31] Jacob Beck, Risto Vuorio, Evan Zheran Liu, Zheng Xiong, Luisa Zintgraf, Chelsea Finn, and Shimon Whiteson. survey of meta-reinforcement learning. arXiv preprint arXiv:2301.08028, 2023. [32] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pages 11261135. PMLR, 2017. 12 [33] Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better llm-based evaluators through multi-agent debate. arXiv preprint arXiv:2308.07201, 2023. [34] Shyam Sundar Kannan, Vishnunandan LN Venkatesh, and Byung-Cheol Min. Smart-llm: Smart multi-agent robot task planning using large language models. In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1214012147. IEEE, 2024. [35] Chuanneng Sun, Songjun Huang, and Dario Pompili. Llm-based multi-agent reinforcement learning: Current and future directions. arXiv preprint arXiv:2405.11106, 2024. [36] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [37] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. generalist agent. arXiv preprint arXiv:2205.06175, 2022. [38] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. [39] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024. [40] Bill Yuchen Lin, Yicheng Fu, Karina Yang, Faeze Brahman, Shiyu Huang, Chandra Bhagavatula, Prithviraj Ammanabrolu, Yejin Choi, and Xiang Ren. Swiftsage: generative agent with fast and slow thinking for complex interactive tasks. Advances in Neural Information Processing Systems, 36, 2024. [41] Keith Frankish. Dual-process and dual-system theories of reasoning. Philosophy Compass, 5(10):914926, 2010. [42] Daniel Kahneman. Thinking, fast and slow/farrar. Straus and Giroux, 2011. [43] Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. Adaplanner: Adaptive planning from feedback with language models. Advances in Neural Information Processing Systems, 36, 2024. [44] Bernal Jiménez Gutiérrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. Hipporag: Neurobiologically inspired long-term memory for large language models. arXiv preprint arXiv:2405.14831, 2024. [45] Neil Burgess, Eleanor Maguire, and John OKeefe. The human hippocampus and spatial and episodic memory. Neuron, 35(4):625641, 2002. [46] Xiaojun Chen, Shengbin Jia, and Yang Xiang. review: Knowledge reasoning over knowledge graph. Expert systems with applications, 141:112948, 2020. [47] Leslie Pack Kaelbling, Michael Littman, and Anthony Cassandra. Planning and acting in partially observable stochastic domains. Artificial intelligence, 101(1-2):99134, 1998. [48] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [49] Qwen Team. Qwen2.5: party of foundation models, September 2024. [50] Zach Nussbaum, John X. Morris, Brandon Duderstadt, and Andriy Mulyar. Nomic embed: Training reproducible long context text embedder, 2024. [51] Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, et al. Livebench: challenging, contamination-free llm benchmark. arXiv preprint arXiv:2406.19314, 2024. 13 Level Rooms 1 2 3 6 9 9 12 Ingredients to Find 0 1 2 3 Task Difficulty Settings Steps in Recipe 4 6 8 10 Ingredients in Recipe Total Score 1 2 3 4 4 7 10 Scoring Points and Task Failures Scoring Points Collecting specified ingredients Processing ingredients with correct cooker Handling ingredients properly (cut, dice, slice) Repeating heat-based cooking steps causes burning Executing final steps (e.g., \"prepare meal\") Using an incorrect cooking appliance Incorrect processing technique (cut, dice, slice) Exceeding 50 turns without task completion Task Failures Table 3: Task Difficulty Settings, Scoring Points, and Task Failures"
        },
        {
            "title": "A Experiment Settings",
            "content": "A.1 Environment Details Task Settings. We design four progressive difficulty levels with the following characteristics (summarized in Table A.1): Level 1 requires no ingredient search in 6-room environment, while higher levels (2-4) introduce larger environments (9-12 rooms), more required ingredients (1-3), and longer recipes (6-10 steps). All tasks enforce 50-turn limit to prioritize deliberate planning over random exploration. Difficulty Settings. Environments feature three key constraints: (1) Ingredients require knife processing (cut/dice/slice) before cooking, (2) Three cooking methods (grill/fry/roast) require specific appliances (BBQ/stove/oven), (3) Randomized room layouts with door connections requiring explicit open actions. Agent starting positions are randomized per scenario. Scoring Mechanism. As detailed in Table A.1, maximum scores increase with difficulty (4-13 points). Agents earn 1 point per: ingredient collection, correct appliance usage, proper ingredient processing, and final meal preparation. Critical failures include: incorrect appliance/processing selection, repeated heat application causing burns, and turn limit expiration. Game rules are explicitly provided to agents via prompts to focus evaluation on strategic execution rather than knowledge acquisition. Common settings. To prevent large models from obtaining scores through random action generation and to better evaluate their planning capabilities, we impose constraint that each task must be completed within 50 turns. This configuration isolates evaluation to strategic reasoning through three control mechanisms: explicit rule provision, randomized spatial configurations, and time-constrained execution. The sample instructions for constructing games in different difficulty levels are shown below: Level 1: tw - make tw - cooking -- recipe 1 -- take 0 -- go 6 -- open -- cook -- cut -- output ./ game_0_1 . ulx -f -v -- seed 1001 Level 2: tw - make tw - cooking -- recipe 2 -- take 1 -- go 9 -- open -- cook -- cut -- output ./ game_1_1 . ulx -f -v -- seed 1001 Level 3: tw - make tw - cooking -- recipe 3 -- take 2 -- go 9 -- open -- cook -- cut -- output ./ game_2_2 . ulx -f -v -- seed 20002 Level 4: tw - make tw - cooking -- recipe 4 -- take 3 -- go 12 -- open -- cook -- cut -- output ./ game_3_3 . ulx -f -v -- seed 303 A.2 Hyperparameters. In our experiments, STMA utilizes three key hyperparameters: Number of vertices retrieved in the first step of the retrieval algorithm In the first step of the retrieval process, we use cosine similarity to extract 8 vertices. This provides sufficient initial set of vertices for the subsequent K-hop neighbor search. value in the K-hop algorithm: During experiments, we set = 3 for the K-hop algorithm. This choice is based on the complexity of the environment. Given that the task environment contains maximum of 12 rooms, depth of 3 ensures that the model can access at least three-hop neighbors, covering the majority of spatial relationships required for task completion. Record length in the history buffer: We limit the history buffer to store records from the most recent 25 iterations. Since each task is constrained to maximum of 50 iterations, maintaining record of the last 25 iterations is sufficient and helps reduce the input length for the model. This not only minimizes memory usage but also lowers computational costs without compromising task performance."
        },
        {
            "title": "B Additional Experiments",
            "content": "To further illustrate the behavior of our agent in real embodied environments, we select two representative scenarios from all test cases. These scenarios are used to compare the performance of STMA with the best-performing baseline model in the same context. The goal is to highlight the advantages of STMA relative to other baselines within these scenarios. For this case study, all agents are powered by GPT-4o. B.1 Case 1: The Role of the Critic. Environment 1 represents scenario classified under Difficulty Level 2. Among the baseline models evaluated in this setting, Reflexion achieved the highest performance. Consequently, we conducted comparative analysis between Reflexion and the STMA agent, focusing on representative subtask of this environment. The experimental results are illustrated in Figure 5. In this scenario, both Reflexion and STMA successfully explored the kitchen environment and retrieved recipe. The acquired recipe and the agents initial inventory are displayed in the right panel of Figure 5. Both agents executed operations based on the recipe instructions. However, Reflexion failed to complete the task due to selecting an incorrect cooker for ingredient processing at critical step. In contrast, STMA recognized the necessity of using the appropriate cooker and generated logically coherent plan. Notably, the final step of STMAs initial plan proposed by the Planner constituted an invalid action within the Textworld environment. Upon execution, the Critic module identified this discrepancy, prompting the Planner to revise its strategy. After reformulation, the Planner successfully generated valid sequence of operations adhering to procedural constraints. This experiment demonstrates that the collaborative framework between the Critic and Planner in STMA enables the agent to achieve superior long-term planning capabilities while maintaining robustness against errors caused by LLM hallucinations or inherent limitations. Compared to baseline models such as Reflexion, STMA exhibits enhanced planning reliability and adaptability, underscoring the efficacy of its self-corrective mechanism in dynamic task environments. B.2 Case 2: Demonstrating Collaborative Capabilities of Spatio-temporal Memory and Planner-Critic in the STMA Framework. Environment 2 represents Difficulty Level 3 task designed to evaluate the agents capacity to handle intricate operational workflows and manage larger set of required ingredients. Similar to Environment 1, we compare the performance of the STMA framework against Reflexion, the strongest baseline framework. representative subtask is illustrated in Figure 6. In this environment, the agent is initialized in the Garden\" sub-environment, which contains the critical ingredient red 15 Figure 5: STMA versus Reflexion in Case 1. 16 Figure 6: STMA versus Reflexion in Case 2. bell pepper.\" However, since the recipe resides in the Kitchen,\" the agent initially lacks awareness of this ingredients necessity. This configuration evaluates the agents memory retention capability, requiring it to recall the Gardens relevance after discovering the recipe in the Kitchen. As shown in the top-left panel of Figure 6, both Reflexion and STMA successfully navigate to the Kitchen and retrieve the recipe, subsequently recognizing the need to return to the Garden. However, the second-left panel reveals critical divergence: Due to the absence of spatial memory, Reflexion executes an erroneous decision, leading to the exploration of an irrelevant room. In contrast, STMA leverages its spatial memory to formulate an optimal path. Although the final planning step initially contains an error, the Critic module intervenes by analyzing the Spatial Belief state, enabling the Planner to self-correct and select the correct trajectory. This case highlights three key observations regarding the STMA framework. 1) While spatial memory enhances the Planners spatial reasoning, it does not fully mitigate erroneous planninga limitation potentially attributable to large language model hallucinations. 2) The Critic module demonstrates advanced functionality beyond binary action validation, providing actionable suggestions for plan refinement based on Spatial Belief analysis. 3) For Reflexion, we note that despite it do not have spatial memory like STMA, with GPT-4os capabilities, the framework exhibits inferior spatiotemporal reasoning compared to STMA. This performance gap underscores the critical role of explicit spatio-temporal memory integration in complex embodied agent tasks. This case demonstrates the synergistic collaboration among core components within the STMA framework. The temporal memory module maintains chronological records of event sequences, while the spatial memory module reconstructs environmental states. These complementary memory systems establish bidirectional connections with the planner-critic decision-making module through shared spatio-temporal belief representations. The Planner and Critic components collaboratively operate based on these unified beliefs: The Planner formulates subgoals and devises corresponding action plans, while the Critic simultaneously performs real-time validation of plan feasibility through dual criteria - environmental consistency check and temporal obsolescence detection. Upon identifying potential deficiencies, the Critic generates actionable refinements through structured feedback loops. This collaborative mechanism effectively addresses the inherent challenges associated with longhorizon planning tasks by maintaining dynamic alignment between memorialized environmental states, temporal context awareness, and situated action planning."
        },
        {
            "title": "C Implementation Details",
            "content": "C.1 Main Algorithm The overall framework is outlined in Alg. 2. C.2 K-hop After extracting the initial vertices, we utilize K-hop graph search algorithm to perform relationshipbased exploration. This involves retrieving all K-hop neighbors of the selected vertices within the KG. The algorithm enables semantically associated extraction, effectively performing associative reasoning\" through semantic connections. 18 A(Gs) {Relation aggregation}; Record history: {(fi, oi, ai)} {Update history buffer}; Generate temporal belief: bt S(H) {Temporal summarization}; Extract spatial relations: R(bt i) {Relation retriever}; Update knowledge graph: {Dynamic KG maintenance}; Retrieve relevant subgraph: Gs Retrieve(G, oi) {Algorithm 1}; Generate spatial belief: bs Generate plan: {ˆai:k}m for = 1 to do Algorithm 2 STMA Execution Process Require: Environment interface env, initial observation o1, max steps . Ensure: Task completion status. 1: Initialize environment: done False, 1, f1 ; 2: Initialize memory: , ; 3: while and done do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: end if 22: end for 23: 24: end while 25: return done end if Execute action: ai ˆai:k; Observe environment: (oi+1, done, info) env(ai); Update timestep: + 1; if done then break Verify action: (pk, fk) Critic(ˆai:k, bt if pk = False then Update feedback: fi fk; break {Re-planning triggered}; k=1 Planner(bt , oi); , oi); i, bs i, bs Algorithm 3 K-Hop Input: Graph = (V, E), vertex list Vlist, integer K. Initialize subgraph_nodes {Set to store nodes in the subgraph}; for each node in Vlist do if node is in G.V then neighbors single_source_shortest_path_length(G, node, cutoff=K); subgraph_nodes subgraph_nodes keys(neighbors) {Add neighbors within hops to the subgraph}; end if end for Output: subgraph_nodes"
        },
        {
            "title": "D Sample Prompt",
            "content": "D.1 Summarizer ** Prompt :** You are an intelligent agent tasked with analyzing robot interactions with its environment . The robot activity is provided as trajectory consisting of series of ** Observations ** and ** Actions **. Your goal is to summarize the robot history up to the latest point , creating detailed description of what the robot has done so far . This summary will be referred to as the robot **\" belief .\"** ** Trajectory Format :** Observation : [ Details of the first observation ] Action : [ Details of the first action ] Observation : [ Details of the second observation ] Action : [ Details of the second action ] ... Observation : [ Details of the latest observation ] FeedBack : [ Details of the latest feedback ] * Note :* The last iteration includes ** FeedBack ** section instead of an ** Action ** because it represents the most recent observation that the agent has not yet acted upon . ** Instructions :** 1. ** Read the Trajectory :** Carefully review each ** Observation ** and the corresponding ** Action ** taken by the robot in the provided trajectory . 2. ** Analyze Interactions :** For each pair of ** Observation ** and ** Action ** , understand the context and purpose of the robot behavior . Consider how each action responds to the preceding observation . 3. ** Exclude Latest Feedback :** Do not include the ** FeedBack ** section in your summary , as it represents the current state awaiting the robot next action . 4. ** Summarize the History :** Compile comprehensive and detailed summary of the robot actions and interactions based on the analyzed trajectory . Your summary should : - Describe the sequence of actions the robot has performed . - Highlight key behaviors , patterns , and any objectives the robot appears to be pursuing . - Provide insights into the robot understanding and adaptation to its environment . 20 5. ** Format the Summary :** Present the summary in clear , coherent English , ensuring that it accurately reflects the robot historical behavior as derived from the trajectory . 6. Additional Knowledge for domain specific task : - Please First summarize each iteration for reference , then summarize them all . Plases Notice the objects and direction of rooms .! entrance - The cookbook in the kitchen is extremely important . Please record every detail of it . The agent need to follow the instruction from the cookbook !!!! - Please add detailed cookbook record section - Please record all the direction movement action like \" go xxx \". This is essential D.2 Relation Retriever You are an advanced AI agent tasked with extracting only the spatial relationships between objects in passage . Your output should focus solely on spatial positions and directions without referencing actions , events , or other non - spatial information . The relationships between objects or rooms can involve various positional prepositions such as \" on ,\" \" in ,\" \" at ,\" \" in front of ,\" \" behind ,\" and relative directional terms . ** Important :** Please track any changes in the position of items and only capture the latest spatial relationship . Use the sequence of actions to identify the most recent location of each item . For example , if an item like the \" purple potato \" is moved , only its final recorded location should be included in the output . Respond with list of RDF triples structured to include only the most recent spatial relationships between entities . ** Note :** ** Do not include any comments or additional explanations in your output !!!** Provide only the list of RDF triples as specified . Respond with list of RDF triples structured to include only the most recent spatial relationships between entities . Format : Each triple should follow this format : [\" object 1\" , \" relation \" , \" object 2\"] . Use atomic objects for clarity . Output the list in the following format , surrounded by triple backticks : [ [\" object 1\" , \" relation \" , \" object 2\"] , ... ] This will ensure clear representation of only the latest spatial relationships based on the passage provided . ** One - shot sample :** input : Paragraph : began the task with clear directive : to find recipe in the cookbook located in the kitchen and prepare meal . My initial location was the bathroom , room that , despite its familiarity , lacked any items of immediate use . The toilet was present but empty . noted the exits and decided to move north . Upon exiting the bathroom , found myself in corridor , typical passageway with exits to the east , north , and south . checked my inventory and confirmed that was carrying raw purple potato and raw yellow potato . chose to continue north . The next room was bedroom . It contained bed , which was currently unoccupied . noted the exits and proceeded east into the living room . This room , like the others , was standard and contained sofa , which was also empty . considered the exits and decided to go west , returning to the bedroom . In the bedroom , examined the bed and placed the purple potato on it . then placed the yellow potato on the bed as well . With the potatoes stored , moved east back into the living room and then south into the kitchen . The kitchen was more promising . It contained fridge , an oven , massive table with knife , counter with cookbook , and stove . examined the fridge , which was solid and closed , and then the table , which was unstable and held knife . opened the cookbook and read the recipe for dish that required purple potato . The instructions were straightforward : chop the purple potato , roast it , and prepare the meal . With the recipe in mind , returned to the living room and then went west to the bedroom . retrieved the yellow potato from the bed and carried it back to the living room , placing it on the sofa . then returned to the bedroom to retrieve the purple potato . Back in the kitchen , prepared to follow the recipe . took the purple potato and used the knife from the table to chop it . Next , placed the chopped potato on the stove and roasted it according to the cookbook directions . Once the potato was roasted , prepared the meal . With the meal ready , enjoyed the dish , fulfilling the initial directive to cook and eat delicious meal . Entity : [\" bathroom \" , \" corridor \" , \" bedroom \" , \" living room \" , \" kitchen \" , \" toilet \" , \" purple potato \" , \" yellow potato \" , \" sofa \" , \" fridge \" , \" oven \" , \" table \" , \" knife \" , \" counter \" , \" cookbook \" , \" stove \"] output : [ [\" bathroom \" , \" is at north of \" , \" corridor \"] , [\" corridor \" , \" is at south of \" , \" bathroom \"] , [\" corridor \" , \" is at north of \" , \" bedroom \"] , [\" bedroom \" , \" is at south of \" , \" corridor \"] , [\" bedroom \" , \" is at east of \" , \" living room \"] , [\" living room \" , \" is at west of \" , \" bedroom \"] , [\" living room \" , \" is at south of \" , \" kitchen \"] , [\" kitchen \" , \" is at north of \" , \" living room \"] , [\" kitchen \" , \" contains \" , \" fridge \"] , [\" kitchen \" , \" contains \" , \" oven \"] , 22 [\" kitchen \" , \" contains \" , \" table \"] , [\" table \" , \" holds \" , \" knife \"] , [\" kitchen \" , \" contains \" , \" counter \"] , [\" counter \" , \" holds \" , \" cookbook \"] , [\" kitchen \" , \" contains \" , \" stove \"] , [\" sofa \" , \" holds \" , \" yellow potato \"] , [\" stove \" , \" holds \" , \" purple potato \"] ] D.3 Relation Aggregator You are smart agent with excellent spatial reasoning skills . will provide you with list of triples representing spatial relationships between objects . Your task is to reconstruct the environment and describe it in plain text . ** Input :** list of interconnected tuples in the format [ obj1 , relation , obj2 ] . ** Output :** coherent , factual description of the environment based on the given relationships . Each tuple describes key fact about the world , such as object locations or containment relationships . Your output should clearly explain the spatial layout and contents of the environment . ** Sample Input :** [ [\" bathroom \" , \" is at north of \" , \" corridor \"] , [\" corridor \" , \" is at south of \" , \" bathroom \"] , [\" corridor \" , \" is at north of \" , \" bedroom \"] , [\" bedroom \" , \" is at south of \" , \" corridor \"] , [\" bedroom \" , \" is at east of \" , \" living room \"] , [\" living room \" , \" is at west of \" , \" bedroom \"] , [\" living room \" , \" is at south of \" , \" kitchen \"] , [\" kitchen \" , \" is at north of \" , \" living room \"] , [\" kitchen \" , \" contains \" , \" fridge \"] , [\" kitchen \" , \" contains \" , \" oven \"] , [\" kitchen \" , \" contains \" , \" table \"] , [\" table \" , \" holds \" , \" knife \"] , [\" kitchen \" , \" contains \" , \" counter \"] , [\" counter \" , \" holds \" , \" cookbook \"] , [\" kitchen \" , \" contains \" , \" stove \"] , [\" sofa \" , \" holds \" , \" yellow potato \"] , [\" stove \" , \" holds \" , \" purple potato \"] ] ** Sample Output :** The bathroom is located to the north of the corridor , which in turn is to the south of the bathroom . The corridor is also to the north of the bedroom , and the bedroom is to the south of the corridor . The bedroom is situated to the east of the living room , and the living room is to the west of the bedroom . The living room is located to the south of the kitchen , and the kitchen is to the north of the living room . The kitchen contains several items : fridge , an oven , table , counter , and stove . The table holds knife , and the counter 23 holds cookbook . Additionally , the sofa holds yellow potato , and the stove holds purple potato . D.4 Planner ** Role :** You are an advanced AI operating as the central processor of robot designed to interact with and adapt to real - world environments . Your main function is to assist in achieving user - defined goals by analyzing and interpreting environmental data . Please keep in mind the following key points : 1. ** Dynamic Environment **: Each time you take an action , the environment will change . This means that the available actions and their meanings will shift after each step . It crucial that you carefully consider how each current action will affect the environment , and how the available choices and their consequences will evolve after the action is executed . You are only provided with the current set of available actions - your understanding of what they mean should be grounded in the state of the environment at the time of decision - making . 2. ** Plan Validity **: The plan you create is based on the current environment , but the environment is dynamic , and the set of available actions may change significantly during execution . This means your plan might quickly become outdated as you proceed . Always consider that actions may lead to changes in the environment that affect subsequent choices . Before finalizing plan , ensure that you have thought through how each step might alter the available options and their implications . 3. ** Exploration and Experimentation **: We encourage you to embrace exploration and learning through trial and error . Don hesitate to test different approaches and learn from outcomes , as this will help refine your strategies and adapt to the unpredictable nature of real - world environments . 4. ** Only generate action that is proper **: only valid action allowed , ** No if statement action allowed !** ** Your task is :** 1. ** Think through each step in chain of thought ** to determine the next useful subgoal that will help achieve the primary task . 2. ** Based on what you have done and the spatial memory which describe the relationship of the object . Understand the overall environment ( your belief ) .** 3. ** Propose subgoal ** based on the current environment and information , ensuring it contributes to the main goal . 4. ** Plan sequence of actions ** that will complete the proposed subtask , drawing from the list of available actions . 5. ** Ensure ** that the sequence of actions effectively fulfills the subtask and progresses toward the overall goal . 6. ** Fully Consider what spatial memory shows ( This provide you with overall insight of the whole environment ) .** 7. ** Get as much score as you can **: You can get 1 point if you : - 1. use correct action chop / cut / slice the correct ingredient follow the cookbook . - 2. use correct method fry / grill / roast to cook the ingredient follow the cookbook . - 3. collect correct ingredient to your inventory . ** Output Format :** - ** Subgoal :** Describe the specific subtask needed to advance the main goal . - ** Action Plan :** Select and arrange actions in valid sequence that will complete this subtask . 24 ** Thought :** You are suggested to think in this way ( this is just basic thought , you have to think in other way additionally ) : 1. What is your goal ? 2. Base on the Spatio Memory , What situation you are facing ? 3. Base on the What you have done , conclude an overall belief of you and envrionment . 4. What is your next subgoal that can achieve the task ? Why ? - Is this subgoal redundent ? - What is the consequences when this subgoal completed ? - Whether this subgoal fit ** Every command ** in your Knowledge . 5. Re - think whether it is good subgoal . You can modify it . 6. What is your plan to achieve the subgoal ? 7. Please go through ** each step ** in your plan ( each step is selected_action ) , consider : - what may be the consequences of the action ? - Is this proper action ? - Is this useless or redundent action ? - Whether this action fit ** Every command ** in your Knowledge . 8. As the environment change during your plan execution , please consider possible situation ( including environment and possible actions ) after each step executed . ... ( more thought ) ** Replan **: after go thought the thought step above , please think and review your previous plan , modify your plan base on the thought . This plan is your final plan . Use the following YAML format : Thought : 1. ... 2. ... 3. ... 4. ... 5. ... 6. ... ... Plan : YAML Subgoal : \"...\" # The subgoal has to be detailed , including what you want to achieve , your strategies to achieve it . ( and your plan action sequence in natural language ) Action Plan : - \" < selected_action 1 >\" - \" < selected_action 2 >\" ... - \" < selected_action >\" D.5 Critic You are highly advanced language model tasked with critically evaluating the suitability of robot proposed next action given its assigned subgoal , summarized execution history , and current environment . Your evaluation must carefully consider the following : 1. Dynamic Environment Awareness : Each action executed by the robot alters the environment . This means that the observations available , the significance of current candidate actions , and the 25 consequences of executing an action will change with every step . When reasoning about the proposed action , reflect on its immediate significance in the current context and anticipate how the meaning and availability of future actions might evolve after execution . Your feedback should explicitly address this dynamic nature of the environment . 2. Encouragement of Exploration : While ensuring actions align with the subgoal and contribute meaningfully toward its completion , we also encourage the robot to explore and experiment . Exploration and trial - and - error are integral to discovering effective strategies , even if some actions may not yield immediate results . Highlight in your feedback how exploration may be beneficial in the current situation . Your evaluation should provide clear and reasoned analysis of the proposed action in the context of the subgoal and environment . Conclude with actionable feedback that integrates both the dynamic nature of the environment and the value of exploration . You will be given the following information : 1. Robot Subgoal : clear description of the specific subgoal the robot is currently working toward . 2. Execution History : summarized report detailing the robot actions up to this point . 3. ** spatial memory **: This memory record spatial relationship you have discoverd . Please Fully Consider what spatial memory shows ( This provide you with overall insight of the whole environment ) .* 4. Knowledge : This is the most important part in your input . The Knowledge part will tell you how to achieve the goal ! 5. Current Environment : The robot current environment , showing details of its surroundings , relevant objects , and available paths . 6. All possible action : All possible action you can choose in this situation . Please judge the proposed action based on possible action choice . 7. Proposed Action : The action the robot is considering but has not yet executed . Your responsibilities : 1. Reasoning : - Base on the Spatio Memory , What situation you are facing ? - Base on the What you have done , conclude an overall belief of envrionment . - Analyze whether the subgoal is suitable according to the final task and current circumstances . ( If not , you should told planner change the subgoal ) - Analyze the subgoal , execution history , current environment , and proposed action . - Carefully assess whether the action is suitable and supports progress toward the subgoal . - Consider the environmental context and prior actions to judge if the proposed action is logical and beneficial . - Consider whether this action is valid ( in the possible action list ) if not , please strictly reject the action . - Whether the movement fit ** Every command ** in your Knowledge . 2. Action Suitability : Explicitly indicate if the proposed action is suitable by selecting either True ( action is relevant and beneficial ) or False ( action is unsuitable or potentially counterproduc tive ) . 3. Constructive Feedback : Provide detailed explanation if the action is unsuitable , covering : - Why it was not suitable . 26 - The key problem with the action in relation to the current environment and subgoal . - How to improve : Provide specific , actionable recommendations to adjust the robot approach or action . - Any additional observations to help the robot advance toward the subgoal ( or just suggest it to change the subgoal ) . - Please Encourage Exploration , Give less False prediction . Output Format Requirements : 1. Internal Reasoning : Before providing the output , use Chain of Thought approach to reason through your decision based on the subgoal , current state , and action suitability . 2. Structured Output : Follow this format : YAML Action Suitability : True / False Feedback : \" Detailed explanation covering the reasons for suitability or unsuitability , key issues , actionable improvements , and additional insights for achieving the subgoal .\""
        }
    ],
    "affiliations": [
        "FNii-Shenzhen, The Chinese University of Hong Kong, Shenzhen, China",
        "Harbin Engineering University, China",
        "Infused Synapse AI"
    ]
}