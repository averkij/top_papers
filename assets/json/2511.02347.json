{
    "paper_title": "LTD-Bench: Evaluating Large Language Models by Letting Them Draw",
    "authors": [
        "Liuhao Lin",
        "Ke Li",
        "Zihan Xu",
        "Yuchen Shi",
        "Yulei Qin",
        "Yan Zhang",
        "Xing Sun",
        "Rongrong Ji"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current evaluation paradigms for large language models (LLMs) represent a critical blind spot in AI research--relying on opaque numerical metrics that conceal fundamental limitations in spatial reasoning while providing no intuitive understanding of model capabilities. This deficiency creates a dangerous disconnect between reported performance and practical abilities, particularly for applications requiring physical world understanding. We introduce LTD-Bench, a breakthrough benchmark that transforms LLM evaluation from abstract scores to directly observable visual outputs by requiring models to generate drawings through dot matrices or executable code. This approach makes spatial reasoning limitations immediately apparent even to non-experts, bridging the fundamental gap between statistical performance and intuitive assessment. LTD-Bench implements a comprehensive methodology with complementary generation tasks (testing spatial imagination) and recognition tasks (assessing spatial perception) across three progressively challenging difficulty levels, methodically evaluating both directions of the critical language-spatial mapping. Our extensive experiments with state-of-the-art models expose an alarming capability gap: even LLMs achieving impressive results on traditional benchmarks demonstrate profound deficiencies in establishing bidirectional mappings between language and spatial concept--a fundamental limitation that undermines their potential as genuine world models. Furthermore, LTD-Bench's visual outputs enable powerful diagnostic analysis, offering a potential approach to investigate model similarity."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 7 4 3 2 0 . 1 1 5 2 : r LTD-Bench LTD-Bench: Evaluating Large Language Models by Letting Them Draw Youtu-Agent Team Current evaluation paradigms for large language models (LLMs) represent critical blind spot in AI researchrelying on opaque numerical metrics that conceal fundamental limitations in spatial reasoning while providing no intuitive understanding of model capabilities. This deficiency creates dangerous disconnect between reported performance and practical abilities, particularly for applications requiring physical world understanding. We introduce LTD-Bench, breakthrough benchmark that transforms LLM evaluation from abstract scores to directly observable visual outputs by requiring models to generate drawings through dot matrices or executable code. This approach makes spatial reasoning limitations immediately apparent even to non-experts, bridging the fundamental gap between statistical performance and intuitive assessment. LTD-Bench implements comprehensive methodology with complementary generation tasks (testing spatial imagination) and recognition tasks (assessing spatial perception) across three progressively challenging difficulty levels, methodically evaluating both directions of the critical language-spatial mapping. Our extensive experiments with state-of-the-art models expose an alarming capability gap: even LLMs achieving impressive results on traditional benchmarks demonstrate profound deficiencies in establishing bidirectional mappings between language and spatial conceptsa fundamental limitation that undermines their potential as genuine world models. Furthermore, LTD-Benchs visual outputs enable powerful diagnostic analysis, offering potential approach to investigate model similarity. Date: November 4, 2025 Code: https://github.com/walktaster/LTD-Bench Data: https://huggingface.co/datasets/walktaster/LTD_Bench"
        },
        {
            "title": "1 Introduction",
            "content": "Large language models (LLMs) have demonstrated remarkable progress in recent years [Wei et al., 2022][Wang et al., 2022][Besta et al., 2024], achieving impressive results on numerous benchmarks spanning language understanding [Hendrycks et al., 2020][Lin et al., 2021], mathematical reasoning [Cobbe et al., 2021][Hendrycks et al., 2021][Clark et al., 2018][Chollet et al., 2024], code generation [Chen et al., 2021][Austin et al., 2021] and instruction following [Zhou et al., 2023][Jiang et al., 2023][Qin et al., 2024]. However, these apparent successes mask critical blind spot: current evaluation paradigms, which rely heavily on aggregate scores and opaque metrics, offer limited insight into models true capabilitiesparticularly their understanding of the physical world. This disconnect is especially concerning as LLMs are increasingly deployed in domains such as robotics, autonomous systems, and design tools [Zeng et al., 2023][Wang et al., 2023][Yao et al., 2023][Shinn et al., 2023], where spatial reasoning is essential. What makes this problem particularly pernicious is the abstract nature of traditional evaluation. When model scores 85% on benchmark, what specific capabilities and limitations does this number reveal? How can researchers, developers, and end-users gain an intuitive understanding of what the model can and cannot do in spatial domains? These questions remain largely unanswered by current methodologies. *Full author list in contributions. 1 LTD-Bench (a) Easy Level (b) Normal Level (c) Hard Level Figure 1. The data examples of three levels in LTD-Bench. The model outputs in the generation tasks have all been rendered into images. To address this gap, we introduce LTD-Bench (Let Them Draw Benchmark), novel evaluation framework that shifts LLM assessment from abstract numerical scores to directly observable visual outputs. Figure 1 shows the data examples of LTD-Bench. Unlike conventional benchmarks, LTD-Bench requires models to generate visual artifactseither as dot matrices or executable codebased on textual instructions, making their spatial reasoning abilities immediately apparent even to non-experts. This approach bridges the disconnect between statistical metrics and intuitive understanding of model capabilities. LTD-Bench comprises two complementary evaluation paths: generation tasks, which assess spatial imagination by requiring models to translate textual descriptions into visual representations, and recognition tasks, which evaluate spatial perception by asking models to interpret visual patterns. These tasks span three progressively challenging levels, from basic character representation to complex real-world object visualization, enabling fine-grained analysis of spatial reasoning abilities. Our experiments with state-of-the-art LLMs reveal significant capability gap overlooked by existing benchmarks. Even models that perform well on traditional reasoning tasks exhibit profound deficiencies in mapping between language and spatial concepts, undermining their potential as genuine world models. Furthermore, LTD-Benchs visual outputs facilitate diagnostic analyses not possible with traditional benchmarks, such as comparing stylistic characteristics of generated images to investigate model similarities. By making model limitations visible rather than obscured behind abstract metrics, LTD-Bench represents paradigm shift in the evaluation and understanding of large language models. Our framework lays the foundation for developing AI systems with more robust spatial reasoninga critical requirement for applications that must interact with and reason about the physical world. Our contributions are summarized as follows: We introduce LTD-Bench, the first benchmark that transforms LLM evaluation from opaque metrics 2 LTD-Bench to visually interpretable outputs. By requiring models to generate visual artifacts through drawing, we enable direct human assessment of spatial reasoning capabilities, addressing fundamental gap between statistical performance and intuitive understanding of model limitations. We design structured evaluation methodology with complementary generation and recognition tasks across three difficulty levels, providing comprehensive assessment of both how LLMs translate language into spatial arrangements (imagination) and interpret spatial patterns into language (perception). Our experimental results quantify significant capability gap in current LLMs, showing that even models with strong reasoning abilities struggle to establish the bidirectional mapping between language and spatial concepts - critical finding that identifies priority direction for improvement in the next generation of AI systems. We demonstrate how visual output comparison provides powerful diagnostic tool for model development, revealing stylistic similarities among various models and offering insights into the model similarity that is not well captured by traditional evaluation metrics."
        },
        {
            "title": "2 Related Work and Discussion",
            "content": "Existing Benchmarks. Current LLM evaluation frameworks primarily emphasize symbolic and procedural competencies. Comprehensive benchmarks such as MMLU [Hendrycks et al., 2020] and TruthfulQA [Lin et al., 2021] assess cross-domain knowledge retention and factual accuracy, while mathematical reasoning datasets like GSM8K [Cobbe et al., 2021] and MATH [Hendrycks et al., 2021] focus on multi-step problemsolving in abstract domains. Code generation benchmarks (e.g., HumanEval [Chen et al., 2021], MBPP [Austin et al., 2021]) further evaluate the translation of natural language into executable algorithms. While these benchmarks effectively quantify core symbolic manipulation skills, they are confined to text-tosymbol paradigm and lack intuitive, visual, and directly interpretable assessments of model capabilities. Consequently, they do not reveal whether LLMs can establish robust, bidirectional mappings between linguistic symbols and spatial entities. Spatial Perception and Imagination. The lack of spatial evaluation in LLMs is partly rooted in the assumption that visual perception is essential for spatial reasoning. However, neurocognitive studies of congenitally blind individuals demonstrate that robust spatial cognition can arise through nonvisual modalities, such as linguistic descriptions and haptic feedback. For instance, Striem-Amit et al. [2018] provide evidence for neural dissociation between abstract semantic knowledge and sensory attributes, while Cooney et al. [2024] show that spatial reasoning relies on innate neural mechanisms rather than visual experience. These findings challenge the necessity of visual input for spatial reasoning and establish biological precedent for text-based spatial cognition. This suggests that text-based LLMs, even without visual input, should be capable of developing spatial understanding. Moreover, Transformer architectures, which underpin modern LLMs, excel at modeling relationships between abstract tokens, theoretically enabling the inference of geometric and topological patterns from textual descriptions. Recent work supports this potential: LLMs have demonstrated the ability to generate code for rendering simple shapes [Achiam et al., 2023][Gupta and Kembhavi, 2023], indicating latent spatial reasoning capabilities. Nevertheless, no existing benchmark systematically evaluates these abilities, despite their importance for meaningful interaction with the physical world. Intuitive Visual Evaluation. LTD-Bench addresses this gap by providing an intuitive and visual assessment of LLMs spatial perception and imagination. In generation tasks, models are required to produce either 3 LTD-Bench renderable dot matrices or Python code for image drawing, both of which can be visualized as images. Prior research has shown that prompting LLMs for evaluation is effective not only in NLP tasks [Zheng et al., 2023][Chiang and Lee, 2023][Liu et al., 2023][Fu et al., 2023] but also in multimodal domains [Zheng et al., 2025][Yu et al., 2023]. Accordingly, LTD-Bench leverages GPT-4.1 to assess the quality of images generated by LLMs in certain open-ended generative tasks, enabling more direct and interpretable evaluation of spatial reasoning abilities."
        },
        {
            "title": "3 LTD-Bench",
            "content": "The gap between reported LLM performance and actual spatial reasoning capabilities represents critical blind spot in AI evaluation. LTD-Bench addresses this gap through novel approach that transforms abstract metrics into directly observable visual evidence, enabling intuitive assessment of how well models can establish bidirectional mappings between language and spatial concepts."
        },
        {
            "title": "3.1 Design Principles and Problem Addressing",
            "content": "LTD-Bench addresses three fundamental problems in current LLM evaluation approaches through carefully designed principles: Problem 1: Invisibility of Spatial Reasoning Limitations. Current benchmarks provide numerical scores that obscure whether models can actually establish bidirectional mappings between language and spatial concepts. Solution 1: Visual Interpretability. LTD-Benchs core innovation is its transformation of abstract model capabilities into concrete visual artifacts. All outputs from generation tasks are rendered into images, enabling direct inspection by both humans and automated systems. This approach makes model limitations immediately apparent to anyone - regardless of technical background - revealing capabilities that remain hidden in traditional benchmarks. Problem 2: Incomplete Assessment of Spatial Cognition. Existing evaluations rarely assess both directions of the critical language-spatial mapping. Solution 2: Dual-Path Evaluation. LTD-Bench systematically evaluates both aspects of spatial cognition through complementary pathways: Generation Tasks (Spatial Imagination): Models translate textual descriptions into visual representations (dot matrices or drawing code), testing their ability to convert linguistic concepts into spatial arrangements. Recognition Tasks (Spatial Perception): Models interpret visual patterns from given representations, testing their ability to understand spatial configurations through language. Problem 3: Inability to Pinpoint Capability Thresholds. Traditional benchmarks often fail to identify precisely where models begin to struggle with increasingly complex spatial reasoning. Solution 3: Progressive Complexity. LTD-Bench implements hierarchical structure with three difficulty levels: 4 LTD-Bench Table 1. The structure of LTD-Bench Level Easy Normal Hard Total Task Generation Recognition Total 50 36 25 111 36 36 - 72 86 72 25 1. Easy Level: Basic character representation using discrete dot matrices in finite grid space, establishing baseline spatial capabilities. 2. Normal Level: Character drawing using continuous curves in infinite coordinate space, requiring more sophisticated spatial reasoning. 3. Hard Level: Complex real-world object representation, requiring advanced spatial conceptualization and compositional understanding. Through these design principles, LTD-Bench not only evaluates model performance but fundamentally transforms how we understand and interpret model capabilities, making limitations visible that were previously hidden behind abstract metrics."
        },
        {
            "title": "3.2 Benchmark Structure and Task Design",
            "content": "Based on the design principles outlined in Section 3.1, LTD-Bench comprises comprehensive evaluation framework with 183 distinct data distributed across three difficulty levels, as summarized in Table 1. Each level presents unique challenges designed to assess specific aspects of spatial reasoning. 3.2.1 Easy Level: Discrete Grid-Based Spatial Understanding The Easy level is designed to assess the fundamental spatial abilities of LLMs within two-dimensional finite grid space represented in the form of dot matrix. In accordance with the dual-path evaluation principle, this level consists of both generation and recognition tasks: Generation Task: Given textual instruction (e.g. \"Please draw character in 0-1 matrix with 3 rows and 3 columns.\"), the model is required to output dot matrix, where 1 denotes filled cell and 0 denotes blank cell, representing the specified character. The output dot matrix can be rendered into grid-like image for intuitive and visual display. This task evaluates whether models can: Conceptualize the spatial arrangement of simple characters Translate this conceptualization into precise grid-based representation Maintain correct proportions and spatial relationships within constraints 5 LTD-Bench Recognition Task: In the complementary recognition pathway, models are presented with dot matrix and must identify which character it represents. This evaluates the reverse mappingfrom spatial arrangement to symbolic representationtesting models ability to interpret visual patterns expressed as matrices. As shown in Figure 1(a), the outputs of generation tasks at this level can be directly verified through visual inspection, while recognition tasks have unambiguous ground-truth answers. This level establishes whether models possess even the most basic capacity for bidirectional mapping between language and discrete spatial representations. 3.2.2 Normal Level: Curve Composition in Infinite 2D Coordinate Space The Normal level increases complexity by transitioning from discrete grid spaces to continuous coordinate systems, requiring models to reason about characters as compositions of mathematical curves in an unbounded space. Generation Task: The model is tasked with generating Python code that draws specified characters (such as letters or digits) using only combinations of curves. Prompts are designed to enforce the constraint that only curve-based drawing methods are allowed, explicitly prohibiting the use of direct text rendering functions (e.g., TextPrint). The generated Python code can be executed directly to produce images, enabling an intuitive and visual evaluation of the correctness of the models drawings. This level evaluates whether models can: Translate character concepts into continuous rather than discrete representations Generate executable code that correctly implements spatial understanding Create visual outputs that maintain character recognizability despite implementation constraints Recognition Task: For recognition, models are presented with Python code that draws character through curve combinations and must identify which character the code would render. This requires parsing code, understanding how mathematical functions translate to visual shapes, and recognizing the resulting pattern. As illustrated in Figure 1(b), this level significantly increases the complexity of the bidirectional mapping between language and spatial concepts, requiring models to operate in continuous rather than discrete space and to translate between linguistic, mathematical and visual representations. 3.2.3 Hard Level: Real-world Object Drawing in Infinite 2D Space The Hard level represents the most advanced spatial reasoning challenge, requiring models to conceptualize and render complex real-world objects as compositions of multiple curves. Generation Task: Models receive open-ended instructions to draw real-world objects with specific attributes, such as \"Draw cat with pointed ears, long whiskers and round eyes.\" This requires not only understanding what these objects look like but also decomposing them into geometric primitives and implementing them through code. This level evaluates whether models can: 6 LTD-Bench Conceptualize complex multi-part objects from linguistic descriptions Translate abstract object features into concrete spatial relationships Generate code that produces coherent and recognizable visual representation Evaluation Approach. Due to the open-ended and subjective nature of these tasks, evaluation at this level employs GPT-4.1 as an automated assessor following established practices in LLM-based evaluation [Yu et al., 2023][Zheng et al., 2023][Chiang and Lee, 2023][Liu et al., 2023][Fu et al., 2023]. The evaluation protocol assigns scores between 0.0 and 1.0 based on predefined criteria that consider both adherence to specified features and overall visual coherence. Additionally, as shown in Figure 1(c), the stylistic diversity of outputs at this level enables comparative analysis between different model architectures. By examining stylistic similarities in generated images, researchers can identify shared tendencies across model families, providing insights into the similarity among various LLMs that is not captured by traditional evaluation metrics. Together, these three levels form comprehensive framework for evaluating spatial perception and imagination capabilities in LLMs, making visible the specific strengths and limitations that remain hidden in traditional text-based benchmarks."
        },
        {
            "title": "4.1 Experiment Settings",
            "content": "Models. Since our approach places certain demands on both the reasoning and coding abilities of LLMs, and its tasks are relatively challenging for smaller LLMs, we primarily selected some of the most advanced models for evaluation, including DeepSeek-R1 [Guo et al., 2025], DeepSeek-V3 [Liu et al., 2024], GPT-4o, GPT-4.1-mini, QwQ32B [Team, 2025], Qwen2.5-72B-Instruct [Yang et al., 2024] and Llama3.3-70B-Instruct [Grattafiori et al., 2024]. Evaluation Methods. The evaluation of our benchmark is tailored to the nature of each task type and difficulty level. For generation tasks, which lack fixed ground-truth answers, we employ both human evaluation and GPT-4.1-based automated evaluation at the Easy and Normal levels. An analytical comparison between human and GPT-4.1 evaluations is provided in Appendix A.3. For Hard-level generation tasks, the open-ended nature of the outputs introduces considerable subjectivity into human assessment, as personal aesthetic preferences and other factors may influence scoring. Therefore, we rely exclusively on GPT-4.1 for evaluation at this level, utilizing detailed system prompt with explicit scoring criteria to guide GPT-4.1 in assigning score between 0.0 and 1.0 to each generated image. If models output code fails to execute and does not produce valid image, score of 0 is assigned for that instance. For recognition tasks, which have well-defined ground-truth answers, we directly compare model outputs to the correct answers to compute accuracy. More implementation details are provided in Appendix A.1, and the prompt templates used in LTD-Bench are included in Appendix B. 7 LTD-Bench Table 2. The model output examples on the generation tasks across different difficulty levels in LTD-Bench. All outputs are rendered as images to facilitate an intuitive visual assessment of the models capabilities. Easy Generation Please draw character in 0-1 matrix with 5 rows and 4 columns Normal Generation Draw green letter Hard Generation Draw cat with pointed ears, long whiskers and round eyes Deepseek-r1 Deepseek-v3 GPT-4.1-mini GPT-4o QwQ-32B Qwen2.5-72B-Instruct Llama3.3-70B-Instruct"
        },
        {
            "title": "4.2 Result Analyses",
            "content": "Since the intuitive visual evaluation of LLM capabilities is central contribution of our work, we first present representative model outputs for generation tasks across different difficulty levels in Table 2. These examples clearly illustrate performance disparities among models. For instance, advanced models such as Deepseek-r1 and GPT-4.1-mini significantly outperform smaller models like Qwen2.5-72B-Instruct and Llama3.3-70B-Instruct. Overall performance metrics are summarized in Table 3, from which we draw the following conclusions: LLMs generally exhibit poor spatial perception and imagination. As shown in Table 3, only Deepseek-r1 achieves an average accuracy above 70%, with GPT-4.1-mini exceeding 60%. In contrast, Qwen2.5-72BInstruct and Llama3.3-70B-Instruct achieve only around 30%. Notably, human experts can solve Easy and Normal tasks with near-perfect accuracy, even in text-only settings, whereas LLMs fall far short. These results 8 Table 3. LTD-Bench evaluation results. Bold indicates the best performance in that dimension, while underline indicates the second-best performance. For generation tasks at Easy and Normal level, the data in blue is the results evaluated by human and data in orange is the results evaluated by GPT-4.1. Numbers are presented in % with full score of 100%. LTD-Bench Model Deepseek-r1 Deepseek-v3 GPT-4.1-mini GPT-4o QwQ-32B Easy Normal Hard Generation Recognition Generation Recognition Generation 82.00 (80.00 / 84.00) 72.00 (66.00 / 78.00) 85.00 (82.00 / 88.00) 81.00 (76.00 / 86.00) 65.00 (58.00 / 72.00) 69. 36.11 38.89 41.67 36.11 13.89 11. 65.28 (55.56 / 75.00) 54.17 (47.22 / 61.11) 70.83 (66.67 / 75.00) 45.83 (36.11 / 55.56) 38.89 (33.33 / 44.44) 18.06 (13.89 / 22.22) 23.61 (16.67 / 30.56) 77.78 63.89 55.56 44.44 58. 25.00 19.44 63.20 66.40 71.60 48. 42.00 40.80 35.20 Average 71.54 58. 64.38 52.19 48.07 30.75 27.07 Qwen2.5-72B-Instruct 56.00 (42.00 / 70.00) Llama3.3-70B-Instruct 46.00 (32.00 / 60.00) indicate that current LLMs lack robust spatial reasoning and fail to establish reliable bidirectional mappings between linguistic symbols and spatial entitiesan essential capability for genuine world comprehension. Further analysis is provided in Appendix C. Deep reasoning improves recognition but not generation tasks. Table 4 shows that Deepseek-r1, equipped with deep reasoning, outperforms GPT-4.1-mini by over 25% in recognition accuracy, but lags behind in generation tasks. Similarly, within the Deepseek family, Deepseek-r1 consistently surpasses Deepseek-v3, yet the relative improvement is much less pronounced for generation than for recognition. QwQ-32B, another model with deep reasoning, matches GPT-4.1-mini on recognition but underperforms on generation. We hypothesize that recognition tasks benefit from enhanced spatial perception via reasoning, while generation tasks, which rely more on spatial imagination, are less amenable to such improvements. Table 5 further supports this: Llama3.3-70B distilled with Deepseek-r1 data shows an 18.05% accuracy gain on recognition tasks, but even 2.91% decline on generation tasks. This suggests that deep reasoning may lead to overthinking in tasks where LLMs inherent abilities are insufficient, potentially degrading performance. Multimodal LLMs do not show clear advantages on text-based spatial tasks. Contrary to human intuitionwhere visual experience enhances spatial abilitiesmultimodal models like GPT-4.1-mini and GPT-4o do not consistently outperform text-only models such as Deepseek-r1 and Deepseek-v3 on LTDBench (Tables 3 and 4). While GPT-4.1-mini excels in generation tasks, its overall performance is still lower than Deepseek-r1, and GPT-4o underperforms compared to Deepseek-v3. Although these results may be influenced by differences in language modeling capabilities, they challenge expectations based on human cognition and highlight the need for further research on aligning visual and textual features in multimodal learning."
        },
        {
            "title": "4.3 Model Similarity",
            "content": "For Hard-level generation tasks, different models tend to produce images with distinct stylistic characteristics. Analyzing the stylistic similarity among these images provides promising avenue for investigating model similarity. As shown in Table 6, we conduct style similarity comparison among three models: Qwen2.59 Table 4. Model performance on generation and recognition tasks. Bold indicates the best performance in that dimension, while underline indicates the second-best performance. LTD-Bench Model Generation Recognition Average Deepseek-r1 Deepseek-v3 GPT-4.1-mini GPT-4o QwQ-32B Qwen2.5-72B-Instruct Llama3.3-70B-Instruct 72.88 64.71 77.46 62.07 51.33 39.17 36.62 73.61 50.00 47.22 43.06 47.22 19.44 15.28 73.24 57.36 62.34 52.56 49.28 29.31 25.95 Table 5. Comparison of the performance of Llama3.3-70B-Instruct and Deepseek -r1-distill-Llama3.3-70B on generation and recognition tasks. Model Generation Recognition Average Llama3.3-70B-Instruct Deepseek-r1-distill-Llama3.3-70B 36.62 33.71 2.91 15.28 33.33 18.05 25.95 33.52 7.57 72B-Instruct, Qwen2.5-32B-Instruct, and GPT-4.1-mini. The results indicate that the highest proportion of stylistically similar images occurs between the two models from the Qwen2.5 series, accounting for over 50% of the total valid samples. In contrast, only three samples were found to be more similar to GPT-4.1-mini. Table 2 further presents representative examples, offering more intuitive illustration of the stylistic similarities among images generated by the three models. It is evident from these examples that the Qwen2.5 series models exhibit greater stylistic resemblance to each other. This suggests positive correlation between model similarity and the stylistic similarity of the open-ended images they generate. Through this exploratory study, we identify potential new approach for assessing model similarity, which may serve as valuable reference for future research in this area."
        },
        {
            "title": "5 Limitations and Future Work",
            "content": "While LTD-Bench offers an intuitive and visual framework for evaluating LLM capabilities, several limitations remain. First, the current benchmark features relatively small dataset and assesses narrow range of abilities, focusing solely on spatial perception and imagination. This limits both the comprehensiveness and generalizability of our findings. Future work will expand the dataset and incorporate wider array of tasks to enable more thorough evaluation of LLMs. Second, our analysis of model similarity is preliminary, relying only on stylistic comparisons of generated images as proxy. More systematic and quantitative approaches are needed to rigorously assess similarities Table 6. Style similarity comparison on the generation task at Hard level. (1) is Qwen2.5-72B-Instruct, (2) is Qwen2.532B-Instruct and (3) is GPT-4.1-mini. The total sample size is 22, excluding the images that failed to be generated. (1) and (2) more similar (1) and (3) more similar (2) and (3) more similar All three are different Rate 12/22 1/22 2/22 7/ 10 Table 7. The specific image examples generated by Qwen2.5-72B-Instruct, Qwen2.5-32B-Instruct and GPT-4.1-mini. Qwen2.5-72B-Instruct Qwen2.5-32B-Instruct GPT-4.1-mini LTD-Bench Flower House Rabbit between models. In future research, we aim to develop more sophisticated metrics and analytical methods to achieve deeper understanding of model similarity."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we present LTD-Bench, novel benchmark that enables intuitive and visual evaluation of LLMs by letting them draw. LTD-Bench specifically targets two fundamental aspects: spatial perception and spatial imagination. Our experimental results demonstrate that current LLMs face substantial challenges in both areas, often failing to establish robust bidirectional mappings between linguistic symbols and spatial concepts. Additionally, LTD-Bench provides promising new avenue for exploring model similarity, offering valuable insights to guide future research in this domain."
        },
        {
            "title": "Contributions",
            "content": "Authors Liuhao Lin1,2* Ke Li1* Zihan Xu1 Yuchen Shi1 Yulei Qin1 Yan Zhang2 Xing Sun1 Rongrong Ji2 Affiliations puting, Ministry of Education of China, Xiamen University 1Tencent Youtu Lab 2Key Laboratory of Multimedia Trusted Perception and Efficient ComEqual Contributions Liuhao Lin Ke Li Acknowledgments Acknowledgments should be placed at the end of the paper, before the bibliography. This section can be used to thank individuals, organizations, or funding sources that contributed to the research. LTD-Bench"
        },
        {
            "title": "References",
            "content": "[1] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [2] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. [3] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: In Proceedings of the AAAI Conference on Solving elaborate problems with large language models. Artificial Intelligence, volume 38, pages 1768217690, 2024. [4] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [5] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021. [6] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [7] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [8] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [9] Francois Chollet, Mike Knoop, Gregory Kamradt, and Bryan Landers. Arc prize 2024: Technical report. arXiv preprint arXiv:2412.04604, 2024. [10] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [11] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [12] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. [13] Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin Jiang, Qun Liu, and Wei Wang. Followbench: multi-level fine-grained constraints following benchmark for large language models. arXiv preprint arXiv:2310.20410, 2023. [14] Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei Liu, Pengfei Liu, and Dong Yu. Infobench: Evaluating instruction following ability in large language models. arXiv preprint arXiv:2401.03601, 2024. [15] Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, and Philip Yu. Large language models for robotics: survey. arXiv preprint arXiv:2311.07226, 2023. 12 LTD-Bench [16] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023. [17] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. [18] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. [19] Ella Striem-Amit, Xiaoying Wang, Yanchao Bi, and Alfonso Caramazza. Neural representation of visual concepts in people born blind. Nature communications, 9(1):5250, 2018. [20] Sarah Cooney, Corinne Holmes, Giulia Cappagli, Elena Cocchi, Monica Gori, and Fiona Newell. Express: Susceptibility to spatial illusions does not depend on visual experience: evidence from sighted and blind children. Quarterly Journal of Experimental Psychology, page 17470218251336082, 2024. [21] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [22] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1495314962, 2023. [23] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. [24] Cheng-Han Chiang and Hung-yi Lee. Can large language models be an alternative to human evaluations? arXiv preprint arXiv:2305.01937, 2023. [25] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023. [26] Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166, 2023. [27] Chanjin Zheng, Zengyi Yu, Yilin Jiang, Mingzi Zhang, Xunuo Lu, Jing Jin, and Liteng Gao. Artmentor: Ai-assisted evaluation of artworks to explore multimodal large language models capabilities. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, pages 118, 2025. [28] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. [29] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [30] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [31] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https: //qwenlm.github.io/blog/qwq-32b/. LTD-Bench [32] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [33] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 14 LTD-Bench"
        },
        {
            "title": "A Experiment details",
            "content": "A."
        },
        {
            "title": "Implementation details",
            "content": "For open-source LLMs used in this paper, we utilized the API on the Bailian1 platform, which hosts models identical to those on HuggingFace. For GPT-4.1, GPT-4.1-mini and GPT-4o, we use their official API2. In our experiments, we set the temperature to 0 for all models. During GPT-4.1-based automated evaluation, it is found that the outputs of GPT-4.1 still exist variance, although the temperature is set as 0. Therefore, we utlize GPT-4.1 to evaluate the outputs of all models by 5 times. A.2 More details of human evaluation For human evaluation, we assigned independent annotators to tasks of different difficulty levels, with 10 annotators dedicated to each level. The annotators cover diverse range of technical backgrounds, including newly enrolled undergraduates (computer beginners), masters students, and laboratory engineersensuring evaluations are not biased toward single expertise group. For each difficulty level, the final experimental result is determined by averaging the scores from the 10 annotators, which helps mitigate individual subjectivity. A.3 Additional evaluation methods experiment As mentioned in Section 4.1, we employ both human evaluation and GPT-4.1-based automated evaluation for generation tasks at the Easy and Normal levels. Human evaluation provides more accurate and reliable results, as it avoids the hallucinations and occasional misjudgment issues that GPT-4.1 may suffer from. However, human evaluation is labor-intensive and costly, requiring manual inspection of each sample. In contrast, GPT-4.1-based evaluation offers convenient and fully automated approach, enabling rapid, end-toend computation of accuracy metrics. For further analysis, we conduct an additional experiment with both evaluation methods. As shown in Figure 2, although GPT-4.1 tends to yield slightly higher accuracy scores due to occasional hallucinations, our experiment confirms that the relative ranking of model performance remains consistent between human and GPT-4.1 evaluations. Therefore, GPT-4.1 can be reliably used for large-scale, automatic evaluation, while human evaluation serves as more precise but resource-intensive reference. Figure 2. Comparison of human evaluation results and GPT-4.1-based automated evaluation results. 1https://bailian.console.aliyun.com/ 2https://platform.openai.com/"
        },
        {
            "title": "B Prompt templates",
            "content": "We list all prompt templates used in LTD-Bench here. LTD-Bench You are dot matrix drawing robot. will ask you to draw specified character on 0-1 matrix with specified number of rows and columns. Requirements: 1. You need to draw the specified character on 0-1 matrix with specified number of rows and columns by setting the elements to 1; 2. Please strictly follow the following format to output the 0-1 matrix you drew in <Mat></Mat>: <Mat> mat = [] </Mat> Here is the question: question Figure 3. The prompt for the the Easy-level generation task. The matrix mat given below is dot matrix representation of certain character. Please identify which character it is and fill the answer in the . Here is the matrix: {matrix} Here is your answer: Figure 4. The prompt for the Easy-level recognition task. You are code generation robot. You need to generate runnable Python code based on the drawing requirements provided by the user to create the image the user needs. Requirements: 1. Draw the pattern required by the user in two-dimensional coordinate system, ensuring that the axes are hidden at the end, and do not use the Text or TextPath functions directly for drawing 2. The generated image should be saved as \"test.jpg\" 3. Please output in the following format, filling in the generated Python code within the <Code></Code> tags, without adding comments at the beginning or end <Code> </Code> Here is the question: {question} Figure 5. The prompt for the Normal-level generation task. 16 LTD-Bench Here is piece of Python code that draws certain letter or number. Please determine what the letter or number is and fill the answer in the . Here is the code: {code} Here is your answer: Figure 6. The prompt for the Normal-level recognition task. You are code generation robot. The user will provide drawing requirements for certain object. You need to generate directly executable Python code according to the drawing requirements to draw the image required by the user. Requirements: 1. First, analyze the basic features of the drawing object. On this basis, according to the additional drawing requirements proposed by the user, sort out all the feature details that need to be drawn, and conceive how to draw it with Python code 2. Then, generate Python code according to your ideas to draw the image required by the user. Pay attention to the correctness of the library function call 3. Save the drawn image as \"test.jpg\" 4. Please output in the following format. Fill in all the features and details you need to draw and your ideas in <Thought></Thought>, and fill in the Python code you generated in <Code></Code>. Do not add comments at the beginning and end <Thought> </Thought> <Code> </Code> Here is the question: {question} Figure 7. The prompt for the Hard-level generation task. The following is dot matrix, where 1 represents fill and 0 represents blank. You need to determine whether the character drawn by the dot matrix is ground truth. If so, output [[Yes]], otherwise output [[No]]. Here is the dot matrix: output matrix Figure 8. The prompt for evaluating the model outputs on the Easy-level generation task. Please judge whether the character drawn in the given image is ground truth. If so, output [[Yes]], otherwise output [[No]]. Figure 9. The prompt for evaluating the model outputs on the Normal-level generation task. 17 LTD-Bench {System prompt} You are an evaluation assistant. Please analyze and score the input image according to the given object and drawing requirements. Requirements: 1. First determine whether the image can be identified as the given object, then determine whether the image meets the drawing requirements, and finally score based on the analysis 2. The score range (scoring standard) is: 0.0: The image cannot identify the object at all 0.1: The image can hardly identify the object 0.2: The image is difficult to identify the object 0.3: The image can barely identify the object, but the main features are blurred and do not meet the drawing requirements 0.4: The image can basically identify the object, but does not meet the drawing requirements 0.5: The image can identify the object, but only meets few drawing requirements 0.6: The image can identify the object, but few drawing requirements are not met 0.7: The image can identify the object and basically meets all drawing requirements 0.8: The image can clearly identify the object and fully meets all drawing requirements, but the painting details and overall aesthetics are poor 0.9: The image can clearly identify the object, fully meets all drawing requirements, and the drawing details and overall aesthetics are also excellent 1.0: The image can perfectly identify the object, fully meets all drawing requirements, the details are extremely rich, and the overall effect is excellent 3. Strictly follow the format below to output your analysis and final score <Analysis>***</Analysis> <Score>***</Score> {User prompt} Object: {object} Drawing requirements: {question} Figure 10. The system prompt and user prompt for evaluating model outputs on the Hard-level generation task. You are an impartial judge. Please evaluate which two of the three provided images are most similar in style only. Begin your evaluation by comparing the three images and provide short explanation. Avoid any position biases and ensure that the order in which the images were presented does not influence your decision. After providing your explanation, output your final verdict by strictly following this format: [[A]] if the first and second images are more similar, [[B]] if if the first and third images are more similar, [[C]] if the second and third images are more similar, and [[D]] if all three images have different styles. Figure 11. The prompt for comparing the style similarity of images generated by three different models on the Hard-level generation task. LTD-Bench"
        },
        {
            "title": "C Case study",
            "content": "In this section, we list several failed cases of different models on generation tasks of three difficulty levels, to further analyze the current limitations in model capabilities. Easy level. Table 8 shows the failed cases on the generation task at Easy level. We can observe that models often mistakenly generate the characters > and as their mirrored counterparts < and within the dot matrix, revealing their insufficient understanding of basic spatial orientations such as left-right and up-down. Additionally, the way models render the character in the dot matrix further highlights their limitations in spatial imagination. Normal level. And for the generation task at Normal level, the failed cases shown in Table 9 more clearly reveal the limitations of the models spatial capabilities. When questioned with \"Draw blue letter W\", QwQ-32B produced an image with the same issue observed in the Easy level earlier: the letter was upside down. Other than that, images generated by other models for other questions listed in the table are even more problematic, featuring completely incorrect outputs with numerous chaotic lines. This suggests that the models may not have proper understanding of how their actions correspond to spatial states, resulting in outputs that deviate significantly from the intended results. Such shortcomings are critical obstacles for LLMs in achieving true understanding of the world. Table 8. The failed cases on the Easy-level generation task. Question Model Outputs Deepseek-r1: Deepseek-v3: Please draw character > in 0-1 matrix with 5 rows and 3 columns. Please draw character in 0-1 matrix with 5 rows and 3 columns. Please draw character in 0-1 matrix with 3 rows and 7 columns. Deepseek-r1: QwQ-32B: Deepseek-r1: GPT-4.1-mini: Hard level. Table 10 further shows the failed cases in Hard level. Firstly, the \"Draw clock with...\" case demonstrates that when spatial requirements are introduced, the models tend to perform poorly in easy math and coding tasks. Even advanced models like Deepseek-r1, GPT-4o, and QwQ-32B made mistakes. 19 LTD-Bench Specifically, these models may easily understand what the pointer pointing to 9:30 means, but they often make various errors when asked to translate this understanding into spatial representation. Further more, in the latter two cases, Airplane and Leaf, it is even more apparent that the models limited spatial imagination, combined with their inadequate ability to map linguistic symbols to spatial entities, leads to these unsatisfactory results. Through the failed cases and analyses above, we provide clear and intuitive visualization of the current models significant shortcomings in spatial capabilities. These findings offer valuable insights for future research on how large language models understand the world. Table 9. The failed cases on the Normal-level generation task. Question Model Outputs GPT-4.1-mini: GPT-4o: Draw purple number 9. Draw blue letter W. Draw purple letter J. Deepseek-v3: QwQ-32B: Deepseek-v3: GPT-4.1-mini: 20 LTD-Bench Table 10. The failed cases on the Hard-level generation task. Question Model Outputs Deepseek-r1: GPT-4o: QwQ-32B: Draw clock with round face and the pointer pointing to 9:30 Draw plane, from birds-eye view, with two wings and two tails, and the wings should be much longer than the tail Draw leaf with veins and irregular jagged edges Deepseek-r1: GPT-4.1-mini: QwQ-32B: GPT-4.1-mini: QwQ-32B: Qwen2.5-72B-Instruct:"
        }
    ],
    "affiliations": [
        "Youtu-Agent Team"
    ]
}