{
    "paper_title": "TLB-VFI: Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation",
    "authors": [
        "Zonglin Lyu",
        "Chen Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video Frame Interpolation (VFI) aims to predict the intermediate frame $I_n$ (we use n to denote time in videos to avoid notation overload with the timestep $t$ in diffusion models) based on two consecutive neighboring frames $I_0$ and $I_1$. Recent approaches apply diffusion models (both image-based and video-based) in this task and achieve strong performance. However, image-based diffusion models are unable to extract temporal information and are relatively inefficient compared to non-diffusion methods. Video-based diffusion models can extract temporal information, but they are too large in terms of training scale, model size, and inference time. To mitigate the above issues, we propose Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation (TLB-VFI), an efficient video-based diffusion model. By extracting rich temporal information from video inputs through our proposed 3D-wavelet gating and temporal-aware autoencoder, our method achieves 20% improvement in FID on the most challenging datasets over recent SOTA of image-based diffusion models. Meanwhile, due to the existence of rich temporal information, our method achieves strong performance while having 3times fewer parameters. Such a parameter reduction results in 2.3x speed up. By incorporating optical flow guidance, our method requires 9000x less training data and achieves over 20x fewer parameters than video-based diffusion models. Codes and results are available at our project page: https://zonglinl.github.io/tlbvfi_page."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 4 8 9 4 0 . 7 0 5 2 : r TLB-VFI: Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation Zonglin Lyu Chen Chen Center for Research in Computer Vision, University of Central Florida zonglin.lyu@ucf.edu chen.chen@crcv.ucf.edu Figure 1. Overview of the proposed method. (a) Training autoencoder. The autoencoder is trained with video clip = [I0, In, I1] and aims to reconstruct In. It contains an image encoder (shared for all frames) and an image decoder, where multi-level encoder features from I0, I1 are passed to the decoder. Temporal blocks extract temporal information in the latent space and aggregate video features into single image feature for the image decoder, and 3D Wavelet extracts temporal information in the pixel space. (b) Training Denoising UNet. The video clip is encoded to x0 by Encoder (spatial + temporal). Since In is unknown, we replace it by 0 and obtain another video clip = [I0, 0, I1], which is encoded to xT . With the Brownian Bridge Diffusion Process, xt is computed and sent to denoising UNet to predict xt x0. (c) Inference. During inference, we encode to xT and sample with the Brownian Bridge Sampling Process to get ˆx0, which is decoded to the output frame ˆIn."
        },
        {
            "title": "Abstract",
            "content": "Video Frame Interpolation (VFI) aims to predict the intermediate frame In (we use to denote time in videos to avoid notation overload with the timestep in diffusion models) based on two consecutive neighboring frames I0 and I1. Recent approaches apply diffusion models (both image-based and video-based) in this task and achieve strong performance. However, image-based diffusion models are unable to extract temporal information and are relatively inefficient compared to non-diffusion methods. Videobased diffusion models can extract temporal information, but they are too large in terms of training scale, model size, and inference time. To mitigate the above issues, we propose Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation (TLB-VFI), an efficient video-based diffusion model. By extracting rich temporal information from video inputs through our proposed 3D-wavelet gating and temporal-aware autoencoder, our method achieves 20% improvement in FID on the most challenging datasets over recent SOTA of image-based diffusion models. Meanwhile, due to the existence of rich temporal information, our method achieves strong performance while having 3 fewer parameters. Such parameter reduction results in 2.3 speed up. By incorporating optical flow guidance, our method requires 9000 less training data and achieves over 20 fewer parameters than videobased diffusion models. Codes and results are available at our Project Page. 1. Introduction Video Frame Interpolation (VFI) is crucial task in computer vision that aims to predict the intermediate frame In between two known neighboring frames (past frame I0 and future frame I1). Good quality interpolation results benefit wide range of applications, including novel-view synthesis [12] and video compression [45]. Recent VFI methods generally fall into two categories: diffusion-based and traditional (a.k.a non-diffusion-based) methods. Traditional methods use either kernel-based methods to generate convolution kernels that predict intermediate frames from pixels of neighboring images [5, 21, 31, 32, 39] or flow-based methods that estimate optical flows [1, 6, 11, 16, 23, 26, 33, 35, 46, 50, 51]. Advances in deep learning make researchers prefer flow-based methods over kernel-based ones because the motion estimation in flow-based methods benefit from deep architecture, whereas kernel-based methods do not model motion explicitly. Diffusion-based methods can be split into image-based diffusion models [9, 27] and video-based diffusion models [17, 38, 43, 49], both achieving strong performances. However, they encounter several limitations: Traditional methods and image-based diffusion methods (generally less efficient than traditional methods) [9, 27], which incorporate kernelor flow-based methods to explicitly guide frame interpolation, only extract spatial information in I0, I1. Therefore, they lack explicit temporal information extraction to enhance temporal consistency. Video-based diffusion models [17, 38, 43, 49] directly generate pixels. Though they successfully extract temporal information by taking video clips as inputs, the training and inference costs of these methods are extremely high. They require more than ten million videos to train for strong performance due to the lack of explicit pixel-level information from I0, I1 guided by optical flow, and the inference time is also extremely long. temporal Our goal is to extract information while keeping training scale, model size, and inference time reasonable. To achieve so, we notice that video-based diffusion model is necessary. Video-based diffusion models can gradually build up temporal information in their sampling process with 3D UNet, and the autoencoder can further extract temporal information. This is not feasible in traditional methods as In is not available as input. Therefore, we propose Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation (TLB-VFI), shown in Fig. 1. Starting with the selection of diffusion models, Consec. BB [27] points out that Brownian Bridge Diffusion Model (BBDM) [22] better fits the VFI task than traditional Diffusion Models [14] since the sampling variance of BBDM is lower. This is because VFI expects deterministic interpolation results rather than diverse ones. However, in the setup of Consec. BB [27], Brownian Bridge is applied between adjacent frames, but the latent features are almost identical for adjacent frames. Therefore, the Brownian Bridge becomes approximately an identity mapping and loses its functionality. To mitigate this problem, given input video clip = [I0, In, I1], we construct = [I0, 0, I1] and apply Brownian Bridge Diffusion between their latent features, shown in Fig. 1 (b). The latent features of largely differ from that of , resolving the issue of identity mapping. Details and justifications are in Sec. 4.3 and Fig. 6. overcome the limitation on lack of temporal information, simple approach is to use 3D Convolution and Spatiotemporal attention. However, since In is replaced with 0 during inference (see Fig. 1 (c)), the multi-level encoder features, which benefits the decoding stage [9], in Fig. 1 (a) will contain incomplete information due to zero replacement, which harms the performance (see Sec. 4.3). We mitigate this issue by splitting the temporal information extraction into pixel space and latent space as shown in Fig. 1 (a). We include image-level encoder and decoder for spatial features only and add temporal feature extraction between the image-level encoder and decoder to extract temporal information in the latent space. This enables us to utilize the multi-level encoder features of I0, I1 during the decoding phase as they are not impacted by zero replacement. To extract temporal information in the pixel space, we propose our 3D-wavelet feature gating mechanism to extract high-frequency temporal information, since high-frequency information represents the changes along temporal dimension. To address the limitation on efficiency, instead of directly predicting pixel values, we utilize optical flow estimation to warp neighboring frames and refine the output. Under the guidance of optical flow at the pixel level in I0 and I1, our method achieves better efficiency than directly generating pixels from scratch. Importantly, due to rich temporal features extracted, our method achieves the stateof-the-art performance while requiring less parameters and inference time than image-based diffusion models [9, 27]. Our contributions are summarized as: Our method requires 9000 fewer training data, has over 20 fewer the number of parameters, and is over 10 faster than video-based diffusion models [17, 38, 49]. Comparing to Image-based diffusion methods, our method has 3 fewer parameters and over 2 faster. We introduce our temporal design of autoencoder to extract temporal information in the latent space and 3D wavelet feature gating to extract temporal information in the pixel space, serving as complement with each other. We propose theoretical constraint on when Brownian Bridge Diffusion is effective in Sec. 3.3. Our temporal design of autoencoder adheres to our proposed constraint, validating the effectiveness of the Brownian Bridge. Through extensive experiments, our method achieves state-of-the-art performance in various datasets. Specifically, our method achieves around 20% improvement in FID in the most challenging datasets: SNU-FILM extreme subset [7] and Xiph-4K [30] over recent SOTAs. 2. Related Work 2.1. Traditional Methods in VFI Next, it is important to construct an autoencoder so that diffusion models can efficiently run in latent space. To Video Frame Interpolation (VFI) is task to predict the intermediate frame In given its neighboring frames I0 and I1. 2 Traditional (non-diffusion) VFI methods fall into two categories: flow-based methods [1, 6, 11, 16, 18, 23, 26, 33, 35, 50] and kernel-based methods [5, 21, 31, 32, 39]. Flowbased methods utilize optical flow estimations via deep neural networks. Some estimate optical flows from the In to I0 and I1 and apply backward warping [1, 6, 11, 16, 23, 26, 33, 35, 50]. Others estimate bidirectional flows from I0, I1 to each other, and apply forward splatting [18]. In addition to the basic framework, various architectures like transformers [42] are introduced together with techniques such as multi-resolution recurrence refinement [18], 4Dcorrelations extraction [23], and asymmetric flow blending [46] to enhance interpolation quality. Kernel-based methods, initially proposed by [31], try to estimate local convolution kernels that are applied to pixels of neighboring frames to generate pixels in the intermediate frame. Several improvements, such as adaptive and deformable convolutions [5, 21], are proposed to improve performance under large motion change. 2.2. Diffusion Models in VFI Denoising Diffusion Probabilistic Models (DDPM) [14] are proposed to generate realistic and diverse images. In DDPM, an image is transformed into standard Gaussian noise with predefined diffusion process, and the noise is transformed back to an image with sampling (denoising) process. The sampling process contains steps of iteration, where is set to 1000 experimentally. The following works [24, 25, 40] improve the efficiency of sampling by reducing the number of steps for iterative sampling while keeping high-quality generation. Latent Diffusion Models [36] introduce autoencoders with vector-quantized or KL-divergence regularization to compress images into latent space, where diffusion models operate more efficiently. Beyond image generation, BBDM [22] proposes Brownian Bridge Diffusion for image-to-image translation, and video diffusion models [3, 15] are developed to generate highquality videos. LDMVFI [9] incorporates the idea of Latent Diffusion Models (LDMs) [36] and kernel-based methods, proposing an autoencoder that utilizes kernel-based methods to reconstruct In. Recent work [27] claims that VFI prefers deterministic prediction of the intermediate frames and introduces the Consecutive Brownian Bridge, which has small sampling variance and achieves state-of-the-art performance. Other diffusion-based works [17, 38, 43, 49] do not employ kernelor flow-based methods but instead generate raw pixels. This approach allows these diffusion methods to extract explicit temporal information by taking videos as inputs to autoencoder and build up temporal information during the sampling process from random noise. However, these methods require large-scale training as they lack pixel guidance from neighboring frames I0, I1: most works [17, 38, 49] require over ten million videos to train, while the common datasets for VFI only contain 51K triplets [48]. MCVD [43], diffusion approach directly generating pixels, is trained with smallscale dataset, resulting in unsatisfactory performance in VFI [9]. Our method addresses these challenges by extracting temporal information from video inputs and taking advantage of optical flow estimation. 3. Methodology 3.1. Preliminary Diffusion Models. DDPM [14] defines diffusion process that converts images into standard Gaussian noise: q(xtx0) = (xt; αtx0, (1 αt)I), (1) where αt = (cid:89) s=1 (1 βs). {βt} are small pre-defined constants. The sampling process (i.e. how to denoise xt1 from xt) is given by [14]: pθ(xt1xt) = (xt1; µt, βt), where µt = 1 1 βt (cid:18) xt βt 1 αt (cid:19) ϵ , βt = 1 αt1 1 αt βt, and ϵ (0, I). (2) (3) (4) Based on Eq. (3), the only unknown term is ϵ, which is estimated with deep neural network ϵθ(xt, t). Brownian Bridge Diffusion Models. Similar to DDPM, Brownian Bridge Diffusion Models (BBDM) [22] replace its diffusion and sampling process with Brownian Bridge Diffusion [37] for image-to-image translation tasks such as image inpainting and image colorization. Consecutive Brownian Bridge [27] introduces cleaner formulation of BBDM and suggests that it fits the VFI task since the sampling process has low variance, which VFI prefers. The diffusion process is: q(xtx0, xT ) = (cid:18) x0 + (1 )xT , (cid:19) t(T t) . (5) x0 and xT are the latent features of two images to be translated. controls the maximum variance of the Brownian Bridge. The sampling process is given as [27]: pθ(xsxt, xT ) = xt (cid:18) (xt x0), (cid:19) . st (6) Here is an arbitrary time before t, and = s. Analogous to DDPM [14], the only unknown term is xt x0, which can be estimated with deep neural network. 3 Figure 2. (a) Model Pipeline. The Image Encoder is shared across all frames, and temporal blocks extract temporal information in the latent space. (b) Multi-level Feature Sharing. The Image Encoder and Decoder consist of several levels of resolution due to downsampling/upsampling latent features. At the ith level of the encoder and the decoder, features from I0 and I1 in the encoder are warped and concatenated with the original copy (when the downsampling rate is larger than 8, warped features are excluded). The concatenated features are used as keys and values in cross attention [42] where the decoder feature at the same level is the query. (c) Encoder/Decoder Temporal Block. Each temporal block consists of two sets of 3D convolution + attention. In the decoder, the second attention is cross-attention between the intermediate frame (query) and all frames (key and value) to aggregate the video feature into one feature map. (d) 3D-wavelet Feature Gating. Wavelet information is extracted from the input video clip and encoded by CNNs. sigmoid activation is applied, and the result is element-wise multiplied by the output of the Image Encoder with skip connection. 3.2. Problem Definition Video Frame Interpolation aims to predict In R3HW , which is the intermediate frames between I0 and I1. Following the common strategy of recent VFI works [27, 46], the goal is to predict ˆIn that predicts In: ˆIn = warp(I0) + (1 ) warp(I1) + (7) is mask with values between 0 and 1. warp denotes warping based on estimated optical flows. is the residual term with values between -1 and 1. Therefore, our model will predict the mask and residual , with flow estimation embedded in the model. The basics of optical flow and warping are included in the Supplementary Material Sec. 7. Method Pipeline. Our method is divided into two main components: an autoencoder and Brownian Bridge Diffusion Model (referred to as diffusion model later), shown in Fig. 1 (a) and (b), respectively. The autoencoder contains an encoder and decoder D, shown in yellow and green dashes in Fig. 1 (a) respectively. The encoder contains an image encoder and temporal blocks, and the decoder contains temporal blocks and an image decoder. The reason for this design is discussed in Sec. 3.3. The objective of the autoencoder is to predict M, given input video clip = [I0, In, I1]: M, = D(E(V )). During inference, In is replaced with zero matrix, resulting in = [I0, 0, I1]. The goal of the diffusion model (denoted as BB) is to estimate E(V ) from E( ). In summary, our pipeline to predict In, with = 0.5, involves the following steps: 1. = BB(E( )). 2. M, = D(x). 3. ˆIn = warp(I0) + (1 ) warp(I1) + 3.3. Temporal Aware Latent Brownian Bridge Temporal Feature Extraction in Latent Space. Our goal is to extract temporal information in the autoencoder, but it is important to provide multi-level encoder features of I0, I1 to guide the decoder [9, 27]. Details of how these multilevel features can provide guidance are shown in Fig. 2 (b). At ith level of the encoder and decoder, the features are downsampled to 2i of the images height and width. We denote encoder features of I0, I1 at ith level as f0, f1. Then concat(f0, f1, warp(f0), f1, warp(f1)) is sent to the decoder at the ith level via cross-attention [42] (concat is channel-wise concatenation). To ensure multi-level encoder features of I0, I1 are not affected by changing to during inference, we use shared image encoders to encode each frame into low-resolution latent features and apply temporal feature extraction of those latent features. The temporal blocks for the encoder, shown in Fig. 2 (c), utilize 3D convolution and spatiotemporal attention. The decoder is mostly symmetric with the encoder, beginning with 3D convolution and spatiotemporal attention followed by an image decoder. To convert the video latent representation into the image latent representation, we include spatiotemporal cross-attention aggregation in the decoder, illustrated in Fig. 2 (c). Given the video latent 4 representation RCT HW where C, T, H, represents the channel, temporal, height, and weight. The spatiotemporal cross-attention aggregation is defined as: Vout = sof tf max (cid:18) QK (cid:19) = WQFQ, = WKFKV , = WV FKV FQ = [t].f latten(1), FKV = F.f latten(1) FQ and FKV are in PyTorch style notation. This approach allows us to remove the temporal dimension, retaining only the image-level feature. 1 2 1 2 Temporal Feature Extraction in Pixel Space with 3D Wavelet Transform. Since we use shared image encoder to encode images, the encoder and decoder only extract temporal information in latent space. However, extracting temporal information at the pixel level is essential as the latent space is highly compressed. Therefore, we apply 3D wavelet transform [28] to extract spatial and temporal frequency information with low-pass filter [ 1 ] and high2 pass filter [ 1 ]. These filters are applied on height, 2 width, and temporal dimensions, respectively. The reason to choose 3D wavelet transform is that it can apply different combinations of high pass and low pass filters across different dimensions, resulting in rich pixel-level information. Then, the frequency information is encoded with convolution layers. The 3D wavelet transform can be applied twice. With the first pass, we can extract temporal information between I0 and In and between In and I1, which is not achievable with input only containing I0 and I1. The second pass extracts the temporal information throughout all frames. Such pixel-level temporal information tells the model where the motion changes more drastically, and with this intuition, we design our 3D wavelet feature gating, If fw is the encoded feature of freshown in Fig. 2 (d). quency information, and if fi is the latent features encoded by the shared image encoder, then the 3D wavelet gating is: = σ(fw) fi + fi, (8) where σ is the sigmoid activation, and is the element-wise product. This implicitly guides the model to learn which parts of the video clip have more changes in the temporal dimension. The details about the implementation of the wavelet transform are included in the Supplementary Material Sec. 9.2. Temporal Information Restoration with Brownian Bridge Diffusion. During inference, (original video clip) is replaced with (In replaced by zero matrix), leading to distribution shift of encoded features E( ) from E(V ) due to loss of temporal information. We employ the Brownian Bridge Diffusion to align the distribution of E(v) and E( ) to restore such information because Brownian Bridge 5 diffusion demonstrates efficacy in VFI [27] by achieving low sampling variance. In addition, restoring the temporal information is conceptually similar to spatial information restoration in image inpainting in BBDM [22], where the Brownian Bridge Diffusion is originally proposed. To effectively restore temporal information, temporal features need to be extracted. Therefore, we replace the convolution in the denoising U-Net with 3D convolution, and self-attentions are performed in spatial and temporal dimensions. Notably, by Eq. (6), the denoising UNet ϵθ aims to predict xt x0. When = , the ϵθ aims to predict xT x0. If xT = x0, then by Eq. (5), E(xt) = 0 at any time step. The training objective of ϵθ becomes 0 on expectation based on Eq. (6). The variance is negligible because = 1 1000 and = 2 during training [27]. In this case, when ϵθ learns to predict 0, we can easily compute that the sampling process defined by Eq. (6) is an identity mapping with small variance. Therefore, we need significantly large distribution shift between x0 and xT to prevent such problem of identity mapping. Specifically, big shift in the mean is required, which is described in the following proposition: Proposition 1. If the Brownian Bridge Diffusion is applied to translate between two distributions x0 and xT , there should be large shift between E(x0) and E(xT ). sufficient constraint is to reject the Null Hypothesis H0 : E(x0 xT ) = 0 at significance level α. The proof of this proposition is given in the Supplementary Material Sec. 9.3. We include experiments in Sec. 4.3 to show that our method satisfies this constraint, while the Consec. BB [27] does not. 4. Experiments 4.1. Datasets, Evaluation Metrics, and Baselines Datasets. Following recent works [27, 50], our autoencoder and diffusion models are trained on the Vimeo 90K triplets dataset [48], which comprises 51,312 training triplets. We employ random flipping, cropping, rotation, and temporal order reversing as data augmentation. To evaluate our methods, we include Xiph [30], DAVIS [34], and SNUFILM [7]. SNU-FILM contains four subsets based on the magnitude of motion changes: easy, medium, hard, and extreme, and Xiph contains 4K and 2K resolution for evaluation. These datasets contain various resolutions (480p to 4K) and motion changes. Among these datasets, Xiph and SNU-FILM-extreme are the most challenging and important due to high resolution and extremely large motion changes respectively, and strong performance on these two datasets indicates strong real-world applicability. Evaluation Metric and Baselines. Recent works [9, 27, 46] identify that pixel-based metrics such as PSNR and SSIM [44] are less consistent with visual quality and Table 1. Quantitative results in LPIPS/FloLPIPS/FID, the lower the better, on evaluation datasets. The best performances are blue-boldfaced, and the second-best performances are red-underlined. Results from PerVFI are in gray background and excluded from the ranking because their training scale is significantly larger. Results for baselines, except for PerVFI, are adopted from Consec.BB [27]. OOM indicates that the inference with one image exceeds the 24GB GPU memory of an Nvidia RTX A5000 GPU. Runtime measures seconds per frame to interpolate 480 720 image with A5000 GPU. To ensure fair comparison, we report the runtime of Consec. BB [27] and LDMVFI [9] in 10 step sampling (our setup) and also report the runtime of their setup in orange and parenthesis. Note that results from [17, 38, 49] are not included as their training scales are millions of videos. Methods Xiph-4K Xiph-2K DAVIS SNU-FILM easy LPIPS/FloLPIPS/FID LPIPS/FloLPIPS/FID LPIPS/FloLPIPS/FID LPIPS/FloLPIPS/FID LPIPS/FloLPIPS/FID LPIPS/FloLPIPS/FID LPIPS/FloLPIPS/FID medium extreme hard MCVD22 [43] VFIformer [26] IFRNet22 [20] AMT23 [23] UPR-Net23 [18] EMA-VFI23 [50] LDMVFI24 [9] PerVFI24 [46] Consec. BB24 [27] OOM OOM 0.136/0.164/23.647 0.199/0.230/29.183 0.230/0.269/31.043 0.241/0.260/28.695 OOM 0.086/0.128/18.852 0.097/0.135/24.424 OOM OOM 0.068/0.093/11.465 0.089/0.126/13.100 0.103/0.144/12.909 0.110/0.132/12.167 OOM 0.038/0.069/10.078 0.042/0.080/12.011 0.247/0.293/28.002 0.127/0.184/14.407 0.106/0.156/12.422 0.109/0.145/13.018 0.134/0.172/15.002 0.132/0.166/15.186 0.107/0.153/12.554 0.081/0.122/8.217 0.092/0.136/9. 0.199/0.230/32.246 0.018/0.029/5.918 0.021/0.031/6.863 0.022/0.034/6.139 0.018/0.029/5.669 0.019/0.038/5.882 0.014/0.024/5.752 0.014/0.022/5.917 0.012/0.019/4.791 0.213/0.243/37.474 0.033/0.053/11.271 0.034/0.050/12.197 0.035/0.055/11.039 0.034/0.052/10.983 0.033/0.053/11.051 0.028/0.053/12.485 0.024/0.040/10.395 0.022/0.039/9.039 0.250/0.292/51.529 0.061/0.100/22.775 0.059/0.093/23.254 0.060/0.092/20.810 0.062/0.097/22.127 0.060/0.091/20.679 0.060/0.114/26.520 0.046/0.077/18.887 0.047/0.091/18.589 0.320/0.385/83.156 0.119/0.185/40.586 0.116/0.182/42.824 0.112/0.177/40.075 0.112/0.176/40.098 0.114/0.170/39.051 0.123/0.204/47.042 0.090/0.151/32.372 0.104/0.184/36.631 Runtime Seconds 52.55 4.34 0.10 0.11 0.70 0.72 2.48 (22.32) 1.52 1.62 (2.60) Ours 0.077/0.113/19.114 0.032/0.067/9.901 0.086/0.126/8.299 0.012/0.018/4. 0.021/0.036/8.518 0.044/0.085/17.470 0.095/0.151/29.868 0.69 humans evaluation than learning-based metrics such as FID [13], LPIPS [52], and FloLPIPS [8]. There are examples shown in [27] indicating that PSNR/SSIM are inconsistent with visual qualities in VFI task, and we also include examples in Sec. 4.2. Thus, FID, LPIPS, and FloLPIPS are selected as the evaluation metrics. FID and LPIPS measure perceptual similarity by Frechet distance or normalized distances between features extracted by deep learning models. FloLPIPS, developed on top of LPIPS, accounts for motion consistency. Recent state-of-the-art methods including PerVFI (2024) [46], Consec. BB (2024) [27], LDMVFI (2024) [9], EMA-VFI (2023) [50], AMT (2023) [23], UPR-Net (2023) [18], IFRNet (2022) [20], VFIformer (2022) [26], and MCVD (2022) [43] are selected as our baselines. For models with different versions on the number of parameters, the largest version is chosen. For completeness of our experiment, we include PSNR/SSIM and implementation details in our Supplementary Material Sec. 8.1. 4.2. Experimental Results Quantitative Evaluation. The quantitative results are shown in Tab. 1, with 10 sampling steps in diffusion. Notably, PerVFI uses combination of flow estimators, RAFT [41] and GMflow [47], to guide their model. These flow estimators are trained with three datasets: FlyingThings3D [29], FlyingChairs [10], and Sintel [4], where Sintel and Flyingthings3D contain high-resolution images. They additionally train their model on the Vimeo 90K Triplets dataset [48]. In contrast, all other methods are trained purely on the Vimeo 90K Triplets dataset without highresolution images. Therefore, the training scale of PerVFI is almost doubled with high-resolution data included compared to other methods. To ensure fair comparison, we display the results of PerVFI in gray background and exclude them from ranking. However, we still include it as reference to assess our methods capability. Even with much smaller training scale than PerVFI, our method still achieves better performance than PerVFI in most datasets and metrics. Under fair comparison, our method achieves the best performance in all metrics and datasets, as shown in Tab. 1. In the most challenging datasets: SNU-FILM extreme [7] and Xiph-4K [30], our proposed methods achieve approximately 20% improvements in FID and FloLPIPS over the second-best results. In relatively easier datasets like Xiph2K, our method achieves around 20% improvements in all metrics over the second-best result. In some datasets like DAVIS and Xiph-2K, we notice that newer methods can sometimes underperform older methods (like EMAVFI23 vs IFRNet22), but our method consistently outperforms others, indicating strong generalization capability over different datasets. Moreover, we observe that the improvement margin increases when the task gets harder (such as SNU-FILM), indicating our strong capability for challenging cases, which is more crucial than incrementing quantitative scores on near-perfect results such as SNUFILM-easy and medium. Runtime Analysis. Other than interpolation quality, our method achieves good efficiency, shown in the last column of Tab. 1. Under the same number of sampling steps (10 in our setup), our method achieves 2.3 speedup over Consec. BB and 4.3 speedup over LDMVFI. Our method also achieves 2.2 speedup than recent non-diffusion SOTA: PerVFI. This is an important step toward efficiency of diffusion-based methods, as previous diffusion-based methods are inefficient. Moreover, even PerVFI contains larger training scale, our method achieves comparable results and much faster inference speed. Qualitative Evaluation. We select Consec. BB [27], PerVFI [46], EMAVFI [50], and IFRNet [20] to visually compare the interpolation results since they achieve strong results in challenging cases like SNU-FILM-extreme. The qualitative results are shown in Fig. 3, with examples selected from challenging cases in SNU-FILM-extreme, Xiph-4K, and DAVIS. Overlaid Input means the blended image of I0 and I1. In the first row, results from IFRNet, EMAVFI, and Consec. BB are largely distorted, and the result in PerVFI contains distorted eye. Our method aligns 6 Figure 3. Qualitative Comparison between our method and recent SOTAs. The leftmost image is the overlaid image of I0 and I1 (blended image). Areas with drastic motion changes are cropped with blue boxes to better visualize the results. Red circles and boxes indicate the area where we perform significantly better. Our method achieves better visual quality than recent SOTAs. Additional qualitative results are included in the Supplementary Material Sec. 8.2. with the ground truth. In the second row, our method is the only one that does not contain the third leg in the middle. The persons right leg in I0 and left leg in I1 are at almost the same position, resulting in an artificial third leg generated by other methods. In the third row, IFRNet, EMAVFI, and Consec. BB generate distorted results, and the vehicle generated by PerVFI misses some parts. However, our method generates high-quality image. In the fourth row, EMAVFI generates blurred results, and IFRNet and Consec. BB generate distorted results. PerVFI, though without large distortion, contains artifacts shown in the red circles, but our result is realistic. We additionally include 8 interpolation results in our Supplementary Material Sec. 8.2. Inconsistency Between PSNR/SSIM and Visual Quality. Examples in Fig. 4 indicate that PSNR/SSIM are inconsistent with visual quality. In the first row, the females hand in the red circle is blurred and distorted in the result of EMAVFI, but our result is consistent with ground truth. In the second row, there is distortion (pointed by arrow) on hairs in the result of EMAVFI, and ours is also consistent with the ground truth. However, in both examples, the PSNR/SSIM in EMAVFI is better, and our LPIPS is better, indicating that LPIPS is consistent with visual quality while Figure 4. Inconsistency between PSNR/SSIM and Visual Quality. Red circles and arrows indicate where the results from EMAVFI are distorted. PSNR/SSIM are not. Training Cost of Diffusion Based Models. As claimed, our method achieves 20 fewer model parameters and requires 9000 less training data than video-based diffusion models [17, 38, 49]. We include quantitative results of the number of parameters and training data size of both imagediffusion-based methods and video-based diffusion methods in Tab. 2. Since MCVD [43] gets much worse performance than recent SOTAs (see Tab. 1), we exclude it for comparison. VIDIM [17] is trained with WebVid-10M [2] and private videos, where WebVid-10M is an extension of Table 2. Training and inference cost of recent diffusion-based methods. means VIDIM [17] is trained additionally with private datasets. means the performance of MCVD is much worse than SOTAs of recent years. Runtime is measured in seconds per frame with 480 720 resolution. N/A indicates that VIDIM [17] is not open-sourced, and NF indicates that Dreammover [38] needs fine-tuning for every single inference example, which takes minutes. Method Input type # Parameters # Training Data Pretrained Runtime LDMVFI24 [9] Consec. BB24 [27] MCVD22 [43] VIDIM24 [17] Dreammover24 [38] ViBiDSampler24 [49] Ours Images Images Videos Videos Videos Videos Videos 439.0M 146.4M 27.3M >1B 943.2M 943.2M 46.7M 51K Triplets 51K Triplets 51K Triplets >10M videos 244.7M Videos 244.7M Videos 51K Triplets 2.48 1.62 52.55 N/A NF 8.48 0. Table 3. Ablation studies. The result is in FID. The - signs mean that the component is removed. The removal is one at time. means that we use the full version of our method, but the Image Encoder is temporal-aware. means we disable feature sharing from . Dataset Xiph-4K Xiph-2K SNU-FILM-extreme Ours - 3D Wavelet - Cross-attention aggregation - Temporal attention - 3D Convolution Ours Ours 19.114 19.247 19.663 19.944 23.481 22.731 20.004 9.901 10.092 10.499 10.911 12.679 13.410 10. 29.868 30.717 30.903 32.061 33.155 34.982 32.018 Figure 5. Visualization of 3D Wavelet Transform.The 3D wavelet transform can extract frequency information along different directions (time, height, width). We visualize results by applying high pass filter in the time dimension with combination of (low, low), (low, high), and (high, low) filters in spatial dimensions. WebVid-2M, which has 13K hours of videos, corresponding to 1.4T frames. Vimeo Triplet 90K contains 51K triplets, corresponding to 153K frames, and therefore, our method gets at least 9000 less training data than VIDIM. The number of parameters, based on Tab. 2, is 20 fewer. Even comparing with image-based diffusion methods [9, 27], our number of parameters is over 3 fewer. 4.3. Ablation Studies Temporal Aware Design. We include ablation studies in Tab. 3 to indicate the effectiveness of each component. We start with our method and gradually remove 3D wavelet, replace cross-attention aggregation by average pooling, remove temporal attention, and replace 3D convolution by 2D. We also include Ours, where we replace Image Encoder by temporal aware design in our full model. The performance gets worse since multi-level features are changed due to zero replacement in this setup, indicating the necessity of the Image Encoder. We additionally show three example visualizations of 3D wavelet transform in Fig. 5. We apply high-pass and low-pass filters as convolution kernels to the input videos. By switching between high-pass and low-pass filters in temporal, height, and width dimensions, we can effectively extract 8 different frequency maps. Our temporal autoencoder plays the primary role, with the 3D wavelet feature gating contributing pixel-level information that further enhances the results. Figure 6. AP E(E(In), E(I0)) in the setup of Consec. BB [27] and AP E(E(V ), E( )) in our method. is the video clip with the intermediate frame replaced with 0s. The MAPE in Consec. BB is less than 1%, resulting in rough identity transformation. In our method, MAPE is 40-50%, and therefore the Brownian Bridge learns to reduce this gap. Distribution Shift in Brownian Bridge. The Brownian Bridge in Consec. BB [27] connects the encoder features of adjacent frames. Since adjacent frames are similar, features connected with Brownian Bridge Diffusion are almost identical, and therefore the diffusion is approximately an identity mapping. However, our design mitigates this issue by replacing In in the video clip (results denoted as ). Following proposition 1, we do t-test for our method and Consec. BB [27]. The dataset is selected to be SNU-FILM extreme [7] for the t-test. The t-statistic computed in our method is more than 21, which is much larger than the threshold of significance level 0.001: 3.291. This means that we can reject H0. However, the t-statistic computed in the setup of Consec. BB is about 0.0001, where we cannot reject H0. We show some examples in Fig. 6. The Mean Absolute Percentage Error (MAPE) is used to measure the distribution shift of encoded features, where AP E(x, y) = xy , indicating the average percentage change. In these examples, we can see that the MAPE in Consec. BB is close to 0, but our method gives large MAPE, experimentally showing the distributional shift. 5. Conclusion In this paper, we introduce our TLB-VFI to extract temporal information in both latent space (temporal blocks) and pixel space (3D wavelet feature gating). With such design, our method achieves state-of-the-art results and solves the limitations of recent diffusion-based methods. Our method is 8 highly flexible as video diffusion-based models that we can handle more than 3 frames as input even though our method is trained with triplets. Meanwhile, we do not require largescale training like video diffusion-based models because we take advantage of flow estimation to guide the generation."
        },
        {
            "title": "References",
            "content": "[1] Dawit Mureja Argaw and In So Kweon. Long-term video frame interpolation via feature propagation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 2, 3 [2] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for In IEEE International Conference on end-to-end retrieval. Computer Vision, 2021. 7 [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 3 [4] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. naturalistic open source movie for optical flow evaluation. In European Conf. on Computer Vision (ECCV), 2012. 6 [5] Xianhang Cheng and Zhenzhong Chen. Video frame interpolation via deformable separable convolution. In Proceedings of the AAAI Conference on Artificial Intelligence, 2020. 2, 3 [6] Jinsoo Choi, Jaesik Park, and In So Kweon. High-quality frame interpolation via tridirectional inference. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2021. 2, 3 [7] Myungsub Choi, Heewon Kim, Bohyung Han, Ning Xu, and Kyoung Mu Lee. Channel attention is all you need for video frame interpolation. In Proceedings of the AAAI Conference on Artificial Intelligence, 2020. 2, 5, 6, 8, 1 [8] Duolikun Danier, Fan Zhang, and David Bull. Flolpips: bespoke video quality metric for frame interpolation. In 2022 Picture Coding Symposium (PCS). IEEE, 2022. [9] Duolikun Danier, Fan Zhang, and David Bull. Ldmvfi: Video frame interpolation with latent diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 14721480, 2024. 2, 3, 4, 5, 6, 8, 1 [10] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazırbas, V. Golkov, P. v.d. Smagt, D. Cremers, and T. Brox. Flownet: Learning optical flow with convolutional networks. In IEEE International Conference on Computer Vision (ICCV), 2015. 6 [11] Saikat Dutta, Arulkumar Subramaniam, and Anurag Mittal. Non-linear motion estimation for video frame interIn Proceedings of polation using space-time convolutions. the IEEE/CVF conference on computer vision and pattern recognition, 2022. 2, 3 [12] John Flynn, Ivan Neulander, James Philbin, and Noah Snavely. Deepstereo: Learning to predict new views from the worlds imagery. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2016. 1 [13] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 2017. [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2, 3 [15] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. 3 [16] Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi, and Shuchang Zhou. Real-time intermediate flow estimation for video frame interpolation. In European Conference on Computer Vision, 2022. 2, 3 [17] Siddhant Jain, Daniel Watson, Eric Tabellion, Ben Poole, Janne Kontkanen, et al. Video interpolation with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7341 7351, 2024. 2, 3, 6, 7, 8 [18] Xin Jin, Longhai Wu, Jie Chen, Youxin Chen, Jayoon Koo, and Cheul-hee Hahm. unified pyramid recurrent netIn Proceedings of work for video frame interpolation. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. 3, 6, 2 [19] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. In International Conference on Learning Representations, 2015. [20] Lingtong Kong, Boyuan Jiang, Donghao Luo, Wenqing Chu, Xiaoming Huang, Ying Tai, Chengjie Wang, and Jie Yang. Ifrnet: Intermediate feature refine network for efficient frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 6, 2 [21] Hyeongmin Lee, Taeoh Kim, Tae-young Chung, Daehyun Pak, Yuseok Ban, and Sangyoun Lee. Adacof: Adaptive colIn Prolaboration of flows for video frame interpolation. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020. 2, 3 [22] Bo Li, Kaitao Xue, Bin Liu, and Yu-Kun Lai. Bbdm: Imageto-image translation with brownian bridge diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. 2, 3, 5 [23] Zhen Li, Zuo-Liang Zhu, Ling-Hao Han, Qibin Hou, ChunLe Guo, and Ming-Ming Cheng. Amt: All-pairs multi-field transforms for efficient frame interpolation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 2, 3, 6 [24] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 2022. 3 [25] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022. [26] Liying Lu, Ruizheng Wu, Huaijia Lin, Jiangbo Lu, and Jiaya Jia. Video frame interpolation with transformer. In Proceed9 [41] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field In Computer VisionECCV transforms for optical flow. 2020: 16th European Conference, Glasgow, UK, August 23 28, 2020, Proceedings, Part II 16, 2020. 6 [42] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 3, 4, 2 [43] Vikram Voleti, Alexia Jolicoeur-Martineau, and Chris Pal. Mcvd-masked conditional video diffusion for prediction, generation, and interpolation. Advances in neural information processing systems, 2022. 2, 3, 6, 7, [44] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 2004. 5 [45] Chao-Yuan Wu, Nayan Singhal, and Philipp Krahenbuhl. Video compression through image interpolation. In Proceedings of the European conference on computer vision (ECCV), 2018. 1 [46] Guangyang Wu, Xin Tao, Changlin Li, Wenyi Wang, Xiaohong Liu, and Qingqing Zheng. Perception-oriented video In Proceedframe interpolation via asymmetric blending. ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 27532762, 2024. 2, 3, 4, 5, 6, 1 [47] Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, and Dacheng Tao. Gmflow: Learning optical flow via global matching. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022. 6 [48] Tianfan Xue, Baian Chen, Jiajun Wu, Donglai Wei, and William Freeman. Video enhancement with task-oriented International Journal of Computer Vision (IJCV), flow. 2019. 3, 5, 6 [49] Serin Yang, Taesung Kwon, and Jong Chul Ye. Vibidsampler: Enhancing video interpolation using bidirectional diffusion sampler. arXiv preprint arXiv:2410.05651, 2024. 2, 3, 6, 7, 8 [50] Guozhen Zhang, Yuhan Zhu, Haonan Wang, Youxin Chen, Gangshan Wu, and Limin Wang. Extracting motion and appearance via inter-frame attention for efficient video frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. 2, 3, 5, [51] Guozhen Zhang, Chunxu Liu, Yutao Cui, Xiaotong Zhao, Kai Ma, and Limin Wang. Vfimamba: Video frame arXiv preprint interpolation with state space models. arXiv:2407.02315, 2024. 2 [52] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 6 ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 2, 3, 6 [27] Zonglin Lyu, Ming Li, Jianbo Jiao, and Chen Chen. Frame interpolation with consecutive brownian bridge diffusion. arXiv preprint arXiv:2405.05953, 2024. 2, 3, 4, 5, 6, 8, 1 [28] S.G. Mallat. theory for multiresolution signal decomposition: the wavelet representation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1989. 5 [29] N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Dosovitskiy, and T. Brox. large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 6 [30] Simon Niklaus and Feng Liu. Softmax splatting for video frame interpolation. In IEEE Conference on Computer Vision and Pattern Recognition, 2020. 2, 5, 6, [31] Simon Niklaus, Long Mai, and Feng Liu. Video frame interpolation via adaptive convolution. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2017. 2, 3 [32] Simon Niklaus, Long Mai, and Feng Liu. Video frame interpolation via adaptive separable convolution. In Proceedings of the IEEE international conference on computer vision, 2017. 2, 3 [33] Junheum Park, Jintae Kim, and Chang-Su Kim. Biformer: Learning bilateral motion estimation via bilateral transformer for 4k video frame interpolation. In Computer Vision and Pattern Recognition, 2023. 2, 3 [34] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander SorkineHornung. benchmark dataset and evaluation methodology for video object segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 5, 1 [35] Markus Plack, Karlis Martins Briedis, Abdelaziz Djelouah, Matthias Hullin, Markus Gross, and Christopher Schroers. Frame interpolation transformer and uncertainty guidance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. 2, 3 [36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, 2022. 3, [37] Sheldon Ross. Stochastic processes. 1995. 3 [38] Liao Shen, Tianqi Liu, Huiqiang Sun, Xinyi Ye, Baopu Li, Jianming Zhang, and Zhiguo Cao. Dreammover: Leveraging the prior of diffusion models for image interpolation with large motion. arXiv preprint arXiv:2409.09605, 2024. 2, 3, 6, 7, 8 [39] Zhihao Shi, Xiaohong Liu, Kangdi Shi, Linhui Dai, and Jun Chen. Video frame interpolation via generalized deformable convolution. IEEE transactions on multimedia, 2021. 2, 3 [40] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. 3 10 TLB-VFI: Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Overview The supplementary material is structured as follows: Sec. 7 includes optical basics: what are optical flows and warping. Sec. 8.1 contains PSNR/SSIM evaluated on our selected datasets. Sec. 8.2 contains additional qualitative results. Sec. 9.1 contains implementation details. Sec. 9.2 contains additional details on 3D wavelet transforms. Sec. 9.3 contains the proof to our proposition in Sec. 3.3 of our main paper. 7. Optical Flow Basics Optical flow is the pixel-wise movement from frame to frame. If we have two images I0 and I1, and for given pixel I0[i, j] the corresponding pixel appears in I1 I1[i, j] then low(I0, I1)[i, j] is [i i, j], indicating the pixel movement. Warping is to move each pixel according to the movements defined by optical flows. Importantly, optical flows do not explicitly estimate the motion speed, and motion speed is implicitly contained in training data. If all training data consists of constant speed motion, and the test data contains motion speed that is not evenly distributed between two frames, the predicted position will not align. This would be an interesting research problem but out of scope of our research. An example is the third row of Fig. 3 in our main paper, where our method and PerVFI predict basically the same location but the ground truth location is different. One possible explanation is that the vehicle is accelerating, but the location that our method and PerVFI predicts is based on constant speed. As result, at the first half of the time interval between I0, I1, the vehicle is slow so the ground truth is closer to the first frame, while our method and PerVFI make it approximately right in the middle. 8. Additional Results 8.1. Results in PSNR/SSIM We include the results in PSNR/SSIM on our selected datasets in Tab. 4. We can see that PSNR/SSIM tends to be unstable and not correlated to visual qualities for methods in 2024. For example, our method underperforms Consec. BB in Xiph [30] dataset but our visual comparisons and LPIPS/FloLPIPS/FID indicate that our method is better. Similarly, PerVFI underperforms Consec. BB in Xiph-2K, but its LPIPS/FLoLPIPS/FID is much better. 8.2. Additional Qualitative Results More Input Frames. Our method is highly flexible and can take more than three input frames in one forward call. An example is shown in Fig. 7, where our model can receives five frames I0, I1, I2, I3, I4 and treat either I2 or I1, I3 as target. This enables us to do only one run of the sampling process for the second scenario, where LDMVFI [9] and Consec. BB [27] needs to sample twice. To achieve this, we only need to replace the second and fourth frames by zeros and send the video clip to the autoencoder, and the diffusion model only needs one sampling process to obtain latents for the decoder. However, LDMVFI and Consec. BB needs to sample frame 2 and 4 separately. Additional Visual Comparisons. We include additional visual comparisons in Fig. 9 and Fig. 10, where examples are selected from SNU-FILM extreme [7], DAVIS [34], and Xiph-4K [30]. Our method achieves the best visual quality. Other methods exhibit distortion, blurring, or artifacts in their generation, but our method does not. Red circles and squares emphasize the area where our method achieves better quality. We encourage reviewers to do 500% zoom-in to see the results as many results contain multiple details in one frame. 8 Interpolation. 8 interpolation is interpolating 7 frames between I0, I1, which can be done iteratively. When motion change is large, 2 interpolation does not provide good video clip and therefore we need to interpolate more frames. We include two examples of 8 interpolation results in Fig. 11 just for reference. It is better to visualize 8 with videos, and therefore we include more examples compared with more methods in our Project Page. 8 interpolation is to interpolate 7 intermediate frames between I0 and I1 (i.e. only first and last frames are provided), which can be done iteratively. The upper example is taken from DAVIS [34] and the latter one from Xiph-4K [30]. The Project Page contains results from SNU-FILM extreme [7], DAVIS [34], and Xiph-4K [30]. In the upper examples of Fig. 11, bicycle tires interpolated by PerVFI [46] are missing in some frames. In the lower example, the womans right eye becomes an artifact in PerVFI. Flow visualization. We visualize the optical flow from interpolated results ( ˆIn) to neighboring frames I0, I1, shown in Fig. 8. Our primary contribution is the temporal aware latent Brownian Bridge diffusion framework instead 1 Figure 7. (a) Given four neighboring frames I0, I1, I3, I4, we can predict the intermediate frame I2. (b) Given sequence of frames I0, I2, I4, we can predict the intermediate frame between each adjacent pair I1, I3. Table 4. Quantitative results (PSNR/SSIM) on test datasets (the higher the better).OOM indicates that the inference with one image exceeds the 24GB GPU memory of an Nvidia RTX A5000 GPU. Methods Xiph-4K Xiph-2K DAVIS SNU-FILM easy PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM medium extreme hard MCVD22 [43] VFIformer22 [26] IFRNet22 [20] AMT23 [23] UPR-Net23 [18] EMA-VFI23 [50] LDMVFI24 [9] PerVFI24 [46] Consec.BB [27] OOM OOM 33.970/0.943 34.653/0.949 33.647/0.946 34.698/0.948 OOM 32.395/0.926 32.153/0.927 OOM OOM 36.570/0.966 36.415/0.967 36.749/0.967 36.935/0.967 OOM 34.741/0.953 34.964/0.956 18.946/0.705 26.241/0.850 27.313/0.877 27.234/0.877 26.894/0.870 27.111/0.871 25.073/0.819 26.502/0.866 26.391/0. 22.201/0.828 40.130/0.991 40.100/0.991 39.880/0.991 40.440/0.991 39.980/0.991 38.890 0.988 38.065/0.986 39.637/0.990 21.488/0.812 36.090/0.980 36.120/0.980 36.120/0.981 36.290/0.980 36.090/0.980 33.975/0.971 34.588/0.973 34.886/0.974 20.314/0.766 30.670/0.938 30.630/0.937 30.780/0.939 30.860/0.938 30.940/0.939 29.144/0.911 29.821/0.928 29.615/0.929 18.464/0.694 25.430/0.864 25.270/0.861 25.430/0.865 25.630/0.864 25.690/0.866 23.349 0.827 25.033/0.854 24.376/0.848 Ours 32.441/0. 35.748/0.959 26.272/0.860 39.460/0.990 35.308/0.977 29.529/0.929 24.513/0. of advancements in optical flows in other works [23, 46, 50], so we directly adopt the flow estimation architecture from Consec. BB [27]. Though the flow estimator is the same architecture as Consec. BB, our temporal design can implicitly improve it through back-propagation (see first and third row). 9. Additional Details 9.1. Implementation Details Flow Estimator. Optical flow estimation is not our research purpose, so we use the same architecture of flow estimator in Consec. BB [27] and trained together with our autoencoder. The code for differentiable warping is available at [26, 27]. Autoencoder. The autoencoder is based on the VQ version of LDM [36]. It consists of 5 levels of image encoder and decoder, resulting in 32 downsampling rate. Image decoders contain output channels of 64,128,128,128,256, respectively (reverse for decoder). Between the image encoder and decoder, there are four 3D convolutions with spatiotemporal attention (the last one is cross-attention) [42], where VQ-Layer is inserted after the second 3D conv + attention. The channel dimension is 256. The VQ-Layer quantizes features into 3 channels. To predict masks and residual , we use sigmoid activation to normalize the output. The autoencoder is trained with Adam optimizer [19] and learning rate of 105 for 35 epochs. The training loss is L1 loss and LPIPS loss, following LDM. Brownian Bridge Diffusion. The Brownian Bridge Diffusion is implemented with 3D denoising U-Net [36] with channel dimension 32 and 3 downsample blocks as well as 3 upsample blocks, where the optimizer is Adam [19] with learning rate of 104 and the model is trained for 50 epochs. The for the diffusion process is set to 2. The training loss is MSE loss. The training algorithm is shown in Algorithm 1, and the sampling algorithm is shown in Algorithm 2. 2 along the temporal dimension, both with stride of 1. Therefore, after the first extraction, it provides 8 different feature maps, where each feature map contains 2 temporal channels (the convolution reduces the temporal dimension by 1). The LLL is further extracted, resulting in 8 feature maps with only 1 temporal dimension. Feature maps other than LLL are concatenated in the temporal dimension, and we consider the temporal dimension as channels for model input to extract latent features. Therefore, we have feature map with 21 channels as input. 9.3. Proof We include the proof of our proposition in Sec. 3.3 of the main paper here: Proof. If H0 is rejected, then it is statistically significant to conclude that xT x0 = 0. Therefore, we can use simple induction to prove this. Recall that we have the diffusion and sampling processes defined as: q(xtx0, xT ) = (cid:18) (cid:18) pθ(xsxt, xT ) = xt x0 + (1 )xT , (cid:19) t(T t) I . (9) (xt x0), (cid:19) . (10) st 1. Based on Eq. (10) in the main paper, suppose that at given time step t, xt x0 = 0, then the expectation of sampled latent at any previous step is E(xsxt) = (xt x0) + x0. 2. By the inductive assumption, E(xsxt) = x0 + δ, where δ = 0 E(xsxt) = x0. 3. Then, on expectation, we conclude that xsxt = x0. Note that this is especially important in DDIM [40] sampling because the variance term is removed, in which case we can directly conclude that xsxt = x0 without expectation. 4. Therefore, we can prove this proposition by the above induction because the sampling process is discretized into finite steps. As result, the sampling process is not an identity map. On the other hand, the sampling process is trivial because it does not change the expectation, which can be achieved with an identity map. 3 Figure 8. Visualization of optical flows. Algorithm 1 Diffusion Training Algorithm 1: Let be the encoder part of our autoencoder. 2: for = 1 to Ntraining steps do 3: Sample ContinuousU nif orm(0, ). Sample [I0, In, I1] from Dataset. x0 = E([I0, In, I1]). Compute xt with Eq. (5). Take gradient step on SE(ϵθ(xt), xt x0) 4: 5: 6: 7: 8: end for Algorithm 2 Diffusion Sampling Algorithm 1: Let be the encoder part of our autoencoder. 2: Initialize = T, xt = E([I0, 0, I1]) 3: while > 0 do 4: Predict xt x0 with ϵθ(xt) Sample xs with Eq. (6) s, xt xs 5: 6: 7: end while 9.2. 3D Wavelet Details , 1 , 1 2 ] and low-pass filter [ 1 2 The 3D wavelet transform can be considered as convolution layer with two types of filter: high-pass filter [ 1 ].The input videos 2 (with 3 frames) are converted to grayscale, and filters are applied in height, width, and temporal dimensions respectively. There are 8 different combinations of filters: [HHH, HHL, HLH, HLL, LHH, LHL, LLH, LLL], corresponding to height, width, and temporal dimensions. We use same padding along height and width to keep the feature map size unchanged and use no padding Figure 9. Additional qualitative comparison between our method and recent SOTAs. The leftmost image is the overlaid image of I0 and I1 (blended image). Images inside blue boxes contain drastic motion changes and are cropped out to show details of interpolation results. Red circles, boxes, and arrows indicate the area where we significantly perform better. Our method achieves better visual quality than recent SOTAs. 4 Figure 10. Additional qualitative comparison between our method and recent SOTAs. The leftmost image is the overlaid image of I0 and I1 (blended image). Images inside blue boxes contain drastic motion changes and are cropped out to show details of interpolation results. Red circles, boxes, and arrows indicate the area where we significantly perform better. Our method achieves better visual quality than recent SOTAs. 5 Figure 11. Visual comparison of 8x interpolation results. We include visual comparison of 8 interpolation between our method and PerVFI. Red arrows indicate where our method is visually better. Additional comparisons (in video form) are provided in our Project Page."
        }
    ],
    "affiliations": [
        "Center for Research in Computer Vision, University of Central Florida"
    ]
}