{
    "paper_title": "MG-Nav: Dual-Scale Visual Navigation via Sparse Spatial Memory",
    "authors": [
        "Bo Wang",
        "Jiehong Lin",
        "Chenzhi Liu",
        "Xinting Hu",
        "Yifei Yu",
        "Tianjia Liu",
        "Zhongrui Wang",
        "Xiaojuan Qi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present MG-Nav (Memory-Guided Navigation), a dual-scale framework for zero-shot visual navigation that unifies global memory-guided planning with local geometry-enhanced control. At its core is the Sparse Spatial Memory Graph (SMG), a compact, region-centric memory where each node aggregates multi-view keyframe and object semantics, capturing both appearance and spatial structure while preserving viewpoint diversity. At the global level, the agent is localized on SMG and a goal-conditioned node path is planned via an image-to-instance hybrid retrieval, producing a sequence of reachable waypoints for long-horizon guidance. At the local level, a navigation foundation policy executes these waypoints in point-goal mode with obstacle-aware control, and switches to image-goal mode when navigating from the final node towards the visual target. To further enhance viewpoint alignment and goal recognition, we introduce VGGT-adapter, a lightweight geometric module built on the pre-trained VGGT model, which aligns observation and goal features in a shared 3D-aware space. MG-Nav operates global planning and local control at different frequencies, using periodic re-localization to correct errors. Experiments on HM3D Instance-Image-Goal and MP3D Image-Goal benchmarks demonstrate that MG-Nav achieves state-of-the-art zero-shot performance and remains robust under dynamic rearrangements and unseen scene conditions."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 9 0 6 2 2 . 1 1 5 2 : r MG-Nav: Dual-Scale Visual Navigation via Sparse Spatial Memory Bo Wang1, Jiehong Lin1, Chenzhi Liu1 Xinting Hu1 Yifei Yu1 Zhongrui Wang2, Xiaojuan Qi1, Tianjia Liu1 1The University of Hong Kong 2Southern University of Science and Technology wangzr@sustech.edu.cn xjqi@eee.hku.hk Equal contribution Corresponding authors Figure 1. Overview of the proposed MG-Nav, dual-scale framework that unifies global planning and local control for zero-shot visual navigation. (a) Global planning: MG-Nav plans over Sparse Spatial Memory Graph (SMG), compact region-centric memory that mirrors human navigation by providing node-level guidance without requiring dense 3D reconstruction. (b) Local navigation: MG-Nav employs navigation foundation policies enhanced with VGGT geometric features to improve goal recognition and enable obstacle-aware control. MG-Nav operates global planning and local navigation at different frequencies and uses periodic re-localization to correct errors, which effectively handles dynamic changes and avoids collisions compared to methods that rely on dense 3D reconstruction."
        },
        {
            "title": "Abstract",
            "content": "We present MG-Nav (Memory-Guided Navigation), dual-scale framework for zero-shot visual navigation that unifies global memory-guided planning with local geometry-enhanced control. At its core is the Sparse Spatial Memory Graph (SMG), compact, region-centric memory where each node aggregates multi-view keyframe and object semantics, capturing both appearance and spatial structure while preserving viewpoint diversity. At the global level, the agent is localized on SMG and goal-conditioned node path is planned via an image-to-instance hybrid retrieval, producing sequence of reachable waypoints for long-horizon guidance. At the local level, navigation foundation policy executes these waypoints in point-goal mode with obstacle-aware control, and switches to imagegoal mode when navigating from the final node towards the visual target. To further enhance viewpoint align1 ment and goal recognition, we introduce VGGT-adapter, lightweight geometric module built on the pre-trained VGGT model, which aligns observation and goal features in shared 3D-aware space. MG-Nav operates global planning and local control at different frequencies, using periodic re-localization to correct errors. Experiments on HM3D Instance-Image-Goal and MP3D Image-Goal benchmarks demonstrate that MG-Nav achieves state-ofthe-art zero-shot performance and remains robust under dynamic rearrangements and unseen scene conditions. 1. Introduction Navigation is fundamental capability for embodied agents to interact intelligently with their surroundings [18, 32]. From domestic robots and delivery drones to AR/VR telepresence systems, the ability to reach visual target in previously unseen environments, without explicit maps or dense supervision, remains key milestone toward general embodied intelligence. Within this context, visual navigation, where the goal is specified by an image depicting either target object instance [11, 12, 36] or scene [1, 17, 34], is particularly valuable yet challenging. The agent must infer the underlying 3D structure from single goal image, reason over novel spatial layouts, and accurately reach the corresponding viewpoint or object while avoiding collisions and adapting to dynamic scene changes. Existing frameworks for visual navigation can be broadly categorized into three families, each facing inherent limitations in unseen scenarios. (i) Foundation policy models [2, 29] leverage large-scale trajectory pretraining to achieve strong generalization, yet struggle when the goal is invisible, often degenerating into unguided exploration with (ii) Reinforcement weak long-range reasoning [25, 27]. Learning (RL) methods [15, 21, 30] excel in fine-grained control within known environments but demand massive interaction data and fail to generalize under domain shifts. (iii) Memory-based zero-shot approaches construct persistent global maps [9, 12, 24] or scene graphs [36] to enable long-horizon planning without retraining. However, these methods typically rely on dense RGB-D reconstructions, which are expensive to build and brittle to even mild rearrangements or dynamic changes after memory construction. In light of these limitations, it is instructive to revisit how humans navigate, which offers striking contrast and source of inspiration. Unlike artificial agents that depend on dense maps or precise metric reconstructions, humans navigate effectively through sparse visual memories, limited set of distinctive snapshots that anchor our sense of place and orientation [5, 6, 28]. Such memories provide coarse yet reliable global guidance, enabling us to recall spatial relations and approximate locations even after long intervals or environmental changes. Concurrently, humans engage in continuous local replanning, dynamically adapting motion to avoid obstacles, resolve occlusions, and respond to unforeseen scene variations [5, 6]. This hierarchical strategy, i.e., global guidance from sparse memory coupled with local reactive contron, forms the foundation of robust and adaptive navigation in complex, dynamic environments. Inspired by this, we propose MG-Nav (Memory-Guided Navigation), dual-scale framework that integrates global memory-guided planning with local geometry-enhanced control. Fig. 1 gives an overview of MG-Nav. At its core lies the Sparse Spatial Memory Graph (SMG), compact, region-centric representation of the explored environment. Each node in the SMG corresponds to spatial region and aggregates small set of multi-view keyframes together with instance-level semantics, while edges encode navigable connectivity between regions. This sparse abstraction mirrors human navigation practice, capturing distinctive visual memories that preserve viewpoint diversity and semantic consistency without relying on dense 3D reconstruction. At the global level, MG-Nav performs high-level planning over SMG. Given visual goal, the agent is first localized on SMG through an image-to-instance hybrid retrieval mechanism that jointly matches both the current observation and the goal image to their corresponding nodes. Based on the matched nodes, goal-conditioned node path is planned along the edges of SMG, yielding sequence of reachable waypoints that provide global guidance and decompose the long-horizon navigation into node-to-node traversals. At the local level, we employ zero-shot foundation policy, pre-trained on large-scale data, which demonstrates strong obstacle avoidance and robustness to dynamic scene changes, to execute motion between adjacent SMG nodes. To further enhance geometric reasoning and visual goal alignment, we introduce VGGT-adapter, which incorporates geometry-aware features from the foundational Visual Geometry Group Transformer (VGGT) model [31] into the policy. This integration preserves spatial consistency under rapid viewpoint changes and improves precise alignment with the visual goal, particularly during the final approach. Global planning and local control operate at different frequencies, with periodic re-localization and re-planning, to absorb execution noise and recover from errors. By seamlessly combining global node-level guidance with local, goal-directed control, MG-Nav achieves robust zeroshot visual navigation in complex, dynamic environments. Extensive experiments show that MG-Nav attains stateof-the-art results on both HM3D Instance-Image-Goal and MP3D Image-Goal, reaching 78.5/59.3 and 83.8/57.1 (SR/SPL). Ablation studies verify the effectiveness of the different components of MG-Nav. Moreover, in dynamic environments with newly inserted obstacles, MG-Nav remains stable with only minor performance drops, demonstrating strong robustness. 2 Our main contributions are summarized as follows: Dual-scale memory-guided navigation. We present MG-Nav, unified architecture that integrates high-level semantictopological planning with low-level geometryenhanced control, enabling robust zero-shot navigation. Sparse Spatial Memory Graph (SMG). regioncentric, multi-view memory graph representation that encodes both keyframe and object semantics, supporting efficient hybrid retrieval and node-level global planning. Geometric enhancement for local navigation. We introduce lightweight VGGT-adapter that aligns observation and goal embeddings in shared 3D-aware space using VGGT features, substantially improving viewpoint robustness and goal matching precision. Empirical results. MG-Nav achieves the state-of-theart zero-shot performance on HM3D/MP3D benchmarks, and maintains robustness under dynamic scene changes. 2. Related Work Memory-Free Foundation Policy Methods. Foundation policies (e.g., GNM, ViNT, NoMaD, NavDP [2, 26, 27, 29]) provide reliable short-horizon, zero-shot control but remain memory-free, relying on reactive visual similarity without global state and leading to failures when the goal is out of view and weak long-range reasoning. Memory-Free Reinforcement Learning Methods. Endto-end RL approaches [10, 15, 21, 30, 35] target finegrained control and incorporate LLMs for high-level command interpretation (e.g., CompassNav [14]). Yet they lack explicit global memory and often generalize poorly to unseen layouts and object configurations, struggling to maintain instanceimage goal consistency over long horizons. Memory-Based Methods. long-range planning, memory-based methods build global scene representations. Metric or dense maps (e.g., BSC-Nav, GOAT, IEVE, MODIIN [9, 11, 12, 24]) and neural volumetric models (e.g., GaussNav, GSplatVNM [8, 13]) offer accurate pose fidelity but are heavy and brittle under rearrangements. Sparse graphs are more efficient, yet object-centric graphs (e.g., UniGoal [36]) lose local context, while topology-heavy systems (e.g., Astra, Mobility-VLN [4, 33]) add complexity by intertwining graph reasoning with LLM-based planning. We address these trade-offs with sparse, regioncentric Scene Memory Graph (SMG) that preserves local context for hybrid retrieval and dual-scale design combining SMG-based global planning with robust local control. For 3. Memory-Guided Navigation 3.1. Overview Visual navigation aims to predict sequence of actions = (a1, . . . , aM ) that guide an agent to visual goal Igoal depicting the target object or scene, where each action is defined by planar displacement and orientation. To address this task, we propose MG-Nav (Memory-Guided Navigation), dual-scale framework that integrates global planning with local execution. An overview of the MG-Nav workflow is shown in Fig. 2. The core idea is that small number of distinctive visual memories provide coarse global guidance, while local motion is continuously adapted to avoid obstacles and dynamic changes. To embody this idea, we introduce Sparse Spatial Memory Graph (SMG) as an abstract scene representation (Sec. 3.2). Formally, SMG is defined as = (V, E), where nodes function as sparse, memorable spatial anchors, and edges encode their navigable connectivity. Upon G, MGNav performs dual-scale navigation: (i) global planning, which retrieves the memory node vgoal best aligned with the goal image Igoal and plans node-level path (v1, . . . , vK1, vK) along (Sec. 3.3), where vK corresponds to the exact visual goal Igoal with vK1 = vgoal; and (ii) local navigation, where geometry-enhanced diffusion policy navigates between consecutive nodes to constitute the whole action sequence = (A1, . . . , AK1, AK) (Sec. 3.4). By operating global planning and local navigation at different frequencies, with periodic re-localization and re-planning, MG-Nav mitigates execution noise and drift, and thus enables robust zero-shot navigation in dynamic and previously unseen environments (Sec. 3.5). 3.2. Sparse Spatial Memory Graph Sparse Spatial Memory Graph (SMG) = (V, E) forms the core of MG-Nav to enable global planning. The construction process of is illustrated in Fig. 3. More specifically, for each indoor scene, we first follow standard practice to collect posed tour demonstrations [13, 24]. We then apply Farthest-Point Sampling (FPS) to the extrinsic camera poses of the demonstration frames to obtain sparse but spatially representative locations. Around each sampled location, we define region of radius as spatial node V, while the edge set is derived from temporal adjacency during the tours, which naturally ensures that connected nodes correspond to mutually reachable regions in the physical environment. We further represent each spatial node as structural embedding = (P, F, O), where R3 denotes the 3D center of the region, RNf contains Nf keyframe embeddings, and RNoC contains No representative object embeddings, with as the feature dimension. Concretely, is set as the sampled camera location of v, while and are obtained as follows: (i) Keyframe embeddings - We extract the CLS token from DINOv2 [19] for demonstration frames within the local region of v. To ensure diversity, we first select frames with the lowest feature similarity and then pick the final Nf frames with the largest camera rotation variance 3 Figure 2. Illustration of the navigation process of MG-Nav, dual-scale framework combining global planning with local execution. (a) Sparse Spatial Memory Graph (SMG) serves as compact, region-centric memory; each node aggregates multi-view keyframes and object semantics, while edges encode navigable connectivity. (b) Global Planning with SMG: Both the agent and the goal are localized on the SMG via an image-to-instance hybrid node retrieval. goal-conditioned path from the current observation at time to the goal is then planned along the graph edges to provide global guidance. (c) Local Navigation via Geometry-Enhanced Policy: navigation foundation policy, geometrically enhanced with the VGGT-adapter, moves the agent between adjacent nodes while maintaining obstacle avoidance and accurate visual goal alignment. By running global planning (f1) and local navigation (f2) at different frequencies with periodic re-localization, MG-Nav achieves robust zero-shot navigation in dynamic, unseen environments. among them. The embeddings of these selected frames form F, providing rich regional context. (ii) Object embeddings - We apply the open-vocabulary segmentation model Grounded-SAM [23] to all frames within the region of to extract object instances. For each segmented object, we use DINOv2 to obtain embeddings from the CLS token and the average patch features, which are combined via weighted summation to form robust object embeddings. Objects of the same category whose embedding cosine similarity exceeds threshold τ are further considered the same instance, and their embeddings are averaged to produce unique feature in O. SMG provides sparse yet spatially representative structure that integrates information from both multi-view keyframes and object instances, thus preserving distinctive visual memory anchors to simplify the planning space. 3.3. Global Planning with SMG As shown in Fig. 2, global planning on SMG = (V, E) consists of two steps, including (i) matching both the current observation Iobs and the visual goal Igoal to nodes in V, and (ii) planning node-level trajectory along SMG edges that provides high-level guidance for local navigation. 3.3.1. Image-Node Matching Given an input image (Iobs or Igoal), we aim to retrieve the node in most semantically and visually aligned with it, using hybrid retrieval strategy combining global scene similarity and object-level semantics: (i) Keyframe retrieval - For each node V, we compute the cosine similarity between the DINOv2 CLS embed4 viating the difficulty of fine-grained motion control toward the final visual goal. Building upon this, we further address the zero-shot local navigation between adjacent nodes using general navigation foundation policies, and introduce VGGT-adapter for geometric enhancement to improve visual goal matching capacity of policies. 3.4.1. Geometry-Enhanced Policy with VGGT-adapter Navigation foundation policies are typically formulated as = π(Iobs, Pgoal, Igoal), where the goal is specified either as point Pgoal or an image Igoal. Although these policies are trained on large-scale trajectories and demonstrate strong short-horizon navigation capabilities with obstacle avoidance and robustness to dynamic scene changes, they still struggle to precisely localize and pursue visual goals, especially under significant viewpoint variations. To mitigate this limitation, we introduce the VGGTadapter, which integrates geometry-aware features derived from the pre-trained Visual Geometry Group Transformer (VGGT) model [31] into the policy. VGGT inherently models multi-view 3D structure and pixel correspondences, making it suitable for enhancing spatial reasoning and improving visual goal matching during navigation. obs and FG More specifically, we use VGGT to extract geometryaware feature maps from both the current observation Iobs and the visual goal Igoal. After applying spatial average pooling, the resulting feature maps are flattened into token sequences FG goal. The two token sequences are then concatenated and passed through the VGGTadapter, implemented as lightweight multi-layer perceptron (MLP), which projects the geometry tokens into the policy embedding space as FG goal]). Finally, FG adpt is concatenated with the original visual condition tokens of the navigation policy, thereby enriching it with 3D geometric cues and inter-frame correspondence awareness to strengthen visual goal matching. adpt = MLP([FG obs FG 3.4.2. Node-based Local Navigation With the node-level path {v1, . . . , vK} obtained from SMG, our geometric-enhanced policy π is employed to perform local navigation in node-to-node manner to progressively approach the final visual goal while avoiding obstacles. For each step k, the agent navigates from the current node vk1 to the next node vk by predicting chunk of executable actions: Ak = (cid:40) π(Ik, Pk, ) = π(Ik, , Igoal) = , (1) where Ik denotes the current observation and Pk is the 3D location of node vk. Accordingly, the policy operates in point-goal mode when moving between intermediate nodes, and switches to image-goal mode only at the final stage to precisely align with the visual goal Igoal, befitting from the geometric enhancement of VGGT-adapter. 5 Figure 3. Illustration of the construction of Sparse Spatial Memory Graph. Each node in SMG represents spatial region, aggregating small set of both multi-view keyframe and object embeddings, while edges between nodes encode navigable connectivity. ding of the input image and each of the Nf keyframe embeddings F, taking the maximum similarity as the keyframe score of the node. The top-No nodes with the highest keyframe scores are retained as candidates for subsequent object-level verification. (ii) Object retrieval - Following the procedure in Sec. 3.2, we apply Grounded-SAM for open-vocabulary segmentation and DINOv2 for feature extraction to obtain object embeddings of the image. For each object embedding in the image, we compute its cosine similarity with object embeddings of the same category in each node candidate, taking the maximum score; if node does not contain that category, the score is set to 0. For each node, we average the scores across all objects to obtain its object similarity score. Finally, for the top-No node candidates, we average their keyframe and object similarity scores, and select the node with the highest combined score as the retrieval result. 3.3.2. Node-Level Path Planning Based on the hybrid retrieval results, we obtain the matched nodes vobs and vgoal corresponding to Iobs and Igoal. We then perform high-level planning on SMG by applying A* search [16] over the edge set E. Since each edge denotes physically feasible transition observed in demonstrations, the resulting path provides reliable structural prior for navigation. The planned node-level trajectory is denoted as (v1, . . . , vK1, vK), where v1 = vobs and vK1 = vgoal; we additionally append vK to represent Igoal, ensuring that the node sequence explicitly terminates at the goal state. 3.4. Local Navigation via Geometry-Enhanced Policy The node-level path planned on SMG decomposes navigation into sequence of intermediate sub-goals, thereby alleThe overall navigation action sequence is thus given as = (A1, . . . , AK1, AK). Table 1. InstanceImageNav on HM3D. Foundation models are shown on top, RL-based methods are shown on middle, and Memory-based methods are shown on bottom. 3.5. Dual-scale Planning Loop MG-Nav coordinates planning and control at two frequencies: slow global loop (every Tg steps) and fast local loop (every Tℓ Tg). The global loop re-localizes the agent on SMG via hybrid retrieval and updates the node-level path (Sec. 3.3) whenever confidence drops or edges become blocked, yielding an updated node sequence. The local loop continuously executes short-horizon actions toward the current node while performing obstacle-aware control (Sec. 3.4). At runtime, MG-Nav alternates between two tightly coupled processes: Local navigation (Sec. 3.4): The geometry-enhanced policy executes Ak to reach the current node vk, updating visual feedback at each step. Periodic global re-localization and planning (Sec. 3.3): Every Tg local stepsor when the visual-confidence score dropsthe agent is re-localized on SMG and A* is re-invoked to refine the remaining path. This asynchronous, dual-frequency scheme mitigates drift, adapts to dynamic scene changes, and preserves longhorizon goal intent with minimal computational overhead. 4. Experiments 4.1. Experimental Setup Benchmarks: Our method is tested on two 3D scene datasets, Habitat-Matterport 3D (HM3D) [22] for instance image-goal navigation (InstanceImageNav) and Matterport3D (MP3D) [3] for image-goal navigation (ImageNav). Both tasks require the agent to navigate toward the target depicted in given goal image, but ImageNav targets an entire scene or viewpoint, while InstanceImageNav specifies particular object instance. For HM3D, we conduct experiments on 1000 episodes of 36 validation scenes following the Instance ImageNav-v3. For MP3D ImageNav, we conduct experiments on 1014 episodes of 5 testing scenes. Evaluation Metrics: We evaluate with Success Rate (SR) to measure the proportion of successful episodes, and Success Rate weighted by Path Length (SPL) [11, 36] to assess navigation efficiency by accounting for path optimality. Implementation Details: Our agent is deployed in Habitat Simulator [20]. We employ DINOv2-ViT-L/14 [19] for visual encoding, Grounded-SAM-2 [23] for semantic segmentation, and VGGT [31] for geometric embedding extraction. We use NavDP [2] as the foundation policy for MG-Navs local navigation module. The success distance threshold is 1.0 and the maximum episode length is 500 steps. Hyperparameters are detailed in appendix. Method ViNT* [27] GNM* [26] NoMAD* [29] NavDP* [2] RL Baseline [10] FGPrompt [30] OVRL-v2 IIN [35] CompassNav [14] Memory SR SPL 6.7 7.7 5.6 11.4 16.8 7.0 24.7 12. 3.5 8.3 9.9 2.8 24.8 11.8 35.6 14.8 GOAT [9] MOD-IIN [11] UniGoal [36] IEVE [12] BSC-Nav [24] GaussNav [13] MG-Nav(Ours) * Result from our re-implementation following their official code. Result from GaussNav. 37.4 16.1 Scene Map (RGBD) Scene Map (RGBD) 56.1 23.3 Object Graph (RGBD) 60.2 23.7 70.2 25.2 Scene Map (RGBD) Scene Map (RGBD) 71.4 57.2 3DGS Map (RGBD) 72.5 57.8 78.5 59.3 SMG (RGB) Table 2. ImageNav on MP3D. Foundation models are shown on top, RL-based methods are shown on middle, and Memory-based methods are shown on bottom. Method ViNT* [27] GNM* [26] NoMAD* [29] NavDP* [2] RSFG* [7] FGPrompt-MF* [30] REGNav* [15] FGPrompt-EF* [30] Memory SR SPL 6.12 10.06 10.95 15.49 68.54 71.20 74.66 77.71 4.98 7.36 5.64 7.96 43.18 39.74 47.24 51.09 MG-Nav(Ours) * Result from our re-implementation following their official code. SMG (RGB) 83.77 57.15 4.2. Main Results HM3D InstanceImageNav. Table 1 reports HM3D InstanceImageNav results. Foundation policies (e.g., NavDP [2], 24.7/12.6) and RL methods (e.g., CompassNav [14], 35.6/14.8) suffer from poor zero-shot generalization due to reactive local planning and weak global reasoning. While Memory-based methods (e.g., BSC-Nav [24], 71.4/57.2; GaussNav [13], 72.5/57.8) achieve notable accuracy, they remain limited by maps that lack robust instance semanFigure 4. Illustration of the decision process of MG-Nav. Step 1 shows initial selfand goal-localization on SMG followed by global planning. Step 73 shows node-to-node navigation using point mode of NavDP, with periodic global re-localization. Step 140 shows the agent entering the matched goal-node region. Step 164 shows the policy switching to image mode and successfully verifying the target. tics and rely on unstable low-level planners. MG-Nav addresses these limitations by pairing sparse, region-centric SMG with foundation policy enhanced by VGGT-based adapter for robust navigation and viewpoint alignment. This synthesis establishes new state-of-the-art of SR 78.5 and yields shorter, more reliable paths (SPL 59.3). The successful example of the task executed by MG-Nav is shown in Fig. 4, and other examples are detailed in appendix. MP3D ImageNav. Table 2 reports overall MP3D ImageNav results. MG-Nav attains 83.77/57.15 (SR/SPL), surpassing the strongest RL baseline FGPrompt-EF [30] 77.71/51.09 by 6.06/6.06. Foundation policies remain far lower, for example, NavDP [2] performs 15.49 and 7.96, where our method surpasses by 68.28/49.19. The huge improvement demonstrates that our dual-scale navigation system via SMG provide reliable global planning and robust view alignment, yielding state-of-the-art performance. 4.3. Ablation Study Component ablation. Table 3 reports detailed ablation on model components. The foundation model achieves only 24.7/12.6 (SR/SPL), limited by its purely reactive control and lack of long-range planning. Introducing SMG for global planning significantly boosts performance to 74.04/56.14, as it decomposes long-range navigation into reachable node-to-node subgoals, providing global guidance for the agent. Further incorporating the VGGT-adapter improves results to 78.50/59.27, confirming that geometryaware observationgoal alignment enhances robustness during viewpoint shifts and final approach stages. Retrieval strategy ablation. Our hybrid retrieval strategy, which combines keyframe (global) and object-level semantics, achieves the best performance (78.50/59.27 in SR/SPL). We perform an ablation to study the necessity of each component. Relying only on global keyframe retrieval degrades performance by large margin (from 78.50/59.27 to 73.52/52.90), and using only object-level retrieval also results in substantial drop (to 72.20/52.79). This confirms the need for hybrid strategy. Global appearance enables coarse scene localization and long-range recall, while instance semantics enhance object-level discrimination and viewpoint invariance. Combining both yields more reliable and robust matching. Graph sparsity ablation. We analyze the impact of the SMG structure by adjusting node spacing (d) and coverage radius (r). The configuration (d=1.0 m, r=0.5 m) achieves the best 78.50/59.27 (SR/SPL). Making the graph moderately sparser to (1.5 m, 0.8 m) yields 77.10/54.79 (1.40/4.48). Further sparsifying to (2.0 m, 1.0 m) degrades to 70.69/46.89 (7.81/12.38). The sharper decline in SPL indicates that overly sparse graphs reduce viewpoint diversity and weaken topological connectivity, leading to longer local execution segments and detours. 4.4. Robustness to Dynamic Scene Changes To assess robustness to unmodeled dynamic scene changes, we first construct the Memory Graph on the original HM3D scenes. We then insert different numbers (0/5/10) of random obstacles into the environment during the navigation phase to simulate dynamic environment  (Fig. 5)  . These newly inserted objects serve as dynamic obstacles that the agent must robustly avoid to reach its goal. The performance of MG-Nav under these conditions is then compared with representative mapping-based methods  (Table 4)  . We observe that methods degrade sharply (e.g., BSC-Nav [24] SR 25.49 7.84, UniGoal [36] SR 56.43 44.2), while MG-Nav shows only minor drops (SR 73.53 68.63 and SPL 56.28 7 Table 3. Ablation study of our method. We report SR/SPL while jointly exposing graph sparsity (node spacing d) and coverage radius r. Global means keyframe retrieval, Object means object retrieval. Ablation Variant Graph Retrieval VGGT-adapter (m) (m) SR / SPL Component Retrieval Graph sparsity Foundation Policy (NavDP) + SMG + VGGT-adapter Instance Match only Global Match only Global + Instance Match Graph 1 Graph 2 Graph 3 Global+Object Global+Object Global+Object Global+Object Global+Object Global+Object Object Global 1.0 1.0 1.0 1.0 1.0 2.0 1.5 1. 0.5 0.5 0.5 0.5 0.5 1.0 0.8 0.5 24.70 / 12.60 74.04 / 56.14 78.50 / 59.27 72.20 / 52.79 73.52 / 52.90 78.50 / 59.27 70.69 / 46.89 77.10 / 54.79 78.50 / 59. Figure 5. Illustration of the robustness to dynamic scene changes of MG-Nav and UniGoal. 10 additional obstacles are added to scene mv2HUxq3B53 (left) to model dynamic scenarios (middle). UniGoal becomes trapped near the inserted obstacles and keeps wandering in local region until timeout (right, green path), whereas MG-Nav successfully avoids the newly added obstacles and reaches the goal (right, red path), demonstrating strong robustness to unmodeled scene rearrangements. Table 4. Robustness to dynamic scene changes. We evaluate each model on 100 InstanceImageNav episodes from HM3D under our dynamic-scene setting. Results for UniGoal and BSC-Nav are obtained from our re-implementations based on their official code. Obs. Num 0 5 Method SR SPL SR SPL SR SPL BSC-Nav [24] 25.49 19.91 8.64 UniGoal [36] Ours 4.94 56.43 20.44 52.94 19.68 44.21 17.17 73.53 56.28 72.55 52.20 68.63 50.15 7.84 4. 50.15). This resilience stems from decoupled dualscale design. The sparse SMG delivers robust region-level global planning, keeping the navigational goal stable, while the zero-shot local policy handles unmodeled obstacles and avoids them without relying on the global map, resulting in only minor drops in SR and SPL. 8 5. Conclusion We introduced MG-Nav, dual-scale navigation framework that combines global memory-guided planning with local geometry-enhanced control for zero-shot visual navigation. The core Sparse Spatial Memory Graph (SMG) provides compact, region-centric representation, enabling long-horizon reasoning and robustness to moderate scene changes. Global planning retrieves goal-conditioned node paths, while pre-trained navigation policy, augmented with the VGGT-adapter, executes node-to-node motion with enhanced visual and geometric alignment. Experiments show that MG-Nav achieves state-of-the-art zero-shot performance on challenging Image-Goal and Instance-Goal tasks, generalizing effectively to novel and dynamic environments. We believe that the dual-scale navigation with sparse spatial memory can inspire future research in scalable and robust embodied navigation, especially for agents operating in complex, unseen, or dynamic spaces."
        },
        {
            "title": "References",
            "content": "[1] Ziad Al-Halah, Santhosh Kumar Ramakrishnan, and Kristen Grauman. Zero experience required: Plug & play modular transfer learning for semantic visual navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1703117041, 2022. 2 [2] Wenzhe Cai, Jiaqi Peng, Yuqiang Yang, Yujian Zhang, Meng Wei, Hanqing Wang, Yilun Chen, Tai Wang, and Jiangmiao Pang. Navdp: Learning sim-to-real navigation diffusion policy with privileged information guidance. arXiv preprint arXiv:2505.08712, 2025. 2, 3, 6, 7 [3] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgbd data in indoor environments. International Conference on 3D Vision (3DV), 2017. 6 [4] Sheng Chen, Peiyu He, Jiaxin Hu, Ziyang Liu, Yansheng Wang, Tao Xu, Chi Zhang, Chongchong Zhang, Chao An, Shiyu Cai, et al. Astra: Toward general-purpose mobile robots via hierarchical multimodal learning. arXiv preprint arXiv:2506.06205, 2025. 3 [5] Arne Ekstrom and Paul Hill. Spatial navigation and memory: review of the similarities and differences relevant to brain models and age. Neuron, 111(7):10371049, 2023. 2 [6] Russell Epstein, Eva Zita Patai, Joshua Julian, and Hugo Spiers. The cognitive map in humans: spatial navigation and beyond. Nature neuroscience, 20(11):15041513, 2017. [7] Zhicheng Feng, Xieyuanli Chen, Chenghao Shi, Lun Luo, Zhichao Chen, Yun-Hui Liu, and Huimin Lu. Image-goal navigation using refined feature guidance and scene graph enhancement. arXiv preprint arXiv:2503.10986, 2025. 6 [8] Kohei Honda, Takeshi Ishita, Yasuhiro Yoshimura, and Ryo Yonetani. Gsplatvnm: Point-of-view synthesis for visual navigation models using gaussian splatting. arXiv preprint arXiv:2503.05152, 2025. 3 [9] Mukul Khanna, Ram Ramrakhya, Gunjan Chhablani, Sriram Yenamandra, Theophile Gervet, Matthew Chang, Zsolt Kira, Devendra Singh Chaplot, Dhruv Batra, and Roozbeh Mottaghi. Goat-bench: benchmark for multi-modal lifelong In Proceedings of the IEEE/CVF Conference navigation. on Computer Vision and Pattern Recognition, pages 16373 16383, 2024. 2, 3, 6 [10] Jacob Krantz, Stefan Lee, Jitendra Malik, Dhruv Batra, and Devendra Singh Chaplot. Instance-specific image goal navigation: Training embodied agents to find object instances. arXiv preprint arXiv:2211.15876, 2022. 3, 6 [11] Jacob Krantz, Theophile Gervet, Karmesh Yadav, Austin Wang, Chris Paxton, Roozbeh Mottaghi, Dhruv Batra, Jitendra Malik, Stefan Lee, and Devendra Singh Chaplot. NavIn Proceedings of igating to objects specified by images. the IEEE/CVF International Conference on Computer Vision, pages 1091610925, 2023. 2, 3, 6 [12] Xiaohan Lei, Min Wang, Wengang Zhou, Li Li, and Houqiang Li. Instance-aware exploration-verificationexploitation for instance imagegoal navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1632916339, 2024. 2, 3, 6 [13] Xiaohan Lei, Min Wang, Wengang Zhou, and Houqiang Li. IEEE Gaussnav: Gaussian splatting for visual navigation. Transactions on Pattern Analysis and Machine Intelligence, 2025. 3, [14] LinFeng Li, Jian Zhao, Yuan Xie, Xin Tan, and Xuelong Li. Compassnav: Steering from path imitation to decision understanding in navigation. arXiv preprint arXiv:2510.10154, 2025. 3, 6 [15] Pengna Li, Kangyi Wu, Jingwen Fu, and Sanping Zhou. RegIn Pronav: Room expert guided image-goal navigation. ceedings of the AAAI Conference on Artificial Intelligence, pages 48604868, 2025. 2, 3, 6 [16] Xiang Liu and Daoxiong Gong. comparative study of astar algorithms for search and rescue in perfect maze. In 2011 international conference on electric information and control engineering, pages 2427. IEEE, 2011. 5 [17] Lina Mezghan, Sainbayar Sukhbaatar, Thibaut Lavril, Oleksandr Maksymets, Dhruv Batra, Piotr Bojanowski, and Karteek Alahari. Memory-augmented reinforcement learning for image-goal navigation. In 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 33163323. IEEE, 2022. 2 [18] Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew Ballard, Andrea Banino, Misha Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu, et al. Learning to navigate in complex environments. arXiv preprint arXiv:1611.03673, 2016. 2 [19] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, ShangWen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. 3, [20] Xavi Puig, Eric Undersander, Andrew Szot, Mikael-Dallaire Cote, Ruslan Partsey, Jimmy Yang, Ruta Desai, Alexander William Clegg, Michal Hlavac, Tiffany Min, Theo Gervet, Vladimır Vondruˇs, Vincent-Pierre Berges, John Turner, Oleksandr Maksymets, Zsolt Kira, Mrinal Kalakrishnan, Jitendra Malik, Devendra Singh Chaplot, Unnat Jain, Dhruv Batra, Akshara Rai, and Roozbeh Mottaghi. Habitat 3.0: co-habitat for humans, avatars and robots, 2023. 6 [21] Zheng Qin, Le Wang, Yabing Wang, Sanping Zhou, Gang Hua, and Wei Tang. Rsrnav: Reasoning spatial relationship for image-goal navigation. arXiv preprint arXiv:2504.17991, 2025. 2, 3 [22] Santhosh Kumar Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alexander Clegg, John Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel Chang, Manolis Savva, Yili Zhao, and Dhruv Batra. Habitat-matterport 3d dataset (HM3d): 1000 largescale 3d environments for embodied AI. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. 6 9 [35] Karmesh Yadav, Ram Ramrakhya, Santhosh Kumar Ramakrishnan, Theo Gervet, John Turner, Aaron Gokaslan, Noah Maestre, Angel Xuan Chang, Dhruv Batra, Manolis Savva, et al. Habitat-matterport 3d semantics dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 49274936, 2023. 3, 6 [36] Hang Yin, Xiuwei Xu, Linqing Zhao, Ziwei Wang, Jie Zhou, and Jiwen Lu. Unigoal: Towards universal zero-shot goaloriented navigation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1905719066, 2025. 2, 3, 6, 7, [23] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks, 2024. 4, 6 [24] Shouwei Ruan, Liyuan Wang, Caixin Kang, Qihui Zhu, Songming Liu, Xingxing Wei, and Hang Su. From reactive to cognitive: brain-inspired spatial intelligence for embodied agents. arXiv preprint arXiv:2508.17198, 2025. 2, 3, 6, 7, 8 [25] Matt Schmittle, Rohan Baijal, Nathan Hatch, Rosario Scalise, Mateo Guaman Castro, Sidharth Talia, Khimya Khetarpal, Byron Boots, and Siddhartha Srinivasa. Long range navigator (lrn): Extending robot planning horizons beyond metric maps. arXiv preprint arXiv:2504.13149, 2025. 2 [26] Dhruv Shah, Ajay Sridhar, Arjun Bhorkar, Noriaki Hirose, and Sergey Levine. Gnm: general navigation model to drive any robot. arXiv preprint arXiv:2210.03370, 2022. 3, 6 [27] Dhruv Shah, Ajay Sridhar, Nitish Dashora, Kyle Stachowicz, Kevin Black, Noriaki Hirose, and Sergey Levine. Vint: foundation model for visual navigation. arXiv preprint arXiv:2306.14846, 2023. 2, 3, 6 [28] Alexander Siegel and Sheldon White. The development of spatial representations of large-scale environments. Advances in child development and behavior, 10:955, 1975. 2 [29] Ajay Sridhar, Dhruv Shah, Catherine Glossop, and Sergey Levine. Nomad: Goal masked diffusion policies for navIn 2024 IEEE International Conigation and exploration. ference on Robotics and Automation (ICRA), pages 6370. IEEE, 2024. 2, 3, [30] Xinyu Sun, Peihao Chen, Jugang Fan, Jian Chen, Thomas Li, and Mingkui Tan. Fgprompt: fine-grained goal prompting for image-goal navigation. Advances in Neural Information Processing Systems, 36:1205412073, 2023. 2, 3, 6, 7 [31] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 52945306, 2025. 2, 5, 6 [32] Lik Hang Kenny Wong, Xueyang Kang, Kaixin Bai, and Jianwei Zhang. survey of robotic navigation and manipulation with physics simulators in the era of embodied ai. arXiv preprint arXiv:2505.01458, 2025. 2 [33] Zhuo Xu, Hao-Tien Lewis Chiang, Zipeng Fu, Mithun George Jacob, Tingnan Zhang, Tsang-Wei Edward Lee, Wenhao Yu, Connor Schenck, David Rendleman, Dhruv Shah, et al. Mobility vla: Multimodal instruction navigation with long-context vlms and topological graphs. In 8th Annual Conference on Robot Learning, 2024. 3 [34] Karmesh Yadav, Ram Ramrakhya, Arjun Majumdar, Vincent-Pierre Berges, Sachit Kuhar, Dhruv Batra, Alexei Baevski, and Oleksandr Maksymets. Offline visual representation learning for embodied navigation. In Workshop on Reincarnating Reinforcement Learning at ICLR 2023, 2023."
        }
    ],
    "affiliations": [
        "Southern University of Science and Technology",
        "The University of Hong Kong"
    ]
}