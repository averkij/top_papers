{
    "paper_title": "M$^3$FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial Meeting Understanding Evaluation Dataset",
    "authors": [
        "Jie Zhu",
        "Junhui Li",
        "Yalong Wen",
        "Xiandong Li",
        "Lifan Guo",
        "Feng Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent breakthroughs in large language models (LLMs) have led to the development of new benchmarks for evaluating their performance in the financial domain. However, current financial benchmarks often rely on news articles, earnings reports, or announcements, making it challenging to capture the real-world dynamics of financial meetings. To address this gap, we propose a novel benchmark called $\\texttt{M$^3$FinMeeting}$, which is a multilingual, multi-sector, and multi-task dataset designed for financial meeting understanding. First, $\\texttt{M$^3$FinMeeting}$ supports English, Chinese, and Japanese, enhancing comprehension of financial discussions in diverse linguistic contexts. Second, it encompasses various industry sectors defined by the Global Industry Classification Standard (GICS), ensuring that the benchmark spans a broad range of financial activities. Finally, $\\texttt{M$^3$FinMeeting}$ includes three tasks: summarization, question-answer (QA) pair extraction, and question answering, facilitating a more realistic and comprehensive evaluation of understanding. Experimental results with seven popular LLMs reveal that even the most advanced long-context models have significant room for improvement, demonstrating the effectiveness of $\\texttt{M$^3$FinMeeting}$ as a benchmark for assessing LLMs' financial meeting comprehension skills."
        },
        {
            "title": "Start",
            "content": "M3FinMeeting: Multilingual, Multi-Sector, and Multi-Task Financial Meeting Understanding Evaluation Dataset Jie Zhu1,2, Junhui Li1*, Yalong Wen2, Xiandong Li3, Lifan Guo2, Feng Chen2 1School of Computer Science and Technology, Soochow University 2Qwen DianJin Team, Alibaba Cloud Computing 3Nanjing University zhujie951121@gmail.com, lijunhui@suda.edu.cn {lifan.lg, betterman.chenf}@alibaba-inc.com"
        },
        {
            "title": "Abstract",
            "content": "Recent breakthroughs in large language models (LLMs) have led to the development of new benchmarks for evaluating their performance in the financial domain. However, current financial benchmarks often rely on news articles, earnings reports, or announcements, making it challenging to capture the real-world dynamics of financial meetings. To address this gap, we propose novel benchmark called M3FinMeeting, which is multilingual, multi-sector, and multi-task dataset designed for financial meeting understanding. First, M3FinMeeting supports English, Chinese, and Japanese, enhancing comprehension of financial discussions in diverse linguistic contexts. Second, it encompasses various industry sectors defined by the Global Industry Classification Standard (GICS), ensuring that the benchmark spans broad range of financial activities. Finally, M3FinMeeting includes three tasks: summarization, question-answer (QA) pair extraction, and question answering, facilitating more realistic and comprehensive evaluation of understanding. Experimental results with seven popular LLMs reveal that even the most advanced long-context models have significant room for improvement, demonstrating the effectiveness of M3FinMeeting as benchmark for assessing LLMs financial meeting comprehension skills."
        },
        {
            "title": "Introduction",
            "content": "Financial meetings, whether in person or virtual, serve as critical venues for decision-making, negotiation, and strategy formulation among various participants. Although large language models (LLMs) have demonstrated impressive performance across multiple natural language processing (NLP) tasks (OpenAI, 2023), it remains uncertain how they can effectively understand and process *Corresponding Author 1We make our dataset and project available on https: //github.com/aliyun/qwen-dianjin. lengthy speech texts to help financial professionals expedite their work. Key capabilities, such as summarizing crucial points, responding to inquiries, and extracting question-answer pairs, are particularly beneficial for enhancing productivity and facilitating informed discussions in this context. In the financial domain, there are various benchmarks available in different languages, such as FinQA (Chen et al., 2021) and ConvFinQA (Chen et al., 2022) in English, as well as CFLUE (Zhu et al., 2024) and the CCKS series shared tasks in Chinese (Tianchi, 2019, 2020, 2021, 2022). However, these datasets are primarily sourced from financial news and earnings reports, lacking content from real-world financial meetings. Additionally, they are monolingual, limited to English or Chinese. To address this gap, we introduce new dataset called M3FinMeeting, designed for multilingual and multi-sector evaluation of financial meeting understanding, featuring multiple tasks. First, M3FinMeeting supports multiple languages, including English, Chinese and Japanese, enhancing the understanding of financial discussions across different linguistic contexts. Second, it encompasses all 11 industry sectors defined by Global Industry Classification Standard (GICS). Finally, M3FinMeeting includes multiple classical NLP tasks such as summarization, question answering, and question-answer pair extraction. Importantly, since financial meetings typically last one to two hours, M3FinMeeting provides long-context data, which is essential for assessing the ability of LLMs to handle complex tasks. Additionally, the tasks within M3FinMeeting require LLMs to generate long responses, allowing for thorough assessment of their capabilities in producing coherent and relevant outputs in challenging scenarios. Based on M3FinMeeting, we assess the effectiveness of seven representative LLMs including two OpenAI GPTs and five open-sourced LLMs. The experimental results indicate that Qwen2.5-72B5 2 0 2 3 ] . [ 1 0 1 5 2 0 . 6 0 5 2 : r Instruct (Yang et al., 2024b) significantly outperforms other large language models (LLMs), achieving overall scores above 70 when evaluated by GPT4 (OpenAI, 2023). However, this also suggests that even the most advanced LLMs currently available struggle with the tasks in M3FinMeeting, revealing substantial rooms for performance improvement. Our main contributions can be summarized as follows: M3FinMeeting introduces novel evaluation benchmark specifically designed for financial meetings, addressing the lack of real-world financial meeting data in existing benchmarks. M3FinMeeting supports multilingual evaluation in English, Chinese, and Japanese, spans the 11 industry sectors defined by GICS, and includes three key NLP tasks: summarization, question-answer pair extraction, and question answering. All documents are carefully annotated by financial analysts to ensure highquality and accurate evaluation. Through extensive experiments and detailed analyses, we thoroughly assess the performance of state-of-the-art long-context LLMs, offering valuable insights into their limitations and potential improvements for better longcontext modeling in financial scenarios."
        },
        {
            "title": "2.1 Financial Evaluation Benchmarks",
            "content": "Financial NLP has become key application area for LLMs, attracting increasing attention for its benchmarks. Table 1 summarizes recent benchmarks in this field. In English, FINQA by Chen et al. (2021) contains 8,281 question-answering pairs, with their numerical reasoning, from the earnings reports of S&P 500 companies. ECTSum by (Mukherjee et al., 2022) contains 2,425 transcripts of earnings calls, paired with short telegramstyle bullet point summaries. FLUE by (Shah et al., 2022) and FLARE by Xie et al. (2024) offer heterogeneous benchmarks with various financial NLP tasks from existing datasets, including financial sentiment detection (Malo et al., 2014), named entity recognition (Alvarado et al., 2015), news headline classification (Sinha and Khandait, 2020), question answering (Maia et al., 2018), boundary detection tasks (Au et al., 2021), text summarization (Zhou et al., 2021; Mukherjee et al., 2022), and stock movement prediction (Xu and Cohen, 2018; Wu et al., 2018; Soun et al., 2022). FinTextQA by Chen et al. (2024) includes 1,262 long-form QA pairs from finance textbooks and government websites. BizBench by Krumdick et al. (2024) features eight quantitative reasoning tasks from professional exams, earnings reports, and other financial sources. FINANCEBENCH by Islam et al. (2023) includes 10,231 question-answer-evidence triplets from public filings and earnings reports. In Chinese, the CCKS series has released several datasets specifically designed for various event extraction tasks (Tianchi, 2019, 2020, 2021, 2022). Additionally, significant effort has gone into creating evaluation datasets for documentlevel extraction from financial announcements, judgments, and news articles. These include DCFEE (Yang et al., 2018), DuEE-Fin (Han et al., 2022), Doc2EDAG (Zheng et al., 2019), and CFinDEE (Zhang et al., 2024a). Moreover, both BBT-CFLEB dataset (Lu et al., 2023) and CFLUE dataset (Zhu et al., 2024) serve as heterogeneous benchmarks that cover wide range of NLP tasks, including multiple-choice question answering and reasoning, news and text classification, summarization, relation extraction, sentiment classification, question answering, and reading comprehension. In addition to the benchmarks mentioned above, several datasets have been developed in the financial domain to support the training of financial LLMs, such as FLANG (Shah et al., 2022), Pixiu (Xie et al., 2024), InvestLM (Yang et al., 2023b), FinGPT (Yang et al., 2023a), and DianJinR1 (Zhu et al., 2025). However, it is important to note that most of these datasets primarily rely on sources such as financial news, announcements, filings, and earnings reports. In contrast, our focus is on financial meetings, which provide unique insights and discussions that are often absent from existing benchmarks."
        },
        {
            "title": "Benchmarks",
            "content": "Summarization Benchmarks. Recent studies (Goyal et al., 2022; Zhang et al., 2024b; Pu et al., 2023) indicate that human preferences strongly favor summaries produced by LLMs over those generated by fine-tuned models or even reference summaries. This highlights the need to create new datasets that can comprehensively evaluate the summarization abilities of LLMs."
        },
        {
            "title": "Tasks",
            "content": "FinQA (Chen et al., 2021) Earnings reports Question answering & reasoning ConvFinQA (Chen et al., 2022)"
        },
        {
            "title": "English",
            "content": "FLUE (Shah et al., 2022)"
        },
        {
            "title": "Multiple",
            "content": "Question answering & reasoning 5 tasks ECTSum (Mukherjee et al., 2022)"
        },
        {
            "title": "Summarization",
            "content": "FLARE (Xie et al., 2024)"
        },
        {
            "title": "Multiple",
            "content": "8 tasks FinTextQA (Chen et al., 2024)"
        },
        {
            "title": "Question answering",
            "content": "BizBench (Krumdick et al., 2024)"
        },
        {
            "title": "Multiple",
            "content": "8 quantitative reasoning tasks FinanceBench (Islam et al., 2023) Public filings, Earnings reports"
        },
        {
            "title": "Question answering",
            "content": "CCKS Tianchi (2019, 2020, 2021, 2022) Multiple Multiple event-related tasks DCFEE (Yang et al., 2018)"
        },
        {
            "title": "Announcements",
            "content": "Document-level event extraction"
        },
        {
            "title": "Chinese",
            "content": "Doc2EDAG (Zheng et al., 2019)"
        },
        {
            "title": "Announcements",
            "content": "Document-level event extraction DuEE-Fin (Han et al., 2022)"
        },
        {
            "title": "Multiple",
            "content": "Document-level event extraction CFinDEE (Zhang et al., 2024a)"
        },
        {
            "title": "News articles",
            "content": "Document-level event extraction BBT-CFLEB (Lu et al., 2023) CFLUE (Zhu et al., 2024) English, Chinese, Japanese M3FinMeeting (Ours)"
        },
        {
            "title": "Meetings",
            "content": "6 tasks Multiple-choice Question Answering & Reasoning and 5 tasks Summarization, Question answering, QA pair extraction Table 1: Summary of recent financial benchmarks. For instance, SumSurvey by Liu et al. (2024a) is new dataset focused on summarizing long scientific survey papers. REFINESUMM by Patil et al. (2024) is tailored for image-text multimodal summarization. MovieSum by Saxena and Keller (2024) comprises 2,200 pairs of movie screenplays and their corresponding summaries. Additionally, HeSum by Paz-Argaman et al. (2024) is novel benchmark dataset specifically designed for the low-resource Hebrew language. Our research also emphasizes long context, but we concentrate on meetings as our primary source, allowing us to analyze communication dynamics within this particular area. Several meeting speech summarization benchmarks are available, such as AMI (Carletta et al., 2005) and ICSI (Janin et al., 2003), which consist of English-language video (or audio) recordings of meetings, typically spanning tens of hours. Question-Answering Benchmarks. Question Answering (QA) is key task in NLP, with increasingly challenging benchmarks being developed, including those focused on the financial domain. For example, several benchmarks have been introduced to assess the long context understanding capabilities of LLMs (Li et al., 2024; Wang et al., 2024; Bai et al., 2024b). Additionally, many multimodal QA benchmarks have emerged to assess LLMs capabilities in detecting and retrieving relevant information from various inputs, such as images, videos, texts, tables, and meetings (Li et al., 2023; Fu et al., 2023; Jin et al., 2024; Prasad et al., 2023). 3 M3FinMeeting: Financial Meeting Benchmark"
        },
        {
            "title": "3.1 Overview",
            "content": "A formal financial meeting typically involves few participants and lasts one to two hours. It promotes discussions among key participants, allowing for decision-making and strategic planning based on real-time information and reports. Participants express their opinions verbally, which distinguishes both the content and style of financial meetings from financial news, earnings reports, or announcements. The M3FinMeeting benchmark, based on hundreds real financial meetings, includes three NLP tasks: summarization, question answering, and QA pair extraction. To reflect real-world sce-"
        },
        {
            "title": "Language",
            "content": "#Meeting"
        },
        {
            "title": "EN\nZH\nJA",
            "content": "100 400 100 0.96 1.15 1.01 10,086 11,740 13,"
        },
        {
            "title": "Language",
            "content": "#Meeting Avg token Com. Services Con. Dis. Con. Staples Energy Financials Healthcare Industrials IT Materials RealEstate Utilities EN, ZH, JA 36 EN, ZH, JA 122 EN, ZH, JA 52 EN, ZH, JA 32 EN, ZH, JA 49 EN, ZH, JA 47 EN, ZH, JA 111 EN, ZH, JA 98 EN, ZH, JA 32 EN, ZH, JA 13 EN, ZH, JA 8 Length Set 11,240 11,104 11,699 15,841 12,835 13,937 12,062 13,430 11,393 15,270 17,"
        },
        {
            "title": "Language",
            "content": "#Meeting Avg token Set1 (0-5K) Set2 (5-10K) Set3 (10-15K) Set4 (15-20K) Set5 (>20K) EN, ZH, JA 59 EN, ZH, JA 164 EN, ZH, JA 195 EN, ZH, JA 124 EN, ZH, JA 58 3,546 7,509 12,476 17,419 25,281 Table 2: Data statistics of M3FinMeeting benchmark. narios, we gather financial meetings from various sectors defined by GICS: Communication Services, Consumer Discretionary, Consumer Staples, Energy, Financials, Healthcare, Industrials, Information Technology (IT), Materials, Real Estate, and Utilities. Additionally, all audio recordings of the meetings are transcribed using state-of-the-art automatic speech recognition (ASR) toolkit, followed In total, M3FinMeeting by manual corrections. includes 100 meetings in English (EN), 400 in Chinese (ZH), and 100 in Japanese (JA). Each meeting lasts an average of one hour. We employ the tiktoken tokenizer2 to process all transcriptions. Table 2 presents detailed data statistics."
        },
        {
            "title": "3.2 Evaluation Tasks",
            "content": "Given that financial meetings typically last one to two hours, users often need summary and answers to specific questions. To address this, we propose three tasks that closely align with real-world needs."
        },
        {
            "title": "3.2.1 Summarization",
            "content": "The summarization task aims to evaluate LLMs ability to efficiently condense lengthy speeches while preserving the main ideas. Typically, these 2https://platform.openai.com/tokenizer (cl100k_base)"
        },
        {
            "title": "Language AST",
            "content": "# SS"
        },
        {
            "title": "EN\nZH\nJA",
            "content": "927 2,524 1,149 9.20 15.17 8.24 10.88 4.65 11.56 10.49 3.62 11.92 Table 3: Statistics for the summarization task. Here, AST is for average summary token, #SS is for averaged number of section summary, Ctoken and Csent for the compression ratio at token-level and sentence-level, respectively. transcribed speech documents can be structured into sections based on discussion topics, with each section having its own summary, which we refer to section summary. We concatenate these section summaries sequentially to create summary of the entire document. LLMs must implicitly identify and segment the document into various sections and then extract key point from each. Given transcribed speech documents and their reference summaries, we follow Koh et al. (2022) to compute the compression ratio of source document length against its reference summary length at both tokenlevel and sentence-level. As shown in Table 3, on average an English meeting contains 9.20 section summaries totaling 927 tokens."
        },
        {
            "title": "3.2.3 Question Answering\nThe question answering (QA) task evaluates the\nability of LLMs to localize knowledge, which is es-\nsential for effective long-context processing (Wang",
            "content": "Lang."
        },
        {
            "title": "QAtoken",
            "content": "#QA"
        },
        {
            "title": "EN\nZH\nJA",
            "content": "2,239 2,852 2,934 17.23 16.10 10.84 17.62 36.44 34.55 110.19 148.95 178.99 Table 4: Statistics for both QA Pair Extraction and Question Answering Tasks. QAtoken is the average token length of all QA pairs per meeting, #QA is the average number of QA pairs per meeting, and Qtoken and Atoken denote the average token lengths of question and answer, respectively. et al., 2024). For simplicity, we use the QA pairs described above for this task. As mentioned, the transcribed speech text can be divided into multiple sections, the QA task tests the LLMs capability to find evidence within that designated section, while other sections with similar but unrelated content act as noise. This setup ensures focused assessment of the models information retrieval skills."
        },
        {
            "title": "3.3.1 Data Collection\nWe have established four criteria for the manual col-\nlection of audio files from financial meetings, cov-\nering a range of events such as public roadshows,\nbrokerage strategy meetings, industry exchanges,\nand earnings presentations: (1) Timeliness: Most\nmeetings should be from recent years; (2) Length:\nPreference is given to longer audio files; (3) Cate-\ngorizability: The audio files must align with cate-\ngories defined in the GICS; (4) Authoritativeness:\nAll audio files are sourced from our financial firm\npartners and are protected by our copyright.",
            "content": "All audio files are transcribed into text using ASR toolkit Whisper,3 followed by thorough manual correction process. Strict measures are used to ensure that no sensitive or personally identifiable information is included in the transcripts."
        },
        {
            "title": "3.3.2 Annotation Process and Quality Control\nThe datasets for manual annotation are drawn from\nvarious projects that recruit experienced analysts\nfluent in different languages. These annotators fol-\nlow the annotation guidelines (See Figure 10 in\nAppendix F), undergo comprehensive training, and\nhave access to onboarding and guidance materials.\nAdditionally, other analysts review the annotators’\nwork and provide ongoing feedback.",
            "content": "Transcribed Speech Correction: Annotators correct ASR-generated transcriptions with 3https://openai.com/index/whisper/ original audio files available for reference during the correction process. Summarization: The annotators segment each document into sections based on distinct topics, ensuring that only sections with clear boundaries are selected for summarization. For each valid section, annotators are encouraged to use simple sentences in the summary. Prior to the annotation process, annotators undergo extensive training and discussions to achieve high level of consensus. In the annotation process, the original audio files are available for reference. QA Pair Extraction: With corrected speech documents, annotators manually extract financially relevant questions from the text. For each identified question, they search for corresponding answers in the subsequent content. Only questions with valid answers are retained for further analysis. This meticulous process guarantees the quality and relevance of the extracted QA pairs, significantly enhancing the datasets value for financial insights. Figure 4 in Appendix shows screenshot of an annotated example. For additional complete examples, please refer to the attached file."
        },
        {
            "title": "4.1 Experimental Settings",
            "content": "Models. We evaluate seven advanced longcontext LLMs with context windows ranging from 16K to 1000K, including two API-based LLMs: GPT-4o-2024-08-06-128K (OpenAI, 2023) and GPT-3.5.turbo-0125-16K, as well as five open-source LLMs: GLM4-9B-Chat-1000K (Zeng et al., 2022), Llama3.1-8b-Instruct-128K (Dubey et al., 2024), Qwen2-7B-chat-128K (Yang et al., 2024a), Qwen2-72B-Instruct-128K, and Qwen2.572B-Instruct-128K (Yang et al., 2024b). All models support the languages in M3FinMeeting. Prompts. We evaluate LLMs in zero-shot setting. For summarization, we prompt the LLMs to implicitly identify document sections and generate individual summaries, which are then combined into final document summary. For QA pair extraction, we first prompt the LLMs to extract all questions, then provide answers for each sequentially. For question answering, instead of addressing one Figure 1: Illustration of the summarization evaluation. question at time, we combine related questions into single prompt, allowing the LLM to produce comprehensive response that includes all the answers. This better aligns with real-world tasks, like writing reviews or reports, and reduces API calls. Prompt examples are provided in Appendix C. Metrics. For summarization, as shown in Figure 1, the final summary consists of multiple section summaries. We evaluate this using precision, recall, and F1 scores rather than traditional metrics like BLEU and ROUGE, as they better reflect how well the generated section summaries align with the reference (gold) summaries. Specifically, we align the automatic and gold section summaries using cosine similarity score above 0.75, calculated with the OpenAI Embedding model4. Appendix gives details of the three metrics. Meanwhile, recent studies (Wang et al., 2023; Zhang et al., 2023; Liu et al., 2024b) show that GPT-4 (OpenAI, 2023) aligns closely with human evaluations. Therefore, we use GPT-4 (gpt-4-turbo-2024-04-09) as the judge (GPT-4-Judge) to evaluate documentlevel summaries based on five criteria: Coverage, Redundancy, Readability, Accuracy, and Consistency, with scores ranging from 0 to 100. We report the average GPT-4-Judge score, based on the prompt in Appendix C. For QA pair extraction, we assess the quality of generated questions using precision, recall, and F1 scores, comparing them to the reference questions, similar to the summarization evaluation. Additionally, GPT-4-Judge evaluates the generated QA pairs against the gold pairs using the same five criteria, and we report the average score. For question answering, we group all questions together and prompt the LLMs to answer them sequentially, repeating each question before generating its corresponding answer. Therefore, we evaluate performance using the same metrics as in QA pair extraction, assessing both the quality of repeated questions and the overall performance of 4https://platform.openai.com/docs/models/ embeddings (text-embedding-3-small) the generated QA pairs. For the three tasks, Appendix presents performance metrics using BLEU (Papineni et al., 2002) and ROUGE (Lin and Hovy, 2002). Appendix includes detailed performance across languages, lengths, and GICS sectors. To address potential self-bias in LLM evaluations (e.g., GPT-4-Judge favoring GPT-4-generated answers), we follow Bai et al. (2024a) and use Qwen-plus5 as an alternative judge model. Results, shown in Section 4.3, indicate minimal bias, as the performance trend from GPT-4-Judge and Qwen-plus-Judge are very consistent. Moreover, we perform human evaluation and calculate Fleiss Kappa (Scott, 1995) to measure agreement between GPT-4-Judge and human annotators, further supporting our conclusions. The results are presented in Section 4.4."
        },
        {
            "title": "4.2 Results",
            "content": "Main Results. Table 5 shows the main results over all meetings.6 From it, we have the following observations: Overall performance: The seven LLMs can be divided into three groups. Group 1 consists of Qwen2.5-72B-Instruct, Qwen2-72BInstruct, and GPT-4o, all achieving an overall GPT-4-Judge score near or above 70.0. Among these, Qwen2.5-72B-Instruct performs best, followed by GPT-4o and Qwen272B-Instruct, which deliver comparable results. Group 2 includes Qwen2-7B-Instruct and GLM4-9B-Chat, both scoring around 60.0. Group 3 consists of GPT-3.5-turbo and LLaMA3.1-8B-Instruct, with LLaMA3.1-8BInstruct outperforming GPT-3.5-turbo. Summarization: The precision, recall, and F1 scores for section-level summaries are all below 30%, indicating poor alignment between generated and gold section-level summaries. These low scores suggest that LLMs struggle both with semantic accuracy and document segmentation. QA Pair Extraction: The low precision, recall, and F1 scores suggest poor alignment between generated and gold questions. 5https://help.aliyun.com/zh/model-studio/ developer-reference/what-is-qwen-llm 6The total cost for OpenAI API calls was about $2,500, while experiments with other LLMs use eight NVIDIA A100/80G GPUs."
        },
        {
            "title": "Overall",
            "content": "P. R. F1 GPT-4 P. R. GPT-4 P. R. F1 GPT-4 GPT-4 GPT-4o GPT-3.5-turbo GLM4-9B-Chat LLaMA3.1-8B-Instruct Qwen2-7B-Instruct Qwen2-72B-Instruct Qwen2.5-72B-Instruct 27.82 17.13 10.30 6.24 28.67 29.59 28. 12.07 9.39 11.26 8.34 19.39 20.18 15.56 16.83 12.13 10.76 7.14 23.14 23.99 20.25 73.61 44.56 67.71 52.01 73.59 74.17 74.51 23.33 9.66 15.08 13.05 11.98 22.43 32.61 41.98 25.07 5.22 7.30 16.40 28.82 45.65 29.99 13.95 7.76 9.37 13.85 25.22 38. 66.85 31.13 46.06 44.64 37.33 60.85 68.03 93.93 84.16 93.37 57.21 89.83 93.27 93.75 93.16 92.60 92.20 39.97 93.01 93.58 93.59 93.55 88.18 92.78 47.06 91.40 93.42 93.66 71.79 42.78 67.72 40.01 69.99 73.50 74.81 70.66 39.55 60.76 45.76 60.71 69.66 72. Table 5: Performance of LLMs on three evaluation tasks. The overall GPT-4-Judge score is the micro-average of its scores across all three tasks. Scores in bold/underline denote the top/second-best performances. Figure 2: Performance based on GPT-4-Judge scores across languages (a), GICS sectors (b), and input lengths (c). For example, even the best-performing LLM, Qwen2.5-72B-Instruct, achieves only 45.65% recall, missing more than half of the gold questions. This highlights significant room for improvement in extracting relevant QA pairs. crepancy is not surprising, as in the question answering task, the questions are explicitly provided in the prompt. High F1 scores (over 90%) show that most LLMs can follow instructions well and properly repeat questions. Question Answering: The performance of all LLMsmeasured by precision, recall, F1, and GPT-4-Judge scoresis significantly higher than for QA pair extraction.7 This dis7One exception is LLaMA3.1-8B-Instruct, which has difficulty following instructions and often fails to repeat the questions, resulting in lower GPT-4-Judge scores for the question Effect over Different Languages. Figure 2 (a) shows the GPT-4-Judge scores across three languages. Most models perform best in Japanese, but there is no clear advantage in either Chinese or English. closer look reveals that LLMs are more answering task. consistent in Japanese, likely because they adhere to the instructions more effectively in this language. The three Qwen models perform similarly in the Summarization task across all languages, followed by the Question Answering task. However, their performance varies most in the QA pair extraction task, where Qwen2.5-72B-Instruct obtains the highest scores, outperforming Qwen2-7B-Instruct with GPT-4-Judge scores of 72.76, 63.59, and 83.45 for English, Chinese, and Japanese, respectively. Effect over Different Sectors. Figure 2 (b) compares model performance across sectors. Communication Services, Consumer Discretionary, and IT generally achieve higher GPT-4-Judge scores in summarization and question answering. However, the performance trend becomes more complex for the QA pair extraction task, showing increased variability across sectors. Overall, the performance gaps among sectors for GPT-4o, Qwen2-72B-Instruct, and Qwen2.5-72B-Instruct are much smaller compared to GPT-3.5-turbo and LLaMA3.1-8B-Instruct.8 This suggests that the former models are less affected by sector differences. Effect over Different Input Lengths. Figure 2 (c) compares the performance across varying input lengths. key observation is the sharp drop in GPT-3.5-turbos performance when the input exceeds 15K tokens, due to its 16K token context limit. In contrast, both Qwen2.5-72B-Instruct and GPT-4o demonstrate stable and competent performance across the three tasks, particularly excelling in handling longer contexts exceeding 15K tokens. On the other hand, Qwen2-72B-Instruct shows declining trend in the QA pair extraction task as input length increases, indicating reduced capability to maintain performance with longer inputs in this specific task. Future research could explore structured modeling, as outlined by (Zhu et al., 2019), to improve handling of long input contexts. Effect of RAG-based Question Answering. Instead of prompting the LLMs to answer list of questions in single response, we also explore RAG-based question answering, where the LLM answers questions individually based on retrieved document chunks. Following Wang et al. (2024), we divide the document into 1,024-token chunks. For embedding, we utilize the OpenAI Embedding model. Figure 3 compares the performance of 8GPT-3.5-turbo fails for the Utilities sector due to input length exceeding its 16K token limit. Figure 3: Performance (in GPT-4-Judge score) across different input lengths before or after adding RAG module. Baseline 1 refers to prompting the LLM to answer list of questions, while Baseline 2 refers to answering one question per response. Qwen2.5-72B-Instruct before and after the integration of the RAG module. This comparison is based on random selection of 10 meetings from each length set, resulting in total of 50 meetings. The results indicate that for documents exceeding 15K tokens, answering all questions in single response (Baseline 1) outperforms all other variants that answer questions one at time. Additionally, for variants that respond to one question at time in documents longer than 10K tokens, we observe that larger context leads to better performance, specifically: Baseline 2 > RAG (top 5) > RAG (top 3) > RAG (top 1). Notably, RAG (with top 5) only surpasses the non-RAG variants for documents shorter than 10K tokens."
        },
        {
            "title": "4.3 Performance Evaluated by",
            "content": "Qwen-plus-Judge In addition to the performance assessed by the GPT-4-Judge, as presented in Table 5, Table 6 shows the performance evaluated by the QwenPlus-Judge. The prompt templates used for the Qwen-Plus-Judge are the same as those for GPT4-Judge, as illustrated in Figure 8 and Figure 9. The performance trends in Table 6 align closely with those in Table 5, where Qwen2.5-72B-Instruct remains the top-performing model, followed by GPT-4o and Qwen2-72B-Instruct, which show similar performance. Next are Qwen2-7B-Instruct and GLM4-9B-Chat, both of which also exhibit comparable performance. This consistency further demonstrates that Qwen-Plus-Judge is reliable alternative evaluator. Model GPT-4o GPT-3.5-turbo GLM4-9B-Chat LLaMA3.1-8B-Instruct Qwen2-7B-Instruct Qwen2-72B-Instruct Qwen2.5-72B-Instruct Summarization QA Pair Extraction Question Answering Overall 71.12 38.48 58.86 42.75 62.38 71.01 73.85 69.84 37.88 60.39 32.05 67.87 70.73 74.01 67.02 31.43 47.06 43.90 42.65 65.58 70. 76.01 45.46 68.20 51.46 75.39 76.22 76.99 Table 6: The results using Qwen-plus as the judge model. Evaluation Method GPT-4-Judge Human Annotators"
        },
        {
            "title": "Summarization QA Pair Extraction Question Answering",
            "content": "72.83 3.68 60.76 3.13 66.44 3.36 Table 7: Comparison of average performance ratings between GPT-4-Judge and human annotators. Note that GPT-4-Judge uses 1-100 scale, while human annotators use 1-5 scale. Agreement GPT-4-Judge & Human Annotators Human Annotators Kappa 0.701 0. Table 8: Fleiss Kappa score between GPT-4-Judge and human annotators."
        },
        {
            "title": "4.4 Human Evaluation and Fleiss’ Kappa\nAgreement Between GPT-4-Judge and\nHuman Evaluators",
            "content": "We randomly select 100 meetings and recruit five expert human annotators to assess the overall quality of GPT-4os responses, including summarization, QA pair extraction, and question answering. Each response is rated on scale from 1 to 5, based on the same five criteria used to prompt GPT-4Judge. The final score for each response is the average rating across the five annotators. Table 7 compares the performance assessed by GPT-4-Judge and human annotators. Moreover, to assess the agreement among the six evaluators (five human annotators and GPT-4Judge), we compute Fleiss Kappa score, metric for inter-annotator agreement. Specifically, we calculate two types of Kappa scores: 1) the agreement among all six evaluators, where GPT-4-Judges ratings (on 1-100 scale) are converted to 1-5 scale by dividing by 20; and 2) the agreement among the five human annotators. As shown in Table 8, the agreement among GPT-4-Judge and human annotators is still higher than that among humans."
        },
        {
            "title": "4.5 Performance in BLEU and ROUGE",
            "content": "The analysis of model performances, based on BLEU and ROUGE metrics, highlights notable differences across various tasks. Specifically, Qwen272B-Instruct excels in the summarization task, consistently generating concise and coherent summaries that underscore its strength in synthesizing information. Meanwhile, Qwen2.5-72B-Instruct leads in both QA pair extraction and question answering, demonstrating its adeptness at understanding queries and providing precise responses. This comparison underscores the unique advantages and task-specific expertise of each model, offering more comprehensive insight into their capabilities. For detailed data and specific scores, please refer to the full table located in the appendix D."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we have introduced M3FinMeeting, novel multilingual, multi-sector, and multi-task benchmark specifically designed to evaluate financial meeting understanding in large language models (LLMs). By incorporating real-world dialogue from financial meetings, our dataset fills significant gaps in existing benchmarks, which often rely on static sources like news articles and earnings reports. Supporting English, Chinese, and Japanese, and encompassing 11 industry sectors defined by GICS, M3FinMeeting enables comprehensive evaluation of LLMs through tasks such as summarization, question-answer pair extraction, and question answering. Experimental results with seven representative LLMs, including GPT-4o and Qwen2.5-72B-Instruct, demonstrate notable performance challenges, highlighting significant room for improvement and establishing M3FinMeeting as valuable resource for advancing research in financial language processing."
        },
        {
            "title": "Acknowledgments",
            "content": "The authors express their heartfelt gratitude to the anonymous reviewers for their invaluable feedback and insightful comments, which greatly enhanced the quality of this work. We also offer our sincere appreciation to the members of the Qwen DianJin Team for their exceptional contributions, dedication, and hard work, which were instrumental in the success of this project. This work was supported by the Alibaba Innovative Research Program."
        },
        {
            "title": "Limitations",
            "content": "Our work has several limitations. High Annotation Costs and Challenges: Both summarization and question-answer pair extraction require annotators to summarize content and extract question-answer pairs from audio recordings lasting 1-2 hours and text exceeding 10K tokens. This process relies heavily on the annotators professional expertise. Summarization, in particular, demands high level of skill from professional analysts and often involves open-ended responses. Consequently, the annotation process necessitates significant investment of time and effort. Limited Coverage for Question Answering: In the question-answering task, we focus exclusively from the extracted QA pairs. This limits our ability to evaluate the LLMs capacity to search for evidence within the document beyond the provided questions. As result, the dataset may not fully capture the models potential in more complex reasoning scenarios that require deeper comprehension of the content. Evaluation Issues: The performance of summary alignment in summarization and question alignment in both question-answer pair extraction and question answering relies on the embeddings used. Due to budget constraints, we have not conducted more extensive evaluation, which may affect the robustness of our findings."
        },
        {
            "title": "Ethics",
            "content": "Our dataset is sourced from publicly mandated disclosures, such as earnings calls and roadshows. While this information is publicly available, we anonymize it to respect corporate preferences for dissemination. This ensures both the academic utility and ethical integrity of our benchmark. Specifically, we remove company identifiers and sensitive details using GPT-4, followed by manual verification, to maintain privacy without compromising research value. All data annotators are part of the funded projects, ensuring consistent and responsible data handling. The dataset, excluding original audio files, will be available online."
        },
        {
            "title": "References",
            "content": "Julio Cesar Salinas Alvarado, Karin Verspoor, and Timothy Baldwin. 2015. Domain adaption of named entity recognition to support credit risk assessment. In Proceedings of the Australasian Language Technology Association Workshop, pages 8490. Willy Au, Abderrahim Ait-Azzi, and Juyeon Kang. 2021. Finsbd-2021: The 3rd shared task on structure boundary detection in unstructured text in the financial domain. In Companion Proceedings of WWW, page 276279. Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, and Wanli Ouyang. 2024a. MT-bench-101: fine-grained benchmark for evaluating large language models in multi-turn dialogues. In Proceedings of ACL, pages 74217454. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2024b. LongBench: bilingual, multitask benchmark for long context understanding. In Proceedings of ACL, pages 31193137. Jean Carletta, Simone Ashby, Sebastien Bourban, Mike Flynn, Mael Guillemot, Thomas Hain, Jaroslav Kadlec, Vasilis Karaiskos, Wessel Kraaij, Melissa Kronenthal, Guillaume Lathoud, Mike Lincoln, Agnes Lisowska, Iain McCowan, Wilfried Post, Dennis Reidsma, and Pierre Wellner. 2005. The ami meeting corpus: pre-announcement. In Proceedings of MLMI, page 2839. Jian Chen, Peilin Zhou, Yining Hua, Loh Xin, Kehui Chen, Ziyuan Li, Bing Zhu, and Junwei Liang. 2024. FinTextQA: dataset for long-form financial question answering. In Proceedings of ACL, pages 6025 6047. Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, and William Yang Wang. 2021. FinQA: dataset of numerical reasoning over financial data. In Proceedings of EMNLP, pages 36973711. Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and William Yang Wang. 2022. ConvFinQA: Exploring the chain of numerical reasoning in conversational finance question answering. In Proceedings of EMNLP, pages 62796292. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. Computing Research Repository, arXiv:2407.21783. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. 2023. Mme: comprehensive evaluation benchmark for multimodal large language models. Computing Research Repository, arXiv:2306.1339. Tanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022. News summarization and evaluation in the era of gpt-3. Computing Research Repository, arXiv:2209.12356. Cuiyun Han, Jinchuan Zhang, Xinyu Li, Guojin Xu, Weihua Peng, and Zengfeng Zeng. 2022. Duee-fin: large-scale dataset for document-level event extraction. In Proceedings of NLPCC, pages 172183. Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie Vidgen. 2023. Financebench: new benchmark for financial question answering. Computing Research Repository, arXiv:2311.11944. Adam Janin, Don Baaron, Jane Edwards, Dan Ellis, David Gelbart, Nelson Morgan, Barbara Peskin, Thilo Pfau, Elizabeth Shriberg, Andreas Stolcke, and Chunk Wooters. 2003. The icsi meeting corpus. In Proceedings of ICASSP, pages 364367. Chuanyang Jin, Yutong Wu, Jing Cao, Jiannan Xiang, Yen-Ling Kuo, Zhiting Hu, Tomer Ullman, Antonio Torralba, Joshua Tenenbaum, and Tianmin Shu. 2024. MMToM-QA: Multimodal theory of mind question answering. In Proceedings of ACL, pages 16077 16102. Huan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan. 2022. An empirical survey on long document summarization: Datasets, models, and metrics. ACM Comput. Surv., 55(8). Michael Krumdick, Rik Koncel-Kedziorski, Viet Lai, Varshini Reddy, Charles Lovering, and Chris Tanner. 2024. BizBench: quantitative reasoning benchmark for business and finance. In Proceedings of ACL, pages 83098332. Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. 2024. LooGLE: Can long-context language models understand long contexts? In Proceedings of ACL, pages 1630416333. Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, Lingpeng Kong, and Qi Liu. 2023. M3it: large-scale dataset towards multimodal multilingual instruction tuning. Computing Research Repository, arXiv:2306.04387. Chin-Yew Lin and Eduard Hovy. 2002. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of ACL, page 311318. Ran Liu, Ming Liu, Min Yu, He Zhang, Jianguo Jiang, Gang Li, and Weiqing Huang. 2024a. SumSurvey: An abstractive dataset of scientific survey papers for long document summarization. In Findings of ACL, pages 96329651. Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, and Qi Zhang. 2024b. Calibrating llmbased evaluator. In Proceedings of LREC-COLING, pages 26382656. Dakuan Lu, Hengkui Wu, Jiaqing Liang, Yipei Xu, Qianyu He, Yipeng Geng, Mengkun Han, Yingsi Xin, and Yanghua Xiao. 2023. Bbt-fin: Comprehensive construction of chinese financial domain pre-trained language model, corpus and benchmark. Computing Research Repository, arXiv:2302.09432. Macedo Maia, Siegfried Handschuh, André Freitas, Brian Davis, Ross McDermott, Manel Zarrouk, and Alexandra Balahur. 2018. Www18 open challenge: Financial opinion mining and question answering. In Companion Proceedings of WWW, page 19411942. Pekka Malo, Ankur Sinha, Pyry Takala, Pekka Korhonen, and Jyrki Wallenius. 2014. Good debt or bad debt: Detecting semantic orientations in economic texts. Journal of the American Society for Information Science and Technology, 65:782796. Rajdeep Mukherjee, Abhinav Bohra, Akash Banerjee, Soumya Sharma, Manjunath Hegde, Afreen Shaikh, Shivani Shrivastava, Koustuv Dasgupta, Niloy Ganguly, Saptarshi Ghosh, and Pawan Goyal. 2022. ECTSum: new benchmark dataset for bullet point summarization of long earnings call transcripts. In Proceedings of EMNLP, pages 1089310906. OpenAI. 2023. Gpt-4 technical report. Computing Research Repository, arXiv:2303.08774. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of ACL, page 311318. Vaidehi Patil, Leonardo Ribeiro, Mengwen Liu, Mohit Bansal, and Markus Dreyer. 2024. REFINESUMM: Self-refining MLLM for generating multimodal summarization dataset. In Proceedings of ACL, pages 1377313786."
        },
        {
            "title": "Tzuf",
            "content": "Paz-Argaman, Itai Mondshine, Asaf Achi Mordechai, and Reut Tsarfaty. 2024. HeSum: novel dataset for abstractive text summarization in Hebrew. In Findings of ACL, pages 63786388. Archiki Prasad, Trung Bui, Seunghyun Yoon, Hanieh Deilamsalehy, Franck Dernoncourt, and Mohit Bansal. 2023. MeetingQA: Extractive questionanswering on meeting transcripts. In Proceedings of ACL, pages 1500015025. Xiao Pu, Mingqi Gao, and Xiaojun Wan. 2023. Summarization is (almost) dead. Computing Research Repository, arXiv:2309.09558. Yumo Xu and Shay B. Cohen. 2018. Stock movement prediction from tweets and historical prices. In Proceedings of ACL, pages 19701979. Rohit Saxena and Frank Keller. 2024. MovieSum: An abstractive summarization dataset for movie screenplays. In Findings of ACL, pages 40434050. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, et al. 2024a. Qwen2 technical report. Computing Research Repository, arXiv:2407.10671. William A. Scott. 1995. Reliability of content analysis: The case of nominal scale coding. The Public Opinion Quarterly, 19:321325. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, et al. 2024b. Qwen2.5 technical report. Computing Research Repository, arXiv:2412.15115. Raj Shah, Kunal Chawla, Dheeraj Eidnani, Agam Shah, Wendi Du, Sudheer Chava, Natraj Raman, Charese Smiley, Jiaao Chen, and Diyi Yang. 2022. When flue meets flang: Benchmarks and large pretrained language model for financial domain. In Proceedings of EMNLP, pages 23222335. Ankur Sinha and Tanmay Khandait. 2020. Impact of news on the commodity market: Dataset and results. Computing Research Repository, arXiv:2009.04202. Yejun Soun, Jaemin Yoo, Minyong Cho, Jihyeong Jeon, , and Kang. 2022. Accurate stock movement prediction with self-supervised learning from sparse noisy tweets. In Proceedings of CKIM, pages 16911700. Tianchi. 2019. Ccks2019 financial domain documenthttps:// level event entity extraction dataset. tianchi.aliyun.com/dataset/111237. Tianchi. 2020. Ccks2020 financial domain documenthttps:// level event entity extraction dataset. https://tianchi.aliyun.com/dataset/111209. Tianchi. 2021. Ccks2021 financial domain event causal relationship extraction dataset. https://tianchi. aliyun.com/dataset/110901. Tianchi. 2022. Ccks2022 financial domain fewshot event extraction dataset. https://tianchi. aliyun.com/dataset/136800. Cunxiang Wang, Sirui Cheng, Qipeng Guo, Yuanhao Yue, Bowen Ding, Zhikun Xu, Yidong Wang, Xiangkun Hu, Zheng Zhang, and Yue Zhang. 2023. Evaluating open-qa evaluation. In Proceedings of NeurIPs, page 36. Minzheng Wang, Longze Chen, Cheng Fu, Shengyi Liao, Xinghua Zhang, Bingli Wu, Haiyang Yu, Nan Xu, Lei Zhang, Run Luo, Yunshui Li, Min Yang, Fei Huang, and Yongbin Li. 2024. Leave no document behind: Benchmarking long-context llms with extended multi-doc qa. Computing Research Repository, arXiv:2406.17419. Huizhe Wu, Wei Zhang, Weiwei Shen, and Jun Wang. 2018. Hybrid deep sequential modeling for social text-driven stock prediction. In Proceedings of CKIM, pages 16271630. Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro Lopez-Lira, and Jimin Huang. 2024. Pixiu: large language model, instruction data and evaluation benchmark for finance. In Proceedings of NerIPS, pages 3346933484. Hang Yang, Yubo Chen, Kang Liu, Yang Xiao, and Jun Zhao. 2018. DCFEE: document-level Chinese financial event extraction system based on automatically labeled training data. In Proceedings of ACL: System Demonstrations, pages 5055. Hongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. 2023a. Fingpt: Open-source financial large language models. Computing Research Repository, arXiv:2306.06031. Yi Yang, Yixuan Tang, and Kar Yan Tam. 2023b. Investlm: large language model for investment using financial domain instruction tuning. Computing Research Repository, arXiv:2309.13064. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An open bilingual pre-trained model. Computing Research Repository, arXiv:2210.02414. Tian Zhang, Maofu Liu, and Bingying Zhou. 2024a. Cfindee: chinese fine-grained financial dataset for document-level event extraction. In Companion Proceedings of the ACM Web Conference, pages 1511 1520. Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B. Hashimoto. 2024b. Benchmarking large language models for news summarization. Transactions of the Association for Computational Linguistics, 12:3957. Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, and Yongbin Li. 2023. Wider and deeper llm networks are fairer llm evaluators. Computing Research Repository, arXiv:2308.01862. Shun Zheng, Wei Cao, Wei Xu, and Jiang Bian. 2019. Doc2EDAG: An end-to-end document-level framework for Chinese financial event extraction. In Proceedings of EMNLP-IJCNLP, pages 337346. Zhihan Zhou, Liqian Ma, and Han Liu. 2021. Trade the event: Corporate events detection for news-based event-driven trading. In Findings of ACL-IJCNLP, pages 21142124. Jie Zhu, Qian Chen, Huaixia Dou, Junhui Li, Lifan Guo, Feng Chen, and Chi Zhang. 2025. Dianjin-r1: Evaluating and enhancing financial reasoning in large language models. arXiv preprint arXiv:2504.15716. Jie Zhu, Junhui Li, Yalong Wen, and Lifan Guo. 2024. Benchmarking large language models on CFLUE - Chinese financial language understanding evaluation dataset. In Findings of ACL, pages 56735693. Jie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min Zhang, and Guodong Zhou. 2019. Modeling graph structure in transformer for better AMR-to-text generation. In Proceedings of EMNLP, pages 54595468, Hong Kong, China. Examples of M3FinMeeting For brevity, Figure 4 presents screenshot of an annotated M3FinMeeting example. Complete examples are available in the attached file. Details of Precision, Recall and F"
        },
        {
            "title": "Metrics",
            "content": "For summarization, as shown in Figure 1, we align the generated and gold section summaries based on cosine similarity score above 0.75. Let us assume that there are section summaries in the generated summarization and section summaries in the reference (gold) summarization. After aligning the section summaries between the two, let ma represent the number of generated section summaries aligned, and na represent the number of gold section summaried aligned. Note that ma and na do not need to be equal, as one section summary from one side can be aligned to multiple section summaries on the other side. We compute the Precision, Recall, and F1 as follows: prompt template for the question answering task. In this template, all questions from the document are listed, and the LLMs are instructed to answer them in single response. When evaluating with GPT-4-Judge, Figure 8 presents the prompt template for the summarization task. Meanwhile, Figure 9 displays the prompt template for both the QA pair extraction task and the question-answering task."
        },
        {
            "title": "D Performance in BLEU and ROUGE",
            "content": "Table 9 shows the overall performance in BLEU and ROUGE. It shows that Qwen2-72B-Instruct achieves the best performance in summarization while Qwen2.5-72B-Instruct leads in both QA pair extraction and question answering."
        },
        {
            "title": "E Detailed Performance Across",
            "content": "Languages, Lengths, and GICS Sectors Table 10, Table 11, Table 12 show the performance of LLMs with respect to the languages, the length sets, and the GICS sectors, respectively."
        },
        {
            "title": "F Annotation Guidelines",
            "content": "Figure 10 shows the annotation guideline of M3FinMeeting. Precision = , Recall = ma na , F1 = 2 Precision Recall Precision + Recall . (1) (2) (3) The Precision, Recall, and F1 scores for QA pair extraction and question answering are computed similarly, with the generated and gold section summaries replaced by generated and gold questions."
        },
        {
            "title": "C Prompt Examples for Tasks in",
            "content": "M3FinMeeting We use the same prompt templates, written in English, for English, Japanese, and Chinese. In these templates, we clearly specify that the output language must align with the meeting content. Figure 5 and Figure 6 show the prompt template for the summarization task and the QA pair extraction task, respectively. Figure 7 displays the Figure 4: Example of annotated M3FinMeeting."
        },
        {
            "title": "Model",
            "content": "GPT-4o GPT-3.5-turbo GLM4-9B-Chat LLaMA3.1-8B-Instruct Qwen2-7B-Instruct Qwen2-72B-Instruct Qwen2.5-72B-Instruct"
        },
        {
            "title": "Question Answering",
            "content": "B-4 R-1 R-2 R-L 9.47 6.03 9.27 4.77 14.72 14.89 11.71 49.32 35.02 47.35 36.37 55.63 56.35 51. 19.02 12.87 18.89 11.82 24.12 24.39 20.84 29.23 20.05 29.59 21.38 35.98 36.37 32.76 B-4 1.58 0.45 1.78 1.16 0.34 1.65 2.71 R-1 8.40 2.38 5.26 3.97 2.00 6.66 11. R-2 3.57 1.02 2.59 1.93 0.75 3.04 5.04 R-L 5.94 1.71 3.80 2.98 1.36 4.81 7.91 B-4 5.21 2.84 5.80 2.76 6.67 6.90 7. R-1 R-2 R-L 27.53 16.55 28.01 14.20 29.78 30.39 32.35 11.50 6.27 11.66 5.98 11.87 12.22 13.65 19.65 11.72 20.27 10.58 20.04 21.15 22. Table 9: Performance (in BLEU and ROUGE) of LLMs on three evaluation tasks. Here B-4 is for BLEU-4, R-1/2/L for ROUGE-1/2/L. Figure 5: Prompt template used for the summarization task. Figure 6: Prompt template used for the QA pair extraction task. Figure 7: Prompt template used for the question answering task. All questions from document are listed, and the LLMs are prompted to answer them in single response. Figure 8: Prompt template utilized for assessing summarization with GPT-4. Figure 9: Prompt template utilized for assessing both QA pair extraction and question answering with GPT-4-Judge. Model Summarization QA Pair Extraction Question Answering Overall P. R. F1 GPT-4 P. R. GPT-4 P. R. F1 GPT-4 GPT-4 GPT-4o GPT-3.5-turbo GLM4-9B-Chat LLaMA3.1-8B-Instruct Qwen2-7B-Instruct Qwen2-72B-Instruct Qwen2.5-72B-Instruct GPT-4o GPT-3.5-turbo GLM4-9B-Chat LLaMA3.1-8B-Instruct Qwen2-7B-Instruct Qwen2-72B-Instruct Qwen2.5-72B-Instruct GPT-4o GPT-3.5-turbo GLM4-9B-Chat LLaMA3.1-8B-Instruct Qwen2-7B-Instruct Qwen2-72B-Instruct Qwen2.5-72B-Instruct 5.58 6.84 0.63 1.37 13.15 15.43 6.40 31.49 18.34 21.24 7.72 28.69 30.21 32.38 37.03 24.14 33.65 3.12 43.32 41.41 35.67 3.62 4.17 3.29 1.65 12.84 16.02 4. 11.55 4.52 10.79 9.03 16.22 16.99 14.63 25.03 16.77 22.15 6.51 50.81 48.93 34.91 4.39 5.18 1.06 1.49 12.99 15.72 5.51 16.90 7.24 14.31 8.32 20.72 21.75 20.16 29.87 19.79 26.71 4.22 46.77 44.86 35.29 72.83 33.68 60.45 45.98 72.71 71.92 73. 72.03 48.42 67.90 55.23 71.68 72.48 72.74 80.06 39.75 74.32 44.90 82.42 83.49 82.40 English 40.12 19.32 2.54 5.78 3.91 15.26 39.81 19.21 3.31 7.31 9.05 2.03 10.38 22.57 Chinese 40.76 25.11 5.31 6.61 20.89 32.20 45.82 23.22 6.64 15.18 12.46 12.74 25.97 34.47 Japanese 61.04 41.95 19.85 39.34 13.92 36.87 67.74 35.51 5.53 29.33 26.56 6.64 25.00 46.49 25.98 5.65 3.77 7.06 2.67 12.36 28. 29.58 10.51 7.86 8.64 15.83 28.75 39.34 44.89 9.77 23.67 31.71 8.99 29.79 55.14 68.37 26.70 39.69 45.53 29.85 54.85 72.76 63.43 31.41 43.45 39.87 40.47 58.85 63.59 81.50 35.55 66.88 66.66 31.82 78.46 83.45 98.25 98.58 98.31 87.09 97.75 98.31 98. 91.49 92.20 90.32 30.30 91.66 92.02 92.22 90.95 91.98 90.56 91.07 91.07 90.89 89.75 98.25 36.27 97.62 85.02 86.01 94.89 96.45 91.86 59.67 91.61 48.19 88.38 91.83 92.48 90.86 17.98 90.31 90.40 90.40 90.22 89.66 98.25 53.03 97.96 86.04 91.50 96.57 97. 91.68 72.45 90.96 37.21 89.99 91.92 92.35 90.91 30.09 90.43 90.74 90.74 90.55 89.70 67.02 35.68 69.76 52.75 71.95 75.03 76.76 69.33 47.42 64.14 34.07 66.75 70.57 71.88 85.88 27.36 82.43 52.30 83.17 85.78 86.95 69.41 32.02 56.63 48.04 58.16 67.26 74. 68.36 42.57 58.81 43.38 59.95 67.43 69.49 82.62 34.68 74.52 53.79 67.20 82.65 84.10 Table 10: Performance of LLMs on three evaluation tasks with different languages. Model Summarization QA Pair Extraction Question Answering Overall P. R. F1 GPTP. R. F1 GPT-4 P. R. F1 GPT-4 GPT-4 GPT-4o GPT-3.5-turbo GLM4-9B-Chat LLaMA3.1-8B-Instruct Qwen2-7B-Instruct Qwen2-72B-Instruct Qwen2.5-72B-Instruct GPT-4o GPT-3.5-turbo GLM4-9B-Chat LLaMA3.1-8B-Instruct Qwen2-7B-Instruct Qwen2-72B-Instruct Qwen2.5-72B-Instruct GPT-4o GPT-3.5-turbo GLM4-9B-Chat LLaMA3.1-8B-Instruct Qwen2-7B-Instruct Qwen2-72B-Instruct Qwen2.5-72B-Instruct GPT-4o GPT-3.5-turbo GLM4-9B-Chat LLaMA3.1-8B-Instruct Qwen2-7B-Instruct Qwen2-72B-Instruct Qwen2.5-72B-Instruct GPT-4o GPT-3.5-turbo GLM4-9B-Chat LLaMA3.1-8B-Instruct Qwen2-7B-Instruct Qwen2-72B-Instruct Qwen2.5-72B-Instruct 25.00 18.25 23.10 10.63 25.25 23.82 28.02 26.01 16.22 16.34 9.90 25.59 25.41 27.60 29.80 17.76 10.15 9.93 28.48 30.68 29.57 23.84 15.31 15.27 7.13 32.26 34.69 27.47 37.81 0 3.90 1.89 34.06 32.30 34. 28.52 28.52 26.17 19.14 39.84 39.45 37.11 17.01 11.67 13.75 12.22 26.94 27.01 22.29 11.30 6.59 9.92 6.80 17.65 18.97 13.97 8.96 0.79 10.92 6.31 18.60 19.87 12.98 9.24 0 8.02 5.73 11.99 11.92 11.15 26.64 22.26 24.54 13.67 30.91 29.71 31. 20.57 13.57 14.93 10.94 26.25 26.19 24.66 16.39 9.61 10.03 8.07 21.79 23.44 18.98 13.02 1.51 12. 73 6.69 23.60 25.27 17.63 14.86 0 5.25 2.84 17.74 17.41 16.91 69.75 58.76 66.76 54.35 67.18 70.29 70.40 73.51 62.07 68.46 56.25 71.97 73.39 72. 73.34 61.43 68.80 51.80 74.21 73.43 74.41 74.62 19.00 67.41 45.04 76.08 76.60 76.67 75.01 0 65.84 52.82 75.52 75.70 76.79 Set1 (0-5K) 36.90 27.06 14.27 16.94 26.33 25.78 41.60 24.29 15.29 16.19 17.24 17.84 23.69 33. Set2 (5-10K) 44.78 27.47 17.11 4.63 21.98 37.96 48.25 26.76 9.10 15.05 13.27 14.79 30.70 35.38 Set3 (10-15K) 43.45 22.18 3.03 8.85 14.55 32.47 47.71 24.04 6.31 17.21 14.01 10.11 25.39 35. Set4 (15-20K) 42.40 31.25 8.49 18.48 11.14 20.87 48.23 23.14 0.79 15.01 13.27 4.77 16.11 33.58 Set5 (>20K) 44.14 0 6.18 9.45 10.93 16.67 46.97 17.68 0 11.42 10.68 2.49 8.84 25. 29.30 19.54 15.17 17.09 21.27 24.69 37.07 33.50 13.68 16.01 6.87 17.68 33.93 40.82 30.96 9.83 5.15 10.85 11.93 28.49 40.50 29.94 1.54 10.85 15.45 6.68 18.18 39.59 25.25 0 8.02 10.03 4.05 11.55 33.21 69.55 44.38 39.20 39.90 50.95 56.48 60. 65.37 45.45 45.31 41.96 43.90 61.36 65.91 67.56 38.86 48.28 44.69 36.04 61.37 67.44 71.32 14.36 50.09 48.97 31.29 62.49 73.37 65.42 0 41.12 45.14 31.36 59.15 68.59 93.70 93.70 93.77 15.84 93.54 93.55 93.70 91.97 91.98 90.55 25.73 91.90 92.20 92. 91.90 93.90 91.86 38.50 92.26 92.62 92.30 94.76 100.00 93.82 70.59 94.41 94.90 94.97 92.36 0 90.19 77.85 91.87 92.63 93.09 93.70 93.70 90.26 38.23 93.40 93.55 93.70 92.31 79.26 90.75 42.29 88.48 92.20 92.27 93.48 65.30 93.48 52.85 88.86 91.72 91. 94.96 0.68 94.81 78.44 87.72 93.65 95.02 89.04 0 92.27 77.35 83.24 90.34 93.09 93.70 93.70 91.98 22.40 93.47 93.55 93.70 92.14 85.15 90.65 31.99 90.16 92.20 92.27 92.68 77.03 92.67 44.55 90.53 92.17 92.06 94.86 1.36 94.31 74.30 90.94 94.27 94. 90.67 0 91.22 77.60 87.34 91.42 93.09 64.33 62.67 57.33 31.81 65.62 66.24 67.07 69.10 64.69 63.73 30.91 66.61 70.09 71.93 72.54 56.94 68.94 36.16 70.89 74.14 75.89 75.26 11.16 71.86 52.89 72.85 76.97 76.65 69.99 0 70.89 47.42 71.71 76.33 78. 65.25 55.89 56.24 42.96 61.93 64.74 66.74 69.42 57.71 59.10 43.95 61.47 68.57 70.59 70.49 53.22 61.42 44.48 60.06 69.66 72.69 74.54 6.19 64.54 50.52 61.83 72.91 76.73 70.91 0 58.63 47.13 57.21 69.83 73.28 Table 11: Performance of LLMs on three evaluation tasks with different length sets. Model Summarization QA Pair Extraction Question Answering Overall P. R. F1 GPT-4 P. R. GPT-4 P. R. F1 GPT-4 GPT-4 Communication Services GPT-4o Qwen2-72B-Instruct Qwen2.5-72B-Instruct 23.72 23.05 24.13 11.02 18.63 14.69 15.05 20.61 18.27 73.78 74.94 75.25 37.23 30.57 40. 24.70 26.11 28.47 29.70 28.17 33.42 68.71 62.55 70.06 94.02 95.29 95.29 96.23 95.29 95.29 95.11 95.29 95. 71.35 73.45 76.65 Consumer Discretionary GPT-4o Qwen2-72B-Instruct Qwen2.5-72B-Instruct 23.82 25.04 22.06 10.85 17.08 12.34 14.91 20.31 15. 72.24 72.61 72.62 41.86 29.95 44.60 22.16 23.38 30.92 28.98 26.26 36.52 64.63 60.16 67.19 93.04 93.68 93. 94.49 93.55 93.83 93.76 93.61 93.51 69.22 71.03 72.84 Consumer Staples GPT-4o Qwen2-72B-Instruct Qwen2.5-72B-Instruct 27.43 26.63 24. 10.90 16.92 13.05 15.60 20.70 17.23 71.77 73.83 73.75 41.91 32.41 44.79 18.51 22.08 29.13 25.68 26.27 35. 65.98 60.14 64.36 91.27 91.76 92.20 92.38 91.07 92.29 91.82 91.41 92.24 70.58 71.71 73.00 Energy GPT-4o Qwen2-72B-Instruct Qwen2.5-72B-Instruct 48.60 50.53 53.81 29.09 47.49 40.13 36.40 48.96 45.97 80.13 81.26 81.39 55.80 26.93 52. 38.54 25.19 45.41 45.59 26.03 48.87 74.14 58.57 73.90 95.40 95.38 95.40 95.03 94.65 95.03 95.21 95.01 95. 79.14 82.81 85.10 Financials GPT-4o Qwen2-72B-Instruct Qwen2.5-72B-Instruct 24.13 29.62 26.92 8.68 16.87 12.15 12.77 21.50 16. 73.81 71.37 75.02 41.34 28.38 46.91 29.62 24.31 36.71 34.52 26.19 41.19 68.50 61.41 68.43 95.49 94.47 95. 95.49 90.82 90.49 95.49 92.61 92.89 68.59 72.61 72.70 Healthcare GPT-4o Qwen2-72B-Instruct Qwen2.5-72B-Instruct 24.00 28.67 29. 9.25 16.97 12.76 13.36 21.32 17.73 73.28 73.72 73.37 41.51 22.22 44.52 25.72 20.71 37.54 31.76 21.44 40. 67.57 58.41 68.43 93.52 93.41 93.37 93.52 89.48 93.52 93.52 91.40 93.45 74.07 75.59 76.43 Industrials GPT-4o Qwen2-72B-Instruct Qwen2.5-72B-Instruct 27.15 28.93 29.35 10.56 18.15 13.53 15.20 22.30 18.52 72.13 73.33 73.94 43.81 31.95 50. 23.08 23.60 36.27 30.23 27.15 42.34 63.86 60.25 65.46 92.22 92.52 92.53 89.85 92.37 92.53 91.02 92.44 92. 68.74 71.75 73.67 Information Technology (IT) GPT-4o Qwen2-72B-Instruct Qwen2.5-72B-Instruct 23.82 28.78 27.38 11.02 21.13 16.29 15.07 24.37 20. 75.28 75.58 75.44 46.21 29.32 49.45 23.66 20.02 32.00 31.30 23.79 38.85 70.25 62.55 72.26 92.25 92.59 92. 93.07 91.72 92.77 92.66 92.15 92.72 73.47 75.30 76.67 Materials GPT-4o Qwen2-72B-Instruct Qwen2.5-72B-Instruct 30.17 31.63 29. 13.71 25.00 18.54 18.85 27.92 22.73 70.63 71.47 70.53 42.80 38.03 49.67 27.95 29.87 36.86 33.81 33.46 42. 64.58 61.42 65.96 91.34 91.58 91.56 91.56 91.80 91.56 91.45 91.69 91.56 73.04 75.19 76.42 Real Estate GPT-4o Qwen2-72B-Instruct Qwen2.5-72B-Instruct 66.66 48.00 61.29 38.16 36.64 43.51 48.54 41.55 50.89 82.23 82.85 83.54 38.80 16.92 48. 40 16.92 43.07 39.39 16.92 45.52 74.50 66.50 67.25 89.70 93.84 93.84 93.84 93.84 93.84 91.72 93.84 93. 78.88 82.75 83."
        },
        {
            "title": "Utilities",
            "content": "GPT-4o Qwen2-72B-Instruct Qwen2.5-72B-Instruct 57.77 48.64 53.70 43.33 60.00 48.33 49.52 53.73 50.87 79.25 80.25 79.50 39.28 22.72 65. 22.44 20.40 63.26 28.57 21.50 64.58 63.33 62.33 69.33 83.67 83.67 83.67 83.67 83.67 83.67 83.67 83.67 83. 82.00 85.67 86.33 71.41 70.55 73.10 68.74 68.00 73.10 69.47 68.63 70.41 78.12 75.17 80.30 70.37 68.52 72. 71.63 69.57 72.74 68.28 68.50 71.05 73.05 71.25 74.80 69.5 69.51 70.94 79.17 77.35 79.10 76.42 77.35 78. Table 12: Performance of LLMs on three evaluation tasks with different GICS sectors. To save space, we only report the performance of three LLMs. Figure 10: Annotation guidelines of M3FinMeeting."
        }
    ],
    "affiliations": [
        "Nanjing University",
        "Qwen DianJin Team, Alibaba Cloud Computing",
        "School of Computer Science and Technology, Soochow University"
    ]
}