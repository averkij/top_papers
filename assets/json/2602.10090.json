{
    "paper_title": "Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning",
    "authors": [
        "Zhaoyang Wang",
        "Canwen Xu",
        "Boyi Liu",
        "Yite Wang",
        "Siwei Han",
        "Zhewei Yao",
        "Huaxiu Yao",
        "Yuxiong He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model."
        },
        {
            "title": "Start",
            "content": "Agent World Model: Zhaoyang Wang 1 Canwen Xu 2 Boyi Liu 2 Yite Wang 2 Siwei Han 1 Zhewei Yao 2 Huaxiu Yao * 1 Yuxiong He * 2 6 2 0 2 0 1 ] . [ 1 0 9 0 0 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are codedriven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com /Snowflake-Labs/agent-world-model. 1. Introduction Large language models (LLMs) have achieved remarkable performance in instruction following, reasoning, code generation, and tool-use (Chen et al., 2021; Schick et al., 2023; These authors contributed equally and share last authorship. Work done during Zhaoyang Wangs internship at Snowflake. 1University of North Carolina at Chapel Hill 2Snowflake. Correspondence to: Canwen Xu <canwen.xu@snowflake.com>. Preprint. February 11, 2026. Figure 1. Agent World Model (AWM) is synthetic environment generation pipeline that synthesizes 1,000 diverse code-driven agentic environments with databases for training tool-use agents. OpenAI, 2025; Anthropic, 2025a; Comanici et al., 2025; Guo et al., 2025). Agents powered by LLMs emerge as promising paradigm for handling multi-step complex tasks in realistic environments (Nakano et al., 2021; Yao et al., 2023; Yang et al., 2024; Qin et al., 2024). However, training such agents often requires performing large-scale reinforcement learning (RL) on diverse environments that are relatively resource-scarce and expensive to scale. Using real-world environments for training is prohibitively expensive and hard to scale, since many scenarios do not expose public APIs and RL training often requires agents to interact with them thousands of times in stable and efficient manner (Dulac-Arnold et al., 2021; Qin et al., 2024; Xu et al., 2024a; Luo et al., 2025). Human-created environments are hard to scale and often lack diversity. For example, τ 2-bench (Barres et al., 2025) and TheMCPCompany (Esfandiarpoor et al., 2025) only contain three and five environments, respectively, which is far from enough for training generic AI agents. Most existing synthetic research on agents focuses on task synthesis (Wang et al., 2023; Chen et al., 2024; Xie et al., 2025; Patil et al., 2024; Wang et al., 2026) and trajectory collection (Xu et al., 2024b; Li et al., 2025a; Song et al., 2024) rather than environment synthesis. Another line of research simulates the tool response or even the environment, where each state transition is generated by LLMs (Liu et al., 2024b; Lu et al., 2025; Li et al., 2025b;c; Chen et al., 2025). However, this approach is not reliable or efficient due to the hallucination issue (Wang et al., 2024; Kalai et al., 2025) and LLMs high inference cost. These limitations highlight missing piece: scalable environment synthesis. In particular, The challenge is to synInfinity Synthetic Environments for Agentic Reinforcement Learning thesize executable, reliable environments at scale, enabling replicable agent interaction and learning. In recent months, DeepSeek-V3.2 (DeepSeek-AI et al., 2025) introduces synthesis pipeline to create thousands of executable environments for general agents, while Qwen Tongyi (Fang et al., 2025) also describes an environment synthesis pipeline but for supervised fine-tuning (SFT) rather than RL training. The adopted code-based approach is promising to build such environments at scale, since it can control the state transition and ensure the consistency of the environment. However, neither of them releases the generation pipeline nor opensources their environments. Several concurrent community efforts (Feng et al., 2025; Sullivan et al., 2025; Cai et al., 2025; Zhang et al., 2025a; Song et al., 2026) explore environment synthesis through programming. However, they either target game-like environments, rely on human priors (e.g., code documentation), lack strong guarantees of state consistency, or remain limited in scale. To address this, we propose Agent World Model (AWM), an open-source pipeline that synthesizes executable tooluse environments at scale. The key insight is that agent environments share common structure: stateful backend, tools interface layer, and task-specific success criteria. By decomposing synthesis into these three components, we can leverage LLMs to generate each part systematically while maintaining consistency. Our AWM mirrors how software is built in practice. Starting from high-level scenario description (e.g., an online shopping platform), we first generate common user requirements (i.e., tasks) that users are likely to perform in this scenario. Then, we generate the database schema to define what entities and relations exist to fulfill these user requirements. This schema can guide the design of exposed interfaces (toolset) and help generate the backend code for the interfaces, ensuring each tool has clear data model to operate on. The interface is exposed via Model Context Protocol (MCP) (Anthropic, 2024) for unified agent tool interaction with the environment. Finally, we generate verification code that compares the database state before and after agent execution, which augments an LLM-as-a-Judge to provide robust reward signals for RL training. Critically, each stage includes automated execution and simple self-correction: if generated code fails to run, we feed the error information back to the LLM for correction. This pipeline enables us to scale to 1,000 unique environments spanning most real-world scenarios such as shopping, social media, finance, and travel. Each environment provides an executable sandbox where agents can interact with dozens of tools, and they fully support parallel isolated instances and are easy to reset or restart, which are important for efficient online RL. To validate AWM, we perform large-scale RL (each step with 1,024 environment instances) to train agents to use MCP tools to complete the task. In contrast to some works performing training on test environments, our environments and tasks are not tailored for any benchmarks or specific scenarios. Experimental results on three tool-use benchmarks demonstrate the generalization performance of our framework. In summary, our contributions are three-fold: (1) AWM, an open-source pipeline for automatic generation of executable tool-use environments with database-backed state consistency, (2) 1,000 ready-touse diverse environments suitable for large-scale RL training, and (3) empirical results show that agents trained on AWM generalize to out-of-distribution environments. 2. Related Work Tool-use Agents. Recent work shows that LLMs can use external tools to solve complex tasks (Qin et al., 2024; OpenAI, 2025; DeepSeek-AI et al., 2025; Team et al., 2025). Toolformer (Schick et al., 2023) trains tool-use LLMs by supervised learning. ToolLLM (Qin et al., 2024) curates real-world APIs and trains on LLM-generated trajectories, but uses simulated responses instead of tool execution. Gorilla (Patil et al., 2024) fine-tunes with API documentation to improve tool-use accuracy. ReAct (Yao et al., 2023) and SWE-agent (Yang et al., 2024) alternate reasoning and acting in interactive environments. However, most training data remains static or comes from small-scale environments. Existing benchmarks (Yao et al., 2024; Barres et al., 2025; Esfandiarpoor et al., 2025; Luo et al., 2025; Fan et al., 2025; Mo et al., 2025) either use real-world APIs or provide smallscale environments. This makes them hard to use as largescale RL training grounds. The gap is the lack of diverse and executable environments that are efficient enough for RL. Such training needs extensive agent interactions, and it benefits from fast interactions and reliable state transitions. Agent Data Synthesis. Synthetic data is widely used to scale agent training (Patil et al., 2024; Schick et al., 2023; Qin et al., 2024; Liu et al., 2024b; 2025). SelfInstruct (Wang et al., 2023) popularizes using LLMs to generate data for fine-tuning. Subsequent work uses LLMs to synthesize tasks, tools specifications, and agent trajectories (Xie et al., 2025; Xu et al., 2024b; Li et al., 2025a; Song et al., 2024; Liu et al., 2024b; 2025; Wang et al., 2026; Prabhakar et al., 2025). These methods provide diverse tasks and tool-use demonstrations, but do not offer executable environments for agent interaction. The key limitation is that they treat the environment as given or simulate tool responses with LLMs. Without environment synthesis, agents cannot explore alternative actions or receive grounded feedback from state changes, which limits applicability to RL. Environment Synthesis. As agentic RL grows, the need for diverse and executable environments becomes more urgent. Environment synthesis largely follows two directions: LLM-based simulation (Wang et al., 2024; Li et al., 2025c; Chen et al., 2025; Li et al., 2025b) and programming-based 2 Infinity Synthetic Environments for Agentic Reinforcement Learning Figure 2. Overview of AWM. Starting from scenario synthesis, we progressively generate tasks, database, interface and verification to obtain fully executable environments. Then, we perform multi-turn RL training for tool-use agents in our synthesized environments. synthesis (Tang et al., 2024; Sullivan et al., 2025; Cai et al., 2025; Zhang et al., 2025a; Song et al., 2026). LLM-based simulation often simulates the environment by prompting reasoning model to generate state transition and observation. However, it suffers from hallucinations in state transition (Kalai et al., 2025; Wang et al., 2024). It is also expensive and inefficient for RL, since each environment step may require an LLM call. Programming-based synthesis builds environments through programming and database, where each state transition and observation are driven by the code. Some methods extract tool graphs from documentation and sample tool call sequences to build environments (Fang et al., 2025; Cai et al., 2025). Procedural generation has also been explored, for example with human designed type systems (Sullivan et al., 2025). AutoEnv (Zhang et al., 2025a) creates 36 game-like environments (e.g., maze navigation) for simulating heterogeneous worlds. EnvScaler (Song et al., 2026) is concurrent work that synthesizes 191 interactive environments via code generation given existing task sets. The proposed AWM differs in three aspects: (1) We synthesize environments from scratch without assuming an existing task set or API documentation. This avoids potential copyright infringement. (2) We use database-backed state management to enforce consistency and enable codeaugmented verification for RL. (3) With 1,000 environments, 35,062 tools and 10,000 tasks paired with verification code synthesized, to the best of our knowledge, AWM is the largest open-source tool-use environment set to date. 3. Agent World Model In this section, we describe AWM in details, as shown in Figure 2. We synthesize environments as partially observable Markov decision processes (POMDPs) suitable for agentic RL training. Each environment Ei comprises five components: state space SEi, an action space AEi, an observation space OEi, transition function TEi : SEi AEi SEi OEi , and task-specific reward functions Rτ for each τ TEi that is the task set for the environment. The goal of the agent is to complete tasks by interacting with the environment through multi-turn tool interactions. The synthesis pipeline in Sec. 3.3.1 would instantiate: the database defines SEi , the interface layer defines AEi, OEi, and TEi, while verification provides Rτ for each user task. 3.1. Scenario Generation We leverage the vast world knowledge of modern LLMs to generate diverse scenario descriptions (websites, apps, or common tools collection), seeded with 100 popular domain names. Unlike general web agents that navigate static content sites (e.g., news, wikis), we focus on stateful applications (e.g., e-commerce, CRM, management) that necessitate database interactions instead of information retrieval. To ensure quality, we employ filtering pipeline: an LLMbased classifier selects scenarios involving core CRUD operations (create, read, update, delete), while embeddingbased deduplication ensures diversity. We also cap overrepresented categories to prevent the collection from collapsing to few dominant types. This process yields 1,000 unique scenarios spanning finance, travel, retail, social media, and more, as shown in Figure 6 and Table 9. 3.2. Task Generation Following software engineering principles, we then synthesize user tasks to serve as functional requirements for the environment. These tasks would dictate the necessary database entities and API endpoints in subsequent synthesis steps. 3 Infinity Synthetic Environments for Agentic Reinforcement Learning For each scenario, we prompt the LLM to generate = 10 different tasks TEi = {τi,j}k j=1 covering diverse functionalities of the scenario. We enforce two design principles: (1) API-solvability, avoiding purely UI-dependent actions (e.g., clicking, page navigation), and (2) post-authentication context, assuming login is completed to focus on deep functionalities rather than access control, because such authentication is typically handled by human in realistic settings. This results in 10,000 executable tasks that would drive the synthesis of the environment backends. 3.3. Environment Synthesis Given scenario description and its task set TEi, we synthesize an executable environment Ei by instantiating POMDP components. First, we construct the state space SEi and initial state by generating SQLite (Gaffney et al., 2022) schema and populating it with synthetic sample data that supports the entities and constraints implied by TEi. Then, we generate an MCP (Anthropic, 2024) interface layer that exposes toolset in Python, which defines the action space AEi as tool calls and the observation space OEi as tool responses. Calling tool runs database operations that read and write, which implements the environment transition function TEi. Finally, we synthesize verification logic to define the task-specific reward functions Rτ . This design grounds environment state in structures, while ensuring agents interact with Ei only via unified MCP interface. 3.3.1. ENVIRONMENT Database. The database grounds each environment in concrete and persistent state. We use SQLite (Gaffney et al., 2022) as the state backend for structured relational state, unlike the simplified NoSQL or key-value stores used in concurrent works (Zhang et al., 2025a; Cai et al., 2025; Song et al., 2026). Given the scenario and task set TEi, the LLM infers required entities/attributes/relations to make every task feasible, generating tables only when required by the tasks. We exclude authentication-related fields to align with the task generation step. The schema defines the state space SEi and constrains all transitions through explicit keys and constraints. However, schema alone is insufficient, as many tasks require querying or updating existing records. We therefore synthesize sample data that instantiates realistic initial state s0, ensuring every task in TEi is executable from the start. The LLM analyzes preconditions for each task and creates records satisfying these constraints, including variation data needed for robust execution. Interface. Agents cannot directly manipulate the database without breaking abstraction. We introduce Python interface layer exposed via MCP (Anthropic, 2024) that defines the action space AEi and observation space OEi. We use two stages: first toolset schema design, then code generation. Pilot experiments show that environments may require over 3,000 lines of code, making direct generation challenging without schema guidance. Given the task set and database schema, the LLM infers the minimal set of operations required to make every task executable, generating endpoints only when necessary. The schema also serves as documentation for agents through summaries, typed parameters, and response schemas, as shown in Table 10. We then generate an executable Python file where each endpoint becomes an MCP tool. Tool execution triggers database operations, implementing the transition function TEi. Verification. To complete the POMDP specification, we define task-specific reward functions Rτ to enable RL training. We design task-associated verification module for each task τ that grounds evaluation in the environment state. Specifically, this module inspects the database state before and after agent execution, extracting task-relevant signals and success or failure criteria that describe how to interpret state differences. However, because our environments are fully synthetic, verification can occasionally be affected by environment imperfections, such as incomplete state updates, unexpected execution failures, or infrastructurerelated issues (e.g., timeouts). To improve reward robustness, the ultimate decision is made by an LLM-as-aJudge (Zheng et al., 2023), which combines the agent trajectory with structured verification signals. LLM-as-a-Judge complements code-based checks by leveraging trajectorylevel context which helps mitigate misjudgments caused by imperfect environment signals. Finally, the verification step returns one of {Completed, Partially Completed, Agent Error, Environment Error}. natural question arises: why not rely entirely on codedriven verification? While appealing, this approach assumes that task success is perfectly specifiable and reliably observable from state alone. In practice, this assumption is fragile. Even realistic services exhibit imperfect behavior due to transient failures, partial executions, or infrastructure issues; synthetic environments are no exception. In general task settings, brittle verification logic can introduce false positives or negatives, an issue that has surfaced repeatedly in existing benchmarks such as WebArena, τ 2-bench, and SWEBench, which were later revised to correct earlier misjudgments (hattami et al., 2025; Cuadron et al., 2025; OpenAI, 2024). Our code-augmented LLM-as-a-Judge addresses this by combining the precision of code-based verification with the flexibility and context-awareness of LLM reasoning. Specifically, structured verification signals ground the LLM in concrete evidence, enabling it to resolve ambiguities that rigid code alone cannot handle. We also prepare the codedriven verification for AWM, providing the community with both options, though our analysis in Sec. 6.2 and case study of verification failure cases in Appendix B.2 confirm that code-augmented LLM-as-a-Judge is more robust. 4 Infinity Synthetic Environments for Agentic Reinforcement Learning Table 1. Statistics of the synthesis pipeline. Success rates and trial counts reflect the self-correction. Costs are averaged per 100 samples using GPT-5 (OpenAI, 2025) as the generation model. Table 3. Comparison with existing programming-based environments. Our AWM generates environments with minimal reliance (only seed set of scenario names) on human or real APIs, achieving the largest scale with SQL-backed state consistency. Synthesis Stage Success (%) # Trial Cost ($) Scenario Task Database Sample Data Toolset Schema Env Code Verification Total 88.3 88.2 86.8 1.12 1.12 1.13 0.43 0.56 3.59 13.75 23.74 12.81 2.21 57.09 Table 2. Environment complexity statistics. Agent steps and unique tools are measured using Claude-4.5-Sonnet (Anthropic, 2025a) as the agent backbone, with maximum step budget of 20. About 13.7% of tasks exceeded this budget. Metric (#) Mean Median Top 90% Database Tables Sample Data Records Exposed Tools Environment Code Lines Agent Steps per Task Unique Tools used per Task 18.5 129.3 35.1 1,984.7 8.5 7.1 18.0 121.0 35.0 1,944.0 6.0 6.0 25.0 192.0 45.0 2,586.0 20.0 12.0 Execution-based Self-Correction. Across all synthesis steps described above, we employ simple self-correction mechanism to handle generation errors. After generating each component of the environment, we attempt to run it in an isolated environment and test the functionality. If any error occurs (e.g., runtime exceptions), we capture the error message and feed it back to the LLM together with the problematic code snippet, prompting the LLM to regenerate corrected version. This process is repeated for up to five iterations or until the component executes successfully. In practice, we often find that this lightweight retry strategy is effective in repairing generated code, without requiring more complex correction mechanism. 3.3.2. PIPELINE RESULTS AND ANALYSIS Through AWM, we synthesize 1,000 executable environments along with 10,000 tasks. Table 1 shows the statistics of the synthesis process. The pipeline achieves over 85% success rates, and the self-correction mechanism requires only 1.13 iterations on average to repair, confirming the rationality of the pipeline design. Table 2 reports the complexity of synthesized environments. These statistics further show that each stage generates non-trivial artifacts, far exceeding toy environments. Table 3 compares AWM with existing environment sets. Our method achieves the largest scale, with 5 more environments than the closest concurrent work EnvScaler (Song et al., 2026), while requiring minimal human participation beyond 100 scenario names. This indicates that large-scale synthesis of executable environments is both feasible and cost-effective via AWM. 5 Method Syn. Reliance SQL # Envs # Tools # Code τ -bench τ 2-bench MCP-Universe AutoForge EnvScaler AWM Human Human Real APIs Tool Doc Task Set Names Only 2 3 11 10 191 1,000 12.5 22.7 12. 18.6 35.1 662.1 1984.7 4. Agentic Reinforcement Learning With the synthesized environments, we perform online reinforcement learning for tool-use agents using Group Relative Policy Optimization (GRPO) (Shao et al., 2024). Agentic interaction involves long-horizon trajectories with interleaved observations and tool calls, requiring both careful reward design and alignment between training and inference. 4.1. Reward Design Purely outcome-based rewards have shown success in mathematical reasoning (Guo et al., 2025); however, in agentic environments, they may be insufficient and inefficient to regularize tool-use behavior. We therefore adopt hybrid reward design that combines step-level format correctness with task-level outcome verification. At each step t, we check whether the tool call follows the required format (see Appendix A.4). Any violation triggers early termination of the rollout and an immediate negative reward. This both discourages invalid actions and saves computation in long-horizon multi-turn settings. After rollout terminates normally, we evaluate the task-level outcome via our codeaugmented LLM-as-a-Judge, defining the final reward as: Rτ = 1.0, if task τ Completed, 0.1, if task τ Partially Completed, 0.0, otherwise. (1) The step-level reward rt follows: if early termination occurs at step t, rt = 1.0; if the rollout terminates normally, rt = Rτ is broadcast to all action steps; otherwise rt = 0. This design encourages syntactically valid tool usage while preserving outcome-driven optimization. 4.2. History-Aware Training When deploying agents, history context is often managed by dedicated framework that strategically truncates long interaction histories to avoid attention sink and improve efficiency (Liu et al., 2024a; Anthropic, 2025b; Zhang et al., 2025b). However, existing RL training pipelines may optimize policies using full histories, creating distribution mismatch issue between training and inference. Infinity Synthetic Environments for Agentic Reinforcement Learning Let ht = (o1, a1, o2, a2, . . . , ot) denote the full interaction history. In practice, many RL frameworks (Sheng et al., 2024; Hu et al., 2025) optimize all actions from completed rollout in one model forward pass for efficiency: = log πθ(o1, a1, o2, a2, . . . , oT , aT (cid:125) (cid:124) (cid:123)(cid:122) one forward pass , [0, 1, 0, 1, . . . , 0, 1] (cid:123)(cid:122) (cid:125) (cid:124) loss mask ) (2) where πθ is the agent parameterized by θ, and the mask selects action tokens while ignoring observations. At inference time, however, the agent may condition on truncated history htrunc = (omax(1,tw+1), amax(1,tw+1), . . . , ot), which results in distribution shift if training always uses full history. To address this issue, we align training with inference by applying the same truncation during optimization. Under GRPO, for each task τ in environment Ei, we sample group of rollout trajectories {y(k)}G k=1, where y(k) = (a(k) ), and optimize: 1 , . . . , a(k) Tk LGRPO = τ,Ei,{y(k)} (cid:34) 1 (cid:88) k=1 A(k) Tk(cid:88) t=1 log πθ(a(k) htrunc,(k) ) , (3) (cid:35) where A(k) = (R(k) R)/σR is the group-relative advantage computed from the rollout rewards {R(j)}G j=1. This objective splits the trajectory into multiple individual subtrajectories, each conditioned on its own truncated history, ensuring consistency with inference-time execution. 5. Experiments 5.1. Experimental Setup Benchmarks. We evaluate agents on three tool-use and MCP benchmarks to assess out-of-distribution generalization: (1) The verified version of τ 2-bench (Barres et al., 2025; Cuadron et al., 2025), which consists of multi-turn conversational agentic tasks across three representative scenarios: airline, retail, and telecom; (2) BFCLv3 (Patil et al., 2025), comprehensive benchmark for function-calling ability evaluation, covering single-turn, multi-turn (longcontext), synthetic tools, real-world tools, and hallucination tests, resulting in four evaluation categories: non-live, live, multi-turn, and hallucination; (3) MCP-Universe (Luo et al., 2025), collection of real-world MCP servers spanning location navigation, financial analysis, browser automation, web search, and multi-server workflows. We exclude 3D design tasks that require GUI and repository management tasks that require authenticated access to GitHub or Notion. Baselines. We compare against the following baselines: (1) Base: the original LLMs equipped with reasoning and tool-use capabilities, without additional training; (2) Simulator (Li et al., 2025b; Chen et al., 2025): agents trained with 6 RL in LLM-simulated environments, where GPT-5 (OpenAI, 2025) serves as the environment transition model, using the same tasks and toolsets as AWM, to highlight the advantage of executable environments over simulated ones; (3) EnvScaler (Song et al., 2026): concurrent method that synthesizes 191 programming-based environments for SFT and RL, included to compare the synthesis quality. Implementation Details. The AWM pipeline is instantiated using GPT-5 (OpenAI, 2025), which is also used as the code-augmented LLM-as-a-Judge for task verification. We select Qwen3 thinking models (Yang et al., 2025) across 4B, 8B, and 14B scales as base agents due to their popular community adoption. Multi-turn RL training is implemented on top of AgentFly (Wang et al., 2025a) and verl (Sheng et al., 2024). Due to limited computation budget, we train agents on subset of AWM consisting of 526 environments and 3,315 tasks. Each model is trained for up to 96 optimization steps with fixed learning rate of 7 107. We use batch size of 64 and 16 rollouts, resulting in 1,024 isolated environment instances launched per step. The sliding window size and maximum interaction turn are set to = 3 and 20, respectively. More details are in Appendix A. 5.2. Main Results Table 4 presents results on three out-of-distribution benchmarks. AWM does not target conversational interaction, whereas τ 2-bench requires multi-turn dialogue. AWM omits refusal scenarios, while BFCLv3 stresses hallucination resistance. AWM also excludes browser automation and information retrieval, both central to MCP-Universe. On BFCLv3, AWM improves performance across all models. For 8B model, the overall score increases from 53.83 to 65.94, surpassing Simulator and EnvScaler. Gains are broadly distributed, with modest weakness on hallucination, likely due to our format correctness reward that always encourages tool use and penalizes refusals (Appendix A.4). On τ 2-bench, AWM is competitive with EnvScaler and consistently exceeds Simulator. Notably, EnvScaler regresses on BFCLv3 (-8.93) and MCP-Universe (-1.39) on average, whereas ours improves over Base across all benchmarks; plausibly because EnvScaler relies on existing tasks for synthesis that may overlap with τ 2-bench. On MCP-Universe, AWM achieves the best overall results, with large gains in Financial and Location. These results indicate that training on our synthetic environments builds robust tool-use capabilities that transfer to real-world scenarios. Moreover, the comparison with Simulator suggests that programmingbased state consistency provides more stable learning signal than LLM-generated interactions, while substantially reducing RL latency, since Simulator requires an LLM call at each interaction step. Overall, AWM shows the strongest generalization, validating the effectiveness of our synthesis. Infinity Synthetic Environments for Agentic Reinforcement Learning Table 4. Results across BFCLv3, τ 2-bench, and MCP-Universe benchmarks. Best performance is in bold. For BFCLv3, Hall. refers to the hallucination category. For τ 2-bench, Pass@k denotes task success rate allowing attempts, with Pass@1 averaged over 4 runs. For MCP-Universe, we report task success rates. Note that EnvScaler did not release the 14B model. Table 5. Quality analysis of 100 sampled environments. We compare AWM with EnvScaler using two LLM judges (GPT-5.1 and Claude-4.5-Sonnet). Top section shows scores (15 scale, higher is better). The bottom shows code bug analysis ( lower is better). Note that bugs may only affect few tools and edge use cases. GPT-5. Claude 4.5 BFCLV3 LEADERBOARD Metric AWM EnvScaler AWM EnvScaler Method Non-Live Live Multi-Turn Hall. Overall LLM-as-a-Judge Scores () Base Simulator EnvScaler AWM Base Simulator EnvScaler AWM Base Simulator AWM 61.44 66.10 60.31 78.60 59.58 56.35 33.67 80. 69.48 77.23 81.46 63.95 62.69 64.25 76.91 58.03 59.36 31.83 72.39 67.28 73.43 77.20 39.38 37.75 37.63 39.38 43.88 41.88 45.00 45. 47.00 52.38 51.88 73.93 75.48 85.25 73.70 76.42 74.06 90.30 70.80 77.94 76.91 78.37 54.92 55.52 54.06 64.50 53.83 52.53 36.83 65. 61.25 67.68 70.18 τ 2-BENCH Domain (Pass@1) Overall Method Airline Retail Telecom Pass@1 Pass@4 Base Simulator EnvScaler AWM Base Simulator EnvScaler AWM Base Simulator AWM 21.00 15.50 31.50 19.00 26.50 34.00 31.50 38.50 27.00 21.50 31.50 19.96 18.64 44.30 30.26 34.43 32.24 49.56 41. 65.35 48.90 63.60 9.43 7.90 12.50 16.45 18.42 29.17 32.68 23.47 12.28 18.20 17.76 MCP-UNIVERSE 15.83 13.67 28.96 22. 26.44 31.30 39.39 33.45 36.69 31.39 39.03 34.89 35.25 51.80 43.89 50.72 54.32 63.31 55.40 55.40 55.40 57.19 Method Location Financial Browser Web Multi Overall Base Simulator EnvScaler AWM Base Simulator EnvScaler AWM Base Simulator AWM 2.86 0.00 0.00 0. 0.00 5.71 0.00 8.57 0.00 2.86 8.57 25.00 15.00 17.50 22.50 22.50 10.00 17.50 35.00 27.50 30.00 32.50 0.00 5.88 2.94 2. 5.88 8.82 5.88 8.82 5.88 8.82 11.77 0.00 2.00 0.00 2.00 2.00 4.00 2.00 0.00 2.00 6.00 4.00 0.00 0.00 0.00 5. 0.00 0.00 0.00 5.00 5.00 0.00 0.00 6.15 6.15 4.47 6.70 6.70 6.15 5.59 11.17 8.38 10.62 12.29 8 4 1 4 8 4 1 8 4 1 6. Analysis 6.1. Quality of Synthesized Environments We evaluate synthesized environments based on two critical criteria for scalable agentic RL: quality (tasks are executable with coherent schema and tool interfaces) and diversity (training does not collapse to near-duplicate environments). Quality. Table 5 reports LLM-as-a-Judge scores on Task Feasibility (whether tasks are executable within the environment), Data Alignment (whether the data schema is coherent with the task), and Toolset Completeness (whether the 7 Task Feasibility Data Alignment Toolset Completeness 3.681.02 4.040.91 3.650.87 2.941.25 3.730.89 2.890.79 3.990.81 4.840.50 4.980.14 3.141.29 4.111.02 4.060. Envs w/ Bugs # Bugs per Env Blocked Tasks Bug Analysis () 74% 4.13 14.0% 88% 1.82 57.1% 83% 2.70 11.5% 83% 2.21 46.8% Figure 3. Diversity analysis of 1,000 synthesized environments. (a) Embedding diversity is calculated by encoding the scenario description, database schema and toolset schema. (b) Category coverage counts the number of unique topics of scenarios. toolset is complete and usable), where AWM consistently outperforms EnvScaler, indicating stronger end-to-end consistency from tasks database interface. Despite our environments containing roughly 3x more code than those generated by EnvScaler (see Table 3), this increase in scale results in only moderate rise in bugs, highlighting effective scalability. Both methods exhibit implementation issues at this scale, manual inspection on our environments attributes 44% of bugs to not handling edge input cases and 14% to operations conflicting with database constraints. During RL training, the environment error rate consistently remains low, around 4%, indicating overall reliability and usability of our environments. AWM yields fewer blocked tasks than EnvScaler, which is critical for RL because blocked tasks truncate exploration and inject systematically incorrect negative signals. To mitigate such imperfect environment issue, we design Environment Error reward in Sec. 3.3.1. Diversity. Figure 3 analyzes diversity from semantic and topical perspectives. Embedding diversity remains stable as pool size grows, suggesting newly generated environments continue to add novel content rather than forming duplicates. Meanwhile, category coverage steadily increases, showing that AWM globally expands into new regions instead of collapsing to few dominant domains. Overall, quality and diversity results suggest that AWM can scale to thousands Infinity Synthetic Environments for Agentic Reinforcement Learning Table 6. Overall performance across three verification strategies: LLM-only, code-only, and code-augmented. LLM-only verification decides the rewards based on the agent trajectory. Code-only verification inspects database state differences and the final answer, deciding either Completed (rt = 1) or Others (rt = 0). Size Verification BFCLv3 τ 2 P@1 τ 2 P@4 MCP 4B 8B 14B LLM Code Augmented LLM Code Augmented LLM Code Augmented 51.92 55.66 64.50 55.46 60.00 65.94 65.44 65.04 70. 15.65 14.93 22.57 26.44 29.59 33.45 31.03 29.50 39.03 35.97 32.01 43.89 52.52 52.88 55.40 55.76 53.60 57. 6.70 6.15 6.70 10.62 5.59 11.17 10.62 8.38 12.29 Table 7. Analysis of history-aware training with aligned and misaligned inference settings with 4B model. Aligned uses the same history limit (HL) setting for training and inference (i.e., no truncation for model w/o HL, while truncated history for model w/ HL). Misaligned uses opposite settings for training and inference. Setting Method BFCLv3 τ 2 P@1 τ 2 P@4 MCP Aligned Misaligned w/ HL w/o HL w/ HL w/o HL 64.50 55.35 61.85 56.80 22.57 15.92 9.35 16.10 43.89 36. 15.11 33.09 6.70 6.15 5.03 6.15 of high-quality environments while maintaining diversity. 6.2. Analysis on Verification Design Table 6 compares three verification strategies. LLM-only verification relies on trajectory without being grounded in database state changes, which makes the reward signal unreliable. Consequently, it yields the weakest performance. Replacing the judge with code-only verification improves over LLM-only, but it can be brittle: when environment imperfections occur, rigid checks may incorrectly assign false negatives, as discussed in Sec. 3.3.1. In contrast, our code-augmented strategy combines structured verification signals (from state diffs and rule-based checks) with an advanced reasoning LLM, allowing the judge to use grounded evidence while remaining tolerant to imperfect environment signals. This hybrid design consistently achieves the best results across model scales and benchmarks, indicating that it provides more robust reward for RL training in imperfect synthetic environments. Finally, the extra cost of invoking GPT-5 as the judge is about $1.80 on average per training step (at most 1,024 samples), and the asynchronous setting makes augmented judge introduce negligible latency. We also study the format correctness reward in Appendix B.1. 6.3. Analysis on History-Aware Training Table 7 compares our history-aware training against variant using full history (w/o HL), under aligned and misFigure 4. Scaling of AWM over environment sizes with 4B model. aligned inference settings. When aligned, AWM w/ HL achieves the best results among all settings, indicating benefits from optimizing directly under truncated histories. In contrast, the full-history variant is relatively insensitive to misalignment. When truncated at inference, it even shows slight improvement in τ 2, consistent with truncation suppressing interference from earlier irrelevant turns. We believe history management should be treated as part of policy optimization rather than purely inference-time heuristic. 6.4. Environment Scaling Curve Figure 4 shows how agent scales with the number of training environments. 10 environments lead to severe performance degradation across all benchmarks, likely due to overfitting to the limited environment distribution. Scaling to 100 environments yields substantial gains, and further scaling to 526 environments continues to improve performance. This monotonic improvement highlights the importance of environment diversity for agentic RL. Due to compute constraints, we train on 526 environments rather than the full synthesized set. However, the aforementioned analysis confirms that diversity remains stable as the environment pool expands, suggesting that AWM can support scaling well beyond 1,000 environments with sustained benefits. 7. Conclusion In this paper, we propose Agent World Model, scalable pipeline that synthesizes executable agentic environments by mirroring practical software development. Using AWM, we successfully scale to 1,000 environments with 35,062 tools and 10,000 tasks. These environments are code-driven and backed by SQL databases exposed through unified MCP interface, supporting parallel isolated instances for largescale agentic RL. Experiments on three benchmarks show that agents trained on our synthetic environments generalize well to out-of-distribution domains, outperforming both LLM-simulated training and concurrent synthesis methods. We believe AWMs 1,000 synthesized environments and its scalable pipeline are valuable resources to the community. 8 Infinity Synthetic Environments for Agentic Reinforcement Learning"
        },
        {
            "title": "Impact Statement",
            "content": "This work presents an open-source pipeline for synthesizing executable environments to train tool-use agents. By releasing both the generation pipeline and the synthesized environments, we aim to lower the barrier for the research community to study agentic systems. However, synthetic environments may not fully reflect real-world scenarios, thus we encourage the community to apply safeguards when deploying agents trained on AWM to real-world scenarios."
        },
        {
            "title": "Limitations",
            "content": "Opportunities for Self-Evolving. Our AWM pipeline synthesizes environments through fixed generation process, inherently limiting the models ability to autonomously improve and evolve beyond the initial capabilities. While this approach has shown improvements in our experiments, incorporating self-evolving paradigm where trained agent contributes to synthesizing new environments allows the model to continuously adapt and improve its capabilities. We leave this as promising direction for future work. Synthesis Pipeline Optimization. Current AWM pipeline provides opportunities for further optimization. For example, self-correction currently operates primarily through trial-and-error, addressing runtime errors without deeper semantic validation. Integrating an LLM to proactively detect logical inconsistencies or subtle bugs not caught by runtime checks could enhance the robustness and semantic coherence of generated environments. It is also valuable to take human inspection to further improve the quality of synthesized environments if resources are available. Moreover, synthesizing tasks that span multiple scenarios or environments represents another promising area for extension. Both enhancements are readily compatible with our existing architecture, offering clear pathways for future improvements. Training Scale and Model Coverage. Due to limited computing resources, we train agents on 526 of the 1,000 synthesized environments. We also mainly focus on Qwen3 model family (Yang et al., 2025) across 4B, 8B, and 14B scales. Despite these limitations, the experiments already demonstrate consistent out-of-distribution generalization across three benchmarks, suggesting that even subset of AWM provides strong training signal. Importantly, the core contribution of this paper is the synthesis pipeline itself. The released environments and pipeline are model-agnostic and can benefit the broader community regardless of the specific models used in our experiments."
        },
        {
            "title": "Use of AI Assistants",
            "content": "We acknowledge the use of AI assistants in the writing of this paper, primarily for polishing text and creating figure icons. All generated content is reviewed by the authors, and necessary corrections are made."
        },
        {
            "title": "Acknowledgment",
            "content": "We would like to thank Jeff Rasley for valuable discussions and for helping organize relevant resources."
        },
        {
            "title": "References",
            "content": "Anthropic. Introducing the model context protocol, November 2024. URL https://www.anthropic.com/ news/model-context-protocol. Anthropic. Introducing claude-4.5-sonnet, September 2025a. URL https://www.anthropic.com/ news/claude-sonnet-4-5. Anthropic. Effective context engineering for ai agents, September 2025b. URL https://www.anthropi c.com/engineering/effective-contextengineering-for-ai-agents. Barres, V., Dong, H., Ray, S., Si, X., and Narasimhan, K. τ 2-Bench: Evaluating Conversational Agents in DualControl Environment. arXiv.org, abs/2506.07982, 2025. Cai, S., Fang, R., Wu, J., Li, B., Wang, X., Jiang, Y., Su, L., Zhang, L., Yin, W., Zhang, Z., Feng, F., Xie, P., and Wang, X. Autoforge: Automated environment synthesis for agentic reinforcement learning, 2025. URL https: //arxiv.org/abs/2512.22857. Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., et al. Evaluating large language models trained on code, 2021. URL https: //arxiv.org/abs/2107.03374. Chen, Z., Liu, K., Wang, Q., Zhang, W., Liu, J., Lin, D., Chen, K., and Zhao, F. Agent-flan: Designing data and methods of effective agent tuning for large language models. arXiv preprint arXiv:2403.12881, 2024. Chen, Z., Zhao, Z., Zhang, K., Liu, B., Qi, Q., Wu, Y., Kalluri, T., Cao, S., Xiong, Y., Tong, H., Yao, H., Li, H., Zhu, J., Li, X., Song, D., Li, B., Weston, J., and Huynh, D. Scaling agent learning via experience synthesis, 2025. URL https://arxiv.org/abs/2511.03773. Comanici, G., Bieber, E., Schaekermann, M., Pasupat, I., Sachdeva, N., Dhillon, I., Blistein, M., Ram, O., Zhang, D., Rosen, E., Marris, L., Petulla, S., Gaffney, C., Aharoni, A., et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, 9 Infinity Synthetic Environments for Agentic Reinforcement Learning and next generation agentic capabilities, 2025. URL https://arxiv.org/abs/2507.06261. URL https://openreview.net/forum?id= 94tlGxmqkN. Cuadron, A., Yu, P., Liu, Y., and Gupta, A. Saber: Small actions, big errors - safeguarding mutating steps in llm agents, 2025. URL https://arxiv.org/abs/25 12.07850. DeepSeek-AI, Liu, A., Mei, A., Lin, B., Xue, B., Wang, B., Xu, B., Wu, B., Zhang, B., Lin, C., Dong, C., Lu, C., Zhao, C., Deng, C., Xu, C., Ruan, C., et al. DeepseekV3.2: Pushing the Frontier of Open Large Language Models. https://api-docs.deepseek.com/news/news251201, 2025. Dulac-Arnold, G., Levine, N., Mankowitz, D. J., Li, J., Paduraru, C., Gowal, S., and Hester, T. Challenges of real-world reinforcement learning: definitions, benchmarks and analysis. Mach. Learn., 110(9):24192468, September 2021. ISSN 0885-6125. doi: 10.1007/s10994021-05961-4. URL https://doi.org/10.1007/ s10994-021-05961-4. Esfandiarpoor, R., Suryanarayanan, V., Bach, S. H., Chowdhary, V., and Aue, A. Themcpcompany: Creating generalpurpose agents with task-specific tools, 2025. URL https://arxiv.org/abs/2510.19286. Fan, S., Ding, X., Zhang, L., and Mo, L. Mcptoolbench++: large scale ai agent model context protocol mcp tool use benchmark, 2025. URL https://arxiv.org/ abs/2508.07575. Fang, R., Cai, S., Li, B., Wu, J., Li, G., Yin, W., Wang, X., Wang, X., Su, L., Zhang, Z., Wu, S., Tao, Z., Jiang, Y., Xie, P., Huang, F., and Zhou, J. Towards General Agentic Intelligence via Environment Scaling. arXiv.org, abs/2509.13311, 2025. Feng, J., Zhang, Y., Zhang, C., Lu, Y., Liu, S., and Wang, M. Web world models, 2025. URL https://arxiv. org/abs/2512.23676. Gaffney, K. P., Prammer, M., Brasfield, L., Hipp, D. R., Kennedy, D., and Patel, J. M. Sqlite: past, present, and future. Proc. VLDB Endow., 15(12):35353547, August 2022. ISSN 2150-8097. doi: 10.14778/3554821.3554842. URL https://doi.org/10.14778/3554821 .3554842. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. hattami, A. E., Thakkar, M., Chapados, N., and Pal, C. Webarena verified: Reliable evaluation for web agents. In Workshop on Scaling Environments for Agents, 2025. Hu, J., Wu, X., Shen, W., Liu, J. K., Zhu, Z., Wang, W., Jiang, S., Wang, H., Chen, H., Chen, B., Fang, W., Xianyu, Cao, Y., Xu, H., and Liu, Y. Openrlhf: An easy-touse, scalable and high-performance rlhf framework, 2025. URL https://arxiv.org/abs/2405.11143. Kalai, A. T., Nachum, O., Vempala, S. S., and Zhang, E. Why language models hallucinate, 2025. URL https: //arxiv.org/abs/2509.04664. Li, K., Zhang, Z., Yin, H., Zhang, L., Ou, L., Wu, J., Yin, W., Li, B., Tao, Z., Wang, X., et al. Websailor: Navigating super-human reasoning for web agent. arXiv preprint arXiv:2507.02592, 2025a. Li, Y., Inan, H. A., Yue, X., Chen, W.-N., Wutschitz, L., Kulkarni, J., Poovendran, R., Sim, R., and Rajmohan, S. Simulating environments with reasoning models for agent training, 2025b. URL https://arxiv.org/ abs/2511.01824. Li, Y., Wang, H., Qiu, J., Yin, Z., Zhang, D., Qian, C., Li, Z., Ma, P., Chen, G., Ji, H., and Wang, M. From word to world: Can large language models be implicit text-based world models?, 2025c. URL https://arxiv.org/ abs/2512.18832. Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173, 2024a. doi: 10.1162/tacl 00638. URL https: //aclanthology.org/2024.tacl-1.9/. Liu, W., Huang, X., Zeng, X., Hao, X., Yu, S., Li, D., Wang, S., Gan, W., Liu, Z., Yu, Y., Wang, Z., Wang, Y., Ning, W., Hou, Y., et al. Toolace: Winning the points of llm function calling, 2025. URL https://arxiv.org/ abs/2409.00920. Liu, Z., Hoang, T. Q., Zhang, J., Zhu, M., Lan, T., Kokane, S., Tan, J., Yao, W., Liu, Z., Feng, Y., N, R. R., Yang, L., Savarese, S., Niebles, J. C., Wang, H., Heinecke, S., and Xiong, C. APIGen: Automated PIpeline for generating verifiable and diverse function-calling datasets. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024b. URL https://openreview.net/forum?id= Jfg3vw2bjx. Lu, J., Holleis, T., Zhang, Y., Aumayer, B., Nan, F., Bai, H., Ma, S., Ma, S., Li, M., Yin, G., Wang, Z., and Pang, R. ToolSandbox: stateful, conversational, interactive evaluation benchmark for LLM tool use capabilities. In 10 Infinity Synthetic Environments for Agentic Reinforcement Learning Chiruzzo, L., Ritter, A., and Wang, L. (eds.), Findings of the Association for Computational Linguistics: NAACL 2025, pp. 11601183, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-195-7. doi: 10.18653/v1/2025.findingsnaacl.65. URL https://aclanthology.org/2 025.findings-naacl.65/. Luo, Z., Shen, Z., Yang, W., Zhao, Z., Jwalapuram, P., Saha, A., Sahoo, D., Savarese, S., Xiong, C., and Li, J. Mcp-universe: Benchmarking large language models with real-world model context protocol servers, 2025. URL https://arxiv.org/abs/2508.14704. Mo, G., Zhong, W., Chen, J., Chen, X., Lu, Y., Lin, H., He, B., Han, X., and Sun, L. Livemcpbench: Can agents navigate an ocean of mcp tools?, 2025. URL https: //arxiv.org/abs/2508.01780. Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. OpenAI. Introducing swe-bench verified. https://op enai.com/index/introducing-swe-benchverified/, August 2024. Published August 13, 2024; updated February 24, 2025. OpenAI. Introducing gpt-5, August 2025. URL https:// openai.com/index/introducing-gpt-5/. Patil, S. G., Zhang, T., Wang, X., and Gonzalez, J. E. Gorilla: Large language model connected with massive APIs. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https: //openreview.net/forum?id=tBRNC6YemY. Patil, S. G., Mao, H., Yan, F., Ji, C. C.-J., Suresh, V., Stoica, I., and Gonzalez, J. E. The berkeley function calling leaderboard (BFCL): From tool use to agentic evaluation of large language models. In Forty-second International Conference on Machine Learning, 2025. URL https: //openreview.net/forum?id=2GmDdhBdDk. Prabhakar, A., Liu, Z., Zhu, M., Zhang, J., Awalgaonkar, T., Wang, S., Liu, Z., Chen, H., Hoang, T., Niebles, J. C., et al. Apigen-mt: Agentic pipeline for multi-turn data generation via simulated agent-human interplay. arXiv preprint arXiv:2504.03601, 2025. Qin, Y., Liang, S., Ye, Y., Zhu, K., Yan, L., Lu, Y., Lin, Y., Cong, X., Tang, X., Qian, B., Zhao, S., Hong, L., Tian, R., Xie, R., Zhou, J., Gerstein, M., dahai li, Liu, Z., and Sun, M. ToolLLM: Facilitating large language models to master 16000+ real-world APIs. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum ?id=dHng2O0Jjr. Schick, T., Dwivedi-Yu, J., Dessi, R., Raileanu, R., Lomeli, M., Hambro, E., Zettlemoyer, L., Cancedda, N., and Scialom, T. Toolformer: Language models can teach themselves to use tools. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=Yacm pz84TH. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Sheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Song, X., Chang, H., Dong, G., Zhu, Y., Dou, Z., and Wen, J.-R. Envscaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis. arXiv, 2026. Song, Y., Xiong, W., Zhao, X., Zhu, D., Wu, W., Wang, K., Li, C., Peng, W., and Li, S. AgentBank: Towards generalized LLM agents via fine-tuning on 50000+ interaction trajectories. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 21242141, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findingsemnlp.116. URL https://aclanthology.org /2024.findings-emnlp.116/. Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. ISSN 0925-2312. doi: https://doi.org/10.1016/j.neucom.202 3.127063. URL https://www.sciencedirect. com/science/article/pii/S09252312230 11864. Sullivan, M., Hartmann, M., and Koller, A. ProceduIn ral environment generation for tool-use agents. Christodoulopoulos, C., Chakraborty, T., Rose, C., and Peng, V. (eds.), Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 1854418562, Suzhou, China, November 2025. Association for Computational Linguistics. ISBN 979-8-89176332-6. doi: 10.18653/v1/2025.emnlp-main.936. URL https://aclanthology.org/2025.emnlpmain.936/. Tang, H., Key, D. Y., and Ellis, K. Worldcoder, modelbased LLM agent: Building world models by writing 11 Infinity Synthetic Environments for Agentic Reinforcement Learning code and interacting with the environment. In The Thirtyeighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview .net/forum?id=QGJSXMhVaL. Xu, Y., Lu, D., Shen, Z., Wang, J., Wang, Z., Mao, Y., Xiong, C., and Yu, T. Agenttrek: Agent trajectory synthesis via guiding replay with web tutorials. arXiv preprint arXiv:2412.09605, 2024b. Team, K., Bai, Y., Bao, Y., Chen, G., Chen, J., Chen, N., Chen, R., Chen, Y., Chen, Y., Chen, Y., Chen, Z., Cui, J., Ding, H., Dong, M., Du, A., Du, C., Du, D., Du, Y., Fan, Y., Feng, Y., Fu, K., Gao, B., Gao, H., Gao, P., et al. Kimi k2: Open agentic intelligence, 2025. URL https://arxiv.org/abs/2507.20534. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., Zheng, C., Liu, D., Zhou, F., Huang, F., Hu, F., Ge, H., Wei, H., Lin, H., Tang, J., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Zhou, J., Lin, J., et al. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Wang, R., Todd, G., Xiao, Z., Yuan, X., Cˆote, M.-A., Clark, P., and Jansen, P. Can language models serve as textIn Ku, L.-W., Martins, A., based world simulators? and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 117, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-short.1. URL https: //aclanthology.org/2024.acl-short.1/. Wang, R., Genadi, R. A., Bouardi, B. E., Wang, Y., Koto, F., Liu, Z., Baldwin, T., and Li, H. Agentfly: Extensible and scalable reinforcement learning for lm agents, 2025a. URL https://arxiv.org/abs/2507.14897. Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language models with self-generated instructions, 2023. URL https://arxiv.org/abs/2212.10560. Wang, Z., Wang, K., Wang, Q., Zhang, P., Li, L., Yang, Z., Jin, X., Yu, K., Nguyen, M. N., Liu, L., Gottlieb, E., Lu, Y., Cho, K., Wu, J., Fei-Fei, L., Wang, L., Choi, Y., and Li, M. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning, 2025b. URL https://arxiv.org/abs/2504.20073. Wang, Z., Liang, Y., Zhang, X., Wu, Q., Han, S., Bastos, A., Wang, R., Bansal, C., Peng, B., Gao, J., Rajmohan, S., and Yao, H. Adapting web agents with synthetic supervision, 2026. URL https://arxiv.org/ab s/2511.06101. Xie, J., Xu, D., Zhao, X., and Song, D. Agentsynth: Scalable task generation for generalist computer-use agents, 2025. URL https://arxiv.org/abs/2506.14205. Xu, F. F., Song, Y., Li, B., Tang, Y., Jain, K., Bao, M., Wang, Z. Z., Zhou, X., Guo, Z., Cao, M., Yang, M., Lu, H. Y., Martin, A., Su, Z., Maben, L., Mehta, R., Chi, W., Jang, L., Xie, Y., Zhou, S., and Neubig, G. Theagentcompany: Benchmarking llm agents on consequential real world tasks, 2024a. URL https://arxiv.org/abs/24 12.14161. Yang, J., Jimenez, C. E., Wettig, A., Lieret, K., Yao, S., Narasimhan, K. R., and Press, O. SWE-agent: Agentcomputer interfaces enable automated software engineering. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https: //openreview.net/forum?id=mXpq6ut8J3. Yao, F., Liu, L., Zhang, D., Dong, C., Shang, J., and Gao, J. Your efficient rl framework secretly brings you off-policy rl training, August 2025. URL https://fengyao. notion.site/off-policy-rl. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. React: Synergizing reasoning and acting in language models, 2023. URL https://arxiv.or g/abs/2210.03629. Yao, S., Shinn, N., Razavi, P., and Narasimhan, K. τ -bench: Benchmark for Tool-Agent-User Interaction in RealWorld Domains. arXiv.org, abs/2406.12045, 2024. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., Liu, X., Lin, H., Lin, Z., Ma, B., Sheng, G., Tong, Y., Zhang, C., Zhang, M., Zhang, W., Zhu, H., Zhu, J., Chen, J., Chen, J., Wang, C., Yu, H., Song, Y., Wei, X., Zhou, H., Liu, J., Ma, W.-Y., Zhang, Y.-Q., Yan, L., Qiao, M., Wu, Y., and Wang, M. Dapo: An open-source llm reinforcement learning system at scale, 2025. URL https://arxiv.org/abs/2503.1 4476. Zhang, J., Peng, Y., Kong, F., Yang, C., Wu, Y., Yu, Z., Xiang, J., Ruan, J., Wang, J., Song, M., Liu, H., Tang, X., Liu, B., Wu, C., and Luo, Y. Autoenv: Automated environments for measuring cross-environment agent learning, 2025a. URL https://arxiv.org/abs/2511 .19304. Zhang, Q., Hu, C., Upasani, S., Ma, B., Hong, F., Kamanuru, V., Rainton, J., Wu, C., Ji, M., Li, H., Thakker, U., Zou, J., and Olukotun, K. Agentic context engineering: Evolving contexts for self-improving language models, 2025b. URL https://arxiv.org/abs/2510.04618. 12 Infinity Synthetic Environments for Agentic Reinforcement Learning Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., Zhang, H., Gonzalez, J. E., and Stoica, I. Judging llm-as-a-judge with mt-bench and chatbot arena. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 4659546623. Curran Associates, Inc., 2023. URL https://proceedings.neurip s.cc/paper_files/paper/2023/file/91f 18a1287b398d378ef22505bf41832-PaperDatasets_and_Benchmarks.pdf. Infinity Synthetic Environments for Agentic Reinforcement Learning A. Implementation Details A.1. Scenario Generation We adopt Self-Instruct style expansion (Wang et al., 2023) to scale from 100 seed websites to 1,000 diverse scenarios. The seed set is drawn from popular domain names1, and the LLM generates new candidates conditioned on few-shot examples that emphasize stateful, database-backed interactions. To maintain diversity, we apply two complementary filters: (1) an LLM classifier that scores each candidate on suitability for CRUD operations (create, read, update, delete), rejecting content-centric or read-only scenarios (e.g., news), and (2) embedding-based deduplication using cosine similarity with threshold of 0.85, ensuring newly generated scenarios are sufficiently distinct from the existing pool. We also enforce category caps to prevent over-representation of dominant types such as e-commerce. The prompts used for generation and classification are provided in Figures 10 and 11. Table 9 shows random sample of 100 generated scenarios. We also show the distribution of the synthesized scenarios in Figure 6 and the wordcloud of the scenario descriptions in Figure 7. The distribution shows that the synthesized scenarios span diverse domains without collapsing to few dominant types. A.2. Task Generation For each scenario, we prompt the LLM to generate = 10 diverse user tasks that serve as functional requirements for downstream synthesis. This modest count keeps the database schema and toolset implementation tractable for LLMs. The prompt enforces two constraints: (1) tasks must be API-solvable without UI dependencies such as clicking or page navigation, and (2) tasks assume post-authentication context to unlock deeper functionalities. Each task is required to be specific and self-contained, including concrete parameters (e.g., product IDs, user names) so that it can be executed without additional clarification. This requirement-driven design ensures that the subsequently generated database schema and toolset are aligned with actual user needs rather than being over-specified or incomplete. The prompt template is shown in Figure 12. And the generated tasks are shown in Table 11. A.3. Environment Synthesis This section details the synthesis of the three core POMDP components: state space (database), action and observation spaces (interface), and reward function (verification). Database Schema Generation. Given the scenario description and task set TEi , the LLM infers the minimal relational schema required to make all tasks feasible. The output is set of SQLite DDL statements defining tables, columns, types, primary keys, foreign keys, and indexes. We instruct the LLM to reason about entity relationships implied by the tasks (e.g., cancel order task implies an Orders table with status column) and to avoid authentication-related fields. Based on the introduced self-correction mechanism, we attempt to run the generated DDL statements in an isolated environment and test the functionality. If any DDL statement fails execution, we capture the error message, summarize it via separate LLM call, and append the summary to the prompt for regeneration. This feedback loop repeats with an error threshold of 10%: if fewer than 10% of tables fail, we accept the schema. The prompt is shown in Figure 13. And we visualize part of the SQLite database schema with sample data in Figure 8. Sample Data Synthesis. An empty schema is insufficient for task execution, since many tasks require querying or updating existing records. We synthesize sample data by prompting the LLM to analyze task preconditions and generate INSERT statements that satisfy them. For example, if task requires updating product inventory, the synthesized data must include at least one product with non-zero stock level. The LLM outputs JSON structure containing table names and corresponding INSERT statements, which are executed in dependency order respecting foreign key constraints. We apply the same error-feedback loop as schema generation, with an error threshold of 10% for acceptable insert failures. The prompt is shown in Figures 14 and 15. Interface Specification Design. Before generating implementation code, we first synthesize an interface specification that defines the toolset schema. This two-stage approach (schema then code) reduces hallucination in long code generation, since pilot experiments show that direct code generation for environments with 30+ tools often produces inconsistent interfaces. Given the task set and database schema, the LLM infers the minimal set of endpoints required to make every task executable. Each endpoint specification includes: name, method, path, summary, typed input parameters, and response schema. The specification also serves as documentation for agents at inference time, shown in Table 10. The prompt is 1https://www.similarweb.com/top-websites/ 14 Infinity Synthetic Environments for Agentic Reinforcement Learning shown in Figures 16 and 17, and an example of the synthesized MCP toolset is shown in Figure 25. Environment Implementation. With the API specification and database schema, we generate complete Python file implementing an MCP server. The generated code includes: SQLAlchemy ORM models mirroring the database schema, Pydantic request/response models, and FastAPI endpoint handlers that perform database operations. Each environment averages about 2,000 lines of code and exposes 35 tools on average. After generation, we validate the environment by launching and checking that the MCP server starts successfully and responds to health checks and list tools calls. Failed environments enter the self-correction loop to fix the errors. The prompt is shown in Figures 18, 19, and 20. Verification Code Synthesis. For each task, we generate Python verification function that compares database states before and after agent execution. The function takes two database paths (initial and final) as input, executes SQL queries to extract task-relevant information, and returns structured dictionary containing: changed records, expected outcomes, and diagnostic signals. We also generate success and failure criteria in natural language that guide the LLM-as-a-Judge in interpreting the verification results. During RL training, this code-augmented verification provides grounded evidence to the judge, reducing hallucination in reward assignment. The prompt for generating the verification code is shown in Table 21, and the example of the synthesized verification code is shown in Figure 26 and 27. Self-Correction Mechanism. All synthesis stages share common error-recovery pattern. When generated code fails execution, we capture the full error traceback and invoke separate LLM call to produce concise error summary (typically 200-500 tokens) that identifies the root cause and suggests fixes. This summary is appended to the original prompt for the next generation attempt. We limit retries to 5 iterations per component. If an error threshold (10% for schema/data, 0% for environment startup) is exceeded after all retries, we select the best attempt based on error rate and proceed. This approach achieves over 85% first-attempt success rate across all stages, with an average of 1.13 correction iterations for failed cases. A.4. Tool Calling Format and Validation We adopt the Qwen3 tool-calling format (Yang et al., 2025), which wraps each tool invocation in XML tags: <tool call>{name: . . . , arguments: . . . }</tool call>. Rather than exposing all environment-specific tools directly to the agent, we design two-level abstraction that decouples the agent from environment details. Two-Level Tool Abstraction The agent interacts with MCP environments through exactly two meta-tools: (1) list tools, which queries the MCP server to retrieve all available tools in the current environment along with their metadata (input/output schemas, descriptions), and (2) call tool, which invokes an environment-specific tool by name with the required arguments passed as JSON string. This design enables the agent to dynamically discover and invoke different toolsets across environments without hardcoding any tool information. The complete system prompt is shown in Figure 9. Format Validation Rules During RL training, we enforce format constraints to ensure well-formed tool calls and prevent hallucinated or malformed interactions. We apply rule-based validation at each step of the trajectory, classifying it as either valid, format error, or an environment/server error. The validation checks the following conditions: (1) Reasoning format: All assistant messages must contain non-empty reasoning within <think>...</think> tags. (2) Tool name validity: The agent must not call hallucinated tools (tools not returned by list tools) or use invalid tool names. (3) Argument validity: Tool arguments must be well-formed JSON that conforms to the tool schema. (4) Protocol adherence: The agent must call list tools exactly once, and it must be the first tool call in the trajectory. (5) Interaction consistency: If the agent produces multiple interaction turns, it must make at least one successful tool call beyond the initial list tools. (6) Server response: Each tool call must receive non-error response from the MCP server. For example, if the server returns an error message indicating timeout or internal failure, we classify it as an environment error. For each step t, if any of the 1-5 conditions are violated, we classify it as format error, assigning negative reward rt = 1. If condition 6 is violated, we classify it as an environment error with reward rt = 0 following the reward shapes in Sec. 4.1. Early Termination To improve training efficiency, we implement early stopping for unrecoverable errors. When the validator detects format error or server error during rollout, we terminate the trajectory immediately rather than continuing to generate additional steps. This saves computational resources and prevents the agent from receiving reward signals for fundamentally broken trajectories. A.5. Training and Evaluation Details Training Hyperparameters. Table 8 summarizes the hyperparameters used for GRPO training (Shao et al., 2024). According to the common agentic RL training settings (Li et al., 2025b; Wang et al., 2025b), we set the KL coefficient to 15 Infinity Synthetic Environments for Agentic Reinforcement Learning 0.001. Beyond standard GRPO settings, we use higher clip ratio of 0.28, following the recommendation in DAPO (Yu et al., 2025) to allow more exploration for the agent. Also, we use sequence-level importance sampling to mitigate the distribution shift issue between the rollout engine and the model training engine (Yao et al., 2025). Environment Management. We implement multi-turn RL training on top of AgentFly (Wang et al., 2025a) and verl (Sheng et al., 2024). Each training step launches 1,024 isolated environment instances in parallel, with each instance running as an independent MCP server backed by its own SQLite database copy. This isolation ensures that concurrent rollouts do not interfere with each others state transitions. Environment instances are reset by restoring the database to its initial state after each rollout completes. Environment startup (spawning MCP servers, copying databases) is bottleneck in online RL, as it blocks rollout collection. To overlap environment preparation with policy training, we further implement pre-fetching mechanism: while the current batch undergoes gradient updates, background thread pre-configures environments for the next batch. This significantly reduces per-step wall-clock time compared to sequential environment startup. Sample Splitting for History-Aware Training. As described in Section 4.2, we align training with inference by applying history truncation during optimization. We use simple history context management, specifically sliding window truncation, to study the mismatch issue between training and inference. Specifically, given completed rollout with assistant turns, we split it into separate training samples. For sample t, the input consists of the system prompt, initial user message, the first assistant-tool exchange (which contains the list tools call), and the = 3 most recent turns preceding turn t. The loss is computed only on the tokens of turn t, while all preceding context tokens have their loss mask set to zero. This sample splitting approach ensures each training sample mirrors the truncated context the agent will see at inference time. Although this increases the number of forward passes per rollout by factor of , it eliminates the distribution shift that would otherwise occur when training on full histories but deploying with truncated contexts. More complex history context management is possible, but it is beyond the scope of this paper. Reward Computation. At the end of each rollout, we invoke the code-augmented LLM-as-a-Judge to determine task completion status. The verification code is executed on the final database state, comparing it against the initial state, and its structured output (changed records, success criteria) is provided to GPT-5 (OpenAI, 2025) along with the agent trajectory. The judge returns one of four classifications: Completed, Partially Completed, Agent Error, or Environment Error. We map these to rewards as specified in Sec. 4.1. To reduce latency, verification calls are batched and executed asynchronously while the next rollout batch is being collected, adding negligible overhead to the training loop. Evaluation Protocol. We evaluate trained agents on three benchmarks that differ substantially from our training distribution. key challenge is that each benchmark uses different tool-calling format: τ 2-bench (Barres et al., 2025; Cuadron et al., 2025) expects direct tool names (e.g., get user details), BFCLv3 (Patil et al., 2025) uses function-calling syntax, and MCP-Universe (Luo et al., 2025) uses the MCP protocol. To bridge this gap, we implement format converters that translate between our unified call tool abstraction and each benchmarks native format. For τ 2-bench, we wrap each tool call as call tool(tool name=\"mcp tool {name}\", arguments={...}) and unwrap responses accordingly. For BFCLv3, we aggregate multi-turn trajectories into the expected single-turn or parallel function call format, skipping list tools meta-calls. This ensures evaluation measures the agents actual task-solving ability rather than format compliance. Decoding and Context Configuration. During evaluation, we use temperature 0.6 with top-k=20 and top-p=0.95, following the recommended decoding settings in Qwen3 (Yang et al., 2025). We extend the context window to 131,072 tokens via RoPE scaling (Su et al., 2024) for all evaluations, as some benchmark tasks (especially BFCLv3 multi-turn categories) require processing lengthy interaction histories. When processing long context tasks, the history limit is loosened to = 10 turns during evaluation, allowing agents to leverage more context information. B. Analysis B.1. Analysis on Step-level Format Correctness Reward We study the impact of the step-level format correctness reward in Figure 5. With this reward, agents quickly learn to follow the tool interface contract, and the format error ratio rapidly converges to low level. It also improves training efficiency by reducing the average rollout time by about 27%. In contrast, without this reward, the format error ratio remains above 20% even after 50 optimization steps, leading to frequent invalid actions and noisier learning signals. This degradation further translates into lower task completion rate, which saturates below 40%. Overall, the step-level format correctness reward improves both agent performance and training efficiency. 16 Infinity Synthetic Environments for Agentic Reinforcement Learning Table 8. Hyperparameters for GRPO training. Hyperparameter Value GRPO Learning rate Batch size Mini-batch size Rollouts per task Instances per step Max optimization steps KL coefficient Entropy coefficient Clip ratio (high) Rollout 7 107 64 16 16 1,024 96 0.001 0.0 0.28 Temperature Max response length Max model context 1.0 2,048 32,000 Agent Max interaction turn History window size 20 3 Figure 5. Format error ratio comparison of AWM. w/o Format means disabling the step-level format correctness reward. B.2. Case Study for Verification We provide three representative cases in Figures 28, 29, and 30 to illustrate why our code-augmented LLM-as-a-Judge is more robust than either rigid code-only verification or LLM-only judging from agent trajectories. For the first case  (Fig. 28)  , the task is clean, database-grounded query: the agent calls the correct tool to retrieve the full bid history of the item and summarize the findings; the verifier can deterministically confirm these statistics from the underlying state/returned records, and the judge aligns, illustrating the ideal regime where structured verification evidence is decisive. For the second case  (Fig. 29)  , the environment exhibits an imperfection when the agent tries to create routine; strict code-only verifier that expects state delta would incorrectly flag failure because the initial and final snapshots appear identical, yet the trajectory shows an idempotent success path, and the code-augmented judge uses this context to correctly mark Completed, reducing false negatives under transient tool/infrastructure issues. For the last case  (Fig. 30)  , an API/tool calling error misleads the agent into creating duplicate event and then adding the session under the wrong event ID; tool calls succeed locally so judge without verifier grounding could be fooled into marking Completed, but the verifier reveals the real target event remains unchanged, enabling the code-augmented judge to identify wrong operation and correctly reject the spurious success claim. Overall, the key theme is that synthetic interactive environments are imperfect: transient tool failures, idempotent tasks, and ambiguous tool calling behaviors can all break the assumptions of rigid verification. Our design mitigates this by letting the verifier extract structured evidence from database snapshots and rule-based checks, while the judge reasons over trajectory context to resolve ambiguity and reduce both false negatives and false positives. 17 Infinity Synthetic Environments for Agentic Reinforcement Learning Figure 6. Distribution of synthesized scenarios of AWM. Figure 7. Wordcloud of the scenario descriptions of AWM. Figure 8. Visualization of part of the SQLite database schema of Spotify environment. Infinity Synthetic Environments for Agentic Reinforcement Learning Table 9. random sample of 100 generated scenarios from AWM. The collection includes both synthetic applications and real-world services that represent common enterprise and consumer tool-use patterns. Scenario Name Scenario Name Scenario Name ArenaPlay Competitive Gaming Hub AutoGrid Dealer Inventory Manager BlockBridge HOA & Condo Community Manager Charles Schwab ClassBench Music School Scheduling & Attendance ClinicPaws Veterinary Practice Suite CraftBench Custom Manufacturing Job Tracker DeviceFleet Cloud Appliance Manager Eventbrite Fleetio FlowMesh Workflow Automation Hub FormPilot Enterprise Surveys & Workflows GitHub Gmail GuildForge Esports Hub HireBoard Remote Jobs Marketplace HostEase Property Bookings KitchenGrid Restaurant Ops Hub LootPlaza Virtual Goods Marketplace Microsoft Account (Live.com) NeighborLink Mutual Aid Exchange PantryPicks Grocery & Recipe Wishlist PayStream Merchant Payment Hub PrepLine Kitchen Display & Routing RegShield Policy Compliance & Attestation Center Salesforce SitBuddy Pet Sitting & Home Visit Marketplace StackList Personal Collections Manager SubScript Subscription & Billing Manager TailCart Pet Supplies Marketplace Toast POS VenueLane Event Space Booker VolunteerShift Scheduler WishLoom Multi-Store Gift Registry AT&T Basecamp CampusEnroll Admissions Registration Suite CineRealm Digital Cinema ClassTrack Micro-LMS ClubNest Membership Hub DeskQueue IT Support Discord EventHarbor Conference & Session Manager FleetPath Manager FlowPay Subscription Billing Engine FundHarbor Retail Investing & Portfolio Tracker GiveStream Community Grants Hub GrantPath Scholarship Application Portal HelpLane Support Desk HomeFlow Smart Device Console InsightLoop Feedback & NPS Tracker LeaseLink Corporate Workspace MacroMate Corporate Meal Program Portal Mindbody Notion PawSitter Network & Booking Hub PetCrate Supply Subscriptions QuickBite Campus Food Ordering Restaurant Backoffice Pro ShelfStack Personal Media Library SkinMart Virtual Goods Marketplace StockCrate Warehouse Inventory Cloud SubStream Creator Membership Hub TalentLoop Recruiting CRM TutorLane Subject Sessions VetConnect Telehealth Hub WalkLoop Pet Sitting Network AutoCareLane Service Scheduler Bill.com Canva ClaimWise Property & Auto Insurance Claims Desk ClinicConnect Multi-Specialty Appointment Center CodeMentor Classroom LMS DevForum Hub (Technical Q&A Forum) Duo Security (Cisco Duo) FleetAxis Commercial Fleet Command FlexDesk Workspace Reservations FlowTrack Team Workflow Hub GhostKitchenOS Virtual Brand Manager GiveWell-style Donation Portal GreenLedger Utility & Sustainability Dashboard HireBench Coding Graduate Pipeline HomeGrid Smart Panel Jira ListNest Shared Collections & Bookmarks Microsoft 365 (Office) NeighborHands Volunteer Network OpenTable PayBridge Merchant Payments Gateway PinHarbor Visual Bookmarks QuickLift Auto Service Booking Reverb SignSure Enterprise E-Signature & Policy Acknowledgment SlotBand Event & Vendor Scheduler Stripe Dashboard SyncRoom Remote Collaboration Suite TeleVisit360 Virtual Care VendorVault Procurement & Vendor Portal VolunteerGrid Scheduling & Shift Manager WishCart Social Shopping Lists Table 10. Example of interface schema snippet with rich annotations and explicit database constraints. \"operation id\": \"get_product_by_id\", \"request params\": {\"product_id\": {\"type\": \"integer\", \"required\": true}}, \"response\": { \"product\": {\"id\": {\"type\": \"integer\"}, \"title\": {\"type\": \"string\"}, ....}, \"aggregates\": {\"average_rating\": {\"type\": \"float\"}, ....}, ....}, \"required tables\": [\"products\", \"product_aggregates\", \"product_offers\"], \"required fields\": { \"products\": [\"id\", \"title\", \"category_id\", ....], ....} 19 Infinity Synthetic Environments for Agentic Reinforcement Learning # MCP Tools You are at MCP environment. You need to call MCP tools to assist with the user query. At each step, you can only call one function. You have already logged in, and your user id is 1 if required for the MCP tool. list tools You are provided with TWO functions within <tools></tools> XML tags: <tools> 1. - Description: List all available MCP tools for the current environment to help you finish the user task. - Arguments: None - Output: list of MCP environment-specific tools and their descriptions call tool 2. - Description: Call MCP environment-specific tool - Arguments: - tool name: str, required, the tool name in the list tools output - arguments: str, required, the arguments for calling <tool name>. pass valid JSON string without any markdown fences or additional commentary. This JSON str will be parsed by the tool and executed. JSON str if no arguments are required by <tool name>. - Output: </tools> The result of the <tool name> tool call"
        },
        {
            "title": "You must",
            "content": "You should always call list tools function first to get the available tools, and should only call it once. You should always directly output the answer or summary at the final step instead of calling any function. For each function call, return json object with function name and arguments within <tool call></tool call> XML tags: <tool call> {\"name\": </tool call> <function-name>, \"arguments\": <args-json-object>} Example Function Call #1: <tool call> {\"name\": </tool call> \"list tools\", \"arguments\": null} Example Function Call #2: <tool call> {\"name\": \"{\"city\": </tool call> \"call tool\", \"arguments\": \"Beijing\"}\"}} {\"tool name\": \"get weather\", \"arguments\": Figure 9. System prompt for agent in MCP environments. 20 Infinity Synthetic Environments for Agentic Reinforcement Learning You are an expert at identifying websites, apps, and digital platforms that are HIGHLY SUITABLE for API environment simulation and database-driven interactions. ## KEY PRINCIPLE: Can the data be SYNTHESIZED? We need platforms where data can be FAKED but still be REALISTIC and USEFUL. temperatures, prices, quantities, ratings, coordinates, stock prices users, products, orders, bookings, employees, devices, sensors ### Data that CAN be synthesized: - Numbers: - Entities: - Status values: - Short text: - Timestamps: - Geographic: order status, flight status, device state names, titles, addresses, categories, descriptions dates, schedules, deadlines, historical records cities, airports, coordinates (static data) ### Data that CANNOT be synthesized meaningfully: - Long articles: - Media: - AI inference: - Real search: actual video/audio content (not metadata) search engine results with ranking chatbot responses, ML-based recommendations news content, blog posts, encyclopedia entries CRUD on orders, reviews (products, orders, cart, reviews) CRUD on reservations (listings, bookings, availability) Query weather data (locations, forecasts, alerts, history) CRUD on tasks (tasks, boards, assignments) CRUD on transactions (accounts, transactions, transfers) Search & book flights (flights, airports, bookings, prices) Trade & portfolio (stocks, trades, portfolio, prices) ## HIGH Suitability Examples (GENERATE THESE): E-commerce: Task Management: Banking/Fintech: Booking/Reservation: Weather Services: Flight/Travel: Stock Trading: IoT/Smart Home: Fitness Tracking: Healthcare: HR/Payroll: CRM/Sales: Inventory: Logistics: Restaurant/Food: Property/Real Estate: Education/LMS: Courses & grades (courses, enrollments, assignments, grades) Event Management: Manage leads (leads, deals, contacts, activities) Manage stock (products, inventory, warehouses) Track shipments (shipments, packages, routes, status) Orders & menu (menu items, orders, reservations) Control devices (devices, sensors, readings, schedules) Log workouts (workouts, exercises, metrics, goals) Manage appointments (patients, appointments, records, prescriptions) Manage employees (employees, timesheets, payroll) Events & tickets (events, tickets, attendees, venues) Listings & viewings (properties, listings, viewings, offers) ## LOW Suitability (AVOID THESE): News sites (BBC, CNN): Article CONTENT is the product (cannot synthesize meaningful articles) Wikipedia/Encyclopedia: Article CONTENT is the product (cannot fake knowledge) Search Engines: AI Assistants (ChatGPT): Need real AI inference (cannot fake AI responses) Translation (Google Translate): Content platforms focus: If CONTENT is core value (metadata is not enough) Need real search index + ranking (cannot simulate relevance) Need real NLP models (cannot fake translations) ## MEDIUM Suitability (Focus on CRUD aspects): YouTube: videos, recommendations Spotify: music discovery Reddit: content CAN simulate playlists, subs, comments, channel mgmt; CANNOT simulate actual CAN simulate playlists, library, following; CANNOT simulate actual audio, CAN simulate posts (short), comments, votes; CANNOT simulate long-form quality ## Categories to Cover (for diversity): E-commerce, Booking/Reservation, Task/Project Management, Finance/Banking, Weather/Environmental, Flight/Travel, Stock/Investment, IoT/Smart Home, Fitness/Health Tracking, Healthcare/Medical, HR/Recruiting, CRM/Sales, Inventory/Warehouse, Logistics/Shipping, Restaurant/Food, Real Estate, Education/LMS, Event/Ticketing, Legal/Contracts, Subscription Management, Customer Support, Forms/Surveys, Analytics Dashboards, Fleet Management, Utility Management (electricity, water), Insurance, Pet Services, Automotive ## Your Task: Generate NEW platforms that are HIGHLY SUITABLE where data can be realistically SYNTHESIZED and users perform meaningful CRUD operations. Figure 10. System prompt for scenario generation (part 1 of 2). 21 Infinity Synthetic Environments for Agentic Reinforcement Learning Here are {num examples} examples of suitable website/app scenarios: {examples} --- Now generate {num to generate} NEW and DIVERSE website/app/platform scenarios that are DIFFERENT from all examples above. Each scenario must be suitable for API environment synthesis (CRUD operations, Requirements: 1. database interactions) Cover DIFFERENT interaction patterns and industries than the examples 2. 3. Include mix of: and specialized tools 4. are available, what workflows exist 5. services 6. The description should emphasize: Each description should be 150-300 words, focusing on actionable features Avoid content-heavy sites, real-time data sites, search engines, AI inference well-known platforms, niche services, B2B tools, mobile apps, what entities users can manage, what operations Output format (JSON array, no markdown fences): [ {{\"name\": focusing on CRUD operations, entities, and workflows...\"}}, ... ] \"Platform Name\", \"url\": \"example.com\", \"description\": \"Description Generate exactly {num to generate} new scenarios: Figure 11. User prompt template for scenario generation (part 2 of 2). 22 Infinity Synthetic Environments for Agentic Reinforcement Learning System Prompt: You are an expert in web automation and user task analysis. generate realistic, diverse user tasks for scenarios."
        },
        {
            "title": "Your job is to",
            "content": "User Prompt: Generate {num tasks} realistic and diverse user tasks for the following scenario. scenario can be website (e.g., Amazon), mobile app (e.g., Uber), Note: or collection of tools and services that provide related functionality (e.g., Google Workspace combining Gmail, Drive, Calendar). is part of an automatic environment synthesis pipeline where we generate executable tool-use environments."
        },
        {
            "title": "This",
            "content": "{scenario name} Scenario: Description: {scenario description} Tasks should be specific and actionable Cover different user scenarios (beginner to advanced) Include both common and less common use cases Tasks should be practical and realistic Each task should be single sentence including all the necessary Requirements: 1. 2. 3. 4. 5. information to complete. For example, if the task is to post tweet, the task should include the tweet content. the weather, the task should include specific location. 6. If the scenario typically requires authentication, EXCLUDE any authentication, login, logout, or user registration tasks - assume the user is already logged in 7. from the perspective of the current authenticated user 8. 9. completion. Avoid generating tasks that require direct user interaction such as download file, open page, etc. Return ONLY JSON array of tasks, no additional text or annotations The scenario is simplified version providing API endpoints for task If the scenario typically requires authentication, all tasks should be"
        },
        {
            "title": "If the task is querying",
            "content": "Examples: - For scenario Amazon, task could be Search for laptop and add the cheapest result to the cart - For scenario Reddit, task could be Get the number of posts in the r/python subreddit - For scenario Expedia, task could be Book flight from New York to London on October 1st - For scenario Twitter/X, task could be Post tweet with photo, add alt text Sunset over the city, include the hashtag #Photography, and mention @Adobe. - For scenario LinkedIn, task could be Update my profile headline to Senior Data Analyst SQL, Python, Tableau and rewrite the About section to concise 3-paragraph summary highlighting business impact. - For scenario Facebook, task could be Share your latest post to your friend list. - For scenario Google Maps, task could be Get 5 most popular restaurants in San Francisco. Output format: { tasks: [ Task 1 description, Task 2 description, ... Task {num tasks} description ] } Figure 12. Prompts for task generation given scenario description. 23 Infinity Synthetic Environments for Agentic Reinforcement Learning System Prompt: You are an expert database architect specializing in SQLite schema design. Your job is to create complete database schemas that can fully support the given user intentions. User Prompt: Design complete SQLite database schema to support the following user intentions for simplified version of {scenario name}. scenario can be website, mobile app, or collection of tools and Note: services. This is part of an automatic environment synthesis pipeline where we generate executable tool-use environments. User Intentions ({num tasks} tasks): {user intentions} Create ALL necessary tables to cover all the given user intentions Include proper primary keys, foreign keys, indexes, and constraints Use appropriate data types for SQLite (TEXT, INTEGER, REAL, BLOB) Add timestamps (created at, updated at) where appropriate Do not include any example records in the DDL statements Only create tables and fields that are necessary to cover all the given Requirements: 1. 2. 3. 4. 5. 6. user intentions 7. session - assume authentication is handled externally 8. username, email, profile data) 9. 10. If users table is needed, only include essential profile fields (id, EXCLUDE authentication-related fields like password hash, salt, token, All operations will be performed as the authenticated user with user id="
        },
        {
            "title": "Return ONLY valid JSON with DDL statements",
            "content": "CREATE TABLE users (id INTEGER PRIMARY KEY, username TEXT UNIQUE Output format (without any comments): { tables: [ { name: users, ddl: NOT NULL, email TEXT UNIQUE NOT NULL, full name TEXT, created at DATETIME DEFAULT CURRENT TIMESTAMP);, indexes: [ CREATE INDEX idx users email ON users(email); ] } ] } Figure 13. Prompts for database schema generation given user tasks. 24 Infinity Synthetic Environments for Agentic Reinforcement Learning System Prompt: You are an expert database engineer and data generator specializing in creating comprehensive test datasets for AI agent task execution. Your job is to generate realistic, diverse sample data that ensures agents can successfully complete ALL given user tasks through API calls. You must: - Strictly follow the provided database schema - Never invent or guess table or column names that are not present in the schema - Follow the exact output JSON format specified in the user prompt - Output ONLY the requested JSON, with no extra explanations or surrounding text User Prompt (Part 1/2): Generate comprehensive sample data for simplified {scenario name} that FULLY SUPPORTS agent execution of ALL the given user tasks. scenario can be website, mobile app, or collection of tools and Note: services. This is part of an automatic environment synthesis pipeline where we generate executable tool-use environments. User Tasks to Support: {tasks list} Existing Database Schema: {database schema} Never invent, shorten, pluralize, or rename columns."
        },
        {
            "title": "You may only generate INSERT statements for tables that appear in the",
            "content": "The parentheses MUST contain ONLY valid column names from the schema, When writing INSERT statements, always explicitly list the column names"
        },
        {
            "title": "Carefully read the CREATE TABLE statement for each table to understand all",
            "content": "CRITICAL: Schema Compliance Rules (MUST FOLLOW): 1. columns, defaults, and autoincrement behavior. 2. you are inserting into in the parentheses after the table name. no values or literals. 3. defined. 4. Existing Database Schema section. present in the schema, DO NOT use it. 5. number of values provided in the VALUES(...) 6. Double-check each INSERT statement: they MUST be equal. 7. listed column has exactly one corresponding value in the VALUES(...) clause. 8. their CREATE TABLE (or CREATE VIRTUAL TABLE) statements. Do NOT add foreign key columns such as * id unless they explicitly exist in the schema. For special/virtual/FTS/config tables, use exactly the columns defined in For every INSERT statement, the number of listed columns MUST equal the For tables with many columns (10+), be extra careful to ensure every count listed columns, count values,"
        },
        {
            "title": "If a table name is not",
            "content": "clause. Data Generation Strategy: For EACH task listed above, analyze what data is required and ensure: 1. 2. 3. 4. All entities referenced in the task exist in the database All relationships needed to complete the task are properly established Query results will return meaningful, non-empty data Edge cases and variations are covered for robust testing Figure 14. Prompts for sample data generation (part 1 of 2). 25 Infinity Synthetic Environments for Agentic Reinforcement Learning User Prompt (Part 2/2): Follow SQLite syntax for INSERT statements. respect foreign key relationships and constraints. DO NOT include authentication fields like password hash, token, session, even if You must strictly follow the provided database schema - use EXACT table names, Do NOT Hard Requirements: 1. column names, and valid column counts. invent new tables or columns. 2. Generate INSERT statements for ALL tables necessary to support the tasks, but ONLY for tables that exist in the schema. 3. Ensure data integrity: 4. 5. Insert data in the correct order to satisfy foreign key constraints. 6. If users table exists, ALWAYS create user with id=1 as the first entry - this is the current authenticated user. 7. they appear in the schema. 8. 9. reasoning field explaining how that tables insert statements support the given user tasks and comply with the database schema. 10. (...), or wrap the JSON in backticks or code fences. 11. ending with single semicolon, and MUST NOT contain multiple statements or any JSON/text before or after the SQL. 12. objects or nested JSON structures. For user-owned data (orders, posts, etc.), create MOST data for user id=1. For each table in the output, provide brief reasoning (<= 100 words) in its Return ONLY valid JSON, no additional text. Do NOT include comments, ellipsis Each element in insert statements MUST be single SQL INSERT statement string, All items in insert statements MUST be plain strings (SQL statements only), not INSERT INTO table name (col1, col2, ..., colN) VALUES (val1, val2, ..., INSERT Statement Format (MANDATORY): - Always explicitly list the column names you are inserting into in each INSERT statement - Format: valN); - The number of listed columns MUST equal the number of values - The parentheses after the table name MUST contain ONLY column names, never literal values - For NULL values, use NULL (not empty string) - For boolean columns, use 0 or 1 - For optional columns with defaults or autoincrement primary keys, either include them with value or omit both the column and the corresponding value from the INSERT create diverse data that matches AND does not match create multiple records (at least 5-10) to return meaningful ensure all referenced entities (users, categories, etc.) For LIST/GET tasks: For CREATE/POST tasks: Agent Task Coverage Requirements: 1. For SEARCH/FILTER tasks: search criteria 2. results 3. exist 4. 5. 6. meaningful statistics 7. For RELATIONSHIP tasks: For UPDATE/PATCH tasks: For DELETE tasks: For AGGREGATION tasks (count, sum, avg): create existing records that can be modified create expendable records that can be safely deleted create sufficient data volume for ensure all foreign key references are valid and queryable Use realistic values (real product names, proper email formats, realistic prices, Create temporal diversity (records from different dates/times) Include status variations (active/inactive, pending/completed, etc.) Cover numeric ranges (low/medium/high prices, quantities, ratings) Include text variations (short/long descriptions, different categories) For timestamps, use ISO 8601 format: YYYY-MM-DD HH:MM:SS or datetime(now, -N Data Quality Requirements: 1. etc.) 2. 3. 4. 5. 6. days) 7. Create enough data volume to support robust testing Output format: explanation..., insert statements: {tables: [{table name: users, reasoning: Brief [INSERT INTO ..., ...]}, ...]} Figure 15. Prompts for sample data generation (part 2 of 2). 26 Infinity Synthetic Environments for Agentic Reinforcement Learning System Prompt: You are an expert API designer and backend architect. machine-readable RESTful API documentation that can support all the given user tasks based on the existing database schema. Your job is to design User Prompt (Part 1/2): Design complete, agent-friendly interface specification to support ALL the following tasks for simplified {scenario name} based on the existing database schema. detailed implementation of the interface layer. The generated specification will be used to guide the scenario can be website, mobile app, or collection of tools and services. Note: This is part of an automatic environment synthesis pipeline where we generate executable tool-use environments. User Tasks: {tasks list} Existing SQLite Database Schema: {database schema} Design ATOMIC API endpoints - each endpoint should perform ONE specific, The API spec MUST be compatible with FastAPI, SQLAlchemy ORM, and Pydantic v2 Maximize REUSABILITY - create base CRUD operations that can be composed together to The API MUST fully follow the database schema - explicitly use the exact table Use RESTful conventions (GET, POST, PUT, DELETE, PATCH) with proper resource paths Group related endpoints logically by resource type Prefer multiple small, composable endpoints over fewer complex endpoints For complex tasks, design individual atomic operations that can be chained together Ensure each endpoint maps directly to one or more tables in the provided database Hard Requirements: 1. well-defined operation 2. 3. fulfill the given tasks 4. names, column names, and relationships from the schema 5. 6. 7. 8. 9. schema 10. refresh) - assume user is already authenticated 11. 12. user id as parameter 13. Return ONLY valid JSON, no additional text DO NOT include any authentication endpoints (login, logout, register, token All operations implicitly use the current authenticated user with user id=1 For user-specific data, always filter by user id=1 automatically - do not require one-line purpose (<= 80 chars) - clear and actionable for AI agents SINGLE LINE (<= 200 chars; no line breaks) - explains what the endpoint unique, snake case identifier - agents use this to identify and call Agent-Friendly Requirements (REQUIRED for every endpoint): - summary: - description: does and when to use it - operation id: endpoints programmatically - tags: discover related endpoints - request params: (query/path/body), required flag, description, and example - response: enables agents to parse and understand responses logical grouping array (e.g., [products], [orders]) - helps agents detailed response schema with field types, descriptions, and examples - complete parameter specifications with type, param type Request Parameter Requirements: - Each parameter MUST include: - param type MUST be one of: - Use descriptive names that clearly indicate the parameters purpose - Provide realistic examples that demonstrate expected values query, path, body type, param type, required, description, example Figure 16. Prompts for interface specification generation (part 1 of 2). Infinity Synthetic Environments for Agentic Reinforcement Learning User Prompt (Part 2/2): Response Schema Requirements: - Define complete response structure with all fields - Each field MUST include: - For array types, include items with full field definitions - Use consistent naming conventions across all endpoints type, description, example { { [ false, query, query, string, [products], list products, /api/products, { float, Products, [ Filter products by category name, Output format: { api groups: { group name: endpoints: { path: method: GET, summary: List all products with optional filters, description: Retrieve paginated list of products. or search the product catalog., operation id: tags: request params: category: type: param type: required: description: example: Electronics }, min price: type: param type: required: description: example: 10.0 } }, response: products: type: items: { id: example: 1}, name: {type: example: iPhone 15 Pro}, price: {type: example: 999.99} } } }, required tables: required fields: products: } } ] } ] } [id, name, price, category] Minimum price filter in USD, { { array, [products], { integer, description: string, description: float, description: {type: false, Figure 17. Prompts for interface specification generation (part 2 of 2). 28 Use this endpoint to browse Unique product identifier, Product display name, Product price in USD, Infinity Synthetic Environments for Agentic Reinforcement Learning System Prompt: You are an expert FastAPI backend developer specializing in RESTful APIs that are agent-friendly: OpenAPI metadata, complete request/response typing, and machine-readable docs. generate clean, executable FastAPI endpoint implementations from an API specification and SQLite database schema. follow the user prompts constraints and return output in the exact required format. every endpoint must include clear You You strictly User Prompt (Part 1/3): Generate single, fully self-contained interface implementation (one Python file using FastAPI and exposed via MCP) that simulates simplified version of {scenario name} based on the provided interface specification and database schema. scenario can be website, mobile app, or collection of tools and services. Note: This is part of an automatic environment synthesis pipeline where we generate executable tool-use environments. Assumptions: - Python version: - FastAPI version compatible with Pydantic v2 (do NOT use v1-only features such as orm mode in Config) {PYTHON VERSION} API Specification: Database Schema: {api spec} {database schema} Environment & Configuration Requirements: - Import os and read the SQLite database URL from environment variable DATABASE PATH - If DATABASE PATH is not set or empty, default to: - In the uvicorn entry point, read HOST from environment variable HOST (default to 127.0.0.1 if not set) - In the uvicorn entry point, read PORT from environment variable PORT (default to 8000 if not set), and cast it to int sqlite:///xxxx.db The SQLAlchemy Setup Requirements: - Use SQLAlchemy ORM with declarative base for all tables - Database URL: use the value of DATABASE PATH (or the default described above). DATABASE PATH is complete URL so do not add any prefixes (e.g., sqlite://) or suffixes to the URL - Create engine and SessionLocal (sessionmaker) for database sessions - Define Base = declarative base() for ORM inheritance - Define ORM models for every table present in the database schema (no extra tables/columns) - Call Base.metadata.create all(engine) after all ORM models are defined - NEVER define ORM attributes named metadata, query, or query class. schema has column with one of these names, use safe Python attribute name with trailing underscore (e.g. the real column name via Column(metadata, ...) - Do NOT define any other ORM class attribute that conflicts with SQLAlchemy declarative internals - When there are multiple foreign key paths between two tables, either: - specify relationship(..., foreign keys=[...]) AmbiguousForeignKeysError, OR - omit the relationship entirely and access related rows via explicit queries instead of relationship() - When importing from SQLAlchemy, only use standard, public symbols that actually exist and are needed. non-existent or internal symbols such as PRIMARY KEY CONSTRAINT or any other invented uppercase constants - Import ORM-related helpers from sqlalchemy.orm (for example: relationship, sessionmaker). internals from sqlalchemy. init Do NOT import or reference metadata ) and map it to explicitly to avoid Do not import ORM declarative base, If the Figure 18. Prompts for interface implementation generation (part 1 of 3). 29 Infinity Synthetic Environments for Agentic Reinforcement Learning User Prompt (Part 2/3): The API Do NOT create dummy or paths, HTTP methods, parameters correct table names, columns, types, and foreign Path parameters in routes (e.g., /api/products/{product id}) MUST appear as function No placeholders; write complete, executable code. All endpoint handler functions MUST be async and self-contained. Implement EVERY endpoint from the API spec with COMPLETE, working code. The source of truth is Hard Requirements: 1. spec is for reference only. the database schema. 2. Follow the API spec as closely as possible: (query/path/body), and response formats. 3. Follow the EXACT database schema: keys; do not invent tables or columns. 4. 5. Use ONLY SQLAlchemy ORM (no raw SQL). 6. User-specific operations MUST implicitly filter by user id=1 where applicable. Session lifecycle per endpoint: 7. - session = SessionLocal() at the start - For INSERT/UPDATE/DELETE call session.commit() - session.close() before returning 8. placeholder endpoints. 9. parameters. 10. STRICTLY PROHIBITED in the code: beyond types, HTTPException, JSONResponse, global exception handlers, duplicate route registration for the same path and HTTP method, references to undefined models/fields/tables, schema-external FTS or helper tables, or any dynamic/introspective tricks to construct response models. 11. 12. 13. variables: if import uvicorn, os host = os.getenv(HOST, 127.0.0.1) port = int(os.getenv(PORT, 8000)) uvicorn.run(app, host=host, port=port) 14. errors. function parameter names or keyword argument names. name is Python keyword (e.g., return, class, global), use safe Python identifier with trailing underscore (e.g., return ) and map it to the real column name or JSON field via Column(return, ...) 15. NOT declare multiple handlers for the same path and HTTP method, even temporarily. Booleans must be real bool fields in Pydantic responses, not 0/1. The FastAPI app must be defined BEFORE any route decorators. Include the uvicorn entry point at the end, using HOST and PORT from environment The generated Python file MUST be valid Python {PYTHON VERSION} with no syntax There MUST be exactly one route function for each (path, HTTP method) pair. Do NOT use Python reserved keywords as comments, try/except, error handling, validation If database column or API field or Field(..., alias=return). == main : name Do Pydantic Model Requirements (Pydantic v2): - Import from Pydantic v2 (e.g., from pydantic import BaseModel, Field, ConfigDict) - Define request and response models for ALL endpoints - Use Field for EVERY field with both description and example - The response model specified in each route decorator MUST exactly match what the function returns - The response model argument in each route decorator MUST be direct reference to concrete Pydantic BaseModel subclass defined in this file (for example, MyResponseModel, or List[MyResponseModel]), NOT dynamically computed or introspected expression. use response model - Endpoint return type annotations MUST be consistent with the response model (e.g., MyResponseModel, List[MyResponseModel]) and MUST be valid Pydantic field types. mixtures like Union[Response, dict, None] mro , . class , metaclasses, or any other tricks to generate Do NOT use Union[...] annotations , Do NOT return types, Response types, or Figure 19. Prompts for interface implementation generation (part 2 of 3). 30 Infinity Synthetic Environments for Agentic Reinforcement Learning User Prompt (Part 3/3): - When returning ORM objects, configure models for Pydantic v2 using model config, for example: class SomeModel(BaseModel): model config = ConfigDict(from attributes=True) ... - Do NOT use the old Pydantic v1 Config with orm mode = True - Do NOT use Annotated or other advanced type tricks that might cause unevaluable type annotations - Use ONLY standard typing types: Dict[str, T] - Field names MUST NEVER be identical (case-sensitive or case-insensitive) to the name of their own type annotation. by the API spec or database schema, always choose different snake case field name (e.g. datetime), and configure serialization alias, for example: Field(..., serialization alias=date, ...) - Ensure that no Pydantic field name clashes with type name or class name used in the same module int, float, str, bool, Optional[T], List[T], date, event datetime: If it is required schedule date: schedule date: date = Agent-Friendly Enhancements (REQUIRED on every endpoint decorator): - summary: one-line purpose (<= 80 chars) - description: - tags: - operation id: - response model: directly corresponds to the returned value logical grouping array (e.g., [products], [orders]) SINGLE LINE (<= 200 chars; no line breaks) unique, snake case identifier concrete Pydantic model class (or typing such as List[Model]) that MUST appear before any @app.get/post/put/patch/delete decorators Code Style (MANDATORY): - app = FastAPI(...) - Use Query/Path/Body/Depends from fastapi where appropriate - Use relationship for ORM relations only when they are unambiguous OR explicitly specify foreign keys to avoid SQLAlchemy AmbiguousForeignKeysError - For SELECT: session.query(Model).filter(...).all()/first() - For INSERT: session.add(...); session.commit() - For UPDATE: fetch the ORM objects, modify attributes, then session.commit() - For DELETE: session.delete(obj); session.commit() - Return Python dicts/lists or Pydantic models that conform exactly to the declared response model - Do NOT use Python reserved keywords as attribute names, function parameters, or keyword argument names. If the schema or API uses such names, use safe Python name with trailing underscore and map it via Column(name, ...) - No comments, no exception handlers, no defensive programming, no placeholder code, and no dynamic hacks around FastAPI or Pydantic or Field(..., alias=name) Output Format (CRITICAL): - You MUST return ONLY complete and valid Python source code of the FastAPI app - The response MUST consist of Python code only. explanations, JSON wrappers, keys, or code fences - Do NOT wrap the code in JSON. Do NOT add any outer structure. the file content - The FIRST line of your response MUST be valid Python statement such as import or from (e.g. import os or from fastapi import FastAPI) - Do NOT escape newlines. - If you include ANY non-Python text (such as Here is the code:, backticks, or JSON), the output will be INVALID - Your entire reply must be single, self-contained Python module that can be saved directly as .py file and executed Output the code exactly as it would appear in .py file Do NOT include any Markdown, prose, The response itself is If the API spec conflicts with the database Reminder: - Use only entities present in the provided database schema. reference. schema, prioritize the database schema. cause bugs, you MUST use the database schema as the source of truth. following the API spec text - Ensure all endpoints run without undefined names, missing imports, or missing models - Ensure the code creates the SQLite database specified by the environment variable DATABASE PATH on first run and successfully serves all endpoints with the specified response models Prioritize making the code executable without errors over If following the API spec as written would The API spec is only for Figure 20. Prompts for interface implementation generation (part 3 of 3). 31 Infinity Synthetic Environments for Agentic Reinforcement Learning [System Prompt] You are an expert Python and SQL developer. uses SQLite queries to verify if user task was completed successfully. output UTF-8 encoded strings and English text. Your job is to generate Python code that You only [User Prompt] You need to generate Python function with SQLite queries to collect useful information from the given databases to verify if user task was completed. You are provided with the environment name, the user task to verify, the database schema, and the initial database state. Simplified API Server Name: User Task to Verify: {task} Database Dump (Initial State, before the agent takes any action): {scenario} {db dump} Generate complete Python function that takes initial db path and final db path as The function should connect to the SQLite database and execute queries to return --- ### Requirements 1. parameters. 2. useful information to assist another LLM to judge the task completion. 3. You can use complex SQLite query combinations and Python logic. return dictionary containing useful information for judging task completion. 4. or columns. 5. 6. state after agent execution. 7. 8. bool, list, and dict types. 9. success criteria, failure criteria. Use the sqlite3 library, and import any other libraries you need. You will be provided with two database states: You must NOT modify the databases in any way. Ensure the returned dictionary can be JSON serialized. The function and queries should follow the database dump. You can only read from them. You must output dictionary including: reasoning, python code, function name, Use the initial state to compare with the final state. the initial state and the final The function must Do not invent new tables Use only string, int, float, str) -> dict: str, final db path: ### Example Structure def verify task(initial db path: import sqlite3 conn initial = sqlite3.connect(initial db path) conn final = sqlite3.connect(final db path) # Query initial and final database states # Compare states to extract task-relevant signals # ... conn initial.close() conn final.close() return { # Return any valuable data to help determine task completion. # Ensure this dictionary can be JSON serialized. } --- ### Output Format (must be valid JSON, no markdown fences) { \"reasoning\": \"Explanation of why the function can verify task success or failure\", \"python code\": \"function name\": \"success criteria\": \"failure criteria\": } \"Description of expected results indicating task success\", \"Description of expected results indicating task failure\" \"Complete Python function code as string\", \"verify task\", Figure 21. Prompt for verification code generation used for providing verification signals from the database states. 32 Infinity Synthetic Environments for Agentic Reinforcement Learning You are an impartial evaluator for tool-use agent task results with access to database verification. results from querying the database, decide the task outcome. generated by an MCP agent interacting with synthesized environment. provides set of MCP tools to help the agent complete the task. Based on the provided agent trajectory AND the code-based verification The trajectory is The environment ### Input - task json: and the agent trajectory. - verification json: failure criteria, and code execution results that verified the database state changes. dict containing the user task, execution budget, actual execution steps, dict containing verification code, reasoning, success criteria, all required steps were successfully executed, AND the database state ### Classification Categories - Completed: confirms the task was completed. - Partially Completed: partial progress was made, or the database state shows the task is not fully completed. - Environment Error: errors such as Internal Server Error, or the MCP server cannot process valid tool calls. - Agent Error: the users instruction due to agent-side issues. the agent made mistakes, used invalid parameters, or failed to complete the agent is blocked by MCP server or environment error, e.g., 5xx ### Priority Order for Classification 1. Completed (trajectory shows success AND database confirms it) 2. 3. 4. Partially Completed (everything else unfinished or database state mismatch) Environment Error (blocked by MCP server or environment error) Agent Error (agent-side issues, e.g., invalid tool arguments, hallucination) ### Key Considerations - The verification json contains checks performed on the database states before and after agent execution. - Use the verification code execution results to help judge task completion. - The verification json provides success criteria and failure criteria describing how to interpret state differences. - The verification results may be empty, erroneous, or inaccurate. them. Comprehensively consider the trajectory information to judge task completion. Do not fully rely on \"<explanation considering both trajectory and verification results>\", [<int>, <int>, <int>, <int>], \"<Completed Partially Completed Environment Error Agent ### Output Format (must be valid JSON) { \"reasoning\": \"confidence score\": \"classification\": Error>\", \"evidence\": { \"iterations\": \"error signals\": \"last actions\": \"database verification\": } } <int>, [\"<important error messages>\"], [\"<summaries of last few actions>\"], \"<summary of database state changes>\" Figure 22. Prompt for code-augmented LLM-as-a-Judge verification. The judge receives both the agent trajectory and structured verification signals from database state inspection to provide robust reward signals for RL training. 33 Infinity Synthetic Environments for Agentic Reinforcement Learning Table 11. Example tasks from three synthesized environments. Each task requires multiple tool calls, conditional logic, or complex filtering, demonstrating the diversity and complexity of our generated tasks. Scenario Generated Tasks Create new playlist named Morning Focus 2025 with the description Upbeat but not distracting and add the top 10 most popular tracks by Daft Punk to it. Generate personalized playlist of 30 songs based on my recent listening history that match the chill mood and save it as Chill Evening Mix. Create collaborative playlist named Road Trip to Yosemite and add the top 5 rock tracks from the 1990s plus the top 5 pop tracks from the 2010s. Music Streaming Search for wireless noise cancelling headphones, sort results by average customer rating, and add the top-rated item under $200 to my cart in quantity 1. E-commerce Platform Locate my order for Instant Pot Duo 7-in-1 placed within the last 6 months and initiate return request selecting Item defective or doesnt work as the reason and requesting refund to my original payment method. Subscribe to household paper towels product with at least 4-star rating using Subscribe & Save, delivering 12-roll pack every 2 months to my default address. Create and save multi-destination trip itinerary that includes hotel in Rome (Italy) from August 58, 2025 and hotel in Florence (Italy) from August 811, 2025 for 2 adults, choosing mid-range properties (3 or 4 stars) with guest ratings of at least 8.5. Travel Booking Search for vacation packages that bundle hotel and flight from Berlin (BER) to Barcelona (BCN) for April 1217, 2025 for 2 adults, and return the cheapest package that includes at least 4-star hotel within 2 km of the city center. Search for pet-friendly apartments in Lisbon, Portugal for one-month stay from January 5, 2026 to February 5, 2026 for 2 adults and 1 child with budget of at most C80 per night, and return the top three options sorted by guest rating. 34 Infinity Synthetic Environments for Agentic Reinforcement Learning 1 from fastapi import FastAPI, Query, Path, Body 2 from pydantic import BaseModel, Field, ConfigDict 3 from typing import Optional, List, Dict 4 from sqlalchemy import create engine, Column, Integer, String, DateTime, Date, ForeignKey, Text, UniqueConstraint, Index, Float 5 from sqlalchemy.orm import declarative base, relationship, sessionmaker 6 import os 7 from datetime import datetime 8 9 database url = os.getenv(\"DATABASE PATH\", \"sqlite:///outputs/databases/spotify.db\") 10 engine = create engine(database url, connect args={\"check same thread\": False} if database url.startswith(\" sqlite\") else {}) tablename tablename = \"users\" = \"artists\" id = Column(Integer, primary key=True) name = Column(Text, nullable=False) sort name = Column(Text) primary genre id = Column(Integer, ForeignKey(\"genres.id\")) created at = Column(DateTime) updated at = Column(DateTime) id = Column(Integer, primary key=True) username = Column(Text, nullable=False, unique=True) email = Column(Text, nullable=False, unique=True) display name = Column(Text) profile data = Column(Text) created at = Column(DateTime) updated at = Column(DateTime) 11 SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine) 12 Base = declarative base() 13 14 app = FastAPI(title=\"Simplified Spotify API\") 15 16 class User(Base): 17 18 19 20 21 22 23 24 25 26 class Artist(Base): 27 28 29 30 31 32 33 34 35 class Track(Base): 36 37 38 39 40 41 42 43 44 45 46 class Playlist(Base): tablename 47 48 49 50 51 52 53 54 = \"playlists\" id = Column(Integer, primary key=True) owner user id = Column(Integer, ForeignKey(\"users.id\"), nullable=False) name = Column(Text, nullable=False) description = Column(Text) is collaborative = Column(Integer, default=0) created at = Column(DateTime) updated at = Column(DateTime) id = Column(Integer, primary key=True) title = Column(Text, nullable=False) album id = Column(Integer, ForeignKey(\"albums.id\")) duration ms = Column(Integer) track number = Column(Integer) popularity = Column(Integer, default=0) created at = Column(DateTime) updated at = Column(DateTime) = \"tracks\" tablename Figure 23. Synthesized Spotify environment code (Part 1 / 2): Imports and core database models including User, Artist, Track, and Playlist tables. The full implementation includes 25 database models covering genres, albums, podcasts, listening history, and more. Note that this is simplified example and the full implementation contains approximately 2,400 lines of code, which is not shown here for brevity. 35 Infinity Synthetic Environments for Agentic Reinforcement Learning id=p.id, owner user id=p.owner user id, name=p.name, description=p.description, is collaborative=bool(p.is collaborative), created at=p.created at.isoformat() if p.created at else None, \"/api/playlists\", response model=PlaylistsResponse, summary=\"Get all playlists for current user\", description=\"Retrieve all playlists owned by or shared with the current user.\", tags=[\"playlists\"], operation id=\"get playlists\", limit: int = Query(20, description=\"Maximum number of playlists\"), offset: int = Query(0, description=\"Offset for pagination\"), session = SessionLocal() playlists = session.query(Playlist).filter( ] session.close() return PlaylistsResponse(playlists=result) PlaylistModel( ) for in playlists Playlist.owner user id == 1 ).offset(offset).limit(limit).all() result = [ 200 201 .....about 150 lines of code here..... 202 203 @app.get( 204 205 206 207 208 209 210 ) 211 async def get playlists( 212 213 214 ) -> PlaylistsResponse: 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 @app.post( 231 232 233 234 235 236 237 ) 238 async def create playlist( 239 240 ) -> PlaylistModel: 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 .....about 2000 lines of code here..... 260 261 @app.on event(\"startup\") 262 async def startup(): 263 264 265 if 266 267 268 269 ) session.add(playlist) session.commit() result = PlaylistModel( session = SessionLocal() now = datetime.utcnow() playlist = Playlist( ) session.close() return result == \" main \": name import uvicorn host = os.getenv(\"HOST\", \"0.0.0.0\") port = int(os.getenv(\"PORT\", \"8000\")) uvicorn.run(app, host=host, port=port) Base.metadata.create all(bind=engine) \"/api/playlists\", response model=PlaylistModel, summary=\"Create new playlist\", description=\"Create new playlist for the current user.\", tags=[\"playlists\"], operation id=\"create playlist\", body: PlaylistCreateBody = Body(..., description=\"Playlist data\"), owner user id=1, name=body.name, description=body.description, is collaborative=1 if body.is collaborative else 0, created at=now, updated at=now, id=playlist.id, owner user id=playlist.owner user id, name=playlist.name, description=playlist.description, is collaborative=bool(playlist.is collaborative), created at=playlist.created at.isoformat(), Figure 24. Synthesized Spotify environment code (Part 2 / 2): Pydantic response models and example toolset endpoints for playlist operations. Each endpoint includes OpenAPI metadata (summary, description, tags, operation id) for automatic documentation and agent discovery via the MCP protocol. In the last, the environment reads configuration from environment variables and creates database tables on startup, which is designed for isolated launching for RL training. Note that this is simplified example and the full implementation contains approximately 2,400 lines of code, which is not shown here for brevity. 36 Infinity Synthetic Environments for Agentic Reinforcement Learning Available MCP Tools (45 tools): ================================================================================ 1. mcp tool search artists Description: Search artists by name Search artists by partial or full name, optionally sorted by name. Parameters: - query: string (required) Description: Case-insensitive partial artist name to search - limit: integer (optional, default: 20) Description: Maximum number of artists to return - offset: integer (optional, default: 0) Description: Number of artists to skip for pagination Response Example: {\"artists\": [{\"id\": 1, \"name\": \"Name\"}]} 2. mcp tool get artist by id Description: Get artist by ID Retrieve single artist by its ID. Parameters: - artist id: integer (required) Description: Artist ID to retrieve 3. mcp tool get artist top tracks Description: Get top tracks for an artist Return the most popular tracks for specific artist ordered by popularity. Parameters: - artist id: integer (required) Description: Artist ID whose top tracks to retrieve - limit: integer (optional, default: 10) Description: Maximum number of top tracks to return Response Example: {\"tracks\": [{\"id\": 1, \"title\": \"Title\", \"is podcast episode\": true}]} ..... omitted 18 tools (follow artist, search tracks, get track by id, etc.) ..... 22. mcp tool create playlist Description: Create new playlist Create new playlist owned by the current user. Parameters: - name: string (required) Description: Playlist name (unique per user) - description: string (optional) Description: Playlist description text - is collaborative: boolean (optional, default: False) - is personalized: boolean (optional, default: False) - is podcast mixed: boolean (optional, default: False) ..... omitted 19 tools (playlist management, mood, recommendations, etc.) ..... 45. mcp tool get top tracks grouped by genre Description: Get top streamed tracks grouped by genre Return users top tracks over period grouped by primary genre. Parameters: - months: integer (optional) Description: Lookback period in months from now - limit: integer (required) Description: Total number of top tracks to consider Response Example: {\"genres\": [{\"genre id\": 1, \"genre name\": \"Genre Name\", \"tracks\": [{\"track id\": 1, \"title\": \"Title\", \"play count\": 1, \"total listened ms\": 1}]}]} Figure 25. The example of the unified MCP interface for the Spotify environment. Each tool includes descriptive name, natural language description, typed parameters with defaults, and example response schemas. The agent discovers these tools dynamically via the list tools at the beginning of interaction. 37 Infinity Synthetic Environments for Agentic Reinforcement Learning def get conn(path): return sqlite3.connect(path) def fetchone dict(cur, query, params=()): my existing playlist called Driving Vibes.\" cur.execute(query, params) row = cur.fetchone() if row is None: return None Returns dictionary with task-relevant signals for LLM-as-a-Judge. \"\"\" import sqlite3 \"\"\" Verification function that compares initial and final database states to determine if the task was completed successfully. 1 # ============================================================================= 2 # Scenario: Spotify (Music Streaming Platform) 3 # Task: \"Search for the song Blinding Lights by The Weeknd and save it to 4 # 5 # ============================================================================= 6 7 def verify task(initial db path: str, final db path: str) -> dict: 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 cur.execute(query, params) rows = cur.fetchall() cols = [c[0] for in cur.description] return [{cols[i]: row[i] for in range(len(cols))} for row in rows] cur final, \"SELECT id, username FROM users WHERE username = ?\", (\"current user\",), # Connect to both databases (read-only) conn initial = get conn(initial db path) conn final = get conn(final db path) ) user id = current user[\"id\"] if current user else None cols = [c[0] for in cur.description] return {cols[i]: row[i] for in range(len(cols))} # Step 1: Identify the current user current user = fetchone dict( cur init = conn initial.cursor() cur final = conn final.cursor() def fetchall dicts(cur, query, params=()): try: Figure 26. Synthesized verification code for Spotify environment (Part 1 / 2): Task description, helper functions, and user identification. The function takes two database paths (initial and final states) and returns task-relevant signals for verification. Infinity Synthetic Environments for Agentic Reinforcement Learning 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 # Step 2: Locate the playlist Driving Vibes in both database states playlist query = \"\"\" SELECT id, owner user id, name, description, created at, updated at FROM playlists WHERE owner user id = ? AND name = ? \"\"\" playlist initial = fetchone dict(cur init, playlist query, (user id, \"Driving Vibes\")) playlist final = fetchone dict(cur final, playlist query, (user id, \"Driving Vibes\")) playlist id = playlist final[\"id\"] if playlist final else None # Step 3: Find the track Blinding Lights by The Weeknd track query = \"\"\" SELECT t.id AS track id, t.title, a.name AS artist name FROM tracks JOIN track artists ta ON ta.track id = t.id JOIN artists ON a.id = ta.artist id WHERE t.title = ? AND a.name = ? \"\"\" track final = fetchone dict(cur final, track query, (\"Blinding Lights\", \"The Weeknd\")) track id = track final[\"track id\"] if track final else None # Step 4: Get playlist tracks before and after agent execution playlist tracks query = \"\"\" SELECT pt.track id, pt.position, t.title AS track title FROM playlist tracks pt JOIN tracks ON t.id = pt.track id WHERE pt.playlist id = ? ORDER BY pt.position \"\"\" tracks initial = fetchall dicts(cur init, playlist tracks query, (playlist id,)) if playlist id else [] tracks final = fetchall dicts(cur final, playlist tracks query, (playlist id,)) if playlist id else [] # Step 5: Determine if Blinding Lights was added to the playlist initial track ids = {t[\"track id\"] for in tracks initial} final track ids = {t[\"track id\"] for in tracks final} newly added tracks = [t for in tracks final if t[\"track id\"] not in initial track ids] blinding lights added = track id in final track ids and track id not in initial track ids # Step 6: Build result dictionary with task-relevant signals result = { \"environment\": \"spotify\", \"task description\": \"Search for Blinding Lights by The Weeknd and save to Driving Vibes\", \"playlist exists\": playlist final is not None, \"track exists\": track final is not None, \"playlist tracks before\": len(tracks initial), \"playlist tracks after\": len(tracks final), \"newly added tracks\": newly added tracks, \"blinding lights added to playlist\": blinding lights added, } return result finally: conn initial.close() conn final.close() Figure 27. Synthesized verification code for Spotify environment (Part 2 / 2): Core verification logic that queries the playlist and track tables, compares initial and final states, and returns structured signals. The key output blinding lights added to playlist indicates whether the target track was successfully added. 39 Infinity Synthetic Environments for Agentic Reinforcement Learning Figure 28. Case study for verification: code-based verifier and LLM judge fully align on clean, database-grounded success signal. Figure 29. Case study for verification: Tool/Infrastructure error produces false negative for code-only verification, while the judge uses trajectory context to recover the correct judgment. 40 Infinity Synthetic Environments for Agentic Reinforcement Learning Figure 30. Case study for verification: Tool calling ambiguity causes wrong-entity action that looks successful locally; verification grounded in the true database state prevents false positive."
        }
    ],
    "affiliations": [
        "Snowflake",
        "University of North Carolina at Chapel Hill"
    ]
}