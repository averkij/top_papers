{
    "paper_title": "Decoder-Hybrid-Decoder Architecture for Efficient Reasoning with Long Generation",
    "authors": [
        "Liliang Ren",
        "Congcong Chen",
        "Haoran Xu",
        "Young Jin Kim",
        "Adam Atkinson",
        "Zheng Zhan",
        "Jiankai Sun",
        "Baolin Peng",
        "Liyuan Liu",
        "Shuohang Wang",
        "Hao Cheng",
        "Jianfeng Gao",
        "Weizhu Chen",
        "Yelong Shen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in language modeling have demonstrated the effectiveness of State Space Models (SSMs) for efficient sequence modeling. While hybrid architectures such as Samba and the decoder-decoder architecture, YOCO, have shown promising performance gains over Transformers, prior works have not investigated the efficiency potential of representation sharing between SSM layers. In this paper, we introduce the Gated Memory Unit (GMU), a simple yet effective mechanism for efficient memory sharing across layers. We apply it to create SambaY, a decoder-hybrid-decoder architecture that incorporates GMUs in the cross-decoder to share memory readout states from a Samba-based self-decoder. SambaY significantly enhances decoding efficiency, preserves linear pre-filling time complexity, and boosts long-context performance, all while eliminating the need for explicit positional encoding. Through extensive scaling experiments, we demonstrate that our model exhibits a significantly lower irreducible loss compared to a strong YOCO baseline, indicating superior performance scalability under large-scale compute regimes. Our largest model enhanced with Differential Attention, Phi4-mini-Flash-Reasoning, achieves significantly better performance than Phi4-mini-Reasoning on reasoning tasks such as Math500, AIME24/25, and GPQA Diamond without any reinforcement learning, while delivering up to 10x higher decoding throughput on 2K-length prompts with 32K generation length under the vLLM inference framework. We release our training codebase on open-source data at https://github.com/microsoft/ArchScale."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 7 0 6 6 0 . 7 0 5 2 : r Decoder-Hybrid-Decoder Architecture for Efficient Reasoning with Long Generation Liliang Ren1 Congcong Chen1 Haoran Xu1 Young Jin Kim1 Jiankai Sun2 Baolin Peng1 Adam Atkinson1 Zheng Zhan1 Liyuan Liu1 Shuohang Wang1 Hao Cheng1 Jianfeng Gao1 Weizhu Chen1 Yelong Shen1 1Microsoft 2Stanford University {liliangren,yeshe}@microsoft.com"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in language modeling have demonstrated the effectiveness of State Space Models (SSMs) for efficient sequence modeling. While hybrid architectures such as Samba and the decoder-decoder architecture, YOCO, have shown promising performance gains over Transformers, prior works have not investigated the efficiency potential of representation sharing between SSM layers. In this paper, we introduce the Gated Memory Unit (GMU), simple yet effective mechanism for efficient memory sharing across layers. We apply it to create SambaY, decoder-hybrid-decoder architecture that incorporates GMUs in the cross-decoder to share memory readout states from Samba-based self-decoder. SambaY significantly enhances decoding efficiency, preserves linear pre-filling time complexity, and boosts long-context performance, all while eliminating the need for explicit positional encoding. Through extensive scaling experiments, we demonstrate that our model exhibits significantly lower irreducible loss compared to strong YOCO baseline, indicating superior performance scalability under large-scale compute regimes. Our largest model enhanced with Differential Attention, Phi4-mini-Flash-Reasoning, achieves significantly better performance than Phi4-mini-Reasoning on reasoning tasks such as Math500, AIME24/25, and GPQA Diamond without any reinforcement learning, while delivering up to 10 higher decoding throughput on 2K-length prompts with 32K generation length under the vLLM inference framework. We release our training codebase on open-source data at https://github.com/microsoft/ArchScale."
        },
        {
            "title": "Introduction",
            "content": "State Space Models (SSMs) [GGR21, GGGR22, GD23, DG24], including linear attention [HDLL22, SDH+23, QHS+22, YWS+23, YWZ+24, YKH25] and modern Recurrent Neural Networks (RNNs) [BPS+24, BPL+25, PAA+23, GOA+24], have recently shown promising results for more efficient sequence modeling over Transformers [VSP+17]. While pure SSMs/RNNs offer computational advantages with linear complexities, hybrid architectures [LLB+24, DSF+24, RLL+25, WBR+24, DFD+25, Min25] can leverage as few as one additional attention layer to bridge the theoretical expressiveness gap of SSMs to Transformers [WDL24]. Recently, the decoder-decoder architecture, YOCO [SDZ+24], achieves linear complexity for long context processing through storing the Key-Value (KV) pairs from single self-attention layer and re-using them across all subsequent layers through cross-attentions. In practice, YOCO has delivered substantial efficiency gains when processing user prompts with long sequences, but challenges remain; it does not mitigate the attention memory I/O cost for its cross-attentions during the generation stage of the model responses. This limitation Preprint. becomes particularly pronounced for modern large language models (LLMs) [Ope24, DA25] that generate extensively long Chains-of-Thought (CoTs) [WWS+22] for hard reasoning tasks. In this paper, we investigate the potential of representation sharing between SSM layers to enhance decoding efficiency. We introduce the Gated Memory Unit (GMU), versatile, simple yet effective mechanism for efficient memory sharing across layers. Applying GMUs to the cross-decoder of YOCO, we create novel model with our decoder-hybrid-decoder architecture named SambaY that uses Samba [RLL+25] for the self-decoder and replaces half of the cross-attention layers with GMUs to share the inner representations of the final SSM layer in the self-decoder. Since around 50% of expensive cross-attention layers are replaced with cheap element-wise gating, SambaY significantly improves decoding efficiency and maintains linear pre-filling time complexity, all while removing the need for explicit positional encoding such as RoPE [SLP+21]. To enable robust comparison of the scaling capabilities across different architectures, we first design principled µP++ hyperparameter transfer scheme that accounts for both depth and width scaling, as well as the application of weight decay to vector-like parameters. We then conduct extensive experiments up to 3.4B parameters/600B tokens to verify the scaling behaviors of both our µP++ scaling laws and the SambaY architecture. Comparing to Samba+YOCO, an architecture that naively combines Samba with YOCO, we show that SambaY has significantly lower irreducible loss [HNA+17] on the validation set when scaling with the training FLOPs, indicating better scaling potential with large-scale computes. We also conduct extensive experiments to verify the long-context retrieval capabilities of our architecture. Our results reveal that SambaY achieves superior performance on challenging long-context tasks like Phonebook and RULER [HSK+24] benchmark, even with modest Sliding Window Attention (SWA) size of 256. To further explore the capabilities of hybrid models with single set of full attention memory, we augment SambaY with Differential Attention [YDX+24], resulting in the Phi4-mini-Flash architecture. We pre-train our 3.8B-parameter model Phi4-mini-Flash with 5T tokens from the same Phi4-mini data corpus and further follow Phi4-mini-Reasoning [XPA+25] to conduct the multi-stage distillation with Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to produce our reasoning model, Phi4-mini-Flash-Reasoning. Our model achieves significantly better performance than the strong Phi4-mini-Reasoning baseline on challenging reasoning benchmarks such as Math500, AIME24/25, and GPQA Diamond, while excluding any stage of Reinforcement Learning that is used by Phi4mini-Reasoning. Critically, our Phi4-mini-Flash-Reasoning delivers up to 10 higher decoding throughput on 2K-length prompts with 32K generation length under the vLLM [KLZ+23] inference framework, showcasing its substantial and practical efficiency gains for the LLM reasoning paradigm of generating long Chain-of-Thoughts."
        },
        {
            "title": "2 Decoder-Hybrid-Decoder Architecture",
            "content": "Inspired by the gating mechanism that broadly exists in Gated Linear Units [Sha20], Gated Attention Units [HDLL22] and SSMs [GD23, YWS+23, YKH25], we first introduce our Gated Memory Unit (GMU) that takes the current layers input representation and previous layers memory state as the inputs and outputs the gated representations with learnable projections. We then explore specific application of GMUs to YOCO which produces our decoder-hybrid-decoder architecture. dedicated related works section is included in Appendix I. Gated Memory Unit (GMU). From an inter-layer perspective, we define \"memory\" as hidden representations passed from preceding layers. Specifically, at given layer l, GMU operates on two inputs: the current layers input hidden state, xl Rdm , and memory state, ml Rdh, from previous layer (where < l). The GMU then produces an output yl Rdm through gating mechanism modulated by learnable projections. Formally, the GMU can be expressed as: yl = (ml σ(W1xl))W2 where σ() is the SiLU [EUD17] activation function, denotes element-wise multiplication, and W1, W2 Rdhdm are learnable weight matrices. One can also add RMSNorm [ZS19] after the element-wise multiplication, which we denote as the nGMU (normalized GMU), for improved training stability. Intuitively, this gating mechanism allows the current layers input xl to effectively act as dynamic fine-grained recalibration of token mixing that occurred in previous layers based on the current query context for each of the memory channels. While in this work we primarily focus 2 Figure 1: Our decoder-hybrid-decoder architecture taking Samba [RLL+25] as the self-decoder. Gated Memory Units (GMUs) are interleaved with the cross-attention layers in the cross-decoder to reduce the decoding complexity. As in YOCO [SDZ+24], the full attention layer only need to compute the KV cache during prefilling with the self-decoder, leading to linear computation complexity for the prefill stage. on gating memory from SSM layers (where dh would correspond to the SSM inner dimension), the concept is generalizable. For instance, ml could be the intermediate output of preceding attention layer, allowing GMUs to diversify the attention map for each channel of the value vectors based on the input representation of the current layer. Similarly, it could gate intermediate outputs from MLP layers, enabling retrieval from projected summarization of all the layer output representations before MLP. In both cases, GMUs save parameters and computation compared to the vanilla attention or the MLP layers. We include more theoretical analyses of GMU and nGMU in Appendix A. Model architecture. In Figure 1, we illustrate the SambaY architecture, using our decoder-hybriddecoder architecture with Samba [RLL+25] as the self-decoder. We apply GMUs to the crossdecoder of YOCO to replace half of its cross-attention layers. The GMUs share the representation from the last SSMs layers in the self-decoder so that the pre-filling time complexity is still linear. Compared to YOCO, our approach only requires caching an additional SSM kernel output state Rdh, dh = 2dm from the final Mamba layeran overhead that is negligible in sizealongside the KV cache from the last full-attention layer during pre-filling. During decoding, we reduce the memory I/O complexity for half of the cross-attention layers from linear cost of O(dkvN ) to constant O(dh), where is the sequence length and dkv is the dimension of key-value pairs. This leads to significant efficiency gains when dh/dkv, condition that is easily met in practice since the ratio dh/dkv typically does not exceed 128."
        },
        {
            "title": "3 Experiments & Results",
            "content": "Motivated by the theoretical efficiency of our SambaY architecture, we aim to address the following research questions: Does the architecture scale effectively? Does it compromise long-context performance? Can it support reasoning over long generations? Given that neural architectures performance is tightly coupled with its optimization and initialization settings, we begin by establishing generic scaling setup to encourage fair comparison of scaling behavior across different architectures."
        },
        {
            "title": "3.1 Scaling Experiments on Open-Source Data",
            "content": "Architecture scaling setup. We use simple linear rule from the previous works on Transformer models [KMH+20, TJY+24] for scaling the architectural shape of our Transformer++ baseline, 3 including model width w, model depth d, number of attention query heads hq and the MLP inner dimension wmlp, i.e., = αd, α = α0 = 128, hq = d, where the Transformer-specific aspect ratio α0 and the number of key-value heads hkv are computed based on the Llama-3-8B [Met24] architecture. We use SwiGLU [Sha20] for MLP and RoPE [SLP+21] with the base frequency of 10,000. The total number of non-embedding parameters for the Transformer++ architecture can then be calculated as, hkv = d/4, wmlp = 4w, Nattn(d) = 2.5dw2, Nmlp(d) = 12dw2, (d) = Nattn(d) + Nmlp(d) = 14.5dw2 = 237568d3. Baseline Architectures. We consider several architectural variants alongside our proposed SambaY architecture and Transformer++, including Samba+YOCO (which uses Samba as self-decoder for the original YOCO architecture), TransformerLS (interleaving SWA with full attention using layer ratio of 3:1), and SambaY+DA (which uses Differential Attention (DA) [YDX+24] for all attention layers). All hybrid architectures maintain consistent hyperparameter settings with 4 MLP inner dimension expansion ratio and GQA [ALTdJ+23] group size of 4 for self-attention layers, matching our Transformer++ baseline. To ensure fair comparison, we standardize the sliding window size to 128 for all architectures with SWA while conducting extensive ablations on window size effects in Section 3.2. Following the studies in recent hybrid models [LLB+24, RLL+25], we omit explicit positional encodings (NoPE) for all hybrid SSMs architectures. Detailed configurations for the implementation of DA are provided in Appendix F. Scaling transfer for hybrid architectures. Since different token mixers have their own inner dimension expansion ratio, it is hard to balance the number of parameters between hybrid models and Transformers to make fair comparisons. Previous works [DA24a, RLL+25, YKH25] often adjust the model depth to tie the total number of parameters, but this could change the memory cache size significantly (e.g. adding two attention layers in 12-layer Transformer resulting in 16.7% increase of KV cache size), making unfair comparisons regarding the inference time cost. We propose simple solution that (1) builds an iso-parametric equation with respect to the aspect ratio via aligning the total number of non-embedding parameters to the Transformer baseline, (2) solves the equation to obtain the specific aspect ratio (which is rounded up to an even integer to guarantee the activation of Tensor Cores1) for the hybrid architectures. We also fix the head dimension to be α0 = 128, and set the inner dimension of the attention layers to be wattn = α0d so that the number of key-value heads hkv is valid integer. Specifically, for SambaY, we have Nattn(d) = 2.5dw wattn/4 + 2dw wattn/4, Nmamba(d) = 6dw2/4, Ngmu(d) = 4dw2/4, (d) = Nattn(d) + Nmamba(d) + Nmlp(d) + Ngmu(d) = 144αd3 + 14.5α2d3 = 237568d3. Solving for α, we get α1 124. For Samba+YOCO, we can similarly solve an iso-parametric equation to obtain α2 126, with more details in Appendix B. Hyperparameter scaling with µP++. To account for both width and depth scaling of model architectures, we propose µP++ hyperparameter scaling laws that integrate µP [YHB+22], Depth-µP [YYZH23], and apply zero weight decay to vector-like or scalar-like parameters2 for training stability. B. The Since we use the AdamW optimizer [LH18], we apply batch-size-based scaling with η learning rate is further scaled as η 1/ following Depth-µP. For studying the FLOPs scaling behavior across model architectures, we adopt the Chinchilla scaling law [HBM+22] to scale the number of training tokens linearly with the number of model parameters. Formally, we have η = η0 (cid:114) Bd0 B0d , = B0, = T0 (d) (d0) , where the base learning rate is set as η0 = 4 104 and the base batch size B0 = 221 = 2M number of tokens. We also explore scaling the batch size sub-linearly with respect to the training tokens 1https://developer.nvidia.com/blog/optimizing-gpu-performance-tensor-cores/ 2Following the definition in µP, parameters are vector-like when exactly one dimension scales with model width (e.g., embedding and unembedding layers), and scalar-like when no dimension scales with width. 4 [MKAT18, SWW+24, LZH+25], but find that it harms the data scaling behavior of the models, so we keep the batch size as constant across scales. The base model depth is set as d0 = 16 so that (d0) 109 number of parameters. The base training tokens T0 is set to 100B. We adopt µP to scale the output logits and the learning rate of matrix-like parameters proportional to 1/w. The output of α. each layer is divided by We fix other hyper-parameters of the optimizer with β1 = 0.9, β2 = 0.95, ϵ = 108 and weight decay of 0.1. learning rate schedule is applied with 1B warm-up tokens linearly increasing to the peak learning rate η, followed by linear decay to zero. We use LeCun uniform initialization (i.e. PyTorch default initialization) [LBOM12] for the weight matrices following [GD23] and [RLL+25], and tie the input and output embedding matrices which are initialized from the normal distribution (0, 0.022). We summarize the key differences between Standard Parametrization, µP and µP++ in Table 6 while providing large scale ablation studies in Appendix C. 2d following Depth-µP. The base attention logits multiplier is set to 1/ Scaling experiment setups. common concern with SSMs is that they are not theoretically more expressive than self-attention for in-context retrieval [WDL24]. This raises the question of whether the better performance of hybrid SSM models is owing to their fast convergence from the recency bias, while Transformers could eventually match their performance given more training tokens. With the scaling laws we established in the previous paragraphs, we can now examine this hypothesis systematically. We first study the data scaling behavior across architectures. Specifically, we fix the model size at 1B parameters with the architecture parameterization of = 16 and scale the number of training tokens from 100B to 600B. We also study the FLOPs scaling behaviors of the model architectures with up to 3.4B parameters and 342B tokens through varying the model depth = {8, 12, 16, 20, 24}. We use 4K training sequence length and the SlimPajama [SAKM+23] dataset for all our scaling experiments and measure the model performances on its validation set. Comparison of scaling behaviors. To quantitatively compare the scaling trajectories, we fit the validation loss as function of compute (FLOPs), denoted as DFLOPs, to power law [HNA+17, HBM+22] of the form: L(DFLOPs) = Db FLOPs + This model enables us to estimate the irreducible loss which represents the lower bound of achievable loss for given architecture or parameterization under infinite compute, and the scaling exponent that reflects the learning efficiency with respect to compute. We fit the curves with least squares and the LMA algorithm [LEV44, Mar63]. similar power law model is employed for data scaling experiments, where loss is modeled as function of the number of training tokens Dtokens. (a) Compute scaling comparisons (b) Data scaling comparisons Figure 2: Validation Loss v.s. FLOPs (left) or Training Tokens (right) on the SlimPajama dataset. Besides the architecture comparisons, we also compare our µP++ based scaling with the Standard Parametrization (SP). In Figure 2, we present the results of both FLOPs scaling and data scaling experiments, showing validation loss on the SlimPajama dataset as function of total training FLOPs or number of training tokens. We show both the original data points and the fitted power-law curves. The goodness of fit for each curve is assessed using the R2 statistic and we observe that all plots have fitness score R2 0.999, indicating near-perfect fits. While larger values of the scaling exponent or the coefficient indicate that model may converge more rapidly given small-scale compute or data budget, these parameters alone do not necessarily predict superior performance at larger scales. Therefore, we emphasize the irreducible loss obtained from scaling law fitting as the principal metric for assessing an architectures long-term scaling potential. As illustrated in Figure 2a, the SambaY architecture exhibits the lowest irreducible loss (C = 0.58) for FLOPs scaling, suggesting that it can attain superior validation loss compared to other architectures when scaled further with substantially increased computational resources. We also observe that µP++ yields lower irreducible loss than Standard Parameterization (SP) under both data and compute scaling, indicating more favorable scaling potentials. More experimental details are included in Appendix D. Notably in Figure 2b, the Transformer++ model trained with µP++ exhibits substantial validation loss gap compared to SambaY and Samba+YOCO within the measured range of training tokens. However, its fitted irreducible loss (C = 1.82) is nearly identical to those of the hybrid models, suggesting that with an infinite amount of data, Transformer++ can eventually catch upalbeit with slower convergence. This aligns with our expectation, as there is no theoretical expressiveness gap between Transformers and our hybrid models when the number of parameters is held constant. Interestingly, this convergence no longer holds when both model size and data scale proportionally. As illustrated in Figure 2a, under the µP++ setting, Transformer++ exhibits the highest irreducible loss = 0.64, indicating that hybrid architectures could offer superior scalability under limited data regimes."
        },
        {
            "title": "3.2 Efficient Long Context Retrieval",
            "content": "Given the presence of full-attention layers, we aim to determine the minimal sliding window size required for our hybrid models to retain effective long-context retrieval capabilitiesan essential property for supporting advanced reasoning behaviors that involve long generation with backtracking. Specifically, we pre-train 1.0B parameter models with µP++ and = 16 using TransformerLS, SambaY, SambaY+DA and Samba+YOCO architectures respectively on the ProLong-64k [GWYC24] dataset with 32K sequence length and 40B tokens, varying the window size of their Sliding Window Attention (SWA) in the range {64, 128, . . . , 2048}. We align the number of parameters between different architectures through building the iso-parametric equations as in Section 3.1. We adopt variable-length training, where short documents are packed together and self-attended within the same segment. We evaluate the long-context retrieval capabilities of the models using difficult Phonebook benchmark [JBKM24] with 32K context length (containing 1,850 name-number pairs). We choose this benchmark because it is realistic multi-key-value retrieval task with minimal instructions, which minimizes the confounding influence of instruction-following ability when evaluating long-context retrieval performance. We use RoPE base of 640K for TransformerLS and Transformer++, following the lower bounds proposed in [XMW+24]. We also examine how the training corpus and methods affect the long context performance of these models, with more details in Appendix E. Figure 3: Accuracy (with error bars) v.s. Sliding Window Size on Phonebook with 32K evaluation length. Table 1: Retrieval accuracy on Needle-In-A-Haystack (NIAH) tasks with 32K context from the RULER [HSK+24] long context benchmark. MK: Multi-Key, MQ: Multi-Query, MV: Mutli-Value, S: Single-needle. We use the best Sliding Window Attention (SWA) size found on the Phonebook benchmark for each hybrid architecture. Best results are in bold, second best underlined."
        },
        {
            "title": "Model",
            "content": "SWA MK-1 MK-2 MK-3 MQ MV S-1 - Transformer++ 256 TransformerLS Samba+YOCO 1024 256 SambaY 512 SambaY+DA 36.4 42.8 49.0 54.6 64.6 3.8 6.0 28.0 27.8 27.6 27.9 29.8 12.8 12.7 12. 24.1 27.5 18.3 19.4 19.9 94.8 91.8 100.0 83.2 99.8 0.0 0.0 2.6 0.4 0.2 6 S-2 66.0 49.6 63.2 81.2 86. S-3 Avg. 31.0 23.4 23.6 63.8 69.6 35.5 33.9 37.2 42.9 47.6 As shown in Figure 3, which plots accuracy against SWA size on the Phonebook dataset (32K evaluation length), surprisingly, larger SWA sizes do not consistently provide better results. Since learned full attention involves both sliding window (local) patterns and non-local patterns like global retrieval or attention sinks, using small sliding window sizes, where models like SambaY and SambaY+DA show strong performance, could enable the model to focus on local patterns more easily and mitigate issues like attention sinks [GPD+25]. Moreover, shorter sliding windows can facilitate faster convergence, crucial factor in long context training scenarios that are often characterized by limited high-quality data. The lower scores of TransformerLS, which consistently underperform the SambaY variants, could be indicative of pure Transformer-based models requiring more substantial data for long-context training. Table 2: Downstream short-context evaluation on language modeling and common-sense reasoning tasks in zero-shot for 1B-parameter models with the tuned sliding window size. The training speed is measured in MTPS (Million Tokens Per Second) with 64 A100-80GB GPUs. Best results are in bold, second best underlined. Model SWA Speed Wiki. LMB. ARC-c ARC-e Hella. acc_n mtps acc_n acc acc ppl - Transformer++ 256 TransformerLS Samba+YOCO 1024 256 SambaY 512 SambaY+DA 0.89 1.46 0.99 1.11 0.91 19.75 18.49 16.73 17.83 16.59 45.45 48.77 50.53 50.40 49.68 27.56 28.84 28.50 29.44 28.33 54.17 57.11 60.02 57.87 60. 43.86 45.85 48.85 49.08 49.53 PIQA Wino. Avg. acc acc acc 68.77 69.21 71.55 71.00 71.38 50.28 53.67 52.57 55.25 53.43 48.35 50.57 52.00 52.17 52.17 Using the optimal sliding window size from the Phonebook benchmark, we evaluate our architectures on both long-context retrieval tasks  (Table 1)  and traditional downstream benchmarks  (Table 2)  . Across both contexts, hybrid models with SSMs consistently outperform pure Transformer architectures. SambaY variants demonstrate notable advantages in long-context retrieval while maintaining strong performance on short-context tasks, despite using much smaller sliding window sizes than Samba+YOCO. The addition of DA further enhances multi-key and single-needle retrieval capabilities, while Transformer-based models show specific strengths in multi-query and multi-value scenarios. TransformerLS outperforms Transformer++ on short-context tasks but falls behind on the RULER benchmark, highlighting the trade-off on long-context performance caused by introducing SWA to full attention models. Overall, our results suggest that GMUs facilitate efficient representation sharing across layers and enable strong performance with smaller SWA sizes."
        },
        {
            "title": "3.3 Large-Scale Pre-training on High-quality Data",
            "content": "Table 3: Downstream evaluation performance of post-trained models. We use the completion split for BigCodeBench evaluation. Bold indicates the best result per row. Benchmark Metric Phi4-mini Phi4-mini-Flash MMLU [HBB+21] MMLU-Pro [WMZ+24] Arena Hard [LCF+24] GSM8K [CKB+21] Qasper [DLB+21] SummScreenFD [CCWG22] BigCodeBench [ZVC+25] MBPP [AON+21] 5-shot 0-shot, CoT Win Rate 0-shot, CoT F1 ROUGE-L pass@1 pass@1 67.3 52.8 32.8 88.6 40.4 16.0 43.0 65.3 71.9 54.7 34.9 89.5 40.2 17.0 44.5 69.8 We apply our SambaY+DA architecture to pre-train larger-scale prototype model named Phi4-miniFlash with 3.8B parameters. It uses an SWA size of 512 and GQA of group size 2. Compared to the configuration described in Section 3.1, this model uses different aspect ratio α = 80 and an attention head dimension of 64. It is trained with standard parameterization rather than µP++ due to resource constraints at the time of scaling study. We pre-train our model on 5T tokens from the data corpus used by Phi4-mini [MAA+25] on 1K A100-80GB GPUs for 14 days. During training, we encounter severe loss divergence, which we mitigate by introducing label smoothing of 0.1 and attention dropout of 0.05. The optimization setup here is by no means optimal, as the primary goal of 7 this experiment is to evaluate the viability of our architecture at larger scales. Phi4-mini-Flash uses 200K token vocabulary size consistent with Phi4-mini. Additional training and architectural details are provided in Appendix F. Table 3 demonstrates that Phi4-mini-Flash outperforms the Phi4-mini baseline across diverse range of tasks, with notable improvements on knowledge-intensive benchmarks like MMLU (4.6% absolute gain) and coding tasks such as MBPP (4.5% absolute gain). The consistent performance advantage, winning on 7 out of 8 benchmarks, is particularly significant given that Phi4-mini-Flash achieves these gains while maintaining substantially higher computational efficiency during inference."
        },
        {
            "title": "3.4 Efficient Reasoning with Long Generation",
            "content": "Table 4: Pass@1 performance of models on reasoning benchmarks measured with maximum generation length of 32K. We report Pass@1 accuracy averaged over 64 samples for AIME24/25 and 8 samples for Math500 and GPQA Diamond to ensure evaluation robustness. We also evaluate popular open-source distilled reasoning models [DA25, Bes, Ope] as reference baselines."
        },
        {
            "title": "Model",
            "content": "AIME24 AIME25 Math500 GPQA Diamond DeepSeek-R1-Distill-Qwen-1.5B DeepSeek-R1-Distill-Qwen-7B DeepSeek-R1-Distill-Llama-8B Bespoke-Stratos-7B OpenThinker-7B Phi4-mini-Reasoning (3.8B) Phi4-mini-Flash-Reasoning (3.8B) 29.58 53.70 43.96 21.51 29.69 48.13 52.29 20.78 35.94 27.34 18.28 24. 31.77 33.59 84.50 93.03 87.48 80.73 87.25 91.20 92.45 37.69 47.85 45.83 38.51 41.60 44.51 45.08 Our Phi4-mini-Flash-Reasoning model is continually trained from the Phi4-mini-Flash model with the same multi-stage distillation data following Phi4-mini-Reasoning [XPA+25]. Notably, due to the limited resources, we only conduct the distillation with the Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) stages and leave Reinforcement Learning (RL) for future works. As shown in Table 4 and Figure 4, our reasoning model achieves significantly better performance than Phi4-mini-Reasoning (which has final RL training stage) on AIME24/25 [Art], Math500 [HBK+21], and GPQA Diamond [RHS+23], while employing substantially more efficient architecture, achieving up to 10 higher throughput in long-generation scenarios and 4.9 speedup in long-context processing. (a) Prompt: 32000, Generation: 500 (b) Prompt: 2000, Generation: 32000 Figure 4: Throughput and latency of text generation with various architectures under the vLLM inference framework (using one A100-80GB GPU and no Tensor Parallelism). normal distribution with 30% variance was applied to prompt and generation lengths with averages of 32000/2000 and 500/32000 respectively, following the setting in [HTW+24]. 8 In Figure 4, we evaluate the throughput of our vLLM implementation3 using random model weights to eliminate the influence of potentially shorter generation lengths on speed measurements, ensuring fair comparison across different architectures. The same hyperparameter configurations as Phi4mini-Flash are applied for the YOCO and SambaY based baseline architectures. We observe that SambaY achieves the best throughput in both long-context and long-generation settings across various numbers of concurrent clients, highlighting the significant practical efficiency gains enabled by our GMU modules. Notably, our Differential Attention implementation relies on naive four-pass of the FlashAttention [Dao23] operator for vLLM compatibility, rather than the optimized custom kernel proposed in the original paper, leaving significant room for further speed optimization of Phi4-mini-Flash-Reasoning to catch up the efficiency of SambaY. More evaluation details and case studies on our models general reasoning ability are provided in Appendix G."
        },
        {
            "title": "4 Ablation Study",
            "content": "Figure 5: Major architectural variants explored in this section. For GDNY, we use Gated DeltaNet [YKH25] with normalization after output gate (GDN-A) for self-decoder, and apply normalized GMU (nGMU) in cross-decoder. As illustrated in Figure 5, we systematically study the design choices in our decoder-hybrid-decoder architecture through the following architectural modifications of SambaY: SambaY-2 or S-GDNY substitutes Mamba layers with Mamba-2 or Gated DeltaNet with normalization after output gate (denoted as GDN-A with more details in Appendix A) respectively in the self-decoder; MambaY/MambaY-2/GDNY employs only Mamba/Mamba2/GDN-A respectively in the self-decoder except the full attention layer. Apart from normalization placement, we find it is crucial to use nGMU for Mamba-2/GDN-A based models to achieve strong long context performance, with ablation studies in Appendix H. SambaY-A applies GMUs to gate intermediate representations from the last full attention layer in the self-decoder rather than from Mamba. SambaY-AA entirely removes cross-attention in the cross-decoder and instead uses GMU to gate the intermediate representations from the middle full attention layer. SambaY-MLP uses GMUs to gate the intermediate representation from the linear projection branch of the SwiGLU right following the full attention layer. All ablation models are trained with 1.0B parameters on the ProLong-64K dataset with 40B tokens and maximum of 32K sequence length with variable length training, using consistent SWA size of 128 as in the scaling experiments. We leverage µP++ with depth = 16 and construct 3We customize the official vLLM framework with the version 0.7.3 to support our Phi4-mini-Flash architecture. iso-parameter equations to maintain parameter count equivalence across all variants, with more details in Appendix F. We aim to answer the following research questions given the ablation results in Table 5. Table 5: Downstream evaluation on Phonebook 32K (PB-32k), language modeling and commonsense reasoning tasks in zero-shot for 1B-parameter models with sliding window size of 128. We measure the training speed in MTPS (Million Tokens Per Second) with 64 A100-80GB GPUs. The average accuracy excludes PB-32K due to its relatively high variability, with standard deviation of around 5%. Best results in bold, second best underlined. Model SambaY MambaY SambaY-2 GDNY MambaY-2 S-GDNY SambaY-A SambaY-AA SambaY-MLP Speed Wiki. PB-32K LMB. ARC-c ARC-e Hella. acc_n acc mtps acc_n acc acc ppl 1.10 0.94 1.43 1.22 1.38 1.34 1.11 1.25 1.15 16.89 17.29 17.17 16.92 18.63 16.78 18.12 17.03 18.70 78.13 12.50 40.63 89.84 50.78 83.59 58.59 46.88 64. 50.22 50.24 48.96 50.38 49.58 50.94 49.85 49.93 50.16 28.58 28.84 28.84 28.84 28.24 29.61 30.29 28.50 30.38 59.18 59.64 59.18 60.61 58.75 58.96 59.60 59.05 60.69 49.07 48.27 48.01 48.01 48.29 48.93 48.41 48.69 48.46 PIQA Wino. Avg. acc acc acc 70.84 71.44 70.18 71.27 70.13 71.55 71.33 72.25 71. 55.09 52.80 50.83 51.38 51.07 51.85 54.06 53.91 54.78 52.16 51.87 51.00 51.75 51.01 51.97 52.26 52.06 52.65 Alternative architectures for self-decoder in SambaY? As shown in Table 5, while SambaY performs well on the PB-32K benchmark, replacing its Mamba layers with Mamba-2 leads to significant drop in accuracy. This may be attributed to Mamba-2s coarse, scalar-valued forget gates, which can reduce the self-decoders capacity to encode fine-grained positional information. The weaker PB-32K performance of MambaY compared to SambaY underscores the importance of local retrieval ability provided by SWA; recency bias alone appears insufficient for the self-decoder to support the cross-decoder in completing complex retrieval tasks. While GDN-based models achieve impressive PB-32K accuracy due to their enhanced retrieval capabilities with delta update rules, interleaving GDN with short-range SWA notably accelerates training without significantly degrading performance on either short or long-context tasks. Is GMU effective for other memories besides SSMs? We examine SambaY-A and SambaYAA, which gate attention inner representations, and SambaY-MLP, which gates MLP intermediate representations. As shown in Table 5, these variants achieve respectable performance on downstream tasks, with SambaY-MLP even surpassing the original SambaY on average accuracy for short-context tasks. However, for the long-context task, PB-32K, we observe clear hierarchy: SambaY > SambaYMLP > SambaY-A > SambaY-AA. This pattern indicates that GMUs remain effective with alternative memory sources, but their performance on retrieval tasks depends significantly on the memory sources inherent characteristics. Gating attention/MLP representations performs worse than the original SambaY on Phonebook because they lack the recency bias that SSMs naturally provide, which is beneficial for retrieving contiguous information. SambaY-AA, which completely removes cross-attention, shows significant degradation, highlighting the importance of having enough number of cross-attention layers for the successful retrievals from large pool of multiple key-value pairs."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduce the Gated Memory Unit (GMU), simple yet effective mechanism for efficient memory sharing across layers in sequence models. Replacing expansive cross attention layers with GMUs, we propose SambaY, decoder-hybrid-decoder architecture with Samba as the self-decoder, which achieves significant improvements in both computation efficiency and longcontext performance. Our extensive scaling experiments demonstrate that SambaY exhibits lower irreducible loss compared to strong baselines, indicating superior scaling properties with increasing computational resources. Our largest model, Phi4-mini-Flash-Reasoning, outperforms Phi4-miniReasoning on challenging reasoning benchmarks while delivering substantially higher decoding throughput on long-context generations. Given that our architecture still retains full attention layer with linear decoding complexity, future work could explore dynamic sparse attention mechanisms to further improve efficiency on extremely long sequence generation, particularly in agentic application scenarios."
        },
        {
            "title": "Acknowledgement",
            "content": "We want to thank Yutao Sun, Li Dong, Songlin Yang and Yang Liu for helpful discussions and insights. We also want to thank Yi Zhu for an early version of the vLLM implementation of YOCO."
        },
        {
            "title": "References",
            "content": "[ALTdJ+23] J. Ainslie, J. Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit K. Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. Conference on Empirical Methods in Natural Language Processing, 2023. [ANZ22] [AON+21] [Art] [BBC+25] Ibrahim Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. Revisiting neural scaling laws in language and vision. Advances in Neural Information Processing Systems, 35:2230022312, 2022. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large language models. arXiv preprint arXiv: 2108.07732, 2021. Art of Problem Solving. Aime problems and solutions. https://artofproblemsolving.com/ wiki/index.php/AIME_Problems_and_Solutions. Accessed: 2025-04-20. Johan Bjorck, Alon Benhaim, Vishrav Chaudhary, Furu Wei, and Xia Song. Scaling optimal LR across token horizons. In The Thirteenth International Conference on Learning Representations, 2025. [BDK+24] Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining neural scaling laws. Proceedings of the National Academy of Sciences, 121(27):e2311878121, 2024. [Bes] [BKH16] Bespoke Labs. Bespoke-Stratos-7B. Accessed: 2025-06-18. Bespoke-stratos-7b. https://huggingface.co/bespokelabs/ Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. arXiv preprint arXiv: 1607.06450, 2016. [BMN+24] William Brandon, Mayank Mishra, Aniruddha Nrusimha, Rameswar Panda, and Jonathan Ragan Kelly. Reducing transformer key-value cache size with cross-layer attention. arXiv preprint arXiv: 2405.12981, 2024. [BPC20] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv: Arxiv-2004.05150, 2020. [BPL+25] Maximilian Beck, Korbinian Pöppel, Phillip Lippe, Richard Kurle, Patrick M. Blies, Günter Klambauer, Sebastian Böck, and Sepp Hochreiter. xlstm 7b: recurrent llm for fast and efficient inference. arXiv preprint arXiv: 2503.13427, 2025. [BPS+24] Maximilian Beck, Korbinian Poppel, M. Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, G. Klambauer, Johannes Brandstetter, and Sepp Hochreiter. xlstm: Extended long short-term memory. Neural Information Processing Systems, 2024. [BZB+20] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning about physical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 74327439. AAAI Press, 2020. [CCE+18] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv: 1803.05457, 2018. [CCWG22] Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. SummScreen: dataset for abstractive screenplay summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 86028615, Dublin, Ireland, May 2022. Association for Computational Linguistics. [CKB+21] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv: 2110.14168, 2021. [DA24a] DeepSeek-AI. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv: 2405.04434, 2024. [DA24b] DeepSeek-AI. Deepseek-v3 technical report. arXiv preprint arXiv: 2412.19437, 2024. [DA25] [Dao23] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv: 2501.12948, 2025. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv: 2307.08691, 2023. [DFD+25] Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, Zijia Chen, A. Mahabaleshwarkar, ShihYang Liu, Matthijs Van Keirsbilck, Min-Hung Chen, Yoshi Suhara, Yingyan Celine Lin, Jan Kautz, and Pavlo Molchanov. Hymba: hybrid-head architecture for small language models. International Conference on Learning Representations, 2025. [DG24] [DLB+21] [DSF+24] Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. dataset of information-seeking questions and answers anchored in research papers. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, page 45994610, Online, June 2021. Association for Computational Linguistics. Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv: 2402.19427, 2024. [DYL+24] Haojie Duanmu, Zhihang Yuan, Xiuhong Li, Jiangfei Duan, Xingcheng Zhang, and Dahua Lin. Skvq: Sliding-window key and value cache quantization for large language models. arXiv preprint arXiv:2405.06219, 2024. [EUD17] Stefan Elfwing, E. Uchibe, and K. Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural Networks, 2017. [FDS+22] Daniel Fu, Tri Dao, Khaled Saab, Armin Thomas, Atri Rudra, and Christopher Ré. Hungry hungry hippos: Towards language modeling with state space models. arXiv preprint arXiv:2212.14052, 2022. [GD23] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [GGGR22] Albert Gu, Ankit Gupta, Karan Goel, and Christopher Ré. On the parameterization and initialization of diagonal state space models. ARXIV.ORG, 2022. [GGR21] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. International Conference On Learning Representations, 2021. [GOA+24] Daniel Goldstein, Fares Obeid, Eric Alcaide, Guangyu Song, and Eugene Cheah. Goldfinch: High performance rwkv/transformer hybrid with linear pre-fill and extreme kv-cache compression. arXiv preprint arXiv: 2407.12077, 2024. [GPD+25] Xiangming Gu, Tianyu Pang, Chao Du, Qian Liu, Fengzhuo Zhang, Cunxiao Du, Ye Wang, and Min Lin. When attention sink emerges in language models: An empirical view. In The Thirteenth International Conference on Learning Representations, 2025. [GSF+25] Riccardo Grazzi, Julien Siems, Jörg K.H. Franke, Arber Zela, Frank Hutter, and Massimiliano Pontil. Unlocking state-tracking in linear RNNs through negative eigenvalues. In The Thirteenth International Conference on Learning Representations, 2025. [GWYC24] Tianyu Gao, Alexander Wettig, Howard Yen, and Danqi Chen. How to train long-context language models (effectively). arXiv preprint arXiv: 2410.02660, 2024. [HBB+21] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. [HBD+19] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. International Conference on Learning Representations, 2019. [HBK+21] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. 12 [HBM+22] Jordan Hoffmann, Sebastian Borgeaud, A. Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, K. Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and L. Sifre. Training compute-optimal large language models. ARXIV.ORG, 2022. [HDLL22] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V. Le. Transformer quality in linear time. International Conference On Machine Learning, 2022. [HNA+17] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv: 1712.00409, 2017. [HSK+24] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. Ruler: Whats the real context size of your long-context language models? arXiv preprint arXiv: 2404.06654, 2024. [HTH+24] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zheng Leng Thai, Kaihuo Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv: 2404.06395, 2024. [HTW+24] Connor Holmes, Masahiro Tanaka, Michael Wyatt, Ammar Ahmad Awan, Jeff Rasley, Samyam Rajbhandari, Reza Yazdani Aminabadi, Heyang Qin, Arash Bakhtiari, Lev Kurilenko, and Yuxiong He. Deepspeed-fastgen: High-throughput text generation for llms via mii and deepspeed-inference. arXiv preprint arXiv: 2401.08671, 2024. [JBKM24] Samy Jelassi, David Brandfonbrener, S. Kakade, and Eran Malach. Repeat after me: Transformers are better than state space models at copying. International Conference on Machine Learning, 2024. [KKL20] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020. [KLZ+23] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Haotong Zhang, and I. Stoica. Efficient memory management for large language model serving with pagedattention. Symposium on Operating Systems Principles, 2023. [KMH+20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv: 2001.08361, 2020. [LBOM12] Yann A. LeCun, Léon Bottou, Genevieve B. Orr, and Klaus-Robert Müller. Efficient BackProp, pages 948. Springer Berlin Heidelberg, 2012. [LCF+24] [LEV44] [LH18] Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv: 2406.11939, 2024. KENNETH LEVENBERG. method for the solution of certain non-linear problems in least squares. Quarterly of Applied Mathematics, 2(2):164168, 1944. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2018. [LLB+24] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida, Amir Bergman, Roman Glozman, Michael Gokhman, Avashalom Manevich, Nir Ratner, Noam Rozen, Erez Shwartz, Mor Zusman, and Yoav Shoham. Jamba: hybrid transformer-mamba language model. arXiv preprint arXiv: 2403.19887, 2024. [LLSS24] Wonbeom Lee, Jungi Lee, Junghwan Seo, and Jaewoong Sim. InfiniGen: Efficient generative inference of large language models with dynamic KV cache management. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), pages 155172, Santa Clara, CA, July 2024. USENIX Association. [LZH+25] Houyi Li, Wenzhen Zheng, Jingcheng Hu, Qiufeng Wang, Hanshan Zhang, Zili Wang, Shijie Xuyang, Yuantao Fan, Shuigeng Zhou, Xiangyu Zhang, and Daxin Jiang. Predictable scale: Part - optimal hyperparameter scaling law in large language model pretraining. arXiv preprint arXiv: 2503.04715, 2025. [MAA+25] Microsoft, Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, Dong Chen, Dongdong Chen, Junkun Chen, Weizhu Chen, Yen-Chun Chen, Yi ling Chen, Qi Dai, Xiyang 13 Dai, Ruchao Fan, Mei Gao, Min Gao, Amit Garg, Abhishek Goswami, Junheng Hao, Amr Hendy, Yuxuan Hu, Xin Jin, Mahmoud Khademi, Dongwoo Kim, Young Jin Kim, Gina Lee, Jinyu Li, Yunsheng Li, Chen Liang, Xihui Lin, Zeqi Lin, Mengchen Liu, Yang Liu, Gilsinia Lopez, Chong Luo, Piyush Madan, Vadim Mazalov, Arindam Mitra, Ali Mousavi, Anh Nguyen, Jing Pan, Daniel Perez-Becker, Jacob Platin, Thomas Portet, Kai Qiu, Bo Ren, Liliang Ren, Sambuddha Roy, Ning Shang, Yelong Shen, Saksham Singhal, Subhojit Som, Xia Song, Tetyana Sych, Praneetha Vaddamanu, Shuohang Wang, Yiming Wang, Zhenghao Wang, Haibin Wu, Haoran Xu, Weijian Xu, Yifan Yang, Ziyi Yang, Donghan Yu, Ishmam Zabir, Jianwen Zhang, Li Lyna Zhang, Yunan Zhang, and Xiren Zhou. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras, 2025. Donald W. Marquardt. An algorithm for least-squares estimation of nonlinear parameters. Journal of the Society for Industrial and Applied Mathematics, 11(2):431441, 1963. MetaAI. Introducing meta llama 3: The most capable openly available llm to date, 2024. URL: https://ai.meta.com/blog/meta-llama-3/. Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. Leave no context behind: Efficient infinite context transformers with infini-attention. arXiv preprint arXiv:2404.07143, 101, 2024. MiniMax. Minimax-01: Scaling foundation models with lightning attention. arXiv preprint arXiv: 2501.08313, 2025. [Mar63] [Met24] [MFG24] [Min25] [MKAT18] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model of large-batch training. arXiv preprint arXiv: 1812.06162, 2018. [MLPA22] Sadhika Malladi, Kaifeng Lyu, Abhishek Panigrahi, and Sanjeev Arora. On the SDEs and scaling rules for adaptive gradient algorithms. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. [MXBS16] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. [MYX+24] Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, and Chunting Zhou. Megalodon: Efficient llm pretraining and inference with unlimited context length. Advances in Neural Information Processing Systems, 37:7183171854, 2024. [MZK+22] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. Mega: Moving average equipped gated attention. arXiv preprint arXiv:2209.10655, 2022. [Ope] [Ope24] [PAA+23] Open Thoughts. OpenThinker-7B. Accessed: 2025-06-18. Openthinker-7b. https://huggingface.co/open-thoughts/ OpenAI. Openai o1 system card. arXiv preprint arXiv: 2412.16720, 2024. Bo Peng, Eric Alcaide, Quentin G. Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, G. Kranthikiran, Xuming He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, J. S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, P. Zhou, Jian Zhu, and Rui Zhu. Rwkv: Reinventing rnns for the transformer era. Conference on Empirical Methods in Natural Language Processing, 2023. [PFF+23] Jonathan Pilault, Mahan Fathi, Orhan Firat, Chris Pal, Pierre-Luc Bacon, and Ross Goroshin. Block-state transformers. Advances in Neural Information Processing Systems, 36:73117329, 2023. [PKL+16] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Q. N. Pham, R. Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and R. Fernández. The lambada dataset: Word prediction requiring broad discourse context. Annual Meeting of the Association for Computational Linguistics, 2016. [QHS+22] Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. The devil in linear transformer. Conference on Empirical Methods in Natural Language Processing, 2022. [RHS+23] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: graduate-level google-proof q&a benchmark. arXiv preprint arXiv: 2311.12022, 2023. [RLL+25] Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba: Simple hybrid state space models for efficient unlimited context language modeling. In The Thirteenth International Conference on Learning Representations, 2025. [RLW+23] Liliang Ren, Yang Liu, Shuohang Wang, Yichong Xu, Chenguang Zhu, and ChengXiang Zhai. Sparse modular activation for efficient sequence modeling. NEURIPS, 2023. [RWC+19] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. arXiv preprint, 2019. [SAKM+23] Daria Soboleva, Faisal Al-Khateeb, Robert Myers, tness, cated version of slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama. Joel HesA 627b token cleaned and dedupliURL: https://www.cerebras.net/blog/ Jacob Steeves, and Nolan Dey. Slimpajama: redpajama, 2023. [SBBC21] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. [SDH+23] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023. [Sha19] [Sha20] [SLP+21] [SDZ+24] Yutao Sun, Li Dong, Yi Zhu, Shaohan Huang, Wenhui Wang, Shuming Ma, Quanlu Zhang, Jianyong Wang, and Furu Wei. You only cache once: Decoder-decoder architectures for language models. Neural Information Processing Systems, 2024. Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv: 1911.02150, 2019. Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv: 2002.05202, 2020. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv: 2104.09864, 2021. [SWW+24] Xian Shuai, Yiding Wang, Yimeng Wu, Xin Jiang, and Xiaozhe Ren. Scaling law for language models training considering batch size. arXiv preprint arXiv: 2412.01505, 2024. Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Neural Information Processing Systems, 2024. [TJY+24] [TMS+23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv: 2307.09288, 2023. Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. NIPS, 2017. Xi Wang and Laurence Aitchison. How to set adamws weight decay as you scale model and dataset size. arXiv preprint arXiv: 2405.13698, 2024. [VSP+17] [WA24] [WBR+24] Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika Singh, Jared Casper, Jan Kautz, Mohammad Shoeybi, and Bryan Catanzaro. An empirical study of mamba-based language models. arXiv preprint arXiv: 2406.07887, 2024. Kaiyue Wen, Xingyu Dang, and Kaifeng Lyu. Rnns are not transformers (yet): The key bottleneck on in-context retrieval. arXiv preprint arXiv: 2402.18510, 2024. [WDL24] [WLX+24] Mitchell Wortsman, Peter Liu, Lechao Xiao, Katie Everett, Alexander Alemi, Ben Adlam, John Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, Jeffrey Pennington, Jascha Sohl-Dickstein, Kelvin Xu, Jaehoon Lee, Justin Gilmer, and Simon Kornblith. Small-scale proxies for large-scale transformer training instabilities. In The Twelfth International Conference on Learning Representations, 2024. [WMZ+24] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv: 2406.01574, 2024. 15 [WRHS22] Yuhuai Wu, Markus N. Rabe, DeLesley S. Hutchins, and Christian Szegedy. Memorizing transformers. International Conference On Learning Representations, 2022. [WT24] Haoyi Wu and Kewei Tu. Layer-condensed kv cache for efficient inference of large language In Proceedings of the 62nd Annual Meeting of the Association for Computational models. Linguistics (Volume 1: Long Papers), pages 1117511188, 2024. [WWS+22] [WWZ+24] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, E. Chi, F. Xia, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. Neural Information Processing Systems, 2022. Jialong Wu, Zhenglin Wang, Linhai Zhang, Yilong Lai, Yulan He, and Deyu Zhou. Scope: Optimizing key-value cache compression in long-context generation. arXiv preprint arXiv:2412.13649, 2024. [XMW+24] Mingyu Xu, Xin Men, Bingning Wang, Qingyu Zhang, Hongyu Lin, Xianpei Han, and Weipeng Chen. Base of rope bounds context length. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. [XPA+25] Haoran Xu, Baolin Peng, Hany Awadalla, Dongdong Chen, Yen-Chun Chen, Mei Gao, Young Jin Kim, Yunsheng Li, Liliang Ren, Yelong Shen, Shuohang Wang, Weijian Xu, Jianfeng Gao, and Weizhu Chen. Phi-4-mini-reasoning: Exploring the limits of small reasoning language models in math. arXiv preprint arXiv: 2504.21233, 2025. [XYH+20] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. On layer normalization in the transformer architecture. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 1052410533. PMLR, 2020. [YCQ+21] Yu Yan, Jiusheng Chen, Weizhen Qi, Nikhil Bhendawade, Yeyun Gong, Nan Duan, and Ruofei Zhang. El-attention: Memory efficient lossless attention for generation. In International Conference on Machine Learning, pages 1164811658. PMLR, 2021. [YDX+24] Tianzhu Ye, Li Dong, Yuqing Xia, Yutao Sun, Yi Zhu, Gao Huang, and Furu Wei. Differential transformer. arXiv preprint arXiv:2410.05258, 2024. [YHB+22] Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer. arXiv preprint arXiv: 2203.03466, 2022. [YKH25] Songlin Yang, Jan Kautz, and Ali Hatamizadeh. Gated delta networks: Improving mamba2 with delta rule. In The Thirteenth International Conference on Learning Representations, 2025. [YWS+23] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [YWZ+24] Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length. Neural Information Processing Systems, 2024. [YYZH23] Greg Yang, Dingli Yu, Chen Zhu, and Soufiane Hayou. Tensor programs vi: Feature learning in infinite-depth neural networks. International Conference on Learning Representations, 2023. [ZHB+19] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? Annual Meeting of the Association for Computational Linguistics, 2019. [ZLJ+22] [ZS19] [ZVC+25] Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Charles, Eren Manavoglu, Tuo Zhao, and Jianfeng Gao. Efficient long sequence modeling via state space augmented transformer. arXiv preprint arXiv: 2212.08136, 2022. Biao Zhang and Rico Sennrich. Root mean square layer normalization. Neural Information Processing Systems, 2019. Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen Gong, James Hoang, Armel Randy Zebaze, Xiaoheng Hong, Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, Prateek Yadav, and et al. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025."
        },
        {
            "title": "A Theoretical Analysis",
            "content": "Token mixing as matrix operator. Both state-space models (SSMs) and self-attention layers perform token mixing through linear operator that can be written as matrix Rnn, where is the sequence length. In SSMs, is highly structured matrix that captures the parallel form of an underlying recurrent update, whereas in self-attention is the row-aggregating attention matrix whose entries are the query-key softmax probabilities. For given head at layer l, the mixed representation is m(l) = A(l) v(l), where v(l) Rnd denotes either the SSM state inputs or the attention value vectors. Gated Memory Unit inserted at later layer > recalibrates this mixing by applying an element-wise gating matrix Rnd with sequence length and the channel dimension d. For each element hik in the gated output Rnd , we have Gik A(l) ij x(l) jk hik = Gik (cid:88) ij x(l) A(l) jk = (cid:88) (cid:88) = A(l) ij Gik (cid:125) (cid:123)(cid:122) (cid:124) Aijk x(l) jk , which shows that the gate effectively lifts A(l) into third-order tensor Aijk = GikA(l) ij , yielding learned, channel-specific reweighting of the original token-mixing operator while maintaining linearity in the original value vectors at layer l. Normalization placement in linear attention. In linear attention architectures such as Gated DeltaNet (GDN) [YKH25], normalization operator is applied immediately after token mixing to stabilize training at layer l: m(l) = Norm(cid:0)A(l)v(l)(cid:1), y(l) = (cid:0)m(l) G(l)(cid:1)W (l) where Norm is typically RMSNorm [ZS19] and x(l) is the layer input and y(l) is the layer output. Placing Norm before the output gating, however, breaks the associativity between the gating matrix G(l) and the token-mixing operator A(l), so the Gated Memory Unit (GMU) can no longer directly re-weight the token mixing based on the current layer input with x(l). To resolve this issue, we propose to postpone the Norm after output gating for GDN (denoted as GDN-A), following the design of Mamba-2 [DG24]. Concretely, for layer we instead compute G(l) = σ(W (l) 1 x(l)), , m(l) = A(l)v(l), y(l) = Norm(cid:0)m(l) G(l)(cid:1) (l) 2 , and employ the normalized GMU (nGMU) at layer > in the cross-decoder to maintain training stability while allowing the gate to modulate A(l) directly, i.e. y(l) = Norm(cid:0)m(l) G(l)(cid:1) (l) 2 . This simple reordering preserves associative re-weighting and, as demonstrated empirically in Appendix H, substantially improves the long context performance when compared to the original normalization before gating design in GDN."
        },
        {
            "title": "B Additional Aspect Ratio Calculations",
            "content": "Based on the Samba+YOCO architecture, we can derive the iso-parametric equation through calculating the number of non-embedding parameters as follows, Nattn(d) = 2.5dw wattn/4 + 2dw wattn/2, Nmamba(d) = 6dw2/4, (d) = Nattn(d) + Nmamba(d) + Nmlp(d) = 208αd3 + 13.5α2d3 = 237568d3. Solving for α, we get α2 126. For the SambaY+DA architecture, the aspect ratio is not changed because the number of extra parameters introduced by DA is negligible. For MambaY, we have Nattn(d) = 2dw wattn/4, Nmamba(d) = 6dw2/2, Ngmu(d) = 4dw2/4, 17 (d) = Nattn(d) + Nmamba(d) + Nmlp(d) + Ngmu(d) = 64αd3 + 16α2d3 = 237568d3. Solving for α, we get α3 120. For SambaY-MLP, we have Nattn(d) = 2.5dw wattn/4 + 2dw wattn/4, Nmamba(d) = 6dw2/4, Ngmu(d) = 8dw2/4, (d) = Nattn(d) + Nmamba(d) + Nmlp(d) + Ngmu(d) = 144αd3 + 15.5α2d3 = 237568d3. Solving for α, we get α4 120. For SambaY-Attn, we have Nattn(d) = 2.5dw wattn/4 + 2dw wattn/4, Nmamba(d) = 6dw2/4, Ngmu(d) = 2dw wattn/4, (d) = Nattn(d) + Nmamba(d) + Nmlp(d) + Ngmu(d) = 208αd3 + 13.5α2d3 = 237568d3. Solving for α, we get α5 126, which is the same as Samba+YOCO. For SambaY-Attn-All, we similarly have Nattn(d) = 2.5dw wattn/4, Nmamba(d) = 6dw2/4, Ngmu(d) = 2dw wattn/2, (d) = Nattn(d) + Nmamba(d) + Nmlp(d) + Ngmu(d) = 208αd3 + 13.5α2d3 = 237568d3. Solving for α, we get α6 126. For the GDNY architecture, we use fixed head dimension of 256 and SwiGLU output gating with the Gated DeltaNet layers. The query-key projections with 0.75 expansion ratio are used, while the value, gating, and output projections are using 1.5 expansion ratio with respect to the model width. We also allow negative eigenvalues for improved expressiveness of the transition matrices [GSF+25]. Specifically, we have Nattn(d) = 2dw wattn/4, NGDN(d) = 6dw2/2, Ngmu(d) = 3dw2/4, (d) = Nattn(d) + NGDN(d) + Nmlp(d) + Ngmu(d) = 64αd3 + 15.75α2d3 = 237568d3. Solving for α, we get α7 120. As in SambaY, we can similarly solve for S-GDNY to get α8 126. Ablation Study on Hyper-parameter Scaling Laws Table 6: Key differences between µP, µP++ and Standard Parameterization (SP). LR mult. denotes the per-parameter multiplier applied on top of the global learning-rate (η), Res. mult. is the multiplier applied to the output of residual branches and WD denotes the weight decay. For µP++, η 1/ and zero weight decay is also applied to other scalar or vector-like parameters such as RMSNorm weights. In this work, σ = 104 for untied embedding and σ = 0.02 for tied embedding, and in both cases τ = 0.02 and β = 1. fan_in means the input dimension of weight matrices. Parameter Scheme LR mult. Initialization Res. mult. Weight mult. WD Embedding Unembedding Hidden Weights SP µP µP++ SP µP µP++ SP µP µP++ 1 1 1 1 1 1 1 1/w 1/w (0, σ2) (0, σ2) (0, σ2) 0 or tied 0 or tied 0 or tied (0, τ 2) β , β , U( β U( β fan_in fan_in fan_in fan_in 1 1 1/ 2d ) ) 1 1 1 1/w 1/w 1 1 1 1 1 0 1 1 0 1 1 1 We conduct comprehensive ablation study of our µP++ scaling laws to validate their scaling behavior. All experiments are performed using Transformer++ trained with 4K sequence length on the SlimPajama dataset. To ensure that the linear learning rate schedule fully decays to zero, we train six models at different training token budgets: {100B, 200B, . . . , 600B} for each of the scaling curves. We examine the scaling performance under both tied and untied embedding setups. For the untied setting, we follow RWKV [PAA+23] by applying normal initialization with zero mean and standard deviation of 104. The unembedding layer is initialized to zero, following the zero-out trick proposed in µP [YHB+22]. As shown in Figure 6a, we observe that the original µP setup (which uses LeCun initialization and does not include Depth-µP or weight decay modifications as in µP++) can 18 (a) Tied Embedding (b) Untied Embedding Figure 6: Validation Loss v.s. Training Tokens on the SlimPajama dataset for Transformer++ trained with tied (left) or untied (right) embedding layers. For the training on 600B tokens with µP, the model encountered NaN losses after 204K gradient update steps. We report the last valid validation loss prior to divergence as its final performance. lead to severe training instability when scaling to 600B tokens. Since we observe increasing gradient norms with large spikes for vector-like parameters shortly before the model diverges, this highlights the importance of the µP++ strategy of applying zero weight decay to vector-like parameters to enhance training stability at large scales. We also explore batch size scaling with respect to training token size, following [SWW+24, LZH+25], i.e. = (cid:114) T0 . As in Figure 6a, µP++ (Batch Scaling) shows both worse learning efficiency and irreducible loss than µP++. Generally, we think the batch size mainly affects parallelism and the computation efficiency as long as the batch size is not too large, and the worse scaling behavior can be because (1) when scaling up, the batch size can surpass the critical batch size [MKAT18], which leads to worse model performance, (2) other optimizer hyper-parameters are not adjusted accordingly with batch size as in [MLPA22] and we leave it for future works to study the large batch size training with µP++. We also try using Normal Initialization with 0.02 standard deviation for the weight matrices, and scale the variance with respect to 1/d. However, µP++ (Normal Init.) shows worse scaling than µP++, indicating that it is better to adjust the initialization multipliers based on each matrix dimension as adopted by LeCun initialization, rather than global factor related to model width. We explore integrating the empirical scaling law of the learning rate η scaling with respect to training tokens [BBC+25] to µP++, i.e., (cid:114) Bd0 B0d and adjust weight decay to maintain the same regularization effect across different training tokens with the setup of Independent Weight Decay [WLX+24], i.e., (cid:18) T0 η = η (cid:19) 1 , 3 λ = λ0 η0 η , where λ is the weight decay in AdamW [LH18] and λ0 = 0.1. We denote this scaling law as µP++ (LR scaling + Indep. WD). As in Figure 6b, while the irreducible loss is comparable, we observe worse learning efficiency with smaller compared to µP++. We think that future work is needed to have an empirical study of the learning rate scaling with respect to dataset size under µP++, instead of transferring the empirical law directly to our theoretical laws. We also explore using the WSD [HTH+24] learning rate scheduler for µP++, where we set the final decay period to be 2/7 of the total period following [DA24b]. Unfortunately, it depicts worse scaling behavior than µP++ with linear learning rate schedule, as shown in Figure 6b. Interestingly, when comparing the performance of µP++ with tied versus untied embeddings, we observe that µP++ with untied embeddings achieves significantly lower validation loss with 100B training tokens, but its irreducible loss remains 19 comparable to that of tied embeddings. This suggests that the additional parameters from untied embeddings primarily accelerate training convergence without improving the final model performance if sufficient amount of data is given."
        },
        {
            "title": "D Additional Details on Scaling Comparisons",
            "content": "Table 7: Model and training configurations for the architectures explored in this work. TransformerLS adopts the same architecture as Transformer++, with Sliding Window Attention (SWA) applied to all attention layers except every fourth layer, which uses full attention. MLP Size denotes the intermediate dimension of the MLP, i.e., the input dimension of the output projection. Phi4-mini and Phi4-mini-Flash are trained with batch size of 8M tokens, using linear learning rate schedule with 3,000 warm-up steps. We allow the intermediate dimension of attention layers to be larger than the model width, so that the head dimension can be power of 2. Variants enhanced with Differential Attention adopt the same architectural configurations as their respective baselines. All models use tied embeddings. The 3.8B-parameter SambaY and Samba+YOCO models are randomly initialized for benchmarking under the vLLM inference framework. Except for 3.8B-parameter models which use vocabulary of 200K tokens, we apply Llama-2 [TMS+23] tokenizer with 32K vocabulary for all other models. Architecture Transformer++ SambaY Samba+YOCO MambaY GDNY S-GDNY MambaY-2 SambaY-2 SambaY-A SambaY-AA SambaY-MLP Phi4-mini Pih4-mini-Flash SambaY Samba+YOCO Depth Model Query KV Head MLP Non-Embed Params Learning Training Tokens (B) Width Heads Heads Dim Size Params (M) Rate (M) 8 12 16 20 8 12 16 20 24 8 12 16 20 24 16 16 16 16 16 16 16 16 32 32 32 32 1024 1536 2048 2560 3072 992 1488 1984 2480 1008 1512 2016 2520 3024 1920 1920 2016 1920 1984 2016 2016 1920 3072 2560 2560 2560 8 12 16 20 24 8 12 16 20 24 8 12 16 20 16 16 16 16 16 16 16 16 24 40 40 40 2 3 4 5 6 2 3 4 5 6 2 3 4 5 6 4 4 4 4 4 4 4 8 20 20 20 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 64 64 4096 6144 8192 10240 12288 3968 5952 7936 9920 11904 4032 6048 8064 10080 12096 7680 7680 7680 7680 7936 8064 8064 7680 8192 10240 10240 10240 121.6 410.5 973.1 1900.5 3284. 123.3 416.1 986.3 1926.5 3328.9 123.2 415.6 985.2 1924.3 3325.1 975.2 960.4 1001.0 975.2 986.3 985.2 985.2 985.0 3221.2 3329.2 3329.2 3224.4 154.4 459.7 1038.6 1982.5 3382.4 155.0 463.7 1049.8 2005.8 3424. 155.4 464.0 1049.7 2004.9 3421.9 1036.6 1021.9 1065.5 1036.6 1049.8 1049.7 1049.7 1046.4 3835.8 3841.4 3841.4 3736.5 5.66e-04 4.62e-04 4.00e-04 3.58e-04 3.27e-04 5.66e-04 4.62e-04 4.00e-04 3.58e-04 3.27e-04 5.66e-04 4.62e-04 4.00e-04 3.58e-04 3.27e4.00e-04 4.00e-04 4.00e-04 4.00e-04 4.00e-04 4.00e-04 4.00e-04 4.00e-04 5.00e-04 5.00e-04 - - 12.5 42.2 100.0 195.3 337.5 12.7 42.8 101.4 198.0 342.1 12.7 42.7 101.2 197.8 341.7 40.0 40.0 40.0 40.0 40.0 40.0 40.0 40. 5000 5000 - - All models are trained with 4K sequence length for drawing the scaling curves. For Standard Parameterization (SP), we dont apply any µP++ scaling laws, and since LeCun initialization already scales its initialization variance with respect to 1/din as the same as proposed in µP, where din is the fan-in dimension of the weight matrix, we instead use normal initialization with standard deviation of 0.02 for weight matrices to rule out this confounding effect. We divide the initialization standard 2d, following [RWC+19, GD23, RLL+25]. deviation of the output projection of each layer by The detailed architecture and optimization setups for each of the scales are shown in Table 7. Following [GD23, YWZ+24, RLL+25, YKH25], our downstream evaluations are conducted on the following benchmarks: Wikitext [MXBS16], LAMBADA (LMB) [PKL+16], Arc-Easy/Challenge (ARC-e/ARC-c) [CCE+18], HellaSwag (Hella.) [ZHB+19], WinoGrande (Wino.) [SBBC21] and 20 PIQA [BZB+20], where we measure character normalized accuracy (acc_n) for Arc-Challenge and HellaSwag. Additional Long-context Retrieval Experiments (a) SlimPajama (b) ProLong-64K Figure 7: Accuracy (with error bars) v.s. Sliding Window Size on Phonebook with 32K evaluation length using 40B training tokens from SlimPajama (left) or ProLong-64K (right). As an ablation to Figure 3, variable-length training is not applied for both setting. Figure 7 illustrates how different model architectures perform on the Phonebook long-context task as the sliding window size increases, using either SlimPajama or ProLong-64K for pre-training with 32K sequence length and without variable-length training. Specifically, we concatenate the data samples with EOS tokens as separation to form 32K length training sequences. On SlimPajama, overall accuracy is modest, with SambaY+DA showing some initial promise at smaller window sizes (peaking at 128) before declining, while Samba+YOCO performs best at moderate window size of 512. Transformer-based models generally struggle to achieve competitive accuracy across window sizes. Notably, reducing RoPE base from 640K to 10k for TransformerLS significantly harms the performance across window sizes. Switching to the ProLong-64K dataset leads to notable performance boost across all architectures compared to SlimPajama, even without variable-length training. We can observe that SSM-based models enjoy larger boosts on accuracies than Transformer++. This indicates that SSM-based models can learn to switch contexts between different data samples within the packed sequences more easily than pure attention models. Notably, SambaY+DA achieves competitive accuracy using smaller sliding window (512), matching the performance of Samba+YOCO at larger window sizes. While Samba+YOCO continues to benefit from increasing window sizes, reaching peak accuracy at 2048, SambaY+DA demonstrates greater efficiency by achieving strong results with smaller sliding window size. Given that variable-length training on ProLong-64K generally yields better results as in Figure 3, these fixed-length training results indicate that while ProLong-64K benefits long-context performance, the full potential, especially for pure attention models that are sensitive to sliding window size (e.g. TransformerLS), can be further unlocked by training methodologies that explicitly account for varying sequence lengths of each data sample. The different optimal sliding window sizes and performance trajectories underscore that both the pre-training dataset and the training methodology significantly influence how effectively the training context length can be utilized for long-context pre-training. More Details on Architecture and Large-scale Pre-training We provide comprehensive summary of the architectures explored in this work, along with the large-scale pre-training setup, in Table 7. In our architectures, Differential Attention uses depthdependent initialization factor, λinit = 0.8 0.6 exp(0.3 l), where is the depth index. For each attention head, it employs two sets of learnable parameters, (λq1, λk1) and (λq2, λk2), each of dimension equal to the head dimension and initialized with normal distribution of zero mean and 0.1 standard deviation. RMSNorm [ZS19] with learnable element-wise affine parameters is adopted 21 for attention output normalization. For each of the intermediate layers, LayerNorm [BKH16] is used with Pre-LN [XYH+20] for Phi4-mini-Flash architecture."
        },
        {
            "title": "G Additional Details on Efficiency and Reasoning Results",
            "content": "Following Phi4-mini-Reasoning [XPA+25], the evaluation is conducted with sampling temperature of 0.6, top-p [HBD+19] value of 0.95, and maximum sequence length of 32,768 tokens. We leverage the Math-Verify library4 (version 0.7.0) and Lighteval5 (version 0.10.0) to enable efficient and robust evaluation on reasoning tasks. We prepend the instruction: Please reason step by step, and put your final answer within boxed{{}}. for the evaluation on AIME24/25 and MATH500 and Please reason step by step, and put your final choice of one letter from A/B/C/D within boxed{{}}. for the evaluation on GPQA Diamond. Figure 8: Generation latencies at length of 1K, 2K, 4K, 8K, 16K and 32K for prompt length of 2000. Given certain generation length, we measure the average latency of all the requests in all the loads of 1, 2, 4, 8, 16 concurrent requests. The data-points of latency and throughput in Figure 4 are measured by varying the concurrency levels of the requests sent to the vLLM server, ranging from {1, 2, 4, 8, 16}. We also compare the latency of our Phi4-mini-Flash architecture with Phi4-mini across various generation lengths in Figure 8. We can see that our model can achieve faster generation speeds starting from generation length as small as 4K. Apart from the sub-optimal Differential Attention implementation, our vLLM implementation is also sub-optimal in calculating unnecessary full attention operations with the following MLP layer for the user prompt tokens during the prefill stage, leaving additional opportunities for optimizing the efficiency results in Figure 4 and Figure 8. We conduct case studies for Phi4-mini-Flash-Reasoning in Example 1 and Example 2 for both mathematical reasoning and daily-life reasoning. Notably, while our model is only trained on distillation data with math problems, we can see our model exhibits advanced reasoning techniques such as double checking (e.g. Alternatively, maybe could have completed the square instead?, Breaking it down:), reflection (e.g. That means there are no real solutions, right? , Wait, maybe miscounted.), and backtracking (e.g. But the problem might be expecting complex solutions., Wait, maybe Im overcomplicating.) in both cases. Surprisingly, in Example 2, our model will even guess the user intention (e.g. But wait, sometimes people might confuse the number of Rs in double letter. ) to finally provide the correct answer. Example 1: Mathematical Reasoning (Part 1) Query: solve xˆ2+3x+4= Models reasoning process: Think Okay, lets see. need to solve the equation squared plus 3x plus 4 equals 1. Hmm, so first, maybe should get all the terms on one side so that the equation equals zero. That usually makes it easier to solve quadratic equations. Let me subtract 1 from both sides. That would give me x² + 3x + 4 - 1 = 0. Simplifying that, 4 minus 1 is3, so the equation becomes x² + 3x + 3 = 0. To be continued... 4https://github.com/huggingface/Math-Verify 5https://github.com/huggingface/lighteval Example 1: Mathematical Reasoning (Part 2) Models reasoning process (continued): Think 3)/2. (b² - 4ac)] / (2a). Alright, now have quadratic equation in standard form, which is ax² + bx + = 0. Here, is 1, is 3, and is 3. To solve this, can use the quadratic formula, which is equals negative plus or minus the square root of squared minus four ac all over 2a. Let me write that down: = [-b Plugging in the values from my equation, is 3, so negative would be -3. Then, squared is 3 squared, which is 9.Then, 4ac is 4 times 1 times 3, which is 12. So the discriminant, which is the part under the square root, would be 9 - 12. That equals -3. Wait, so the discriminant is negative. That means there are no real solutions, right? Because you cant take the square root of negative number in the real number system. But the problem might be expecting complex solutions. Let me check. If complex numbers are allowed, then the square root of -3 can be written as times the square root of 3,where is the imaginary unit. So, substituting back into the quadratic formula, we have equals [-3 (-3)] / (2*1). Which simplifies to = [-3 3]/2. Let me double-check my steps to make sure didnt make mistake. Starting equation: x² + 3x + 4 = 1. Subtract 1: x² + 3x + 3 = 0. Coefficients a=1, b=3, c=3. Discriminant: 3² - 4*1*3 = 9 - 12 = -3. Yep, thats right. So the solutions are complex: (-3 Alternatively, maybe could have completed the square instead? Let me try that method to verify. Starting with x² + 3x + 3 = 0. Move the constant term to the other side: x² + 3x = -3. To complete the square, take half of the coefficient of x, which is 3/2, square it to get (9/4). Add that to both sides: x²+ 3x + 9/4 = -3 + 9/4. The left side becomes (x + 3/2)². The right side: -3 is -12/4, so -12/4 + 9/4 is -3/4. So (x + 3/2)² = -3/4. Taking the square root of both sides gives + 3/2 = 3)/2. Which is the same as (-3 Therefore, both methods lead to the same complex solutions. Since the original equation has negative discriminant, there are no real solutions, only complex ones. The answer should be expressed in terms of complex numbers. Just to be thorough, let me plug one of the solutions back into the original equation to verify. Lets take = [-3 + First, compute x²: [(-3 + 3)ˆ2 = (-3)ˆ2 + 2*(-3)*(i (-3 + 3 -3 = 6 - 6i 9 - 6i Next, compute 3x: 3*[(-3 + Add x² + 3x + 4: (3 - 3i 3]/2 + 4 = (-6)/2 + 4 = -3 + 4 = 1. Which matches the right side of the original equation. So the solution checks out. Similarly, if plug in the other solution = [-3 - imaginary parts would cancel out as well, resulting in 1. Therefore, the solutions are correct. So the final answer is two complex numbers: (-3 3)/2. think thats all. The key steps were moving all terms to one side, calculating the discriminant, recognizing its negative, and proceeding with complex solutions. Both quadratic formula and completing the square worked, and verifying by substitution confirmed the answers are correct. 3]/2. Plugging into x² + 3x + 4. 3)/2]ˆ2. Lets expand that. 3)ˆ2 = 9 - 6i 3)/2. So that matches the quadratic formula result. 3)/2. Subtract 3/2 from both sides: = -3/2 (i 3 + (i²*( 3)/4 = (3 - 3i 3]/2, the process would be the same, and the 3)/2 + 4. Combine the fractions: [3 - 3i 3. Then divide by 4: (6 - 6i 3)ˆ2) = 9 - 6i 3)/2. 3)/2] = (-9 + 3i 3)/2 + (-9 + 3i 3 + (-1)*3 = (3/4) = i( (-3/4) = 3 -9 + 3i 3) + (i 3)/2. Models Response: To solve the equation x2 + 3x + 4 = 1: 1. **Rearrange the equation** to standard quadratic form: x2 + 3x + 4 1 = 0 = x2 + 3x + 3 = 0 2. **Identify coefficients**: = 1, = 3, = 3. 3. **Calculate the discriminant**: = b2 4ac = 32 4(1)(3) = 9 12 = 3 Since the discriminant is negative, solutions are complex. To be continued... 23 Example 1: Mathematical Reasoning (Part 3) Models Response (continued): 4. **Apply the quadratic formula**: = 2a 3 2 = 3 = 3 3 2 5. **Verify via completing the square**: x2 + 3x = 3 = (cid:18) + (cid:19)2 3 2 = 3 4 Solving gives = 2 **Final Answer**: The solutions are 3 2 , confirming the solutions. 3 2 3 2 . Example 2: Letter Counting in Misspelled Words (Part 1) Query: how many rs in \"strawberrry\"? Models reasoning process: Think Okay, lets see. The question is asking how many times the letter \"r\" appears in the word \"strawberrry\". Hmm, first need to make sure spell the word correctly. The user wrote \"strawberrry\". Let me check that. Normally, the plural of \"strawberry\" is \"strawberries\", but maybe they made typo or spelling mistake. Anyway, the word given here is \"strawberrry\". Let me count the letters one by one. Breaking it down: s-t-r-a-w-b-e-r-r-r-y. Lets write each letter separately: S, T, R, A, W, B, E, R, R, R, Y. Wait, so starting from the beginning: (1), (2), (3), (4), (5), (6), (7), (8), (9), (10), (11). So total letters are 11. Now, how many times does appearhere? Looking at the letters: positions 3, 8, 9, 10. Thats four Rs. Let me count again to be sure. S-T-R-A-W-B-E-R-R-R-Y. So after comes R, then another R, another R, and then Y. So thats three Rs in row? Wait, no. Lets see: after is the first R, then the second R, third R, and then Y. Wait, no, the word is \"strawberrry\". So after E, there are three Rs in row, right? Because its written as \"berrry\". So breaking down: s-t-r-a-w-b-e-r-r-r-y. So after E, there are three Rs before Y. So thats three Rs. Wait, but when listed them earlier, counted four Rs. Let me check again. Wait, maybe miscounted. Lets write out each letter with their positions: 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. So positions 3,8,9,10 are Rs. Thats four Rs. But wait, the word is spelled as \"strawberrry\". So after \"straw\" and \"b\", its \"errry\". So \"e\", then three Rs, then \"y\". So thats three Rs. But according to the breakdown above, there are four Rs. Wait, maybe the user added an extra by mistake? Let me check the exact spelling again. The user wrote \"strawberrry\". So \"berrr\". So \"b\", then \"e\", then three Rs, then \"y\". So thats three Rs. But in my positional list, have four Rs. Wait, maybe made an error in counting. Wait, lets write the word step by step: (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) So letters 3,8,9,10 are Rs. Thats four Rs. But the user wrote \"strawberrry\", which would be \"berrr\" at the end. So \"b\", \"e\", \"rrr\", \"y\". So three Rs. But according to the breakdown, its four. Wait, maybe the user intended \"strawberry\" but misspelled it as \"strawberrry\". Let me confirm. The correct plural of \"strawberry\" is \"strawberries\", which has two Rs. But the user wrote \"strawberrry\", which has three Rs in row. So perhaps the answer is three? Or four? To be continued... 24 Example 2: Letter Counting in Misspelled Words (Part 2) Models reasoning process (continued): Think Wait, maybe Im overcomplicating. Lets just take the word as given: \"strawberrry\". Lets count each letter: s, t, r, a, w, b, e, r, r, r, y. So thats 11 letters. The Rs are at positions 3,8,9,10. So four Rs. Therefore, the answer should be 4. But wait, maybe the userintended \"strawberry\" with two Rs, but wrote \"strawberrry\" by accident. But the question is about the given word, so we have to go with that. So in \"strawberrry\", there are four Rs. Wait, but let me check again. Lets write the word out: a r Yes, after the E, there are three Rs. So thats three Rs. Wait, but in the list above, positions 3,8,9,10. Wait, position 3 is the third letter, which is R. Then after (position 7), positions 8,9,10 are Rs, and Yis 11. So thats three Rs after the E. But position 3 is an as well. So total Rs are four. Wait, but in the word \"strawberrry\", the first is in \"straw\", then after \"straw\", theres \"b\", \"e\", then three Rs. So total Rs would be 1 (from \"straw\") plus 3 (from \"berrry\") equals 4. So the answer is 4. But maybe Im miscounting. Let me write it again: (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) Yes, thats four Rs. So the answer is4. But maybe the user intended different count. Alternatively, maybe the word is spelled differently. Wait, \"strawberry\" is spelled s-t-r-a-w-b-er-r-y, which has two Rs. But the user wrote \"strawberrry\", which adds an extra R, making it three Rs in row. Wait, no: \"strawberrry\" would be s-t-r-a-w-b-e-r-r-r-y, so after \"ber\" there are three Rs. So total Rs: in \"straw\" theres one R, then \"berrry\" has three Rs. So total 1 + 3 = 4. Therefore, four Rs. So the answer is4. But need to make sure. Let me check the exact spelling again. The user wrote \"strawberrry\". Lets break it down: straw + berry, but with rrrr instead of rr. So \"straw\" has one R, \"berry\" normally has two Rs, but here its written as \"rrrr\", which would be three Rs? Wait, no. Wait, \"berry\" is b-e-r-r-y. So two Rs. If its written as \"berrry\", thats replacing the double with triple R? Wait, no. Let me think. The correct plural is \"strawberries\", which is s-t-r-a-w-s-b-e-r-r-i-e-s. But perhaps the user made typo. However, the question is about the given word \"strawberrry\". So regardless of the correct spelling, we have to count the Rs in that exact spelling. So \"strawberrry\" is s-t-r-a-w-b-e-r-r-r-y. Lets count the Rs: 1. The in \"straw\" (third letter) 2. The in \"ber\" (eighth letter) 3. The next (ninth) 4. The next (tenth) So four Rs. Therefore, the answer is 4. But maybe Im missing something. Let me check oncemore. Lets write each letter with its case, but the problem didnt specify case sensitivity, so assuming all lowercase aswritten. So the letters are s, t, r, a, w, b, e, r, r, r, y. So Rs at positions 3,8,9,10. Four of them. So the answer should be 4. But wait, sometimes people might confuse the number of Rs in double letter. For example, \"berrry\" might be considered as three Rs, but in this case, the word is \"strawberrry\" with three Rs after \"ber\". So yes, three Rs there. Plus the one in \"straw\" makes four. So the answer is 4. think thats correct. Models Response: The word \"strawberrry\" is spelled as s-t-r-a-w-b-e-r-r-r-y. Breaking it down: 1. **s** 2. **t** 3. **r** 4. **a** 5. **w** 6. **b** 7. **e** 8. **r** 9. **r** 10. **r** 11. **y** The letter **r** appears at positions 3, 8, 9, and 10, totaling **4** occurrences."
        },
        {
            "title": "H Additional Ablation Study",
            "content": "How does normalization placement and nGMU affect the model performances3? Table 8 reveals consistent pattern: retaining the nGMU and applying RMSNorm after the output gating is critical for long-context retrieval performance. In contrast, shifting the normalization before the gate and replacing nGMU with the simpler GMU (NB + GMU rows) leaves short-context benchmarks largely unaffected but leads to severe performance degradation on PB-32K across all linear-attention variants. For example, PB-32K accuracy drops by 56.3 points for S-GDNY (from 25 Table 8: Ablation study results on Phonebook 32K (PB-32k), language modeling and common-sense reasoning tasks in zero-shot for 1B-parameter models with sliding window size of 128. The average accuracy does not include PB-32K. NB means applying normalization before output gating in the last Mamba-2 layer of self-decoder, GMU means replacing nGMU with GMU and using the memory after normalization but before output gating. Model SambaY-2 w/ NB + GMU MambaY-2 w/ NB + GMU S-GDNY w/ GDN + GMU GDNY w/ GDN + GMU Speed Wiki. PB-32K LMB. ARC-c ARC-e Hella. acc_n acc mtps acc_n acc acc ppl 1.43 1. 1.38 1.35 1.34 1.33 1.22 1.24 17.17 17.76 18.63 16.99 16.78 16. 16.92 16.87 40.63 21.88 50.78 17.19 83.59 27.34 89.84 54.69 48.96 49. 49.58 49.76 50.94 51.08 50.38 50.30 28.84 29.69 28.24 27.39 29.61 28. 28.84 27.73 59.18 59.68 58.75 58.46 58.96 57.49 60.61 60.48 48.01 48. 48.29 48.43 48.93 48.25 48.01 47.91 PIQA Wino. Avg. acc acc acc 70.18 71.22 70.13 70. 71.55 69.15 71.27 70.62 50.83 52.17 51.07 50.28 51.85 53.04 51.38 52. 51.00 51.83 51.01 50.76 51.97 51.20 51.75 51.53 83.6 to 27.3) and by 35.1 points for GDNY (from 89.8 to 54.7), despite minimal changes (3%) in Wiki perplexity, zero-shot commonsense scores, and throughput. These results underscore the importance of maintaining the associativity between gating and token mixing by (1) normalizing after the output gating and (2) using memory before normalization with nGMU for achieving effective long-range retrieval performance with linear attention layers in self-decoder."
        },
        {
            "title": "I Related Work",
            "content": "KV Cache Sharing. Efficient inference in transformer-based models has been significantly advanced through techniques that reduce memory consumption, particularly concerning key-value (KV) caching. Traditional approaches like Multi-Query Attention (MQA) [Sha19] and Grouped-Query Attention (GQA) [ALTdJ+23] have enabled multiple query heads to share single key/value head within the same layer, effectively reducing the number of distinct key/value heads with minimal impact on accuracy. Apart from YOCO [SDZ+24], Cross-Layer Attention (CLA) [BMN+24] extends KV sharing across adjacent layers, achieving up to two times reduction in KV cache size while maintaining performance. In contrast, our work studies representation sharing across SSM/RNN layers, and proposes to directly share the output from the SSM kernel to avoid materializing recurrent states, thereby preserving the parallel training efficiency of linear recurrent models. Efficient Long Generation. Efficient long-sequence generation in transformer models has been focus of recent research on LLM efficiency, primarily due to the substantial memory demands associated with key-value (KV) caching during inference with long CoTs [KKL20, WWZ+24, YCQ+21, DYL+24]. To address these challenges, several techniques have been proposed to optimize memory usage without compromising model performance. One notable approach is the LayerCondensed KV Cache (LCKV) [WT24], which computes and caches KV pairs for only subset of layers, significantly reducing memory consumption and improving inference throughput. Another advancement is InfiniGen [LLSS24], dynamic KV cache management framework that selectively prefetches essential KV cache entries, thereby mitigating fetch overhead from host memory in offloading-based LLM serving systems. These methods collectively contribute to more efficient long-sequence generation by optimizing KV cache usage, and are orthogonal to our work, as we can also apply these techniques to improve the memory I/O efficiency of our full attention layer. Hybrid Neural Architectures. Recent hybrid models have explored combining different types of token mixing operatorsincluding Sliding Window Attention (SWA) [BPC20], full self-attention [VSP+17] and SSMs/RNNseither in an inter-layer [FDS+22, DSF+24, LLB+24, RLL+25, Min25] or an intra-layer manner [MZK+22, RLW+23, PFF+23, MYX+24, MFG24, DFD+25]. As typical design of intra-layer hybridization, the efficiency of hybrid-head architecture [WRHS22, ZLJ+22, MFG24, DFD+25] is bottlenecked by the slowest token-mixing head, resulting in theoretically lower GPU utilization than inter-layer hybridization. Samba [RLL+25], an inter-layer hybrid model that interleaves Mamba with SWA, achieves improved extrapolation perplexity on extremely long sequences while maintaining linear complexity. However, its zero-shot retrievable 26 context length remains limited to its sliding window size. The decoder-decoder architecture, YOCO [SDZ+24], proposes to use linear complexity modules (either SSMs or SWA) in the first half of the layers with single full attention in the middle, and reuse the kv cache of the middle full attention layers for the second half of the attention layers. It shows comparable performance on retrievable context length as the full attention models while providing much more efficient linear pre-filling complexity. This design also offers unique advantage that allows skipping inference computation in half of the total layers at the prefill stage, yielding substantial efficiency gainseven for short sequences, where MLPs dominate the computational cost. Our proposed GMU module opens up new opportunities for the pure RNN-based models to be YOCO-compatible, potentially mitigating the significant overhead that linear RNNs typically incur on short sequences. Neural Scaling Laws. Understanding how model performance scales with size and data is crucial for efficient and effective large-scale training. Empirical studies have shown that Transformer models exhibit predictable scaling behaviors, where performance improves with increased model parameters and training data [HNA+17, KMH+20, BDK+24, ANZ22, HBM+22]. Numerous works have also investigated scaling laws for hyper-parameters, based on either empirical studies [BBC+25, WLX+24] or theoretical analyses [MLPA22, YHB+22, YYZH23, WA24]. In this work, we focus on theoretical hyper-parameter scaling laws since they are not over-tuned for the Transformer architectures, and fairer comparisons can be made for the emerging neural architectures. We also conduct extensive scaling experiments with large-scale compute to verify the empirical effectiveness of these theoretical scaling laws. In doing so, we find an improved version of the original µP [YHB+22] that accounts for scaling of depth, width, and training stability, and demonstrate that it provides better scaling behavior in both data and compute scaling scenarios. More importantly, we introduce principled approach for comparing the scaling behaviors of different neural architectures by solving iso-parametric equations, providing solid foundation for evaluating the scaling potential of future architectures."
        },
        {
            "title": "J Limitation",
            "content": "We validate our models reasoning capability using distillation-based Supervised Fine-Tuning (SFT), but Reinforcement Learning (RL) remains under-explored in the context of hybrid architectures. Due to resource constraints, we do not perform an exhaustive hyperparameter search for each architecture. Instead, we adopt generic optimization setup based on Transformer++ for learning rate, initializer range, weight decay, warm-up schedule, batch size, AdamW betas and epsilon, and other parameters. It is likely that aggressive tuning of these optimization settings could yield improved results. We leave more comprehensive study of the interplay between optimization setups and architecture designs for future work. Lastly, our architecture still includes full-attention layer, which leads to linear per-token computation complexity during decoding. This underscores future research direction on designing models for extremely long sequence generation that can maintain constant decoding complexity while effectively leveraging long-context memory."
        }
    ],
    "affiliations": [
        "Microsoft",
        "Stanford University"
    ]
}