{
    "paper_title": "EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech Emotion Detection",
    "authors": [
        "Christoph Schuhmann",
        "Robert Kaczmarczyk",
        "Gollam Rabby",
        "Felix Friedrich",
        "Maurice Kraus",
        "Kourosh Nadi",
        "Huu Nguyen",
        "Kristian Kersting",
        "Sören Auer"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The advancement of text-to-speech and audio generation models necessitates robust benchmarks for evaluating the emotional understanding capabilities of AI systems. Current speech emotion recognition (SER) datasets often exhibit limitations in emotional granularity, privacy concerns, or reliance on acted portrayals. This paper introduces EmoNet-Voice, a new resource for speech emotion detection, which includes EmoNet-Voice Big, a large-scale pre-training dataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions, and 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human expert annotations. EmoNet-Voice is designed to evaluate SER models on a fine-grained spectrum of 40 emotion categories with different levels of intensities. Leveraging state-of-the-art voice generation, we curated synthetic audio snippets simulating actors portraying scenes designed to evoke specific emotions. Crucially, we conducted rigorous validation by psychology experts who assigned perceived intensity labels. This synthetic, privacy-preserving approach allows for the inclusion of sensitive emotional states often absent in existing datasets. Lastly, we introduce Empathic Insight Voice models that set a new standard in speech emotion recognition with high agreement with human experts. Our evaluations across the current model landscape exhibit valuable findings, such as high-arousal emotions like anger being much easier to detect than low-arousal states like concentration."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 2 7 2 8 9 0 . 6 0 5 2 : r EMONET-VOICE: Fine-Grained, Expert-Verified Benchmark for Speech Emotion Detection Christoph Schuhmann LAION e.V. christoph.schuhmann@laion.ai Robert Kaczmarczyk LAION e.V. Technical University of Munich Gollam Rabby L3S Research Center Leibniz University of Hannover Felix Friedrich TU Darmstadt Hessian.AI Maurice Kraus TU Darmstadt Kourosh Nadi LAION e.V. Huu Nguyen Ontocord LAION e.V. Kristian Kersting TU Darmstadt Centre for Cognitive Science Hessian.AI DFKI Sören Auer TIBLeibniz Information Centre for Science and Technology L3S Research Center Leibniz University of Hannover"
        },
        {
            "title": "Abstract",
            "content": "The advancement of text-to-speech and audio generation models necessitates robust benchmarks for evaluating the emotional understanding capabilities of AI systems. Current speech emotion recognition (SER) datasets often exhibit limitations in emotional granularity, privacy concerns, or reliance on acted portrayals. This paper introduces EMONET-VOICE, new resource for speech emotion detection, which includes EMONET-VOICE BIG, large-scale pre-training dataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions, and 4 languages), and EMONET-VOICE BENCH, novel benchmark dataset with human expert annotations. EMONET-VOICE is designed to evaluate SER models on fine-grained spectrum of 40 emotion categories with different levels of intensities. Leveraging state-of-the-art voice generation, we curated synthetic audio snippets simulating actors portraying scenes designed to evoke specific emotions. Crucially, we conducted rigorous validation by psychology experts who assigned perceived intensity labels. This synthetic, privacy-preserving approach allows for the inclusion of sensitive emotional states often absent in existing datasets. Lastly, we introduce EMPATHICINSIGHT-VOICE models that set new standard in speech emotion recognition with high agreement with human experts. Our evaluations across the current model landscape exhibit valuable findings, such as high-arousal emotions like anger being much easier to detect than low-arousal states like concentration."
        },
        {
            "title": "Introduction",
            "content": "Synthetic speech technology has reached unprecedented fidelity, with state-of-the-art text-to-speech (TTS) and audio generation models, e.g., GPT-4 OmniAudio [25], achieving prosody, timbre, and expressiveness comparable to humans. These advancements significantly enhance human-computer interaction (HCI), enabling virtual assistants to convey appropriate emotional qualities across diverse Contributed equally and jointly supervised this project. 2links to our data, models and code Preprint. Under review. contexts [14]. However, this advancement remains asymmetric: while machines can to some extent effectively synthesize convincing affective speech, they still struggle to recognize the nuanced, contextdependent emotional information humans naturally convey [15, 34], critical capability for truly conversational AI. Despite steady progress in speech emotion recognition (SER) through deep architectures and selfsupervised representations, evaluation remains constrained by datasets predominantly built around limited set of basic emotions [8, 38]. Established benchmarks such as IEMOCAP [3], RAVDESS [19], and CREMA-D [4] have been invaluable for the field but exhibit three fundamental limitations: (i) Insufficient Granularity. Coarse taxonomies fail to capture subtle or compound emotional states (e.g., bittersweet, embarrassment, envy) that are essential for naturalistic interaction [5]. (ii) Limited Representativeness. Current datasets predominantly consist of studio-quality acted speech, lacking linguistic diversity and omitting sensitive emotional states due to privacy constraints [20, 33]. (iii) Restricted Scalability. Licensing restrictions, privacy concerns, and annotation costs severely limit dataset size, impeding the data-intensive training regimes required by modern deep learning approaches [37, 28], specifically for open-source and -science. These limitations are further compounded by evolving perspectives in affective science. Constructionist theories, particularly Barretts Theory of Constructed Emotion [1], conceptualize emotions as context-dependent constructions rather than universal biological packages. This perspective aligns with dimensional frameworks such as Russells valencearousal circumplex [30] and supports multilabel approaches that treat affect as overlapping estimates rather than discrete categories [22, 23]. Consequently, SER research must evolve along two parallel trajectories: developing richer datasets with evaluation protocols that respect emotional complexity, and creating modeling strategies that go beyond simplistic classification paradigms. To address these challenges, we introduce two complementary datasets. First, EMONET-VOICE BIG, foundational dataset for pretraining models on SER. It is comprehensive synthetic voice corpus exceeding 4,500 hours in four languages (English, German, Spanish, French), featuring 11 distinct voices with different gender identities and fine-grained taxonomy of 40 emotion categories. As such, it provides an open, privacy-compliant foundation for emotional TTS research and multilingual speech analysis at scale. Second, from this corpus we curate EMONET-VOICE BENCH, comprising 12,600 audio clips annotated by psychology experts using strict consensus protocol that evaluates both the presence and intensity of each target emotion across our 40-category emotion taxonomy. This approach yields high-quality, multilingual benchmark for fine-grained SER while circumventing the privacy barriers that inhibit the collection of authentic sensitive vocal expressions. Building on these datasets, we develop EMPATHICINSIGHT-VOICE (Small and Large), novel SER models that achieve state-of-the-art performance in fine-grained emotion recognition while demonstrating strong alignment with human expert judgments. Through comprehensive evaluation across the concurrent SER model landscape, we reveal critical insights into current SER capabilities, including systematic patterns in which emotions prove more challenging to recognize (e.g., lowarousal states like concentration versus high-arousal emotions like anger). In summary, our contributions are four-fold: (1) We build EMONET-VOICE BIG, pretraining, openaccess, 4,500-hour multilingual synthetic speech corpus featuring 11 distinct synthetic voices across 4 languages and 40 emotion categories. (2) We introduce EMONET-VOICE BENCH, meticulously curated and expert-verified benchmark dataset of 12,600 high-quality audio samples for fine-grained SER, featuring 40 emotion categories with 3 intensity levels. (3) We build EMPATHICINSIGHTVOICE (Small and Large), novel SER models designed for nuanced emotion estimation. (4) We conduct comprehensively evaluations on our novel benchmark, providing critical insights into current SER capabilities and limitations."
        },
        {
            "title": "2 Related Work",
            "content": "Current SER research operates on constrained empirical foundation. The field still relies on small set of acted corpora recorded in controlled studio conditionsIEMOCAP (12 h, 9 emotions) [3], RAVDESS (1 h, 8 emotions, speech & song) [19], SAVEE (0.8 h, 7 emotions, four male speakers) [12], the German EMODB [2], and the multi-ethnic CREMA-D [4]. While these corpora 2 Table 1: Comparison of SER datasets. Key aspects include licensing, scale, emotional range, speaker diversity, synthetic origin, and multilingual support. Open license means CC-BY 4.0 or equivalent; var. means varies across pooled corpora."
        },
        {
            "title": "Size",
            "content": "(#Utts/Hours) #Emo. #Spk. Synth. Multilin."
        },
        {
            "title": "Dataset",
            "content": "IEMOCAP [3] RAVDESS [19] SAVEE [12] EmoDB [2] CREMA-D [4] SERAB [31] EmoBox [21] SER Evals [26] BERSt [36] Open Licence 10k / 12h 9 1.4k / 1h 8 480 / <1h 7 535 / <1h 7 7.4k / 6h 6 9 corpora / var. 6 32 corpora / var. 8 18 corpora / var. 8 6 4h 10 (5M/5F) 24 (12M/12F) 4 (Male) 10 (5M/5F) 91 (48M/43F) var. var. var. 98 EMONET-VOICE BIG EMONET-VOICE BENCH >1M / >4,500h 12k / 35.8h 40 40 11 (Synth) 11 (Synth) provide clean labels and high acoustic quality, they share four persistent weaknesses. First, they use restrictive taxonomiestypically Ekmans six basic emotions [8]omitting compound or socially nuanced states such as embarrassment, envy, or contemplation [27, 5]. Second, their acted prosody exaggerates emotional cues and reduces generalization to spontaneous speech [20]. Third, privacy and ethics hinder collection of intimate or stigmatizing emotions (e.g. shame, desire, grief) [33]. Fourth, scale and linguistic diversity remain limited: most corpora contain < 100 speakers, just few hours of audio, and are largely English-centric. Recent efforts to expand this foundation include early multilingual sets such as EMOREACT and the parallel EnglishMandarin ESD, which broaden language coverage but still cap labels at six basic categories [24, 39]. Aggregation benchmarks go furtherSERAB pools nine legacy corpora in six languages [31]; EMOBOX widens the scope to 32 datasets in 14 languages with turnkey evaluation splits [21]; SER EVALS organises 18 minoritylanguage corpora into inand out-of-domain test beds for robustness analysis [26]; and BERST collects 4 of shouted and distanced English speech from 98 actors at 19 smartphone positions [36]. Yet these resources still inherit the core constraints of their sources: acted or scripted speech, narrow taxonomies ( 8 emotions), modest duration per language, and lack of expert-validated intensity labels or sensitive affective states. These existing datasets, summarized and contrasted with our contributions in Table 1, highlight clear gap. While valuable, they are often restricted by licensing, limited in scale (both in total hours and number of utterances), offer narrow range of emotion categories (typically 9 or fewer), rely on human actors which limits the privacy-preserving access to sensitive emotions, and many lack multilingual support. EMONET-VOICE BIG and EMONET-VOICE BENCH directly address these shortcomings by providing large-scale, openly licensed, synthetic, multilingual corpus with significantly expanded emotion taxonomy. Taxonomic limitations exacerbate data-scarcity and theoretical gaps. Modern affective science models emotions as context-dependent and graded rather than discrete [1, 18]. Dimensional (valencearousaldominance) and multi-label schemes [30, 37] better capture blended affect, yet almost all benchmarks still assign single discrete label per clip. When intensity annotations exist, they typically rely on crowdsourcing and show low agreement [13, 35]. Consequently, the community lacks benchmarks that reflect contemporary understanding of emotion as multidimensional and graded, particularly for sensitive affective states that cannot be ethically collected from human participants. Expert-validated intensity annotations across multidimensional affective spaces are missing from existing benchmarks, and we fill this critical gap by contributing EMONET-VOICE BENCH with 12,600 carefully chosen clips whose emotional presence and intensity we had annotated by psychology experts, yielding high-agreement subset. We overcome the taxonomic, scale, and ethical limitations of existing corpora by combining broad multilingual coverage, 40-category taxonomy grounded in contemporary affective science [6, 1], and privacy-preserving synthetic speech generation, offering the first benchmark that provides expert-validated ratings across multidimensional affective space. 3 Table 2: Overview of EMONET-VOICE BIG Table 3: Overview of EMONET-VOICE BENCH Category Hours Category Value Playtime by Language English (en) German (de) Spanish (es) French (fr) Acting Chal. (en+de) total English Accent Distribution"
        },
        {
            "title": "Louisiana\nValley Girl\nBritish\nChinese\nFrench\nGerman\nIndian\nItalian\nMexican\nRussian\nSpanish\nTexan\nVulgar Street\nNo accent specified",
            "content": "2,156 716 888 881 111 4,752 133 159 132 126 140 135 129 134 131 134 132 131 149 391 Number of Clips English (en) German (de) Spanish (es) French (fr) Total Clips Avg. Clip Duration Total Playtime 6,156 (48.9%) 1,886 (15.0%) 2,193 (17.4%) 2,365 (18.8%) 12,600 10.36 36.26 Table 4: Number of voice audios annotated by human experts across batches for EMONETVOICE BENCH. Mainly samples with at least positive weak agreement (emotion weakly / strongly present annotated by two human experts) were used in next batch. Batch Unique Human Annotators Annotated Voice Audios 1 2 3 2 3 4 4,538 7,"
        },
        {
            "title": "3 The EMONET-VOICE Suite: Dataset Construction",
            "content": "This section describes how we built the EMONET-VOICE resources, beginning with our emotion taxonomy, followed by the creation of the large-scale dataset EMONET-VOICE BIG, and concluding with the expert-validated EMONET-VOICE BENCH subset used for final evaluation. Lastly, we introduce EMPATHICINSIGHT-VOICE models setting new standard in SER. 3.1 Emotion Taxonomy For EMONET-VOICE, we adopt the comprehensive 40-category emotion taxonomy originally developed for EMONET-FACE [32]. The taxonomy includes diverse set of categories spanning positive emotions (e.g., Elation, Contentment, Affection, Awe), negative emotions (e.g., Distress, Sadness, Bitterness, Contempt), cognitive states (e.g., Concentration, Confusion, Doubt), physical states (e.g., Pain, Fatigue), and socially mediated emotions (e.g., Embarrassment, Shame, Pride, Teasing). This fine-grained structure enables the evaluation of models beyond binary or basic categorical classification. The full set of 40 emotion categories and their descriptive terms can be found in App.A.1. comprehensive description of the methodology used to construct the taxonomy, including literature-based extraction and expert-guided refinement, is provided in App.A.4. 3.2 EMONET-VOICE BIG: Building large-scale synthetic SER Dataset The foundational dataset, EMONET-VOICE BIG, consists of emotionally expressive speech samples synthesized using the GPT-4 OmniAudio model3. An overview of EMONET-VOICE BIGs scale and language distribution is provided in Table 2. Our prompting strategy cast the model as an actor auditioning for film, tasked with performing texts designed to evoke one of 40 emotion categories (from the taxonomy in Section 3.1). Key prompt elements included directives for strong emotional expression from the outset and naturalistic human speech patterns (e.g., varied rhythm, volume, tone, and appropriate vocal bursts). This aimed to ensure perceptible emotional content and avoid monotonous delivery. Audio was generated as 3to 30-second, 24kHz WAV files, utilizing 11 synthetic voices (6 female/5 male) across English, German, French, and Spanish to build diverse 3Accessed via the HyperLab API. 4 Figure 1: Annotator agreement for human ratings on perceived emotions in audio samples. Stacked horizontal bars display the proportion of audio-emotion instances for each emotion, categorized by agreement type. These categories include full agreement on emotion presence (e.g., 3:0 (+), 2:0 (+)), partial agreement where presence is favored (e.g., 2:1 (+ favored)), disagreement (e.g., 1:1), partial agreement where absence is favored (e.g., 1:2 (- favored)), and full agreement on emotion absence (e.g., 2:0 (-), 3:0 (-)). Instances with other rating configurations are grouped under Other. The numbers to the right of each bar indicate the total number of instances (n) for that emotion, along with the percentage of these instances rated by two (%2r) or three (%3r) annotators; Other denotes instances with four annotators. The annotation process ensured all audio-emotion pairs were initially rated by two annotators. If both these annotators marked an emotion as present (rating > 0), the instance was subsequently rated by third annotator. Additionally, random subset of instances received fourth annotation. multilingual corpus. The full prompting template and detailed methodology, including the importance of specific instructions and language-specific adaptations for vocal burst generation, are presented in the Supplement. 3.3 EMONET-VOICE BENCH: Human Expert Benchmark for SER From EMONET-VOICE BENCH, we created subset of 12,600 unique audio files annotated for emotion by human experts on three-point annotation scale, summarized in Table 3. We depict the annotation platform for our human experts in Appendix Figures 2 and 3. The dataset features 11 distinct synthetic voices (6 female and 5 male) across four languages: English (48.9%), German (15.0%), Spanish (17.4%), and French (18.8%). The average clip duration is 10.36 seconds, resulting in total playtime of 36.26 hours. Table 4 summarizes our annotation procedure. Ensuring the quality and reliability of the emotion annotations was central priority in constructing the EMONET-VOICE BENCH. We recruited team of six human experts with at least Bachelors degree in Psychology to serve as benchmark annotators, thereby guaranteeing familiarity with emotional theory and terminology. In total, 33,605 singleemotion labels across 12,600 unique audio samples were contributed some samples ultimately received more than three annotations. Each audio clip was first labeled independently by two experts who were presented with the audio alongside one specific target emotion category from our taxonomy in addition to three-point scale: 0 indicating the emotion was not perceived, 1 indicating it was 5 mildly present at low intensity, and 2 indicating it was intensely present and clearly perceptible. If both human experts agreed that the emotion was present (either weakly present or strongly present), the clip was sent to third expert for confirmation. Additionally, we randomly selected subset of clips to receive third or even fourth annotation regardless of whether the first two annotators agreed. To reduce potential gender biases in emotional perception, each group assigned per snippet was balanced in gender composition. Importantly, annotators performed their assessments independently and were blinded to the ratings of others. Figure 1 illustrates inter-annotator agreement patterns across emotion categories, showing the distribution of full agreement, partial agreement, and disagreement for each emotion-audio pair. The numbers alongside each bar indicate total instances and rating distributions across multiple annotators. The analysis reveals clear consensus patterns: emotions like concentration and bitterness achieve strong expert agreement, while others such as numbness and awe show notable disagreement even among psychology professionals. The overall inter-rater reliability measured by Cronbachs α is 0.14 (95% CI [0.12, 0.15]), with per-emotion values detailed in Appendix Table 9. While this low α might initially suggest poor reliability, it actually reflects the inherent complexity of fine-grained emotion perception rather than annotation deficiencies. Unlike simpler emotion taxonomies, our 40-category framework captures subtle distinctions that legitimately evoke different interpretations among experts. These patterns demonstrate that while human agreement is robust for many emotions, certain categories naturally elicit diverse interpretationsunderscoring the nuanced nature of affective expression in speech. Rather than indicating weak annotation quality, this variability highlights EMONET-VOICEs sensitivity to the inherent complexity of emotional perception. Our annotations thus capture both the challenges and opportunities in modeling authentic emotional diversity at scale. 3.4 EMPATHICINSIGHT-VOICE: Training state-of-the-art SER models Another contribution of this work, based on the datasets we built, is to establish novel state-of-the-art speech emotion recognition model. First linear probing experiments as well as previous works [17, 7] show that the off-the-shelf Whisper encoders [29] are not capable of reflecting on emotionsan essential capability for emotion-aware audio generation and captioning. Specifically, at fine-grained level, existing TTS models fail to recognize emotions effectively, as we will discuss later. To address this limitation, we continually pre-trained Whisper encoders as the backbone of our EMPATHICINSIGHT-VOICE. Specifically, we leverage EMONET-VOICE BIG as pretraining dataset and train emotion-experts in two stages. We base our experiments on Whisper-Small to optimize for the performance-efficiency tradeoff. In the first stage, the Whisper encoder is trained on combination of EMONET-VOICE BIG and another 4,500 hours of public emotion-related content4 to develop general emotional acoustic representations. This data was annotated using an iterative process with Gemini Flash 2.0 to obtain emotion scores (04 scale) for all audio snippets. In the second stage, we freeze the Whisper encoder and train MLP expert headsone per emotion dimensionon top of the fixed encoder embeddings. This way, each MLP receives the full voice audio sequence from the Whisper encoder as sequence flattened token embeddings and then regresses single emotion intensity score. We propose two model sizes to accommodate different performance requirements, namenly EMPATHICINSIGHT-VOICE SMALL with 74M paremeter MLP heads and EMPATHICINSIGHT-VOICE LARGE with 148M paremeter MLP heads. We optimize them using mean absolute error (MAE) on the Gemini Flash 2.0generated emotion scores. Through this two-stage fine-tuning and dedicated MLP ensemble, EMPATHICINSIGHT-VOICE effectively captures and predicts fine-grained emotional content from speech with high human alignment, as we demonstrate in the following. Further details are outlined in Appendix A.2."
        },
        {
            "title": "4 Experiments: Do they hear what we hear?",
            "content": "In this section, we evaluate current SER models on our novel benchmark. Before that, we start by introducing our experimental setup. 4https://huggingface.co/datasets/mitermix/audiosnippets 6 Table 5: Performance comparison of audio language models on the EMONET-VOICE BENCH. Models are evaluated against human emotion ratings using correlation metrics (Spearman and Pearson r, higher is better) and error metrics (MAE and RMSE, lower is better). Our EMPATHICINSIGHT-VOICE models demonstrate superior performance across all metrics, with LARGE achieving the highest Pearson correlation and lowest error and refusal rates. Refusal rates indicate the percentage of samples where models declined to provide emotion assessments. Best scores in bold. Model Refusal () Spearman () Pearson () MAE () RMSE () Gemini 2.0 Flash Gemini 2.5 Pro GPT-4o Mini Audio Preview GPT-4o Audio Preview 2024-12-17 Hume Voice EMPATHICINSIGHT-VOICE SMALL EMPATHICINSIGHT-VOICE LARGE 0.01% 0.00% 2.26% 27.59% 39.16% 0.00% 0.00% 0.355 0.417 0.326 0.337 0.274 0.418 0. 0.350 0.416 0.327 0.336 0.231 0.414 0.421 3.608 3.008 3.320 3.432 4.744 2.997 2.995 4.453 3.785 4.124 4.247 5.474 3.757 3.756 Experimental Setup. EMONET-VOICE BENCH assesses models proficiency in discerning emotional intensity from audio. To facilitate nuanced comparison across models, many of which output continuous scores, our primary evaluation employs metrics suited for regression and correlation analysis on common scale. The 3-level intensity human judgments (0: Not Present, 1: Mildly Present, 2: Intensely Present) are mapped to 0-10 scale for this evaluation, becoming 0, 5, and 10, respectively. Model predictions are likewise generated or normalized to this 0-10 continuous scale. We benchmarked general-purpose multimodal models (e.g., Gemini, GPT-4o) via zero-shot prompting, as well as specialized speech models (e.g., Hume Voice). Hume Voice was subject to constraints on input length (5s) and taxonomy coverage. Initial experiments with Whisper failed, due to general lack of emotion understanding, which led to our development of EMPATHICINSIGHT-VOICE, which pair continually pre-trained Whisper encoders with MLP regressors on our EMONET-VOICE dataset. We report four key metrics: Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) to quantify the average magnitude and larger deviations of prediction error on this 0-10 scale. Additionally, Pearson Correlation (Pearson r) and Spearman Rank Correlation (Spearman r) are used to assess the linear and monotonic agreement, respectively, between model-predicted intensities and human judgments. These metrics collectively provide comprehensive view of how well models capture both the absolute values and the relative ordering of perceived emotional intensities. 4.1 Evaluating Speech Emotion Recognition Models Table 5 presents performance across seven models, revealing clear performance tiers. Our EMPATHICINSIGHT-VOICE models achieve state-of-the-art results, with EMPATHICINSIGHT-VOICE LARGE obtaining the highest Pearson correlation (0.421) and lowest error rates (MAE: 2.995, RMSE: 3.756). EMPATHICINSIGHT-VOICE SMALL demonstrates competitive performance with the highest Spearman correlation (0.418). Gemini 2.5 Pro emerges as the strongest foundation model competitor (Pearson r: 0.416, Spearman r: 0.417), while other commercial models show significantly lower correlations and higher error and refusal rates. This shows that current audio models show decent alignment with human expert ratings on speech emotion recognition. Notably, refusal rates vary dramatically across models. While EMPATHICINSIGHT-VOICE models and Gemini variants process all samples (0-0.01% refusal), GPT-4o Audio Preview refuses 27.59% of samples, and Hume Voice refuses 39.16%reflecting safety constraints around sensitive emotional content, such as intoxication and pleasure/ecstasy. Overall, this indicates that our specialized AI models can, to some extent, hear what humans hear and demonstrate reasonable alignment with human emotion ratings, while several (general-purpose) models struggle in this task. Yet, this recognition capability proves more complex than initially apparent, as we will explore further next. Emotion-Specific Performance Patterns. Per-emotion analysis in Table 6 reveals clear performance hierarchies. High-arousal emotions prove most detectable across all models: teasing (average Spearman r: 0.617), embarrassment (0.585), and anger (0.536) show strong human-model alignment, 7 Table 6: Spearmans ρ by Emotion for Audio Models. Emotions are sorted by average performance across models, with best values in bold, runner-up underlined, and color-coded by correlation strength (gradient between red = 1 and blue = 1, NaN in gray). Several key patterns emerge: (i) Audio models demonstrate strong alignment with human annotations for high-arousal emotions like teasing. (ii) Our EMPATHICINSIGHT-VOICE models consistently outperform existing audio models across most emotions, or score second best. (iii) Some commercial models show systematic refusal patterns (NaNs) for sensitive emotions (sexual content, intoxication). (iv) Performance dramatically drops for subtle, low-arousal emotions like concentration. (v) Even state-of-the-art models struggle with complex cognitive-emotional states (e.g., contemplation), suggesting current audio models may be limited to more physiologically manifest emotions. GPT-4o Mini Audio GPT-4o Audio Hume Voice Gemini 2.0 Flash Gemini 2.5 Pro EMPATHICINSIGHTVOICE SMALL (ours) EMPATHICINSIGHTVOICE LARGE (ours) emotion Teasing Embarrassment Anger Impatience and Irritability Malevolence/Malice Shame Sadness Helplessness Astonishment/Surprise Pleasure/Ecstasy Disgust Contempt Fear Amusement Relief Pain Jealousy/ Envy Elation Pride Confusion Disappointment Doubt Triumph Infatuation Bitterness Fatigue/Exhaustion Thankfulness/Gratitude Intoxication/ Altered States of Consciousness Distress Sexual Lust Affection Longing Awe Hope/Enthusiasm/Optimism Sourness Interest Contemplation Contentment Emotional Numbness Concentration 0.569 0.550 0.496 0.455 0.345 0.437 0.470 0.347 0.487 0.364 0.421 0.412 0.355 0.412 0.317 0.365 0.334 0.390 0.348 0.379 0.301 0.379 0.333 0.315 0.330 0.221 0.297 0.198 0.374 0.203 0.310 0.289 0.298 0.250 0.158 0.161 0.187 -0.044 0.139 0. 0.636 0.637 0.555 0.471 NaN 0.393 0.404 0.375 NaN NaN 0.493 0.433 0.367 0.362 0.361 0.345 0.361 0.330 0.308 0.339 0.466 0.347 0.279 0.317 0.324 NaN NaN NaN 0.369 0.279 0.390 0.330 0.276 NaN 0.180 0.169 0.128 -0.019 0.092 0.019 NaN 0.416 0.418 NaN NaN 0.441 0.357 NaN NaN NaN 0.330 0.324 0.437 0.380 0.398 0.370 0.264 0.313 0.259 0.331 0.249 0.241 0.216 NaN NaN NaN NaN NaN -0.138 NaN 0.182 0.214 0.058 NaN NaN 0.119 0.177 0.195 NaN 0. 0.556 0.529 0.526 0.448 0.333 0.419 0.466 0.462 0.454 0.342 0.419 0.407 0.353 0.355 0.349 0.386 0.425 0.344 0.415 0.358 0.370 0.403 0.370 0.354 0.286 0.297 0.281 0.269 0.375 0.356 0.330 0.326 0.314 0.175 0.250 0.148 0.252 0.140 0.099 0.186 0.626 0.618 0.602 0.504 0.529 0.516 0.529 0.483 0.459 0.462 0.483 0.466 0.441 0.432 0.463 0.413 0.487 0.466 0.482 0.451 0.426 0.402 0.482 0.413 0.360 0.400 0.418 0.241 0.450 0.450 0.349 0.348 0.314 0.203 0.303 0.287 0.282 0.224 0.125 0. 0.649 0.669 0.578 0.554 0.562 0.552 0.483 0.536 0.451 0.538 0.419 0.478 0.470 0.454 0.462 0.472 0.469 0.475 0.484 0.423 0.432 0.459 0.460 0.392 0.411 0.455 0.358 0.486 0.432 0.332 0.359 0.365 0.329 0.345 0.331 0.351 0.263 0.231 0.139 0.055 avg. 0.617 0.585 0.536 0.500 0.477 0.474 0.461 0.457 0.456 0.447 0.432 0.427 0.411 0.408 0.407 0.404 0.402 0.401 0.396 0.390 0.386 0.385 0.371 0.367 0.352 0.351 0.347 0.662 0.678 0.577 0.570 0.615 0.558 0.521 0.535 0.428 0.529 0.460 0.469 0.458 0.462 0.501 0.474 0.471 0.487 0.474 0.451 0.461 0.463 0.455 0.408 0.404 0.384 0. 0.487 0.336 0.430 0.334 0.356 0.350 0.332 0.343 0.323 0.315 0.247 0.330 0.145 0.068 0.327 0.326 0.325 0.317 0.275 0.263 0.258 0.221 0.219 0.151 0.123 0.118 suggesting these acoustic signatures are most reliably encoded in prosody. Conversely, performance drops dramatically for subtle, low-arousal states like concentration (0.118) and emotional numbness (0.123), highlighting fundamental limitations in detecting nuanced emotional states from audio alone. Moreover, the table reveals systematic differences in emotion detection across our 40-category taxonomy. It demonstrates that EMPATHICINSIGHT-VOICE models consistently outperform competitors across most emotions, particularly excelling in complex states often missed by other systems. For instance, EMPATHICINSIGHT-VOICE achieves superior performance on challenging emotions like intoxication (where EMPATHICINSIGHT-VOICE scores 0.48 compared to 0.269 by the runner-up and many commercial models often completely refuse assessment), and similar for malevolence emotions that require nuanced prosodic understanding. Commercial Model Limitations. Commercial models exhibit systematic refusal patterns for sensitive content, with GPT-4o Audio and Hume Voice showing nearly identical NaN patterns for emotions like sexual content and intoxicationindicating shared (safety) constraints. This creates evaluation gaps precisely where human emotional complexity is especially relevant for applications. Even state-of-the-art models struggle with complex cognitive-emotional states (contemplation, interest, 8 contentment), suggesting current architectures may be fundamentally limited to more physiologically manifest emotions rather than subtle internal states."
        },
        {
            "title": "5 Discussion",
            "content": "Our analysis reveals fundamental relationship between human annotation consensus and model performance in audio-based emotion recognition, with implications that extend beyond the specific task to the broader understanding of machine learning on subjective human judgments. We demonstrated that EMPATHICINSIGHT-VOICE models advance the state-of-the-art significantly, with best error and ordering values. Yet, MAE values around 3.0 on 0-10 scale indicate substantial room for improvement even in our best model. The consistent pattern of high-arousal emotions being more detectable than low-arousal states across all architectures suggests this represents fundamental challenge in audio-based emotion recognition rather than limitation of specific models. ASR models dont (yet) understand emotions. ASR models like Whisper currently lack the ability to accurately understand and represent nuanced emotions [17, 7]. However, by continually pretraining these models, we can enable them to perceive and interpret emotions in way that supports more human-like predictions, as we demonstrated. Our EMONET-VOICE BIG dataset represents crucial first step toward equipping foundation models with this emotional understanding. Annotation Ambiguity Predicts Model Performance. The most striking finding from our comparative analysis is the systematic correlation between inter-annotator agreement and model performance across the emotional spectrum. Emotions exhibiting strong human consensus, such as Teasing (Spearmans ρ = 0.617), Embarrassment (ρ = 0.583), and Anger (ρ = 0.536), demonstrate both high agreement rates (predominantly green regions in Figure 1) and superior model alignment (top, dark blue in Table 6). Conversely, cognitively complex emotions like Concentration (ρ = 0.118), Contemplation (ρ = 0.151), and Contentment (ρ = 0.123) exhibit substantial human disagreement and correspondingly poor model performance. This pattern suggests that model failures may not represent algorithmic inadequacies but rather reflect genuine perceptual ambiguities inherent in the emotional recognition task itself. We propose that inter-annotator agreement might establish practical upper bound for model performance, as it is not be expected from computational systems to exceed human consensus on subjective human judgments. Arousal-Dependent Recognition Bias. Our results demonstrate clear arousal-based performance hierarchy, with high-energy emotions consistently outperforming their low-arousal counterparts. This bias appears across all model architectures, from transformer-based systems (GPT-4o variants) to specialized audio models (Hume Voice), suggesting fundamental limitation in current acoustic feature extraction paradigms. High-arousal emotions like Anger, Embarrassment, and Impatience and Irritability likely produce more distinctive acoustic signaturesincreased pitch variance, amplitude fluctuations, and prosodic changesthat are readily captured by existing audio processing pipelines. In contrast, low-arousal states such as Contemplation and Concentration may manifest through subtle changes in speech patterns that fall below current model sensitivity thresholds. This finding has significant implications for real-world applications: current audio emotion recognition systems may be inherently biased toward detecting emotional extremes while systematically underperforming on the nuanced, everyday emotional states that characterize much of human interaction. Furthermore, the arousal-dependent performance bias indicates that current audio processing architectures may be learning acoustic stereotypes of emotions rather than developing genuine emotional understanding. Models excel at detecting prototypical emotional expressions while failing on subtle variations, suggesting they may be capturing surface-level patterns rather than underlying emotional concepts. The Cognitive Emotion Recognition Gap. particularly noteworthy pattern emerges for cognitively-oriented emotionsstates that require contextual understanding beyond immediate acoustic features. Emotions such as Contemplation, Interest, and Concentration represent mental pro9 cesses rather than affective responses, and their recognition may fundamentally require understanding why someone is in particular state, not merely how they sound while experiencing it. This limitation points to broader challenge in current emotion recognition paradigms: the reliance on acoustic features alone may be insufficient for detecting emotions that are primarily cognitive rather than affective. Future architectures might need to incorporate contextual information, dialogue history, or multimodal inputs to bridge this gap, going toward multimodal AI assistants. 5.1 Limitations and Future Directions While our analysis provides valuable insights, several limitations should be acknowledged and addressed in future work. The fidelity of synthetic data from GPT-4o Audio generations underlying our datasets, while stateof-the-art, may still exhibit subtle differences from genuine human vocalizations, meaning model performance on this benchmark might not directly generalize to spontaneous real-world speech. The benchmark primarily evaluates the recognition of emotional portrayals in synthetic speech driven by acting scenarios, which differs from the often more nuanced or mixed cues in authentic, spontaneous expressions. Furthermore, the dataset inherently reflects the capabilities and potential biases of the specific audio generation model used. While EMONET-VOICE incorporates 11 voices and 4 languages, this speaker and linguistic diversity does not yet encompass the full spectrum of human identities, accents, dialects, or age ranges. Finally, emotion perception is inherently subjective; while expert consensus minimizes variability, the labels represent reliable approximation of perceived emotion in synthetic stimuli rather than an objective internal state. 5.2 Ethical Considerations This work responds to growing concerns about the unintended effects of emotionally uncalibrated AI systems. As AI models become more capable of producing emotionally charged content, it is essential to understand how people interpret and respond to these synthetic expressions. Our datasets offer basis for exploring potential risks, including miscommunication and emotional manipulation. We recognize the ethical challenges, especially regarding misuse for manipulative endsconcerns that underscore our commitment to transparency and safety. In response, we advocate for the development of safeguards to mitigate such misuse [10]. The development of EMONET-VOICE was guided by strong ethical commitment, primarily addressed through the exclusive use of synthetic voice generation. This approach deliberately avoids the privacy risks associated with collecting real human emotional expressions, particularly those tied to sensitive or deeply personal experiencessuch as pain, shame, or sexual desirethat would be difficult, if not impossible, to collect ethically and at scale from human participants. All voice samples in EMONET-VOICE are artificially generated using TTS models, with manual filtering and prompt diversification to reflect broad range of gender, demographic, and accent representations while minimizing problematic content, motivated by Friedrich et al. [9]. Although the likelihood is extremely low, we acknowledge the remote possibility that some samples may resemble real individuals [11]; however, no personally identifiable data was used at any stage. We release EMONET-VOICE as research artifact with the recommendation to use it for academic purposes and encourage thorough examination of potential downstream biases and ethical implications. We invite users to engage with our tools, transparently report any unexpected behaviors, and contribute feedback to help advance responsible data curation and safer AI development."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduced EMONET-VOICE, novel datasets for fine-grained speech emotion estimation, designed to address critical limitations in existing SER resources. We create large-scale, pretraining datasets EMONET-VOICE BIG, which is synthetic multilingual voice dataset. Derived from it, we establish EMONET-VOICE BENCH which has psychology expert annotation, utilizing 40-category emotion taxonomy with 3-level ratings. Their synthetic nature, combined with diverse voice and language coverage (11 voices, 4 languages, with balanced representations), prevents privacy concerns inherent in collecting authentic sensitive emotional data and broadens diversity. Furthermore, we create 10 EMPATHICINSIGHT-VOICE models (Small and Large), which establish state-of-the-art in speech emotion recognition. Existing foundation models like Gemini, GPT4o and Hume Voice perform significantly worse. Our results indicate gaps in current emotion recognition and hint to several future research paths. Future research should investigate whether the agreement-performance relationship holds across different modalities (text, video, physiological signals) and develop targeted architectures to handle low-agreement emotional categories more effectively. The development of context-aware models that can leverage situational information may be particularly promising for addressing the cognitive emotion recognition gap. Expanding EMONET-VOICE with more samples, languages, and speaker profiles using nextgeneration voice synthesis represents key priority, along with exploring multiple generative models to mitigate single-model bias. Investigating cross-modal consistency by generating corresponding facial expressions or scenarios for the same emotional prompts offers path toward richer multimodal benchmarks. Further analysis could also explore model performance variations across different languages or speaker voices within the current dataset to better understand the scope and limitations of current approaches."
        },
        {
            "title": "7 Acknowledgements",
            "content": "We gratefully acknowledge the support of Intel (oneAPI Center of Excellence), DFKI, Nous Research (providing cluster access and compute), TU Darmstadt, TIBLeibniz Information Centre for Science and Technology, and hessian.AI (providing compute and helpful discussions), and the open-source community for contributing to emotional AI. This work benefited from the ICT-48 Network of AI Research Excellence Center TAILOR (EU Horizon 2020, GA No 952215), the Hessian research priority program LOEWE within the project WhiteBox, the HMWK cluster projects Adaptive Mind and Third Wave of AI, and from the NHR4CES. Furthermore, this work was partly funded by the Federal Ministry of Education and Research (BMBF) project XEI (FKZ 01IS24079B)."
        },
        {
            "title": "References",
            "content": "[1] Lisa Feldman Barrett. The theory of constructed emotion. Social Cognitive and Affective Neuroscience, 12(1):123, 2017. [2] Felix Burkhardt, Astrid Paeschke, Miriam Rolfes, Walter Sendlmeier, and Benjamin Weiss. database of german emotional speech. In Ninth European Conference on Speech Communication and Technology, 2005. [3] Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette Chang, Sungbok Lee, and Shrikanth Narayanan. Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation, 42:335359, 2008. [4] Houwei Cao, David Cooper, Michael Keutmann, Ruben Gur, Ani Nenkova, and Ragini Verma. Crema-d: Crowd-sourced emotional multimodal actors dataset. IEEE transactions on affective computing, 2014. [5] Alan Cowen, Hillary Anger Elfenbein, Petri Laukka, and Dacher Keltner. Mapping 24 emotions conveyed by brief human vocalization. American Psychologist, 74(6):698, 2019. [6] Alan Cowen, Petri Laukka, Dacher Keltner, and Klaus Scherer. The sound of 24 emotions: new audio dataset of emotional vocalizations and cross-cultural validation. Behavior research methods, 52:11011119, 2020. [7] Soumya Dutta and Sriram Ganapathy. Leveraging content and acoustic representations for efficient speech emotion recognition. arXiv preprint arXiv:2409.05566, 2024. [8] Paul Ekman. An Argument for Basic Emotions. Psychology Press, 1992. [9] Felix Friedrich, Katharina Hämmerl, Patrick Schramowski, Manuel Brack, Jindˇrich Libovický, Alexander Fraser, and Kristian Kersting. Multilingual text-to-image generation magnifies gender stereotypes. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL, main), 2025. [10] Lukas Helff, Felix Friedrich, Manuel Brack, Patrick Schramowski, and Kristian Kersting. Llavaguard: An open vlm-based framework for safeguarding vision datasets and models. In Proceedings of the 41st International Conference on Machine Learning (ICML), 2025. [11] Dominik Hintersdorf, Lukas Struppek, Manuel Brack, Felix Friedrich, Patrick Schramowski, and Kristian Kersting. Does clip know my face? Journal of Artificial Intelligence Research (JAIR), 2024. [12] Philip Jackson and Syed Haq. Surrey audio-visual expressed emotion (savee) database. University of Surrey: Guildford, UK, 2014. [13] Tomoyuki Kajiwara, Chenhui Chu, Noriko Takemura, Yuta Nakashima, and Hajime Nagahara. Wrime: new dataset for emotional intensity estimation with subjective and objective annotations. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 20952104. Association for Computational Linguistics, 2021. [14] Hannah Rose Kirk, Iason Gabriel, Chris Summerfield, Bertie Vidgen, and Scott A. Hale. Why humanai relationships need socioaffective alignment. Nature, Humanities and Social Sciences Communications, 2025. [15] Clara Lee and Rafael Gomez. Emotion recognition in human-computer interaction: Why affective understanding matters. ACM Transactions on Interactive Intelligent Systems (TiiS), 11(3):125, 2021. [16] M. Lewis, J. M. Haviland-Jones, and L. F. Barrett. Handbook of Emotions. Guilford Press, 4th edition, 2016. [17] Yuanchao Li, Zeyu Zhao, Ondrej Klejch, Peter Bell, and Catherine Lai. Asr and emotional speech: word-level investigation of the mutual impact of speech and emotion recognition. arXiv preprint arXiv:2305.16065, 2023. [18] Kristen A. Lindquist. Emotions emerge from more basic psychological ingredients: modern psychological constructionist model. Emotion Review, 5(4):356368, 2013. Place: US Publisher: Sage Publications. [19] Steven Livingstone and Frank Russo. The ryerson audio-visual database of emotional speech and song (ravdess): dynamic, multimodal set of facial and vocal expressions in north american english. PLoS ONE, 2018. [20] Jaime Lorenzo-Trueba, Junichi Yamagishi, Tomoki Toda, Daisuke Saito, Fernando Villavicencio, Tomi Kinnunen, and Zhenhua Ling. Acted vs. natural emotional speech: How different are they? Proceedings of Interspeech, 2017. [21] Ziyang Ma, Mingjie Chen, Hezhao Zhang, Zhisheng Zheng, Wenxi Chen, Xiquan Li, Jiaxin Ye, Xie Chen, and Thomas Hain. EmoBox: Multilingual multi-corpus speech emotion recognition toolkit and benchmark. arXiv:2406.07162, 2024. [22] Stacy Marsella, Jonathan Gratch, and Paolo Petta. Computationally modeling human emotion. Communications of the ACM, 57(12):5667, 2010. [23] Emily Mower, Maja Mataric, and Shrikanth Narayanan. framework for automatic human IEEE Transactions on Audio, Speech, and emotion classification using emotion profiles. Language Processing, 19(5):10571070, 2011. [24] Niloofar Nojavan and Mohammad Soleymani. Emoreact: Detecting emotions in childrens speech and gestures. https://github.com/bnojavan/EmoReact, 2021. Accessed: 202505-06. [25] OpenAI. Gpt-4 omniaudio: Unified audio generation for speech and sound. https://openai. com, 2024. Accessed via HyperLab reseller. [26] Mohamed Osman, Daniel Z. Kaplan, and Tamer Nadeem. SER Evals: In-domain and out-ofdomain benchmarking for speech emotion recognition. arXiv:2408.07851, 2024. [27] Robert Plutchik. The nature of emotions. American Scientist, 89(4):344350, 2001. [28] Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, and Rada Mihalcea. Beneath the tip of the iceberg: Current challenges and new directions in sentiment analysis research. arXiv preprint arXiv:2005.00357, 2020. [29] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, and Ilya Sutskever. In Proceedings of the 40th Robust speech recognition via large-scale weak supervision. International Conference on Machine Learning, 2023. [30] James Russell. circumplex model of affect. Journal of Personality and Social Psychology, 39(6):1161, 1980. [31] Neil Scheidwasser-Clow, Mikolaj Kegler, Pierre Beckmann, and Milos Cernak. SERAB: multi-lingual benchmark for speech emotion recognition. arXiv preprint arXiv:2110.03414, 2021. [32] Christoph Schuhmann, Robert Kaczmarczyk, Gollam Rabby, Felix Friedrich, Maurice Kraus, Krishna Kalyan, Kourosh Nadi, Huu Nguyen, Kristian Kersting, and Sören Auer. EmoNet-Face: An expert-annotated benchmark for synthetic emotion recognition, 2025. [33] Björn Schuller, Stefan Steidl, Anton Batliner, Alessandro Vinciarelli, Klaus Scherer, Fabien Ringeval, Mohamed Chetouani, Felix Weninger, Florian Eyben, Erik Marchi, Marcello Mortillaro, Hugues Salamin, Anna Polychroniou, Fabio Valente, and Samuel Kim. The INTERSPEECH 2013 computational paralinguistics challenge: social signals, conflict, emotion, autism. In Interspeech 2013, ISCA, August 2013. ISCA. [34] Björn Schuller. Speech emotion recognition: Two decades in nutshell, benchmarks, and ongoing trends. Communications of the ACM, 61(5):9099, 2018. [35] Lukas Stappen, Alice Baird, Lukas Christ, Lea Schumann, Benjamin Sertolli, Eva-Maria Messner, Erik Cambria, Guoying Zhao, and Bjoern Schuller. The muse 2021 multimodal sentiment analysis challenge: Sentiment, emotion, physiological-emotion, and stress. In Proceedings of the 2nd International Multimodal Sentiment Analysis Challenge and Workshop, 2021. [36] Paige Tuttösí, Mantaj Dhillon, Luna Sang, Shane Eastwood, Poorvi Bhatia, Quang Minh Dinh, Avni Kapoor, Yewon Jin, and Angelica Lim. BERSting at the screams: benchmark for distanced, emotional and shouted speech recognition. arXiv:2505.00059, 2025. [37] Dong Zhang, Xincheng Ju, Junhui Li, Shoushan Li, Qiaoming Zhu, and Guodong Zhou. Multimodal multi-label emotion detection with modality and label dependence. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 35843593. Association for Computational Linguistics, 2020. [38] Lin Zhao and Anand Kumar. Speech emotion recognition: review of datasets, methods, and challenges. IEEE Transactions on Affective Computing, 13(1):305325, 2022. [39] Kun Zhou, Jia Liu, and Yuexian Chen. Emotional speech dataset (esd): parallel corpus in English and Mandarin. In Interspeech, 2021."
        },
        {
            "title": "A Appendices",
            "content": "A.1 EMONET-VOICE Taxonomy The 40 emotion categories used in EMONET-VOICE, adapted from EMONET-FACE [32], are listed below with associated descriptive terms used during conceptualization and prompting: Amusement: lighthearted fun, amusement, mirth, joviality, laughter, playfulness, silliness, jesting Elation: happiness, excitement, joy, exhilaration, delight, jubilation, bliss, Cheerfulness 13 Pleasure/Ecstasy: ecstasy, pleasure, bliss, rapture, Beatitude Contentment: contentment, relaxation, peacefulness, calmness, satisfaction, Ease, Serenity, fulfillment, gladness, lightness, serenity, tranquility Thankfulness/Gratitude: thankfulness, gratitude, appreciation, gratefulness Affection: sympathy, compassion, warmth, trust, caring, Clemency, forgiveness, Devotion, Tenderness, Reverence Infatuation: infatuation, having crush, romantic desire, fondness, butterflies in the stomach, adoration Hope/Enthusiasm/Optimism: hope, enthusiasm, optimism, Anticipation, Courage, Encouragement, Zeal, fervor, inspiration, Determination Triumph: triumph, superiority Pride: pride, dignity, self-confidently, honor, self-consciousness Interest: interest, fascination, curiosity, intrigue Awe: awe, awestruck, wonder Astonishment/Surprise: astonishment, surprise, amazement, shock, startlement Concentration: concentration, deep focus, engrossment, absorption, attention Contemplation: contemplation, thoughtfulness, pondering, reflection, meditation, Brooding, Pensiveness Relief: relief, respite, alleviation, solace, comfort, liberation Longing: yearning, longing, pining, wistfulness, nostalgia, Craving, desire, Envy, homesickness, saudade Teasing: teasing, bantering, mocking playfully, ribbing, provoking lightly Impatience and Irritability: impatience, irritability, irritation, restlessness, shorttemperedness, exasperation Sexual Lust: sexual lust, carnal desire, lust, feeling horny, feeling turned on Doubt: doubt, distrust, suspicion, skepticism, uncertainty, Pessimism Fear: fear, terror, dread, apprehension, alarm, horror, panic, nervousness Distress: worry, anxiety, unease, anguish, trepidation, Concern, Upset, pessimism, foreboding Confusion: confusion, bewilderment, flabbergasted, disorientation, Perplexity Embarrassment: embarrassment, shyness, mortification, discomfiture, awkwardness, Self-Consciousness Shame: shame, guilt, remorse, humiliation, contrition Disappointment: disappointment, regret, dismay, letdown, chagrin Sadness: sadness, sorrow, grief, melancholy, Dejection, Despair, Self-Pity, Sullenness, heartache, mournfulness, misery Bitterness: resentment, acrimony, bitterness, cynicism, rancor Contempt: contempt, disapproval, scorn, disdain, loathing, Detestation Disgust: disgust, revulsion, repulsion, abhorrence, loathing Anger: anger, rage, fury, hate, irascibility, enragement, Vexation, Wrath, Peevishness, Annoyance Malevolence/Malice: spite, sadism, malevolence, malice, desire to harm, schadenfreude Sourness: sourness, tartness, acidity, acerbity, sharpness (Note: Primarily gustatory, vocal correlates might be subtle reactions) Pain: physical pain, suffering, torment, ache, agony Helplessness: helplessness, powerlessness, desperation, submission Fatigue/Exhaustion: fatigue, exhaustion, weariness, lethargy, burnout, Weariness Emotional Numbness: numbness, detachment, insensitivity, emotional blunting, apathy, existential void, boredom, stoicism, indifference Intoxication/Altered States of Consciousness: being drunk, stupor, intoxication, disorientation, altered perception Jealousy & Envy: jealousy, envy, covetousness A.2 More Details on SOTA SER Model Training Methodology This section provides an in-depth description of the training procedures for the models discussed in Section 4: i.e. the Whisper backbone and the EMPATHICINSIGHT-VOICE ensembles. 14 Data Curation and Fine-tuning for Emotion Captioning. Our goal was to adapt pre-trained Whisper models [29] for the task of generating nuanced emotional captions from speech. The data generation and fine-tuning pipeline involved several key steps: 1. Initial Large-Scale Data Sources: The primary data source was the EMONET-VOICE BIG synthetic voice-acting dataset. This was augmented with approximately 4,500 hours of audio extracted from publicly available online videos (vlogs, diaries, documentaries). We applied voice activity detection (VAD) to isolate speech segments ranging from 3 to 12 seconds. 2. Dimensional Emotion Scoring with Gemini Flash 2.0: All audio snippetsboth from EMONET-VOICE BIG and the VAD-extracted clipswere annotated using Gemini Flash 2.0. complex, multi-shot prompt (detailed in the supplementary materials) guided the model to produce intensity scores on 04 scale (0 = absent, 4 = extremely present) for each of our 40 emotion dimensions simultaneously. This provided structured, dimensional representation of perceived emotional content. 3. Iterative Caption Generation for Whisper Training: Our initial attempt was to fine-tune Whisper to directly regress these 40-dimensional scores (i.e., to output numerical values), but this approach consistently collapsed into predicting nonsensical sequences of numbers. Similarly, training specialized output head to perform ordinal regression utilizing Wasserstein distance loss did not yield more sophisticated or coherent captions. We then converted the dimensional scores into procedurally generated string captions using predefined templates (e.g., The speaker sounds strongly amused and slightly joyful.). Training on these templated captions improved over direct regression, but the resulting Whisper outputs still tended toward repetitive or syntactically unnatural phrasing. The most effective strategy was to take those procedurally generated captions and run them back through Gemini Flash 2.0 for paraphrasing. This second pass introduced significant linguistic diversity and more natural sentence structures, while preserving the original 40-dimensional semantics. The paraphrasing prompt specifically encouraged varied wording and sentence complexity. 4. Training Data Preparation: All EMONET-VOICE BIG audio segments longer than 30 seconds were truncated to their first 30 seconds, to meet Whispers input constraints. Very long segments were further subdivided at silent regions into shorter clips, resulting in final training pool of over 2 million audiocaption pairs when combined with the processed VAD data. 5. Whisper Fine-tuning: Various sizes of OpenAIs Whisper models were then fine-tuned on this dataset of audio paired with the paraphrased emotional captions. The objective was to teach Whisper to generate fluid, context-sensitive descriptions of emotional content given raw speech input. Iteratively refining the captions via paraphrasing proved crucial for yielding outputs that were both semantically accurate and linguistically natural. We also experimented with incorporating synthetic emotion bursts during fine-tuning, but this led to degraded embedding quality and was therefore not used in the final models. EMPATHICINSIGHT-VOICE: MLP Ensembles for Dimensional Emotion Prediction. The EMPATHICINSIGHT-VOICE models were designed to provide direct predictions for each of the 40 emotion dimensionscomplementing the captioning approach with explicit scalar estimates. 1. Feature Extraction: We used the encoder from our best-performing Whisper variant as fixed feature extractor. For any input audio, we ran it through the Whisper encoder and collected the full sequence of token embeddings (sequence length = 1,500; embedding dimension = 768), yielding 1,152,000 features when flattened. Preliminary experiments showed that preserving the entire unpooled sequence outperformed all tested pooling strategies (mean, max, min, concatenation) for downstream MLP regression. 2. MLP Expert Heads: We trained an ensemble of 40 independent MLP models. Each MLP served as an expert head dedicated to regressing the intensity score for exactly one of the 40 emotion dimensions using the corresponding flattened Whisper embeddings as input. 3. Training Targets: The regression targets were the direct 04 intensity scores produced by Gemini Flash 2.0 (via the multi-shot prompt described in the supplementary files). During the encoder fine-tuning stage, we experimented with injecting synthetic emotion burstsartificially boosting certain dimension signals in the audioto encourage more robust embedding space. However, this augmentation degraded the underlying Whisper embeddings and ultimately hurt downstream MLP performance. Consequently, no synthetic bursts were used for final training. 4. MLP Architecture: Both the Small and Large EMPATHICINSIGHT-VOICE variants share the same overall architectural pattern for regressing from the high-dimensional flattened embeddings: Input Projection: first linear layer reduces the 1,152,000-dimensional input to much smaller embedding space. Hidden Layers: Three fully connected layers with ReLU activations, each followed by dropout for regularization to mitigate overfitting. Output Layer: final linear projection that outputs single continuous value in [0, 4], corresponding to the predicted intensity for that emotion. 5. Model Sizes: EMPATHICINSIGHT-VOICE SMALL: The initial projection reduces 1,152,000 inputs to 64 dimensions. The subsequent hidden layer sizes are 64 32 16. Each MLP head has about 73.73 million trainable parameters, the vast majority residing in that first projection layer. EMPATHICINSIGHT-VOICE LARGE: The initial projection reduces 1,152,000 inputs to 128 dimensions. The subsequent hidden layers are 128 64 32. This yields approximately 147.48 million trainable parameters per head, again dominated by the input projection. 6. Parallel Inference and Training Loss: At inference time, we evaluate all 40 MLP experts in parallel to predict the full 40-dimensional emotion profile (i.e., different strengths of emotionality across dimensions). During training, each MLP head is optimized independently using the mean absolute error (MAE) between predicted and target emotion strength. All trained EMPATHICINSIGHT-VOICE models (Small and Large) and the associated inference code are available via our project page. A.3 Hume Voice mapping A.4 Detailed Taxonomy Construction Methodology The 40-category emotion taxonomy utilized in both the EMONET-VOICE foundation and benchmark datasets was originally developed for the EmoNet-Face Benchmark [32]. The primary objective was to create taxonomy that supports more fine-grained and nuanced understanding of affective states in AI, moving beyond the limitations of traditional basic emotion models. This development was rooted in contemporary psychological research and significantly informed by the principles of the Theory of Constructed Emotion (TCE) [1]. The taxonomy was designed to encompass wide array of affective experiences, including not only common positive and negative emotions but also intricate social emotions (e.g., Embarrassment, Shame, Pride), cognitive states (e.g., Concentration, Doubt, Confusion), and bodily states (e.g., Pain, Fatigue, Intoxication). Less typical but experientially relevant categories like Sourness and Helplessness were also incorporated. The full list of 40 categories and their descriptive word clusters can be found in App. A.1 (cross-referencing the list you already have, which is similar to App. Tab. 4 from the EmoNet-Face paper). The construction process involved several key stages: 16 Hume Voice Label Our Taxonomy Joy Empathic Pain Guilt Nostalgia Determination Surprise (positive) Horror Calmness Desire Awkwardness Satisfaction Aesthetic Appreciation Awe Entrancement Romance Love Excitement Realization Tiredness Envy Anxiety Boredom Adoration Sympathy Admiration Craving Surprise (negative) Elation Distress - Longing - Surprise Fear Contentment Sexual Lust Embarrassment Pleasure Concentration Infatuation Affection Arousal Contemplation Fatigue Jealousy & Envy - - - - Admiration Craving Astonishment Table 7: Mapping of Hume Voice labels to our emotion taxonomy. Note that if one Hume Voice label fits to more than one emotion from our taxonomy, only one item was chosen. 1. Literature-Driven Candidate Extraction: The comprehensive \"Handbook of Emotions\" (946 pages) [16] was digitized using Optical Character Recognition (OCR). The digitized text was then divided into manageable 500-word segments. 2. AI-Assisted Term Identification: GPT-4 was employed to analyze these text segments and extract potential nouns representing emotion concepts. 3. Refinement and Deduplication: The initially extracted terms were aggregated, and duplicates were removed, resulting in candidate list of approximately 170 unique emotionrelated nouns. 4. Expert-Guided Clustering and Categorization: This refined list of terms underwent an iterative process of clustering. This involved independent categorization efforts by team members, followed by critical reviews and discussions. Psychologists and researchers in affective computing provided expert guidance throughout this phase to ensure the semantic coherence and psychological relevance of the emerging categories. Each of the final 40 categories represents cluster of these semantically related emotion words. In line with the Theory of Constructed Emotion, this taxonomy does not presuppose the biological universality or fixedness of these emotional categories. Instead, it is intended to facilitate contextaware and socially informed interpretations of affective expressions by AI systems. Recognizing the inherent ambiguity in perceiving emotions (e.g., high-arousal vocal expression might be interpreted as amusement, elation, or excitement depending on context and observer), the taxonomy was specifically designed to support plausible multi-label annotations rather than forcing rigid, single-label classifications. This approach aims to enable richer and more contextually sensitive representations of emotion in AI."
        },
        {
            "title": "B Annotation Platform Instructions and UI",
            "content": "17 Table 8: Summary of key dataset statistics for EMONET-VOICE. *Hume Voice provides 46 emotions on continuous scale from 0-1, of which we were able to map 29 to our emotion taxonomy. Human annotators voted on discrete scale: 0 (emotion not present), 1 (emotion weakly present), 2 (emotion strongly present). All scales were transformed to 0-10 scale for further analysis. Note that GPT-4o Audio Preview was not able to process 2,100 samples (e.g., returned an empty response)."
        },
        {
            "title": "Scale",
            "content": "Human 1 Human 2 Human 3 Human 4 Human 5 Human 6 EMPATHICINSIGHT-VOICE LARGE EMPATHICINSIGHT-VOICE SMALL GPT-4o Audio Preview 2024-12-17 GPT-4o Mini Audio Preview Gemini 2.0 Flash Gemini 2.5 Pro Hume Voice 6837 6620 2600 11605 343 5600 12600 12600 10500 12600 12600 12600 12600 1 1 1 1 1 1 40 40 40 40 40 40 *29 0-2 0-2 0-2 0-2 0-2 0-2 0-4 0-4 0-10 0-10 0-10 0-10 0-1 Figure 2: Instructions given to the human annotator for the expert annotation of EMONET-VOICE BENCH. emotion alpha alpha ci lower alpha ci upper items Embarrassment Teasing Pain Anger Shame Sadness Distress Malevolence Contentment Relief Jealousy / Envy Intoxication Authenticity Disappointment Fear Impatience and Irritability Helplessness Pride Sexual Lust Triumph Elation Overall Fatigue Concentration Disgust Thankfulness Pleasure Doubt Amusement Infatuation Confusion Contempt Affection Bitterness Astonishment Contemplation Sourness Hope Longing Arousal Interest Emotional Numbness Awe 0.272 0.271 0.247 0.220 0.216 0.211 0.208 0.204 0.197 0.196 0.194 0.193 0.185 0.176 0.161 0.159 0.158 0.156 0.149 0.145 0.138 0.138 0.129 0.103 0.103 0.088 0.082 0.078 0.068 0.063 0.060 0.046 0.043 0.033 0.021 0.021 0.004 -0.005 -0.046 -0.066 -0.094 -0.099 -0.127 0.186 0.178 0.160 0.129 0.122 0.111 0.121 0.098 0.109 0.098 0.095 0.104 0.093 0.071 0.066 0.057 0.070 0.057 0.048 0.043 0.040 0.124 0.034 0.023 0.004 -0.008 -0.011 -0.020 -0.031 -0.031 -0.027 -0.045 -0.052 -0.051 -0.068 -0.065 -0.088 -0.106 -0.149 -0.170 -0.178 -0.179 -0.218 0.368 0.362 0.334 0.310 0.297 0.301 0.297 0.294 0.281 0.280 0.282 0.279 0.279 0.271 0.241 0.247 0.246 0.241 0.243 0.246 0.229 0.152 0.217 0.186 0.195 0.178 0.177 0.171 0.155 0.150 0.148 0.130 0.133 0.116 0.109 0.104 0.083 0.090 0.046 0.030 -0.009 -0.017 -0.035 300 300 300 300 300 300 300 300 300 300 300 300 300 300 300 300 300 300 300 300 300 12600 300 300 300 300 300 300 300 300 300 300 300 300 300 300 300 300 300 300 300 300 300 Table 9: Cronbachs α inter-rater reliability (0 = emotion absent; 1 = weakly present; 2 = strongly present) for each emotion category (n = 300 items per label), with 95% confidence intervals obtained via non-parametric bootstrap (1 000 resamples, seed = 42). Overall reports α and CI computed across all 40 emotion categories + 2 extra categories (12 000 + 600 = 12 600 total annotations). Note that the analysis contains two extra categories (authenticity and arousal) that is not present in the narrow emotion category definition A.1. Figure 3: UI of our expert annotation tool for EMONET-VOICE BENCH."
        }
    ],
    "affiliations": [
        "Centre for Cognitive Science",
        "DFKI",
        "Hessian.AI",
        "L3S Research Center Leibniz University of Hannover",
        "LAION e.V.",
        "Ontocord",
        "TIBLeibniz Information Centre for Science and Technology",
        "TU Darmstadt",
        "Technical University of Munich"
    ]
}