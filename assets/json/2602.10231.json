{
    "paper_title": "Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards",
    "authors": [
        "Kirill Pavlenko",
        "Alexander Golubev",
        "Simon Karasik",
        "Boris Yangel"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Group Relative Policy Optimization (GRPO) assigns a single scalar advantage to all tokens in a completion. For structured generations with explicit segments and objectives, this couples unrelated reward signals across segments, leading to objective interference and misattributed credit. We propose Blockwise Advantage Estimation, a family of GRPO-compatible methods that assigns each objective its own advantage and applies it only to the tokens in the corresponding text block, reducing reliance on hand-designed scalar rewards and scaling naturally to additional objectives. A key challenge is estimating advantages for later blocks whose rewards are conditioned on sampled prefixes; standard unbiased approaches require expensive nested rollouts from intermediate states. Concretely, we introduce an Outcome-Conditioned Baseline that approximates intermediate state values using only within-group statistics by stratifying samples according to a prefix-derived intermediate outcome. On math tasks with uncertainty estimation, our method mitigates reward interference, is competitive with a state-of-the-art reward-designed approach, and preserves test-time gains from confidence-weighted ensembling. More broadly, it provides a modular recipe for optimizing sequential objectives in structured generations without additional rollouts."
        },
        {
            "title": "Start",
            "content": "Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards Kirill Pavlenko * 1 Alexander Golubev * 1 Simon Karasik 1 Boris Yangel 2 6 2 0 2 0 1 ] . [ 1 1 3 2 0 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Group Relative Policy Optimization (GRPO) assigns single scalar advantage to all tokens in completion. For structured generations with explicit segments and objectives, this couples unrelated reward signals across segments, leading to objective interference and misattributed credit. We propose Blockwise Advantage Estimation, family of GRPO-compatible methods that assigns each objective its own advantage and applies it only to the tokens in the corresponding text block, reducing reliance on hand-designed scalar rewards and scaling naturally to additional objectives. key challenge is estimating advantages for later blocks whose rewards are conditioned on sampled prefixes; standard unbiased approaches require expensive nested rollouts from intermediate states. Concretely, we introduce an OutcomeConditioned Baseline that approximates intermediate state values using only within-group statistics by stratifying samples according to prefixderived intermediate outcome. On math tasks with uncertainty estimation, our method mitigates reward interference, is competitive with state-ofthe-art reward-designed approach, and preserves test-time gains from confidence-weighted ensembling. More broadly, it provides modular recipe for optimizing sequential objectives in structured generations without additional rollouts. 1. Introduction Reinforcement Learning (RL) has gained significant traction as the primary paradigm for post-training Large Language Models (LLMs), enabling them to solve complex tasks that are difficult to supervise with static datasets (Ouyang et al., 2022). Among recent methods, Group Relative Policy Optimization (GRPO) (Shao et al., 2024) and its mod- *Equal contribution 1Nebius 2The Humanoid, Work done while at Nebius. Correspondence to: Alexander Golubev <alex golubev@nebius.com>. Preprint. February 12, 2026. ifications (DAPO (Yu et al., 2025), GSPO (Zheng et al., 2025), CISPO (MiniMax et al., 2025), ScaleRL (Khatri et al., 2025)) have become particularly popular due to their stability and memory efficiency. By normalizing rewards within sampled group of trajectories rather than relying on separate value network, these methods avoid the need for maintaining critic model, significantly reducing the computational footprint of training inherent to classical PPO. Despite these advantages, standard GRPO typically treats each generated completion as monolithic trajectory: it computes single scalar advantage and applies it uniformly to all tokens. This formulation is often suboptimal for complex tasks involving sequential subgoals, independent rubrics, or multi-step refinements. Given sequence of rewards r1, ..., rn, the standard approach is to construct scalar function = (r1, r2, ..., rn) that maps all rewards into one number and then apply GRPO-like algorithm (Shao et al., 2025; Feng et al., 2025a). This exacerbates credit assignment and requires careful handcrafting of to avoid score degradation described in RLCR (Damani et al., 2025), mode collapse, and reward hacking (Feng et al., 2025a). Many LLM tasks, however, are naturally segmented: each rk can be assigned to non-overlapping text blocks (e.g., solution to problem gets r1, and the following self-reflection block gets r2). Our core insight is that, in this case, optimization does not have to be framed as reward design problem. Instead, we can update each block independently based on signals tied specifically to its local objective rk. Motivated by this observation, we introduce Blockwise Advantage Estimation, family of GRPO-compatible methods that, using only the initial set of rollouts, compute separate proxy-advantages for different text segments and update each segment using only its local objective signal. central technical challenge is baseline estimation for later segments. Consider generation partitioned into segments X1, X2, . . ., where Xi is sampled conditioned on the prefix up to Xi1. To form low-variance advantage for segment Xi, one ideally normalizes the segment-level reward relative to the value of the intermediate state at the end of segment Xi1. naive solution is to perform Monte Carlo rollouts from the intermediate state, similar to VinePPO (KazemneBlockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards jad et al., 2025) or SPO (Guo et al., 2025), yielding unbiased estimates but incurring prohibitive compute for long-context LLMs and multi-thousand-token generations. We therefore focus on scalable baseline estimators that use only data already produced within the current GRPO group. We study series of progressively more informative estimators and find that an Outcome-Conditioned Baseline performs best in practice. This allows us to approximate the state-value function and reduce variance without additional inference costs. Our contributions are as follows: We introduce Blockwise Advantage Estimation, critic-free, GRPO-compatible framework for multiobjective RL on structured generations that routes objective-specific advantages to the segments of tokens that control them, reducing reliance on hand-tuned scalar reward combinations. We formalize the conditional baseline problem that arises because later segments are sampled conditioned on earlier sampled segments, and show that the Group Mean Baseline proposed in vanilla GRPO (normalizing rewards for all turns by the within-group mean and variance) can be suboptimal for credit assignment in multisegment generations. We then propose computeefficient Outcome-Conditioned Baseline that approximates boundary-state values using only within-group statistics, reducing variance for later segments without Monte-Carlo rollouts or extra inference. We demonstrate the efficacy of our method on the task of joint reasoning and uncertainty estimation across three models. Our approach performs on par with state-of-the-art (SoTA) reward-designed baseline like RLCR in terms of Expected Calibration Error (ECE) and reasoning accuracy. We demonstrate broader applicability in the additional setting of two-attempt RL for math, and discuss implications for long-horizon agentic tasks where segmentlevel decoupling enables efficient optimization over extremely long contexts. 2. Related Work Credit assignment beyond trajectory-level advantages. classical response to coarse trajectory-level credit assignment is to move toward finer-grained advantage estimates. Token-level methods (e.g., PPO with GAE) can in principle provide per-token learning signals, but require fitting value function, which is empirically difficult in LLM settings due to the diversity of prompt-conditioned states and limited per-prompt data. Recent critic-free approaches instead estimate intermediate advantages by Monte Carlo rollouts from 2 selected intermediate states. VinePPO partitions generations into heuristic steps (e.g., line breaks) and resamples continuations to estimate step-level advantages. Segment Policy Optimization generalizes this idea to arbitrary segment partitions and estimates segment-level advantages via MC. These methods primarily address credit assignment for single terminal objective and still rely on additional sampling from intermediate prefixes to estimate intermediate values. In contrast, our setting is multi-objective with rewards naturally attached to disjoint text blocks, and our goal is compute-efficient GRPO-compatible method: to estimate per-block baselines using only the rollouts already present in the group. Stratified and conditional advantage estimation. Several recent works show that advantage normalization can be systematically miscalibrated when trajectories are heterogeneous in ways that affect reward distributions. Stratified GRPO (Zhu et al., 2025) identifies cross-stratum bias in search agents: using global baseline across trajectories with different tool-use structures induces an apples-tooranges offset that distorts credit assignment; their Stratified Advantage Normalization computes normalized advantages within homogeneous strata and can be blended with global estimator for finite-sample stability. CANON (Chen et al., 2025) proposes conditional regrouping based on intrinsic rollout metrics (e.g., entropy or response length), computing both inter-group and intra-group advantages. Both lines use trajectory-level stratification to sharpen learning signals for single reward. Our conditionality problem is different: in segmented generations, later blocks are sampled from different intermediate prefixes inside the same prompt group, so the appropriate baseline for block is conditional value E[rk x, X<k]. supervision, Reward shaping, process and selfverification objectives. Another common approach to multi-signal training is reward design: one constructs single scalar reward that mixes multiple desiderata, often requiring careful tuning to avoid trade-offs or exploitation. Posterior-GRPO (Fan et al., 2025) incorporates processbased rewards for code reasoning by training reasoningquality reward model and then applying process rewards only to successful outcomes to mitigate reward hacking; their posterior assignment also helps distinguish among uniformly successful samples where GRPO would otherwise yield near-zero learning signal. GRPO-Verif (Wang et al., 2025) optimizes both solution generation and verification responses by computing separate group-normalized advantages for solutions and verifications and combining them with weighting hyperparameter. This approach is one representative of our proposed family of methods; we additionally show that our Outcome-Conditioned Baseline improves credit assignment for later blocks without requiring extra Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards rollouts. 3. Preliminaries 3.1. Group Relative Policy Optimization We briefly review GRPO and introduce notations used throughout the paper. Let denote prompt sampled from dataset, and let πθ be language-model policy that generates completion = (y1, . . . , yT ) in an autoregressive way: πθ(y x) = (cid:89) πθ(yt x, y<t). t=1 Given scalar reward function R(x, y) (e.g., score from verifier or preference model), GRPO estimates relative advantage using group of samples from the same prompt. Concretely, for each prompt x, GRPO draws group of completions {y(i)}G i=1 πθold( x) and evaluates rewards r(i) = R(x, y(i)). For each completion, it then forms group-normalized advantage, how well models reported confidence matches empirical correctness. For each example {1, . . . , }, let ci {0, 1} denote the correctness of solution determined by verifier, and let qi [0, 1] denote the models confidence, interpreted as predicted probability that the solution is correct. Brier score. The Brier score measures the squared error between predicted probability and the binary outcome: Brier ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (qi ci)2. For binary events, the Brier score is strictly proper scoring rule: it is minimized (in expectation) when qi equals the true conditional probability of correctness. Expected Calibration Error (ECE). ECE compares average confidence to empirical accuracy within confidence bins. Let {Bm}M m=1 be partition of [0, 1] into bins, and let Bm = {i : qi bin m}. Define ˆA(i) = r(i) µr σr + ϵ , µr = 1 (cid:88) j=1 r(j), σr = (cid:118) (cid:117) (cid:117) (cid:116) 1 (cid:88) j=1 (r(j) µr)2, Then acc(Bm) = conf(Bm) = 1 Bm 1 Bm (cid:88) iBm (cid:88) iBm ci, qi. where ϵ > 0 is small constant for numerical stability. GRPO optimizes PPO-style clipped policy-gradient objective, applying the same scalar advantage ˆA(i) to all tokens in the completion: LGRPO(θ) = 1 (cid:88) Ti(cid:88) i=1 t= (cid:32) (cid:16) min ρi,t(θ) ˆA(i), clip(ρi,t(θ), 1 η, 1 + η) ˆA(i)(cid:17) βDKL[πθπref ] (cid:33) , where η is the PPO clip parameter and ρi,t(θ) = (cid:16) πθ y(i) (cid:16) y(i) x, y(i) <t x, y(i) <t (cid:17) (cid:17) πθold (1) (2) is the per-token likelihood ratio between the updated policy and the behavior policy used to generate the samples. 3.2. Calibration Metrics for Confidence As will be discussed in Section 4.1, we study models that optimize for both correctness and calibration. We evaluate 3 ECE = (cid:88) m=1 Bm (cid:12)acc(Bm) conf(Bm)(cid:12) (cid:12) (cid:12). ECE is widely used in practice but depends on the binning scheme (choice of and bin boundaries). 4. Method 4.1. Self-Confidence Estimation for Math We study joint reasoning and uncertainty estimation in math problem solving. Each example consists of prompt (a math question) and ground-truth final answer y. Given x, the model generates structured completion that contains (i) solution/answer segment and (ii) self-analysis segment that includes numerical confidence score: = [ Xsol; Xconf ], [0, 1] parsed from Xconf . The illustrative example of completion from model is demonstrated in Appendix E. In our implementation, blocks are defined by deterministic parsing rules based on the structured tags (Appendix A): we treat the full solution/answer content as Xsol and the self-analysis plus the numeric confidence report as Xconf . We extract the predicted final answer ˆy from Xsol and compute verifiable correctness indicator = 1{ˆy y} {0, 1}, Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards where denotes equivalence under task-specific verifier (e.g., exact match up to formatting). In standard RL with verifiable rewards (RLVR), the reward depends only on correctness: RRLVR(ˆy, y) = c. While effective for improving accuracy, this reward provides no incentive for calibrated confidence and can encourage overconfident guessing. Adding an additional reward to incentivize uncertainty estimation makes it possible to do in number of ways, where it is not obvious which one to choose. RLCR work claims SoTA performance in the described task and suggests augmenting the binary correctness reward with calibration term based on proper scoring rule. Concretely, RLCR uses the Brier score for the reported confidence q: RRLCR(ˆy, q, y) = = RRLVR(ˆy, y) RBrier(q, c) = (q c)2. This reward penalizes being confidently wrong (large when = 0) and also penalizes under-confidence when correct (small when = 1), encouraging to track the empirical probability of correctness while still rewarding accurate solutions. The reward design for the available segments represents crucial part of training model. In Section 5.4, we observe that when the confidence objective is implemented via binary cross-entropy reward, vanilla GRPO can suffer markedly, motivating the segment-aware credit assignment and baselines we develop. In the remainder of this section, we use this task as running example to motivate our approach: correctness is primarily controlled by Xsol, whereas calibration is primarily controlled by Xconf . 4.2. Blockwise Decomposition We consider completions that decompose into contiguous blocks, = [ X1; X2; . . . ; XK ], where each block Xk is associated with an objective-specific reward rk.1 For prompt x, GRPO samples group {y(i)}G i=1 and yields block rewards {r(i) }G i=1. The modification we explore in this paper is to replace the single completion-level advantage with blockwise proxyk=1, and apply ˆA(i) advantages { ˆA(i) only to tokens that }K 1The reward rk may depend on the full prefix up to block (e.g., confidence reward depends on whether the preceding solution is correct), but it is primarily controlled by the tokens in block k. 4 belong to block k. Concretely, let (i) denote the token indices in block of sample i. Using the standard tokenlevel importance ratio ρ(i) (θ), defined in Equation 2, we optimize the sum of PPO-style clipped objectives across blocks: LBAE(θ) ="
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) (cid:88) i=1 k=1 1 (i) (cid:88) min tT (i) (cid:16) (θ) ˆA(i) ρ(i) , clip(ρ(i) (θ), 1 ε, 1 + ε) ˆA(i) (cid:17) , Block weighting. We average the token loss within each block by (i) so that blocks with very different lengths (e.g., short confidence report vs. long solution) have comparable per-token influence. More generally, one could introduce explicit block weights wk and optimize (cid:80) wk (i) ℓk,t; we use the uniform choice wk = 1 throughout to avoid additional tuning. 1 (cid:80) tT (i) For the first block X1, all group samples share the same starting state (the prompt x), and GRPOs group normalization provides natural baseline. For blocks > 1, however, the starting state includes the sampled prefix [X1; . . . ; Xk1], which differs across group members. statistically appropriate advantage for block should therefore normalize r(i) relative to the value of the intermediate state at the end of block (k1), i.e., conditional baseline of the form (cid:16) bk x, (i) <k (cid:17) (cid:104) rk x, (i) <k (cid:105) . (3) Estimating this quantity naively would require Monte Carlo rollouts from each intermediate prefix (i) <k, which is prohibitively expensive for long-context LLM training and multi-objective scenarios. In the next subsection, we therefore study practical estimators that approximate these conditional baselines using only the samples already available within the GRPO group, avoiding additional rollouts. In Section 5.1, we show how such baselines correspond to true Monte-Carlo estimates. See Appendix for pseudocode. 4.3. Conditional Baselines We consider the following estimators for block > 1, and apply them to form proxy-advantages ˆA(i) ˆb(i) = r(i) (with optional variance normalization as in GRPO): 1. No baseline (raw reward). ˆb(i) = 0, = r(i) ˆA(i) . This is an unbiased estimator for state-action function which typically has high variance. Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards 2. Batch Mean Baseline. Using all samples in the current batch (across prompts), r(i) cal. The ideal advantage for the confidence block subtracts the intermediate-state value = Ejbatch[ r(j) ˆb(i) ]. MC = r(i) A(i) cal (cid:104) rcal x, (i) sol (cid:105) , This reduces variance, but may be poorly matched to given prompts difficulty and to the specific prefix (i) <k. 3. Group Mean Baseline. Using the GRPO group for the same prompt x, ignoring that prefixes differ, ˆb(i) ="
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) j=1 r(j) . This is prompt-adaptive, but can be miscalibrated for > 1 because rewards are averaged across trajectories with different intermediate prefixes. This is the original proposition from GRPO work how it can be applied in multi-turn scenarios. which depends on the particular sampled solution prefix. To approximate this quantity, we select random 100 prompts from MATH500 (Hendrycks et al., 2021) and perform 32 rollouts for the confidence block from each fixed prefix (x, (i) MC. We then compute proxy-advantages ˆA(i) using the estimators from Sec 4.3. sol), producing reference estimate of A(i) Figure 1 compares our baselines to the Monte-Carlo reference. To quantify fidelity, we measure the RMSE between ˆA(i) and A(i) MC, split by whether the solution is correct. The Outcome-Conditioned Baseline achieves the lowest error in both strata and shows unbiased estimation. 5.2. Main Results 4. Outcome-Conditioned Baseline (OCB). Partitioning the group into subgroups based on discrete intermediate outcome o(i) of the prefix (i) <k. Let Go = {j {1, . . . , G} : o(j) = o}. We set We now evaluate Blockwise Advantage Estimation on joint reasoning and uncertainty estimation for math across Qwen2.5-7B-Base, Qwen2.5-7B-Instruct, and Qwen2.53B-Instruct. ˆb(i) = 1 Go(i) (cid:88) r(j) . jG o(i) In our main experiments, {correct, incorrect}, yielding simple two-bin baseline. This formulation allows for very broad range of applications which we cover in Sec 6.1. See Appendix for discussion of when OCB is unbiased. 5. Experiments and Results In this section, we ablate components of the proposed approach and evaluate our methods against baselines. 5.1. How Accurate Are Conditional Baselines? The baseline designs described in Section 4.3 naturally raise the question of how close they are to MC estimates. To investigate this, we focus on the second block in our structured generation, whose reward depends on the sampled solution prefix. We start from strong, well-calibrated reference policy obtained with RLCR for Qwen2.5-7B (Qwen et al., 2025), and collect trajectories of the form: = [Xsol; Xconf ]. For each prompt and sampled solution block (i) sol, the calibration reward (a Brier score) is denoted GRPO training setup. Unless otherwise specified, we use the same GRPO backbone across methods, with following implementation choices: No advantage standardization. We center advantages but do not divide by the within-group standard deviation, following common GRPO variants such as Dr. GRPO (Liu et al., 2025); Dynamic entropy regularization. We adapt the entropy coefficient over training to avoid premature entropy collapse (He et al., 2025); No KL regularization. We rely on conservative PPO clipping and entropy control for stability and set β = 0 in Eq. (1), following Dr.GRPO. The models are trained using mixture of MATH and DAPO training datasets having in total 25k training prompts (Feng et al., 2025b). All methods use the same prompts, verifier, hyperparameters and parsing logic for extracting answers and confidences. All prompts can be found in Appendix and hyperparameters in Table 4. We report Pass@1 accuracy and confidence quality using AUROC, ECE, and Brier Score. We report standard errors of the mean (SEM) for all evaluation metrics; details on the SEM computation procedure are provided in Appendix and in Table 7. Tables 1, 2, 6 summarize the main outcomes on MATH500 (in-distribution), GSM8K (Cobbe et al., 2021) (easy OOD), 5 Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards Figure 1. The error distribution comparing to true advantage MC estimates for three methods: Batch Mean, Group Mean, OCB w.r.t. correct/incorrect solutions. Batch Mean shows skewed approximation unlike Group Mean, while OCB demonstrates the lowest RMSE error across all groups. Table 1. Effect of conditional-baseline approximations for the confidence block for Qwen2.5-3B-Instruct trained for 1024 steps, evaluated on MATH500 Table 2. Main results for Qwen2.5-7B-Base trained for 512 steps on self-confidence estimation for math. Method Acc (%) AUROC ECE Brier RLCR Group Mean OCB Batch Mean None (raw reward) 68.7 66.4 67.9 66.2 66. 0.881 0.878 0.893 0.858 0.882 0.059 0.069 0.030 0.078 0.099 0.115 0.124 0.110 0.128 0.125 and AIME2325 (Mathematical Association of America) (hard OOD). Across model sizes and base/instruct variants, we observe three consistent trends. OCB is the strongest compute-free conditional baseline. On Qwen2.5-3B-Instruct, OCB yields large calibration gain on MATH500, improving ECE (0.030 vs. 0.059 for RLCR) as well as AUROC/Brier, with only modest accuracy gap. On Qwen2.5-7B-Base and Qwen2.5-7B-Instruct, OCB matches RLCR. Unconditioned means can be brittle. The Group Mean baseline can reduce ECE (e.g., MATH500 for 7B-Base: 0.020 vs. 0.032 for OCB), but may degrade accuracy and proper scoring (Brier). Under distribution shift, it can fail severely (e.g., GSM8K for 7BInstruct: ECE 0.125, Brier 0.146, AUROC 0.641; Table 6). Batch Mean and no-baseline variants are consistently worse on the 3B setting  (Table 1)  , aligning with our advantage-quality analysis described in Section 5. Method Acc (%) AUROC ECE Brier MATH500 (in-distribution) RLCR OCB Group Mean 75.0 75.1 72. 0.914 0.917 0.912 GSM8K (easy OOD) 88.9 RLCR 89.7 OCB 87.8 Group Mean AIME2325 (hard OOD) RLCR OCB Group Mean 9.4 9.5 8.2 0.835 0.847 0.849 0.942 0.902 0. 0.043 0.032 0.020 0.037 0.039 0.030 0.092 0.126 0.094 0.093 0.094 0.099 0.075 0.073 0.079 0.052 0.064 0. RLCR remains strong baseline, while BAE+OCB is competitive without reward scalarization. RLCR is often best on accuracy and/or hard-OOD discrimination (e.g., AUROC on AIME2325 for 7B-Base; Table 2), but BAE+OCB remains competitive and often yields better accuracycalibration trade-off on in-domain and easy OOD regimes, while avoiding reward scalarization across segments. 5.3. Test-Time Scaling Evaluation practical benefit of self-confidence estimation is test-time scaling (TTS): at inference time we can sample multiple candidate solutions and use the models reported confidence 6 Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards to select final answer. This can significantly improve accuracy without any additional training. For each prompt x, we sample independent completions {y(i)}n i=1 πθ( x) at fixed decoding settings, specified in Appendix Section D. From each completion we extract (i) final answer ˆa(i) and (ii) reported confidence q(i) [0, 1]. We then aggregate the candidates into single prediction ˆa using the following strategies: Pass@1, Pass@k, Majority Vote (Maj), Max confidence selection (MaxConf), Confidence-weighted majority (WeightedMaj). WeightedMaj is defined by the following selection rule: ˆawmaj = arg max (cid:88) i=1 q(i) 1{ˆa(i) = a}. Intuitively, Maj exploits consensus across samples, while MaxConf exploits self-reported uncertainty. Confidenceweighted majority combines both: answers that appear repeatedly and with consistently high confidence receive the largest total weight. When q(i) is well calibrated, these rules become principled: q(i) approximates the probability that candidate is correct, so higher-confidence candidates should be preferred, and confidence-weighting acts as natural soft aggregation. Figure 2. TTS evaluation for RLCR and OCB methods across three datasets: (a) MATH500, (b) GSM8K, (c) AIME2325 Figure 2 shows test-time scaling curves. Across datasets, increasing the number of samples improves accuracy for realizable selection rules, while Pass@k indicates substantial remaining headroom from generating diverse candidates. Crucially, OCB preserves the key downstream benefit of calibrated confidence. Performance tracks closely with RLCR, indicating that our segment-level credit assignment does not merely improve calibration metrics in isolation, but produces actionable confidence estimates that translate into better inference-time decision making. We also notice 7 Table 3. Qwen2.5-7B-Base trained BAE with Brier score and BCE as reward for the confidence block Method Acc (%) AUROC ECE Brier MATH500 (in-distribution) OCB (Brier, 512 st.) OCB (BCE, 512 st.) OCB (BCE, 1024 st.) 75.1 69.7 76. GSM8K (easy OOD) OCB (Brier, 512 st.) OCB (BCE, 512 st.) OCB (BCE, 1024 st.) AIME2325 (hard OOD) OCB (Brier, 512 st.) OCB (BCE, 512 st.) OCB (BCE, 1024 st.) 89.7 86.9 90.2 9.5 8.1 10.4 0.92 0.92 0.93 0.85 0.84 0. 0.90 0.87 0.92 0.03 0.03 0.03 0.04 0.02 0.02 0.13 0.08 0.09 0.09 0.10 0.09 0.07 0.08 0. 0.06 0.06 0.06 consistent empirical pattern how accuracy and calibration metrics contribute to overall TTS performance, the details and discussion of which can be found in Appendix G. 5.4. Alternative Rewards for Confidence So far, our confidence block has been trained with Brierstyle reward: rBrier(q, c) = (q c)2. key advantage of Blockwise Advantage Decomposition is that it does not depend on any particular scalarization of objectives; as long as the confidence objective is computable from (q, c), it can be optimized locally on the confidence block. To demonstrate this, we also consider the Bernoulli loglikelihood, another strictly proper scoring rule: rBCE(q, c) = (cid:16) log + (1 c) log(1 q) (cid:17) , with clipped to [ϵ, 1 ϵ] for numerical stability. naive adaptation of RLCR replaces the Brier term inside single scalar reward with BCE. In our experiments, this frequently converges to degenerate local optimum in which the model avoids the calibration penalty by emitting extremely low confidence and refusing to answer, rather than improving solution quality. The example of such behavior can be found in Appendix F. In contrast, training OCB using BCE for confidence block gives solid results. Empirically, we find that training the confidence block with BCE converges more slowly than Brier in terms of accuracy, but reaches strong performance once trained for sufficiently long. The corresponding evaluation results are demonstrated in Table 3. 5.5. Beyond Self-Confidence Our framework is not specific to uncertainty estimation. To illustrate broader applicability, we consider simple sequential refinement setting in which the model produces two Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards consecutive solution attempts for the same math problem. This mirrors common agentic workflows with reflection and subsequent adjustments, but in the simplest possible form with fixed number of refinement steps. Given prompt x, the model generates two attempts in single completion = [X1; X2], where X1 contains first reasoning trace and answer, and X2 contains reflection followed by second answer. We refer to this setup as Two-Attempt Refinement.2 The exact prompt for this task is provided in Appendix A. The reward structure for each attempt segment is described in Appendix C. To illustrate the applicability of our method, we train Qwen2.5-3B-Instruct under the BAE+OCB scheme. Figure 3 shows that the second attempt consistently improves over the first attempt across all sampling budgets. outcome-conditioned grouping provides simple and effective variance-reduction mechanism for later-block advantages without requiring additional rollouts. 6.2. Limitations Outcome-conditioned baselines require populated strata. The Outcome-Conditioned Baseline relies on estimating subgroup means within each GRPO group (e.g., correct vs. incorrect). When one stratum is rare, subgroup sizes can become small, increasing variance and potentially degrading calibration metrics. In practice, this suggests that OCB benefits from either sufficiently large groups, sufficiently high diversity of the model being trained, or additional smoothing or shrinkage toward the unconditioned Group Mean. We study how the number of completions influences OCB performance in Appendix H. Segment boundaries are assumed known and stable. BAE assumes that block boundaries are well defined (e.g., solution vs. confidence report), so that which tokens control which objective is unambiguous. For tasks with fuzzy boundaries, defining segments may require additional design choices. Incorrect segmentation could reintroduce crossobjective interference. 6.3. Future Work Richer conditioning and broader evaluation. Beyond binary correctness, one can define multi-bin outcomes (e.g., verifier margin, intermediate goal achievement, tool feedback) or learn outcome clusters, yielding higher-fidelity conditional baseline without additional rollouts; evaluating such variants beyond verifiable rewards is natural next step. Overall, we view our method as step toward more modular and scalable multi-objective RL for LLMs: replacing handtuned reward scalarization with structured credit assignment aligned to how models actually generate text."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning, specifically improving credit assignment in multi-objective reinforcement learning tasks. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Chen, G., Li, Y., Jiang, Y., Qian, C., Ren, Q., Yang, J., Cheng, Y., Liu, D., and Shao, J. Conditional advantage estimation for reinforcement learning in large reasoning models, 2025. URL https://arxiv.org/abs/ Figure 3. TTS evaluation within Two-Attempt Refinement on MATH500 dataset. Second attempt demonstrates higher performance across all aggregations. 6. Discussion, Limitations and Future Work 6.1. Discussion The motivation behind OCB is that this baseline imposes an equivalence relation on prefixes that share the same taskrelevant outcome and can be used for efficient approximation. When the block-k reward depends primarily on such an outcome (as in uncertainty estimation, where calibration is chiefly determined by whether the preceding solution is correct), conditioning the baseline on yields closer approximation to the true conditional value E[rk x, X<k] while retaining the compute benefits of group-based normalization. More broadly, many objectives only care about what was achieved rather than how it was achieved: for instance, in code generation there may be many distinct patches that fix bug, but downstream evaluation typically only depends on whether the tests pass. In these settings, 2We avoid the term two-shot to prevent confusion with fewshot prompting. 8 Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards 2509.23962. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems, 2021. URL https://arxiv. org/abs/2110.14168. Damani, M., Puri, I., Slocum, S., Shenfeld, I., Choshen, L., Kim, Y., and Andreas, J. Beyond binary rewards: Training lms to reason about their uncertainty, 2025. URL https://arxiv.org/abs/2507.16806. Fan, L., Zhang, Y., Chen, M., and Liu, Z. Posterior-grpo: Rewarding reasoning processes in code generation, 2025. URL https://arxiv.org/abs/2508.05170. Feng, X., Veeriah, V., Chiam, M., Dennis, M., Pachauri, R., Tumiel, T., Barbero, F., Obando-Ceron, J., Shi, J., Singh, S., Hou, S., Tomaˇsev, N., and Zahavy, T. Generating creative chess puzzles, 2025a. URL https://arxiv. org/abs/2510.23881. Feng, Y., Jain, P., Hartshorn, A., Duan, Y., and Kempe, J. Dont waste mistakes: Leveraging negative rl-groups via confidence reweighting, 2025b. URL https:// arxiv.org/abs/2510.08696. Guo, Y., Xu, L., Liu, J., Ye, D., and Qiu, S. Segment policy optimization: Effective segment-level credit assignment in rl for large language models, 2025. URL https: //arxiv.org/abs/2505.23564. He, J., Liu, J., Liu, C. Y., Yan, R., Wang, C., Cheng, P., Zhang, X., Zhang, F., Xu, J., Shen, W., Li, S., Zeng, L., Wei, T., Cheng, C., An, B., Liu, Y., and Zhou, Y. Skywork open reasoner 1 technical report, 2025. URL https://arxiv.org/abs/2505.22312. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. Kazemnejad, A., Aghajohari, M., Portelance, E., Sordoni, A., Reddy, S., Courville, A., and Roux, N. L. Vineppo: Refining credit assignment in rl training of llms, 2025. URL https://arxiv.org/abs/2410.01679. Khatri, D., Madaan, L., Tiwari, R., Bansal, R., Duvvuri, S. S., Zaheer, M., Dhillon, I. S., Brandfonbrener, D., and Agarwal, R. The art of scaling reinforcement learning compute for llms, 2025. URL https://arxiv.org/ abs/2510.13786. Liu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., Lee, W. S., and Lin, M. Understanding r1-zero-like training: critical perspective, 2025. URL https://arxiv. org/abs/2503.20783. Mathematical Association of America. American invitational mathematics examination (aime). MAA American Mathematics Competitions. URL https://maa. org/maa-invitational-competitions/. MiniMax, :, Chen, A., Li, A., Gong, B., Jiang, B., Fei, B., Yang, B., Shan, B., Yu, C., Wang, C., Zhu, C., Xiao, C., Du, C., Zhang, C., Qiao, C., Zhang, C., Du, C., Guo, C., Chen, D., Ding, D., Sun, D., Li, D., Jiao, E., Zhou, H., Zhang, H., Ding, H., Sun, H., Feng, H., Cai, H., Zhu, H., Sun, J., Zhuang, J., Cai, J., Song, J., Zhu, J., Li, J., Tian, J., Liu, J., Xu, J., Yan, J., Liu, J., He, J., Feng, K., Yang, K., Xiao, K., Han, L., Wang, L., Yu, L., Feng, L., Li, L., Zheng, L., Du, L., Yang, L., Zeng, L., Yu, M., Tao, M., Chi, M., Zhang, M., Lin, M., Hu, N., Di, N., Gao, P., Li, P., Zhao, P., Ren, Q., Xu, Q., Li, Q., Wang, Q., Tian, R., Leng, R., Chen, S., Chen, S., Shi, S., Weng, S., Guan, S., Yu, S., Li, S., Zhu, S., Li, T., Cai, T., Liang, T., Cheng, W., Kong, W., Li, W., Chen, X., Song, X., Luo, X., Su, X., Li, X., Han, X., Hou, X., Lu, X., Zou, X., Shen, X., Gong, Y., Ma, Y., Wang, Y., Shi, Y., Zhong, Y., Duan, Y., Fu, Y., Hu, Y., Gao, Y., Fan, Y., Yang, Y., Li, Y., Hu, Y., Huang, Y., Li, Y., Xu, Y., Mao, Y., Shi, Y., Wenren, Y., Li, Z., Li, Z., Tian, Z., Zhu, Z., Fan, Z., Wu, Z., Xu, Z., Yu, Z., Lyu, Z., Jiang, Z., Gao, Z., Wu, Z., Song, Z., and Sun, Z. Minimax-m1: Scaling test-time compute efficiently with lightning attention, 2025. URL https://arxiv.org/abs/2506.13585. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback, 2022. URL https: //arxiv.org/abs/2203.02155. Qwen, :, Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Tang, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report, 2025. URL https: //arxiv.org/abs/2412.15115. Shao, R., Asai, A., Shen, S. Z., Ivison, H., Kishore, V., Zhuo, J., Zhao, X., Park, M., Finlayson, S. G., Sontag, D., Murray, T., Min, S., Dasigi, P., Soldaini, L., Brahman, F., tau Yih, W., Wu, T., Zettlemoyer, L., Kim, Y., Hajishirzi, H., and Koh, P. W. Dr tulu: Reinforcement learning with evolving rubrics for deep research, 2025. URL https: //arxiv.org/abs/2511.19399. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., 9 Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards Zhang, H., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. Wang, X., Liu, B., Jiang, S., Liu, J., Qi, J., Chen, X., and He, B. From solving to verifying: unified objective for robust reasoning in llms, 2025. URL https://arxiv. org/abs/2511.15137. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., Liu, X., Lin, H., Lin, Z., Ma, B., Sheng, G., Tong, Y., Zhang, C., Zhang, M., Zhang, W., Zhu, H., Zhu, J., Chen, J., Chen, J., Wang, C., Yu, H., Song, Y., Wei, X., Zhou, H., Liu, J., Ma, W.- Y., Zhang, Y.-Q., Yan, L., Qiao, M., Wu, Y., and Wang, M. Dapo: An open-source llm reinforcement learning system at scale, 2025. URL https://arxiv.org/ abs/2503.14476. Zheng, C., Liu, S., Li, M., Chen, X.-H., Yu, B., Gao, C., Dang, K., Liu, Y., Men, R., Yang, A., Zhou, J., and Lin, J. Group sequence policy optimization, 2025. URL https://arxiv.org/abs/2507.18071. Zhu, M., Chen, X., Yu, B., Zhao, H., and Jia, J. Stratified grpo: Handling structural heterogeneity in reinforcement learning of llm search agents, 2025. URL https:// arxiv.org/abs/2510.06214. 10 Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards A. System Prompts All self-confidence experiments on Math were run with the following prompt. Self-Confidence System Prompt conversation between User and Assistant. The user asks question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind, provides the user with the final answer, then analyzes its confidence about the solution and provides the user with its confidence level. The confidence level is number between 0 and 1 (inclusive) enclosed within <confidence> </confidence> tags. The final answer is enclosed between <answer> </answer> tags. The analysis about confidence and uncertainty is enclosed within <analysis> </analysis> tags. The Assistant should reason about its confidence in the solution and its uncertainty in the solution within these tags. The final format that must be followed is: <reasoning> reasoning process here </reasoning><answer> final answer here </answer><analysis> analysis about confidence and uncertainty here </analysis><confidence> confidence level here (number between 0 and 1) </confidence>"
        },
        {
            "title": "EXAMPLES",
            "content": "Q: Who was awarded the Oceanography Societys Jerlov Award in 2018? <reasoning> The question asks who was awarded the Oceanography Societys Jerlov Award in 2018. The Jerlov Award is given by the Oceanography Society, and it is named after the late Professor Gunnar Jerlov, pioneer in the field of ocean optics. To find the winner of the 2018 award, would need to look up the most recent information on the Oceanography Societys website or other reliable sources.</reasoning> <answer> Dr. David W. Schiedek </answer> <analysis> The Oceanography Societys website or other reliable sources do not provide information on the winner of the 2018 Jerlov Award. It is possible that the information is not readily available or that the award was not given in 2018. Additionally, the name Dr. David W. Schiedek is not well-known figure in the field of oceanography, which raises uncertainty about the accuracy of this answer. </analysis> <confidence> 0.25 </confidence> END OF EXAMPLES 11 Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards The Two-Attempt Refinement experiment was run with the following system prompt. Two-Attempt Refinement conversation between User and Assistant. The user asks question, and the Assistant solves it using multiple attempts. For each attempt, the Assistant thinks about the reasoning process and provides an answer. Each attempt consists of reasoning enclosed in <reasoning> </reasoning> tags followed by an answer enclosed in <answer> </answer> tags. After the first attempt, the Assistant reflects on the previous solution and may provide an improved answer. The format for 2 attempts is: <reasoning> first attempt reasoning </reasoning> <answer> first attempt answer </answer> <reasoning> reflection and second attempt reasoning </reasoning> <answer> second attempt answer (same or corrected) </answer> EXAMPLES Q: What is 2 + 2? <reasoning> This is simple arithmetic problem. 2 + 2 equals 4. </reasoning> <answer> 4 </answer> <reasoning> Let me verify: 2 + 2 = 4. This is correct. </reasoning> <answer> 4 </answer> Q: Who was awarded the Oceanography Societys Jerlov Award in 2018? <reasoning> The question asks who was awarded the Oceanography Societys Jerlov Award in 2018. The Jerlov Award is given by the Oceanography Society, and it is named after the late Professor Gunnar Jerlov, pioneer in the field of ocean optics. To find the winner of the 2018 award, would need to look up the most recent information on the Oceanography Societys website or other reliable sources. </reasoning> <answer> Dr. David W. Schiedek </answer> <reasoning> Let me reconsider. The Oceanography Societys website or other reliable sources do not provide information on the winner of the 2018 Jerlov Award under the name Dr. David W. Schiedek. However, there is information that the award was given to Dr. Annick Bricaud in 2018, which sounds more plausible given her contributions to ocean optics. </reasoning> <answer> Dr. Annick Bricaud </answer> END OF EXAMPLES 12 Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards Table 4. Canonical configuration shared by all runs (YAML-like rendering for readability). # Common settings (all GRPO-like runs) sequence_length: 4096 temperature: 1.0 batch_size: 2048 group_size: 32 # unless stated otherwise optimizer: AdamW adam_beta1: 0.9 adam_beta2: 0.999 weight_decay: 0.1 lr: 1e-6 warmup: linear(64 steps) # Dynamic entropy controller target_entropy: 0.2 eta_delta: 5e-4 kl_beta: 0.0 # we dont use KL regularization weights_dtype: bfloat16 B. Additional Method Details Algorithm 1 BAE with outcome-conditioned baselines (for one prompt x) i=1 πθold( x) and parse each completion into blocks y(i) = [X (i) } and intermediate outcomes o(i) = ok(x, (i) <k) for > 1. 1 ; . . . ; (i) ]. 1: Sample GRPO group {y(i)}G 2: Compute per-block rewards {r(i) 3: for = 1 to do 4: 5: 6: 7: Set baseline ˆb(i) if = 1 then 1 1 (cid:80)G else j=1 r(j) 1 . For each outcome value o, compute stratum mean µk,o 1 Go Set ˆb(i) . µk,o(i) 8: end if Form proxy-advantage ˆA(i) 9: 10: 11: end for 12: Update θ by optimizing the blockwise clipped objective, applying ˆA(i) r(i) ˆb(i) . only to tokens in block k. (cid:80) jGo where Go = {j : o(j) r(j) = o}. When is OCB unbiased? For fixed prompt and block k, suppose there exists discrete outcome variable = o(x, X<k) such that E[rk x, X<k] = E[rk x, o]. Equivalently, conditioning on captures all dependence of rk on the sampled prefix X<k relevant for the conditional value. In this case, the within-stratum mean used by OCB is an unbiased estimator of E[rk x, X<k] (up to finite-sample noise). When this assumption is violated, OCB introduces bias; empirically, it is most effective when rk depends primarily on coarse task-relevant outcome (e.g., correctness) rather than on fine-grained properties of the prefix. C. Reward Structure for 2 Attempts Experiment Each attempt yields verifiable final answer ˆa1 and ˆa2, scored by the same correctness verifier, and is the ground-truth answer. Let c1 and c2 be the correctness of the corresponding attempts. In this experiment, we use shaped, attempt-specific reward scheme: r1 = c1, 13 Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards and for the second attempt, r2 = 1, 0.1, 0, if c2 = 1, if c2 = 0 and ˆa2 ˆa1, if c2 = 0 and ˆa2 ˆa1. Intuitively, r2 assigns full credit for solving the problem, but when the model remains incorrect it mildly rewards trying different answer rather than repeating the same mistake. This discourages degenerate behavior where the second attempt simply restates the first attempt verbatim, while keeping correctness as the dominant learning signal. D. Evaluation Hyperparameters Inference protocol. We evaluate each model by sampling = 16 independent completions per prompt with temperature = 1.0 using vLLM v0.10.0. We allow long generations with maximum generation length of 32k tokens to avoid truncation on difficult instances. We evaluate reasoning accuracy by extracting final answer from the model output and checking it with math verifier math-verify. After extraction we put each answer in boxed{...}. Empirically, enforcing boxed{} substantially reduces parsing failures. Calibration metrics and benchmarks. For Expected Calibration Error (ECE), we use fixed-width bins [0.0, 0.1), [0.1, 0.2), . . . , [0.9, 1.0), [1.0, 1.1), i.e., 11 bins total. Figure 4. Expected Calibration Error (ECE) computed with fixed-width bins across three datasets: (a)MATH500, (b)GSM8K, (c) AIME2325 E. Completion Example with Reasoning and Self-Analysis To illustrate the format of generations in our task, we present simple completion from Qwen2.5-7B-Base before any training:"
        },
        {
            "title": "Completion Example",
            "content": "<reasoning> To find the length of segment BN, can use the properties of similar triangles. Since MN is parallel to AB, triangles CAN and CBM are similar, where side AC corresponds to side BC and side AN corresponds to side BM. Using the property of similar triangles, we have the proportion: AC/BC = AN/BM 14 Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards We know that AC is 5 units long, BC is 2.4 units long, and AN is 3 units long. We can find the length of BM by using the fact that AC + AN = AP (where AP is the sum of the lengths of segments AC and AN). So, BM = AP - AM - MC. Knowing these values, can substitute them into the proportion and solve for the length of BM.</reasoning> <answer>11.5</answer> <analysis>Given that MN is parallel to AB, triangles CAN and CBM are similar, and the proportions can be set up with the given lengths. Substituting the values and solving for BM gives the required length of BN. However, since used approximate values for AC and BM, there may be some uncertainty in the final answer, and thus the confidence in the solution is relatively low.</analysis> <confidence>0.6</confidence> F. GRPO Calibration Trained with BCE Reward Here we illustrate that naive reward scheme for standard GRPO, = rRLVR + rBCE, can lead to local optimum where the model learns to refuse to answer while minimizing its confidence. This behavior can be interpreted as form of reward hacking. Mitigating it would require hyperparameter tuning or the introduction of additional terms into the reward. RLVR + BCE Reward Let and be positive integers satisfying 3a < and a2 + ab + b2 = (b + 3)2 + 27. Find the minimum possible value of + b. Remember to put your answer on its own line after Answer:. <reasoning> am not familiar with the equation presented in the question, involving positive integers and b. know what positive integers are, but do not recognize this specific equation or how to solve it. Additionally, am unsure about the context of the question, as it involves symbol () and square that looks like an algebraic expression, but it is not clear to me what it means. </reasoning> <answer> Unable to determine </answer> <analysis> This question involves complex equation with symbol and square that am unfamiliar with. There are no guidelines or previous examples to follow, and the equation does not resemble any standard mathematical format that am aware of. The lack of context and familiarity with the equation make it impossible to provide an accurate answer or even plausible guess. </analysis> <confidence> 0.001 </confidence> In contrast, OCB with BCE optimizing the confidence block shows no signs of reward hacking."
        },
        {
            "title": "OCB with BCE Reward For Confidence Block",
            "content": "In the circle with center Q, radii AQ and BQ form right angle. The two smaller regions are tangent semicircles, as shown. The radius of the circle with center is 14 inches. What is the radius of the smaller semicircle? Express your answer as common fraction. <reasoning> To find the radius of the smaller semicircles in the given geometric configuration, we need to follow these steps: 15 Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards 1. Understand the Initial Setup: - Theres circle centered at with radius 14 inches. - Radii AQ and BQ form right angle. - Two smaller regions are tangent semicircles, as shown. 2. Visualize the Problem: - The circle with center provides structural guide for the configuration. - Radii AQ and BQ are perpendicular to each other and hence form right angle at Q. - The two semicircles are tangent to each other and tangential to the larger circles boundary. 3. Establish Relationships and Names: - Let the radius of the larger circle be = 14 inches. - Let the radius of each of the smaller semicircles be r. 4. Analyze the Geometry: - The right angle formed by AQ and BQ divides the larger circle into four sectors. - Each smaller semicircle is tangent to the larger circle and each other at points along the radius of the larger circle. 5. Use Geometric Properties: - The combined diameter of the two smaller semicircles is equal to the diameter of the larger circle minus the diameter of each semicircle. - The part of the radius of occupied by the semicircles traces the perimeter of the square inscribed within the circle. 6. Set Up the Equations: - The diagonal of the inscribed square is the diameter of the circle. - The diameter of the circle is 2R = 2 14 = 28 inches. - For an inscribed square within the circle, the length of one side of the square can be denoted as a. The diagonal of the square forms right triangle with two sides of length each: 2 = 2R = 2 = 28 = = = 14 2. 28 2 - The area of the square is a2 = (14 2)2 = 392 square inches. 7. Apply Tangency Conditions: - The tangency condition requires the semicircles to align appropriately within the larger circle. - Considering the sum of the diameters of the two semicircles along with the right angle setup mentioned, the combined diameter of the two smaller semicircles equals the remaining radius of the larger circle after accounting for the diameter of the square portion. 8. Calculate the Radius of the Smaller Semicircles: - Each semicircle with radius has diameter of 2r. - The combined diameter of the two semicircles is 2r + 2r = 4r. - This combined diameter needs to fit into the space remaining within the circle as its larger circle portion: 16 Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards 4r = 14 (One side of square per radius) = 14 + 14 14 = 14 2 14 = 4r = 14. - Solving for r: = 14 4 = 7 . Therefore, the radius of the smaller semicircle is 7 2 inches. </reasoning> <answer> 7 2 </answer> <analysis> First, lets verify and refine the geometric configuration and relationships we have worked with. 1. Circle and Radii Setup: We have circle centered at with radius = 14 inches. Radii AQ and BQ are perpendicular, splitting the circle into four quadrants. 2. Semicircles Positioning: The smaller semicircles are tangent to each other and tangent to the larger circles boundary. However, the radius ratio and placements need clear geometric consideration. 3. Geometry Revisit: Let the radius of the smaller semicircles be r. The two tangent semicircles form width equal to their combined diameters along AQ and BQ. The radius of the larger circle is composed of segments + along with another segment equal to in one direction. 4. Right Triangle Insight: Given that the two semicircles form right angle at Q, and the semicircles radius sum along with the square inscribed within provides the key geometric property: - The diameters 2r of each semicircle sum up to 2r + 2r = 4r. - The diagonal of the square inscribed in the circle of radius 14 inches is 2R = 28 inches, forming the right 2. The area calculation as a2 = 392 square inches in this context adds triangle with insignificant value for finding the smaller semicircles radii. 2 = 28, giving = 14 5. Mistakes and Clarifications: The linking of squared terms and square side lengths probably needs better connection. The right triangle and square properties were considered but not correctly tied to the radius r. Taking simpler approach, realizing the relationship directly between the inscribed squares and semicircles, lets re-center: - The diameters of the semicircles and their tangency precisely dictate the relationship with the large circle. - Confirmation through an initial insight that 2r equates to R, = 3 . Finally, upon thorough review and correction of the geometric relationships and calculations: </analysis> <confidence> 0.15 </confidence> = 14 3 . G. Accuracy/Calibration Contribution into TTS Performance When calibration is good enough, accuracy dominates TTS. consistent empirical pattern is that once model is already reasonably calibrated (e.g., ECE on the order of few percent), marginal improvements in calibration metrics do not necessarily translate into better test-time scaling unless they also improve (or at least preserve) solution accuracy and sample quality. For example, in our 7B-Base experiments  (Table 2)  , the group-mean baseline achieves lower ECE than OCB on MATH500, yet it yields worse TTS accuracy under confidence-weighted ensembling because its Pass@1 accuracy is Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards Table 5. Effect of GRPO group size for BAE with Outcome-Conditioned Baselines (OCB) on Qwen2.5-7B-Base (512 steps). Larger groups slightly reduce ECE but can trade off against accuracy and discrimination (AUROC). Setting Acc (%) AUROC ECE Brier MATH500 (in-distribution) OCB (G=32) OCB (G=64) 75.1 75. GSM8K (easy OOD) OCB (G=32) OCB (G=64) 89.7 88.4 AIME2325 (hard OOD) OCB (G=32) OCB (G=64) 9.5 8.6 0.92 0.90 0.85 0. 0.90 0.89 0.03 0.03 0.04 0.02 0.13 0.12 0.09 0.09 0.07 0. 0.06 0.07 Table 6. Main results for Qwen2.5-7B-Instruct trained for 512 steps on self-confidence estimation for math. Method Acc (%) AUROC ECE Brier MATH500 (in-distribution) RLCR OCB Group Mean 78.6 77.3 76.6 0.922 0.929 0.893 GSM8K (easy OOD) 89.8 RLCR 89.1 OCB 82.4 Group Mean AIME2325 (hard OOD) RLCR OCB Group Mean 11.9 12.9 12. 0.760 0.827 0.641 0.932 0.907 0.937 0.031 0.040 0.032 0.032 0.032 0.125 0.132 0.096 0.094 0.086 0.087 0. 0.081 0.076 0.146 0.072 0.074 0.058 substantially lower. This suggests that in the well-calibrated regime, the dominant driver of TTS is the quality/diversity of sampled solutions (accuracy and Pass@k headroom), while confidence primarily serves as tie-breaker and robustness mechanism. H. Group Size Ablation for OCB Method Outcome-Conditioned Baselines estimate the conditional value for later blocks by averaging rewards within outcome-defined subgroups of GRPO group (e.g., correct vs. incorrect solutions). This estimator therefore depends on the group size G: larger groups typically provide more samples per outcome class, reducing variance of the subgroup mean but increasing sampling cost. We study the effect of group size on OCB using Qwen2.5-7B-Base trained for 512 steps with the same batch size of 2048 completions. All settings are identical except for the GRPO group size, comparing = 32 and = 64. Table 5 shows that increasing the group size from 32 to 64 yields slightly improved calibration (lower ECE) on all three benchmarks, while causing reduction in accuracy and AUROC in our runs. We attribute this to twofold lower number of distinct examples seen during training. I. Full Results with Standard Errors Table 7 reports evaluation metrics in meanSEM format. Uncertainty estimation for evaluation metrics. We report standard errors of the mean (SEM) for all evaluation metrics to quantify estimation uncertainty. For Pass@1, we compute SEM directly from the empirical distribution of metric values across sampled combinations. the SEM is computed as σ/ n, where σ is the sample standard deviation and is the total number of combination-level evaluations. For calibration metrics (Expected Calibration Error, Brier score, and AUROC), Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards Table 7. Evaluation metrics with standard errors (meanSEM). Model Dataset Method Acc (%) AUROC ECE Brier Qwen2.5-3B Instruct MATH500 (in-dist.) MATH500 (in-dist.) Qwen2.5-7B Base GSM8K (easy OOD) AIME2325 (hard OOD) MATH500 (in-dist.) Qwen2.5-7B Instruct GSM8K (easy OOD) AIME2325 (hard OOD) RLCR Group Mean OCB Batch Mean None (raw reward) RLCR OCB Group Mean RLCR OCB Group Mean RLCR OCB Group Mean RLCR OCB Group Mean RLCR OCB Group Mean RLCR OCB Group Mean 68.651.04 66.351.06 67.901.04 66.151.06 66.101.06 74.990.48 75.100.48 72.710.50 88.860.22 89.700.21 87.770.23 9.410.77 9.480.78 8.220. 78.550.46 77.300.47 76.590.47 89.780.21 89.110.21 82.430.52 11.940.86 12.850.89 12.081.73 0.88140.0085 0.87810.0086 0.89270.0081 0.85840.0090 0.88180.0083 0.91400.0037 0.91690.0035 0.91170.0037 0.83450.0047 0.84730.0046 0.84870. 0.94160.0115 0.90180.0180 0.91050.0141 0.92150.0036 0.92930.0031 0.89260.0043 0.75950.0055 0.82690.0050 0.64130.0084 0.93200.0103 0.90730.0124 0.93650.0219 0.05910.0071 0.06880.0074 0.02970.0064 0.07750.0074 0.09900.0076 0.04260.0033 0.03220.0030 0.01990. 0.03680.0018 0.03940.0019 0.02970.0019 0.09200.0053 0.12630.0055 0.09410.0055 0.03050.0030 0.03950.0031 0.03230.0031 0.03180.0019 0.03150.0019 0.12470.0047 0.13190.0061 0.09570.0065 0.09360.0110 0.11470.0046 0.12410.0056 0.11040.0046 0.12830.0044 0.12480. 0.09320.0021 0.09350.0020 0.09910.0021 0.07530.0013 0.07290.0012 0.07930.0013 0.05180.0031 0.06390.0035 0.05630.0033 0.08640.0022 0.08670.0023 0.09790.0024 0.08130.0016 0.07590.0016 0.14590.0043 0.07180.0035 0.07390.0042 0.05810. which are computed globally across all predictions, we employ bootstrap resampling with 1,000 iterations. In each bootstrap iteration, we resample the (q, c) pairs (confidence, correctness) with replacement and recompute each calibration metric; the SEM is estimated as the standard deviation of the resulting bootstrap distribution."
        }
    ],
    "affiliations": [
        "Nebius",
        "The Humanoid"
    ]
}