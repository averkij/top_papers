{
    "paper_title": "Training Step-Level Reasoning Verifiers with Formal Verification Tools",
    "authors": [
        "Ryo Kamoi",
        "Yusen Zhang",
        "Nan Zhang",
        "Sarkar Snigdha Sarathi Das",
        "Rui Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Process Reward Models (PRMs), which provide step-by-step feedback on the reasoning generated by Large Language Models (LLMs), are receiving increasing attention. However, two key research gaps remain: collecting accurate step-level error labels for training typically requires costly human annotation, and existing PRMs are limited to math reasoning problems. In response to these gaps, this paper aims to address the challenges of automatic dataset creation and the generalization of PRMs to diverse reasoning tasks. To achieve this goal, we propose FoVer, an approach for training PRMs on step-level error labels automatically annotated by formal verification tools, such as Z3 for formal logic and Isabelle for theorem proof, which provide automatic and accurate verification for symbolic tasks. Using this approach, we synthesize a training dataset with error labels on LLM responses for formal logic and theorem proof tasks without human annotation. Although this data synthesis is feasible only for tasks compatible with formal verification, we observe that LLM-based PRMs trained on our dataset exhibit cross-task generalization, improving verification across diverse reasoning tasks. Specifically, PRMs trained with FoVer significantly outperform baseline PRMs based on the original LLMs and achieve competitive or superior results compared to state-of-the-art PRMs trained on labels annotated by humans or stronger models, as measured by step-level verification on ProcessBench and Best-of-K performance across 12 reasoning benchmarks, including MATH, AIME, ANLI, MMLU, and BBH. The datasets, models, and code are provided at https://github.com/psunlpgroup/FoVer."
        },
        {
            "title": "Start",
            "content": "Training Step-Level Reasoning Verifiers with Formal Verification Tools"
        },
        {
            "title": "Sarkar Snigdha Sarathi Das Rui Zhang",
            "content": "Penn State University {ryokamoi, rmz5227}@psu.edu"
        },
        {
            "title": "Abstract",
            "content": "Process Reward Models (PRMs), which provide step-by-step feedback on the reasoning generated by Large Language Models (LLMs), are receiving increasing attention. However, two key research gaps remain: collecting accurate step-level error labels for training typically requires costly human annotation, and existing PRMs are limited to math reasoning problems. In response to these gaps, this paper aims to address the challenges of automatic dataset creation and the generalization of PRMs to diverse reasoning tasks. To achieve this goal, we propose FOVER, an approach for training PRMs on step-level error labels automatically annotated by formal verification tools, such as Z3 for formal logic and Isabelle for theorem proof, which provide automatic and accurate verification for symbolic tasks. Using this approach, we synthesize training dataset with error labels on LLM responses for formal logic and theorem proof tasks without human annotation. Although this data synthesis is feasible only for tasks compatible with formal verification, we observe that LLM-based PRMs trained on our dataset exhibit cross-task generalization, improving verification across diverse reasoning tasks. Specifically, PRMs trained with FOVER significantly outperform baseline PRMs based on the original LLMs and achieve competitive or superior results compared to state-of-the-art PRMs trained on labels annotated by humans or stronger models, as measured by steplevel verification on ProcessBench and Best-of-K performance across 12 reasoning benchmarks, including MATH, AIME, ANLI, MMLU, and BBH. The datasets, models, and code are provided at https://github.com/psunlpgroup/FoVer. 5 2 0 2 1 2 ] . [ 1 0 6 9 5 1 . 5 0 5 2 : r Figure 1: Overview of our approach, FOVER. We train PRMs on step-level error labels automatically annotated by formal verification tools on LLM responses to tasks compatible with formal verification. We observe that the resulting LLM-based PRMs improve verification across broad reasoning tasks. (a) PRMs based on Llama 3.1 8B (b) PRMs based on Qwen 2.5 7B Figure 2: Best-of-K (K=5) performance of PRMs on 12 reasoning tasks. FOVER creates training datasets for PRMs on tasks where we can automatically annotate step-level error labels using formal verification tools. FOVER significantly improves verification performance on broad out-of-distribution reasoning tasks, compared to baseline PRMs based on the original LLMs. Details are in Table 3."
        },
        {
            "title": "Introduction",
            "content": "Reasoning verifiers for large language models (LLMs) have been increasingly studied in the context of enhancing the reasoning capabilities of LLMs via reinforcement learning and inference-time improvement [32, 50, 41]. In this work, we study process reward models (PRMs) [49, 17, 18], which provide step-level verification for LLM-generated reasoning. Compared to outcome reward models (ORMs), which generate score for the entire solution, PRMs provide fine-grained feedback and often achieve better performance in downstream applications to improve LLMs [49, 18]. This paper tackles two research gaps in PRMs  (Table 1)  . First, it is difficult to collect accurate step-level error labels for training PRMs. While human annotation is primary approach [49, 18], it involves costly step-level annotations. To create training data without human annotation, previous studies propose approaches to generating estimated step-level error labels synthetically, such as Monte Carlo roll-outs from each step in the solution [50, 22, 65]. However, these approaches can yield inaccurate error labels. Second, PRMs have often been studied only on mathematical reasoning. The verification capabilities of PRMs outside of math reasoning are not well studied, and popular methods of training dataset creation for PRMs [50] are primarily designed for math reasoning tasks. To address these gaps, this work tackles two challenges in PRMs: automatic creation of training datasets and generalization to diverse reasoning tasks. For this purpose, we propose FOVER, an approach to train PRMs on step-level error labels automatically annotated using formal verification tools, such as Z3 [7] and Isabelle [30], which provide automatic and accurate verification for symbolic logic tasks like formal theorem proof. (1) Automatic creation of training datasets: LLM responses to tasks compatible with formal verification tools can be verified automatically, and we can acquire accurate step-level error labels for training PRMs without human annotation. In this work, we synthesize training dataset with tool-annotated error labels on formal logic and theorem proof tasks. (2) Generalization to diverse reasoning tasks: Although this data synthesis is feasible only for tasks compatible with formal verification, our goal is to improve PRMs across broad reasoning tasks. We investigate whether LLM-based PRMs trained on labels annotated by formal verification tools can transfer error detection capabilities to other reasoning tasks. This direction is motivated by recent work demonstrating that symbolic and synthetic training datasets can enhance the general reasoning abilities of LLMs [27, 56]. Building on this insight, we hypothesize that LLM-based PRMs can acquire generalizable step-level verification capabilities through training on symbolic tasks. To analyze our approach, we train PRMs based on Llama 3.1 8B [20] and Qwen 2.5 7B [36] on our dataset and evaluate them in two settings, Best-of-K [6, 17, 64] performance on reasoning tasks (4.1) and step-level verification performance on ProcessBench [66] (4.2), both of which are standard practices for assessing PRMs [18, 50, 66, 65]. Our experiments demonstrate that FOVER significantly improves the verification capabilities of PRMs on diverse reasoning tasks, compared with baseline PRMs based on the original LLMs, as measured in the two settings. Surprisingly, our experiments on 12 reasoning benchmarks show that training on our datasetcomprising of error labels on formal logic and theorem proof tasksenhances Best-of-K performance of LLM-based PRMs across diverse reasoning tasks, including MATH [11], AIME [2], ANLI [29], MMLU-PRO [52], and BBH [45] (Figure 2). We also compare our models with PRMs proposed in prior work, and observe that PRMs trained with FOVER are competitive with or better than state-of-the-art PRMs built on the same LLMs [59, 66, 65]. This is notable result because these PRMs are trained on error labels annotated by humans or stronger models, whereas FOVER does not rely on such labels. 2 Table 1: Comparison of research on process reward models (PRMs). Training datasets in prior work involve human annotation or inaccurate synthetic labels. In addition, prior work often trains and evaluates PRMs only on math reasoning tasks. To address these research gaps, we create training dataset with accurate and automatic annotation and evaluate our PRMs on diverse reasoning tasks. Step-level Error Annotation Training Evaluation PRMs Automatic Accurate Math (informal) Formal Logic Formal Proof Math (informal) Academic Exams Logic Reasoning BBH [45] PRM800K [18] Math-Shepherd [50] Qwen2.5-Math-PRM [65] FOVER (Ours) This work aligns with two fundamental needs in the development of reasoning verifiers. First, accurate automatic annotation offers practical solution for adapting PRMs to errors made by new models. Automatic annotation is required to update datasets periodically because training data based on errors from older models becomes outdated. Second, our analysis of cross-task generalization provides foundation for applying PRMs to broad reasoning tasks, including low-resource tasks. Our main contributions are as follows: We propose FOVER, an approach to train PRMs on step-level error labels automatically annotated by formal verification tools, such as Z3 and Isabelle. FOVER aims to address two core challenges in PRMs: automatic training dataset creation and generalization to diverse reasoning tasks. We synthesize training dataset for PRMs consisting of accurate step-level error labels on LLM responses for formal logic and theorem proof tasks, without relying on human annotation. Although this data synthesis is feasible only for tasks compatible with formal verification, we empirically demonstrate that training on our dataset improves LLM-based PRMs across broad reasoning tasks, as measured in Best-of-K performance on 12 reasoning benchmarks. We show that PRMs trained with FOVER perform competitively with state-of-the-art PRMs built on the same LLMs across broad reasoning tasks. This is noteworthy result because these PRMs are trained on labels annotated by humans or stronger models, which FOVER does not employ."
        },
        {
            "title": "2 Related Work",
            "content": "Process Reward Models (PRMs). PRMs [49, 18] are models that provide feedback for each step in solutions, which are often used to provide supervision for training and inference of LLMs [32, 63, 23]. challenge in developing PRMs lies in obtaining step-level error labels for training [50, 65]. Human annotation [49, 18] is primary approach, but is particularly costly in this step-level labeling task. Recent studies explore synthetic annotations, such as rule-based or model-based perturbations [60, 23, 33] and Monte Carlo roll-outs [50, 22, 65]. However, these approaches can produce unnatural or inaccurate step-level error labels. In contrast, our approach leverages formal verification tools to automatically and accurately annotate step-level error labels for training PRMs. Formal logic. Formal logic tasks, such as logical entailment, are popular task for evaluating [5, 46] and improving [27] reasoning capabilities of LLMs. However, much of the existing work uses these tasks for dataset generation without fully leveraging their verifiability, despite the availability of automatic solvers such as Z3 [7], Vampire [14], and [39] that can rigorously verify logical correctness. Our work is the first to use formal logic to create training dataset for PRMs. Formal theorem proof. Using LLMs as tool for automatic theorem proof is popular research topic [35, 61, 57]. In another direction, recent work attempts to use formal theorem proof to improve the math reasoning capabilities of LLMs during inference [68]. These studies use proof assistants such as Isabelle/HOL [30], Coq [47], and Lean [28] to provide feedback for formal theorem proof generation or math reasoning tasks during training and inference. Our work is the first to use these tools to annotate step-level labels for training PRMs. 3 Figure 3: Creation process of the FOVER dataset. (1) We first generate LLM reasoning in the format compatible with formal verification tools: Z3 and Isabelle. (2) We use the formal verification tools to automatically annotate step-level error labels, without involving human annotation. (a) Formal logic task. (b) Formal theorem proof task. Figure 4: Automatic step-level error annotation by formal verification tools."
        },
        {
            "title": "3 FOVER",
            "content": "We propose FOVER, an approach to train PRMs using step-level error labels automatically annotated by formal verification tools, which provide automatic and accurate verification for symbolic logic tasks like formal theorem proof. FOVER is designed to address two key challenges in PRM development: automatic creation of training datasets and generalization to diverse reasoning tasks. As shown in Figure 1, FOVER synthesizes training datasets for PRMs by automatically annotating step-level error labels on LLM responses using formal tools, thereby eliminating reliance on costly human annotation while ensuring label accuracy. Using tool-verifiable tasks for PRM training is major departure from previous work because most prior approaches do not expect cross-task generalization and train PRMs on target tasks, typically in mathematical reasoning [18, 50, 65]. In addition, formal verification tools are often used to verify entire proofs [68, 58], and their adaptation for step-level annotation represents another technical contribution of this work. Although our data synthesis is only applicable to tasks verifiable using formal tools, we aim to improve LLM-based PRMs across broad range of reasoning tasks by leveraging the cross-task generalization capabilities of LLMs [37, 53]. We synthesize dataset that includes step-level error labels on formal logic (3.1) and formal theorem proof tasks (3.2), which we refer to as the FOVER dataset. The FOVER dataset includes step-level error labels on responses from Llama 3.1 8B [20] and Qwen 2.5 7B [36]. The step-level error labels are automatically annotated using formal verification tools, Z3 [7] and Isabelle [30]. Figure 3 outlines the dataset creation process. (1) First, we generate initial responses from LLMs on the formal logic and theorem proof tasks with few-shot prompting. The responses should follow the format compatible with formal verification tools. Specifically, we use Z3 for the formal logic 4 Table 2: Statistics of the FOVER dataset. Refer to Appendix D.2 for further details. (a) Number of instances. (b) Raw training split. (c) Balanced training data. Responses Tasks Number of Instances Solution-level Step-level Step-level Train Validation Test # % Error # % Error # of Steps % Error Llama 3.1 8B Qwen 2.5 7B Formal Logic Formal Proof 10,000 10, Formal Logic Formal Proof 10,000 10,000 360 360 360 360 360 360 360 10,000 10,000 10,000 10,000 30.0% 43,082 30.0% 69,214 30.0% 34,879 30.0% 69,301 43.7% 13.2% 41.3% 14.1% 20,000 20,000 20,000 20,000 50% 50% 50% 50% task and Isabelle for the formal theorem proof task. In the formal logic task, we directly instruct the LLMs to generate formal solutions because the format is relatively simple. In the formal theorem proof task, we generate informal reasoning from the LLMs and convert it to formal format using stronger LLMs (Figure 3b) because Isabelles format is too complex to follow for the relatively small LLMs we use. (2) Second, we acquire step-level error labels using formal verification tools. This dataset creation process does not involve error annotation by humans or stronger models. Although we use stronger models for autoformalization in this paper, autoformalization is unnecessary when we apply this framework to strong models that can directly generate formal theorems. The FOVER dataset includes the training, validation, and test splits (Table 2a). The raw verification results often include more correct than incorrect steps (Table 2b). We construct training datasets with balanced step-level labels by selectively masking certain steps during training (Table 2c)."
        },
        {
            "title": "3.1 Formal Logic",
            "content": "We use the logical entailment task in first-order logic (FOL) reasoning, in which the goal is to determine whether hypothesis is logically entailed by given set of premises. This logical entailment task has been widely studied for evaluating or improving LLMs [5, 27]. Base dataset. We use responses from LLMs to the symbolic version of FLDx2 [27], dataset for multi-step FOL deduction. To ensure that each step can be independently verified, we exclude instances where reasoning processes involve assumptions. Refer to Appendix E.1 for details. Automatic error annotation. We use Z3 [7] for the step-level error annotation. Each step is checked independently by supplying Z3 with the current premises and the steps conclusion to determine logical validity, as in Figure 3a. Refer to Appendix E.2 for details."
        },
        {
            "title": "3.2 Formal Theorem Proof",
            "content": "We use the task of formal theorem proving for verifying solutions for math word problems, which are studied in research for formally verifying informal solutions from LLMs [55, 68]. We use Isabelle/HOL [30] to automatically annotate step-level error labels on formal proofs generated by LLMs. Since we use relatively small LLMs for generating initial responses, which often cannot produce valid Isabelle syntax directly, we employ stronger LLM to perform automatic formalization by translating their informal reasoning into formal Isabelle proofs. Base datasets. We use informal responses from LLMs to GSM8K [6], GSM8K-based cases in MetaMathQA [62], and math word problems in Big-Math [1]. Refer to Appendix F.1 for details. Informal to formal conversion. Inspired by prior work [55, 68], we convert informal problems from math reasoning datasets, as well as informal reasoning generated by LLMs, into formal proofs in Isabelles format using Llama 3.3 70B [20] with few-shot prompting (Figure 3b). To facilitate this process, we use Isabelles Sledgehammer tool [34] for verification to avoid specifying supporting lemmas during conversion. While the conversion is not always perfect, the accuracy of the final dataset is guaranteed because error labels are annotated relative to the converted formal proof, and only the converted proofs are included in the final dataset. Refer to Appendix F.2 for details. Automatic error annotation. Isabelle is designed for solution-level verification and stops verification at the first error. To obtain step-level error labels, we implement code for step-level verification, based on an existing library for verifying Isabelle proofs [54]. Specifically, our code assumes that the other steps are correct and evaluates whether the target step is valid. Refer to Appendix F.3 for details."
        },
        {
            "title": "4 Experiments",
            "content": "We evaluate our PRMs trained on the FOVER dataset on broad reasoning benchmarks. We compare our PRMs with baseline PRMs based on the same LLMs without additional training and state-of-theart PRMs in prior work built on the same LLMs. Specifically, we evaluate PRMs in Best-of-K on 12 reasoning benchmarks (4.1) and step-level verification on ProcessBench [66] (4.2) , and also conduct ablation study (4.3) and manual analysis (4.4). The results are summarized as follows: LLM-based PRMs trained with FOVER significantly improve verification capabilities on broad range of reasoning tasks compared to baseline PRMs based on the same LLMs without additional training, demonstrating effective cross-task generalization of our approach. PRMs trained with FOVER are competitive with or better than state-of-the-art PRMs built on the same LLMs. This result is remarkable because the state-of-the-art PRMs are trained on labels annotated by humans or stronger models, which FOVER does not use. Our manual analysis verifies that FOVER genuinely improves step-level verification and rarely degrades it from baseline PRMs based on the same LLMs without additional training. Evaluation Datasets. We evaluate our PRMs in two settings on diverse reasoning datasets. First, we evaluate Best-of-K performance of PRMs on 12 reasoning tasks, including math reasoning: GSM8K [6], MATH [11], AQuA-RAT [19], AIME (2016-2024) [2], logic reasoning: FOLIO [8], LogicNLI [48], NLI: ANLI [29], HANS [25], MMLU: MMLU-Pro-NoMath [11, 52, 31], BIG-Bench Hard: temporal sequences, tracking shuffled objects (three objects), and word sorting [45] (4.1). For large datasets, we use 250 randomly sampled examples from each dataset for evaluation. Second, we evaluate step-level verification performance on ProcessBench [66] (4.2). Models. We use Llama 3.1 8B [20] and Qwen 2.5 7B [36] as base LLMs for LLM-based PRMs. Our method trains these LLMs on the FOVER dataset, which we refer to as FOVER PRMs. Baseline PRMs: We compare with baselines that use the base LLMs as PRMs without additional training. State-of-the-art PRMs: We also compare with five state-of-the-art PRMs that are built on the same LLMs. Among PRMs based on Llama 3.1 8B, we evaluate RLHFlow-Llama3.1-8B trained on the DeepSeek or Mistral data [59], which include error labels on GSM8K and MATH acquired via the Monte Carlo rollouts [50] generated using stronger models. Among PRMs based on Qwen 2.5 7B, we evaluate Qwen2.5-Math-7B-PRM800K [66], which is trained on human-annotated labels on MATH, and Qwen2.5-Math-PRM-7B [65], which is trained on labels synthesized using both the Monte Carlo rollout and verification by stronger model. We also evaluate Qwen2.5-7B-Skywork-PRM [10], which are trained on math reasoning and coding tasks, but the training details are not revealed. Training on their own mistakes. As explained above, existing PRMs are often trained on labels annotated using stronger models. While this approach effectively enhances performance, it is less suitable for evaluating training methodologies, as it offers limited insight into improving PRMs built on top of state-of-the-art LLMsin settings where stronger models for annotation are unavailable. In contrast, we train LLM-based PRMs using labels from formal verification tools on their own responses, avoiding distilling any capabilities from stronger models. By showing effectiveness in this challenging setting, we expect FOVER to be used to improve PRMs based on state-of-the-art LLMs using their own responses. We also note that the goal of this work is not to outperform state-of-the-art PRMs, as FOVER is complementary to existing approaches and can be combined with them. Implementation Details. We create inputs to LLM-based PRMs by preprocessing step-by-step solutions into conversation format where each input contains single step, and the expected output is single token: correct or incorrect. To obtain step-level scores, we extract logits for the two words and apply the softmax function to compute the prediction probability for correct. This is popular approach to use LLMs as PRMs [59]. As the baseline PRMs are not fine-tuned, we provide zero-shot instructions about this format (Appendix G). Training: For the results in Section 4.1 and 4.2, we train PRMs on the balanced training datasets (Table 2c) that include both formal logic and proof data, consisting of 40k steps. Refer to Appendix for details."
        },
        {
            "title": "4.1 Results of Best-of-K on Reasoning Tasks",
            "content": "We evaluate PRMs using Best-of-K, which is popular approach to assess PRMs [6, 17, 64]. Best-ofK (or sample-and-rank) selects the best response from multiple candidates generated for the same 6 Table 3: Best-of-K (K=5) performance of PRMs on 12 reasoning benchmarks. FOVER PRMs significantly outperform the baseline PRMs in the second row. In addition, while the state-of-the-art PRMs in the following rows are trained on labels annotated by humans or stronger models, FOVER PRMs frequently outperform them in tasks other than math reasoning. µ: Trained on math reasoning tasks. : Trained on human annotated labels. : Trained on labels annotated by stronger models. ?: Training details have not been disclosed. : Statistically significant improvement over the baseline in the second row (p < 0.05, paired bootstrap [12]). Best values in each column are shown in bold. (a) PRMs based on Llama 3.1 8B select the best response from = 5 responses generated by Llama 3.1 8B PRMs Math Logic NLI MMLU BBH GSM8K MATH AQuA AIME FOLIO LogicNLI ANLI HANS Pro-NoMath Temporal Tracking Sorting Random Selection Llama 3.1 8B RLHFlow-Llama3.1-8B-Mµ RLHFlow-Llama3.1-8B-Dµ 85.2 86.8 92.8 91.6 35.2 42.4 57.1 65.0 45.2 64.6 46.8 67. FOVER-Llama3.1-8B-PRM (ours) 86.4 43.2 65.7 3.2 3.2 2.8 4. 4.0 57.1 57.6 59.1 60.6 36.4 38.8 44.0 39.6 64.0 44.8 30.4 27.2 29.2 29.2 28.8 74.8 73.6 79.2 76. 82.8 54.0 56.4 54.0 57.2 57.2 87.2 90.0 92.0 98.8 97.6 85.6 90.0 91.2 92.0 93.2 36.4 40.0 38.0 38. 38.4 Average 53.6 55.9 57.7 58.5 58.8 (b) PRMs based on Qwen 2.5 7B select the best response from = 5 responses generated by Qwen 2.5 7B PRMs Random Selection Qwen 2.5 7B Qwen2.5-7B-Skywork-PRMµ? Qwen2.5-Math-7B-PRM800Kµ Qwen2.5-Math-PRM-7Bµ FOVER-Qwen2.5-7B-PRM (ours) Math Logic NLI MMLU BBH GSM8K MATH AQuA AIME FOLIO LogicNLI ANLI HANS Pro-NoMath Temporal Tracking Sorting 90.4 90.4 96.0 94.8 94.8 92.8 61.6 64.4 77.6 79.5 10.0 12.0 64.5 62.1 68.8 78.0 68.8 81.1 69.2 82. 62.6 13.6 66.5 13.2 15.2 65.5 66.4 82.3 13.2 65.5 49.2 46. 44.8 47.6 48.8 48.8 34.0 32.0 30.0 34.0 28.0 84.4 84.8 82.0 83.6 84. 32.4 85.2 56.4 58.8 60.0 58.8 58.4 60.8 84.4 90. 82.4 81.6 84.4 91.6 88.0 89.2 90.8 90.4 92.0 89.6 28.8 28. 30.8 28.4 29.6 31.2 Average 60.8 61.6 61.6 62.4 62.8 63.3 Table 4: Step-level binary classification performance of PRMs on ProcessBench (AUROC). The first two rows represent PRMs based on LLMs without additional training. The next set of rows contains PRMs trained on math reasoning tasks. The last row shows our FOVER PRMs. : Statistically significant improvement over the baseline PRMs in the first row (p < 0.05, paired bootstrap [12]). (a) Llama 3.1 as PRMs (b) Qwen 2.5 as PRMs PRMs GSM8K MATH Olympiad Omni Ave. PRMs GSM8K MATH Olympiad Omni Ave. Llama 3.1 8B Llama 3.1 70B RLHFlow-Llama3.1-8B-Mµ RLHFlow-Llama3.1-8B-Dµ FOVER-Llama3.1-8B-PRM (ours) 70.9 92.0 91.2 88.5 80.0 68.8 82.2 75.2 77.2 74.1 67.3 83.4 68.2 73.6 74.8 59.0 80.5 66.5 84.5 Qwen 2.5 7B Qwen 2.5 72B 73.8 60.7 63.1 75.6 74.7 75.9 Qwen2.5-7B-Skywork-PRMµ? Qwen2.5-Math-7B-PRM800Kµ Qwen2.5-Math-PRM-7Bµ FOVER-Qwen2.5-7B-PRM (ours) 77.9 87.5 92.3 94.3 97.7 90.8 76.0 83. 82.6 91.7 95.3 89.1 74.9 81.8 72.6 91.5 94.8 84.6 73.6 80. 75.6 83.4 64.8 78.1 90.3 92.0 93.2 95.3 86.0 87.6 input using verifier to score and rank them. By comparing the performance in Best-of-K, we can indirectly assess PRMs without requiring evaluation datasets with step-level error labels. Settings. We generate = 5 responses using few-shot prompting with temperature of 0.5 from the base LLMs. Our few-shot demonstrations follow step-by-step format with line breaks separating individual steps. Accordingly, we post-process the generated responses by splitting them at line breaks to extract step-level inputs for PRMs. We compute the solution-level score as the minimum of the step-level scores and select the response with the highest solution-level score as the Best-of-K output, which is standard way to use PRMs in Best-of-K [18, 50]. We provide baseline performance of Random Selection, which randomly selects response without using PRMs. Results. Table 3 compares the performance of Best-of-K using different PRMs for re-ranking. Remarkably, training on the FOVER datasetconsisting of formal logic and theorem proof tasks significantly improves performance from the baseline PRMs on broad out-of-distribution reasoning tasks. This result demonstrates that our approach, which only relies on error labels from formal verification tools, enhances PRMs across broad range of reasoning tasks. We also observe that FOVER PRMs achieve performance competitive with that of state-of-the-art PRMs. Specifically, while existing PRMs tend to perform better on math reasoning tasks, likely 7 Table 5: Ablation study of PRMs trained on different versions of the FOVER datasets in Best-ofK (K=5). : Statistically significant improvement over the baseline (p < 0.05, paired bootstrap [12]). Training Dataset Math Logic NLI MMLU BBH Average Formal Logic Formal Proof GSM8K MATH AQuA AIME FOLIO LogicNLI ANLI HANS Pro-NoMath Temporal Tracking Sorting PRMs based on Llama 3.1 8B select the best response from = 5 responses generated by Llama 3.1 8B 69.7 70.1 65. 30.8 83.6 77.2 28.8 82.8 28.8 65.5 61.1 64.0 43.6 39.2 44.8 40.8 41.2 43.2 88.4 89.2 86.4 4.4 2.0 4. 55.2 55.6 57.2 PRMs based on Qwen 2.5 7B select the best response from = 5 responses generated by Qwen 2.5 7B 81.1 81.5 82.3 92.8 91.6 92.8 87.2 84.0 85. 46.8 50.4 48.8 31.2 32.0 32.4 61.6 62.1 65.5 65.2 66.0 66.4 10.8 12.4 13.2 61.2 60.4 60.8 89.2 97.2 97.6 90.8 91.6 91.6 96.0 94.0 93.2 89.2 90.8 89. 40.8 40.0 38.4 32.8 30.4 31.2 59.0 58.0 58.8 62.6 62.8 63.3 Table 6: Ablation study of FOVER PRMs on step-level verification on ProcessBench (AUROC). : Statistically significant improvement over the baseline PRMs (p < 0.05, paired bootstrap [12]). (a) Llama 3.1 as PRMs (b) Qwen 2.5 as PRMs Logic Proof GSM8K MATH Olympiad Omni Average Logic Proof GSM8K MATH Olympiad Omni Average 79.2 87.9 80.0 79.8 82.5 74.1 81.2 82.7 74.8 76.5 80.9 74.7 79.2 83.5 75.9 84.9 90.5 90.8 83.3 87.9 89.1 83.5 84.3 84.6 82.5 84.8 86.0 83.5 86.9 87.6 due to being trained on math datasets, FOVER PRMs often outperform them on the other reasoning tasks. This result suggests that FOVER is better at improving PRMs across broad out-of-distribution reasoning tasks. This is notable result because the state-of-the-art PRMs are trained on labels annotated by humans or stronger models, which FOVER does not rely on."
        },
        {
            "title": "4.2 Results of Step-level Verification on ProcessBench",
            "content": "We evaluate PRMs on the step-level binary classification task in ProcessBench [66], which includes human-annotated step-level error labels (correct vs. incorrect) for responses from multiple LLMs on math reasoning tasks. ProcessBench includes labels only for the earliest error in each response, so we evaluate PRMs on steps up to the first error in each solution. Table 4 shows the step-level verification performance of PRMs, measured in AUROC. The result shows that training on the FOVER dataset, which includes error labels on GSM8K-level problems in Isabelles format, significantly improves step-level verification performance in more complex math reasoning tasks when compared to the baseline PRMs. Furthermore, we observe that FOVER PRM based on Llama 3.1 8B is competitive to PRMs that are trained on GSM8K and MATH, and FOVER PRM based on Qwen 2.5 7B is better than PRMs based on Qwen 2.5 72B. These results demonstrate that training on FOVER exhibits easy-to-difficult generalization, improving verification performance of PRMs on reasoning tasks that are more complex than those seen during training."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "We evaluate PRMs trained on three variants of the FOVER dataset: formal logic only, formal theorem proving only, and combination of both. Table 5 and 6 show results of Best-of-K on 12 reasoning datasets and step-level verification in ProcessBench. We observe that both formal logic and theorem proof datasets significantly improve performance from the baseline PRMs in both settings, verifying that both of these tasks are effective at improving PRMs on broad reasoning tasks. However, we observe that training using both formal logic and theorem proof tasks (the last rows) is not always effective and sometimes degrades performance from training on individual tasks. This is counterintuitive result showing that more diverse training datasets are not effective. We expect that more stable training strategy should be used to further improve the cross-task generalization of FOVER. We provide ablation studies on other hyperparameters in Appendix J.2."
        },
        {
            "title": "4.4 Manual Analysis",
            "content": "We manually analyze step-level scores for cases where FOVER improves or degrades re-ranking in Best-of-K from baseline PRMs (4.1). As Best-of-K evaluates the final answers of selected solutions 8 Table 7: Verification of an LLM response to MMLU-Pro-NoMath by PRMs based on Llama 3.1 8B. FOVER PRM improves the scores from the baseline and successfully detects the wrong step (Step 1). Problem: Nagel clarifies that absolutism is the view that we should (A) [...] (B) never prevent murder [...] (I) avoid murder at all costs Ground truth answer: (I) avoid murder at all costs Step Step Solution steps generated by Llama 3.1 8B Baseline FOVER Absolutism is philosophical position that holds that particular statement is always true or always false, regardless of context or circumstance. In the context of morality, absolutism is the view that particular action, such as murder, is always right or always wrong, regardless of the circumstances. Step 1 Option (B) \"never prevent murder\" is the correct interpretation of absolutism in this context, as it implies that murder is always wrong and should never be prevented, which is characteristic of an absolutist view. Step 2 The answer is (B). Solution-level Score 0.84 0.91 0.99 0. 0.84 0.47 0.99 0.47 without directly evaluating step-level verification by PRMs, we conduct manual analysis to verify the improvement by FOVER. We analyze step-level scores in GSM8K, ANLI, and the temporal sequence and word sorting task in BBH. Refer to Appendix J.3 for detailed results. Our manual analysis shows that FOVER genuinely improves step-level verification and rarely leads to degradation compared to the baseline PRMs. Among cases where FOVER improved final answers in Best-of-K, we observe that FOVER often improves the step-level verification, particularly in GSM8K and the temporal sequence task, where over 50% of cases are improved. Among cases where FOVER degrades the final answers in Best-of-K, we observe that FOVER degrades the step-level scores in less than 20% of the cases on most of the tasks. Our analysis shows that much of the degradation in the final answers does not represent decline in step-level verification. For example, there are many cases where solutions selected by the baselines included incorrect steps, although they produced the correct final answers. When intermediate steps are wrong, PRMs should reject such solutions even if the final answers are correct, so this case does not represent degradation by FOVER."
        },
        {
            "title": "4.5 Generalization in PRMs Trained on FOVER",
            "content": "Our results show an effective generalization of PRMs trained with FOVER. First, FOVER exhibits promising symbol-to-text generalization. The FOVER dataset includes symbolic responses in the formats that are compatible with Z3 and Isabelle, but it improves PRMs on LLM responses in informal natural language. Furthermore, our results show easy-to-difficult generalization. Although our formal theorem proof task includes errors in responses to GSM8K-level problems, we observe that FOVER improves PRMs on more complex math reasoning tasks, such as MATH and OlympiadBench (4.2). Finally, FOVER exhibits cross-task generalization and improves PRMs on reasoning tasks that are largely different from those in training data (4.1). For example, surprisingly, FOVER improves verification performance on MMLU-Pro-NoMath, task that is distant from both of our training tasks. As in Figure 7, problems in this dataset require complex reasoning and domain knowledge to verify solutions. In this example, FOVER PRM correctly understands the concept of absolutism in Step 0 and detects logical mistake in Step 1. Although the FOVER dataset does not include knowledge-intensive tasks, it makes PRMs generalize to tasks that require logical reasoning over domain knowledge. This is notable result that motivates further research into the creation of synthetic data to enhance the verification capabilities of LLMs across diverse reasoning tasks."
        },
        {
            "title": "5 Conclusion",
            "content": "This work aims to propose an approach that addresses two challenges in PRMs: automatic creation of training datasets and generalization to diverse reasoning tasks. We introduce FOVER, an approach to train PRMs using step-level error labels annotated by formal verification tools, such as Z3 and Isabelle, without relying on human annotation or inaccurate synthetic labels. We show that training on the tool-annotated error labels from formal logic and theorem proof tasks exhibits cross-task generalization, improving the verification capabilities of LLM-based PRMs across broad range of reasoning tasks. Our results demonstrate that formal-tool-based annotation provides scalable and complementary supervision signal for enhancing fine-grained reasoning verification by LLMs."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "This work was supported by NSF CAREER Award IIS-2338418. We thank Jin Peng Zhou for providing guidance on the use of his code [68]. We also thank Terufumi Morishita for the valuable discussions and for his assistance with the FLDx2 dataset [27], and we appreciate NLP Colloquium JP for providing the opportunity to connect with him."
        },
        {
            "title": "References",
            "content": "[1] Alon Albalak, Duy Phung, Nathan Lile, Rafael Rafailov, Kanishk Gandhi, Louis Castricato, Anikait Singh, Chase Blagden, Violet Xiang, Dakota Mahan, and Nick Haber. Big-math: large-scale, high-quality math dataset for reinforcement learning in language models. arXiv preprint arXiv:2502.17387, 2025. [2] Art of Problem Solving. Aime problems and solutions, 2025. URL https://artofproblemsolving. com/wiki/index.php/AIME_Problems_and_Solutions. [3] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, Ilya Sutskever, and Jeff Wu. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390, 2023. [4] Sanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che, Ting Liu, and Xiangzhan Yu. Recall and learn: Fine-tuning deep pretrained language models with less forgetting. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 78707881, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.634. URL https://aclanthology. org/2020.emnlp-main.634/. [5] Peter Clark, Oyvind Tafjord, and Kyle Richardson. Transformers as soft reasoners over language. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI20, 2021. ISBN 9780999241165. [6] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [7] Leonardo de Moura and Nikolaj Bjørner. Z3: an efficient smt solver. In 2008 Tools and Algorithms for Construction and Analysis of Systems, pages 337340. Springer, Berlin, Heidelberg, March 2008. URL https: //www.microsoft.com/en-us/research/publication/z3-an-efficient-smt-solver/. [8] Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Wenfei Zhou, James Coady, David Peng, Yujie Qiao, Luke Benson, Lucy Sun, Alexander Wardle-Solano, Hannah Szabó, Ekaterina Zubova, Matthew Burtell, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Alexander Fabbri, Wojciech Maciej Kryscinski, Semih Yavuz, Ye Liu, Xi Victoria Lin, Shafiq Joty, Yingbo Zhou, Caiming Xiong, Rex Ying, Arman Cohan, and Dragomir Radev. FOLIO: Natural language reasoning with first-order logic. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2201722031, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.1229. URL https://aclanthology. org/2024.emnlp-main.1229/. [9] Peter Hase, Mohit Bansal, Peter Clark, and Sarah Wiegreffe. The unreasonable effectiveness of easy training data for hard tasks. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 70027024, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.378. URL https://aclanthology.org/2024.acl-long.378/. [10] Jujie He, Tianwen Wei, Rui Yan, Jiacai Liu, Chaojie Wang, Yimeng Gan, Shiwen Tu, Chris Yuhao Liu, Liang Zeng, Xiaokun Wang, Boyang Wang, Yongcong Li, Fuxiang Zhang, Jiacheng Xu, Bo An, Yang Liu, and Yahui Zhou. Skywork-o1 open series. https://huggingface.co/Skywork, November 2024. URL https://huggingface.co/Skywork. [11] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum?id=7Bywt2mQsCe. 10 [12] Philipp Koehn. Statistical significance tests for machine translation evaluation. In Dekang Lin and Dekai Wu, editors, Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 388395, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-3250/. [13] Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 2219922213. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ 8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf. [14] Laura Kovács and Andrei Voronkov. First-order theorem proving and vampire. In Natasha Sharygina and Helmut Veith, editors, Computer Aided Verification, pages 135, Berlin, Heidelberg, 2013. Springer Berlin Heidelberg. ISBN 978-3-642-39799-8. [15] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [16] Aitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum? id=IFXTZERXdM7. [17] Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making language models better reasoners with step-aware verifier. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 53155333, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.291. URL https://aclanthology.org/ 2023.acl-long.291/. [18] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=v8L0pN6EOi. [19] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Regina Barzilay and Min-Yen Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 158167, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1015. URL https://aclanthology.org/P17-1015/. [20] Llama Team, AI @ Meta. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7. [22] Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Meiqi Guo, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, and Abhinav Rastogi. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592, 2024. [23] Qianli Ma, Haotian Zhou, Tingkai Liu, Jianbo Yuan, Pengfei Liu, Yang You, and Hongxia Yang. Lets reward step by step: Step-level reward model as the navigators for reasoning. arXiv preprint arXiv:2310.10080, 2023. [24] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 4653446594. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ 91edff07232fb1b55a505a9e9f6c0ff3-Paper-Conference.pdf. 11 [25] R. Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428 3448, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1334. URL https://aclanthology.org/P19-1334/. [26] Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. Orca-math: Unlocking the potential of slms in grade school math. arXiv preprint arXiv:2402.14830, 2024. [27] Terufumi Morishita, Gaku Morio, Atsuki Yamaguchi, and Yasuhiro Sogawa. Enhancing reasoning capabilities of LLMs via principled synthetic logic corpus. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=mljDUaQpln. [28] Leonardo de Moura and Sebastian Ullrich. The lean 4 theorem prover and programming language. In Automated Deduction CADE 28: 28th International Conference on Automated Deduction, Virtual Event, July 1215, 2021, Proceedings, page 625635, Berlin, Heidelberg, 2021. Springer-Verlag. ISBN 978-3-030-79875-8. doi: 10.1007/978-3-030-79876-5_37. URL https://doi.org/10.1007/ 978-3-030-79876-5_37. [29] Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial NLI: new benchmark for natural language understanding. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 48854901, Online, July 2020. Association for Computational Linguistics. doi: 10. 18653/v1/2020.acl-main.441. URL https://aclanthology.org/2020.acl-main.441/. [30] Tobias Nipkow, Markus Wenzel, and Lawrence C. Paulson. Isabelle/HOL: Proof Assistant for HigherOrder Logic, volume 2283 of Lecture Notes in Computer Science. Springer, 2002. ISBN 978-3-540-43376-7. doi: 10.1007/3-540-45949-9. [31] Sam Paech. Mmlu-pro-nomath, 2024. mmlu-pro-nomath. URL https://huggingface.co/blog/sam-paech/ [32] Sarah Pan, Vladislav Lialin, Sherin Muckatira, and Anna Rumshisky. Lets reinforce step by step. In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023. URL https:// openreview.net/forum?id=QkdRqpClab. [33] Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. REFINER: Reasoning feedback on intermediate representations. In Yvette Graham and Matthew Purver, editors, Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11001126, St. Julians, Malta, March 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.eacl-long.67. [34] Lawrence C. Paulson and Jasmin Christian Blanchette. Three years of experience with sledgehammer, practical link between automatic and interactive theorem provers. In Geoff Sutcliffe, Stephan Schulz, and Eugenia Ternovska, editors, IWIL 2010. The 8th International Workshop on the Implementation of Logics, volume 2 of EPiC Series in Computing, pages 111. EasyChair, 2012. doi: 10.29007/36dt. URL /publications/paper/wV. [35] Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving, 2020. URL https://arxiv.org/abs/2009.03393. [36] Qwen Team. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [37] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 21(140):167, 2020. URL http://jmlr.org/papers/v21/ 20-074.html. [38] William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802, 2022. [39] Stephan Schulz. - brainiac theorem prover. AI Commun., 15(2,3):111126, August 2002. ISSN 0921-7126. [40] Thomas Scialom, Tuhin Chakrabarty, and Smaranda Muresan. Fine-tuned language models are continual learners. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 61076122, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main. 410. URL https://aclanthology.org/2022.emnlp-main.410/. 12 [41] Amrith Setlur, Nived Rajaraman, Sergey Levine, and Aviral Kumar. Scaling test-time compute without verification or rl is suboptimal. arXiv preprint arXiv:2502.12118, 2025. [42] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/ 0efbe98067c6c73dba1250d2beaa81f9-Paper.pdf. [43] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [44] Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, and Chuang Gan. Easy-to-hard generalization: Scalable alignment beyond human supervision. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum? id=qwgfh2fTtN. [45] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and Jason Wei. Challenging BIG-bench tasks and whether chain-of-thought can solve them. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 1300313051, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.824. URL https://aclanthology.org/2023.findings-acl.824/. [46] Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. ProofWriter: Generating implications, proofs, and abductive statements over natural language. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 36213634, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.317. URL https://aclanthology.org/2021.findings-acl.317/. [47] The Coq Development Team. The Coq reference manual release 8.19.0. https://coq.inria.fr/ doc/V8.19.0/refman, 2024. [48] Jidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao He, and Yaohui Jin. Diagnosing the firstorder logical reasoning ability through LogicNLI. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 37383747, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.303. URL https: //aclanthology.org/2021.emnlp-main.303/. [49] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with processand outcomebased feedback. arXiv preprint arXiv:2211.14275, 2022. [50] Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations. In LunWei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 94269439, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.510. URL https://aclanthology.org/2024.acl-long.510. [51] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=1PL1NIMMrw. [52] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. MMLU-pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. URL https://openreview.net/forum?id=y10DM6R2r3. [53] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=gEZrGCozdqR. [54] Sean Welleck. Neural theorem proving tutorial. https://github.com/wellecks/ntptutorial, 2023. 13 [55] Yuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus Rabe, Charles Staats, Mateja Jamnik, and Christian Szegedy. Autoformalization with large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 3235332368. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_ files/paper/2022/file/d0c6bc641a56bebee9d985b937307367-Paper-Conference.pdf. [56] Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768, 2025. [57] Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, and Xiaodan Liang. Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data. arXiv preprint arXiv:2405.14333, 2024. [58] Huajian Xin, Daya Guo, Zhihong Shao, Z.Z. Ren, Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, and Xiaodan Liang. Advancing theorem proving in LLMs through large-scale synthetic data. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24, 2024. URL https://openreview.net/ forum?id=TPtXLihkny. [59] Wei Xiong, Hanning Zhang, Nan Jiang, and Tong Zhang. An implementation of generative prm. https: //github.com/RLHFlow/RLHF-Reward-Modeling, 2024. [60] Kaiyu Yang, Jia Deng, and Danqi Chen. Generating natural language proofs with verifier-guided search. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 89105, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.7. URL https://aclanthology.org/2022.emnlp-main.7. [61] Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger, and Animashree Anandkumar. Leandojo: Theorem proving with retrieval-augmented language models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 2157321612. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ 4441469427094f8873d0fecb0c4e1cee-Paper-Datasets_and_Benchmarks.pdf. [62] Longhui Yu, Weisen Jiang, Han Shi, Jincheng YU, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=N8N0hgNDRt. [63] Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. ReST-MCTS*: LLM self-training via process reward guided tree search. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=8rcFOqEud5. [64] Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24, 2024. URL https://openreview.net/forum?id=CxHRoTLmPX. [65] Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical reasoning. In Findings of the Association for Computational Linguistics: ACL 2025 (to appear), 2025. [66] Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. Processbench: Identifying process errors in mathematical reasoning. In The 63rd Annual Meeting of the Association for Computational Linguistics (ACL 2025) (to appear), 2025. [67] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, and Zheyan Luo. LlamaFactory: Unified efficient fine-tuning of 100+ language models. In Yixin Cao, Yang Feng, and Deyi Xiong, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 400410, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-demos.38. URL https://aclanthology.org/2024.acl-demos.38/. [68] Jin Peng Zhou, Charles Staats, Wenda Li, Christian Szegedy, Kilian Weinberger, and Yuhuai Wu. Dont trust: Verify grounding LLM quantitative reasoning with autoformalization. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=V5tdi14ple."
        },
        {
            "title": "B Additional Related Work",
            "content": "C Model Access and Software Libraries . . C.1 Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Existing PRMs Compared . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Libraries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "D FOVER Dataset",
            "content": "D.1 Examples . . . . D.2 Dataset Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Dataset Creation: Formal Logic E.1 Base Dataset and Initial Response Generation for Formal Logic . . . . . . . . . . . E.2 Step-level Verification in Formal Logic . . . . . . . . . . . . . . . . . . . . . . . . Dataset Creation: Formal Proof F.1 Base Dataset and Initial Response Generation for Formal Proof . . . . . . . . . . . F.2 F.3 Step-level Proof Verification in Isabelle Informal to Formal Conversion in Formal Proof . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Input Format and Postprocessing for LLM-based PRMs G.1 Input Format . . G.2 Postprocessing . G.3 Existing PRMs . . . . . . ."
        },
        {
            "title": "H Training Settings",
            "content": "H.1 Training Data Format H.2 Training Parameters ."
        },
        {
            "title": "I Evaluation Settings",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . I.1 Evaluation Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . I.2 Downstream Evaluation: Best-of-K . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "J Additional Results",
            "content": "J.1 Reference Methods for Best-of-K: Majority-of-K and Pass@K. . . . . . . . . . . . 16 16 17 17 17 17 18 20 21 21 21 22 22 22 28 28 28 29 30 31 32 32 32 33 33 J.2 Hyperparameters that Influence the Cross-task Generalization of Training on FOVER 34 J.3 Manual Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "L Computational Resources and Execution Time",
            "content": "41"
        },
        {
            "title": "A Limitations",
            "content": "We expect future research to explore the following limitations in this paper. Small model size. Due to computational constraints, this work evaluates PRMs only with 8Bclass LLMs. Prior work also typically uses these relatively small LLMs to create LLM-based PRMs [66, 65, 10], but evaluating larger models could show further insights into the scalability. Evaluation approaches. We evaluate PRMs using the step-level verification task in ProcessBench and Best-of-K performance on reasoning tasks, both of which are standard practices for assessing PRMs [18, 50, 66, 65]. recent paper [65] employs the same combination of evaluation approaches, which we take as support for the sufficiency of our current evaluation setting. Nonetheless, additional evaluation methods, such as step-level search [23, 43] and reinforcement learning [32, 63], would offer further insight into the performance of our PRMs. Difficulty of source tasks for our training data. Our training data in this project targets improving PRMs based on 8B-class LLMs and includes tasks that may not be difficult for larger state-of-the-art LLMs, such as GSM8K-level math reasoning tasks in the formal theorem proof task. When applying our approach to improve PRMs based on stronger models, we may need to use more difficult tasks to make the LLMs introduce meaningful mistakes. Methods to improve cross-task generalization. Our approach to generalizing PRMs to broad reasoning tasks relies on the inherent cross-task generalization capabilities of LLMs. Although our results already exhibit an effective cross-task generalization (4.5), we expect future work to explore transfer learning methods to further enhance cross-task generalization. For example, using pretraining data during fine-tuning, technique known as rehearsal [42, 40], is popular approach to avoid catastrophic forgetting. In another direction, Morishita et al. [27] show that training on their formal logic datasets with the RecAdam optimizer [4] improves LLMs on broad reasoning tasks."
        },
        {
            "title": "B Additional Related Work",
            "content": "Applications of PRMs. PRMs can be used to supervise LLM reasoning during training and inference. First, for training, PRMs can generate reward signals, particularly in reinforcement learning settings [32, 63]. They can be applied either to re-rank candidate responses from the policy or to provide direct reward supervision [49]. Second, for inference-time improvement, PRMs can guide response selection and refinement through methods such as Best-of-K [17], self-correction [38, 24], and step-level search [23, 43]. Cross-task generalization in LLMs. Our work is the first to conduct an in-depth analysis of crosstask generalization in PRMs, although cross-task generalization in LLMs has been widely studied in general tasks. Early studies of LLMs, such as T5 [37] and FLAN [53], already observe their generalization to unseen tasks. Easy-to-hard generalization [3, 9, 44] is challenging case, where LLMs trained on simpler tasks show improved performance on more complex ones. Directly related to our work, recent work [27, 56] shows that synthetic and symbolic training data can improve general reasoning capabilities of LLMs. These successful observations motivate us to use formal logic and theorem proof to improve LLM-based PRMs in diverse out-of-distribution reasoning tasks, assuming the generalization of LLMs in verification capabilities."
        },
        {
            "title": "C Model Access and Software Libraries",
            "content": "This section provides details of LLMs and libraries used in our experiments. C.1 Models We use models that are provided at Hugging Face Hub. models. Base and Qwen/Qwen2.5-7B-Instruct as base models for our PRMs. We also use these models to generate initial responses used in creating the FOVER dataset, and also for generating = 5 responses in the Best-of-K evaluation (4.1). meta-llama/Llama-3.1-8B-Instruct use We in Conversion use As meta-llama/Llama-3.3-70B-Instruct for converting informal statements and solutions to the Isabelle format. explained"
        },
        {
            "title": "Section",
            "content": "Proof. we in 3, C.2 Existing PRMs Compared This section provides details of state-of-the-art PRMs we evaluate in Section 4. Table 8 shows the details of the models we evaluate. We acquire these models at Hugging Face Hub and use vLLM [15] to generate reward scores. Table 8: State-of-the-art PRMs we evaluate in Section 4.1. PRMs Source Base Datasets Error Annotation RLHFlow-Llama3.1-8B-DeepSeek [59] RLHFlow/Llama3.1-8B-PRM-Deepseek-Data GSM8K, MATH Math-Shepherd [50] RLHFlow-Llama3.1-8B-Mistral [59] RLHFlow/Llama3.1-8B-PRM-Mistral-Data GSM8K, MATH Math-Shepherd [50] Qwen2.5-Math-7B-PRM800K [66] Qwen/Qwen2.5-Math-7B-PRM800K MATH Human annotation Qwen2.5-Math-PRM-7B [65] Qwen/Qwen2.5-Math-PRM-7B Private Data Math-Shepherd [50] & LLM-as-a-Judge Qwen2.5-7B-Skywork-PRM [10] Skywork/Skywork-o1-Open-PRM-Qwen-2.5-7B Hidden Hidden C.3 Libraries Inference Code. We use vLLM [15] for accelerating LLM inference. Training Code. We use LLaMA-Factory [67] for training. 1https://docs.vllm.ai/en/latest/models/supported_models.html# reward-modeling-task-reward"
        },
        {
            "title": "D FOVER Dataset",
            "content": "This section provides details of our FOVER dataset. D.1 Examples We provide examples from the FOVER dataset. FOVER-FormalLogic. Here is an example of the data in the formal logic task based on the initial responses from Llama 3.1 8B. { 'id': 'fldx2-train-058709_Llama-3.1-8B-Instruct', 'error_labels': [True, False, False, True, False], 'messages': [ { 'role': 'user', 'content': '** Problem ** Based on the provided facts ($context$), either prove or disprove the (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) hypothesis or state that it is unknown. The facts and the hypothesis are written in logical formulas as follows: capital letters such as \"{A}\", \"{B}\", \"{AB}\" are predicates, small letters such as \"{a}\", \"{b}\", \"{ab}\" are constants, \"&\" is logical conjunction, \"v\" is logical disjunction, \"\" is negation, \"->\" is implication, \"(x)\" is \"for all x\", and \"(Ex)\" is \"for some x\". $hypothesis$: {AB}{a} $context$: fact1: ({B}{a} & {A}{a}) -> {A}{dk} fact2: ({AQ}{et} & {DF}{et}) fact3: {CO}{a} -> ({DR}{a} & {CK}{a}) fact4: {HS}{a} fact5: (x): {C}x -> ({B}x & {A}x) fact6: {JF}{a} -> {AA}{a} fact7: ({JJ}{du} & {AA}{du}) fact8: ({BB}{a} & {HQ}{a}) fact9: {AA}{a} fact10: {A}{a} -> {AA}{a} fact11: {DK}{a} fact12: {E}{b} -> ({D}{b} & {C}{b}) fact13: ({D}{b} & {C}{b}) -> {C}{a} fact14: {A}{ci} fact15: {JC}{a} fact16: {A}{r} -> {DQ}{r} fact17: {A}{a} -> ({AA}{a} & {AB}{a}) fact18: ({HP}{a} & {C}{a}) fact19: {A}{jg} ** Task ** Your task is to evaluate the accuracy of each step in the provided (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) solution to the above question. For each step, respond with \"correct\" if the reasoning is logically valid and mathematically sound, or if the step is general statement or transition that does not contain reasoning. Respond with \"incorrect\" if the step includes any errors or flawed logic. ** Sotluion ** fact17 & fact10 -> int1: {A}{a} -> {AB}{a}' }, {'role': 'assistant', 'content': 'correct'}, {'role': 'user', 'content': 'fact10 -> int2: {A}{a}'}, {'role': 'assistant', 'content': 'incorrect'}, {'role': 'user', 'content': 'int2 -> int3: {AB}{a}'}, 18 {'role': 'assistant', 'content': 'incorrect'}, {'role': 'user', 'content': 'int3 -> hypothesis'}, {'role': 'assistant', 'content': 'correct'}, {'role': 'user', 'content': 'The final answer is DISPROVED'}, {'role': 'assistant', 'content': 'incorrect'} ] FOVER-FormalProof. Here is an example of the data in the formal proof task based on the initial responses from Llama 3.1 8B. { ] 'id': 'bigmath_orca_math_138943_Llama-3.1-8B-Instruct', 'error_labels': [True, False, True, True, True], 'messages': [ { 'role': 'user', 'content': '** Problem **"
        },
        {
            "title": "Generate a proof for the following theorem in the Isabelle proof assistant",
            "content": "(cid:44) format. theorem example: assumes \"(Stella_Artois_speed::real) = 150\" and \"(total_distance::real) = 500\" and \"(total_time::real) = 1.694915254237288\" and \"(first_car_speed::real) = (total_distance - Stella_Artois_speed * (cid:44) total_time) / total_time\" shows \"first_car_speed = 145.2\" (cid:44) ** Task ** Your task is to evaluate the accuracy of each step in the provided solution to the above question. For each step, respond with \"correct\" if the reasoning is logically valid and mathematically sound, or if the step is general statement or transition that does not contain reasoning. Respond with \"incorrect\" if the step includes any errors or flawed logic. (cid:44) (cid:44) (cid:44) (cid:44) ** Sotluion ** have \"Stella_Artois_speed * total_time = 150 * total_time\"' }, {'role': 'assistant', 'content': 'correct'}, { 'role': 'user', 'content': 'then have \"Stella_Artois_speed * total_time = 254.1794318175\" then have \"total_distance - Stella_Artois_speed * total_time = (cid:44) 245.8205681825\"' }, {'role': 'assistant', 'content': 'incorrect'}, {'role': 'user', 'content': 'then have \"(total_distance - Stella_Artois_speed * total_time) / total_time = 145.2\"'}, (cid:44) {'role': 'assistant', 'content': 'correct'}, {'role': 'user', 'content': 'then have \"first_car_speed = 145.2\"'}, {'role': 'assistant', 'content': 'correct'}, {'role': 'user', 'content': 'thus ?thesis'}, {'role': 'assistant', 'content': 'correct'} 19 D.2 Dataset Statistics This section provides the detailed statistics of the FOVER dataset. Please also refer to Table 2. Raw dataset. Table 9 shows the statistics of the raw FOVER dataset, which is the full version of Table 2b. Table 9: Statistics of the FOVER dataset. Train Validation Test Responses Tasks Solution-level Step-level Solution-level Step-level Solution-level Step-level # % Error # % Error # % Error # % Error # % Error # % Error Llama 3.1 8B Formal Logic Formal Proof 10000 10000 70.0% 43082 70.0% 69532 43.8% 360 13.3% 360 70.0% 1428 70.0% 2286 44.5% 360 14.0% 70.0% 1417 70.0% 2499 Qwen 2.5 7B Formal Logic Formal Proof 10000 10000 70.0% 34879 70.0% 69452 41.3% 360 14.1% 70.0% 1208 70.0% 2297 39.2% 360 16.7% 360 70.0% 1225 70.0% 2501 45.2% 13.8% 41.5% 16.6% Dataset Creation: Formal Logic This section provides details of the creation process of the formal logic task in the FOVER dataset, outlined in Section 3. E.1 Base Dataset and Initial Response Generation for Formal Logic We use FLDx2 [27] as base dataset for our formal logic task. We use the symbolic version of the dataset and remove cases where step-level verification is difficult. Specifically, we removed cases whose reasoning steps include assump, which is used in cases such as proof by contradiction. The following is an example of few-shot demonstration for the initial generation in our formal logic task. We provide six examples as demonstration. { 'role': 'user', 'content': 'Based on the provided facts ($context$), either prove or disprove the (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) hypothesis or state that it is unknown. The facts and the hypothesis are written in logical formulas as follows: capital letters such as \"{A}\", \"{B}\", \"{AB}\" are predicates, small letters such as \"{a}\", \"{b}\", \"{ab}\" are constants, \"&\" is logical conjunction, \"v\" is logical disjunction, \"\" is negation, \"->\" is implication, \"(x)\" is \"for all x\", and \"(Ex)\" is \"for some x\". Don't generate anything other than the proof and proof_label. Don't generate the proof by contradiction. If the hypothesis is disproved, (cid:44) (cid:44) (cid:44) provide direct counterexample. If the proof_label is PROVED, your proof should end with the 'hypothesis'. If the proof_label is DISPROVED, your proof should end with 'hypothesis'. $hypothesis$: ({AB}{c} & {B}{c}) $context$: fact1: (x): {D}x -> ({A}x & {C}x) fact2: ({B}{a} & {A}{a}) fact3: {AB}{b} -> {C}{d} fact4: {C}{a} -> {A}{d} fact5: {A}{a} fact6: ({AB}{c} & {B}{c}) -> {C}{d} fact7: ({F}{b} {D}{b}) -> {D}{a} fact8: {A}{a} -> ({AB}{c} & {B}{c}) fact9: ({AB}{c} & {AA}{c}) -> {B}{b} fact10: ({AB}{c} & {B}{c}) -> {C}{b}' }, { } 'role': 'assistant', 'content': '$proof$: fact6 -> int1: {C}{d} -> ({AB}{c} & {B}{c}); fact3 & int1 -> int2: {AB}{b} -> ({AB}{c} & {B}{c}); $proof_label$: UNKNOWN' E.2 Step-level Verification in Formal Logic We use Z3 [7] for step-level verification of the formal logic task. Specifically, we use the checker provided by the FLDx2 paper [27].2 As explained in Figure 3a, we first postprocess each step in solutions to independent logical step and check the validity using Z3. 2https://github.com/hitachi-nlp/FLD-generator/blob/00d12c4a9132a4fb43cd77f24db03ea7f5b27877/ FLD_generator/formula_checkers/z3_logic_checkers/checkers.py#L179 21 Dataset Creation: Formal Proof This section provides details of the creation process of the formal proof task in the FOVER dataset, outlined in Section 3. F.1 Base Dataset and Initial Response Generation for Formal Proof We use informal responses from LLMs to GSM8K-level math word problems: GSM8K [6], GSM8Kbased cases in MetaMathQA [62], and math word problems in Big-Math [1]. We select these relatively simple math reasoning problems to make the informal-to-formal conversion feasible. Math reasoning problems that involve more complex concepts make it difficult for existing LLMs to convert the solutions into Isabelles format because they are often not well-trained on Isabelle, but we expect further work to create training datasets based on those cases to improve PRMs further. The following is an example of few-shot demonstration for the initial generation from GSM8K. We use different set of few-shot demonstrations for each dataset. Refer to our code for further details. [ { 'role': 'user', 'content': 'There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?' (cid:44) (cid:44) }, { 'role': 'assistant', 'content': 'There were originally 15 trees in the grove. After the grove workers planted trees today, there are now 21 trees. So, the grove workers planted 21 - 15 = 6 trees today. Therefore, the answer (arabic numerals) is 6.' } ] F.2 Informal to Formal Conversion in Formal Proof As explained in Section 3, we use meta-llama/Llama-3.3-70B-Instruct with few-shot prompting for converting the informal solutions from LLMs into the Isabelle format. Our conversion is based on the ideas in prior work [55, 68]. The following is an example of few-shot demonstration for the conversion. Few-Shot Demonstration for Statement Conversion { }, 'role': 'user', 'content': 'Your task is to convert the following informal statement into (cid:44) formal statement in Isabelle 2022. (cid:44) * Make your formal statement consistent with the provided Informal Statement. * The final answer can be wrong, but your formal statement should be faithful to the informal statement and should not correct the mistakes in the informal statement. (cid:44) * You should not use new variables in \"shows\". * The \"shows\" part is expected to be formatted as \"variable = number\" (e.g., (cid:44) \"x = 3\"). (cid:44) ** Informal Statement ** Gloria wants to buy the $129,000 mountain cabin that her friend Alfonso is selling. She only has $150 in cash. She intends to raise the remaining amount by selling her mature trees for lumber. She has 20 cypress trees, 600 pine trees, and 24 maple trees. She will get $100 for each cypress tree, $300 for maple tree, and $200 per pine tree. After paying Alfonso for the cabin, how much money will Gloria have left?' (cid:44) (cid:44) (cid:44) (cid:44) 22 { } { 'role': 'assistant', 'content': 'theorem example: (* Gloria wants to buy the $129,000 mountain cabin that her friend Alfonso is selling. *) (cid:44) assumes \"(Cabin_cost::nat) = 129000\" (* She only has $150 in cash. *) and \"(Gloria_cash::nat) = 150\" (* She intends to raise the remaining amount by selling her mature trees for lumber. *) (cid:44) (* She has 20 cypress trees, 600 pine trees, and 24 maple trees. *) and \"(Cypress_count::nat) = 20\" and \"(Pine_count::nat) = 600\" and \"(Maple_count::nat) = 24\" (* She will get $100 for each cypress tree, $300 for maple tree, and (cid:44) $200 per pine tree. *) and \"(Cypress_price::nat) = 100\" and \"(Maple_price::nat) = 300\" and \"(Pine_price::nat) = 200\" (* After paying Alfonso for the cabin, how much money will Gloria have (cid:44) left? *) and \"(After_paying_Alfonso::nat) = Gloria_cash + Cypress_count * Cypress_price + Maple_count * Maple_price + Pine_count * Pine_price - Cabin_cost\" (* Final Answer -- The answer is 200. *) shows \"After_paying_Alfonso = 200\"' (cid:44) (cid:44) Few-Shot Demonstration for Proof Conversion 'role': 'user', 'content': 'Your task is to convert the following informal proof into formal (cid:44) proof in Isabelle 2022. * The input informal proof can be wrong, but your formal proof should be (cid:44) faithful to the informal proof and should not correct the mistakes in the informal proof. (cid:44) * In your formal proof, use variables defined in the provided Formal Statement. (cid:44) * Use sledgehammer. You should use defined variables whenever possible and should not write equations that only contain numbers. (cid:44) * The last step is expected to be the same as the equation shown in the shows section of the Formal Statement. (cid:44) * Include informal statements and proof as comments. (cid:44) ** Informal Statement ** Gloria wants to buy the $129,000 mountain cabin that her friend Alfonso is selling. She only has $150 in cash. She intends to raise the remaining amount by selling her mature trees for lumber. She has 20 cypress trees, 600 pine trees, and 24 maple trees. She will get $100 for each cypress tree, $300 for maple tree, and $200 per pine tree. After paying Alfonso for the cabin, how much money will Gloria have left? (cid:44) (cid:44) (cid:44) (cid:44) Final Answer -- The answer is 200. ** Informal Proof ** Gloria needs 129,000 dollars for the cabin, but she only has 150 dollars. So she needs 129,000 - 150 = 128,850 dollars. She will get 100 dollars for each of the 20 cypress trees, which is 20 100 = 2,000 dollars. (cid:44) She will get 300 dollars for each of the 24 maple trees, which is 24 300 = 7,200 dollars. (cid:44) She will get 200 dollars for each of the 600 pine trees, which is 600 200 = (cid:44) 120,000 dollars. In total, she will get 2,000 + 7,200 + 120,000 = 129,200 dollars from selling the trees. (cid:44) After selling the trees, she will have 129,200 dollars. She will pay 129,000 dollars for the cabin, so she will have 129,200 - 129,000 = 200 dollars left. (cid:44) Therefore, the answer (arabic numerals) is 200. ** Formal Statement ** theorem example: (* Gloria wants to buy the $129,000 mountain cabin that her friend Alfonso is selling. *) (cid:44) assumes \"(Cabin_cost::nat) = 129000\" (* She only has $150 in cash. *) and \"(Gloria_cash::nat) = 150\" (* She intends to raise the remaining amount by selling her mature trees for lumber. *) (cid:44) (* She has 20 cypress trees, 600 pine trees, and 24 maple trees. *) and \"(Cypress_count::nat) = 20\" and \"(Pine_count::nat) = 600\" and \"(Maple_count::nat) = 24\" (* She will get $100 for each cypress tree, $300 for maple tree, and (cid:44) $200 per pine tree. *) and \"(Cypress_price::nat) = 100\" and \"(Maple_price::nat) = 300\" and \"(Pine_price::nat) = 200\" (* After paying Alfonso for the cabin, how much money will Gloria have (cid:44) left? *) and \"(After_paying_Alfonso::nat) = Gloria_cash + Cypress_count * Cypress_price + Maple_count * Maple_price + Pine_count * Pine_price - Cabin_cost\" (* Final Answer -- The answer is 200. *) shows \"After_paying_Alfonso = 200\"' (cid:44) (cid:44) }, { 'role': 'assistant', 'content': 'theorem example: (* Gloria wants to buy the $129,000 mountain cabin that her friend Alfonso is selling. *) (cid:44) assumes \"(Cabin_cost::nat) = 129000\" (* She only has $150 in cash. *) and \"(Gloria_cash::nat) = 150\" (* She intends to raise the remaining amount by selling her mature trees for lumber. *) (cid:44) (* She has 20 cypress trees, 600 pine trees, and 24 maple trees. *) and \"(Cypress_count::nat) = 20\" and \"(Pine_count::nat) = 600\" and \"(Maple_count::nat) = 24\" (* She will get $100 for each cypress tree, $300 for maple tree, and (cid:44) $200 per pine tree. *) and \"(Cypress_price::nat) = 100\" and \"(Maple_price::nat) = 300\" and \"(Pine_price::nat) = 200\" (* After paying Alfonso for the cabin, how much money will Gloria have (cid:44) left? *) and \"(After_paying_Alfonso::nat) = Gloria_cash + Cypress_count * Cypress_price + Maple_count * Maple_price + Pine_count * Pine_price - Cabin_cost\" (* Final Answer -- The answer is 200. *) shows \"After_paying_Alfonso = 200\" (cid:44) (cid:44) proof - (* Gloria needs 129,000 dollars for the cabin, but she only has 150 dollars. *) (cid:44) (* So she needs 129,000 - 150 = 128,850 dollars. *) have \"Cabin_cost - Gloria_cash = 128850\" sledgehammer 24 (* She will get 100 dollars for each of the 20 cypress trees, which is 100 = 2,000 dollars. *) (cid:44) then have \"Cypress_count * Cypress_price = 2000\" sledgehammer (* She will get 300 dollars for each of the 24 maple trees, which is 24 300 = 7,200 dollars. *) (cid:44) then have \"Maple_count * Maple_price = 7200\" sledgehammer (* She will get 200 dollars for each of the 600 pine trees, which is 600 200 = 120,000 dollars. *) (cid:44) then have \"Pine_count * Pine_price = 120000\" sledgehammer (* In total, she will get 2,000 + 7,200 + 120,000 = 129,200 dollars from selling the trees. *) (cid:44) then have \"Cypress_count * Cypress_price + Maple_count * Maple_price + (cid:44) Pine_count * Pine_price = 129200\" sledgehammer (* After selling the trees, she will have 129,200 dollars. *) (* She will pay 129,000 dollars for the cabin, so she will have 129,200 - 129,000 = 200 dollars left. *) (cid:44) then have \"(Cypress_count * Cypress_price + Maple_count * Maple_price + (cid:44) Pine_count * Pine_price) - Cabin_cost = 200\" sledgehammer (* Therefore, the answer (arabic numerals) is 200. *) then have \"After_paying_Alfonso = 200\" sledgehammer show ?thesis sledgehammer qed' } F.3 Step-level Proof Verification in Isabelle Sledgehammer in Isabelle. From the above conversion process, we get formal statements and proofs with sledgehammer. In Isabelle, we need to manually specify lemmas and theorems to construct proofs. For example, in the below toy example, we need to manually specify rule add_assoc to properly prove the theorem. theory Addition_Assoc_Manual imports Main begin lemma add_assoc_manual: \"(a + b) + = + (b + c)\" proof - show ?thesis by (rule add_assoc) qed end However, this process can be labor-intensive and time-consuming. Isabelles Sledgehammer tool [34] automates this step by invoking external automated theorem provers (ATPs) to suggest proof steps or entire proofs. It translates the current proof state into form suitable for ATPs, runs them, and then attempts to reconstruct the suggested proof within Isabelles logical framework. This significantly reduces the manual effort required, improves productivity, and bridges the gap between interactive and automated proving. Using Sledgehammer, we can simplify the above theorem as follows to verify it using Isabelle. theory Addition_Assoc_Sledgehammer imports Main begin lemma add_assoc_manual: \"(a + b) + = + (b + c)\" proof - show ?thesis sledgehammer qed end We use Sledgehammer to facilitate the informal-to-formal conversion by removing the need to specify lemmas during conversion. Note that we use wrapper for Isabelle [54] for using Sledgehammer. Format verification. As we use LLMs for the conversion, it is possible that the conversion generates an invalid format that cannot be verified using Isabelle. To filter out theorems in the invalid format, we use the sorry keyword of Isabelle. The sorry keyword serves as placeholder for incomplete or unproven proofs, allowing the theorem to be accepted by the system without formal justification. By inserting sorry into all generated proof steps, we can isolate and verify only the syntactic and semantic well-formedness of theorems. For example, if the following theorem gets errors from Isabelle, we can detect syntax errors and filter this theorem out from our dataset. In this case, the expression babysitting_minutes (Weng_hourly_wage / 60) contains the symbol , which is not valid multiplication operator in Isabelle syntax. theorem example: assumes \"(Weng_hourly_wage::real) = 12\" and \"(babysitting_minutes::real) = 50\" and \"(babysitting_hours::real) = babysitting_minutes / 60\" and \"(Weng_earnings::real) = Weng_hourly_wage * babysitting_hours\" shows \"Weng_earnings = 10\" proof - have \"Weng_hourly_wage / 60 = 0.20\" sorry then have \"babysitting_minutes (Weng_hourly_wage / 60) = 10\" sorry then have \"Weng_earnings = 10\" sorry thus ?thesis sorry qed For this input, Isabelle returns the following error. Step error: Inner syntax error (line 1)nat \"? ( Weng_hourly_wage / 60 ) = (cid:44) 10\"nFailed to parse propnAt command \"have\" (line 1) Step-level verification. By default, Isabelle halts at the first encountered error and does not provide step-by-step verification of proof. To enable independent verification of each step in multi-step proof, we insert the sorry keyword in all but one step. This allows Isabelle to type-check and parse each step individually, even if other steps are incomplete or invalid. The following example is for verifying the third step independently. For each theorem, we run Isabelle once per step to isolate and validate its correctness. theorem example: assumes \"(wallet_cost::nat) = 100\" and \"(betty_savings::nat) = wallet_cost div 2\" and \"(parent_contribution::nat) = 15\" and \"(grandparent_contribution::nat) = 2 * parent_contribution\" and \"(total_savings::nat) = betty_savings + parent_contribution + grandparent_contribution\" (cid:44) and \"(additional_needed::nat) = wallet_cost - total_savings\" shows \"additional_needed = 5\" 26 proof - have \"betty_savings = wallet_cost div 2\" sorry then have \"betty_savings = 50\" sorry have \"grandparent_contribution = 2 * parent_contribution\" sledgehammer then have \"grandparent_contribution = 30\" sorry then have \"parent_contribution + grandparent_contribution = 45\" sorry then have \"total_savings = 95\" sorry then have \"additional_needed = wallet_cost - total_savings\" sorry then have \"additional_needed = 5\" sorry thus ?thesis sorry 27 Input Format and Postprocessing for LLM-based PRMs This section provides details of the input format and postprocessing of LLM-based PRMs, including our FOVER PRMs, the baseline PRMs, and the state-of-the-art PRMs G.1 Input Format We describe the input format for FOVER PRMs and the baseline LLM-based PRMs, which are based on Llama 3.1 8B and Qwen 2.5 7B. FOVER PRMs are trained on the FOVER dataset, so the input format has the same format as the training data. The only difference is that we replace all step-level labels with correct in the input. This preprocessing allows us to provide the whole input once to get the step-level predictions for all steps. The following is an example input for GSM8K. [ { 'role': 'user', 'content': '** Problem ** Alice is 7 years older than Beth, who is 5 years older than Erica. What is (cid:44) (cid:44) the difference between the ages of Alice and Erica, if Erica is 30 years old? (cid:44) ** Task ** Your task is to evaluate the accuracy of each step in the provided solution to the above question. For each step, respond with \"correct\" if the reasoning is logically valid and mathematically sound, or if the step is general statement or transition that does not contain reasoning. Respond with \"incorrect\" if the step includes any errors or flawed logic. (cid:44) (cid:44) (cid:44) (cid:44) ** Sotluion ** Since Erica is 30 years old, and Beth is 5 years older than Erica, Beth is (cid:44) 30 + 5 = 35 years old.' }, {'role': 'assistant', 'content': 'correct'}, {'role': 'user', 'content': 'Alice is 7 years older than Beth, who is 35 years old. '}, (cid:44) {'role': 'assistant', 'content': 'correct'}, {'role': 'user', 'content': 'So, Alice is 35 + 7 = 42 years old.'}, {'role': 'assistant', 'content': 'correct'}, {'role': 'user', 'content': \"The difference between Alice's age and Erica's age is 42 - 30 = 12 years.\"}, (cid:44) {'role': 'assistant', 'content': 'correct'}, {'role': 'user', 'content': 'Therefore, the answer (arabic numerals) is 12.'}, {'role': 'assistant', 'content': 'correct'} ] G.2 Postprocessing We describe the postprocessing for FOVER PRMs and the baseline LLM-based PRMs. Extracting logits. Since we use causal LLMs as PRMs, we extract the models predictions for the tokens immediately preceding the dummy step-level labels (e.g., correct) in the input. Computing step-level scores. At each identified position, we extract the logits corresponding to the tokens correct and incorrect. We then apply the softmax function over these two logits to compute the probability assigned to the correct token. This probability serves as the step-level score in our PRMs. 28 G.3 Existing PRMs PRMs based on Llama 3.1 8B. In RLHFlow-Llama3.1-8B-DeepSeek and RLHFlow-Llama3.18B-Mistral [59], the input format is mostly similar to ours, with the key difference being the use of + and - instead of correct and incorrect.3 For these models, we apply our input format and postprocessing procedures with simple substitution of correct with +. PRMs based on Qwen 2.5 7B. Qwen2.5-Math-7B-PRM800K [66] and Qwen2.5-Math-PRM-7B [65] are supported by vLLM [15]. We follow the input format specified in their respective model descriptions and adopt the reward modeling implemented in vLLM.4 For Qwen2.5-7B-SkyworkPRM [10], we use code provided by the authors.5 3https://github.com/RLHFlow/RLHF-Reward-Modeling/tree/main/math-rm 4https://docs.vllm.ai/en/latest/models/pooling_models.html 5https://github.com/SkyworkAI/skywork-o1-prm-inference"
        },
        {
            "title": "H Training Settings",
            "content": "This section provides details of the training settings for our FOVER PRMs. H.1 Training Data Format To balance step-level error labels (correct or incorrect), we set mask_history: True in LLaMA-Factory [67], which configures the model to use only the last step of each conversation during training. We postprocess the training data by truncating reasoning steps so that the final steps are balanced between correct and incorrect labels. This approach is functionally equivalent to selectively masking certain steps to achieve label balance, but is more straightforward to implement within LLaMA-Factory. For example, from the case in Appendix D.1, we can make training case for the incorrect label. In each instance, models are only trained on the prediction for single token, correct or incorrect in the last step, and the other tokens are masked. { ] 'id': 'bigmath_orca_math_138943_Llama-3.1-8B-Instruct', 'error_labels': [True, False, True, True, True], 'messages': [ { 'role': 'user', 'content': '** Problem **"
        },
        {
            "title": "Generate a proof for the following theorem in the Isabelle proof assistant",
            "content": "(cid:44) format. theorem example: assumes \"(Stella_Artois_speed::real) = 150\" and \"(total_distance::real) = 500\" and \"(total_time::real) = 1.694915254237288\" and \"(first_car_speed::real) = (total_distance - Stella_Artois_speed * (cid:44) total_time) / total_time\" shows \"first_car_speed = 145.2\" (cid:44) ** Task ** Your task is to evaluate the accuracy of each step in the provided solution to the above question. For each step, respond with \"correct\" if the reasoning is logically valid and mathematically sound, or if the step is general statement or transition that does not contain reasoning. Respond with \"incorrect\" if the step includes any errors or flawed logic. (cid:44) (cid:44) (cid:44) (cid:44) ** Sotluion ** have \"Stella_Artois_speed * total_time = 150 * total_time\"' }, {'role': 'assistant', 'content': 'correct'}, { 'role': 'user', 'content': 'then have \"Stella_Artois_speed * total_time = 254.1794318175\" then have \"total_distance - Stella_Artois_speed * total_time = (cid:44) 245.8205681825\"' }, {'role': 'assistant', 'content': 'incorrect'}, 30 H.2 Training Parameters We fine-tune all model parameters and do not use parameter-efficient techniques. We use the AdamW optimizer [21] and select the learning rate based on the average Best-of-K performance on the validation tasks: Orca-Math [26] and two tasks in BBH [45] (Logical Deduction (three objects) and Boolean Expressions). We evaluate models trained with the learning rate 1e-6, 2e-6, 5e-6, and 1e-5, and select the model with the best average performance on the validation tasks. We use the parameters in Table 10 in all models, and we did not conduct hyperparameter tuning for these parameters. Please refer to the configuration files in our code for further details. Table 10: Hyperparameters in training on FOVER"
        },
        {
            "title": "Value",
            "content": "1 32 Linear 0."
        },
        {
            "title": "I Evaluation Settings",
            "content": "This section provides details of the evaluation in Section 4. I.1 Evaluation Datasets Table 11 shows sources of datasets evaluated in Best-of-K in Section 4. Table 11: Datasets evaluated in Best-of-K experiments."
        },
        {
            "title": "Dataset",
            "content": "GSM8K MATH AQuA-RAT AIME (2016-2024) Source https://huggingface.co/datasets/openai/gsm8k https://github.com/hendrycks/math https://huggingface.co/datasets/deepmind/aqua_rat https://huggingface.co/datasets/di-zhang-fdu/AIME_1983_"
        },
        {
            "title": "BBH",
            "content": "https://huggingface.co/datasets/yale-nlp/FOLIO https://huggingface.co/datasets/tasksource/LogicNLI https://huggingface.co/datasets/facebook/anli https://github.com/tommccoy1/hans https://huggingface.co/datasets/sam-paech/mmlu-pro-nomath-sml https://github.com/suzgunmirac/BIG-Bench-Hard/tree/main/bbh I.2 Downstream Evaluation: Best-of-K This section provides detailed settings of Best-of-K experiments in Section 4. Initial generation prompts. Table 12 shows detailed settings of generating = 5 responses for the Best-of-K evaluation in Section 4.1. We create new few-shot examples or modify fewshot demonstrations used in prior work to enhance the quality and to simplify the post-processing procedure. For example, we add line breaks between reasoning steps in all tasks. An example prompt for GSM8K is provided in Appendix E.1. Please also refer to our code for further details. Table 12: Detailed settings for Best-of-K downstream evaluation"
        },
        {
            "title": "Dataset",
            "content": "GSM8K MATH AQuA-RAT AIME"
        },
        {
            "title": "BBH",
            "content": "Few-shot Examples for Initial Generation [13] 6 [16, Appendix D.2] Made by us (3-shot) Made by us (3-shot) Made by us (2-shot) Made by us (3-shot) Made by us (3-shot) Made by us (2-shot) Made by us (4-shot) [45]"
        },
        {
            "title": "Answer Matching",
            "content": "Exact match after extraction and conversion to integer [16, Appendix G] Exact match after extraction Exact match after extraction and conversion to integer"
        },
        {
            "title": "Exact match after extraction",
            "content": "6https://github.com/kojima-takeshi188/zero_shot_cot/blob/ 5ef330fcdeec0cd26aee27943504f91f8ec1c33c/utils.py#L328 7https://github.com/suzgunmirac/BIG-Bench-Hard/blob/main/cot-prompts"
        },
        {
            "title": "J Additional Results",
            "content": "This section provides additional results. J.1 Reference Methods for Best-of-K: Majority-of-K and Pass@K. Table 13 shows the performance of reference methods on datasets evaluated in Section 4.1. Majority@K Majority@K (or self-consistency) [51], which takes the majority vote of the final answers. It is known to be strong baseline and is competitive with the Best-of-K performance of our verifiers. Oracle Verification. Oracle Verification (or Pass@K) is the performance when we always select the solution with the correct final answer if it exists, which is the upper-bound of Best-of-K. Table 13: Performance of reference methods on datasets in Table 3. (a) PRMs based on Llama 3.1 8B select the best response from = 5 responses generated by Llama 3.1 8B Methods Math Logic NLI MMLU BBH Average GSM8K MATH AQuA AIME FOLIO LogicNLI ANLI HANS Pro-NoMath Temporal Tracking Sorting FOVER PRM Majority@K Oracle Verification 86.4 89.2 97. 43.2 44.4 58.4 65.7 66.1 83.9 4.0 4.8 8. 64.0 62.6 90.1 44.8 46.0 83.6 28.8 33.2 54. 82.8 76.4 94.4 57.2 58.8 75.2 97.6 92.8 100. 93.2 94.0 98.4 38.4 42.4 60.0 58.8 59.2 75. (b) PRMs based on Qwen 2.5 7B select the best response from = 5 responses generated by Qwen 2.5 7B Methods Math Logic NLI MMLU BBH Average GSM8K MATH AQuA AIME FOLIO LogicNLI ANLI HANS Pro-NoMath Temporal Tracking Sorting FOVER PRM Majority@K Oracle Verification 92.8 91.6 96. 66.4 65.2 73.2 82.3 83.9 91.7 13.2 12.8 19. 65.5 65.0 78.3 48.8 50.0 80.0 32.4 32.4 49. 85.2 84.0 89.2 60.8 61.6 75.2 91.6 88.4 93. 89.6 86.4 93.2 31.2 31.2 38.8 63.3 62.7 73. 33 J.2 Hyperparameters that Influence the Cross-task Generalization of Training on FOVER We analyze hyperparameters that may influence cross-task generalization of FOVER. We train PRMs on the FOVER dataset with different learning rates, label distributions, and dataset sizes, and evaluate performance on the validation tasks: Orca-Math [26] and two tasks in BBH [45]. However, although these parameters influence the performance of PRMs, we do not observe any consistent trends, and the performance of different models on different evaluation datasets exhibits inconsistent behaviors. These results indicate that hyperparameter tuning is necessary but remains non-trivial under the current training setup. We expect future research to explore more stable training strategies. Note that, for the models evaluated in Sections 4.2 and 4.1, we perform hyperparameter search only over the learning rate, while fixing the label distribution to 0.5 and using the dataset size of 40k. Learning rate. We evaluate PRMs trained on the FOVER datasets using different learning rates: 1e-6, 2e-6, 5e-6, and 1e-5. Figure 5a reports the average accuracy on the validation tasks for PRMs trained on the 40k-instance training set. We observe that the learning rate has substantial impact on the transferability and that the optimal learning rate varies across models and evaluation tasks. Label distribution. We evaluate PRMs trained on FOVER with varying proportions of correct and incorrect step-level labels. Specifically, we construct versions of the FOVER dataset that include both formal logic and proof tasks, with 25%, 50%, and 75% correct labels, each containing 20k instances. Figure 5b reports the average accuracy on the validation tasks. We do not observe consistent trend across label distributions; different models and evaluation datasets exhibit varying behavior. Dataset size. We evaluate PRMs trained on different sizes of the FOVER dataset, which includes both formal logic and proof tasks. Specifically, we train models on 5k, 10k, 20k, and 40k instances. To isolate the effect of training data diversity, all models are trained for the same number of steps. Figure 5c presents the accuracy on the validation tasks, showing that larger and more diverse training data often improves cross-task generalization. However, the results also indicate that increased diversity can negatively impact performance in Orca-Math, which is counterintuitive and highlights that cross-task generalization is inherently complex and highly task-dependent. (a) Learning rate. (b) Label distribution. (c) Dataset size. Figure 5: Influence of hyperparameters on the cross-task generalization of PRMs trained on FOVER. We do not observe consistent trends across different models and evaluation datasets. The thick lines represent the average accuracy in the validation tasks. The dashed lines represent performance on validation tasks: Logical Deduction, and Boolean Expressions. Orca-Math, 34 J.3 Manual Analysis We provide the detailed results of the manual analysis in Section 4.4. We analyze cases where FOVER PRMs improve or degrade re-ranking in Best-of-K with regard to the final answers from the baseline PRMs based on the original LLMs. We categorize the reasoning for the improvement and degradation of the final answers into the following categories: The baseline PRMs assign wrong step-level scores, although FOVER PRMs assign reasonable step-level scores, which represent the genuine improvement by FOVER. The original questions are ambiguous or the ground-truth labels are incorrect. The selected solutions with the correct final answers include incorrect intermediate steps. PRMs are expected to assign low scores to those solutions, although they produce the correct final answers. Both PRMs assign incorrect step-level scores, with the baseline selecting the correct answer by chance. FOVER PRMs assign wrong step-level scores, although the baseline PRMs assign reasonable step-level scores, which represent the genuine improvement by FOVER. Only the first and last categories represent cases where FOVER PRMs genuinely improve or degrade step-level verification performance compared to the baseline PRMs. Figure 6 shows the results of the manual analysis. We observe that FOVER PRMs often improve step-level verification while rarely degrade step-level scores. We note that GSM8K results of Qwen 2.5 7B and the temporal sequence results of both models only have three cases where FOVER degrades the final answers. Therefore, the 33% degradation in these cases represents that FOVER degrades step-level scores only in one case out of 250 solutions evaluated in Best-of-K, which is positive result. (a) FOVER PRMs select solutions with correct final answers while baseline PRMs select incorrect ones. (b) FOVER PRMs select solutions with incorrect final answers while baseline PRMs select correct ones. Figure 6: Manual analysis of step-level scores in cases where FOVER PRMs improve (6a) or degrade (6b) re-ranking in Best-of-K with regard to the final answers. The blue and red areas represent degrades the step-level verification scores. the cases in which FOVER genuinely The other cases do not represent genuine improvement or degradation. : The original questions are ambiguous or the ground-truth labels are incorrect. : The selected solutions with the correct final answers contain incorrect intermediate steps. : Both PRMs assign incorrect step-level scores. improves and We provide an example for each category. 35 The baseline PRMs assign wrong step-level scores, although FOVER PRMs assign reasonable step-level scores. The following example shows response from Qwen 2.5 7B on ANLI. In this example, Step 1 of the above solution is wrong, which is correctly identified by FOVER PRMs while the baseline PRM wrongly put high score. In addition, in the second solution, the baseline PRM puts low score to Step 2, which is correct. This case shows an example FOVER improves step-level verification. Problem: Premise: Cooper Manning SWAG-JACKED Don Magic Juan??? Call the fashion police ... cause Cooper Manning straight-up STOLE his Super Bowl swag from Snoops spiritual adviser, Bishop Don \"\"Magic\"\" Juan! Peyton and Elis older bro was cruisin around Super Bowl media night this week in money suit the same kind of outfit weve seen The Bishop rockin for years. Twinsies. Hypothesis: Payton and Eli Manning are both older than Cooper Manning. Question: do the premises derive the hypothesis? Provide reasoning and answer with either \"\"entailment\"\", \"\"contradiction\"\" or \"\"neutral\"\". Ground truth: neutral"
        },
        {
            "title": "Explanation",
            "content": "Solution selected by the baseline PRM (wrong final answer) Step 0 Step 1 Step 2 Step 3 Premise states, \"Peyton and Elis older bro was cruisin around Super Bowl media night this week in money suit,\" implying that Cooper Manning is the brother of Peyton and Eli Manning. Since Cooper Manning is described as the brother, it follows that Peyton and Eli Manning are older than Cooper Manning. Therefore, the premise supports the hypothesis that Peyton and Eli Manning are both older than Cooper Manning. The final answer is entailment. Solution-level Score Solution selected by FOVER PRMs (correct final answer) Step 0 Step 1 Step 2 Step Solution-level Score Premise states: \"Peyton and Elis older bro was cruisin around Super Bowl media night this week in money suit.\" This implies that Cooper Manning is the older brother of Peyton and Eli Manning. Given that Cooper Manning is the older brother, it follows that Peyton and Eli Manning are both younger than Cooper Manning. Therefore, the hypothesis \"Peyton and Eli Manning are both older than Cooper Manning\" is contradicted by the premise. The final answer is contradiction."
        },
        {
            "title": "FOVER\nPRM",
            "content": "1.0000 0.9609 1.0000 0.5625 1.0000 1. 1.0000 0.9961 1.0000 0.5625 0.9922 0. 1.0000 0.9922 0.0000 0.9531 0.0000 0. 0.9961 0.9258 36 The original questions are ambiguous or the ground-truth labels are incorrect. The following example shows response generated from Llama 3.1 8B on ANLI. In this case, the ground truth answer is neutral. However, this problem is ambiguous, and both neutral and entailment are reasonable final answers. Therefore, this case does not represent degradation by FOVER. Problem: Premise: Trying Tofu<br>Alicia was feeling little adventurous. She wanted to try Tofu. She had never had it before. So she took small bite. To her delight it was delicious. Hypothesis: Tofu contains the letters to Question: do the premises derive the hypothesis? Provide reasoning and answer with either \"\"entailment\"\", \"\"contradiction\"\" or \"\"neutral\"\". Ground truth: neutral Step Explanation Solution selected by the baseline PRM (correct final answer) Step 0 Step 1 Step Solution-level Score The premise does not mention the letters in the word \"tofu\" at all. It talks about Alicia trying tofu and enjoying it. The premise does not provide any information about the letters in the word \"tofu.\" Therefore, the final answer is neutral. Solution selected by FOVER PRMs (wrong final answer) Step 0 The premise mentions the word \"tofu\", which contains the letters \"to\". The premise does not say anything about the taste, nutritional content, or any other properties of tofu, but it does provide evidence that the word \"tofu\" contains the letters \"to\". Therefore, the final answer is entailment. Step 1 Step Solution-level Score Baseline PRM FOVER PRM 0.3496 0.9922 0.9453 0. 0.2813 0.9922 0.5313 0.2813 0.7773 0.9922 0. 0.7773 0.9805 0.9727 0.9570 0.9570 Solutions selected by the baseline contain incorrect intermediate steps, although producing the correct final answers. The following example shows response generated from Llama 3.1 8B on the word sorting task in BBH. In the first solution, the final answer is correct; however, the first step includes [\"farther\" \"insolent\"] in the sorted list, which is incorrect. When intermediate steps contain such errors, PRMs are expected to assign lower scores, potentially causing them to fail in selecting solutions with correct final answers. This behavior is expected and does not indicate that FOVER degrades verification performance. Problem: Sort the following words alphabetically: List: stephen amperage protocol crimea farther raillery tech insolent ping Ground truth: amperage crimea farther insolent ping protocol raillery stephen tech Step Explanation Solution selected by the baseline PRM (correct final answer) Step 0 Step 1 Step 2 Step 3 First, we look at the first letters of each word. \"stephen\": \"s\" (19th letter). \"amperage\": \"a\" (1st letter). \"protocol\": \"p\" (16th letter). \"crimea\": \"c\" (3rd letter). \"farther\": \"f\" (6th letter). \"raillery\": \"r\" (18th letter). \"tech\": \"t\" (20th letter). \"insolent\": \"i\" (9th letter). \"ping\": \"p\" (16th letter). We now have: [\"amperage\"] < \"crimea\" < [\"farther\" \"insolent\"] < [\"ping\" \"protocol\"] < \"raillery\" < \"stephen\" < \"tech\". Now lets sort this subpart [\"farther\" \"insolent\"] by looking at their second letters. The second letter: \"farther\": \"a\" (1st letter). \"insolent\": \"n\" (14th letter). We now have: \"farther\" < \"insolent\" < [\"ping\" \"protocol\"] < \"raillery\" < \"stephen\" < \"tech\". Now lets sort this subpart [\"ping\" \"protocol\"] by looking at their second letters. The second letter: \"ping\": \"i\" (9th letter). \"protocol\": \"r\" (18th letter). We now have: \"ping\" < \"protocol\" < \"raillery\" < \"stephen\" < \"tech\". So the answer is amperage < crimea < farther < insolent < ping < protocol < raillery < stephen < tech. Solution-level Score Solution selected by FOVER PRMs (wrong final answer) First, we look at the first letters of each word. \"stephen\": \"s\" (19th letter). \"amperage\": \"a\" (1st letter). \"protocol\": \"p\" (16th letter). \"crimea\": \"c\" (3rd letter). \"farther\": \"f\" (6th letter). \"raillery\": \"r\" (18th letter). \"tech\": \"t\" (20th letter). \"insolent\": \"i\" (9th letter). \"ping\": \"p\" (16th letter). We now have: [\"amperage\"] < \"crimea\" < \"farther\" < [\"protocol\" \"ping\"] < \"insolent\" < \"raillery\" < \"stephen\" < \"tech\". Now lets sort this subpart [\"protocol\" \"ping\"] by looking at their second letters. The second letter: \"protocol\": \"r\" (18th letter). \"ping\": \"i\" (9th letter). We now have: \"amperage\" < \"crimea\" < \"farther\" < \"ping\" < \"protocol\" < \"insolent\" < \"raillery\" < \"stephen\" < \"tech\". Now lets sort this subpart [\"ping\" \"protocol\"] by looking at their third letters. The third letter: \"ping\": \"n\" (14th letter). \"protocol\": \"o\" (15th letter). We now have: \"amperage\" < \"crimea\" < \"farther\" < \"ping\" < \"protocol\" < \"insolent\" < \"raillery\" < \"stephen\" < \"tech\". So the answer is amperage < crimea < farther < ping < protocol < insolent < raillery < stephen < tech. Step 0 Step 1 Step Step 3 Solution-level Score Baseline PRM FOVER PRM 0.5938 0. 0.9883 0.6523 0.9844 0.9258 0.9961 0. 0.5938 0.1641 0.5625 0.1826 0.9961 0. 0.9727 0.9844 0.9922 0.8672 0.5625 0. 38 Both PRMs assign incorrect step-level scores, with the baseline selecting the correct answer by chance. The following example shows response generated from Llama 3.1 8B on GSM8K. In this case, both PRMs assign high scores to both solutions, indicating that neither effectively distinguishes correctness. The baseline PRM happens to select the better solution, but this is due to chance rather than meaningful score differences. Since both PRMs are unsuccessful, this case does not show the degradation by FOVER. Problem: Mark is copy-editor. He edits an equal number of sentences each week for two different publishers, who each pay him different rate per sentence. Publisher pays Mark twice what Publisher pays. Mark edits total number of 1000 sentences each week, and Publisher pays him 5 cents per sentence. How much does Mark make in week, in cents? Ground truth: 7500 Step Explanation Solution selected by the baseline PRM (correct final answer) Step 0 Mark edits total of 1000 sentences each week. He edits an equal number of sentences for both Publisher and Publisher B, so he edits 1000 / 2 = 500 sentences for Publisher and 500 sentences for Publisher B. Publisher pays 5 cents per sentence, so Mark earns 500 * 5 = 2500 cents from Publisher A. Publisher pays twice what Publisher pays, so Publisher pays 2 * 5 = 10 cents per sentence. Mark earns 500 * 10 = 5000 cents from Publisher B. In total, Mark earns 2500 + 5000 = 7500 cents per week. Therefore, the answer (arabic numerals) is 7500. Solution selected by FOVER PRMs (wrong final answer) Step 0 Mark edits 1000 sentences each week. Publisher pays him 5 cents per sentence, so Publisher pays him 1000 5 = 5000 cents per week. Since Publisher pays twice what Publisher pays, Publisher pays 2 5000 = 10,000 cents per week. The total amount Mark makes in week is 5000 + 10,000 = 15,000 cents. Therefore, the answer (arabic numerals) is 15000. Step Step 2 Step 3 Step 4 Step 5 Solution-level Score Step 1 Step Step 3 Step 4 Solution-level Score Baseline PRM FOVER PRM 0.9922 1. 1.0000 1.0000 1.0000 1.0000 0.9922 0.9922 1. 0.9961 0.9961 1.0000 0.9922 1.0000 0.9258 0. 0.9922 0.9922 0.9727 0.9258 1.0000 0.9883 0. 0.9453 0.9453 0.9453 39 FOVER PRMs assign wrong step-level scores, although the baseline PRMs assign reasonable step-level scores. The following example shows response generated by Llama3.18B on GSM8K. In this case, the FOVER PRM assigns relatively low score to Step 3 of the first solution, even though the step is correct. It also fails to assign low score to Step 0 of the second solution, which contains an error. This represents relatively rare instance in which the FOVER PRM genuinely degrades step-level verification performance. Problem: Each person in certain household consumes 0.2 kg of rice every meal. Supposing 5 members of the household eat rice every lunch and dinner, how many weeks will 42 kg bag of rice last? Ground truth: 3 Step Explanation Solution selected by the baseline PRM (correct final answer) Step 0 Step Step 2 Step 3 Step 4 Each person consumes 0.2 kg of rice per meal. There are 2 meals per day (lunch and dinner), so each person consumes 0.2 * 2 = 0.4 kg of rice per day. There are 5 members in the household, so the total rice consumption per day is 0.4 * 5 = 2 kg. There are 7 days in week, so the total rice consumption per week is 2 * 7 = 14 kg. 42 kg bag of rice will last for 42 / 14 = 3 weeks. Therefore, the answer (arabic numerals) is 3. Solution-level Score Solution selected by FOVER PRMs (wrong final answer) Step 0 Step 1 Step 2 Step 3 Step 4 Step Solution-level Score Each person consumes 0.2 kg of rice per meal, and there are 3 meals day (breakfast, lunch, and dinner). So, each person consumes 0.2 * 3 = 0.6 kg of rice per day. Since there are 5 members, the total rice consumption per day is 0.6 * 5 = 3 kg. 42 kg bag of rice will last for 42 / 3 = 14 days. Since there are 7 days in week, the rice will last 14 / 7 = 2 weeks. Therefore, the answer (arabic numerals) is 2. Baseline PRM FOVER PRM 0.9688 0. 1.0000 0.9961 0.9922 1.0000 0.9688 0.7656 1.0000 1.0000 0.9609 0.9961 1.0000 0. 0.9961 1.0000 0.8516 0.9961 0.8516 0.8945 1.0000 0.9961 0.9531 0.9805 0.9883 0."
        },
        {
            "title": "K License",
            "content": "We release our dataset under Creative Commons Attribution 4.0 International and our code under Apache License 2.0. Our dataset and code are based on the following resources. We consider our license to be (one-way) compatible with all licenses listed below. Datasets. Our training data is based on the following datasets. FLDx2 [27]: Creative Commons Attribution 4.0 International 8 GSM8K [6]: MIT9 MetaMathQA [62]: MIT10 Big-Math [1]: Apache License 2.011 Code and packages. Our code is partially based on the following resources. FLD [27]: Apache License 2.012 Isabelle: BSD-style regulations13 Neural theorem proving tutorial [54]: MIT14 DTV [68]: MIT"
        },
        {
            "title": "L Computational Resources and Execution Time",
            "content": "LLM training and inference. We use four NVIDIA A100 SXM4 80GB GPUs for training and inference. Training each 8B-class model on our dataset takes approximately 30 minutes using our training data, which consists of 10,000 instances. Evaluation requires considerably more time; reproducing all the training and evaluation in this paper will take approximately three days, excluding the dataset creation. Dataset Creation. Dataset creation in the formal logic tasks is efficient, and the verification for our dataset takes few hours. However, verification of formal proofs using Isabelle is CPU-intensive and takes long time. We run 40 parallel Isabelle processes across four servers, verifying approximately 200 proofs per hour. The servers have different CPUs, with the most powerful one equipped with two AMD EPYC 7763 64-Core Processors. The verification of our formal proof dataset takes about two weeks in total. We can accelerate the verification by running more Isabelle processes in parallel. 8https://github.com/hitachi-nlp/FLD-corpus/blob/neurips_2025/LICENSE 9https://github.com/openai/grade-school-math/blob/master/LICENSE 10https://huggingface.co/datasets/meta-math/MetaMathQA/blob/main/README.md 11https://huggingface.co/datasets/SynthLabsAI/Big-Math-RL-Verified/blob/main/ README.md 12https://github.com/hitachi-nlp/FLD/blob/neurips_2025/LICENSE 13https://isabelle.in.tum.de/ 14https://github.com/wellecks/ntptutorial/blob/main/LICENSE 15https://github.com/jinpz/dtv/blob/main/LICENSE"
        }
    ],
    "affiliations": [
        "Penn State University"
    ]
}