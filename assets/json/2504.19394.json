{
    "paper_title": "LLMs for Engineering: Teaching Models to Design High Powered Rockets",
    "authors": [
        "Toby Simonds"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have transformed software engineering, but their application to physical engineering domains remains underexplored. This paper evaluates LLMs' capabilities in high-powered rocketry design through RocketBench, a benchmark connecting LLMs to high-fidelity rocket simulations. We test models on two increasingly complex design tasks: target altitude optimization and precision landing challenges. Our findings reveal that while state-of-the-art LLMs demonstrate strong baseline engineering knowledge, they struggle to iterate on their designs when given simulation results and ultimately plateau below human performance levels. However, when enhanced with reinforcement learning (RL), we show that a 7B parameter model outperforms both SoTA foundation models and human experts. This research demonstrates that RL-trained LLMs can serve as effective tools for complex engineering optimization, potentially transforming engineering domains beyond software development."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 2 4 9 3 9 1 . 4 0 5 2 : r LLMS FOR ENGINEERING: TEACHING MODELS TO DESIGN HIGH POWERED ROCKETS PREPRINT Toby Simonds Tufa Labs toby@tufalabs.ai May 1,"
        },
        {
            "title": "ABSTRACT",
            "content": "Large Language Models (LLMs) have transformed software engineering, but their application to physical engineering domains remains underexplored. This paper evaluates LLMs capabilities in high-powered rocketry design through RocketBench, benchmark connecting LLMs to highfidelity rocket simulations. We test models on two increasingly complex design tasks: target altitude optimization and precision landing challenges. Our findings reveal that while state-of-the-art LLMs demonstrate strong baseline engineering knowledge, they struggle to iterate on their designs when given simulation results and ultimately plateau below human performance levels. However, when enhanced with reinforcement learning (RL), we show that 7B parameter model outperforms both SoTA foundation models and human experts. This research demonstrates that RL-trained LLMs can serve as effective tools for complex engineering optimization, potentially transforming engineering domains beyond software development."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have significantly transformed software engineering practices, yielding quantifiable improvements in code generation, debugging processes, and documentation development. Studies demonstrate productivity enhancements of 55% on various software development task completion Git [1]. Despite these documented efficiency gains in computational domains, LLMs applications to mechanical, aerospace, civil, and other physical engineering disciplines remain underdeveloped. This disparity raises fundamental research question: Can LLMs function as effective tools for engineering tasks beyond software development? To explore this hypothesis, we selected high-powered rocketry as an ideal test domain for several compelling reasons. First, rocket design represents well-bounded yet complex engineering challenge with clear performance metrics. Second, the physics of rocketry incorporates multiple engineering disciplines (aerodynamics, structural mechanics, propulsion systems) while remaining computationally tractable. Third, the domain offers straightforward way to quantify success through metrics such as altitude accuracy and landing precision. Finally, model rocketry provides practical analogue to more complex aerospace engineering challenges while remaining accessible for research purposes. This paper presents an interface enabling LLMs to design model rockets using RocketPy[2], high-fidelity trajectory simulation tool. We evaluate these models on rocket design tasks inspired by the Spaceport America Cup competition, requiring optimization for target altitude, structural integrity, landing precision, and cost efficiencychallenges that mirror real-world engineering constraints. Our findings reveal that while state-of-the-art LLMs demonstrate impressive baseline knowledge of engineering principles, they struggle to iteratively improve designs through simulation feedback critical capability for realworld engineering tasks. Even after numerous iterations, these models plateau below human performance, indicating limitations in their ability to fully optimize complex multi-parameter designs. By implementing reinforcement learning (RL) approaches, we demonstrate that these limitations can be overcome, enabling LLMs to achieve performance that surpasses human experts. Our RL-trained models achieved precision arXiv Template PREPRINT landings within 12 meters of targets and consistently outperformed human designs across multiple metrics, despite using relatively modest 7B parameter model architecture. The results point to future where LLMs could transform engineering practice across numerous disciplines once effective interfaces with domain-specific tools are established. This work offers glimpse of engineerings future, where LLMs serve not merely as information retrieval systems but as creative problem-solvers capable of generating and optimizing novel designs beyond human capabilities. Key Contributions RocketBench: benchmark for evaluating LLMs rocket design capabilities Analysis of current LLMs strengths in engineering domains Demonstration that RL-trained LLMs significantly outperform both larger foundation models and human experts."
        },
        {
            "title": "2 Related Work",
            "content": "RL for Engineering Design Reinforcement learning has demonstrated significant efficacy in advancing engineering and scientific capabilities. AlphaFold transformed protein structure prediction[10], while AlphaTensor [8] discovered novel matrix multiplication algorithms with superior computational efficiency (Fawzi et al., 2022). Similarly, AlphaDev[7] optimized low-level sorting algorithms beyond human-designed solutions, and Alpha Chip[9] enhanced semiconductor design through systematic exploration of parameter spaces. These systems primarily leverage traditional RL methodologies with domain-specific reward structures. Our approach diverges by employing language models as the base policy, leveraging their baseline engineering knowledge and physical reasoning capabilities. This integration of structured domain priors with reinforcement learning enables more sample-efficient optimization while maintaining domain flexibility, addressing fundamental limitation of previous approaches that require extensive task-specific engineering."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Simulation Env Our simulations were built on RocketPy, high-fidelity trajectory simulation library for high-power rocketry. RocketPy provides complete 6 degrees of freedom (6-DOF) simulation framework that accounts for variable mass effects, aerodynamic forces, and parachute descent phases with exceptional accuracy. The library has been validated against real-world flight data, demonstrating relative errors of less than 2% for apogee predictions across multiple documented test flights[2]. For our research, we enhanced RocketPy with custom design rule checks (DRCs) and timeout mechanisms to address common failure modes observed during initial testing. DRCs included basic constraints including verification that body diameter exceeded motor diameter and that body length was greater than motor length to ensure proper component integration. These additions prevent simulation failures caused by physically impossible configurations and terminate excessively long computations that often result from unrealistic rocket parameters. To enable LLMs to interface with the simulation environment, we developed JSON interface that abstracts the complexity of RocketPy into structured configuration format. Through this interface, models can specify all critical rocket parameters including motor selection, dimensions, materials, and launch conditions. See Table 1 for full list. Both motors and materials were limited to pre-curated lists of commercially available options to ensure physical realism and manufacturability. For motors, we selected range of commercial solid rocket motors commonly used in high-power rocketry, varying in total impulse, burn duration, and thrust profiles. LLMs were passed detailed specifications for physical dimensions, propellant mass, and thrust information derived from manufacturer data. We also implemented material stress simulation module that evaluates structural integrity throughout the flight profile. This module identifies the point of maximum stress during the mission and determines whether the rocket maintains structural integrity, adding critical dimension of realism to the performance evaluation. The simulation incorporated an economic model to introduce realistic cost constraints into the design process. Total costs were computed as function of motor selection and material volume, with standardized per-unit prices assigned to each material type based on market averages. Motor costs were assigned fixed values reflecting their relative price 2 arXiv Template PREPRINT differences while maintaining consistent cost pressure across designs. This economic dimension compelled LLMs to navigate performance-cost tradeoffs, simulating the budget optimization challenges inherent in real-world engineering projects. By incorporating this cost function into the reward signal, we ensured that design solutions reflected not only technical performance but also resource efficiencya critical constraint in practical rocket engineering. Table 1: Configurable Rocket Parameters Component Parameter Description Motor Motor Choice Selection from available commercial motors Rocket Body Nose Cone Fins Tail Parachutes Launch Payload Radius Length Material Thickness Shape Length Material Number Root Chord Tip Chord Span Cant Angle Material Thickness Body radius in meters (must exceed motor radius) Body length in meters Selection from available materials Wall thickness in meters Conical, ogive, elliptical, tangent, von karman, etc. Nose cone length in meters Selection from available materials Number of fins around the rocket body Length of fin base in meters Length of fin tip in meters Fin height in meters Fin angle in degrees Selection from available materials Fin thickness in meters Length Top Radius Bottom Radius Material Tail section length in meters Upper radius of tail section in meters Lower radius of tail section in meters Selection from available materials Main CDS Main Trigger Drogue CDS Drogue Trigger Deployment trigger condition Main parachute drag area Deployment trigger condition Drogue parachute drag area Rail Length Inclination Heading Mass Position Launch rail length in meters Launch angle from horizontal in degrees Compass heading in degrees Payload mass in kilograms Relative position from rocket center in meters 3.2 Competition Tasks We developed two increasingly complex tasks to evaluate model performance across spectrum of engineering challenges. While the weighting and selection criteria for each scoring component were chosen heuristically, they were designed to approximate the evaluation frameworks used in actual rocket competitions. 3.2.1 Target Altitude Challenge The first task was inspired by the 10,000-foot category of the Spaceport America Cup, an international collegiate rocket engineering competition. In this event, teams must design rockets that reach as close as possible to the target altitude while ensuring safe recovery and operational integrity. Our reward function was designed to capture the essential performance metrics of this competition, focusing on flight performance aspects that could be simulated. The reward balanced multiple objectives with the following components: Altitude Accuracy (50%): linear reward based on the percentage difference between the achieved apogee and the target altitude: percent_difference = apogee target_apogee target_apogee (1) 3 arXiv Template PREPRINT distance_reward = max(0, 1.0 percent_difference) (2) Structural Integrity (10%): binary component that awarded points only if the rocket maintained structural integrity throughout the flight: structural_failure_reward = (cid:26)0 1 if structural failure occurred otherwise (3) Horizontal Drift Control (10%): Models received higher scores for minimizing the horizontal displacement of landing position: max_horz_distance = target_apogee 0.3 horz_distance_reward = max(0, 1 horizontal_distance max_horz_distance ) Cost Efficiency (15): linear reward calculated based on the total cost of materials and components: cost_reward = max(0, 1 total_cost 1000 ) (4) (5) (6) Landing Safety (15%): Models were rewarded for designs with safe descent velocities. Impact velocity in m/s: impact_reward = max(0, 1 impact_velocity 25 ) The total reward was calculated as weighted sum of these components: Rtotal = 0.5 Raltitude + 0.1 Rstructural + 0.1 Rdrift + 0.15 Rcost + 0.15 Rlanding (7) (8) 3.2.2 Precision Landing Challenge The Second and most complex task shifted focus from altitude targeting to precision landing. Models were tasked with designing rockets capable of landing as close as possible to target location 5.65 kilometers from the launch site (4000m horizontally and 4000m vertically) while maintaining cost efficiency and safety. The reward function for this challenge was structured as follows: Landing Accuracy (75%): The primary component based on the distance between the actual landing location and the target coordinates: landing_error = (cid:112)(landing_x target_x)2 + (landing_y target_y)2 (cid:112) target_distance = target_x2 + target_y2 landing_error_percent = landing_error target_distance (9) (10) (11) landing_reward = max(0, 1.0 landing_error_percent) (12) Structural Integrity (15%): binary component that awarded points only if the rocket maintained structural integrity throughout the flight: structural_failure_reward = (cid:26)0 if structural failure occurred otherwise Cost Efficiency (5%): linear reward calculated based on the total cost of materials and components: cost_reward = max(0, 1 total_cost 1000 ) (13) (14) Landing Safety (5%): Models were rewarded for designs with safe descent velocities. Impact velocity in m/s: impact_reward = max(0, 1 impact_velocity 25 ) The total reward was calculated as weighted sum of these components: Rtotal = 0.75 Rlanding + 0.05 Rstructural + 0.05 Rcost + 0.05 Rsafety (15) (16) This challenge represented significant increase in complexity, as it required models to reason about the entire flight trajectory, including both the powered ascent phase and the descent under parachutes. Success demanded sophisticated understanding of parachute deployment timing, wind drift compensation, and optimal flight path planning. 4 arXiv Template PREPRINT 3.3 RL Dynamics To explore the potential of reinforcement learning on LLMs for engineering tasks, we implemented Group Relative Policy Optimization (GRPO) using Qwen 2.5 7B model. Training was conducted with batch size of 64 across all experiments. For each training instance, the model received standardized prompt containing the complete environmental specifications(wind conditions, target altitude or landing coordinates), available materials and engines, and the exact code implementation of the reward function. To establish baseline understanding, we provided single example design in the expected output format. This ensured the model understood the task structure while minimizing prior solution bias. See appendix for full prompt 3.4 Iterative Prompting Protocol For our comparative analysis of foundation models iterative capabilities, we implemented structured prompting protocol. Initial prompts contained identical information to the RL setup: environmental conditions, available components, and reward calculation methods. In subsequent iterations, we augmented the prompt with the models full output including reasoning for the previous solution and the comprehensive performance metrics. We include all previous solutions not just previous attempt. For the Target Altitude Challenge, feedback included maximum apogee achieved, structural integrity status, and final cost. For the Precision Landing Challenge, we substituted landing position for apogee information. This approach enabled models to learn from previous design attempts while maintaining consistent evaluation conditions. 3.5 Human Baseline To establish comparative baseline, we recruited an individual with multiple years of experience in university-level rocketry competitions similar to those our benchmarks are based off. This participant completed identical design tasks with the same interface, components, and feedback mechanisms used by the models. While this approach provides meaningful comparison point, we note that this represents single expert rather than peak human performance or professional expertise in the aerospace industry. The baseline serves as practical reference point for assessing model performance against experienced human engineering capabilities under identical constraints."
        },
        {
            "title": "4 Results",
            "content": "4.1 Rocket Bench 4.1.1 Target Altitude Challenge The Target Altitude Challenge results revealed interesting performance patterns across tested models. Claude 3.7[5] achieved the highest average performance (62.14), with Deepseek v3[3] (57.36) and o1[6] (60.57) following closely behind. The relative similarity between these top three models scores suggests comparable engineering reasoning capabilities for this task. All models demonstrated strong initial designs, generally outperforming early human attempts. This finding is particularly noteworthy as it indicates that LLMs possess effective baseline engineering intuition even without specialized training. The models ability to immediately generate viable rocket designs suggests they have internalized fundamental physics and engineering principles during their general training. We next explored how models performed at iterating and improving on their designs. Figure 1 (right panel) reveals distinct patterns in how models improved their designs through iteration. All models demonstrated the ability to learn from feedback and improve their designs, but at significantly different rates. Claude 3.7 showed the strongest overall progression, starting at 63.04 and reaching 74.79 by iteration 9. This consistent upward trajectory suggests effective learning from simulation feedback. o1 began with similarly strong baseline score of 64.37 and stabilized around 68-69 after several iterations, showing good but less dramatic improvement over time. Deepseek v3 demonstrated steady improvement from moderate starting score of 52.14 to approximately 66.71 by iteration 10. GPT-4o [4] showed inconsistent performance between 43-57 without clear convergence, indicating difficulty in systematically incorporating feedback into design improvements. Human experts began at comparable performance level (44.27) but improved more rapidly and effectively, reaching maximum score of 76.57 by iteration 5 - the highest score achieved in our study. This demonstrates humans 5 arXiv Template PREPRINT Average Best-of-10 c n o P 70 60 50 40 Model Performance Across Iterations 2 3 4 5 6 8 9 10 GPT-4o Claude 3.7 Deepseek v3 GPT-4o Claude 3.7 Iteration o1 Deepseek v3 Humans c n o P 100 80 60 40 20 Figure 1: Performance across Target Altitude Challenge. Left shows average performance across 10 runs right shows sample progress over time (Best over 3 runs for LLMs) Model Performance Over 30 Iterations c n o P 80 70 60 40 30 20 1 5 10 15 20 25 30 Claude Iteration Best Human Performance Figure 2: Performance comparison of Claude vs. o1 over 30 iterations with best human performance indicated. superior ability to iterate on designs based on simulation feedback. Despite strong baseline capabilities and consistent improvement trends, all LLMs ultimately fell short of human performance in iterative design optimization. We next investigated whether additional iterations would enable models to surpass human performance. Our 30-iteration experiment with Claude 3.7 and o1 (Figure 2 yielded particularly insightful results about the models improvement limitations. Both models plateaued below the human maximum despite extensive iteration opportunities. Claude 3.7 stabilized around 75.73, coming close to but never surpassing the human record of 76.57, while o1 plateaued at lower 67.7. This persistent gap indicates that neither model could match the human level performance despite having triple the iteration cycles compared to the baseline experiment. We also evaluated \"best-of-10\" sampling alongside our primary methods, observing significant performance gains across models. The improvements varied by model: GPT-4o showed the largest relative improvement, gaining 12.89 points to reach 56.51; o1 increased from 60.57 to 68.3; Claude 3.7 rose from 62.14 to 69.69; and Deepseek v3 improved from 57.3 to 63.28. Despite these gains, all models achieved even higher peak scores through iterative prompting compared to best-of-10 sampling. 4.1.2 Precision Landing Challenge The Precision Landing Challenge results seeked to explore how LLMs handled more complex environments. Figure 3 (left panel) showed O1 achieved the highest average performance (31.21), with Deepseek-v3 (29.29) and Claude 3.7 6 arXiv Template PREPRINT Average Best-of-10 c n o P 100 80 60 40 20 Model Performance Across Iterations 2 4 6 8 GPT-4o Claude 3.7 Deepseek v3 o1 GPT-4o Claude Iteration Deepseek-v3 o1 Humans c n o P 80 40 20 0 Figure 3: Performance across Precision Landing Challenge. Left shows average and best performance across 10 runs right shows sample progress over time (Best over 3 runs for LLMs) (28.54) following closely. GPT-4o scored substantially lower (15.29), indicating particular difficulty with this more complex task. Unlike the Altitude Challenge, all models demonstrated lower initial performance relative to human experts in this task, suggesting that the precision landing requirements introduce complexity that current LLMs struggle to address effectively. Figure 3 (right panel) reveals more pronounced differences in iterative capabilities. Human experts demonstrated rapid improvement, beginning at 46 and rapidly improving to scores above 90 by iteration 5. O1 showed the strongest model performance, reaching scores around 70 in iterations 4-9, but with inconsistent patterns. Claude 3.7 demonstrated highly variable performance, with peaks near 69 but substantial fluctuations. Deepseek-v3 showed moderate improvement but plateaued around 40-45. GPT-4o struggled significantly, never exceeding 23 points. We note high variance in iteration scores between runs but general performance trends remained consistent throughout all trials. The performance gap between humans and even the best-performing models is substantially wider than in the Altitude Challenge. While the best human score reached 91.60, the top model score (o1) peaked at 71.78, showing 19.82-point differential compared to the 2-point gap in the previous task. This pronounced performance difference highlights how increasing task complexity disproportionately affects LLM capabilities. Unlike the Target Altitude Challenge where models approached human performance, the Precision Landing Challenge reveals more substantial limitations in models ability to reason through complex engineering environments We also tested \"best-of-10\" sampling alongside iterative prompting, finding substantial performance improvements: o1 increased from 31.21 to 60.11, Claude 3.7 from 28.54 to 56.59, and Deepseek v3 gained 21.39 points to reach 50.68. GPT 4o struggled to make any substantial improvements. These improvements again fell short of those achieved through iterative prompting, where models reached higher peak scores, demonstrating that LLMs genuinely learn from feedback rather than merely benefiting from sampling variation. Our results across both challenges reveal consistent pattern: LLMs demonstrate impressive baseline engineering knowledge but significant limitations in iterative design capabilities. In the Target Altitude Challenge, models produced initial designs comparable to human experts, with some models approaching (but never surpassing) human performance after extensive iteration. However, the more complex Precision Landing Challenge exposed much wider performance gap, with even the best models scoring 19.82 points below human experts. This pattern suggests that while current LLMs have internalized substantial engineering principles from their training, they lack the strategic iteration abilities that human experts employ when refining designs. These findings indicate that while LLMs show promise as engineering tools for generating initial designs and baseline solutions, they currently cannot match human experts ability to iteratively refine complex engineering systems through feedback-driven optimization. 7 arXiv Template PREPRINT Target Altitude Challenge Precision Landing Challenge 0.8 0.7 0.6 0.5 0.4 0.3 0. a 0 10 20 30 50 Training Step w 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0 20 50 40 30 Training Step 60 80 Figure 4: Comparison of RL performance. Reward represents mean batch reward 4.2 Reinforcement Learning 4.2.1 Target Apogee Challenge We next explored the effectiveness of applying RL on LLMs to engineering tasks. Figure 4 (left) shows the training progression for our RL with Qwen-2.5 7B model. Over 50 training steps, the model demonstrated consistent improvement in mean batch reward from 0.19 to 0.76, indicating systematic optimization beyond what iterative prompting achieved. The RL-trained model achieved peak score of 79.98 on the Target Altitude Challenge, surpassing both the best human expert score (76.57) and all tested state-of-the-art LLMs. This performance differential is particularly notable given the models significantly worse initial performance demonstrating that we do not need relatively high initial performance to make progress with RL. 4.2.2 Precision Landing Challenge The Precision Landing Challenge results showed even more dramatic improvements when applying reinforcement learning. As illustrated in Figure 4 (right), the RL-trained 7B Qwen-2.5 model demonstrated exceptional progress over 84 training steps, with mean batch scores increasing from near-zero (0.02) to approximately 0.94. Most significantly, the RL approach achieved peak score of 95.6 on this challenge, substantially exceeding both human expert performance (91.6) and the best-performing foundation models (71.78 for o1). The trained model achieved remarkable precision, landing within just 12 meters of the target locationa feat that neither humans nor standard LLMs could match. The dramatic performance improvements achieved through reinforcement learning across both challenges demonstrate the exceptional potential of RL-optimized LLMs for engineering applications. While iterative prompting of SoTA foundation models showed clear performance plateaus below human expert levels, RL training enabled consistent improvement beyond human performance thresholds. The ability of relatively compact 7B parameter model to systematically outperform both larger foundation models and skilled human engineers indicates that RL optimization effectively addresses the core limitation we observed in standard LLMs: their difficulty in effectively altering designs given feedback. As engineering complexity increased from altitude targeting to precision landing, the RL advantage became more pronounced, suggesting this approach may be particularly valuable for more complex, multi-objective engineering optimization problems."
        },
        {
            "title": "5 Limitations",
            "content": "While our simulation environment provides reasonable proxy for evaluating LLMs engineering capabilities, several limitations should be acknowledged. First, RocketPy, though high-fidelity, cannot account for all manufacturing complexities and environmental factors that physical rocket construction would entail. Second, we observed high variance in model performance, where small parameter adjustments sometimes led to disproportionately large differences in flight outcomes. This sensitivity reflects real engineering challenges but complicates consistent performance assessment. Third, our human baseline was limited to single expert with ten attempts, matching the constraints imposed 8 arXiv Template PREPRINT Figure 5: Best Rocket Design from RL on Precision Landing Challenge on the models. While this provides fair comparison, we acknowledge that human experts with additional iterations or collaborative environments might develop superior solutions. Despite these limitations, our results demonstrate that RL-enhanced LLMs can outperform individual human experts under controlled conditions, establishing meaningful benchmark for future research."
        },
        {
            "title": "6 Discussion",
            "content": "6.1 RL vs Inference Time Compute Our results indicate that inference time compute scaling works across both simple and complex engineering tasks but current methods still plateau below human level performance. We observe that reasoning-focused models (o1) outperform foundation models (GPT-4o), suggesting that further scaling of the reasoning model paradigm could yield additional performance improvements. Test-time reinforcement learning (TTRL) represents another promising axis for scaling compute to enhance performance. Beyond providing an incremental boost in specific domains, TTRL offers methodology for extending performance boundaries in individual domains, potentially previewing future capabilities of large language models. 6.2 Why use LLMs for RL? Despite impressive successes like AlphaGo and AlphaZero in game environments, reinforcement learning has seen limited application in engineering domains. One of the reasons for this is the cold-start problem in complex domainstraditional RL approaches require extensive random exploration before achieving meaningful performance, making them impractical for computationally expensive engineering simulations. Additionally, these systems typically need meticulously crafted environments with specialized reward functions, limiting their broad applicability. LLMs offer compelling solution to these challenges by leveraging their pre-trained knowledge of physics, engineering principles, and domain-specific concepts. This built-in prior knowledge enables them to begin optimization from reasonable starting points rather than random exploration. Our experiments demonstrate remarkable sample efficiency, with the RL-trained model requiring only approximately 3,000 simulation samples to exceed human-level performance on both tasksorders of magnitude fewer than traditional RL approaches would require for problems of similar complexity. This efficiency is particularly important given the long simulation times for complex engineering tasks, where each evaluation may require significant computational resources. The efficiency stems from the LLMs ability to make informed parameter adjustments based on underlying principles rather than blind trial-and-error. The combination of broad knowledge representation and targeted optimization through RL creates powerful paradigm for accelerating engineering design processes that would be prohibitively expensive using either approach independently. 6.3 Engineering Implication Our research demonstrates that given appropriate simulation environments, RL-trained LLMs can surpass human-level performance in complex engineering domains. Currently, two primary bottlenecks limit broader application: creating environments that interface effectively with LLMs and developing appropriate reward models. As models agentic capabilities increase, we anticipate access to wider range of possible RL environments. Similarly, improvements 9 arXiv Template PREPRINT in models self-verification abilities will enable setting more abstract goals, potentially significantly openning up the scope of possible RL tasks. If these challenges are addressed, we foresee rapid progress in engineering domains with strong simulation tools. The pattern mirrors what weve observed with diffusion models in image generationwhere effective training methodologies led to explosive capability improvements that transformed creative industries within remarkably short timeframe. The economic and scientific implications of this acceleration could be substantial across multiple engineering disciplines. In the short term, we anticipate these systems will function as next-generation CAD tools for engineers, automatically exploring design spaces and suggesting optimized solutions. Just as traditional CAD software transformed engineering by automating drafting and visualization tasks, LLMs could automate aspects of design optimization while enabling human engineers to focus on higher-level innovation. This human-AI collaboration could significantly accelerate engineering progress across fields from aerospace to renewable energy and biomedical devices. 6.4 Safety This research highlights broader security concerns as RL-enhanced LLMs could significantly lower barriers to developing dangerous technologies. The ability to rapidly optimize designs through simulation feedback potentially democratizes knowledge that was previously restricted through classification and specialized expertise requirements. These developments raise critical challenges for open source communities and AI regulation. With even current models demonstrating concerning capabilities through new inference scaling techniques, traditional regulatory approaches focused primarily on limiting training compute appear increasingly ineffective. Novel governance frameworks must address the emerging reality that test-time optimization and inference-focused methods can bypass conventional safeguards 6.5 Conclusion Our research demonstrates that reinforcement learning applied to LLMs creates powerful paradigm for engineering optimization that surpasses both foundation models and human expert performance. While state-of-the-art LLMs show strong baseline engineering knowledge, they consistently plateau below human capabilities when iteratively refining designs. In contrast, RL-trained models achieved performance breakthroughs in both target altitude and precision landing challenges using only modest parameter architecture. This approach addresses core limitations of both traditional RL (sample inefficiency) and foundation models (limited optimization capabilities), combining LLMs domain knowledge with structured exploration. The results suggest future where RL-optimized LLMs serve as force-multipliers across engineering disciplines, dramatically accelerating design cycles and enabling performance improvements previously unattainable. As interfaces between LLMs and simulation environments improve, we anticipate widespread adoption across aerospace, civil engineering, energy systems, and beyondfundamentally transforming how complex engineering challenges are solved."
        },
        {
            "title": "References",
            "content": "[1] Measuring Impact of GitHub Copilot. URL https://resources.github.com/learn/pathways/copilot/ essentials/measuring-the-impact-of-github-copilot/. [2] RocketPy Documentation RocketPy 1.9.0 documentation. URL https://docs.rocketpy.org/en/ latest/. [3] Introducing DeepSeek-V3 DeepSeek API Docs. URL https://api-docs.deepseek.com/news/news1226. [4] Hello GPT-4o OpenAI. URL https://openai.com/index/hello-gpt-4o/. [5] Claude 3.7 Sonnet and Claude Code. URL https://www.anthropic.com/news/claude-3-7-sonnet. [6] Introducing OpenAI o1, September 2024. URL https://openai.com/o1/. [7] AlphaDev discovers faster sorting algorithms, April 2025. URL https://deepmind.google/discover/blog/ alphadev-discovers-faster-sorting-algorithms/. [8] Discovering novel algorithms with AlphaTensor, April 2025. URL https://deepmind.google/discover/ blog/discovering-novel-algorithms-with-alphatensor/. [9] How AlphaChip transformed computer chip design, April 2025. URL https://deepmind.google/discover/ blog/how-alphachip-transformed-computer-chip-design/. 10 arXiv Template PREPRINT [10] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, Alex Bridgland, Clemens Meyer, Simon A. A. Kohl, Andrew J. Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Ellen Clancy, Michal Zielinski, Martin Steinegger, Michalina Pacholska, Tamas Berghammer, Sebastian Bodenstein, David Silver, Oriol Vinyals, Andrew W. Senior, Koray Kavukcuoglu, Pushmeet Kohli, and Demis Hassabis. Highly accurate protein structure prediction with AlphaFold. Nature, 596(7873):583589, August 2021. ISSN 1476-4687. doi:10.1038/s41586-021-03819-2. URL https://www.nature.com/articles/s41586-021-03819-2. Publisher: Nature Publishing Group."
        },
        {
            "title": "A Sample Prompt",
            "content": "# Rocket Design Task ## Design Requirements - ** Target Apogee **: 3048.0 meters - ** Wind Conditions **: 5 / from direction ## You are scored off the following Distance to max apogee Cost : Cheaper the rocket the better the score Does it land safely ( less than 5 / ) Does it not break Horz distance : How far is it from the intial launch site # Score func code shown below : structural_failure = None distance_reward = 1.0 - percent_di ff ere nc distance_reward = max (0 , distance_reward ) # Whether the rocket structure failed during flight # Structural failure reward u tu _ l e_ a = 0 if structural_failure else 1 # Horizontal distance reward ( linear version ) max_horz_distance = target_apogee * 0.3 horz_distan ce _r ew ar = max (0 , 1 - horizontal_distance / max_horz_distance ) # Scale factor # Cost reward ( linear version ) max_cost = 1000.0 cost_factor = total_cost / max_cost cost_reward = 1.0 - cost_factor cost_reward = max (0 , cost_reward ) # Base cost scale # Clamp to minimum of # Impact velocity reward ( linear version ) max_impact _vel ocit = 25 # / impact_factor = abs ( impact_velocity ) / ax _im pact_velocity impact_reward = 1.0 - impact_factor impact_reward = max (0 , impact_reward ) # Clamp to minimum of 0 # Add additional rewards with weights reward = ( distance_reward *0.5 + orz _d ist an ce _r ew ar * 0.1 + cost_reward * 0.15 + impact_reward * 0.15 + u tu _ l _ a * 0.1) ## Available Materials arXiv Template PREPRINT The following materials are available for the rocket components : aluminum , composite , fiberglass , carbon_fiber , balsa_wood , plywood , ABS_plastic ## Available Motors Name , Manufacturer , Radius ( mm ) , Length ( mm ) , Dry Mass ( kg ) , Max Thrust ( ) , Avg Thrust (cid:44) ( ) , Burn Time ( ) , Total Impulse ( Ns ) , Isp ( ) , Cost ( $ ) Pro75M1670 , CTI ,75 ,757 ,1.815 ,2200 ,1533.9 ,3.9 ,6023.6 ,198 , 520 AeroTechK700W , AT ,54 ,568 ,0.732 ,1029.3 ,658.7 ,3.5 ,2249 ,177.5 , 180 CesaroniM1670 , CTI ,75 ,757 ,3.101 ,2200 ,1533.9 ,3.6 ,6023.6 ,198 , 550 AeroTechH128W , AT ,29 ,194 ,0.108 ,190.5 ,141.2 ,1.29 ,176.5 ,191.3 , 65 CesaroniO3700 , CTI ,161 ,957 ,14.194 ,4030.3 ,2836.9 ,8.2 ,29930.2 ,177.8 , 1250 CesaroniO5800 , CTI ,150 ,754 ,12.418 ,6395.5 ,5040.2 ,5.2 ,30382.7 ,222 , 1100 CesaroniK160 , CTI ,54 ,404 ,0.7 ,272.2 ,190.2 ,9.7 ,1521.7 ,182.9 , ## Design Task Based on the requirements and available components , design rocket that will (cid:44) reach the target apogee . Your design should include : 1. Motor selection ( choose from the available motors list ) 2. Body dimensions and material 3. Nose cone dimensions and material 4. Fin design and material 5. Parachute specifications 6. Launch rail configuration ### Notes DRC are run on the design so you need to make sure the design is feasible . Here are some of the checks : Notes for the tail the top and bottom radius cannot be the same ( causes error ) The material must be specified exactly as listed above The body radius must be greater than the motor radius Nose cone but by exactly one of the listed Don include any additional python code ( other than config . Putting calculation (cid:44) in there is ok like 32/4 but not whole functions ## Response Format Please provide your design as Python dictionary that can be directly used in our (cid:44) simulation software . Use the following format : python config = { \" motor_choice \": \" MOTOR_NAME \" , \" rocket_body \": { # Choose from available motors \" radius \": RADIUS_IN_METERS , # Body radius in meters ( must be greater than (cid:44) motor Radius ) \" length \": LENGTH_IN_METERS , \" material \": \" MATERIAL \" , \" thickness \": THICKNESS_IN_METERS , # Body length in meters # Choose from available materials # Wall thickness in meters } , \" aerodynamics \": { \" nose_cone \": { \" kind \": \" SHAPE \" , # \" conical \" , \" ogive \" , \" elliptical \" , \" tangent \" , \" von (cid:44) karman \" , \" parabolic \" , \" powerseries \" or \" lvhaack \". \" length \": LENGTH_IN_METERS , \" material \": \" MATERIAL \" , } , \" fins \": { \" number \": NUMBER_OF_FINS , \" root_chord \": LENGTH_IN_METERS , \" tip_chord \": LENGTH_IN_METERS , \" span \": LENGTH_IN_METERS , 12 arXiv Template PREPRINT \" cant_angle \": ANGLE_IN_DEGREES , \" material \": \" MATERIAL \" , \" thickness \": THICKNESS_IN_METERS , } , \" tail \": { \" length \": LENGTH_IN_METERS , \" top_radius \": RADIUS_IN_METERS , \" bottom_radius \": RADIUS_IN_METERS , \" material \": \" MATERIAL \" , } , } , \" parachutes \": { \" main \": { \" name \": \" Main \" , \" cd_s \": AREA , \" trigger \": \" apogee \" , \" sampling_rate \": 105 , \" lag \": 1.5 , \" noise \": (0 , 8.3 , 0.5) , } , \" drogue \": { \" name \": \" Drogue \" , \" cd_s \": AREA , \" trigger \": \" apogee \" , \" sampling_rate \": 105 , \" lag \": 1.5 , \" noise \": (0 , 8.3 , 0.5) , } , } , \" launch \": { \" rail_length \": LENGTH_IN_METERS , \" inclination \": ANGLE_IN_DEGREES , #90 is vertical launch \" heading \": ANGLE_IN_DEGREES , # Heading in degrees 0 is up } , \" payload \": { # point mass as position specified \" mass \": MASS_IN_KG , \" position \": POSITION_IN_METERS , # relative to rocket center } , } Here an example valid design . This is not at all indicative of what you should (cid:44) do just an example : python config = { \" motor_choice \": \" CesaroniO5800 \" , \" rocket_body \": { \" radius \": 0.1 , \" length \": 1.2 , \" material \": \" fiberglass \" , \" thickness \": 0.01 , # Body radius in meters # Body length in meters # Wall thickness in meters } , \" aerodynamics \": { \" nose_cone \": { \" kind \": \" ogive \" , \" length \": 0.3 , \" material \": \" composite \" , # Nose cone length in meters } , \" fins \": { \" number \": 4 , \" root_chord \": 0.15 , \" tip_chord \": 0.075 , \" span \": 0.3 , # Fin root chord in meters # Fin tip chord in meters # Fin span in meters 13 arXiv Template PREPRINT \" cant_angle \": 0.5 , \" material \": \" carbon_fiber \" , \" thickness \": 0. # Cant angle in degrees # Fin thickness in meters } , \" tail \": { \" length \": 1.2 , \" top_radius \": 0.04 , \" bottom_radius \": 0.05 , \" material \": \" carbon_fiber \" , # Tail length in meters # Top radius in meters # Bottom radius in meters } , } , \" parachutes \": { \" main \": { # Main parachute CD_s \" name \": \" Main \" , \" cd_s \": 0.25 , \" trigger \": \" apogee \" , \" sampling_rate \": 105 , \" lag \": 1.5 , \" noise \": (0 , 8.3 , 0.5) , } , \" drogue \": { # Drogue parachute CD_s \" name \": \" Drogue \" , \" cd_s \": 0.2 , \" trigger \": \" apogee \" , \" sampling_rate \": 105 , \" lag \": 1.5 , \" noise \": (0 , 8.3 , 0.5) , } , } , \" launch \": { \" rail_length \": 1.2 , \" inclination \": 90 , \" heading \": 0 , # Length of the launch rail in meters # Rail inclination in degrees ( vertical ) # Launch heading in degrees ( straight up ) } , \" payload \": { \" mass \": 0.5 , \" position \": 0.6 # Payload mass in kg # Payload position relative to rocket center in meters } } Before answering you should provide your full reasoning for the design choices you (cid:44) made thinking like rocket scientist . Run sample calulations , make (cid:44) approximations etc to find best design"
        }
    ],
    "affiliations": [
        "Tufa Labs"
    ]
}