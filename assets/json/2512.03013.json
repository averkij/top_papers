{
    "paper_title": "In-Context Sync-LoRA for Portrait Video Editing",
    "authors": [
        "Sagi Polaczek",
        "Or Patashnik",
        "Ali Mahdavi-Amiri",
        "Daniel Cohen-Or"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Editing portrait videos is a challenging task that requires flexible yet precise control over a wide range of modifications, such as appearance changes, expression edits, or the addition of objects. The key difficulty lies in preserving the subject's original temporal behavior, demanding that every edited frame remains precisely synchronized with the corresponding source frame. We present Sync-LoRA, a method for editing portrait videos that achieves high-quality visual modifications while maintaining frame-accurate synchronization and identity consistency. Our approach uses an image-to-video diffusion model, where the edit is defined by modifying the first frame and then propagated to the entire sequence. To enable accurate synchronization, we train an in-context LoRA using paired videos that depict identical motion trajectories but differ in appearance. These pairs are automatically generated and curated through a synchronization-based filtering process that selects only the most temporally aligned examples for training. This training setup teaches the model to combine motion cues from the source video with the visual changes introduced in the edited first frame. Trained on a compact, highly curated set of synchronized human portraits, Sync-LoRA generalizes to unseen identities and diverse edits (e.g., modifying appearance, adding objects, or changing backgrounds), robustly handling variations in pose and expression. Our results demonstrate high visual fidelity and strong temporal coherence, achieving a robust balance between edit fidelity and precise motion preservation."
        },
        {
            "title": "Start",
            "content": "In-Context Sync-LoRA for Portrait Video Editing Sagi Polaczek1 Or Patashnik1 1Tel Aviv University Ali Mahdavi-Amiri2 Daniel Cohen-Or1 2Simon Fraser University 5 2 0 2 2 ] . [ 1 3 1 0 3 0 . 2 1 5 2 : r Figure 1. Given source video and an edited first frame, our method propagates the visual edit. The resulting video precisely retains the subjects identity and frame-accurate source motion, while faithfully adopting the new appearance from the edited frame."
        },
        {
            "title": "Abstract",
            "content": "Editing portrait videos is challenging task that requires flexible yet precise control over wide range of modifications, such as appearance changes, expression edits, or the addition of objects. The key difficulty lies in preserving the subjects original temporal behavior, demanding that every edited frame remains precisely synchronized with the corresponding source frame. We present SyncLoRA, method for editing portrait videos that achieves high-quality visual modifications while maintaining frameaccurate synchronization and identity consistency. Our approach uses an image-to-video diffusion model, where the edit is defined by modifying the first frame and then propagated to the entire sequence. To enable accurate synchronization, we train an in-context LoRA using paired videos that depict identical motion trajectories but differ in appearance. These pairs are automatically generated and curated through synchronization-based filtering process that selects only the most temporally aligned examples for training. This training setup teaches the model to combine motion cues from the source video with the visual changes introduced in the edited first frame. Trained on compact, highly curated set of synchronized human portraits, Sync-LoRA generalizes to unseen identities and diverse edits (e.g., modifying appearance, adding objects, or changing backgrounds), robustly handling variations in pose and expression. Our results demonstrate high visual fidelity and strong temporal coherence, achieving robust balance between edit fidelity and precise motion preservation. Project page: https://sagipolaczek.github.io/Sync-LoRA/ . 1. Introduction With the rapid advancement of diffusion-based video editing techniques, textand image-driven methods have gained prominence for achieving high-quality, temporally consistent edits [19, 27, 57]. These techniques hold strong promise for applications in advertising, film production, game development, and interactive media, where finegrained control over visual content is essential. particular interest in video editing lies in the editing of portrait videos, as large portion of visual media features humans [17, 20, 35]. Editing portrait videos presents uniquely demanding challenge. On the one hand, the visual appearance of the subject must be modified according to user-defined instruction. On the other hand, the resulting video is expected to preserve the exact motion patterns of the source video. This requirement goes beyond visual consistency across frames. The edited video should follow the source on frame-by-frame basis so that every blink, gaze shift, or articulation occurs at the same moment as in the source. This level of temporal synchronization is critical in talking-head scenarios, where even small misalignment may render the output unfaithful to the source performance. At the same time, the desired edit is often localized and specific, such as adding mustache, changing an expression, or inserting visual accessory. The goal is to alter only what is strictly required by the instruction, leaving all other visual content and motion untouched. Achieving this combination of precise synchronization, minimal editing footprint, and temporal identity preservation remains an open challenge for most existing video editing techniques [17, 27, 29, 37]. Inspired by the In-Context LoRA (IC-LoRA) paradigm [1, 9, 11, 26, 46, 65], we introduce Sync-LoRA, method designed to generate precisely synchronized edited portrait videos. The edit is guided by modifying only the first frame of the source video, which, together with text prompt, defines the visual transformation to be applied. Our key idea is to fine-tune an image-to-video diffusion model using curated pairs of videos, one showing the original sequence and the other its edited counterpart. These paired videos are constructed to be frame-accurately synchronized, meaning that corresponding frames exhibit identical motion trajectories while differing only in the aspects altered by the edit. To obtain such pairs, we develop two-stage pipeline: generation followed by synchronization-based filtering. In the generation stage, vision-language model produces diverse subject prompts and edit instructions, which are used to synthesize matching portrait-image pairs via textto-image generation and image editing. These are then converted into side-by-side talking-head videos by leveraging the in-context generation capabilities of diffusion imageto-video model, concept explored in [14]. However, these naively generated pairs often suffer from temporal misalignments or motion drift. To address this critical data quality issue, our filtering stage evaluates each pairs temporal alignment using human motion metrics derived from facial and pose landmarks. We compute synchronization scores across four complementary channels (speech, gaze, blink, and pose) and retain only the most aligned pairs for training (see Figure 3). By training on our curated set of synchronized examples, the model learns to propagate motion cues from the source to the edited view, applying only localized changes defined by the first frame while maintaining framelevel alignment with the source video (see Figure 1). synthesizing an output video that adopts the new appearance while remaining perfectly synchronized with the source. In contrast to image-based IC-LoRA approaches that require separate training for each editing task, our single Sync-LoRA module generalizes across diverse edits and achieves robust balance between high visual fidelity and precise motion preservation, as demonstrated in our experiments (see section 4). 2. Related Work In-Context Learning in Diffusion Models In-context learning (ICL) enables models to generalize to new tasks by conditioning on structured input examples without explicit supervision. While extensively studied in language models [5], recent research has explored ICL in diffusionbased image generation [9, 26, 46, 65]. In these works, the model generates panel of images, where the images within the panel provide context to each other. This enables applications that require consistency across different generated images (e.g., storyboard generation) [3, 24, 52]. When conditioning such panels on given real image, ICL-based approaches can be used for wide range of image-to-image tasks, such as image personalization [6, 15, 16, 31, 44, 49] and image editing [18, 23, 38, 41, 42]. The most closely related work to ours is IC-LoRA [26]. This work observes that transformer-based diffusion models possess an inherent capability to generate panels in which certain attributes remain consistent across the different images in the panel. They show that this capability can be enhanced by fine-tuning the model with low-rank adapter (LoRA) on structured, consistent panels, and using an appropriate template prompt. Their approach has been applied to diverse tasks such as film storyboard generation, font design, and home decoration. Our work extends the paradigm to video, and presents an IC-LoRA for video editing which focuses on frame-accurate portrait synchronization. Additionally, recent progress has shown that large video diffusion transformers naturally exhibit in-context learning behavior, enabling them to generate multiple temporally or spatially related clips within single diffusion process [14]. That work focuses on analyzing this emergent ability for in-context generation, using inpainting-based conditioning to achieve controlled scene composition, but without any mechanism to ensure temporal correspondence across clips. Building on this insight, PoseGen [22] introduces in-context LoRA finetuning for pose-controllable human video generation, achieving long and coherent motion from reference image. In contrast, our approach explicitly trains this incontext behavior toward synchronized editing. At inference time, the user simply edits the first frame using any image editing tool. Sync-LoRA then takes the original video, the edited frame, and an edit prompt as input, LoRA for Video Diffusion Models Recent works have extended LoRA-based fine-tuning to video diffusion models, primarily focusing on subject or motion personalizaFigure 2. Overview of Sync-LoRA. Given source video S, an edited first frame I, and an edit prompt , Sync-LoRA denoises the target video conditioned on these inputs. During training, only the edited branch is noised, while the source branch stays clean and provides motion and identity cues through shared attention, so the model copies motion from and propagates the local edit across all frames. tion. Customize-A-Video [43] applies LoRA to the temporal attention layers of text-to-video diffusion model, enabling motion customization. CustomTTT [4] introduces method for generating customized videos in which the subject is taken from reference image and the motion from source video. This work identifies motionand appearancespecific layers and applies LoRA modules for per-layer adaptation. Abdal et al. [1] introduce the notion of personalizing dynamic concepts. In this task, the model learns both the appearance and motion of concept from reference video. They achieve this by training LoRA on the reference video, which then enables generation of the learned dynamic concept in novel scenes. While these works focus on personalizing appearance or motion, our method is designed for frame-accurate synchronized editing, which demands far stricter preservation of the source videos motion. Video Editing Recent research on diffusion-based video editing has introduced methods that use text or image prompts to enable user-guided modifications. Some methods [8, 33, 37, 50, 57] focus on text-based editing, typically excelling at appearance modifications that preserve motion. Others [27, 29, 39, 40] apply edits to the first frame using existing image editing techniques, and then propagate the changes across the video. Additional methods have been developed for specific tasks such as object insertion [60] or overlaying hand-drawn doodles onto videos [62]. In portrait video editing, earlier approaches rely on keypoints [2, 20, 36, 47, 54] or 3DMM template meshes [28, 51] to transfer expressions from driving video to source image. More recently, diffusion models have enabled new methods that generate videos from single image guided by driving video. These methods typically establish correspondence either through keypoints [35] or 3DMM [64] as an intermediate representation or by leveraging the attention mechanisms within diffusion models [55, 58, 59]. In contrast, our approach uses in-context learning: the edited first frame sets the appearance context, while the source video provides the motion context, eliminating the need for explicit correspondence modeling. 3. Method 3.1. In-Context LoRA Inspired by the IC-LoRA paradigm, we develop framework for portrait video editing. Our model learns to generate an edited video conditioned on clean source video and its edited first frame, which together serve as contextual input. This setup enables synchronized, context-aware editing during inference, where conditioning on the input video naturally steers the generation of its edited counterpart. While IC-LoRA was originally designed for static image panels, extending it to video requires the model to reason not only about appearance correspondence but also about temporal alignment across frames. To handle this additional temporal dimension, we build on transformer-based diffusion model (DiT) designed for image-to-video generation (see Figure 2). In this architecture, the input image (first frame), source video, and text prompt are represented as token sequences, and joint-attention layers operate over them to iteratively denoise the edited video representation. During training, only the edited view is denoised, while the source view remains noise-free and provides motion guidance through shared attention. This design directs the model to learn temporal synchronization between the two views rather than re-solving the edit, as the appearance change is already defined by the conditioning panel. At inference, the user edits the first frame, and the model generates coherent video that preserves the original motion. Alongside the visual conditioning, the model is also guided by text prompt that describes the intended relation between the two views (green tokens in Figure 2). Conditioned on the first frame and text prompt, the model learns to propagate the edit consistently across the entire sequence while maintaining precise motion correspondence. 3.2. Data Generation and Curation Training Sync-LoRA requires high-quality talking-head video pairs that differ in appearance but exhibit tight temporal synchronization. These video pairs serve as the foundational supervision signal, and, as shown in our abFigure 3. Data generation and curation pipeline. Our process constructs synchronized video pairs for Sync-LoRA (Top) Portrait images are generated, edited, and contraining. verted into side-by-side talking-head videos. (Middle) Facial and pose landmarks yield motion signals for speech, gaze, blink, and pose. (Bottom) Pairs are scored and filtered by synchronization quality, keeping only the most aligned examples for training. lations, their precision is critical for faithfully transferring motion dynamics while applying the necessary edits (see subsection 4.3). To create such data, we devise large-scale generation followed by two-stage pipeline: synchronization-based filtering. The full process is illustrated in Figure 3. Generation. We generate diverse, large-scale synthetic talking-head pairs. vision-language model (VLM) produces subject prompts and edit instructions (e.g., change hair color, add hat), to create aligned portrait-image pairs via image generation and editing. Each pair is composed into dual-panel image and converted into sideby-side talking-head video using an image-to-video model. The process leverages the in-context generation ability of our base data-generation model (Wan2.1 [53]), which we further refine by LoRA fine-tuning on small, curated set of synchronized examples to improve motion consistency. Filtering. Despite being designed for synchronization, generated pairs often contain misalignments. To ensure data quality, we introduce filtering stage that quantifies temporal correspondence using motion-based metrics derived from facial and pose landmarks. Human facial and pose Figure 4. Synchronization signal visualization. Two synchronization cues used in our filtering process. Top: Eye landmarks are used to compute the Eye Aspect Ratio (EAR). Note how the plotted peaks (reference in green, edited in orange) correspond directly to the blink event shown in the frames above. Bottom: Upper-body pose landmarks are used to track the right elbow angle. The plots again show tightly correlated motion, confirming the arm movement is synchronized across both videos. landmarks are extracted using MediaPipe [34], producing per-frame scalar signals for analysis. For each pair, trajectories of key landmarks are analyzed across four motion channels ,speech, gaze, blink, and pose, capturing complementary aspects of talking-head dynamics. Signals are interpolated to fill missing detections, smoothed with SavitzkyGolay filter [45], z-normalized, and correlated between the left and right views at zero temporal lag. Visualization is shown in Figure 4 and full implementation details are provided in the supplementary material. Pairs with insufficient detection coverage are discarded, and top-ranked examples are retained, yielding high-quality synchronized videos. Synchronization Scoring Metrics. We measure speech using the mouth aspect ratio over time. Blink uses the Eye Aspect Ratio (EAR) [7], with peaks indicating closure events. Gaze tracks normalized 2D iris motion, while pose uses six upper-body angles or relative heights (shoulder, torso, elbows, wrists). Each signal is independently correlated between the original and edited videos and then combined into weighted synchronization score (weights: 40% speech, 30% gaze, 15% blink, 15% pose). This curated dataset forms the foundation of Sync-LoRA, enabling precise motion preservation while applying localized visual edits. Full definitions and implementation details are provided in the supplementary material. Figure 5. Comparison of portrait video editing methods. The rows show the source video and results from LucyEdit, VACE, AnyV2V, FlowEdit, and Sync-LoRA (Ours). The columns depict different temporal positions. Our method, VACE, AnyV2V, and FlowEdit utilize the same edited first frame as visual input, whereas the text-based LucyEdit operates from text guidance alone. Implementation Details. We use QwenImage and QwenImageEdit-2509 [56] for portrait generation and editing the first frame, respectively. For video synthesis, we employ Wan2.1 [53] as the base image-to-video model, generating each paired video with 30 denoising steps. We generate over 20,000 paired examples and apply synchronization-based filtering to retain the most temporally aligned samples. The final training set consists of only 512 video pairs, with 3:1 ratio of edited to identical (unmodified) samples to stabilize training. 3.3. Training Base Model. We build our method upon LTX-Video [21], transformer-based latent diffusion model that integrates the video-VAE and the denoising transformer in unified framework. LTX-Video operates in highly compressed spatio-temporal latent space and employs full 3D attention, enabling efficient generation of high-resolution, temporally coherent videos. To adapt this model for synchronized portrait editing, we fine-tune it using low-rank adaptation (LoRA) [25] with rank of 128. This lightweight adaptation preserves the rich prior of LTX-Video while encouraging the model to rely on the provided first frame for appearance and motion cues. This setup ensures consistent temporal alignment and visual coherence, without altering the base architecture. Training is facilitated by the LTX-Video-Trainer code [61]. Positional Encoding. Following LTX-Video [21], each latent token is represented by continuous 3D coordinate (t, h, w) within the spatio-temporal volume, scaled by VAE compression factors and normalized by frame rate. We embed these coordinates using 3D Rotary Positional Embeddings (RoPE) [48]. While both source and target streams share identical positional coordinates for spatial alignment, they are differentiated by per-token timesteps. This mechanism, inherited from the LTX-Video base model [21], uses each tokens timestep to generate unique scale and shift parameters for its Adaptive Layer Normalization (AdaLN). The model thus learns to treat source stream tokens (assigned = 0) as clean conditioning, and target stream tokens (assigned > 0) as the noisy sequence to be denoised. Loss Function. We adopt the rectified flow objective [13, 32], also used in LTX-Video, to train our model to predict the velocity field that transforms noise into the clean latent of the edited video. Unlike conventional diffusion setups generating single video from text or an image prompt, our formulation introduces an additional conditioning stream, the source video, which remains noise-free throughout denoising and provides motion and appearance cues. Given noisy latent sample of the edited branch xt = (1 t) x1 + x0 with timestep [0, 1], the model is optimized to match the ground-truth velocity vt = xt x0: LRF = (cid:104) u(xt, ctext, cimg, cvid, t; Î¸LoRA) vt2 2 (cid:105) , where x0 (0, I) is Gaussian noise, x1 is the target latent of the edited video, ctext is the editing prompt, cimg is the edited first frame, and cvid represents the latent features of the source video. 4. Experiments Evaluation Setup. We evaluate Sync-LoRA on curated benchmark of 166 portrait videos assessing spatial fidelity, and temporal alignment. The benchmark spans diverse edit types, including object insertion, background replacement, colorization, and appearance modification, with source clips from CelebV [63], CelebV-HQ [66], TalkVid [10], and high-quality YouTube content. To ensure consistency, we apply strict curation, retaining only sharp, well-lit clips of at least five seconds. Clips with subtitles, letterboxing, cropping, or jump cuts are excluded. Representative accepted and rejected samples are shown in the supplementary. For each sequence, the first frame is edited using QwenImageEdit-2509 [56], guided by VLM-generated instructions to cover wide range of transformations. We compare our method against strong baselines, VACE [27], LucyEdit [50], FlowEdit [30], and AnyV2V [29], covering both qualitative and quantitative evaluations across state-of-the-art video editing paradigms. The VACE, LucyEdit, and AnyV2V results are based on their official implementations, and for FlowEdit, we used the public LTX-Video implementation. All clips contain 81 frames at 20 FPS. four 4.1. Qualitative Evaluation Qualitative Comparison. As shown in Figure 5, SyncLoRA produces temporally coherent and visually faithful results across the diverse editing tasks illustrated in the figure. While all baselines attempt the edit, they exhibit notable failures. Using the third column as representative example, LucyEdit fails to apply the edit faithfully, while VACE, AnyV2V, and FlowEdit all introduce temporal artifacts, inconsistent textures, or motion drift. In contrast, Sync-LoRA faithfully preserves the source videos motion while consistently applying the edits across all frames. Our method achieves superior synchronization and visual fidelity in all examples, demonstrating frame-accurate alignment and consistent visual appearance throughout the sequence. The full temporal performance of our method and all baselines is best assessed in the supplementary video, which highlights Sync-LoRAs ability to maintain precise synchronization while achieving high edit fidelity. 4.2. Quantitative Comparisons We evaluate the generated outputs across three key aspects: Synchronization, Edit Fidelity, and Identity Preservation. Metric VACE LucyEdit FlowEdit AnyV2V Ours Synchronization Speech Corr. Gaze Corr. Blink Corr. Pose Corr. Edit Fidelity Directional CLIP (image) Directional CLIP (text-dual) CLIP-Text Align. Identity Preservation ArcFace Sim. Pose Canny Depth 0.37 0.63 0.34 0. 0.57 0.20 0.33 0.11 0.63 0.24 0.39 0.55 0.20 0.33 0.39 0.69 0.36 0.51 0.45 0.16 0.32 0.80 0.82 0.70 0. N/A 0.11 0.32 0.50 0.56 0.33 0.38 0.53 0.18 0.32 0.70 0.71 0.48 0.51 0.39 0.17 0.32 0.72 0.75 0.55 0. 0.57 0.21 0.33 0.73 0.71 0.72 0.69 0. 0.63 0.75 Table 1. Quantitative Video Comparisons. We evaluate across three axes: Synchronization, Edit Fidelity, and Identity Preservation. The Directional CLIP (image) score is omitted for LucyEdit, as it operates solely from text prompts, making this metric not directly applicable. (1) Synchronization. We measure frame-level temporal correspondence between the source and edited videos using four correlation-based metrics: speech, gaze, blink, and pose. Each captures distinct aspect of motion dynamics: mouth aspect ratio for speech, iris trajectory for gaze, eye aspect ratio for blinking, and upper-body joint angles for pose. While these metrics are identical to those used for our training data filtering (subsection 3.2), they serve only to curate the dataset and perform this final evaluation. They are never used as loss function or otherwise included in the models training objective. (2) Edit Fidelity. We evaluate whether the intended edit is accurately applied and remains consistent throughout the video using two complementary CLIP-based metrics. Directional CLIP Score. Originally proposed for image editing, we adapt the Directional CLIP metric to the video domain to measure how consistently the per-frame visual transformation aligns with the overall edit direction in CLIPs embedding space. We report two complementary variants: an image-based version that quantifies alignment with the visual edit implied by the edited first frame, and text-dual version that measures alignment with the semantic edit described by the text prompt. The text-dual variant is included for fairness, since some baselines such as LucyEdit operate purely from textual guidance and do not have access to the edited first frame. Intuitively, the image-based variant asks whether the change from source frames to edited frames looks like the change between the unedited and edited first frames, while the text-dual variant asks whether that same visual change looks like the change suggested by the edit text. CLIP-Text Alignment Score. To further assess semantic accuracy, we measure how well each generated frame matches the textual description in CLIP space. This score captures whether the video remains faithful to the prompt semantics across time, complementing the directional measure above. In contrast to the directional score, CLIP-Text Alignment Figure 6. Necessity of all synchronization cues. Each column shows results when training without the specified motion cue (pose, gaze, speech, or blink) from the filtering stage, compared to our full setup. The source video is shown on the left. Omitting any cue causes motion drift or misalignment across frames. ignores the source frames and judges only how well each edited frame by itself matches the target description. Together, these metrics jointly evaluate both the directional consistency of the applied edit and its semantic alignment throughout the sequence. (3) Identity Preservation. To evaluate how well subject identity is maintained throughout the edited video, we compute the mean cosine similarity between face embeddings extracted by pre-trained ArcFace [12] model for each frame and the edited first frame. This measure reflects how faithfully the appearance and facial structure of the person are preserved during temporal propagation, where higher values correspond to stronger identity consistency. Furthermore, user study confirmed our method is perceptually preferred for visual quality and temporal alignment. Full details for this study and all quantitative metric formulations are provided in the supplementary material. Results Discussion. As shown in Table 1, while text-based methods like LucyEdit achieve high scores on some sync metrics (e.g., Speech Corr. 0.80 vs. our 0.72), this comes at the cost of failing the edit itself, as shown by its low 0.11 Directional CLIP (text-dual) score (vs. our 0.21). Our method achieves the best overall balance, strongly performing on synchronization while also faithfully applying the visual edit. VACE, conversely, benefits from strong external conditioning (pose/canny/depth), yields higher CLIPbased alignment, yet it exhibits weaker temporal correspondence and identity consistency. By explicitly training for synchronized editing, Sync-LoRA maintains strong temporal alignment while producing precise edits that preserve subject identity. This trade-off highlights the strength of our in-context formulation, which balances motion coherence and edit fidelity rather than excelling in single axis. Figure 7. Effect of dataset composition strategy. Qualitative comparison of different dataset composition strategies. Each column shows distinct training setup: ID-Only (identical pairs), Edit-Only (edited pairs), Random (unfiltered pairs). Our full method is on rightmost column and the source video is on the left. 4.3. Ablation Studies In Figure 6, we present leave-one-out experiment where we test the contribution of each synchronization channel. This is done by training models on datasets curated without one component (speech, gaze, blink, or pose), which we achieve by setting its respective weight to zero in our filtering score (subsection 3.2). Concretely, in the first example (top two rows), excluding pose leads to visible errors in head orientation, while in the second (bottom two rows), removing any channel introduces spatial drift and temporal misalignment in gaze or blinking. Omitting any channel degrades synchronization, notably in the mans mouth and the womans eye motion, confirming that all four cues provide complementary motion information. In Figure 7 we ablate different dataset compositions and filtering on model performance. We evaluate three baseID-Only, containing identical (unmodified) pairs; lines: Edit-Only, containing only edited pairs; and Random, using unfiltered, randomly generated pairs. As illustrated in the top two rows, ID-Only maintains alignment but fails to preserve the edit, while Edit-Only and Random introduce drift in facial regions such as the eyes, mouth, and gaze. Our full model achieves both precise synchronization and faithful edit preservation, for example keeping the womans eye motion aligned and the painted edit on Batmans mouth stable, confirming the importance of balanced data and synchronization-based filtering. formal quantitative analysis in Table 2 confirms these qualitative observations. The leave-one-out study shows removing any single motion cue degrades performance. NoMetric Ablations (w/o component) Baselines Ours Speech Gaze Blink Pose Only Edit Only ID Random Synchronization Speech Corr. Gaze Corr. Blink Corr. Pose Corr. Edit Fidelity Directional CLIP Directional CLIP (text-dual) CLIP-Text Align. Identity Preservation ArcFace Sim. 0.53 0.68 0.46 0.47 0.55 0.20 0.33 0.56 0.67 0.45 0. 0.57 0.20 0.33 0.55 0.70 0.49 0.46 0.56 0.20 0.33 0.55 0.67 0.45 0.46 0.56 0.20 0.33 0.58 0.68 0.48 0. 0.57 0.21 0.33 0.80 0.83 0.70 0.66 0.05 0.01 0.31 0.56 0.68 0.47 0.46 0.55 0.20 0.33 0.72 0.75 0.55 0. 0.57 0.21 0.33 0.72 0.72 0.72 0.72 0. 0.70 0.72 0.75 Table 2. Impact of Data Curation Strategy. We compare our full method (Ours) against two ablation categories: models trained without specific motion cues (w/o component) and models trained on alternative dataset compositions (Baselines) . tably, removing the speech cue causes the largest drop in Speech Corr. (0.72 to 0.53), confirming its necessity for lipsync. Removing the pose cue degrades Gaze Corr. (0.75 to 0.67), revealing learned correlation between head pose and eye movement. The baseline results in Table 2 further prove our data strategy is essential. The ID-Only baseline achieves the highest synchronization (e.g., 0.80 Speech Corr.) but completely fails the edit, scoring only 0.05 in Directional CLIP. Conversely, the Edit-Only and Random baselines, which lack our filtering and data balance, suffer from poor synchronization and identity drift. Our full method is the only one to achieve strong balance across all three core metrics, confirming our design choices. 4.4. Application - Expression Modification Demonstrating its capability beyond spatial and appearance modifications, Sync-LoRA also performs expression editing by propagating facial emotion changes while maintaining the underlying articulation. As illustrated in Figure 8, our method accurately transfers expressions such as happiness, anger, or sadness to the same source motion, even in the presence of partial occlusions. To train an expressionspecific Sync-LoRA, we employ LivePortrait [20] to synthesize multiple expressive videos (e.g., happy, angry) from single neutral reference frame. While LivePortrait its warpingachieves impressive real-time reenactment, based synthesis often struggles under extreme head rotations or when occlusions, such as microphones, masks, or hands, partially obscure the face. In these challenging cases, Sync-LoRA produces more stable and photorealistic results, maintaining structural consistency and accurate synchronization throughout the sequence. This demonstrates the robustness of our diffusion-based in-context formulation, which models temporal coherence directly in the latent space rather than relying on geometric warping. Comprehensive comparisons highlighting these robustness gains are presented in the supplementary materials. Figure 8. Expression editing. Sync-LoRA performs expression editing (Happy, Angry, Sad) while keeping motion synchronized and geometry consistent, even under occlusions. 5. Conclusions, Limitations and Future work Sync-LoRA introduces an in-context LoRA framework for synchronized portrait video editing. By conditioning generation on the original video and its edited first frame, the model learns to propagate local appearance modifications while preserving precise temporal alignment and subject identity. Trained on automatically generated and synchronization-filtered video pairs, Sync-LoRA balances edit fidelity, motion correspondence, and visual realism across diverse subjects and edit types including background manipulation, object insertion, expression modification. While our method performs robustly across wide range of scenarios, some limitations remain. The quality of the applied edits may degrade in sequences with fast or large-scale motion, where maintaining fine spatial coherence becomes challenging. Our approach also assumes that the edited first frame is geometrically aligned with the source; significant misalignments can lead to temporal drift or local artifacts during propagation. We provide visual examples of these failure cases in the supplementary material. Resolving these issues remains future work. In addition, since Sync-LoRA builds upon specific base video diffusion model, future versions could benefit from stronger architectures with enhanced temporal reasoning and multi-view consistency. Overall, Sync-LoRA establishes new paradigm for controllable and temporally faithful video editing. This level of frame-accurate synchronization is especially crucial for personalized talking-head applications, where faithfulness to the original action, speech, and performance is essential. Looking ahead, extending this in-context formulation to emerging multi-modal video models that jointly reason over video and audio signals presents an exciting and challenging direction for future research."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Sigal Raab, Ellie Arar, Nadav Magar, and Mickey Finkelson for their valuable feedback and insightful reviews of this work. This research was supported in part by the Israel Science Foundation (grants no. 2492/20 and 1473/24), Len Blavatnik and the Blavatnik family foundation."
        },
        {
            "title": "References",
            "content": "[1] Rameen Abdal, Or Patashnik, Ivan Skorokhodov, Willi Menapace, Aliaksandr Siarohin, Sergey Tulyakov, Daniel Cohen-Or, and Kfir Aberman. Dynamic concepts personalization from single videos, 2025. 2, 3 [2] Hadar Averbuch-Elor, Daniel Cohen-Or, Johannes Kopf, and Michael F. Cohen. Bringing portraits to life. ACM Trans. Graph., 36(6), 2017. 3 [3] Omri Avrahami, Amir Hertz, Yael Vinker, Moab Arar, Shlomi Fruchter, Ohad Fried, Daniel Cohen-Or, and Dani Lischinski. The chosen one: Consistent characters in textto-image diffusion models. In ACM SIGGRAPH 2024 Conference Papers, New York, NY, USA, 2024. Association for Computing Machinery. 2 [4] Xiuli Bi, Jian Lu, Bo Liu, Xiaodong Cun, Yong Zhang, WeiSheng Li, and Bin Xiao. Customttt: Motion and appearance customized video generation via test-time training. arXiv preprint arXiv:2412.15646, 2024. 3 [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. 2 [6] Shengqu Cai, Eric Chan, Yunzhi Zhang, Leonidas Guibas, Jiajun Wu, and Gordon. Wetzstein. Diffusion self-distillation for zero-shot customized image generation. In CVPR, 2025. 2 [7] Jan Cech and Tereza Soukupova. Real-time eye blink detection using facial landmarks. Cent. Mach. Perception, Dep. Cybern. Fac. Electr. Eng. Czech Tech. Univ. Prague, pages 18, 2016. [8] Duygu Ceylan, Chun-Hao Huang, and Niloy J. Mitra. Pix2video: Video editing using image diffusion. 2023. 3 [9] Lan Chen, Qi Mao, Yuchao Gu, and Mike Zheng Shou. Edit transfer: Learning image editing via vision in-context relations, 2025. 2 [10] Shunian Chen, Hejin Huang, Yexin Liu, Zihan Ye, Pengcheng Chen, Chenghao Zhu, Michael Guan, Rongsheng Wang, Junying Chen, Guanbin Li, Ser-Nam Lim, Harry Yang, and Benyou Wang. Talkvid: large-scale diversified dataset for audio-driven talking head synthesis, 2025. 6 [11] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Yuwei Fang, Kwot Sin Lee, Ivan Skorokhodov, Kfir Aberman, Jun-Yan Zhu, Ming-Hsuan Yang, and Sergey Tulyakov. Multi-subject open-set personalization in video generation. arXiv preprint arXiv:2501.06187, 2025. 2 gin loss for deep face recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(10):59625979, 2022. 7, 3 [13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024. 5 [14] Zhengcong Fei, Di Qiu, Debang Li, Changqian Yu, and Mingyuan Fan. Video diffusion transformers are in-context learners, 2025. 2 [15] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion, 2022. 2 [16] Rinon Gal, Or Lichter, Elad Richardson, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. Lcmlookahead for encoder-based text-to-image personalization, 2024. [17] Xuan Gao, Haiyao Xiao, Chenglai Zhong, Shimin Hu, Yudong Guo, and Juyong Zhang. Portrait video editing emIn ACM SIGpowered by multimodal generative priors. GRAPH Asia Conference Proceedings, 2024. 1, 2 [18] Daniel Garibi, Or Patashnik, Andrey Voynov, Hadar Averbuch-Elor, and Daniel Cohen-Or. Renoise: Real image inversion through iterative noising, 2024. 2 [19] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. arXiv preprint arxiv:2307.10373, 2023. 1 [20] Jianzhu Guo, Dingyun Zhang, Xiaoqiang Liu, Zhizhou Zhong, Yuan Zhang, Pengfei Wan, and Di Zhang. Liveportrait: Efficient portrait animation with stitching and retargeting control. arXiv preprint arXiv:2407.03168, 2024. 1, 3, 8, 4 [21] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. Ltx-video: Realtime video latent diffusion, 2024. 5, 1 [22] Jingxuan He, Busheng Su, and Finn Wong. Posegen: Incontext lora finetuning for pose-controllable long human video generation, 2025. [23] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. 2022. 2 [24] Amir Hertz, Andrey Voynov, Shlomi Fruchter, and Daniel Cohen-Or. Style aligned image generation via shared attention. 2023. 2 [25] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. 5 [12] Jiankang Deng, Jia Guo, Jing Yang, Niannan Xue, Irene Kotsia, and Stefanos Zafeiriou. Arcface: Additive angular mar- [26] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, and Jingren Zhou. In-context lora for diffusion transformers. 2024. [27] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. 1, 2, 3, 6 [28] Taras Khakhulin, Vanessa Sklyarova, Victor Lempitsky, and Egor Zakharov. Realistic one-shot mesh-based head avatars. In European Conference of Computer vision (ECCV), 2022. 3 [29] Max Ku, Cong Wei, Weiming Ren, Harry Yang, and Wenhu Chen. Anyv2v: tuning-free framework for any video-tovideo editing tasks. arXiv preprint arXiv:2403.14468, 2024. 2, 3, 6 [30] Vladimir Kulikov, Matan Kleiner, Inbar HubermanSpiegelglas, and Tomer Michaeli. Flowedit: Inversion-free text-based editing using pre-trained flow models, 2025. 6 [31] Nupur Kumari, Xi Yin, Jun-Yan Zhu, Ishan Misra, and Samaneh Azadi. Generating multi-image synthetic data for text-to-image customization. ArXiv, 2025. [32] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling, 2023. 5 [33] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-attention control. arXiv:2303.04761, 2023. 3 [34] Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris McClanahan, Esha Uboweja, Michael Hays, Fan Zhang, ChuoLing Chang, Ming Guang Yong, Juhyun Lee, Wan-Teh Chang, Wei Hua, Manfred Georg, and Matthias Grundmann. Mediapipe: framework for building perception pipelines, 2019. 4, 1 [35] Yue Ma, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu, et al. Follow-your-emoji: Fine-controllable and expressive freestyle portrait animation. arXiv preprint arXiv:2406.01900, 2024. 1, 3 [36] Arun Mallya, Ting-Chun Wang, and Ming-Yu Liu. Implicit warping for animation with image sets. In Advances in Neural Information Processing Systems, pages 2243822450. Curran Associates, Inc., 2022. 3 [37] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid Hoshen. Dreamix: Video diffusion models are general video editors. arXiv preprint arXiv:2302.01329, 2023. 2, 3 [38] Ivona Najdenkoska, Animesh Sinha, Abhimanyu Dubey, Dhruv Mahajan, Vignesh Ramanathan, and Filip Radenovic. Context diffusion: In-context aware image generation, 2025. [39] Hao Ouyang, Qiuyu Wang, Yuxi Xiao, Qingyan Bai, Juntao Zhang, Kecheng Zheng, Xiaowei Zhou, Qifeng Chen, and Yujun Shen. Codef: Content deformation fields for arXiv preprint temporally consistent video processing. arXiv:2308.07926, 2023. 3 [40] Wenqi Ouyang, Yi Dong, Lei Yang, Jianlou Si, and Xingang Pan. I2vedit: First-frame-guided video editing via imageto-video diffusion models. In SIGGRAPH Asia 2024 Conference Papers, New York, NY, USA, 2024. Association for Computing Machinery. 3 [41] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In ACM SIGGRAPH 2023 conference proceedings, pages 111, 2023. 2 [42] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip: Text-driven manipulation In Proceedings of the IEEE/CVF Inof stylegan imagery. ternational Conference on Computer Vision (ICCV), pages 20852094, 2021. 2 [43] Yixuan Ren, Yang Zhou, Jimei Yang, Jing Shi, Difan Liu, Feng Liu, Mingi Kwon, and Abhinav Shrivastava. Customize-a-video: One-shot motion customization of textto-video diffusion models. In European Conference on Computer Vision, pages 332349. Springer, 2024. 3 [44] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. [45] Abraham. Savitzky and M. J. E. Golay. Smoothing and differentiation of data by simplified least squares procedures. Analytical Chemistry, 36(8):16271639, 1964. 4, 1 [46] Chaehun Shin, Jooyoung Choi, Heeseung Kim, and Sungroh Yoon. Large-scale text-to-image model with inpainting is zero-shot subject-driven image generator. 2024. 2 [47] Aliaksandr Siarohin, Stephane Lathuili`ere, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. Curran Associates Inc., Red Hook, NY, USA, 2019. 3 [48] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2023. 5 [49] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer, 2024. 2 [50] DecartAI Team. Lucy edit: Open-weight text-guided video editing. 2025. 3, 6 [51] Ayush Tewari, Mohamed Elgharib, Gaurav Bharaj, Florian Bernard, Hans-Peter Seidel, Patrick Perez, Michael Zollhofer, and Christian Theobalt. Stylerig: Rigging stylegan for 3d control over portrait images. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 61416150, 2020. 3 [52] Yoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior Wolf, Gal Chechik, and Yuval Atzmon. Training-free consistent text-to-image generation, 2024. 2 [53] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun [66] Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang, Li Zhang, Ziwei Liu, and Chen Change Loy. Celebvhq: large-scale video facial attributes dataset, 2022. 6 Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models, 2025. 4, [54] Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot free-view neural talking-head synthesis for video conferencing. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1003410044, 2021. 3 [55] Huawei Wei, Zejun Yang, and Zhisheng Wang. Aniportrait: Audio-driven synthesis of photorealistic portrait animations, 2024. 3 [56] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025. 5, 6 [57] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 76237633, 2023. 1, 3 [58] You Xie, Hongyi Xu, Guoxian Song, Chao Wang, Yichun Shi, and Linjie Luo. X-portrait: Expressive portrait animation with hierarchical motion attention. In ACM SIGGRAPH 2024 Conference Papers, New York, NY, USA, 2024. Association for Computing Machinery. 3 [59] Shurong Yang, Huadong Li, Juhao Wu, Minhao Jing, Linze Li, Renhe Ji, Jiajun Liang, and Haoqiang Fan. Megactor: Harness the power of raw video for vivid portrait animation, 2024. [60] Danah Yatim, Rafail Fridman, Omer Bar-Tal, and Tali Dekel. Dynvfx: Augmenting real videos with dynamic content, 2025. 3 [61] Matan Ben Yosef, Naomi Ken Korem, and Tavi Halperin. Ltx-video community trainer, 2025. 5 [62] Emilie Yu, Kevin Blackburn-Matzen, Cuong Nguyen, Oliver Wang, Rubaiat Habib Kazi, and Adrien Bousseau. Videodoodles: Hand-drawn animations on videos with scene-aware canvases. ACM Trans. Graph., 2023. 3 [63] Jianhui Yu, Hao Zhu, Liming Jiang, Chen Change Loy, Weidong Cai, and Wayne Wu. Celebv-text: large-scale facial text-video dataset, 2023. 6 [64] Bohan Zeng, Xuhui Liu, Sicheng Gao, Boyu Liu, Hong Li, Jianzhuang Liu, and Baochang Zhang. Face animation with an attribute-guided diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 628637, 2023. [65] Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image editing with in-context generation in large scale diffusion transformer. arXiv, 2025. 2 In-Context Sync-LoRA for Portrait Video Editing"
        },
        {
            "title": "Supplementary Material",
            "content": "Hyperparameter Value Base Model LoRA Rank Optimizer Learning Rate Batch Size Total Training Steps Resolution Frames per Clip Training Hardware LTX-Video [21] 128 AdamW 2 104 1 5,000 512 512 81 1 NVIDIA A6000 (48GB) Table 3. Training Hyperparameters. Summary of the configuration used for fine-tuning Sync-LoRA. 6. Additional Implementation Details 6.1. Reference and Target Stream Differentiation We implement in-context conditioning by spatially concatenating the source and target streams along the sequence dimension, and differentiating them via per-token timestep conditioning while preserving spatial correspondence. Algorithm 1 details the masking strategy that blocks gradients through the reference stream so it acts as frozen context while gradients flow only through the target stream. 6.2. Hyperparameters We provide detailed summary of the hyperparameters used for training Sync-LoRA in Table 3. 7. Data Curation Pipeline This section details the synchronization-based filtering process used to curate our training dataset. Figure 13 illustrates this curation process, showing representative examples of generated pairs that were accepted or rejected based on their synchronization scores. 7.1. Synchronization Scoring Metrics Our filtering pipeline relies on four motion channels derived from MediaPipe [34] landmarks. Let pi R2 denote the 2D coordinates of landmark index i. For each paired video (source and edited), we extract per-frame scalar signals for each channel as follows: Speech (40% weight). We compute the Mouth Aspect Ratio (MAR) as the ratio of the average vertical lip separation to the mouth width. Using the standard MediaPipe mesh indices, we obtain: MAR = 1 4 (cid:80)4 j=1 puj pdj 2 p61 p2912 (1) the where {(13, 14), (82, 87), (312, 317), (0, 17)}, 61, 291 represent the mouth corners. vertical pairs (uj, dj) and are indices Gaze (30% weight). We calculate the normalized gaze vector = [gx, gy] representing the iris center relative to the eye bounding box. For the iris center piris and eye boundary landmarks (inner xin, outer xout, upper yup, lower ydn), the normalized coordinates are: gx = 2 piris,x xin xout xin 1, gy = 2 piris,y yup ydn yup 1 (2) We compute this for both eyes and average the motion vectors. Blink (15% weight). We utilize simplified Eye Aspect Ratio (EAR) based on the ratio of vertical to horizontal eye landmarks. The signal is averaged across both eyes and negated so that blink events appear as positive peaks: Signalblink = 1 2 (cid:18) p159 p145 p33 p133 + p386 p374 p263 p362 (cid:19) (3) Pose (15% weight). We track six upper-body geometric features to capture torso dynamics: 1. Shoulder Orientation: Angle of the vector p12 p11. 2. Torso Inclination: Angle of the vector connecting the shoulder midpoint to the hip midpoint (p23, p24). 3. Elbow Angles (Left/Right): Angles formed by the Shoulder-Elbow-Wrist triplets (11-13-15 and 12-1416). 4. Relative Wrist Heights (Left/Right): The vertical displacement between shoulder and wrist, defined as = yshoulder ywrist. Signal Processing: To ensure robustness against detection noise, all raw signals undergo the following processing steps: 1. Interpolation: Missing detections (NaNs) are filled via linear interpolation. 2. Smoothing: We apply Savitzky-Golay [45] filter with window length of = 9 and polynomial order = 2. 3. Normalization: Signals are z-score normalized, = xÂµ Ï+Ïµ , with Ïµ = 106. Final Score: The final synchronization score is computed as the weighted sum of the Pearson correlation coefficients (at zero temporal lag) between the processed signals of the source and edited videos. Algorithm 1 Pseudocode for Reference and Target Stream Differentiation 1 def training_step(source_video, edited_video, diffusion_t): 2 4 5 6 7 8 10 11 12 13 14 16 17 18 19 20 22 23 24 25 26 28 29 30 31 32 34 35 36 37 38 40 41 42 43 \"\"\" source_video: Clean latent frames from the reference video edited_video: Noisy latent frames (at time t) from the target video diffusion_t: The noise level (timestep) for the target video \"\"\" # 1. Sequence Concatenation # We concatenate along the sequence dimension (frames). # This allows joint attention between source and target. model_input = torch.cat([source_video, edited_video], dim=2) # 2. Per-Token Timestep Conditioning # Source tokens get t=0 (signaling \"clean context\"). # Target tokens get t=diffusion_t (signaling \"denoise me\"). # These timesteps generate distinct AdaLayerNorm parameters. t_source = torch.zeros(source_video.shape[0]) t_target = diffusion_t model_timesteps = torch.cat([t_source, t_target], dim=0) # 3. Shared Positional Embeddings (RoPE) # Both streams use identical grid coordinates (t, h, w). # token at (Frame 1, x, y) in Source has the exact same # positional embedding as (Frame 1, x, y) in Target. rope_source = get_3d_rope(source_video.shape) rope_target = get_3d_rope(edited_video.shape) # Identical to source model_rope = torch.cat([rope_source, rope_target], dim=1) # 4. Forward Pass # The model uses the shared RoPE to find spatial correspondences # and the split timesteps to apply different normalization. velocity_pred = model( x=model_input, timesteps=model_timesteps, pos_embed=model_rope ) # 5. Loss Masking # We only compute loss on the target (edited) half of the sequence. pred_source, pred_target = torch.chunk(velocity_pred, 2, dim=2) loss = F.mse_loss(pred_target, target_velocity_ground_truth) return loss 8. Evaluation Benchmark and Metrics 8.1. Benchmark Curation As mentioned in Section 4 of the main paper, our evaluation benchmark was strictly curated to ensure high-quality assessment. We applied rigorous criteria, retaining only clips with consistent lighting and high sharpness, while excluding those with jump cuts, subtitles, or significant motion blur. Figure 9 illustrates typical rejection case, such as clip with noticeable letterboxing. 8.2. Quantitative Evaluation Metric Definitions This subsection provides the full mathematical definitions of the metrics reported in Subsection 4.2 of the main paper. All notations follow those used in the main paper. Given source frames src , and an edited keyframe key, the following metrics are used for quantitative evaluation. , edited frames edit Directional CLIP Score. This metric measures how consistently the per-frame visual transformation aligns with the intended edit direction in CLIPs embedding space. We report two variants: an image-based version and text-dual version. The CLIP model used is ViT-B-32. Image-based Direction. The global edit direction in CLIP image space is defined as: dimg = Eimg(I key) Eimg(I src 0 ) Eimg(I key) Eimg(I src 0 ) . For each frame t, we compute its local transformation diArcFace Similarity. To quantify identity preservation, we compute the mean cosine similarity between ArcFace [12] embeddings of each frame and the edited keyframe: ArcFaceSim ="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 Eface(I edit ) Eface(I key) Higher values correspond to stronger identity consistency across frames. 9. User Study We conducted user study to evaluate Sync-LoRA based on four distinct criteria: (1) Edit Fidelity (adherence to the visual instruction), (2) Synchronization (preservation of timing and movement), (3) Identity Preservation (consistency with the subjects original identity), and (4) Overall Preference. To this end, we performed blinded, randomized Two-Alternative Forced Choice (2AFC) comparison between our method and four state-of-the-art baselines: VACE, LucyEdit, AnyV2V, and FlowEdit. We curated robust pool of 36 distinct video comparison pairs distributed across three unique evaluation forms. The study involved 23 independent participants, where each completed 12 comparisons (3 per baseline), yielding total of 69 pairwise judgments for each competitor method. The complete results are presented in Figure 10, where we report the preference rates for Sync-LoRA against each baseline. As shown, users strongly favored our approach by significant margin across all metrics. Notably, our method achieved the highest gains in Identity Preservation and Synchronization, confirming our core contribution of maintaining temporal coherence and subject identity while faithfully applying the intended edits. 10. Supplementary Videos Qualitative evaluation of video editing requires assessing motion stability and temporal coherence, which static frames cannot fully convey. Therefore, we strongly encourage readers to view the results in the attached supplementary webpage. This interactive file features comprehensive side-by-side comparisons against all baselines (VACE, LucyEdit, FlowEdit, AnyV2V), detailed ablation studies verifying our synchronization cues, and extensive examples of expression editing and failure cases. 11. Expression Modification This section provides comparisons for the expression editing application mentioned in Subsection 4.4 of the main paper. Specifically, we compare our results against LivePortrait [20] to demonstrate how our diffusion-based approach resolves emergent issues typical of warping-based modules. Figure 9. Benchmark Examples. examples rection: Vt = Eimg(I edit Eimg(I edit t ) Eimg(I src ) Eimg(I src ) ) . The Directional CLIP score is their mean cosine similarity: DirectionalCLIPimg = 1 (cid:88) t=1 Vt dimg. Text-dual Direction. For methods guided by text rather than edited keyframes, the semantic direction is obtained from CLIPs text encoder: dtext = Etext(ttarget) Etext(tsource) Etext(ttarget) Etext(tsource) . The local per-frame directions Vt are identical image-based case, yielding: to the DirectionalCLIPtext-dual = 1 (cid:88) t=1 Vt dtext. Higher values indicate stronger alignment between framewise visual changes and the intended edit. CLIP-Text Alignment Score. This metric evaluates the semantic consistency of the edited video with the target description. Given normalized text embedding = Etext() and per-frame image embeddings Eimg(I edit ): CLIPTextAlign = 1 (cid:88) t=1 Eimg(I edit ) Figure 11. Expression editing robustness. Comparison between LivePortrait [20] and Sync-LoRA (Ours). Figure 12. Limitations of Sync-LoRA. The figure illustrates two primary failure modes. (Left) Spatial and synchronization degradation during non-aligned geometric edits (zoom-out), resulting in blurred facial features and temporal drift. (Right) Loss of detail and warping artifacts on fast-moving, complex regions (hands) during complex appearance modifications. oncile two conflicting spatial signals. This often compromises temporal alignment, leading to noticeable synchronization issues that propagate through the sequence. Rapid Motion Degradation. In sequences involving very fast or large-scale motion (such as rapid hand movements, dancing, or camera pans), the optical flow guidance from the source video can become ambiguous, resulting in blurred textures or loss of temporal coherence. We anticipate that this limitation could be mitigated by future base models with enhanced temporal reasoning capabilities. Figure 10. User Study Results. Pairwise preference rates for Sync-LoRA (Ours) against four baselines. Our method is strongly preferred across all four criteria: Edit Fidelity, Synchronization, Identity Preservation, and Overall Preference. Robustness to Occlusion and Rotation. LivePortrait employs warping fields to transfer expressions from driving video. As shown in Figure 11, this approach often produces artifacts when the face is partially occluded by objects or when background elements are in close proximity. For instance, in the left example, the warping field erroneously deforms the background persons face and blurs the hand holding the pistol. Similarly, in the right example, the rigid structure of the microphone is unnaturally bent near the subjects jawline. In contrast, Sync-LoRA treats expression editing as an in-context generation task. By leveraging the strong generative prior of the diffusion model, our method synthesizes geometrically plausible pixels even in occluded regions, maintaining structural integrity and photorealism while accurately transferring the target expression. 12. Limitations and Failure Cases As discussed in Section 5 of the main paper, our method exhibits specific failure modes. We categorize these into two primary types, illustrated in Figure 12. Geometric Misalignment. Our method conditions the generation on both the edited first frame and the source video context. If the edited frame structurally contradicts the spatial information inherent in the source video (e.g., zoom-out as shown in Figure 12), the model attempts to recFigure 13. Data Curation Filtering. Examples from our automatic filtering pipeline. (Top) Accepted clips exhibit high synchronization (Bottom) Rejected clips receive low synchronization scores due to scores, indicating precise temporal alignment and stable motion. temporal or spatial misalignments and motion drift."
        }
    ],
    "affiliations": [
        "Simon Fraser University",
        "Tel Aviv University"
    ]
}