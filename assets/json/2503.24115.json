{
    "paper_title": "TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud Detection",
    "authors": [
        "Zhiming Ma",
        "Peidong Wang",
        "Minhua Huang",
        "Jingpeng Wang",
        "Kai Wu",
        "Xiangzhao Lv",
        "Yachun Pang",
        "Yin Yang",
        "Wenjie Tang",
        "Yuchen Kang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The detection of telecom fraud faces significant challenges due to the lack of high-quality multimodal training data that integrates audio signals with reasoning-oriented textual analysis. To address this gap, we present TeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset specifically designed for automated telecom fraud analysis. Our dataset is constructed through three strategies: (1) Privacy-preserved text-truth sample generation using automatically speech recognition (ASR)-transcribed call recordings (with anonymized original audio), ensuring real-world consistency through text-to-speech (TTS) model regeneration; (2) Semantic enhancement via large language model (LLM)-based self-instruction sampling on authentic ASR outputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that simulates emerging fraud tactics through predefined communication scenarios and fraud typologies. The generated dataset contains 28,511 rigorously processed speech-text pairs, complete with detailed annotations for fraud reasoning. The dataset is divided into three tasks: scenario classification, fraud detection, fraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a standardized evaluation benchmark comprising proportionally sampled instances from the dataset, to facilitate systematic testing of model performance on telecom fraud detection tasks. We also contribute a production-optimized supervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while open-sourcing the data processing framework to enable community-driven dataset expansion. This work establishes a foundational framework for multimodal anti-fraud research while addressing critical challenges in data privacy and scenario diversity. The project will be released at https://github.com/JimmyMa99/TeleAntiFraud."
        },
        {
            "title": "Start",
            "content": "TeleAntiFraud-28k: Audio-Text Slow-Thinking Dataset for Telecom Fraud Detection Minhua Huang huangminhua@cmic.chinamobile.com China Mobile Internet Company Ltd. Guangzhou, Guangdong, China Jingpeng Wang wangjinpeng@cmic.chinamobile.com China Mobile Internet Company Ltd. Guangzhou, Guangdong, China Zhiming Ma Peidong Wang mazhiming312@outlook.com pdongwang@163.com China Mobile Internet Company Ltd. Guangzhou, Guangdong, China 5 2 0 2 1 ] . [ 1 5 1 1 4 2 . 3 0 5 2 : r Kai Wu wukai6@cmic.chinamobile.com China Mobile Internet Company Ltd. Guangzhou, Guangdong, China Xiangzhao Lv lvxiangzhao@cmic.chinamobile.com China Mobile Internet Company Ltd. Guangzhou, Guangdong, China Yachun Pang pangyachun@cmic.chinamobile.com China Mobile Internet Company Ltd. Guangzhou, Guangdong, China Yin Yang yangyin@cmic.chinamobile.com China Mobile Internet Company Ltd. Guangzhou, Guangdong, China Wenjie Tang tangwenjie@cmic.chinamobile.com China Mobile Internet Company Ltd. Guangzhou, Guangdong, China Yuchen Kang kangyuchen@cmic.chinamobile.com China Mobile Internet Company Ltd. Guangzhou, Guangdong, China Abstract The detection of telecom fraud faces significant challenges due to the lack of high-quality multimodal training data that integrates audio signals with reasoning-oriented textual analysis. To address this gap, we present TeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset specifically designed for automated telecom fraud analysis. Our dataset is constructed through three strategies: (1) Privacy-preserved text-truth sample generation using automatically speech recognition (ASR)-transcribed call recordings (with anonymized original audio), ensuring real-world consistency through text-to-speech (TTS) model regeneration; (2) Semantic enhancement via large language model (LLM)-based self-instruction sampling on authentic ASR outputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that simulates emerging fraud tactics through predefined communication scenarios and fraud typologies. The generated dataset contains 28,511 rigorously processed speech-text pairs, complete with detailed annotations for fraud reasoning. The dataset is divided into three tasks: scenario classification, fraud detection, fraud type classification. Furthermore, we construct TeleAntiFraud-Bench, standardized evaluation benchmark comprising proportionally sampled instances from the dataset, to facilitate systematic testing of model performance on telecom fraud detection tasks. We also contribute production-optimized supervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while open-sourcing the data processing framework to enable community-driven dataset expansion. This work establishes foundational framework for multimodal anti-fraud research while addressing critical challenges in data privacy and scenario diversity. The project will be released at https://github.com/JimmyMa99/TeleAntiFraud. Both authors contributed equally to this research. Keywords Telecom Fraud Detection, Large Language Models, Large Audio Language Models, Audio-Text Processing, Anti-Fraud Systems"
        },
        {
            "title": "1 Introduction\nAs telecommunications fraud techniques become increasingly com-\nplex and sophisticated, the threat to social security and economic\nstability continues to escalate. Global economic losses due to fraud\nhave reached $1.02 trillion, representing 1.05% of the global GDP, a\nsignificant increase from 2020 and 2021 figures, with over a quarter\nof respondents reporting encounters with fraud[?]. Therefore, devel-\noping effective fraud detection methods has become an urgent prior-\nity. Traditional fraud detection methods rely on manual verification\nand rule-based pattern matching, which often have low accuracy\nand struggle to adapt to the rapidly evolving strategies in fraudulent\ncommunications. Recent advancements in large language models\n(LLMs), particularly the emergence of slow-thinking reasoning ca-\npabilities, offer new solutions for combating telecommunications\nfraud. However, the primary source data for telecommunications\nfraud is voice calls, which exhibit a significant modality gap with\ntext data, limiting the direct application of LLMs in this domain.\nCurrent industry methods typically employ automatic speech recog-\nnition (ASR) to convert audio data into text, followed by carefully\ndesigned prompt engineering to invoke LLMs for fraud judgment.\nWhile this approach has achieved some success, it heavily relies on\nmeticulously crafted prompt engineering, and performance vari-\nations across different models and deployment environments can\nsignificantly impact judgment accuracy. Moreover, information loss\nduring the ASR conversion process may lead to the omission of key\nfraud indicators, as vocal features such as tone and pauses often\ncontain important clues about fraudulent intent. Slow-thinking\nreasoning enhances the accuracy and interpretability of large lan-\nguage models in judgment-oriented tasks. Meanwhile, the rapid\ndevelopment of large audio language models (LALMs) capable of",
            "content": "Trovato et al. Figure 1: An overview of TeleAntiFraud-30k. Our system addresses the challenges of telecom fraud detection. We create the TeleAntiFraud-28k dataset through three strategies: Real-Data ASR Processing, LLM-Based Imitation adn Augmention, and multi-agent adversarial synthesis. We also construct TeleAntiFraud-Bench for model evaluation and contribute supervised fine-tuning model with an open-sourced data processing framework. directly processing audio signals presents an opportunity to bridge the modality gap in anti-fraud applications. However, the lack of slow-thinking audio datasets specifically designed for telecommunications fraud scenarios severely limits the application and performance improvement of LALMs in the anti-fraud domain. To address this gap, this study proposes for the TeleAntiFraud28k, audio-text slow-thinking dataset for telecommunication fraud detection. The dataset is constructed through three methodologies: 1) Transcribe anonymized call recordings using Automatic Speech Recognition (ASR) technology to generate privacy-protected text samples. Subsequently, use Text-to-Speech (TTS) model to regenerate the samples, ensuring they align with real-world language expressions to enhance their applicability in real-world scenarios. 2) Implementing self-instructed sampling strategies with large language models to semantically enhance and enrich generated content, and further expanding audio data through textto-speech (TTS) augmentation techniques. 3) Designing multiagent adversarial framework that simulates emerging fraud tactics through predefined communication scenarios and fraud typologies. TeleAntiFraud-28k not only contains rich audio data but also provides detailed slow-thinking annotations covering three core tasks: communication scenario classification, fraud determination, and fraud type identification. Additionally, we systematically extract representative samples from TeleAntiFraud-28k to construct TeleAntiFraud-Bench, an evaluation benchmark that preserves the original datasets scenario distribution and fraud type proportions, ensuring representative and reliable assessment outcomes. This benchmark provides researchers with unified evaluation platform for comparative analysis of different models performance in telecommunication scenario classification, fraud detection, fraud type classification. Experiments conducted on multiple state-of-the-art Large Audio Language Models (LALMs) indicate that current, un-fine-tuned LALMs are insufficient for telecommunications anti-fraud tasks. After fine-tuning Qwen2Audio using the Telecom Anti-Fraud 28k training set, we observed significant improvement in performance on the Telecom Anti-Fraud Benchmark test set (TeleAntiFraudBench). This demonstrates the effectiveness and practical value of this dataset in developing audio-based anti-fraud models. The main contributions of this research can be summarized as follows: (1) Proposing the first multi-task slow-thinking audio-language dataset (TeleAntiFraud-28k) for telecommunication fraud prevention, encompassing three tasks: communication scenario classification, fraud determination, and fraud type analysis; (2) Designing an new data generation pipeline that maximizes coverage of diverse fraud scenarios through real-call ASR processing, LLM-based simulation, and multi-agent adversarial generation; (3) Establish the TeleAntiFraud-Bench evaluation benchmark to provide standardized testing standards for telecom fraud detection models, while designing series of evaluations for anti-telecom fraud slow-thinking capabilities; TeleAntiFraud-28k: Audio-Text Slow-Thinking Dataset for Telecom Fraud Detection (4) Conducting comprehensive evaluations on multiple leadingedge LALM models using TeleAntiFraud-Bench, validating the datasets training effectiveness and establishing performance baselines for future audio-based anti-fraud research. This work bridges critical research gaps in multimodal fraud detection and provides valuable resources for advancing intelligent anti-fraud systems. The dataset and benchmark are publicly available to facilitate community-wide research efforts in combating evolving telecommunication fraud."
        },
        {
            "title": "Systems",
            "content": "With the rapid advancement of artificial intelligence, large language models (LLMs) have been applied to telecom fraud detection. However, existing research predominantly focuses on text analysis alone, lacking comprehensive methodologies that integrate audio signals with textual analysis. Singh et al. [1] proposed real-time fraud detection mechanism based on Retrieval-Augmented Generation (RAG) technology. This system transcribes call content in real-time and verifies the legitimacy of conversations. It also implements real-time user identity authentication checks to ensure caller authenticity. Although the system achieved 97.98% accuracy in 100 synthetic call tests, it primarily relies on text analysis and lacks direct processing capabilities for audio features. Shen et al. [2] explored the application of LLMs in real-time telecom fraud detection, developing framework that assesses fraudulent intent in conversations and provides immediate warnings to users. Their study analyzed key factors influencing detection performance, particularly the trade-off between recall rate and timeliness. Nonetheless, this work remains text-centric and fails to fully leverage the rich information embedded in audio. In another study, Shen et al. [3] reviewed the potential and challenges of LLMs in telecom fraud detection. They highlighted that while LLMs demonstrate promising results in identifying fraud patterns, challenges such as data bias, relatively low recall rates, and hallucinations persist. This underscores the critical need for constructing high-quality, diverse training datasets. Additionally, Changs research [4] revealed vulnerabilities of LLMs against adversarial scam messages. By creating comprehensive dataset containing both original and adversarial scam messages, the study analyzed LLM misclassification rates and proposed strategies to enhance robustness. This work emphasizes the necessity of considering adversarial scenarios in anti-fraud system development, though its analysis remains confined to textual domains. Existing anti-fraud systems, while performing well under specific conditions, predominantly rely on predefined rules or text-only analysis, lacking the ability to integrate audio and textual information for joint reasoning. Furthermore, they are typically trained and evaluated on limited-scale datasets, making them inadequate for rapidly evolving fraud tactics. These limitations highlight the urgent need to develop large-scale, multimodal telecom fraud dataset to advance research in this field."
        },
        {
            "title": "2.2 Multimodal audio-Text Models\nAdvances in deep learning have enabled significant progress in mul-\ntimodal models for processing audio and textual information, yet\nthese innovations remain underutilized in telecom fraud detection.\nRecent large multimodal models such as GLM-4-Voice [5] demon-\nstrate the potential of end-to-end voice dialogue systems. This\nmodel supports real-time bilingual (Chinese-English) voice inter-\naction and can adjust audio characteristics such as emotion, tone,\nspeed, and dialect based on user instructions. Similarly, Qwen2-\nAudio [6], a large-scale audio-language model, accepts diverse audio\ninputs and performs audio analysis or direct text responses. Among\nproprietary models, GPT-4o [7], an autoregressive omni-model, ac-\ncepts arbitrary combinations of text, audio, image, and video inputs\nto generate text, audio, and image outputs.",
            "content": "Existing multimodal models, though capable of processing audio and text, are primarily designed for general conversational purposes or content comprehension, lacking domain-specific optimization for telecom fraud detection. Moreover, insufficient consideration of privacy-sensitive data handling in these models hinders their direct deployment in real-world fraud detection scenarios. Multimodal anti-fraud research also faces challenges including data acquisition difficulties, stringent privacy requirements, and diverse fraud scenarios. In summary, while LLM-based fraud detection systems and multimodal audio-text models have made individual progress, their integration for combating telecom fraud remains insufficient. Our TeleAntiFraud-28k dataset addresses these gaps by providing largescale, slow-thinking-annotated audio-text pairs, establishing infrastructure for multimodal anti-fraud research while addressing critical challenges such as data privacy and scenario diversity."
        },
        {
            "title": "3 Method\nThis chapter elaborates on the construction method of the TeleAntiFraud-\n28k dataset and the design of the evaluation framework, TeleAntiFraud-\nBench. As depicted in figure 1, our approach encompasses three\nmain components: voice data generation, text data annotation based\non slow-thinking, and the establishment of an evaluation bench-\nmark.",
            "content": "Firstly, the voice data generation process integrates three innovative methods to produce high - quality and diverse telephone - call voice data, covering various scenarios of both telecom fraud and normal calls. Subsequently, in the stage of text data annotation based on slow-thinking, the generated voice data is input into an audio-understanding model for processing. Then, the results of audio analysis and ASR text are provided as prompts to large language model (DeepSeek - R1), guiding it to act as \"voice model with thinking\" for in-depth analysis and generating annotated texts that include explicit thinking processes and conclusions. Finally, the evaluation benchmark stage constructs unified test platform for assessing the performance of different models in telecom anti-fraud tasks."
        },
        {
            "title": "3.1 Audio Data Generation Pipeline\nTo construct a high-quality, diverse TeleAntiFraud-28k dataset, we\nfirst need to create rich voice data. We employed three complemen-\ntary methods for generating conversational content, which were",
            "content": "Trovato et al. shown that the self-instruct method can generate high-quality, diverse training data that is as reliable as manually annotated data across various tasks. Based on this theoretical foundation, we designed specific prompt templates, extracting typical samples from 𝑆1 and 𝐷𝐹 𝐷𝑁 𝑆1 as few-shot examples to guide the LLM in generating large number of dialogue scripts that follow similar patterns but vary in content. Specifically, we extracted key patterns and linguistic features from real calls, including the structure of fraudulent tactics, common expressions, and interaction patterns. We then incorporated these features along with scenario descriptions into the prompts to guide the LLM in generating dialogue content that fits specific scenarios. For fraud-related data, we particularly focused on preserving the typical characteristics of fraudulent tactics while varying the details to increase diversity. Through this approach, we generated large amount of high-quality normal call text 𝐷𝑁 𝑆2 and fraudulent call text 𝐷𝐹 𝑆2, significantly expanding the dataset. The advantage of the self-instruct method lies in its ability to maintain the original data features while substantially increasing data volume and diversity, with the generation process being efficient and controllable. Our experiments demonstrate that this method effectively generates high-quality data. 3.1.3 Multi-Agent Adversarial Framework. The third method of obtaining conversational text is our proposed multi-agent adversarial framework, aimed at simulating various emerging fraud tactics and expanding data distribution. Our preliminary analysis (see Appendix A) indicates that while existing telecommunications fraud call recordings are authentic and reliable, their conversational patterns and scenario distributions are relatively concentrated. This makes it difficult to cover diverse fraud scenarios and new fraud techniques. To address this issue, we designed multi-agent adversarial framework that effectively expands the data distribution space by simulating fraudulent behaviors in various business scenarios. As illustrated in the figure 3, the framework comprises three collaborative agents: two agents act as the caller (fraudster) and the callee (potential victim), respectively, while the third agent assumes the role of the Manager. In this framework, we first provide the callers large language model with information about the type of fraud to make the scammer act more purposefully and realistically. Additionally, we set specific identity profiles and reaction patterns for the callee agent to ensure more realistic interaction. The conversation proceeds in turn-based manner, with the caller and callee taking turns to speak, while the Manager monitors the conversation to ensure it adheres to the preset scenario and follows natural flow. To ensure the diversity and expanded distribution of the generated data, we designed various scenario-fraud type combinations based on seven common call scenarios and eight fraud types found in real-world business settings. These combinations guide the multiagent system to generate conversations that cover these scenarios. This method effectively expands the range of the original data distribution and fills gaps in the existing data distribution. To achieve natural conversation termination, we designed special end signal mechanism. Both the caller and callee can indicate their intention to hang up by sending the special symbol Figure 2: Data Flow of Audio Data Generation subsequently converted into realistic call recordings using text-tospeech technology. The overall process is illustrated in Figure 2. Our voice data generation process consists of two stages: dialogue text generation and voice synthesis. In the dialogue text generation stage, we used three methods to obtain diverse call scripts: real data ASR processing, large language model imitation generation, and multi-agent adversarial framework. These three methods, each with its own focus, collectively ensured the authenticity, diversity, and coverage of the dataset. Subsequently, we converted the generated dialogue texts into dual-channel voice data using TTS technology, corresponding to the caller and the callee, thereby completing the voice synthesis stage. 3.1.1 Real-Data ASR Processing. The first method for obtaining dialogue texts is based on real telephone call data and their automatic speech recognition (ASR) results. We initially collected real telephone recordings containing various types of telecom fraud, denoted as 𝐷𝐹 𝑇 , and real normal data, denoted as 𝐷𝑁 𝑇 . Using ASR technology, we converted the audio into left and right channel texts. These ASR texts preserve the key features and speech patterns of the original fraudulent conversations while anonymizing the data. We then used TTS models to generate realistic normal data, 𝐷𝑁 𝑆1, and realistic fraudulent data, 𝐷𝐹 𝑆1. Through this method, we ensured that the generated voice data closely matched real fraud scenarios in content while avoiding potential privacy issues associated with using the original recordings directly. 3.1.2 LLM-Based Imitation and Augmentation. The second method for obtaining conversational text employs the self-instruct paradigm, utilizing large language model (LLM) to mimic and expand upon the ASR results of real calls. Previous research has TeleAntiFraud-28k: Audio-Text Slow-Thinking Dataset for Telecom Fraud Detection 2. Natural Speech Feature Preservation: During the TTS process, we fully utilized ChatTTSs fine-grained prosody control capabilities to retain natural speech features such as pauses, stress, and emotional variations, making the synthesized voices more akin to real conversations. Particularly in fraud scenarios, we enhanced specific emotional expressions commonly found in fraudulent tactics, such as urgency, authority, or false affinity. 3. Precise Temporal Control: We precisely controlled the timing relationship between the callers and callees voices to ensure natural turn-taking, including appropriate response delays, overlapping speech, and interruptions, which are common in real conversations. This further enhanced the realism of the audio data. 𝐷𝐹 𝐷𝑁 𝐷𝐹 𝑆2 By employing three text generation methods and voice synthesis strategy based on ChatTTS, we constructed comprehensive 𝐷𝑁 audio dataset comprising normal calls 𝐷𝑁 = 𝐷𝑁 𝑆3 and 𝑆1 𝑆2 fraudulent calls 𝐷𝐹 = 𝐷𝐹 𝑆3. It is important to note 𝑆1 that 𝐷𝑁 𝑆1 and 𝐷𝐹 𝑆1 contain data derived from real phone recordings, which involve potential privacy concerns. To address these issues, we thoroughly anonymized these data segments. In the publicly released dataset, the fully anonymized 𝐷𝑁 𝑆1 are included only in the test set, alongside data generated through LLM imitation and the multi-agent adversarial framework, 𝐷𝑁 𝐷𝐹 𝑆3 and 𝐷𝐹 𝑆3, 𝑆2 𝑆2 to ensure compliance and usability. 𝑆1 and 𝐷𝐹 𝐷𝑁"
        },
        {
            "title": "3.2 Slow Thinking-Based Text Annotation\nThe TeleAntiFraud-28k dataset’s core characteristic is the integra-\ntion of a \"Slow Thinking\" mechanism that simulates anti-fraud\nexperts’ analysis process and professional judgment of voice calls.\nTo generate high-quality text annotations, we designed a two-stage\nprocessing workflow:",
            "content": "1. Audio Analysis Phase: The generated voice data is input into professional audio understanding model, extracting voice features and key information, such as emotional changes, tone characteristics, pause patterns, and other audio-level anti-fraud clues. 2. Expert Reasoning Phase: The audio analysis results are combined with ASR text as prompts, providing the DeepSeekR1 model guidance to act as \"voice analysis expert with antifraud professional knowledge\" to conduct systematic reasoning and analysis. The model is required to use the specific symbol \"<think></think>\" to mark its professional thinking process, showcasing the complete reasoning chain from clue identification to professional judgment, using \"<answer></answer>\" to mark the output result. This design captures the anti-fraud experts systematic and professional analysis framework, including speech pattern recognition, fraud technique identification, risk assessment, and type determination. The model uses specific symbols \"<think></think>\" to mark its professional thinking process, including key clue extraction from the conversation content, speech pattern analysis, and risk assessment steps, ultimately providing clear fraud determination. Our designed prompt template first provides conversation content and audio feature analysis, then requires the model to output three key pieces of information in JSON format: reasoning grounds (reason), confidence level (confidence), and boolean fraud determination Figure 3: Architecture Diagram of the Multi-Agent Adversarial Framework ##ENDCALL_SIGNAL##. Additionally, the Manager can decide to terminate the conversation based on its progress, preventing ineffective loops or excessive deviation from the topic. 𝑆3 and fraudulent call data 𝐷𝐹 Through the multi-agent adversarial framework, we generated new set of normal call data 𝐷𝑁 𝑆3. As shown in Figure 3.3 (see Appendix), we visualized the semantic embeddings of all generated data. The results demonstrate that the data generated by the multi-agent framework significantly expanded the distribution space of the original data, particularly in new fraud types and complex scenarios, providing more comprehensive training data for the model. 3.1.4 Audio Synthesis. After obtaining diverse conversational texts, we employed high-quality text-to-speech (TTS) technology to convert them into dual-channel audio data, corresponding to the caller and callee, respectively. To achieve highly realistic voice synthesis, we utilized the advanced open-source TTS model ChatTTS, which is optimized for conversational scenarios. This model can produce natural and expressive synthetic speech, supports multispeaker scenarios, and allows for fine-grained control over prosodic features, surpassing most open-source TTS models in naturalness. Leveraging the powerful capabilities of ChatTTS, we implemented the following strategies during the voice synthesis process to further enhance the realism and diversity of the audio data: 1. Diverse audio Characteristics: We randomly generated different voice parameters, including timbre, speech rate, and pitch, to ensure that the synthesized voices exhibit diverse auditory characteristics. By utilizing ChatTTSs multi-speaker support, we configured distinct voice characteristics for different roles, making it easier to distinguish between the two parties in the conversation. (is_fraud). The model is first required to demonstrate comprehensive analysis process under the \"<think></think>\" marker, then provide the final judgment marked with \"<answer></answer>\". This structure enables the model to simulate an anti-fraud experts thinking process, comprehensively displaying the progression from evidence collection to professional judgment. The multi-round in-depth analysis task is divided into three consecutive steps: 1. Call Scene Classification: The model first needs to analyze the basic scene and theme of the call to lay the foundation for subsequent judgments. We have defined seven common call scenarios: \"Dining Service\", \"Customer Consultation\", \"Appointment Service\", \"Transportation Inquiry\", \"Routine Shopping\", \"Ride-Hailing Service\", and \"Food Delivery Service\". The model needs to determine the most appropriate scene category by analyzing dialogue content and voice characteristics. 2. Fraud Determination: Fraud Determination: Based on the scene classification results, the model further judges whether the call contains fraudulent behavior and explains the basis. This step requires the model to apply professional anti-fraud knowledge, analyzing speech characteristics, the reasonableness of requests, information disclosure patterns, and other professional indicators to provide clear fraud judgment and detailed evidence chain. 3. Fraud Type Identification: For calls identified as fraudulent, the model needs to further determine the specific fraud type. We have defined seven main fraud types: \"Investment Fraud\", \"Phishing Fraud\", \"Identity Theft\", \"Lottery Fraud\", \"Banking Fraud\", \"Extortion Fraud\", and \"Customer Service Fraud\". The model needs to determine the most matching fraud type based on professional classification standards and explain the classification basis. In the multi-round in-depth analysis task, each step requires the model to provide JSON-formatted output, including key judgment results and confidence levels. For call scene classification, the output includes scene type (scene), judgment reason (reason), and confidence (confidence); the fraud determination step requires reason (reason), confidence (confidence), and boolean value for fraud status (is_fraud); fraud type identification includes fraud type (fraud_type), judgment reason (reason), and confidence (confidence). In each step, the model must first demonstrate its professional thought process, then provide structured result. This progressive analysis design not only conforms to the workflow of professional anti-fraud personnel but also helps the model establish necessary contextual understanding before complex decisionmaking, thereby improving the accuracy and reliability of final judgments. By recording the thought process of each step, our dataset provides rich analysis samples for subsequent research."
        },
        {
            "title": "3.3 Construction of Evaluation Benchmark\nTo systematically evaluate the performance of different models in\ntelecom anti-fraud tasks, we established the TeleAntiFraud-Bench\nevaluation benchmark. This benchmark draws representative sam-\nples from the TeleAntiFraud-26k dataset proportionally, ensuring\nthe preservation of original scenario distributions and fraud type\ndistributions from the source dataset, thereby guaranteeing the\nrepresentativeness and reliability of evaluation results.",
            "content": "Trovato et al. 3.3.1 Evaluation Process Design. To systematically evaluate the performance of different models on telecommunications antifraud tasks, we constructed the TeleAntiFraud-Bench evaluation benchmark. This benchmark proportionally extracts representative samples from the TeleAntiFraud-28k dataset and incorporates the generated results of genuine normal data 𝐷𝑁 𝑆1 and genuine fraud data 𝐷𝐹 𝑆1 into the test set, ensuring the preservation of the original datasets scenario distribution and fraud type distribution, thereby guaranteeing the representativeness and reliability of the evaluation results. 3.3.2 Design of the Evaluation Process. TeleAntiFraud-Bench employs hybrid evaluation mechanism that combines rule-based extraction with LLM analysis to ensure the comprehensiveness and accuracy of the evaluation process. Specifically, we design structured prompts to guide the evaluation LLM in analyzing the output of the model under test, while simultaneously utilizing regular expressions to precisely extract key information. These two approaches complement each other to jointly accomplish the evaluation task. The evaluation process comprises the following steps: 1. Regular Expression-Based Key Information Extraction: Regular expressions are used to accurately extract key results from the models output, such as scenario classification, fraud judgment, and fraud type. This step is independent of LLM analysis and aims to provide clear and precise foundational data for subsequent evaluation, ensuring accuracy and consistency in the initial stages of the evaluation process. 2. LLM Analysis of the Thought Process: The thought process of the model under test (content marked as \"<think></think>\") is extracted and analyzed by the evaluation LLM for completeness, logical coherence, and effective use of evidence. This step leverages the LLMs robust language understanding and analytical capabilities to evaluate the models reasoning pathway. 3. LLM Verification of Final Judgment Consistency with Standard Answers: The LLM verifies whether the models final judgment aligns with the standard answer, capitalizing on the LLMs strengths in logical judgment and comparison to ensure the reliability of the evaluation results. 4. Comprehensive Evaluation Score and Report Generation: By integrating the key information extracted via regular expressions and the LLMs analysis of the thought process and final judgment, comprehensive evaluation score is generated. Additionally, detailed analytical report is produced based on these multidimensional evaluation data, providing thorough presentation of the models performance across various aspects. Through this hybrid evaluation mechanism, we enhance the reliability of the evaluation process and fully exploit the strengths of both regular expressions and LLMs, ensuring that the evaluation accurately and comprehensively reflects the performance of the model under test. Inference Evaluation Process. To comprehensively assess 3.3.3 the quality of models reasoning process, we have designed an evaluation framework based on three dimensions: logical consistency, practicality, and clarity. This framework employs structured prompting mechanism [langGPT] to guide large language model TeleAntiFraud-28k: Audio-Text Slow-Thinking Dataset for Telecom Fraud Detection (LLM) in performing the scoring task. Specifically, the input data includes the models reasoning process 𝑅𝑚, the models final answer 𝐴𝑚, the reference answer 𝐴𝑟 , and the reference reasoning process 𝑅𝑟 . The scoring process is automated through probabilistic mechanism that combines expert rules and logical constraints. In the scoring process, logical consistency evaluates the completeness of the reasoning chain, the reasonableness of assumptions, and the rigor of conclusion derivation. The score is influenced by the correctness of the conclusion and the presence of logical leaps. When 𝐴𝑚 𝐴𝑟 , the probability of relevant scoring points significantly decreases, and each logical leap or missing evidence results in reduction in the corresponding dimensions probability value. Practicality focuses on the models depth of understanding of the problems essence, the effectiveness of the solution, and the completeness of demand coverage. If 𝐴𝑚 𝐴𝑟 , the probabilities related to problem identification and solution effectiveness are set to zero. Clarity emphasizes the presentation of key points in reasoning, conciseness of language, and efficiency in information organization. Unclear or redundant expressions trigger probability decay in the corresponding dimension. Each dimensions score is allocated using probabilistic distribution, quantifying the likelihood of different scores. For example, for scoring point that can receive score 𝑆 {0, 1, 2}, we assign probability 𝑃 (𝑆) and calculate the expected value using 𝐸 = (cid:205)𝑆 𝑆 𝑃 (𝑆). Finally, the expected scores for logical consistency 𝐸𝐿, practicality 𝐸𝑈 , and clarity 𝐸𝐶 are summed to obtain the total score: 𝐸total = 𝐸𝐿 + 𝐸𝑈 + 𝐸𝐶 This evaluation system combines quantitative metrics with probabilistic calculations to achieve an objective and comprehensive assessment of the models reasoning quality. 3.3.4 Evaluation Metric System. TeleAntiFraud-Bench adopts balanced scoring mechanism, dividing overall performance into four equally important dimensions, each accounting for 25 We use the weighted F1 score as the quantitative metric, with the general formula: 𝑊 𝑒𝑖𝑔ℎ𝑡𝑒𝑑 𝐹 1𝑡𝑎𝑠𝑘 = 𝑛 𝑖=1 𝑤𝑡𝑎𝑠𝑘𝑖 2 𝑃𝑡𝑎𝑠𝑘𝑖 𝑅𝑡𝑎𝑠𝑘𝑖 𝑃𝑡𝑎𝑠𝑘𝑖 + 𝑅𝑡𝑎𝑠𝑘𝑖 Where 𝑤𝑡𝑎𝑠𝑘𝑖 represents the weight of the 𝑖-th class in the task, which is the ratio of the number of samples in class 𝑖, 𝑛𝑖 , to the total number of samples, (cid:205)𝑛 . 𝑃𝑡𝑎𝑠𝑘𝑖 represents the precision of the 𝑖-th class in the task. 𝑅𝑡𝑎𝑠𝑘𝑖 represents the recall of the 𝑖-th class in the task. 𝑛 represents the total number of classes. 𝑛 𝑗 , given by 𝑤𝑡𝑎𝑠𝑘𝑖 = 𝑛𝑖 𝑗 =1 𝑗=1 (cid:205)𝑛 𝑛 𝑗 1. Scene Classification F1 Score: The models performance in correctly identifying seven types of call scenarios, calculated using the above formula, where 𝑃𝑠𝑐𝑒𝑛𝑒𝑖 and 𝑅𝑠𝑐𝑒𝑛𝑒𝑖 are the precision and recall for scene classification, respectively. 2. Fraud Judgment F1 Score: The models performance in correctly determining whether call is fraudulent, calculated using the above formula, where 𝑃𝑓 𝑟𝑎𝑢𝑑𝑖 and 𝑅𝑓 𝑟𝑎𝑢𝑑𝑖 are the precision and recall for fraud judgment, respectively. 3. Fraud Type Identification F1 Score: The models performance in correctly identifying seven types of fraud, calculated using the above formula, where 𝑃𝑡 𝑦𝑝𝑒 and 𝑅𝑡 𝑦𝑝𝑒 are the precision and recall for fraud type identification, respectively. 4. Quality of Thought Process: Evaluated by large language model for completeness, use of evidence, application of professional knowledge, and logical coherence. The comprehensive scoring method is: 𝑆𝑐𝑜𝑟𝑒𝑡𝑜𝑡𝑎𝑙 = 0.25 𝐹 1𝑠𝑐𝑒𝑛𝑒 + 0.25 𝐹 1𝑓 𝑟𝑎𝑢𝑑 + 0.25 𝐹 1𝑡 𝑦𝑝𝑒 + 0.25 𝑆𝑐𝑜𝑟𝑒𝑝𝑟𝑜𝑐𝑒𝑠𝑠"
        },
        {
            "title": "4 Evaluation\n4.1 Experimental Setup\n4.1.1 Dataset Statistics. The constructed TeleAntiFraud-28k dataset\ncomprises 28,511 utterance samples, which are divided into training\nand test sets. The training set contains 21,490 samples, constituting\n75.38% of the total dataset, while the test set contains 7,021 samples,\nconstituting 24.62%. This 3:1 partition ratio ensures sufficient sam-\nples for benchmark evaluation while adhering to the conventional\ntraining-test split protocol widely adopted in machine learning\nmethodology. Table 1 presents the basic statistical information of\nthe dataset",
            "content": "Type Total 21,490 Train 7,021 Test 28,511 Total Scam Calls 9,950 (46.3%) 3,697 (52.66%) 13,647 (47.86%) Table 1: Distribution of Scam and Normal Calls in the Dataset Normal Calls 11,540 (53.7%) 3,324 (47.34%) 14,864 (52.13%) In terms of data type distribution, both the training and test sets maintain relative balance between fraudulent and normal calls. In the training set, fraudulent calls constitute 46.3% (9,950 samples), while normal calls make up 53.7% (11,540 samples). In the test set, fraudulent calls account for 52.66% (3,697 samples), and normal calls account for 47.34% (3,324 samples). Overall, in the entire dataset, fraudulent calls represent 47.86% (13,647 samples), and normal calls represent 52.13% (14,864 samples). In constructing the dataset, we carefully considered the diversity and representativeness of conversational scenarios. Through meticulous selection and annotation, the training set encompasses rich variety of interaction types. As shown in Table 2 with customer service inquiries being predominant, comprising 6,421 samples. This adequately illustrates the typical needs of users seeking professional guidance and problem resolution. Concurrently, appointment service scenarios were also given significant attention, with 1,714 samples included, reflecting critical interaction points in the service process. The design of the test set continues the principle of diversity established in the training set, ensuring that the model maintains robust generalization performance across different scenarios, thereby laying solid foundation for subsequent practical applications. In terms of fraud types, the dataset encompasses seven major categories: phishing fraud, kidnapping fraud, lottery fraud, customer service fraud, banking fraud, investment fraud, and identity theft. Notably, there are significant variations in the number of samples for different fraud types. As shown in 3, customer service fraud has Table 2: Distribution of scenario types in training and test sets Scenario Type Training Set Test Set Customer Service Inquiries Appointment Services Daily Shopping Food Ordering Services Delivery Services Ride-hailing Services Transportation Inquiries Total 6,421 1,714 924 581 494 353 10,710 4,632 867 340 154 448 489 91 7,021 Table 3: Distribution of fraud types in training and test sets Fraud Type Training Set Test Set Customer Service Fraud Banking Fraud Investment Fraud Phishing Fraud Lottery Fraud Kidnapping Fraud Identity Theft Total 2,022 1,626 785 443 418 324 105 5,723 725 2,408 216 123 99 91 35 3, 2,022 samples in the training set and 725 in the test set, whereas identity theft has relatively fewer samples (105 in the training set and 35 in the test set). This distribution disparity may affect the models detection performance for different fraud types but aligns with the proportions observed in real-world scenarios. 4.1.2 Model Selection for Evaluation. In the field of telecommunications fraud detection, the combination of Automatic Speech Recognition (ASR) and large language models has emerged as the predominant technological approach. This study selected 10 representative models, including ASR combined with large language models and multimodal models, to comprehensively evaluate the performance of the TeleAntiFraud-28k dataset. To control experimental variables, SenseVoice was chosen as the uniform ASR technology, standardizing the speech recognition process. For the ASR combined with large language models, high-performance opensource models such as DeepSeek-V3, DeepSeek-R1, Doubao-1.5-Pro, InternLM2.5-20B-Chat, GLM-4-9B-Chat, and Qwen2.5-72B-Instruct were selected, all of which excel in contextual understanding and multi-task learning. In the realm of multimodal models, commercial models such as GPT-4o and Gemini-2.0-Flash, as well as opensource models like GLM-4-Voice, Step-1o-audio, and Qwen2Audio, were chosen. These models span various scales (9B-130B) and types, ensuring the comprehensiveness and representativeness of the study."
        },
        {
            "title": "4.2 Main Experimental Results\n4.2.1 Scenario Classification Performance. In the scenario\nclassification task, DeepSeek-V3 achieved the highest F1 score of",
            "content": "Trovato et al. 88.53%, demonstrating the advantages of the ASR+LLM approach in this task. DeepSeek-R1 followed closely with an F1 score of approximately 83%. The fine-tuned AntiFraud-Qwen2Audio model achieved an F1 score of 81.31%, exhibiting relatively stable performance, indicating that language models optimized for anti-fraud scenarios can effectively enhance task performance. Multimodal models performed reliably in this task, with GPT-4o (F1 score of 80.25%) and Gemini-2.0-Flash (F1 score of 80.51%) providing more comprehensive information retrieval compared to ASR+LLM solutions, although they still fell short of the top ASR+LLM models. In contrast, the performance of InternLM2.5-20B-Chat (F1 score of 78.34%) and GLM-4-9B-Chat (F1 score of 75.1%) further highlighted the limitations of pure language models in scenario classification. 4.2.2 Fraud Detection Performance. In fraud detection tasks, the performance differences among models are more pronounced. DeepSeek-R1 excels with an F1 score of 79.25%, showcasing the strong recognition capabilities of the ASR+slow-thinking LLM approach. The fine-tuned AntiFraud-Qwen2Audio model performs best in this task, achieving an F1 score of 84.78%, demonstrating that specially optimized speech-language models can effectively enhance the reliability of fraud detection. In contrast, the performance of multimodal models in this task is relatively varied. GPT-4o (F1 score of 50%) and Step-1o-audio (F1 score of 40.65%) underperform compared to the ASR+LLM approach, indicating challenges in integrating audio and language information for fraud detection tasks. However, Gemini-2.0-Flash (F1 score of 59.61%) shows balanced performance, highlighting its potential in integrating information. Additionally, InternLM2.5-20B-Chats F1 score of 36.67% reveals significant shortcomings in practical fraud detection. GLM-4-Voices F1 score of 26.83% may be related to limitations in its modality fusion strategy or training data. 4.2.3 Fraud Type Classification Performance. In the fraud type classification task, the models performances were relatively close. DeepSeek-R1 led with an F1 score of 85.16%, closely followed by Doubao-1.5-Pro and InternLM2.5-20B-Chat with F1 scores of 82.25% and 85.42%, respectively. The fine-tuned AntiFraud-Qwen2Audio model achieved an impressive F1 score of 82.91%, showcasing stable performance and proving that language models optimized for specific tasks can maintain strong classification capabilities. In contrast, the same model without fine-tuning achieved an F1 score of only 58.51%, highlighting the effectiveness of fine-tuning for anti-fraud scenarios. This significant improvement underscores the importance of task-specific optimization in enhancing model performance and reliability in specialized applications. The multimodal model GPT-4o performed exceptionally well with an F1 score of 86.26%, indicating that the integration of multimodal information can provide advantages in fraud type classification tasks. Additionally, Gemini-2.0-Flash (F1 score of 74.55%) also exhibited balanced performance. 4.2.4 Quality Assessment of the Thinking Process. To comprehensively evaluate the thinking quality of various models, we assessed their performance from three dimensions: logical rigor, practical value, and expressive quality, using DeepSeek-R1 for systematic evaluation (shown in Table 6). It is well-known that models reasoning ability is closely related to the capability of its backbone TeleAntiFraud-28k: Audio-Text Slow-Thinking Dataset for Telecom Fraud Detection Table 4: Comparison of Model Performance on Fraud Detection Tasks Type LLM Scenario (%) Fraud(%) Fraud Type(%) Avg F1 (%) Reason Final SenseVoice (ASR) + LLM Multimodal Models DeepSeek-V3 DeepSeek-R1 Doubao-1.5-Pro InternLM2.5-20B-Chat GLM-4-9B-Chat Qwen2.5-72B-Instruct GPT-4o* Gemini-2.0-Flash GLM-4-Voice* Step-1o-audio* Qwen2Audio FT Anti-Fraud LALM AntiFraud-Qwen2Audio 88.53 83.60 71.14 78.34 75.10 78.31 80.25 80.51 - 76.35 70.22 81. 14.62 79.25 36.11 36.67 46.91 51.44 50.00 59.61 26.83 40.65 58.51 84.78 66.71 85.16 82.25 85.42 82.22 81.24 86.26 83.53 38.33 79.71 20.47 82. 56.62 82.67 63.17 66.81 68.08 70.33 72.17 74.55 32.58 65.57 49.73 83.00 7.51 9.94 6.31 6.37 5.25 7.01 6.79 7.25 1.89 5.26 4.91 6. 21.30 31.08 23.74 25.11 25.57 26.43 27.12 28.02 12.23 24.63 18.69 31.18 language model. According to the scaling law theory, as the number of model parameters increases, the capability of the backbone language model significantly improves, thereby enhancing reasoning performance. Overall, DeepSeek-R1 and DouBao demonstrated the most outstanding reasoning abilities, benefiting from their inherent advantages as dedicated reasoning models. In contrast, the thinking quality of AntiFraud-Qwen2Audio not only surpassed that of similarly sized models such as GLM4-9B-Chat and GLM4-9B-Voice but also exceeded that of much larger models like InternLM2.5-20B and Step-Audio-Chat 130B, achieving reasoning level comparable to GPT-4o. This result fully reflects the optimization effect of AntiFraud-Qwen2Audio in specific tasks. Meanwhile, the untrained Qwen2-Audio-7B-Instruct exhibited lower reasoning quality, further highlighting the crucial role of our slow-thinking training set in enhancing the quality of the models thinking process. 4.2.5 Comprehensive Evaluation Model. In the telecommunications fraud detection task, we comprehensively evaluated the performance of various models, with results indicating that AntiFraudQwen2Audio led with an average F1 score of 83%, validating the effectiveness of the data synthesis and fine-tuning strategies proposed in this paper. Through an in-depth analysis of the models reasoning abilities, we found that this model excelled in the evaluation of thinking process quality. Within the evaluation framework constructed by DeepSeek-R1, AntiFraud-Qwen2Audio significantly outperformed other models in logical rigor (2.06), practical value (2.07), and expressive quality (2.31), achieving total score of 6.44. This not only surpassed similarly sized models like GLM4-9B-Chat but also approached the performance level of GPT-4o. Notably, the untrained Qwen2-Audio-7B-Instruct scored only 4.91 in the thinking process quality evaluation, while the slow-thinking trained AntiFraud-Qwen2Audio demonstrated significant performance improvement, fully validating the effectiveness of our proposed training strategy. The multimodal model GPT-4o* achieved an average F1 score of 72.17%, highlighting both the potential and challenges of multimodal methods, whereas InternLM2.5-20B-Chat and GLM-4-9B-Chat had average F1 scores of 66.81% and 68.08%, respectively, showing relatively weaker performance. The experimental results underscore the critical role of targeted data synthesis, modality fusion optimization, and slow-thinking training in enhancing model performance, providing important insights for future research in the field of telecommunications fraud detection."
        },
        {
            "title": "4.3 Ablation Studies\nInfluence of Audio on the Judgment Performance of\n4.3.1\nthe Model. We analyzed the impact of audio and text features\non model performance from two dimensions. First, models trained\nsolely using ASR text achieved an average F1 score of 73.58%, sig-\nnificantly improving over the un-fine-tuned Base model (49.73%).\nThis indicates that text features effectively capture semantic in-\nformation in conversations. Furthermore, the Without Think and\nThink models, which incorporate audio features, achieved aver-\nage F1 scores of 73.93% and 83.00%, respectively, demonstrating\nthat multimodal fusion significantly enhances the model’s fraud\ndetection performance. The introduction of audio features not only\nsupplements textual information with vocal nuances such as in-\ntonation and pauses but also strengthens the model’s ability to\nunderstand complex fraud scenarios.",
            "content": "Influence of Slow Thinking on the Performance of the 4.3.2 Model. The slow-thinking mechanism is key innovation of this study. By comparing models trained with Without Think and Think datasets, we found that incorporating the slow-thinking strategy significantly enhanced model performance. The F1 score for scenario classification improved from 72.08% to 80.91%, the F1 score for fraud involvement detection increased from 69.32% to 84.78%, and the F1 score for fraud type classification rose from 80.39% to 82.91%. This performance improvement validates the value of slow thinking in telecommunications fraud detection: the multi-step reasoning mechanism enables the model to analyze call scenarios more deeply, capture more potential fraud features, and reduce hasty but inaccurate judgments. The average F1 score improved from 73.93% to 83.00%, fully demonstrating the effectiveness of the slow-thinking approach. Moreover, slow thinking can enhance the systems explainability and credibility, aligning more closely with the judgment logic of human anti-fraud experts. Table 5: Performance Metrics Comparison of Different Qwen2-Audio Model Variants Model Type Variant Scenario Classification Fraud Detection Fraud Type Acc Pre Recall F1 Acc Pre Recall F1 Acc Pre Recall F1 Trovato et al. Avg Qwen2-Audio Base ASR-text NO Think Think 67.36 71.01 72.08 80.91 78.99 79.76 80.38 82.60 67.36 71.01 72.08 80.91 70.22 71.55 72.08 81. 61.83 71.27 68.31 84.22 68.40 76.39 74.04 86.15 51.12 66.80 65.17 83.45 58.51 71.27 69.32 84.78 32.38 76.79 78.83 82.67 78.39 80.39 85.06 83. 32.38 76.79 78.83 82.67 20.47 77.93 80.39 82.91 49.73 73.58 73.93 83.00 Table 6: Quality Evaluation of Different Models Model Type Model Name Logical Rigor Practical Value Expression Quality Total Score (15) ASR+LLM GLM4-9B-Chat InternLM2.5-20B Qwen2.5-72B DeepSeek-V3 Doubao ASR+Reasoning LLM DeepSeek-R1 LALM Ours GLM4-9B-Voice Qwen2-Audio-7B-Instruct Gemini-2-Flash GPT4-o Step-Audio-Chat 130B AntiFraud-Qwen2Audio 1.61 1.99 2.21 2.32 1.94 3.18 0.89 1.51 2.25 2.12 1.64 2."
        },
        {
            "title": "6 Limitations and Future Work\nDespite the significant achievements of our research, several limita-\ntions remain, pointing to directions for future work: 1. Challenges\nin Identifying Certain Fraud Types: Fraud types such as \"identity\ntheft\" remain difficult to identify effectively even after fine-tuning,\nindicating that some fraud types lack sufficient distinguishing fea-\ntures or adequate training samples. Future work could focus on\nincreasing specific samples for these types or designing specialized\nfeature extraction mechanisms. 2. Room for Improvement in Data",
            "content": "1.43 1.93 2.16 2.34 1.75 3.26 0.64 1.42 2.29 2.10 1.62 2.07 2.20 2.43 2.70 2.85 2.60 3. 0.65 1.96 2.72 2.56 2.01 2.31 5.25 6.37 7.01 7.51 6.31 9.94 1.89 4.91 7.25 6.79 5.26 6. Diversity: Although the current dataset includes samples from various sources, its coverage of emerging fraud tactics is limited. Future efforts could consider incorporating more real-world examples of new fraud schemes and utilizing multi-agent adversarial frameworks for continuous updates to adapt to evolving fraud methods. 3. Expansion to Multilingual and Dialect Support: The current research primarily focuses on Mandarin Chinese speech. Future work could extend to multilingual and dialectal environments to address broader range of application scenarios. This research has made significant strides in the application of speech-language models for anti-fraud purposes, establishing the TeleAntiFraud-28k dataset and the AntiFraud-Qwen2Audio model as solid foundation for subsequent studies. By integrating audio modality features and structured thinking processes, we have achieved high-precision identification of telecommunications fraud, providing robust technical support for societal security and economic stability. In the future, we will continue to optimize the dataset and models to enhance the identification of new fraud tactics and expand to more languages and application scenarios, further increasing the practical value of speech-language models in anti-fraud efforts. References [1] Gurjot Singh, Prabhjot Singh, and Maninder Singh. Advanced Real-Time Fraud Detection Using RAG-Based LLMs. arXiv preprint arXiv:2501.15290, 2025. [2] Zitong Shen, Sineng Yan, Youqian Zhang, Xiapu Luo, Grace Ngai, and Eugene Yujun Fu. It Warned Me Just at the Right Moment: Exploring LLM-based Realtime Detection of Phone Scams. arXiv preprint arXiv:2502.03964, 2025. TeleAntiFraud-28k: Audio-Text Slow-Thinking Dataset for Telecom Fraud Detection [3] Zitong Shen, Kangzhong Wang, Youqian Zhang, Grace Ngai, and Eugene Fu. Combating phone scams with llm-based detection: Where do we stand? arXiv preprint arXiv:2409.11643, 2024. [4] Chen-Wei Chang, Shailik Sarkar, Shutonu Mitra, Qi Zhang, Hossein Salemi, Hemant Purohit, Fengxiu Zhang, Michin Hong, Jin-Hee Cho, and Chang-Tien Lu. Exposing LLM Vulnerabilities: Adversarial Scam Detection and Performance. In 2024 IEEE International Conference on Big Data (BigData), pages 35683571, 2024. doi:10.1109/BigData62323.2024.10825256. [5] Aohan Zeng, Zhengxiao Du, Mingdao Liu, Kedong Wang, Shengmin Jiang, Lei Zhao, Yuxiao Dong, and Jie Tang. Glm-4-voice: Towards intelligent and human-like end-to-end spoken chatbot. arXiv preprint arXiv:2412.02612, 2024. [6] Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, and others. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759, 2024. [7] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, and others. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [8] Keyu An, Qian Chen, Chong Deng, Zhihao Du, Changfeng Gao, Zhifu Gao, Yue Gu, Ting He, Hangrui Hu, Kai Hu, and others. Funaudiollm: Voice understanding and generation foundation models for natural interaction between humans and llms. arXiv preprint arXiv:2407.04051, 2024."
        }
    ],
    "affiliations": [
        "China Mobile Internet Company Ltd. Guangzhou, Guangdong, China"
    ]
}