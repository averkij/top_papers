{
    "paper_title": "Native Hybrid Attention for Efficient Sequence Modeling",
    "authors": [
        "Jusen Du",
        "Jiaxi Hu",
        "Tao Zhang",
        "Weigao Sun",
        "Yu Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Transformers excel at sequence modeling but face quadratic complexity, while linear attention offers improved efficiency but often compromises recall accuracy over long contexts. In this work, we introduce Native Hybrid Attention (NHA), a novel hybrid architecture of linear and full attention that integrates both intra \\& inter-layer hybridization into a unified layer design. NHA maintains long-term context in key-value slots updated by a linear RNN, and augments them with short-term tokens from a sliding window. A single \\texttt{softmax attention} operation is then applied over all keys and values, enabling per-token and per-head context-dependent weighting without requiring additional fusion parameters. The inter-layer behavior is controlled through a single hyperparameter, the sliding window size, which allows smooth adjustment between purely linear and full attention while keeping all layers structurally uniform. Experimental results show that NHA surpasses Transformers and other hybrid baselines on recall-intensive and commonsense reasoning tasks. Furthermore, pretrained LLMs can be structurally hybridized with NHA, achieving competitive accuracy while delivering significant efficiency gains. Code is available at https://github.com/JusenD/NHA."
        },
        {
            "title": "Start",
            "content": "Jusen Du1,2*, Jiaxi Hu3, Tao Zhang1(cid:66), Weigao Sun2(cid:66), Yu Cheng4(cid:66) 1Tsinghua University 2Shanghai AI Laboratory 3The Hong Kong University of Science and Technology (Guangzhou) 4The Chinese University of Hong Kong 5 2 0 2 8 ] . [ 1 9 1 0 7 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Transformers excel at sequence modeling but face quadratic complexity, while linear attention offers improved efficiency but often compromises recall accuracy over long contexts. In this work, we introduce Native Hybrid Attention (NHA), novel hybrid architecture of linear and full attention that integrates both intra & inter-layer hybridization into unified layer design. NHA maintains longterm context in key-value slots updated by linear RNN, and augments them with shortterm tokens from sliding window. single softmax attention operation is then applied over all keys and values, enabling pertoken and per-head context-dependent weighting without requiring additional fusion parameters. The inter-layer behavior is controlled through single hyperparameter, the sliding window size, which allows smooth adjustment between purely linear and full attention while keeping all layers structurally uniform. Experimental results show that NHA surpasses Transformers and other hybrid baselines on recall-intensive and commonsense reasoning tasks. Furthermore, pretrained LLMs can be structurally hybridized with NHA, achieving competitive accuracy while delivering significant efficiency gains. Code is available at https://github.com/JusenD/NHA."
        },
        {
            "title": "Introduction",
            "content": "Self-Attention (Vaswani, 2017) has become the primary architecture for sequence modeling due to its exceptional ability to capture long-term dependencies. However, this power comes at steep computational cost. The self-attention mechanism has computational complexity of O(n2) with respect to the sequence length n. This quadratic scaling * Intern at Shanghai AI Laboratory; (cid:66) Corresponding Authors (taozhang@tsinghua.edu.cn, sunweigao@outlook.com, chengyu@cse.cuhk.edu.hk); Project Lead. 1 presents major obstacle for processing long sequences, common requirement in domains like long-document analysis (Beltagy et al., 2020; Qin et al., 2023b; Team, 2023; Qin et al., 2023a; Qu et al., 2025) and bioinformatics (Bai et al., 2025; Dalla-Torre et al., 2025; Lin et al., 2023). To address this limitation, research community has largely pursued two divergent paths (Sun et al., 2025a). The first path involves sparse attention. As shown in Fig. 1, such methods compute softmax attention over sparsely selected tokens, thereby reducing the computational cost (Beltagy et al., 2020; Tang et al., 2023; Qu et al., 2024; Lu et al., 2025; Yuan et al., 2025). Among these approaches, Sliding Window Attention (SWA) currently remains the most efficient implementation, which limits attention computations to local neighborhood. The second direction explores linear sequence modeling, such as linear attention models (Sun et al., 2025c; Du et al., 2025; Sun et al., 2025d; von Oswald et al., 2025; Qin et al., 2024c) and state space models (Hu et al., 2025; Gu and Dao, 2024; Dao and Gu, 2024). These models achieve remarkable O(n) efficiency by compressing the entire sequence history into fixed-size state, thereby enabling global receptive field. While SWA cannot capture tokens beyond its local window, the extreme compression of linear models often results in the loss of precise token information. Given that these two approaches have complementary strengths and weaknesses, intralayer hybrid models (Munkhdalai et al., 2024; McDermott et al., 2025; Lan et al., 2025) that combine them are natural next step. At the same time, different trade-off motivates another form of hybridization. On one hand, pure linear model struggles with the theoretical impossibility of perfectly preserving infinite information within fixed-size memory state. On the other hand, maintaining full KV cache for every token at every layer of standard Transformer is not only computationally prohibitive but often unnecessary, Figure 1: Token Dependencies. Illustration for token dependencies of sparse attention, linear attention, and NHA. as not all layers require the same level of granular global information. This all-or-nothing\" dilemma in information storage across the network depth has motivated the development of inter-layer hybrid models (Lieber et al., 2024; Glorioso et al., 2024b,a; Sun et al., 2025b; Li et al., 2025)."
        },
        {
            "title": "These two kinds of hybrid architectures can be",
            "content": "summarized as follows: 1. Intra-layer Hybrid: Typically involving computing the linear attention and local softmax attention separately, then combining them through weighted summation. 2. Inter-layer Hybrid: Typically involving stacking or alternating different kinds of layers. For instance, stacking 1 softmax attention layer after every 7 linear attention layers to form an inter-layer hybrid architecture. In this paper, we introduce Native Hybrid Attention (NHA), new hybrid form that natively unifies intraand inter-layer hybrid architectures into single, cohesive design. For intra-layer hybrid, NHA compresses long-term information into fixed number of slots using linear RNN model (Qin et al., 2024d) and concatenates them with the local tokens that are inside the window. This combined set of precise, short-term tokens and summarized, long-term slots is then processed by single, unified softmax attention operation. This enables representing long-term memory in the same (m d) keyvalue slot format as SWA, rather than the outer-product form used in many prior hybrids, ensuring compatibility and efficient concatenation. Unlike prior hybrids that compute separate attentions and fuse them through weighted summation, in NHA, the softmax attention mechanism itself learns to dynamically allocate attention between short-term and long-term memory for each query, achieving an automatic and context-aware weighting without the need for manually-tuned or additional parameters. For inter-layer hybrid, all NHA layers share the same design. We can simply change the window size of sliding window attention without modifying the model architecture to control the behavior of each layer. Setting the window size to zero yields pure linear RNN layer, whereas setting it to the full sequence length recovers full attention layer. This contrasts with previous inter-layer models that stack different types of layers, which requires managing heterogeneous blocks and their alignment. NHA, therefore, enables flexible, layer-specific configurations without altering the models fundamental structure. Our contributions can be summarized as follows: Unified intraand inter-layer hybrid architecture. We introduce NHA, which integrates sliding window attention and linear attention within single, unified softmax attention operation, and achieves inter-layer hybridization simply by adjusting the window size, without changing the model architecture. Furthermore, we develop chunkwise-parallel Triton kernel for efficient GPU computation. Comprehensive evaluation of hybrid and non-hybrid architectures. We pretrain and evaluate models from each architectural category. Results demonstrate that hybrid architectures consistently surpass standard Transformers on recall-intensive tasks, with NHA attaining the strongest overall performance. Hybridization of pretrained Transformer models. We demonstrate that NHA can be applied to pretrained Transformer LLMs. With only brief finetuning, these pretrained models can be adapted into the NHA architecture, achieving competitive performance with improved inference speed."
        },
        {
            "title": "2 Preliminary",
            "content": "To improve the efficiency of Transformers, numerous methods have been developed that restrict the 2 Figure 2: Intra-Layer Hybrid in NHA. (a) NHA compresses historical tokens into fixed-size long-term memory slots (brown) through an RNN update, then concatenates them with recent local context (gray) before applying unified softmax attention. (b) Previous intra-layer hybrid approaches generally compute long-term and short-term outputs separately and combine them through weighted summation. (c) In contrast, NHA employs non-parametric, context-dependent softmax attention operation to dynamically determine their respective contributions. attention window to fixed number of tokens. softmax attention over these slots. Sliding Window Attention (SWA) SWA (Beltagy et al., 2020) focuses on fixed-size window of tokens near the current position, storing these tokens precisely. This approach ensures that immediate context is captured accurately but can miss important information outside the window, limiting its long-term sequence comprehension. Gated Slot Attention (GSA) GSA (Zhang et al., 2024b) compresses the entire sequence into set of fixed memory slots. Instead of storing precise tokens, GSA uses gating mechanism to update these slots, blending new and past information as in Eq. (1). This compression allows GSA to maintain broader context and adaptively manage long-term dependencies. Unlike SWAs localized precision focus, GSA balances memory efficiency with comprehensive sequence understanding."
        },
        {
            "title": "3.1 Level 1: Architectural Hybrid of RNN\nMemory and Softmax Attention",
            "content": "Transformer stores the KV cache for the sequence and performs softmax attention over them to compute the output. In contrast, linear attention compresses the entire sequence into fixed-size matrix memory and replaces the softmax operation with more efficient matrix multiplication. Prior work (Peng et al., 2021; Zhang et al., 2024b) has explored hybrid designs that compress past tokens into small set of memory slots and then apply"
        },
        {
            "title": "Klong\nV long\nt",
            "content": "t = Diag(αt) Klong = Diag(αt) long t1 + (1 αt) kt, t1 + (1 αt) vt, (1) , long where represents the current token index, Klong Rmd, while denotes the numt ber of memory slots and αt denotes an inputdependent tensor of gating value where each element is in the range [0, 1]. However, performing softmax attention solely on this compressed memory does not appear to be an ideal solution, as the precise tokens near the current position often play crucial role. This limitation motivates us to take it step further by exploring an intra-layer memory hybrid approach."
        },
        {
            "title": "3.2 Level 2: Intra-Layer Hybrid",
            "content": "SWA applies softmax attention within fixed-size window, focusing on the context of nearby tokens. This approach can be seen as primarily leveraging precise local tokens. The KV cache for tokens within the window remains fixed in size and can be regarded as precise short-term memory."
        },
        {
            "title": "Kshort\nt\nV short\nt",
            "content": "= {kT = {vT tw+1, kT tw+1, vT tw+2, ..., kT tw+2, ..., vT }T , }T , (2) where represents the current token index, denotes the window size, Kshort Rwd. , short Memory Hybridization The long-term memory in Eq. 1 naturally aligns with the short-term memory described above, and the two complement each 3 other effectively. We concatenate them to form unified, hybrid memory that integrates both longterm and short-term information. Subsequently, the softmax function is used to compute the attention scores, enabling dynamic weighting of long-term and short-term memory."
        },
        {
            "title": "Size Adjustment",
            "content": "t = Concat(Klong KH = Concat(V long t , Kshort , short ) R(m+w)d, ) R(m+w)d, ot = softmax( )T qt(KH )V , (3) (4) where represents the current token index, denotes the number of memory slots, denotes the window size, and is the dimension of q. Token shift ensures that only tokens outside the sliding window update the long-term memory. For w, no tokens are removed, and zero prefix is used to initialize the memory slots. For positional encoding, RoPE is applied within the sliding window, while no positional embedding is added to the long-term memory. Further details and ablation analysis are provided in Appendix C.4.2. Context-Dependent Fusion In NHA, the unified softmax over concatenated long-term and shortterm memory assigns the proportion of attention to the long-term memory as ) (cid:80) ilong exp(qtk ) + (cid:80) , ωL = (cid:80) ilong exp(qtk jshort exp(qtk ) (5) where qt is the current query, and ki, kj are keys from longand short-term memory respectively. This proportion is computed jointly with tokenlevel weights and depends on similarity scores with all keys in both memory types, thus incorporating information from all preceding tokens. To further highlight the distinction between unified softmax and output-level weighted fusion, we provide gradient analysis in Appendix and memory analysis in Appendix E. This shows that unified softmax naturally couples gradient flow between longand short-term memories. We compare the performance of unified softmax fusion and learnable weighted fusion in ablations (Sec. 4.7). Since the window size can vary across different layers in the model, it creates the possibility for an inter-layer hybrid. This flexibility leads us to deeper exploration of model architecture. 4 Figure 3: Inter-Layer Hybrid in NHA. All layers share the same NHA architecture. By varying the sliding window size w, each layer can behave as full attention (w = ), intra-layer hybrid (w > 0), or pure linear RNN (w = 0), enabling flexible inter-layer hybridization without changing the model structure. For tasks requiring precise retrieval of long past sequences, pure linear models struggle as it is infeasible to retain all information in fixed-size memory as the sequence length increases. Storing every kv in each layer, as transformers do, appears excessive. This trade-off motivates an inter-layer hybrid approach: strategically placing full attention mechanisms only in select layers, using linear models to sparsify kv storage across the depth of the Transformer. This leverages the efficiency of linear models while retaining the robust memory capabilities of attention where most needed. NHA is inherently suited for this hybrid strategy. The size of the sliding window can vary, ranging from 0 to the entire sequence length. This flexibility allows us to adjust the \"linear ratio\" of an NHA layer by modifying the window size. Setting the window size to 0 reduces the NHA to the pure linear attention model. Conversely, setting the window size to the sequence length transforms the NHA model into Transformer with prefix. NHA enables inter-layer hybridization within unified architecture. By adjusting the window size in selected layers to span the entire sequence, the model naturally realizes an inter-layer hybrid design without requiring any structural modifications."
        },
        {
            "title": "3.4 Chunkwise Parallel Form",
            "content": "NHA is able to implement in chunkwise parallel form. Let the sequence is divided into = /C chunks of size C. For each chunk [t], let the cuj=1 A[t],j mulative and reverse gates be [t],i = (cid:81)i and [t],i = (cid:81)C j=i+1 A[t],j. Linear Memory Update We use gated linear RNN update (Yang et al., 2023), following the associative structure of GSA (Zhang et al., 2024b): S[t] = Diag( [t],C) S[t1] + (K[t] [t])V[t]. (6) Hybrid Attention Logits. For each chunk, we compute two sets of attention logits: 1. Linear channel logits: Ok [t] = Q[t]S[t1]+(Q[t]K [t]M[t])I[t], (7) [t], K[t] = K[t] [t],C), I[t] = 1 A[t] and M[t] is the where Q[t] = Q[t] ( [t]/ causal mask. 2. Shifted sliding window logits: Lswa [t] = Q[t] (cid:101)K win([t]) + log Mwin, (8) where win([t]) indexes positions in the current chunk and its recent successor, (cid:101)K = (cid:2) 0W ; (cid:3) :T , (cid:101)V = (cid:2) 0W ; (cid:3) The two logits are concatenated for softmax: :T . [t] = softmax(cid:0)[Ok Pmix [t]; Lswa [t] ](cid:1). (9) The first columns correspond to the linear channel weights Qv [t], and the remainder to the sliding window weights Pswa [t] Value Aggregation. We compute the outpus of linear memory branch and the sliding window branch: . (cid:16) i=1 αi vi kiS2(cid:17) squares computations, where the models are optimized under global L2 loss, specifically, = (cid:80)t In practice, the accu- . mulation is typically confined to limited range, thereby preserving relatively precise short-term memory while compressing long-term memory. The range aligns with the conjugate gradient step size in MesaNet and the window size in Atlas. SWA can be viewed as Householder-like transformation constructed from unit vectors et Rm. From this perspective, SWA is an unlearnable extreme case of the Delta rule, where the model retains all precise information within the window while discarding everything beyond it. NHA adopts this mechanism to preserve short-term precise memory while leveraging linear recurrence to compress long-term memory for key and value, respectively. In contrast, the KV updates in MesaNet and Atlas are performed together, which precludes the incorporation of the softmax operation."
        },
        {
            "title": "4 Experiments",
            "content": "To validate the design and capabilities of NHA, our experiments are structured to answer the following key research questions (RQs), with concise answers provided in Appendix A. RQ1: How does NHA perform against Transformer and other hybrid models? (Sec. 4.2, 4.3, 4.4) RQ2: Can NHA achieve competitive performance to standard Transformers while offering lower cost? (Sec. 4.5, 4.6) Ov [t]Sv [t] = [t]I [t1] + (Q [t] = Pswa Oswa [t] (cid:101)Vwin([t]), [t] M[t])V[t], RQ3: How do NHAs hybrid components con- (Sec. 4.7, tribute to performance? App. E, C.4.1) (10) , [t], V[t] the same is absorbed into where as in Eq. 7, and (cid:101)V represents the shifted V. The final chunkwise output is: O[t] = Ov [t] + Oswa [t] . (11)"
        },
        {
            "title": "3.5 Connections to MesaNet and Atlas",
            "content": "In the following, we show that our proposed NHA is closely related to recent architectures such as MesaNet (von Oswald et al., 2025) and Atlas (Behrouz et al., 2025). In particular, when equipped with matrix memory, MesaNet and Atlas can be interpreted as performing recursive least RQ4: Is NHA scalable for production-level LLMs? (Sec. 4.6, App. C.3, C.5)"
        },
        {
            "title": "4.1 Experiment Setup",
            "content": "To answer the research questions, we conduct comprehensive experiments using consistent setup. We pretrain all baseline hybrid models from scratch to ensure fair and consistent comparison. Following standard practice, we adopt one Transformer every eight layers stacking strategy: the 340Mparameter models insert Transformer layer as the seventh layer in every eight-layer block, while 1.3B-parameter models insert it as the first layer in every block. Comprehensive evaluation is con5 ducted at both 340M and 1.3B scales. Additional implementation details, including dataset and training schedule, are provided in Appendix C. 4.2 Recall-Intensive Tasks Linear models inherently face challenges in recallintensive tasks due to their fixed-size memory states, which often leads to an performance gap when compared to Transformers. To evaluate NHAs capability in balancing long-term memory with efficient retrieval, we conducted experiments on six recall-intensive benchmarks including FDA (Arora et al., 2023), SWDE (Arora et al., 2023; Lockard et al., 2019), SQuAD (Rajpurkar et al., 2018), NQ (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017) and Drop (Dua et al., 2019). We compared NHAs performance against various pure linear models, standard Transformers, and other hybrid architectures, as summarized in Table 1. Our results demonstrate that hybrid models, in general, achieve significantly better performance than Transformer models with only limited number of full attention layers and NHA consistently achieves the best results across these benchmarks."
        },
        {
            "title": "4.3 Commonsense Reasoning Tasks",
            "content": "Commonsense reasoning benchmarks evaluate broader and more fundamental set of capabilities, such as semantic understanding, and general world knowledge. We evaluated NHA on suite of widely-recognized commonsense reasoning tasks, including WikiText (Merity et al., 2016), LAMBADA (Paperno et al., 2016), ARC-Easy, ARCChallenge (Clark et al., 2018), HellaSwag (Zellers et al., 2019), PiQA (Bisk et al., 2020) and WinoGrande (Sakaguchi et al., 2019). All evaluations were performed using the lm-eval-harness framework (Gao et al., 2024). As shown in Table 1, NHA achieves the highest average score across these tasks, demonstrating that its hybrid design effectively preserves strong general reasoning abilities while enhancing efficiency."
        },
        {
            "title": "4.5 Operator Efficiency",
            "content": "We benchmark the forward and backward computation time of different attention operators using the Triton-Testing-Benchmark (Tillet et al., 2019) on one NVIDIA H100-80G GPU. For NHA, we set the long-term memory slot size to 32 and the sliding window size to 32. For GSA, we use 64 memory slots. Figure 4 reports the results for varying input sequence lengths. FlashAttention is the fastest on short sequences due to its optimized full-attention kernel, but its quadratic complexity causes computation time to grow sharply for long sequences. In contrast, NHA and GSA maintain near-linear scaling, and NHA matches GSAs speed across all lengths, as its intra-layer hybridization with small sliding window adds negligible overhead."
        },
        {
            "title": "4.6 Pretrained LLM Hybridization",
            "content": "Having established the fundamental performance and efficiency of NHA on smaller models through pretraining from scratch, we sought to further validate its practical utility and scalability. For this purpose, we conducted an experiment to structurally hybridize pre-trained open-source Transformer LLMs into NHA-based hybrids. This process involves replacing selected full-attention layers with NHA modules to obtain faster and more efficient model without requiring full retraining."
        },
        {
            "title": "4.6.1 Experiment Setup",
            "content": "We applied NHA hybridization to two pretrained LLMs: Llama-3-8B and Qwen2.5-7B, each configured with 4 full-attention layers within the hybrid architecture. This hybridization is followed by lightweight finetuning stage, ensuring that our findings are not tied to single architecture. Detailed initialization, layer configuration, and training procedures are provided in Appendix C.2. To further test scalability, we scale up to the Qwen3-30B-A3B model with detailed configuration in Appendix C.3."
        },
        {
            "title": "4.6.2 Results",
            "content": "As shown in Table 2, we evaluate models on tasks from the RULER (Hsieh et al., 2024) benchmark, including NIAH-MK, NIAH-MQ, RULER-CWE, and RULER-Hotpot. The models are trained with context length of 2K and tested on extrapolation up to 8K. While Transformers still hold advantages on in-length needle-in-a-haystack tasks, NHA exhibits stronger extrapolation and achieves the best results across multiple tasks. We evaluated the finetuned NHA-Qwen2.5-7B and NHA-Llama-3-8B against their original models, leading linear models and hybrid architectures  (Table 3)  . NHA-Llama closely matches the original Llama and consistently offers better trade-off between recall accuracy and computational efficiency than other efficient models. NHA-Qwen2.5 maintains strong performance on commonsense reasoning tasks. The larger gap on recall-intensive tasks 6 Model Wiki. ppl LMB. ppl 340M Params 15B Tokens 42.15 Trans++ 26.88 GLA GSA GDN 28.78 28.17 26.47 Mamba2-H 25.81 27.94 GLA-H 26.19 GSA-H 25.67 GDN-H 25.97 NHA 39.00 42.57 31. 43.35 40.12 46.05 51.79 38.38 1.3B Params 100B Tokens 15.86 Trans++ 17.61 GLA GSA GDN 17.61 16.69 17.14 Mamba2-H 16.00 17.58 GLA-H 16.22 GSA-H 16.02 GDN-H 16.16 NHA 15.38 12.62 11.35 12.77 15.72 23.09 11.68 12.58 ARCe ARCc PIQA Hella. LMB. Wino. Avg. accn acc accn acc acc acc acc FDA SWDE TQA NQ SQD Drop Avg. acc acc acc acc acc acc acc 44.91 25.94 64.31 34.95 32. 51.07 42.34 46.14 25.87 45.97 18.94 33.22 20.03 31.70 44.53 45.50 46.04 48.23 45.20 45.54 45.75 47. 22.27 24.23 23.55 24.49 24.32 23.72 23.21 24.32 63.93 64.85 66.05 65.23 63.93 65.23 65.56 65.67 34.84 35.00 35.18 36.31 34.42 35.19 35.80 36. 32.27 30.78 34.35 30.86 31.83 30.91 29.30 32.72 51.38 50.43 50.83 48.62 49.96 50.99 51.38 52.09 41.54 11.26 41.80 6.36 42.67 20.53 42.29 55.68 41.61 58.86 41.93 62.13 41.83 52.32 43.09 53. 16.78 16.87 23.24 39.55 39.36 45.36 38.80 48.55 43.90 12.77 27.85 17.68 21.71 42.18 14.60 21.90 16.72 19.77 44.91 14.98 28.55 16.48 24.78 45.50 18.34 27.98 17.63 34.11 44.14 17.01 31.37 18.21 34.83 43.78 20.62 31.17 18.78 36.97 39.40 15.24 29.83 17.92 32.25 46.56 20.56 38.60 23.48 38.60 55.01 28. 70.08 49.21 45.60 56.27 50.71 44.32 32. 58.47 24.49 42.59 21.56 37.31 55.18 58.33 56.82 57.74 56.73 57.79 58.12 58.71 27.56 28.33 27.39 28.24 27.56 28.07 27.99 28.50 69.86 72.25 71. 71.93 70.40 71.71 70.57 72.03 48.89 50.98 49.77 52.73 48.71 51.70 52.52 52.08 46.05 47.43 49.10 48.46 46.71 41.92 49.78 49.19 53.91 53.43 51. 55.49 54.46 53.35 56.27 56.83 50.24 27.61 51.79 23.25 51.10 30.25 52.43 65.30 50.76 69.48 50.76 68.30 52.54 66.76 52.89 68.30 30.93 32.80 27.65 50.89 46.77 51.83 49.02 52.48 56.28 22.27 35.04 19.45 31.93 57.05 22.96 35.57 20.65 32.05 58.23 23.22 34.06 20.36 32. 55.92 28.54 41.99 22.46 44.18 57.05 27.15 40.28 21.47 43.70 58.59 26.42 42.36 22.42 44.99 59.06 29.30 41.15 24.01 44.88 59.60 27.18 45.58 25.44 46.43 Table 1: Results on Common-Sense Reasoning Tasks and Recall-Intensive Tasks. GDN denotes Gated DeltaNet; -H denotes hybrid with Transformer layers. Model NIAH-MK NIAH-MQ RULER-CWE RULER-Hotpot 1K 2K 4K 1K 2K 4K 1K 2K 4K 2K 4K 8K Transformer 90.2 63. Mamba2-H 13.6 3.8 0.4 4.8 81.8 64.3 13.1 42.9 20.9 52.5 55.8 35.0 14. 9.0 0.0 8.5 21.3 2.8 0. 4.4 19.4 16.0 GLA-H GSA-H GDN-H NHA 46.0 23.6 12.4 37.9 30.1 25.0 50.4 12.7 10.4 26.2 20.6 19.2 20.0 11.2 6.4 77.7 68.5 34.4 42.1 24.4 15.4 24.8 20.6 18.0 59.6 29.4 15.0 56.1 41.4 27.1 64.7 31.0 81.4 51.8 21.6 62.7 50.6 28.4 63.8 33. 4.1 9.3 26.5 21.6 16.8 27.4 26.0 24.8 Figure 4: Operator Training Throughput. Throughput for forward and backward pass. Table 2: Results on RULER tasks. Models are trained with context length of 2K. may be partly due to the limited 10B-token finetuning budget, the distribution shift between the SlimPajama corpus and Qwens pretraining data, and the hardware constraints that limited training context to 2K tokens. Both models also show drop on MMLU, benchmark where even state-ofthe-art hybrid models underperform. Notably, despite the small finetuning budget, NHA reaches performance comparable to the strongest same-scale hybrid baselines. We further note that for existing hybrid models, the average recall accuracy tends to correlate with the number of full attention layers. Remarkably, our NHA-Llama3-8B achieves the best performance with only 4 full attention layers, highlighting its efficiency advantage (see Fig. 6). Scaling further, hybridizing Qwen3-30B-A3B also yields competitive results across diverse benchmarks (Table 4, Appendix C.3), confirming that NHA can be applied to very large-scale models while reducing reliance on full attention layers."
        },
        {
            "title": "4.6.3 Efficiency",
            "content": "Figure 5: Efficiency. Inference latency and memory usage of NHA-Llama3-8B compared with Llama3-8B. To evaluate the efficiency of NHA, we com7 Model & Scale Training Tokens (B) ARCe ARCc Hella. LMB. PIQA Wino. MMLU Avg. FDA SWDE SQD. NQ TQA. Drop Avg. acc acc acc acc acc acc acc acc acc acc acc acc acc accn accn Transformer Mistral-7B Qwen3-8B Qwen2.5-7B Llama-3-8B 8000 36000 18000 15000 80.85 54.01 81.08 69.49 80.90 73.64 83.42 56.74 74.90 61.11 76.61 68.11 80.43 51.45 78.93 64.70 78.78 73.09 80.30 53.07 79.14 68.89 79.60 72.77 62.50 71.78 75.30 65.32 54.01 42.76 73.16 27.55 56.35 74.93 70.83 73.48 76.85 59.89 45.01 72.63 36.61 60.75 74.18 71.65 82.38 75.45 54.28 48.43 77.49 38.28 62.72 65.34 71.30 82.92 72.07 54.22 43.24 74.17 33.88 60. Linear/Subquadratic Mamba-7B FalconMamba-7B RWKV-6-World-7B Griffin-7B Hybrid StripedHyena-7B(16) Zamba-7B(13) Zamba2-7B(9) NHA-Qwen2.5-7B(4) NHA-Llama-3-8B(4) 1200 5500 1420 300 - 1000 2100 10 77.61 46.84 77.93 65.53 79.87 71.74 83.33 58.45 80.22 61.60 80.20 75.53 73.61 43.86 75.17 69.26 78.35 67.72 81.00 72.60 75.40 47.90 78.60 - 33.19 64.67 37.06 43.30 46.93 33.39 71.56 25.30 42.92 60.27 71.37 48.32 59.51 49.31 37.85 75.06 30.09 50.02 43.39 64.48 56.86 51.55 46.62 37.03 69.85 28.70 48.44 39.30 - - - - - - - - 78.62 53.75 79.11 60.78 80.03 70.32 71.72 45.56 80.81 64.84 80.41 72.14 80.13 56.23 81.52 59.09 79.05 77. 54.07 68.10 75.30 69.82 53.81 42.41 71.86 32.34 57.59 57.68 67.59 76.02 71.70 50.69 42.10 70.56 25.78 56.14 67.27 71.50 71.48 64.48 50.15 40.13 71.86 29.23 54.56 82.74 55.97 76.49 64.54 78.73 71.03 80.76 52.47 78.93 67.26 79.71 72.85 68.80 71.19 62.58 44.61 47.26 37.47 71.98 37.47 50.23 60.21 70.31 73.84 67.67 55.16 42.10 73.05 34.02 57.64 Table 3: Comparison of Pretrained LLMs and Their NHA Hybrids. Performance of original Transformer, linear/subquadratic, and hybrid baselines versus our NHA-hybridized Llama-3-8B and Qwen2.5-7B. Hybrid models show the number of full-attention layers in subscript parentheses."
        },
        {
            "title": "Model",
            "content": "ARCe ARCc Hella. LMB. PIQA Wino. Avg. GPQA IFVL. HEval. MaQA. MMLU Avg. 78.87 56.57 77.63 63.05 79.49 69.61 70.87 40.62 Qwen3-30BA3B NHA-Qwen3-30BA3B 82.24 56.40 79.10 68.85 80.96 73.80 73.56 41.96 30.58 30.94 15.85 26.22 58.09 61.64 77.84 75. 44.60 47.25 Table 4: Performance Comparison. Qwen3-30B-A3B versus NHA-Qwen3-30B-A3B on standard benchmarks. pare the inference speed and GPU memory usage of NHA-Llama3-8B with the original Llama3-8B when generating 1K tokens under varying input. As shown in Figure 5, Llama3-8B scales poorly as input length increases, whereas NHA-Llama3-8B shows much slower growth in both metrics, demonstrating superior scalability and efficiency. of NHAs key components. Removing either longterm or short-term memory leads to clear drops in performance, and eliminating token shift further degrades results by allowing overlap between the two memories. These findings highlight the complementary roles of longand short-term memory and the necessity of keeping them distinct."
        },
        {
            "title": "4.7 Ablation Study",
            "content": "In this section, we provide detailed analysis of the components of NHA to offer insights for the design of future hybrid architectures."
        },
        {
            "title": "Variant",
            "content": "Recall Common NHA (full) - w/o Long-Mem - w/o Short-Mem - w/o Token Shift - Weighted Sum(F) - Weighted Sum(L) 38.60 29.58 36.97 35.76 34.06 33.59 43.09 40.83 41.93 41. 42.69 43."
        },
        {
            "title": "5 Future Work",
            "content": "Table 5: Ablation on Memory and Fusion."
        },
        {
            "title": "We conduct ablations to assess the contributions",
            "content": "The introduction of fixed memory slots and unified longshort memory opens up several promising 8 directions. For instance, parameter-efficient finetuning (PEFT) could be applied to learn slot initial states tailored for downstream tasks. In reasoning scenarios such as chain-of-thought (CoT), reasoning chains could be selectively compressed into long-term memory, retaining essential information while reducing computational overhead."
        },
        {
            "title": "6 Limitations",
            "content": "NHA achieves strong accuracy and efficiency, but it introduces additional hyperparameters such as slot size and window size that may require careful tuning to fully realize its potential. The current implementation also leaves room for further operatorlevel optimization, which could further improve deployment efficiency. Moreover, although NHA supports flexible adjustment of window sizes across layers, we primarily consider uniform settings, while more structured strategies such as progressive variation across depth may further enhance adaptability."
        },
        {
            "title": "References",
            "content": "Aida Amini, Saadia Gabriel, Peter Lin, Rik KoncelKedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. arXiv preprint arXiv:1905.13319. Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, and Christopher Ré. 2024. Simple linear attention language models balance arXiv preprint the recall-throughput arXiv:2402.18668. tradeoff. Simran Arora, Brandon Yang, Sabri Eyuboglu, Avanika Narayan, Andrew Hojel, Immanuel Trummer, and Christopher Ré. 2023. Language models enable simple systems for generating structured views of heterogeneous data lakes. Preprint, arXiv:2304.09433. Lei Bai, Zhongrui Cai, Maosong Cao, Weihan Cao, Chiyu Chen, Haojiong Chen, Kai Chen, Pengcheng Chen, Ying Chen, Yongkang Chen, and 1 others. 2025. Intern-s1: scientific multimodal foundation model. arXiv preprint arXiv:2508.15763. Ali Behrouz, Zeman Li, Praneeth Kacham, Majid Daliri, Yuan Deng, Peilin Zhong, Meisam Razaviyayn, and Vahab Mirrokni. 2025. Atlas: Learning to optimally memorize the context at test time. arXiv preprint arXiv:2505.23735. Iz Beltagy, Matthew Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150. Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. 2020. Piqa: Reasoning about physical commonsense in natural language. In ThirtyFourth AAAI Conference on Artificial Intelligence. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, and 1 others. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457. Hugo Dalla-Torre, Liam Gonzalez, Javier MendozaRevilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, Evan Trop, Bernardo de Almeida, Hassan Sirelkhatim, and 1 others. 2025. Nucleotide transformer: building and evaluating robust foundation models for human genomics. Nature Methods, 22(2):287297. Tri Dao and Albert Gu. 2024. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060. Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, Zijia Chen, Ameya Sunil Mahabaleshwarkar, ShihYang Liu, Matthijs Van Keirsbilck, Min-Hung Chen, Yoshi Suhara, and 1 others. 2024. Hymba: hybridhead architecture for small language models. arXiv preprint arXiv:2411.13676. Jusen Du, Weigao Sun, Disen Lan, Jiaxi Hu, and Yu Cheng. 2025. Mom: Linear sequence modarXiv preprint eling with mixture-of-memories. arXiv:2502.13685. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. Drop: reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint arXiv:1903.00161. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, and 5 others. 2024. framework for few-shot language model evaluation. Paolo Glorioso, Quentin Anthony, Yury Tokpanov, Anna Golubeva, Vasudev Shyam, James Whittington, Jonathan Pilault, and Beren Millidge. 2024a. The zamba2 suite: Technical report. arXiv preprint arXiv:2411.15242. Paolo Glorioso, Quentin Anthony, Yury Tokpanov, James Whittington, Jonathan Pilault, Adam Ibrahim, 9 and Beren Millidge. 2024b. Zamba: Compact 7B SSM Hybrid Model. arXiv preprint. Albert Gu and Tri Dao. 2024. Mamba: Lineartime sequence modeling with selective state spaces. Preprint, arXiv:2312.00752. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. 2024. Ruler: Whats the real context size of your long-context language models? arXiv preprint arXiv:2404.06654. Jiaxi Hu, Yongqi Pan, Jusen Du, Disen Lan, Xiaqiang Tang, Qingsong Wen, Yuxuan Liang, and Weigao Sun. 2025. Comba: Improving nonlinear rnns with closed-loop control. arXiv preprint arXiv:2506.02475. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, and 1 others. 2019. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466. Disen Lan, Weigao Sun, Jiaxi Hu, Jusen Du, and Yu Cheng. 2025. Liger: Linearizing large language models to gated recurrent structures. arXiv preprint arXiv:2503.01496. Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, and 1 others. 2025. Minimax-01: Scaling foundation models with lightning attention. arXiv preprint arXiv:2501.08313. Jhonathan Osin, Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida, Amir Bergman, Roman Glozman, Michael Gokhman, Avashalom Manevich, Nir Ratner, Noam Rozen, and 3 others. 2024. Jamba: Hybrid Transformer-Mamba Language Model. arXiv preprint. ArXiv:2403.19887 [cs]. Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, and 1 others. 2023. Evolutionary-scale prediction of atomic-level protein structure with language model. Science, 379(6637):11231130. Colin Lockard, Prashant Shiralkar, and Xin Luna Dong. 2019. OpenCeres: When open information extraction meets the semi-structured web. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 30473056, Minneapolis, Minnesota. Association for Computational Linguistics. Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi Wang, and 1 others. 2025. Moba: Mixture of block attention for long-context llms. arXiv preprint arXiv:2502.13189. Luke McDermott, Robert W. Heath Jr, and Rahul Parhi. 2025. LoLA: Low-Rank Linear Attention With Sparse Caching. arXiv preprint. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models. Preprint, arXiv:1609.07843. Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. 2024. Leave no context behind: Efficient infinite context transformers with infiniattention. arXiv preprint arXiv:2404.07143, 101. Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. 2016. The lambada dataset: Word prediction requiring broad discourse context. arXiv preprint arXiv:1606.06031. Hao Peng, Jungo Kasai, Nikolaos Pappas, Dani Yogatama, Zhaofeng Wu, Lingpeng Kong, Roy Schwartz, and Noah Smith. 2021. Abc: Attention with bounded-memory control. arXiv preprint arXiv:2110.02488. Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Xiao Luo, Yu Qiao, and 1 others. 2023a. Transnormerllm: faster and better large language model with improved transnormer. arXiv preprint arXiv:2307.14995. Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Fei Yuan, Xiao Luo, and 1 others. 2023b. Scaling transnormer to 175 billion parameters. arXiv preprint arXiv:2307.14995. Zhen Qin, Xuyang Shen, Dong Li, Weigao Sun, Stan Birchfield, Richard Hartley, and Yiran Zhong. 2024a. Unlocking the secrets of linear complexity sequence model from unified perspective. arXiv preprint arXiv:2405.17383. Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. 2024b. Lightning attention-2: free lunch for handling unlimited sequence lengths in large language models. arXiv preprint arXiv:2401.04658. 10 Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. 2024c. Various lengths, constant speed: Efficient language modeling with lightning attention. arXiv preprint arXiv:2405.17381. Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. 2024d. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904. Xiaoye Qu, Daize Dong, Xuyang Hu, Tong Zhu, Weigao Sun, and Yu Cheng. 2024. Llama-moe v2: Exploring sparsity of llama from perspective of mixture-of-experts with post-training. arXiv preprint arXiv:2411.15708. Xiaoye Qu, Yafu Li, Zhaochen Su, Weigao Sun, Jianhao Yan, Dongrui Liu, Ganqu Cui, Daizong Liu, Shuxian Liang, Junxian He, and 1 others. 2025. survey of efficient reasoning for large reasoning models: Language, multimodality, and beyond. arXiv preprint arXiv:2503.21614. Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you dont know: Unanswerable questions for squad. Preprint, arXiv:1806.03822. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. 2024. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. 2025. Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling. arXiv preprint. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019. Winogrande: An adversarial winograd schema challenge at scale. arXiv preprint arXiv:1907.10641. Xuyang Shen, Dong Li, Ruitao Leng, Zhen Qin, Weigao Sun, and Yiran Zhong. 2024. Scaling laws for linear complexity language models. arXiv preprint arXiv:2406.16690. Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob Steeves, Joel Hestness, and Nolan Dey. 2023. SlimPajama: 627B token cleaned and deduplicated version of RedPajama. Weigao Sun, Jiaxi Hu, Yucheng Zhou, Jusen Du, Disen Lan, Kexin Wang, Tong Zhu, Xiaoye Qu, Yu Zhang, Xiaoyu Mo, and 1 others. 2025a. Speed always wins: survey on efficient architectures for large language models. arXiv preprint arXiv:2508.09834. Weigao Sun, Disen Lan, Yiran Zhong, Xiaoye Qu, and Yu Cheng. 2025b. Lasp-2: Rethinking sequence parallelism for linear attention and its hybrid. arXiv preprint arXiv:2502.07563. Weigao Sun, Disen Lan, Tong Zhu, Xiaoye Qu, and Yu Cheng. 2025c. Linear-moe: Linear sequence modeling meets mixture-of-experts. arXiv preprint arXiv:2503.05447. Weigao Sun, Yongtuo Liu, Xiaqiang Tang, and Xiaoyu Mo. 2025d. Sequence accumulation and beyond: Infinite context length on single gpu and large clusters. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2072520733. Weigao Sun, Zhen Qin, Dong Li, Xuyang Shen, Linear atarXiv preprint Yu Qiao, and Yiran Zhong. 2024a. tention sequence parallelism. arXiv:2404.02882. Weigao Sun, Zhen Qin, Weixuan Sun, Shidi Li, Dong Li, Xuyang Shen, Yu Qiao, and Yiran Zhong. 2024b. Co2: Efficient distributed training with full communication-computation overlap. arXiv preprint arXiv:2401.16265. Xiaqiang Tang, Weigao Sun, Siyuan Hu, Yiyang Sun, and Yafeng Guo. 2023. Ms-net: multi-path sparse model for motion prediction in multi-scenes. IEEE Robotics and Automation Letters. InternLM Team. 2023. Internlm: multilingual language model with progressively enhanced capabilities. Philippe Tillet, Hsiang-Tsung Kung, and David Cox. 2019. Triton: an intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, pages 1019. Vaswani. 2017. Attention is all you need. Advances in Neural Information Processing Systems. Johannes von Oswald, Nino Scherrer, Seijin Kobayashi, Luca Versari, Songlin Yang, Maximilian Schlegel, Kaitlin Maile, Yanick Schimpf, Oliver Sieberling, Alexander Meulemans, and 1 others. 2025. Mesanet: Sequence modeling by locally optimal test-time training. arXiv preprint arXiv:2506.05233. Songlin Yang, Jan Kautz, and Ali Hatamizadeh. 2024. Gated delta networks: Improving mamba2 with delta rule. arXiv preprint arXiv:2412.06464. Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. 2023. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635. Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, YX Wei, Lean Wang, Zhiping Xiao, and 1 others. 2025. Native sparse attention: Hardware-aligned and naarXiv preprint tively trainable sparse attention. arXiv:2502.11089. 11 Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Michael Zhang, Simran Arora, Rahul Chalamala, Alan Wu, Benjamin Spector, Aaryan Singhal, Krithik Ramesh, and Christopher Ré. 2024a. Lolcats: On low-rank linearizing of large language models. arXiv preprint arXiv:2410.10254. Yu Zhang, Songlin Yang, Ruijie Zhu, Yue Zhang, Leyang Cui, Yiqiao Wang, Bolun Wang, Freda Shi, Bailin Wang, Wei Bi, and 1 others. 2024b. Gated slot attention for efficient linear-time sequence modeling. arXiv preprint arXiv:2409.07146. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911."
        },
        {
            "title": "A Answers to Research Questions",
            "content": "We provide concise answers to the research questions raised in Sec. 4, with supporting evidence from the corresponding sections. RQ1: How does NHAs native hybridization design perform against Transformer and other hybrid models? NHA outperforms hybrid baselines on both recall-intensive (Sec. 4.2), commonsense reasoning tasks (Sec. 4.3) and long-context benchmarks (Sec. 4.4), confirming stronger overall effectiveness. RQ2: Can NHA achieve competitive performance to standard Transformers while offering lower cost? Yes. NHA achieves efficiency (Sec. 4.5) and, when applied to pretrained LLMs, closely matches Transformer accuracy with reduced inference time and memory (Sec. 4.6.2, 4.6.3). RQ3: How do NHAs hybrid components contribute, and does unified softmax improve over weighting? Ablations show both memories and token shift are essential, and unified softmax fusion clearly outperforms fixed or learned weighted fusion (Sec. 4.7). We also analyze the long-short memory (App. E) and conduct ablation study on the slot size and window size (App. C.4.1). RQ4: Is NHA scalable for production-level LLMs? Yes. Finetuned NHA-Llama and NHAQwen demonstrate scalability to billion-scale LLMs with strong accuracy and efficiency gains (Sec. 4.6.2). We scale to Qwen3-30BA3B model and verify the effect of NHA (App. C.3). And NHA can utilize fewer full attention layers to get better performance than other hybrid models (App. C.5)."
        },
        {
            "title": "B Related Work and Positioning",
            "content": "Intra-layer hybrids. Existing intra-layer hybrid models typically combine linear attention mechanism (Sun et al., 2024a; Qin et al., 2024a; Shen et al., 2024; Qin et al., 2024b), which maintains long-term information in matrix memory updated via the outer product of and v, with local sliding window attention (SWA). Most of these approaches compute the outputs from the linear and local modules separately and then merge them via fixed or learnable weighting. For example, LoLCATs (Zhang et al., 2024a) and Liger (Lan et al., 2025) use fixed, pre-specified fusion coefficients; Infini-attention (Munkhdalai et al., 2024) employs 12 learnable but input-independent global coefficient; Griffin (Dong et al., 2024) progressively replaces SWA layers with linearSWA hybrids according to predefined schedule. In contrast, NHA maintains an explicit KV cache for the local window and compressed slot-based KV memory for the long-term context, concatenating the two and applying single softmax over all keys. This yields context-dependent weights that allow the model to dynamically allocate attention between longand short-term memory without manual fusion rules. Inter-layer hybrids. Inter-layer hybrid architectures mix different layer types within the same model depth. Representative examples include Zamba (Glorioso et al., 2024b), Jamba (Lieber et al., 2024), Samba (Ren et al., 2025), and Minimax-01 (Li et al., 2025; Sun et al., 2025b), which insert Transformer layers at fixed ratios among linear or recurrent layers. Such designs require heterogeneous module types and careful alignment of their hidden representations. By contrast, NHA uses single unified layer design throughout the network. Inter-layer hybridization is achieved simply by adjusting the sliding window size per layer: setting it to zero yields purely linear layer, while setting it to the full sequence length recovers Transformer-like layer. This eliminates the need for architectural alignment while still allowing flexible control over the depth-wise allocation of linear and full-attention behavior. Sharper Contrast to Prior Hybrids Many local+global hybrid models, such as LoLCATs and Infini-attention, follow two-stage design in which the long-range context is first compressed into global tokens or slots, the local and global attentions are computed independently, and the two outputs are then fused through either fixed or learned scalar coefficient. In these approaches, the fusion occurs at the output level, which means the trade-off between global and local information is determined outside the attention computation, is uniform across all tokens for given query, and is unaffected by the similarity structure between the query and the keys. Gradient flow is also separated: each branch is updated independently based solely on its own attention distribution. Computationally, this design requires running two separate attention operations per query, one for the local branch and one for the global branch. NHA differs fundamentally in both representation and computation. Long-term context is stored in the same keyvalue slot format as local SWA tokens, enabling direct concatenation and single softmax over all keys. This unified computation produces the globallocal allocation ωL as part of the attention distribution itself, allowing it to vary per token and per head according to similarity scores with all stored keys. As result, gradient updates to one memory type are inherently influenced by the logits of the other, creating cross-memory coupling absent in prior designs. Theoretical differences between unified softmax and output-level fusion, including gradient coupling and contextdependent weighting, are analyzed in Sec. 3.2. In terms of efficiency, this approach requires only one attention computation over the concatenated set of keys, eliminating the duplicated cost of running separate attentions for each memory type. Empirically, we complement this analysis with visualization. Appendix presents heatmaps of attention scores for NHA and for weighted-fusion baselines, showing that NHA develops positionsensitive long/short allocation patterns that prior methods fail to capture. These visualizations provide further evidence that unified softmax learns richer and more context-dependent allocation dynamics than traditional fusion strategies."
        },
        {
            "title": "C Experiment Details",
            "content": "All experiments were performed on 32 NVIDIA H100 GPUs. Training the 340M-parameter model completed in approximately 2 hours, whereas the 1.3B-parameter model required about 1 day. To ensure reproducibility, we used fixed random seed of 42 across all training and evaluation procedures. Results are reported from representative single run, since repeated training confirmed that performance remained consistent across runs. C.1 Pretrain Experiment Setup Given the scarcity of prior work directly investigating hybrid models that integrate modern linear attention mechanisms such as GLA (Yang et al., 2023), GSA (Zhang et al., 2024b), Gated DeltaNet (Yang et al., 2024), and SSMs like Mamba2 (Dao and Gu, 2024), we pretrain all baselines from scratch. Each 340M-parameter model is trained for 15B tokens, and each 1.3B-parameter model for 100B tokens on the SlimPajama (Soboleva et al., 2023) dataset. All models are optimized with AdamW (learning rate 3e-4, cosine schedule, weight decay 0.01, gradient clipping 1.0, random 13 seed 42) (Sun et al., 2024b). C.2 Finetune Experiment Setup We designed our experimental suite to cover broad range of model sizes, datasets, and baselines, given realistic compute constraints. Our goal was to maximize coverage of scenarios most indicative of real-world performance, while ensuring reproducibility. Parameter Inheritance Given the high parameter similarity between NHA and the standard Transformer, we initialize the corresponding Q, K, , and output projection weights in our NHA model directly from the pre-trained checkpoints. For the additional gating parameters introduced by NHA, we adopt method similar to that in (Lan et al., 2025), initializing them by applying average pooling to the pretrained K-projection weights. For the inter-layer hybrid configuration, we designate one layer in every eight-layer block as full attention layer, which is achieved by setting its NHA window size to the full sequence length. Model Hybrid Configuration For the 32-layer NHA-Llama3, we designated the first layer in every 8-layer block as full attention layer, resulting in 4 full attention layers in total. For the 28-layer NHAQwen2.5, we designated the second layer in every 7-layer block as full attention layer, also resulting in 4 full attention layers. Training Configuration We continue the training on the SlimPajama dataset for total of 10B tokens, divided into two distinct finetuning stages. In the first stage, we freeze the FFN parameters and exclusively finetune the attention layers for 5B tokens, allowing the model to adapt to the new hybrid attention mechanism. Subsequently, in the second stage, we apply LoRA to all model parameters and train for an additional 5B tokens to achieve efficient, full-model finetuning. C.3 Large-Scale Hybridization with Qwen3-30B-A3B We further scale up the hybridization experiment to the Qwen3-30B-A3B model. Considering the high computational cost of training such large model, we adopt conservative 2:1 hybridization strategy, and train the model on 5B tokens from the SlimPajama dataset. On standard benchmarks including ARC-Easy, ARC-Challenge (Clark et al., 2018), HellaSwag 64 64 32 64 64 8 16 32 32 Recall Common 31.90 33.97 34.52 38.60 37.83 42.16 42.54 42.86 43.09 43.06 Table 6: Ablation on Slot Size and Window Size. (Zellers et al., 2019), LAMBADA (Paperno et al., 2016), PIQA (Bisk et al., 2020), and WinoGrande (Sakaguchi et al., 2019), as well as broader evaluations such as GPQA (Rein et al., 2024), IFEval (Zhou et al., 2023), HumanEval (Chen et al., 2021), MathQA (Amini et al., 2019) and MMLU (Hendrycks et al., 2020). As shown in Table 4 ,the hybridized Qwen3-30B-A3B achieves competitive performance while reducing the reliance on full attention layers. These results confirm the scalability of NHA to tens-of-billions-parameter models. C.4 Additional Ablations C.4.1 Ablation on Slot Size and Window Size We jointly vary the number of long-term memory slots and the sliding window size to examine their combined effect on NHAs performance. All other architectural and training settings are kept identical to the main experiments. Table 6 shows that recall-intensive tasks generally benefit from larger and moderate w, while commonsense reasoning remains stable across wide range of configurations. Extremely small reduces short-term precision, whereas very large approaches full attention and erodes efficiency gains. C.4.2 Ablation on Positional Embedding"
        },
        {
            "title": "PE Type",
            "content": "Recall Common RoPE (S) RoPE (S+L) None Learnable (S) 38.60 28.46 26.99 30.56 43.09 43.01 41.97 41.85 Table 7: Ablation on PE strategy. We evaluate four positional encoding strategies in NHA, focusing on whether and where to apply position embeddings. As shown in Table 7, denotes PE applied to short-term memory and to long-term memory. The results indicate that applying positional encoding solely to short-term memory yields the best performance, as it enhances 14 softmax attention without interfering with the implicit positional encoding already captured by the long-term memorys recursive update. C.5 Relation Between Full Attention Layers and Recall Performance Figure 6: Average Recall Accuracy vs. Full Attention Layers. NHA-Llama3-8B attains the best performance with only 4 full attention layers."
        },
        {
            "title": "D Gradient Coupling Analysis",
            "content": "In prior hybrids, the fusion coefficient between longand short-term outputs is typically fixed or predicted only from the current input, without access to the similarity distribution across both memories. As result, such coefficients cannot in general reproduce ωL, which reflects both the current query and the aggregate interactions with all stored tokens. Because the weighting is computed jointly over all tokens in both memories, the model can express fine-grained, query-dependent trade-offs that static or input-only fusion coefficients cannot represent. Gradient coupling. Let ZL = (cid:80) ki) and ZS = (cid:80) kj). Define the unified attention distribution over all keys as puni(u) = exp(q . Differentiating Eq. 5 with respect to the jshort exp(q ilong exp(q ku) ZL+ZS logits ℓu = t ku yields: ωL ℓi ωL ℓj = puni(i) (1 ωL), long, (12) = puni(j) ωL, short. (13) These expressions show that the gradient adjusting the long/short allocation is modulated by both memories: changes in long-term logits depend on the total short-term mass 1 ωL, and vice versa. This creates natural cross-memory coupling that aligns both memories in the same similarity space. 15 By contrast, in simple weighted fusion where the coefficient α_t is applied after computing two independent softmaxes, αt = 0 for any token ℓu not used to compute αt. As result, logits in the other memory type receive no gradient signal for adjusting the allocation, leading to weaker coadaptation between memories."
        },
        {
            "title": "E Memory Analysis",
            "content": "We analyze the distribution of attention between longand short-term memory by visualizing the average long-ratio across layers, heads, and positions (Figure 7). The results show that different layers and heads exhibit distinct preferences, indicating specialization in memory usage. Positional patterns are also evident: early tokens before the sliding window is filled rely more on long-term memory, while later tokens increasingly attend to long-term slots for retrieving distant information. For comparison, we examined variant where the fusion weight between longand short-term memory is obtained by applying learnable linear projection to the current input (Figure 8). This variant produces distribution that is overall more uniform across sequence positions, showing little variation between early and late tokens. In contrast, NHA exhibits clear increase in long-term memory usage toward later positions, reflecting its ability to adapt to the growing need for retrieving distant information. This highlights that unified softmax not only enables per-token, context-dependent weighting but also captures position-sensitive patterns. This adaptive positional trend is difficult to obtain with weighted-sum fusion, where the longshort memory trade-off is externally parameterized rather than jointly learned within the attention distribution."
        },
        {
            "title": "F Datasets",
            "content": "We pretrain our models on the SlimPajama dataset. For the 340M variant, we use 15B-token sample, while the 1.3B variant is trained on 100Btoken sample. SlimPajama (Soboleva et al., 2023) is an English-language, high-quality subset of RedPajama that includes Common Crawl, Wikipedia, books, and GitHub code. It is cleaned, deduplicated, and optimized for large-scale model training. For standard language understanding, we evaluate on the following English benchmarks: WikiText (62 test samples) (Merity et al., 2016), derived from Wikipedia articles authored by global volunteers; Figure 7: Visualization of Long-Term Memory Usage. From left to right: (1) layer-wise mean long-ratio averaged over all heads and positions; (2) average long-ratio per layerhead pair aggregated over sequence positions; (3) average long-ratio per layerposition pair aggregated over attention heads. Gray-shaded layers indicate hybrid Transformer layers in the inter-layer configuration. Figure 8: Visualization of Long-Term Memory Usage. Across layers of the input-projection fusion variant. questions with web evidence; and DROP (2087) (Dua et al., 2019), Wikipedia passages requiring discrete reasoning. All datasets are in English, publicly available, and released by their original creators. They are used strictly under their intended purposes and licenses, without modification or derivative dataset creation."
        },
        {
            "title": "G Use of AI Assistants",
            "content": "AI assistants were employed solely for improving the clarity and readability of the manuscript through minor language polishing. They were not involved in study design, theoretical development, experiments, data analysis, or any other substantive aspect of this research. LAMBADA (5153) (Paperno et al., 2016), sourced from narrative books; ARC-Easy (2376) and ARCChallenge (1172) (Clark et al., 2018), consisting of science exam questions written by educators; HellaSwag (10003) (Zellers et al., 2019), constructed from activity descriptions such as WikiHow; PiQA (3084) (Bisk et al., 2020), crowdsourced for physical commonsense reasoning; and WinoGrande (1267) (Sakaguchi et al., 2019), large-scale pronoun resolution dataset generated through crowdsourcing. To assess recall-intensive abilities, we adopt FDA (1102 test samples) (Arora et al., 2024, 2023), containing annotated medical device submissions; SWDE (1111) (Arora et al., 2024, 2023; Lockard et al., 2019), curated from movie and university websites; SQuAD (2984) (Rajpurkar et al., 2018), based on Wikipedia question answering; Natural Questions (3157) (Kwiatkowski et al., 2019), sourced from Google search queries; TriviaQA (1688) (Joshi et al., 2017), composed of trivia-style"
        }
    ],
    "affiliations": [
        "Shanghai AI Laboratory",
        "The Chinese University of Hong Kong",
        "The Hong Kong University of Science and Technology (Guangzhou)",
        "Tsinghua University"
    ]
}