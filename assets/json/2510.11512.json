{
    "paper_title": "LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion Models via Likelihood Preference",
    "authors": [
        "Jianhao Yuan",
        "Fabio Pizzati",
        "Francesco Pinto",
        "Lars Kunze",
        "Ivan Laptev",
        "Paul Newman",
        "Philip Torr",
        "Daniele De Martini"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Intuitive physics understanding in video diffusion models plays an essential role in building general-purpose physically plausible world simulators, yet accurately evaluating such capacity remains a challenging task due to the difficulty in disentangling physics correctness from visual appearance in generation. To the end, we introduce LikePhys, a training-free method that evaluates intuitive physics in video diffusion models by distinguishing physically valid and impossible videos using the denoising objective as an ELBO-based likelihood surrogate on a curated dataset of valid-invalid pairs. By testing on our constructed benchmark of twelve scenarios spanning over four physics domains, we show that our evaluation metric, Plausibility Preference Error (PPE), demonstrates strong alignment with human preference, outperforming state-of-the-art evaluator baselines. We then systematically benchmark intuitive physics understanding in current video diffusion models. Our study further analyses how model design and inference settings affect intuitive physics understanding and highlights domain-specific capacity variations across physical laws. Empirical results show that, despite current models struggling with complex and chaotic dynamics, there is a clear trend of improvement in physics understanding as model capacity and inference settings scale."
        },
        {
            "title": "Start",
            "content": "Jianhao Yuan Paul Newman University of Oxford Fabio Pizzati Philip Torr Daniele De Martini Francesco Pinto Lars Kunze Ivan Laptev MBZUAI University of Chicago UWE Bristol"
        },
        {
            "title": "ABSTRACT",
            "content": "5 2 0 2 3 1 ] . [ 1 2 1 5 1 1 . 0 1 5 2 : r Intuitive physics understanding in video diffusion models plays an essential role in building general-purpose physically plausible world simulators, yet accurately evaluating such capacity remains challenging task due to the difficulty in disentangling physics correctness from visual appearance in generation. To the end, we introduce LikePhys, training-free method that evaluates intuitive physics in video diffusion models by distinguishing physically valid and impossible videos using the denoising objective as an ELBO-based likelihood surrogate on curated dataset of valid-invalid pairs. By testing on our constructed benchmark of twelve scenarios spanning over four physics domains, we show that our evaluation metric, Plausibility Preference Error (PPE), demonstrates strong alignment with human preference, outperforming state-of-the-art evaluator baselines. We then systematically benchmark intuitive physics understanding in current video diffusion models. Our study further analyses how model design and inference settings affect intuitive physics understanding and highlights domain-specific capacity variations across physical laws. Empirical results show that, despite current models struggling with complex and chaotic dynamics, there is clear trend of improvement in physics understanding as model capacity and inference settings scale."
        },
        {
            "title": "INTRODUCTION",
            "content": "Video diffusion models (VDMs) (Brooks et al., 2024; Google DeepMind, 2025; Zhou et al., 2024) have achieved impressive results in producing visually compelling videos, but they still often generate physically implausible outputs (Bansal et al., 2024; Motamed et al., 2025). Ensuring generative models learn the underlying physics that govern the dynamics of visual data is essential not only to improve the outputs quality (Kang et al., 2024; Li et al., 2025), but also to build realistic world models (LeCun, 2022; Ha & Schmidhuber, 2018) with applications in robotics (Hu et al., 2023b; Yang et al., 2023) and autonomous driving (Hu et al., 2023a; Zhao et al., 2025). However, it is challenging to evaluate how visual generative models learn and internalise physics. classic line of work on general vision models uses the violation-of-expectation paradigm (Spelke, 1985; Baillargeon et al., 1985), which frames intuitive physics understanding as the ability to judge the plausibility of observed events (Riochet et al., 2018; Weihs et al., 2022), when presented with unrealistic videos obtained in simulation. However, extending this paradigm to generative models remains challenging. More recent approaches rely on Vision Language Models (VLMs) for plausibility judgments (Bansal et al., 2024; Guo et al., 2025) and text-conditioned compliance measures (Zhang et al., 2025; Meng et al., 2024a). Although these methods provide valuable insights, they usually fail to disentangle physics from visual appearance (Motamed et al., 2025) or introduce subjective biases (Wu & Aji, 2023) and interpretive variance (Gu et al., 2024) through VLM judgments, ultimately struggling to provide grounded assessment of intuitive physics understanding. In this paper we propose an alternative evaluation method named LikePhys, that leverages the VDMs density estimation capacity (Li et al., 2023) rather than relying solely on generated outputs, as we show in Fig. 1. Inspired by the violation-of-expectation paradigm (Riochet et al., 2018), we start from the assumption that the capacity of model to assign higher likelihood to physically-plausible visual sequences is correlated to its intuitive physics understanding. Then, we use simulator to render 1Data and code are available at https://github.com/YuanJianhao508/LikePhys. 1 Figure 1: Overview of LikePhys. Our intuition is that video diffusion model with well learned underlying physics distribution should assign higher likelihoods to valid samples that obey physics laws and lower likelihoods to those invalid samples that violate them. We use Blender to create valid and invalid sample pairs through controlled physics parameters over multiple physics domains and scenarios (left). We then compare the diffusion model likelihood estimates over the constructed dataset to extract quantitative intuitive physics understanding measure, the Plausibility Preference Error (PPE) (middle). Hence, we can compute ranking of average PPE across pre-trained video diffusion models, that correlates with human preference. Lower values indicate stronger intuitive physics understanding (right). paired videos. In one, we render physically-realistic phenomena, while in the other we introduce controlled violation of physics. We keep the visual appearance consistent in the pair, ensuring that any difference resulting from processing the videos can be attributed solely to the breach of physics principles. Both rendered videos are then corrupted with noise and processed by the diffusion denoising network, where noise prediction losses serve as proxy for sample likelihood (Ho et al., 2020). We then calculate the Plausibility Preference Error (PPE), i.e. we assume positive scores if the physically plausible sample has higher likelihood, and negative otherwise. Aggregating PPE results across pairs yields single score that quantifies the intuitive physics understanding of single model. We can compare scores across models to extract ranking. To enable our evaluation, we construct synthetic benchmark of twelve scenarios spanning Rigid Body Mechanics, Continuum Mechanics, Fluid Mechanics, and Optical Effects. Using this benchmark, we rank the performance in terms of PPE of 12 state-of-the-art VDMs with verification from human preference. Furthermore, we analyse the key factors in model design and inference settings influencing their intuitive physics understanding, and examine domain-specific capacity variations across the physics domain and laws to highlight the limitations of current VDMs. Empirical results demonstrate that our proposed metric is robust proxy for physics understanding in VDMs, offering actionable insights into both their current limitations and their potential for progress. Our key contributions are the following: We propose LikePhys, training-free, likelihood-preference evaluation method for intuitive physics in VDMs with verified alignment to human preference. We benchmark pre-trained VDMs with constructed dataset including twelve physics scenarios across rigid-body mechanics, continuum mechanics, fluid mechanics, and optical effects, each designed to isolate specific physics violation under matched visual conditions. We conduct comprehensive analysis on intuitive physics understanding of state-ofthe-art VDMs, showing how architectural design and inference settings affect physics understanding, and highlighting variations in capacity across physics domains."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 VIDEO DIFFUSION MODELS VDMs have emerged as powerful generative frameworks for producing realistic video sequences (Esser et al., 2023; Brooks et al., 2024; Zhou et al., 2024; Girdhar et al., 2024). Early approaches 2 such as VDM (Ho et al., 2022), AnimateDiff (Guo et al., 2024), and ModelScope (Wang et al., 2023) relied on 3D UNets (Ronneberger et al., 2015) to demonstrate the feasibility of spatio-temporal modeling. More recent systems, including CogVideoX (Yang et al., 2024), Hunyuan T2V (Kong et al., 2024), Wan (Wan et al., 2025), and LTX (HaCohen et al., 2024), adopt Transformer-based backbones in the Diffusion Transformer (DiT) style (Peebles & Xie, 2023; Vaswani et al., 2017), advancing long-sequence generation, visual quality, and inference efficiency. Despite these advances, it remains unclear whether current models capture underlying physical principles (Motamed et al., 2025). In this work, we propose general evaluation protocol to assess the extent to which VDMs implicitly learn the laws of physics. 2."
        },
        {
            "title": "INTUITIVE PHYSICS UNDERSTANDING",
            "content": "Intuitive physics understanding is fundamental to models ability to reason about and predict scenes under physics laws (Ates et al., 2020; Bear et al., 2021; Zhan et al., 2024; Meng et al., 2024b). One important line of work builds on the violation-of-expectation paradigm (Spelke, 1985; Baillargeon et al., 1985; Margoni et al., 2024), framing physics understanding in general vision models as the ability to judge the plausibility of observed events (Smith et al., 2019; Riochet et al., 2020; Weihs et al., 2022; Garrido et al., 2025). For example, IntPhys1/2 (Riochet et al., 2018; Bordes et al., 2025) assess intuitive physics using tightly controlled synthetic video pairs that differ only by single violation of permanence, immutability, spatio-temporal continuity, or solidity to study each physics principle in isolation. When specifically applied to generative models, this protocol requires conditioned generation on context frames and computes the pixel reconstruction error on the target segment as plausibility proxy. However, it remains unclear how to adapt to text-conditioned VDMs that have no imagecondition generation capacity. Moreover, these benchmarks are relatively constrained in rigid and solid object interactions. Broader coverage of deformable objects, fluids, and optical effects is limited. 2.3 PHYSICS EVALUATION IN VIDEO GENERATIVE MODELS Some recent works focus on evaluating video generative models. popular branch of methods (Zhang et al., 2025; Guo et al., 2025; Meng et al., 2024a) uses VLM with questionanswering templates to assess adherence to physical laws in generated videos. For example, VideoPhy1/2 (Bansal et al., 2024; 2025) collect human-annotated synthetic videos and finetune VLM judges to align with human preferences on physical plausibility. However, these approaches are subject to visual appearance biases across different generative models (Wu & Aji, 2023) and interpretive variance across prompts and judging templates (Gu et al., 2024). Physics-IQ (Motamed et al., 2025) mitigates these issues through pixel-level pairwise comparisons with ground truth, but it relies on an image-conditioned architecture, and its applicability to text-to-video generation also remains unclear. To move beyond these limitations, we take inspiration from the violation-of-expectation paradigm while removing the need for conditional generation or pixel-level alignment. Our method adapts the density estimation capability (Li et al., 2023) of VDMs and performs pairwise comparisons by directly evaluating sample likelihoods through the denoising loss proxy. This design keeps the metric model-agnostic and appearance-agnostic, avoiding confounds from visual artifacts while focusing directly on physics understanding."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "We aim to evaluate the intuitive physics understanding of diffusion-based video generative models. We first define physics understanding from distributional perspective, starting from preliminary assumptions (Section 3.1) and then deriving likelihood preference score that quantifies the models ability to assign higher probability to physically valid samples than to invalid ones (Section 3.2). For the evaluation of LikePhys, we generate comprehensive dataset of controlled video sequences using simulations (Section 3.3), which we use to test the sensitivity of the model to physical plausibility. 3.1 PRELIMINARIES ON VIDEO DIFFUSION MODELS Diffusion Probabilistic Models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Nichol & Dhariwal, 2021) learn the data distribution by inverting forward Markov noising process: q(xt xt1) = 3 Figure 2: Method Overview. We prepare groups of videos via physics simulations with valid samples obeying physical laws and invalid samples containing deliberate violations. We then inject Gaussian noise into these videos and use diffusion model to predict the noise and compute the denoising loss. For each validinvalid pair, we compute likelihood preference ratio that quantifies how the model favors physically plausible sequences, serving as proxy for physics understanding. (cid:0)xt; 1 βt xt1, βtI(cid:1), βt (0, 1), parameterised model then learns the reverse process I(cid:1) by minimising noise prediction loss which serves as an pθ(xt1 xt) = (cid:0)xt1; µθ(xt, t), σ2 ELBO-based surrogate for the negative log-likelihood. The noise prediction loss is calculated by reconstructing the ground truth noise ϵ injected on xt with denoising network ϵθ, following: Ldenoise(θ; xt) = Et,ϵ (cid:13)ϵ ϵθ(xt, t)(cid:13) (cid:13) 2 (1) (cid:13) VDMs built on the framework that models dynamics in sequence of frames x0 RF CHW . The denoising network is therefore designed to capture both spatial structure within each frame and temporal relationship across frames, allowing the model to implicitly learn motion dynamics in addition to appearance. (cid:2) log pθ(x0)(cid:3) + const. Ex0 3.2 PHYSICS UNDERSTANDING AS LIKELIHOOD PREFERENCE Our intuition is to formalise intuitive physics understanding in distributional perspective. Let pphys(x) denote the distribution over videos that strictly obey physical laws, where is video sequence. We define its support as Mphys = (cid:8) : pphys(x) > 0(cid:9). By definition, any valid sample x+ Mphys satisfies real-world physical laws, whereas any invalid sample / Mphys violates some of them. We consider diffusion model pθ has perfect intuitive physics understanding if, for every validinvalid pair (x+, x), pθ(x+) > pθ(x). Hence, if diffusion model has learned the distribution correctly, it should assign higher likelihood to valid samples when they are paired with visually matched invalid samples. As shown in Eq. (1), the denoising loss is an ELBO-based likelihood surrogate. Thus, lower denoising loss corresponds to higher likelihood under pθ. For any validinvalid pair (x+, x), we therefore have pθ(x+) > pθ(x) Ldenoise(θ; x+) < Ldenoise(θ; x). To evaluate this likelihood-preference protocol in practice, we collect two sets of controlled video pairs following the violation-of-expectation paradigm (Margoni et al., 2024; Bordes et al., 2025): (3) (2) + = {x+ }M j=1, = {x }N k=1, (4) For each pair (x+, x), we perturb both with the same Gaussian noise at sampled diffusion timestep, pass them through the denoising network to predict the noise, and average the loss over timesteps as in Fig. 2. We say the model has violated expectation on the pair (x+ ) if it assigns no greater likelihood to the valid sample than to the invalid one. Aggregating over all validinvalid pairs, we define the Plausibility Preference Error (PPE): , PPE = 1 (cid:88) (cid:88) 1 j= k=1 (cid:104) Ldenoise(θ; x+ ) Ldenoise(θ; ) (cid:105) . (5) Figure 3: Evaluation benchmark. We organise 12 scenarios derived from four physical domains (Rigid Body Mechanics, Fluid Mechanics, Continuum Mechanics, Optical Effects) with their relative proportions (left). Rows from top to bottom (middle and right) show examples from four optical phenomena, rigid-body mechanics, continuum mechanics, and fluid mechanics. We display valid simulation (middle) and corresponding invalid variants (right) for physics violation in each domain. lower PPE indicates that pθ consistently prefers valid samples. Thus, it measures the models likelihood preference for physically plausible videos, ultimately serving as proxy for the learned intuitive physics. 3.3 EVALUATION BENCHMARK CONSTRUCTION To rigorously evaluate the learned physics, we require controlled video pairs that differ only in physical validity. In practice, it is difficult to obtain such matched pairs from natural data where physical laws cannot be systematically violated. We therefore construct synthetic simulation benchmark spanning 12 physical scenarios across 4 domains, which allows precise control over physics parameters. All videos are rendered in Blender (Blender, 2018) at 512 512 resolution with 60 frames. Each scenario is generated with 10 variations that vary both physics parameters and visual factors (e.g., object shape, texture, or environment). Within each variation, valid video strictly obeys the relevant law, while invalid variants introduce single controlled violation (e.g., superelastic bounce or ghosted shadows). By holding camera angle, lighting, textures, and object geometry constant within single variation, the valid and invalid videos differ only in physical plausibility, ensuring that any measured likelihood gap can be attributed to physics violations. As shown in Fig. 3, benchmark composition is as follows (see Sec. for more details): Rigid Body Mechanics: five cases including Ball Collision, Ball Drop, Block Slide, Pendulum Oscillation, and Pyramid Impact. They cover collision dynamics, periodic motion, gravity, and energy transfer. Valid examples conserve momentum and energy, follow free-fall under gravity, and maintain shape and continuity. Invalid variants break these rules through anomalies such as excessive restitution, teleportation, or implausible energy transfer. Continuum Mechanics: two cases including Cloth Drape and Cloth Waving. They capture material deformation under gravity and aerodynamic forces. Valid examples show natural folds and consistent surface behavior. Invalid variants disrupt realism through penetrations, distortions, or scrambled temporal sequences. Fluid Mechanics: three cases including Droplet Fall, Faucet Flow, and River Flow. They address conservation of mass, viscosity, and surface tension. Valid examples generate continuous and plausible fluid patterns. Invalid variants introduce reversed flows, discontinuities, spontaneous mass changes, or temporal glitches. Optical Effects: two cases including Moving Shadow and Orbit Shadow. They capture lightobject interactions. Valid examples show smooth and consistent shadow behaviour. Invalid variants break physical plausibility by inverting, erasing, or misaligning shadows or by adding temporal inconsistencies. Table 1: Model ranking. Plausibility Preference Error (%) across twelve controlled physics scenarios for various video diffusion models. Lower values indicate stronger physics understanding. Models are ordered by increasing average performance (Avg) across all scenarios. Best and second-best per scenario are bold and underlined. Rigid Body Mechanics Cont. Mechanics Fluid Mechanics Optical Effects Ball Collision Ball Drop Block Slide Pendulum Oscillation Pyramid Impact Cloth Drape Cloth Waving Faucet Flow Droplet Fall River Flow Orbit Shadow Moving Shadow Avg. AnimateDiff AnimateDiff SDXL ZeroScope ModelScope Mochi CogVideoX5B CogVideoX-2B Wan2.1-T2V-1.3B LTX v0.9.5 CogVideoX1.55B Wan2.1-T2V-14B Hunyuan T2V 63.3 63.3 53.3 51.7 40.0 43.3 40.0 43.3 36.7 61.7 41.7 33.3 60.0 66.7 55.0 53.3 50.0 41.7 38.3 53.3 58.3 50.0 56.7 51.7 65.0 38.3 46.7 46.7 31.7 61.7 60.0 66.7 35.0 51.7 61.7 25.0 51.7 70.0 58.3 66.7 65.0 65.0 63.3 20.0 68.3 31.7 13.3 21.7 61.1 68.9 73.3 78.9 84.4 83.3 83.3 40.0 18.9 25.6 43.3 50. 52.9 47.1 38.6 40.0 28.6 38.6 35.7 65.7 47.1 52.9 62.9 35.7 78.6 82.9 84.3 84.3 82.9 81.4 81.4 32.9 48.6 22.9 34.3 77.1 62.0 54.0 46.0 40.0 38.0 36.0 36.0 60.0 32.0 54.0 56.0 38.0 63.3 60.0 61.7 61.7 60.0 53.3 50.0 33.3 28.3 21.7 15.0 40.0 44.0 46.0 40.0 40.0 44.0 40.0 40.0 78.0 40.0 68.0 64.0 68.0 71.7 43.3 40.0 33.3 58.3 23.3 18.3 23.3 75.0 23.3 16.7 38. 56.0 32.0 42.0 38.0 40.0 30.0 32.0 60.0 48.0 62.0 60.0 44.0 60.8 56.0 53.3 52.9 51.9 49.8 48.2 48.0 44.7 43.8 43.8 43."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We evaluate the intuitive physics understanding of pre-trained VDMs using LikePhys and perform systematic analysis to answer the following key questions: (1) Which models exhibit the strongest intuitive physics as measured by PPE? (2) How well does PPE align with human preference in physics correctness of generation? (3) To what extent does PPE disentangle visual appearance from intuitive physics? (4) What model design and inference factors influence models intuitive physics? (5) Across which physics domains and laws do current models fall short? Settings: We benchmark pre-trained text-to-video diffusion models including AnimateDiffSDv1.5/SDXL (Guo et al., 2024), ModelScope/ZeroScope (Wang et al., 2023), Mochi (Team, 2024), CogVideoX-2B/5B (Yang et al., 2024), Wan2.1-1.3B/14B (Wan et al., 2025), Hunyuan T2V (Kong et al., 2024) and LTX (HaCohen et al., 2024). To maximise performance, we adopt each models recommended inference settings from its official repository and resample the frame rate and spatial resolution of generated video samples to match. We then perform zero-shot evaluation using the proposed LikePhys protocol, measuring performance via the PPE metric defined in Eq. (5) and ranking models. For each physics scenario, we report the average error over all validinvalid pairs. For the likelihood estimation, we use the same DDIM Euler (Song et al., 2020) scheduler with 10 timesteps uniformly sampled through the noise scale. We use fixed prompt template for each physics scenario. More details of the evaluation implementation (Sec. A), evaluation dataset construction and specification (Sec. B), inference settings (Sec. C), experiment settings (Sec. D), additional ablation study on design factors of evaluation method (Sec. E), further applicability to other intuitive physics benchmarks (Sec. F) and visual samples (Sec. G) are provided in the appendix. 4.1 MAIN RESULTS Model ranking In Tab. 1, we benchmark twelve VDMs and order them by increasing PPE. The results reveal large performance gaps across architectures. Early UNet-based models (Ronneberger et al., 2015) such as AnimateDiff and ZeroScope often record error rates above 50%, reflecting limited ability to distinguish physically valid videos from invalid ones. In contrast, more recent DiT-based designs (Peebles & Xie, 2023), including the top three performers Hunyuan T2V (43.6%), Wan2.1T2V14B (43.8%), and CogVideoX1.55B (43.8%), achieve substantially lower errors across multiple scenarios (see Sec. 4.2 for further discussion). Moreover, while there is clear trend of improvement in recent architectures, only handful of models significantly achieve PPE lower than the 50% random-guess threshold in more than few scenarios, potentially due to simplicity biases, where physically implausible interactions appear visually easier to model than valid ones (Shah et al., 2020; Geirhos et al., 2020). These results highlight that while modern VDMs start to internalise physical principles, their performance varies Table 2: Correlation to Human Preference. Kendalls τ correlation between Plausibility Preference Error and state-of-the-art physics evaluators to human annotation across physics scenarios. Ball Collision VideoPhy VideoPhy2 Qwen2.5 VL LikePhys 20.0 -19.4 39.4 34.4 Ball Drop -11.0 -32.2 59.0 42. Block Slide Pendulum Oscillation Pyramid Impact Cloth Drape Cloth Waving Faucet Flow Droplet Fall River Flow Orbit Shadow Moving Shadow Overall 26.7 -6.8 -25.4 24. -13.8 -15.9 -19.4 15.0 -20.9 -3.1 14.2 55.9 57.6 51.0 49.5 15.4 15.3 10.5 -15.2 0.0 26.3 -20.0 29.1 51.0 -6.4 -41.0 -22.2 -8. 40.8 6.8 26.9 16.8 47.5 70.4 45.2 36.4 -9.6 -33.9 -33.9 12.1 38.9 -8.5 33.3 44.4 substantially across different physics domains (see Sec. 4.3 for further analysis), indicating significant room for progress in physically-accurate training of video generators. Alignment to Human Preference Moreover, we conduct human study to assess how well PPE aligns with human judgments of physical consistency in generated videos (see Sec. D.2 for details). For each model, we generated 120 samples using the same prompts that describe the designed physics scenarios. Following the VideoPhy annotation protocol (Bansal et al., 2025), human annotators rated each video on 15 scale, where 1 indicates severe violation of physics and 5 indicates strong consistency. We then derived ranking of models from the aggregated human scores and compared it against the PPE-based ranking of LikePhys, measuring their agreement with Kendalls τ . We further compare PPE with state-of-the-art automatic evaluators, including human-aligned VLM such as VideoPhy (Bansal et al., 2024) and VideoPhy2 (Bansal et al., 2025), as well as Qwen 2.5 VL (Bai et al., 2025) as general VLM prompted for evaluating physical correctness. We find that PPE achieves stronger overall correlation with human annotations, demonstrating the competitiveness of our zero-shot, training-free approach. In particular, the resulting correlation of τ = 0.44 indicates that models with lower PPE also tend to be judged by humans as more physically consistent. Corr. w. PPE Table 3: Pearson correlation between Plausibility Preference Error (PPE) and visual quality metrics. Disentanglement of Visual Appearance While we verify the correlation between PPE and physics consistency of generated videos, we examine whether it also correlates with established visual quality metrics from VBench (Huang et al., 2024) reported in Tab. 3. By doing so, we aim to determine whether perceptual metrics already capture physical plausibility or represent an independent dimension of evaluation. As shown in the table, aesthetic quality has no significant correlation (r = 0.05) to PPE, indicating our metric targets physical correctness rather than aesthetic appeal. Subject consistency (r = 0.01) and background consistency (r = 0.01) show very weak correlations, consistent with the fact that the feature similarity-based metric does not fully capture physics correctness of dynamic motion. In contrast, motion smoothness (r = 0.15) and temporal flickering (r = 0.12) have moderate positive correlations with PPE. This is expected, as smooth, flicker-free motion can be partial indicator of physical plausibility. Overall, these results confirm that our method evaluates aspects of physical reasoning that are largely orthogonal to visual quality measures, providing complementary insights into video generative models capabilities and failure modes. Aesthetic Quality Subject Consistency Background Consistency Motion Smoothness Temporal Flickering -0.05 -0.01 -0.01 0.15 0.12 4.2 ANALYSIS ON INFLUENCING FACTORS Architecture and Data. We analyse the physics understanding with respect to the scaling of model parameters and training data by plotting the average PPE in correlation to model parameters and training samples. As shown in Fig. 4, larger models consistently outperforms smaller variants. Largest models are almost exclusively DiT-based, showing the scalability of these architectures in capturing complex spatiotemporal dependencies. Overall, the average PPE also decreases with larger training dataset size, indicating the effectiveness of architecture advancement and scaling to improve intuitive physics understanding. Please note that Hunyuan T2V does not disclose the training dataset cardinality. 7 Figure 4: Analysis on influencing factors. Effect of model size, showing steady overall decline in PPE (top left). Effect of training data size, where larger corpora generally yield lower error, though the correlation is weaker than that of model size (bottom left). Effect of context window size, showing consistent improvement in physics understanding as the window increases (top right). Effect of classifier-free guidance (CFG) strength, indicating that physics understanding remains largely stable across different strengths (bottom right). Figure 5: Analysis across physics domains and laws. On the left, we report detailed PPE across the four physics domains. Fluid Mechanics cases exhibit both the highest average error, while Rigid Body and Continuum Mechanics scenarios show moderate errors; Optical Effects cases lie in between. On the right, we map our domains to seven physics laws for fine-grained analysis. Temporal continuity and conservation of energy show wide variation and higher median errors, whereas geometric invariance and optical consistency are handled more reliably. Role of Number of Frames. Physical dynamics unfold over time, and the number of output frames is critical design parameter for video diffusion models physics understanding. As shown in Fig. 4, most models exhibit steady decrease in PPE as the number of frames increases. This trend suggests that providing longer temporal context allows the model to capture more complex interactions and improves its implicit physical reasoning. Role of Classifier-free Guidance. Classifier-free guidance (CFG) (Ho & Salimans, 2022) is widely used to balance fidelity and diversity in diffusion sampling, but its effect on physics understanding remains unclear. We evaluate models across varying CFG strengths and find that guidance scale has only marginal impact on physics understanding. This insensitivity suggests that, unlike visual quality metrics, physics plausibility is governed primarily by the learned model distribution, while inference-time calibration of noise prediction plays only minor role. Consequently, CFG can be tuned for visual quality without compromising the models physics understanding."
        },
        {
            "title": "4.3 ANALYSIS ACROSS PHYSICS DOMAINS AND LAWS",
            "content": "We investigate VDMs physics understanding across different domains and laws presented in Sec. 3.3. As shown in Fig. 5, we report the average PPE across all models for each of the four proposed domains. Rigid Body and Continuum Mechanics show moderate errors with tight agreement across models, while Fluid Mechanics exhibits both higher median error and greater variability, where faucet flow or droplet fall scenes yield 30 to 40% PPE, but complex river flows often exceed 70%. Optical Effects incur the lowest errors, likely because large-scale image and video corpora strongly constrain photometric and geometric regularities. These findings indicate challenges in modelling nonlinear, multiscale dynamics. We further analyse performance by physics law. Following the mapping in Sec. D.1, we group common invalid failure modes into seven laws and report the average PPE for each law. As shown in Fig. 5, temporal continuity shows the largest variance across models, indicating unstable long-range coherence when motion spans many frames or occlusions occur. Spatial continuity and conservation principles such as momentum and mass also yield high errors, which is consistent with the absence of global constraints in standard training objectives and samplers. In contrast, geometric invariance and optical consistency are better satisfied, likely because they align with priors learned from abundant static imagery and short clips. Material response remains moderately challenging, reflecting difficulty at contact events, frictional interactions, and surface compliance that require accurate high-frequency detail. Overall, models capture several structural and photometric regularities but still struggle with laws that require global coupling through time and space, suggesting future work on longer context training, multiscale memory, and physics-aware objectives that promote conservation and continuity."
        },
        {
            "title": "5 DISCUSSION AND LIMITATION",
            "content": "In LikePhys, by leveraging the violation of expectation paradigm united with the density estimation capacity of video diffusion model, we ensure that any difference in denoising loss arises purely from physical implausibility in controlled video pairs. Since generative models can use different spatial and temporal resolutions and inherit style biases from training data, generation-based metrics often capture appearance noise unrelated to physics. In contrast, we compare density estimates for each controlled pair within the same diffusion model, which removes constant biases and makes the evaluation directly comparable across architectures and model scales. Our approach also has limitations and relies on specific assumptions, that we discuss below. Training data. key assumption underlying our approach is the model training data predominantly obey real-world physical laws, since the model learns to approximate the data distribution. We believe this assumption is reasonable, given that widely used dataset such as HD-VILA-100M (Xue et al., 2022), and Panda-70M (Chen et al., 2024) consist mainly of real-world recordings rather than animated or synthetic content. Training on such data should encourage generative models to capture true physical dynamics, forming prerequisite for video generative models that aim to function as physical world simulators. Cost of Test Data Curation One limitation of our method is that it relies on controlled physics violations, either within our designed benchmark or in existing intuitive physics datasets. Extending to broader range of scenarios would require additional data curation, which is more costly compared to text-conditioned generationbased evaluation protocols. On the other hand, our approach offers the unique advantage of precisely controlling validinvalid pairs to probe specific laws and failure patterns, something that text-conditioned generation cannot guarantee. Requirement on Accessing Noise Prediction Another limitation is that our method requires access to the noise prediction error of the model, which makes it difficult to evaluate closed-source models (Google DeepMind, 2025; Brooks et al., 2024). However, this requirement is less restrictive in the open source community. More importantly, our proposed method can be used as an indicator for monitoring training progress and selection of checkpoints during model release."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We introduced LikePhys, training-free likelihood-preference evaluation method and benchmark for assessing the intuitive physics understanding of text-to-video diffusion models. Our systematic 9 evaluation shows that while current models still fall short in intuitive physics in complex and chaotic dynamics and exhibit varying performance across different physics domains, clear progress emerges with model scale, training data size, and inference settings, underscoring practical directions toward physics-aware generative modelling."
        },
        {
            "title": "REFERENCES",
            "content": "Tayfun Ates, Samil Atesoglu, Cagatay Yigit, Ilker Kesen, Mert Kobas, Erkut Erdem, Aykut Erdem, Tilbe Goksun, and Deniz Yuret. Craft: benchmark for causal reasoning about forces and interactions. arXiv preprint arXiv:2012.04293, 2020. 3 Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 7, 17 Renee Baillargeon, Elizabeth Spelke, and Stanley Wasserman. Object permanence in five-month-old infants. Cognition, 20(3):191208, 1985. 1, 3 Hritik Bansal, Zongyu Lin, Tianyi Xie, Zeshun Zong, Michal Yarom, Yonatan Bitton, Chenfanfu Jiang, Yizhou Sun, Kai-Wei Chang, and Aditya Grover. Videophy: Evaluating physical commonsense for video generation. arXiv preprint arXiv:2406.03520, 2024. 1, 3, 7, 17 Hritik Bansal, Clark Peng, Yonatan Bitton, Roman Goldenberg, Aditya Grover, and Kai-Wei Chang. Videophy-2: challenging action-centric physical commonsense evaluation in video generation. arXiv preprint arXiv:2503.06800, 2025. 3, 7, Daniel Bear, Elias Wang, Damian Mrowca, Felix Binder, Hsiao-Yu Fish Tung, RT Pramod, Cameron Holdaway, Sirui Tao, Kevin Smith, Fan-Yun Sun, et al. Physion: Evaluating physical prediction from vision in humans and machines. arXiv preprint arXiv:2106.08261, 2021. 3 Blender. Blender - 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018. URL http://www.blender.org. 5 Florian Bordes, Quentin Garrido, Justine Kao, Adina Williams, Michael Rabbat, and Emmanuel Dupoux. Intphys 2: Benchmarking intuitive physics understanding in complex synthetic environments. arXiv preprint arXiv:2506.09849, 2025. 3, 4 Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 1, 2, 9 Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1332013331, 2024. Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. In Proceedings of the Structure and content-guided video synthesis with diffusion models. IEEE/CVF international conference on computer vision, pp. 73467356, 2023. 2 Quentin Garrido, Nicolas Ballas, Mahmoud Assran, Adrien Bardes, Laurent Najman, Michael Rabbat, Emmanuel Dupoux, and Yann LeCun. Intuitive physics understanding emerges from self-supervised pretraining on natural videos. arXiv preprint arXiv:2502.11831, 2025. 3 Robert Geirhos, Jorn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665673, 2020. 6 Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Factorizing text-to-video generation by explicit image conditioning. In European Conference on Computer Vision, pp. 205224. Springer, 2024. 2 Google DeepMind. Veo: text-to-video generation system. Technical Report Veo-3 Tech Report, 2025. 1, 9 Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024. 1, 3 Xuyang Guo, Jiayan Huo, Zhenmei Shi, Zhao Song, Jiahao Zhang, and Jiale Zhao. T2vphysbench: first-principles benchmark for physical consistency in text-to-video generation. arXiv preprint arXiv:2505.00337, 2025. 1, 3 Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. International Conference on Learning Representations, 2024. 3, 6 David Ha and Jurgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2(3), 2018. Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. 3, 6 Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 8 Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2, 3 Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. Gaia-1: generative world model for autonomous driving. arXiv preprint arXiv:2309.17080, 2023a. 1 Yingdong Hu, Fanqi Lin, Tong Zhang, Li Yi, and Yang Gao. Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning. arXiv preprint arXiv:2311.17842, 2023b. 1 Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2180721818, 2024. 7 Joel Jang, Seonghyeon Ye, Zongyu Lin, Jiannan Xiang, Johan Bjorck, Yu Fang, Fengyuan Hu, Spencer Huang, Kaushil Kundalia, Yen-Chen Lin, et al. Dreamgen: Unlocking generalization in robot learning through video world models. arXiv preprint arXiv:2505.12705, 2025. 17 Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, and Jiashi Feng. How far is video generation from world model: physical law perspective. arXiv preprint arXiv:2411.02385, 2024. 1 Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 3, Yann LeCun. path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1):162, 2022. 1 Alexander C. Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, and Deepak Pathak. Your diffusion model is secretly zero-shot classifier. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 22062217, October 2023. 1, 3, 16 11 Dacheng Li, Yunhao Fang, Yukang Chen, Shuo Yang, Shiyi Cao, Justin Wong, Michael Luo, Xiaolong Wang, Hongxu Yin, Joseph Gonzalez, et al. Worldmodelbench: Judging video generation models as world models. arXiv preprint arXiv:2502.20694, 2025. 1 Francesco Margoni, Luca Surian, and Renee Baillargeon. The violation-of-expectation paradigm: conceptual overview. Psychological Review, 131(3):716, 2024. 3, 4 Fanqing Meng, Jiaqi Liao, Xinyu Tan, Wenqi Shao, Quanfeng Lu, Kaipeng Zhang, Yu Cheng, Dianqi Li, Yu Qiao, and Ping Luo. Towards world simulator: Crafting physical commonsense-based benchmark for video generation. arXiv preprint arXiv:2410.05363, 2024a. 1, 3 Fanqing Meng, Wenqi Shao, Lixin Luo, Yahong Wang, Yiran Chen, Quanfeng Lu, Yue Yang, Tianshuo Yang, Kaipeng Zhang, Yu Qiao, et al. Phybench: physical commonsense benchmark for evaluating text-to-image models. arXiv preprint arXiv:2406.11802, 2024b. 3 Saman Motamed, Laura Culp, Kevin Swersky, Priyank Jaini, and Robert Geirhos. Do generative video models learn physical principles from watching videos? arXiv preprint arXiv:2501.09038, 2025. 1, 3 Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, pp. 81628171. PMLR, 2021. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. 3, 6 Ronan Riochet, Mario Ynocente Castro, Mathieu Bernard, Adam Lerer, Rob Fergus, Veronique Izard, and Emmanuel Dupoux. Intphys: framework and benchmark for visual intuitive physics reasoning. arXiv preprint arXiv:1803.07616, 2018. 1, 3, 19 Ronan Riochet, Josef Sivic, Ivan Laptev, and Emmanuel Dupoux. Occlusion resistant learning of intuitive physics from videos. arXiv preprint arXiv:2005.00069, 2020. 3 Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pp. 234241. Springer, 2015. 3, Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli. The pitfalls of simplicity bias in neural networks. Advances in Neural Information Processing Systems, 33:95739585, 2020. 6 Kevin Smith, Lingjie Mei, Shunyu Yao, Jiajun Wu, Elizabeth Spelke, Josh Tenenbaum, and Tomer Ullman. Modeling expectation violation in intuitive physics with coarse probabilistic object representations. Advances in neural information processing systems, 32, 2019. 3 Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265. PMLR, 2015. 3 Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. 6 Elizabeth Spelke. Preferential-looking methods as tools for the study of cognition in infancy. 1985. 1, 3 Genmo Team. Mochi 1. https://github.com/genmoai/models, 2024. 6 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 3 12 Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 3, 6 Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. ModelScope Text-to-Video Technical Report. arXiv preprint arXiv:2308.06571, 2023. 3, 6 Luca Weihs, Amanda Rose Yuile, Renee Baillargeon, Cynthia Fisher, Gary Marcus, Roozbeh Mottaghi, and Aniruddha Kembhavi. Benchmarking progress to infant-level physical reasoning in ai. TMLR, 2022. 1, 3 Minghao Wu and Alham Fikri Aji. Style over substance: Evaluation biases for large language models. arXiv preprint arXiv:2307.03025, 2023. 1, 3 Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Advancing high-resolution video-language representation with large-scale video transcriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 50365045, 2022. 9 Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators. arXiv preprint arXiv:2310.06114, 1(2):6, 2023. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 3, 6 Guanqi Zhan, Chuanxia Zheng, Weidi Xie, and Andrew Zisserman. general protocol to probe large vision models for 3d physical understanding. In Advances in Neural Information Processing Systems, volume 37, pp. 4346843498, 2024. 3 Chenyu Zhang, Daniil Cherniavskii, Andrii Zadaianchuk, Antonios Tragoudaras, Antonios Vozikis, Thijmen Nijdam, Derck WE Prinzhorn, Mark Bodracska, Nicu Sebe, and Efstratios Gavves. Morpheus: Benchmarking physical reasoning of video generative models with real physical experiments. arXiv preprint arXiv:2504.02918, 2025. 1, 3 Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Xinze Chen, Guan Huang, Xiaoyi Bao, and Xingang Wang. Drivedreamer-2: Llm-enhanced world models for diverse driving video generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 1041210420, 2025. 1 Yuan Zhou, Qiuyue Wang, Yuxuan Cai, and Huan Yang. Allegro: Open the black box of commerciallevel video generation model. arXiv preprint arXiv:2410.15458, 2024. 1,"
        },
        {
            "title": "LLM USAGE",
            "content": "We use LLM for grammar polishing in paper writing. No text, code, or results were accepted without author verification. All technical content, claims, and conclusions are the authors own."
        },
        {
            "title": "A DETAILS ON EVALUATION PROTOCOL",
            "content": "We provide more details on the implementation of our likelihood-preference evaluation protocol. Algorithm 1 Likelihood Preference Based Physics Understanding Evaluation Require: Physics scenario subgroups, each with: + = set of physically valid videos = set of physically invalid videos; Pretrained diffusion model pθ; Number of diffusion timesteps Tmax; Number of noise samples per timestep Nϵ; Step 1: Compute denoising losses for all videos for each video + do Ensure: Plausibility Preference Error 1: Initialize error count 0 2: Initialize total comparisons 0 3: for each physics subgroup do 4: 5: 6: 7: 8: 9: 10: Ldenoise(x) 0 for = 1 to Tmax do for = 1 to Nϵ do Sample noise ϵ (0, I) Compute noisy frame xt αt + 1 αt ϵ 11: 12: 13: 14: 15: Predict ˆϵ ϵθ(xt, t) Accumulate end for end for Normalize: Ldenoise(x) Ldenoise(x) + ϵ ˆϵ2 2 Ldenoise(x) Ldenoise(x) Tmax Nϵ end for Step 2: Pairwise comparisons between valid and invalid videos for each x+ + do for each do + 1 if Ldenoise(x+) > Ldenoise(x) then + 1 {model prefers invalid over valid} 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: end for 27: Compute error rate: end if end for end for T 28: return E"
        },
        {
            "title": "B DETAILS ON BENCHMARK DATASET GENERATION",
            "content": "We provide details on each of the constructed physics scenarios, including the settings and the implementation of how the invalid cases are generated. 14 Ball Collision: Two spheres roll toward each other and collide elastically on flat surface, conserving momentum and energy. In invalid variants, restitution is altered (e.g. perfectly inelastic sticking or super-elastic energy amplification); one sphere penetrates the other at impact; phantom forces are applied during collision; sphere radii change mid-impact; one sphere teleports through the other; or collision timing is scrambled (temporal disorder). Ball Drop: sphere drops from fixed height onto rigid floor and bounces back under gravitational energy conservation. In invalid variants, the spheres color changes mid-flight; its radius is dynamically rescaled during free fall; it bounces higher than gravity permits (over-bounce); it penetrates the floor; it teleports to different height; or its temporal sequence is disrupted (temporal disorder). Block Slide: rectangular block slides down an inclined plane under friction and gravity, following Newtons laws. In invalid variants, the block hovers above the plane; it moves erratically, breaking Newtonian motion; spurious jitter is injected; its dimensions change as it slides; it teleports partway down the slope; or its time evolution is scrambled (temporal disorder). Cloth Drape: rectangular cloth drapes over cylindrical support, responding to gravity and bending forces while preserving surface continuity. In invalid variants, the cloths color changes mid-simulation; it penetrates the cylinder or ground; it folds in impossible ways, violating material continuity; it behaves like rigid sheet with no natural flutter; or its temporal evolution is scrambled (temporal disorder). Cloth Waving: piece of cloth attached to vertical pole flutters under constant wind force, exhibiting realistic wave propagation and strain. In invalid variants, sections freeze instantly; fragments shatter mid-wave; parts teleport from one side to the other; an impossible 180 twist is imposed; it explodes outward as if under internal pressure; or its motion breaks into non-physical jump cuts (temporal disorder). Pyramid Impact: cube drops onto pyramid of stacked spheres, transferring energy and knocking spheres off under gravity. In invalid variants, collision energy is artificially amplified or damped (energy-conservation violation); holes appear in the pyramid (mass continuity violation); spheres or the cube teleport through each other (spatial discontinuity); or gravity is negated so objects float unpredictably. Pendulum Oscillation: pendulum bob swings on rigid rod under constant gravity, following consistent periodic arc. In invalid variants, the rod breaks mid-swing (path discontinuity); the bob disappears for segments; its trajectory deviates from the circular path; time intermittently freezes (temporal disorder); or the length/frequency varies randomly (frequency variation). Droplet Fall: single droplet falls under gravity, forming coherent drop and splash while conserving mass and momentum. In invalid variants, antigravity forces make the droplet rise; the stream fragments into disconnected blobs; fluid mass is spontaneously created or removed (mass-conservation violation); viscosity becomes negative or oscillates; particles self-attract unnaturally; or its temporal sequence is scrambled (temporal disorder). Faucet Flow: faucet pours water into transparent tank, forming continuous stream that collects and splashes, conserving mass and momentum. In invalid variants, fluid color shifts mid-flow; the stream fractures into non-coalescing droplets; viscosity becomes negative or oscillates; fluid mass is injected or removed arbitrarily (mass-conservation violation); phase shifts alternate liquid-gas instantly; particles self-attract unnaturally; droplets teleport across the tank; or temporal order is scrambled (temporal disorder). River Flow: Water flows steadily downstream along riverbed, exhibiting realistic laminar or turbulent patterns under gravity. In invalid variants, the flow fractures into isolated droplets; an invisible barrier blocks water without visual cues; fluid mass vanishes mid-flow; phases shift liquidsolidliquid; or timestamps jump (temporal disorder). Moving Shadow: solid object moves across ground plane under fixed illumination, producing consistent, smoothly moving shadow. In invalid variants, the shadow inverts onto the ceiling; it vanishes entirely; it appears without its caster; its shape mismatches the object; it teleports away; or its temporal sequence is scrambled (temporal disorder). 15 Orbit Shadow: shadow orbits around an object under fixed lighting, tracing smooth circular path. In invalid variants, the shadow inverts direction or plane; it vanishes mid-orbit; it detaches from its caster; its geometry distorts implausibly; it teleports along its path; or its temporal order is scrambled (temporal disorder)."
        },
        {
            "title": "C ADDITIONAL DETAILS OF INFERENCE SETTINGS",
            "content": "Our evaluation leverages off-the-shelf video diffusion pipelines in zero-shot setting. For each model, we adopt the officially recommended spatial and temporal resolutions as shown in Tab. 4 and apply uniform noise scheduler (Euler discrete) over fixed number of timesteps (T = 10) as uniform sampling found optimal in Diffusion Classifier (Li et al., 2023). Each input clip is first uniformly sampled to the prescribed frame count and resized to the models native (H ) resolution. The resulting sequence is encoded into latent representation by the models VAE. To approximate the models internal likelihood, we inject Gaussian noise at evenly spaced timesteps, run the noised latent through the diffusion backbone (with classifier-free guidance at the specified scale), and record the mean-squared error between the true noise and the networks prediction. Averaging these per-step errors yields scalar loss that serves as proxy for log pθ(x). For each physics scenario, valid and invalid variants are processed identically, and their losses are compared pairwise. The Plausibility Preference Error is computed as the fraction of validinvalid pairs in which the invalid sample attains lower denoising loss than the valid one. All sources of randomness (Python, NumPy, PyTorch, CUDA/cuDNN) are fixed for the video-pairs but different across subgroups via global seed to ensure full reproducibility. Table 4: Spatial and temporal settings for zero-shot inference. Model Height Width FPS #Frames AnimateDiff AnimateDiff SDXL CogVideoX 2/5B Zeroscope / ModelScope Wan2.1-T2V (1.3B/14B) Hunyuan (I2V / T2V) LTX variants 512 1024 480 320 480 320 480 512 1024 720 576 832 512 704 16 16 16 16 16 16 25 16 16 49 24 33 61 161 All the model evaluated are text-conditioned video diffusion models, we use prompt template for each scenario that describes the physically valid event, and shared negative prompt to discourage spurious artifacts: \"worst quality, inconsistent motion, blurry, jittery, distorted\". Ball Collision: \"two balls colliding with each other\" Ball Drop: \"ball dropping and colliding with the ground, in empty background\" Block Slide: \"a block sliding on slope\" Pendulum Oscillation: \"a pendulum swinging\" Pyramid Impact: \"a cube crash into pile of spheres\" Cloth Drape: \"a piece of cloth dropping to the obstacle on the ground\" Cloth Waving: \"a piece of cloth waving in the wind\" Droplet Fall: \"a droplet falling\" Faucet Flow: \"fluid flowing from faucet\" River Flow: \"fluid flowing in tank with obstacles\" 16 Moving Shadow: its shadow\" \"light source moving around an object showing Orbit Shadow: \"camera moving around an object\""
        },
        {
            "title": "D ADDITIONAL EXPERIMENTAL SETTING DETAILS",
            "content": "D.1 CLASSIFICATION OF PHYSICS LAWS We provide details on how we classify and summarise specific physics laws in Tab. 5. D.2 HUMAN STUDY EXPERIMENT DETAILS For the human study experiment, we generate 10 samples per physics scenario per model using the same prompts as shown in Sec. with prompt enhancement from GPT5. Human annotators are asked to rate each video on scale of 15, following the annotation protocol of VideoPhy2 (Bansal et al., 2025), where higher scores indicate stronger physical consistency. For baseline physics evaluator comparison, we leverage VideoPhy1 (Bansal et al., 2024), which rates videos on Physics Consistency (PC) and Semantic Adherence (SA) using 01 scale. Following the official evaluation protocol (Bansal et al., 2024), we obtain per-model scores by filtering out samples with PC 0.5 or SA 0.5, and then computing the proportion of samples that pass this joint criterion. For VideoPhy2, which rates PC and SA on 15 scale, we follow the official evaluation practice (Bansal et al., 2025) by filtering out samples with Physics Consistency 4 or Semantic Adherence 4, and then calculating the proportion of valid samples after filtering. For Qwen2.5-VL (Bai et al., 2025), we follow the evaluation protocol of DreamGen (Jang et al., 2025). Each video is rated on 01 scale using the following prompt: Does the video show good physics dynamics and showcase good alignment with the physical world? strict judge. If it breaks the laws of physics, please answer 0. Answer 0 for No or 1 for Yes. only 0 or 1. Please be Reply D.3 INFLUENCING FACTOR EXPERIMENT DETAILS We conduct two ablation experiments on protocol design factors: context-window length and classifierfree guidance strength. We select five representative scenarios (Ball Drop, Block Slide, Fluid, Cloth and Shadow) from our taxonomy, and five exemplar models: AnimateDiff, ModelScope, CogVideoX, Hunyuan T2V and LTX. For number of frame experiment, we vary the number of input frames in {8, 16, 24, 32, 40, 48, 56, 64}. AnimateDiff supports up to 32 frames, so we limit its trials accordingly. For CFG experiment, we evaluate multiple classifier-free guidance scales from 1.0 to 8.0 to measure how guidance scale affects the models ability to discriminate valid from invalid physics cases."
        },
        {
            "title": "E ABLATION ON EVALUATION PROTOCOL DESIGN",
            "content": "Beyond analysing model capacity, we also assess the robustness of our evaluation protocol by examining two key design factors that influence implicit physics understanding. Timestep Selection. To accurately approximate the likelihood, we inject Gaussian noise at multiple timesteps. To adopt proper sampling strategy, we investigate the denoising loss difference between valid and invalid sample pairs for four representative scenarios across the entire noise schedule. We found that (1) the overall preference trend is consistent across most timesteps for each model, and (2) the timesteps at which validinvalid separation peaks vary by model and scenario. These observations imply that oversampling any single region could bias our estimate; therefore, we adopt uniform sampling of ten timesteps per paired comparison, striking balance between computational efficiency and stable estimation. Prompt Robustness. For text-to-video diffusion models, we also evaluate sensitivity to the choice of text prompt. For the same four scenarios, we create eight representative prompt variants. We then 17 Table 5: Classification Criterion for Each Physics Law Law / Principle Scenario Variation key Temporal Continuity Ball Collision Ball Drop Block Slide Cloth Drape Faucet Flow Cloth Waving Droplet Fall Pendulum Oscillation Pyramid Impact River Flow Moving Shadow Orbit Shadow temporal disorder temporal disorder temporal disorder temporal disorder temporal disorder temporal disorder temporal disorder temporal disorder temporal disorder temporal disorder temporal disorder temporal disorder Spatial Continuity Ball Collision Ball Drop Block Slide Cloth Waving Pyramid Impact River Flow Faucet Flow teleportation teleportation teleportation flag teleport teleporting spheres invisible wall teleporting fluid Conservation of Energy Ball Drop Ball Collision Ball Collision Ball Drop Cloth Waving Droplet Fall Pyramid Impact Droplet Fall over bounce momentum amplification phantom force dynamic scaling elastic explosion self attracting momentum multiplication non-conservation momentum Conservation of Mass Droplet Fall Droplet Fall Droplet Fall Faucet Flow Droplet Fall River Flow non-conservation fluid matter creation negative viscosity fracturing fluid phase shifting fluid non-conservation fluid Geometric Invariance Ball Collision Block Slide Pyramid Impact Pyramid Impact Orbit Shadow size change size changing phase shifting sphere fusion varying size Optical Consistency Moving Shadow Moving Shadow Moving Shadow Moving Shadow Orbit Shadow inverted shadow no shadow no object wrong shadow shape impossible reflection Material Response Cloth Drape Cloth Drape Cloth Drape Block Slide Block Slide Block Slide Cloth Waving impossible folding rubber cloth ground penetration hovering irregular motion jittering flag shatter report the mean plausibility-preference error across prompts, with the standard deviation capturing sensitivity to prompt choice. As shown in Fig. 6, we observe no significant change in discriminative 18 Figure 6: Ablation on Evaluation Protocol Design Factors. Left: Denoising loss differences between valid and invalid sample pairs across 1000 uniformly sampled timesteps for four representative physics scenarios. Right: PPE across multiple text-prompt variants, demonstrating robustness to prompt choice. performance when varying the prompt. This indicates that using the likelihood estimation from the text-conditioned distribution under different reasonable prompts would give similar performance. This robustness arises from our controlled data-generation process, which isolates only the physical-law compliance variable as the only error source."
        },
        {
            "title": "F APPLICABILITY TO OTHER INTUITIVE PHYSICS BENCHMARKS",
            "content": "We further demonstrate that LikePhys applies beyond our main setups by evaluating on the IntPhys Development split (Riochet et al., 2018). The Dev split contains three blocks including O1 (object permanence), O2 (shape constancy), and O3 (spatio-temporal continuity). Each constructed as matched pairs of possible against impossible events. We run LikePhys unchanged and report PPE per block and averaged across blocks. As shown in Tab. 6, models exhibit distinct profiles across principles (e.g., some perform better on continuity in O3 than on permanence in O1), mirroring the domain-specific trends we observed on our rigid-body and optics evaluations. This supports the portability of LikePhys and its ability to surface principle-level strengths and weaknesses without task-specific tuning."
        },
        {
            "title": "G VISUAL SAMPLES",
            "content": "Table 6: IntPhys Dev (O1/O2/O3) with LikePhys. We report PPE (%, lower is better) for each block and the average across blocks. Model O3 O1 O2 Avg. Animatediff SDXL 45.8 48.3 Wan2.1-T2V-14B 46.7 LTX v0.9.5 40.0 ModelScope 42.5 ZeroScope 43.3 CogVideoX 45.0 Animatediff 40.0 CogVideoX-5B 35.8 Hunyuan T2V 59.2 51.7 49.2 52.5 48.3 49.2 43.3 45.8 30.0 48.3 52.5 46.7 49.2 50.8 46.7 43.3 45.0 25.8 51.1 50.8 47.5 47.2 47.2 46.4 43.9 43.6 30.5 We further provide visual samples in the constructed benchmark for each scenario as mentioned in Sec. B. As shown in Fig. 7, Fig. 8, and Fig. 9. We show one valid and two invalid cases for each scenario for illustration. 19 Figure 7: Visual samples from Ball Drop, Ball Collision, Pendulum Oscillation, and Block Slide scenarios. 20 Figure 8: Visual sample from Droplet Fall, Faucet Flow, Cloth Drape, Cloth waving scenarios. 21 Figure 9: Visual sample from River Flow, Moving Shadow, Pyramid Impact, Orbit Shadow scenario."
        }
    ],
    "affiliations": [
        "MBZUAI",
        "UWE Bristol",
        "University of Chicago",
        "University of Oxford"
    ]
}