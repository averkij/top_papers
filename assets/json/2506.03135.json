{
    "paper_title": "OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models",
    "authors": [
        "Mengdi Jia",
        "Zekun Qi",
        "Shaochen Zhang",
        "Wenyao Zhang",
        "Xinqiang Yu",
        "Jiawei He",
        "He Wang",
        "Li Yi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Spatial reasoning is a key aspect of cognitive psychology and remains a major bottleneck for current vision-language models (VLMs). While extensive research has aimed to evaluate or improve VLMs' understanding of basic spatial relations, such as distinguishing left from right, near from far, and object counting, these tasks represent only the most fundamental level of spatial reasoning. In this work, we introduce OmniSpatial, a comprehensive and challenging benchmark for spatial reasoning, grounded in cognitive psychology. OmniSpatial covers four major categories: dynamic reasoning, complex spatial logic, spatial interaction, and perspective-taking, with 50 fine-grained subcategories. Through Internet data crawling and careful manual annotation, we construct over 1.5K question-answer pairs. Extensive experiments show that both open- and closed-source VLMs, as well as existing reasoning and spatial understanding models, exhibit significant limitations in comprehensive spatial understanding. We further analyze failure cases and propose potential directions for future research."
        },
        {
            "title": "Start",
            "content": "OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models Mengdi Jia1 Zekun Qi14 Xinqiang Yu4 Jiawei He4 Shaochen Zhang2 He Wang Wenyao Zhang34 Li Yi167 5 2 0 2 3 ] . [ 1 5 3 1 3 0 . 6 0 5 2 : r 1Tsinghua University 2Xian Jiaotong University 3Shanghai Jiao Tong University 4Galbot 5Peking University 6Shanghai Qi Zhi Institute 7Shanghai AI Laboratory"
        },
        {
            "title": "Dataset",
            "content": "Figure 1: Overview of OmniSpatial Benchmark."
        },
        {
            "title": "Abstract",
            "content": "Spatial reasoning is key aspect of cognitive psychology and remains major bottleneck for current vision-language models (VLMs). While extensive research has aimed to evaluate or improve VLMs understanding of basic spatial relations, such as distinguishing left from right, near from far, and object counting, these tasks represent only the most fundamental level of spatial reasoning. In this work, we introduce OmniSpatial, comprehensive and challenging benchmark for spatial reasoning, grounded in cognitive psychology. OmniSpatial covers four major categories: dynamic reasoning, complex spatial logic, spatial interaction, and perspective-taking, with 50 fine-grained subcategories. Through Internet data crawling and careful manual annotation, we construct over 1.5K question-answer pairs. Extensive experiments show that both openand closed-source VLMs, as well as existing reasoning and spatial understanding models, exhibit significant limitations in comprehensive spatial understanding. We further analyze failure cases and propose potential directions for future research. Preprint. Equal contribution. Corresponding author."
        },
        {
            "title": "Introduction",
            "content": "Spatial reasoning plays crucial role in bridging visual observation to robotic action [43, 88, 89], autonomous driving, and AR/VR. For models to execute tasks effectively, they must understand spatial relationships to determine appropriate actions. To enhance the spatial understanding of VisionLanguage Models (VLMs), prior works [15, 17, 11, 68, 102, 123, 121, 89] have integrated spatial information into datasets, enabling basic forms of spatial reasoning. Various benchmarks [26, 104, 101, 48, 34, 102, 121, 89] have been introduced to systematically evaluate such capabilities, focusing on tasks like recognizing left and right, estimating depth, and constructing cognitive maps [121, 106, 72]. Additionally, spatial reasoning has been applied to manipulation tasks [25, 8, 131, 20, 123], allowing systems to position objects according to specified spatial rules [89]. Despite these advances, existing approaches largely focus on basic spatial understanding, which typically includes identifying simple spatial relations such as relative positions (left, right, front, back), proximity (near, far), and object-level counting. Recent models, including state-of-the-art reasoning systems like ChatGPT o3 [79] and Gemini-2.5-Pro [97], have achieved over 90% accuracy on benchmarks such as SpatialBot-Bench [11] and EmbSpatial [26], suggesting that these basic tasks are approaching saturation, as shown in Fig. 1. However, complex spatial reasoning remains significant challenge [35, 5, 83, 50, 82, 16]. Human interaction with the physical world often involves interpreting ambiguous, dynamic, and contextdependent spatial relationships [7, 109, 95]. For example, in an emergency, knowing that an AED is to the right of the door is insufficient without understanding schematic diagrams, correlating maps with real-world environments, and planning an efficient route. Similarly, tasks like inserting knife into rack or flattening box demand reasoning about object rotation, deformation, and spatial compatibilityfar beyond static object placement. From the perspective of cognitive psychology, complex spatial reasoning extends beyond basic relations and includes dynamic world knowledge reasoning, interactive spatial behavior with environments or agents, logical analysis of 3D spatial structures, and perspective-taking abilities. Therefore, to address these limitations and systematically assess higher-order spatial reasoning, we propose OmniSpatial, comprehensive benchmark capturing the breadth and depth of spatial cognition. OmniSpatial categorizes spatial reasoning into four key dimensions: dynamic reasoning, complex spatial logic, spatial interaction, and perspective-taking, providing foundation for advancing the next generation of spatially-aware & physics-aware AI systems. Dataset Embodied Task Categories Data Domain Data Annotation Data Scale Spatial QAs EmbSpatial-Bench [26] Space3D-Bench [104] Visual Spatial [61] SpatialRGPT-Bench [17] BLINK-Spatial [34] Whats up [48] Spatial-MM [101] RoboSpatial [102] SpatialVLM [15] SpatialBot-Bench [11] VSI-Bench [121] OmniSpatial (Ours) 6 6 7 12 14 6 4 4 2 5 8 Indoor (ScanNet, etc.) Indoor (Replica) MSCOCO Urban, Indoor, Sim MSCOCO Household, GQA, COCO Internet Indoor, tabletop WebLi COCO,VG,RTX Indoor Internet Template Manual Template Template Manual Template Template Template Template Manual Template Manual 2.2K 211 10K 1.4K 286 5K 2.3K 1M 546 200 288 1387 3.6K 1K 10K 1.4K 286 5K 2.3K 3M 546 360 5K 1.5K Table 1: Comparison with other spatial reasoning benchmarks. comparison between OmniSpatial and other existing spatial reasoning benchmarks reveals that, under the inclusion of Embodied Frames Compatibility, OmniSpatial avoids template-based annotations, features highly diverse data, and includes significantly larger number of tasks. The OmniSpatial benchmark includes images or video frames across diverse scenes, resolutions, lighting conditions, and weather patterns, collected from multiple countries across different continents. We evaluate state-of-the-art VLMs on our benchmark. Our findings indicate that, while current models perform well on conventional benchmarks, OmniSpatial presents significantly greater challenge due to its comprehensive and complex task design. Our key contributions are as follows: 2 We categorize visual-spatial reasoning into four key dimensionsdynamic reasoning, complex spatial logic, spatial interaction, and perspective-takingbroadening the scope of evaluation and guiding future research on spatial cognition in embodied & physics intelligence. We develop the OmniSpatial dataset, which offers diverse and challenging set of spatial tasks, serving as comprehensive benchmark for assessing VLMs spatial reasoning capabilities. We explore the enhancement of spatial reasoning in VLMs by incorporating auxiliary models in chain-of-thought manner, demonstrating improved reasoning performance through this approach."
        },
        {
            "title": "2 Preliminaries: Visualâ€“Spatial Reasoning",
            "content": "Spatial reasoning constitutes the cognitive bridge between visual perception and action planning in embodied agents. We define visualspatial reasoning as the capacity of an artificial system to infer, predict, and manipulate spatial properties of the world from visual observations. Formally, let an RGB observation stream be I1:T and task-specific query be q. model possesses visualspatial reasoning if it can learn mapping : (I1:T , q) a, where belongs to well-defined action or answer space whose correctness can be verified in the physical or simulated environment. This definition excludes non-visual priors so that improvements can be attributed to visual reasoning itself, yet it remains compatible with multi-modal extensions discussed in 1. 2.1 Taxonomy of VisualSpatial Reasoning Figure 2: Benchmark Statistics of OmniSpatial: The distribution of tasks across 4 main categories. Our classification stems from two complementary motivations. (i) Cognitive-psychology foundations. Decades of research on spatial cognition isolate partly independent facultiese.g., spatial visualization, mental rotation, perspective taking, and spatial updatingthat can be psychometrically evaluated in humans [13, 71]. These constructs offer principled scaffold for analysing how agents perceive, reason about, and act within space. (ii) The need to move beyond basic spatial relations. Current spatial benchmarks are nearly saturated on tasks such as leftright discrimination, frontback identification, and object counting [15, 11, 17, 26, 104]. Real-world embodied tasks, however, require richer reasoning [50, 82, 16, 103, 53] about scene dynamics, multi-step spatial logic, physically grounded interaction, and viewpoint transformation. Guided by these two strands, we partition spatial reasoning into four orthogonal yet complementary dimensionsdynamic reasoning, complex spatial logic, spatial interaction, and perspective taking. Each dimension aligns with specific cognitive faculty and targets distinct class of spatial challenges that is under-represented in prior work, enabling us to construct benchmarks that probe the full spectrum of complex spatial cognition while remaining anchored in established psychological theory. Dynamic Reasoning involves predicting motion and temporal changes in spatial environments. This capability is crucial for tasks such as robotic control and autonomous navigation, where anticipating object trajectories ensures adaptive decision-making in dynamic contexts. Complex Spatial Logic encompasses high-level reasoning about spatial relationships, transformations, and geometric structures. It is fundamental to problem-solving in domains such as engineering, design, and robotic manipulation, where understanding spatial configurations and predicting structural changes are essential. Spatial Interaction focuses on engagement with spatial environments, where reasoning is guided by environmental constraints and task objectives. This includes applications such as path planning, obstacle avoidance, and real-time decision-making based on spatial feedback. 3 Figure 3: Tasks Demonstration of OmniSpatial. Several representative subtasks are selected for demonstration in each of the four task categories. Note: The questions above are slightly simplified for clarity and conciseness. Perspective Taking refers to the ability to adopt different viewpoints, which is crucial for navigation, social cognition, and spatial awareness. It allows for understanding spatial relationships from multiple perspectives, facilitating decision-making in multi-agent environments and adaptive problem-solving. 2.2 Rationale for Classification This taxonomy is designed to be both theoretically comprehensive and practically applicable. Dynamic reasoning emphasizes motion prediction, which is critical for navigation and interaction. Complex spatial logic provides foundation for advanced cognitive tasks involving transformation and abstraction. Spatial interaction highlights real-time engagement with environments, while perspective taking captures cognitive flexibility in spatial reasoning. By structuring visual-spatial reasoning in this manner, we establish framework that enhances the evaluation of spatial cognition models and their applications in artificial intelligence, robotics, and human cognition research."
        },
        {
            "title": "3 OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark",
            "content": "3.1 Overview We introduce OmniSpatial, benchmark for the quantitative evaluation of VLMs in visual-spatial reasoning. Unlike large-scale benchmarks focused on few tasks, OmniSpatial prioritizes task diversity and structured categorization over sheer dataset size. It consists of 1533 high-quality question-answer pairs, designed to challenge models beyond simple pattern matching or statistical learning, ensuring that spatial reasoning capabilities are tested rigorously, even with limited data. Our dataset is curated from web searches, standardized tests, driving exam questions, and existing datasets including MME [57], HOI4D [67], etc.This diverse sourcing enhances the realism, complexity, and cross-domain generalization of the tasks. Web-sourced images cover natural environments, architecture, and daily life, adding visual complexity. Standardized tests from psychology and cognitive science provide scientifically rigorous spatial reasoning challenges. Driving exam questions introduce real-world dynamic interactions, such as road rule comprehension and motion prediction. Additionally, datasets like HOI4D contribute multi-resolution, varying illumination, and viewpoint diversity, as well as embodied intelligence tasks involving human-object interactions. 4 Figure 4: Data Construction of OmniSpatial.The pipeline collects images from multiple sources and ensures their quality and relevance through manual selection. Precise annotations are then applied, ensuring each question has clear, unique answer while maintaining natural, conversational expression. This process supports the effective training of VLMs in spatial reasoning tasks. To ensure comprehensive evaluation, we categorize tasks into 4 major spatial reasoning types, further divided into 50 fine-grained task categories, as shown in Figs. 3 and 7. For example, perspectivetaking tasks range from simple spatial judgments (Is the opponents weapon to the left or right of the blue athlete?) to complex motion prediction (The blue athlete is blocking an incoming attack from the red athletewhat will happen next?). Spatial interaction tasks include both static collision assessments (Is the vehicle too close to the one in front?) and dynamic environment reasoning (A car door ahead is openinga pedestrian exiting?). This multi-level task structure ensures models are tested on diverse and progressively complex spatial reasoning challenges. We emphasize precise image-task alignment in both dataset selection and annotation. Each questionanswer pair is manually curated and reviewed in multiple rounds to ensure accuracy, consistency, and minimal ambiguity. Unlike automated annotation, manual refinement guarantees high correlation between visual scenes and spatial reasoning tasks, strengthening OmniSpatials reliability as benchmark for future research. 3.2 Benchmark Construction This section describes how we collect data and annotations for OmniSpatial, as illustrated in Fig. 4. 3.2.1 Data Collection As outlined in Section 2, our study defines 4 spatial categories with corresponding task types. To ensure diverse and information-rich dataset, we design targeted search strategies for each task type, optimizing data collection for relevance, diversity, and complexity. Web Images Web images constitute major portion of our dataset, particularly for Perspective Taking, Dynamic Reasoning, and Spatial Interaction tasks. To ensure relevance and diversity, we formulate task-specific search terms reflecting distinct spatial scenarios, such as indoor layout and furniture arrangement for spatial relationship tasks. To minimize AI-generated image interference, we append filters like -ai and -generated to search queries. Using Googles Custom Search JSON API, Web Robotic Process Automation (Web RPA), and manual searches, we retrieve thousands of images efficiently. rigorous screening process is followed to eliminate irrelevant, low-resolution, or overly compressed images. We also removed images lacking spatial complexity, such as isolated static objects in Spatial Interaction tasks. The final dataset balances visual complexity and realism, enhancing its generalization capability across multiple tasks and scenarios while ensuring high-quality training data for VLMs. All the images are under MIT or CC-BY 4.0 licence. Exam-Based Test Questions Designed to assess complex spatial reasoning, these questions emphasize abstract spatial logic, including 3D transformations, rotations, and perspective shifts. We collect wide range of publicly available spatial cognition tests via web scraping and manual curation. Noting 5 Figure 5: PointGraph: Enhance Spatial Reasoning through Additional Scene Graphs. Figure 6: SpatialCoT: Enhancing Spatial Imagination through Novel View Synthesis. variations in task focus, difficulty, and reasoning style, we categorized the questions to maintain balanced representation. To prevent overrepresentation of any single reasoning type, we curate the dataset to ensure diverse question formats. Redundant, overly similar, or knowledge-intensive questions are removed, prioritizing those that purely test spatial reasoning. This refinement process significantly enhances the datasets quality and challenge level, providing robust evaluation framework for VLMs. Driving Test Questions These questions evaluate spatial interaction in dynamic environments, focusing on distance estimation, right-of-way rules, and hazard detection. We source them from three primary channels: (i) Image-based multiple-choice questions from driving test websites in at least three countries, capturing diverse traffic rules and scenarios. (ii) Online question banks covering standardized spatial judgment tasks such as turning, lane changes, and parking decisions. (iii) Interactive driving videos from U.S. test resources, where we extract frames, annotate bounding boxes, and formulate contextual questions (Which bounding box highlights potential traffic risk?). This multi-source approach ensures realistic and challenging dataset, enhancing VLM adaptability in traffic-related spatial reasoning. Existing Datasets We integrate two key datasets: MME [57] and HOI4D [67]. MME provides RGBD data, allowing depth-based spatial inference. We leverage its depth information for physics-based questions such as If red car passes me in 5 seconds, what speed should maintain? This ensures realistic distance and motion-based reasoning. HOI4D contains extensive human-object interaction videos. We extract sequential frames to create motion prediction tasks, such as Where will the hand holding the kettle move next? By incorporating these datasets, we introduce real-world motion and interaction complexities, further strengthening VLMs dynamic reasoning capabilities. 3.2.2 Question-Answer Annotation We employ multiple-choice questions, including binary (true/false) and four-option formats, ensuring standardized evaluation and minimizing annotation errors. While adhering to structured formats, we prioritize natural and contextually rich language to prevent overly templated expressions. Instead of rigid, mathematical phrasing (e.g., Is [A] object on the right of [B] object?), We use conversational styles more aligned with real-world reasoning (e.g., If youre entering the classroom, on which side are the students?). This approach encourages models to rely on contextual and relative spatial references rather than predefined templates. To maintain clarity, we conduct multiple rounds of validation, resolving ambiguities in referents and ensuring unique answers. In particular, we ensure the quality of the data by means of cross-validation among different annotators. We employ six human annotators for data labeling. All annotators receive comprehensive training to ensure thorough understanding of the spatial reasoning tasks, enabling them to provide unbiased and discriminative assessments. All the annotators are from the authors of this work. 3. Improving Visual Spatial Reasoning Abilities Given the diversity of spatial reasoning tasks in OmniSpatial, we further explore methods to enhance VLMs spatial reasoning capabilities. 3.3.1 PointGraph: Enhancing Spatial Reasoning via Point Relationships We attempt to employ existing expert models, such as the Segment Anything Model [49], to partition images into distinct regions. This process extracts pixel clusters corresponding to multiple objects in the image, allowing models to estimate object centers and perform subsequent spatial reasoning tasks 6 Dynamic Reasoning Spatial Interaction Complex Logic Perspective Taking Method Avg. Rank Manipulate Motion Analysis Traffic Analysis Locate Geospatial Strategy Pattern Recognition Geometric Reasoning Ego Centric Allo Centric Hypothetical Blind Evaluation Random Choice 24.98 GPT-3.5-turbo [98] 30.67 GPT-4-turbo [77] 34.06 Proprietary Models GPT-4o-mini-2024-07-18 [45] 42.64 GPT-4o-2024-11-20 [45] 47.81 GPT-4.1-nano-2025-04-14 [78] 42.62 GPT-4.1-mini-2025-04-14 [78] 48.87 GPT-4.1-2025-04-14 [78] 51.78 Claude-3-5-sonnet-20241022 [3] 46.86 Claude-3-7-sonnet-20250219 [3] 47.53 Gemini-2.0-flash-lite-02-05 [2] 44.03 Gemini-2.0-flash-exp [2] 48.40 Gemini-2.5-flash-preview-05-20 [2] 52.12 Reasoning Models o1-2024-12-17 [46] 50.36 o4-mini-2025-04-16 [79] 52.77 o3-2025-04-16 [79] 56.33 Claude-3-7-sonnet-20250219-thinking [3] 48.62 Gemini-2.5-flash-05-20-thinking [2] 53.16 Gemini-2.5-pro-preview-05-06 [2] 55. - - - 8 5 9 4 2 6 5 8 2 1 6 3 1 7 3 2 Open-source Models LLaVA-onevision-qwen2-72B [54] 45.66 LLavA-1.5-vicuna-7B [64] 34.97 15 LLaVA-onevision-qwen2-7B [54] 35.68 14 6 Gemma-3-4B [47] 39.79 11 8 Gemma-3-12B [47] 43.71 Gemma-3-27B [47] 44.75 7 InternVL3-2B [130] 37.98 13 9 InternVL3-8B [130] 41.60 5 InternVL3-14B [130] 45.94 2 InternVL3-38B [130] 48.48 1 InternVL3-78B [130] 49.33 Qwen-VL2.5-3B [115] 40.30 10 Qwen-VL2.5-7B [115] 39.18 12 4 Qwen-VL2.5-32B [115] 47.36 3 Qwen-VL2.5-72B [115] 47. Specialized Spatial Reasoning Models SpaceMantis-13B [15] 36.36 SpaceQwen2.5-VL-3B [15] 40.25 SpaceThinker-Qwen2.5VL-3B [15] 40.42 SpatialBot-3B [11] 35.68 RoboPoint-vicuna-v1.5-7B-lora [123] 35.85 RoboPoint-vicuna-v1.5-13B [123] 34.60 SoFar-Qwen2.5VL-3B [89] 45.14 Human Evaluation Human 92.63 6 3 2 6 6 5 1 - 24.86 38.38 42.97 55.95 65.54 50.90 64.32 66.22 54.05 57.57 59.19 61.89 67.57 71.62 72.97 71.89 57.21 70.27 67.57 54.46 43.24 62.16 41.89 54.05 56.76 50.00 52.43 54.32 63.42 63.78 55.41 58.38 63.06 58.38 47.03 58.11 47.84 43.24 57.03 55.68 56.49 26.30 29.19 37. 50.29 57.23 53.85 56.53 64.74 54.57 55.95 46.71 56.01 62.72 60.98 59.83 66.18 59.73 64.74 71.39 31.23 38.15 50.29 49.71 54.91 55.78 40.58 40.87 60.17 63.58 63.12 47.51 35.09 55.09 60.12 36.59 39.88 53.06 38.15 28.61 28.15 51.16 25.88 38.35 41.18 23.43 28.76 28. 54.59 56.47 54.90 59.06 60.00 58.12 56.71 60.24 51.76 68.24 57.65 60.00 61.18 53.73 61.18 62.35 35.29 32.94 54.12 56.47 54.12 57.65 43.29 48.94 50.35 54.59 56.24 46.12 50.12 51.76 50.12 40.94 41.18 43.29 32.94 34.82 42.82 54.12 43.43 52.38 40.95 60.19 65.33 68.38 63.81 49.52 63.43 73.33 63.81 73.33 68.57 67.94 72.38 75. 36.19 29.52 60.95 27.62 47.62 50.48 40.00 51.05 51.81 58.29 59.24 42.29 45.33 66.29 59.81 34.86 40.95 35.43 29.52 37.33 32.19 53.14 27.27 36.91 40.00 44.91 54.09 42.42 56.36 60.18 53.09 59.09 53.27 59.09 60.91 60.00 61.82 65.45 57.27 58.18 64.55 33.94 41.82 56.36 36.36 45.45 52.73 40.55 44.77 51.45 50.55 51.45 44.73 44.00 56.91 53. 33.09 40.91 38.73 41.82 40.55 32.55 52.73 21.44 0.82 22.27 22.47 26.29 24.40 29.28 31.75 26.60 29.48 21.65 20.82 38.14 39.18 34.02 40.21 30.24 35.05 43.30 29.01 28.87 22.68 23.71 16.49 27.84 21.86 24.95 28.04 29.90 27.63 32.16 31.13 26.39 26.19 22.27 29.90 24.33 28.87 29.90 24.12 31. 24.77 24.00 26.32 29.42 25.48 30.11 30.19 30.06 31.74 28.39 31.23 33.81 34.19 27.10 36.77 29.68 28.17 36.13 34.84 24.18 22.58 25.81 24.52 30.32 29.03 28.52 28.63 28.26 28.52 30.19 23.87 29.42 27.48 33.03 24.39 25.81 28.00 22.58 22.71 27.74 22.88 22.55 24.84 42.16 33.67 31.37 33. 61.57 36.76 75.98 39.49 53.59 37.23 72.55 39.57 70.98 40.64 70.00 34.79 72.16 36.06 66.47 36.81 72.75 39.20 75.49 35.90 71.57 38.03 73.53 40.69 77.06 48.40 68.63 37.94 74.12 40.96 74.51 38.03 55.60 34.66 47.06 36.17 76.47 37.23 59.80 36.17 63.73 36.70 64.71 33.51 55.49 35.11 64.20 38.62 68.04 35.37 72.16 36.76 74.51 38.46 59.41 33.30 64.51 33.19 68.04 37.50 71.37 36.81 49.22 38.25 63.73 38.83 58.04 35.11 47.06 36.17 50.20 38.72 49.02 37.66 71.60 36.56 25.78 35.90 35.42 34.22 39.76 33.73 39.28 39.04 39.52 36.63 38.80 39.28 33. 36.14 40.96 48.19 36.95 32.53 37.35 36.14 37.35 33.73 38.55 33.73 32.53 33.01 40.96 34.46 33.49 35.90 30.84 37.35 40.24 36.39 39.28 39.76 31.08 37.35 40.96 33.49 41.69 96.53 97.30 92. 97.14 94.55 91.30 87.63 99.02 95.74 93. Table 2: Evaluation on OmniSpatial. All models were tested 5 times and averaged to reduce randomness. Dark green indicates the best result within the group, and light green indicates the second-best result within the group. Gemini-2.5-Pro [97], InternVL3-78B [130], and SoFar [89] achieve the best performance within their respective groups. Among the various spatial intelligence categories, Complex Logic is the most challenging. more effectively. As shown in Fig. 5, We attempt to enhance the spatial relationship comprehension of VLMs by explicitly incorporating additional spatial information cues into the models. 3.3.2 SpatialCoT: Enhancing Spatial Imagination via Novel View Synthesis We observe that VLMs are largely constrained by textual reasoning paradigms, whereas humans inherently possess robust spatial imagination, commonly known as mental imagery. To some extent, this capability of spatial imagination corresponds to novel view synthesis of objects or scenes. Fortunately, recent advances in 3D generative models [100, 118, 87, 66] can perform such tasks effectively. In our work, we employ InstantMesh [118] to synthesize novel views from input images, providing additional visual cues to enhance spatial imagination."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Evaluation Setup To assess the spatial reasoning capabilities of existing VLMs, we conduct comprehensive experiments on OmniSpatial. This evaluation compares both proprietary and open-source models across various spatial tasks, using standardized benchmarks to ensure fairness and reproducibility. 7 Dynamic Reasoning Spatial Interaction Complex Logic Perspective Taking Method Avg. Improve Manipulate Motion Analysis Traffic Analysis Locate Geospatial Strategy Pattern Recognition Geometric Reasoning Ego Centric Allo Centric Hypothetical GPT-4.1-mini - (w/o CoT) 48.86 (w/ Zero-shot CoT) 49.81 +0.95 (w/ Manual CoT) 49.76 +0.90 (w/ PointGraph) 50.49 +1.63 Gemini-2.5-Flash - (w/o CoT) 51.47 (w/ Zero-shot CoT) 51.53 +0.06 (w/ Manual CoT) 52.12 +0.65 (w/ PointGraph) 53.23 +1. - - - - - Qwen-VL2.5-3B - - (w/o CoT) 41.45 -0.81 (w/ Zero-shot CoT) 40.64 -1.38 (w/ Manual CoT) 40.07 (w/ PointGraph) 44.36 +2.91 - 64.05 62.97 65.68 67. - 66.22 63.51 67.57 62.16 - 58.65 59.73 55.68 55.68 - 58.55 58.96 58.90 62.14 - 65.90 61.27 62.72 69.94 - 43.06 43.87 46.65 55.20 - 57.65 59.06 58.59 57. - 63.53 58.82 68.24 64.71 - 39.53 46.12 47.29 48.94 - 59.43 62.48 64.38 64.76 - 71.43 67.62 73.33 67.62 - 50.67 48.38 40.57 52.19 - 56.91 58.55 56.91 58. - 66.36 65.45 60.91 59.09 - 48.73 43.27 46.00 52.36 - 28.87 27.63 28.45 28.87 - 32.99 42.27 38.14 29.90 - 32.78 25.36 28.04 29.90 - 34.06 32.52 32.13 30. - 34.84 34.84 34.19 38.06 - 22.58 22.84 24.39 25.55 - - 68.82 37.18 69.41 40.11 69.61 39.31 70.59 38.83 - - 70.59 31.91 79.41 35.90 75.49 35.90 74.51 37.77 - - 61.96 37.66 59.61 36.54 60.39 33.35 66.08 35.11 - 41.20 40.96 41.20 42. - 38.55 32.53 33.73 37.35 - 37.35 37.59 31.57 31.08 Table 3: Camparison of textual Chain-of-Thought and PointGraph on OmniSpatial. Benchmark Models We evaluate range of state-of-the-art vision-language models, including: Proprietary Models: These include GPT-4o series [45], GPT-4.1 series, Claude series [3], and Gemini series [2], which are state-of-the-art multimodal models accessed via an API under zeroshot settings with standardized prompts. All the system prompts can be found at Appendix D. Reasoning Models: We refer to current models employing internal long CoT as reasoning models. These models typically leverage reinforcement learning to enhance their reasoning capabilities, including models such as the o1, o3, o4-mini [46, 79], and Gemini-2.5 series [2, 97]. Due to the explicit nature of CoT, we utilize an additional text-based evaluation model [98] to determine the correctness of responses, similar to LLM-as-a-Judge [129]. Open Source Models: Our open-source set comprises Qwen-VL series [6], InternVL series [130], Gemma series [47] and LLaVA-Onevision series [63]. These models are currently the most advanced open-source VLMs. All models are deployed locally and use standardized prompts. Specialized Spatial Models: These models, including SpatialVLM [15], RoboPoint [123], SpatialBot [11], and SoFar [89], are designed specifically for spatial reasoning tasks. SpatialVLM integrates metric spatial information through 3D data synthesis and scene segmentation. RoboPoint and SpatialBot emphasize the role of point affordance in spatial reasoning. 4.2 Evaluation Metrics We evaluate the spatial reasoning capabilities of models by measuring their accuracy on multiplechoice questions. For standard open-source and closed-source models, we experiment with four approaches: enforcing direct output result, regular expression parsing, JSON format parsing and LLM-as-a-Judge [129]. For Reinforcement Learning-based reasoning models, due to the inherent challenges in enforcing structured output, we employ GPT-4.1-mini to assess the correctness of the models responses by comparing them with the ground truth. More detailed ablation studies are discussed in the Appendix C.1. 4.3 Main Results Overall Model Performance As illustrated in Table 2, we observe the following findings: (i) Proprietary reasoning models, such as ChatGPT o3 [79] and Gemini-2.5-pro [2], achieve the highest performance, surpassing 56% overall success rate; however, there remains significant gap compared to human-level understanding, and require lot of inference time and tokens. (ii) Open-source models also demonstrate competitive results, with large-scale models like InternVL3-78B [130] and Qwen-VL2.5-72B [115] achieving comparable performance to GPT-4.1-mini and Gemini-2.0-flashexp. (iii) Specialized Spatial Reasoning Models, due to limitations in dataset coverage and model capacity, struggle to achieve substantial improvements on comprehensive benchmarks. Category-wise Analysis Breaking down the results, we observe notable performance differences across spatial reasoning categories: (i) Leveraging their extensive world knowledge and local understanding capabilities, proprietary models have demonstrated strong performance in Dynamic Reasoning and Spatial Interaction, indicating that reasoning models possess high proficiency in temporal understanding, spatial relationship analysis, and map-based comprehension. (ii) For Pattern Geometric and Recognition Reasoning, which involve spatial imagination in planar geometry, even reasoning models designed for extended thinking can only achieve an accuracy of around 30% to 40%, slightly surpassing the random baseline. (iii) Current models exhibit limited Perspective Taking abilities, predominantly analyzing scenarios from an ego-centric viewpoint while struggling to imagine perspectives from others viewpoints. Hypothetical Ego Centric Allo Centric Method Avg. Improve GPT-4.1-mini (w/ Zero-shot CoT) 45.56 Impact of PointGraph & Spatial CoT To test whether structured segmentation improves performance, we apply PointGraph as preprocessing step for GPT-4.1, Gemini-2.5-flash and Qwen-VL2.5-7B. Results in Section 4.2 show clear accuracy boost, particularly in the Dynamic Reasoning and Perspective-Taking Track, validating the benefits of integrating structured object representation, while traditional textual CoT difficult to bring about significant improvement. Fig. 6 further demonstrates the effectiveness of our proposed Spatial CoT on the OmniSpatial Perspective-Taking track. Through novel view synthesis facilitated by InstantMesh, both GPT-4.1 and Qwen-VL2.5-7B exhibit significant performance improvements, validating the effectiveness of explicit spatial imagination. Table 4: Performance of Spatial CoT on OmniSpatial Perspective-Taking track. (w/ Spatial CoT) 47.58 +2.02 (w/ Spatial CoT) 42.90 +2.01 69.41 40.11 69.43 42. 59.61 36.54 60.80 39.25 (w/ Zero-shot CoT) 40.89 40.96 44.34 37.59 37.44 Qwen-VL2.5-3B - -"
        },
        {
            "title": "5 Related Works",
            "content": "5.1 Benchmarking Spatial Reasoning The development of standardized benchmarks [55, 122, 65, 124, 44, 51] has been critical in assessing the performance of spatial reasoning models. Various studies have introduced innovative benchmarking methodologies [104, 15, 17, 11, 102, 89, 96] to advance spatial reasoning evaluation. For instance, Spatial VQA [26] was among the first to incorporate spatial information into vision-language models, enabling fundamental spatial relationship reasoning. SpatialRGPT-Bench [17] further introduced depth-aware reasoning, utilizing large language models to analyze spatial relationships, such as understanding relative directions and distances between specified regions in 3D scenes [104]. SpatialBench [11] categorized spatial reasoning into various hierarchical levels, extending its applicability to robotic manipulation tasks. RoboSpatial [102] proposes large-scale template-based spatial relationship dataset, focusing on positional relationships from different perspectives. VSI-Bench [121] combined video data [14, 65, 33, 56, 32] with cognitive maps [72, 4] to simulate human-like spatial cognition and optimize reasoning in dynamic environments. Recently, SoFar [89] introduced the 6-DoF SpatialBench to evaluate the understanding of orientation. While these benchmarks have made significant contributions, unified framework encompassing wide range of complex spatial reasoning tasks remains lacking. Inspired by prior spatial reasoning research [119, 114, 58, 76], we identified several limitations in existing benchmarks, such as reliance on generated images [104, 34, 48, 61, 92, 101], LLM-generated templates [59, 26, 91, 93], and domain-specific focus [117, 19, 21, 12, 28]. These issues hinder their comprehensiveness and realworld applicability. To address these gaps, our study proposes comprehensive and integrative spatial reasoning benchmark."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce OmniSpatial, benchmark for comprehensive visualspatial reasoning. OmniSpatial distills spatial cognition into four primary categoriesdynamic reasoning, complex spatial logic, spatial interaction, and perspective-takingspanning 50 fine-grained subtasks and 1.5 manually-curated questionanswer pairs. Extensive experiments show that state-of-theart proprietary and open-source VLMs peak at 57% accuracyover 30 points below human performancestruggling especially with geometric reasoning and non-egocentric perspective taking. By exposing these systematic gaps, OmniSpatial provides rigorous yardstick and rich error analysis playground for future work on physicsand viewpoint-aware multimodal models."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: visual language model for few-shot learning. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2022. 26 [2] Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P. Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, AnaÃ¯s White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, and et al. Gemini: family of highly capable multimodal models. CoRR, abs/2312.11805, 2023. 7, 8, 26 [3] Anthropic. The claude 3 model www.anthropic.com, de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf. 8 https:// Opus, haiku. https://www-cdn.anthropic.com/ 7, family: URL sonnet, 2024. [4] Ioannis Apostolopoulos and Peter Groumpos. Fuzzy cognitive maps: their role in explainable artificial intelligence. Applied Sciences, 13(6):3412, 2023. 9 [5] Alan Baddeley. Working memory. Comptes Rendus de lAcadÃ©mie des Sciences-Series III-Sciences de la Vie, 321(2-3):167173, 1998. 2 [6] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. CoRR, abs/2308.12966, 2023. 8, 26 [7] Yoav Bar-Anan, Nira Liberman, and Yaacov Trope. The association between psychological distance and construal level: evidence from an implicit association test. Journal of experimental psychology: General, 135(4):609, 2006. 2 [8] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J. Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael S. Ryoo, Grecia Salazar, Pannag R. Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong T. Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. RT-1: robotics transformer for real-world control at scale. In Robotics: Science and Systems XIX, Daegu, Republic of Korea, July 10-14, 2023, 2023. [9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. 26 [10] Jeffrey Buckley, Niall Seery, and Donal Canty. heuristic framework of spatial ability: review and synthesis of spatial factor literature to support its translation into stem education. Educational Psychology Review, 30(3):947972, 2018. 26 [11] Wenxiao Cai, Yaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang, Hao Dong, and Bo Zhao. Spatialbot: Precise spatial understanding with vision language models. CoRR, abs/2406.13642, 2024. 2, 3, 7, 8, 9, 26 [12] Xu Cao, Tong Zhou, Yunsheng Ma, Wenqian Ye, Can Cui, Kun Tang, Zhipeng Cao, Kaizhao Liang, Ziran Wang, James M. Rehg, and Chao Zheng. MAPLM: real-world large-scale vision-language benchmark for map and traffic scene understanding. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pp. 2181921830. IEEE, 2024. 9 10 [13] Christopher Chabris, Thomas Jerde, Anita Woolley, Margaret Gerbasi, Jonathon Schuldt, Sean Bennett, Richard Hackman, and Stephen Kosslyn. Spatial and object visualization cognitive styles: Validation studies in 3800 individuals. Group brain technical report, 2(1-20):2, 2006. [14] Keshigeyan Chandrasegaran, Agrim Gupta, Lea Hadzic, Taran Kota, Jimming He, CristÃ³bal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, and Fei-Fei Li. Hourvideo: 1-hour video-language understanding. Advances in Neural Information Processing Systems, 37:5316853197, 2024. 9, 27 [15] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter, Dorsa Sadigh, Leonidas J. Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pp. 1445514465. IEEE, 2024. 2, 3, 7, 8, 9, 26 [16] Shiqi Chen, Tongyao Zhu, Ruochen Zhou, Jinghan Zhang, Siyang Gao, Juan Carlos Niebles, Mor Geva, Junxian He, Jiajun Wu, and Manling Li. Why is spatial reasoning hard for vlms? an attention mechanism perspective on focus areas. CoRR, abs/2503.01773, 2025. 2, 3 [17] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. 2, 3, 9, 26 [18] Junmo Cho, Jaesik Yoon, and Sungjin Ahn. Spatially-aware transformers for embodied agents. 2024. 26 [19] Wei Chow, Jiageng Mao, Boyi Li, Daniel Seita, Vitor Guizilini, and Yue Wang. Physbench: Benchmarking and enhancing vision-language models for physical world understanding. CoRR, abs/2501.16411, 2025. [20] Open X.-Embodiment Collaboration, Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alexander Herzog, Alex Irpan, Alexander Khazatsky, Anant Raj, Anikait Singh, Anthony Brohan, Antonin Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard SchÃ¶lkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter BÃ¼chler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Giulio Schiavi, Gregory Kahn, Hao Su, Haoshu Fang, Haochen Shi, Heni Ben Amor, Henrik I. Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, and et al. Open x-embodiment: Robotic learning datasets and RT-X models. CoRR, abs/2310.08864, 2023. 2 [21] Muhammad Sohail Danish, Muhammad Akhtar Munir, Syed Roshaan Ali Shah, Kartik Kuckreja, Fahad Shahbaz Khan, Paolo Fraccaro, Alexandre Lacoste, and Salman H. Khan. Geobench-vlm: Benchmarking vision-language models for geospatial tasks. CoRR, abs/2411.19325, 2024. 9 [22] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. 27 [23] Runpei Dong, Zekun Qi, Linfeng Zhang, Junbo Zhang, Jianjian Sun, Zheng Ge, Li Yi, and Kaisheng Ma. Autoencoders as cross-modal teachers: Can pretrained 2d image transformers help 3d representation learning? In Int. Conf. Learn. Represent. (ICLR), 2023. 27 [24] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, and Li Yi. DreamLLM: Synergistic multimodal comprehension and creation. In Int. Conf. Learn. Represent. (ICLR), 2024. 27 [25] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied multimodal language model. In Int. Conf. Mach. Learn. (ICML), 2023. 2, 26 [26] Mengfei Du, Binhao Wu, Zejun Li, Xuanjing Huang, and Zhongyu Wei. Embspatial-bench: Benchmarking spatial understanding for embodied tasks with large vision-language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, ACL 2024 - Short Papers, Bangkok, Thailand, August 11-16, 2024, pp. 346355. Association for Computational Linguistics, 2024. 2, 3, 9 [27] Jiafei Duan, Wilbert Pumacay, Nishanth Kumar, Yi Ru Wang, Shulin Tian, Wentao Yuan, Ranjay Krishna, Dieter Fox, Ajay Mandlekar, and Yijie Guo. AHA: vision-language-model for detecting and reasoning over failures in robotic manipulation. CoRR, abs/2410.00371, 2024. 26 [28] Zhizhao Duan, Hao Cheng, Duo Xu, Xi Wu, Xiangxie Zhang, Xi Ye, and Zhen Xie. Cityllava: Efficient fine-tuning for vlms in city scenario. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024 - Workshops, Seattle, WA, USA, June 17-18, 2024, pp. 71807189. IEEE, 2024. 9 [29] Eliot and IM Smith. An international directory of spatial tests. atlantic highlands, 1983. 26 [30] Guofan Fan, Zekun Qi, Wenkai Shi, and Kaisheng Ma. Point-gcc: Universal self-supervised 3d scene pre-training via geometry-color contrast. In Jianfei Cai, Mohan S. Kankanhalli, Balakrishnan Prabhakaran, Susanne Boll, Ramanathan Subramanian, Liang Zheng, Vivek K. Singh, Pablo CÃ©sar, Lexing Xie, and Dong Xu (eds.), Proceedings of the 32nd ACM International Conference on Multimedia, MM 2024, Melbourne, VIC, Australia, 28 October 2024 - 1 November 2024, pp. 47094718. ACM, 2024. [31] Kuan Fang, Fangchen Liu, Pieter Abbeel, and Sergey Levine. MOKA: Open-World Robotic Manipulation through Mark-Based Visual Prompting. In Proceedings of Robotics: Science and Systems, Delft, Netherlands, July 2024. 27 [32] Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbench-video: long-form multi-shot benchmark for holistic video understanding. 2024. 9 [33] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Rongrong Ji, and Xing Sun. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. CoRR, abs/2405.21075, 2024. 9 [34] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A. Smith, Wei-Chiu Ma, and Ranjay Krishna. BLINK: multimodal large language models can see but not perceive. In Ales Leonardis, Elisa Ricci, Stefan Roth, Olga Russakovsky, Torsten Sattler, and GÃ¼l Varol (eds.), Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part XXIII, volume 15081 of Lecture Notes in Computer Science, pp. 148166. Springer, 2024. 2, 9 [35] Howard Gardner. Frames of mind: The theory of multiple intelligences. Basic books, 2011. 2, [36] Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Yiwen Tang, Xianzheng Ma, Jiaming Han, Kexin Chen, Peng Gao, Xianzhi Li, Hongsheng Li, and Pheng-Ann Heng. Point-bind & point-llm: Aligning point cloud with multi-modality for 3d understanding, generation, and instruction following. CoRR, abs/2309.00615, 2023. 27 [37] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr DollÃ¡r, and Ross B. Girshick. Masked autoencoders are scalable vision learners. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2022. 26 [38] Xialin He, Runpei Dong, Zixuan Chen, and Saurabh Gupta. Learning getting-up policies for real-world humanoid robots. CoRR, abs/2502.12152, 2025. 27 [39] Mary Hegarty. Components of spatial intelligence. In Psychology of learning and motivation, volume 52, pp. 265297. Elsevier, 2010. [40] Mary Hegarty and Waller. Individual differences in spatial abilities. The Cambridge handbook of visuospatial thinking, pp. 121169, 2005. 26 12 [41] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in neural network. In Adv. Neural Inform. Process. Syst. (NeurIPS), volume abs/1503.02531, 2015. 27 [42] Haoxu Huang, Fanqi Lin, Yingdong Hu, Shengjie Wang, and Yang Gao. Copa: General robotic manipulation through spatial constraints of parts with foundation models. CoRR, abs/2403.08248, 2024. 26, [43] Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, and Li Fei-Fei. Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. In Annu. Conf. Robot. Learn. (CoRL), 2024. 2, 26, 27 [44] Drew A. Hudson and Christopher D. Manning. GQA: new dataset for real-world visual reasoning and compositional question answering. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pp. 67006709. Computer Vision Foundation / IEEE, 2019. 9 [45] Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander Madry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll L. Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, and Dane Sherburn. Gpt-4o system card. CoRR, abs/2410.21276, 2024. 7, 8, 26 [46] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian Osband, Ignasi Clavera Gilaberte, and Ilge Akkaya. Openai o1 system card. CoRR, abs/2412.16720, 2024. 7, 8, 27 [47] Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre RamÃ©, Morgane RiviÃ¨re, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean-Bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, GaÃ«l Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, RÃ³bert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen, Abhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, AndrÃ¡s GyÃ¶rgy, AndrÃ© Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson, Ashish Shenoy, Ayan Chakrabarti, Bilal Piot, Bo Wu, Bobak Shahriari, Bryce Petrini, Charlie Chen, Charline Le Lan, Christopher A. Choquette-Choo, CJ Carey, Cormac Brick, Daniel Deutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree Shivakumar Sreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eugene Kharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna Klimczak-Plucinska, Harman Singh, Harsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian Ballantyne, Idan Szpektor, and Ivan Nardini. Gemma 3 technical report. CoRR, abs/2503.19786, 2025. 7, 8 [48] Amita Kamath, Jack Hessel, and Kai-Wei Chang. Whats \"up\" with vision-language models? investigating their struggle with spatial reasoning. pp. 91619175, 2023. 2, 9 [49] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, ChloÃ© Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr DollÃ¡r, and Ross B. Girshick. Segment anything. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pp. 39924003. IEEE, 2023. 6 [50] Eliza Kosoy, Annya Dahmani, Andrew K. Lampinen, Iulia M. Comsa, Soojin Jeong, Ishita Dasgupta, and Kelsey Allen. Decoupling the components of geometric understanding in vision language models. CoRR, abs/2503.03840, 2025. 2, 3 [51] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense image annotations. Int. J. Comput. Vis., 123 (1):3273, 2017. [52] Jongwon Lee and Robert Bednarz. Components of spatial thinking: Evidence from spatial thinking ability test. Journal of geography, 111(1):1526, 2012. 26 [53] Phillip Lee, Jihyeon Je, Chanho Park, Mikaela Angelina Uy, Leonidas Guibas, and Minhyuk Sung. Perspective-aware reasoning in vision-language models via mental imagery simulation. CoRR, abs/2504.17207, 2025. 3 [54] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. CoRR, abs/2408.03326, 2024. 7 [55] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. CoRR, abs/2307.16125, 2023. [56] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Lou, Limin Wang, and Yu Qiao. Mvbench: comprehensive multi-modal video understanding benchmark. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pp. 2219522206. IEEE, 2024. 9 [57] Zijing Liang, Yanjie Xu, Yifan Hong, Penghui Shang, Qi Wang, Qiang Fu, and Ke Liu. survey of multimodel large language models. In Proceedings of the 3rd International Conference on Computer, Artificial Intelligence and Control Engineering, pp. 405409, 2024. 4, 6 [58] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÃ¡r, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In David J. Fleet, TomÃ¡s Pajdla, Bernt Schiele, and Tinne Tuytelaars (eds.), Computer Vision - ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V, volume 8693 of Lecture Notes in Computer Science, pp. 740755. Springer, 2014. 9 [59] Xiongkun Linghu, Jiangyong Huang, Xuesong Niu, Xiaojian (Shawn) Ma, Baoxiong Jia, and Siyuan Huang. Multi-modal situated reasoning in 3d scenes. 2024. 9 [60] Benlin Liu, Yuhao Dong, Yiqin Wang, Yongming Rao, Yansong Tang, Wei-Chiu Ma, and Ranjay Krishna. Coarse correspondence elicit 3d spacetime understanding in multimodal language model. CoRR, abs/2408.00754, 2024. [61] Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial reasoning. Trans. Assoc. Comput. Linguistics, 11:635651, 2023. 2, 9 [62] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with blockwise ringattention. CoRR, abs/2402.08268, 2024. 26 [63] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2023. [64] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2024. 7 [65] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player? CoRR, abs/2307.06281, 2023. 9 14 [66] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. In The Twelfth Syncdreamer: Generating multiview-consistent images from single-view image. International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. 7 [67] Yunze Liu, Yun Liu, Che Jiang, Kangbo Lyu, Weikang Wan, Hao Shen, Boqiang Liang, Zhoujie Fu, He Wang, and Li Yi. HOI4D: 4d egocentric dataset for category-level human-object interaction. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2022. 4, 6, [68] Chenyang Ma, Kai Lu, Ta Ying Cheng, Niki Trigoni, and Andrew Markham. Spatialpin: Enhancing spatial reasoning capabilities of vision-language models through prompting and interacting 3d priors. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. 2 [69] Margherita Malanchini, Kaili Rimfeld, Nicholas Shakeshaft, Andrew McMillan, Kerry Schofield, Maja Rodic, Valerio Rossi, Yulia Kovas, Philip Dale, Elliot Tucker-Drob, et al. Evidence for unitary structure of spatial cognition beyond general intelligence. npj Science of Learning, 5(1):9, 2020. 26 [70] Lynn McGarvey, Lixin Luo, Zachary Hawes, and Spatial Reasoning Study Group. Spatial skills framework for young engineers. Early engineering learning, pp. 5381, 2018. 26 [71] Chiara Meneghetti, Laura Miola, Tommaso Feraco, Veronica Muffato, and Tommaso Feraco Miola. Individual differences in navigation: an introductory overview. Prime archives in psychology, 2, 2022. 3 [72] Ida Momennejad, Hosein Hasanbeig, Felipe Vieira Frujeri, Hiteshi Sharma, Nebojsa Jojic, Hamid Palangi, Robert Osazuwa Ness, and Jonathan Larson. Evaluating cognitive maps and planning in large language models with cogeval. 2023. 2, [73] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. Embodiedgpt: Vision-language pre-training via embodied chain of thought. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2023. 26 [74] Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita Dasgupta, Annie Xie, Danny Driess, Ayzaan Wahid, Zhuo Xu, Quan Vuong, Tingnan Zhang, Tsang-Wei Edward Lee, Kuang-Huei Lee, Peng Xu, Sean Kirmani, Yuke Zhu, Andy Zeng, Karol Hausman, Nicolas Heess, Chelsea Finn, Sergey Levine, and Brian Ichter. PIVOT: iterative visual prompting elicits actionable knowledge for vlms. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. 26 [75] Nora Newcombe and Thomas Shipley. Thinking about spatial thinking: New typology, new assessments. In Studying visual and spatial reasoning for design creativity, pp. 179192. Springer, 2014. 26 [76] Linus Nwankwo, Bjoern Ellensohn, Vedant Dave, Peter Hofer, Jan Forstner, Marlene Villneuve, Robert Galler, and Elmar Rueckert. Envodat: large-scale multisensory dataset for robotic spatial awareness and semantic reasoning in heterogeneous environments. CoRR, abs/2410.22200, 2024. 9 [77] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. 7 [78] OpenAI. Introducing gpt-4.1 in the api, 2025. URL https://openai.com/index/gpt-4-1. [79] OpenAI. Openai o3 and o4-mini system card, 2025. URL https://openai.com/research/ o3-o4-mini-system-card. 2, 7, 8, 27 [80] Maxime Oquab, TimothÃ©e Darcet, ThÃ©o Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, HervÃ© JÃ©gou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision. Trans. Mach. Learn. Res., 2024, 2024. 26 [81] Yuang Peng, Yuxin Cui, Haomiao Tang, Zekun Qi, Runpei Dong, Jing Bai, Chunrui Han, Zheng Ge, Xiangyu Zhang, and Shu-Tao Xia. Dreambench++: human-aligned benchmark for personalized image generation. CoRR, abs/2406.16855, 2024. 27 15 [82] Atin Pothiraj, Elias Stengel-Eskin, Jaemin Cho, and Mohit Bansal. Capture: Evaluating spatial reasoning in vision language models via occluded object counting. CoRR, abs/2504.15485, 2025. 2, 3 [83] Fred Previc. The neuropsychology of 3-d space. Psychological bulletin, 124(2):123, 1998. 2 [84] Charles Ruizhongtai Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 7785, 2017. 27 [85] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J. Guibas. Pointnet++: Deep hierarchical feature learning on point sets in metric space. In Adv. Neural Inform. Process. Syst. (NIPS), pp. 50995108, 2017. 27 [86] Zekun Qi, Runpei Dong, Guofan Fan, Zheng Ge, Xiangyu Zhang, Kaisheng Ma, and Li Yi. Contrast with reconstruct: Contrastive 3d representation learning guided by generative pretraining. In Int. Conf. Mach. Learn. (ICML), 2023. 27 [87] Zekun Qi, Muzhou Yu, Runpei Dong, and Kaisheng Ma. VPP: efficient conditional 3d generation via voxel-point progressive representation. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2023. 7, [88] Zekun Qi, Runpei Dong, Shaochen Zhang, Haoran Geng, Chunrui Han, Zheng Ge, Li Yi, and Kaisheng Ma. Shapellm: Universal 3d object understanding for embodied interaction. In Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part XLIII, volume 15101 of Lecture Notes in Computer Science, pp. 214238. Springer, 2024. 2, 26, 27 [89] Zekun Qi, Wenyao Zhang, Yufei Ding, Runpei Dong, Xinqiang Yu, Jingwen Li, Lingyun Xu, Baoyu Li, Xialin He, Guofan Fan, Jiazhao Zhang, Jiawei He, Jiayuan Gu, Xin Jin, Kaisheng Ma, Zhizheng Zhang, He Wang, and Li Yi. Sofar: Language-grounded orientation bridges spatial reasoning and object manipulation. CoRR, abs/2502.13143, 2025. 2, 7, 8, 9, 26, 27 [90] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Int. Conf. Mach. Learn. (ICML), volume 139 of Proceedings of Machine Learning Research, pp. 87488763. PMLR, 2021. 26 [91] Tim RÃ¤dsch, Leon D. Mayer, Simon Pavicic, A. Emre Kavur, Marcel Knopp, Baris Ã–ztÃ¼rk, Klaus H. Maier-Hein, Paul F. Jaeger, Fabian Isensee, Annika Reinke, and Lena Maier-Hein. Bridging vision language model (VLM) evaluation gaps with framework for scalable and cost-effective benchmark generation. CoRR, abs/2502.15563, 2025. 9 [92] Navid Rajabi and Jana Kosecka. Towards grounded visual spatial reasoning in multi-modal vision language models. CoRR, abs/2308.09778, 2023. [93] Navid Rajabi and Jana Kosecka. GSR-BENCH: benchmark for grounded spatial reasoning evaluation via multimodal llms. CoRR, abs/2406.13246, 2024. 9 [94] Santhosh Kumar Ramakrishnan, Erik Wijmans, Philipp KrÃ¤henbÃ¼hl, and Vladlen Koltun. Does spatial cognition emerge in frontier models? CoRR, abs/2410.06468, 2024. 26 [95] Tiago Ramalho, TomÃ¡s KociskÃ½, Frederic Besse, S. M. Ali Eslami, GÃ¡bor Melis, Fabio Viola, Phil Blunsom, and Karl Moritz Hermann. Encoding spatial relations from natural language. CoRR, abs/1807.01670, 2018. 2 [96] Arijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan A. Plummer, Ranjay Krishna, Kuo-Hao Zeng, and Kate Saenko. SAT: spatial aptitude training for multimodal language models. CoRR, abs/2412.07755, 2024. [97] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy P. Lillicrap, Jean-Baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud, Andrew M. Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James Molloy, Jilin Chen, Michael Isard, Paul Barham, Tom Hennigan, Ross McIlroy, Melvin Johnson, Johan Schalkwyk, Eli Collins, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Clemens Meyer, Gregory Thornton, Zhen Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher, Ankesh Anand, Richard Ives, James Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri, Pranav Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezener, and et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. CoRR, abs/2403.05530, 2024. 2, 7, 8, 26 16 [98] Konstantinos I. Roumeliotis and Nikolaos D. Tselikas. Chatgpt and open-ai models: preliminary review. Future Internet, 15(6):192, 2023. 7, 8 [99] Julia Rozanova, Deborah Ferreira, Krishna Dubba, Weiwei Cheng, Dell Zhang, and AndrÃ© Freitas. Grounding natural language instructions: Can large language models capture spatial information? CoRR, abs/2109.08634, 2021. 26 [100] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: single image to consistent multi-view diffusion base model. CoRR, abs/2310.15110, 2023. [101] Fatemeh Shiri, Xiao-Yu Guo, Mona Far, Xin Yu, Reza Haf, and Yuan-Fang Li. An empirical analysis on spatial reasoning capabilities of large multimodal models. pp. 2144021455, 2024. 2, 9 [102] Chan Hee Song, Valts Blukis, Jonathan Tremblay, Stephen Tyree, Yu Su, and Stan Birchfield. Robospatial: Teaching spatial understanding to 2d and 3d vision-language models for robotics. CoRR, abs/2411.16537, 2024. 2, 9, 26 [103] Ilias Stogiannidis, Steven McDonagh, and Sotirios A. Tsaftaris. Mind the gap: Benchmarking spatial reasoning in vision-language models. CoRR, abs/2503.19707, 2025. 3, 26 [104] Emilia Szymanska, Mihai Dusmanu, Jan-Willem Buurlage, Mahdi Rad, and Marc Pollefeys. Space3dbench: Spatial 3d question answering benchmark. CoRR, abs/2408.16662, 2024. 2, 3, 9 [105] Yihong Tang, Ao Qu, Zhaokai Wang, Dingyi Zhuang, Zhaofeng Wu, Wei Ma, Shenhao Wang, Yunhan Zheng, Zhan Zhao, and Jinhua Zhao. Sparkle: Mastering basic spatial capabilities in vision language models elicits generalization to composite spatial reasoning. CoRR, abs/2410.16162, 2024. 26 [106] Edward Tolman. Cognitive maps in rats and men. Psychological review, 55(4):189, 1948. 2 [107] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, AurÃ©lien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023. 26 [108] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, AurÃ©lien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023. 26 [109] Yaacov Trope and Nira Liberman. Construal-level theory of psychological distance. Psychological review, 117(2):440, 2010. 2 [110] David Uttal, Nathaniel Meadow, Elizabeth Tipton, Linda Hand, Alison Alden, Christopher Warren, and Nora Newcombe. The malleability of spatial skills: meta-analysis of training studies. Psychological bulletin, 139(2):352, 2013. 26 [111] David Uttal, Kiley McKee, Nina Simms, Mary Hegarty, and Nora Newcombe. How can we best assess spatial skills? practical and conceptual challenges. Journal of Intelligence, 12(1):8, 2024. 26 [112] Jonathan Wai, David Lubinski, and Camilla Benbow. Spatial ability for stem domains: aligning over 50 years of cumulative psychological knowledge solidifies its importance. Journal of educational Psychology, 101(4):817, 2009. 26 [113] Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, and Katsushi Ikeuchi. Gpt-4v(ision) for robotics: Multimodal task planning from human demonstration. IEEE Robotics Autom. Lett., 9(11): 1056710574, 2024. [114] Jiayu Wang, Yifei Ming, Zhenmei Shi, Vibhav Vineet, Xin Wang, Sharon Li, and Neel Joshi. Is picture worth thousand words? delving into spatial reasoning for vision language models. 2024. 9 17 [115] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. CoRR, abs/2409.12191, 2024. 7, 8, 26 [116] Wenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Lei Cui, and Furu Wei. Visualizationof-thought elicits spatial reasoning in large language models. CoRR, abs/2404.03622, 2024. 26 [117] Shaoyuan Xie, Lingdong Kong, Yuhao Dong, Chonghao Sima, Wenwei Zhang, Qi Alfred Chen, Ziwei Liu, and Liang Pan. Are vlms ready for autonomous driving? an empirical study from the reliability, data, and metric perspectives. CoRR, abs/2501.04003, 2025. [118] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models. CoRR, abs/2404.07191, 2024. 7 [119] Liuchang Xu, Shuo Zhao, Qingming Lin, Luyao Chen, Qianqian Luo, Sensen Wu, Xinyue Ye, Hailin Feng, and Zhenhong Du. Evaluating large language models on spatial tasks: multi-task benchmarking study. CoRR, abs/2408.14438, 2024. 9 [120] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in GPT-4V. CoRR, abs/2310.11441, 2023. 26 [121] Jihan Yang, Shusheng Yang, Anjali W. Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. CoRR, abs/2412.14171, 2024. 2, 9, 27 [122] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. In Int. Conf. Mach. Learn. (ICML), 2024. [123] Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan Murali, Arsalan Mousavian, and Dieter Fox. Robopoint: vision-language model for spatial affordance prediction for robotics. CoRR, abs/2406.10721, 2024. 2, 7, 8, 26 [124] Xiang Yue, Yuansheng Ni, Tianyu Zheng, Kai Zhang, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. MMMU: massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pp. 95569567. IEEE, 2024. 9 [125] Junyu Zhang, Runpei Dong, Han Wang, Xuying Ning, Haoran Geng, Peihao Li, Xialin He, Yutong Bai, Jitendra Malik, Saurabh Gupta, and Huan Zhang. Alphaone: Reasoning models thinking slow and fast at test time. CoRR, abs/2505.24863, 2025. 27 [126] Linfeng Zhang, Xin Chen, Runpei Dong, and Kaisheng Ma. Region-aware knowledge distillation In 34th British Machine Vision Conference 2023, BMVC for efficient image-to-image translation. 2023, Aberdeen, UK, November 20-24, 2023, pp. 345346. BMVA Press, 2023. URL http:// proceedings.bmvc2023.org/345/. 27 [127] Shaochen Zhang, Zekun Qi, Runpei Dong, Xiuxiu Bai, and Xing Wei. Positional prompt tuning for efficient 3d representation learning. CoRR, abs/2408.11567, 2024. [128] Zhihao Zhang, Jun Zhao, Qi Zhang, Tao Gui, and Xuanjing Huang. Unveiling linguistic regions in large language models. pp. 62286247, 2024. 26 [129] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-asa-judge with mt-bench and chatbot arena. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. 8 [130] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. CoRR, abs/2504.10479, 2025. 7, 8 18 [131] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Quan Vuong, Vincent Vanhoucke, Huong T. Tran, Radu Soricut, Anikait Singh, Jaspiar Singh, Pierre Sermanet, Pannag R. Sanketi, Grecia Salazar, Michael S. Ryoo, Krista Reymann, Kanishka Rao, Karl Pertsch, Igor Mordatch, Henryk Michalewski, Yao Lu, Sergey Levine, Lisa Lee, Tsang-Wei Edward Lee, Isabel Leal, Yuheng Kuang, Dmitry Kalashnikov, Ryan Julian, Nikhil J. Joshi, Alex Irpan, Brian Ichter, Jasmine Hsu, Alexander Herzog, Karol Hausman, Keerthana Gopalakrishnan, Chuyuan Fu, Pete Florence, Chelsea Finn, Kumar Avinava Dubey, Danny Driess, Tianli Ding, Krzysztof Marcin Choromanski, Xi Chen, Yevgen Chebotar, Justice Carbajal, Noah Brown, Anthony Brohan, Montserrat Gonzalez Arenas, and Kehang Han. RT-2: vision-language-action models transfer web knowledge to robotic control. In Conference on Robot Learning, CoRL 2023, 6-9 November 2023, Atlanta, GA, USA, volume 229 of Proceedings of Machine Learning Research, pp. 21652183. PMLR, 2023. 2,"
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Preliminaries: VisualSpatial Reasoning"
        },
        {
            "title": "2.1 Taxonomy of Visualâ€“Spatial Reasoning . . . . . . . . . . . . . . . . . . . . . . .\n2.2 Rationale for Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "3 OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark . . . . . . ."
        },
        {
            "title": "3.1 Overview .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.2 Benchmark Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.2.1 Data Collection .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.2.2 Question-Answer Annotation . . . . . . . . . . . . . . . . . . . . . . . .\nImproving Visual Spatial Reasoning Abilities . . . . . . . . . . . . . . . . . . . .\nPointGraph: Enhancing Spatial Reasoning via Point Relationships . . . . .\n3.3.1\nSpatialCoT: Enhancing Spatial Imagination via Novel View Synthesis . . .\n3.3.2",
            "content": "3.3 . 4 Experiments"
        },
        {
            "title": "4.1 Evaluation Setup .\n4.2 Evaluation Metrics\n.\n4.3 Main Results",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Related Works 5.1 Benchmarking Spatial Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Conclusion 2 3 3 4 4 4 5 5 6 6 6 7 7 8 8 9 9 9 Detailed Task Design . . . . . A.2 Complex Logic . A.1 Dynamic Reasoning . . A.1.1 Manipulation . A.1.2 Motion Analysis . 21 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 . . 22 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 A.2.1 Pattern Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 A.2.2 Geometric Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.1 Traffic Analysis . 23 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.2 Localization . . . . 24 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.3 Geospatial Strategy . 24 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Perspective Taking . 24 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4.1 Egocentric . 25 A.4.2 Allocentric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 A.4.3 Hypothetical Perspective Taking . . . . . . . . . . . . . . . . . . . . . . . A.3 Spatial Interaction . . . . . . . . . . Additional Related Works B.1 Spatial Vision-Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Spatial Reasoning in Psychology . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Ablation Studies C.1 Evaluation and Format Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . System Prompts Additional Visualization Limitation & Future Works Broader Impacts 20 26 26 26 26 26 27"
        },
        {
            "title": "A Detailed Task Design",
            "content": "OmniSpatial aims to comprehensively evaluate the spatial reasoning capabilities of Vision-Language Models, covering four major categories: Dynamic Reasoning, Complex Logic, Spatial Interaction, and Perspective Taking. Each category not only focuses on different types of spatial reasoning tasks but also includes challenges based on real-world application scenarios. This approach helps researchers better understand and enhance models multi-domain spatial reasoning abilities. The following presents the underlying considerations and practical value behind each task design. A.1 Dynamic Reasoning The Dynamic Reasoning category focuses on the models understanding of object movement and its changes, assessing the ability to make accurate judgments in uncertain or rapidly changing environments. Spatial dynamics are critical not only in robot control but also have broad applications in fields like autonomous driving and intelligent surveillance. A.1.1 Manipulation Operational Position Selection This task evaluates how models determine the optimal interaction point with objects in complex environments. Selecting the best grasping point can prevent tilting or damage to objects and improve the efficiency and precision of operations. This task is crucial in robotic grasping, especially when environmental conditions are unstable. Movement Direction Determination This task assesses the models ability to predict the movement direction of an object or itself, providing decision support for automated systems and helping to optimize robot motion strategies. Intent Recognition Intent recognition involves inferring the purpose or goal behind movement. This task is particularly important for contextual analysis, such as determining whether person is reaching for door handle to open or close door. This capability enhances the reasoning of human-robot interactions and optimizes the interaction experience of intelligent assistants and robots. Figure 7: OmniSpatial tasks.The tasks are organized into three levels, with each of the four categories of spatial abilities containing no fewer than two subtasks. The final level features more detailed subdivision, inspired by real-life scenarios . 21 A.1.2 Motion Analysis Uniform Motion The models ability to reason about uniform motion reflects its fundamental understanding of time and spatial relationships, such as estimating the speed or time required for target to move. This is applicable in areas like object tracking and path prediction, such as estimating vehicle travel time or train arrival time. Variable Motion Variable motion analysis focuses on understanding acceleration and deceleration processes. By predicting the position changes of an object during variable motion, the model can better simulate dynamic phenomena in the physical world, such as calculating braking distance for vehicles. This is critical in autonomous driving and robot control. Spatial Compatibility Tasks that assess whether an object fits within specific space directly relate to the precision of robots and automated devices in real-world operations. For instance, determining whether luggage can fit into an overhead compartment is applicable in logistics and automated warehouses, helping systems make effective object adaptation decisions. A.2 Complex Logic The Complex Logic category focuses on higher-order spatial reasoning, including tasks such as geometric transformations and pattern recognition, challenging the models ability to abstractly understand and reason in multi-dimensional spatial environments. A.2.1 Pattern Recognition Style Style recognition tasks assess the models ability to infer visual rules in structured patterns. Typical operations include completing missing parts, combining or subtracting shapes, comparing similarities and differences, and performing visual logic like black-white inversion. This skill is crucial for tasks like intelligence tests and diagrammatic reasoning. Quantity Quantity-based tasks evaluate the models ability to perform visual numerosity reasoning, focusing on implicit quantitative patterns such as the number of points, lines, regions, elements, or strokes. These tasks require abstract counting under diverse spatial configurations, often without explicit numerical labels or symbols. Attributes Attribute-based tasks evaluate the models ability to reason about non-numeric visual properties such as symmetry (axial or radial), curvature, and openness. These tasks require the recognition of structural features that do not rely on quantity but rather on geometric or perceptual traits. Location Location tasks evaluate the models ability to reason about spatial changes such as translation, rotation, and reflection. The focus is on how objects shift in space while preserving their structure. This skill is essential for grid-based reasoning, motion prediction, and geometric pattern understanding. A.2.2 Geometric Reasoning Polyhedron Unfolding This task examines whether the model can infer the 2D net of 3D object, reflecting its ability to mentally construct spatial layouts. It is particularly relevant to applications such as packaging design, industrial manufacturing, and aerospace engineering. Sections and Projections By evaluating how models interpret 2D cross-sections or projections from different viewpoints, this task challenges their capacity to connect visual appearance with internal structurekey to fields like medical imaging, architecture, and mechanical design. Mental Rotation Mental rotation requires the model to simulate the rotation of objects in mind and track changes across views. It underpins spatial reasoning in tasks ranging from CAD modeling to object manipulation in virtual and augmented reality. 22 Assembly This task involves reasoning about how separate parts fit together into coherent whole, testing the models understanding of geometric constraints. It has broad relevance in robotics, structural analysis, and physical assembly planning. Analytical Geometry These tasks are inspired by classic geometry problems, requiring models to reason about spatial relations using angles, distances, and symmetry. They bridge mathematical logic with visual structure, supporting applications in structured reasoning and spatial abstraction. A.3 Spatial Interaction The Spatial Interaction category evaluates models ability to reason about interactions with objects and environments. It includes tasks such as Traffic Analysis, Localization, and Geospatial Strategy, reflecting the models understanding and application of spatial knowledge in real-world scenarios. A.3.1 Traffic Analysis Anomaly Detection This task focuses on identifying potential dangers or traffic violations in complex scenes, such as unsafe following distances or unusual vehicle behavior. It plays key role in ensuring safety in autonomous driving systems. Sign Recognition This task evaluates the models ability to detect and interpret traffic signs, including speed limits, no-entry zones, and yield signs. Accurate recognition is critical for safe and rule-compliant decision-making. Action Recognition This task involves identifying or predicting the actions of traffic participants, such as driver gestures, police signals, or pedestrian intentions. It is important for understanding dynamic human behavior in traffic environments. Risk Detection This task aims to detect immediate hazards in the environment, such as an opening car door or pedestrian crossing the road. Timely detection supports effective avoidance and control strategies. Behavior Guidance This task provides context-aware behavioral suggestions, such as advising to turn off high beams or reminding that parking is prohibited. It enhances overall driving safety and compliance. Contextual Analysis This task assesses the models ability to interpret spatial relations and behaviors based on environmental cues. For example, estimating wind conditions to anticipate overtaking behavior or understanding road status to infer potential risks. A.3.2 Localization UI Interaction This task requires the model to determine which icon should be selected within user interface based on contextual understanding, and to accurately localize its position. It reflects the models ability to integrate semantic interpretation with spatial reasoning, supporting applications in intelligent assistants and automated interface control. Object Detection This task involves identifying specific target objects within an image. It is often paired with spatial localization to jointly assess what the object is and where it is located. Spatial Localization This task focuses on determining the precise position of objects within scene. It is commonly evaluated alongside object detection to answer questions like What is at this location? or Where is this object? Pose Estimation This task estimates the orientation and spatial configuration of objects, such as detecting whether cup is upright or tipped over. It is frequently integrated with spatial localization to enable more nuanced scene understanding. 23 A.3.3 Geospatial Strategy Spatial Recognition Assesses the models capacity to identify spatial structures such as rooms, corridors, or zones within scene. This is essential for semantic navigation and indoor mapping. Location Recognition This task evaluates the models ability to identify specific locations on map or scene, such as recognizing the position marked as You are here or locating designated landmark. It reflects the models capacity to associate spatial markers with real-world positions. Region Recognition Focuses on distinguishing and classifying regions in broader spatial context, such as residential vs. industrial zones on map. Route Interpretation Tests the models ability to follow or explain route depicted in map or scene. It requires understanding directional arrows, route labels, and spatial transitions. Route Design Involves selecting or generating an optimal path to reach given goal, considering spatial constraints and possible alternatives. Route Selection Compares multiple candidate routes and chooses the most suitable one based on efficiency, safety, or contextual requirements. Navigation Evaluates the models ability to understand smartphone or in-vehicle navigation interfaces, including interpreting turn-by-turn directions, identifying route segments, and understanding map overlays. This is crucial for building intelligent voice assistants and real-time guidance systems. Map/Scene Conversion Tests the ability to mentally convert between map views and real-world scenes, which is critical in correlating schematic representations with physical surroundings. Legend Recognition It requires identifying and interpreting map symbols (e.g., stairs, elevators, emergency exits) and foundational skill in navigation and spatial reasoning. Terrain Identification Focuses on distinguishing types of terrain (e.g., flat, uphill, water-crossing), which is essential for planning safe and feasible paths in outdoor navigation or robotics. A.4 Perspective Taking This category evaluates the models ability to understand spatial relationships from different viewpoints. Since changes in perspective directly affect what is observed, the ability to reason across varying angles is essential for robotic perception and interaction. A.4.1 Egocentric Count Counting the number of visible objects from the current perspective is crucial for dynamic interaction. For example, in robotic grasping tasks, knowing how many targets are visible helps determine the appropriate operation strategy. Size Judging the size of an object from the observers viewpoint aids robots or virtual systems in depth perception, helping assess whether an object can be grasped or properly placed. Direction This refers to the direction directly seen by the observer. It is especially important in autonomous driving scenarios, where understanding object movement helps predict traffic conditions and enables timely responses. Order Analyzing the arrangement of multiple objects in an image is essential for robotic operations, helping prioritize which objects to interact with first. Distance The distance between objects as perceived from the observers viewpoint is key capability in navigation systems, enabling path planning and obstacle avoidance. 24 A.4.2 Allocentric Count Understanding how the perceived quantity of objects changes under different viewpoints is key. Due to variations in position and orientation, occlusion often occursfor example, driver may have blind spots, while road surveillance camera can see objects hidden in those areas. This task evaluates the models ability to judge the difference in object counts from various observation points. Size This task involves evaluating an objects size from specified observers viewpoint. Due to the general principle that closer objects appear larger and distant ones appear smaller, objects of the same size may look different to different observers. The model is expected to either infer the true size based on reference objects or estimate how perceived size changes with viewing position. Direction This task emphasizes judging directions from abstract positions, such as another agents viewpoint or map. The answers often differ from what is directly observed, requiring one to adopt the targets perspectiveengaging in perspective-taking. It is crucial not only for large-scale path planning but also for understanding an objects intrinsic orientation. Order This task requires observing the arrangement of objects from specified viewpointfor example, the seating order of students in the front row as seen by teacher on the podium, which is exactly reversed from what camera at the back of the classroom would capture. Only by understanding what the target sees can one make accurate predictions or judgments about the scene. Distance Differences in the observers position and orientation lead to variations in perceived size and distance. For example, in the same scene, photographer taking pictures of the same object from different camera angles will capture different impressions of its size and distance. Changing the camera position essentially means changing the perspective. This ability is vital for coordination and environmental assessment in human-robot interaction or multi-robot collaborative tasks. A.4.3 Hypothetical Perspective Taking This category focuses on imagining the scene from specified but non-existent viewpoint, requiring the model to mentally adopt fictional positionan advanced form of perspective-taking in spatial reasoning. Count Predicting how many target objects would be visible from hypothetical viewpointfor example, person standing at the opposite corner of the street may see different distribution of objects. The model must reason about occlusion, orientation, and visibility from the imagined perspective. Size Inferring the apparent size of objects from hypothetical location and direction. For instance, the same object may appear larger when viewed up close or smaller when viewed from above. The model needs to simulate how visual scale changes with altered viewpoints. Direction Reasoning about how an objects direction appears from location where no observer is present. For example, pedestrian walking toward doorway would appear head-on from the entrance, but present different direction from side view. Order Simulating the arrangement of objects from another location helps assess how spatial sequences change across viewpoints. For example, the seating order seen from the podium may be the reverse of whats seen from the back of the room. Distance Estimating relative distances between objects from hypothetical position requires mentally adopting new viewpoint. This supports effective planning and coordination in tasks such as multirobot navigation or collaborative manipulation."
        },
        {
            "title": "B Additional Related Works",
            "content": "B.1 Spatial Vision-Language Models Spatial vision-language models integrate computer vision [1, 37, 80, 90] and natural language processing [6, 115, 9, 2, 107, 108, 128] to enhance machine understanding of spatial relationships. Recent research [88, 15, 17, 11, 102, 123, 89] has increasingly focused on extending vision-language models to support spatial reasoning in dynamic and 3D environments. One of the pioneering works in this domain, SpatialVLM [15], has significantly advanced spatial reasoning by constructing an RGB-D based visual question-answering dataset. These models [45, 45, 97, 115] effectively process multimodal data containing spatial information, serving as bridge between visual perception and linguistic reasoning. Further advancements [62, 60, 94, 105, 120, 99, 116, 103] include SpatialRGPT [17], which extends RGB-D spatial understanding by incorporating 3D scene graphs and spatial data to improve inference capabilities. Similarly, SpatialBot [11] explores hierarchical deep reasoning mechanisms to handle depth and spatial structures in complex environments. Recently, SoFar [89] proposed semantic orientation and trained an open-world orientation model to enhance the orientation understanding of VLMs, significantly improving spatial understanding and robotic operation capabilities [18, 25, 73, 131, 113, 43, 74, 27, 42, 89]. However, existing research remains limited in addressing the full complexity and comprehensiveness of spatial reasoning tasks. To bridge this gap, our study aims to develop more comprehensive benchmark that rigorously evaluates spatial reasoning capabilities. B.2 Spatial Reasoning in Psychology In psychology, spatial reasoning refers to an individuals ability to acquire, organize, utilize, and adapt spatial knowledge, recognized as one of the nine primary reasonings [35]. To systematically characterize this ability, Buckley et al. [10]proposed factor analysis-based framework, distinguishing between spatial visualization, spatial relations (such as mental rotation), and spatial orientation. Malanchini et al. [69] further identified strong correlation between spatial orientation and object manipulation skills. Additionally, Newcombe & Shipley [75]introduced classification system for spatial thinking, dividing spatial reasoning into two dimensions: intrinsic-extrinsic and static-dynamic. Furthermore, many studies also have contributed to the development of frameworks [112, 52, 69, 39, 70] and evaluation methods [29, 111, 110, 40] for spatial reasoning. Their framework highlights the distinction between object-centric spatial properties and external reference frames, offering valuable insights for applications such as navigation and path planning. These psychological frameworks provide complementary perspectives for understanding and assessing spatial reasoning, offering theoretical foundations for embodied intelligent systems. Inspired by these classifications, our study proposes novel taxonomy for visual-spatial reasoning, aiming to advance spatial reasoning research in vision-language models."
        },
        {
            "title": "C Additional Ablation Studies",
            "content": "C.1 Evaluation and Format Strategy The choice of evaluation format and answer-extraction strategy can materially influence how fully large language models capabilities are expressed. Because our benchmark introduces novel visualspatial reasoning task, Appendix C.1 reports the performance impact of different prompting and evaluation schemes. We consider three prompting paradigmsDirect QA, Zero-shot CoT, and Manual CoTand three evaluation methods: (i) regex-based field extraction (RE), in which the VLM is instructed to place its answer in fixed slot at the end of the response; (ii) JSON extraction, where the answer must appear in dedicated answer field; and (iii) LLM-based free-form evaluation, for which we employ GPT-4.1-mini as an adjudicator. We observe that reasoning-centric models that strongly obey formatting directives (e.g., Gemini-2.5flash) cannot be reliably scored with regex extraction, because their compulsory chain-of-thought interferes with the required output template. For standard models, all three evaluation strategies yield comparable scores, and both CoT variants consistently outperform the Direct QA baseline by small margin. Accordingly, in the main results table we evaluate non-reasoning models with the 26 Dynamic Reasoning Spatial Interaction Complex Logic Perspective Taking Promopt Type Eval Type Avg. Manipulate Motion Analysis Traffic Analysis Locate Geospatial Strategy Pattern Recognition Geometric Reasoning Ego Centric Allo Centric Hypothetical - RE RE RE GPT-4.1-mini None Direct None Zero-shot CoT Manual CoT None Zero-shot CoT Manual CoT None Zero-shot CoT Manual CoT - 39.22 48.86 49.81 49.76 JSON 48.23 JSON 48.70 JSON 48.87 LLM 48.02 LLM 48.36 LLM 49. Gemini-2.5-flash None Zero-shot CoT Manual CoT Qwen-VL2.5-3B - - LLM 51.47 LLM 51.53 LLM 52.12 - RE RE RE None Direct None Zero-shot CoT Manual CoT None Zero-shot CoT Manual CoT None Zero-shot CoT Manual CoT - 44.04 41.45 40.64 40.07 JSON 38.08 JSON 39.20 JSON 38.37 LLM 42.10 LLM 35.89 LLM 36.26 - 57.57 64.05 62.97 65.68 61.08 58.67 64.32 60.54 64.05 62.97 - 66.22 63.51 67.57 - 60.27 58.65 59.73 55.68 62.97 61.35 58.11 66.22 54.59 53. - 36.18 58.55 58.96 58.90 55.55 57.17 56.53 56.82 56.71 59.48 - 65.90 61.27 62.72 - 52.20 43.06 43.87 46.65 32.49 40.40 34.28 43.99 36.24 34.10 - 58.12 57.65 59.06 58.59 57.41 57.88 59.06 58.59 58.59 58.12 - 63.53 58.82 68.24 - 47.53 39.53 46.12 47.29 46.59 48.94 42.35 44.00 49.41 50. - 44.95 59.43 62.48 64.38 58.86 57.71 60.19 58.10 58.86 61.33 - 71.43 67.62 73.33 - 50.48 50.67 48.38 40.57 48.95 45.52 46.29 51.24 40.19 42.10 - 53.45 56.91 58.55 56.91 54.00 55.09 56.36 57.45 57.27 58.36 - 66.36 65.45 60.91 - 52.73 48.73 43.27 46.00 47.64 47.27 48.55 50.73 40.18 44. - 12.58 28.87 27.63 28.45 28.89 28.89 29.28 28.04 27.84 28.04 - 32.99 42.27 38.14 - 22.68 32.78 25.36 28.04 24.54 22.27 27.63 27.84 24.95 23.51 - 18.58 34.06 32.52 32.13 29.16 29.29 30.19 32.90 33.29 31.61 - 34.84 34.84 34.19 - 30.32 22.58 22.84 24.39 23.61 24.00 24.65 25.42 20.13 26. - - 62.94 37.61 68.82 37.18 69.41 40.11 69.61 39.31 70.20 40.96 69.02 40.16 72.55 39.57 68.24 37.55 67.06 38.30 69.02 41.12 - - 70.59 31.91 79.41 35.90 75.49 35. - - 65.49 36.49 61.96 37.66 59.61 36.54 60.39 33.35 62.16 35.85 65.29 33.51 62.75 36.54 70.00 35.90 61.37 29.20 60.39 31.38 - 37.83 41.20 40.96 41.20 40.48 42.89 39.28 38.31 38.80 39.28 - 38.55 32.53 33.73 - 30.84 37.35 37.59 31.57 27.47 27.71 26.75 29.40 33.98 23. Table 5: Comparative analysis of various prompting and evaluation strategies. Manual CoT + RE setting, whereas reasoning-oriented models are assessed with Manual CoT + LLM evaluation."
        },
        {
            "title": "D System Prompts",
            "content": "We present all the system prompts used in our experiments in Figs. 8 and 9 to facilitate reproducibility. We observe that some models are sensitive to the choice of system prompt, which may stem from distributional biases in their training data. In contrast, inference-oriented models generally exhibit stronger generalization capabilities and are less reliant on carefully crafted prompts."
        },
        {
            "title": "E Additional Visualization",
            "content": "We present more examples of question-answer pairs from the OmniSpatial, with perspective taking, spatial interaction, dynamic reasoning, and complex logic, shown in Figs. 10 to 13, respectively. Our benchmark comprises rich collection of data samples spanning diverse scenarios, resolutions, lighting conditions, and geographical regions. It includes both absolute numerical analyses and relative spatial relationships, aiming to comprehensively evaluate the spatial reasoning capabilities of vision-language models. Limitation & Future Works Although OmniSpatial includes some image clips with dynamic information from HOI4D [67], the complexity of the operational tasks still lags behind that of long videos [121, 14]. Moreover, while PointGraph & Spatial CoT enhances VLMs spatial understanding through point cues, the improvement is not fundamental in nature. Spatial reasoning tasks are more like mathematics and coding tasks, require longer and more complex reasoning [22, 46]. 3D information is crucial for spatial reasoning, and future work involves introducing 3D representation [23, 86, 88, 87] and perception [84, 85, 127, 30], as well as 3D VLMs [81, 24, 36, 88, 89], reasoning model [125, 22, 79] and knowledge distillation [126, 41, 23, 86]. The ultimate goal of spatial reasoning is to empower robots, and future work also involves robot execution tasks [42, 31, 89, 43, 38]."
        },
        {
            "title": "G Broader Impacts",
            "content": "OmniSpatial promises several positive societal benefits. By pushing models to reason about motion, collision risk and traffic scenes, it can hasten the arrival of safer autonomous vehicles and service robots that foresee hazards and navigate crowded spaces responsibly. Its geometric-reasoning tasksfrom polyhedron unfolding to assemblyoffer data that can streamline product design, packaging and manufacturing, lowering material use and energy waste. The benchmarks localization, UI-interaction and perspective-taking challenges cultivate spatially aware assistants that improve AR/VR experiences, access tools for visually impaired users and more natural human-computer interfaces. 28 Figure 8: System prompts used in OmniSpatial evaluation. 29 Figure 9: System prompts used in OmniSpatial evaluation. Figure 10: Visualization example of OmniSpatial data samples. 31 Figure 11: Visualization example of OmniSpatial data samples. 32 Figure 12: Visualization example of OmniSpatial data samples. Figure 13: Visualization example of OmniSpatial data samples."
        }
    ],
    "affiliations": [
        "Galbot",
        "Peking University",
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "Shanghai Qi Zhi Institute",
        "Tsinghua University",
        "Xian Jiaotong University"
    ]
}