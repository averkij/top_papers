{
    "paper_title": "SCALE: Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models",
    "authors": [
        "Hyeonbeom Choi",
        "Daechul Ahn",
        "Youhan Lee",
        "Taewook Kang",
        "Seongwon Cho",
        "Jonghyun Choi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language-Action (VLA) models have emerged as a promising paradigm for general-purpose robotic control, with test-time scaling (TTS) gaining attention to enhance robustness beyond training. However, existing TTS methods for VLAs require additional training, verifiers, and multiple forward passes, making them impractical for deployment. Moreover, they intervene only at action decoding while keeping visual representations fixed-insufficient under perceptual ambiguity, where reconsidering how to perceive is as important as deciding what to do. To address these limitations, we propose SCALE, a simple inference strategy that jointly modulates visual perception and action based on 'self-uncertainty', inspired by uncertainty-driven exploration in Active Inference theory-requiring no additional training, no verifier, and only a single forward pass. SCALE broadens exploration in both perception and action under high uncertainty, while focusing on exploitation when confident-enabling adaptive execution across varying conditions. Experiments on simulated and real-world benchmarks demonstrate that SCALE improves state-of-the-art VLAs and outperforms existing TTS methods while maintaining single-pass efficiency."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 4 ] . [ 1 8 0 2 4 0 . 2 0 6 2 : r SCALE: Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models Hyeonbeom Choi * 1 Daechul Ahn * 1 Youhan Lee 1 Taewook Kang 1 Seongwon Cho 1 Jonghyun Choi 1 Abstract Vision-Language-Action (VLA) models have emerged as promising paradigm for generalpurpose robotic control, with test-time scaling (TTS) gaining attention to enhance robustness beyond training. However, existing TTS methods for VLAs require additional training, verifiers, and multiple forward passes, making them impractical for deployment. Moreover, they intervene only at action decoding while keeping visual representations fixedinsufficient under perceptual ambiguity, where reconsidering how to perceive is as important as deciding what to do. To address these limitations, we propose SCALE, simple inference strategy that jointly modulates visual perception and action based on self-uncertainty, inspired by uncertainty-driven exploration in Active Inference theoryrequiring no additional training, no verifier, and only single forward pass. SCALE broadens exploration in both perception and action under high uncertainty, while focusing on exploitation when confidentenabling adaptive execution across varying conditions. Experiments on simulated and real-world benchmarks demonstrate that SCALE improves state-of-theart VLAs and outperforms existing TTS methods while maintaining single-pass efficiency. Figure 1. Motivation of SCALE. (a) Existing VLA inference relies on fixed pipeline, where visual attention may miss task-relevant cues (left; red and green boxes) and greedy decoding commits to single action despite plausible alternatives (right). (b) SCALE addresses these limitations by jointly modulating visual perception and action based on self-uncertainty: under low uncertainty, it sharpens attention and performs near-greedy execution; under high uncertainty, it broadens attention and enables explorative sampling. 1. Introduction VisionLanguageAction (VLA) models map multimodal observations and language goals to actions under closedloop control, offering promising path toward generalpurpose embodied agents (Brohan et al., 2023; Zitkovich et al., 2023; Kim et al., 2024; Pertsch et al., 2025). Among these, autoregressive VLAs have emerged as predominant paradigm: they encode visual observations through vision encoder and sequentially decode action tokens conditioned *Equal contribution JC is with ECE, IPAI and ASRI in Seoul National University. 1Seoul National University. Correspondence to: Jonghyun Choi <jonghyunchoi@snu.ac.kr>. Preprint. February 5, 2026. on language instructions, seamlessly leveraging pretrained visionlanguage models (VLMs) with minimal architectural modifications (Zitkovich et al., 2023; Kim et al., 2024; Qu et al., 2025; Driess et al., 2023; Pertsch et al., 2025). Despite their strong generalization capabilities inherited from VLMs, the diversity of real-world environmentswhere robots encounter novel scenarios that cannot be fully anticipated during traininghas driven recent research toward enhancing VLA robustness at test time, rather than relying solely on training-time optimization (Kwok et al., 2025; Nakamoto et al., 2024; Yang et al., 2025; Jang et al., 2025). prominent strategy for such test-time enhancement is TestTime Scaling (TTS) (Snell et al., 2025), which allocates additional compute at inference to improve performance. Proven 1 SCALE : Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models effective in LLMs (Snell et al., 2025; Wang et al., 2023) and VLMs (Chen et al., 2024; Zhu et al., 2025), TTS has been recently extended to VLAs via Best-of-N selection (Gao et al., 2023) with external verifier (Kwok et al., 2025; Yang et al., 2025) or self-verification (Jang et al., 2025). However, these approaches have limitations. Practically, they require additional training for verification, degrade under domain shift beyond the verifiers training distribution (Yin et al., 2025; Jang et al., 2025), and incur multiple forward passes that conflict with real-time constraints. Methodologically, existing TTS methods involve only action decoding while keeping the visual representation fixed. Yet under perceptual ambiguity (e.g., similar distractors), selecting the best action among candidates may be insufficient without reconsidering how to perceive the scene (Bajcsy, 1988; Bohg et al., 2017; Xiong et al., 2025; Qu et al., 2025). To address these limitations, we propose SCALE (Selfuncertainty Conditioned Adaptive Looking and Execution), simple inference strategy that jointly modulates visual perception and action based on self-uncertainty, requiring no additional training or external verifier, and running in single forward pass. Our approach draws inspiration from uncertainty-driven exploration in Active Inference theory (Friston et al., 2016; Schwartenbeck et al., 2019), where agents reduce uncertainty by adapting both perception and actiona principle observed in humans (Daw et al., 2006; Wilson et al., 2014) and formalized in robotics as active perception (Bohg et al., 2017; Bajcsy et al., 2018). This principle naturally raises question: how can we quantify self-uncertainty to enable such adaptive modulation? Recent work in LLMs has estimated self-uncertainty by measuring how close the predicted output distribution is to uniform, i.e., full ambiguity (Kang et al., 2025). While this captures overall distributional uncertainty, it does not account for the models decisiveness about its top-1 choice, i.e., how confidently it commits to that selection. In VLAs, this decisiveness is equally important: greedy decoding, used in most VLAs (Kim et al., 2024; Qu et al., 2025), selects the top-1 action for immediate executionoften affecting the environment irreversiblymaking confidence in this selection essential for execution reliability. proper measure of self-uncertainty must therefore capture not only distributional uncertainty but also the models decisiveness about its top-1 action. To satisfy this requirement, our key idea is to measure where the predicted distribution lies between opposite ends of the certainty spectrum: full certainty (reflecting decisiveness in the top-1 action) and full ambiguity (reflecting overall distributional uncertainty), yielding measure that captures both aspects simultaneously. Specifically, inspired by log-likelihood ratio testing (Neyman & Pearson, 1933; Kullback & Leibler, 1951), which compares two competing hypotheses by measuring their relative likelihood, we formalize this idea by defining two reference distributionsa one-hot distribution centered on the most probable token (full certainty) and uniform distribution over all tokens (full ambiguity). This yields bounded, continuous self-uncertainty score computed solely from output logits without additional training (Sec. 3.2). We leverage this to modulate two complementary aspects in VLA  (Fig. 1)  : (1) what to do, by adjusting action sampling temperature based on token-level uncertainty (Sec. 3.3.1), and (2) how to perceive, by adjusting visual attention temperature based on step-level uncertainty (Sec. 3.3.2). In closed-loop control, these mechanisms form feedback loop: uncertainty at one timestep modulates action sampling while simultaneously adjusting visual attention for the next, enabling the model to adapt to varying conditions and execute tasks robustly. We validate SCALE on simulated and real-world benchmarks across diverse autoregressive VLA architectures, including both seen and unseen scenarios. Our method consistently improves over state-of-the-art (SoTA) VLAs and even outperforms recent TTS VLA approaches that require additional training and multiple inference passes, while maintaining single-pass efficiency suitable for real-time deployment. 2. Related Work Test-time scaling on VLA models. Allocating additional compute at inference time has proven effective in LLMs for reasoning and code generation (Snell et al., 2025; Wang et al., 2023), motivating recent extensions to VLAs through generate-and-verify strategies (Nakamoto et al., 2024; Kwok et al., 2025; Yang et al., 2025). V-GPS (Nakamoto et al., 2024) trains an offline RL value function to re-rank sampled actions, and RoboMonkey (Kwok et al., 2025) scales up action verifier training. MG-Select (Jang et al., 2025) avoids external verifiers by using the models own distribution for self-verification, yet still requires additional training and multiple samples. Despite their effectiveness, these methods share common drawbacks: additional training for verifiers or multiple forward passes, and limited generalization to unseen conditions (Nakamoto et al., 2024; Kwok et al., 2025). In contrast, we propose SCALE that leverages selfuncertainty in the output distribution, enabling adaptive inference in single forward pass without auxiliary training. Uncertainty estimation in generative models. Quantifying prediction uncertainty has been studied in generative models, particularly LLMs. Early work used output distributions for truncation-based decoding such as top-k (Fan et al., 2018) and top-p (Holtzman et al., 2020), while recent methods adaptively adjust temperature based on entropy (Zhang et al., 2024) or token difficulty (Zhu et al., 2024; Nguyen et al., 2025; Basu et al., 2021). Beyond decoding, uncertainty has guided reasoning path selection for test-time scal2 SCALE : Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models Figure 2. Overview of SCALE. (a) Adaptive Visual Attention modulates the vision encoders attention temperature γt based on uncertainty deviation from recent historysharpening focus when confident (γt < 1) and broadening exploration when uncertain (γt > 1). (b) Self-Uncertainty Estimation quantifies self-uncertainty uk by measuring where the predicted distribution pk lies relative to two references: one-hot qlow (full certainty) and uniform qhigh (full ambiguity). (c) Adaptive Action Decoding scales sampling temperature τ based on token-level uncertainty ukenabling near-greedy execution under confidence and diverse sampling under ambiguity. (d) Visual Attention Temperature Update compares the current step-level uncertainty ut against its recent history (EMA, ut1) to obtain deviation ut := ut ut1, then converts it into attention temperature γt+1when ut exceeds the EMA (ut > 0), γt+1 > 1 broadens attention (explore); when below (ut < 0), γt+1 < 1 sharpens attention (focus). ing (Fu et al., 2025), with extensions to VLMs (Fang et al., 2025) and VLAs (Valle et al., 2025; Zollo & Zemel, 2025; Gu et al., 2025; Romer et al., 2025). However, these VLA methods leverage uncertainty for failure prediction or calibration analysis, rather than modulating inference behavior. Recently, Self-certainty (Kang et al., 2025) in LLMs proposed distributional confidence, measuring divergence from uniform distribution to estimate uncertainty from output logitsoffering training-free alternative to methods requiring external modules. However, this formulation captures only overall distributional uncertainty, not how confidently the model commits to its top-1 choicecritical for VLAs where greedy decoding selects the top-1 action for immediate execution. We address this by proposing dual-reference measure that captures both distributional spread and top-1 decisiveness, leveraging it for jointly modulating perception and action in VLAs. Visual attention for VLMs and VLAs. Effectively allocating attention to task-relevant image regions is crucial for VLM accuracy and hallucination mitigation (Zhang et al., 2025a; Chen et al., 2025). This extends to VLAs, where focusing on manipulation-relevant regions improves performance (Wu et al., 2025; Zhang et al., 2025b; Xiao et al., 2025; Song et al., 2025). However, existing methods rely on contrastive masking or trained modules to regulate visual processing. In contrast, we propose training-free approach that dynamically modulates visual attention during executionbroadening exploration under uncertainty and sharpening focus under confidenceenabling adaptive perception specifically suited for closed-loop VLA control. 3. Approach To enhance VLA robustness at test time without external verifiers or multiple rollouts, we present SCALE (Selfuncertainty Conditioned Adaptive Looking and Execution), single-pass adaptive inference strategy that jointly modulates action decoding and visual attention based on the models own predictive uncertainty  (Fig. 2)  . Our key insight is that self-uncertainty derived from the output distribution serves as an intrinsic signal to balance exploitation and explorationa principle central to Active Inference theory (Friston et al., 2016), where agents minimize expected free energy by maximizing information gain under ambiguity. Below, we first formalize the problem setup (Sec. 3.1), introduce our self-uncertainty measure (Sec. 3.2), and describe how SCALE leverages this signal for adaptive inference (Sec. 3.3). 3.1. Preliminaries and Motivation We consider autoregressive VLA policies πθ that, at each timestep t, predict actions discretized into sequence of tokens at = (a1 ). Each action is mapped to discrete token from an action vocabulary V, allowing πθ to factorize as: , . . . , aK πθ(at vt, I) = (cid:89) k=1 (cid:0)ak πθ vt, I, a<k (cid:1), (1) where is the language instruction, a<k denotes previously decoded tokens, and vt = fϕ(ot; γ) is the visual representation obtained by processing the raw observation ot through 3 SCALE : Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models transformer-based vision encoder fϕ (e.g., SigLIP (Zhai et al., 2023)) with attention temperature γ (default γ = 1). (x). At each token position k, the πθ produces the logit vect RV, yielding categorical distribution pk tor ℓk = softmax(ℓk ), and we denote the top-1 probability as t,max = maxxV pk pk In practice, most autoregressive VLAs follow fixed inference pipeline: the frozen vision encoder processes observations, and greedy decoding selects the top-1 token at each step (Kim et al., 2024; Qu et al., 2025). While effective in many cases, this fixed pipeline may struggle under ambiguous situationsperceptual ambiguity (i.e., visually similar distractors are present) or action multimodality (i.e., multiple plausible actions exist for given state)where greedy decoding overlooks viable alternatives, and fixed visual processing may miss task-relevant cues, as suggested by work on active perception in robotics (Bajcsy, 1988; Bohg et al., 2017) and attention analysis in VLMs (Chen et al., 2025). In closed-loop control, where each action influences subsequent observations, such rigidity can compound mistakes over time (Ross et al., 2011). 3.2. Self-Uncertainty via Distributional Positioning To move beyond the rigidity of fixed inference, we aim to adaptively modulate both action and perception based on the models internal uncertaintybroadening exploration under ambiguity while focusing on exploitation when confident. As motivated in Sec. 1, such modulation requires measure that captures both overall distributional uncertainty and decisiveness in the top-1 action; we achieve this by comparing distances to opposite ends of the certainty spectrum. Specifically, inspired by log-likelihood ratio testing (Neyman & Pearson, 1933; Kullback & Leibler, 1951), which compares two competing hypotheses by measuring their relative likelihood, we define two reference distributions representing each extreme: Low-uncertainty reference (qlow): one-hot distribution on the models top-1 token, representing full certaintycomplete commitment to its current choice (see Appendix for discussion). Implemented as qlow(x) = 1ϵ for = arg max pk V1 otherwise, with small ϵ for numerical stability (Appendix B). and ϵ High-uncertainty reference (qhigh): uniform distribution qhigh(x) = 1/V for all V, representing full ambiguitycomplete distributional uncertainty. (cid:0)pk uk = DKL The self-uncertainty uk at position is then defined as: qlow(cid:1) DKL This formulation compares how well each reference explains the predicted distribution, effectively positioning it on the certainty spectrum between full certainty and full ambiguity. qhigh(cid:1) . (cid:0)pk (2) Interpretation. Expanding Equation 2 yields: = uk xpk (cid:20) log qhigh(x) qlow(x) (cid:21) , (3) which is precisely the expected log-likelihood ratio between the two references under the models own predictive distribution (see Appendix for derivation). Intuitively, uk > 0 indicates the distribution lies closer to full ambiguity than full certainty, signaling high uncertainty. This formulation inherits the benefits of distributional approaches (Kang et al., 2025)requiring only output logits without additional trainingwhile directly reflecting top-1 confidence through qlow. We empirically show that this measure outperforms existing uncertainty proxies (Sec. 4.2.1) and consistently improves diverse VLA backbones (Sec. 4.2). 3.3. SCALE: Uncertainty-Driven Adaptive Inference Having defined our self-uncertainty uk , we leverage it to jointly modulate action decoding and visual attention, following simple design principle: explore broadly under uncertainty, focus sharply under confidence. 3.3.1. ADAPTIVE ACTION DECODING To realize this principle at the action level, we dynamically adjust the sampling temperature τ of VLA policy πθ based on action token-level self-uncertainty: ), = T0 σ(uk τ where T0 is the maximum temperature defining the exploration range, and the sigmoid σ(uk ) acts as gate that adjusts exploration based on situational uncertaintyhigh uncertainty opens the gate for diverse sampling, low uncertainty closes it for focused execution. (4) Since uk can be interpreted as log-likelihood ratio between the hypotheses uncertain and confident, applying the sigmoid function recovers the posterior probability of the uncertain hypothesis: σ(uk ) (see Appendix for details), serving as soft gate (Hochreiter & Schmidhuber, 1997; Dauphin et al., 2017) that yields τ 0 (near-greedy) under low uncertainty and τ T0 (explorative) under high uncertainty. ) = (uncertain pk The action token is then sampled from the temperaturescaled distribution: Cat(softmax(ℓk ak /τ )), (5) where Cat denotes the categorical distribution. 3.3.2. ADAPTIVE VISUAL ATTENTION At the perception level, the same principle applies: broaden attention under uncertainty to gather information, sharpen 4 SCALE : Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models it under confidence for focused execution (Bajcsy, 1988; Bohg et al., 2017). This modulation can occur at two points: the vision encoder fϕ, which controls what visual information is extracted via uni-modal attention (Zou et al., 2024), or the VLA backbone πθ, which controls which extracted features are attended to via cross-modal attention (Chen et al., 2025). We choose to modulate the vision encoder, as it is the first stage that directly determines what visual information to extractcross-modal attention can only select from what has already been encoded; we empirically compare both strategies in Sec. 4.2.1 and find that vision encoder modulation is more effective for VLAs. Given this choice, the remaining question is how uncertainty should guide this modulation. For action decoding, each tokens instantaneous uncertainty uk directly determines its sampling temperature without considering temporal context. Visual modulation, however, must respond to evolving scene conditions across timesteps (Bajcsy, 1988), as perception inherently relies on temporal context to interpret the current observation (Rao & Ballard, 1999; Clark, 2013). We therefore argue that comparing current uncertainty to recent historyrather than using its instantaneous value alonebetter captures transitions in scene complexity; we validate this empirically in Sec. 4.2.1. To implement this, we first aggregate token-level uncertainties into step-level uncertainty ut by averaging following prior work (Kang et al., 2025), as visual modulation operates at the step-level: ut = 1 (cid:88) k=1 uk . We then maintain an exponential moving average (EMA) of recent uncertainty: ut = αut1 + (1 α)ut, (7) where hyperparameter α controls temporal smoothing. We detect transitions in scene complexity via the deviation from this average. In particular, for efficient single-pass execution, we modulate visual attention at timestep using the preceding steps deviation ut1 = ut1 ut2, since ut is only available after action decoding, which occurs downstream of visual encoding. We posit that consecutive visual frames are highly correlated (Bovik, 2010), making uncertainty at 1 reliable proxy for timestep t; empirically, using ut with an additional forward pass yields similar performance (see Appendix E), supporting this assumption. We convert this deviation into an attention temperature γt for modulating the vision encoders attention. Following prior work (Dinh et al., 2017), we obtain γt by applying tanh and exponentiation to the deviation, yielding value Algorithm 1 SCALE; Adaptive Looking and Execution 1: Input: Observation ot, instruction I, uncertainty deviation ut1 2: Output: Sequence of action tokens at, uncertainty deviation ut // Visual attention scaled by γt qhigh) // Token3: 4: // Adaptive visual attention (Sec. 3.3.2) 5: γt κtanh(ut1) 6: vt fϕ(ot; γt) 7: 8: // Adaptive action decoding (Sec. 3.3.1) 9: for = 1 to do 10: 11: 12: πθ(vt, I, a<k ℓk softmax(ℓk pk ) uk DKL(pk level uncertainty (Sec. 3.2) τ T0 σ(uk ) Cat(softmax(ℓk ak qlow) DKL(pk /τ )) ) 13: 14: 15: end for 16: (cid:80)K 17: ut 1 18: ut αut1 + (1 α)ut 19: ut ut ut1 20: Return: (a1, . . . , aK), ut k=1 uk // Step-level uncertainty // Update EMA // Compute uncertainty deviation centered at 1 so that zero deviation produces no modulation: γt = κtanh(ut1), (8) where κ > 1 bounds γt (1/κ, κ) for stability. (6) We then apply γt across all layers of the vision encoder fϕ to scale the self-attention, following Zou et al. (2024): Attn(Q, K, ) = softmax (cid:19) (cid:18) QK γt V. (9) This yields γt > 1 (flattened attention for broader exploration) when uncertainty rises above its recent average, and γt < 1 (sharpened attention for focused perception) when it falls below, as shown in Fig. 3. Together with adaptive action decoding (Sec. 3.3.1), the entire procedure requires only single forward pass per control step: uncertainty is computed from logits during action decoding, and visual modulation reuses the previous steps uncertaintyrequiring no additional rollouts, external verifiers, or auxiliary training (see Appendix for cost comparison). Algorithm 1 summarizes the procedure. 4. Experiments 4.1. Setups We evaluate SCALE in both simulation and real-world settings using multiple autoregressive VLA backbones: OpenVLA (Kim et al., 2024), π0-FAST (Pertsch et al., 2025), and 5 SCALE : Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models Table 1. SR (%) on LIBERO with OpenVLA backbone. All methods, including ours, use OpenVLA fine-tuned on LIBERO as the base model. Results for TTS methods are taken from their respective papers; all others are reproduced in our experiments. denotes results reproduced using authors implementation with greedy decoding. indicates results not reported in prior work. Table 3. SR (%) on SIMPLER-WidowX with π0-FAST and SpatialVLA backbones. π0-FAST and SpatialVLA (fine-tuned) are trained on BridgeData V2; SpatialVLA (zero-shot) is trained on OXE (ONeill et al., 2024) and evaluated zero-shot. reproduced with official implementation using greedy decoding. Method Spatial Object Goal Long Avg. Training-required, test-time scaling RoboMonkey TACO MG-Select 81.7 Training-free, single inference OpenVLA (fine-tuned) + Sampling (t=1.0) + Top-k (k=40, t=0.7) + Top-p (p=0.9) + SCALE (Ours) 86.2 85.1 85.2 86.9 89. 72.5 86.2 87.9 88.2 88.1 91.0 73.6 77.7 78.9 78.3 78.6 82.3 56.5 60.0 55.4 52.7 54.7 55.2 55.1 63. 70.8 75.7 76.7 76.7 77.2 81.5 Table 2. SR (%) on LIBERO with π0-FAST backbone fine-tuned on LIBERO. reproduced with greedy decoding. Method π0-FAST (fine-tuned) + Sampling (t=1.0) + Top-k (k=40, t=0.7) + Top-p (p=0.9) + SCALE (Ours) Spatial Object Goal Long Avg. 96.6 87.0 93.7 90.2 97. 98.1 94.6 96.5 95.3 98.7 93.7 83.5 87.5 85.9 94.7 76.3 72.2 74.8 73.4 80.9 91.2 84.3 88.1 86.2 93.0 SpatialVLA (Qu et al., 2025) in simulation, and OpenVLA and π0-FAST in real-world experiments. We build upon the authors official codebases1 to reproduce baseline results, applying SCALE and other sampling strategies on top of these implementations. Following prior work, each model is evaluated either fine-tuned or zero-shot depending on the benchmark, as indicated in each table. See Appendix for all hyperparameter settings, including T0, κ, and α. Simulation benchmarks. We use three complementary benchmarks: LIBERO (Liu et al., 2023) for multi-task generalization across object, layout, goal, and long-horizon variations; SIMPLER-WidowX (Li et al., 2024) for execution precision in real-to-sim pick-and-place; and LIBERO-PROLong (Zhou et al., 2025), the most challenging split, as an unseen benchmark for robustness beyond memorization (see Appendix G.1 for more details about benchmarks). Real-world setup. Following prior work (Jang et al., 2025), we evaluate under both in-distribution (ID) and out-ofdistribution (OOD) conditions using 6-DoF UR10e arm equipped with Robotiq 2F-85 gripper. ID tasks consist of three Put on pick-and-place tasks involving objects of different geometries (e.g., carrot, eggplant, lemon); OOD tasks follow similar task setup, but introduce unseen objects with more challenging geometries and compliances 1Official repositories: openvla/openvla, PhysicalIntelligence/openpi, SpatialVLA/SpatialVLA all on GitHub. Method π0-FAST (fine-tuned) + Sampling (t=1.0) + Top-k (k=40, t=0.7) + Top-p (p=0.9) + SCALE (Ours) SpatialVLA (fine-tuned) + Sampling (t=1.0) + Top-k (k=40, t=0.7) + Top-p (p=0.9) + SCALE (Ours) SpatialVLA (zero-shot) + Sampling (t=1.0) + Top-k (k=40, t=0.7) + Top-p (p=0.9) + SCALE (Ours) Spoon Carrot Cube Eggplant Avg. 20.8 41.7 48.6 48.6 58.3 20.8 18.1 12.5 18.1 22.2 12.5 16.7 8.3 13.9 22. 62.5 54.2 61.1 62.5 69.4 25.0 20.8 23.6 23.6 31.9 20.8 23.6 29.2 26.4 34.7 37.5 29.2 41.7 33.3 48.6 20.8 25.0 20.8 25.0 26.4 25.0 22.2 20.8 22.2 34. 16.7 16.7 15.3 16.7 19.4 100.0 100.0 100.0 98.6 100.0 66.7 66.7 61.1 61.1 75.0 34.4 35.4 41.7 40.3 49.0 41.7 41.0 39.2 41.3 45.1 31.3 32.3 29.9 30.9 41. Table 4. SR (%) on LIBERO-PRO-Long under various perturbations with OpenVLA and π0-FAST backbones. Both models are trained on LIBERO and evaluated zero-shot. reproduced with official implementation using greedy decoding. Method OpenVLA (zero-shot) + Sampling (t=1.0) + Top-k (k=40, t=0.7) + Top-p (p=0.9) + SCALE (Ours) π0-FAST (zero-shot) + Sampling (t=1.0) + Top-k (k=40, t=0.7) + Top-p (p=0.9) + SCALE (Ours) Language Object Task Swap Avg. 42.0 44.8 44.6 44.2 51.2 78.0 74.8 75.6 75.4 84.0 26.6 27.6 26.0 28.2 30. 49.0 46.2 44.6 44.8 51.8 3.2 3.2 4.0 3.6 4.8 13.6 13.8 14.6 14.2 15.8 0.0 0.0 0.0 0.0 0.0 2.2 2.4 2.6 2.6 3.4 18.0 18.9 18.7 19.0 21. 35.7 34.3 34.4 34.3 38.8 (e.g., soft teddy bear, small cube). We fine-tune OpenVLA and π0-FAST on 48 teleoperated demonstrations per ID task prior to evaluation (see Appendix G.2). Baselines and metric. For simulation, we compare against: (1) Training-required TTS: RoboMonkey (Kwok et al., 2025), TACO (Yang et al., 2025), and MG-Select (Jang et al., 2025), which perform Best-of-N selection with external or self-verification; (2) Training-free decoding: temperature (Radford et al., 2019), top-k (Fan et al., 2018), and top-p sampling (Holtzman et al., 2020). Hyperparameters are detailed in the tables, with sensitivity analysis in Appendix I. For real-world experiments, we compare against greedy decoding, as training-free alternatives show similar performance in simulation. We report success rate (SR, %) as the metric , averaged over three seeds for simulation and 24 episodes per task for real-world evaluation. 6 SCALE : Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models Table 5. SR (%) on real-world Put on pick-and-place tasks (A/B = object/receptacle) under ID and OOD conditions. We compare SCALE against greedy decoding on OpenVLA and π0-FAST backbones fine-tuned on our teleoperated demonstrations. In-Distribution Out-of-Distribution Method OpenVLA + SCALE (Ours) π0-FAST + SCALE (Ours) Carrot/Towel Eggplant/Bowl Lemon/Plate Avg. Teddy Bear/Bowl Cube/Plate Avg. 45.8 75.0 66.7 87.5 45.8 62.5 75.0 87. 16.7 29.2 75.0 83.3 36.1 55.6 72.2 86.1 29.2 45.8 37.5 50. 16.7 33.3 50.0 62.5 22.9 39.6 43.8 56.3 Table 6. We evaluate the contribution of adaptive decoding (Ada. Decoding) and adaptive visual attention (Ada. Visual Attention). Both components provide complementary gains, achieving the best performance when combined. Without adaptive decoding, we use greedy decoding by default. The last row corresponds to SCALE. Ada. Decoding Ada. Visual Attention. SR (%) 52.7 58.0 56.0 63.3 Table 7. Comparison of different uncertainty metrics for adaptive decoding and perception. Confidence-based methods are inverted to represent uncertainty. All variants use the same adaptive action decoding and visual attention modulation pipeline. Table 8. Design choices for visual modulation. We compare three dimensions: (1) modulation targetuni-modal attention (attn.) in vision encoder fϕ vs. cross-modal attention in VLA πθ; (2) modulation strategyFixed (w/ sign; binary switch at ut1 = 0) vs. Adaptive (continuous scaling); and (3) uncertainty signalinstantaneous (ut1) vs. change-based (ut1). All variants include adaptive action decoding. The last row shows SCALE. Modulation Target Strategy Signal SR (%) Baseline; OpenVLA πθ cross-modal attn. πθ cross-modal attn. Adaptive Fixed sign(ut1) ut1 fϕ uni-modal attn. fϕ uni-modal attn. Adaptive Adaptive ut1 ut1 52.7 54.8 57.4 55.4 63.3 Metric Baseline; OpenVLA Confidence-based pmax (Hendrycks & Gimpel, 2017) Self-certainty (Kang et al., 2025) Uncertainty-based Gini Impurity (Breiman et al., 2017) Entropy (Malinin & Gales, 2021) Self-uncertainty (Ours) SR (%) 52.7 56.0 53.8 57.8 55.4 63. 4.2. Quantitative Analyses Tables 15 summarize our main results across simulation and real-world benchmarks. We highlight three key findings. (1) SCALE consistently improves over greedy decoding across all benchmarks and backbones. In simulation, SCALE achieves gains on LIBERO (+5.8 avg. with OpenVLA, +1.8 with π0-FAST; Tables 1, 2), SIMPLER-WidowX (+3.4/+10.4 with SpatialVLA fine-tuned/zero-shot, +14.6 with π0-FAST; Table 3), and LIBERO-PRO-Long (+3.5 with OpenVLA, +3.1 with π0-FAST; Table 4). In realworld experiments  (Table 5)  , gains are more pronounced under both in-distribution (+19.5 with OpenVLA, +13.9 with π0-FAST) and out-of-distribution conditions (+16.7 with OpenVLA, +12.5 with π0-FAST). These results demonstrate that SCALE generalizes across architectures, task complexities, and deployment settings. mance, while SCALE provides robust improvements. On LIBERO with π0-FAST  (Table 2)  , temperature sampling, top-k, and top-p sampling all underperform greedy decoding (91.2 84.3/88.1/86.2). We attribute this to fixed hyperparameters that cannot adapt to varying uncertainty across states and tasks; indeed, no single setting consistently outperforms greedy decoding across all benchmarks (Appendix I). In contrast, SCALE dynamically adjusts sampling temperature based on the models predicted uncertainty, achieving robust improvements. (3) SCALE outperforms training-required TTS methods while remaining training-free and single-pass. On LIBERO with OpenVLA  (Table 1)  , SCALE outperforms MG-Select by +10.7 points on average (81.5 vs. 70.8), with notable gap on long-horizon tasks (63.3 vs. 55.4). SCALE also surpasses RoboMonkey and TACO on LIBERO-Long by +6.8 and +3.3 points, respectively (see Appendix for per-task breakdown). This demonstrates that adaptive modulation achieves superior performance without external verification or additional forward passes. 4.2.1. DETAILED ANALYSES For detailed analyses, we use OpenVLA (Kim et al., 2024) on the LIBERO-Long benchmark (Liu et al., 2023), as it provides challenging long-horizon tasks well suited to evaluate adaptive inference across diverse scenarios (Appendix J). (2) Naive decoding strategies often degrade perforAblation study. Table 6 shows the contribution of each 7 SCALE : Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models Figure 3. Qualitative result of adaptive visual attention. We visualize attention from SigLIP, the vision encoder fϕ of OpenVLA, at t=45 when self-uncertainty suddenly increases; color indicates attention intensity (blue: low, green: medium, red: high). component in SCALE. Introducing either adaptive decoding or adaptive visual attention alone improves performance over the baseline (rows 23 vs. row 1). Combining both (row 4; SCALE) achieves the best result, demonstrating that the two mechanisms are complementary. Self-uncertainty measure. We compare our selfuncertainty against alternatives (Tab. 7): pmax, Gini impurity, Entropy and Self-Certainty. For fair comparison, we normalize each metric to [0, 1]inverting confidencebased scores (e.g., pmax and Self-Certainty) to represent uncertaintyand apply the same inference pipeline: replacing σ(uk ) for action decoding and using the momentumbased scheme (Eq. 8) for visual attention (see Appendix for details). While all uncertainty measures improve over the baseline (row 1), our formulation achieves the highest success rate, outperforming the next best by 5.5. Notably, Self-Certainty (Kang et al., 2025), which captures only distributional uncertainty, yields marginal gains. These results suggest that our dual-reference measure is better suited for VLAs, as it captures both uncertainty and decisiveness, simultaneouslyessential for adaptive looking and execution. Design choices for visual modulation. Tab. 8 compares three design choices: (1) modulation targetuni-modal attention in vision encoder fϕ vs. cross-modal attention in VLA πθ, (2) modulation strategyfixed (with binary switching based on sign(ut1)) vs. adaptive (continuous scaling), and (3) uncertainty signalinstantaneous (ut1) vs. change-based (u). The last row corresponds to SCALE. Modulating uni-modal attention outperforms cross-modal attention (63.3 vs. 57.4), confirming that adjusting what is captured before fusion is more effective than adjusting how it is integrated. Adaptive outperforms fixed modulation (57.4 vs. 54.8), suggesting that continuous scaling provides finer-grained control than binary switching. Change-based outperforms instantaneous uncertainty (63.3 vs. 55.4), supporting our hypothesis that tracking uncertainty transitions better captures changes in scene complexity. 8 Figure 4. Qualitative results of adaptive action decoding. We compare greedy decoding (top) and SCALE (middle) on the realworld task using π0-FAST; blue arrows indicate robot motion. 4.3. Qualitative Analyses Figures 3 and 4 illustrate the effect of adaptive visual attention and action decoding, respectively. For visual attention  (Fig. 3)  , at t=45 when self-uncertainty suddenly increases, the baseline with fixed γt=1 attends to task-irrelevant regions such as the microwave door while underattending to the target mug, leading to task failure. In contrast, SCALE dynamically increases γt to broaden attention across the scene, redirecting focus to the task-relevant mugenabling successful grasp (t=90) and eventual task completion. For action decoding  (Fig. 4)  , the bottom plot shows ut temporal dynamics: initially high due to multiple viable options, then dropping during grasping. Greedy decoding follows direct path and collides with the bowl, whereas SCALE leverages high uncertainty to find an elevated trajectory that clears the obstacle (yellow phase); once the robot reaches stable position, uncertainty drops, leading to task success (green phase). These results demonstrate that adaptive modulation of both perception and action is effective for robust closed-loop control. 5. Conclusion We presented SCALE, simple inference strategy that enhances VLA robustness by jointly modulating perception and action based on self-uncertaintyrequiring no additional training, no external verifier, and only single forward pass. Inspired by uncertainty-driven exploration in Active Inference theory, SCALE broadens exploration under ambiguity while focusing on exploitation when confident. Central to our approach is self-uncertainty measure that compares distances to both extremes of the certainty spectrum, capturing both distributional concentration and top-1 confidence. Experiments on simulated and real-world benchmarks show that SCALE consistently improves SoTA VLAs and outperforms existing TTS methods. SCALE : Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents SCALE, test-time inference strategy for Vision-Language-Action models in robotic control. Our work aims to enhance the robustness of embodied AI systems by enabling adaptive perception and action under uncertainty, potentially improving the safety and reliability of robots operating in diverse real-world environments. The broader implications of more capable robotic systems include both benefitssuch as improved automation in manufacturing, healthcare, and assistive technologiesand potential concerns around workforce displacement and safety in human-robot interaction. However, as our contribution is primarily methodological and focuses on improving existing VLA architectures without introducing new capabilities, we do not anticipate unique ethical concerns beyond those inherent to the broader field of robot learning. We encourage practitioners deploying such systems to carefully consider safety protocols and human oversight in their applications."
        },
        {
            "title": "References",
            "content": "Bajcsy, R. Active perception. Proceedings of the IEEE, 76 (8):9661005, 1988. Bajcsy, R., Aloimonos, Y., and Tsotsos, J. K. Revisiting active perception. Autonomous Robots, 42(2):177196, 2018. Basu, S., Ramachandran, G. S., Keskar, N. S., and Varshney, L. R. Mirostat: neural text decoding algorithm that directly controls perplexity. In ICLR, 2021. Bohg, J., Hausman, K., Sankaran, B., Brock, O., Kragic, D., Schaal, S., and Sukhatme, G. S. Interactive perception: Leveraging action in perception and perception in action. IEEE Transactions on Robotics, 33(6):12731291, 2017. Bovik, A. C. Handbook of image and video processing. Academic press, 2010. Breiman, L., Friedman, J., Olshen, R. A., and Stone, C. J. Classification and regression trees. Chapman and Hall/CRC, 2017. Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Hsu, J., et al. Rt-1: Robotics transformer for real-world control at scale. In RSS, 2023. Chen, S., Zhu, T., Zhou, R., Zhang, J., Gao, S., Niebles, J. C., Geva, M., He, J., Wu, J., and Li, M. Why is spatial reasoning hard for vlms? an attention mechanism perspective on focus areas. In ICML, 2025. with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. Clark, A. Whatever next? predictive brains, situated agents, and the future of cognitive science. Behavioral and brain sciences, 36(3):181204, 2013. Dauphin, Y. N., Fan, A., Auli, M., and Grangier, D. Language modeling with gated convolutional networks. In ICML, 2017. Daw, N. D., Odoherty, J. P., Dayan, P., Seymour, B., and Dolan, R. J. Cortical substrates for exploratory decisions in humans. Nature, 441(7095):876879, 2006. Dinh, L., Sohl-Dickstein, J., and Bengio, S. Density estimation using real nvp. In ICLR, 2017. Driess, D., Xia, F., Sajjadi, M. S. M., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, W., Chebotar, Y., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint, M., Greff, K., Zeng, A., Mordatch, I., and Florence, P. Palm-e: an embodied multimodal language model. In ICML, 2023. Fan, A., Lewis, M., and Dauphin, Y. Hierarchical neural story generation. In ACL, 2018. Fang, Y., Yang, Z., Chen, Z., Zhao, Z., and Zhou, J. Scalable best-of-n selection for large language models via selfcertainty. In NeurIPS, 2025. Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., ODoherty, J., and Pezzulo, G. Active inference and learning. Neuroscience & Biobehavioral Reviews, 68: 862879, 2016. Fu, Y., Wang, X., Tian, Y., and Zhao, J. Deep think with confidence. arXiv preprint arXiv:2508.15260, 2025. Gao, L., Schulman, J., and Hilton, J. Scaling laws for reward model overoptimization. In ICML, 2023. Gu, Q., Ju, Y., Sun, S., Gilitschenski, I., Nishimura, H., Itkina, M., and Shkurti, F. Safe: Multitask failure detection for vision-language-action models. In NeurIPS, 2025. Hendrycks, D. and Gimpel, K. baseline for detecting misclassified and out-of-distribution examples in neural networks. In ICLR, 2017. Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural computation, 9(8):17351780, 1997. Chen, Z., Wang, W., Cao, Y., Liu, Y., Gao, Z., Cui, E., Zhu, J., Ye, S., Tian, H., Liu, Z., et al. Expanding performance boundaries of open-source multimodal models Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. The curious case of neural text degeneration. In ICLR, 2020. 9 SCALE : Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. In ICLR, 2022. Iyer, A., Peng, Z., Dai, Y., Guzey, I., Haldar, S., Chintala, S., and Pinto, L. Open teach: versatile teleoperation system for robotic manipulation. In CoRL, 2024. Jang, S., Kim, D., Kim, C., Kim, Y., and Shin, J. Verifierfree test-time sampling for vision language action models. arXiv preprint arXiv:2510.05681, 2025. Kang, Z., Zhao, X., and Song, D. Scalable best-of-n selection for large language models via self-certainty. In NeurIPS, 2025. Kim, M. J., Pertsch, K., Karamcheti, S., Xiao, T., Balakrishna, A., Nair, S., Rafailov, R., Foster, E. P., Sanketi, P. R., Vuong, Q., Kollar, T., Burchfiel, B., Tedrake, R., Sadigh, D., Levine, S., Liang, P., and Finn, C. Openvla: An open-source vision-language-action model. In CoRL, 2024. ONeill, A., Rehman, A., Maddukuri, A., Gupta, A., Padalkar, A., Lee, A., Pooley, A., Gupta, A., Mandlekar, A., Jain, A., et al. Open x-embodiment: robotic learning datasets and rt-x models. In ICRA, 2024. Pertsch, K., Stachowicz, K., Ichter, B., Driess, D., Nair, S., Vuong, Q., Mees, O., Finn, C., and Levine, S. Fast: Efficient action tokenization for vision-language-action models. In RSS, 2025. Qu, D., Song, H., Chen, Q., Yao, Y., Ye, X., Ding, Y., Wang, Z., Gu, J., Zhao, B., and Wang, D. Spatialvla: Exploring spatial representations for visual-language-action model. In RSS, 2025. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Rao, R. P. and Ballard, D. H. Predictive coding in the visual cortex: functional interpretation of some extra-classical receptive-field effects. Nature neuroscience, 2(1):7987, 1999. Kullback, S. and Leibler, R. A. On information and sufficiency. The annals of mathematical statistics, 22(1): 7986, 1951. Romer, R., Kobras, A., Worbis, L., and Schoellig, A. P. Failure prediction at runtime for generative robot policies. In NeurIPS, 2025. Kwok, J., Agia, C., Sinha, R., Foutter, M., Li, S., Stoica, I., Mirhoseini, A., and Pavone, M. Robomonkey: Scaling test-time sampling and verification for vision-languageaction models. In CoRL, 2025. Li, X., Hsu, K., Gu, J., Pertsch, K., Mees, O., Walke, H. R., Fu, C., Lunawat, I., Sieh, I., Kirmani, S., et al. Evaluating real-world robot manipulation policies in simulation. In CoRL, 2024. Liu, B., Zhu, Y., Gao, C., Feng, Y., Liu, Q., Zhu, Y., and Stone, P. Libero: Benchmarking knowledge transfer for lifelong robot learning. In NeurIPS, 2023. Malinin, A. and Gales, M. Uncertainty estimation in autoregressive structured prediction. In ICLR, 2021. Nakamoto, M., Mees, O., Kumar, A., and Levine, S. Steering your generalists: Improving robotic foundation models via value guidance. In CoRL, 2024. Neyman, J. and Pearson, E. S. Ix. on the problem of the most efficient tests of statistical hypotheses. Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of Mathematical or Physical Character, 231(694-706):289337, 1933. Nguyen, M. N., Baker, A., Neo, C., Roush, A., Kirsch, A., and Shwartz-Ziv, R. Turning up the heat: Min-p sampling for creative and coherent llm outputs. In ICLR, 2025. Ross, S., Gordon, G., and Bagnell, D. reduction of imitation learning and structured prediction to no-regret online learning. In AISTATS, 2011. Schwartenbeck, P., Passecker, J., Hauser, T. U., FitzGerald, T. H., Kronbichler, M., and Friston, K. J. Computational mechanisms of curiosity and goal-directed exploration. elife, 8:e41703, 2019. Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm test-time compute optimally can be more effective than scaling model parameters. In ICLR, 2025. Song, W., Zhou, Z., Zhao, H., Chen, J., Ding, P., Yan, H., Huang, Y., Tang, F., Wang, D., and Li, H. Reconvla: Reconstructive vision-language-action model as effective robot perceiver. arXiv preprint arXiv:2508.10333, 2025. Valle, P., Lu, C., Ali, S., and Arrieta, A. Evaluating uncertainty and quality of visual language action-enabled robots. arXiv preprint arXiv:2507.17049, 2025. Walke, H., Black, K., Lee, A., Kim, M. J., Du, M., Zheng, C., Zhao, T., Hansen-Estruch, P., Vuong, Q., He, A., Myers, V., Fang, K., Finn, C., and Levine, S. Bridgedata v2: dataset for robot learning at scale. In CoRL, 2023. Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. In ICLR, 2023. SCALE : Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models Wilson, R. C., Geana, A., White, J. M., Ludvig, E. A., and Cohen, J. D. Humans use directed and random exploration to solve the exploreexploit dilemma. Journal of experimental psychology: General, 143(6):2074, 2014. Wu, S., Luo, X., Zhang, J., Xie, J., Song, J., Shen, H. T., and Gao, L. Policy contrastive decoding for robotic foundation models. arXiv preprint arXiv:2505.13255, 2025. Zhu, Y., Li, J., Li, G., Zhao, Y., Jin, Z., and Mei, H. Hot or cold? adaptive temperature sampling for code generation with large language models. In AAAI, 2024. Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F., Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In CoRL, 2023. Zollo, T. P. and Zemel, R. in vision-language-action models. arXiv:2507.17383, 2025."
        },
        {
            "title": "Confidence calibration\narXiv preprint",
            "content": "Zou, Y., Ma, R., Li, Y., and Li, R. Attention temperature matters in ViT-based cross-domain few-shot learning. In NeurIPS, 2024. Xiao, L., Li, J., Gao, J., Ye, F., Jin, Y., Qian, J., Zhang, J., Wu, Y., and Yu, X. Ava-vla: Improving vision-languageaction models with active visual attention. arXiv preprint arXiv:2511.18960, 2025. Xiong, H., Xu, X., Wu, J., Hou, Y., Bohg, J., and Song, S. Vision in action: Learning active perception from human demonstrations. In CoRL, 2025. Yang, S., Zhang, Y., He, H., Pan, L., Li, X., Bai, C., and Li, X. Steering vision-language-action models as antiexploration: test-time scaling approach. arXiv preprint arXiv:2512.02834, 2025. Yin, Z., Sun, Q., Zeng, Z., Cheng, Q., Qiu, X., and Huang, X. Dynamic and generalizable process reward modeling. In ACL, 2025. Zhai, X., Mustafa, B., Kolesnikov, A., and Beyer, L. SigIn ICCV, moid loss for language image pre-training. 2023. Zhang, J., Khayatkhoei, M., Chhikara, P., and Ilievski, F. Mllms know where to look: Training-free perception of small visual details with multimodal llms. In ICLR, 2025a. Zhang, J., Memmel, M., Kim, K., Fox, D., Thomason, J., Ramos, F., Bıyık, E., Gupta, A., and Li, A. Peek: Guiding and minimal image representations for zero-shot generalization of robot manipulation policies. arXiv preprint arXiv:2509.18282, 2025b. Zhang, S., Bao, Y., and Huang, S. Edt: Improving large language models generation by entropy-based dynamic temperature sampling. arXiv preprint arXiv:2403.14541, 2024. Zhou, X., Xu, Y., Tie, G., Chen, Y., Zhang, G., Chu, D., Zhou, P., and Sun, L. Libero-pro: Towards robust and fair evaluation of vision-language-action models beyond memorization. arXiv preprint arXiv:2510.03827, 2025. Zhu, J., Chen, Z., Wang, W., et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 11 SCALE : Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models A. On the Design of Low-Uncertainty Reference potential concern with our low-uncertainty reference qlow is its self-referential nature: it anchors to the models own top-1 prediction rather than an external ground truth. However, this design choice aligns with our goal. Goal: Measuring conviction, not correctness. Our objective is not to assess whether the models prediction is correct, but rather how decisive it is about its current choice. By anchoring qlow to the models own top-1 token, our measure directly quantifies this internal convictionhigh conviction signals low uncertainty requiring less exploration, while diffuse distribution signals ambiguity warranting broader exploration. Empirical validation. For this self-referential design to be meaningful, conviction should carry task-relevant information. We analyzed the relationship between episode-level average pmax and task success rate across 6,000 episodes on LIBERO benchmarks. As shown in Fig. 5, success rates decline sharply in the lowest pmax regime, indicating that conviction reliably identifies high-risk trajectories and thus serves as valid basis for adaptive control. Connection to Active Inference. This self-referential design aligns with Active Inference theory (Friston et al., 2016; Schwartenbeck et al., 2019), where agents estimate uncertainty from their own generative models rather than external oracles, and reduce it by adapting both perception and actiona principle observed in humans (Daw et al., 2006; Wilson et al., 2014) and formalized in robotics as active perception (Bohg et al., 2017; Bajcsy et al., 2018). This provides theoretical grounding for why self-referential uncertainty can effectively guide adaptive behavior. Figure 5. Task success rate by average pmax. Results aggregated over 6,000 episodes across LIBERO benchmarks using OpenVLA. Episodes with low average pmax exhibit significantly lower success rates, indicating that pmax serves as reliable signal for the models conviction and potential failure risk. B. Sensitivity Analysis of ϵ In this section, we analyze the sensitivity of our method to ϵ, which defines the probability mass assigned to non-top-1 tokens in the low-uncertainty reference distribution qlow. To isolate the effect of ϵ, we evaluated the performance using OpenVLA with only the adaptive action decoding component of SCALE on the LIBERO-Long benchmark. Table 9 presents the success rates across varying magnitudes of ϵ, ranging from 1e-10 to 1e-14. The results indicate that our method consistently outperforms the OpenVLA baseline (52.7%) regardless of the specific ϵ value. Furthermore, performance remains largely robust to variations within this range, with the highest success rate observed at ϵ = 1e-12. This suggests that as long as ϵ remains sufficiently small to maintain the near-deterministic nature of the reference, the exact numerical value has marginal impact on the overall effectiveness of the uncertainty estimation. C. Derivation of Self-uncertainty Formulation In this section, we provide the derivation for the self-uncertainty metric uk as the difference between the KL divergence of the prediction pk of uk presented in Equation 3. Recall the definition from the low-uncertainty reference qlow and the 12 SCALE : Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models Table 9. Sensitivity of Success Rate to ϵ. We report the average success rate on LIBERO-Long. We compare the baseline OpenVLA against our Adaptive Decoding strategy with varying ϵ values. Performance is stable across small magnitudes of ϵ. Method OpenVLA (fine-tuned) + Ada. Decoding (ϵ=1e-10) + Ada. Decoding (ϵ=1e-11) + Ada. Decoding (ϵ=1e-12) + Ada. Decoding (ϵ=1e-13) + Ada. Decoding (ϵ=1e-14) Success Rate (%) 52.7 57.2 57.4 58.0 57.0 57.6 high-uncertainty reference qhigh: By expanding the KL divergence terms using the definition DKL(P Q) = ExP [log (x) log Q(x)], we obtain: = DKL(pk uk qlow) DKL(pk qhigh). = uk = = xpk xpk xpk = xpk (x) log qlow(x)(cid:3) xpk (x) log qlow(x)) (log pk (cid:2)log pk (cid:2)(log pk (cid:2)log qhigh(x) log qlow(x)(cid:3) (cid:21) (cid:20) log qhigh(x) qlow(x) . (x) log qhigh(x)(cid:3) (cid:2)log pk (x) log qhigh(x))(cid:3) (10) (11) (12) (13) (14) This result confirms that uk references under the models current predictive distribution. represents the expected log-likelihood ratio between the high-uncertainty and low-uncertainty D. Probabilistic Interpretation: σ(uk ) as Posterior Probability We interpret our scaling mechanism within binary hypothesis testing framework. Consider two hypotheses regarding the models state: an uncertain state Hhigh (modeled by qhigh) and confident state Hlow (modeled by qlow). Using Bayes rule, the log-odds of the posterior probability (Hhigh pk ratio (LLR) and the prior log-odds: ) can be expressed as the sum of the log-likelihood log (Hhigh pk ) (Hlow pk ) = log (cid:124) (pk Hhigh) (pk Hlow) (cid:125) (cid:123)(cid:122) LLRuk + log (Hhigh) (Hlow) . (15) Our metric uk corresponds to the expected LLR (Kullback & Leibler, 1951). Assuming uninformative priors (P (Hhigh) = (Hlow)), the prior term vanishes, and the relation simplifies to: logit(cid:0)P (Hhigh pk )(cid:1) uk . (16) Since the sigmoid function is the inverse of the logit, applying it to uk yields the posterior probability: σ(uk ) (Hhigh pk ). (17) This suggests that σ(uk) serves as an estimate of the probability that the current state is uncertain, providing probabilistic basis for its use in temperature scaling. E. Empirical Validation of Previous-Step Deviation for Single-Pass Inference In our proposed method, SCALE, we modulate the visual attention mechanism at timestep using the uncertainty deviation derived from the previous timestep (i.e., ut1 = ut1 ut2). This design choice enables efficient, single-pass inference. 13 SCALE : Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models Table 10. Performance Comparison with Two-step Inference Oracle. Total evaluation time denotes the wall-clock time required for evaluating all 500 episodes of LIBERO-Long on single NVIDIA A6000 GPU. Method OpenVLA (fine-tuned) + Two-step Oracle + SCALE (Ours) LIBERO-Long Total Evaluation Time (h) 52.7 64.6 63.3 13 26 13 However, an ideal Oracle modulation strategy would condition the vision encoder on the uncertainty of the current observation ut. To assess the trade-off between computational efficiency and modulation accuracy, following Chen et al. (2025) which addresses spatial reasoning in VLMs via confidence-based two-step inference, we implement Two-step Inference Oracle. This baseline requires two forward passes per control step to utilize the exact current uncertainty: 1. Probe Pass: The model processes the observation with standard visual attention (γt = 1) and performs greedy decoding to compute the exact step-level uncertainty deviation ut = ut ut1. 2. Execution Pass: The vision encoder is re-run with the attention temperature γt adjusted based on the computed current deviation ut from the probe pass, followed by our adaptive action decoding. Table 10 presents the performance comparison and inference cost on the LIBERO-Long benchmark. As hypothesized, the Two-step Oracle yields the highest performance (64.6%), serving as the empirical upper bound for our modulation strategy. However, similar to test-time scaling methods, this accuracy comes at the cost of doubling the evaluation time (26 hours vs. 13 hours), rendering it impractical for real-time robotic control applications. Crucially, SCALE achieves success rate of 63.3%, performing within negligible margin (1.3%) of the Oracle while maintaining the same single-pass efficiency as the base OpenVLA model. This result empirically validates our reliance on the previous steps deviation ut1. In high-frequency control loops, visual states and their corresponding uncertainties exhibit high temporal correlation; consequently, the uncertainty signal from the preceding step serves as sufficiently accurate proxy for the current state, enabling SCALE to capture the benefits of adaptive looking without the computational penalty of iterative inference. F. Comparative Analysis of Inference and Training Efficiency In this section, we analyze the efficiency of SCALE relative to existing test-time scaling (TTS) methods across two dimensions: inference latency and training overhead. Inference Latency. To evaluate the computational cost of generating multiple action candidates, we measured the wall-clock time for OpenVLA and π0-FAST on the LIBERO-Spatial. We conducted evaluations across 50 episodes, recording the time required to generate action tokens as the number of samples increased. As shown in Fig. 6, the latency for both models increases significantly with . Specifically, at = 16, OpenVLA and π0-FAST exhibit approximately 15.9 and 3.2 increases in latency compared to single-sample generation, respectively. This disparity highlights the bottleneck inherent in TTS methods that require multiple forward passes, particularly for models like OpenVLA that lack efficient batch inference support. Note that these measurements reflect only action generation latency and exclude the additional computational overhead associated with the verification process. Training Overhead. Beyond inference-time rollouts, many TTS methods rely on auxiliary training to develop external verifiers, reward models, or additional joint training for self-verification (Kwok et al., 2025; Nakamoto et al., 2024; Yang et al., 2025; Jang et al., 2025). Such processes necessitate additional data collection and substantial training compute, which limits their scalability and immediate deployment to unseen domains. In contrast, SCALE is entirely trainingfree and operates in single inference pass, eliminating both the computational bottleneck of multiple rollouts and the resource-intensive requirement for auxiliary training. G. Detailed Experimental Setup and Benchmarks In this section, we provide brief descriptions of the simulation benchmarks and the real-world experimental setup used in our evaluation. Task examples of both simulation and real-world environments are provided in Fig. 8. 14 SCALE : Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models Figure 6. Comparison of Action Generation Latency. We compare the per-step latency of OpenVLA and π0-FAST across varying numbers of generated samples. Latency increases as the number of generated samples grows, with OpenVLA and π0-FAST exhibiting 15.9 and 3.2 increases at 16 samples, respectively. SCALE maintains the efficiency of single inference pass, avoiding the latency costs associated with generating multiple candidates. G.1. Simulation Benchmarks LIBERO (Liu et al., 2023) is widely adopted benchmark for robotic manipulation, designed to evaluate knowledge transfer across diverse distribution shifts. It consists of four distinct task suitesSpatial, Object, Goal, and Longwhich cover variations in spatial layouts, object types, task objectives, and complex multi-stage manipulation sequences, respectively. Each suite comprises 10 unique tasks, with each task containing 50 episodes. SIMPLER-WidowX (Li et al., 2024) is real-to-sim evaluation framework utilizing the WidowX robot and BridgeData V2 (Walke et al., 2023). It consists of four representative tasks: Put Spoon on Towel, Put Carrot on Plate, Stack Green Block on Yellow Block, and Put Eggplant in Yellow Basket, with each task evaluated over 24 episodes. By focusing on these precise manipulation tasks with narrow tolerances, the benchmark enables high-fidelity evaluation in simulation environments that closely mirror real-world deployment conditions. LIBERO-PRO (Zhou et al., 2025) evaluates model robustness beyond rote memorization by introducing systematic perturbations to the standard LIBERO suites. It encompasses five dimensions: Object attribute (Object), Initial position (Swap), Task (Task), Semantic (Language), and Environment (Env). Notably, the authors report that even state-of-the-art models suffer significant performance degradation under these perturbations. We focus on the most challenging LIBEROPRO-Long suite to assess whether model possesses genuine environmental perception or merely relies on memorizing trajectories. We evaluate on the four released perturbation types: Object, Swap, Task, and Language. For an original task such as turn on the stove and put the moka pot on it, these dimensions manifest as: replacing the moka pot with an odd moka pot (Object), exchanging the positions of the stove and moka pot (Swap), switching the goal to moving frypan (Task), rephrasing the language instruction to turn stove and place moka pot (Language), or relocating the scene to living room table (Env). G.2. Real-World Experimental Setup As shown in Fig. 7, our real-world tabletop experimental setup comprises UR10e arm equipped with Robotiq 2F-85 gripper and two Intel RealSense D455 cameras: third-person camera for global scene understanding and wrist-mounted camera for localized manipulation. The dataset is collected via teleoperation using Meta Quest 3 (Iyer et al., 2024) at 30Hz. We consider three in-distribution (ID) and two out-of-distribution (OOD) pick-and-place tasks. The ID tasks include Put Carrot on Towel (Carrot/Towel), Put Eggplant in Bowl (Eggplant/Bowl), and Put Lemon on Plate (Lemon/Plate). The 15 SCALE : Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models Figure 7. (Left) Real-world experimental setup using UR10e robot with thirdperson and wrist-mounted cameras. (Right) Examples of seen and unseen objects used in real-world experiments. OOD tasks consist of Put Teddy Bear in Bowl (Teddy Bear/Bowl) and Put Cube on Plate (Cube/Plate). Each ID task involves objects with different geometries, whereas the OOD tasks introduce unseen object compliance (a soft teddy bear) and geometry (a small cube), requiring more precise manipulation. Each task consists of 12 distinct episodes with different initial object locations. For the ID tasks, we collect total of 144 demonstrations, corresponding to 48 demonstrations per task and 4 demonstrations per initial object location. Evaluation is performed over 24 episodes per task, with 2 evaluation episodes for each initial object location. H. Implementation Details In this section, we provide comprehensive details on the architectural distinctions of the VLA backbones, training configurations for simulation and real-world experiments, and specific deployment strategies. H.1. Architectural Distinctions across VLA Backbones While SCALE is backbone-agnostic, its implementation is tailored to the unique vision processing and action tokenization schemes of each model: OpenVLA (Kim et al., 2024): Built upon standard VLM paradigm with fused vision encoder (DINOv2 and SigLIP), this model predicts seven discrete tokens per control step. Each token corresponds to dimension in the action vector (x, y, z, roll, pitch, yaw, g)representing translation, rotation, and gripper state, respectivelywhere each dimension is discretized into bins using unified action token vocabulary where = 256. π0-FAST (Pertsch et al., 2025): Utilizes SigLIP vision tower but diverges in its actuation interface via the FAST tokenizer. FAST applies Discrete Cosine Transform (DCT) and Byte-Pair Encoding (BPE) to compress action chunks, generating variable-length action token sequence. Actions are recovered by inverting the BPE and DCT operations, with all tokens drawn from single shared action vocabulary where = 2048. SpatialVLA (Qu et al., 2025): Augments SigLIP features with 3D positional encodings derived from predicted depth. For action tokenization, it compresses each control step into three autoregressive tokens representing translation (x, y, z), rotation (roll, pitch, yaw), and gripper state. By employing ensemble-style action chunking with horizon of = 4, the model generates sequence of 12 spatial tokens (3 4) per forward pass. Crucially, it utilizes factorized action vocabularies for translation, rotation, and the binary gripperwith sizes of 4096, 4096, and 2, respectivelyforming total action vocabulary where = 8194. H.2. Training Configurations H.2.1. SIMULATION EXPERIMENTS We evaluate performance on the LIBERO suite (Liu et al., 2023) and SIMPLER-WidowX (Li et al., 2024) for seen tasks, and LIBERO-PRO-Long (Zhou et al., 2025) for unseen generalization. 16 SCALE : Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models Figure 8. Task examples from LIBERO (top two rows), SIMPLER-WidowX (middle two rows), and real-world experiments (bottom two rows). OpenVLA: We utilize the official model checkpoints provided by the authors, which are fine-tuned for each respective LIBERO task suite. For LIBERO-PRO-Long evaluation, we apply the checkpoints fine-tuned on LIBERO-Long to assess zero-shot robustness. π0-FAST: We perform full fine-tuning on the LIBERO datasets for 10k steps using two NVIDIA H100 GPUs (global batch size 32, action chunk size 10) and evaluate the model on both LIBERO (fine-tuned) and LIBERO-PRO (zero-shot). For SIMPLER-WidowX, the backbone is fine-tuned on BridgeData V2 (Walke et al., 2023) for 10k steps (batch size 64, action chunk size 5). SpatialVLA: We employ the official checkpoints provided by the authors: spatialVLA-4B-224-pt for zero-shot evaluation and spatialvla-4b-224-sft-bridge for fine-tuned tasks. H.2.2. REAL-WORLD EXPERIMENTS OpenVLA: Since OpenVLA is pre-trained on BridgeData V2 at 5 Hz, we downsample our dataset to 5 Hz before fine-tuning. We fine-tune the official openvla-7b checkpoint using LoRA (r = 32) (Hu et al., 2022) on 4 NVIDIA A100 GPUs for 15k steps (batch size 8). π0-FAST: Unlike other VLA models used in simulation experiments and OpenVLA in real-world experiments, π0-FAST additionally leverages the robots proprioceptive state and wrist-mounted camera in real-world experiments to ensure stable execution. We fully fine-tuned the official base π0-FAST checkpoint on 4 NVIDIA A100 GPUs for 10k steps (batch size 32). The model predicts chunk horizon of 20, with the first 15 steps executed during inference. 17 SCALE : Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models H.3. Deployment and Inference The following sampling strategies and hyperparameter configurations were applied consistently across all simulation benchmarks and real-world experiments to ensure uniform evaluation of SCALE. H.3.1. SAMPLING STRATEGY OF SCALE To accommodate tokenization variances, we tailor the sampling strategy to the unique properties of each model. For OpenVLA, we apply sampling to all 7 action tokens. For π0-FAST, as the FAST tokenizer generates action tokens in order from lowto high-frequency coefficients, we sample the first 5 tokens corresponding to the primary low-frequency components, following prior work (Jang et al., 2025). For SpatialVLA, we sample the first 3 tokens representing the current step. Note that SpatialVLA utilizes factorized vocabularies (Appendix H.1); thus, uk is calculated using the logits corresponding to the respective token type (translation, rotation, or gripper). Notably, due to the autoregressive nature of these backbones, sampling the initial tokens ensures that the influence of the intervention inherently propagates to all subsequent tokens in the sequence. H.3.2. VISUAL ATTENTION MODULATION OF SCALE While all models utilize SigLIP encoder, OpenVLA employs fused DINOv2-SigLIP architecture. We apply visual attention modulation to all vision encoders, including both the DINOv2 and SigLIP in OpenVLA. H.3.3. HYPERPARAMETERS The base temperature T0 is set to 1.0 for OpenVLA and 0.3 for π0-FAST and SpatialVLA. For adaptive visual attention, we set κ = 2, constraining the attention temperature γt within the interval (0.5, 2.0). The temporal smoothing factor is set to α = 0.8 for OpenVLA and SpatialVLA, and α = 0.66 for π0-FAST to account for its more frequent uncertainty updates in autoregressive chunk generation. We use same hyperparameters for both simulation and real-world experiments. Table 11. Performance comparison of OpenVLA on LIBERO across various decoding strategies and hyperparameters. We evaluate standard sampling, Top-k, and Top-p with different settings. Method OpenVLA (fine-tuned) + Sampling (t=0.3) + Sampling (t=0.5) + Sampling (t=0.7) + Sampling (t=1.0) + Top-k (k=10, t=0.7) + Top-k (k=20, t=0.7) + Top-k (k=40, t=0.7) + Top-k (k=40, t=1.0) + Top-p (p=0.9) + Top-p (p=0.95) + SCALE (Ours) Spatial Object Goal Long Avg. 86.2 85.1 83.9 85.2 85.1 84.7 84.7 85.2 84.3 86.9 85.7 89.5 86.2 87.6 88.5 87.6 87.9 88.3 89.0 88.2 88.2 88.1 88.4 91.0 77.7 79.5 78.0 78.9 78.9 79.1 78.2 78.3 80.7 78.6 77.7 82.3 52.7 53.6 54.2 54.4 54.7 53.8 55.0 55.2 53.5 55.1 55.4 63.3 75.7 76.5 76.2 76.5 76.7 76.5 76.7 76.7 76.7 77.2 76.8 81.5 I. Sensitivity Analysis of Baseline Decoding Strategies In this section, we provide detailed sensitivity analysis of standard decoding strategiestemperature sampling, top-k sampling, and top-p samplingto justify our selection of baseline hyperparameters. We evaluated the OpenVLA model on the LIBERO benchmark across wide range of hyperparameter configurations. For all baseline strategies, we mask non-action tokens to ensure that sampling is performed only over each models specific action token vocabulary. As summarized in Table 11, while various decoding strategies generally improve upon the fine-tuned OpenVLA baseline (75.7% average success rate), the performance gains are relatively marginal and insensitive to specific hyperparameter tuning. For instance, varying the temperature from 0.3 to 1.0 or adjusting the top-k and top-p thresholds results in average success rates that fluctuate within narrow range of approximately 76.2% to 77.2%. 18 SCALE : Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models Table 12. Per-task SR (%) on LIBERO-Long with OpenVLA backbone. We compare SCALE against training-free baseline (greedy decoding) and training-required test-time scaling methods. Reproduced using authors official implementation. Tasks Soup and Sauce in Basket Cheese and Butter in Basket Turn on Stove and Place Moka Black Bowl in Drawer Mugs on Plate Book in Caddy Mug and Pudding on Plate Soup and Cheese in Basket Moka Pots on Stove Mug in Microwave Average Training-free, single inference OpenVLA SCALE (Ours) Training-required, test-time scaling RoboMonkey TACO 62.7 69.3 58.0 40.7 49.3 76.0 46.7 63.3 20.7 40.0 52.7 66.0 82.7 58.7 58.0 51.3 86.0 62.7 76.0 38.0 54.0 63.3 59.0 79.0 58.0 37.0 55.0 86.0 59.0 62.0 26.0 44. 56.5 66.0 82.0 52.0 50.0 50.0 90.0 54.0 80.0 28.0 48.0 60.0 SCALE significantly outperforms all fixed-parameter decoding strategies, achieving an average success rate of 81.5%. This result underscores the limitation of fixed hyperparameters: no single manual setting can consistently adapt to the varying levels of predictive uncertainty encountered across different tasks and environmental states. Since the performance of these baseline strategies did not exhibit significant variance across different parameters, we selected the following representative configurations for comparison across all models and benchmarks in our main experiments: temperature sampling with t=1.0, top-k sampling with k=40 and t=0.7, and top-p sampling with p=0.9. J. Comparison with Test-Time Scaling Methods: Per-Task Breakdown We provide detailed comparison between SCALE and existing TTS approaches on LIBERO-Long, challenging benchmark featuring long-horizon manipulation tasks that require sustained precision over extended episodes. Table 12 reports per-task success rates, where all methods build upon OpenVLA fine-tuned on LIBERO data (OpenVLA*) and apply their respective inference strategies. We compare against two representative TTS methods: RoboMonkey (Kwok et al., 2025), which trains VLM-based action verifier, and TACO (Yang et al., 2025), which employs learned value function for action selectionboth requiring additional training and multiple forward passes. SCALE achieves the highest average success rate (63.3%), outperforming RoboMonkey (56.5%) by 6.8%p and TACO (60.0%) by 3.3%p, while requiring no additional training and only single forward pass. Notably, SCALE attains the best performance on 7 out of 10 tasks, with substantial gains on challenging tasks such as Moka Pots on Stove (+10.0%p over TACO) and Mug in Microwave (+6.0%p over TACO). These results demonstrate that adaptively modulating perception and action based on self-uncertainty can be more effective than selecting from multiple candidates via learned verifiers, particularly for long-horizon tasks where sustained adaptability is crucial. K. Details on Baseline Uncertainty Metrics To ensure fair comparison with our proposed method, we evaluate several baseline uncertainty metrics: normalized entropy, confidence (pmax), Gini impurity (Breiman et al., 2017), and Self-certainty (Kang et al., 2025). We map all metrics to the unit interval [0, 1], where = 0 represents maximum certainty and = 1 represents maximum uncertainty. The specific normalization formulations are defined as follows, given the categorical distribution pk over the action vocabulary at token position k: Normalized Entropy: We use the Shannon entropy normalized by the logarithm of the vocabulary size V: uk ent = (cid:80) log pk iV pk log (18) Maximum Probability (pmax): Since pk max = maxi pk corresponds to the models confidence, we use its complement SCALE : Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models to represent uncertainty: uk pmax = 1 max iV pk Gini Impurity: We adopt the Gini impurity measure, defined as: uk gini = 1 (pk )2 (cid:88) iV (19) (20) Self-certainty: Following Kang et al. (2025), we consider the Kullback-Leibler divergence from the uniform distribution qhigh to the policy distribution pk, denoted as DKL(qhighpk). To map this unbounded measure to [0, 1] while preserving the order of uncertainty, we apply an exponential decay transformation: sc = exp(DKL(qhighpk)) uk (21) where high divergence (certain) yields uk sc 0, and zero divergence (uncertain) yields uk sc = 1. In our comparative experiments, we substitute σ(uk) in Equation 2 directly with these normalized Implementation. baseline metrics. These surrogate values are used to modulate the action decoding process and visual attention, exactly as described in our proposed method. All other algorithmic components remain identical to the proposed framework, ensuring that the performance differences are attributable solely to the choice of the uncertainty metric."
        }
    ],
    "affiliations": [
        "ECE, IPAI and ASRI in Seoul National University",
        "Seoul National University"
    ]
}