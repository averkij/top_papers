{
    "paper_title": "Rethinking Sample Polarity in Reinforcement Learning with Verifiable Rewards",
    "authors": [
        "Xinyu Tang",
        "Yuliang Zhan",
        "Zhixun Li",
        "Wayne Xin Zhao",
        "Zhenduo Zhang",
        "Zujie Wen",
        "Zhiqiang Zhang",
        "Jun Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large reasoning models (LRMs) are typically trained using reinforcement learning with verifiable reward (RLVR) to enhance their reasoning abilities. In this paradigm, policies are updated using both positive and negative self-generated rollouts, which correspond to distinct sample polarities. In this paper, we provide a systematic investigation into how these sample polarities affect RLVR training dynamics and behaviors. We find that positive samples sharpen existing correct reasoning patterns, while negative samples encourage exploration of new reasoning paths. We further explore how adjusting the advantage values of positive and negative samples at both the sample level and the token level affects RLVR training. Based on these insights, we propose an Adaptive and Asymmetric token-level Advantage shaping method for Policy Optimization, namely A3PO, that more precisely allocates advantage signals to key tokens across different polarities. Experiments across five reasoning benchmarks demonstrate the effectiveness of our approach."
        },
        {
            "title": "Start",
            "content": "Xinyu Tang1*, Yuliang Zhan1*, Zhixun Li2*, Wayne Xin Zhao1, Zhenduo Zhang3 , Zujie Wen3 , Zhiqiang Zhang3 Jun Zhou3 1Gaoling School of Artificial Intelligence, Renmin University of China 2The Chinese University of Hong Kong 3Ant Group 5 2 0 2 5 2 ] . [ 1 5 2 6 1 2 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large reasoning models (LRMs) are typically trained using reinforcement learning with verifiable reward (RLVR) to enhance their reasoning abilities. In this paradigm, policies are updated using both positive and negative self-generated rollouts, which correspond to distinct sample polarities. In this paper, we provide systematic investigation into how these sample polarities affect RLVR training dynamics and behaviors. We find that positive samples sharpen existing correct reasoning patterns, while negative samples encourage exploration of new reasoning paths. We further explore how adjusting the advantage values of positive and negative samples at both the sample level and the token level affects RLVR training. Based on these insights, we propose an Adaptive and Asymmetric tokenlevel Advantage shaping method for Policy Optimization, namely A3PO, that more precisely allocates advantage signals to key tokens across different polarities. Experiments across five reasoning benchmarks demonstrate the effectiveness of our approach."
        },
        {
            "title": "Introduction",
            "content": "Large reasoning models (DeepSeek-AI et al., 2025; Bai et al., 2025; Yang et al., 2025) have recently gained significant attention due to their impressive performance in mathematical, coding, and scientific reasoning tasks. These models typically adopt the reinforcement learning with verifiable reward (RLVR) paradigm (Yu et al., 2025; Chen et al., 2025), where they generate multiple long chain-of-thought reasoning trajectories and use verifiable binary rewards to assess the correctness of the final answers. The reward signals are then used to update the models policy. Unlike supervised fine-tuning (Zhang et al., 2023), which imitates external teachers by memorizing correct examples, * Equal contribution. Corresponding author. 1 RLVR enables models to learn from their own generated rollouts, including both positive and negative samples. Positive samples help reinforce reasoning paths that the model already handles correctly, while negative samples facilitate self-correction by learning from mistakes. However, within the RLVR framework, the distinct roles of positive and negative samples, which are referred to as sample polarity, remain underexplored. To explore this question, prior studies have attempted to analyze the respective contributions of positive and negative samples in RLVR. For instance, Zhu et al. (2025) decomposes RLVR into two learning paradigms: positive and negative sample reinforcement. Their findings show that training solely with negative samples consistently improves Pass@k metrics. However, their experiments were constrained to simple math training dataset and small set of models, which limits the generalizability of their conclusions. Subsequent studies observe an asymmetry between positive and negative samples and propose methods to improve importance sampling (Wang et al., 2025a), advantage shaping (Zhu et al., 2025), and clipping mechanisms (Hao et al., 2025; Xi et al., 2025). Nevertheless, thorough analysis of how positive and negative samples influence RLVR training dynamics remains incomplete. In this paper, we systematically analyze the roles of positive and negative samples in RLVR by applying PSR and NSR to three different base LLMs. We find that positive samples sharpen the models existing correct reasoning paths, reduce entropy, and result in shorter outputs. In contrast, negative samples promote the discovery of new reasoning patterns, increase entropy, and encourage longer responses. However, using only one sample polarity impairs reasoning performance and boundary, demonstrating both types are important for RLVR. We further investigate how modulating the influence of positive and negative samples at different granularities affects RLVR training. At the sample level, assigning higher weights to positive samples accelerates reward improvement but narrows exploration diversity, whereas emphasizing negative samples encourages broader exploration at the expense of slower reward progress. To examine the training process in finer granularity, we perform token-level advantage shaping to determine which specific tokens in positive and negative samples contribute more to the training dynamics. Our results indicate that weighting tokens based on their entropy and probability has distinct effects for each polarity. Building on these findings, we propose an Adaptive and Asymmetric token-level Advantage shaping method for Policy Optimization, namely A3PO. This approach dynamically adjusts the advantages of high-probability tokens in negative samples and low-probability tokens in positive samples, enabling finer-grained advantage allocation. Experiments across three LLMs and five reasoning benchmarks validate the effectiveness of A3PO. Our contributions are summarized as follows: We conduct comprehensive analysis of sample polarity in RLVR. We identify distinct training dynamics between them and observe that both sample polarities are crucial for RLVR. We investigate how varying the influence of positive and negative samples at different granularities affects RLVR training through sample-level and token-level advantage shaping. We propose an adaptive and asymmetric tokenlevel advantage shaping method, which enables finer-grained advantage allocation and leads to more effective and stable RLVR training."
        },
        {
            "title": "Rewards",
            "content": "Reinforcement learning with verifiable rewards (RLVR) effectively improves the reasoning ability of large language models. Under this paradigm, an LLM acts as policy model that generates multiple long chain-of-thought reasoning paths. The model is optimized using binary outcome-based rewards, which removes the need for learned reward model. As representative algorithm, Group Relative Policy Optimization (GRPO) (DeepSeekAI et al., 2025) computes advantages directly from groups of rollouts, avoiding reliance on learned value network and enabling scalable reasoning through zero-RL. Following GRPO, subsequent studies have refined the algorithm by introducing enhanced techniques for advantage estimation (Cui et al., 2025; Yue et al., 2025b), loss aggregation (Zhao et al., 2025; Zheng et al., 2025), importance sampling (Chen et al., 2025), and sampling strategies (Li et al., 2025; Guo et al., 2025)."
        },
        {
            "title": "2.2 Sample Polarity in RLVR",
            "content": "In RLVR, both positive and negative samples are important for policy optimization. Positive samples reinforce correct reasoning paths, while negative samples allow models to learn from their mistakes. Prior work has examined the distinct effects of these two sample types (Zhu et al., 2025). They propose methods that treat them differently, including importance sampling (Wang et al., 2025a), advantage reweighting (Zhu et al., 2025; Hao et al., 2025), and clipping mechanisms (Xi et al., 2025). In this paper, we provide more thorough investigation of how different sample polarities affect training dynamics and analyze their respective contributions to RLVR. In addition, we conduct finergrained analysis of different sample polarities via polarity-level and token-level advantage shaping."
        },
        {
            "title": "Negative Samples in RLVR",
            "content": "In this section, we analyze how positive and negative samples affect RLVR training across different base LLMs. Specifically, we study reinforcement using only positive and negative samples and compare their training dynamics and model behaviors."
        },
        {
            "title": "3.1 Experimental Setup",
            "content": "We conduct experiments with three different types of LLMs: math-enhanced LLM Qwen2.5-7BMath (Yang et al., 2024), general pretrained LLM Qwen3-8B-Base (Yang et al., 2025), and distilled LLM after supervised fine-tuning DeepSeek-R1Distill-Qwen-7B (DeepSeek-AI et al., 2025). Following Zhu et al. (2025), we perform reinforcement separately using only positive and negative samples for each LLM, and include DAPO, which utilizes both types of samples, for comparison. More details on positive and negative sample reinforcement are included in Appendix B.1, and experimental setups are provided in Appendix A. 2 (a) Entropy (b) Response Length (c) Reward (d) AIME25 Entropy (e) AIME25 Avg@32 (f) AIME25 Pass@32 Figure 1: RLVR training dynamics under three training paradigms on Deepseek-R1-Distilled-Qwen-7B."
        },
        {
            "title": "3.2 Different Training Dynamics of Positive\nand Negative Sample Reinforcement",
            "content": "Positive samples reduce entropy, negative samples maintain it. As shown in Figure 1a and 1d, reinforcement with only positive samples leads to rapid decline in model entropy, while reinforcement with only negative samples helps maintain higher entropy levels on both training and validation data. This occurs because positive reinforcement amplifies the logits of tokens that appear in correct solutions, making the model more confident in narrow set of high-probability predictions. In contrast, negative reinforcement reduces the logits of tokens present in incorrect solutions and indirectly boosts alternatives, thus preserving greater exploration diversity and higher entropy. Positive samples produce shorter responses, negative samples yield longer ones. Figure 1b shows that models trained with positive samples alone generate increasingly shorter responses, whereas those trained with negative samples produce longer outputs. This is because positive reinforcement rewards the most efficient path to correct answers, implicitly penalizing extra reasoning steps. On the other hand, negative reinforcement suppresses incorrect tokens without encouraging brevity, allowing models to explore longer reasoning chains. Using only one sample polarity harms reasoning abilities and boundaries. As shown by the training reward in Figure 1c) and validation performance in Figure 1e and 1f), training with only positive or negative samples damages the models reasoning ability and boundary, with further degradation over training. It is worth noting that although Zhu et al. (2025) suggests that negative-only reinforcement can improve reasoning boundaries, we find that it only maintains Pass@32 performance comparable to DAPO on Qwen2.5-7B-MATH, indicating that such conclusion is limited to certain models. This further confirms that both positive and negative samples are essential in RLVR. Negative samples are key to preserving generalization. As illustrated in Figure 1c and Figure 1e, reward on the training set declines faster or grows slower with negative sample reinforcement compared to positive sample reinforcement. However, models trained with negative samples achieve better performance on the validation set. This suggests that negative samples are crucial for maintaining the models generalization ability in RLVR."
        },
        {
            "title": "LLMs",
            "content": "Figure 2 illustrates how reward changes during RLVR training across various base LLMs. For Qwen2.5-7B-Math, using only positive or negative samples can improve reward, but is less effective than using both together. Here, both types of samples jointly accelerate RL training and lead to better final performance. In contrast, for Deepseek-R1Distilled-Qwen-7B, training with only one polarity damages reasoning ability. Only when both positive and negative samples are combined does reward improve consistently. 3 (a) Qwen2.5-7B-Math (b) Qwen3-8B-Base (c) Deepseek-R1-Distilled-Qwen-7B Figure 2: RLVR training reward across different training paradigms and base LLMs. spective. Here, we define two metrics: Sharpening: The proportion of n-grams in the current rollout that have appeared in previously correct rollouts, which measures how much the model reinforces existing correct patterns. Discovery: The proportion of n-grams in the current rollout that have never appeared before, reflecting the models exploration of new paths. The results are shown in Figure 3. We observe that as training proceeds, the model increasingly reinforces previously correct reasoning processes while reducing the frequency of exploration. For sharpening, the ranking is PSR > DAPO > NSR. For discovery, the ranking is NSR > DAPO > PSR. These findings indicate that positive samples help models exploit and strengthen previously correct trajectories, while negative samples facilitate exploration of unseen reasoning paths."
        },
        {
            "title": "Impacts of Advantage Shaping with\nDifferent Sample Polarities at Varying\nGranularities on RLVR Training",
            "content": "Our previous analyses show that training with only one sample polarity impairs performance, confirming the importance of both positive and negative samples in RLVR. In this section, we further investigate how adjusting the influence of each polarity at different granularities affects RLVR training."
        },
        {
            "title": "4.1 Polarity-level Advantage Shaping",
            "content": "In this part, we conduct polarity-level advantage shaping using Qwen2.5-7B-Math. Specifically, we scale the advantage values of one sample type by factors of 0.2, 0.5, 2, and 5, while keeping the other type fixed. More details on polarity-level advantage shaping are provided in Appendix B.2. Standard RLVR training (1 for both) is included for comparison. The results are presented in Figure 4. Higher positive advantage speeds up reward improvement but limits exploration diversity. In- (a) Sharpen (b) Discovery Figure 3: Training behaviors of different paradigms. When training Qwen3-8B-Base with only positive samples, the reward initially drops sharply and then recovers in later stages. We observe that the model exhibits reward hacking, where it learns to guess answers directly rather than perform stepby-step reasoning. On the other hand, using only negative samples results in reward fluctuation without steady progress. This occurs because negative sample reinforcement continuously shifts probability away from high-probability tokens to others, which increases the likelihood of generating irrelevant tokens and ultimately leads to mojibake output. case study on Qwen3-8B-Base is provided in Appendix E. Detailed analyses of accuracy changes on validation samples are presented in Appendix D."
        },
        {
            "title": "3.4 Positive Samples Encourage Shapren,\nNegative Samples Help Discovery",
            "content": "There are two prevailing views on RLVR: sharpening and discovery, which appear to be in direct opposition (Zhang et al., 2025). The sharpening view (Yue et al., 2025a) posits that RLVR does not create genuinely new patterns, but instead refines and reweights correct responses already available in the base model. In contrast, the discovery view (Liu et al., 2025) suggests that RLVR can uncover new reasoning patterns not acquired during pre-training and not generated through repeated sampling. To investigate how sample polarities contribute to each perspective, we examine the models generated rollouts from an n-gram per4 Figure 4: Polarity-level advantage shaping results on Qwen2.5-7B-Math. Each label is formatted as PXNY, where and represent the advantage scaling factors for positive and negative samples. For example, P1N5 denotes positive sample weight 1 and negative sample weight 5. creasing the advantage values of positive samples accelerates reward growth on the training set, as the model learns more quickly from correct examples. However, this also makes the model more confident and focused on reinforcing existing successful patterns, thereby limiting exploration diversity. Consequently, the model produces responses with lower entropy and shorter lengths. Higher negative advantage encourages exploration but slows reward improvement. Conversely, assigning higher advantages to negative samples encourages the model to avoid mistakes and explore alternatives. This leads to higher entropy and longer responses, as the model tests various reasoning paths. While this maintains diversity, it slows reward improvement on the training set because the model spends more time exploring rather than directly learning from previous successes. The relative ratio between positive and negative advantage values determines training dynamics. Our results show that training dynamics depend primarily on the relative ratio between positive and negative advantage values, not their absolute values. For example, settings P2N1 and P1N0.5 have the same relative ratio and exhibit similar training trends. Besides, we also find that excessively high positive advantage causes overfitting to familiar patterns and limits exploration, while overly high negative advantage makes the model overly cautious and slows learning. Among all settings, positive-to-negative advantage ratio of 0.5 achieves the best performance on the validation set. This balanced ratio enables effective learning from both positive and negative samples, which maintains exploration and ensures steady reward improvement."
        },
        {
            "title": "4.2 Token-level Advantage Shaping",
            "content": "To better understand which specific tokens in positive and negative samples contribute more to RLVR training dynamics, we perform token-level advantage shaping. Specifically, we adjust the advantages assigned to tokens with different entropy and probability distributions, and observe the changes in RLVR training dynamics. Following prior work (Wang et al., 2025b), we amplify the advantages of tokens in the top and bottom 20% based on either entropy or probability using scaling factors of 0.2 and 5, respectively. More details on token-level advantage shaping are provided in Appendix B.3. This scaling value amplifies the training dynamics, as larger scaling factors produce more pronounced effects. Additionally, we also explore different token ratio settings in Appendix and find that varying the proportion of weighted tokens does not change the overall training trends. Entropy-based token-level advantage shaping. The experimental results of entropy-based tokenlevel advantage shaping are presented in Figure 5. Reinforcing positive samples with highentropy tokens accelerates entropy reduction, as these tokens often represent critical decision points where the model explores multiple reasoning paths. Reinforcing positive samples with lowentropy tokens has little effect on training dynamics, since these tokens typically reflect familiar rea5 Figure 5: Token-level entropy-based advantage shaping. The x-axis indicates the entropy of shaped tokens (right: high entropy H; left: low entropy L). The y-axis shows shaped token polarity (top: positive P; bottom: negative N). Each label follows the format [Polarity][Entropy][Scaling Factor], where the first letter denotes token polarity, the second indicates entropy level, and the numeric value specifies the scaling factor applied to the advantage of those tokens. In the figure, lines with darker colors correspond to amplifying the advantage values of these tokens, while lighter colors indicate reducing their advantage values. soning patterns where the model is already highly confident. likely incorrect paths reinforces avoidance of these tokens and narrows the policy distribution. Reinforcing negative samples with highprobability tokens slows the decrease in entropy, as the model remains uncertain about alternatives even when the current path is incorrect, which helps preserve exploration capacity. Reinforcing negative samples with lowentropy tokens speeds up entropy reduction, enabling the model to quickly identify and suppress confident but incorrect reasoning patterns, thereby increasing the confidence of models. Probability-based token-level advantage shaping. The results of Probability-based token-level advantage shaping are illustrated in Figure 6. Reinforcing high-probability positive tokens accelerates entropy reduction, as these tokens represent correct reasoning paths the model has already mastered, which sharpens the policy distribution around these established patterns. Reinforcing low-probability positive tokens leads to entropy increase, because encouraging these low-confidence correct alternatives widens the policy distribution and promotes exploration. Reinforcing high-probability negative tokens raises entropy. This is because penalizing confidently wrong predictions reduces the models certainty in high-probability outcomes, encouraging it to reconsider and diversify its predictions. Reinforcing low-probability negative tokens reduces entropy, as further suppressing already un-"
        },
        {
            "title": "5 Adaptive and Asymmetric Advantage\nShaping for Policy Optimization",
            "content": "After analyzing how sample polarity influences RLVR training dynamics, we further explore how to leverage this property to enhance the reasoning capabilities of LLMs. To this end, we propose an adaptive and asymmetric token-level advantage shaping method to achieve stable and effective RLVR optimization. In this section, we first introduce the method, then describe the experiment setup, and finally present the results."
        },
        {
            "title": "5.1 Method",
            "content": "In our previous analysis, we identified two token types that play important roles in the early stages of RLVR training: low-probability tokens from positive samples and high-probability tokens from negative samples. These tokens help maintain higher entropy, which encourages continued exploration and prevents premature convergence. Based on this finding, we propose an adaptive and asymmetric token-level advantage shaping method that dynamically adjusts the weighting of different token categories during training. Our approach assigns larger advantage values to the above token types early in training to actively encourage exploration. However, keeping such asymmetric weighting for too long can cause training-inference engine mis6 Figure 6: Token-level probability-based advantage shaping. The x-axis indicates the probability of shaped tokens (right: high probability H; left: low probability L). The y-axis shows shaped token polarity (top: positive P; bottom: negative N). Each label follows the format [Polarity][Probability][Scaling Factor], where the first letter denotes shaped token polarity, the second indicates their probability level, and the number is the scaling factor applied to the advantage for these tokens. In the figure, lines with darker colors correspond to amplifying the advantage values of these tokens, while lighter colors indicate reducing their advantage values. match and performance collapse (See Appendix G). Therefore, we gradually reduce these weights in controlled manner as training progresses, allowing the optimization to smoothly transition to standard training regime. Our method builds on the DAPO (Yu et al., 2025) framework with modified objective function: JA3PO(θ) = EqD,oπθold (q) (cid:40) (cid:88) min (cid:104) rt ˆAt, clip(rt, 1 εlow, 1 + εhigh) ˆAt t=1 (cid:41) (cid:105) , πθold (1) where rt = πθ(otq,o<t) (otq,o<t) denotes the ratio between the current and old policies, ˆAi is our shaped advantage, and εlow and εhigh are clipping bounds that constrain policy updates. The asymmetric and adaptive advantage shaping is defined as: ˆAt = At max(ρ+ α+s, 1) At > 0, pt τ + At max(ρ αs, 1) At < 0, pt τ At else. (2) Here, At is the normalized accuracy across groups, τ + is the threshold for the lowest-probability token in positive rollout, and τ is the threshold for the highest-probability token in negative rollout. ρ+ and ρ denote the initial advantage scaling factors, α+ and α control their decay coefficients for positive and negative samples, respectively."
        },
        {
            "title": "5.2 Experimental Setup",
            "content": "We run our experiments on three LLMs (i.e., Qwen2.5-7B-Math (Yang et al., 2024)), Qwen3-8BBase (Yang et al., 2025), and Deepseek-R1-DistillQwen-7B (DeepSeek-AI et al., 2025)). For comparison, we include GRPO (DeepSeek-AI et al., 2025), DAPO (Yu et al., 2025), polarity-level advantage shaping method (i.e., W-REINFORCE (Zhu et al., 2025)), and token-level advantage shaping methods (i.e., w/ Fork Tokens (Wang et al., 2025b) and Lp-Reg(Huang et al., 2025)) as baselines. Detailed descriptions of the baselines are provided in Appendix C. To evaluate the reasoning ability of the methods, we test them on three mathematical (i.e., AIME24, AIME25, and MATH500 (Hendrycks et al., 2021)) and two other reasoning benchmarks (i.e., GPQA (Rein et al., 2023) and LiveCodeBench (Jain et al., 2025)). Detailed experimental setups are presented in Appendix A."
        },
        {
            "title": "5.3 Main Results",
            "content": "Figure 2 compares the training dynamics of DAPO and A3PO. We observe that A3PO maintains higher entropy and longer responses throughout training, suggesting that the model preserves richer probability distribution and avoids premature convergence to narrow output mode. Although A3PO shows slightly slower growth in training reward compared to DAPO, it achieves higher validation accuracy, with the performance gap widening as training progresses. These results Table 1: Performance comparison of different methods on various reasoning benchmarks. We highlight the best performance across different RLVR methods. Numbers marked with * indicate that the improvement is statistically significant compared with baselines (t-test with p-value < 0.05)."
        },
        {
            "title": "Method",
            "content": "AIME24 AIME25 MATH500 GPQA LiveCodeBench Average Qwen2.57B-Math Qwen38B-Base Deepseek-R1Distill-Qwen-7B GRPO DAPO DAPO w/ Fork Tokens W-REINFORCE Lp-Reg A3PO GRPO DAPO DAPO w/ Fork Tokens W-REINFORCE Lp-Reg A3PO GRPO DAPO DAPO w/ Fork Tokens W-REINFORCE Lp-Reg A3PO 19.30.5 26.41.0 21.41.2 27.61.0 22.50.9 28.60.7 21.40.7 28.30.9 22.21.1 29.21.1 31.5*0.8 24.8*0.6 23.10.5 32.40.9 26.11.0 34.20.9 25.70.8 35.40.6 26.30.6 35.30.8 35.91.0 25.80.9 37.8*0.7 30.4*0.6 49.20.5 59.40.5 50.80.6 60.80.7 51.60.8 61.20.3 51.30.4 61.60.6 61.90.6 52.00.6 65.2*0.8 54.1*0.6 83.80.7 85.40.6 86.80.5 87.31.0 87.10.8 90.4*0.6 82.30.7 84.50.6 86.20.5 86.90.9 87.40.7 91.3*0. 95.20.5 95.50.3 95.80.8 95.70.4 96.20.3 96.9*0.2 33.70.7 34.60.8 36.50.7 36.21.1 36.90.6 39.1*1.2 45.30.6 45.80.8 47.20.6 47.41.0 47.80.6 50.2*1.0 48.40.3 50.20.5 50.40.2 51.40.6 51.20.7 53.8*0.2 11.61.1 12.40.5 14.31.0 13.80.5 13.81.3 16.4*1.0 29.40.9 29.70.5 31.20.9 30.40.5 30.91.1 33.8*0. 42.50.7 43.20.4 44.10.7 44.60.5 44.70.8 47.2*0.3 35.00.8 36.30.8 37.70.8 37.40.8 37.81.0 40.4*0.8 42.50.7 44.10.8 45.10.7 45.30.8 45.60.9 48.7*0.8 58.90.5 60.10.5 60.60.6 60.90.5 61.20.6 63.4*0.4 (a) Entropy (b) Response Length (c) Reward (d) Accuracy Figure 7: RLVR training dynamics of DAPO and A3PO on Qwen3-8B-Base. suggest that the policy learned by A3PO generalizes better, allowing the model to acquire more general reasoning capabilities rather than merely memorizing patterns in the training data. The main results are presented in Table 1. We observe that DAPO further improves performance over GRPO, which can be attributed to its clip higher mechanism, as it retains low-probability positive tokens and helps the model learn novel reasoning paths. The sentence-level advantage shaping method further boosts performance by assigning higher advantage values to negative samples. Additionally, DAPO w/Fork Tokens and Lp-Reg yield gains by assigning higher weights to high-entropy tokens and regularizing low-probability tokens, respectively. However, these methods do not account for the opposing effects that high-entropy and lowprobability tokens can have in positive versus negative samples on RLVR training dynamics. Treating all tokens uniformly may partially counteract their respective contributions. To address this question, we propose an adaptive and asymmetric token-level advantage shaping method, which dynamically adjusts the advantage values of high-probability tokens in negative samples and low-probability tokens in positive samples. This finer-grained allocation of advantages enables more stable and effective RLVR training, ultimately achieving the best performance. More detailed analyses of A3PO are presented in Appendix H."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we systematically analyzed the roles of positive and negative samples in RLVR, demonstrating their distinct contributions to training dynamics. Our findings showed that positive samples sharpen correct reasoning patterns, while negative samples promote exploration, and both are essential for RLVR training. Based on these findings, we proposed an adaptive and asymmetric tokenlevel advantage shaping method that allowed more precise allocation of advantages and led to stable 8 and improved RLVR training. Experiments across multiple models and benchmarks validate the effectiveness of our approach."
        },
        {
            "title": "7 Limitations",
            "content": "In this paper, we provide comprehensive analysis of Reinforcement Learning with Verifiable Rewards (RLVR) from the perspectives of sample polarity, revealing the different roles of positive and negative samples during RLVR training. One limitation of this work is that our experiments are conducted only on the text-based reasoning tasks. In future work, we plan to extend our analysis and methods to other model families, including visionlanguage models. Additionally, due to constraints in computational resources and budget, we have not evaluated our analysis and approach in agent-based scenarios, such as search or code agents."
        },
        {
            "title": "References",
            "content": "Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, Zhuofu Chen, Jialei Cui, Hao Ding, Mengnan Dong, Angang Du, Chenzhuang Du, Dikang Du, Yulun Du, Yu Fan, Yichen Feng, Kelin Fu, Bofei Gao, Hongcheng Gao, Peizhong Gao, Tong Gao, Xinran Gu, Longyu Guan, Haiqing Guo, Jianhang Guo, Hao Hu, Xiaoru Hao, Tianhong He, Weiran He, Wenyang He, Chao Hong, Yangyang Hu, Zhenxing Hu, Weixiao Huang, Zhiqi Huang, Zihao Huang, Tao Jiang, Zhejun Jiang, Xinyi Jin, Yongsheng Kang, Guokun Lai, Cheng Li, Fang Li, Haoyang Li, Ming Li, Wentao Li, Yanhao Li, Yiwei Li, Zhaowei Li, Zheming Li, Hongzhan Lin, Xiaohan Lin, Zongyu Lin, Chengyin Liu, Chenyu Liu, Hongzhang Liu, Jingyuan Liu, Junqi Liu, Liang Liu, Shaowei Liu, T. Y. Liu, Tianwei Liu, Weizhou Liu, Yangyang Liu, Yibo Liu, Yiping Liu, Yue Liu, Zhengying Liu, Enzhe Lu, Lijun Lu, Shengling Ma, Xinyu Ma, Yingwei Ma, Shaoguang Mao, Jie Mei, Xin Men, Yibo Miao, Siyuan Pan, Yebo Peng, Ruoyu Qin, Bowen Qu, Zeyu Shang, Lidong Shi, Shengyuan Shi, Feifan Song, Jianlin Su, Zhengyuan Su, Xinjie Sun, Flood Sung, Heyi Tang, Jiawen Tao, Qifeng Teng, Chensi Wang, Dinglu Wang, Feng Wang, and Haiming Wang. 2025. Kimi K2: open agentic intelligence. CoRR, abs/2507.20534. Aili Chen, Aonian Li, Bangwei Gong, Binyang Jiang, Bo Fei, Bo Yang, Boji Shan, Changqing Yu, Chao Wang, Cheng Zhu, Chengjun Xiao, Chengyu Du, Chi Zhang, Chu Qiao, Chunhao Zhang, Chunhui Du, Congchao Guo, Da Chen, Deming Ding, Dianjun Sun, Dong Li, Enwei Jiao, Haigang Zhou, Haimo Zhang, Han Ding, Haohai Sun, Haoyu Feng, Huaiguang Cai, Haichao Zhu, Jian Sun, Jiaqi Zhuang, Jiaren Cai, Jiayuan Song, Jin Zhu, Jingyang Li, Jinhao Tian, Jinli Liu, Junhao Xu, Junjie Yan, Junteng Liu, Junxian He, Kaiyi Feng, Ke Yang, Kecheng Xiao, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Li, Lin Zheng, Linge Du, Lingyu Yang, Lunbin Zeng, Minghui Yu, Mingliang Tao, Mingyuan Chi, Mozhi Zhang, Mujie Lin, Nan Hu, Nongyu Di, Peng Gao, Pengfei Li, Pengyu Zhao, Qibing Ren, Qidi Xu, Qile Li, Qin Wang, Rong Tian, Ruitao Leng, Shaoxiang Chen, Shaoyu Chen, Shengmin Shi, Shitong Weng, Shuchang Guan, Shuqi Yu, Sichen Li, Songquan Zhu, Tengfei Li, Tianchi Cai, Tianrun Liang, Weiyu Cheng, Weize Kong, Wenkai Li, Xiancai Chen, Xiangjun Song, Xiao Luo, Xiao Su, Xiaobo Li, Xiaodong Han, Xinzhu Hou, Xuan Lu, Xun Zou, Xuyang Shen, Yan Gong, Yan Ma, Yang Wang, Yiqi Shi, Yiran Zhong, and Yonghong Duan. 2025. Minimax-m1: Scaling test-time compute efficiently with lightning attention. CoRR, abs/2506.13585. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, Jiarui Yuan, Huayu Chen, Kaiyan Zhang, Xingtai Lv, Shuo Wang, Yuan Yao, Xu Han, Hao Peng, Yu Cheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou, and Ning Ding. 2025. Process reinforcement through implicit rewards. CoRR, abs/2502.01456. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948. Yiran Guo, Lijie Xu, Jie Liu, Dan Ye, and Shuang Qiu. 2025. Segment policy optimization: Effective segment-level credit assignment in RL for large language models. CoRR, abs/2505.23564. Zhezheng Hao, Hong Wang, Haoyang Liu, Jian Luo, Jiarui Yu, Hande Dong, Qiang Lin, Can Wang, and 9 Jiawei Chen. 2025. Rethinking entropy interventions in rlvr: An entropy change perspective. Preprint, arXiv:2510.10150. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the MATH dataset. In NeurIPS Datasets and Benchmarks. Guanhua Huang, Tingqiang Xu, Mingze Wang, Qi Yi, Xue Gong, Siheng Li, Ruibin Xiong, Kejiao Li, Yuhao Jiang, and Bo Zhou. 2025. Low-probability tokens sustain exploration in reinforcement learning with verifiable reward. CoRR, abs/2510.03222. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. 2025. Livecodebench: Holistic and contamination free evaluation of large language models for code. In ICLR. OpenReview.net. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In SOSP, pages 611626. ACM. Yizhi Li, Qingshui Gu, Zhoufutu Wen, Ziniu Li, Tianshun Xing, Shuyue Guo, Tianyu Zheng, Xin Zhou, Xingwei Qu, Wangchunshu Zhou, Zheng Zhang, Wei Shen, Qian Liu, Chenghua Lin, Jian Yang, Ge Zhang, and Wenhao Huang. 2025. Treepo: Bridging the gap of policy optimization and efficacy and inference efficiency with heuristic tree-based modeling. CoRR, abs/2508.17445. Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, and Yi Dong. 2025. Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models. CoRR, abs/2505.24864. Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In ICLR (Poster). OpenReview.net. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. 2023. GPQA: graduate-level google-proof q&a benchmark. CoRR, abs/2311.12022. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2025. Hybridflow: flexible and efficient RLHF framework. In EuroSys, pages 12791297. ACM. Jiakang Wang, Runze Liu, Lei Lin, Wenping Hu, Xiu Li, Fuzheng Zhang, Guorui Zhou, and Kun Gai. 2025a. Aspo: Asymmetric importance sampling policy optimization. Preprint, arXiv:2510.06062. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, Yuqiong Liu, An Yang, Andrew Zhao, Yang Yue, Shiji Song, Bowen Yu, Gao Huang, and Junyang Lin. 2025b. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for LLM reasoning. CoRR, abs/2506.01939. Zhiheng Xi, Xin Guo, Yang Nan, Enyu Zhou, Junrui Shen, Wenxiang Chen, Jiaqi Liu, Jixuan Huang, Zhihao Zhang, Honglin Guo, Xun Deng, Zhikai Lei, Miao Zheng, Guoteng Wang, Shuo Zhang, Peng Sun, Rui Zheng, Hang Yan, Tao Gui, Qi Zhang, and Xuanjing Huang. 2025. Bapo: Stabilizing off-policy reinforcement learning for llms via balanced policy optimization with adaptive clipping. Preprint, arXiv:2510.18927. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jian Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. 2025. Qwen3 technical report. CoRR, abs/2505.09388. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. 2024. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. CoRR, abs/2409.12122. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Weinan Dai, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. 2025. DAPO: an open-source LLM reinforcement learning system at scale. CoRR, abs/2503.14476. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, and Gao Huang. 2025a. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? CoRR, abs/2504.13837. Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Cheng-Xiang Wang, 10 Tiantian Fan, Zhengyin Du, Xiangpeng Wei, Xiangyu Yu, Gaohong Liu, Juncai Liu, Lingjun Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Ru Zhang, Xin Liu, Mingxuan Wang, Yonghui Wu, and Lin Yan. 2025b. VAPO: efficient and reliable reinforcement learning for advanced reasoning tasks. CoRR, abs/2504.05118. Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, Yu Fu, Xingtai Lv, Yuchen Zhang, Sihang Zeng, Shang Qu, Haozhan Li, Shijie Wang, Yuru Wang, Xinwei Long, Fangfu Liu, Xiang Xu, Jiaze Ma, Xuekai Zhu, Ermo Hua, Yihao Liu, Zonglin Li, Huayu Chen, Xiaoye Qu, Yafu Li, Weize Chen, Zhenzhao Yuan, Junqi Gao, Dong Li, Zhiyuan Ma, Ganqu Cui, Zhiyuan Liu, Biqing Qi, Ning Ding, and Bowen Zhou. 2025. survey of reinforcement learning for large reasoning models. CoRR, abs/2509.08827. Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, and Guoyin Wang. 2023. Instruction tuning for large language models: survey. CoRR, abs/2308.10792. Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Pritam Damania, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, Ajit Mathews, and Shen Li. 2023. Pytorch FSDP: experiences on scaling fully sharded data parallel. Proc. VLDB Endow., 16(12):38483860. Yuzhong Zhao, Yue Liu, Junpeng Liu, Jingye Chen, Xun Wu, Yaru Hao, Tengchao Lv, Shaohan Huang, Lei Cui, Qixiang Ye, Fang Wan, and Furu Wei. 2025. Geometric-mean policy optimization. CoRR, abs/2507.20673. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, and Junyang Lin. 2025. Group sequence policy optimization. CoRR, abs/2507.18071. Xinyu Zhu, Mengzhou Xia, Zhepei Wei, Wei-Lin Chen, Danqi Chen, and Yu Meng. 2025. The surprising effectiveness of negative reinforcement in LLM reasoning. CoRR, abs/2506.01347."
        },
        {
            "title": "A Detailed Experimental Setup",
            "content": "Models. We conduct experiments on three models: Qwen2.5-Math-7B (Yang et al., 2024), Qwen38B-Base (Yang et al., 2025), and DeepSeek-R1Distill-Qwen-7B (DeepSeek-AI et al., 2025). For DeepSeek-R1-Distill-Qwen-7B and Qwen3-8BBase, we set context length of 16384 tokens. For Qwen2.5-Math-7B, we use its maximum supported length of 4,096 tokens. Training. Our implementation is based on the Verl (Sheng et al., 2025) pipeline, with rollouts performed using vLLM (Kwon et al., 2023). Models are trained on 16H200 GPUs. We use the DAPOMath dataset (Yu et al., 2025) for training. During rollouts, we set the temperature to 1 and sample 8 responses per prompt. Training follows an offpolicy RL setup with batch size of 512 and minibatch size of 32. Similar to prior work Yue et al. (2025b), we remove both the KL divergence loss and the entropy loss. All models are trained for 300 steps, optimized with the AdamW (Loshchilov and Hutter, 2019) optimizer using constant learning rate of 1e-6. The actor module is trained efficiently with Fully Sharded Data Parallel (FSDP) (Zhao et al., 2023). The chat template used is: User: [question] Please reason step by step, and put your final answer within boxed{}. n Assistant:. For hyperparameter settings, we apply adaptive advantage shaping to the lowest 20% probability tokens in positive samples and the highest 20% probability tokens in negative samples. The initial advantage scaling factors ρ+ and ρ are set to 2, and the decay coefficients α+ and α are set to 0.005. Evaluation. We evaluate model performance on three mathematical reasoning benchmarks (i.e., AIME24, AIME25, and Math500 (Hendrycks et al., 2021)) and two additional reasoning benchmarks (i.e., GPQA (Rein et al., 2023) and LiveCodeBench (Jain et al., 2025)). Models are evaluated every 5 training steps, and we report results from the checkpoint that achieves the highest average performance across five benchmarks. All evaluations are performed in zero-shot setting. Following DeepSeek-AI et al. (2025), we set the temperature to 0.6 and top-k to 0.95 during inference. To ensure stable measurements, each test set is evaluated 32 times, and we report the average accuracy."
        },
        {
            "title": "B Detailed Descriptions of Methods",
            "content": "In this section, we provide detailed descriptions of several methods used in the main text, including positive and negative sample reinforcement in Section 3 and the polarity-level and token-level advantage shaping method in Section 4. 11 B.1 Positive and Negative Sample"
        },
        {
            "title": "Reinforcement",
            "content": "In Section 3, we follow previous work (Zhu et al., 2025) to decompose the RLVR objective into two different learning paradigms: learning from correct rollouts and learning from incorrect rollouts. This decomposition allows us to examine how positive and negative responses affect training dynamics. The RLVR objective can be expressed as the sum of two sub-objectives: LRLVR(θ) = LPSR(θ) + LNSR(θ), (3) where the two sub-objectives correspond to each learning paradigm: LPSR(θ) = ExD LNSR(θ) = ExD (cid:88) y:r(x,y)=1 (cid:88) y:r(x,y)=0 πθ(yx) , (4) πθ(yx) . (5) We refer to these two learning paradigms as positive sample reinforcement and negative sample reinforcement. Positive sample reinforcement resembles supervised fine-tuning, increasing the likelihood of correct responses. In contrast, negative sample reinforcement acts like likelihood minimization, reducing the probability of incorrect responses. B.2 Polarity-level Advantage Shaping To explore how adjusting the influence of positive and negative samples affects RLVR training, we introduce polarity-level advantage shaping method in Section 4.1. This approach assigns different weights to the advantage values derived from positive and negative samples, allowing us to control their relative contributions during policy optimization. Formally, the objective is defined as: LPolarity-AS(θ) = βP LPSR(θ) + βN LNSR(θ) (6) Here, βP and βN are scaling factors that control the advantage values for positive and negative samples, respectively. By adjusting these scaling factors, we can study how emphasizing or de-emphasizing each sample polarity impacts RLVR training dynamics. B.3 Token-level Advantage Shaping To further examine the contribution of specific tokens to RLVR training, we introduce token-level advantage shaping approach in Section 4.2. This method allows us to reweight the advantage assigned to selected tokens and observe how such adjustments affect overall training dynamics. The modified policy optimization objective can be expressed as follows: JToken-AS(θ) = EqD,oπθold (q) clip(rt, 1 εlow, 1 + εhigh) ˆAt (cid:41) (cid:105) , (cid:40) (cid:88) min (cid:104) rt ˆAt, t=1 πθold (7) where rt = πθ(otq,o<t) (otq,o<t) denotes the probability ratio between the current and old policies, εlow and εhigh are clipping thresholds that constrain policy updates, and ˆAi represents the shaped advantage. The shaped advantage ˆAi is defined based on whether token is selected for reweighting: ˆAt = (cid:40) At βT At if selected else. (8) where Ai is the original advantage computed from group rollouts, and βT is token-level scaling factor applied to the selected token. Tokens are selected based on specific criteria, such as entropy and probability, enabling us to study how reweighting distinct token categories influences training dynamics."
        },
        {
            "title": "C Detailed Description of Baselines",
            "content": "In this part, we provide detailed descriptions of the baseline methods used for comparison in our experiments. Specifically, we compare our method with GRPO (DeepSeek-AI et al., 2025), DAPO (Yu et al., 2025), the polarity-level advantage shaping method (i.e., W-REINFORCE (Zhu et al., 2025)), and the token-level advantage shaping method (i.e., w/ Fork Tokens (Wang et al., 2025b) and LpReg(Huang et al., 2025)). GRPO (DeepSeek-AI et al., 2025) is reinforcement learning algorithm that improves LLM reasoning without training separate value model. For each question, it samples multiple outputs from the current policy and optimizes the policy using group-relative advantage, making it scalable for long chain-of-thought reasoning tasks. DAPO (Yu et al., 2025) is an enhanced RL method that introduces several improvements for LLM training. It prevents entropy collapse by using higher clipping threshold to encourage explo12 ration, applies dynamic sampling to filter prompts with zero variance, adopts token-level policy gradient loss to handle varying response lengths, and removes the KL divergence term for RL training. W-REINFORCE (Zhu et al., 2025) is polarity-level advantage shaping method, which assigns higher weights to self-generated negative rollouts, enabling effective RLVR training. DAPO w/Fork Tokens (Wang et al., 2025b) is token-level advantage shaping method that focuses policy gradient updates on high-entropy forking tokens. By masking gradients for the 80% lowest-entropy tokens and updating only the top 20% high-entropy tokens, it improves the reasoning performance of LLMs. Lp-Reg (Huang et al., 2025) is token-level method designed to mitigate exploration collapse. It maintains useful low-probability tokens through regularization while filtering out noisy tokens, thereby sustaining exploration throughout RLVR training."
        },
        {
            "title": "LLMs",
            "content": "In this part, we analyze the training dynamics of three RLVR training paradigms (i.e., positive sample reinforcement (PSR), negative sample reinforcement (NSR), and DAPO) across three base LLMs (i.e., Qwen2.5-7B-Math, Qwen38B-Base, and Deepseek-R1-Distilled-Qwen-7B). Specifically, we monitor accuracy changes on all validation samples from AIME24 and AIME25 during training and categorize them into five patterns: Sharpen: Accuracy improves by more than k%, indicating that training strengthens the models ability to solve the problem. Degradation: Accuracy drops by more than k%, meaning training reduces reasoning ability Fluctuation: Accuracy fluctuates within k% of the original value, showing training has little effect. Mastery: Accuracy remains above 1 k%, meaning the model consistently solves the problem correctly. Struggle: Accuracy stays below k%, meaning the problem remains too difficult for the model. We set to 10 in our analyses. The results for Qwen2.5-7B-Math, Qwen3-8B-Base, and DeepSeek-R1-Distilled-Qwen-7B are shown in Figures 8, 9, and 10, respectively. These results reveal distinct patterns of validation accuracy changes across different base LLMs during training. For Qwen2.5-7B-Math, which has been extensively exposed to reasoning data during pretraining, both PSR and NSR produce more sharpened samples than degraded ones. This shows that either polarity alone can improve performance, and combining them yields further complementary boost. In contrast, Qwen3-8B-Base exhibits reward hacking when using only positive samples, causing degradation in most samples. Negative sample reinforcement leads to garbled outputs, leaving the majority of samples in the struggle phase. Only when both polarities are combined does RLVR training become effective and improve accuracy. For the distilled model DeepSeek-R1Distilled-Qwen-7B, relying on single sample polarity leads to significant degradation, and both polarities are needed together to achieve further performance improvement."
        },
        {
            "title": "E Case Study",
            "content": "In this part, we present case study examining the distinct behaviors of positive sample reinforcement and negative sample reinforcement on Qwen3-8BBase. The results are shown in Figure 11. We observe that continuous positive sample reinforcement leads the model to strengthen its existing correct reasoning paths. Over time, this causes the model to progressively shorten its responses. Eventually, this results in reward hacking, where the model outputs only the final answer without step-by-step reasoning. In contrast, continuous negative sample reinforcement encourages the model to repeatedly learn from its own mistakes, which drives it to explore alternative reasoning paths more broadly. As result, the model ventures into low-probability regions of the output space, which sometimes leads to garbled or nonsensical outputs."
        },
        {
            "title": "F Different Weighted Ratios of",
            "content": "Token-level Advantage Shaping In this part, we investigate how the proportion of tokens selected for token-level advantage shaping affects RLVR training dynamics. Following prior work (Wang et al., 2025b), our main experiments adopt shaping ratio of 20%, meaning that advantages are reweighted for 20% of the tokens in each response. To assess the sensitivity of this choice, we conduct additional experiments with ratios of 5%, 10%, and 50%, while keeping the 13 Figure 8: Training dynamics of sample accuracy changes in the validation set on Qwen2.5-7B-Math. Figure 9: Training dynamics of sample accuracy changes in the validation set on Qwen3-8B-Base. Figure 10: Training dynamics of sample accuracy changes in the validation set on DeepSeek-R1-Distilled-Qwen-7B. 14 Table 2: Ablation study on three math benchmarks. Dataset A3PO w/o PL adv w/o NH adv w/o both w/o adaptive AIME 24 AIME 25 MATH500 Average 37.8 36.5 35.8 34.2 37.5 30.4 27.8 29.1 26. 27.1 91.3 88.1 87.4 84.5 90.9 53.2 50.8 50.8 48. 51.8 Building on this observation, we adopt an adaptive advantage shaping strategy: we increase the weight of high-probability negative samples in early training to promote exploration, then gradually reduce it until it aligns with the weight of positive samples, thereby ensuring stable training. Detailed Analysis of A3PO In this section, we present detailed analysis of our proposed method, A3PO. H.1 Ablation Study To evaluate the effectiveness of each component in our method, we conduct ablation studies on three math benchmarks using Qwen3-8B-Base. As shown in Table 2, removing any component leads to performance degradation, confirming that all components are essential. We observe that shaping advantages for both positive low-probability tokens and negative high-probability tokens improve performance, as both help maintain entropy and encourage exploration. In addition, the adaptive strategy ensures stable RLVR training. For instance, without this strategy, training instability due to traininginference mismatch limits further improvements on AIME25. H.2 Different Scales of LLMs and Training"
        },
        {
            "title": "Datasets",
            "content": "To assess the robustness and effectiveness of our proposed method, we conduct experiments using different scales of LLMs and datasets on DeepseekR1-Distilled-Qwen-7B. The results are shown in Figure 14. We find that our proposed method consistently achieves the best performance across both varying model scales and different training datasets, demonstrating its effectiveness and generalizability. H.3 Hyperparameter Analysis In this part, we analyze three key hyperparameters of A3PO: the token-shaped ratios, the initial scaling factors ρ, and the decay coefficients α. The Figure 11: Case study of positive and negative sample reinforcement on Qwen3-8B-Base. scaling factor for low-probability positive tokens fixed at 0.2. Figure 12 presents the results on Qwen2.5-7BMath. We observe that the shaping ratio mainly affects the magnitude and speed of training dynamics but does not change the overall learning trend. Specifically, in this setting, smaller ratios lead to faster entropy reduction and smoother transition in response length (from an initial decrease to later increase). Similarly, reward improvements occur more quickly in early training but slow down later when using smaller ratios. These results show that adjusting the proportion of advantage shaping tokens does not alter the fundamental training dynamics. Instead, it acts as factor that modulates the rate of policy updates. Negative Samples Amplify the Training-Inference Mismatch Training-inference mismatch is critical issue in RLVR training, where token probabilities in training and inference engines exhibit significant discrepancies, potentially leading to training collapse. In this section, we investigate which sample types contribute to this mismatch. Figure 13a shows the difference in token probabilities between training and inference engines for three training paradigms (i.e., PSR, NSR, and DAPO). Our results reveal that utilizing negative samples widens the probability gap between training and inference engines. Furthermore, we study the effect of polarity-level advantage shaping on negative samples. As shown in Figure 13b, assigning higher advantages to negative samples further increases the traininginference probability difference. This phenomenon suggests that although weighting negative samples can raise model entropy and encourage exploration, consistently giving them higher weights enlarges the mismatch and may lead to instability. 15 (a) Entropy (b) Response Length (c) Reward Figure 12: Impact of different ratios of advantage-shaped tokens when low-probability positive tokens are weighted at 0.2 of their original values on Qwen2.5-7B-Math."
        },
        {
            "title": "I Detailed Results",
            "content": "In this part, we provide detailed training dynamics in our experiments. Figure 16, 17, 18 present training dynamics of positive sample reinforcement, negative sample reinforcement, and DAPO across three different LLMs. Figure 19 and Figure 20 present training behaviors (i.e., sharpen and discovery) on Qwen2.5-7B-Math and DeepseekR1-Distilled-Qwen-7B. Figure 21 and Figure 22 show the results of polarity-level advantage shaping. Figure 23, Figure 24, Figure 25 and Figure 26 illustrate the entropy-based token-level advantage shaping. Figure 27, Figure 28, Figure 29 and Figure 30 illustrate the probability-based token-level advantage shaping. Figure 31 shows the results of different shaped token ratios in token-level advantage shaping. (a) Positive and negative sample reinforcement (b) Polarity-level advantage shaping Figure 13: Difference in token probabilities between training and inference engines. (a) Different scales of LLMs (b) Different training datasets Figure 14: Different LLMs and training datasets results are shown in Figure 15. Our experimental results indicate that the model achieves optimal performance with the token-shaped ratios of 20%. If the ratio is too low, the model does not explore the token space sufficiently. If it is too high, performance drops because many less relevant tokens receive advantage shaping. Next, we examine the initial scaling factors ρ. Setting appropriate values is important for stable training. If the hyperparameter is too high, the traininginference mismatch increases. If it is too low, the model does not explore the solution space effectively. Therefore, we set ρ to 2 in our main experiments. Finally, we find that the decay coefficients α of 0.005 yield the best performance. If α is too small, the gap between training and inference grows. If α is too large, exploration becomes insufficient and the model may converge to suboptimal solutions. 16 (a) Token-shaped ratios (b) Decay coefficients (c) Initial scaling factors Figure 15: Hyperparameter Anlaysis. (a) Entropy (b) Length (c) Reward (d) AIME24 Entropy (e) AIME24 Length (f) AIME24 Avg@ (g) AIME24 Pass@32 (h) AIME25 Entropy (i) AIME25 Length (j) AIME25 Avg@32 (k) AIME25 Pass@32 Figure 16: RLVR training dynamics on Qwen2.5-7B-Math. 17 (a) Entropy (b) Length (c) Reward (d) AIME24 Entropy (e) AIME24 Length (f) AIME24 Avg@32 (g) AIME24 Pass@32 (h) AIME25 Entropy (i) AIME25 Length (j) AIME25 Avg@32 (k) AIME25 Pass@ Figure 17: RLVR training dynamics on Qwen3-8B-Base. 18 (a) Entropy (b) Length (c) Reward (d) AIME24 Entropy (e) AIME24 Length (f) AIME24 Avg@32 (g) AIME24 Pass@32 (h) AIME25 Entropy (i) AIME25 Length (j) AIME25 Avg@ (k) AIME25 Pass@32 Figure 18: RLVR training dynamics on DeepSeek-R1-Distill-Qwen-7B. (a) Ds Sharpen (b) Qwen Math Sharpen (c) Ds Discovery (d) Qwen Math Discovery Figure 19: Training behaviors of different RLVR training when n_gram is 3. (a) Ds Sharpen (b) Qwen Math Sharpen (c) Ds Discovery (d) Qwen Math Discovery Figure 20: Training behaviors of different RLVR training when n_gram is 4. 19 (a) Entropy (b) Length (c) Reward (d) AIME24 Entropy (e) AIME24 Length (f) AIME24 Avg@32 (g) AIME25 Entropy (h) AIME25 Length (i) AIME25 Avg@32 Figure 21: Different training dynamics of polarity-level positive sample advantage shaping on Qwen2.5-7B-Math. (a) Entropy (b) Length (c) Reward (d) AIME24 Entropy (e) AIME24 Length (f) AIME24 Avg@32 (g) AIME25 Entropy (h) AIME25 Length (i) AIME25 Avg@32 Figure 22: Different training dynamics of polarity-level negative sample advantage shaping on Qwen2.5-7B-Math. 20 (a) Entropy (b) Length (c) Reward (d) AIME24 Entropy (e) AIME24 Length (f) AIME24 Avg@32 (g) AIME25 Entropy (h) AIME25 Length (i) AIME25 Avg@32 Figure 23: RLVR training dynamics on positive high entropy token advantage shaping. (a) Entropy (b) Length (c) Reward (d) AIME24 Entropy (e) AIME24 Length (f) AIME24 Avg@32 (g) AIME25 Entropy (h) AIME25 Length (i) AIME25 Avg@32 Figure 24: RLVR training dynamics on positive low entropy token advantage shaping. 21 (a) Entropy (b) Length (c) Reward (d) AIME24 Entropy (e) AIME24 Length (f) AIME24 Avg@32 (g) AIME25 Entropy (h) AIME25 Length (i) AIME25 Avg@32 Figure 25: RLVR training dynamics on negative high entropy token advantage shaping. (a) Entropy (b) Length (c) Reward (d) AIME24 Entropy (e) AIME24 Length (f) AIME24 Avg@32 (g) AIME25 Entropy (h) AIME25 Length (i) AIME25 Avg@32 Figure 26: RLVR training dynamics on negative low entropy token advantage shaping. 22 (a) Entropy (b) Length (c) Reward (d) AIME24 Entropy (e) AIME24 Length (f) AIME24 Avg@32 (g) AIME25 Entropy (h) AIME25 Length (i) AIME25 Avg@32 Figure 27: RLVR training dynamics on positive high probability token advantage shaping. (a) Entropy (b) Length (c) Reward (d) AIME24 Entropy (e) AIME24 Length (f) AIME24 Avg@32 (g) AIME25 Entropy (h) AIME25 Length (i) AIME25 Avg@32 Figure 28: RLVR training dynamics on positive low probability token advantage shaping. 23 (a) Entropy (b) Length (c) Reward (d) AIME24 Entropy (e) AIME24 Length (f) AIME24 Avg@32 (g) AIME25 Entropy (h) AIME25 Length (i) AIME25 Avg@32 Figure 29: RLVR training dynamics on negative high probability token advantage shaping. (a) Entropy (b) Length (c) Reward (d) AIME24 Entropy (e) AIME24 Length (f) AIME24 Avg@32 (g) AIME25 Entropy (h) AIME25 Length (i) AIME25 Avg@32 Figure 30: RLVR training dynamics on negative low probability token advantage shaping. 24 (a) Entropy (b) Length (c) Reward (d) AIME24 Entropy (e) AIME24 Length (f) AIME24 Avg@32 (g) AIME25 Entropy (h) AIME25 Length (i) AIME25 Avg@32 Figure 31: RLVR training dynamics under different token-shaped ratios for low-probability positive tokens (scaled by 0.2)."
        }
    ],
    "affiliations": [
        "Ant Group",
        "Gaoling School of Artificial Intelligence, Renmin University of China",
        "The Chinese University of Hong Kong"
    ]
}