{
    "paper_title": "RynnVLA-002: A Unified Vision-Language-Action and World Model",
    "authors": [
        "Jun Cen",
        "Siteng Huang",
        "Yuqian Yuan",
        "Kehan Li",
        "Hangjie Yuan",
        "Chaohui Yu",
        "Yuming Jiang",
        "Jiayan Guo",
        "Xin Li",
        "Hao Luo",
        "Fan Wang",
        "Deli Zhao",
        "Hao Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce RynnVLA-002, a unified Vision-Language-Action (VLA) and world model. The world model leverages action and visual inputs to predict future image states, learning the underlying physics of the environment to refine action generation. Conversely, the VLA model produces subsequent actions from image observations, enhancing visual understanding and supporting the world model's image generation. The unified framework of RynnVLA-002 enables joint learning of environmental dynamics and action planning. Our experiments show that RynnVLA-002 surpasses individual VLA and world models, demonstrating their mutual enhancement. We evaluate RynnVLA-002 in both simulation and real-world robot tasks. RynnVLA-002 achieves 97.4% success rate on the LIBERO simulation benchmark without pretraining, while in real-world LeRobot experiments, its integrated world model boosts the overall success rate by 50%."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 2 2 0 5 7 1 . 1 1 5 2 : r RynnVLA-002: Unified Vision-Language-Action and World Model Jun Cen1,2,3,, Siteng Huang1,2,3,, Yuqian Yuan1,3,, Kehan Li1,2,, Hangjie Yuan1,2,3, Chaohui Yu1, Yuming Jiang1, Jiayan Guo1, Xin Li1,2, Hao Luo1,2, Fan Wang1, Deli Zhao1,2, Hao Chen3 1DAMO Academy, Alibaba Group 2Hupan Lab 3Zhejiang University We introduce RynnVLA-002, unified Vision-Language-Action (VLA) and world model. The world model leverages action and visual inputs to predict future image states, learning the underlying physics of the environment to refine action generation. Conversely, the VLA model produces subsequent actions from image observations, enhancing visual understanding and supporting the world models image generation. The unified framework of RynnVLA-002 enables joint learning of environmental dynamics and action planning. Our experiments show that RynnVLA-002 surpasses individual VLA and world models, demonstrating their mutual enhancement. We evaluate RynnVLA-002 in both simulation and real-world robot tasks. RynnVLA-002 achieves 97.4% success rate on the LIBERO simulation benchmark without pretraining, while in real-world LeRobot experiments, its integrated world model boosts the overall success rate by 50%. Date: November 25, 2025 Code: https://github.com/alibaba-damo-academy/RynnVLA-002 Correspondence: cenjun.cen@alibaba-inc.com"
        },
        {
            "title": "1 Introduction",
            "content": "The Vision-Language-Action (VLA) model has emerged as promising paradigm for grounding languageconditioned decision making in visual environments, enabling robots to map instructions and observations to actions (Zitkovich et al., 2023, 2024). These models are constructed by augmenting large-scale pre-trained Multimodal Large Language Models (MLLMs) (Liu et al., 2023b; Li et al., 2024a; Zhang et al., 2025a; Bai et al., 2025) with either an action head or additional action expert module to generate actions. MLLMs contribute robust capabilities in perception and decision making, enabling VLA models to exhibit enhanced generalization across wide range of robotic tasks (Black et al., 2024; Intelligence et al., 2025). However, standard VLA architectures face three fundamental drawbacks. First, they cannot fully understand actions because actions reside only on the output side, preventing the model from forming an explicit internal representation of action dynamics. Second, they lack imagination: they do not predict how the world might evolve given candidate actions, hindering foresight and counterfactual reasoning. Third, they have no explicit understanding of physics. Without capturing physical dynamics, the model cannot internalize object interactions, contact, or stability. World models directly address these limitations by learning to forecast future observations conditioned on current images and actions, providing agents with action-aware internal states, imagination, and physics-informed representation of environment dynamics. (Ha and Schmidhuber, 2018; Wu et al., 2025a). Despite this advantage, world models are constrained by their inability to directly generate action outputs, resulting in functional gap that limits their application in scenarios requiring explicit action planning. To address the constraints inherent in both VLA models and world models, we introduce RynnVLA-002, an autoregressive action world model for unified action and image understanding and generation. As depicted in Fig. 1, RynnVLA-002 employs three separate tokenizers to encode images, text, and actions. The tokens from different modalities are set to share the same vocabulary so that understanding and generation across these modalities can be unified within single LLM architecture. The world model component captures the underlying physical dynamics of the environment by generating visual representations based on input actions. This process of action interpretation and environmental physics learning is essential for enabling effective Figure 1 (a) VLA model generates actions based on image understanding; (b) World model generates the image based on image and action understanding; (c) Action World Model unifies both image and action understanding and generation. decision making within the VLA model. At the same time, the VLA model embedded within RynnVLA-002 refines the understanding of visual data, thereby improving the precision of image generation performed by the world model. This bidirectional enhancement creates more robust and comprehensive model capable of understanding and generating both actions and images. In this work, we explore different action generation mechanisms. Our initial approach (i.e., WorldVLA (Cen et al., 2025)) involved discretizing actions and unifying them with image and text tokens within single vocabulary. However, we find that this method struggles with generating coherent action chunks. The primary reason for this is that pretrained multimodal language models have predominantly been exposed to images and text rather than actions, resulting in limited action generalization capabilities. Furthermore, in autoregressive models where subsequent actions are conditioned on preceding ones, error propagation becomes critical issue, as earlier incorrect predictions influence subsequent actions over time. To alleviate this issue, we proposed an action attention masking strategy that selectively masks prior actions during the generation of current actions. This approach effectively mitigates error accumulation and yields substantial improvements in the task of action chunk generation in simulation. However, in real-world robot experiments, this discrete design exhibits limited generalization capability and slow inference. We attribute the poor generalization to the high-volume data requirement of discrete autoregressive models (Kaplan et al., 2020), which is often unavailable in robotics. The slow inference, meanwhile, stems from the sequential nature of the autoregressive generation process. To address these issues, we evolve our architecture into hybrid model that retains the original discrete joint modeling while incorporating continuous Action Transformer head (Zhao et al., 2023). This new head is significantly smaller than the base LLM, which alleviates overfitting and improves generalization. Furthermore, the Action Transformers parallel decoding and bidirectional attention mechanism reduce the number of decoding steps, accelerating inference, and generating smoother trajectories. In summary, our contributions are as follows: We propose RynnVLA-002, an action world model that unifies VLA and World Model in single framework. We introduce an action attention masking strategy for the discrete action chunk generation, addressing the challenge of action error accumulation when autoregressively generating action sequences. An additional continuous Action Transformer head is added for stronger generalization ability and smoother trajectory. Our experiments demonstrate that RynnVLA-002 outperforms the standalone VLA and world models, Figure 2 Overview of RynnVLA-002. RynnVLA-002 involves VLA model data and world model data during the training process. highlighting the mutual enhancement between the world model and VLA model. Additionally, RynnVLA002 achieves 97.4% success rate on the LIBERO simulation benchmark without pretraining, while in real-world LeRobot experiments, its integrated world model boosts the overall success rate by 50%."
        },
        {
            "title": "2.1 Vision-Language-Action Models\nVLM-based VLA. Vision-Language Model (VLM)-based VLA models (Brohan et al., 2023; Cheang et al., 2024;\nWen et al., 2025b; Li et al., 2023; Huang et al., 2024; Belkhale and Sadigh, 2024; Zhao et al., 2025b; Wang\net al., 2025a,b) map visual–language inputs to actions. RT-2 (Zitkovich et al., 2023) first co-trained VLMs on\nrobotic trajectories and web-scale vision-language data, producing actions as discrete tokens. Subsequent\nworks (Wu et al., 2023; Zitkovich et al., 2024; Li et al., 2025b; Zhen et al., 2024; Pertsch et al., 2025) extend\nthis architecture to enhance generalization and representation efficiency. To address precision loss from discrete\ntokens, LCB (Shentu et al., 2024) introduced a dual-system with a continuous policy head, inspiring variants\nwith different policy head models like diffusion transformers (Peebles and Xie, 2023), and incorporating\ndiverse training strategies across multiple embodiments (Zhang et al., 2024; Wen et al., 2024, 2025a; Zhou\net al., 2025; Li et al., 2024b). Recent frameworks such as π0 (Black et al., 2024) leverage conditional flow\nmatching (Lipman et al., 2022), the open-source GR00T (Bjorck et al., 2025) scales it to complex humanoid\ncontrol, and π0.5 (Intelligence et al., 2025) further improves generalization by leveraging large-scale multimodal\nweb and cross-embodiment data, enabling direct zero-shot deployment across robot platforms.",
            "content": "Visual Generation-based VLA. Beyond static perception, visual-generation approaches model dynamics by predicting future visual states. UniPi (Du et al., 2023), DREAMGEN (Jang et al., 2025) and GeVRM (Zhang et al., 2025b) generate future visual observations to guide action generation. Joint frameworks (Guo et al., 2024; Zheng et al., 2025b; Li et al., 2025a) co-generate future frames and corresponding actions, enhancing temporal consistency and policy learning. Others exploit future video prediction as powerful pretraining objective, including GR-2 (Cheang et al., 2024), VPP (Hu et al., 2024) and RynnVLA-001 (Jiang et al., 2025). Collectively, these approaches highlight the potential of predictive visual modeling to bridge perception and action, though challenges remain in visual fidelity, domain transfer, and computational efficiency. Our RynnVLA-002 is built on Chameleon (Team, 2024), unified model for image understanding and generation, thus combining the benefits of both VLM and visual-generation-based approaches."
        },
        {
            "title": "2.2 World Models",
            "content": "World models endow embodied AI with internal representations (Chen et al., 2022; Robine et al., 2023; Wang et al., 2024) and predictive dynamics of the external world (Hafner et al., 2019, 2021; Okada and Taniguchi, 2022), enabling physics-consistent interaction in dynamic environments (Xiang et al., 2023; Mazzaglia et al., 2024; Ha and Schmidhuber, 2018). Recent advances have realized world models with transformer-based architectures (Wu et al., 2025b; Robine et al., 2023; Micheli et al., 2022). Notably, Googles Genie framework (Bruce et al., 2024) constructs synthetic interactive environments through large-scale selfsupervised video pretraining. Nowadays, such world models are widely utilized to generate varied training data (Agarwal et al., 2025), support model-based reinforcement learning algorithms (Wu et al., 2025a), and aid in selecting the most suitable policies from pool of generated options (Li et al., 2025a; Bar et al., 2024). In this work, we show that world model and VLA could enhance each other."
        },
        {
            "title": "3.1 Overview",
            "content": "The overall architecture of RynnVLA-002 is shown in Fig. 2. As can be seen, our RynnVLA-002 unifies two foundational models in embodied AI. The first is the VLA model, where policy π generates an action at based on language goal l, proprioceptive state st1, and an observation history oth:t: at π(at l, st1, oth:t). (1) The second is the world model, where the model predicts the next observation ot from past observations and actions: ˆot (ot oth:t1, ath:t1). We mix the VLA model data and the world model data to train the RynnVLA-002, an integrated model Mψ that consolidates the capabilities of action prediction and world modeling. The dual nature of our model is captured by its ability to be queried either as VLA or as world model, leveraging shared group of parameters ψ. (2)"
        },
        {
            "title": "3.2 Data Tokenization\nTokenizers. We initialize the model from Chameleon (Team, 2024) since it is a unified model for image\nunderstanding and generation. Four tokenizers are involved, including an image tokenizer, a text tokenizer, a\nstate tokenizer, and an action tokenizer. The image tokenizer is a VQ-GAN model (Esser et al., 2021) with\nadditional perceptual losses to specific image regions, e.g., faces and salient objects (Gafni et al., 2022). The\ncompression ratio of the image tokenizer is 16 and the codebook size is 8192. The image tokenizer generates\n256 tokens for 256 × 256 images and 1024 tokens for 512 × 512 images. The text tokenizer is a trained BPE\ntokenizer (Sennrich et al., 2015). The image and text tokenizers are inherited from Chameleon. The state and\naction tokenizer discretizes each dimension of continuous robot proprioceptive states and actions into one of\n256 bins, with bin widths determined by the range of the training data (Zitkovich et al., 2024, 2023). All\nimage, text, action, and state tokens are in a single token vocabulary with the size of 65536. The continuous\nactions generated by the Action Transformer are raw actions without tokenization.",
            "content": "VLA Model Data. The overall token sequence of VLA model data is: {text} {state} {image-front-wrist} (cid:125) (cid:124) (cid:123)(cid:122) Ldis_action (cid:123) (cid:125)(cid:124) (cid:122) {action} (cid:124) (cid:123)(cid:122) (cid:125) The VLA model generates actions based on the language instruction, proprioceptive state, and historical image observations. The text inputs are What action should the robot take to + <task>+ ?. Ldis_action refers to the cross-entropy loss of discrete action tokens. World Model Data. World model generates the next image frame given the current image observation and action. The overall token sequence is: Figure 3 Attention mask of (a) default VLA model, (2) our proposed VLA model, and (c) world model. {text} {images-front-wrist}{action} (cid:124) (cid:123)(cid:122) Limg (cid:122) (cid:123) (cid:125)(cid:124) {images-front-wrist} (cid:125) All of the training instances for world model share the same text prefix Generate the next frame based on the current image and the action. and there are no other task instructions since the action could totally determine the next state of the world. The overall generation could repeat times in an autoregressive manner. Limg refers to the cross-entropy loss of discrete image tokens. Training Objective. We mix the VLA model data and world model data to train our RynnVLA-002. The overall loss function is Ldis = Ldis_action + Limg. In this way, RynnVLA-002 could behave as the VLA model or world model depending on the user queries."
        },
        {
            "title": "3.3 Action Chunk Generation\nAttention Mask for Discrete Action Chunk. Generating multiple actions for execution is critical for efficiency\nand a higher success rate (Kim et al., 2025). However, we find that naively generating consecutive actions in\nthe autoregressive model degrades the performance. Although the foundational MLLM demonstrates robust\ngeneralization capabilities across the image and text domains, its capacity to generalize effectively in the\naction domain is comparatively limited. Consequently, errors originating from earlier actions propagate to\nsubsequent actions under the default causal attention mask, resulting in performance degradation. To address\nthis limitation, we introduce an alternative attention mask tailored for action generation, depicted in Fig. 3\n(b). This modified mask ensures that current actions rely solely on textual and visual input, while prohibiting\naccess to prior actions. Such design enables the autoregressive framework to generate multiple actions in\nisolation, mitigating the error accumulation problem. The world model part adheres to the conventional\nattention mask, as shown in Fig. 3 (c).",
            "content": "Action Transformer for Continuous Action Chunk. Although our discrete action chunking model performs well in simulation, it rarely succeeds in real-world robot experiments. This discrepancy arises because real-world applications demand significantly higher generalization to cope with dynamic variables like lighting and object positioningfactors not fully captured in simulation. The failure of our discrete model is rooted in two key issues. First, its large autoregressive architecture is prone to severe overfitting when trained on limited real-world dataset, leading to poor generalization. Second, our designed attention mask makes the autoregressive model generate each action in isolation within the same chunk, which cannot ensure trajectory continuity, resulting in severe shaking and non-smooth movements that drastically reduce the success rate. To overcome these challenges, we propose to augment our architecture with dedicated Action Transformer to generate continuous action sequences (Zhao et al., 2023). This module processes the full contextincluding language, image, and state tokensand utilizes learnable action queries to output an entire action chunk in parallel. This design provides two distinct advantages. First, the Action Transformers more compact Table 1 Evaluation results on LIBERO benchmark. Pretraining means the model is pretrained on the large-scale robot manipulation data. Methods LAPA (Ye et al., 2024) TraceVLA (Zheng et al., 2025a) OpenVLA (Zitkovich et al., 2024) SpatialVLA (Qu et al., 2025) NORA (Hung et al., 2025) CoT-VLA (Zhao et al., 2025a) π0-FAST (Black et al., 2024) MolmoAct (Lee et al., 2025) FlowVLA (Zhong et al., 2025) UniVLA (Bu et al., 2025) Diffusion Policy (Chi et al., 2025) Octo (Team et al., 2024) MDT (Reuss et al., 2024) DiT Policy (Hou et al., 2024) MaIL (Jia et al., 2024) ThinkAct (Huang et al., 2025) π0 (Black et al., 2024) SmolVLA (Shukor et al., 2025) OpenVLA-OFT (Kim et al., 2025) Seer (Tian et al., 2024) UVA (Li et al., 2025a) RynnVLA-002-Discrete RynnVLA-002-Continuous Pretraining Action Type Spatial Object Goal Long Average Discrete Discrete Discrete Discrete Discrete Discrete Discrete Discrete Discrete Discrete Continuous Continuous Continuous Continuous Continuous Continuous Continuous Continuous Continuous Continuous Continuous"
        },
        {
            "title": "Discrete\nContinuous",
            "content": "73.8 84.6 84.7 88.2 85.6 87.5 96.4 87.0 93.2 96.5 78.3 78.9 78.5 84.2 74.3 88.3 90.0 93.0 97.6 94.2 99.0 74.6 85.2 88.4 89.9 89.4 91.6 96.8 95.4 95.0 96.8 92.5 85.7 87.5 96.3 90.1 91.4 86.0 94.0 98.4 96.8 99.8 58.8 75.1 79.2 78.6 80.0 87.6 88.6 87.6 91.6 95.6 68.3 84.6 73.5 85.4 81.8 87.1 95.0 91.0 97.9 94.6 96. 55.4 54.1 53.7 55.5 63.0 69.0 60.2 77.2 72.6 92.0 50.5 51.1 64.8 63.8 78.6 70.9 73.0 77.0 94.5 87.7 93.0 87.6 94.4 65.7 74.8 76.5 78.1 79.5 83.9 85.5 86.6 88.1 95.2 72.4 75.1 76.1 82.4 83.5 84.4 86.0 88.8 97.1 93.3 97.4 architecture is less prone to overfitting on limited data, thereby improving generalization and producing fluid, stable actions. Second, by parallelly generating all actions in single forward pass, it substantially accelerates the inference speed compared to the autoregressive baselines that generate actions sequentially. We use L1 regression loss Lconti_action to supervise the Action Transformer. The overall loss function is = Ldis + αLconti = Ldis_action + Limg + αLconti_action."
        },
        {
            "title": "4.1 Simulation Results\nBenchmark. We evaluate our method on the LIBERO benchmark (Liu et al., 2023a). This benchmark is\ncomposed of four distinct suites designed to test a range of robotic capabilities: (1) LIBERO-Spatial focuses\non spatial relationships by tasking the robot with placing a bowl in various locations; (2) LIBERO-Object\nemphasizes object recognition and manipulation with unique objects; (3) LIBERO-Goal tests procedural\nlearning by varying task goals while using a fixed set of objects; (4) LIBERO-Long contains 10 complex\nlong-horizon manipulation tasks.",
            "content": "Dataset and Preprocessing. We first curate the dataset by removing unsuccessful trajectories and filtering out no-operation actions, procedure also adopted by OpenVLA (Zitkovich et al., 2024). For world model evaluation, which relies on ground-truth video-action pairs, we partition the cleaned data into 90% training set and 10% validation set. Hyperparameter Settings. The VLA model takes = 2 historical image frames as input. We set the action chunk size = 10 for the longer LIBERO-Long and LIBERO-Spatial tasks and = 5 for the shorter LIBERO-Object and LIBERO-Goal tasks. For the world model, we use single prediction round (N = 1) to maintain computational efficiency. The loss weighting parameter α = 10. Table 2 Evaluation results on real-world SO100 robots. Success rate is reported. Place the block inside the circle. GR00T N1.5 (Bjorck et al., 2025) π0 (Black et al., 2024) RynnVLA-002 Place the strawberries into the cup. GR00T N1.5 (Bjorck et al., 2025) π0 (Black et al., 2024) RynnVLA-002 Pretraining Single-Target Multi-Target w/ Distractors 90.0 100.0 90. 60.0 70.0 90.0 50.0 50.0 80.0 Pretraining Single-Target Multi-Target w/ Distractors 50.0 80.0 80.0 50.0 70.0 80.0 70.0 40.0 50.0 Metrics. Our evaluation is twofold. To assess the VLA model, we measure its success rate across 50 deployment rollouts per task, each initialized in different state. To assess the world model, we measure its video prediction accuracy on the held-out validation set using four standard metrics: Fréchet Video Distance (FVD), Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS). Figure 4 Real-world robot settings. (a) Place the block inside the circle. (b) Place the strawberries into the cup. (c) Task with distractors. Benchmark Results. We evaluate the performance of discrete actions and continuous actions separately. As shown in Tab. 1, our RynnVLA-002 achieves high success rates of 93.3% with discrete actions and 97.4% with continuous actions, demonstrating the effectiveness of our core design principles: jointly learning the VLA modeling and world modeling, an attention mask mechanism for discrete action generation, and the added continuous Action Transformer. Surprisingly, our RynnVLA-002, without any pretraining, is still on par with strong baseline models pretrained on either LIBERO-90 or massive real-robot datasets (Tian et al., 2024; Bu et al., 2025; Kim et al., 2025)."
        },
        {
            "title": "4.2 Real-World Robot Results\nDatasets. We curate a new real-world manipulation dataset collected with a LeRobot SO100 robotic\narm (Cadene et al., 2024). All trajectories are expert demonstrations obtained via human teleoperation. We\ndefine two pick and place tasks for evaluation. (1) Place the block inside the circle: Emphasizing basic object\ndetection and grasp execution (248 demonstrations); (2) Place strawberries in the cup: Requiring fine-grained\nlocalization and grasp-point prediction (249 demonstrations).",
            "content": "Baselines. We compare with two strong open-source baselines: GR00T N1.5 (Bjorck et al., 2025) and π0 (Black et al., 2024). For both methods, we initialize from the official pretrained checkpoints and finetune them on the same SO100 dataset used for our model. We adopt the same recipe from the official codebases of these baselines to do finetuning. Evaluation. As shown in Fig. 4, our evaluation spans three scenarios: (1) Single-target manipulation, with exactly one target object on the desktop; (2) Multi-target manipulation, with multiple target objects present; and (3) Instruction-following with distractors, where both targets and distractors appear. trial is deemed successful if the robot places at least one target object into its designated location within predefined time budget. trial fails if: (1) the time limit is exceeded; (2) the robot accrues more than five consecutive failed grasp attempts on target; (3) in the instruction-following with distractors setting, the agent attempts to manipulate any distractor objects. Each task is tested for 10 times and we report the success rate. Results. Tab. 2 shows the results of real-world robot experiments. Our RynnVLA-002 achieves competitive results with GR00T N1.5 (Bjorck et al., 2025) and π0 (Black et al., 2024) without pretraining. Notably, RynnVLA-002 performs better than the baselines in cluttered environments. For instance, RynnVLA-002 has over 80% success rate on both multi-target tasks and distractor-filled scenarios for the \"Place the block\" task, surpassing the baselines by 10% to 30%. Table 3 Ablation study of VLA model using discrete actions on LIBERO benchmark. Index VLA Discrete World Model Action Chunking Our VLA model Attention Mask Goal Object Spatial Long Average 1 2 3 4 5 67.3 73.1 79.6 84.4 85. 82.9 88.0 82.9 90.9 90.9 77.8 80.2 36.7 81.8 84.0 23.0 27.3 16.9 49.3 52.4 62.8 67.2 54.0 76.6 78.1 Table 4 Ablation study of VLA model using continuous actions on LIBERO benchmark. Index VLA Continuous World Model Wrist Camera Proprioceptive State Goal Object Spatial Long Average 1 2 3 4 90.2 91.4 96.0 96.4 92.4 95.4 97.4 99.8 88.4 98.2 99.0 99.0 67.0 81.4 85.8 94.4 84.5 91.6 94.6 97.4 Figure 5 VLA model visualization on LIBERO. Task: put the cream cheese in the bowl. Top: w/o world model. Bottom: w/ world model."
        },
        {
            "title": "4.3 Ablation Study\nWorld Model Benefits the VLA Model. On the LIBERO simulation benchmark, incorporating world model data\nduring training consistently improves performance. Specifically, in Tab. 3, the success rate for discrete actions\nincreases from 62.8% (Line 1) to 67.8% (Line 2), and from 76.6% (Line 4) to 78.1% (Line 5). A similar trend\nis observed for continuous actions in Tab. 4, where the success rate increases from 91.6% (Line 2) to 94.6%\n(Line 3). In real-world robot experiments, the benefit of world model data is even more pronounced. As shown\nin Line 4 of Tab. 5, the model trained without world model data achieves a very low success rate on real-world\nrobot tasks, which is below 30%. In contrast, augmenting VLA training with world model significantly boosts\nperformance to over 80%. Fig. 5 shows that the model trained without world model data moves directly\ntoward the target location without successfully grasping the cheese or bottle, while the model jointly trained\nwith world model keeps retrying to grasp the target object when encountering failures. This behavior suggests\nthat the world model data helps the VLA model focus more on the manipulated objects—because the world\nmodel’s training objective requires accurate prediction of object motion, thereby reinforcing attention to\nobject interaction dynamics.",
            "content": "VLA Model Enhances the World Model. As shown in Tab. 6, the model trained on mixture of VLA and world model data achieves generation results that are comparable to or better than those of the world model trained solely on world data. Furthermore, Fig. 7 compares video generations from our action world model with baseline world model trained without VLA data. The baseline world model fails to predict successful grasp of the bowl from the front camera perspective in both two examples. Table 5 Ablation study of VLA model on real-world robots. Index Action Type World Model Wrist Camera Proprioceptive State Single Target Multi Target with Distractors 1 2 3 4 5 Discrete Continuous Continuous Continuous Continuous 0 0 0 30.0 80.0 0 0 0 10.0 80.0 0 0 0 0 50. Figure 6 Ablation study of chunk length with discrete actions. In contrast, our action world model consistently generates correct videos depicting successful grasp. Notably, the baseline world model also exhibits significant inconsistency: as seen in Fig. 7 (a), the front camera shows failed grasp while the wrist camera shows successful one. This highlights critical disalignment between the models predictions for different viewpoints. The visualization results validate that the image understanding capabilities inherited from the VLA model strengthen the world models generation performance. Table 6 Ablation study of world model on LIBERO validation set."
        },
        {
            "title": "Goal",
            "content": "78.13 77."
        },
        {
            "title": "Object",
            "content": "22.25 22.13 FVD PSNR SSIM LPIPS World Model 19.70 370.0 Action World Model 336.8 19.43 FVD PSNR SSIM LPIPS World Model 27.30 1141.6 Action World Model 877.2 22.60 FVD PSNR SSIM LPIPS World Model 20.28 405.4 Action World Model 373."
        },
        {
            "title": "Spatial",
            "content": "79.15 22.32 59.59 20.31 65.03 82. 22."
        },
        {
            "title": "Long",
            "content": "23.88 69.16 World Model 18.24 Action World Model 427.86 19.36 16.33 FVD PSNR SSIM LPIPS 31.60 557.73 Attention Mask for Discrete Action Chunk Generation. Simultaneous generation of multiple actions is essential for achieving effective and efficient grasping. However, we observe that vanilla autoregressive approachwhere actions are generated sequentiallycan degrade model performance, as evidenced by the results in row 3 of Table 3 and Fig. 6. The grasping success rate gradually decreases with longer action chunks. This degradation arises because later actions become overly dependent on preceding ones since they share the same space, rather than being grounded in visual input which is distinct modality. The generalization of the action is not that strong as this modality was not involved during pretraining the MLLM. Consequently, errors tend to accumulate as the sequence of generated actions increases. The proposed attention masking mechanism ensures that each action is generated independently and solely determined by the visual input, thereby mitigating the issue of error propagation within the action sequence. As illustrated in Fig. 6, the model incorporating the proposed attention mask demonstrates superior performance compared to the vanilla attention mask, particularly under conditions of longer chunk lengths. This highlights the efficacy of the introduced masking approach. If the length of the action chunk is excessively prolonged, the robots ability to timely adapt its policy becomes constrained, leading to decline in overall performance. 27. 72.19 Discrete Actions Accelerate the Convergence. We retain discrete actions during training alongside the continuous Action Transformer, as we find that this hybrid approach not only speeds up the convergence of VLA training Figure 7 World model visualization. but also improves the ultimate success rate. As shown in Fig. 8, models trained with discrete action tokens achieve substantially higher success rate than those trained without them, with the advantage being most pronounced during the initial stages of training. Ablation Study of Wrist Camera and Proprioceptive State. As shown in Line 1 of Tab. 4, our model could achieve reasonable performance without the wrist camera or proprioceptive state on the LIBERO simulation benchmark. Incorporating these two sources of information brings in further performance Figure 8 Discrete action tokens accelerate the convergence of continuous action generation. Table 7 Ablation study of VLA model on efficiency and action chunking on LIBERO benchmark. Frequency is measured in Hz. Index 1 2 3 4 5 6 7 8 Action Type Discrete Discrete Discrete Discrete Discrete Discrete Continuous Continuous Continuous Action Chunking Wrist Camera History Length Frequency Goal Object Frequency Spatial Long Chunk Size = 5 Chunk Size = 10 0 0 1 1 1 1 0 1 1 2.50 3.69 1.88 3. 1.25 2.74 24.94 14.97 7.75 60.0 83.2 74.6 92.8 82.6 92.8 80.0 90.2 91. 71.0 90.0 72.8 91.6 89.6 97.8 89.6 92.4 99.4 2.50 3.69 1.88 3. 1.25 2.74 48.20 28.30 15.78 77.0 83.6 78.6 86.2 96.6 96.8 84.0 88.4 98. 10.4 46.0 17.0 64.0 61.2 80.8 48.6 67.0 81.4 Figure 9 Performance comparison between discrete action and continuous action. gains (see Line 2 and Line 4 in Tab. 4). In real-world experiments (Line 2 and Line 3 in Tab. 5), the robot consistently fails when the wrist camera or proprioceptive state is absent. On one hand, the wrist camera provides crucial visual feedback on the relative pose between the gripper and the object, especially when the robot is outside the field of view of the front camera. On the other hand, we find proprioceptive state is essential for accurately timing gripper closure and object lifting during manipulation. Efficiency Analysis. As shown in Tab. 7, incorporating additional input images, such as images from the wrist camera or historical frames, improves performance but reduces speed. For discrete actions, action chunking yields both higher inference speed and better performance compared to generating single action per inference step. Continuous action generation is significantly faster owing to its parallel generation nature, and frequency scale almost linearly with chunk size as generating additional actions incurs negligible extra time. Discrete Action and Continuous Action. Our model supports both discrete and continuous action generation, and the experimental results reveal clear advantage for the latter. Concretely, in the LIBERO simulation benchmark, continuous actions result in significantly faster convergence  (Fig. 9)  . Although the final performances of these two models in simulation are comparable, their performance gap becomes considerably significant in real-robot experiments. As shown in Table 5, our real-world experiments demonstrate much more substantial improvement when using continuous actions over discrete ones. World Model Pretraining for VLA Model. Our RynnVLA-002 unifies VLA model and world model into single training stage. We further investigate the possibility of cold starting VLA model with world model pretraining. This form of pretraining necessitates that the model develop an understanding of visual inputs, actions, and the underlying physical dynamics governing state transitions. We pretrain the model using the same data source as the VLA model. As presented in Table 8, employing the world model for pretraining leads to w/o World Model Pretrain 67.3 w/ World Model Pretrain Table 8 Ablation study of world model pretraining."
        },
        {
            "title": "Goal Object Spatial Long",
            "content": "77.8 82.9 23.0 84.0 30.2 79. 73.1 notable improvements in grasping performance. These findings highlight the potential of leveraging world model pretraining in robotic applications, particularly in enhancing task-specific performance through prior exposure to general world knowledge."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we propose RynnVLA-002, unified framework that integrates the VLA and world model, demonstrating that they enhance each other. Through this contribution, we aim to offer the embodied AI research community concrete methodology for enabling the synergistic interplay between the VLA and world model. Furthermore, we believe this work helps lay the groundwork for unified foundation for multi-modal understanding and generation that spans text, vision, and action."
        },
        {
            "title": "References",
            "content": "Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical AI. arXiv preprint arXiv:2501.03575, 2025. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-VL technical report. arXiv preprint arXiv:2502.13923, 2025. Amir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, and Yann LeCun. Navigation world models, 2024. https: //arxiv.org/abs/2412.03572. Suneel Belkhale and Dorsa Sadigh. MiniVLA: better VLA with smaller footprint, 2024. https://github.com/ Stanford-ILIAD/openvla-mini. Johan Bjorck, Fernando Castañeda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al. GR00T N1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734, 2025. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: vision-language-action flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. RT-1: Robotics transformer for real-world control at scale. Robotics: Science and Systems, 2023. Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. Qingwen Bu, Yanting Yang, Jisong Cai, Shenyuan Gao, Guanghui Ren, Maoqing Yao, Ping Luo, and Hongyang Li. Learning to act anywhere with task-centric latent actions. arXiv preprint arXiv:2502.14420, 2025. Remi Cadene, Simon Alibert, Alexander Soare, Quentin Gallouedec, Adil Zouitine, Steven Palma, Pepijn Kooijmans, Michel Aractingi, Mustafa Shukor, Dana Aubakirova, Martino Russi, Francesco Capuano, Caroline Pascal, Jade Choghari, Jess Moss, and Thomas Wolf. LeRobot: State-of-the-art machine learning for real-world robotics in Pytorch. https://github.com/huggingface/lerobot, 2024. Jun Cen, Chaohui Yu, Hangjie Yuan, Yuming Jiang, Siteng Huang, Jiayan Guo, Xin Li, Yibing Song, Hao Luo, Fan Wang, et al. Worldvla: Towards autoregressive action world model. arXiv preprint arXiv:2506.21539, 2025. Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, et al. GR-2: generative video-language-action model with web-scale knowledge for robot manipulation. arXiv preprint arXiv:2410.06158, 2024. Chang Chen, Yi-Fu Wu, Jaesik Yoon, and Sungjin Ahn. TransDreamer: Reinforcement learning with transformer world models. arXiv preprint arXiv:2202.09481, 2022. Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research, pages 16841704, 2025. Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation. Advances in Neural Information Processing Systems, 36:91569172, 2023. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1287312883, 2021. Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-A-Scene: Scene-based text-to-image generation with human priors. In European Conference on Computer Vision, pages 89106. Springer, 2022. Yanjiang Guo, Yucheng Hu, Jianke Zhang, Yen-Jen Wang, Xiaoyu Chen, Chaochao Lu, and Jianyu Chen. Prediction with action: Visual policy learning via joint denoising process. Advances in Neural Information Processing Systems, 37:112386112410, 2024. David Ha and Jürgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018. Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019. Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering Atari with discrete world models. In International Conference on Learning Representations, 2021. Zhi Hou, Tianyi Zhang, Yuwen Xiong, Hengjun Pu, Chengyang Zhao, Ronglei Tong, Yu Qiao, Jifeng Dai, and Yuntao Chen. Diffusion transformer policy. arXiv preprint arXiv:2410.15959, 2024. Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, and Jianyu Chen. Video prediction policy: generalist robot policy with predictive visual representations. arXiv preprint arXiv:2412.14803, 2024. Chi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, Yu-Chiang Frank Wang, and Fu-En Yang. ThinkAct: Vision-languageaction reasoning via reinforced visual latent planning. arXiv preprint arXiv:2507.16815, 2025. Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang. An embodied generalist agent in 3D world. In International Conference on Machine Learning, 2024. Chia-Yu Hung, Qi Sun, Pengfei Hong, Amir Zadeh, Chuan Li, Tan, Navonil Majumder, Soujanya Poria, et al. NORA: small open-sourced generalist vision language action model for embodied tasks. arXiv preprint arXiv:2504.19854, 2025. Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et al. π0.5: vision-language-action model with open-world generalization. arXiv preprint arXiv:2504.16054, 2025. Joel Jang, Seonghyeon Ye, Zongyu Lin, Jiannan Xiang, Johan Bjorck, Yu Fang, Fengyuan Hu, Spencer Huang, Kaushil Kundalia, Yen-Chen Lin, et al. DreamGen: Unlocking generalization in robot learning through neural trajectories. arXiv preprint arXiv:2505.12705, 2025. Xiaogang Jia, Qian Wang, Atalay Donat, Bowen Xing, Ge Li, Hongyi Zhou, Onur Celik, Denis Blessing, Rudolf Lioutikov, and Gerhard Neumann. MaIL: Improving imitation learning with selective state space models. In Conference on Robot Learning, 2024. Yuming Jiang, Siteng Huang, Shengke Xue, Yaxi Zhao, Jun Cen, Sicong Leng, Kehan Li, Jiayan Guo, Kexiang Wang, Mingxiu Chen, et al. RynnVLA-001: Using human demonstrations to improve robot manipulation. arXiv preprint arXiv:2509.15212, 2025. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Moo Jin Kim, Chelsea Finn, and Percy Liang. Fine-tuning vision-language-action models: Optimizing speed and success. arXiv preprint arXiv:2502.19645, 2025. Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang, Sangho Lee, et al. MolmoAct: Action reasoning models that can reason in space. arXiv preprint arXiv:2508.07917, 2025. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. LLaVA-OneVision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024a. Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, et al. CogACT: foundational vision-language-action model for synergizing cognition and action in robotic manipulation. arXiv preprint arXiv:2411.19650, 2024b. Shuang Li, Yihuai Gao, Dorsa Sadigh, and Shuran Song. Unified video action model. arXiv preprint arXiv:2503.00200, 2025a. Xiang Li, Cristina Mata, Jongwoo Park, Kumara Kahatapitiya, Yoo Sung Jang, Jinghuan Shang, Kanchana Ranasinghe, Ryan Burgert, Mu Cai, Yong Jae Lee, et al. LLaRA: Supercharging robot learning data for vision-language policy. In International Conference on Learning Representations, 2025b. Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, et al. Vision-language foundation models as effective robot imitators. arXiv preprint arXiv:2311.01378, 2023. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. LIBERO: Benchmarking knowledge transfer for lifelong robot learning. Advances in Neural Information Processing Systems, 36:4477644791, 2023a. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in Neural Information Processing Systems, 36:3489234916, 2023b. Pietro Mazzaglia, Tim Verbelen, Bart Dhoedt, Aaron Courville, and Sai Rajeswar. GenRL: Multimodal-foundation world models for generalization in embodied agents. Advances in Neural Information Processing Systems, 37: 2752927555, 2024. Vincent Micheli, Eloi Alonso, and François Fleuret. Transformers are sample-efficient world models. arXiv preprint arXiv:2209.00588, 2022. Masashi Okada and Tadahiro Taniguchi. Dreamingv2: Reinforcement learning with discrete world models without reconstruction. In IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 985991. IEEE, 2022. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, arXiv preprint and Sergey Levine. FAST: Efficient action tokenization for vision-language-action models. arXiv:2501.09747, 2025. Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, et al. SpatialVLA: Exploring spatial representations for visual-language-action model. arXiv preprint arXiv:2501.15830, 2025. Moritz Reuss, Ömer Erdinç Yağmurlu, Fabian Wenzel, and Rudolf Lioutikov. Multimodal diffusion transformer: Learning versatile behavior from multimodal goals. arXiv preprint arXiv:2407.05996, 2024. Jan Robine, Marc Höftmann, Tobias Uelwer, and Stefan Harmeling. Transformer-based world models are happy with 100k interactions. arXiv preprint arXiv:2303.07109, 2023. Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015. Yide Shentu, Philipp Wu, Aravind Rajeswaran, and Pieter Abbeel. From LLMs to actions: Latent codes as bridges in hierarchical robot control. In IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 85398546. IEEE, 2024. Mustafa Shukor, Dana Aubakirova, Francesco Capuano, Pepijn Kooijmans, Steven Palma, Adil Zouitine, Michel Aractingi, Caroline Pascal, Martino Russi, Andres Marafioti, et al. SmolVLA: vision-language-action model for affordable and efficient robotics. arXiv preprint arXiv:2506.01844, 2025. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, et al. Octo: An open-source generalist robot policy. arXiv preprint arXiv:2405.12213, 2024. Yang Tian, Sizhe Yang, Jia Zeng, Ping Wang, Dahua Lin, Hao Dong, and Jiangmiao Pang. Predictive inverse dynamics models are scalable learners for robotic manipulation. arXiv preprint arXiv:2412.15109, 2024. Xiaofeng Wang, Zheng Zhu, Guan Huang, Boyuan Wang, Xinze Chen, and Jiwen Lu. WorldDreamer: Towards general world models for video generation via predicting masked tokens. arXiv preprint arXiv:2401.09985, 2024. Yating Wang, Haoyi Zhu, Mingyu Liu, Jiange Yang, Hao-Shu Fang, and Tong He. VQ-VLA: Improving vision-languageaction models via scaling vector-quantized action tokenizers. arXiv preprint arXiv:2507.01016, 2025a. Yihao Wang, Pengxiang Ding, Lingxiao Li, Can Cui, Zirui Ge, Xinyang Tong, Wenxuan Song, Han Zhao, Wei Zhao, Pengxu Hou, Siteng Huang, Yifan Tang, Wenhui Wang, Ru Zhang, Jianyi Liu, and Donglin Wang. VLA-Adapter: An effective paradigm for tiny-scale vision-language-action model. In Proceedings of the 40th AAAI Conference on Artificial Intelligence, 2025b. Junjie Wen, Minjie Zhu, Yichen Zhu, Zhibin Tang, Jinming Li, Zhongyi Zhou, Chengmeng Li, Xiaoyu Liu, Yaxin Peng, Chaomin Shen, et al. Diffusion-VLA: Scaling robot foundation models via unified diffusion and autoregression. arXiv preprint arXiv:2412.03293, 2024. Junjie Wen, Yichen Zhu, Jinming Li, Zhibin Tang, Chaomin Shen, and Feifei Feng. DexVLA: Vision-language model with plug-in diffusion expert for general robot control. arXiv preprint arXiv:2502.05855, 2025a. Junjie Wen, Yichen Zhu, Jinming Li, Minjie Zhu, Zhibin Tang, Kun Wu, Zhiyuan Xu, Ning Liu, Ran Cheng, Chaomin Shen, et al. TinyVLA: Towards fast, data-efficient vision-language-action models for robotic manipulation. IEEE Robotics and Automation Letters, 2025b. Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, and Tao Kong. Unleashing large-scale video generative pre-training for visual robot manipulation. arXiv preprint arXiv:2312.13139, 2023. Jialong Wu, Shaofeng Yin, Ningya Feng, Xu He, Dong Li, Jianye Hao, and Mingsheng Long. iVideoGPT: Interactive videogpts are scalable world models. Advances in Neural Information Processing Systems, 37:6808268119, 2025a. Weijia Wu, Zhuang Li, Yefei He, Mike Zheng Shou, Chunhua Shen, Lele Cheng, Yan Li, Tingting Gao, and Di Zhang. Paragraph-to-image generation with information-enriched diffusion model. International Journal of Computer Vision, pages 122, 2025b. Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, and Zhiting Hu. Language models meet world models: Embodied experiences enhance language models. Advances in Neural Information Processing Systems, 36:7539275412, 2023. Seonghyeon Ye, Joel Jang, Byeongguk Jeon, Sejune Joo, Jianwei Yang, Baolin Peng, Ajay Mandlekar, Reuben Tan, Yu-Wei Chao, Bill Yuchen Lin, et al. Latent action pretraining from videos. arXiv preprint arXiv:2410.11758, 2024. Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. VideoLLaMA 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025a. Hongyin Zhang, Pengxiang Ding, Shangke Lyu, Ying Peng, and Donglin Wang. GEVRM: Goal-expressive video generation model for robust visual manipulation. In International Conference on Learning Representations, 2025b. Jianke Zhang, Yanjiang Guo, Xiaoyu Chen, Yen-Jen Wang, Yucheng Hu, Chengming Shi, and Jianyu Chen. HiRT: Enhancing robotic control with hierarchical robot transformers. In Conference on Robot Learning, 2024. Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, et al. CoT-VLA: Visual chain-of-thought reasoning for vision-language-action models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 17021713, 2025a. Tony Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware. Robotics: Science and Systems, 2023. Wei Zhao, Pengxiang Ding, Min Zhang, Zhefei Gong, Shuanghao Bai, Han Zhao, and Donglin Wang. VLAS: Visionlanguage-action model with speech instructions for customized robot manipulation. In International Conference on Learning Representations, 2025b. Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong, and Chuang Gan. 3D-VLA: 3D vision-language-action generative world model. In International Conference on Machine Learning, 2024. Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal Daumé III, Andrey Kolobov, Furong Huang, and Jianwei Yang. TraceVLA: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. In International Conference on Learning Representations, 2025a. Ruijie Zheng, Jing Wang, Scott Reed, Johan Bjorck, Yu Fang, Fengyuan Hu, Joel Jang, Kaushil Kundalia, Zongyu Lin, Loic Magne, et al. FLARE: Robot learning with implicit world modeling. arXiv preprint arXiv:2505.15659, 2025b. Zhide Zhong, Haodong Yan, Junfeng Li, Xiangchen Liu, Xin Gong, Wenxuan Song, Jiayi Chen, and Haoang Li. FlowVLA: Thinking in motion with visual chain of thought. arXiv preprint arXiv:2508.18269, 2025. Zhongyi Zhou, Yichen Zhu, Minjie Zhu, Junjie Wen, Ning Liu, Zhiyuan Xu, Weibin Meng, Ran Cheng, Yaxin Peng, Chaomin Shen, et al. ChatVLA: Unified multimodal understanding and robot control with vision-language-action model. arXiv preprint arXiv:2502.14420, 2025. Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, et al. RT-2: Vision-language-action models transfer web knowledge to robotic control. In Conference on Robot Learning, pages 21652183. PMLR, 2023. Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, et al. OpenVLA: An open-source vision-language-action model. In Conference on Robot Learning, pages 26792713. PMLR, 2024."
        }
    ],
    "affiliations": [
        "DAMO Academy, Alibaba Group",
        "Hupan Lab",
        "Zhejiang University"
    ]
}