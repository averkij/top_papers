{
    "paper_title": "Consolidating Reinforcement Learning for Multimodal Discrete Diffusion Models",
    "authors": [
        "Tianren Ma",
        "Mu Zhang",
        "Yibing Wang",
        "Qixiang Ye"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Optimizing discrete diffusion model (DDM) with rewards remains a challenge: the non-autoregressive paradigm makes importance sampling intractable and rollout complex, puzzling reinforcement learning methods such as Group Relative Policy Optimization (GRPO). In this study, we introduce MaskGRPO, the first viable approach to enable scalable multimodal reinforcement learning in discrete diffusion with effective importance sampling and modality-specific adaptations. To this end, we first clarify the theoretical foundation for DDMs, which facilitates building an importance estimator that captures valuable token fluctuation for gradient updates. We then delicately tailored the rollout method for visual sequences, which yields diverse completions and reliable optimization gradients. Upon math reasoning, coding, and visual generation benchmarks, MaskGRPO brings more stable and efficient updates, leading to stronger reasoning performance and better generation quality. This study establishes MaskGRPO as a systematic policy optimization approach and the first practical way for discretized visual diffusion."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 0 8 8 2 0 . 0 1 5 2 : r Preprint."
        },
        {
            "title": "CONSOLIDATING REINFORCEMENT LEARNING FOR\nMULTIMODAL DISCRETE DIFFUSION MODELS",
            "content": "Tianren Ma, Mu Zhang, Yibing Wang, Qixiang Ye University of Chinese Academy of Sciences matianren18@mails.ucas.ac.cn;qxye@ucas.ac.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "Optimizing discrete diffusion model (DDM) with rewards remains challengethe non-autoregressive paradigm makes importance sampling intractable and rollout complex, puzzling reinforcement learning methods such as Group Relative Policy Optimization (GRPO). In this study, we introduce MaskGRPO, the first viable approach to enable scalable multimodal reinforcement learning in discrete diffusion with effective importance sampling and modality-specific adaptations. To this end, we first clarify the theoretical foundation for DDMs, which facilitates building an importance estimator that captures valuable token fluctuation for gradient updates. We then delicately tailored the rollout method for visual sequences, which yields diverse completions and reliable optimization gradients. Upon math reasoning, coding, and visual generation benchmarks, MaskGRPO brings more stable and efficient updates, leading to stronger reasoning performance and better generation quality. This study establishes MaskGRPO as systematic policy optimization approach and the first practical way for discretized visual diffusion. Our code is available at https://github.com/martian422/MaskGRPO. Figure 1: Left: MaskGRPO consistently improves the base model with significant RL income across text and image generation tasks. Right: an intuitive demonstration of our method, integrated with modality-specific innovations on importance estimation and sampling methods."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent progress of post-training generative models has been driven by the advances of optimization algorithms, architectural design, and large-scale reward-based learning Rafailov et al. (2023); Wang et al. (2025b); Liu et al. (2025). Among these, Group Relative Policy Optimization (GRPO) Shao et al. (2024) has emerged as powerful and scalable paradigm, improving reasoning performance of large language models and enhancing preference alignment of visual generative models. However, extending such policy optimization to discrete diffusion models (DDMs) remains challenge. Unlike autoregressive models that decode sequentially, discrete diffusion generates tokens in parallel at arbitrary positions Sahoo et al. (2024). This parallelism complicates both rollout generation, where stochastic yet coherent samples are required Liu et al. (2025) for exploration, and importance estimation, which is crucial for optimization Schulman et al. (2017). Existing approaches offer only partial solutions: semi-autoregressive samplers Arriola et al. (2025); Nie et al. (2025) mitigate inference issues for text, while low-confidence re-masking for images Chang et al. (2022) lack stochastic flexibility for robust group comparisons. Similarly, early attempts Zhao et al. (2025); Tang et al. (2025) at importance estimation relied on masking heuristics that violated conditioning 1 Preprint. assumptions. Monte Carlobased estimators Zhu et al. (2025); Yang et al. (2025) improve faithfulness but remain computationally expensive. In this study, we propose MaskGRPO, consolidated extension of GRPO to multimodal discrete diffusion (shown in Fig. 1), built upon modality-specific innovations in both sampling and importance estimation. MaskGRPO is guided by the distinct structural properties of language and vision: Language. While training native discrete diffusion models depart from the autoregressive1 paradigm, their prediction on natural language still exhibits degree of ARness\" Gong et al. (2025): tokens closer to observed context are predicted with higher certainty, and rollouts diverse as length extends. Leveraging this property, we introduce fading-out masking estimator, which progressively increases the masking rate toward later tokens with well-controlled randomness. This concentrates estimation on high-uncertainty regions, towards more efficient and empirically reliable objective. Vision. Images lack sequential structure and exhibit strong global token correlations Chan et al. (2024). We argue that effective likelihood estimation requires highly truncated mask rates to capture informative variation. Furthermore, we propose sampler that relaxes rigid scheduling constraints in existing methods via probabilistic decoding. By encouraging diverse yet high-quality rollouts, our sampler better aligns with the GRPO principle of exploiting group-wise relative advantages. Through these contributions, we build the first systematic GRPO approach for multimodal discrete diffusion. Beyond empirical results in mathematical reasoning and coding that almost double the income from RL, our method also demonstrates significant improvement on text-image alignment, and visual fidelity. Building upon clarified foundation for DDMs, our analysis highlights that, policy optimization in discrete diffusion is only effective when samplers and estimators are designed in modality-aware fashion. This establishes new foundation for reward-based learning in DDMs and points toward more general theory of preference-driven optimization across modalities."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "2.1 DISCRETE DIFFUSION MODEL DDM defines forward process over discrete variables by gradually corrupting tokens to absorbing state through continuous-time Markov process. We denote the clean data as xt=0 (x0 for short), and noise it gradually as 1. Let αt be the noise scheduler (a monotonically decreasing survival function that satisfies α0 = 1, α1 = 0 ), the corrupted data distribution at time is determined as xt q(xtx0, t), q(xtx0, t) = Cat(xt; αtx0 + (1 αt)m) (1) Let δ(x(t,i), m) be the indicator function that is only activated if the i-th position of xt is m. For linear scheduler, the objective is derived as the evidence lower bound (ELBO) of log πθ(x0xt): LDDM = Et, x0, xt[ 1 (cid:88) i=1 δ(x(t,i), m) log πθ(x(0,i)xt)] = Et, x0, xt[ℓπθ (xt, x0)]. (2) We denote the loss term as ℓπθ (xt, x0) for later usage. For conditional generation where prompt is given, we write ℓπθ (xt, x0c) for simplicity. Following MDLMs deduction Sahoo et al. (2024), assume that the network can reconstruct x0 perfectly, we use πθ(xt) to approximate this denoising process, and get the sampling rule as pθ(xsxt) = 1, 1αs , 1αt αsαt 1αt 0, πθ(xt), if xs = xt, xt = m, if xs = m, xt = m, if xs = m, xt = m, otherwise. (3) 2.2 GRPO FOR AUTOREGRESSIVE MODEL GRPO Shao et al. (2024) introduces two key innovations to simplify online-RL approaches like PPO Schulman et al. (2017). First, GRPO eliminates the explicit modeling of value function and 1In this paper, we use autoregressive in its conventional sense of causally ordered next-token prediction. 2 Preprint. instead computes advantages in group-relative manner. This design enables GRPO focusing on relative performance within local context, reducing sensitivity to absolute reward scales. Second, GRPO extends the clipped objective by incorporating an explicit KL divergence penalty term between the current policy πθ and reference policy πref. Formally, for each question D, GRPO samples group of responses (also addressed as rollouts) {o1, o2, . . . , oG} from the old policy πθold. For rollout oi, the reward system gives action value ri, and the relative advantage of it is normalized as Ai = ri mean({rj}G std({rj}G j=1) j=1) . For position 1 oi , the token-level importance is calculated as ρk = πθ(ok πθold (ok c, o<k ) c, o<k ) = exp (cid:0) log πθ(ok c, o<k ) log πθold(ok c, o<k With ϵ controlling the clip range, the reward component is defined as R(θ, c) = 1 G (cid:88) i=1 1 oi oi (cid:88) k=1 (cid:16) min Ai, clip(ρk ρk , 1 ϵ, 1 + ϵ)Ai (cid:17) , Finally, the GRPO objective is expressed as reward-penalty tradeoff, as max θ EcD,o1:Gπθ(c) (cid:104) R(θ, c) βDKL (cid:2)πθ(c) πref(c)(cid:3)(cid:105) , where β regulates the strength of the KL regularization. 2.3 ACCOMMODATING GRPO FOR DISCRETE DIFFUSION MODEL )(cid:1). (4) (5) (6) (7) We accommodate GRPOs objective for DDM in this section. To avoid confusion and align with settings in Eq. 7, we move the timestep notation to the top-right corner of the variable. Accordingly, let ot q(oto, t) denote the corrupted (reversed) response with strength t. Rolling back sequentially on AR models response can be regarded as reversing on the timeline of the response. In other words, the first tokens, ok, of AR models response can be regarded as re-masked with = oik . oi Therefore, for each completion from DDM, we can calculate the sub-sequence level importance ρt by gradually reversing it. Recalling ℓπθ from Eq. 2, for small interval δt, let ot = ot ot+δt denote the tokens that are unmasked from timestep + δt to t. We propose that the differentiation on DDMs intractable log-likelihood can be approximated (refer to the Appendix for details) using log π1( otc, ot+δt) log π2( otc, ot+δt) ℓπ1 (ot, t, oc) ℓπ2 (ot, t, oc) The above expression indicates that, to evaluate the fluctuation of likelihood for newly unmasked tokens in ot, we can utilize the difference of models prediction for the full sequence at time t. Hence, we derive the calculable importance estimation and KL divergence as i, oic)(cid:1). i, oic) ℓπθ (ot (ot We temporarily skip the clip operation for simplicity, and accommodate Eq. 7 as = exp (cid:0)ℓπθ (ot ˆρt i, oic) ℓπθ (ot i, oic) ℓπθold i, oic)(cid:1) (cid:0)ℓπθref KL = exp (cid:0)ℓπθref ˆDi,t i, oic)(cid:1) 1. (10) (ot (ot (9) (8) max θ EcD,o1:Gπθ(c) (cid:104) 1 (cid:88) i=1 Ai oi µ (cid:88) (ˆρtj β ˆDi,tj KL ) j=1 (cid:105) , tj = j/µ (11) Upon this foundation, we revisit the prior endeavors on DDM optimization: diffu-GRPO Zhao et al. (2025) applies masks to prompts and extract likelihood on the entirely masked completions ot=1. Following LLaDA-1.5 Zhu et al. (2025), UniGRPO Yang et al. (2025) iteratively masks varying ratio of completions. While these strategies provide gradient signals, they either disrupt the conditional dependency or pose high budget for Monte Carlo style estimations. In summary, current inefficiency of likelihood estimation ties DDM to limited settings and obscures its potential in broader contexts, especially in reasoning or visual generation that may involve thousands of tokens per sample. 3 Preprint. Figure 2: demonstration of reversing (re-mask) methods. We set mask raio = 0.6. Random reversing (right) applies masks to all the tokens with equal probability, while AR-like reversing (left) adapts fading-out strategy. See Appendix for complete showcases."
        },
        {
            "title": "3 MASKGRPO",
            "content": "3.1 IMPORTANCE ESTIMATION Importance sampling is central to the GRPO objective, where it functions as an effective \"reweighting\" of the rewards, aiming to align with the current policys distribution with reduced bias. In practice, the importance is calculated by the differentiate of predictions as in Eq. 8. Inspired by recent studies Wang et al. (2025a) that suggest tokens with high entropy as the most informative signal, we modify the estimator to capture valuable fluctuation instead of highly confident predictions from low-mask sequences. First, to allocate the timestep budget effectively, we clamp the sampling range from (0, 1) to (γ, 1), where γ serves as cut-off of low mask ratios. Second, rather than relying on randomly masking, we design low-discrepancy estimators that utilize the autoregressive biases in language and localitydriven correlations in vision. We implement the reverse process by managing independent sets of random seeds on each device, which is crucial for stable importance and KL computation. With our designed operator Rev(, t), we obtain stable, low-variance estimates driven by the stochasticity of ˆot Rev(o, t). Let language tokens fade out. Discrete diffusion models exhibit causal bias for language Gong et al. (2025), particularly in logically related tasks such as math and code. This property, referred to as AR-ness, has been identified to have strong correlation with models overall performance. Besides, as the semi-autogressive sampler (Alg. 3) is utilized, the rollouts also exhibit higher divergence as block extends, i.e., at the start of response, the models reasoning are rather simple setups, while real divergence or aha! moment emerges as the reasoning proceeds. This observation motivates us to exploit the importance estimation through an AR-like reversing procedure, and assign higher attention to the latter tokens. The algorithm shown in Alg. 1 maintains delicate balance between randomness and fading-out property with almost no additional calculation, and serves as plug-and-play module. Algorithm 1 AR-like Reversing (for text, ours) Require: Token batch RBL, prompt indicator C, mask token m, seed s, ratio 1: Set random seed with 2: Prompt (padded) length Lc (cid:80) 3: Non-prompt length Lo Lc 4: Linear decay linspace(1, 0, Lo) 5: Normalize pn d((1r)Lo) 6: 0Lc pn,P repeat(p, B) 7: (0, 1)BL, (C) (R > ) 8: Apply masking where(M, m, X) 9: return X, (cid:80) Algorithm 2 Random Reversing (for image) Require: Token batch RBL, prompt indicator C, mask token m, seed s, ratio # randomness is managed 1: Set random seed with # similar as in q(xtx0, t) 2: Constant curve rL 3: Expand to batch size repeat(p, B) 4: Sample random matrix (0, 1)BL 5: Determine mask (C) (R < ) 6: Apply masking where(M, m, X) 7: return X, 4 Preprint."
        },
        {
            "title": "3.2 ROLLOUT ADAPTION",
            "content": "Figure 3: comparison of sampled results. With identical sampling parameters on MMaDA (equipped with 8192-vocab visual tokenizer Xie et al. (2024)), images sampled by our emerge method (below) demonstrate better texture and expressiveness. Complementing the estimators, we align the rollout sampler with structural patterns, enabling efficient and stable training with GRPO. For text generation, we employ the widely adopted semiautoregressive sampler (Alg. 3) for optimal generation quality. This sampler uses low-confidence re-mask strategy with block-wise decoding, reflecting the inherently sequential structure of language. Let visual tokens emerge from masks. While expressed as discrete vocabularies, visual tokens differ from language significantly in terms of entropy, bits of information and so on Chan et al. (2024). The low-confidence based MaskGIT sampler, which was proposed with 1024-vocab tokenizer Chang et al. (2022), while viable, does not perform as expected for high-fidelity tokenizers. This curse has been partly discussed as sampling inaccuracy Zheng et al. (2025), yet we noticed that it becomes severe on large-vocabulary visual tokenizers and cannot be addressed by simply operating at higher precision. To overcome this problem which hinders models potential on visual generation, we refer to MDLM Sahoo et al. (2024) and propose the probability-based emerging sample strategy. As shown in Fig 3 and Alg. 4, our sampler does not enforce decoding quantity per prediction, but let the visual tokens emerge from masks naturally with probabilistic control. While faithful to the principled DDM theory (Eq. 3), our sampler shows significantly better expressiveness for vision. As for reversing, unlike continuous diffusion RL methods Liu et al. (2025) where the importance is calculated across almost all traversed timesteps, we find that discrete visual diffusion requires large truncation on reverse range.The tokenized patches show strong global correlations, making the prediction largely insensitive to small mask ratios (see Sec.4.3 for details), and small truncation may even lead to exploded variance. Therefore, we keep the reversing random as Alg. 2, while the reverse strength is held at high level, e.g., setting γ = 0.8 to obtain meaningful importance estimates. Algorithm 3 Semi-autoregressive (text) Algorithm 4 Token Emerge (image, ours) range [(b 1)Lblock, bLblock] xb x[range] for = 1 to Lblock/s do 1: Inputs: prompt c, completion length 2: Settings: block length Lblock, token per step 3: Initialize: mL. 4: for = 1 to L/Lblock do 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: end for 15: Return: fully unmasked sequence pconf fθ(x, c)[range] xb where(xb = m, argmax(pconf), xb) mre argsort(pscore)[: Lblock ks] xb where(mre, m, xb)) end for x[range] xb 5 1: Inputs: prompt c, completion length 2: Settings: total steps K, scheduler αt 3: Initialize: x1 mL. 4: for = 1 to do 5: 6: Kk+1 logits fθ(xt, c) , Kk # Infer from probability 7: 8: π Softmax(logits) qs αsαt 1αt π + δm 1αs 1αt # is also sampled xpred ˆx Cat(ˆx; qs) xs where(xt = m, xpred, xt) 9: 10: 11: end for 12: Return: fully unmasked sequence x0 Preprint."
        },
        {
            "title": "3.3 ALGORITHM FRAMEWORK",
            "content": "To summarize, we implement GRPO for discrete diffusion models with integrated efficient modalityspecific importance estimators, and modification on visual sequence sampling. The overall framework consists of: (i) Sample set of full completions for each prompt with method 3, 4, (ii) for each completion o, generate multiple masked ˆot Rev(o, t), where Rev(, t) is our designed reverse (iii) estimating per-completion function with controllable randomness, as shown in Alg. 1, 2. advantages based on reward r, and (iv) updating the policy using importance ˆρtj and divergence ˆDtj KL. An algorithmic demonstration of MaskGRPO is provided in Alg. 5. Algorithm 5 MaskGRPO Policy Gradient Optimization (ours) Require: Reference model πref, prompt distribution D, number of completions per prompt G, number of inner updates µ, random seed set Sj=1µ, 1: Initialize policy πθ πref 2: while not converged do 3: 4: 5: 6: 7: 8: 9: πold πθ Sample prompt Sample completions oi πold( c), [G] For each oi, compute reward ri and advantage Ai using Eq. 4 for gradient update iterations = 1, . . . , µ do Get timestep: tj γ + (1 γ) µ Construct masked completion ˆoi,tj Rev(oi, tj, Sj) For πθ, πold, πref, use Eq. 9, 10 to estimate importance ˆρtj KL with ˆoi,tj Compute MaskGRPO objective in Eq. 11 and update πθ via gradient descent and ˆDi,tj 10: 11: end for 12: 13: end while 14: return πθ"
        },
        {
            "title": "4 EXPERIMENT",
            "content": "4.1 SETUP We conduct experiments across multimodal scenarios and evaluated MaskGRPO extensively on math reasoning, coding, and text-to-image generation benchmarks. For language tasks, following the practice of diffu-GRPO, we conduct with similar learning rate of 3e6, rollouts per prompt = 6, and global batch size of 96 (bs = 6 on 8A100 GPU, with gradient accumulation = 2). The rollout is sampled with block length of 16, and 2 tokens per step. We use iteration µ = 6, and train 6000 steps on all language tasks, which takes up to 25% fewer training steps and 50% less iteration than that of diffu-GRPO. For image generation, we use rollouts per prompt = 9 for exploration, iteration µ = 8, and global batch size of 72 (we exclude two GPUs for reward servers). Each rollout is sampled with our emerge sampler, using 12 steps with cosine scheduler. The RL training takes 1500 global steps. Reward design and evaluation details are provided in Appendix. B. Models We select LLaDA-8B-Instruct Nie et al. (2025), an open-sourced native DDM and its multimodal adaption MMaDA-8B-Base Yang et al. (2025), which unlocks the ability to perform discrete diffusion on image sequences, as the start point of optimization. Both models are initialized from publicly available pre-trained checkpoints. Metrics We evaluate the proposed MaskGRPO framework on text and image generation tasks, using suite of standard benchmarks to assess its performance gain. (1) Text Generation. To evaluate models mathematical reasoning and coding capabilities, we use GSM8K Cobbe et al. (2021), MATH500 Lightman et al. (2023) and MBPP Austin et al. (2021) benchmarks. We also compare with the recent RL baselines including diffu-GRPO Zhao et al. (2025), wd1 Tang et al. (2025) and UniGRPO (re-implemented due to unavailable codebase) Yang et al. (2025) on these tasks. Preprint. Table 1: Evaluation on math reasoning and coding benchmarks. For fair comparison, we choose LLaDA-8B-Instruct as the initial point. All results are reported with zero-shot prompting and pass@1 metric. refers to our re-implementation. RL Method / Seq Len LLaDA-8B-Instruct GSM8K MATH 256 76.7 512 78.2 256 32. 512 36.2 MBPP 256 39.0 w/ diffu-GRPO Zhao et al. (2025) w/ UniGRPO Yang et al. (2025) w/ wd1 Tang et al. (2025) w/ TraceRL Wang et al. (2025c) w/ MaskGRPO (ours) 79.8 (+3.1) 81.1 (+4.4) 80.8 (+4.1) 82.1 (+5.4) 84.2 (+7.5) 81.9 (+3.7) 82.0 (+3.8) 82.3 (+4.1) 83.3 (+5.1) 85.3 (+7.1) 34.4 (+2.0) 35.0 (+2.6) 34.4 (+2.0) 35.9 (+3.5) 37.6 (+5.2) 39.0 (+2.8) 38.8 (+2.6) 39.0 (+2.8) 39.5 (+3.3) 41.5 (+5.2) 42.1 (+3.1) 43.1 (+4.1) 43.9(+4.9) 45.4 (+6.4) (2) Image Generation. To evaluate models text-image alignment, we first utilize the widely adopted GenEval Ghosh et al. (2023) and DPG-Bench Hu et al. (2024) (see Appendix C) as the metrics. Then, we evaluate the generated samples aesthetic quality using human preference scorers like DeQA You et al. (2025), ImageReward 2023, and HPSv3 Ma et al. (2025b). For references rather than definitive comparisons, we include both specialized diffusion models, such as SDXL, and leading discrete generation models like Show-o Xie et al. (2024) and Janus-Pro Chen et al. (2025b). 4.2 PERFORMANCE AND COMPARISON RESULTS Language Tasks MaskGRPO substantially enhances the mathematical reasoning and coding capabilities of LLaDA. As shown in Table 1, our method achieves over 5% absolute improvement in solution accuracy on GSM8K, MATH500, and MBPP, nearly doubling the RL gains compared to prior methods with less steps (6000 vs 7000+). On GSM8K, MaskGRPO allows the model to surpass previous approaches while requiring only half the completion length (256 vs 512), demonstrating its effective improvement of reasoning ability. Representative examples are included in Appendix C. Visual Generation Tasks To our knowledge, MaskGRPO is the first method to achieve effective GRPO optimization of aesthetic quality and textimage alignment in discrete diffusion models. Table 3 shows consistent improvements in alignment with human preferences, which are not reported in previous DDM works. Moreover, results on GenEval (Tab. 2) and DPG-Bench (Tab. 4) further confirm the effectiveness of our framework: with well-designed RL setup, discrete generation models can approach the performance of leading commercial systems. Qualitative samples are provided in Fig. 4. Table 2: Evaluation on GenEval. SFT indicates that we SFT the base model with BLIP3-o dataset Chen et al. (2025a) for clean instruction-tuning data distilled from GPT-4o. Model Single. Two. Count. Color. Pos. Attr. Overall GenEval Continuous Generation SDXL Podell et al. (2023) DALL-E 3 Betker et al. (2023) SD3.5-L Esser et al. (2024) FLUX.1-dev Labs (2025) Discrete Generation Show-o Xie et al. (2024) Janus-Pro Chen et al. (2025b) MMaDA Yang et al. (2025) w/ UniGRPOYang et al. (2025) w/ MaskGRPO (ours) w/ SFT+MaskGRPO (ours) 0.98 0.96 0.98 0.98 0.95 0.99 0.96 0.99 0.99 0.99 0.39 0.47 0.73 0. 0.49 0.59 0.45 0.61 0.66 0.69 0.85 0.83 0.83 0.93 0.82 0.90 0.81 0.84 0.89 0.90 0.15 0.43 0.34 0.68 0.11 0.79 0.14 0.20 0.73 0.77 0.23 0.45 0.47 0. 0.28 0.66 0.25 0.37 0.69 0.72 0.55 0.67 0.71 0.82 0.53 0.80 0.56 0.63 0.80 0.84 0.74 0.87 0.89 0.93 0.52 0.89 0.60 0.76 0.85 0.85 Preprint. Figure 4: Qualitative comparison. Results are generated with identical sampling parameters and shown in {original, w/ RL} pairs. MaskGRPO demonstrates substantial improvement on the aesthetic quality of generated images, in terms of artistic style, photographic details and overall atmosphere. We strongly recommend that the readers view more portrait samples at Fig. 7. Table 3: Evaluation on compositional generation and human preference metrics. We calculate the Preference Scores on samples generated by DPG-Bench prompts. Model Compositional Generation Preference Scores GenEval DPG-Bench DeQA ImageReward HPSv3 MMaDA Yang et al. (2025) w/ MaskGRPO w/ SFT+MaskGRPO 0. 0.80 0.84 0.71 0.75 0.82 3.99 4.10 4.18 0. 1.18 1.30 8.81 9.40 9.63 4.3 DISCUSSION Truncation hyper-parameters We perform ablation studies on GSM8K with timestep truncation ratios γ 0.2, 0.4, 0.6, 0.8 for 4000 steps. As shown in Fig. 5 (a), both the absence of truncation and overly aggressive truncation degrade training stability. To promote stable learning rather than premature convergence, we adopt γ = 0.6 as the default setting. For vision tasks, we conduct experiments with HPSv3 as the primary reward model in (c). At γ = 0.4, the strong correlation among tokenized image patches leads to boosting divergence between old and new policies, causing training failure. Based on this, we set γ = 0.8 for efficient and stable training in image generation. Besides, we also conduct experiments on the clip parameter ϵ. As shown in (d), relatively small ϵ ensures better training dynamics for visual tasks, unlike language tasks where ϵ is usually set to 0.2 or larger. We attribute this to the global correlation and higher entropy of visual token prediction, core reason that we adapt modality-specific design for reinforcing DDMs. Reverse method Concurrent work TraceRL Wang et al. (2025c) proposes to track the generation trace of DDMs and reverses strictly along these recorded traces. This mechanism relies on predefined paths and has only been demonstrated on block-attention architecture (SDAR JetAstra-ML (2025)). Moreover, TraceRL requires maintaining trace maps throughout training, and its deterministic reversal leads to limited flexibility in estimating prior tokens. We re-implement TraceRL on the full-attention language model, namely LLaDA-8B-Instruct, and report results in Tab. 1. In addition, we fix γ = 0.6 and ablate the reverse strategies in Fig. 5 (b). Our proposed AR-like reversing method consistently outperforms TraceRL in reinforcement learning. We attribute this performance gap to TraceRLs 8 Preprint. path-dependent formulation, which constrains exploration and induces biased estimation of sequencelevel importance. For further qualitative evidence, we provide visualized comparison of reversing strategies under varying ratios in Fig. 6. Figure 5: Figures for ablative studies. a: ablation on timestep truncation in language tasks. b: ablation on reverse methods in language tasks. c: ablation on timestep truncation in vision tasks. d: ablation for clip range in vision tasks. See text for detailed explanation. Figure 6: comparison of reversing methods. Language are decoded in semi-autoregressive manner, and the trace method Wang et al. (2025c) (above) reverses the decoding path accordingly. AR-like methods (below, ours), in contrast, balance the autoregressive bias and randomness with controlled probability, and can capture front fluctuations in the sequence at smaller masking ratios. 9 Preprint. Rollout comparison We investigate how the proposed emerge sampler improves generation quality under reinforcement learning. As an initial step, we substitute the vanilla MaskGIT style sampler with our method and evaluate performance on MMaDA using GenEval. Before RL, our method produces samples with better textures, but the GenEval score is worse than that of vanilla method (0.51 vs. 0.56). This discrepancy arises because some of our outputs sometimes exhibit unstable or deformed object boundaries, which negatively affect detector-based metrics. After RL training, however, these instabilities are largely eliminated. The emerge sampler not only facilitates broader exploration during policy optimization, but also guides the model toward higherquality local optima that would otherwise be inaccessible to MaskGIT sampling. Consequently, our method achieves higher GenEval score (0.84 vs. 0.77), while also producing more stable and expressive generations. This progression highlights key advantage: although the emerge sampler may underperform at the pre-RL stage, its enhanced exploration dynamics ultimately yield stronger convergence and superior sample quality compared to the vanilla baseline."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Text Diffusion Models Discrete diffusion models have emerged as powerful alternative Sahoo et al. (2024); Nie et al. (2025) to autoregressive paradigms for language generation. Recent works Zhu et al. (2025); Gong et al. (2025) have shown competitive performance in math or code tasks. To obtain optimal results, while current scaled DDMs are trained on fully random noised corpses, state-of-the-art performances are usually obtained via semi-autoregressive decoding Arriola et al. (2025); Nie et al. (2025). While this inconsistency leaves space for more sophisticated training design, it also partially demonstrates the causal nature of language modality. Discrete Visual Diffusion Models Discrete diffusion, or its core idea of predicting on multiple discrete targets, has been applied to visual generation with prior endeavors like MaskGIT Chang et al. (2022), where low-confidence re-mask sampler with Gumbel noise is used for inference to enhance sample quality. However, this can limit output diversity and makes reliable likelihood estimation challenging Zheng et al. (2025); Ma et al. (2025a), creating bottleneck for online RL. Group Relative Policy Optimization GRPO and related reward-based optimization techniques have shown success in reinforcement learning for text generation and reasoning in autoregressive LLMs Shao et al. (2024). Continuous flow-matching methods also adapt this paradigm in recent works via designed SDE solver Liu et al. (2025). However, GRPOs application is fundamentally limited in discrete diffusion models, as it renders intractability on computing importance sampling weight, due to the lack of factorized likelihood Zhu et al. (2025)."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we introduce MaskGRPO, modality-aware extension of Group Relative Policy Optimization for discrete diffusion models. Recalling rollout sampling and likelihood estimation, we developed tailored strategies for language and vision generation: fading-out masking for text and probabilistic decoding for images. Our experiments demonstrate that these design choices substantially improve reasoning accuracy, text-image alignment, and sample diversity. These results highlight the importance of modality-specific samplers and estimators for effective policy optimization, and pave the way for unified reinforcement learning approaches across multimodal discrete diffusion."
        },
        {
            "title": "REFERENCES",
            "content": "Marianne Arriola, Aaron Gokaslan, Justin T. Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models. arXiv preprint arXiv 2503.09573, May 2025. doi: 10.48550/arXiv.2503.09573. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program Synthesis with 10 Preprint. Large Language Models. arXiv preprint arXiv 2108.07732, August 2021. doi: 10.48550/arXiv. 2108.07732. James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, and Aditya Ramesh. Improving Image Generation with Better Captions, 2023. URL cdn.openai. com/papers/dall-e-3.pdf. David M. Chan, Rodolfo Corona, Joonyong Park, Cheol Jun Cho, Yutong Bai, and Trevor Darrell. Analyzing The Language of Visual Tokens. arXiv preprint arXiv 2411.05001, November 2024. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. MaskGIT: Masked Generative Image Transformer. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 1130511315. IEEE, 2022. doi: 10.1109/CVPR52688.2022.01103. Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, Le Xue, Caiming Xiong, and Ran Xu. BLIP3-o: Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset. arXiv preprint arXiv 2505.09568, May 2025a. doi: 10.48550/arXiv.2505.09568. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling. arXiv preprint arXiv 2501.17811, January 2025b. doi: 10.48550/arXiv.2501.17811. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training Verifiers to Solve Math Word Problems. arXiv preprint arXiv 2110.14168, November 2021. doi: 10.48550/arXiv.2110.14168. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling Rectified Flow Transformers for High-Resolution Image Synthesis. arXiv preprint arXiv 2403.03206, March 2024. doi: 10.48550/arXiv.2403.03206. Zigang Geng, Yibing Wang, Yeyao Ma, Chen Li, Yongming Rao, Shuyang Gu, Zhao Zhong, Qinglin Lu, Han Hu, Xiaosong Zhang, Linus, Di Wang, and Jie Jiang. X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image Generative Models Great Again. arXiv preprint arXiv 2507.22058, July 2025. doi: 10.48550/arXiv.2507.22058. Dhruba Ghosh, Hannaneh Hajishirzi, Object-Focused An Advances Neural Information Processing Systems 2023, NeurIPS 2023, 2023. in Neural Information Processing Systems Framework 37: for and Ludwig Evaluating Schmidt. Text-to-Image GENEVAL: Alignment. on Annual Conference Shansan Gong, Ruixiang Zhang, Huangjie Zheng, Jiatao Gu, Navdeep Jaitly, Lingpeng Kong, and Yizhe Zhang. DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation. arXiv preprint arXiv 2506.20639, June 2025. doi: 10.48550/arXiv.2506.20639. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: Reference-free Evaluation Metric for Image Captioning. arXiv preprint arXiv 2104.08718, March 2022. doi: 10.48550/arXiv.2104.08718. Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment. arXiv preprint arXiv 2403.05135, March 2024. doi: 10.48550/arXiv.2403.05135. JetAstra-ML. JetAstra/SDAR, 2025. URL github.com/JetAstra/SDAR. Black Forest Labs. Flux-dev.1, 2025. URL github.com/black-forest-labs/flux. 11 Preprint. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets Verify Step by Step. arXiv preprint arXiv 2305.20050, May 2023. doi: 10.48550/arXiv.2305.20050. Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-GRPO: Training Flow Matching Models via Online RL. arXiv preprint arXiv 2505.05470, July 2025. doi: 10.48550/arXiv.2505.05470. Tianren Ma, Xiaosong Zhang, Boyu Yang, Junlan Feng, and Qixiang Ye. ReDDiT: Rehashing Noise for Discrete Visual Generation. arXiv preprint arXiv 2505.19656, 2025a. doi: 10.48550/arXiv. 2505.19656. Yuhang Ma, Yunhao Shui, Xiaoshi Wu, Keqiang Sun, and Hongsheng Li. HPSv3: Towards WideSpectrum Human Preference Score. arXiv preprint arXiv 2508.03789, August 2025b. doi: 10. 48550/arXiv.2508.03789. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large Language Diffusion Models. arXiv preprint arXiv 2502.09992, February 2025. doi: 10.48550/arXiv.2502.09992. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis. arXiv preprint arXiv 2307.01952, July 2023. doi: 10.48550/arXiv.2307.01952. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher Manning, and Chelsea Finn. Direct Preference Optimization: Your Language Model is Secretly Reward Model. arXiv preprint arXiv 2305.18290, 2023. Subham S. Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin T. Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and Effective Masked Diffusion Language Models. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. arXiv, 2024. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy Optimization Algorithms. arXiv preprint arXiv 1707.06347, August 2017. doi: 10.48550/arXiv. 1707.06347. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. arXiv preprint arXiv 2402.03300, April 2024. doi: 10.48550/arXiv.2402.03300. Xiaohang Tang, Rares Dolga, Sangwoong Yoon, and Ilija Bogunovic. Wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models. arXiv preprint arXiv 2507.08838, July 2025. doi: 10.48550/arXiv.2507.08838. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, Yuqiong Liu, An Yang, Andrew Zhao, Yang Yue, Shiji Song, Bowen Yu, Gao Huang, and Junyang Lin. Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning. arXiv preprint arXiv 2506.01939, June 2025a. doi: 10.48550/arXiv.2506.01939. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang, Liangdong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun Huang, and Zhongyuan Wang. Emu3: Next-Token Prediction is All You Need. arXiv preprint arXiv 2409.18869, September 2024. Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, and Jiaqi Wang. Unified Reward Model for Multimodal Understanding and Generation. arXiv preprint arXiv 2503.05236, March 2025b. doi: 10.48550/ arXiv.2503.05236. Preprint. Yinjie Wang, Ling Yang, Bowen Li, Ye Tian, Ke Shen, and Mengdi Wang. Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models. arXiv preprint arXiv 2509.06949, September 2025c. doi: 10.48550/arXiv.2509.06949. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One Single Transformer to Unify Multimodal Understanding and Generation. arXiv preprint arXiv:2408.12528, 2024. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation. Advances in Neural Information Processing Systems 37: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, 2023. Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. MMaDA: Multimodal Large Diffusion Language Models. arXiv preprint arXiv 2505.15809, May 2025. doi: 10.48550/arXiv.2505.15809. Zhiyuan You, Xin Cai, Jinjin Gu, Tianfan Xue, and Chao Dong. Teaching Large Language Models to Regress Accurate Image Quality Scores using Score Distribution. arXiv preprint arXiv 2501.11561, March 2025. doi: 10.48550/arXiv.2501.11561. Siyan Zhao, Devaansh Gupta, Qinqing Zheng, and Aditya Grover. D1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning. arXiv preprint arXiv 2504.12216, June 2025. doi: 10.48550/arXiv.2504.12216. Kaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. Fengqi Zhu, Rongzhen Wang, Shen Nie, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou, Jianfei Chen, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language Diffusion Models. arXiv preprint arXiv 2505.19223, May 2025. doi: 10.48550/arXiv.2505.19223."
        },
        {
            "title": "A DISCUSSING THE APPROXIMATION",
            "content": "A.1 ELBO ESTIMATION The intractable log-likelihood log πθ(yx) in DDMs is often approximated from its evidenced lower bound: Bπ(yx) EtU (0,1)Eytq(yty,t)ℓπ(yt, t, yx) log π(yx). And LLaDA-1.5 provided an estimation with proved low variance as ˆBπ(yx) = 1 ν ν (cid:88) j=1 1 tj (cid:88) k=1 δ(ytj , m) log πθ(ytj , yx) log π(yx), (12) (13) where tj = j/ν is discretization of timeline t, and ytj is sampled as ytj q(ytj ; y, tj). In practice, the time-weighted term is absorbed in to ℓπ as an average on masked tokens, and the simplified form is written as: ˆBπ(yx) = 1 ν ν (cid:88) j=1 ℓπ(ytj , yx), (14) A. IMPORTANCE ESTIMATION We discuss the importance estimation in Eq. 8. Note that we do not intend to establish strict proof for this, but kindly discuss the viable implementation which is shared among current RL works. First, 13 Preprint. let os subscript index temporarily represent its k-th element, we recall the loss expression as ℓπ(ot, oc) (cid:88) k=1 δ(ot k, m) log π(okot, c). (15) Given partially masked completion ot, the above term describes deviation between models prediction π on ots masked positions. We also use ot = ot ot+δt to denote the tokens that are unmasked at timestep t, with δt = ot . For clarity, let = (c, ot+δt), = ot. Using the low-variance estimation in Eq. 14, we discuss Eq. 8 as follows: log π1(yx) log π2(yx) ˆBπ1 (yx) ˆBπ2(yx) = 1 ν ν (cid:88) j=1 (cid:105) (cid:104) ℓπ1 (ytj , yx) ℓπ2(ytj , yx) (16) Considering the small incremental of comparing to x, and the computational resource allocated to the inner-loop ν is limited, we can make further approximation by calculating on step tj = 1, where all tokens in ot are pending: 1 ν ν (cid:88) j=1 (cid:104) (cid:105) ℓπ1(ytj , yx) ℓπ2(ytj , yx) ℓπ1(my, yx) ℓπ2 (my, yx) = ℓπ1( ot, oc, ot+δt) ℓπ2( ot, oc, ot+δt) = ℓπ1( ot + ot+δt, oc) ℓπ2( ot + ot+δt, oc) = ℓπ1(ot, oc) ℓπ2(ot, oc) (17)"
        },
        {
            "title": "B IMPLEMENTATION DETAILS",
            "content": "B.1 REWARD FUNCTION We follow common practices and use reward system for reinforcement learning. For language tasks, we utilize simple composed function of formatting and correctness. For image generation, following recent RL works Geng et al. (2025), the reward is composed with Unified-Reward Wang et al. (2025b), for scoring text-image alignment, HPSv3 Ma et al. (2025b), for assessing the aesthetic quality of the image alongside its alignment, and the classic CLIP Score Hessel et al. (2022). Text generation. For GSM8K and MATH500, the reward consists of two components: Correctness reward: returns 2 for an extracted and correct final answer, and 0 otherwise. Format the reasoning process is properly enclosed in returns 0.5 if <reasoning>*</reasoning>, and 0 otherwise. reward: For MBPP, we adopt DiffuCoders scheme, combining correctness and format rewards. The format reward ensures completions are wrapped in , while the correctness reward tests generated code against predefined test cases. Image generation. For MaskGRPO on image generation tasks, the reward is composed with: UnifiedReward, evaluates imageprompt alignment, divided by 5 to [0, 1]. HPSv3, assess visual quality and textimage alignment, the score is divided by 5 to an approximate range [0, 2]. CLIP Score, measures similarity between encoded text and image features, ranging from [0, 1] (typically 0.20.4). We retain this to stabilize training and mitigate reward hacking. The final image reward is computed as the sum of these three components. 14 Preprint. B.2 EVALUATION For language tasks, we evaluate all tasks with 0-shot prompting. We use block length of 16 and decodes 2 tokens per step for math tasks, and the MBPP protocol is specified in the following paragraph. All performances are reported using the pass@1 metric. For image generation, the sampler decodes an visual sequence of 1024 tokens (which represents an image with resolution 512 512) with 32 steps, and is equipped with classifier-free guidance at 3.5, consistent with the original MMaDA configuration. MBPP Evaluation Protocol. We specify the standardized protocol used for evaluating models on the Mostly Basic Python Problems (MBPP) benchmark Austin et al. (2021). We clarify this protocol to address the significant variance in results reported in the literature, which stems from inconsistent settings for generation parameters (gen_len/step/block_len), different prompt designs, and distinct data subsets (e.g., sanitized-mbpp.json). Such variations impede direct model comparisons. Following the evaluation setup of LLaDA-8B-Instruct, we specify our standard as follows: The test set consists of the first 500 samples (1-500) from the mbpp.jsonl file in the official dataset. The evaluation is conducted in zero-shot setting, using the same prompt format as the lm-eval library. The generation parameters are fixed at gen_len/step/block_len = 256/256/32. Performance is reported using the pass@1 metric, which measures the percentage of test cases passed on the first attempt. B.3 DATA USAGE Language tasks. We use the standard training sets for GSM8K and MATH500. For MBPP, we follow DiffuCoder and use Acecode-87K, an open-source code dataset. Notably, diffu-GRPO reported using KodCodeLight-RL-10K, which refers to multiple traverse over the dataset, given its reported 7500 steps. Besides, as the corresponding implementation is missing from its codebase, we reimplement it and discovered limited effectiveness. We report its best performance on MBPP with our re-implementation on Acecode-87K. Image generation tasks. For general prompt following ability, we follow X-Omni by randomly sampling 90K prompts from midjourney-prompts, dataset of real user instructions, and augment it with 60K compositional GenEval-style prompts from Blip3-o (guaranteed that there is no overlap with the benchmarks). This yields roughly 150K samples, on which we train for one epoch. Additionally, we utilize instruction tuning data from Blip3-o (which is distilled from GPT-4o) for SFT. This procedure is optional, and we have denoted the corresponding results with explicit SFT mark in the tables. As observed, MMaDAs prompt following ability can be improved with such extremely clean supervision signals, and the generated images have more accurate demonstration of spatial relationship, and clearer boundary among objects, compared to soley RL results. We train on this dataset for 1000 steps with global batch size 128, with learning rate of 3e6. B.4 CLARIFICATION We select LLaDA-8B-Instruct for language tasks and MMaDA-8B-Base for image generation tasks. Both models share similar architecture and are initialized from LLaDA-8B-Base. While this choice does not affect our claims on multimodal reinforcement learning, we clarify our rationale: although MMaDA released MixCoT checkpoint, its performance on math and coding tasks is severely limited. With reasoning enabled, it achieves only 48% accuracy on GSM8K (zero-shot prompting), about 30% lower than LLaDA-8B-Instruct with same sampling parameters. We attribute this to potentially insufficient training or suboptimal recipe leading to catastrophic forgetting. Although MaskGRPO applied to MMaDA-8B-MixCoT yields an improvement of over 6%, the results are not comparable since prior works consistently use LLaDA as the baseline. For this reason, we do not pursue language experiments with MMaDA, and look forward to truly unified multimodal DDM foundation without performance compromise. 15 Preprint."
        },
        {
            "title": "C MORE RESULTS",
            "content": "Detailed results on DPG-Bench is shown in Fig 4. We also provide more generated portraits, math reasoning and coding examples as follows. Table 4: Evaluation on DPG-Bench. SFT indicates that we tune the base model with BLIP3-o dataset (distilled from GPT-4o) for clean supervision data before RL training. Model DPG-Bench Global Entity Attribute Relation Other Overall SDXL Podell et al. (2023) Emu3-Gen Wang et al. (2024) MMaDA Yang et al. (2025) w/ MaskGRPO (ours) w/ SFT+MaskGRPO (ours) 83.27 85.21 77.52 80.92 85.96 82.43 86.68 77.52 79.52 84.05 80.91 86.84 77.67 85.41 89.76 86.76 90.22 81.56 83.05 83.43 80.41 83.15 63.13 69.12 79.88 74.65 80.60 70.51 75.81 81. Figure 7: More generated portrait samples with identical prompt and sampling parameters (CFG=3.5, 64 steps). Above: sampled by default MaskGIT-style sampler before RL training. Below: sampled by our emerge sampler after RL training. 16 Preprint. Generation Sequence Length: 256 Question: There are 29 pupils in class. The teacher has 9 coupons; each coupon can be redeemed for 100 bottles of apple juice. The teacher gives each student 2 bottles of apple juice to drink for lunch. After redeeming all her coupons and giving each student their apple juice lunch how many bottles of apple juice does the teacher have for herself? <reasoning> First, we need to determine the total number of bottles of apple juice the teacher can redeem with her coupons. Each coupon can be redeemed for 100 bottles, and the teacher has 9 coupons. Therefore, the total number of bottles the teacher can is: 9 100 = Next, we need to calculate the total number of bottles of apple juice given to the students. There are 29 pupils in the class, and each pupil receives 2 bottles. Therefore, the total number of bottles given to the students is: 29 2 = 58 Now, we subtract the number of bottles given to the students from the total number of bottles the teacher can redeem to find out how many bottles the teacher has for herself: Thus, the teacher has 842 bottles of apple juice for herself. < /reasoning> 842 900 58 = 842 Generation Sequence Length: 512 Question: watermelon farm produced 120 baby watermelons. Ashlyn went to the farm after two months and found out that 30% of the watermelons were ready for harvest so she took them home. When she came back two weeks later 3/4 of the remaining melons were ready so she harvested them. How many melons were not ready to be harvested after the two weeks? <reasoning> To determine how many watermelons were not ready to be harvested after two weeks, we need to follow these steps: 1. Calculate the number of watermelons ready for harvest after two months. 2. Subtract the number of watermelons harvested after two months from the total number of watermelons to find the remaining watermelons. 3. Calculate the number of watermelons ready for harvest two weeks later. 4. Subtract the number of watermelons harvested two weeks later from the remaining watermelons to find the number of watermelons not ready to be harvested. First, lets calculate the number of watermelons ready for harvest after two months: 30% of 120 = 0.30 120 = 36 So, 36 watermelons were ready for harvest after two months. Next, we subtract the number of watermelons harvested after two months from the total number of watermelons to find the remaining watermelons: 120 36 = 84 So, there are 84 watermelons remaining after two months. Now, lets determine how many watermelons were ready for harvest two weeks later: 3 of 84 = 3 4 84 = 63 So, 63 watermelons were ready for harvest two weeks later. Finally, we subtract the number of watermelons harvested two weeks later from the remaining watermelons to find the number of watermelons not ready to be harvested: So, 21 watermelons were not ready to be harvested after two weeks. < /reasoning> 21 84 63 = 17 Preprint. Generation Sequence Length: 256 Question: You are an expert Python programmer and here is your task: Write function to find the vertex of parabola. Response: To find the vertex of parabola given by the equation = ax2 + bx + you can use the following formulas: 1. The x-coordinate of the vertex is given by 2. The y-coordinate of the vertex is given by = 2a = b2 4a Heres Python function that implements these formulas: def parabola_vertex(a, b, c): x_vertex = -b / (2 * a) y_vertex = - (b**2) / (4 * a) return (x_vertex, y_vertex) Test cases: \"assert parabola_vertex(5,3,2)==(-0.3, 1.55)\", rue \"assert parabola_vertex(9,8,4)==(-0.4444444444444444, 2.2222222222222223)\", rue \"assert parabola_vertex(2,4,6)==(-1.0, 4.0)\", rue"
        }
    ],
    "affiliations": [
        "University of Chinese Academy of Sciences"
    ]
}