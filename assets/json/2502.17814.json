{
    "paper_title": "An Overview of Large Language Models for Statisticians",
    "authors": [
        "Wenlong Ji",
        "Weizhe Yuan",
        "Emily Getzen",
        "Kyunghyun Cho",
        "Michael I. Jordan",
        "Song Mei",
        "Jason E Weston",
        "Weijie J. Su",
        "Jing Xu",
        "Linjun Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have emerged as transformative tools in artificial intelligence (AI), exhibiting remarkable capabilities across diverse tasks such as text generation, reasoning, and decision-making. While their success has primarily been driven by advances in computational power and deep learning architectures, emerging problems -- in areas such as uncertainty quantification, decision-making, causal inference, and distribution shift -- require a deeper engagement with the field of statistics. This paper explores potential areas where statisticians can make important contributions to the development of LLMs, particularly those that aim to engender trustworthiness and transparency for human users. Thus, we focus on issues such as uncertainty quantification, interpretability, fairness, privacy, watermarking and model adaptation. We also consider possible roles for LLMs in statistical analysis. By bridging AI and statistics, we aim to foster a deeper collaboration that advances both the theoretical foundations and practical applications of LLMs, ultimately shaping their role in addressing complex societal challenges."
        },
        {
            "title": "Start",
            "content": "Wenlong Ji1, Weizhe Yuan2,3, Emily Getzen4 Kyunghyun Cho2 Michael I. Jordan5,6 Weijie J. Su4, Jing Xu3, Song Mei Jason Weston2,3 Linjun Zhang7, 5 2 0 2 5 2 ] . s [ 1 4 1 8 7 1 . 2 0 5 2 : r 1Stanford University 2New York University 3Meta FAIR 4University of Pennsylvania 5UC Berkeley 6INRIA 7Rutgers University Abstract Large Language Models (LLMs) have emerged as transformative tools in artificial intelligence (AI), exhibiting remarkable capabilities across diverse tasks such as text generation, reasoning, and decision-making. While their success has primarily been driven by advances in computational power and deep learning architectures, emerging problemsin areas such as uncertainty quantification, decision-making, causal inference, and distribution shiftrequire deeper engagement with the field of statistics. This paper explores potential areas where statisticians can make important contributions to the development of LLMs, particularly those that aim to engender trustworthiness and transparency for human users. Thus, we focus on issues such as uncertainty quantification, interpretability, fairness, privacy, watermarking and model adaptation. We also consider possible roles for LLMs in statistical analysis. By bridging AI and statistics, we aim to foster deeper collaboration that advances both the theoretical foundations and practical applications of LLMs, ultimately shaping their role in addressing complex societal challenges."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Background and Fundamentals of LLMs 2.1 Historical Development of LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1.1 Representation Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1.2 Language Modeling 2.2 Architectures of Pre-trained Language Models . . . . . . . . . . . . . . . . . . . . . . . . 2.3 LLM Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 4 4 4 5 5 3 Training Pipelines of LLMs Scaling Laws 8 8 3.1 LLM Pre-training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 3.1.1 Pre-training Objective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1.2 Pre-training Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 3.1.3 3.2 LLM Prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.3 LLM Supervised Fine-Tuning (SFT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 Instruction Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.3.1 3.3.2 Parameter-Efficient Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.4 System 2 Prompting and Chain-of-Thought . . . . . . . . . . . . . . . . . . . . . . . . . 14 3.5 LLM Reinforcement Learning & Preference Optimization . . . . . . . . . . . . . . . . . 15 3.5.1 Approaches to Aligning LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 3.6 LLM Self-Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 Synthetic Data Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 3.6.1 Co-first authors Co-corresponding authors 3.6.2 3.6.3 Challenges Self-Rewarding Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "4 Designing Trustworthy LLMs by Statistical Methods",
            "content": "18 4.1 Uncertainty Quantification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 4.2 LLM Watermarking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 4.3 Privacy and Copyright . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 4.4 Interpretability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 4.5 Algorithmic Fairness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 4.6 LLM Alignment: Statistical Perspective . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "5 LLM-Empowered Statistical Analysis",
            "content": "29 5.1 Data Collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 5.1.1 Extract Structured Data from Unstructured text . . . . . . . . . . . . . . . . . . 29 Synthetic Data Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 5.1.2 5.2 Data Cleaning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 5.3 LLMs for Data Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 5.4 LLM-assisted Medical Research . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 6 Discussion 33 6.1 Statisticians Small Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 6.2 Towards Understanding LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 6.3 Human-AI Collaborative Data Science . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 Historical Development of LLMs 62 A.1 Representation Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 A.2 Language Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 Key Resources: Accelerating Progress in LLM Research 63 B.1 Core Frameworks for Model Development . . . . . . . . . . . . . . . . . . . . . . . . . . 63 B.2 Model Libraries and Pre-trained Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 B.3 Datasets and Data Preparation Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 B.4 Training and Fine-Tuning Utilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 B.5 Inference and Deployment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 B.6 Visualization and Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 B.7 Cloud and Hardware Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "1 Introduction",
            "content": "Large Language Models (LLMs) have emerged as transformative tools in artificial intelligence (AI), exhibiting remarkable capabilities across diverse tasks from text generation to dialog to complex reasoning. As these models increase in scale and sophistication, an important question arises: in the era of LLMs, how can statisticians play role in guiding the design and deployment of large-scale AI models? Figure 1: An illustration of next-token prediction by LLMs. At their core, language models are models that assign probabilities to upcoming words, or sequences of words. Specifically, language model aims to learn mapping that takes sequence of tokens x[T ] = x1, x2, , xT as input, and outputs probability vector (x[T ]) over the token vocabulary for the next-token xT +1. While simple in formulation, when applied to vast corpora of text, this 2 probabilistic approach allows LLMs to capture intricate patterns, including those that were classically the province of syntax, semantics, and logic. For example, given an instruction such as Write an engaging short story or Lets think step by step, LLMs can dynamically adapt their responses to follow the given directive, generating outputs that align with the users intent. This instructionfollowing capability is result of extensive training on diverse datasets combined with fine-tuning techniques that align model outputs with user preferences. While much of this work takes inspiration from statistical principles, as well as principles from optimization theory, there is large gap between problem formulations and the design of statistical procedures that target the specific architectures and data-collection protocols used in LLMs."
        },
        {
            "title": "Example of a LLM response to a user instruction on Creative Writing",
            "content": "User: Write an engaging short story about group of statisticians going hiking. Provide detailed background for the statisticians and describe the fun and challenges they face during the journey. Incorporate rich details to bring the story to life. LLM: Title: The Law of Large Trails It all started as joke in the statistics department. Every Wednesday, after the weekly seminar, group of faculty and graduate students would gather in the common lounge, sharing coffee, complaints, and the occasional proof sketch scribbled on napkins. One particularly dull winter afternoon, Professor Strawderman sighed and said, You know, we spend all our time analyzing uncertainty in data, but when was the last time we faced real uncertainty? Like, say... in nature?... Example of LLM response to probability question with Chain-of-Thought Prompting User: You are an expert in solving advanced probability theory problems. Carefully solve the following step by step, ensuring that every calculation and assumption is explained: What is the probability of getting at least one 6 when rolling two fair six-sided dice? Lets think step by step. LLM: For each die, there are 6 possible outcomes. The total number of possible outcomes when rolling two dice is: 6 6 = 36. We want the probability of getting at least one 6. This includes all cases where one or both dice show 6. Its easier to calculate the complement: the probability of not getting 6 on either die. Probability of not getting 6 on single die: For one die, there are 5 outcomes where we do not get 6 (i.e., 1, 2, 3, 4, or 5). The probability of not getting 6 on one die is: P(not 6 on one die) = 5 6 . Probability of not getting 6 on both dice: Since the dice rolls are independent, the probability of not getting 6 on both dice is: P(not 6 on both die) = 5 6 5 6 = 25 36 . Probability of getting at least one 6 is the complement of the probability of not getting any 6s: P(at least one 6) = 1 P(not 6 on both die) = 1 25 36 = 11 36 . This requires more than simply applying existing statistcal toolsthe rise of LLMs demands advances in statistical methodology and theory to ensure trustworthiness, reliably quantify uncertainty, and enhance interpretabilityall at the massive scale of LLM deployments. For example, there are many open problems in understanding how the architectural design of LLMs yields, or fails to yield, calibrated probabilistic outputs or understanding how the outputs of LLMs interact with downstream decision-making to ensure algorithmic fairness and reliability. One can also view statistics itself as an application domain for LLMs. Indeed, LLMs provide significant opportunities to augment traditional statistical workflows. Tasks like data collection, cleaning, and data analysis can leverage LLMs contextual understanding and scalability to improve accuracy and efficiency. In fields such as medical research and biostatistics, LLMs can synthesize representative datasets, extract valuable insights from unstructured clinical notes, and support predictive modeling 3 in high-stakes applications. Of course, such applications again require closing the gap between the ad hoc nature of current LLM design and the statistical properties of its outputs. These dual perspectives, statistics for LLMs and LLMs for statistics, highlight the urgent need for statisticians to engage deeply with the LLM revolution. This engagement will be challenging. The rapid pace of innovation in LLMs raises pressing questions: How can statistical frameworks adapt to the unprecedented scale and complexity of these models? What role should statistical principles play in ensuring the trustworthy and reliable deployment of LLMs in diverse applications? How can statisticians incorporate LLMs into their workflows without compromising the rigor, transparency, and interpretability that define their discipline? Addressing these questions is critical to bridging the gap between statistical rigor and the transformative potential of LLMs. This survey aims to catalyze the involvement of statisticians in the evolving landscape of LLMs by providing structured overview of the field. It begins by introducing the historical development and foundational principles of LLMs, shedding light on their architectures and training pipelines. It also explores how statistical methods can enhance the trustworthiness and alignment of LLMs, focusing on topics such as conformal prediction, synthetic text detection, and algorithmic fairness. The discussion then shifts to how LLMs can empower statistical analysis in areas like data synthesis, cleaning, and medical research. By bridging the fields of statistics and LLMs, this survey underscores the mutual benefits of collaboration. Statisticians can enrich the development of LLMs with rigor and transparency, while LLMs offer powerful tools to push the boundaries of statistical practice. This interplay promises to redefine both fields, presenting statisticians with unique opportunity to shape the trajectory of AI in promising directions. The paper is organized as follows. In Section 2, we provide background on the fundamentals of LLMs. Section 3 delves into the training pipelines of LLMs, including pre-training, prompting, finetuning, and alignment techniques. Section 4 focuses on designing trustworthy LLMs leveraging statistical insights, covering topics like interpretability, uncertainty quantification, watermarking, privacy, and algorithmic fairness. Section 5 explores the potential of LLMs to empower statistical analysis, with applications to medical research. Finally, Section 6 concludes the paper with discussion on statisticians role in the LLM landscape, highlighting the importance of collaboration and offering suggestions for future research directions. Additional resources are provided in the appendicies, including frameworks for model development, libraries, datasets, training tools, and visualization utilities that can accelerate progress in LLM research."
        },
        {
            "title": "2 Background and Fundamentals of LLMs",
            "content": "The development of LLMs has been landmark event in the field of natural language processing, representing quantum leap in the ability of machines to understand human language. This section provides comprehensive overview of the historical development of LLMs, beginning with foundational concepts in representation learning which illustrate how models encode language into numerical forms. We then transition into advances in language modeling, involves predicting the next word in sequence of texts, allowing for coherent and fluent text generation. Finally, we explore various architectures that have been pivotal in shaping the current landscape of LLMs."
        },
        {
            "title": "2.1 Historical Development of LLMs",
            "content": "2.1.1 Representation Learning The first step in an LLM is to transform natural language into format that computers can understand specifically vectors, matrices, and tensors. The symbolic lexical units (i.e., words) that comprise sentences require effective numerical representation strategies, known as word embeddings. Grounded on the distributional hypothesis, which posits that linguistic items that occur in similar contexts have similar meanings [Har54], such methods have gradually become more sophisticated over time. Early attempts at capturing word meanings involved bag-of-words representations, which represent documents as sparse vectors as in TF-IDF [Spa72], with dimension for each element of the dictionary. Many unsupervised [Hof01; LD02] and supervised methods [Joa98; Bai+09; WBU11] involved learning (dense) word embeddings, but struggled with complex semantic and syntactic nuances of words in challenging NLP tasks. Neural methods first grew to prominence with the advent of Word2vec [Mik+13] 4 and GloVe [PSM14], that directly learn low-dimensional word representations from local word context, with each dimension encodes latent feature of the word. The specific objective functions underlying the architecture of these models were explored in foundational works such as those by [Ben+03; CW08; Col+11; TRB10], setting the stage for subsequent innovations. These approaches treat words as the primary unit, leading to challenges like large vocabulary size and out-of-vocabulary words. Tokenization, such as Byte Pair Encoding (BPE) [Gag94], mitigates these issues by merging frequent byte or character pairs. Thus, The evolution of representation learning, language modeling, and tokenization techniques has become unified and interdependent process."
        },
        {
            "title": "2.1.2 Language Modeling",
            "content": "Early stages of language models (LMs), such as n-gram models [Jel98; GL04; Ros00], were mostly statistical regression models, relying on the Markov assumption to predict the next word from the most recent context. Challenges for these models included the need to tackle data sparsity [CG96] and the exponentially growing number of transition probabilities as increases. Neural language models (NLMs) [Ben+03; Mik+10; SDG06] tackle data sparsity by mapping words to low-dimensional vectors and predicting subsequent words using neural networks. One of the earliest and most influential neural language models is based on Recurrent Neural Network (RNN) architecture, which was first introduced by [Elm90; Jor86] and later popularized by [Mik+10]. RNNs are particularly well-suited for modeling sequential data, such as text, but can sometimes struggled with vanishing gradients and capturing long-term dependencies in the input sequence. To address these limitations, variants of RNNs were introduced, including Long Short-Term Memory (LSTM) networks [HS97] and Gated Recurrent Units (GRUs) [Chu+14]. major advance came with the introduction of the attention mechanism [BCB15], allowing the model to focus on specific parts of the input sequence which can be large number of positions away when generating each output token. More detail on the attention mechanism can be found in 2.2. This innovation quickly led to number of developments including stacking network layers of attention [SWF+15] and use of position embeddings [CW08; SWF+15] which showed strong performance without the use of position recurrence, culminating in the introduction of the Transformer architecture [Vas+17]. The Transformer, introduced by [Vas+17], revolutionized NLP by enabling deeper and more efficient language model training. Unlike LSTMs, Transformers capture global dependencies between input and output regardless of distance while crucially allowing for significantly more parallelization, enhancing their scalability. This led to the ability to train Transformer-based pre-trained models on large amounts of data, as exhibited by GPT [Rad+18] BERT [Dev+19], XLNET [Yan+19], RoBERTa [Liu+19] and T5 [Raf+20]. The results were striking. These models learn general representations of language through pre-training on large text corpora with language modeling objectives, allowing for effective fine-tuning on specific NLP tasks. Most LLMs today are built on the Transformer architecture, which has led to steady improvement in performance on downstream tasks by scaling both the number of model parameters and the volume of training data [Hof+22]. There are numerous popular families of LLMs available, such as the LLama [Tou+23b; Tou+23a; Dub+24a], Mistral [Jia+23; Jia+24b], GPT [Bro+20; Ope+24a], Claude families1, and DeepSeek [Bi+24; Liu+24a; Liu+24b], each offering models of varying sizes. As illustrated in Table 1, LLMs can be categorized based on their hosting requirementsfrom small models that can run on laptop, to medium-sized models that require server cluster, and large proprietary models accessible via API. Whether one requires lightweight model for personal use or powerful model for enterprise-level tasks, there are currently LLM solutions available. Statisticians interested in research on actual LLMs can begin by leveraging resources and tools that lower the barrier to entry while addressing the computational challenges associated with these models. Please refer to Appendix for more details."
        },
        {
            "title": "2.2 Architectures of Pre-trained Language Models",
            "content": "As we have discussed, the evolution of neural language model architectures progressed from sequencebased models such as convolutional [Kim14] and recurrent [HS97] models to the more advanced Transformer models [Vas+17]. Further success came from Pre-trained Language Models (PTMs), which 1https://www.anthropic.com/news/claude-3-family 5 Figure 3: The architecture of decoder-only Transformer. The model consists of stacked Transformer decoder layers. Each layer contains feed-forward network with weight matrices W1 and W2, as well as multi-head attention module comprising the output matrix (Om), value matrix (Vm), key matrix (Km), and query matrix (Qm). integrate self-supervised learning and the Transformer model. Self-supervised learning allows the model to learn from the data without explicit labels or supervision, while the Transformer model uses self-attention mechanisms to capture long-range dependencies and contextual relationships in the input data. Below we will introduce different components of the Transformer architecture. Attention Mechanism The attention mechanism was first introduced to tackle the limitations of the RNN encoder-decoder model, which struggled with long sentences due to its fixed-length vector representation of the source information [BCB15]. This mechanism enhances the model by jointly learning to align and translate. It identifies key positions in the source sentence for each target word prediction adaptively, considering both these specific context vectors and all previously generated target words. This approach effectively handles longer sentences, improving the models overall performance. The Transformer model [Vas+17], built entirely on attention mechanisms, further leverages this approach for superior results. In more detail, an attention function maps query and set of key-value pairs {(k1, v1)}, , {(kn, vn)} to an output o. The output is weighted sum of the values as calculated by = α1v1 + + αnvn, with the weight αi for each value vi determined by compatibility function that matches the query with its corresponding key ki. Transformer Overview The basic Transformer model operates on sequence-to-sequence basis, incorporating both an encoder and decoder, each constructed from series of identical blocks [Vas+17]. The encoder maps an input sequence = (x1, , xn) to sequence of continuous representations = (z1, , zn). Given z, the decoder then generates an output sequence = (y1, , ym). The primary components of each encoder block are multi-head attention (MHA) module and position-wise feed-forward network (FFN). To facilitate the construction of more complex model, residual connection [He+16] is utilized around each block, succeeded by Layer Normalization [BKH16] module. Decoder blocks, in contrast to encoder blocks, incorporate additional cross-attention modules between the multi-head self-attention modules and the position-wise FFNs. Moreover, the self-attention modules within the decoder are modified to inhibit each position from attending to positions that follow 6 it. Since the Transformer model does not inherently encode sequential order, positional encodings are introduced to provide the model with information about the relative or absolute positions of tokens in the input sequence. These positional encodings are added to the input embeddings at the bottom of both the encoder and decoder stacks. The original Transformer employs sinusoidal positional encodings, where each position is represented by combination of sine and cosine functions of varying frequencies. This choice allows the model to extrapolate to sequences longer than those seen during training. Formally, the positional encoding for position pos and dimension is defined as: Epos,2i = sin (cid:16) pos 100002i/dmodel (cid:17) , Epos,2i+1 = cos (cid:16) pos 100002i/dmodel (cid:17) , where dmodel represents the dimension of the input embeddings. For more detail regarding each individual module, including the incorporation of encodings, please refer to the original Transformer paper [Vas+17]. In addition to sinusoidal encodings, several advanced positional encoding methods have been proposed to enhance model performance and flexibility. Rotary Positional Embeddings (RoPE) [Su+24] is one such method, designed to incorporate relative position information directly into the attention mechanism. RoPE applies rotation matrix to the embeddings, enabling the model to better capture the relationship between tokens at different positions. Building on the idea of expanding positional encoding capabilities, the recently proposed Contextual Position Encoding (CoPE) [Gol+24] addresses different limitation. Traditional methods, including RoPE, rely on token counts to derive positions, which restricts their ability to generalize to higher levels of abstraction, such as attending to the i-th sentence. CoPE overcomes this challenge by conditioning positions on context, incrementing positions only for specific tokens determined by the model. This flexibility enables CoPE to handle complex tasks like selective copy, counting, and Flip-Flop, while also improving perplexity in language modeling and coding tasks. Multi-head Attention crucial component within the Transformers encoder and decoder layers is the multi-head attention module which computes representations of the input and output without using recurrence or convolution calculations. The attention function used in [Vas+17] is scaled dotproduct function. The input to this function consists of queries and keys of dimension dk, and values of dimension dv. These queries, keys, and values are obtained by applying linear transformation to the representations output from the previous encoder/decoder layer. The output calculation of the attention function is shown in Equation 1. Attention(Q, K, ) = Softmax (cid:16) QK dk (cid:17) (1) Empirically, it has been found beneficial to linearly project the queries, keys, and values times with different, learned linear projections to dk, dk, and dv dimensions, respectively. The attention function is then performed in parallel on each of these projected queries, keys, and values, yielding dv-dimensional output values. These are concatenated and once again projected, resulting in the final values. The output calculation of the multi-head attention is shown in Equation 2, where multiple projections parameterized by matrices , are applied. Multi-head attention allows , for the capture of more nuanced syntactic and semantic information since the model can attend to information from different subspaces at different positions. ,W MultiHead(Q, K, ) = Concat(head1, , headh)W , where headi = Attention(QW i , KW ) (2) (3) Variants The Transformer architecture is versatile and highly extensible, allowing for the seamless integration of new modules or the implementation of novel attention mechanisms. The application of the Transformer architecture can take three distinct forms, depending upon the utilization of the encoder and decoder components. Encoder-Decoder Transformers The full Transformer architecture, which incorporates both the encoder and decoder components, is typically employed for sequence-to-sequence modeling tasks, such as machine translation and text summarization. Pre-trained models that utilize this 7 full Transformer architecture, such as T5 [Raf+20] and BART [Lew+20], are commonly applied in these contexts. Encoder-only Transformers This involves the exclusive use of the encoder component within the Transformer architecture. The encoder typically functions as feature extractor, with its outputs serving as representation of the input sequence. This method is commonly employed for tasks such as text classification or sequence labeling, which can be viewed as token classification problems. Typical pre-trained encoder models include BERT [Dev+19] and RoBERTa [Liu+19]. Decoder-only Transformers This involves solely utilizing the decoder component within the Transformer architecture, with the cross-attention module between the encoder and decoder being omitted, as shown in Figure 3. It is typically employed for sequence generation tasks, such as language modeling. Recently, the unification of task formats into language modeling and the scalability of decoder-only Transformer training have led to the rise of instruction tuning. This method, which involves fine-tuning language models on collection of tasks described via instructions, significantly enhances zero-shot performance on unseen tasks [Wei+22b]. Consequently, most current large language models are based on the decoder-only Transformer architecture. Notable examples include the GPT series [Rad+19; Bro+20; Ope+24a], Llama series [Tou+23b; Tou+23a], and Mistral series [Jia+23], among others."
        },
        {
            "title": "2.3 LLM Evaluation",
            "content": "As the number of LLMs continues to grow, the challenge of effectively comparing their performance becomes increasingly important. Traditional benchmarks, such as ARC [Cla+18], MMLU [Hen+21a], and MATH [Hen+21b], while valuable for evaluating specific capabilities like multi-task understanding, mathematical reasoning, and commonsense knowledge, rely on fixed datasets with predefined gold answers. This rigid structure limits their ability to assess models adaptability and creativity in generating diverse, contextually appropriate responses. Moreover, as models become more advanced, they tend to saturate these benchmarks, reducing their discriminative power and making it harder to distinguish between state-of-the-art systems. Chatbot Arena [Chi+24] addresses these limitations by providing benchmark platform for evaluating LLMs through anonymous, randomized battles in crowdsourced manner. Utilizing the Elo rating system [MVM09], which is widely used in competitive games such as chess, Chatbot Arena allows users to compare models by engaging with them side-by-side and voting on which one performs better. This approach not only facilitates scalable and incremental evaluation but also provides unique ranking order for the models. By inviting the community to contribute new models and participate in the evaluation process, Chatbot Arena helps maintain an up-to-date leaderboard, offering valuable insights into the relative strengths of various LLMs. This platform exemplifies practical solution for benchmarking LLMs in open-ended tasks, where traditional programmatic evaluation methods with fixed gold answers fall short."
        },
        {
            "title": "3.1 LLM Pre-training",
            "content": "Between 2017 and 2019, paradigm shift occurred in the learning of NLP models. The traditional fully supervised learning paradigm began to be replaced by two-step process: pre-training and finetuning [Pet+18; Rad+18; Dev+19]. In this new paradigm, model with fixed architecture is first pre-trained as language model using unlabeled web data through self-supervised learning. This pre-trained language model is then adapted to various downstream tasks by introducing additional parameters and fine-tuning these parameters using task-specific objective functions. The pre-training and fine-tuning paradigm offers several advantages. Firstly, pre-training on large text corpus enables the model to learn universal language representations, which can be beneficial for many downstream tasks. Secondly, pre-training provides superior model initialization, which often results in better generalization performance and faster convergence on the target task. Lastly, pre-training can serve as form of regularization, helping to prevent overfitting, particularly when dealing with small datasets [Erh+10]."
        },
        {
            "title": "3.1.1 Pre-training Objective",
            "content": "The choice of pre-training tasks plays pivotal role in learning universal language representations. These tasks should ideally be challenging and have substantial training data. In this section, we provide brief overview of one of the most widely used pre-training tasks. Standard Language Modeling. Standard language modeling objectives focus on training the model to learn the probability (x) of texts from training corpus [Rad+19]. Typically, text prediction occurs in an autoregressive manner, predicting the tokens in the sequence one at time, often from left-to-right, although other orders are possible as well. Formally, language model is parameterized by set of parameters θ and learns parameterized mapping from the context x<t to the next-token xt. The goal of the model is to predict the next-token in the sequence given the context, and this is achieved by minimizing the conditional probability Pθ(xtx<t). The loss function of standard language modeling objectives over sequence of tokens = x1, x2, , xT is LSLM = (cid:80)T t=1 log Pθ(xtx<t). Due to their simplicity, efficiency, scalability, and proven performance on wide range of tasks, language modeling objectives have become the preferred choice for pre-training LLMs. In addition to the standard language modeling objective, other primary training objectives such as corrupted text reconstruction [Raf+20] and full text reconstruction [Lew+20] are also widely utilized. Beyond these primary objectives, auxiliary objectives have been developed to enhance the models performance on specific downstream tasks. Common auxiliary objectives include next sentence prediction [Dev+19], sentence order prediction [Lan+20], discourse relation prediction [Sun+20], and token-passage prediction [Liu+20b]. These auxiliary objectives are strategically employed to provide additional training signals that help refine the models understanding and generation capabilities, thereby improving its applicability and effectiveness across variety of NLP tasks. 3.1.2 Pre-training Data The choice of pre-training datasets also plays crucial role in the development and capabilities of LLM models. These datasets, often composed of vast amounts of text data from various sources, serve as the foundation on which LLMs learn the intricacies of language. By training on such massive corpora, LLMs acquire general language understanding and the ability to generate coherent text. Data Source Pre-training data can be broadly categorized into two types: general pre-training corpora and domain-specific pre-training corpora. General pre-training corpora are broad datasets, covering categories such as webpages, language texts, books, academic materials, code, parallel corpora, social media, and encyclopedia [Liu+24f]. Webpages are major source, offering extensive multilingual content that often requires significant cleaning, as seen in derivatives like RefinedWeb [Pen+24] from Common Crawl2. Language texts are sourced from large corpora like the American and British National Corpora,34 often focusing on specific languages or domains like finance. Books provide highquality, lengthy texts from sources such as Project Gutenberg5 improve models understanding of complex language. Academic Materials, such as those in arXiv,6 contribute specialized scholarly content. Code data from repositories like The Stack [Koc+23] and Github7 is essential for programming tasks. Parallel corpora data, involving bilingual text pairs from resources like ParaCrawl [Ban+20] is crucial for translation tasks. Social Media data from platforms such as StackExchange8 and Reddit9 helps models learn conversational dynamics, while Encyclopedia data, particularly from Wikipedia,10 strengthens models general knowledge. Interestingly, there are some unexpected phenomena relating to how certain corpora can enhance the abilities of LLMs. For example, code data not only is essential for programming tasks, but also significantly enhances non-code performance when included in pre-training. Specifically, [Ary+24] found that initializing models with code pre-trained data led to 2https://commoncrawl.org/ 3https://anc.org/ 4http://www.natcorp.ox.ac.uk/ 5https://www.gutenberg.org/ 6https://arxiv.org/ 7https://github.com/ 8https://stackexchange.com/ 9https://www.reddit.com/ 10https://www.wikipedia.org/ 9 relative increase of 8.2% in natural language reasoning and 12 boost in code performance. Moreover, they also found that using high-quality, synthetically generated code and code-adjacent datasets like GitHub commits during pre-training or cooldown phases could result in substantial improvements across various tasks including reasoning and world knowledge tasks. Domain-specific pre-training corpora fine-tune models for specialized fields. For instance, financial datasets like BBT-FinCorpus [Lu+23] and medical corpora like Medical-pt [Xu23] provide targeted content that enhances model performance in finance, law, and other areas. These datasets ensure that models are better equipped for specific applications. Data Selection and Mixture Data selection and mixture in LLM training is essential for optimizing model performance. This process is generally divided into three levels: token-level, samplelevel, and group-level selection. Token-level selection focuses on filtering individual tokens within the data, offering precise control but requiring significant resources [Lin+24b]. Sample-level selection involves choosing specific training examples, commonly used in fine-tuning. While heuristic methods are typical [Rae+21; Sha+24; Sol+24], advanced approaches use optimization algorithms [Che+24b; Min+22b], model perplexity [Mue+23], or even LLMs to guide selection [Wet+24; Sac+24], aiming to enhance the quality of the training data. Group-level selection organizes data into pools, which are then optimally mixed through mixture techniques. Early methods relied on manual mixtures [Gao+20; Bro+20], but more recent approaches use learned mixtures, either through fixed weights determined by proxy models (offline selection) [Rae+21; Xie+23] or dynamically adjusted weights during training (online selection) [Che+23a]. 3.1.3 Scaling Laws With the introduction of increasingly large language models [Bro+20; Rae+21; Smi+22], understanding computational efficiency has become crucial. The compute and energy costs for training these models are substantial [Rae+21; Tho+22], escalating with model size. In practical scenarios, the training compute budget is often predetermined, depending on the availability of accelerators and the duration of their use. Given that training large models is usually feasible only once, accurately estimating optimal hyperparameters for given compute budget is essential. Kaplan et al. [Kap+20] provided initial insights into computational efficiency for language models, discovering power law relationship between the number of parameters in an autoregressive LM and its performance. They suggested that with tenfold increase in computational budget, model size should increase by 5.5X, while the number of training tokens should only increase by 1.8X. This led to trend of training larger models to achieve performance gains. Scaling laws have been shown to apply across various data modalities, including language, images [Che+20], and videos [WTU20], as well as multimodal modeling [Tsa+19] and even mathematical problem solving [Sax+19]. Beyond model size and training tokens, [Tay+23] derived scaling laws for different inductive biases and model architectures, revealing significant variations in scaling coefficients among models. They found that among ten architectures, the vanilla Transformer exhibited the best scaling behavior, despite not having the highest absolute performance at each compute region. In 2022, the Chinchilla scaling law [Hof+22] shifted the focus from model size to the number of training tokens for computational efficiency. This law suggests that training slightly smaller models on larger datasets is often more efficient than the previous approach [Kap+20], which favored larger models on smaller datasets. Compared to Kaplans study, key differences introduced by the Chinchilla study include: (1) different learning rate schedule for all models, unlike [Kap+20], which did not account for the impact of these hyperparameters on the loss; and (2) the inclusion of larger scale models. The Chinchilla model, with 70B parameters trained on 1.4T tokens (approximately 20 tokens per parameter), outperformed its much larger counterpart, Gopher [Rae+21]. The Chinchilla efficient model size and training dataset size, along with the achievable test loss, can be determined as follows: Nopt(C) = 0.6 0.45 Dopt(C) = 0.3 0.55 Lopt(C) = 1070 0.154 + 1.7, (4) where Nopt represents the optimal number of model parameters, Dopt denotes the optimal number of training tokens, and Lopt indicates the optimal final pre-training loss achievable under fixed FLOPs 10 compute budget C. Despite the trend towards larger models, the potential of training smaller models with larger datasets remains relatively underexplored. Instead of focusing solely on training computeoptimal language models, [Tou+23b] emphasized the importance of the inference budget. Inferenceoptimal language models aim for optimal performance within specific inference constraints, achieved by training small models with more tokens than recommended by traditional scaling laws[Hof+22]. [Tou+23b] demonstrated that smaller models trained with more data could match or even surpass the performance of larger models. Additionally, [Tyl23] argued that existing scaling laws [Hof+22] might not accurately predict scenarios where smaller models are trained for extended periods. Consequently, efforts have been made to push the limits of scaling laws by training language models to achieve optimal performance across various inference budgets by using more tokens than typically prescribed by the Chinchilla scaling law. Notable examples include the Llama series [Tou+23b], TinyLlama [Zha+24c], Llama2 series [Tou+23a], and Llama3 [Gra+24], which trained an 8B model on 15T tokens, far exceeding the optimal number of training tokens predicted by the Chinchilla law (approximately 200B tokens). They found that model performance continued to improve log-linearly even after training the 8B and 70B models on up to 15T tokens. After pre-training on massive corpora using self-supervised objectives, the parameters of the LLM are well suited as an initialization for various downstream tasks. The pre-training provides the model with broad understanding of language, which can then be fine-tuned for specific applications. Beyond pre-training strategies, understanding scaling laws plays crucial role in optimizing model training and deployment. For example, [JJS24] highlight how multi-objective considerations (e.g., safety and accuracy) can influence computational efficiency, demonstrating that new entrants to competitive markets can achieve safety alignment with fewer data points due to differing reputational dynamics. Their findings that scaling rates slow with increasing dataset size suggest new strategies for allocating compute resources effectively, particularly in multi-objective settings. These insights, combined with advances in pre-training and fine-tuning, underscore the potential to refine model training workflows and push the boundaries of computational efficiency across diverse applications."
        },
        {
            "title": "3.2 LLM Prompting",
            "content": "For the largest language models, the paradigm has shifted for typical users from traditional supervised learning to prompt-based learning, often called prompt engineering [Liu+23]. In supervised learning, we use labeled data consisting of input-output pairs = (xi, yi)i=1N to tune the model parameter θ so that we can predict the output for an input that is not in the training data using Pθ(yx). However, in prompt engineering, the model parameter θ is fixed, and instead, one tunes template which is combined with to form new input that achieves good performance when using Pθ(yx). In other words, prompt engineering learning involves optimizing template that is used to generate new input from the original input x, rather than optimizing the model parameters θ directly. This approach allows us to adapt the model to new inputs without requiring retraining of the model. We describe two main prompt-based learning techniques as follows. Vanilla Prompt Engineering Vanilla prompt engineering involves the development of effective input prompts for LLMs to generate better outputs [Liu+23]. Traditionally, prompts were manually crafted based on intuitive templates, process that requires considerable expertise and may not always yield optimal results. To overcome this limitation, automated methods have been introduced, categorizing prompts into discrete and continuous types. Discrete prompts, also known as hard prompts, involve natural language phrases and can be discovered through methods like prompt mining from large corpora [Jia+20], paraphrasing existing seed prompts [YNL21], gradient-based search over tokens [Wal+19], and using LLMs to generate prompts based on inputs [GFC21]. Continuous prompts, or soft prompts, operate within the embedding space of the model and do not require human-interpretable language. Prefix-tuning is an example of this approach [LL21], where continuous task-specific vectors are prepended to inputs, allowing the LM to perform the task more effectively without altering its parameters. In addition, some methods, such as P-tuning [Liu+24e] and PTR [Han+22], enhance hard prompt templates by incorporating some tunable embeddings, rather than relying solely on purely learnable prompt templates. These approaches blend the structure of hard prompts with the flexibility of trainable soft tokens, improving prompt performance and adaptability. 11 LLM Size Small (2B) Inference Requirements Training Requirements Examples Hostable on personal laptop (5GB GPU memory) Requires server cluster (1 NVIDIA A100 40GB) Gemma-2-2B, Llama-3.2-1B Medium (2100B) Requires server cluster (8 NVIDIA A100 80GB) Requires server cluster (1 NVIDIA A100 80GB) Llama-3-70B, Mixtral-8x7B Large (100B) Requires server cluster (8 NVIDIA A100 80GB) Requires server cluster (32 NVIDIA A100 80GB) Llama-3.1-405B Proprietary (Size Unknown) Accessed via API Accessed via API GPT-4, Claude3, Gemini-2 Table 1: Recent popular LLMs categorized based on their hosting requirements. Estimates for the inference and training costs are based on floating-point 16 precision. New research is trying to compress further to make larger models available locally [Par+24b; Xia+24a; Hua+24a]. 1, 2, . . . , 1, y1, . . . , In-Context Learning In-context learning is paradigm that allows language models to perform tasks by using few examples as demonstrations within the prompt, without the need for further finetuning [Bro+20]. Formally, given set of demonstrations with inputs {x1, x2, . . . , xn} and corresponding outputs {y1, y2, . . . , yn}, prompt template is used to format each input into {x n}. For new input x, formatted as using the same template, the language model predicts the output by n, yn, x). By prepending these input-output pairs to the estimating the probability Pθ(y current input, the model learns to perform the task through the context provided by these examples, effectively enabling it to generalize to new tasks based on in-context information. Studies have shown that increasing the number of examples leads to significant performance improvements across both generative and discriminative tasks [Aga+24]. Despite its apparent simplicity and effectiveness, in-context learning presents several challenges that significantly impact model performance. The selection and ordering of examples are crucial. Research has shown that the specific examples chosen in few-shot scenario can lead to vastly different outcomes, ranging from near state-of-the-art accuracy to almost random guessing [Lu+22]. To mitigate this variability, advanced techniques such as using sentence embeddings to select examples that are semantically close to the input [GFC21; Liu+22] or employing entropy-based methods to optimize the sequence of examples have been proposed [Lu+22]. Moreover, formal understanding of why in-context learning works has been subject of ongoing research. Recent studies suggest that in-context learning functions as form of Bayesian inference, where the model uses the provided examples to recover latent concepts [Xie+22]. Researchers have empirically observed that replacing gold labels with random labels only marginally impacts performance [Min+22a]. Additionally, in-distribution inputs within the demonstrations significantly contribute to performance gains [Min+22a]. This suggests that in-context learning likely helps elicit knowledge that the model has already acquired during pre-training, rather than teaching the model new information through just few examples."
        },
        {
            "title": "3.3 LLM Supervised Fine-Tuning (SFT)",
            "content": "Even with extensive pre-training, LLMs may not excel at specific tasks without further adjustment. To achieve improvements of this kind, process known as Supervised Fine-Tuning (SFT), which involves fine-tuning with labeled data, is often necessary. For instance, the BERT paper [Dev+19] demonstrated the effectiveness of fine-tuning the model on 11 distinct tasks. Although more recent LLMs can perform tasks through in-context learning [Bro+20] or zero-shot prompting [Liu+23] without prior fine-tuning, they still stand to gain from fine-tuning tailored to specific tasks or datasets. notable example is OpenAIs GPT-3.5 Turbo, which, despite its smaller size compared to GPT-4, could achieve superior performance when fine-tuned with task-specific data.11 SFT does not need to be based on single task. Indeed, to further improve LLMs performance and address data scarcity, researchers are also increasingly adopting Multi-Task Learning (MTL) [Car98] for NLP tasks. This approach trains models on multiple related tasks simultaneously, broadening the training dataset and reducing overfitting risks [CZY24]. MTL not only captures generalized and task11https://platform.openai.com/docs/guides/fine-tuning 12 specific knowledge but also facilitates the development of compact, efficient models through implicit knowledge sharing [DH17; SCN18]. 3.3."
        },
        {
            "title": "Instruction Tuning",
            "content": "The transformative idea that any NLP task can be converted into text-to-text format has significantly aligned with advances in generative language models [Raf+20]. This task paradigm shift allows for the fine-tuning of language models across broad spectrum of NLP tasks using an unified data format [Mis+22], thereby ensuring uniform training objective. pivotal development in this area is instruction tuning where language model is fine-tuned on collection of tasks described via instructions [Wei+22b]. This method has been shown to improve the zero-shot performance of language models on unseen tasks. Models such as FLAN [Wei+22b] and T0 [San+22] exemplify this approach. These models are trained to process variety of NLP tasks through instructional prompts, setting new performance benchmarks and demonstrating an impressive ability to generalize to tasks they were not explicitly trained on. This evolution towards instruction-based task execution highlights the critical role of multi-task learning in increasing the robustness and adaptability of language models [CZY24]. The significance of instruction tuning is further highlighted in the InstructGPT paper [Ouy+22], which utilizes novel dataset comprising prompts crafted by labelers and those submitted to early InstructGPT models via the OpenAI API. This dataset includes wide spectrum of tasks such as brainstorming, rewriting, open-ended question answering, and more, reflecting the diverse and usercentric nature of modern NLP applications. Labelers demonstrate the desired responses to prompts, providing training data for SFT using GPT-3. The InstructGPT findings emphasize the necessity of moving beyond traditional NLP tasks to include user-centric tasks like brainstorming, which are not adequately captured by traditional NLP datasets. This expansion not only broadens the scope of tasks that models are trained on but also enhances their ability to perform effectively on real-world, user-driven tasks, marking significant shift towards more adaptive and user-focused language models. 3.3.2 Parameter-Efficient Fine-Tuning Given the sheer size of LLMs, common approach to fine-tuning is to modify small fraction of the models parameters while leaving most of them unmodified. This approach, called Parameter-Efficient Fine-Tuning (PEFT), focuses on selectively tuning limited number of parameters to achieve the desired performance gains without completely modifying the entire model. The PEFT strategies can be broadly classified into three types [Han+24b]: (1) Additive fine-tuning, which injects new trainable modules or parameters into the original model architecture. (2) Selective fine-tuning, which trains only subset of model parameters during fine-tuning. (3) Reparameterized fine-tuning, which constructs low-dimensional reparameterization of the original model parameters for training. Additive Fine-Tuning Additive fine-tuning strategies, such as adapters [Hou+19; He+22b] and soft prompts [LL21; Liu+24e], introduce only minimal number of trainable parameters that are strategically positioned within the model architecture. Adapters are small layers inserted within Transformer blocks, consisting of down-projection matrix, nonlinear activation function, and an up-projection matrix. These layers act as computational bottlenecks, refining the models output while leveraging the existing pre-trained parameters. On the other hand, soft prompts involve appending adjustable vectors at the beginning of the input sequence, enhancing the models ability to utilize the rich information within the continuous embedding space. This method adjusts the initial conditions of the models input processing, allowing for fine-tuned performance improvements without extensive retraining of the core model components. Both approaches maintain the original model architecture unmodified while providing targeted enhancements for specific tasks. Selective Fine-Tuning Unlike additive PEFT, selective PEFT fine-tunes only specific subset of the existing parameters within model. This is achieved by applying binary mask to the models parameters, where each element of the mask is either 0 or 1, indicating whether the corresponding parameter should be updated during fine-tuning. Only the selected parameters are adjusted based on the gradients of the loss function, using predefined learning rate. This method allows for targeted improvements on downstream tasks by optimizing limited number of model parameters, thereby maintaining the overall efficiency and scalability of the model. Techniques such as Diff pruning [GRK21], PaFi [LMM23], and FishMask [SNR21] exemplify this approach, focusing on refining the models performance through selective parameter updates. Reparameterized Fine-Tuning Reparameterization fine-tuning involves transforming models architecture by adjusting its parameters, often through low-rank parameterization to maintain efficiency during training and restoring the original weight configurations for inference. notable technique in this domain is LoRA (Low Rank Adaptation) [Xu+24b], which introduces small, trainable matrices operating alongside the pre-trained weights to inject task-specific updates without burdening the inference process. This method modifies the output by adding an incremental update, effectively capturing task-specific nuances while maintaining the models original efficiency. Extensions of LoRA, such as DyLoRA [Val+23], dynamically adjust the rank of these matrices within training budget to optimize performance without fixed rank constraint, enhancing training efficiency. Another variant, AdaLoRA [Zha+23], uses singular value decomposition to refine the update matrix, pruning less significant components based on their impact, thus optimizing the parameter count. These reparameterization strategies ensure that PEFT not only preserves but also enhances model functionality with minimal computational overhead, making them suitable for large-scale models. These reparameterization techniques closely align with methodologies in high-dimensional statistics, particularly low-rank matrix estimation and recovery [ZHT06; CP11; CP10; CZ13; KX15]. Lowrank techniques in statistics aim to estimate high-dimensional matrices by uncovering the underlying low-dimensional structure in the data. Similarly, reparameterized fine-tuning leverages the low-rank approximations in the model to adapt pre-trained models efficiently to new tasks, capturing taskspecific information with minimal computational overhead. This connection highlights the theoretical foundation and practical utility of low-rank approaches, as both frameworks exploit the inherently low-dimensional structure in data and models. By doing so, they provide principled framework for achieving balance between complexity and accuracy, making them particularly well-suited for large-scale machine learning models."
        },
        {
            "title": "3.4 System 2 Prompting and Chain-of-Thought",
            "content": "System 2 prompting refers to prompts that elicit deliberate reasoning-like process in AI models that takes the form of the generation of intermediate steps before arriving at final response [WS23; Yu+24]. This contrasts with System 1 prompting, where model directly produces response without intermediate steps. Inspired by human cognitive processes, System 2 prompting is designed to handle complex reasoning tasks that System 1 might struggle with, by using techniques like Chain-of-Thought [Wei+22a], Tree-of-Thoughts [Yao+23], Graph-of-Thoughts [Bes+24], BranchSolve-Merge [Sah+24], System 2 Attention [WS23], Rephrase and Respond [Den+24] and others. These techniques aim to improve performance in areas like multi-step reasoning [Ran+24], mathematical problem solving [Wei+22a], and commonsense reasoning [Zha+24h]. While System 2 methods can lead to more accurate and interpretable outcomes, they typically involve higher computational costs and latency [Ope+24b]. Inference-Time Scaling Law In addition to the established scaling laws for training LLMs, recent shifts in focus have highlighted the significance of inference-time scaling laws, particularly following the introduction of OpenAIs o1 model [Ope+24b], which is designed to extend various computational steps before generating responses. This can be achieved, for example, by (1) generating multiple candidate responses and selecting the best using methods such as automatic verifiers [Bro+24], reward models [Nak+22], or self-consistency [Wan+23d; Che+23b], or (2) enhancing the reasoning process within single trial by introducing more intermediate thinking steps like reflection and revision [Ope+24b; Qin+24; Hua+24b]. For instance, [Bro+24] demonstrated that across multiple tasks and models, the coveragedefined as the fraction of problems solved by any attemptsignificantly scales with the number of samples across four orders of magnitude. Complementing this, [Sne+24] showed that optimizing inference-time computation through combination of (1) searching against dense, processbased verifier reward models, and (2) adaptively updating the models response distribution based on the test-time prompt can yield greater performance improvements than merely scaling model parameters. Furthermore, [Dee+25; Tea+25] observed that by directly optimizing for outcome-based rewards, the system can self-evolve and scale its inference time without external intervention, high14 lighting the dynamic nature of this process. Theoretically, advances have also been made, as shown by [Liu+24g], which revealed that Transformers equipped with Chain-of-Thought (CoT) [Wei+22a] processingallowing the Transformer to auto-regressively generate sequence of intermediate tokens before answering questionscan be adept at handling complex problems that inherently require serial computations. These insights collectively suggest that strategic enhancements in inference-time computation could unlock new capabilities in LLMs, paving the way for more sophisticated and nuanced machine reasoning."
        },
        {
            "title": "3.5 LLM Reinforcement Learning & Preference Optimization",
            "content": "Current strong LLMs can be prompted to perform variety of tasks. However, these models sometimes exhibit unintended behaviors, such as making up facts or producing biased or toxic content [Wei+21; Bom+22; Ken+21]. This issue has been framed as lack of alignment of the models, where alignment in LLMs is defined in terms of desiderata such as ensuring that models are helpful, honest, and harmless [Ask+21]."
        },
        {
            "title": "3.5.1 Approaches to Aligning LLMs",
            "content": "To tackle the issue of aligning LLMs, researchers have developed two distinct categories of approaches: (1) Reward-based methods, which involve training reward model using preference data and subsequently optimizing the models behaviors to maximize the reward received; and (2) Reward-free methods, which eliminate the reward model altogether and instead directly utilize human preferences to train the LLM. Reward-based methods Many leading proprietary LLMs, including GPT-4 [Ope+24a] and Claude 3,12 employ reward-based methods for alignment, specifically Reinforcement Learning from Human Feedback (RLHF) [Sti+20]. The RLHF pipeline usually includes three phases: (1) supervised finetuning (SFT); (2) reward modeling and (3) RL optimization. 1. SFT Phase: RLHF typically starts by fine-tuning pre-trained LM with supervised learning, using high-quality data across large and diverse set of instruction following tasks. This process aims to establish well-prepared initial model, denoted as πSFT, which serves as good foundation for subsequent training stages. 2. Reward Modeling Phase: During this phase, the SFT model is prompted with input to produce pairs of answers (y1, y2) πSFT(yx). These pairs are then evaluated by human labelers who indicate their preference between the two, represented as yw ylx, where yw and yl denotes the preferred and dispreferred response among (y1, y2) respectively. The preferences are assumed to be generated by some latent reward model r(y, x), which we do not have access to. common approach to modeling these preferences is the Bradley-Terry (BT) model [BT52], which posits that the human preference distribution can be expressed as: p(y1 y2x) = exp(r(x, y1)) exp(r(x, y1)) + exp(r(x, y2)) . (5) Given static dataset of comparisons = {x(i), y(i) i=1 sampled from p, we can parametrize reward model rϕ(x, y) and estimate its parameters via maximum likelihood. This setup is treated as binary classification problem, where the negative log-likelihood loss is defined as: , y(i) }N LR(rϕ, D) = E(x,yw,yl)D[log σ(rϕ(x, yw) rϕ(x, yl))], (6) and where σ is the logistic function. In the context of LMs, the network rϕ(x, y) is often initialized from the SFT model πSFT(yx) with an additional linear layer on top of the final Transformer layer that produces single scalar prediction for the reward value [Zie+20]. To ensure reward function with lower variance, it is common practice to normalize the rewards, such that Ex,yD[rϕ(x, y)] = 0 for all x. Additionally, having separate reward model offers the advantage of utilizing it for rejection sampling during inference time. This process involves generating multiple responses 12https://www.anthropic.com/news/claude-3-family 15 to user query and then employing the reward model to identify and select the most suitable response, thereby enhancing the overall performance of the model [Sti+20]. 3. RL optimization Phase: During the RL phase, the learned reward function rϕ(x, y) is used to provide feedback to the language model. Following prior work [Jaq+17; Jaq+20], the optimization is formulated as max πθ ExD,yπθ(yx)[rϕ(x, y)] βDKL[πθ(yx)πref(yx)], (7) where β is hyperparameter controlling the deviation from the reference policy πref, which is initially set as the SFT model πSFT. In practice, the language model policy πθ is also initialized to πSFT. This constraint is crucial as it helps maintain the model within the bounds of the distribution for which the reward model is accurate, ensuring diversity in generation and preventing the model from collapsing to few high-reward responses. Furthermore, the training process requires careful implementation of early stopping to prevent reward distributions from collapsing to the same distribution across all prompts, regardless of their diversity [Son+23]. Due to the discrete nature of language generation, the reward function is typically constructed as r(x, y) = rϕ(x, y) β(log πθ(yx) log πref(yx)), and maximized using PPO [Sch+17]. Additionally, some research has explored alternative RL algorithms, such as REINFORCE [SB18], as potential replacements for PPO in RLHF. This exploration aims to reduce the computational costs and alleviate the challenges of sensitive hyperparameter tuning associated with PPO [Ahm+24]. Reward-free methods RLHF is complex process that often requires significant memory resources and extensive hyperparameter tuning. As result, several recent studies have explored alternatives to RLHF, with Direct Preference Optimization (DPO) emerging as notable method. Described in detail in [Raf+23], DPO is an offline preference optimization algorithm that eliminates the need to train separate reward model, thereby simplifying the process and enhancing training stability. It utilizes novel reward model parameterization that facilitates the extraction of the optimal policy in closed form. The DPO loss function is defined as: LDPO(πθ; πref) = E(x,yw,yl)D (cid:104) (cid:16) β log log σ πθ(ywx) πref(ywx) β log πθ(ylx) πref(ylx) (cid:17)(cid:105) . (8) The DPO pipeline proceeds as follows: (1) Sample completions y1, y2 πref(x) for every prompt x, labelinig them with human preferences to construct the offline preference dataset = {x(i), y(i) }N i=1 and (2) optimize the language model πθ to minimize LDPO for the given πref and and chosen β. Typically, πref is initialized to πSFT whenever available. Despite its advantages, DPO exhibits several limitations, including susceptibility to overfitting [Aza+24], tendency to decrease the likelihood of preferred responses [Pal+24], and inefficiencies in memory and processing speed due to the simultaneous maintenance of reference and current policies for loss computation [MXC24]. Additionally, DPO can lead to an issue known as length explosion in responses [Par+24a]. To mitigate these challenges, numerous studies have proposed different DPO variants [Aza+24; Eth+24; Pal+24; Pan+24; Xu+24a; MXC24; Par+24a]. In particular, Iterative DPO [Xu+23] and variants yield improvements over DPO, indicating the importance of training on model responses that are on-policy. , y(i)"
        },
        {
            "title": "3.6 LLM Self-Alignment",
            "content": "Aligning LLMs using human feedback is often bottlenecked by the size and quality of human-annotated data. As models reach or surpass human-level intelligence in particular domains, it is expected that future models may require feedback that goes beyond what humans can provide in order to provide an adequate training signal. Leveraging the LLM itself to provide such feedback, in particular to create high-quality data for instruction-finetuning, called synthetic data, has become promising and scalable solution. 3.6.1 Synthetic Data Generation Current alignment methods in synthetic data generation often involve several key components: (a) Instructions, (b) Responses, and (c) Feedback on those responses. 16 Instructions have typically been provided by human users and consist of questions or tasks that the user wants the model to address during dialog interaction. Although there are publicly available instruction datasets sourced from human users, such as ShareGPT,13 OpenAssistant [Kop+24], WildChat [Zha+24f], and LMSYS-Chat-1M [Zhe+24], large-scale instruction datasets remain scarce due to privacy and other concerns. Consequently, several studies have focused on generating synthetic instructions. For instance, Self-Instruct [Wan+23e] employs an iterative bootstrapping algorithm, beginning with limited set of manually crafted tasks to guide the generation process. This approach uses few-shot prompting to enable the model to create novel prompts, which are then added to the prompt pool for further bootstrapping. Similarly, MathScale [Tan+24b] extracts topics and concepts from seed math questions to construct concept graph. Concepts are randomly sampled from this graph, and few-shot prompting is used to prompt GPT-3.5 to generate new questions. CodecLM [Wan+24c] utilizes LLMs to summarize use cases and required skills from existing seed instructions, subsequently decoding novel instructions based on different combinations of use cases and skills. Utilizing Responses generated by models in the training loop has been found to have significant potential in enhancing training outcomes, as in the standard RLHF training loop (cf. Sec. 3.5). For example, self-generated responses can easily be leveraged through pairwise preference learning, where positive and negative generations are both used to train the model [Pan+24]. Other examples of self-training include STaR [Zel+22], which collects successful model-generated rationales that solve problems effectively, using them as training data for iterative supervised fine-tuning. To effectively utilize responses for supervised fine-tuning (SFT) or preference learning, it is crucial to assess their quality through feedback. If response is suboptimal, it may still be improved through various methods. Feedback on responses can be binary or scalar, indicating the quality of response, or it can be preference-based, showing relative quality among multiple responses. For scalar or preference feedback, while some tasks with fixed short answers are straightforward to evaluate, tasks requiring long, free-form generation pose challenges. Standard approaches are to use reward model, or by LLM prompting, referred to as LLM-as-a-Judge. Recent studies have shown that LLM-as-a-Judge prompting can yield feedback that aligns well with human judgments [Li+23; Zhe+23]. Feedback can also be provided in natural language, critiquing the response. For instance, Constitutional AI [Bai+22] uses LLMs to generate critiques and revisions based on constitutional principles to enhance model-generated responses. Other specialized critic generators, such as Shepherd [Wan+23c], PandaLM [Wan+24b], Auto-J [Li+24b], and LLMCRIT [YLG24], have been developed. 3.6.2 Self-Rewarding Language Models Current alignment methods, such as RLHF, heavily depend on human annotation for some of the aforementioned components, particularly instructions and feedback. However, there is scarcity of high-quality public instruction sources, especially those involving complex and challenging tasks, such as advanced mathematics and reasoning. As AI systems become increasingly sophisticated, the reliance on human feedback becomes more problematic. Superintelligent AI could generate outputs and exhibit behaviors that surpass human comprehension, such as producing vast amounts of novel code that may be difficult to interpret or potentially dangerous to execute. The question is how to maintain trust and control over these powerful models. Training such AI systems that remain helpful and harmless even as they surpass human-level performance suggests that we will need techniques and data that do not rely primarily on human inputs. One promising approach to address the challenge of aligning advanced AI is iterative preference optimization [Xu+23], which leverages reward model rather than relying on human annotators to score LLM outputs. This method has proven effective when well-trained reward model is available [TGH23; MXC24; Guo+24a]. However, particularly intriguing idea is to forgo the assumption of access to an external reward model. Instead, both the LLM and the reward model are the same, allowing the model to improve itself autonomously. Demonstrating that iterative preference optimization can also be effective with synthetic instructions would further validate its potential to provide an entire autonomous training pipeline. Self-rewarding language models [Yua+24] embody this concept. These models begin with small set of human-authored data that provides basic instruction-following and evaluation capabilities. The model then iteratively improves itself by generating responses to synthetic instructions and scoring 13https://huggingface.co/datasets/RyokoAI/ShareGPT52K 17 these responses using an LLM-as-a-Judge mechanism [Zhe+23]. The best and worst responses for each instruction are selected for further training through DPO, allowing the model to enhance its instruction following and evaluation capabilities without any human intervention. The authors show that it is possible to achieve iterative self-improvement using the models instruction following and evaluation capabilities on pure synthetic generated instructions. Building upon this, meta-rewarding language models [Wu+24a] take the concept further by not only curating training pairs for instruction following but also for evaluation (LLM-as-a-Judge performance). In each iteration, the model compares multiple judgments on single response and selects the best judgment pairs for DPO training in addition to response pairs. This refinement allows the model to improve both its instruction-following and evaluation skills more effectively than the self-rewarding approach. These methods represent significant step toward aligning highly advanced AI, reducing the need for direct human supervision."
        },
        {
            "title": "3.6.3 Challenges",
            "content": "Although synthetic data can greatly enhance the scalability of model training, recent studies have highlighted challenges associated with using data generated by large language models (LLMs). Reward Misspecification In iterative preference optimization, reward models are crucial but inherently imperfect. These models, whether generative or classifier-based, serve as proxies for human judgment, but their flaws can lead to significant issues [GSH23]. This phenomenon is often described by Goodharts law, which states that when measure becomes target, it ceases to be good measure. In this context, over-optimization can amplify the reward models biases, distorting true performance rather than enhancing it. Several types of biases can arise in this process. Position bias occurs when an LLM exhibits propensity to favor certain positions over others, potentially skewing results [Zhe+23]. Verbosity bias is another concern, where an LLM judge favors longer, more verbose responses, even if they lack clarity, quality, or accuracy compared to shorter alternatives [Sin+24; Dub+24b]. Additionally, self-enhancement bias, term adopted from social cognition literature, describes the tendency of LLM judges to favor answers generated by themselves, further complicating the evaluation process [Zhe+23]. To address these challenges, researchers have introduced benchmarks like RewardBench [Lam+24] and JudgeBench [Tan+24a]. Both of these benchmarks systematically evaluate the abilities of LLM judges to correctly identify the better and worse responses within pair. They cover various categories, including chat, safety, and reasoning, among others. These benchmarks help identify the strengths and weaknesses of different reward models, guiding the development of more reliable and effective reward models for preference optimization. Distribution Shift and Lack of Diversity Recent findings indicate that while LLMs can generate text nearly indistinguishable from human-authored content, the overall distribution of these LLMgenerated texts differs notably from human text. Specifically, LLM-generated texts tend to have truncated tails, i.e., they produce fewer extreme or rare outputs, and exhibit reduced variability [Shu+23; Doh+24]. Training on LLM-generated data could potentially lead to phenomenon where narrow range of behaviors results in decreased performance, as seen in tasks like language modeling [Shu+23; Doh+24] and iterative preference optimization for mathematical reasoning [WLL25]. Studies show maintaining mix of clean, human-authored data alongside LLM-generated content during training helps preserve diversity and prevents the model from deteriorating in performance [Shu+23; Doh+24; Ger+24]. few works go beyond the mixing scenario and study how to curate or filter synthetic data to avoid such performance deterioration [Fen+24b; Zha+24b]."
        },
        {
            "title": "4 Designing Trustworthy LLMs by Statistical Methods",
            "content": "As LLMs increasingly permeate various aspects of society, ensuring their trustworthiness has become critical challenge. Trustworthiness encompasses range of dimensions, including interpretability, accountability, and algorithmic fairness. Statistical methods offer rigorous and systematic approach to address these challenges, providing tools to analyze, enhance, and monitor the behavior of LLMs. This section explores how statistical techniques can contribute to the design of trustworthy LLMs across several key areas. First, we discuss mechanistic interpretability, which aims to uncover how LLMs make predictions and generate outputs. Next, we examine uncertainty quantification, vital 18 component for understanding and communicating the reliability of model predictions. The section then delves into methods for LLM watermarking, which ensures the provenance and authenticity of generated content, and privacy-preserving mechanisms to safeguard user data. We also address algorithmic fairness, focusing on statistical strategies to mitigate biases in LLM outputs. Finally, we provide statistical perspective on LLM self-alignment, process for aligning model behavior with human values and goals. Together, these topics underscore the essential role of statistics in fostering trust and accountability in the era of large-scale AI models."
        },
        {
            "title": "4.1 Uncertainty Quantification",
            "content": "While LLMs produce human-like responses with impressive accuracy across various tasks, they are also prone to hallucination [Ji+23; RSD23], raising concerns about their reliability. Quantifying uncertainty is crucial for addressing these limitations, as it allows models to provide not just answers but also confidence in their outputs, enabling users to make more informed decisions. Uncertainty Metrics While uncertainty estimation and calibration are well-established for traditional machine learning models [Abd+21; Gaw+23], the emergence of LLMs has introduced new challenges and demands. Unlike fixed-dimensional outputs typical of traditional models, LLM responses are often complex, requiring uncertainty metrics that can operate on sentence-level outputs. This complexity necessitates innovative approaches to quantify uncertainty effectively. Classical metrics, such as entropy, can be directly calculated on the probability distribution of next-token prediction and averaged over all tokens [MG21]. To incorporate the special structure of language model, existing approaches further considered semantic features such as semantic similarity [Fom+20; LLS22], semantic equivalence [KGF23] and token importance [Dua+24b; Bak+24], as well as internal signals in language models like logits and hidden states [Kad+22; Che+24a; Liu+24c] into the metric design. In general, they can be easily computed in white-box setting where the underlying representation and prediction distribution are available, while for black-box models, some can be calculated via repeated sampling of the response [Kad+22; LTS24; CM24]. Overall, these approaches aim to develop robust uncertainty metrics capable of appropriately assessing the confidence of LLM-generated responses in meaningful and scalable manner. Conformal Prediction in LLMs Conformal Prediction (CP) [VGS05; AB23] has emerged as versatile framework for distribution-free statistical inference. CP constructs confidence sets for predictions based on the empirical distribution of residuals, ensuring validity without assumptions about the underlying model or data. Its flexibility and computational efficiency have made it an appealing tool for LLMs despite challenges like large output spaces and non-exchangeable token sequences. To address these issues, works such as [Kum+23; Ren+23] have restricted the output space, applying CP to tasks like multiple-choice question answering and robot actions, while [RGG23] calibrated nucleus sampling to improve token-level predictions. Other methods, such as [UZM24], adapt CP to non-exchangeable settings by leveraging latent representations for nearest-neighbor searches, resulting in more precise prediction sets. Beyond improving accuracy, CP has been extended to control quantities like toxicity and hallucination risks [Zol+24; Yad+24; MH24; CGC24], enabling safer and more aligned LLM applications. CP methods have also been leveraged for evaluating LLM performance. [Ye+24a] applied CP to benchmark LLMs on five natural language processing tasks, measuring average confidence set sizes for multiple-choice questions to quantify uncertainty. In machine translation, [Gio23] and [ZM24] used CP to assess translation quality, providing calibrated confidence estimates for both human and machine evaluations. Additionally, [Sch+21; Sch+22b] proposed confident early exiting methods for Transformers, where intermediate layers assess uncertainty to speed up inference while maintaining consistency with the full model. Hallucination Detection Recently, there has been growing trend toward adopting uncertainty estimation methods to address hallucination detection in LLMs. The core idea is that the logits and hidden states of LLMs encapsulate information about the models confidence in its generated output, which can be leveraged to identify hallucinations. For example, [AM23; Slo+23; Che+24a] use activations from hidden layers as input to train classifier specifically designed to detect hallucinations, while 19 [Ver+23] develop epistemic neural networks that aim to reduce hallucination occurrences by modeling epistemic uncertainty. Despite these advances, the lack of consistent and rigorous definition of hallucination across the literature poses challenges to standardizing this task. However, uncertainty estimation remains well-defined problem, and insights from uncertainty quantification can be directly applied to improve hallucination detection in LLMs, offering robust statistical foundation for this critical challenge. One example to frame the hallucination detection within hypothesis testing framework. Specifically, given question and an LLM-generated answer (q), the problem can be formulated as follows: H0(q, (q)) : (q) is not correct for vs. H1(q, (q)) : (q) is correct for q. The object is to construct test function ˆT (q, (q)) {0, 1} that, with probability at least 1 δ over its construction, ensures that at least 100(1 α)% of incorrect (q, (q)) pairs in future question- (cid:12)H0(q, (q))(cid:1) > α(cid:1) δ. To achieve this, answering instances are detected: ˆT [Nie+24] propose hypothesis test that provides finite-sample, distribution-free type error control, leveraging set of i.i.d. samples consisting of (q, (q)) along with correct answer for q. (cid:0) ˆT (q, (q)) = 1(cid:12) (cid:0)Pq,M (q) Future Research Although existing uncertainty quantification methods have shown promise in various aspects of language models, many challenges remain. First, many uncertainty metrics rely on the entropy of the next-token prediction distribution in the white-box setting or the consistency between multiple responses in the black-box setting. However, uncertainty in the generation distribution may not appropriately capture the factual accuracy of language model outputs. For example, when language model exhibits high confidence in incorrect facts, its confidence estimates can become miscalibrated, making it difficult to detect errors through uncertainty metrics alone. Furthermore, recent research has demonstrated that these metrics can be fragile and easy to manipulate under adversarial attacks [Zen+24]. Future work should focus on developing more robust uncertainty metrics that better reflect the reliability of responses and improve the alignment of existing metrics with factual correctness. Second, as previously mentioned, the broad applicability of conformal predictions in the context of language models is limited by the challenges of non-exchangeability and the large discrete space of natural language data. Addressing these limitations by designing computationally efficient conformal prediction methods that are better aligned with the structure of language data is an exciting area for future research. Lastly, beyond current applications, we want to highlight that uncertainty quantification has the potential to benefit broader range of applications, such as generalizing to multi-round interactions, guiding data collection, and enhancing model interpretability. Exploring these directions can be helpful in advancing uncertainty-aware language models and improving their trustworthiness in real-world applications."
        },
        {
            "title": "4.2 LLM Watermarking",
            "content": "The capability of LLMs to generate human-like text has raised significant concerns regarding potential misuse. For instance, StackOverflow, leading question-and-answer platform for programmers, implemented temporary ban on AI-generated responses to prevent the dissemination of misleading information [Vin22]. Similar concerns about misinformation proliferation have emerged across various domains, including fake news [Zel+19], academic integrity [MML23], and data authenticity [Shu+23]. Consequently, robust techniques for attributing text to LLMs have become essential for enabling individuals and platforms to detect and enforce policies regarding LLM usage. direct approach to detecting LLM usage in text is to train detectors based on linguistic features that distinguish human-written text from LLM-generated text. Some studies trained these detectors in an end-to-end manner on collected data [GPT23; Zer23; BL24; KKO24], whereas others exploited structural properties of LLMs for detection [Ipp+20; GSR19; Mit+23] or relied on inherent stylistic distinctions without training [Yan+24; Tul+24]. However, these ad-hoc methods have shown degraded performance as LLMs become increasingly capable of generating human-like text [Web+23]. It remains true that contemporary LLM-generated text still exhibits distinguishable features compared to humanwritten text [PCJ25]; furthermore, these methods often exhibit vulnerability to adversarial attacks and can show bias against non-native English writers [Kri+24; Sad+23; Lia+23], but this is an area that is evolving. more proactive and controllable approach involves subtly modifying the text generation processes of LLMs to enable provable detection, known as watermarking, by embedding subtle yet detectable 20 statistical signals into the generated text [Kir+23a]. This technique leverages the probabilistic nature of LLMs, allowing the model provider to utilize pseudorandomness in the generation process. Statistical Formulation The statistical framework for watermarking in text generation can be formulated as follows [Li+25]: let the next-token prediction distribution at the t-th token be Pt, which represents multinomial distribution. The watermarked LLM first generates pseudorandom variable ζt using hash function ζt = A(w1:(t1), Key), where w1:(t1) denotes the first 1 tokens, and Key represents private key. It then samples the next token wt = S(Pt, ζt) through decoder S. The decoder can be deterministic or incorporate randomness that is independent of ζt. This design enables the computation of the pseudorandom number from the observed text and the private key in both the generation and detection phases (transmitted via trusted protocol). Formally, the tuple (A, S, Key) constitutes watermark. Detectability of the watermark signal is enabled by the dependence between the token wt and pseudorandom variable ζt. [Li+25] proposed constructing pivot statistic Yt = (wt, ζt) for = 1, . . . , that follows fixed distribution for human-written text, which formulates the problem of distinguishing between human-written text (null) and LLM-generated text (alternative) as hypothesis testing: H0(human) : Yt µ0 i.i.d. for all vs. H1(LLM) : Yt µ1,Pt for all t. (9) One can then either reject or accept the null hypothesis via Th(Y1:n) = (cid:40) 1 0 if (cid:80)n if (cid:80)n t=1 h(Yt) γn,α, t=1 h(Yt) < γn,α, (10) where γn,α is threshold determined by the sample size and the significance level α. To elucidate the rationale behind the detection rule (9), observe that under H0, human text composition follows complex cognitive processes that are statistically independent of pseudorandom variables ζ1:n. By construction, Yt = (wt, ζt) follows known distribution µ0 provided that wt is independent of ζt. Hence, (cid:80)n t=1 h(Yt) follows known null distribution so that an appropriate threshold can be calculated to control the type error. Under H1, Yt follows µ1,Pt, which depends on the unknown and varying token distribution Pt. An effective choice of the score function would increase this sum under the alternative hypothesis. Indeed, [Li+25] established general framework to determine the optimal score function through minimax optimization problem. Unbiased Watermarks The Gumbel-max watermark [Aar23], implemented internally at OpenAI, exemplifies watermarks that can be analyzed within this statistical framework. It relies on the Gumbelmax trick: let be the token vocabulary, and let ζ = (Uw)wW consist of i.i.d. copies of [0, 1]. Then, the Gumbel-max trick states that14 arg maxwW (log Uw)/Pw follows the multinomial distribution . [Aar23] proposed using the decoder S(P, ζ) = arg max wW log Uw Pw for selecting the next token. Several detection rules of the form (10) have been proposed [Kud+24; Fer+23; Li+25]. Notably, [Li+25] developed an optimal sum-based test under certain conditions, achieving the fastest exponential rate of decrease in type II errors. More recently, [Li+24c] introduced robust detection rule based on truncated family of goodness-of-fit tests. Because the Gumbel-max trick samples exactly from the underlying multinomial distribution, the resulting watermark is unbiased. Formally, watermark is unbiased if, for any token and token dis- (cid:0)S(P, ζ) = w(cid:1) = Pw. Unbiased watermarking schemes are generally preferred, tribution , one has Pζ as they preserve the LLMs token distribution, thereby maintaining text quality. Another unbiased example is the inverse transform watermark [Kud+24], which corresponds to inverse transform sampling of the multinomial distribution. Its optimal detection rule was also derived in [Li+25; CLZ25]. 14In an abuse of notation, in this section Pw denotes the coordinate of the probability vector corresponding to token w, while generally we write Pt to refer to the entire vector at time t. 21 Biased Watermarks Beyond the unbiased regime, the greenred list watermark [Kir+23a] is popular instance of biased watermark, due to its simplicity and intuitive design. In this watermark, the token vocabulary is randomly partitioned into red list and green list, where the probabilities of green tokens are increased. One parameter in this watermarking scheme controls the magnitude of the distortion in the next-token distribution, while another determines the size of the green list. During detection, if the proportion of green tokens in text exceeds specified threshold, the text is classified as LLM-generated. Several studies have refined this approach [Huo+24; Wou24; Cai+24] by proposing methods to optimally select these parameters, thereby balancing watermark detectability with text quality. Meanwhile, [Xie+24b; Hu+24; Wu+23b] introduced unbiased variants of the greenred list watermark by applying techniques such as maximal coupling. Furthermore, an optimal detection rule is established in [CLZ25], addressing both the minimization of type and type II errors and the minimization of type II error for fixed type error rate. Other Watermarking Schemes variety of other watermarking schemes have been proposed, and the list is growing. Many have been evaluated only empirically, and rigorous statistical analysis remains to be developed in these cases. Some of the more statistically inspired approaches include that of [Dat+24], who introduced production-ready watermarking system at Google DeepMind based on tournament sampling for multinomial distributions. [CGZ24; ZWL23] employed secret, hash-based mechanism to subtly modulate token-selection probabilities, ensuring that the watermark remains invisible without the key but verifiable with it. [Fai+23] presented watermark that is highly detectable and embeds publicly verifiable cryptographic signature into the LLM output using rejection sampling. [He+24b] developed watermarking scheme for tabular data following the principles of the green-red [Xia+24b] injected watermarking signals into both the frequency and pixel domains list watermark. of images after generation and employs classifier to detect the watermarks presence. [ZLW24] investigated decoders that transform logit vectors into token probabilities and proposed provably robust watermark. [He+24a] characterized optimal watermarking schemes by minimizing lower bound on the worst-case type II error while ensuring the worst-case type error and watermarked-text distortion remain below specified constants. [GF24] introduced an approach that first selects sound watermark detector producing p-values, then generates multiple candidate texts from an LLM prompt, and finally outputs the text with the lowest p-value. Although this strategy enhances robustness, it increases computational overhead due to multiple text generations. Future Research Looking ahead, several challenges persist in developing comprehensive statistical foundations for watermarking in complex usage scenarios. In practice, watermarked text can be compromised through paraphrasing or content alteration that removes or obscures watermark signals [Kir+23b; TZL24; Zha+24e]. Notably, text often comprises mixture of human-written and LLM-generated content, necessitating further research on unknown and complex source compositions [Li+24c]. One direct solution involves developing algorithms for localizing watermarked segments [Zha+24g; LLZ24]. Another unresolved challenge concerns pseudorandomness collision, where repeated pseudorandom numbers occur in the text sequence [Kud+24]. [Wu+24b] demonstrated that collision introduces bias into distortion-free watermarks and proved the impossibility of perfect distortion-free watermarks under collisions. Certain watermarking schemes are also vulnerable to watermark stealing, where attackers can reverse-engineer and remove or spoof the watermark [JSV24]. crucial challenge in implementing watermarks for LLMs lies in ensuring robustness against adaptive prompting and supporting multi-user tracing [CHS24b; CHS24a]. Moreover, achieving low computational overhead in watermark detection remains critical [Hua+23a]. Additionally, watermarking methods can be utilized to protect copyrighted training data by detecting data misappropriation [CLZ25]. From theoretical perspective, statistical framework of watermarks necessitates assuming simple yet informative structures of next-token prediction distributions [Li+25]. For empirical evaluation of watermarks, current metrics may be insufficient for assessing how biased watermarks affect generated content [SZ23]. Finally, when watermarking schemes are implemented in conjunction with acceleration techniques such as speculative sampling, [HH24] identified an inherent trade-off between watermark strength and sampling efficiency."
        },
        {
            "title": "4.3 Privacy and Copyright",
            "content": "Over the past few years, language models have grown in model size and sample size at an unprecedented speed, making the preservation of user privacy increasingly challenging. Prior work has shown that LLMs are able to memorize large portion of training data [Car+21; Car+23], and adversaries can easily extract gigabytes of training data from LLMs. For example, [Nas+23] showed that ChatGPT reveals persons personal contact information when prompted with repeat this word forever: poem poem poem poem, and the authors have recovered ten thousand examples from ChatGPTs training dataset at query cost of 200 US dollars. This poses significant risks to user privacy as personal information and confidential documents could be disclosed through interaction with the model. Data Sanitization The simplest way to avoid releasing private information is to remove this information from the training data. This approach is often referred to as the data sanitization procedure [OZ03; Ami07]. The data sanitization procedure can be formulated as binary classification problem and can be efficiently implemented by modern machine learning methods [Der+17; Lis+21; Vak+22]. Recently, data sanitization has been widely used as pre-processing step for removing personally identifiable information (PII) or protected health information (PHI) in many companies, such as Microsoft and PayPal [Wil23; BMW20]. However, sanitization relies on formal definition of private information, and for language data, this definition may depend on the context and have no well-defined boundary. Therefore, data sanitization works best for well-formatted private data, such as social security numbers and medical records, and has limited power for general privacy-preserving purposes [Bro+22]. Differential Privacy To preserve data privacy from the model side, standard approach is to exploit the framework of differential privacy (DP) [Dwo06]. DP ensures that adversaries cannot distinguish whether specific data point is included in the training set of the model. The standard method to provide DP guarantees in deep learning is to replace the standard optimizers with DP optimizers (e.g., DPSGD [Aba+16; Bu+20]), an approach that has been extensively used in LLM training [Hoo+21; Ani+22], fine-tuning [Li+21; Yu+21; Hua+23c], and prompt learning [Dua+24a]. However, as DP optimizers require clipping the gradient and injecting noise into the training procedure, it can hurt the model performance and require more computational resources for hyperparameter tuning [Li+21], making many of the existing methods impractical at the scale of current LLMs. This is an area where significant new research is needed. LLM Unlearning After LLMs are trained, it would be desirable to eliminate the influence of specific data from the model while preserving the models utilities on other tasks. This task is often referred to as machine unlearning [CY15]. While exact unlearning requires re-training the language model without the target data from scratch, it is possible to approximately achieve machine unlearning efficiently. [NRS21; Gin+19; Guo+20; Sek+21; Geo+24] introduced theoretical metrics for machine unlearning based on the notion of differential privacy and proposed unlearning methods based on Newton update removal mechanisms. However, these algorithms require computing the Hessian of loss functions, which is intractable for LLMs. Recent research has explored computationally efficient unlearning methods for LLMs. Gradient ascent is commonly used technique that reverts the learning process by minimizing the next-token prediction likelihood on target data [Jan+23; YXL24]. However, reverting the optimization process through gradient ascent can be unstable as it leads the model parameter to diverge. To mitigate the issue, [Zha+24d; Fan+24] designed an alternative loss function named negative preference optimization (NPO). The NPO loss generalizes the gradient ascent objective via adaptive weighting of the unlearning samples and ensures that the loss function remains bounded, thereby achieving slower divergent rate and enhanced stability compared to gradient ascent. Another variant of gradient ascent is to relabel the target data by randomly assigned labels and train the model to fit on the random labels [YXL24]. Beyond the gradient ascent-based method, there are several different approaches, such as localizing and fine-tuning the crucial model units (e.g., layers, weights, neurons) for the unlearning task [Men+22; PHB24; Yu+23; Wu+23a] and using influence functions [Jia+24a]. However, most of the current unlearning methods require specifying target task or content to be unlearned, and there is still lack of standardized corpora for LLM unlearning [Liu+25]. Copyright Beyond user privacy concerns, the ability of LLMs to memorize and reproduce training data raises critical issues regarding copyright protection. Copyrighted material embedded in training datasets can appear in model outputs, either inadvertently or deliberately, potentially violating intellectual property rights [Sam23]. While this issue can be partially addressed through data sanitization, differential privacy, and prompting techniques that mitigate the risk of disclosing copyrighted material [Elk+23; VKB23; CS24], urgent research efforts are needed to ensure that training processes and model outputs remain unconnected to specific instances of copyrighted content. Such research is crucial for establishing pathway toward building models that comply with copyright regulations and support responsible AI deployment. Taking different angle, [Wan+24a] has proposed an economic framework leveraging cooperative game theory principles to enable model developers to compensate copyright owners for using their data in training. Data Misappropriation related issue is data misappropriation, which refers to the unauthorized use, access, or exploitation of data for unintended or unpermitted purposes, often violating legal or ethical regulations. This concern has been at the center of several high-profile debates. For example, the lawsuit between The New York Times and OpenAI [Tim23] highlights tensions surrounding the use of copyrighted data in training LLMs. Additionally, OpenAIs Terms of Service explicitly prohibit the use of ChatGPTs outputs to develop competing models, underscoring the need for mechanisms to detect whether newly trained LLM has incorporated ChatGPT-generated contenta process often referred to as model distillation. Detecting such data misappropriation is challenging, as the probabilistic nature of LLMs generates content that may resemble, but does not directly copy, the original data [Sag23; Ges24]. This difficulty has spurred significant research into methods for identifying and tracing LLMsgenerated data [Sad+23; Mit+23; Ren+24]. statistical hypothesis testing framework is established in [CLZ25], and optimal detection of data misappropriation is established. These challenges and advances highlight the urgent need for robust frameworks that ensure ethical AI development, protect intellectual property, and maintain trust in AI systems. The interplay between statistical methods and practical detection mechanisms will continue to play critical role in navigating the complex issues of unlearning, copyright, and data misappropriation in LLMs. Future Research While significant progress has been made in privacy protection, copyright enforcement, and unlearning, several key challenges remain. One major challenge is the evaluation of privacy-preserving techniques. Current methods, such as differential privacy and data sanitization, lack standardized evaluation metrics, making it difficult to quantify the trade-offs between privacy protection and model performance. Developing robust benchmarks to assess these trade-offs is essential for advancing practical privacy solutions in LLMs. Another challenge lies in context-dependent privacy risks. The definition of private information is highly context-dependent, making it difficult to apply one-size-fits-all privacy solution. While sanitization techniques work well for structured data, sensitive information in free-form text often requires more nuanced handling. Future research should explore adaptive privacy mechanisms that dynamically assess context before enforcing safeguards, ensuring more reliable protection across diverse applications. Furthermore, continual unlearning remains an open problem. Most existing unlearning methods assume static dataset, but in practice, LLMs are often deployed in environments where data continuously evolves. As new information is incorporated, there may be legal or ethical requirements to forget specific data, requiring efficient and scalable unlearning techniques. Developing frameworks that support continual or real-time unlearning will be critical for maintaining compliance with evolving data privacy regulations while preserving model utility. Addressing these challenges will require collaboration between statisticians, machine learning researchers, legal experts, and policymakers. By integrating rigorous statistical methodologies with emerging AI advances, the community can work toward building more transparent, accountable, and privacy-preserving language models."
        },
        {
            "title": "4.4 Interpretability",
            "content": "Interpretable machine learning is broad concept that captures the extraction of the relevant knowledge from machine learning model concerning relationships either contained in data or learned by the model [Mur+19]. As LLMs have been deployed in more and more real-world applications, their interpretability has received an increasing amount of attention as people wish to ensure their alignment with 24 human values and understand their potential risks and failures. Simple machine learning models, such as linear regression or decision trees, are often considered interpretable since the dependency of model output on the model structure and training data is easy to characterize. Language models, however, contain billions of parameters and numerous layers, such that the precise dependency of output on data and model structure can be too complicated for humans to comprehend. Therefore, recent efforts have been focusing on mechanistic interpretability, which aims to explain the LLMs on an algorithmic level through reverse engineering the detailed computation performed by the LLMs. As proposed in [Ola+20], the current mechanistic interpretability research consists of three areas: features, circuits, and their universality. Features Unlike classical tabular data, where each of the coordinates represents concrete variable, textual input is highly structured and it is unclear how the LLMs extract meaningful features from the data. Recent work has found that the features are learned and encoded by groups of neurons in LLMs; for example, [Gur+23] showed that some neurons in LLMs are activated for names of sports. Similarly, neurons that encode various features have been discovered, including sentiment neurons [RJS17], knowledge neurons [Dai+22], and skill neurons [Wan+22]. Moreover, it was found that the LLMs can encode multiple features in single neuron [Elh+22a; Elh+22b; Gur+23]; i.e., the neuron can be activated by different concepts. This leads to the hypothesis of superposition, which implies model can represent greater quantity of features compared to the number of neurons. To extract superposition features, [SBM22; Bri+23; Hub+24] train sparse autoencoders to map the neuron activations in LLM to higher-dimensional representation with sparsity. By jointly minimizing the reconstruction loss with the L1 penalty on the high-dimensional representation, the researchers have successfully extracted features that are more interpretable than the original neuron activations. Circuits Instead of identifying individual features, global approach to mechanistic interpretability is to identify the circuits in LLMs [Ola+20; Elh+21]. This approach is motivated by the circuit hypothesis that views LLMs as computation graph that implements their capability through the composition of several subnetworks within the model. Formally speaking, language model is represented as directed acyclic graph = (V, E), where denotes the set of nodes (the MLP layers or attention layers) and denotes the set of edges (the connection between those layers). For specific task, circuit is defined as sub-graph in LLMs that satisfies the following three criteria [Wan+23b; Shi+24]: Faithfulness: The circuit can perform the task as well as the whole model. Completeness: The circuit contains all the nodes used to perform the task. Minimality The circuit doesnt contain nodes irrelevant to the task. Using this approach, researchers at Anthropic have identified the induction head that is crucial for LLMs to execute in-context learning [Ols+22]. An induction head consists of two attention heads that work together to copy previous pattern, for example, if [A][B] appears in the sequence earlier, the induction head will make the Transformer more likely to predict [B] when the model receives input [A] again. Therefore, the induction head can capture the information in the prompt and provide the primary source of in-context learning ability. Similarly, existing research has identified the corresponding circuits in LLMs for various tasks, including indirect object identification [Wan+23b], doc string completion [HJ23], and acronym prediction [GMT24]. Furthermore, to make the circuit identification process automatic for general tasks, [Con+23] proposed an algorithm that greedily removes the edge on the computational graph in topological order. The evaluation of circuit often involves an ablation study that knocks out the circuit, imputes the computation by either zero or the mean value, and then compares the performance of the remaining model to the original model. To make the comparison more precise, [LJ24] proposed an optimal ablation procedure to optimize the performance of the remaining model on various imputation methods. Furthermore, to formalize the circuit evaluation process statistically, [Shi+24] proposed nonparametric hypothesis testing procedure to test the three criteria for valid circuits above. 25 Universality Although mechanistic interpretability has provided systematic approach to investigating how an LLM implements certain task, critical question is that the features and circuits are specific to given LLM model (usually toy model or smaller model such as GPT-2), and it is unclear whether these findings are universal across all models. Investigation into universality has yielded mixed result: [Ols+22; Wan+23b] has discovered similar circuits in multiple LMs, while [CCN23] also found evidence that LMs trained from different initialization may implement different circuits. Therefore, understanding the degrees of universality of mechanistic interpretability remains crucial open problem. We refer interested readers to [Rai+24] for more practical review of mechanistic interpretability in LLMs. Physics of LLMs Except for the general mechanistic interpretability approach described above, another seminal series of works aimed at interpreting the LLM through the physics perspective and discovering the universal law of all LLMs [All24]. Toward this goal, it divides the intelligence of LLM into multiple different dimensions such as structure, reasoning, and knowledge, and then studies each individual dimension through idealized control experiments. In each experiment, the authors manage the data and tweak hyperparameters, such as data quantity, type, difficulty, and format, to determine the impact of each factor on LLM performance and suggest further improvement. Using this approach, extensive experiments and observations have been made on learning hierarchical language structures [AL23a], graduate-level mathematical reasoning [Ye+24b; Ye+24c], knowledge extraction [AL23b], manipulation [AL23c], and capacity [AL24]. Geometric Laws Recent empirical studies have revealed several geometric laws that emerge in deep learning models after training. This line of research was pioneered by [PHD20], which documented pervasive phenomenon called neural collapse in multilayer neural networks. Neural collapse refers to geometric law in which the last-layer features and weights exhibit symmetric structures that favor large margins, an inductive bias with significant implications for interpretability due to its precise geometry [Fan+21; Ji+22; Thr+22]. This law has since been extended to intermediate layers and LLMs [HS23; WP24; HS24]. In the context of LLMs, [HS24] offers new perspective on feature formation by showing that pre-trained LLMs enhance the predictability of contextualized features evenly across all layers, from the first to the last. Future Research. While recent advances have improved LLM interpretability, several critical challenges remain. major challenge is developing systematic methods for editing internal representations to induce desired behaviors, such as reducing hallucinations or improving factuality, without retraining. Instead of relying solely on post-analysis approaches, future work should explore ways to train AI models with interpretability as built-in objective, potentially enabling more transparent and predictable model behavior from the outset. Additionally, enhancing safety through interpretability is essential for mitigating vulnerabilities like adversarial manipulation or harmful content generation. Collaborative efforts between machine learning researchers and statisticians will be key to building more interpretable and reliable LLMs that support safer and more accountable AI systems."
        },
        {
            "title": "4.5 Algorithmic Fairness",
            "content": "Modern LLMs are trained with massive quantities of text collected from the real world, and biases towards certain groups or individuals are inherited from the training data. As consequence, the usage of LLMs may lead to discrimination against certain gender [KDS23], race [An+24], religion [AFZ21], and many other sensitive characteristics [NCR23] in downstream applications. For example, when asked to write reference letters for Kelly (a common female name) and Joseph (a common male name), it was observed [Wan+23a] that ChatGPT tends to describe Kelly as warm and likable person and Joseph as leader and role model, indicating that LLMs tends to follow the gender stereotypes that males are associated with leadership. Statistical Formulation Before applying any mitigation techniques, it is essential to establish statistical formulation for algorithmic fairness in LLMs. Fairness assessments typically consider paired groups, denoted as and B, such as sentences including male and female words, or young and old groups. The primary objective is to evaluate disparities between these groups. 26 Fairness in LLMs can be formulated using two main approaches: outcome-based metrics and probability-based metrics. Outcome-based metrics treat the LLM as an opaque system, assessing fairness based on the scores (or labels) directly associated with the models outcome response. Following [Liu+20a], dialogue model can be represented as function : (cid:55) that maps context to response R. Given measurement function that assigns scalar score to each response R, the model is considered fair for groups and with respect to if: ECTAM (D(C)) = ECTB (D(C)), where TA and TB denote the distribution of the context related to groups and respectively. In contrast, probability-based metrics analyze the behavior of the language model by examining the probabilities it assigns to outputs. For example, the probability of generating profession-related words like engineer or doctor should be similar for male names like Joseph and female names like Kelly. Likewise, prompts related to African Americans should yield comparable rate of toxic adjectives as those related to European Americans. To formalize fairness, let represent specific set of sensitive words (e.g., high-paying professions, toxic adjectives), denote the collection of all such sets, and = {(Ai, Bi) = 1, 2, . . . I} define the paired groups of interest. fairness criterion, following [ZRZ24], can be expressed as: P(o(x) A) P(o(x) B) α, U, (A, B) G, (11) where the probability is taken over o(x) (x), with the next-token output o(x) drawn from the language model (x). To address these fairness issues, many bias mitigation techniques have been studied in different stages of the LLM workflow, including data pre-processing, model training (in-processing), and postprocessing [Gal+23]. Pre-Processing In the pre-processing stage, the general methodology is to break the imbalance between different groups in the training data. Under this category, typical approaches include data augmentation via alternating group-imbalanced words [Lu+20], subset selection [GMA22], instance reweighting [HBC22]. Collectively, these pre-processing strategies align the data with fairness objectives, enabling LLMs to learn representations that are less prone to perpetuating societal biases. In-Processing In the model training stage, bias can be mitigated by leveraging some fairness metrics into the training objective. common approach is to add fairness metric into the loss function as In particular, family of distance-based metrics is used to shorten the distance of regularizer. different groups in the embedding space, such as ℓ2 distance between sentence embeddings [Liu+20a], cosine similarity [Hua+20] and Jensen-Shannon divergence [Yan+23; Woo+23]. Besides distancebased metrics, another line of work attempts to design metrics that can disentangle the relationship between embeddings and the group identity. Towards this goal, [BB19; KB21; Par+23] proposes to encourage the orthogonality via minimizing the projection length of neutral word (e.g., leadership) embeddings onto gender embeddings, and [CPC21; WCH23] encourage independence between learned word embeddings and gender via minimizing the mutual information. In addition to adding fairness metrics as regularization terms, one may also change the training objective to mitigate bias. Along this line, [Xia+24d] identifies an inherent algorithmic bias emerging from RLHF in the post-training phase of LLMs, which significantly amplifies majority preferences while diminishing minority preferences. To resolve this fairness concern, [Xia+24d] introduces preference matching RLHF, which incorporates an additional regularization term in the reward objective. [Che+21; He+22a; Oh+22] applies contrastive learning with counterfactual data augmentation to maximize the similarity between the original sentences and their counterfactual analogs. [ZLM18; Jin+21; HBC21] use an adversarial training framework, where they train an attacker to predict the protected attribute from the embeddings of the encoder, and an encoder to prevent the attacker from identi- [Ouy+22; Bai+22] utilize reinforcement learning with feedback from fying the protected attribute. human or machine learning models to encourage the model generate fair texts. [Nak+24] establishes theoretical framework of leveraging LLMs to artificially increase the sample size of underrepresented classes in imbalanced classification and spurious correlation problems. 27 Post-Processing After the model is trained, one may enforce fairness by modifying the generating distribution. direct approach is to constrain the distribution to generate texts with only unbiased words and phrases, for example, [Geh+20; Rol+21] forbid the use of toxic words according to list, [SSB22] search for outputs in the distribution with different genders to the highest likelihood output, [Shu+22; Sch+22a] use another ML model to identify the harmful words and replace them with neutral words. An alternative approach is to edit the next-token probability. For example [Geh+20; Kim+23; LKW23] reweight the token probability in the generatve distribution according to fairness score learned from an evaluation model, and [Liu+21; Hal+23] separately train smaller expert and antiexpert models specialized in generating non-toxic and toxic texts, and then in the inference stage, tokens with higher probability in the expert model are up-weighted and tokens with lower probability in the anti-expert model are down-weighted. There are also recent studies that apply multi-group fairness notions in LLMs. [ZRZ24] introduces framework for post-processing machine learning models so that their predictions satisfy multi-group fairness guarantees, and applies this framework to achieve the notion (11). Future Research While significant progress has been made in mitigating algorithmic biases in LLMs, several key challenges and open problems remain. One fundamental issue is the trade-off between fairness and utility. Bias mitigation techniques often come at the cost of reduced model performance, particularly in nuanced real-world applications. Future research should explore principled ways to balance fairness constraints with overall model accuracy, potentially by developing adaptive or task-specific fairness constraints that optimize both fairness and utility. Another critical challenge is context-dependent fairness. Existing fairness metrics and debiasing strategies typically rely on predefined sensitive attributes such as gender or race. However, fairness concerns can be highly context-dependent, varying across applications, languages, and cultural perspectives. promising direction is to develop dynamic, context-aware fairness measures that adapt to different scenarios, ensuring that bias mitigation strategies remain relevant across diverse settings. Furthermore, fairness in multi-modal and interactive AI systems remains an understudied area. Many real-world applications involve interactions between text, images, and structured data, and biases may manifest differently across modalities. Research should focus on extending fairness-aware learning techniques to multimodal LLMs and conversational AI systems to mitigate biases in more complex settings. Additionally, the long-term impact of fairness interventions requires further investigation. Most current evaluations assess bias mitigation based on short-term performance metrics, but biases may re-emerge as models are fine-tuned, updated, or interact with users over time. Developing robust longitudinal studies to assess the durability of fairness interventions will be crucial for ensuring sustainable bias mitigation. Finally, scalability and computational efficiency of fairness interventions remain practical concerns. Many existing approaches, particularly in-processing techniques that modify training objectives, introduce significant computational overhead, making them challenging to implement for large-scale LLMs. Future work should explore lightweight debiasing techniques that maintain fairness guarantees while remaining computationally feasible."
        },
        {
            "title": "4.6 LLM Alignment: A Statistical Perspective",
            "content": "Alignment with RLHF and Preference Optimization Alignment is crucial step in the language model training pipeline, as it aims to ensure that the model generations align with human preferences. Existing alignment procedures are mostly implemented via reinforcement learning from human feedback (RLHF), which attempts to learn reward function based on the pairwise human preference data and learn the corresponding optimal policy. Denote the space of states, the space of actions, and r(s, a) the reward function of taking action at state S, the RL process seeks policy π(s) that maximizes the reward Es[(cid:80)H h=1 r(sh, π(sh)] from step 1 to H. In the context of language model alignment, the state and action correspond to the input prompts and model responses, and the reward function represents the human satisfaction of the model response given the prompt. Under this framework, [ZJJ23] used the Bradley-Terry-Luce model to generate offline human preference data and showed that variant of MLE can achieve near-optimal sample complexity under linear reward function, result that was later generalized to general reward functions [Zha+24a], partially observed reward [Kau+24], and multi-party reward [Zho+24a] settings. Compared to the standard RL, RLHF uses preference data that contains less information compared to the reward, 28 therefore it is often considered harder task. However, [WLJ23] showed that for wide range of preference models, algorithms from traditional reward-based RL can solve the RLHF task with low extra complexity. Self-alignment On the theoretical front, [Ger+24; DD24] analyzed the phenomenon of model collapse on range of statistical models, including linear regression, generalized linear models, and nonparametric regression, and found that the estimation error grows linearly in the setting where we replace the original data with the synthetic data in each iteration, and converges to certain constant when one keeps augmenting the original data with synthetic data. Recent work [DFK24] shows that while increasing model size can mitigate model degradation when training with synthetic data, mixing synthetic data with real human data cannot fully eliminate the model collapse issue."
        },
        {
            "title": "5.1 Data Collection",
            "content": "Recent developments have yielded dramatic improvements on variety of specific NLP tasks, including summarization, reasoning and extraction tasks. Such abilities enable using powerful LLMs for zero-shot or few-shot text mining, and extracting structured data from unstructured texts. 5.1.1 Extract Structured Data from Unstructured text One prominent application of LLMs is sentiment analysis (SA), long-standing text mining task which aims to extract human sentiments from contextual data. These extracted sentiments can then serve as inputs for downstream statistical models, such as predictive models for Twitter engagement, election outcomes [Alv+23], and other social phenomena. Beyond sentiment analysis, LLMs have shown remarkable effectiveness in extracting structured data from complex, unstructured sources such as electronic health records and other medical datasets. For example, recent work [Tan+23] demonstrated that fine-tuning on large volume of synthetic Biomedical Named Entity Recognition (NER) and Biomedical Relation Extraction (RE) data generated by ChatGPT significantly improved performance. Specifically, for biomedical NER tasks (e.g., NCBI Disease, BC5CDR Disease, BC5CDR Chemical), the F1-score improved from 23.37% to 63.99%. Similarly, for RE tasks (e.g., GAD, EU-ADR), the F1-score increased from 75.86% to 83.59%. These results illustrate the potential of LLMs to transform data extraction workflows and improve statistical analysis pipelines in high-stakes domains like healthcare. 5.1.2 Synthetic Data Generation The rise of the research fields of super-human alignment as well as self-rewarding are due to the expensive costs of human-written instructions and annotations that are limited in quantity, diversity, creativity, and more efforts have shifted to synthetic data generation using powerful language models. Recent work [Wan+23e] has shown the ability of language models to self-improve by prompting models to generate natural instructions at human level that are later used for boostrapping. With development of synthetic text generation tasks, adaptation to generating realistic statistical data has become more popular in recent years. Tabular Data The generation of realistic synthetic tabular data has received considerable attention in recent years. Tabular data collection is often expensive and fraught with challenges, including class imbalances, long-tailed label distributions [Cao+19], privacy concerns that restrict data sharing [Gas+16], and data impurities such as noise or missing values [LT20]. Synthetic data generation provides practical solution to these challenges, enabling the development of robust statistical models while addressing privacy and class imbalance issues [Cho+17; Bor+22]. For instance, [Bor+23] introduced synthetic tabular data generation pipeline using an auto-regressive generative LLM. They showed that discriminative models trained on synthetic tabular data outperformed competitors trained on real data. Additionally, [Nak+24] proposed leveraging synthetic tabular data to address imbalanced classification and spurious correlation challenges, demonstrating the potential of LLMs to improve statistical workflows even in complex and resource-constrained settings. 29 Financial Data Rational expectations remains the dominant model of beliefs in much of macroeconomics and finance and in recent years the use of surveys to tie beliefs to observable data has emerged as prominent approach. [Byb23] proposed an alternative method of generating beliefs using LLMs and evaluate how well generated expectations of the stock market match the expectations of the American Association of Individual Investors (AAII) survey and Duke CFO Survey."
        },
        {
            "title": "5.2 Data Cleaning",
            "content": "LLM as coder for tabular data cleaning LLMs can automatically generate code for cleaning, preprocessing, and transforming raw data, saving data scientists considerable time and effort. For example [Tu+24] described how ChatGPT with Code Interpreter can assist statisticians with checking for missing/null values, removing duplicate rows if necessary, checking for inconsistent/invalid values, converting categorical columns to numerical representations. LLM for automatic feature engineering Feature engineering refers to the process of building suitable features from raw data. Given input data {(X, )}, feature engineering seeks transformation ϕ(x) to maximize the prediction accuracy using {(ϕ(X), ))}. Although modern machine learning models are capable of learning wide variety of complex relationships, feature engineering is still crucial when the data is limited and the prediction model is simple, and it often relies on domain knowledge. With the use of LLMs, it is possible to automatically construct features using the domain knowledge embedded in it. For example, [HMH24] prompted LLM with contextual information about the datasets, sample of data, and several few-shot examples of useful feature engineering, and then asked LLM to create new feature. [Han+24a] prompted LLM to generate explicit decision rules to solve classification task and then convert those rules into binary features for fitting the prediction model. LLM as judge for scalable text data filtering It is standard practice to clean up text before feeding it into any kind of machine learning algorithm. Whether one is doing pre-training or finetuning, cleaning data before training helps ensure accuracy and improving text quality. To ensure Llama 3 is trained on data of high quality, series of data-filtering pipelines are applied to pre-training dataset including using heuristic filters, NSFW filters, semantic deduplication approaches, and text classifiers to predict data quality. Besides standard heuristics such as filtering emojis, html tags, xml parsing, lower casing and other text standardization, more recently, LLMs have been employed to assign scores or rankings to text data for filtering high-quality text data for training. One prominent approach is to further fine-tune LLMs as text classifiers. For example, Llama 2 is employed to generate the training data for the text-quality classifiers that are powering Llama 3. Other approaches such as BartScore, directly apply BART, an encoder-decoder pre-trained language model, to texts for evaluating its fluency, factuality, informativeness in an unsupervised fashion. Recent reward scoring methods such as DPO reward also use LLM-predicted logits on texts to judge which one is better among pair of texts. More recently, with emergent abilities of LLMs such as Chain-of-Thoughts(CoT) [Wei+22a] and In-Context Learning (ICL) [Bro+20], have been employed as scalable and explainable way of approximating human judgments which are otherwise expensive to obtain. [Zhe+23] reveal that strong LLM judges such as GPT-4 can match the quality of human-level preference, achieving over 80% agreement with human annotations on two challenging benchmarks consisted of open-ended questions, without finetuning on domain-specific tasks. [Yua+24] can employ LLM-as-judge prompting to score its own training data for filtering and such self-reward pipeline during iterative training can yield model that outperforms many existing state-of-the-art (SOTA) models including Claude 2, Gemini Pro and GPT-4 0613 on prominent NLP leaderboards."
        },
        {
            "title": "5.3 LLMs for Data Analysis",
            "content": "There is evidence that LLMs can perform fundamental mathematical reasoning on graduate-level mathematical tasks. However little prior work has evaluated the reasoning capabilities of LLMs performing statistical analysis, especially on real data. [Liu+24d] reveals that even the strongest models can struggle in data analysis. For example GPT-4 can only achieve 58% accuracy on basic statistical reasoning tasks such as What is 95% confidence interval? 30 Tool Usage For Quantitative Analysis In practice people have shown that ChatGPT can be fed directly with tabular data to perform direct table analysis such as aggregation, averaging etc, and generate SQL statements capable of filtering, sorting, aggregation and summation logics, and execute SQL queries on given database it reads from the prompt. Statistical Analysis for Text Data Text data has long been rich source for statistical analysis, with methods like topic modeling providing interpretable summaries of large corpora. Traditional models such as Latent Dirichlet Allocation (LDA) analyze word co-occurrence patterns to identify latent themes within text corpus, offering probabilistic representation of topics [BNJ03; Ke+23; KW24; WZT23]. These approaches have been widely used to uncover hidden structures in text data, serving as foundation for many downstream applications. With the advent of LLMs, more advanced methods have emerged for extracting structured data from unstructured text in scalable manner, enabling further statistical analysis. promising avenue for such investigations has been the use of word embeddingsa family of techniques that conceive of meaning as emerging from the distribution of words that surround term in text. By integrating text embeddings into statistical models, the model can leverage contextual information and enhance the regression analysis with richer representation of the input text. In nutshell, the method takes contextural embeddings which have been pre-trained on large corpora such as BERT, Llama or GPT, then this requires only simple linear transformation of the averaged embeddings to conduct further statistical modeling such as regression [RSS23]. Such use of word embeddings for prediction has been heavily employed in classification tasks [SK21] to improve model prediction accuracy. Another useful feature of word embeddings is to capture the semantic relationship between words and texts. Statistical Inference with LLM Annotation In many statistical analysis, such as social surveys, human annotation is slow and expensive to obtain. Meanwhile, LLMs can simulate human annotations efficiently with very low cost, but these annotations can be potentially biased and fail to provide valid statistical analysis. Motivated by this dilemma, [Ang+23; ADZ23] proposed PredictionPowered Inference (PPI), general framework that allows researchers to utilize predictions from any black-box machine learning model and perform valid statistical inference, such as computing p-values and confidence intervals. Assume that the researcher collects human annotation on small dataset (Xi, Yi)n i=n+1 from the same distribution. In addition, pre-trained model is available to provide predictions ˆYi = (Xi). The target is to estimate the parameter θ = arg minθΘ E[ℓθ(X, )], such as sample mean or regression coefficient. The PPI estimator [Ang+23; ADZ23; JLZ25] can be written as: i=1 and observes large unannotated dataset (Xi)n+N ˆθPPI = arg min θ 1 (cid:88) i=1 ℓθ(Wi, Yi) 1 (cid:88) i=1 ℓθ(Wi, (Xi)) + 1 n+N (cid:88) i=n+1 ℓθ(Wi, (Xi)). (12) Compared to using human annotation only or naively using machine learning predictions as gold standard labels, the PPI estimator can effectively leverage the information from predictions while preserving statistical validity (e.g., coverage). It has been successfully applied to assist various computational social science studies with annotations generated by LLMs, such as detecting hate speech [Ega+24] and studying the bias in media outlets [Gli+24]. In addition, [JMS24] underscores the potential of scaling laws to guide the effective integration of LLMs-generated surrogate data, addressing critical bottleneck in scenarios where collecting high-quality data is expensive or impractical. The discovery that surrogate data can significantly reduce test error, even when unrelated to the original dataset, raises questions about the limits of this approach and its implications for generalization."
        },
        {
            "title": "5.4 LLM-assisted Medical Research",
            "content": "Today, we have access to more health data than ever, with the potential to revolutionize modern medicine through LLMs. There are many diseases that are difficult to diagnose and cause considerable amounts of damage to the body before discovery [Ram+17]. Many people in the world do not have access to quality medical information or healthcare [BO17]. LLMs can process and analyze large volumes of data quickly, providing healthcare professionals and patients with valuable insights and potential diagnoses. Patterns that may be unobservable or difficult to differentiate by humans may 31 get picked up by an LLM, and they can provide consistent information that is up to date with the latest research and data [Nav+23]. This has the potential to improve diagnostic support and improve accessibility of medical question-answering. Synthetic medical data generated by LLMs can increase data accessibility and advance medical AI research. It is important to keep in mind, however, that there are considerable risks and considerations when it comes to the use of LLMs in medicinebias, hallucinations, ethical violations, lack of accountability, and lack transparency, to name few. Here, we review some important and recent advances in large language models for medicine. For more comprehensive review, see Zhou et al., 2024 [Zho+24b]. Early Transformer-Based Clincial Language Models There are variety of recent language models that are pre-trained on vast amounts of medical data and fine-tuned for specific tasks within medicine and healthcare. The most well-known early example of pre-training and fine-tuning Transformer-based model on clinical data is Clinical-BERT [Als+19]. Using the same encoder architecture as BERT, the authors train the model on clinical notes from ICU admissions. As result, ClinicalBERT is able to uncover semantic relationships between medical concepts. ClinicalBERT is widely used in variety of clinical AI workflows to produce embeddings for medical concepts in EHRs data [YW21; SLG20; BDP20; Kan+23]. LLMs for Clinical Text Generation With the significant advances in generative AI models, medical and clinical large language models were developed for synthetic medical data generation, and fine-tuned for variety of natural language processing tasks. It is difficult to access and share large-scale clinical text and clinical LLMs due to concerns in privacy and maintaining HIPAA compliance. Kweon et al., 2023 developed Asclepius, the first generative shareable clinical large language model [Kwe+23]. There are versions based on the Llama2-7B and Llama2-13B architectures, and versions based on the Llama3-8B and Mistral-7B architectures. The models generate synthetic clinical notes based on anonymized case reports from PubMed Central. Peng et al., 2023 developed GatortronGPT, which uses 277 billion words of text from 126 clinical departments and 2 million patients at the University of Florida Health in conjunction with 195 billion words of diverse English text [Pen+23]. The authors then generate 20 billion words of synthetic text to train synthetic NLP models that accomplish tasks such as clinical concept extraction, event relations, semantic similarity, natural language inference, and question answering. LLMs for Medical Question-Answering There are also variety of large language models specifically constructed for medical question-answering, which can democratize medical knowledge, increase access to underserved populations, and potentially reduce physician burden by improving efficiency. [Sin+23] developed Med-PALM2trained using the PaLM2 architecture and targeting medical domain-specific fine-tuning [Sin+23]. The model incorporates instruction fine-tuning applied to multiple question-answer datasets. The authors train unified model optimized for performance across all datasets using dataset mixture ratios. The model also incorporates ensemble refinement, where the model produces multiple generations via temperature sampling, and is then conditioned on the generations from the previous step to produce refined explanation and answer. This step is performed multiple times, and the final answer is then determined by plurality vote. Han et al., 2023 developed MedAlpaca based on the Llama architecture and trained on various question-answer pairs from medical flash cards, StackExchange, and Wikidoc [Han+23]. Med-PALM2 is closed source model, whereas MedAlpaca is open source. Xie et al., 2024 developed the Me-LLaMa family of open-source foundation large language models for medical application, which combines biomedical papers, clinical notes, and general domain data [Xie+24a]. Me-LLaMa outperformed ChatGPT on many medical datasets. LLMs for the Identification of Social Determinants of Health There have been recent efforts in the use of LLMs to identify social determinants of health (SDoH). SDoH are the conditions in which people are born, grow, live, work, and age that significantly influence health outcomes. Some examples can include economic stability, education, social and community context, neighborhood, and food accessibility. Identifying SDoH could provide more holistic view of patients health, and help stratify patients for more targeted prevention and resources. Guevara et al., 2024 developed models to extract SDoH by applying existing LLMs (Flan T-5, GPT 3.5, GPT 4) and fine-tuning with LLMgenerated synthetic SDoH data [Gue+24]. For fine-tuning, LLMs were prompted to generate new 32 sentences for SDoH categories using annotation guidelines as reference. Sample synthetic sentences were taken as reference to generate more synthetic sentences. The models identified 93.8 percent of patients with adverse SDoH, compared to 2 percent ICD-10, thus showing great promise for the use of LLMs in identifying SDoH. Multimodal LLMs in Medicine Recently, in addition to textual input, large language models have been adapted to handle other types of data such as images, charts, screenshots, pdf, video, audio, etc. For example, for the incorporation of medical images in multimodal medical LLMs, vision Transformers (ViT) are often integrated to handle the image input. ViT produces vector representations of the image, which are concatenated with text representations to form single sequence. LMMs such as FlamingoCXR [Moo+23] and Med-PaLM [Li+24a] are comparable with radiologists in controlled settings for generating radiology reports [Hua+23b]. Googles Gemini models are new generation of multimodal models with novel capabilities. Med-Gemini in particular incorporates multimodal capabilities and the ability to integrate the use of web search, resulting in state of the art performance on complex diagnostic challenges, image challenges, video question answering and text summarization [Ani+23]. Challenges in LLM adoption in Healthcare While LLMs have been rapidly developed for research in healthcare and medicine, several daunting challenges arise that limit their use in many high-stakes applications. These challenges and issues include algorithmic biases, poor calibration, hallucinations, misinformation, and response arbitrariness [San+23; Sim+24; Gao+24]. Due to the unique nature of medical data and the critical need for safety, these models require specialized prompting, fine-tuning, and alignment to ensure they are truly useful and reliable in real-world clinical settings. For example, LLMs demonstrate suboptimal clinical reasoning under uncertainty [Omi+23]. LLMs also have been known to exhibit significant bias [San+23; Cas+23; Aza+24; Cha+24], which can lead to fairness issues, particularly harming underrepresented minority individuals when clinicians rely on LLMs for decision support [PFB24; Ayo+24]. Researchers have cautioned against adopting LLMs in healthcare until these challenges are mitigated [Szo24; FDW24; Tes+24]."
        },
        {
            "title": "6.1 Statisticians’ Small Language Models",
            "content": "Statisticians, often operating with fewer computational resources compared to their computer science counterparts, bring distinctive strength to the development of language models: the ability to design resource-efficient, theoretically grounded models that leverage statistical principles. While the computer science community has focused on scaling LLMs through sheer data size and computational power, statisticians excel in developing small language models (SLMs) that achieve competitive performance in particular domains by emphasizing efficiency and structure over brute force. Statisticians advantage also lies in their expertise in embedding domain knowledge into model design. Techniques such as sparsity assumptions, hierarchical modeling, and structured regularization allow SLMs to achieve more sample efficiency. For instance, sparsity-inducing priors can focus the models capacity on relevant features, reducing the need for excessive data while improving interpretability. Similarly, hierarchical Bayesian approaches can incorporate multi-level dependencies in text data, offering principled way to share information across contexts and reducing the effective sample size needed for training. These methods can outperform large models in scenarios with limited data, noisy observations, or constrained resources, such as personalized medicine, legal text analysis, or financial modeling. SLMs also align with statisticians focus on interpretability. By prioritizing simpler architectures and explicit assumptions, SLMs are inherently more transparent, allowing researchers and practitioners to trace the models predictions back to specific features or interactions. This interpretability is especially critical in high-stakes domains where trust and accountability are paramount. In contrast, LLMs, while powerful, often function as black boxes, limiting their applicability in settings where explanation is necessary. Moreover, statisticians are well-positioned to cooperate with computer scientists in tackling the computational challenges associated with modern AI. Techniques that bridge statistics and computer science, like low-rank approximation, variational inference, and penalized optimization, can be employed to reduce the computational footprint of training and inference. Applied 33 to SLMs, these approaches make the models not only more accessible but also environmentally sustainable, addressing concerns about the carbon footprint of large-scale AI systems. By focusing on principled model design, statisticians can create models that require less computational power while maintaining robustness and adaptability. Statisticians SLMs are not intended to replace LLMs but to complement them. While LLMs excel at broad, open-ended tasks, SLMs can specialize in domain-specific applications, delivering competitive performance with far fewer resources. By leaning into their strengths in theory-driven modeling, sample efficiency, and interpretability, statisticians can carve out unique and impactful role in the rapidly evolving AI landscape. This approach not only democratizes access to advanced language modeling capabilities but also ensures that AI development is aligned with the values of efficiency transparency, and sustainability. Statisticians also have an important role to play in the design of wrappers that surround blackbox LLMsperforming roles such as calibration, uncertainty quantification, and debiasing on the LLM outputs. The Prediction-Powered Inference (PPI) estimator referred to earlier is an instance of such wrapper. Causal inference methods can profit from black-box LLMs if properly deployed around the LLMs. Experimental design methods from statistics remain relevant in the LLM era. Thus, statisticians can both provide alternatives to LLMs and augmentations of LLMs in addition to contributing to theoretical analysis of LLMs."
        },
        {
            "title": "6.2 Towards Understanding LLMs",
            "content": "The remarkable success of LLMs has inspired growing efforts to understand their inner workings, often using tools from statistical learning theory. Initial research has made strides by examining LLMs through the lenses of approximation, optimization, generalization, and prompting mechanisms. For instance, studies have demonstrated the universal approximation properties of Transformers across various architectures [Yun+20a; Yun+20b; KS24], their computational expressiveness as Turing-complete systems [PMB19; WCM22], and their superior optimization dynamics compared to MLPs in specific tasks [Wan+24d]. On generalization, researchers have refined bounds on sequence length [Zha+22; TT24] and extended them to non-i.i.d. settings such as time-series data [Lim+24]. Prompting techniques, such as in-context learning and chain-of-thought prompting, have further advanced our understanding of LLM adaptability and reasoning. Theoretical work interprets ICL as implicit Bayesian inference arising from pre-training distributions like hidden Markov models [Xie+22] and explores its capacity to approximate diverse algorithms, including gradient descent, Newtons method, and reinforcement learning [Aky+22; Von+23; Fu+23; Bai+24; LBM24; ZFB24; Guo+24c]. CoT prompting enhances reasoning by decomposing tasks into sequential steps, improving both accuracy and interpretability [Wei+22a; Yao+23]. For example, CoT has been shown to increase the approximation power of Transformers in arithmetic and equation-solving tasks [Fen+24a] and promote sparse attention patterns that simplify learning [Wen+24]. Furthermore, many pre-trained LLMs exhibit the attention-sink phenomenon [Xia+24c], where certain sink tokens receive disproportionately high [Guo+24b] analyzed this phenomenon using simplified models and attributed it attention weights. primarily to the softmax operation in attention heads. Finally, the pre-training of LLMs has found to follow an empirical scaling law, in which the test error improves polynomially with the model size and sample size. [Lin+24a] theoretically derived the scaling law in the context of infinite-dimensional linear regression, aligning with observed empirical scaling laws. Despite these advances, our understanding of LLMs remains incomplete. Traditional statistical frameworks, while valuable, often fall short of explaining emergent phenomena like zero-shot generalization, in-context learning, and CoT reasoning, which arise from intricate interactions between model architecture, training data, and optimization dynamics. For instance, while ICL can be partially explained through algorithm approximation and Bayesian inference, its ability to adapt flexibly across diverse tasks without parameter updates remains challenging puzzle. Similarly, CoTs capacity to enhance reasoning through structured task decomposition highlights gaps in our theoretical understanding of how attention mechanisms drive complex problem-solving. These behaviors underscore the need for new frameworks that integrate insights from statistical learning theory, optimization, and cognitive science to fully capture the nuanced capabilities of LLMs. Addressing these gaps presents significant opportunity for statisticians to contribute by developing innovative theories and methodologies, ultimately bridging the divide between empirical successes and foundational understanding of these transformative technologies."
        },
        {
            "title": "6.3 Human-AI Collaborative Data Science",
            "content": "As LLMs continue to advance, an important future direction lies in fostering effective human-AI collaboration, where AI systems complement rather than replace human expertise. While LLMs demonstrate remarkable capabilities in reasoning, text generation, and decision support, they lack deep understanding, contextual awareness, and accountabilityfactors that are crucial in high-stakes applications such as medicine, law, and scientific research. Thus, rather than viewing AI as replacement for human intelligence, more promising approach is to develop frameworks that integrate human expertise with AI-driven insights to achieve superior outcomes. One promising avenue for human-AI collaboration is interactive decision-making, where LLMs provide recommendations, assist with exploratory data analysis, or generate potential solutions, while human users retain final control and oversight. For example, in statistical modeling, LLMs can automate tedious tasks such as data preprocessing, feature engineering, and model selection, while human analysts focus on domain-specific reasoning, hypothesis testing, and interpreting results. Similarly, in medical applications, AI can assist doctors by summarizing patient records or suggesting potential diagnoses, but the final decision remains with human experts, ensuring accountability and ethical considerations. However, interactive decision-making introduces new statistical challenges, particularly due to the non-i.i.d. nature of data. In traditional statistical and machine learning settings, models are typically trained on independently and identically distributed (i.i.d.) data. However, in human-AI collaboration, the data distribution adapts dynamically based on prior AI suggestions and human feedback. This feedback loop can introduce selection bias, concept drift, and strategic adaptation, where human decision-making patterns shift based on AI recommendations. For example, if recommendation system in hiring disproportionately suggests candidates from particular demographic, decision-makers may adjust their selection patterns accordingly, reinforcing feedback loops and exacerbating bias. Standard statistical tools that assume fixed distributions become inadequate in such settings, necessitating the development of adaptive inference methods, causal modeling techniques, and robust statistical frameworks that can account for evolving data distributions. See [Per+20; Gar+24; GC24] for more discussions of these issues and further pointers. Another key research direction is designing adaptive AI systems that can dynamically adjust to user expertise and preferences. Current LLMs operate largely as static models, generating responses based solely on input prompts. However, effective collaboration requires AI systems that learn from user feedback, refine their responses over time, and personalize their assistance based on the expertise level of the user. This could involve techniques such as reinforcement learning from human feedback (RLHF), uncertainty-aware AI models that defer decisions to humans when confidence is low, or interactive AI systems that engage users in dialogue to refine understanding and reduce ambiguity. Addressing these challenges requires multidisciplinary effort, bringing together statisticians, computer scientists, social scientists, legal scholars and policymakers to design fair, transparent, and accountable AI systems. By integrating rigorous statistical frameworks with advances in LLMs, the community can work to ensure that human welfare remains central consideration in the development of next-generation AI models."
        },
        {
            "title": "References",
            "content": "[Aar23] [AB23] Scott Aaronson. Watermarking of large language models. In: Workshop on Large Language Models and Transformers, Simons Institute, UC Berkeley. 2023. Anastasios Angelopoulos and Stephen Bates. Conformal prediction: gentle introduction. In: Foundations and Trends in Machine Learning 16.4 (2023), pp. 494591. [Aba+16] Martin Abadi et al. Deep learning with differential privacy. In: Proceedings of the 2016 ACM SIGSAC conference on computer and communications security. 2016, pp. 308318. [Abd+21] Moloud Abdar et al. review of uncertainty quantification in deep learning: Techniques, applications and challenges. In: Information fusion 76 (2021), pp. 243297. [ADZ23] Anastasios Angelopoulos, John Duchi, and Tijana Zrnic. PPI++: Efficient predictionpowered inference. In: arXiv preprint arXiv:2311.01453 (2023). 35 [AFZ21] Abubakar Abid, Maheen Farooqi, and James Zou. Persistent anti-muslim bias in large language models. In: Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. 2021, pp. 298306. [Aga+24] Rishabh Agarwal et al. Many-shot in-context learning. In: Advances in Neural Information Processing Systems. Vol. 37. Curran Associates, Inc., 2024, pp. 7693076966. [Ahm+24] Arash Ahmadian et al. Back to basics: Revisiting REINFORCE-style optimization for learning from human feedback in LLMs. In: Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2024, pp. 12248 12267. [Aky+22] [AL23a] [AL23b] [AL23c] [AL24] [All24] [Als+19] [Alv+23] [AM23] [Ami07] [An+24] [Ang+23] [Ani+22] [Ani+23] [Ary+24] [Ask+21] [Ayo+24] [Aza+24] Ekin Akyurek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. In: arXiv preprint arXiv:2211.15661 (2022). Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 1, context-free grammar. In: arXiv preprint arXiv:2305.13673 (2023). Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.1, knowledge storage and extraction. In: arXiv preprint arXiv:2309.14316 (2023). Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.2, knowledge manipulation. In: arXiv preprint arXiv:2309.14402 (2023). Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.3, knowledge capacity scaling laws. In: arXiv preprint arXiv:2404.05405 (2024). Zeyuan Allen-Zhu. ICML 2024 tutorial: Physics of language models. Project page: https: //physics.allen-zhu.com/. July 2024. Emily Alsentzer et al. Publicly available clinical BERT embeddings. In: Proceedings of the 2nd Clinical Natural Language Processing Workshop (2019), pp. 7278. Quratulain Alvi et al. On the frontiers of Twitter data and sentiment analysis in election prediction: review. In: PeerJ Computer Science 9 (2023), e1517. Amos Azaria and Tom Mitchell. The internal state of an LLM knows when its lying. In: arXiv preprint arXiv:2304.13734 (2023). Ali Amiri. Dare to share: Protecting sensitive knowledge with data sanitization. In: Decision Support Systems 43.1 (2007), pp. 181191. Jiafu An, Difang Huang, Chen Lin, and Mingzhu Tai. Measuring gender and racial biases in large language models. In: arXiv preprint arXiv:2403.15281 (2024). Anastasios Angelopoulos, Stephen Bates, Clara Fannjiang, Michael Jordan, and Tijana Zrnic. Prediction-powered inference. In: Science 382.6671 (2023), pp. 669674. Rohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar, and Pasin Manurangsi. Largescale differentially private BERT. In: Findings of the Association for Computational Linguistics: EMNLP. 2022, pp. 64816491. Rohan Anil et al. Gemini: family of highly capable multimodal models. In: arXiv preprint arXiv:2312.11805 (Dec. 2023). Viraat Aryabumi et al. To code, or not to code? Exploring impact of code in pretraining. In: arXiv preprint arXiv:2408.10914 (2024). Amanda Askell et al. general language assistant as laboratory for alignment. In: arXiv preprint arXiv:2112.00861 (2021). Noel F. Ayoub et al. Inherent bias in large language models: random sampling analysis. In: Mayo Clinic Proceedings: Digital Health 2.2 (2024), pp. 186191. Mohammad Gheshlaghi Azar et al. general theoretical paradigm to understand learning from human preferences. In: International Conference on Artificial Intelligence and Statistics. PMLR. 2024, pp. 44474455. [Bai+09] Bing Bai et al. Supervised semantic indexing. In: Proceedings of the 18th ACM conference on Information and knowledge management. 2009, pp. 187196. 36 [Bai+22] [Bai+24] [Bak+24] Yuntao Bai et al. Constitutional AI: Harmlessness from AI feedback. In: arXiv preprint arXiv:2212.08073 (2022). Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. In: Advances in neural information processing systems 36 (2024). Yavuz Faruk Bakman et al. MARS: Meaning-aware response scoring for uncertainty estimation in generative LLMs. In: Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL. 2024, pp. 7752 7767. [Ban+20] Marta Banon et al. ParaCrawl: Web-scale acquisition of parallel corpora. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020, pp. 45554567. [BB19] [BCB15] [BDP20] [Ben+03] [Bes+24] [Bi+24] [BKH16] [BL24] Shikha Bordia and Samuel R. Bowman. Identifying and reducing gender bias in wordlevel language models. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Student Research Workshop. 2019, pp. 715. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In: 3rd International Conference on Learning Representations, ICLR. 2015. Brent Biseda, Gaurav Desai, and A. Philip. Prediction of ICD codes with clinical BERT embeddings and text augmentation with label balancing using MIMIC-III. In: arXiv preprint arXiv:2008.08980 (2020). Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Janvin. neural probabilistic language model. In: J. Mach. Learn. Res. 3.null (2003), pp. 11371155. issn: 1532-4435. Maciej Besta et al. Graph of thoughts: Solving elaborate problems with large language models. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. 16. 2024, pp. 1768217690. Xiao Bi et al. DeepSeek LLM: Scaling open-source language models with longtermism. In: arXiv preprint arXiv:2401.02954 (2024). Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. In: arXiv preprint arXiv:1607.06450 (2016). Amrita Bhattacharjee and Huan Liu. Fighting fire with fire: Can ChatGPT detect AIgenerated text? In: SIGKDD Explor. Newsl. 25.2 (Mar. 2024), pp. 1421. issn: 19310145. [BMW20] Andreas Balzer, David Mowatt, and Muiris Woulfe. Obfuscating information related to personally identifiable information (PII). US Patent 10,839,104. Nov. 2020. [BNJ03] [BO17] David Blei, Andrew Ng, and Michael Jordan. Latent dirichlet allocation. In: Journal of machine Learning research 3.Jan (2003), pp. 9931022. World Bank and World Health Organization. World Bank and WHO: Half the world lacks access to essential health services, 100 million still pushed into extreme poverty because of health expenses. In: (2017). Accessed: 2024-09-18. url: https : / / www . worldbank . org / en / news / press - release / 2017 / 12 / 13 / world - bank - who - half - worldlacksaccessessential- healthservices100million- pushedintoextreme-poverty-because-of-health-expenses. [Bom+22] Rishi Bommasani et al. On the opportunities and risks of foundation models. In: arXiv preprint arXiv:2108.07258 (2022). [Bor+22] [Bor+23] Vadim Borisov et al. Deep neural networks and tabular data: survey. In: IEEE transactions on neural networks and learning systems (2022). Vadim Borisov, Kathrin Seßler, Tobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. Language models are realistic tabular data generators. In: The Eleventh International Conference on Learning Representations, ICLR. 2023. 37 [Bri+23] [Bro+20] [Bro+22] [Bro+24] [BT52] [Bu+20] [Byb23] [Cai+24] [Cao+19] [Car+21] [Car+23] [Car98] [Cas+23] [CCN23] [CG96] [CGC24] [CGZ24] [Cha+24] Trenton Bricken et al. Towards monosemanticity: Decomposing language models with dictionary learning. In: Transformer Circuits Thread 2 (2023). Tom Brown et al. Language models are few-shot learners. In: Advances in Neural Information Processing Systems. Vol. 33. Curran Associates, Inc., 2020, pp. 18771901. Hannah Brown, Katherine Lee, Fatemehsadat Mireshghallah, Reza Shokri, and Florian Tram`er. What does it mean for language model to preserve privacy? In: Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. 2022, pp. 22802292. Bradley Brown et al. Large language monkeys: scaling inference compute with repeated sampling. In: arXiv preprint arXiv:2407.21787 (2024). Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. The method of paired comparisons. In: Biometrika 39.3/4 (1952), pp. 324345. issn: 00063444, 14643510. Zhiqi Bu, Jinshuo Dong, Qi Long, and Weijie Su. Deep learning with Gaussian differential privacy. In: Harvard Data Science Review 2020.23 (2020). Leland Bybee. The ghost in the machine: Generating beliefs with large language models. In: arXiv preprint arXiv:2305.02823 (2023). Zhongze Cai, Shang Liu, Hanzhao Wang, Huaiyang Zhong, and Xiaocheng Li. Towards better statistical understanding of watermarking LLMs. In: arXiv preprint arXiv:2403.13027 (2024). Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced datasets with label-distribution-aware margin loss. In: Advances in neural information processing systems 32 (2019). Nicholas Carlini et al. Extracting training data from large language models. In: 30th USENIX Security Symposium (USENIX Security 21). 2021, pp. 26332650. Nicholas Carlini et al. Quantifying memorization across neural language models. In: The Eleventh International Conference on Learning Representations, ICLR. 2023. Rich Caruana. Multitask learning. In: Learning to learn. Boston, MA: Springer US, 1998, pp. 95133. isbn: 978-1-4615-5529-2. Stephen Casper et al. Open problems and fundamental limitations of reinforcement learning from human feedback. In: arXiv preprint arXiv:2307.15217 (2023). Bilal Chughtai, Lawrence Chan, and Neel Nanda. toy model of universality: Reverse engineering how networks learn group operations. In: International Conference on Machine Learning. PMLR. 2023, pp. 62436267. Stanley F. Chen and Joshua Goodman. An empirical study of smoothing techniques for language modeling. In: 34th Annual Meeting of the Association for Computational Linguistics. 1996, pp. 310318. John J. Cherian, Isaac Gibbs, and Emmanuel J. Cand`es. Large language model validity via enhanced conformal prediction methods. In: Advances in Neural Information Processing Systems 38, NeurIPS 2024. 2024. Miranda Christ, Sam Gunn, and Or Zamir. Undetectable watermarks for language models. In: The Thirty Seventh Annual Conference on Learning Theory. Vol. 247. Proceedings of Machine Learning Research. PMLR, 2024, pp. 11251139. Souradip Chakraborty et al. MaxMin-RLHF: Towards equitable alignment of large language models with diverse human preferences. In: arXiv preprint arXiv:2402.08925 (2024). [Che+20] Mark Chen et al. Generative pretraining from pixels. In: Proceedings of the 37th International Conference on Machine Learning. Vol. 119. Proceedings of Machine Learning Research. PMLR, July 2020, pp. 16911703. 38 [Che+21] Pengyu Cheng, Weituo Hao, Siyang Yuan, Shijing Si, and Lawrence Carin. FairFil: contrastive neural debiasing method for pretrained text encoders. In: 9th International Conference on Learning Representations, ICLR. 2021. [Che+23a] Mayee F. Chen et al. Skill-it! data-driven skills framework for understanding and training language models. In: Advances in Neural Information Processing Systems 36, NeurIPS 2023. 2023. [Che+23b] Xinyun Chen et al. Universal self-consistency for large language model generation. In: arXiv preprint arXiv:2311.17311 (2023). [Che+24a] Chao Chen et al. INSIDE: LLMs internal states retain the power of hallucination Detection. In: The Twelfth International Conference on Learning Representations, ICLR. 2024. [Che+24b] Xuxi Chen et al. Take the bull by the horns: Hard sample-reweighted continual training improves LLM generalization. In: arXiv preprint arXiv:2402.14270 (2024). [Chi+24] Wei-Lin Chiang et al. Chatbot Arena: An open platform for evaluating LLMs by human preference. In: Forty-first International Conference on Machine Learning, ICML. 2024. [Cho+17] [CHS24a] [CHS24b] [Chu+14] [Cla+18] [CLZ25] [CM24] [Col+11] [Con+23] [CP10] [CP11] [CPC21] Edward Choi et al. Generating multi-label discrete patient records using generative adversarial networks. In: Machine learning for healthcare conference. PMLR. 2017, pp. 286305. Aloni Cohen, Alexander Hoover, and Gabe Schoenbach. Enhancing watermarked language models to identify users. In: arXiv preprint arXiv:2405.11109 (2024). Aloni Cohen, Alexander Hoover, and Gabe Schoenbach. Watermarking language models for many adaptive users. In: 2025 IEEE Symposium on Security and Privacy (SP). IEEE Computer Society. 2024, pp. 8484. Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. In: arXiv preprint arXiv:1412.3555 (2014). Peter Clark et al. Think you have solved question answering? Try ARC, the AI2 reasoning challenge. In: arXiv preprint arXiv:1803.05457 (2018). Yinpeng Cai, Lexin Li, and Linjun Zhang. statistical hypothesis testing framework for data misappropriation detection in large language models. In: arXiv preprint arXiv:2501.02441 (2025). Jiuhai Chen and Jonas Mueller. Quantifying uncertainty in answers from any language model and enhancing their trustworthiness. In: Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2024, pp. 51865200. Ronan Collobert et al. Natural language processing (almost) from scratch. In: J. Mach. Learn. Res. 12.null (2011), pp. 24932537. issn: 1532-4435. Arthur Conmy, Augustine Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adri`a Garriga-Alonso. Towards automated circuit discovery for mechanistic interpretability. In: Advances in Neural Information Processing Systems 36 (2023), pp. 1631816352. Emmanuel Candes and Yaniv Plan. Matrix completion with noise. In: Proceedings of the IEEE 98.6 (2010), pp. 925936. Emmanuel Candes and Yaniv Plan. Tight oracle inequalities for low-rank matrix recovery from minimal number of noisy random measurements. In: IEEE Transactions on Information Theory 57.4 (2011), pp. 23422359. Pierre Colombo, Pablo Piantanida, and Chloe Clavel. novel estimator of mutual information for learning to disentangle textual representations. In: Annual Meeting of the Association for Computational Linguistics. Aug. 2021, pp. 65396550. [CS24] Hiroaki Chiba-Okabe and Weijie Su. Tackling GenAI copyright issues: Originality estimation and genericization. In: Scientific Reports (2024). [CW08] [CY15] [CZ13] [CZY24] [Dai+22] [Dat+24] [DD24] [Dee+25] [Den+24] [Der+17] [Dev+19] [DFK24] [DH17] [Doh+24] Ronan Collobert and Jason Weston. unified architecture for natural language processing: deep neural networks with multitask learning. In: Proceedings of the 25th International Conference on Machine Learning. ICML 08. Association for Computing Machinery, 2008, pp. 160167. isbn: 9781605582054. Yinzhi Cao and Junfeng Yang. Towards making systems forget with machine unlearning. In: 2015 IEEE symposium on security and privacy. IEEE. 2015, pp. 463480. Tony Cai and Anru Zhang. Sparse representation of polytope and recovery of sparse signals and low-rank matrices. In: IEEE transactions on information theory 60.1 (2013), pp. 122132. Shijie Chen, Yu Zhang, and Qiang Yang. Multi-task learning in natural language processing: An overview. In: ACM Comput. Surv. (May 2024). Just Accepted. issn: 03600300. Damai Dai et al. Knowledge neurons in pretrained transformers. In: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022, pp. 84938502. Sumanth Dathathri et al. Scalable watermarking for identifying large language model outputs. In: Nature 634.8035 (2024), pp. 818823. Apratim Dey and David Donoho. Universality of the π2/6 pathway in avoiding model collapse. In: arXiv preprint arXiv:2410.22812 (2024). DeepSeek-AI et al. DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning. In: arXiv preprint arXiv:2501.12948 (2025). Yihe Deng, Weitong Zhang, Zixiang Chen, and Quanquan Gu. Rephrase and respond: Let large language models ask better questions for themselves. In: arXiv preprint arXiv:2311.04205 (2024). Franck Dernoncourt, Ji Young Lee, Ozlem Uzuner, and Peter Szolovits. De-identification of patient notes with recurrent neural networks. In: Journal of the American Medical Informatics Association 24.3 (2017), pp. 596606. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pretraining of deep bidirectional transformers for language understanding. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019, pp. 41714186. Elvis Dohmatob, Yunzhen Feng, and Julia Kempe. Strong model collapse. In: arXiv preprint arXiv:2410.04840 (2024). Tobias Domhan and Felix Hieber. Using target-side monolingual data for neural machine translation through multi-task learning. In: Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Sept. 2017, pp. 15001505. Elvis Dohmatob, Yunzhen Feng, Pu Yang, Francois Charton, and Julia Kempe. tale of tails: Model collapse as change of scaling laws. In: arXiv preprint arXiv:2402.07043 (2024). [Dua+24a] Haonan Duan, Adam Dziedzic, Nicolas Papernot, and Franziska Boenisch. Flocks of stochastic parrots: Differentially private prompt learning for large language models. In: Advances in Neural Information Processing Systems 36 (2024). [Dua+24b] Jinhao Duan et al. Shifting attention to relevance: Towards the predictive uncertainty quantification of free-form large language models. In: Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2024, pp. 50505063. [Dub+24a] Abhimanyu Dubey et al. The llama 3 herd of models. In: arXiv preprint arXiv:2407. (2024). [Dub+24b] Yann Dubois, Balazs Galambosi, Percy Liang, and Tatsunori B. Hashimoto. Lengthcontrolled AlpacaEval: simple way to debias automatic evaluators. In: arXiv preprint arXiv:2404.04475 (2024). 40 [Dwo06] [Ega+24] Cynthia Dwork. Differential privacy. In: International colloquium on automata, languages, and programming. Springer. 2006, pp. 112. Naoki Egami, Musashi Hinck, Brandon Stewart, and Hanying Wei. Using imperfect surrogates for downstream inference: Design-based supervised learning for social science applications of large language models. In: Advances in Neural Information Processing Systems 36 (2024). [Elh+21] Nelson Elhage et al. mathematical framework for transformer circuits. In: Transformer Circuits Thread 1.1 (2021), p. 12. [Elh+22a] Nelson Elhage et al. Softmax linear units. In: Transformer Circuits Thread (2022). [Elh+22b] Nelson Elhage et al. Toy models of superposition. In: arXiv preprint arXiv:2209. (2022). [Elk+23] [Elm90] [Erh+10] [Eth+24] [Fai+23] [Fan+21] [Fan+24] [FDW24] Niva Elkin-Koren, Uri Hacohen, Roi Livni, and Shay Moran. Can copyright be reduced to privacy? In: arXiv preprint arXiv:2305.14822 (2023). Jeffrey L. Elman. Finding structure in time. In: Cognitive Science 14.2 (1990), pp. 179 211. issn: 0364-0213. Dumitru Erhan et al. Why does unsupervised pre-training help deep learning? In: J. Mach. Learn. Res. 11 (Mar. 2010), pp. 625660. issn: 1532-4435. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Model alignment as prospect theoretic optimization. In: Proceedings of the 41st International Conference on Machine Learning. ICML24. JMLR.org, 2024. Jaiden Fairoze et al. Publicly detectable watermarking for language models. In: arXiv preprint arXiv:2310.18491 (2023). Cong Fang, Hangfeng He, Qi Long, and Weijie Su. Exploring deep neural networks via layer-peeled model: Minority collapse in imbalanced training. In: Proceedings of the National Academy of Sciences 118.43 (2021), e2103091118. Chongyu Fan et al. Simplicity prevails: Rethinking negative preference optimization for LLM unlearning. In: Neurips Safe Generative AI Workshop. 2024. Ari B. Friedman, M. Kit Delgado, and Gary E. Weissman. Artificial intelligence for emergency care triagemuch promise, but still much to learn. In: JAMA Network Open 7.5 (May 2024), e248857. [Fen+24a] Guhao Feng et al. Towards revealing the mystery behind chain of thought: theoretical perspective. In: Advances in Neural Information Processing Systems 36 (2024). [Fen+24b] Yunzhen Feng, Elvis Dohmatob, Pu Yang, Francois Charton, and Julia Kempe. Beyond model collapse: Scaling up with synthesized data requires reinforcement. In: arXiv preprint arXiv:2406.07515 (2024). [Fer+23] Pierre Fernandez, Antoine Chaffin, Karim Tit, Vivien Chappelier, and Teddy Furon. Three bricks to consolidate watermarks for large language models. In: 2023 IEEE International Workshop on Information Forensics and Security (WIFS). IEEE. 2023, pp. 16. [Fom+20] Marina Fomicheva et al. Unsupervised quality estimation for neural machine translation. In: Transactions of the Association for Computational Linguistics 8 (2020), pp. 539555. [Fu+23] [Gag94] [Gal+23] [Gao+20] Deqing Fu, Tian-Qi Chen, Robin Jia, and Vatsal Sharan. Transformers learn higherorder optimization methods for in-context learning: study with linear models. In: arXiv preprint arXiv:2310.17086 (2023). Philip Gage. new algorithm for data compression. In: Users J. 12.2 (Feb. 1994), pp. 2338. issn: 0898-9788. Isabel Gallegos et al. Bias and fairness in large language models: survey. In: arXiv preprint arXiv:2309.00770 (2023). Leo Gao et al. The Pile: An 800GB dataset of diverse text for language modeling. In: arXiv preprint arXiv:2101.00027 (2020). 41 [Gao+24] [Gar+24] [Gas+16] [Gaw+23] [GC24] [Geh+20] [Geo+24] [Ger+24] [Ges24] [GF24] [GFC21] [Gin+19] [Gio23] [GL04] [Gli+24] [GMA22] [GMT24] Xiang Gao, Jiaxin Zhang, Lalla Mouatadid, and Kamalika Das. SPUQ: Perturbationbased uncertainty quantification for large language models. In: Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics, EACL. 2024, pp. 23362346. Sumegha Garg, Christopher Jung, Omer Reingold, and Aaron Roth. Oracle efficient online multicalibration and omniprediction. In: Proceedings of the 2024 Annual ACMSIAM Symposium on Discrete Algorithms (SODA). SIAM. 2024, pp. 27252792. Adri`a Gascon et al. Privacy-preserving distributed linear regression on high-dimensional data. In: Cryptology ePrint Archive (2016). Jakob Gawlikowski et al. survey of uncertainty in deep neural networks. In: Artificial Intelligence Review 56.Suppl 1 (2023), pp. 15131589. Isaac Gibbs and Emmanuel Cand`es. Conformal inference for online prediction with arbitrary distribution shifts. In: Journal of Machine Learning Research 25.162 (2024), pp. 136. Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. RealToxicityPrompts: Evaluating neural toxic gegeneration in language models. In: Findings of the Association for Computational Linguistics: EMNLP. Vol. EMNLP 2020. Findings of ACL. 2020, pp. 33563369. Kristian Georgiev et al. Attribute-to-delete: Machine unlearning via datamodel matching. In: arXiv preprint arXiv:2410.23232 (2024). Matthias Gerstgrasser et al. Is model collapse inevitable? Breaking the curse of recursion by accumulating real and synthetic data. In: arXiv preprint arXiv:2404.01413 (2024). Lee Gesmer. Copyright and the challenge of large language models. In: (Oct. 2024). America. url: https : / / www . masslawblog . com / copyright / copyright - and - the - challenge-of-large-language-models-part-2-2/. Eva Giboulot and Teddy Furon. WaterMax: breaking the LLM watermark detectabilityrobustness-quality trade-off. In: Neural Information Processing Systems. 2024. Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. In: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP. 2021, pp. 38163830. Antonio Ginart, Melody Guan, Gregory Valiant, and James Zou. Making AI forget you: Data deletion in machine learning. In: Advances in neural information processing systems 32 (2019). Patrizio Giovannotti. Evaluating machine translation quality with conformal predictive distributions. In: Conformal and Probabilistic Prediction with Applications. PMLR. 2023, pp. 413429. Jianfeng Gao and Chin-Yew Lin. Introduction to the special issue on statistical language modeling. In: ACM Transactions on Asian Language Information Processing (TALIP) 3.2 (2004), pp. 8793. Kristina Gligoric, Tijana Zrnic, Cinoo Lee, Emmanuel Cand`es, and Dan Jurafsky. Can unconfident LLM annotations be used for confident conclusions? In: arXiv preprint arXiv:2408.15204 (2024). Aparna Garimella, Rada Mihalcea, and Akhash Amarnath. Demographic-aware language model fine-tuning as bias mitigation technique. In: Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2022, pp. 311319. Jorge Garcıa-Carrasco, Alejandro Mate, and Juan Carlos Trujillo. How does GPT-2 predict acronyms? Extracting and understanding circuit via mechanistic interpretability. In: International Conference on Artificial Intelligence and Statistics. PMLR. 2024, pp. 33223330. 42 [Gol+24] [GPT23] [Gra+24] [GRK21] [GSH23] [GSR19] Olga Golovneva, Tianlu Wang, Jason Weston, and Sainbayar Sukhbaatar. Contextual position encoding: Learning to count whats important. In: arXiv preprint arXiv:2405.18719 (2024). GPTZero. GPTZero: More than an AI detector preserve whats human. 2023. url: https://gptzero.me/. Aaron Grattafiori et al. The Llama 3 herd of models. In: arXiv preprint arXiv:2407.21783 (2024). Demi Guo, Alexander Rush, and Yoon Kim. Parameter-efficient transfer learning with diff pruning. In: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Aug. 2021, pp. 48844896. Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In: Proceedings of the 40th International Conference on Machine Learning. ICML23. JMLR.org, 2023. Sebastian Gehrmann, Hendrik Strobelt, and Alexander M. Rush. GLTR: Statistical detection and visualization of generated text. In: Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL. 2019, pp. 111116. [Gue+24] Marco Guevara et al. Large language models to identify social determinants of health in electronic health records. In: npj Digital Medicine 7 (Jan. 2024), p. 6. [Guo+20] Chuan Guo, Tom Goldstein, Awni Y. Hannun, and Laurens van der Maaten. Certified data removal from machine learning models. In: Proceedings of the 37th International Conference on Machine Learning, ICML. Vol. 119. Proceedings of Machine Learning Research. PMLR, 2020, pp. 38323842. [Guo+24a] Shangmin Guo et al. Direct language model alignment from online AI feedback. In: arXiv preprint arXiv:2402.04792 (2024). [Guo+24b] Tianyu Guo et al. Active-dormant attention heads: Mechanistically demystifying extremetoken phenomena in LLMs. In: NeurIPS 2024 Workshop on Mathematics of Modern Machine Learning. 2024. [Guo+24c] Tianyu Guo et al. How do transformers learn in-context beyond simple functions? case study on learning with representations. In: The Twelfth International Conference on Learning Representations, ICLR. 2024. [Gur+23] Wes Gurnee et al. Finding neurons in haystack: Case studies with sparse probing. In: Trans. Mach. Learn. Res. 2023 (2023). [Hal+23] [Han+22] [Han+23] [Han+24a] Skyler Hallinan, Alisa Liu, Yejin Choi, and Maarten Sap. Detoxifying text with MaRCo: Controllable revision with experts and anti-experts. In: Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). July 2023, pp. 228242. Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and Maosong Sun. PTR: Prompt tuning with rules for text classification. In: AI Open 3 (2022), pp. 182192. Tianyu Han et al. MedAlpaca An open-source collection of medical conversational AI models and training data. In: arXiv preprint arXiv:2304.08247 (Apr. 2023). Sungwon Han, Jinsung Yoon, Sercan O. Arik, and Tomas Pfister. Large language models can automatically engineer features for few-shot tabular learning. In: Forty-first International Conference on Machine Learning, ICML. 2024. [Han+24b] Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, and Sai Qian Zhang. Parameter-efficient fine-tuning for large models: comprehensive survey. In: arXiv preprint arXiv:2403.14608 (2024). [Har54] Zellig Harris. Distributional structure. In: Word 10.2-3 (1954), pp. 146162. [HBC21] Xudong Han, Timothy Baldwin, and Trevor Cohn. Diverse adversaries for mitigating bias in training. In: Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. Apr. 2021, pp. 27602765. 43 [HBC22] [He+16] [He+22a] [He+22b] [He+24a] Xudong Han, Timothy Baldwin, and Trevor Cohn. Balancing out bias: Achieving fairness through balanced training. In: Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Dec. 2022, pp. 1133511350. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016, pp. 770778. Jacqueline He, Mengzhou Xia, Christiane Fellbaum, and Danqi Chen. MABEL: Attenuating gender bias using textual entailment data. In: Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Dec. 2022, pp. 96819702. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards unified view of parameter-efficient transfer learning. In: International Conference on Learning Representations. 2022. Haiyun He, Yepeng Liu, Ziqiao Wang, Yongyi Mao, and Yuheng Bu. Universally optimal watermarking schemes for LLMs: from theory to practice. In: arXiv preprint arXiv:2410.02890 (2024). [He+24b] Hengzhi He, Peiyu Yu, Junpeng Ren, Ying Nian Wu, and Guang Cheng. Watermarking generative tabular data. In: arXiv preprint arXiv:2405.14018 (2024). [Hen+21a] Dan Hendrycks et al. Measuring massive multitask language understanding. In: 9th International Conference on Learning Representations, ICLR. 2021. [Hen+21b] Dan Hendrycks et al. Measuring mathematical problem solving with the MATH dataset. In: Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks. 2021. [HH24] [HJ23] [HMH24] [Hof+22] [Hof01] [Hoo+21] [Hou+19] [HS23] [HS24] [HS97] [Hu+24] Zhengmian Hu and Heng Huang. Inevitable trade-off between watermark strength and speculative sampling efficiency for language models. In: Advances in Neural Information Processing Systems. Vol. 37. Curran Associates, Inc., 2024, pp. 5537055402. Stefan Heimersheim and Jett Janiak. circuit for Python docstrings in 4-layer attentiononly transformer. 2023. url: https : / / www . %20alignmentforum . %20org / posts / u6KXXmKFbXfWzoAXn/acircuit-for-python-docstrings-in-a-4-layer-attentiononly. Noah Hollmann, Samuel Muller, and Frank Hutter. Large language models for automated data science: Introducing caafe for context-aware automated feature engineering. In: Advances in Neural Information Processing Systems 36 (2024). Jordan Hoffmann et al. Training compute-optimal large language models. In: Proceedings of the 36th International Conference on Neural Information Processing Systems. NIPS 22. Red Hook, NY, USA: Curran Associates Inc., 2022. isbn: 9781713871088. Thomas Hofmann. Unsupervised learning by probabilistic latent semantic analysis. In: Mach. Learn. 42.12 (2001), pp. 177196. issn: 0885-6125. Shlomo Hoory et al. Learning and evaluating differentially private pre-trained language model. In: Findings of the Association for Computational Linguistics: EMNLP. 2021, pp. 11781189. Neil Houlsby et al. Parameter-efficient transfer learning for NLP. In: Proceedings of the 36th International Conference on Machine Learning. Vol. 97. Proceedings of Machine Learning Research. PMLR, June 2019, pp. 27902799. Hangfeng He and Weijie Su. law of data separation in deep learning. In: Proceedings of the National Academy of Sciences 120.36 (2023), e2221704120. Hangfeng He and Weijie Su. law of next-token prediction in large language models. In: arXiv preprint arXiv:2408.13442 (2024). Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. In: Neural Comput. 9.8 (1997), pp. 17351780. issn: 0899-7667. Zhengmian Hu et al. Unbiased watermark for large language models. In: International Conference on Learning Representations. 2024. 44 [Hua+20] Po-Sen Huang et al. Reducing sentiment bias in language models via counterfactual evaluation. In: Findings of the Association for Computational Linguistics: EMNLP. Vol. EMNLP 2020. Findings of ACL. 2020, pp. 6583. [Hua+23a] Baihe Huang et al. Towards optimal statistical watermarking. In: arXiv preprint arXiv:2312.07930 (2023). [Hua+23b] Jonathan Huang et al. Generative artificial intelligence for chest radiograph interpretation in the emergency department. In: JAMA network open 6.10 (2023), e2336100 e2336100. [Hua+23c] Yangsibo Huang, Samyak Gupta, Zexuan Zhong, Kai Li, and Danqi Chen. Privacy implications of retrieval-based language models. In: Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 2023, pp. 1488714902. [Hua+24a] Wei Huang et al. BiLLM: Pushing the limit of post-training quantization for LLMs. In: Forty-first International Conference on Machine Learning, ICML. 2024. [Hua+24b] Zhen Huang et al. O1 replication journey Part 2: surpassing o1-preview through simple distillation, big progress or bitter lesson? In: arXiv preprint arXiv:2411.16489 (2024). [Hub+24] Robert Huben, Hoagy Cunningham, Logan Riggs, Aidan Ewart, and Lee Sharkey. Sparse autoencoders find highly interpretable features in language models. In: The Twelfth International Conference on Learning Representations, ICLR. 2024. [Huo+24] Mingjia Huo et al. Token-specific watermarking with enhanced detectability and semantic coherence for large language models. In: International Conference on Machine Learning. 2024. [Ipp+20] [Jan+23] [Jaq+17] [Jaq+20] [Jel98] [Ji+22] [Ji+23] [Jia+20] Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. Automatic detection of generated text is easiest when humans are fooled. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL. 2020, pp. 1808 1822. Joel Jang et al. Knowledge unlearning for mitigating privacy risks in language models. In: Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL. 2023, pp. 1438914408. Natasha Jaques et al. Sequence tutor: conservative fine-tuning of sequence generation models with KL-control. In: Proceedings of the 34th International Conference on Machine Learning - Volume 70. ICML17. JMLR.org, 2017, pp. 16451654. Natasha Jaques et al. Human-centric dialog training via offline reinforcement learning. In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP. 2020, pp. 39854003. Frederick Jelinek. Statistical methods for speech recognition. Cambridge, MA, USA: MIT Press, 1998. isbn: 0262100665. Wenlong Ji, Yiping Lu, Yiliang Zhang, Zhun Deng, and Weijie Su. An unconstrained layer-peeled perspective on neural collapse. In: The Tenth International Conference on Learning Representations, ICLR. 2022. Ziwei Ji et al. Survey of hallucination in natural language generation. In: ACM Computing Surveys 55.12 (2023), pp. 138. Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. How can we know what language models know. In: Trans. Assoc. Comput. Linguistics 8 (2020), pp. 423438. [Jia+23] Albert Q. Jiang et al. Mistral 7B. In: arXiv preprint arXiv:2310.06825 (2023). [Jia+24a] Jinghan Jia et al. SOUL: Unlocking the power of second-order optimization for LLM unlearning. In: Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP. 2024, pp. 42764292. [Jia+24b] Albert Q. Jiang et al. Mixtral of experts. In: arXiv preprint arXiv:2401.04088 (2024). [Jin+21] Xisen Jin et al. On transferability of bias mitigation effects in language model finetuning. In: Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT. 2021, pp. 37703783. 45 [JJS24] [JLZ25] [JMS24] [Joa98] [Jor86] [JSV24] [Kad+22] [Kan+23] [Kap+20] [Kau+24] [KB21] [KDS23] [Ke+23] [Ken+21] [KGF23] Meena Jagadeesan, Michael Jordan, and Jacob Steinhardt. Safety vs. performance: How multi-objective learning reduces barriers to market entry. In: arXiv preprint arXiv:2409.03734 (2024). Wenlong Ji, Lihua Lei, and Tijana Zrnic. Predictions as surrogates: Revisiting surrogate outcomes in the age of AI. In: arXiv preprint arXiv:2501.09731 (2025). Ayush Jain, Andrea Montanari, and Eren Sasoglu. Scaling laws for learning with real and surrogate data. In: Advances in Neural Information Processing Systems 38, NeurIPS 2024. 2024. Thorsten Joachims. Text categorization with support vector machines: Learning with many relevant features. In: European conference on machine learning. Springer. 1998, pp. 137142. Michael I. Jordan. Attractor dynamics and parallelism in connectionist sequential machine. In: Proceedings of the Eighth Annual Conference of the Cognitive Science Society. 1986. Nikola Jovanovic, Robin Staab, and Martin Vechev. Watermark stealing in large language models. In: International Conference on Machine Learning. 2024. Saurav Kadavath et al. Language models (mostly) know what they know. In: arXiv preprint arXiv:2207.05221 (2022). Beichen Kang et al. Automatic ICD coding based on segmented ClinicalBERT with hierarchical tree structure learning. In: Database Systems for Advanced Applications: 28th International Conference, DASFAA. Springer, 2023, pp. 250265. Jared Kaplan et al. Scaling laws for neural language models. In: CoRR abs/2001.08361 (2020). Chinmaya Kausik, Mirco Mutti, Aldo Pacchiano, and Ambuj Tewari. theoretical framework for partially observed reward-states in RLHF. In: ICML 2024 Workshop on Models of Human Feedback for AI Alignment. 2024. Masahiro Kaneko and Danushka Bollegala. Debiasing pre-trained contextualised embeddings. In: arXiv preprint arXiv:2101.09523 (2021). Hadas Kotek, Rikker Dockum, and David Sun. Gender bias and stereotypes in large language models. In: Proceedings of The ACM Collective Intelligence Conference. 2023, pp. 1224. Zheng Tracy Ke, Pengsheng Ji, Jiashun Jin, and Wanshan Li. Recent advances in text analysis. In: Annual Review of Statistics and Its Application 11 (2023). Zachary Kenton et al. Alignment of language agents. In: arXiv preprint arXiv:2103.14659 (2021). Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. In: The Eleventh International Conference on Learning Representations, ICLR. 2023. [Kim+23] Minbeom Kim et al. Critic-guided decoding for controlled text generation. In: Findings of the Association for Computational Linguistics: ACL. 2023, pp. 45984612. [Kim14] [Kir+23a] [Kir+23b] [KKO24] Yoon Kim. Convolutional neural networks for sentence classification. In: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2014, pp. 17461751. John Kirchenbauer et al. watermark for large language models. In: International Conference on Machine Learning. PMLR. 2023, pp. 1706117084. John Kirchenbauer et al. On the reliability of watermarks for large language models. In: The Twelfth International Conference on Learning Representations. 2023. Ryuto Koike, Masahiro Kaneko, and Naoaki Okazaki. Outfox: LLM-generated essay detection through in-context learning with adversarially generated examples. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. 19. 2024, pp. 21258 21266. 46 [Koc+23] [Kop+24] [Kri+24] [KS24] [Kud+24] Denis Kocetkov et al. The Stack: 3 TB of permissively licensed source code. In: Trans. Mach. Learn. Res. 2023 (2023). Andreas Kopf et al. OpenAssistant conversations - democratizing large language model alignment. In: Proceedings of the 37th International Conference on Neural Information Processing Systems. NIPS 23. Curran Associates Inc., 2024. Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit Iyyer. Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense. In: Advances in Neural Information Processing Systems. Vol. 36. 2024. Tokio Kajitsuka and Issei Sato. Are transformers with one layer self-attention using lowrank weight matrices universal approximators? In: The Twelfth International Conference on Learning Representations, ICLR. 2024. Rohith Kuditipudi, John Thickstun, Tatsunori Hashimoto, and Percy Liang. Robust distortion-free watermarks for language models. In: Transactions on Machine Learning Research (2024). issn: 2835-8856. [Kum+23] Bhawesh Kumar et al. Conformal prediction with large language models for multi-choice question answering. In: arXiv preprint arXiv:2305.18404 (2023). [KW24] [Kwe+23] [KX15] Zheng Tracy Ke and Minzhe Wang. Using SVD for topic modeling. In: Journal of the American Statistical Association 119.545 (2024), pp. 434449. Sunjun Kweon, Junu Kim, Jiyoun Kim, Sujeong Im, et al. Publicly shareable clinical large language model built on synthetic clinical notes. In: arXiv preprint (Sept. 2023). Vladimir Koltchinskii and Dong Xia. Optimal estimation of low rank density matrices. In: J. Mach. Learn. Res. 16.53 (2015), pp. 17571792. [Lam+24] Nathan Lambert et al. RewardBench: Evaluating reward models for language modeling. In: arXiv preprint arXiv:2403.13787 (2024). [Lan+20] [LBM24] [LD02] Zhenzhong Lan et al. ALBERT: lite BERT for self-supervised learning of language representations. In: ICLR. 2020. Licong Lin, Yu Bai, and Song Mei. Transformers as decision makers: provable in-context reinforcement learning via supervised pretraining. In: The Twelfth International Conference on Learning Representations, ICLR. 2024. Tom Landauer and Scott Dooley. Latent semantic analysis: theory, method and application. In: Proceedings of the Conference on Computer Support for Collaborative Learning: Foundations for CSCL Community. CSCL 02. International Society of the Learning Sciences, 2002, pp. 742743. [Lew+20] Mike Lewis et al. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020, pp. 78717880. [Li+21] [Li+23] [Li+24a] [Li+24b] [Li+24c] [Li+25] Xuechen Li, Florian Tramer, Percy Liang, and Tatsunori Hashimoto. Large language models can be strong differentially private learners. In: International Conference on Learning Representations. 2021. Xuechen Li et al. AlpacaEval: An automatic evaluator of instruction-following models. 2023. url: https://github.com/tatsu-lab/alpaca_eval. Chunyuan Li et al. Llava-med: Training large language-and-vision assistant for biomedicine in one day. In: Advances in Neural Information Processing Systems 36 (2024). Junlong Li et al. Generative judge for evaluating alignment. In: The Twelfth International Conference on Learning Representations, ICLR. 2024. Xiang Li, Feng Ruan, Huiyuan Wang, Qi Long, and Weijie Su. Robust detection of watermarks in large language models under human edits. In: arXiv preprint arXiv:2411.13868 (2024). Xiang Li, Feng Ruan, Huiyuan Wang, Qi Long, and Weijie Su. statistical framework of watermarks for large language models: Pivot, detection efficiency and optimal rules. In: The Annals of Statistics 53.1 (2025), pp. 322351. 47 [Lia+23] [Lim+24] [Lin+24a] [Lin+24b] [Lis+21] [Liu+19] [Liu+20a] [Liu+20b] [Liu+21] [Liu+22] [Liu+23] [Liu+24a] [Liu+24b] [Liu+24c] [Liu+24d] Weixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, and James Zou. GPT detectors are biased against non-native English writers. In: ICLR 2023 Workshop on Trustworthy and Reliable Large-Scale Machine Learning Models. 2023. Yannick Limmer, Anastasis Kratsios, Xuwei Yang, Raeid Saqur, and Blanka Horvath. Reality only happens once: Single-path generalization bounds for transformers. In: arXiv preprint arXiv:2405.16563 (2024). Licong Lin, Jingfeng Wu, Sham Kakade, Peter Bartlett, and Jason Lee. Scaling laws in linear regression: compute, parameters, and data. In: Advances in Neural Information Processing Systems. Vol. 37. Curran Associates, Inc., 2024, pp. 6055660606. Zhenghao Lin et al. Rho-1: Not all tokens are what you need. In: arXiv preprint arXiv:2404.07965 (2024). Pierre Lison, Ildiko Pilan, David Sanchez, Montserrat Batet, and Lilja Øvrelid. Anonymisation models for text data: State of the art, challenges and future directions. In: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021, pp. 41884203. Yinhan Liu et al. RoBERTa: robustly optimized BERT pretraining approach. In: arXiv preprint arXiv:1907.11692 (2019). Haochen Liu et al. Does gender matter? Towards fairness in dialogue systems. In: Proceedings of the 28th International Conference on Computational Linguistics, COLING. International Committee on Computational Linguistics, 2020, pp. 44034416. Zhuang Liu, Degen Huang, Kaiyu Huang, Zhuang Li, and Jun Zhao. FinBERT: pretrained financial language representation model for financial text mining. In: Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20. Special Track on AI in FinTech. International Joint Conferences on Artificial Intelligence Organization, July 2020, pp. 45134519. Alisa Liu et al. DExperts: Decoding-time controlled text generation with experts and anti-experts. In: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Aug. 2021, pp. 66916706. Jiachang Liu et al. What makes good in-context examples for GPT-3? In: Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, DeeLIO@ACL 2022. 2022, pp. 100114. Pengfei Liu et al. Pre-train, prompt, and predict: systematic survey of prompting methods in natural language processing. In: ACM Comput. Surv. 55.9 (2023), 195:1 195:35. Aixin Liu et al. DeepSeek-v2: strong, economical, and efficient mixture-of-experts language model. In: arXiv preprint arXiv:2405.04434 (2024). Aixin Liu et al. DeepSeek-v3 technical report. In: arXiv preprint arXiv:2412.19437 (2024). Linyu Liu, Yu Pan, Xiaocheng Li, and Guanting Chen. Uncertainty estimation and quantification for LLMs: simple supervised approach. In: arXiv preprint arXiv:2404.15993 (2024). Xiao Liu et al. Are LLMs capable of data-based statistical and causal reasoning? Benchmarking advanced quantitative reasoning with data. In: Findings of the Association for Computational Linguistics: ACL. Aug. 2024, pp. 92159235. [Liu+24e] Xiao Liu et al. GPT understands, too. In: AI Open 5 (2024), pp. 208215. [Liu+24f] [Liu+24g] Yang Liu, Jiahuan Cao, Chongyu Liu, Kai Ding, and Lianwen Jin. Datasets for large language models: comprehensive survey. In: arXiv preprint arXiv:2402.18041 (2024). Zhiyuan Liu, Hong Liu, Denny Zhou, and Tengyu Ma. Chain of thought empowers transformers to solve inherently serial problems. In: The Twelfth International Conference on Learning Representations, ICLR. 2024. 48 [Liu+25] [LJ24] [LKW23] [LL21] [LLS22] [LLZ24] [LMM23] [LT20] [LTS24] [Lu+20] [Lu+22] [Lu+23] [Men+22] [MG21] [MH24] [Mik+10] [Mik+13] [Min+22a] Sijia Liu et al. Rethinking machine unlearning for large language models. In: Nature Machine Intelligence (Feb. 2025). Maximilian Li and Lucas Janson. Optimal ablation for interpretability. In: Advances in Neural Information Processing Systems 38, NeurIPS 2024. 2024. Xin Liu, Muhammad Khalifa, and Lu Wang. BOLT: Fast energy-based controlled text generation with tunable biases. In: Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). July 2023, pp. 186 200. Xiang Lisa Li and Percy Liang. Prefix-tuning: optimizing continuous prompts for generation. In: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Aug. 2021, pp. 45824597. Zi Lin, Jeremiah Zhe Liu, and Jingbo Shang. Towards collaborative neural-symbolic graph semantic parsing via uncertainty. In: Findings of the Association for Computational Linguistics: ACL 2022. 2022, pp. 41604173. Xingchi Li, Guanxun Li, and Xianyang Zhang. Segmenting watermarked texts from language models. In: arXiv preprint arXiv:2410.20670 (2024). Baohao Liao, Yan Meng, and Christof Monz. Parameter-efficient fine-tuning without introducing new latency. In: Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). July 2023, pp. 42424260. Wei-Chao Lin and Chih-Fong Tsai. Missing value imputation: review and analysis of the literature (20062017). In: Artificial Intelligence Review 53 (2020), pp. 14871509. Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. Generating with confidence: Uncertainty quantification for black-box large language models. In: arXiv preprint arXiv:2305.19187 (2024). Kaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam Amancharla, and Anupam Datta. Gender bias in neural natural language processing. In: Logic, language, and security: essays dedicated to Andre Scedrov on the occasion of his 65th birthday (2020), pp. 189202. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022. 2022, pp. 80868098. Dakuan Lu et al. BBT-Fin: Comprehensive construction of Chinese financial domain pre-trained language model, corpus and benchmark. In: arXiv preprint arXiv:2302.09432 (2023). Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in GPT. In: Advances in Neural Information Processing Systems 35 (2022), pp. 1735917372. Andrey Malinin and Mark J. F. Gales. Uncertainty estimation in autoregressive structured prediction. In: 9th International Conference on Learning Representations, ICLR. 2021. Christopher Mohri and Tatsunori Hashimoto. Language models with conformal factuality guarantees. In: Proceedings of the 41st International Conference on Machine Learning. ICML24. JMLR.org, 2024. Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. Recurrent neural network based language model. In: INTERSPEECH. ISCA, 2010, pp. 10451048. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In: vol. 26. 2013. Sewon Min et al. Rethinking the role of demonstrations: What makes in-context learning work? In: Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022, pp. 1104811064. [Min+22b] [Mis+22] Soren Mindermann et al. Prioritized training on points that are learnable, worth learning, and not yet learnt. In: International Conference on Machine Learning, ICML. Vol. 162. Proceedings of Machine Learning Research. PMLR, 2022, pp. 1563015649. Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. In: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). May 2022, pp. 34703487. [Mit+23] Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher Manning, and Chelsea Finn. Detectgpt: Zero-shot machine-generated text detection using probability curvature. In: International Conference on Machine Learning. PMLR. 2023, pp. 2495024962. [MML23] Silvia Milano, Joshua McGrane, and Sabina Leonelli. Large language models challenge the future of higher education. In: Nature Machine Intelligence 5.4 (2023), pp. 333334. [Moo+23] Michael Moor et al. Med-flamingo: multimodal medical few-shot learner. In: Machine Learning for Health (ML4H). PMLR. 2023, pp. 353367. [Mue+23] Niklas Muennighoff et al. Scaling data-constrained language models. In: Advances in Neural Information Processing Systems 36, NeurIPS 2023. 2023. [Mur+19] James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, and Bin Yu. Definitions, methods, and applications in interpretable machine learning. In: Proceedings of the National Academy of Sciences 116.44 (2019), pp. 2207122080. [MVM09] [MXC24] [Nak+22] [Nak+24] [Nas+23] [Nav+23] [NCR23] [Nie+24] [NRS21] [Oh+22] [Ola+20] [Ols+22] [Omi+23] Frederic P. Miller, Agnes F. Vandome, and John McBrewster. Elo rating system. Alpha Press, 2009. isbn: 6130078846. Yu Meng, Mengzhou Xia, and Danqi Chen. SimPO: Simple preference optimization with reference-free reward. In: Advances in Neural Information Processing Systems. Vol. 37. Curran Associates, Inc., 2024, pp. 124198124235. Reiichiro Nakano et al. WebGPT: Browser-assisted question-answering with human feedback. In: arXiv preprint arXiv:2112.09332 (2022). Ryumei Nakada, Yichen Xu, Lexin Li, and Linjun Zhang. Synthetic oversampling: Theory and practical approach using LLMs to address data imbalance. In: arXiv preprint arXiv:2406.03628 (2024). Milad Nasr et al. Scalable extraction of training data from (production) language models. In: arXiv preprint arXiv:2311.17035 (2023). Humza Naveed et al. comprehensive overview of large language models. In: arXiv preprint arXiv:2307.06435 (2023). Roberto Navigli, Simone Conia, and Bjorn Ross. Biases in large language models: origins, inventory, and discussion. In: ACM Journal of Data and Information Quality 15.2 (2023), pp. 121. Fan Nie et al. FactTest: Factuality Testing in Large Language Models with Statistical Guarantees. In: arXiv preprint arXiv:2411.02603 (2024). Seth Neel, Aaron Roth, and Saeed Sharifi-Malvajerdi. Descent-to-delete: Gradient-based methods for machine unlearning. In: Algorithmic Learning Theory. PMLR. 2021, pp. 931 962. Changdae Oh et al. Learning fair representation via distributional contrastive disentanglement. In: Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2022, pp. 12951305. Chris Olah et al. Zoom in: An introduction to circuits. In: Distill 5.3 (2020), e00024 001. Catherine Olsson et al. In-context learning and induction heads. In: arXiv preprint arXiv:2209.11895 (2022). Jesutofunmi Omiye, Jenna Lester, Simon Spichak, Veronica Rotemberg, and Roxana Daneshjou. Large language models propagate race-based medicine. In: NPJ Digital Medicine 6.1 (2023), p. 195. 50 [Ope+24a] OpenAI et al. GPT-4 technical report. In: arXiv preprint arXiv:2303.08774 (2024). [Ope+24b] OpenAI et al. OpenAI o1 system card. In: arXiv preprint arXiv:2412.16720 (2024). [Ouy+22] Long Ouyang et al. Training language models to follow instructions with human feedback. In: Advances in neural information processing systems 35 (2022), pp. 2773027744. [OZ03] [Pal+24] [Pan+24] [Par+23] Stanley RM Oliveira and Osmar Zaiane. Protecting sensitive knowledge by data sanitization. In: Third IEEE International conference on data mining. IEEE. 2003, pp. 613 616. Arka Pal et al. Smaug: Fixing failure modes of preference optimisation with DPOpositive. In: arXiv preprint arXiv:2402.13228 (2024). Richard Yuanzhe Pang et al. Iterative reasoning preference optimization. In: Advances in Neural Information Processing Systems. Vol. 37. Curran Associates, Inc., 2024, pp. 116617 116637. SunYoung Park, Kyuri Choi, Haeun Yu, and Youngjoong Ko. Never too late to learn: Regularizing gender bias in coreference resolution. In: Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining. 2023, pp. 1523. [Par+24a] Ryan Park, Rafael Rafailov, Stefano Ermon, and Chelsea Finn. Disentangling length from quality in direct preference optimization. In: Findings of the Association for Computational Linguistics: ACL. 2024, pp. 49985017. [Par+24b] Yeonhong Park, Jake Hyun, SangLyul Cho, Bonggeun Sim, and Jae W. Lee. Anyprecision LLM: Low-cost deployment of multiple, different-sized LLMs. In: Forty-first International Conference on Machine Learning, ICML. 2024. [PCJ25] [Pen+23] [Pen+24] [Per+20] [Pet+18] [PFB24] [PHB24] [PHD20] [PMB19] [PSM14] Mose Park, Yunjin Choi, and Jong-June Jeon. Does large language model really speak in human-like language? In: arXiv preprint arXiv:2501.01273 (2025). Cheng Peng et al. study of generative large language model for medical research and healthcare. In: npj Digital Medicine 6 (Nov. 2023), p. 210. Guilherme Penedo et al. The RefinedWeb dataset for falcon LLM: outperforming curated corpora with web data only. In: Proceedings of the 37th International Conference on Neural Information Processing Systems. NIPS 23. Curran Associates Inc., 2024. Juan Perdomo, Tijana Zrnic, Celestine Mendler-Dunner, and Moritz Hardt. Performative prediction. In: Proceedings of the 37th International Conference on Machine Learning. 2020, pp. 75997609. Matthew E. Peters et al. Deep contextualized word representations. In: Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). June 2018, pp. 22272237. Raphael Poulain, Hamed Fayyaz, and Rahmatollah Beheshti. Bias patterns in the application of LLMs for clinical decision support: comprehensive study. In: arXiv preprint arXiv:2404.15149 (2024). Vaidehi Patil, Peter Hase, and Mohit Bansal. Can sensitive information be deleted from LLMs? Objectives for defending against extraction attacks. In: The Twelfth International Conference on Learning Representations. 2024. Vardan Papyan, XY Han, and David Donoho. Prevalence of neural collapse during the terminal phase of deep learning training. In: Proceedings of the National Academy of Sciences 117.40 (2020), pp. 2465224663. Jorge Perez, Javier Marinkovic, and Pablo Barcelo. On the Turing completeness of modern neural network architectures. In: 7th International Conference on Learning Representations, ICLR. 2019. Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global vectors for word representation. In: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Oct. 2014, pp. 15321543. 51 [Qin+24] [Rad+18] [Rad+19] [Rae+21] [Raf+20] [Raf+23] [Rai+24] [Ram+17] [Ran+24] [Ren+23] [Ren+24] [RGG23] [RJS17] [Rol+21] [Ros00] [RSD23] [RSS23] [Sac+24] [Sad+23] [Sag23] Yiwei Qin et al. O1 replication journey: strategic progress report Part 1. In: arXiv preprint arXiv:2410.18982 (2024). Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018. url: https://cdn.openai.com/ research-covers/language-unsupervised/language_understanding_paper.pdf. Alec Radford et al. Language models are unsupervised multitask learners. In: OpenAI (2019). Jack W. Rae et al. Scaling language models: Methods, analysis & insights from training Gopher. In: CoRR abs/2112.11446 (2021). Colin Raffel et al. Exploring the limits of transfer learning with unified text-to-text transformer. In: Journal of Machine Learning Research 21.140 (2020), pp. 167. Rafael Rafailov et al. Direct preference optimization: Your language model is secretly reward model. In: Advances in Neural Information Processing Systems. Vol. 36. Curran Associates, Inc., 2023, pp. 5372853741. Daking Rai, Yilun Zhou, Shi Feng, Abulhair Saparov, and Ziyu Yao. practical review of mechanistic interpretability for transformer-based language models. In: arXiv preprint arXiv:2407.02646 (2024). Rachel Ramoni et al. The undiagnosed diseases network: Accelerating discovery about health and disease. In: American Journal of Human Genetics 100.2 (2017), pp. 185192. Leonardo Ranaldi, Giulia Pucci, Federico Ranaldi, Elena Sofia Ruzzetti, and Fabio Massimo Zanzotto. tree-of-thoughts to broaden multi-step reasoning across languages. In: Findings of the Association for Computational Linguistics: NAACL. June 2024, pp. 1229 1241. Allen Z. Ren et al. Robots that ask for help: Uncertainty alignment for large language model planners. In: Conference on Robot Learning, CoRL. Vol. 229. Proceedings of Machine Learning Research. PMLR, 2023, pp. 661682. Jie Ren et al. Copyright protection in generative AI: technical perspective. In: arXiv preprint arXiv:2402.02333 (2024). Shauli Ravfogel, Yoav Goldberg, and Jacob Goldberger. Conformal Nucleus Sampling. In: Findings of the Association for Computational Linguistics: ACL. July 2023, pp. 27 34. Alec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learning to generate reviews and discovering sentiment. In: arXiv preprint arXiv:1704.01444 (2017). Stephen Roller et al. Recipes for building an open-domain chatbot. In: Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. Apr. 2021, pp. 300325. R. Rosenfeld. Two decades of statistical language modeling: where do we go from here? In: Proceedings of the IEEE 88.8 (2000), pp. 12701278. Vipula Rawte, Amit Sheth, and Amitava Das. survey of hallucination in large foundation models. In: arXiv preprint arXiv:2309.05922 (2023). Pedro Rodriguez, Arthur Spirling, and Brandon Stewart. Embedding regression: Models for context-specific description and inference. In: American Political Science Review 117.4 (2023), pp. 12551274. Noveen Sachdeva et al. How to train data-efficient LLMs. In: arXiv preprint arXiv:2402.09668 (2024). Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil Feizi. Can AI-generated text be reliably detected? In: arXiv preprint arXiv:2303.11156 (2023). Matthew Sag. Copyright safety for generative AI. 2023. url: https://houstonlawreview. org/article/92126. 52 [Sah+24] [Sam23] [San+22] [San+23] [Sax+19] [SB18] [SBM22] [Sch+17] [Sch+21] Swarnadeep Saha et al. Branch-Solve-Merge improves large language model evaluation and generation. In: Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). 2024, pp. 83528370. Pamela Samuelson. Generative AI meets copyright. In: Science 381.6654 (2023), pp. 158 161. Victor Sanh et al. Multitask prompted training enables zero-shot task generalization. In: The Tenth International Conference on Learning Representations, ICLR. 2022. Shibani Santurkar et al. Whose opinions do language models reflect? In: International Conference on Machine Learning. PMLR. 2023, pp. 2997130004. David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. In: 7th International Conference on Learning Representations, ICLR. 2019. Richard S. Sutton and Andrew G. Barto. Reinforcement learning: An introduction. Bradford Book, 2018. isbn: 0262039249. Lee Sharkey, Dan Braun, and Beren Millidge. Taking features out of superposition with sparse autoencoders. In: AI Alignment Forum. Vol. 6. 2022, pp. 1213. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. In: arXiv preprint arXiv:1707.06347 (2017). Tal Schuster, Adam Fisch, Tommi Jaakkola, and Regina Barzilay. Consistent accelerated inference via confident adaptive transformers. In: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Nov. 2021, pp. 49624979. [Sch+22a] Patrick Schramowski, Cigdem Turan, Nico Andersen, Constantin Rothkopf, and Kristian Kersting. Large pre-trained language models contain human-like biases of what is right and wrong to do. In: Nature Machine Intelligence 4.3 (2022), pp. 258268. [Sch+22b] Tal Schuster et al. Confident adaptive language modeling. In: Advances in Neural Information Processing Systems 35 (2022), pp. 1745617472. [SCN18] [SDG06] [Sek+21] [Sha+24] [Shi+24] [Shu+22] [Shu+23] [Sim+24] [Sin+23] [Sin+24] Karan Singla, Dogan Can, and Shrikanth Narayanan. multi-task approach to learning multilingual representations. In: Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). July 2018, pp. 214220. Holger Schwenk, Daniel Dechelotte, and Jean-Luc Gauvain. Continuous space language models for statistical machine translation. In: Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions. 2006, pp. 723730. Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh. Remember what you want to forget: Algorithms for machine unlearning. In: Advances in Neural Information Processing Systems 34 (2021), pp. 1807518086. Vasu Sharma et al. Text quality-based pruning for efficient training of language models. In: arXiv preprint arXiv:2405.01582 (2024). Claudia Shi et al. Hypothesis testing the circuit hypothesis in LLMs. In: Advances in Neural Information Processing Systems 38, NeurIPS 2024. 2024. Kurt Shuster et al. Blenderbot 3: deployed conversational agent that continually learns to responsibly engage. In: arXiv preprint arXiv:2208.03188 (2022). Ilia Shumailov et al. The curse of recursion: Training on generated data makes models forget. In: arXiv preprint arXiv:2305.17493 (2023). Adi Simhi, Jonathan Herzig, Idan Szpektor, and Yonatan Belinkov. Constructing benchmarks and interventions for combating hallucinations in LLMs. In: arXiv preprint arXiv:2404.09971 (2024). Karan Singhal et al. Towards expert-level medical question answering with large language models. In: arXiv preprint arXiv:2305.09617 (2023). Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. long way to go: Investigating length correlations in RLHF. In: arXiv preprint arXiv:2310.03716 (2024). 53 [SK21] [SLG20] [Slo+23] [Smi+22] [Sne+24] [SNR21] [Sol+24] Selva Birunda and Kanniga Devi. review on word embedding techniques for text classification. In: Innovative Data Communication Technologies and Application: Proceedings of ICIDCA 2020 (2021), pp. 267281. Budhaditya Saha, Sanal Lisboa, and Shameek Ghosh. Understanding patient complaint characteristics using contextual clinical BERT embeddings. In: arXiv preprint arXiv:2002.05902 (2020). Aviv Slobodkin, Omer Goldman, Avi Caciularu, Ido Dagan, and Shauli Ravfogel. The curious case of hallucinatory (un) answerability: Finding truths in the hidden states of over-confident large language models. In: Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 2023, pp. 36073625. Shaden Smith et al. Using DeepSpeed and Megatron to train Megatron-Turing NLG 530B, large-scale generative language model. In: arXiv preprint arXiv:2201.11990 (2022). Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute optimally can be more effective than scaling model parameters. In: arXiv preprint arXiv:2408.03314 (2024). Yi-Lin Sung, Varun Nair, and Colin Raffel. Training neural networks with fixed sparse masks. In: Advances in Neural Information Processing Systems. Vol. 34. Curran Associates, Inc., 2021, pp. 2419324205. Luca Soldaini et al. Dolma: An open corpus of three trillion tokens for language model pretraining research. In: Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2024, pp. 1572515788. [Son+23] Ziang Song, Tianle Cai, Jason Lee, and Weijie Su. Reward collapse in aligning large language models. In: arXiv preprint arXiv:2305.17608 (2023). [Spa72] [SSB22] [Sti+20] [Su+24] [Sun+20] Karen Sparck Jones. statistical interpretation of term specificity and its application in retrieval. In: Journal of documentation 28.1 (1972), pp. 1121. Danielle Saunders, Rosie Sallis, and Bill Byrne. First the worst: Finding better gender translations during beam search. In: Findings of the Association for Computational Linguistics: ACL. May 2022, pp. 38143823. Nisan Stiennon et al. Learning to summarize from human feedback. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. NIPS 20. Curran Associates Inc., 2020. isbn: 9781713829546. Jianlin Su et al. RoFormer: Enhanced transformer with rotary position embedding. In: Neurocomputing 568 (2024), p. 127063. Yu Sun et al. ERNIE 2.0: continual pre-training framework for language understanding. In: Proceedings of the AAAI Conference on Artificial Intelligence 34.05 (Apr. 2020), pp. 89688975. [SWF+15] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. In: Advances in neural information processing systems 28 (2015). [SZ23] [Szo24] [Tan+23] [Tan+24a] [Tan+24b] Karanpartap Singh and James Zou. New evaluation metrics capture quality degradation due to LLM watermarking. In: Transactions on Machine Learning Research (2023). Peter Szolovits. Large language models seem miraculous, but science abhors miracles. In: NEJM AI (May 2024). Ruixiang Tang, Xiaotian Han, Xiaoqian Jiang, and Xia Hu. Does synthetic data generation of LLMs help clinical text mining? In: arXiv preprint arXiv:2303.04360 (2023). Sijun Tan et al. JudgeBench: benchmark for evaluating LLM-based judges. In: arXiv preprint arXiv:2410.12784 (2024). Zhengyang Tang, Xingxing Zhang, Benyou Wang, and Furu Wei. MathScale: Scaling instruction tuning for mathematical reasoning. In: Forty-first International Conference on Machine Learning, ICML. 2024. 54 [Tay+23] [Tea+25] [Tes+24] [TGH23] [Tho+22] [Thr+22] [Tim23] Yi Tay et al. Scaling laws vs model architectures: How does inductive bias influence scaling? In: Findings of the Association for Computational Linguistics: EMNLP. 2023, pp. 1234212364. Kimi Team et al. Kimi k1.5: Scaling reinforcement learning with LLMs. In: arXiv preprint arXiv:2405.02225 (2025). Idit Tessler et al. Unlocking the potential of large language models in healthcare: navigating the opportunities and challenges. In: Future Medicine AI 2.1 (Apr. 2024). Hoang Tran, Chris Glaze, and Braden Hancock. Iterative DPO alignment. Tech. rep. Snorkel AI, 2023. Romal Thoppilan et al. LaMDA: Language models for dialog applications. In: arXiv preprint arXiv:2201.08239 (2022). Christos Thrampoulidis, Ganesh Ramachandra Kini, Vala Vakilian, and Tina Behnia. Imbalance trouble: Revisiting neural-collapse geometry. In: Advances in Neural Information Processing Systems 35 (2022), pp. 2722527238. The New York Times. The times sues OpenAI and Microsoft over AI use of copyrighted work. America. Dec. 2023. url: https://www.nytimes.com/2023/12/27/business/ media/new-york-times-open-ai-microsoft-lawsuit.html. [Tou+23a] Hugo Touvron et al. Llama 2: Open foundation and fine-tuned chat models. In: arXiv preprint arXiv:2307.09288 (2023). [Tou+23b] Hugo Touvron et al. LLaMA: Open and efficient foundation language models. In: arXiv preprint arXiv:2302.13971 (2023). [TRB10] [Tsa+19] [TT24] [Tu+24] [Tul+24] [Tyl23] [TZL24] [UZM24] [Vak+22] [Val+23] Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio. Word representations: simple and general method for semi-supervised learning. In: Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. 2010, pp. 384394. Yao-Hung Hubert Tsai et al. Multimodal transformer for unaligned multimodal language sequences. In: Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL. 2019, pp. 65586569. Jacob Trauger and Ambuj Tewari. Sequence length independent norm-based generalization bounds for transformers. In: International Conference on Artificial Intelligence and Statistics. PMLR. 2024, pp. 14051413. Xinming Tu, James Zou, Weijie Su, and Linjun Zhang. What should data science education do with large language models? In: Harvard Data Science Review 6.1 (2024). Eduard Tulchinskii et al. Intrinsic dimension estimation for robust detection of AIgenerated texts. In: Advances in Neural Information Processing Systems. Vol. 36. 2024. Thaddee Tyl. Chinchillas death. Accessed: 2022-05-24. 2023. url: https://espadrine. github.io/blog/posts/chinchilla-s-death.html. Brian Tufts, Xuandong Zhao, and Lei Li. An examination of AI-generated text detectors across multiple domains and models. In: Neurips Safe Generative AI Workshop 2024. 2024. Dennis Ulmer, Chrysoula Zerva, and Andre Martins. Non-exchangeable conformal language generation with nearest neighbors. In: Findings of the Association for Computational Linguistics: EACL. Mar. 2024, pp. 19091929. Thomas Vakili, Anastasios Lamproudis, Aron Henriksson, and Hercules Dalianis. Downstream task performance of bert models pre-trained using automatically de-identified clinical data. In: Proceedings of the Thirteenth Language Resources and Evaluation Conference. 2022, pp. 42454252. Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, and Ali Ghodsi. DyLoRA: Parameter-efficient tuning of pre-trained models using dynamic search-free low-rank adaptation. In: Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. May 2023, pp. 32743287. [Vas+17] [Ver+23] [VGS05] [Vin22] [VKB23] [Von+23] [Wal+19] Ashish Vaswani et al. Attention is all you need. In: Advances in Neural Information Processing Systems. Vol. 30. Curran Associates, Inc., 2017. Shreyas Verma, Kien Tran, Yusuf Ali, and Guangyu Min. Reducing LLM hallucinations using epistemic neural networks. In: arXiv preprint arXiv:2312.15576 (2023). Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. Algorithmic learning in random world. Vol. 29. Springer, 2005. James Vincent. AI-generated answers temporarily banned on coding Q&A site Stack Overflow. In: The Verge 5 (2022). Nikhil Vyas, Sham Kakade, and Boaz Barak. On provable copyright protection for generative models. In: International Conference on Machine Learning. PMLR. 2023, pp. 3527735299. Johannes Von Oswald et al. Transformers learn in-context by gradient descent. In: International Conference on Machine Learning. PMLR. 2023, pp. 3515135174. Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. Universal adversarial triggers for attacking and analyzing NLP. In: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP. 2019, pp. 2153 2162. [Wan+22] Xiaozhi Wang et al. Finding skill neurons in pre-trained transformer-based language models. In: Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022, pp. 1113211152. [Wan+23a] Yixin Wan et al. Kelly is warm person, Joseph is role model: Gender Biases in LLM-Generated Reference Letters. In: Findings of the Association for Computational Linguistics: EMNLP. Dec. 2023, pp. 37303748. [Wan+23b] Kevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in the wild: circuit for indirect object identification in GPT-2 Small. In: The Eleventh International Conference on Learning Representations, ICLR. 2023. [Wan+23c] Tianlu Wang et al. Shepherd: critic for language model generation. In: arXiv preprint arXiv:2308.04592 (2023). [Wan+23d] Xuezhi Wang et al. Self-consistency improves chain of thought reasoning in language models. In: The Eleventh International Conference on Learning Representations, ICLR. 2023. [Wan+23e] Yizhong Wang et al. Self-instruct: Aligning language models with self-generated instructions. In: Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023, pp. 1348413508. [Wan+24a] Jiachen Wang, Zhun Deng, Hiroaki Chiba-Okabe, Boaz Barak, and Weijie Su. An economic solution to copyright challenges of generative AI. In: arXiv preprint arXiv:2404.13964 (2024). [Wan+24b] Yidong Wang et al. PandaLM: An automatic evaluation benchmark for LLM instruction tuning optimization. In: International Conference on Learning Representations (ICLR). 2024. [Wan+24c] Zifeng Wang et al. CodecLM: Aligning Language Models with Tailored Synthetic Data. In: 2024. [Wan+24d] Zixuan Wang, Stanley Wei, Daniel Hsu, and Jason D. Lee. Transformers provably learn sparse token selection while fully-connected nets cannot. In: Proceedings of the 41st International Conference on Machine Learning. ICML24. JMLR.org, 2024. [WBU11] Jason Weston, Samy Bengio, and Nicolas Usunier. Wsabie: Scaling up to large vocabulary image annotation. In: Twenty-Second International Joint Conference on Artificial Intelligence. Citeseer. 2011. 56 [WCH23] [WCM22] [Web+23] [Wei+21] [Wei+22a] [Wei+22b] [Wen+24] [Wet+24] [Wil23] [WLJ23] [WLL25] [Woo+23] [Wou24] [WP24] [WS23] [WTU20] [Wu+23a] [Wu+23b] [Wu+24a] [Wu+24b] Rui Wang, Pengyu Cheng, and Ricardo Henao. Toward fairness in text generation via mutual information minimization based on importance sampling. In: International Conference on Artificial Intelligence and Statistics. PMLR. 2023, pp. 44734485. Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation: case study on approximating Turing machines with transformers. In: Advances in Neural Information Processing Systems 35 (2022), pp. 1207112083. Debora Weber-Wulff et al. Testing of detection tools for AI-generated text. In: International Journal for Educational Integrity 19.1 (2023), p. 26. Laura Weidinger et al. Ethical and social risks of harm from language models. In: arXiv preprint arXiv:2112.04359 (2021). Jason Wei et al. Chain-of-thought prompting elicits reasoning in large language models. In: Advances in neural information processing systems 35 (2022), pp. 2482424837. Jason Wei et al. Finetuned language models are zero-shot learners. In: International Conference on Learning Representations. 2022. Kaiyue Wen, Huaqing Zhang, Hongzhou Lin, and Jingzhao Zhang. From sparse dependence to sparse attention: Unveiling how chain-of-thought enhances transformer sample efficiency. In: arXiv preprint arXiv:2410.05459 (2024). Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen. QuRating: Selecting high-quality data for training language models. In: Forty-first International Conference on Machine Learning, ICML. 2024. David Williams. Systems and methods for automatically scrubbing sensitive data. US Patent 11,645,458. May 2023. Yuanhao Wang, Qinghua Liu, and Chi Jin. Is RLHF more difficult than standard RL? theoretical perspective. In: Advances in Neural Information Processing Systems 36 (2023), pp. 7600676032. Ting Wu, Xuefeng Li, and Pengfei Liu. Progress or regress? Self-improvement reversal in post-training. In: The Thirteenth International Conference on Learning Representations. 2025. Tae-Jin Woo, Woo-Jeoung Nam, Yeong-Joon Ju, and Seong-Whan Lee. Compensatory debiasing for gender imbalances in language models. In: ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. 2023, pp. 15. Bram Wouters. Optimizing watermarks for large language models. In: International Conference on Machine Learning. 2024. Robert Wu and Vardan Papyan. Linguistic collapse: Neural collapse in (large) language models. In: Advances in Neural Information Processing Systems 38, NeurIPS 2024. 2024. Jason Weston and Sainbayar Sukhbaatar. System 2 attention (is something you might need too). In: arXiv preprint arXiv:2311.11829 (2023). Dirk Weissenborn, Oscar Tackstrom, and Jakob Uszkoreit. Scaling autoregressive video models. In: 8th International Conference on Learning Representations, ICLR. 2020. Xinwei Wu et al. DEPN: Detecting and editing privacy neurons in pretrained language models. In: Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Dec. 2023, pp. 28752886. Yihan Wu, Zhengmian Hu, Hongyang Zhang, and Heng Huang. DiPmark: stealthy, efficient and resilient watermark for large language models. In: arXiv preprint arXiv:2310.07710 (2023). Tianhao Wu et al. Meta-rewarding language models: Self-improving alignment with LLM-as-a-meta-judge. In: arXiv preprint arXiv:2407.19594 (2024). Yihan Wu et al. Distortion-free watermarks are not truly distortion-free under watermark key collisions. In: arXiv preprint arXiv:2406.02603 (2024). [WZT23] Ruijia Wu, Linjun Zhang, and Tony Cai. Sparse topic modeling: Computational efficiency, near-optimal algorithms, and statistical inference. In: Journal of the American Statistical Association 118.543 (2023), pp. 18491861. [Xia+24a] Haojun Xia et al. FP6-LLM: Efficiently serving large language models through FP6centric algorithm-system co-design. In: arXiv preprint arXiv:2401.14112 (2024). [Xia+24b] Xun Xian et al. RAW: robust and agile plug-and-play watermark framework for AIgenerated images with provable guarantees. In: The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024. [Xia+24c] [Xia+24d] [Xie+22] [Xie+23] [Xie+24a] [Xie+24b] [Xu+23] [Xu+24a] [Xu+24b] [Xu23] [Yad+24] [Yan+19] [Yan+23] [Yan+24] [Yao+23] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In: The Twelfth International Conference on Learning Representations, ICLR. 2024. Jiancong Xiao et al. On the algorithmic bias of aligning large language models with RLHF: Preference collapse and matching regularization. In: arXiv preprint arXiv:2405.16455 (2024). Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit Bayesian inference. In: The Tenth International Conference on Learning Representations, ICLR. 2022. Sang Michael Xie et al. DoReMi: Optimizing data mixtures speeds up language model pretraining. In: Advances in Neural Information Processing Systems 36, NeurIPS 2023. 2023. Qianqian Xie et al. Me LLaMA: Foundation large language models for medical applications. In: arXiv preprint arXiv:2402.12749 (Feb. 2024). Yangxinyu Xie, Xiang Li, Tanwi Mallick, Weijie Su, and Ruixun Zhang. Debiasing watermarks for large language models via maximal coupling. In: arXiv preprint arXiv:2411.11203 (2024). Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, and Jason Weston. Some things are more cringe than others: Preference optimization with the pairwise cringe loss. In: arXiv preprint arXiv:2312.16682 (2023). Haoran Xu et al. Contrastive preference optimization: Pushing the boundaries of LLM performance in machine translation. In: Forty-first International Conference on Machine Learning, ICML. 2024. Yuhui Xu et al. QA-LoRA: Quantization-aware low-rank adaptation of large language models. In: The Twelfth International Conference on Learning Representations, ICLR. 2024. Ming Xu. MedicalGPT: Training medical GPT model. 2023. url: https://github.com/ shibing624/MedicalGPT. Yasin Abbasi Yadkori et al. Mitigating LLM hallucinations via conformal abstention. In: arXiv preprint arXiv:2405.01563 (2024). Zhilin Yang et al. XLNet: generalized autoregressive pretraining for language understanding. In: Proceedings of the 33rd International Conference on Neural Information Processing Systems. Curran Associates Inc., 2019. Ke Yang, Charles Yu, Yi Fung, Manling Li, and Heng Ji. Adept: debiasing prompt framework. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. 2023, pp. 1078010788. Xianjun Yang et al. DNA-GPT: Divergent n-gram analysis for training-free detection of GPT-generated text. In: The Twelfth International Conference on Learning Representations, ICLR. 2024. Shunyu Yao et al. Tree of thoughts: Deliberate problem solving with large language models. In: Advances in Neural Information Processing Systems 36, NeurIPS 2023. 2023. 58 [Ye+24a] [Ye+24b] [Ye+24c] [YLG24] [YNL21] [Yu+21] [Yu+23] Fanghua Ye et al. Benchmarking LLMs via uncertainty quantification. In: arXiv preprint arXiv:2401.12794 (2024). Tian Ye, Zicheng Xu, Yuanzhi Li, and Zeyuan Allen-Zhu. Physics of language models: Part 2.1, grade-school math and the hidden reasoning process. In: arXiv preprint arXiv:2407.20311 (2024). Tian Ye, Zicheng Xu, Yuanzhi Li, and Zeyuan Allen-Zhu. Physics of language models: Part 2.2, How to learn from mistakes on grade-school math problems. In: arXiv preprint arXiv:2408.16293 (2024). Weizhe Yuan, Pengfei Liu, and Matthias Galle. LLMCrit: Teaching large language models to use criteria. In: Findings of the Association for Computational Linguistics: ACL. 2024, pp. 79297960. Weizhe Yuan, Graham Neubig, and Pengfei Liu. BARTScore: Evaluating generated text as text generation. In: Advances in Neural Information Processing Systems 34, NeurIPS 2021. 2021, pp. 2726327277. Da Yu et al. Differentially private fine-tuning of language models. In: International Conference on Learning Representations. 2021. Charles Yu, Sullam Jeoung, Anish Kasi, Pengfei Yu, and Heng Ji. Unlearning bias in language models by partitioning gradients. In: Findings of the Association for Computational Linguistics: ACL. 2023, pp. 60326048. [Yu+24] Ping Yu, Jing Xu, Jason Weston, and Ilia Kulikov. Distilling system 2 into system 1. In: arXiv preprint arXiv:2407.06023 (2024). [Yua+24] Weizhe Yuan et al. Self-rewarding language models. In: Forty-first International Conference on Machine Learning, ICML. 2024. [Yun+20a] Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J. Reddi, and Sanjiv Kumar. Are Transformers universal approximators of sequence-to-sequence functions? In: 8th International Conference on Learning Representations, ICLR. 2020. [Yun+20b] Chulhee Yun et al. O(n) connections are expressive enough: Universal approximability of sparse transformers. In: Advances in Neural Information Processing Systems 33 (2020), pp. 1378313794. [YW21] [YXL24] [Zel+19] [Zel+22] [Zen+24] [Zer23] [ZFB24] [Zha+22] Bo Yang and Lijun Wu. How to leverage the multimodal EHR data for better medical prediction? In: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021, pp. 329345. Yuanshun Yao, Xiaojun Xu, and Yang Liu. Large language model unlearning. In: Advances in Neural Information Processing Systems 38, NeurIPS 2024. 2024. Rowan Zellers et al. Defending against neural fake news. In: Advances in Neural Information Processing Systems. Vol. 32. 2019. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. STaR: Bootstrapping reasoning with reasoning. In: Advances in Neural Information Processing Systems 35, NeurIPS 2022. 2022. Qingcheng Zeng et al. Uncertainty is fragile: Manipulating uncertainty in large language models. In: arXiv preprint arXiv:2407.11282 (2024). ZeroGPT. ZeroGPT: Trusted GPT-4, ChatGPT and AI detector tool by ZeroGPT. 2023. url: https://www.zerogpt.com/. Ruiqi Zhang, Spencer Frei, and Peter Bartlett. Trained transformers learn linear models in-context. In: Journal of Machine Learning Research 25.49 (2024), pp. 155. Yufeng Zhang, Boyi Liu, Qi Cai, Lingxiao Wang, and Zhaoran Wang. An analysis of attention via the lens of exchangeability and latent variable models. In: arXiv preprint arXiv:2212.14852 (2022). [Zha+23] Qingru Zhang et al. AdaLoRA: Adaptive budget allocation for parameter-efficient fineTuning. In: arXiv preprint arXiv:2303.10512 (2023). 59 [Zha+24a] Wenhao Zhan, Masatoshi Uehara, Nathan Kallus, Jason Lee, and Wen Sun. Provable offline preference-based reinforcement learning. In: The Twelfth International Conference on Learning Representations. 2024. [Zha+24b] Jinghui Zhang, Dandan Qiao, Mochen Yang, and Qiang Wei. Regurgitative training: The value of real data in training large language models. In: arXiv preprint arXiv:2407.12835 (2024). [Zha+24c] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. TinyLlama: An opensource small language model. In: arXiv preprint arXiv:2401.02385 (2024). [Zha+24d] Ruiqi Zhang, Licong Lin, Yu Bai, and Song Mei. Negative preference optimization: From catastrophic collapse to effective unlearning. In: arXiv preprint arXiv:2404.05868 (2024). [Zha+24e] Ruisi Zhang, Shehzeen Samarah Hussain, Paarth Neekhara, and Farinaz Koushanfar. REMARK-LLM: robust and efficient watermarking framework for generative large language models. In: 33rd USENIX Security Symposium (USENIX Security 24). 2024, pp. 18131830. [Zha+24f] Wenting Zhao et al. WildChat: 1M ChatGPT interaction logs in the wild. In: The Twelfth International Conference on Learning Representations, ICLR. 2024. [Zha+24g] Xuandong Zhao, Chenwen Liao, Yu-Xiang Wang, and Lei Li. Efficiently identifying watermarked segments in mixed-source texts. In: arXiv preprint arXiv:2410.03600 (2024). [Zha+24h] Xufeng Zhao et al. Enhancing zero-shot chain-of-thought reasoning in large language models through logic. In: Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING). ELRA and ICCL, 2024, pp. 61446166. [Zhe+23] [Zhe+24] Lianmin Zheng et al. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. In: Advances in Neural Information Processing Systems 36, NeurIPS 2023. 2023. Lianmin Zheng et al. LMSYS-Chat-1M: large-scale real-world LLM conversation dataset. In: The Twelfth International Conference on Learning Representations, ICLR. 2024. [Zho+24a] Huiying Zhong, Zhun Deng, Weijie Su, Zhiwei Steven Wu, and Linjun Zhang. Provable multi-party reinforcement learning with diverse human feedback. In: arXiv preprint arXiv:2403.05006 (2024). [Zho+24b] Hongjian Zhou et al. survey of large language models in medicine: Progress, application, and challenge. In: arXiv preprint arXiv:2311.05112 (2024). [ZHT06] [Zie+20] [ZJJ23] [ZLM18] [ZLW24] [ZM24] [Zol+24] Hui Zou, Trevor Hastie, and Robert Tibshirani. Sparse principal component analysis. In: Journal of Computational and Graphical Statistics 15.2 (2006), pp. 265286. Daniel M. Ziegler et al. Fine-tuning language models from human preferences. In: arXiv preprint arXiv:1909.08593 (2020). Banghua Zhu, Michael Jordan, and Jiantao Jiao. Principled reinforcement learning with human feedback from pairwise or k-wise comparisons. In: International Conference on Machine Learning. PMLR. 2023, pp. 4303743067. Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adversarial learning. In: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society. 2018, pp. 335340. Xuandong Zhao, Lei Li, and Yu-Xiang Wang. Permute-and-Flip: An optimally robust and watermarkable decoder for LLMs. In: arXiv preprint arXiv:2402.05864 (2024). Chrysoula Zerva and Andre F. T. Martins. Conformalizing machine translation evaluation. In: Transactions of the Association for Computational Linguistics 12 (2024), pp. 14601478. Thomas P. Zollo et al. Prompt risk control: Rigorous framework for responsible deployment of large language models. In: The Twelfth International Conference on Learning Representations, ICLR. 2024. 60 [ZRZ24] [ZWL23] Lujing Zhang, Aaron Roth, and Linjun Zhang. Fair risk control: generalized framework for calibrating multi-group fairness risks. In: arXiv preprint arXiv:2405.02225 (2024). Xuandong Zhao, Yu-Xiang Wang, and Lei Li. Protecting language generation models via invisible watermarking. In: International Conference on Machine Learning. Vol. 202. 2023, pp. 4218742199."
        },
        {
            "title": "A Historical Development of LLMs",
            "content": "The capabilities of LLMs arise from two critical components: representation learning and language modeling. Representation learning involves learning numerical representation of input text, in way that captures their underlying relationships and patterns. Language modeling involves predicting the next word in sequence of texts, allowing for coherent and fluent text generation. A.1 Representation Learning The first step in an LLM is to transform natural language into format that computers can understand specifically vectors, matrices, and tensors. The symbolic lexical units (i.e., words) that comprise sentences require effective numerical representation strategies, known as word embeddings. Grounded on the distributional hypothesis, which posits that linguistic items that occur in similar contexts have similar meanings [Har54], such methods have gradually become more sophisticated over time. Early attempts at capturing word meanings involved bag-of-words representations, which represent documents as sparse vectors, with dimension for each element of the dictionary, for example as in TF-IDF [Spa72]. Matrix-decomposition methods were subsequently introduced, such as Latent Semantic Analysis (LSA) [Hof01; LD02], to represent words as (dense) vectors. Many such methods implicitly involved weights that were defined or learned in an unsupervised fashion. Supervised learning methods such as Support Vector Machines were later used to train bag-of-word representations that aimed directly at solving particular tasks [Joa98], leading to performance improvements. Supervised training of dense word embeddings by gradient-based or neural methods similarly gave improvements [Bai+09; WBU11], with the caveat that relatively large datasets were required. While useful, most of these methods fell short of solving challenging tasks in NLP, as they lacked the capacity to convey complex semantic and syntactic nuances of words in context. Neural methods first grew to prominence with the advent of Word2vec [Mik+13] and similar approaches such as GloVe [PSM14] which directly learn low-dimensional word representations from local word context. The specific objective functions underlying the architecture of these models were explored in foundational works such as those by [Ben+03; CW08; Col+11; TRB10], setting the stage for subsequent innovations. Word2vec, for instance, encompasses two specific models: Continuous Bag-Of-Words (CBOW) and Skip-gram. The CBOW model predicts target word based on its surrounding context, whereas the Skip-gram model predicts the surrounding context given target word. Both models utilize straightforward yet efficient neural network architecture that effectively captures semantic and syntactic word relationships [Mik+13]. These approaches generate dense word embeddings, which are compact vectors where each dimension encodes latent feature of the word. After this unsupervised training, these vectors were then often used as input or initialization for supervised training stage for the task of interest. These approaches to representation learning focused on words as the primary unit, which gave rise to several challenges, including large vocabulary size and the issue of out-of-vocabulary words. To address these challenges, the concept of tokenization was introduced. Tokenization plays crucial role in the effectiveness of these models. It involves breaking down text into smaller units, or tokens, which affects the granularity of the learned representations. popular technique used in this context is Byte Pair Encoding (BPE) [Gag94], which reduces vocabulary size by merging the most frequent pairs of bytes or characters. This method not only helps manage vocabulary size but also addresses challenges related to out-of-vocabulary words in new texts. Thus, the evolution of representation learning, language modeling, and tokenization techniques has become unified and interdependent process. A.2 Language Modeling In the early stages of language models (LMs), LMs were mostly statistical regression models, relying on the Markov assumption to predict the next word from the most recent context. The dominant models were n-gram models [Jel98; GL04; Ros00], which determines words probability based on the preceding n1 words. Challenges for these models included the need to tackle data sparsitythe issue of assigning zero probabilities to unseen words or n-grams [CG96]and to cope with the exponential number of transition probabilities that need to be estimated as increases. As was typically thus 62 Framework Key Features Use Case Tutorial Link PyTorch Dynamic graphs, community support Fine-Tuning, prototyping TensorFlow Scalability, readiness production Large-scale training JAX High-performance putation, flexibility comExperimental optimization https://pytorch.org/ tutorials/ https://www.tensorflow. org/tutorials https://jax.readthedocs. io/en/latest/tutorials. html Table 2: Core Frameworks for Model Development. Library Key Features Use Case Tutorial Link Hugging Face Transformers tokPre-trained models, enizers, multi-task support Fine-Tuning, mentation experihttps://huggingface.co/ docs/transformers/index LangChain integration, output Tool chaining Application ment develophttps://python.langchain. com/docs/introduction/ Table 3: Model Libraries and Pre-trained Models. forced to be small (e.g., 1-5 words) the idea of using such models to complete long context input seemed out of scope. Key Resources: Accelerating Progress in LLM Research The development of LLMs require combination of computational resources, robust frameworks, and specialized tools. Over the years, variety of open-source tools and libraries have been developed, significantly lowering the barrier to entry for researchers and practitioners. This section provides detailed overview of the most popular and widely adopted resources for LLM research, focusing on frameworks, model libraries, data preparation tools, and utilities for training, deployment, and analysis. B.1 Core Frameworks for Model Development PyTorch PyTorch has become the dominant deep learning framework for LLM research due to its flexibility, dynamic computation graph, and extensive community support. Its ease of debugging and seamless integration with other libraries make it ideal for developing and fine-tuning LLMs. TensorFlow and JAX TensorFlow remains competitive choice for large-scale training, particularly with its distributed computing capabilities. JAX, with its functional programming paradigm and support for automatic differentiation, is gaining traction for research prototypes and cutting-edge optimization techniques. Please see Table 2 for the comparison of those frameworks. B.2 Model Libraries and Pre-trained Models Hugging Face Transformers Hugging Faces Transformers library is cornerstone of LLM research, providing pre-trained models, tokenizers, and utilities for fine-tuning across multiple tasks. Its intuitive API and extensive documentation make it accessible to researchers at all levels. LangChain LangChain simplifies the process of integrating LLMs into applications by enabling seamless chaining of language model outputs and external tools, such as APIs or databases. Please see Table 3 for more descriptions. 63 Hugging Face Datasets clean-text Tool Key Features Use Case Tutorial Link Curated datasets, efficient streaming Loading and preprocessing datasets https://huggingface.co/ docs/datasets/index Text normalization, cleaning noisy datasets Data preparation SentencePiece Subword tokenization Text tokenization Hugging Face enizers TokSubword tokenization Text tokenization https://github.com/ jfilter/clean-text https://github.com/ google/sentencepiece https://huggingface.co/ docs/tokenizers/index Table 4: Datasets and Data Preparation Tools. B.3 Datasets and Data Preparation Tools Hugging Face Datasets Hugging Face Datasets provides vast library of curated datasets and tools for loading, filtering, and processing data. It supports efficient streaming and preprocessing pipelines, enabling researchers to work with massive datasets without excessive memory usage. The library also integrates seamlessly with Hugging Faces tokenizers and models, making it an essential tool for LLM research. Data Cleaning Tools Preprocessing is crucial for preparing datasets for LLM training. Tools like clean-text simplify the cleaning of noisy datasets by removing special characters, normalizing text, and correcting encoding issues. Additionally, tokenization libraries such as SentencePiece and Hugging Faces tokenizers library are indispensable for converting text into model-ready input formats. Please see Table 4 for summarization. B.4 Training and Fine-Tuning Utilities DeepSpeed and Megatron-LM DeepSpeed and Megatron-LM are indispensable for scaling the training of LLMs. DeepSpeeds ZeRO (Zero Redundancy Optimizer) optimization reduces memory overhead, enabling the efficient training of massive models on limited hardware. Additionally, Megatron-LM excels in model parallelism, splitting large models across multiple GPUs for distributed training. Hugging Face Fine-Tuning Frameworks Hugging Face provides an intuitive and widely used framework for fine-tuning pre-trained models on downstream tasks. Using the Trainer API, researchers can efficiently fine-tune models with minimal code. Hugging Face also supports custom training loops for more advanced fine-tuning setups. Hugging Face PEFT Frameworks Hugging Faces PEFT frameworks build on techniques like LoRA to offer additional methods for adapting large models to specific tasks while keeping the majority of the model frozen. This accelerates fine-tuning and reduces resource requirements. TRL & OpenRLHF TRL (Transformers Reinforcement Learning) and OpenRLHF are powerful libraries designed to simplify the implementation of RLHF and Direct Preference Optimization (DPO). Please see Table 5 for summarization. B."
        },
        {
            "title": "Inference and Deployment",
            "content": "vLLM and TensorRT-LLM vLLM and TensorRT-LLM are optimized tools for low-latency inference of large language models. vLLM focuses on efficient memory usage, enabling faster batch inference for serving applications. TensorRT-LLM, developed by NVIDIA, supports high-throughput 64 Tool Key Features Use Case Tutorial Link DeepSpeed MegatronLM ZeRO optimization, scalability Large-scale training https://www.deepspeed. ai/tutorials/ Model parallelism Large-scale training Hugging Face Trainer Easy fine-tuning setup, Trainer API Fine-Tuning models pre-trained https://github.com/ NVIDIA/Megatron-LM https://huggingface. co/docs/transformers/ training Hugging Face PEFT Efficient fine-tuning techniques Resource-constrained settings https://huggingface.co/ docs/peft/index OpenRLHF RLHF, DPO TRL RLHF, DPO Post training, AI alignment https://github.com/ OpenRLHF/OpenRLHF Post training, AI alignment https://huggingface.co/ docs/trl/index Table 5: Training and Fine-Tuning Utilities. Key Features Use Case Tutorial Link Tool vLLM Memory fast batch inference optimization, Production inference TensorRTLLM High-throughput GPU inference Optimized GPU model serving Hugging Face Endpoints Triton Inference Server Ray Serve Simplified API deployment Rapid integration serving with Scalable multi-framework support Scalable LLM deployment Distributed, parallel ference inScalable LLM deployment Table 6: LLM Inference and Deployment. https://github.com/ vllm-project/vllm https://github.com/ NVIDIA/TensorRT-LLM/ tree/release/0.5.0 https://huggingface.co/ inference-endpoints/ dedicated https://developer. nvidia.com/ triton-inference-server https://docs.ray.io/en/ latest/serve/index.html model inference on GPUs, taking advantage of advanced hardware accelerations. Both tools are ideal for production environments requiring high-performance LLM deployment. Triton Inference Server NVIDIAs Triton Inference Server simplifies LLM deployment by supporting multiple frameworks (e.g., PyTorch, TensorFlow, ONNX). It enables scalable, production-grade model serving with GPU and CPU backends. Hugging Face Inference Endpoints Hugging Face provides an easy-to-use platform for deploying LLMs as APIs, making it simple for developers to integrate models into their applications without managing infrastructure. Ray Serve Ray Serve is distributed model serving library that supports scaling and parallel inference for large LLMs. It integrates seamlessly with distributed computing frameworks. Please see Table 6 for more information. 65 Tool Key Features Use Case Tutorial Link Weights & Biases Experiment tracking, hyperparameter tuning Experimentation, ization visualhttps://docs.wandb.ai TensorBoard Training progress monitoring Model performance analysis https://www.tensorflow. org/tensorboard MLflow Experiment management lifecycle Comparing model versioning experiments, https://mlflow.org/docs/ latest/index.html Table 7: Visualization and Analysis. B.6 Visualization and Analysis Weights & Biases Weights & Biases (W&B) is powerful tool for experiment tracking, hyperparameter optimization, and real-time visualization of training metrics. It integrates seamlessly with most machine learning frameworks, providing interactive dashboards that display loss curves, accuracy trends, and other performance indicators. W&B is particularly useful for collaborative research, allowing teams to share results and insights easily. TensorBoard TensorBoard is widely adopted visualization toolkit for monitoring model training and evaluation. Its key features include plotting scalars (e.g., loss and accuracy), displaying images, and analyzing graph structures. TensorBoards ease of integration with TensorFlow and PyTorch makes it staple in the machine learning community. MLflow MLflow is an open-source platform for managing the lifecycle of machine learning experIt supports logging of metrics, artifacts, and parameters, as well as model versioning and iments. deployment tracking. MLflows UI allows researchers to compare experiments and optimize workflows. Please see Table 7 for summarization. B.7 Cloud and Hardware Resources Cloud computing platforms have become integral to large-scale LLM research and deployment. They provide scalable, on-demand access to powerful computational resources like GPUs, TPUs, and specialized accelerators, enabling researchers to run experiments without heavy upfront investment in infrastructure. AWS (Amazon Web Services) Offers EC2 instances optimized for deep learning, such as p4d instances equipped with NVIDIA A100 GPUs. AWS also provides SageMaker, managed service for building, training, and deploying machine learning models. Google Cloud Platform (GCP) Features TPU (Tensor Processing Unit) instances and Vertex AI, platform for scalable training and inference. GCP is especially advantageous for TensorFlow users. Microsoft Azure Provides Azure Machine Learning, suite of tools and services for machine learning workflows, alongside GPU-powered VMs optimized for deep learning tasks. Please see Table 8 for summarization. 66 Platform Key Features Use Case Tutorial Link AWS EC2 instances GPUs, SageMaker GCP TPUs, Vertex AI with Training, deployment https://aws.amazon.com/ ai/machine-learning/ Scalable training, experimentation https://cloud.google.com/ products/ai Microsoft Azure Azure ML, GPU-powered VMs Experimentation, deployment https://learn.microsoft. com/en-us/azure/ machine-learning Table 8: Cloud and Hardware Resources."
        }
    ],
    "affiliations": [
        "INRIA",
        "Meta FAIR",
        "New York University",
        "Rutgers University",
        "Stanford University",
        "UC Berkeley",
        "University of Pennsylvania"
    ]
}