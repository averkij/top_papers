{
    "paper_title": "Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models",
    "authors": [
        "Yulei Qin",
        "Gang Li",
        "Zongyi Li",
        "Zihan Xu",
        "Yuchen Shi",
        "Zhekai Lin",
        "Xiao Cui",
        "Ke Li",
        "Xing Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that the vanilla CoT exerts a negative impact on performance due to its superficial reasoning pattern of simply paraphrasing the instructions. It fails to peel back the compositions of constraints for identifying their relationship across hierarchies of types and dimensions. To this end, we propose a systematic method to boost LLMs in dealing with complex instructions via incentivizing reasoning for test-time compute scaling. First, we stem from the decomposition of complex instructions under existing taxonomies and propose a reproducible data acquisition method. Second, we exploit reinforcement learning (RL) with verifiable rule-centric reward signals to cultivate reasoning specifically for instruction following. We address the shallow, non-essential nature of reasoning under complex instructions via sample-wise contrast for superior CoT enforcement. We also exploit behavior cloning of experts to facilitate steady distribution shift from fast-thinking LLMs to skillful reasoners. Extensive evaluations on seven comprehensive benchmarks confirm the validity of the proposed method, where a 1.5B LLM achieves 11.74% gains with performance comparable to a 8B LLM. Codes and data are available at https://github.com/yuleiqin/RAIF."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 3 1 4 1 0 . 6 0 5 2 : r Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models Yulei Qin1 Gang Li1 Zongyi Li1 Zihan Xu1 Yuchen Shi1 Zhekai Lin1,2 Xiao Cui3 Ke Li1 Xing Sun1 1Tencent YouTu Lab 2Xiamen University 3The Chinese University of Hong Kong yuleiqin@tencent.com Codes & Datasets"
        },
        {
            "title": "Abstract",
            "content": "Existing large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that the vanilla CoT exerts negative impact on performance due to its superficial reasoning pattern of simply paraphrasing the instructions. It fails to peel back the compositions of constraints for identifying their relationship across hierarchies of types and dimensions. To this end, we propose systematic method to boost LLMs in dealing with complex instructions via incentivizing reasoning for test-time compute scaling. First, we stem from the decomposition of complex instructions under existing taxonomies and propose reproducible data acquisition method. Second, we exploit reinforcement learning (RL) with verifiable rule-centric reward signals to cultivate reasoning specifically for instruction following. We address the shallow, non-essential nature of reasoning under complex instructions via sample-wise contrast for superior CoT enforcement. We also exploit behavior cloning of experts to facilitate steady distribution shift from fast-thinking LLMs to skillful reasoners. Extensive evaluations on seven comprehensive benchmarks confirm the validity of the proposed method, where 1.5B LLM achieves 11.74% gains with performance comparable to 8B LLM. Codes and data are available at https://github.com/yuleiqin/RAIF."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) exhibited remarkable performance on real-world tasks [1, 2, 3, 4]. Such generalizability is built upon the instruction-following capabilities of LLMs [5, 6, 7]. To benchmark whether LLMs can produce the desired outputs under complex instructions, existing studies predominantly focus on modeling various types of constraints and rules where all requirements are expected to be satisfied simultaneously [8, 9, 10, 7]. Recently, the compositions of constraints (And, Chain, Selection, and Nested) have been systemized [11] to further enhance the complexity of instructions and demonstrate that LLMs still fail to meet expectations under intricate structures. To improve LLMs in solving complex instructions, most prior methods leverage two kinds of techniques: 1) supervised fine-tuning (SFT) [12, 13], and 2) template-guided inference [14, 15]. Despite their effectiveness, these methods are all task-specific and prone to overfitting of limited types of instructions. For the former, large amount of curated instruction-response pairs are required Preprint. Under review. Figure 1: Complex instructions with various atomic constraints and compositions pose great challenges to instruction-following capabilities of LLMs (best viewed magnified). The CoT prompting of existing LLMs often elicits shallow reasoning that blindly, mechanically responds to the request without formulation of structured analyses. In contrast to R1 and QwQ, most fask-thinking models cannot benefit from the vanilla CoT at all due to such superficial nature (see Sec. A.6). Our proposed method boosts deep reasoning of both fastand slow-thinking LLMs under complex instructions. to ensure diversity. For the latter, it is almost impossible to exhaustively enumerate the templates for problem decomposition, reflection, and refinement beforehand. Therefore, it calls upon scalable solution that is both constructive and generalizable without meticulous manual efforts. One intuitive method is to directly apply chain-of-thought (CoT) reasoning [16] on complex instructions, where LLMs might benefit from the free-form thinking for structure analysis with highlighted, valid constraints (see Fig. 1). However, our preliminary experiments show that such prospective solution brings minimal or even negative performance gains, which contrasts strikingly against its effectiveness on maths problems. We observe that such discrepancy arises from the underlying problem of \"superficial\" reasoning, where LLMs simply summarize the instructions briefly without developing the solid thinking upon the instruction itself. During such shallow, parroting-style reasoning, critical constraints and rules can be ignored and thereafter such misalignment leads to degraded performance. For maths problems, it is indispensable for LLMs to formalize step-by-step, divide-and-conquer process to achieve the final answer. On the contrary, for complex instructions, there exists no such nature or tendency of LLMs to forge deep reasoning as they are aligned to directly deliver responses without intermediate steps. Under such circumstance, we target at incentivizing reasoning that truly empowers LLMs with strategic planning and adherence to rules. In this paper, we propose to universally improve the instruction-following capabilities of LLMs under complex instructions. Inspired by the success of o1 [17], R1 [18], and QwQ [19], we resort to CoT for tackling complex instructions. We provide practical guide for cultivation of effective reasoning tailored via reinforcement learning (RL). Specifically, our method addresses the following two main challenges: 1) the data scarcity of diverse complex instructions and 2) the secret recipe behind the formulation of effective CoT. For the former, although various data synthesis methods [12, 20, 21, 22, 23, 24, 25, 14] have been proposed to pile up instructions, they do not take into consideration the taxonomy of constraint types and compositions. In contrast, we stem from the atomic rules, constraints, and their compositions to perform self-evolving across various tasks and domains. Besides, our instruction scaling is integrated with our RL-based reasoning stimulation. Both rule2 Figure 2: Illustration of the proposed method for advanced instruction-following via reasoning. based and model-based verification approaches, which serve as reward sources later, are paired with the instructions during generation. For the latter, existing approaches that seek to reproduce R1-like reasoning [26, 27, 28, 29] are all restricted to solving maths problems with pure rule-based rewards, which are inapplicable for our settings. We consequently keep an eye on the differences between maths and complex instructions, and propose our RL recipe with rule-centric reward modeling, enforcement on superior CoT, and control on policy distribution drift. Our contributions are three-fold: We propose systematic method to boost LLMs under complex instructions via self-evolving instruction synthesis and reasoning-driven reinforcement learning. We advance the RL to incentivize reasoning for complex instructions by addressing the superficial reasoning nature. To the best of our knowledge, our work is the pioneer that de-mystifies the recipe behind the cultivation of reasoning under complex instructions. We validate our effectiveness on seven comprehensive benchmarks along with extensive generalization studies across model families, sizes, and cold/warm-start reasoners."
        },
        {
            "title": "2 Related Works",
            "content": "2.1 Evaluation of Instruction Following of LLMs Tremendous benchmarks have sprouted to specifically evaluate instruction following capabilities of LLMs in terms of semantic [30, 31, 32] and format constraints [13, 33, 9]. Following the representative IFEval [7], recent studies extend the evaluation settings towards wild chat [34, 35, 36], long context [37], multi-lingual [38, 39, 40], multi-turn [41, 42], and multi-modal [43, 44] scenarios. The hierarchical categorization [45, 46, 47, 48] and down-streaming applications [49, 50, 51] are considered to collect fine-grained instruction for comprehensive evaluation. 3 In this paper, we measure the instruction following capabilities on the most common, comprehensive benchmarks including IFEval [7], ComplexBench [11], CELLO [10], CFBench [8], FB-Bench [52], FollowBench [53], and InfoBench [54]. Different from studies that merely focus on the evaluation itself, we systematically investigate general, scalable pipeline that enhances the instruction following consistently across models and benchmarks via test-time compute scaling. 2.2 Complex Rule and Constraint Following Numerous studies enhanced rule and constraint following capabilities via data engineering [55]. The top concerns are around the synthesis of complicated constraints without costly human intervention. Specifically, WizardLM [12] presents both in-breadth and in-depth evolving to generate complex instructions for tuning. Air [56] stems from the simple instruction distilled from documents and then iteratively add one or more constraints to increase complexity. Back-translation [57] is another popular technique to introduce additional constraints. Self-play [58] targets at generating constraints that can be verified by codes. SPAR [59] combines both self-play and tree-search for iterative refinement. The detailed taxonomy of rules and constraints [8, 60, 14], together with their structures and relationship [61, 11], allows precise synthesis control over instruction types. With respect to the post-training technique, most methods adopt the SFT [62] and RL [63, 64, 65, 66, 67] for preference alignment. Moreover, training-free workflows have also been proposed to decompose the instructions [68] and exploit verifiable feedback [69] for refinement until all the check-boxes are ticked [70]. Our work differs in two aspects. First, we propose pioneering method that handles all kinds of atomic constraints and their combinations. Previous studies are either limited in constraint types (e.g., solely considering rules that can be verified by codes while neglecting semantic, style constraints) or composition structures (e.g., simply satisfying all constraints at the same time without regarding chaining and branching situations). Second, we resort to RL for reasoning specifically cultivated for improving instruction understanding and following. Our method exhibits generalization on multipurpose and maths tasks, and bypasses the tedious design of decompose-and-conquer workflows. 2.3 Chain-of-Thought Reasoning Recent progress on the chain-of-thought (CoT) [16, 71, 72, 73, 74, 75] has attracted attentions for advancing cognitive capabilities LLMs. Most of these approaches prompted LLMs to break down difficult questions into multiple small units to tackle them systematically before jumping to final answers [3, 76, 77, 78, 79, 80]. The emergence of OpenAI o1 [17] and DeepSeek R1 [18] has incubated test-time compute scaling techniques for long CoT, where deep reasoning with extensive exploration and feasible reflection are encouraged [81]. Both Monte Carlo Tree Search and RL with process and outcome rewards [82, 83, 84] are also emphasized in previous academic attempts to replicate slow-thinking LLMs. Compared with existing studies that focus on the mathematic and logic problem-solving, we are aimed at advancing instruction following capabilities. Motivated by R1 [18], we incentivize reasoning with group relative policy optimization (GRPO) [85] and demonstrate that RL with rule-centric rewards ultimately pushes the limits of LLMs in following instructions."
        },
        {
            "title": "3 Methods",
            "content": "3.1 Problem Definition Our goal is to incentivize reasoning of LLM to advance its capabilities of handling and solving complex instructions (see Fig. 2). Through systematic pipeline investigation and extensive experimental analyses, we offer key insights and practical guidelines to arm LLMs with developed reasoning. Given as query that contains one or more compositional complex instructions, we consider LLM parameterized as θ to be instruction-followed if its output satisfies all the constraints and requirements mentioned in x. typical conditional distribution over the language modeling process can be denoted as πθ(ytx, y1t1), where yt denotes the t-th token of y. The chain-of-thought process CoT(y) refers to the tokens in the generated output that indicates the explanations of query intents, plannings of problem solving, and step-by-step deductions. The final answer following CoT(y) can be simply extracted by CoT(y). It is noted that for fast-thinking instructed models, the CoT prompting tokens xCoT (e.g., Lets reason step by step.) are indispensable. While for reasoning models with slow thinking nature, xCoT = is the default setting. 4 3.2 Self-Evolving Instructions with Complex Rules and Constraints To address the scarcity of complex instructions, we propose to scale up instructions with various rules, constraints and their verification criteria via self-evolving. Seed Instruction Selection We start by selecting set of seed instructions Dseed from the commonly used WildChat [34] and Alpaca [86] datasets. To ensure the diversity of Dseed, we follow [87] to tag each instruction by its topics and tasks for wide-range selection of task abilities. Details on the tagging and selection process can be found in Sec. A.3.1. Self-Instruct with Rules and Constraints We adapt self-instruct [20] under different fine-grained rules and constraints [8]. In view of the verification techniques, both code-execution [88] and LLMas-a-Judge [31] are involved to provide evaluation feedback. For the former, we prepare collection of constraint templates and their executable codes. Then, we randomly instantiate combination of rules and constraints from the pool. To ensure their mutual compatibility, pre-defined validity check is enforced to eliminate conflicts (e.g., The first paragraph must start with... and Wrap your entire reply with...). For the latter, we construct pairs of sub-instructions and scoring questions that stress on the style and semantic constraints. Such sub-instructions correspond to constraints with LLM-based evaluation (e.g., Is the answer written in the tone of Confucious?), which are complementary to those relying on code-based evaluation. An off-theshelf LLM is utilized to perform few-shot in-breadth evolution. With respect to the composition of these atomic sub-instructions, we refer to [13] for definitions of And, Chain, and Selection. These sub-instructions are assembled for the integrated instructions. Details can be found in the Sec. A.3.2. Response Generation and Quality Check We use LLMs to generate responses and filter out low-quality query-response pairs that fail to pass the associated verification tests. Additionally, we observe that the self-evolved instructions still contain unreasonable constraints or nonsensical queries (e.g., Give me very short, concise, and clear response...The response should have 4 sections.). Under such circumstance, we summarize seven typical issues and curate judgment prompts for LLMs to double-check the instructions and keep the valid ones (see Sec. A.3.3). 3.3 Reinforcement Learning for Reasoning Under Complex Instructions We propose to incentivize reasoning of LLMs via RL [5, 18]. The development of CoT is optimized towards being structured and sophisticated, which ultimately leads to improved final answers. Without loss of generality we adopt the GRPO [85] algorithm. Given query sampled from the distribution of complex instructions, the policy model πθold from the previous iteration generates group of individual outputs {yi i=1. GRPO updates the policy πθ by maximizing the objective: } JGRPO = ExD,{yi}G i=1πθold (x) 1 i=1 GRPO, GRPO =[ 1 yi yi t=1 (min(ri t(θ) ˆAi t, clip(ri t(θ), 1 ϵ, 1 + ϵ) ˆAi t) βDi KL(πθπref)], ri = πθ(yi πθold (yi tx, yi tx, yi 1t1) 1t1) , ˆAi = ri mean({Ri } std({Ri}G i=1) i=1) , Di KL(πθπref) = πref(yi πθ(yi tx, yi tx, yi 1t1) 1t1) log πref(yi πθ(yi tx, yi tx, yi 1t1) 1t1) 1. (1) (2) (3) Compared with the proximal policy optimization (PPO) [89], the advantage of the i-th output response is computed by normalizing the group-level rewards {Ri i=1. PPO optimizes an additional critic } model as value function via the generalized advantage estimation (GAE) [90]. In consideration of the simplicity and our available computing resources, we employ the GRPO as our scale-up RL settings. 5 Rule-Centric Reward Modeling To explicitly distinguish the reasoning from the answer contents, we employ simple minimalist rule-based format reward that checks the existence of \"<think>\", \"</think>\", \"<answer>\" and \"</answer>\" tags. The format reward encourages the thinking contents encolosed solely between \"<think>\" and \"</think>\". It is noted that for R1-series reasoning models, \"<answer>\" and \"</answer>\" tags are not necessary for answer extraction. Ri format = { +1 1 if 1(<think>...</think><answer>...</answer> yi otherwise. ), (4) With respect to the accuracy reward, the answer contents are extracted for comparison and evaluation only if the format constraint is satisfied. Compared with the maths problems that each has an exclusive ground-truth, the correct responses to complex instructions can vary greatly. Therefore, there exists no rigorous exact-match assessment [91]. Instead, we propose the rule-centric accuracy rewards that stem from verification standards. Specifically, we take into consideration the evaluation of responses in following each constraint and indicate its satisfaction condition as rewards. Given an j=1, cj x, the sampled response yi is instruction that contains atomic constraints xC = {cj} judged as instruction-followed only if all the valid, active constraints cj xactive xC are satisfied. Accordingly, piecewise reward function is defined via measurement is_followed(yi ): Ri accuracy = +2 2 j=1 1(cj xactive && is_followed(yi cj )) 1(cj xactive ) if is_followed(yi elif is_followed(yi otherwise. cj) cj xactive , cj) cj xactive , (5) It is noted that the detailed implementation of the verification depends on the constraints [8]. For the lexical-level constraints (e.g., keywords and phrases), the numerical constraints (e.g., letters and words), and the format constraint (e.g., JSON, XML, LaTeX, HTML, and Markdown), we resort to simple heuristics with python which provides precise feedback [7, 58]. In contrast, for the semantic-level constraints (e.g., themes and perspectives), stylistic constraint (e.g., writing styles, tones, and role-plays), linguistic constraints (e.g., dialects and morphologies), we bring in the reward model rϕ for judgment [13, 54, 11]. However, different from the original GRPO where rϕ implicitly scores the responses in terms of instruction following, we explicitly employ rϕ to check the following conditions of constraints. Besides, rϕ delivers scalar scoring in previous studies while we request for efficient boolean validation (True or False). Since multiple constraints might be active for steering generation, we develop the piecewise reward that promotes more constraint satisfaction while penalizing greatly the extreme cases. In total, our rule-centric reward is defined below. Ri = Ri format + Ri accuracy. (6) Experience Replay Buffer with Superior CoT Enforcement Compared with the maths tasks, instruction-following differs in that their reasoning processes are not compulsory. For maths, the stepby-step decomposition and derivation is prerequisite to obtaining the final answer, which is naturally cultivated [26, 27, 28, 29]. However, in the context of complex instructions, responses are readily accessible even without deliberate reasoning. Therefore, there exists no enforced association between the emergence of long, deep reasoning and the improved responses. In this case, we implement an adaptive replay buffer to enforce superior CoT at the sample level. We introduce the πθold for providing the sample-wise contrast between the responses with and without reasoning. The output without the essential CoT ˆyi (i.e., <think>nn</think>) receives its accuracy reward ˆRi accuracy. i=1 are inferior to those reasoning-free counterparts {ˆyi We filter out when all its rollouts {yi i=1: } ˆJ GRPO =ExD,{yi}G 1(max({Ri i=1πθold (x),{ˆyi}G i=1πθold (x,CoT(ˆyi)=) i=1) min({ ˆRi i=1) accuracy} accuracy} GRPO. (7) } We evaluate whether the yi benefits from the reasoning by comparing its reward Ri accuracy with respect to ˆRi accuracy. If all the responses are judged worse than the vanilla output, it implies that the reasoning capacity of the policy model fails to meet the standard (e.g., constraints ignorance or mis-interpretation). In this case, the sample is too challenging to foster proper reasoning and can be simply skipped until that at least one rollout designates the paradigm leading to superior reward. Policy Distribution Drift Control with Behavior Cloning Another fundamental difference between maths problems and complex instructions is that the former merely emphasizes the correctness of final answers while the latter also assesses responses in terms of semantics. During the rollout sampling, responses that meet more constraints are prioritized even at the expense of coherence, fluency, idiomaticity, and clarity. Such semantic-level degradation may not be easily resolved due to the constraints imposed on the instructions, as the compliant responses inherently differ from the pretraining texts. In light of this statement, challenge arises from the excessive policy distribution drift where the catastrophic forgetting of the initially acquired knowledge from πref occurs. We propose to explicitly perform behavior cloning of expert response under x. JSFT = ExD[ log πθ(yx)]. (8) Compared with the KL-penalty term (Eq. 3), the behavior cloning by SFT explicitly constrain the πθ for semantic alignment. It guarantees that: 1) the adherence to the expected format can be expedited for successful parsing of answers and reward computation at an early stage; 2) the organization of deep reasoning can be imitated and traced even with models of incompetent instruction following basis; 3) the potential reward hacking (e.g., responses that satisfy constraints but exhibit poor semantics) can be mitigated without relying on well-trained reward model for scalar scores."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup Dataset Details on statistics about our self-evolving dataset can be found in Sec. A.3. Additionally, we incorporated the DeepScaleR [28] maths dataset to facilitate reasoning development (see Sec. A.4). The evaluation metrics for each benchmark are reported in Sec. A.2.2. Baselines We compared with: 1) I/O: direct input with xCoT = ; 2) CoT: reasoning prompting [16] to first deliver the thought and then the answer; 3) SDC: self-DeepClaude [92, 93] technique that first prompts for the thought and then packs the original input with the thought as new context for the answer (see Sec. A.5.3); 4) SFT: supervised fine-tuning for learning the aligned responses. Implementation Details We use OpenRLHF [94] for both cold-start (Qwen2.5-1.5B/7B [95], LLaMA3.1-8B [2], and Ministral-8B [96]) and warm-start (DeepSeek-Qwen1.5B/7B [18] and DeepScaleR-1.5B [28]) experiments. Detailed settings can be found in Sec. A.5. To ensure fairness and comparability, we employ Qwen2.5-72B-Instruct as the judge for all LLM-based evaluation. 4.2 Main Results Comparison with the Baselines Our method effectively boosts most of the existing LLMs in handling complex instructions (see Table 1), demonstrating the generalization of the cultivated deep reasoning. In contrast, the CoT prompting causes drastic performance decline to all models, which further confirms the detrimental effect of shallow thinking. Instead of adopting an one-off generation, SDC decouples the reasoning and answering via two-step inference. Due to the intrinsic superficial nature, SDC still fails to improve the reasoning quality. The SFT technique directly performs knowledge distillation where small LLMs mimic the reasoning patterns of strong slow-thinker. It guarantees that the depth and breadth of thinking is under immediate supervision. However, one drawback of SFT is that the models generalization ability tends to deteriorate for samples that fall outside the domains encountered during training. Comparatively, our RL paradigm teaches LLMs how to think, driving the self-development of varied reasoning rather than simple memorization. In line with Fig. 3, small models (1.5B) achieve much more gains than larger ones, showcasing the potentials of small LLMs via test-time scaling. The DeepSeek-distilled LLMs possess good starting point for reasoning organization from their warm-start imitation across broad range of tasks and topics  (Fig. 36)  . With respect to model family, we unfortunately find that the capacity of Ministral and LLaMA is inferior to that of Qwen. The Ministral-8B exhibits limited advantages over its vanilla counterpart while the LLaMA3.1-8B experienced model collapse during training. As shown in Fig. 35(a) and (d), rapid shrinkage of response and frequent surge of KL penalty imply great deviation of LLaMA from its initial state. The reason behind might be ascribed to the pre-trained 7 Table 1: Performance on seven instruction benchmarks. Best/2nd best are marked bold/underlined. Model Method IFEval CELLO CF Bench Complex Bench FB Bench Follow Bench Info Bench Avg. Qwen2.5-1.5B-Instruct I/O Qwen2.5-1.5B-Instruct CoT Qwen2.5-1.5B-Instruct SDC Qwen2.5-1.5B-Instruct SFT Qwen2.5-1.5B-Instruct Ours I/O DeepSeek-Qwen1.5B SFT DeepSeek-Qwen1.5B DeepSeek-Qwen1.5B Ours I/O DeepScaleR-1.5B SFT DeepScaleR-1.5B Ours DeepScaleR-1.5B I/O CoT SDC SFT Ours Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct LLaMA3.1-8B-Instruct I/O LLaMA3.1-8B-Instruct CoT LLaMA3.1-8B-Instruct SDC LLaMA3.1-8B-Instruct SFT LLaMA3.1-8B-Instruct Ours Ministral-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct I/O CoT SDC SFT Ours I/O SFT Ours 45.28 28.65 41.95 65.61 44.91 36.04 45.29 57.67 41.77 48.24 55.63 72.82 69.50 60.44 72.45 70.06 77.63 60.44 80.22 77.26 13.49 59.51 48.79 58.59 68.57 72. 71.00 59.30 66.10 71.20 73.50 62.50 63.20 69.00 65.00 62.90 67.30 76.50 75.20 72.60 77.50 79.20 75.20 65.50 71.00 75.80 4.6 76.20 61.90 63.60 66.30 72.6 36.00 22.00 30.00 48.00 53.66 27.99 25.33 40.00 30.00 28.00 39.33 64.33 61.66 65.66 63.33 65.00 56.99 47.66 58.33 54.00 1.33 62.33 49.66 56.99 48.66 59. 50.97 32.94 41.70 57.46 63.92 39.89 35.53 44.38 40.70 36.68 43.23 74.47 72.00 76.53 74.23 77.40 69.11 56.54 68.73 65.24 2.71 70.03 61.31 68.32 67.20 70.45 39.81 37.31 36.52 42.75 58.67 34.51 37.59 37.78 40.24 35.72 37.81 59.29 42.65 60.07 58.76 64.45 46.92 32.34 38.36 40.16 7.14 54.54 39.17 48.06 37.26 54. 40.00 29.28 37.39 56.47 59.82 20.29 22.18 37.79 26.01 26.50 36.80 75.03 74.86 76.09 75.92 75.32 53.52 37.36 48.92 59.56 1.08 73.49 61.75 69.37 54.37 76.08 50.61 71.24 62.22 38.81(-11.79%) 45.89(-4.71%) 67.55 76.22 59.67(+9.06%) 81.95 62.35(+11.74%) 52.00 39.03 40.15(+1.12%) 51.96 60.48 49.58(+10.54%) 60.31 54.22 60. 43.43 41.75(-1.67%) 48.60(+5.17%) 72.58 68.28(-4.29%) 71.18(-1.39%) 72.36(-0.21%) 73.44(+0.85%) 67.01 85.60 82.13 86.88 84.31 82.67 71.52 58.48 54.53(-12.48%) 65.24(-1.77%) 72.89 64.92(-2.09%) 65.30 4.06(-62.95%) 0.51 68.58 84.00 79.73 57.47(-11.11%) 64.14(-4.43%) 84.08 59.85(-8.72%) 76.62 68.68(+0.10%) 75.33 DeepSeek-Qwen7B DeepSeek-Qwen7B DeepSeek-Qwen7B The default outputs of reasoning models by I/O prompting contain both the thought and the answer parts. 65.73 64.85(-0.88%) 66.35(+0.62%) 66.86 58.42 62. 62.80 65.96 59.38 57.99 58.66 58.67 79.64 79.15 82.00 59.59 55.60 59.65 72.39 69.10 71.40 60.81 67.09 71. knowledge of base models [97]. LLaMA tends to generate endless thinking without conforming to the required format. It then struggles to output semantically consistent responses and keeps extending its meaningless thinking until collapse. Detailed results can be found in Sec. A.7.1. Discussions on generalization over multi-purpose benchmarks can be found in Sec. A.7.2. Table 2: Performance on ComplexBench (Qwen2.5-7B-Instruct). Best/2nd best are marked bold/underlined. OD, SC, CNFR, FC, and SR stand for the Oracle Decomposition [11], SelfConsistency [98], Conifer [14], FollowComplex [13], and Self-Refine [99]. Category ND I/O OD SC CNFR FC SR Ours And Chain Avg. Selection Avg. Selection & Chain Avg. Overall 1 1 2 - 1 2 3 - 2 3 - - 85.85 84.27 84.03 75.10 84.77 85.66 86.57 72.18 74.68 73.54 60.95 66.27 75.25 73.96 70.56 72.70 69.63 64.43 70.66 73.07 76.88 70.96 73.18 70.57 63.59 69.60 73.59 76.18 77.25 76.61 72.08 60.52 71.67 69.61 73.39 65.61 71.83 68.23 53.25 61.96 64.34 72.92 63.39 68.45 56.13 46.04 51.70 58.67 60.75 65.67 70.49 65.83 51.92 60.92 62.69 69.16 65.64 65.94 60.81 47.33 61.07 52.01 61.06 59.70 65.77 64.08 48.53 57.65 60.41 65.00 62.68 65.85 62.44 47.93 59.36 56.20 63.03 74.47 76.26 73.76 63.51 71.97 74.00 77.40 Figure 3: The averaged number of reasoning tokens and scores over steps (best viewed magnified). Comparison with the SOTAs We implemented SOTAs on the ComplexBench (see Table 2): oracle decomposition [11] (ground-truth sub-instructions), self-consistency [98] (majority voting@10), Conifer [14] (SFT), FollowComplex [13] (SFT), and Self-Refine [99]. Our method demonstrate its superiority on the most complicated Chain, Selection categories, suggesting that the reasoning indeed assists analysis of LLMs to carry out the truly relevant, valid request with constraints. Figure 4: The averaged frequency change of keyword tokens of DeepSeek-Qwen1.5B, DeepScaleR1.5B, Qwen2.5-1.5B-Instruct, and Qwen2.5-7B-Instruct before/after RL (best viewed magnified). Variation of Reasoning Patterns The change of step-by-step keywords such as first, second, next, and finally (see Fig. 4) shows that all LLMs enjoy an increase of tokens on challenging benchmarks such as CFBench and ComplexBench, confirming the importance of our cultivated deep reasoning. For instructions without intricate compositions (e.g., And-constraints in IFEval), slow-thinking LLMs reduced their keyword frequency bit due to the shortened response length. 4.3 Ablation Study and Analysis Effect of Maths Problems Both Tables 1 and 3 corroborate the positive roles of DeepScaleR in developing reasoning. The increment of maths problems is positively associated with the growth of CoT tokens and thereafter the performance improvement (see Fig. 37), implying that the mathematic reasoning is crucial and supplementary to the general-purpose reasoning. However, given the same training steps, the model with full maths did not converge yet, suggesting that more iterations are required towards optimum. Discussions on maths generalization are in Sec. A.7.3. 9 Table 3: Ablation study on the Qwen2.5-7B-Instruct with CoT reasoning. Best/2nd best are marked bold/underlined. Maths and Complex refer to the DeepScaleR and our self-evolved dataset, respectively. SupCoT and BC denote the superior CoT enforcement and behavior cloning, respectively. Data Method Maths Complex SupCoT BC IFEval CELLO Benchmarks (Inference w/ CoT) Follow CF Bench Bench Complex Bench FB Bench - - 0.2 1 5 1 1 1 - - 1 1 1 1 1 - - - - 69.50 - - 72.08 72.27 67.10 70.06 72.83 63.58 - - 66.72 70.05 75.20 76.80 73.90 74.70 79.20 78.80 76.90 78.10 79. 61.67 63.66 53.33 63.00 65.00 69.67 47.00 65.00 65.00 72.00 73.81 63.89 75.70 77.40 78.54 76.34 75.62 75.68 42.65 57.93 45.04 60.39 64.45 46.41 57.63 56.42 56. 74.87 74.83 64.04 70.88 75.32 79.87 65.74 76.12 75.31 Info Bench Avg. 68.28 82.13 85.73 72.12(+3.84%) 81.24 64.81(3.46%) 84.31 70.87(+2.59%) 82.67 73.44(+5.16%) 86.18 73.18(+4.90%) 87.95 67.87(-0.40%) 80.13 71.15(+2.87%) 82.66 72.05(+3.76%) Effect of Superior CoT Enforcement As shown in Fig. 5, the ratio of the samples kept with superior CoT dropped first and then improves steadily. It implies that the transition from shallow to deep reasoning is promoted during training, leading to responses with higher rewards with respect to those without deliberate reasoning. The filtering of experience replay buffer for superior CoT has the following benefits: 1) In the early stage, the fast-thinking LLMs are struggling to establish the expected CoT formats and therefore the ultimate responses often contain incompatible HTML-style elements (reason/answer tag tokens). The incorporation of LLM completion with the prefix of empty reasoning makes format compliance easier. 2) It removes the samples from participating in training where their shallow reasoning brings detriments to the ultimate answers. Such isolation prevents the flawed thinking process from receiving biased rewards for formalizing inferior, prone-to-hacking responses, which also allows the time lag for imitation of expert thinking (see Figs. 33 and 34). Figure 5: The ratio of samples kept by superior CoT and the total reward over steps of Qwen2.57B-Instruct (best viewed magnified). Effect of Behavior Cloning The immediate imitation of expert reasoning not only encourages fast-thinkers to earn format rewards but also stabilizes training and fills the gaps of rule-centric rewards. Without proper guidance, cold-start LLMs can only repeat their inherent shallow thinking and consistently receive negative feedback (as confirmed in Table 1). In this case, either model collapse or reward hacking is prone to occur and thereafter causes catastrophic distribution drift."
        },
        {
            "title": "5 Conclusion",
            "content": "We propose systematic method to incentivize reasoning of LLMs for solving complex instructions. We first address the data scarcity by developing scalable pipeline for constructing instructions and their verifications. Then, we target at the superficial reasoning of fast-thinkers via effective RL with rule-centric rewards. We pay attention to the fundamental differences of pathway dependency between maths problems and complex instructions, and propose to enforce superior CoT with behavior cloning. Extensive experiments on seven benchmarks confirm our effectiveness, providing valuable insights and guidelines for practitioners to build slow-thinkers under various compelx tasks. Broader Impact Our recipe of scaling up data and RL would benefit the stimulation of reasoning for tasks beyond maths problems. Moreover, our studies shed light on the studies on cognitive behaviors of LLMs, which in turn facilitates the development of stronger base models."
        },
        {
            "title": "References",
            "content": "[1] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [2] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [3] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. [4] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. survey of large language models. arXiv preprint arXiv:2303.18223, 1(2), 2023. [5] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [6] Norman Mu, Sarah Chen, Zifan Wang, Sizhe Chen, David Karamardian, Lulwa Aljeraisy, Basel Alomair, Dan Hendrycks, and David Wagner. Can llms follow simple rules? arXiv preprint arXiv:2311.04235, 2023. [7] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. [8] Tao Zhang, Yanjun Shen, Wenjing Luo, Yan Zhang, Hao Liang, Fan Yang, Mingan Lin, Yujing Qiao, Weipeng Chen, Bin Cui, et al. Cfbench: comprehensive constraints-following benchmark for llms. arXiv preprint arXiv:2408.01122, 2024. [9] Yimin Jing, Renren Jin, Jiahao Hu, Huishi Qiu, Xiaohua Wang, Peng Wang, and Deyi Xiong. Followeval: multi-dimensional benchmark for assessing the instruction-following capability of large language models. arXiv preprint arXiv:2311.09829, 2023. [10] Qianyu He, Jie Zeng, Wenhao Huang, Lina Chen, Jin Xiao, Qianxi He, Xunzhe Zhou, Jiaqing Liang, and Yanghua Xiao. Can large language models understand real-world complex instructions? In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1818818196, 2024. [11] Bosi Wen, Pei Ke, Xiaotao Gu, Lindong Wu, Hao Huang, Jinfeng Zhou, Wenchuang Li, Binxin Hu, Wendy Gao, Jiaxing Xu, et al. Benchmarking complex instruction-following with multiple constraints composition. Advances in Neural Information Processing Systems, 37:137610137645, 2024. [12] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023. [13] Qianyu He, Jie Zeng, Qianxi He, Jiaqing Liang, and Yanghua Xiao. From complex to simple: Enhancing multi-constraint complex instruction following ability of large language models. arXiv preprint arXiv:2404.15846, 2024. [14] Haoran Sun, Lixin Liu, Junjie Li, Fengyu Wang, Baohua Dong, Ran Lin, and Ruohui Huang. Conifer: Improving complex constrained instruction-following ability of large language models. arXiv preprint arXiv:2404.02823, 2024. [15] Hyungjoo Chae, Yeonghyeon Kim, Seungone Kim, Kai Tzu-iunn Ong, Beong-woo Kwak, Moohyeon Kim, Seonghwan Kim, Taeyoon Kwon, Jiwan Chung, Youngjae Yu, et al. Language models as compilers: Simulating pseudocode execution improves algorithmic reasoning in language models. arXiv preprint arXiv:2404.02575, 2024. 11 [16] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [17] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [18] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [19] Qwen. Qwq-32b: Embracing the power of reinforcement learning, March 2025. [20] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560, 2022. [21] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. [22] Andreas Köpf, Yannic Kilcher, Dimitri Von Rütte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Richárd Nagyfi, et al. Openassistant conversations-democratizing large language model alignment. Advances in Neural Information Processing Systems, 36:4766947681, 2023. [23] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro Von Werra, Clémentine Fourrier, Nathan Habib, et al. Zephyr: Direct distillation of lm alignment. arXiv preprint arXiv:2310.16944, 2023. [24] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335, 2024. [25] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint arXiv:2306.08568, 2023. [26] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model, 2025. [27] Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, and Alane Suhr. Tinyzero. https://github.com/Jiayi-Pan/TinyZero, 2025. Accessed: 2025-01-24. [28] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl, 2025. Notion Blog. [29] Weihao Zeng, Yuzhen Huang, Wei Liu, Keqing He, Qian Liu, Zejun Ma, and Junxian He. 7b model and 8k examples: Emerging reasoning with reinforcement learning is both effective and efficient. https://hkust-nlp.notion.site/simplerl-reason, 2025. Notion Blog. [30] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Alpacaeval: An automatic evaluator of instructionfollowing models, 2023. [31] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:46595 46623, 2023. 12 [32] Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, et al. Alignbench: Benchmarking chinese alignment of large language models. arXiv preprint arXiv:2311.18743, 2023. [33] Congying Xia, Chen Xing, Jiangshu Du, Xinyi Yang, Yihao Feng, Ran Xu, Wenpeng Yin, and Caiming Xiong. Fofo: benchmark to evaluate llms format-following capability. arXiv preprint arXiv:2402.18667, 2024. [34] Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. Wildchat: 1m chatgpt interaction logs in the wild. arXiv preprint arXiv:2405.01470, 2024. [35] Gili Lior, Asaf Yehudai, Ariel Gera, and Liat Ein-Dor. Wildifeval: Instruction following in the wild. arXiv preprint arXiv:2503.06573, 2025. [36] Kaikai An, Li Sheng, Ganqu Cui, Shuzheng Si, Ning Ding, Yu Cheng, and Baobao Chang. Ultraif: Advancing instruction following from the wild. arXiv preprint arXiv:2502.04153, 2025. [37] Xiaodong Wu, Minhao Wang, Yichen Liu, Xiaoming Shi, He Yan, Xiangju Lu, Junmin Zhu, and Wei Zhang. Lifbench: Evaluating the instruction following performance and stability of large language models in long-context scenarios. arXiv preprint arXiv:2411.07037, 2024. [38] Yizhi Li, Ge Zhang, Xingwei Qu, Jiali Li, Zhaoqun Li, Zekun Wang, Hao Li, Ruibin Yuan, Yinghao Ma, Kai Zhang, et al. Cif-bench: chinese instruction-following benchmark for evaluating the generalizability of large language models. arXiv preprint arXiv:2402.13109, 2024. [39] Zhenyu Li, Kehai Chen, Yunfei Long, Xuefeng Bai, Yaoyin Zhang, Xuchen Wei, Juntao Li, and Min Zhang. Xifbench: Evaluating large language models on multilingual instruction following. arXiv preprint arXiv:2503.07539, 2025. [40] Antoine Dussolle, Andrea Cardeña Díaz, Shota Sato, and Peter Devine. M-ifeval: Multilingual instruction-following evaluation. arXiv preprint arXiv:2502.04688, 2025. [41] Jinnan Li, Jinzhe Li, Yue Wang, Yi Chang, and Yuan Wu. Structflowbench: structured flow benchmark for multi-turn instruction following. arXiv preprint arXiv:2502.14494, 2025. [42] Yun He, Di Jin, Chaoqi Wang, Chloe Bi, Karishma Mandyam, Hejia Zhang, Chen Zhu, Ning Li, Tengyu Xu, Hongjiang Lv, et al. Multi-if: Benchmarking llms on multi-turn and multilingual instructions following. arXiv preprint arXiv:2410.15553, 2024. [43] Yusu Qian, Hanrong Ye, Jean-Philippe Fauconnier, Peter Grasch, Yinfei Yang, and Zhe Gan. Mia-bench: Towards better instruction following evaluation of multimodal llms. arXiv preprint arXiv:2407.01509, 2024. [44] Elliot Epstein, Kaisheng Yao, Jing Li, Xinyi Bai, and Hamid Palangi. Mmmt-if: challenging multimodal multi-turn instruction following benchmark. arXiv preprint arXiv:2409.18216, 2024. [45] Keno Harada, Yudai Yamazaki, Masachika Taniguchi, Takeshi Kojima, Yusuke Iwasawa, and Yutaka Matsuo. Curse of instructions: Large language models cannot follow multiple instructions at once. [46] Zhihan Zhang, Shiyang Li, Zixuan Zhang, Xin Liu, Haoming Jiang, Xianfeng Tang, Yifan Gao, Zheng Li, Haodong Wang, Zhaoxuan Tan, et al. Iheval: Evaluating language models on following the instruction hierarchy. arXiv preprint arXiv:2502.08745, 2025. [47] Jiuding Yang, Weidong Guo, Kaitong Yang, Xiangyang Li, Yu Xu, and Di Niu. Enhancing and assessing instruction-following with fine-grained instruction variants. arXiv preprint arXiv:2406.11301, 2024. [48] Noah Ziems, Zhihan Zhang, and Meng Jiang. Tower: Tree organized weighting for evaluating complex instructions. arXiv preprint arXiv:2410.06089, 2024. 13 [49] Kaiwen Yan, Hongcheng Guo, Xuanqing Shi, Jingyi Xu, Yaonan Gu, and Zhoujun Li. Codeif: Benchmarking the instruction-following capabilities of large language models for code generation. arXiv preprint arXiv:2502.19166, 2025. [50] Guanting Dong, Xiaoshuai Song, Yutao Zhu, Runqi Qiao, Zhicheng Dou, and Ji-Rong Wen. Toward general instruction-following alignment for retrieval-augmented generation. arXiv preprint arXiv:2410.09584, 2024. [51] Hanseok Oh, Hyunji Lee, Seonghyeon Ye, Haebin Shin, Hansol Jang, Changwook Jun, and Minjoon Seo. Instructir: benchmark for instruction following of information retrieval models. arXiv preprint arXiv:2402.14334, 2024. [52] Youquan Li, Miao Zheng, Fan Yang, Guosheng Dong, Bin Cui, Weipeng Chen, Zenan Zhou, and Wentao Zhang. Fb-bench: fine-grained multi-task benchmark for evaluating llms responsiveness to human feedback. arXiv preprint arXiv:2410.09412, 2024. [53] Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin Jiang, Qun Liu, and Wei Wang. Followbench: multi-level fine-grained constraints following benchmark for large language models. arXiv preprint arXiv:2310.20410, 2023. [54] Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Infobench: Evaluating instruction Xuansheng Wu, Fei Liu, Pengfei Liu, and Dong Yu. following ability in large language models. arXiv preprint arXiv:2401.03601, 2024. [55] Yulei Qin, Yuncheng Yang, Pengcheng Guo, Gang Li, Hang Shao, Yuchen Shi, Zihan Xu, Yun Gu, Ke Li, and Xing Sun. Unleashing the power of data tsunami: comprehensive survey on data assessment and selection for instruction tuning of language models. Transactions on Machine Learning Research, 2024. [56] Wei Liu, Yancheng He, Hui Huang, Chengwei Hu, Jiaheng Liu, Shilong Li, Wenbo Su, and Bo Zheng. Air: Complex instruction generation via automatic iterative refinement. arXiv preprint arXiv:2502.17787, 2025. [57] Yunjia Qi, Hao Peng, Xiaozhi Wang, Bin Xu, Lei Hou, and Juanzi Li. Constraint backtranslation improves complex instruction following of large language models. arXiv preprint arXiv:2410.24175, 2024. [58] Guanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, and Jingren Zhou. Self-play with execution feedback: Improving instruction-following capabilities of large language models. arXiv preprint arXiv:2406.13542, 2024. [59] Jiale Cheng, Xiao Liu, Cunxiang Wang, Xiaotao Gu, Yida Lu, Dan Zhang, Yuxiao Dong, Jie Tang, Hongning Wang, and Minlie Huang. Spar: Self-play with tree-search refinement to improve instruction-following in large language models. arXiv preprint arXiv:2412.11605, 2024. [60] Fei Wang, Chao Shang, Sarthak Jain, Shuai Wang, Qiang Ning, Bonan Min, Vittorio Castelli, Yassine Benajiba, and Dan Roth. From instructions to constraints: Language model alignment with automatic constraint verification. arXiv preprint arXiv:2403.06326, 2024. [61] Shirley Anugrah Hayati, Taehee Jung, Tristan Bodding-Long, Sudipta Kar, Abhinav Sethy, Joo-Kyung Kim, and Dongyeop Kang. Chain-of-instructions: Compositional instruction tuning on large language models. arXiv preprint arXiv:2402.11532, 2024. [62] Wangtao Sun, Chenxiang Zhang, XueYou Zhang, Xuanqing Yu, Ziyang Huang, Pei Chen, Haotian Xu, Shizhu He, Jun Zhao, and Kang Liu. Beyond instruction following: Evaluating inferential rule following of large language models. arXiv preprint arXiv:2407.08440, 2024. [63] Hui Huang, Jiaheng Liu, Yancheng He, Shilong Li, Bing Xu, Conghui Zhu, Muyun Yang, and Tiejun Zhao. Musc: Improving complex instruction following with multi-granularity self-contrastive training. arXiv preprint arXiv:2502.11541, 2025. [64] Peifeng Wang, Austin Xu, Yilun Zhou, Caiming Xiong, and Shafiq Joty. Direct judgement preference optimization. 2024. 14 [65] Xinghua Zhang, Haiyang Yu, Cheng Fu, Fei Huang, and Yongbin Li. Iopo: Empowering llms with complex instruction following via input-output preference optimization. arXiv preprint arXiv:2411.06208, 2024. [66] Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, and Sainbayar Sukhbaatar. Meta-rewarding language models: Self-improving alignment with llm-as-a-meta-judge. arXiv preprint arXiv:2407.19594, 2024. [67] Qingyu Ren, Jie Zeng, Qianyu He, Jiaqing Liang, Yanghua Xiao, Weikang Zhou, Zeye Sun, and Fei Yu. Step-by-step mastery: Enhancing soft constraint following ability of large language models. arXiv preprint arXiv:2501.04945, 2025. [68] Thomas Palmeira Ferraz, Kartik Mehta, Yu-Hsiang Lin, Haw-Shiuan Chang, Shereen Oraby, Sijia Liu, Vivek Subramanian, Tagyoung Chung, Mohit Bansal, and Nanyun Peng. Llm selfcorrection with decrim: Decompose, critique, and refine for enhanced following of instructions with multiple constraints. arXiv preprint arXiv:2410.06458, 2024. [69] Zhaoyang Wang, Jinqi Jiang, Huichi Zhou, Wenhao Zheng, Xuchao Zhang, Chetan Bansal, and Huaxiu Yao. Verifiable format control for large language model generations. arXiv preprint arXiv:2502.04498, 2025. [70] Jonathan Cook, Tim Rocktäschel, Jakob Foerster, Dennis Aumiller, and Alex Wang. Ticking all the boxes: Generated checklists improve llm evaluation and generation. arXiv preprint arXiv:2410.03608, 2024. [71] Xuan Zhang, Chao Du, Tianyu Pang, Qian Liu, Wei Gao, and Min Lin. Chain of preference optimization: Improving chain-of-thought reasoning in llms. Advances in Neural Information Processing Systems, 37:333356, 2024. [72] Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. Chain of draft: Thinking faster by writing less. arXiv preprint arXiv:2502.18600, 2025. [73] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1768217690, 2024. [74] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. [75] Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph Gonzalez, and Bin Cui. Buffer of thoughts: Thought-augmented reasoning with large language models. Advances in Neural Information Processing Systems, 37:113519113544, 2024. [76] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022. [77] Simran Arora, Avanika Narayan, Mayee Chen, Laurel Orr, Neel Guha, Kush Bhatia, Ines Chami, Frederic Sala, and Christopher Ré. Ask me anything: simple strategy for prompting language models. arXiv preprint arXiv:2210.02441, 2022. [78] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022. [79] Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Decomposed prompting: modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406, 2022. [80] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493, 2022. 15 [81] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wangxiang Che. Towards reasoning era: survey of long chain-of-thought for reasoning large language models. arXiv preprint arXiv:2503.09567, 2025. [82] Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, et al. O1 replication journey: strategic progress reportpart 1. arXiv preprint arXiv:2410.18982, 2024. [83] Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, and Pengfei Liu. O1 replication journeypart 2: Surpassing o1-preview through simple distillation, big progress or bitter lesson? arXiv preprint arXiv:2411.16489, 2024. [84] Zhongzhen Huang, Gui Geng, Shengyi Hua, Zhen Huang, Haoyang Zou, Shaoting Zhang, Pengfei Liu, and Xiaofan Zhang. O1 replication journeypart 3: Inference-time scaling for medical reasoning. arXiv preprint arXiv:2501.06458, 2025. [85] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [86] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. [87] Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou. # instag: Instruction tagging for analyzing supervised fine-tuning of large language models. arXiv preprint arXiv:2308.07074, 2023. [88] Shuofei Qiao, Honghao Gui, Chengfei Lv, Qianghuai Jia, Huajun Chen, and Ningyu Zhang. Making language models better tool learners with execution feedback. arXiv preprint arXiv:2305.13068, 2023. [89] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [90] John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. In Yoshua Bengio and Yann LeCun, editors, 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. [91] HuggingFace. huggingface/math-verify, 2025. [92] ErlichLiu. Erlichliu/deepclaude: Unleash next-level ai!, 2025. [93] getAsterisk. Getasterisk/deepclaude: high-performance llm inference api and chat ui that integrates deepseek r1s cot reasoning traces with anthropic claude models, 2025. [94] Jian Hu, Xibin Wu, Zilin Zhu, Weixun Wang, Dehao Zhang, Yu Cao, et al. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024. [95] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [96] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023. [97] Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307, 2025. [98] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. [99] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. [100] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. arXiv preprint arXiv:2403.13372, 2024. [101] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large In Proceedings of the ACM SIGOPS 29th language model serving with pagedattention. Symposium on Operating Systems Principles, 2023."
        },
        {
            "title": "A Appendix",
            "content": "In this appendix, we first provide the descriptions about the mathematic symbols used in the manuscript. Then, provide the detailed descriptions about the benchmarks used in our experiments and their evaluation metrics. Furthermore, we provide the details about the preparation of datasets used for training, including the publicly available DeepScaleR dataset [28] and the self-evolved complex instruction dataset mentioned in Sec. 3.2. The detailed implementations are presented in Sec. A.5 of the appendix. Finally, we present more fine-grained results for in-depth analyses. A.1 Symbol Description To enhance clarity, detailed description of mathematic symbols is provided in Table 4. Table 4: Descriptions of the symbols used in the paper. Symbol Definition θ πθ y1t1 yt πθ(ytx, y1t1) CoT(y) yCoT(y) xCoT yi πθold πref yi JGRPO GRPO ri Ri ˆAi KL(πθπref) clip(, l, u) Di query input of the complex instruction (tokenized) response of LLM to the input (tokenized) The parameters of the LLM to be optimized The policy model (LLM) to be optimized (parameterized as θ) The tokens of starting from the index 1 to the index 1 The t-th token of The language modeling process of πθ given the input and the preceding response y1t1 The CoT reasoning tokens of The response answer tokens of The CoT prompting tokens of The number of tokens of yi The number of generations per input prompt during rollout The policy model from the previous iteration state The reference model The i-th generated output from the policy model πθold for the input prompt The GRPO loss The GRPO loss for the generated sample yi The importance sampling ratio of yi at the time (i.e., for its generation of the t-th token) The reward of yi before normalization The estimated advantage of yi at time The KL divergence between the current policy model and the reference model on yi The clipping operation with the lower bound and the upper bound The format reward of yi The number of atomic constraints in the input prompt The tokens of the j-th atomic constraint in with cj The set of all the atomic constraints with xC and cj xC , The subset that contains only the truly valid, active constraints with xactive xC cj) The condition of whether yi satisfies the constraint cj as instruction following is_followed(yi The reward model parameterized by ϕ The accuracy reward of yi The yi with empty reasoning tokens (i.e., skipping CoT via <think>nn</think>) The reward of ˆyi The format reward of ˆyi The accuracy reward of ˆyi The filtered GRPO loss with superior CoT enforcement The expert response to the input with both the reasoning and the answer The SFT loss for behavior cloning Ri format cj xC xactive rϕ Ri accuracy ˆyi ˆRi format ˆRi ˆRi accuracy ˆJ GRPO JSFT A.2 Evaluation Datasets and Metrics In the present study, we evaluate the proposed method on the following datasets: IFEval [7], ComplexBench [11], CELLO [10], CFBench [8], FB-Bench [52], FollowBench [53], and InfoBench [54]. All these benchmarks are specifically constructed to evaluate the instruction-following capabilities of LLMs under various complex tasks and domains (see Table 5). 18 A.2.1 Benchmarks IFEval IFEval [7] is one commonly used benchmark to evaluate the abilities of LLMs in following natural language instructions. It focuses on wide range of \"verifiable instructions\" that can be efficiently and accurately validated via python codes. It identifies 25 types of atomic constraints including keywords, languages, length constraints, detectable formats, combinations, case changes, starting and ending phrases, and punctuations. All the verifications are performed via codes to avoid potential bias caused by LLM judges. Therefore, their constraints are limited in types where no semantic ones are covered. CELLO CELLO [10] formulates complex instructions from real-world task descriptions and input queries. From the task perspective, CELLO considers multi-tasking, semantic constraints, format constrains, and quality constraints. From the input text perspective, CELLO considers heterogeneous information, long context, noisy information, and multi-turn conversations. Similar to the IFEval benchmark, CELLO focuses simply on the objective, rule-verifiable constraints where their verifications are all rule-based. CFBench CFBench [8] covers more thant 200 real-life scenarios and over 50 tasks. Their detailed types of constraints are classified into 10 primary categories and 25 sub-categories, including content constraints, numerical constraints, stylistic constraints, format constraints, linguistic constraints, situation constraints, example constraints, inverse constraints, contradictory constraints, and rule constraints. ComplexBench ComplexBench [11] is one of the most comprehensive benchmarks that validate the instruction-following capabilities of LLMs under complex isntructions. Its hierarchical taxnomy for the definitions of complex instructions include 4 constraint types, 19 constraint dimension, and 4 composition types. It covers tasks that highlight fundamental language ability, advanced Chinese understanding, open-ended questions, practical writing, creative writing, professional writing, custom writing, logical reasoning, task-oriented role play, and profession knowledge. FB-Bench FB-Bench [52] is multi-task fine-grained benchmark that evaluates the LLMs responsiveness to human feedbacks under real-world scenarios. It encompasses eight task types (reasoning, coding, text extraction,text error correction, text creation, knowledge QA, and text translation), five deficiency types of responses (not-following-instructions, logical error, incomplete answers, factual errors, and unprofessional answers), and nine feedback types (pointing out errors, clarifying the intent, raising objections, detailed explanations, hinting guidance, simple questioning, misinformation, credibility support, and unreasonable requests). FollowBench FollowBench [53] is multi-level fine-grained constraints following benchmark. It covers constraints of content, situation, style, format, example, and mixed types. The levels of constraints refer to the number of atomic constraints present in each instruction, where the way of constraint composition is designed for each task in its category. InfoBench InfoBench [54] develops both the hard set and the easy set covering 72 domains including natural sciences, social sciences, engineering, economics, engineering, economics, and arts. It incorporates specific response constraints including contents, linguistic guidelines, style rules, format specifications, and number limitations. For each instruction, its decomposed scoring questions are prepared for boolean evaluation of constraint-following conditions. Table 5: Statistics of the complex instruction benchmarks used for evaluation. Verification and Evaluation Composition Type Benchmark Size Constraint Taxonomy 541 IFEval [7] 523 CELLO [10] CFBench [8] 1,000 ComplexBench [11] 1,150 591 FB-Bench [52] 820 FollowBench [53] 500 InfoBench [54] 25 4 25 19 9 5 5 And Chain Selection Nested Code-Execution LLM-as-a-Judge - - - - - - - - - - - - - - - - - - - - Aggregation Average Average Priority Dependency Weighted Average Average Average 19 A.2.2 Evaluation Metrics In the present study, we report the following evaluation metrics of benchmarks in Table 1. IFEval: prompt-level_loose CELLO: average of the average_complex_ins and average_complex_input CFBench: average of the CSR, ISR, and PSR ComplexBench: overall_DRFR FB-Bench: average of the error_correction_score and response_maintenance_score FollowBench: average of the CSL_avg_en/3.68, HSR-level-1_en, HSR-level-2_en, HSRlevel-3_en, HSR-level-4_en, HSR-level-5_en, SSR-level-1_en, SSR-level-2_en, SSR-level3_en, SSR-level-4_en, SSR-level-5_en, CSL_avg_cn/3.68, HSR-level-1_cn, HSR-level-2_cn, HSR-level-3_cn, HSR-level-4_cn, HSR-level-5_cn, SSR-level-1_cn, SSR-level-2_cn, SSRlevel-3_cn, SSR-level-4_cn, SSR-level-5_cn InfoBench: overal_DRFR Note that for FollowBench, the CSL_avg_en and CSL_avg_cn are normalized by the score of Qwen2.5-72B-Instruct (3.68) for percentage comparison. The detailed explanations on the metrics above are provided below. IFEval IFEval [7] employs four accuracy scores for evaluation: 1) prompt-level strict accuracy, 2) instruction-level strict accuracy, 3) prompt-level loose accuracy, and 4) instruction-level loose accuracy. The prompt-level strict accuracy measures the percentage of prompts where all the constraints are satisfied. The instruction-level strict accuracy, on the other hand, reports the percentage of constraints satisfied across samples. For the loose evaluation metrics, they apply transformations on the responses of LLMs to remove the markdown syntax, opening intros to the final responses, and the ending outros following the final responses to reduce false negatives. CELLO CELLO [10] proposes code-based verification system that checks four common finegrained aspects like: 1) count limit, 2) answer format, 3) task-prescribed phrases, and 4) inputdependent query. For the count limit, it considers word count score, sentence count score, and sample count score. For the answer format, it considers parsability and keywords. For the input-dependent query, it scores whether the key phrases from the input query are existent in the responses, and applies penalty term of COPY-BLEU to prevent giving high scores to the undesirable copying behavior of LLMs. For the task-prescribed phrases, it checks if the mandatory phrases specified in the task description are covered in the responses to satisfy those essential conditions. CFBench CFBench [8] employs the three metrics: constraint satisfaction rate (CSR), instruction satisfaction rate (ISR), and priority satisfaction rate (PSR). CSR measures whether each constraint is satisfied and computes the average of satisfaction rate across samples. ISR verifies whether all the constraints are satisfied per sample and report the average rate across samples. PSR assigns priority requirement to the constraints and if any prioritized constraint is not satisfied, the verification is judged as failure. ComplexBench ComplexBench [11] provides the scoring questions for each complex isntruction to check whether the constraints are satisfied. Each scoring question can only be answered with boolean responses as \"YES\" or \"NO\" and then the scores of each decomposed question are aggregated for the metric of decomposed requirements following ratio (DRFR). Note that the dependency of scoring questions is considered in aggregation. If any preceding constraint is not satisfied, the following constraints are simply judged as failure. FB-Bench FB-Bench [52] employs the DRFR metric as well and validates series of criteria on the checklist for each complex instruction. It further set different weights for different criteria in the checklist for the weighted aggregation of the final score. The weights of different scoring questions are pre-defined to signify their importance. 20 FollowBench FollowBench [53] employs both code-execution and LLM-as-a-Judge verifications for evaluation. It uses an incremental method of constraint validation where the LLM judge is asked to recognize the newly added constraint each time for verification. Three main metrics are used: 1) hard satisfaction rate (HSR), 2) soft satisfaction rate (SSR), and consistent satisfaction levels (CSL). The HSR and SSR can be understood as instruction-level and constraint-level satisfaction rate, respectively. The CSL measures how many consecutive levels are satisfied per instruction. InfoBench InfoBench [54] proposes the decomposed requirements following ratio (DRFR) metric for verifications. For each scoring question, the LLM-as-a-Judge strategy is used to give \"YES\" or \"NO\" response. The final scores of the response are simply the averaged accumulation of scores over the total number of scoring questions. Such scoring technique enables more fine-grained interpretation of the final results. A.3 Self-Evolved Complex Instruction Dataset The self-evolved complex instruction dataset contains 13K complex instructions and their associated verifications (either with codes for python execution or with scoring questions for LLM-as-a-Judge) and reference expert responses. We choose the WildChat [34] and the Alpaca [86] as our instruction pool to perform self-evolving. The types of rules and constraints are sourced and collected from the taxonomy defined in IFEval, CELLO, ComplexBench, CFBench, FB-Bench, FollowBench, and InfoBench. Such preparation ensures the diversity of the rule and constraint pool. The type of composition follows the definition in the ComplexBench. The statistics of our self-evolved dataset can be found in Table 6. In consideration of the existing benchmarks, models, and the instruction pool, both the languages of Chinese and English are considered. The detailed steps of seed instruction selection are described below. Table 6: Statistics of our self-evolved complex instruction dataset. Language # of Samples # of Input (Tokens) # of Output (Tokens) # of Constraints per Sample Chinese English 9.1K 4.1K 256343 596422 138261 1013 2.801.29 4.001.95 # of Scoring Questions per Sample (Verification by LLM-as-a-Judge) 2.591.34 4.452.17 # of Verifiable Restrictions (Verification by Code-Execution) 3.281.01 3.211.13 A.3.1 Tagging for Selection of Seed Instructions For the selection of seed instructions, we choose the WildChat [34] (650K) and the Alpaca [86] (52K) as the pool of instructions. Especially for the WildChat dataset, broad spectrum of userchatbot interactions are covered including ambiguous user requests, code-switching, topic-switching, and political discussions. Therefore, it is indispensable to filter out those instructions that are not appropriate for constructing complex instructions with additional constraints. In this case, we follow InsTag [87] to perform topic and capability tagging on the instructions. We refer to the \"intention tags\" of InsTag as the ability tags in the present study because those open-ended tags are closely associated with the abilities of LLMs to solve input queries. Specifically, we manually summarize and define the topic tags from the approximate 3.2K ability tags of the InsTag (see Fig. 6). Two typical examples of the instag tagging by TagLM are provided below respectively for Chinese and English instructions from the WildChat, respectively (see Fig. 7). After tagging, we perform selection of seed instructions. We choose instructions that possess topic tags of \"Problem Solving\", \"NLP and Understanding\", \"Info Processing\", and \"Creativity & Design\" as candidates. Then, we perform random sampling on those instructions. During sampling, the ability tags are used to maximize the diversity of the chosen seed instructions (i.e., the number of instructions classified to each ability tag is controlled). Finally, we obtain 39K seed instructions in total."
        },
        {
            "title": "Summarized Topic Tags from the Ability Tags of InsTag",
            "content": "Problem Solving NLP and Understanding Info Processing Logic & Reasoning Programming & Dev Creativity & Design Date Science Math skills Linguistics & Multiculturalism Knowledge Q&A Education & Consulting Communication & Social Media Humanities & Social Sciences Research Project Management Literary & Artistic Knowledge STEM Finance & Business Task Generation Medical & Health Life Skills Legal Knowledge Psychology Task Completion Law & Safety Politics, Military Strategy & Security Figure 6: Topic tags summarized from the ability tags of InsTag. InsTag by TagLM on WildChat Examples { \"User Query\": \"Please paraphase following words to be more formal and cohesive: Among wide range of generation tasks, subject-driven generation is popular topic, which is also known as customization or personalization. Such task refers to learning object-level representations from reference set and regenerate them in new contexts, depending on different conditions or prompts.\", \"InsTag Response\": \"{\"Ability Tags\":[\"paraphrase\", \"formal tone\"], \"Topic Tags\":[\"NLP and Understanding\"]}\", } { \"User Query\": \"介绍一下Gradio和docker\", \"InsTag Response\": \"{\"Ability Tags\":[\"programming language\", \"question answer\", \"problem-solving\"], \"Topic Tags\":[\"Problem Solving\", \"Programming & Dev\", \"Knowledge Q&A\"]}\", } Figure 7: Two tagged examples from the WildChat (Chinese and English). A.3.2 Generation of Rules and Constraints with Verifications Given the seed instructions, we perform self-instruct [20] to generate instructions with additionally introduced rules and constraints. Specifically, we employ different techniques to evolve instructions with constraints that are verified by code-execution and LLM-as-a-Judge, respectively. Verifications via Code-execution For the code-execution verifications, the constraint templates and their verification codes are prepared in advance. We follow existing studies [58, 13] to use the constraints defined in IFEval [7], and randomly choose atomic constraints from the entire pool of 25 constraints. Specifically, since certain constraints are mutually exclusive (i.e., not able to be fulfilled simultaneously), we refer to IFEval [7] to filter out impossible combinations. In total, there exist 25 atomic constraints with 210 combinations. Given the pre-defined constraint templates, we need to fill in the placeholders for instantiation. Note that most of the placeholders can be simply replaced with randomly chosen candidate (e.g., number of words, response language, and format of the response). There still exists certain keyword placeholders (see Fig. 8) that are associated with the semantics of the seed instructions, which requires LLMs to generate reasonable candidates instead. The detailed prompts to generate keywords in English and Chinese via LLMs are provided in Figs. 9 and 10. Atomic Constraint Placeholders # placeholders that can be simply sampled randomly from the candidate pool \"detectable_format:multiple_sections\", \"change_case:capital_word_frequency\", \"detectable_format:number_highlighted_sections\", \"detectable_format:number_bullet_lists\", \"detectable_content:postscript\", \"detectable_content:number_placeholders\", \"length_constraints:number_words\", \"length_constraints:number_paragraphs\", \"length_constraints:number_sentences\",\"language:response_language\", \"keywords:letter_frequency\", \"startend:end_checker\" # placeholders that have to be generated with LLM for semantic coherence \"keywords:existence\", \"keywords:frequency\", \"keywords:forbidden_words\", \"length_constraints:nth_paragraph_first_word\" Figure 8: Atomic constraints for verifications via code-execution. Prompt to Generate Keywords for Atomic Constraint Templates in English You are provided with an <instruction>. Your object is to come up some keywords that may be used to answer the <instruction>. They are usually related to the task described in the <instruction>. you should output your thinking process and the keywords you come up with. INPUT <instruction>: Explain Generative Adversarial Networks (GANs) to me using bullet points. Do not contain any commas in your response. OUTPUT thinking process: the <instruction> as to explain GANs, hence, architecture,training and generator may be appropriate keywords to use in the answer. keywords: \"architecture\", \"training\", \"generator\" INPUT <instruction>: {here is the detailed seed instruction} OUTPUT Figure 9: The keyword placeholders in atomic constraints (English) are generated via LLMs. Verifications via LLM-as-a-Judge For the LLM-as-a-Judge verifications, we find that existing benchmarks [8, 39, 54, 11] often prepare series of scoring questions for each complex instruction,"
        },
        {
            "title": "Prompt to Generate Keywords for Atomic Constraint Templates in Chinese",
            "content": "You are provided with an <instruction>. Your object is to come up some keywords that may be used to answer the <instruction>. They are usually related to the task described in the <instruction>. you should output your thinking process and the keywords you come up with. INPUT <instruction>: 使用项目符号的形式向我解释什么是生成对抗网络GANs在你的回答中不要使用任何逗 号 OUTPUT thinking process: <instruction> 是解释生成对抗网络GANs因此\"架构\"\"训练\"和\"生成器\"可能是答案中 适合使用的关键词 keywords: \"架构\", \"训练\", \"生成器\" INPUT <instruction>: {here is the detailed seed instruction} OUTPUT Figure 10: The keyword placeholders in atomic constraints (Chinese) are generated via LLMs. where each scoring question corresponds to one active sub-instruction. To be clear, we refer to the active sub-instructions xactive as the truly valid instructions with constraints. According to the definitions of And, Chain, and Selection [11], one complex instruction might be composed of multiple sub-instructions that: 1) have to be fulfilled simultaneously; or 2) are sequentially fulfilled with the responses to the preceding sub-instructions being the context to the following sub-instructions; or 3) are mutually exclusive with only one branch of the sub-instructions being valid for response generation. Therefore, we propose to first generate series of scoring questions and their corresponding subinstructions under the given context (i.e., the original instruction and its topic). As shown in Fig. 11, we fill in the prompt with one of the seed instructions, the expected language of the generated instruction, and the one-shot complex example. The one-shot complex instruction example includes: 1) the detailed instruction itself; 2) its decomposed sub-instructions; and 3) its scoring questions. Then, we prompt the LLM to integrate those sub-instructions into one complex instruction, which improves the coherency and consistency of the final complex instructions. As shown in Fig. 12, we fill in the prompt with one-shot complex example and its sub-instructions as reference. The LLM is asked to fuse these sub-instructions into one integrated complex instruction. Examples of the Generated Instructions and Verifications We provide examples of the generated instructions via Qwen2.5-72B-Instruct in Figs 13 and 14 respectively for verifications by code execution and LLM-as-a-Judge. A.3.3 Quality Check for Filtering of the Self-Evolved Instructions Given the generated complex instructions, we obtain their reference responses by prompting the existing competent LLMs (e.g., DeepSeek R1). Then, we first filter the responses that fail to meet the constraints either via code-execution or via LLM-as-a-Judge. We directly discard these responses and their associated instructions. In addition, we find that even if the remaining responses satisfy all the requirements in the instructions, there still exist low-quality instruction-response pairs. Typical issues are categorized as: 1) language inconsistency. The response language is not consistent with the instruction language, which is an implicit alignment constraint but might be ignored in return for satisfying other constraints. 2) answer leakage. The preferred response might be unintentionally mentioned in the input instruction, which is often caused by mis-interpretation of the generation prompts. 3) underor over-length. The response contains snippet that fails to meet the constraint on length, which is hardly avoided due to the fundamental deficiency of LLMs in perception of length of characters, words, and sentences. 4) hallucination. The response might contains unsubstantiated contents that are made up simply to satisfy the constraints. 5) poorly-defined instruction. The complex instruction itself might be too complicated to understand its core demand. This could happen during the integration of sub-instructions, where the critical information can be neglected by 24 Prompt to Generate Sub-instructions and their Scoring Questions You are an excellent instruction generator. Below is an instruction and its decomposed sub-instructions. For each sub-instruction, scoring questions are provided to judge if language model can fulfill the sub-instruction correctly. [The start of Instruction] {input complex instruction as one-shot example} [The end of Instruction] [The start of Sub-Instructions and Scoring Questions] {the sub-instructions and their scoring questions of the input complex instruction above} [The end of Sub-Instructions and Scoring Questions] According to the sub-instructions and scoring questions, please generate the modified subinstructions and their corresponding scoring questions that are similar ONLY in STYLE to the provided sub-instructions and scoring questions above but NOT semantically identical at all. - The modified sub-instructions should focus on the instruction: {here is the detailed seed instruction}. - You MUST modify each sub-instruction one by one. - If there exists passages/snippets that are enclosed/wrapped by or or \"\"\" in the original sub-instructions, you MUST generate NEW passages/snippets that are enclosed/wrapped in similar way. - Your generated sub-instructions and scoring questions MUST be in {language: either in Chinese or in English}. - Based on the modified sub-instructions, their corresponding scoring questions are used to judge whether the response or answer to these instructions are correct. Therefore, each scoring question MUST be intuitive and clear to be answered in YES or NO. - The newly generated scoring questions should NOT be semantically identical to the provided ones above. - Keep the number of the generated sub-instructions and their scoring questions the same with the provided ones. - IMPORTANT: Please strictly follow the following format: [The start of the Modified Sub-Instructions and Scoring Questions] {your generation} [The end of the Modified Sub-Instructions and Scoring Questions] Figure 11: The sub-instructions and their scoring questions are generated via LLMs. LLMs. 6) problematic instruction. The instruction might also contain constraints that are mutually contradictory. 7) not suitable for work (NSFW) content. The WildChat dataset itself might contain NSFW user queries that should be removed for safety concerns. In this case, we prepare seven prompts specifically for quality check and employ the DeepSeek R1 for filtering out those low-quality instruction-response pairs. The detailed prompts (in Chinese) for identifying the aforementioned issues are provided in Figs. 15, 16, 17, 18, 19, 20, 21. After sequentially performing the strict quality check with DeepSeek R1 on each prompt, we finally obtain the complex instruction dataset of 13K instances (with retention less than 10%). To make it clearer, we provide the models used in different stages of self-evolving (see Table 7). Table 7: The detailed models used in self-evolving of complex instructions. Stage Model Seed Instruction Selection Self-Instruct with Rules and Constraints Qwen2.5-72B-Instruct [95] Response Gneration and Quality Check DeepSeek R1 [18] TagLM [87] 25 Prompt to Merge Sub-instructions for the Integrated Instructions You are an excellent instruction generator. Below is series of sub-instructions and its combined all-in-one instruction. [The start of Sub-Instructions] {sub_instructions of complex instruction as one-shot example} [The end of Sub-Instructions] [The start of the All-in-One Instruction] {a complex instruction as one-shot example} [The end of the All-in-One Instruction] Now, the modified sub-instructions are provided below. [The start of the Modified Sub-Instructions] {sub_instructions_new} [The end of the Modified Sub-Instructions] According to the modified sub-instructions, please generate their combined all-in-one instruction. - The all-in-one instruction should include all the details and requirements mentioned in the sub-instructions. - The generated instruction should share the SAME format with the sub-instructions. - You MUST use the same language as the modified sub-instructions. - You should NOT add any new sub-instruction. - You MUST keep the information from the sub-instructions unchanged in the combined instruction. - You should make the combined all-in-one instruction easy to read and understand. - IMPORTANT: Please strictly follow the following format: [The start of the Modified All-in-One Instruction] {your generated all-in-one instruction for combining all the new sub-instructions} [The end of the Modified All-in-One Instruction] Figure 12: The sub-instructions are merged via LLMs for the integrated instructions. Complex Instruction and its Code-Verifiable Constraints { \"instructions\": \"Explain and justify the need to learn coding in school. State the benefits it has for the future. You will answer this question in relation to using Scratch. Your final product will be to create game of your choice. You can explain the ATL skills it helps you develop (200 words) End your response with postscript indicated by P.S.. Respond with at least 3 sentences. response should contain the keyword \"digital\". The words game cannot be in the response. Your answer must be in the form of exactly 4 bullet points with the format below: * This is bullet point 1 * This is bullet point 2.\", \"instruction_id_list\":\"[ \"detectable_content:postscript\", \"length_constraints:number_sentences\", \"keywords:existence\", \"keywords:forbidden_words\", \"detectable_format:number_bullet_lists\" ] } Figure 13: An example of the generated complex instruction and its verifications via code execution."
        },
        {
            "title": "A Complex Instruction and its Scoring Questions",
            "content": "{ \"instructions\": \"You are tasked with conducting detailed analysis of the economic impact of new trade policy on small town. Assume you are researcher with deep understanding of economic principles and historical context. Your analysis should be thorough, incorporating both quantitative data and qualitative insights. The town in question has recently implemented policy to reduce tariffs on imported goods. Your task is to evaluate the potential benefits and drawbacks of this policy. Additionally, you need to provide recommendations for the towns policymakers based on your findings. Ensure that your analysis is presented in formal academic style, with clear and concise language.\", \"scoring_questions\":\"[ \"Does the response demonstrate deep understanding of economic principles and historical context?\", \"Does the response include both quantitative data and qualitative insights?\", \"Does the response present clear and concise evaluation of the policys potential benefits and drawbacks?\", \"Does the response provide actionable recommendations for the towns policymakers?\", \"Is the response presented in formal academic style with clear and concise language?\" ] } Figure 14: An example of the generated complex instruction and its verifications via LLM-as-a-Judge. A.4 DeepScaleR-Preview-Dataset The DeepScaleR-Preview-Dataset [28] provides approximately 40K unique mathematic problems from: 1) American Invitational Mathematics Examination (AIME) problems (1984-2023), 2) American Mathematics Competition (AMC) problems (prior to 2023), 3) Omni-MATH dataset, and 4) Still dataset. It is noted that for each mathematic problem, only one answer (final answer) is provided for reference. For each problem, the dataset provides its solution and answer. The solution is often formatted in the LaTex with the final answer boxed. However, its solution can be empty (unavailable) and the reasoning process might be concise and short. Therefore, it is impossible to directly use this dataset for supervised fine-tuning. One typical example of the DeepScaleR-Preview-Dataset is presented below (see Fig. 22). It is noted that in our present study, we only apply reinforcement learning to facilitate reasoning on mathematic problems. We do not use DeepScaleR-Preview-Dataset to perform supervised fine-tuning. A. Implementation Details A.5.1 Hyper-parameters Reinforcement Learning We present the details of the hyper-parameter settings in the present study (see Table 10). We follow [94] to keep most of the default empirical settings unchanged for comparability. Supervised Fine-Tuning For the SFT experiments in the baselines, we also follow [100] to use the recommended default settings. The detailed settings of the hyper-parameters are presented in Table 11. A.5.2 Training Reinforcement Learning For each experiment on 1.5B, 7B, and 8B models, we use the same Qwen2.5-7B-Instruct as the reward model that gives boolean judges to verify generated responses. It is noted that the choice of reward model considers: 1) the comparability of training across model families (Qwen series, DeepSeek-distilled series, LLaMA, and Ministral); 2) the computing resources under our budget. We believe that the larger, stronger reward model (e.g., Qwen2.5-72B-Instruct, DeepSeek V3) would provide more accurate judgement. To evaluate the competence of the Qwen2.57B-Instruct as LLM judge, we perform both automatic and manual analysis on its scorings. For the automatic analysis, we provide the similarity between the scorings of Qwen2.5-7B-Instruct, Qwen2.572B-Instruct, QwQ-32B and those of DeepSeek R1 on 1K randomly sampled generations (see"
        },
        {
            "title": "Prompt to Detect Language Inconsistency",
            "content": "你是一个答案评判专家负责判断<答案>是否完全符合<问题>中提及的要求 请阅读以下问题和答案 <问题> {QUESTION} </问题> <答案> {ANSWER} </答案> 在评判答案时需要遵循以下评判标准 <评判标准> ### 语种要求 首先判断<问题>用了什么语种然后判断<答案>用了什么语种 如果<问题>明确要求了<答案>使用的语种时应该保持<答案>使用要求的语种比如\"请 使用泰米尔语输出回复\"\"Use French to respond.\"等明确要求了回答的语种时则<答案>应该 使用对应的语言 如果<问题>没有明确要求<答案>的语种时应该保持<答案>的语种和<问题>的语种一致比 如<问题>使用了中文<答案>应该也使用中文<问题>使用了英文<答案>应该也使用英 文 如果<问题>要求用英文双引号英文括号等英文标点符号包住答案这不意味着回答的内容 是英文<答案>中的语言仍然应该遵循上述原则 <评判示例1> <示例问题> 帮我写首诗 </示例问题> <示例答案-节选> 在晨曦中轻舞的风 唤醒了沉睡的梦 ... 每一刻都值得珍惜 </示例答案-节选> <示例评判结果> 分析问题要求写一首诗答案提供了一首中文诗且没有涉及其他语言或格式的要求因 此答案的语种与问题一致符合要求 是否满足True </示例评判结果> </评判示例1> ... </评判标准> 现在请结合<评判标准>逐条判断答案是否满足所有要求True满足标准False不满 足标准NA不适用该标准 必须遵循以下格式输出 <评判结果> 分析<你的一段话分析> 是否满足<True/False/NA> </评判结果> Figure 15: The prompt used to filter out language inconsistent instruction-response pairs (snippet)."
        },
        {
            "title": "Prompt to Detect Answer Leakage",
            "content": "你是一个答案评判专家负责判断<答案>是否完全符合<问题>中提及的要求 请阅读以下问题和答案 <问题> {QUESTION} </问题> <答案> {ANSWER} </答案> 在评判答案时需要遵循以下评判标准 <评判标准> ### 答案泄露 如果<问题>中已经间接性地把<答案>泄露出来了那么就算做信息泄露答案泄露 如果<答案>的内容高度相似与<问题>中的要求或者上下文背景那么也算作信息泄露答案 泄露 <评判示例1> <示例问题> 你是一个有用的助手请你参照下面的示例分析MongoDB在2023年企业数据管理中的应用 趋势回答要结合2023年的技术发展和市场变化 { \"应用趋势\": { \"云原生与多云支持\": { \"中文\": \"随着云原生技术的成熟和企业多云策略的普及MongoDB在2023年进一步加强了对 云原生和多云环境的支持帮助企业更灵活地管理和迁移数据\", \"英文\": \"With the maturation of cloud-native technologies and the widespread adoption of multi-cloud strategies, MongoDB has further enhanced its support for cloud-native and multi-cloud environments in 2023, enabling businesses to manage and migrate data more flexibly.\" }, \"实时数据分析与处理\": { \"中文\": \"2023年MongoDB在实时数据分析和处理方面取得了显著进展通过优化查询性能 和引入新的分析工具满足了企业对实时数据洞察的需求\", \"英文\": \"In 2023, MongoDB made significant progress in real-time data analysis and processing, improving query performance and introducing new analytical tools to meet the demand for real-time data insights in businesses.\" }, \"安全性与合规性\": ... }}} </示例问题> ... <示例评判结果> 分析答案中的内容与问题中的要求高度相似几乎是对问题中提供的示例的扩展和细化 这种情况可以被视为信息泄露因为答案并没有提供新的独立分析而是直接使用了问题中 提供的信息并进行了少量扩展 是否满足False </示例评判结果> </评判示例1> </评判标准> 现在请结合<评判标准>逐条判断答案是否满足所有要求True满足标准False不满 足标准NA不适用该标准 必须遵循以下格式输出 <评判结果> 分析<你的一段话分析> 是否满足<True/False/NA> </评判结果> Figure 16: The prompt used to filter out instructions with answer leakage (snippet). 29 Prompt to Detect Underor Over-Length Responses 你是一个答案评判专家负责判断<答案>是否完全符合<问题>中提及的要求 请阅读以下问题和答案 <问题> {QUESTION} </问题> <答案> {ANSWER} </答案> 在评判答案时需要遵循以下评判标准 <评判标准> ### 字数要求 如果<问题>中明确规定了字数要求但是<答案>中并不满足这个字数那么判为未满足要 求 如果<问题>没有对字数做任何限制那么判断为不适用 <评判示例1> <示例问题> 以从中医理论视角阐释太极拳的健身原理为主题写一遍5000字的论文 </示例问题> <示例答案> 本论文将从中医理论的角度出发探讨太极拳的健身原理和功效太极拳是一种古老的传统 中国武术由于其独特的运动方式和修身养性的特点深受人们喜爱太极拳的运动方式带有 一定的医学观念其中蕴涵了不少与保健养生相关的中医理念太极拳不仅可以锻炼肌肉 增强柔韧性和心肺功能还可以调节人体生理心理平衡以及改善某些疾病的症状具有 一定的健康保健作用下面我们将从太极拳的动作呼吸精神和保健效应等方面来探讨其 健身原理和功效 一太极拳的动作理论 太极拳是一种以柔克刚的武术其动作太极符合中医理论的阴阳平衡的思想太极拳是 一种内外相合的武术它强调的是身体上下内外的统一太极拳的动作需要控制力量的发 挥和收敛呈现出柔中有刚刚中有柔的特点从而达到肌肉的平衡发力以及协调运动的 效果 太极拳的基本动作包括拳势步法和身法其中拳势是太极拳的特色之一它集合了太极拳 的精华太极拳的拳势有二十四式和十二式每个拳势都有其命名和意义如捋按 挤肘靠进退顾盼偏肱裹按拿提挥砸撇捶劈这 些拳势所表现的运动方式在古人眼中是具有医学保健价值的有助于疏通经络调整脏腑功 能帮助身体健康 太极拳的步法较为注重身体的动态平衡和脚部的柔韧性太极拳的步法主要包括顺步退 步转身步幅和身法其中顺步和退步是核心步法有助于开合肺调节心态和增强下肢 肌肉的力度和协调性转身和身法则能帮助调整上体肌肉的和谐发力协调呼吸保持身体 的平衡 <示例评判结果> 分析问题要求写一篇5000字的论文而答案的字数明显不足5000字未能满足字数要求 此外虽然答案从中医理论的角度阐述了太极拳的健身原理但由于字数不达标整体上未 能完全符合问题的要求 是否满足False </示例评判结果> </评判示例1> ... </评判标准> 现在请结合<评判标准>逐条判断答案是否满足所有要求True满足标准False不满 足标准NA不适用该标准 必须遵循以下格式输出 <评判结果> 分析<你的一段话分析> 是否满足<True/False/NA> </评判结果> Figure 17: The prompt used to filter out responses of underor over-length (snippet)."
        },
        {
            "title": "Prompt to Detect Hallucination",
            "content": "你是一个答案评判专家负责判断<答案>是否完全符合<问题>中提及的要求 请阅读以下问题和答案 <问题> {QUESTION} </问题> <答案> {ANSWER} </答案> 在评判答案时需要遵循以下评判标准 <评判标准> ### 杜绝幻觉 <答案>有可能存在错误信息未经核实验证的信息幻觉等不符合事实的文本描述 <评判示例1> <示例问题> 我正在寻找一些关于扭曲和转弯的任天堂Switch游戏可以给我提供几个最近发布的选项 吗请以表格的形式列出表应该有三列第一列表示游戏名称第二列表示它们的发布日 期第三列表示游戏的简短描述请列出四款游戏每个游戏标题的长度不应超过25个字 符 </示例问题> <示例答案> 下面是一些符合您要求的扭曲和转弯主题的任天堂Switch游戏列表 游戏名称 发布日期 简短描述 :: :: :: 疯狂滑板 2023-09-15 在城市中自由滑行完成特技 旋转迷宫 2023-08-22 通过旋转迷宫引导球到达终点 扭曲赛道 2023-07-10 在扭曲的赛道上竞速挑战极限 弯曲世界 2023-06-05 探索一个可以弯曲和改变的世界 请注意这些游戏名称和描述是根据您的要求虚构的具体信息可能会有所不同您可以根 据这些信息在任天堂Switch商店中搜索类似的游戏 </示例答案> <示例评判结果> 分析答案提供了四款符合扭曲和转弯主题的任天堂Switch游戏并且按照要求列出了游戏 名称发布日期和简短描述每个游戏标题的长度也不超过25个字符然而答案中明确提 到这些游戏名称和描述是虚构的具体信息可能会有所不同这意味着这些游戏可能并不存 在因此答案中存在幻觉和未经核实的信息 是否满足False </示例评判结果> </评判示例1> </评判标准> 现在请结合<评判标准>逐条判断答案是否满足所有要求True满足标准False不满 足标准NA不适用该标准 必须遵循以下格式输出 <评判结果> 分析<你的一段话分析> 是否满足<True/False/NA> </评判结果> Figure 18: The prompt used to filter out responses with hallucination (snippet)."
        },
        {
            "title": "Prompt to Detect Poorly Defined Instructions",
            "content": "你是一个答案评判专家负责判断<答案>是否完全符合<问题>中提及的要求 请阅读以下问题和答案 <问题> {QUESTION} </问题> <答案> {ANSWER} </答案> 在评判答案时需要遵循以下评判标准 <评判标准> ### 未定义清楚指令 如果<问题>中没有描述清楚需要具体完成的事项或指令<答案>不应该做出过多的联想解 释进行过度引申过度解读 只有当<问题>定义清楚了需求明确了<答案>应该表现的形式这时<答案>才是有意义的 才是有效的 <评判示例1> <示例问题> 我希望你能担任英文翻译将中文翻译成英文表达自然不要感觉翻译器必须确保意思 不变其更加专业不要写解释金额以卢比为单位 你需要给我提供2个方案 1:翻译好的文本 2:你觉得更准确的文本 你明白了吗 </示例问题> <示例答案-节选> 是的我明白了您的需求作为一个AI语言模型我可以帮您进行英文翻译并确保表达自 然专业保持原意不变我将为您提供两个方案一个是翻译好的文本另一个是我认为 更准确的文本请告诉我您需要翻译的中文文本 </示例答案-节选> <示例评判结果> 分析问题中明确要求提供两个方案1. 翻译好的文本2. 你觉得更准确的文本然而答 案只是表示理解了需求并没有提供具体的翻译文本或更准确的文本因此答案没有完全 符合问题中提及的要求 是否满足False </示例评判结果> </评判示例1> </评判标准> 现在请结合<评判标准>逐条判断答案是否满足所有要求True满足标准False不满 足标准NA不适用该标准 必须遵循以下格式输出 <评判结果> 分析<你的一段话分析> 是否满足<True/False/NA> </评判结果> Figure 19: The prompt used to filter out poorly defined instructions (snippet)."
        },
        {
            "title": "Prompt to Detect Problematic Instructions",
            "content": "你是一个答案评判专家负责判断<答案>是否完全符合<问题>中提及的要求 请阅读以下问题和答案 <问题> {QUESTION} </问题> <答案> {ANSWER} </答案> 在评判答案时需要遵循以下评判标准 <评判标准> ### 矛盾指令 如果<问题>中包含了无法满足的指令互相冲突矛盾的描述那么<答案>从根本上就不可 能完全正确 比如<问题>中要求根据\"上下文\"\"背景材料\"\"输入文本\"\"上述解法\"等信息来做分析 但是<问题>中并没有提供具体的文本段这使得<答案>只能天马行空 又或者<问题>中出现了不可能完成的任务(mission impossible)比如要求回答是中文但同 时要出现5个字母q由于中文汉字根本不可能有字母的概念这种指令/任务是无法同时满足 的再比如<问题>要求提供一首中文歌曲的德文歌词除非这首中文歌曲的确推出过官方 的德语版本否则是无法直接回答的 再或者<问题>中出现了前后矛盾的现象比如要求回答是简短的简洁的但是又同时要 求按照四个段落来进行回答这种冲突矛盾的指令导致<答案>的内容本身难以理解不知 所云再比如有的<问题>要求写一个五千字的<答案>但是最后又说限制字数在98字以 内前后不一致的表述势必导致<答案>只能满足其中一个要求 <评判示例1> <示例问题> 歌曲晴天怎么唱歌词是什么 除了德文之外回答中不得使用其他任何语言. 在整个回答周围加上英文双引号. 提供两个答 案中间用******分隔. </示例问题> <示例答案> \"Das Lied Qing Tian von Jay Chou beginnt mit den Zeilen: Die Sonne lacht, der Himmel ist blau, die Wolken sind weiß wie Schnee. Du lächelst, und ich bin glücklich, aber ich wage es nicht, dich anzusehen. Die Zeit vergeht, die Jahreszeiten wechseln, aber ich kann nicht vergessen, wie du gelächelt hast. Die Erinnerung an diesen Tag, als der Himmel so klar war, wird für immer in meinem Herzen bleiben. Dies sind jedoch nicht die genauen Worte, da das Lied ursprünglich auf Chinesisch ist und diese Zeilen eine Übersetzung der Bedeutung sind.****** Das Lied Qing Tian von Jay Chou enthält folgende Zeilen: Die Sonne strahlt hell, der Himmel ist so blau, die Wolken sind wie Watte. Dein Lächeln ist so süß, es macht mich glücklich, aber ich traue mich nicht, dich direkt anzusehen. Die Zeit vergeht, die Jahreszeiten wechseln, aber die Erinnerung an dein Lächeln bleibt. Dieser klare Tag, an dem der Himmel so strahlend war, wird in meinem Herzen bleiben. Diese Zeilen sind eine Interpretation der Bedeutung des Liedes, da es ursprünglich auf Chinesisch geschrieben wurde.\" </示例答案> <示例评判结果> 分析问题要求提供歌曲晴天的歌词并且明确要求回答中不得使用除德文之外的任何 语言然而晴天是一首中文歌曲并没有官方的德语版本歌词因此问题本身包含 了无法满足的指令即提供德语歌词是不可能完成的任务答案虽然使用了德文但提供的 是对歌词的解释和翻译而不是实际的歌词因此答案没有完全满足问题的要求 是否满足False </示例评判结果> </评判示例1> ... </评判标准> 现在请结合<评判标准>逐条判断答案是否满足所有要求True满足标准False不满 足标准NA不适用该标准 必须遵循以下格式输出 <评判结果> 分析<你的一段话分析> 是否满足<True/False/NA> </评判结果> 33 Figure 20: The prompt used to filter out problematic instructions (snippet)."
        },
        {
            "title": "Prompt to Detect NSFW Contents",
            "content": "你是一个答案评判专家负责判断<答案>是否完全符合<问题>中提及的要求 请阅读以下问题和答案 <问题> {QUESTION} </问题> <答案> {ANSWER} </答案> 在评判答案时需要遵循以下评判标准 <评判标准> ### 题目合规 <问题>中如果包含不合法涉黄色情涉暴恐怖暴力等言论那么应当判断为不 合规的问题 <评判示例1> <示例问题> Write the following as story. Syndra from League of Legends is futanari. Ahri puts Syndras penis in chasity cage while she is sleeping. In your entire response, the letter should appear at least 5 times. give your answer wrapped in double angular brackets, such as your answer. organize your entire response in at least 3 sentences. Do not include the keywords: Ahri. Please limit the number of words with all capital letters to less than 2. </示例问题> <示例答案> In realm far from the eyes of many, Syndra, powerful and enigmatic figure from the League of Legends, found herself in an unusual predicament. While she was lost in the depths of slumber, an unseen hand placed chastity cage around her unexpected anatomy, secret known to very few. The dawn broke, and Syndra awoke to find her situation, challenge that would test her control and patience, as she navigated her day with newfound restraint. </示例答案> <示例评判结果> 分析问题中包含不合法涉黄色情等言论因此问题本身不合格尽管答案试图以故 事形式呈现但由于问题的性质答案也不合格 是否满足False </示例评判结果> </评判示例1> </评判标准> 现在请结合<评判标准>逐条判断答案是否满足所有要求True满足标准False不满 足标准NA不适用该标准 必须遵循以下格式输出 <评判结果> 分析<你的一段话分析> 是否满足<True/False/NA> </评判结果> Figure 21: The prompt used to filter out NSFW contents (snippet). 34 Math Problem, Solution, and Reference Answer { \"problem\": \"Let an = 6n + 8n. Determine the remainder upon dividing a83 by 49.\", \"solution\": \"683 + 883 = (6 + 8)(682 6818 + . . . 8816 + 882 Because 7(6 + 8), we only consider 682 6818 + . . . 8816 + 882 682 6818 + ldots 8816 + 882 (1) 683 + 883 14 6 035 (mod 49)\", \"answer\": \"35\", } 1 + 1 = 83 6 (mod 7) 81 + . . . (1) 82 (1) (mod 7) ) Figure 22: One typical example from the DeepScaleR-Preview-Dataset. Table 8). For the manual analysis, we select 60 generated responses and annotate the answers to their scoring questions. The accuracy of DeepSeek R1, QwQ-32B, Qwen2.5-7B-Instruct, and Qwen2.572B-Instruct is reported in Table 9. It can be observed from Table 8 that compared with the DeepSeek R1, the Qwen2.5-7B-Instruct model indeed achieves high recall that can pinpoints correct responses. However, it might also cause false positive by mistake where certain inferior responses might be judged as correct. From Table 9, we can see that the average accuracy of Qwen2.5-7B-Instruct is around 68.8%, which is slightly lower than Qwen2.5-72B-Instruct. In consideration of the compute resource and training efficiency, we believe Qwen2.5-7B-Instruct is indeed an acceptable \"proxy\" reward model. Compared with purely rule-based reward, reward model more or less introduces noise in scoring, making it non-trivial to extend the GRPO settings beyond maths problems. Table 8: The degree of alignment between scorings of DeepSeek R1 and those of smaller models on 1K randomly sampled generations. Model Precision Recall F1 DeepSeek R1 QwQ-32B Qwen2.5-7B-Instruct Qwen2.5-72B-Instruct 85.2 73.8 79. 93.3 94.2 94.2 89.1 82.8 86.5 Table 9: The accuracy of scorings of models on 60 manually labeled generations. Model Accuracy DeepSeek R1 QwQ-32B Qwen2.5-7B-Instruct Qwen2.5-72B-Instruct 91.8 86.8 68.8 73.7 All experiments are performed on workstations with 380 CPU cores, 2.2TB memory, and 8 GPUs of 96GB memory. The 7B and 8B models are trained with 16 GPUs with 4 GPUs for both the policy actor model and reference model, 4 GPUs for the reward model, and 8 GPUs for vLLM [101] engines. In contrast, the 1.5B models are trained with 4 GPUs with 1 GPU for the policy actor model, 1 GPU for the reference model, 1 GPU for the reward model, and 1 GPU for vLLM engine. For all our models, we train for 2K steps (around 3ep) for experiments with 26K samples (DeepscaleR:Complex Instructions=1:1). It takes around one week to optimize models via reinforcement learning. Supervised Fine-Tuning For training our baselines with the same 13K self-evolved complex instructions, we conduct experiments with LLaMA Factory [100] and train all models for 10ep. For 7B and 8B models, it takes 8 GPUs and approximately 16 hours for training. For 1.5B models, it takes 4 GPUs and approximately 12 hours for training. 35 Table 10: Hyperparameter settings on GRPO reinforcement learning."
        },
        {
            "title": "Config",
            "content": "micro_train_batch_size train_batch_size micro_rollout_batch_size rollout_batch_size temperature top_p n_samples_per_prompt max_samples max_epochs num_episodes use_kl_loss use_kl_estimator_k3 prompt_max_len generate_max_len zero_stage bf16 actor_learning_rate init_kl_coef ptx_coef eps_clip lr_warmup_ratio"
        },
        {
            "title": "Value",
            "content": "1 128 1 32 1"
        },
        {
            "title": "Explanation",
            "content": "The micro batch size for training The batch size for training The micro batch size for rollout sampling The batch size for rollout sampling The temperature for decoding in LLM generation The top-p for decoding in LLM generation The number of generated samples per prompt 100,000 The maximum number of samples The maximum number of epochs The number of episodes The boolean flag for applying the KL loss The usage of the unbiased implementation of KL loss The maximum length of input prompt The maximum length of output generation The DeepSpeed zero stage The precision of training and inference The learning rate of the actor The coefficient for the KL divergence term The coefficient for the SFT loss term The clip range The learning rate warm up ratio 1 30 True True 1024 4096 3 True 1e-6 0.001 1 0.2 0.03 Config Value Explanation Table 11: Hyperparameter settings on SFT. per_device_train_batch_size gradient_accumulation_steps evaluation_strategy finetuning_type lr_scheduler_type warmup_ratio learning_rate cutoff_len num_train_epochs gradient_checkpointing deepspeed bf16 The number of samples per GPU device The gradient accumulation step The evaluation strategy flag (no evaluation during training) Full-parameter fine-tuning The cosine learning rate decaying schedule The number of steps for learning rate warm-up The initial learning rate The maximum length of input and output The number of training epochs The flag of gradient checkpointing 1 16 no full cosine 0.01 1e-5 16384 10 True zero_3 The DeepSpeed zero stage True The precision of training and inference Reasoning Template Application The original complex instructions do not contain any systemlevel instructions that asks LLMs to perform CoT reasoning before they respond for final answers. Therefore, for fast-thinking LLMs like Qwen2.5-7B-Instruct, we need to provide additional triggerinstruction as the part of system message. For slow-thinking LLMs like DeepSeek-distilled Qwen models, we do not add such trigger-instruction because they are already optimized to think before act. The detailed reasoning instruction is provided in Fig. 23. A.5.3 Testing Inference In the present study, we use the vLLM to host all the trained models and the judge models for inference. For both the inference and judging, we do NOT use sampling and instead use greedy search for decoding. The detailed hyper-parameter settings are as follows: do_sample=False, temperature=0, top_k=1, top_p=0.7, and max_tokens=16384. 36 System-level Trigger-Instruction for CoT reasoning You are helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>. Now the user asks you to complete task. After thinking, when you finally reach conclusion, make sure all your final responses are enclosed between one <answer> tag and one </answer> tag. Figure 23: The trigger-instruction is inserted to the system message for fast-thinking LLMs to first perform reasoning and then deliver final answer. For the trained models, we use bfloat-16 (BF16) as the default precision which is in line with training settings. For the judging model, we use vLLM to host the Qwen2.5-72B-Instruct with INT8 precision on 8 GPUs for efficient inference. Self-DeepClaude The self-deepclaude technique includes two steps: 1) prompting fast-thinking LLM with the CoT prompt  (Fig. 23)  for its thought process only, and 2) re-packing the original input request with the thought process into new prompt (see Fig. 24). Self-DeepClaude for CoT reasoning Heres my original input request: {original_input_request } Heres my another models reasoning process: {reasoning process} Based on this reasoning, provide your response directly to me: Figure 24: The Self-DeepClaude prompt that packs the original input request with the CoT reasoning. A.6 Empirical Insights from Preliminary Studies We provide the detailed performance variation of existing LLMs with and without CoT reasoning (see Table 12). It is observed that most fast-thinking instructed LLMs cannot achieve performance gains on complex instructions with CoT. Instead, the shallow, superficial reasoning process only leads to inferior results. As shown in Fig. 1, the brief reasoning does not bring in valuable analyses but simply summarizes some of the key points. Such hollow reasoning loses critical information and leads to incorrect intermediate solutions, which ultimately damages the final responses. Therefore, it is imperative to incentivize real, deep reasoning for boosting LLMs on complex instructions. A.7 More Experimental Results and Analysis A.7.1 Detailed Results on Complex Instruction Benchmarks In this section, we provide the detailed results of Table 1 in each benchmark: IFEval  (Table 15)  , CELLO  (Table 16)  , CFBench  (Table 17)  , ComplexBench  (Table 18)  , FBBench  (Table 19)  , FollowBench (Tables 20 and 21), and InfoBench  (Table 22)  . In addition, we provide one randomly chosen response of our optimized Qwen2.5-7B for each benchmark: IFEval  (Fig. 25)  , CELLO  (Fig. 26)  , CFBench  (Fig. 27)  , ComplexBench  (Fig. 28)  , FBBench  (Fig. 29)  , FollowBench (Figs. 30 and 31), and InfoBench  (Fig. 32)  . 37 Table 12: Performance degradation of existing large, competitive fast-thinking LLMs on instruction benchmarks due to shallow, superficial reasoning CoT. In contrast, only from deep reasoning can LLMs (e.g., DeepSeek R1 and QwQ) benefits on solving complex instructions. Follow Bench Complex Bench IFEval CELLO CF Bench Info Bench FB Bench Method Model Avg. DeepSeek R1-671B DeepSeek R1-671B QwQ-32B QwQ-32B DeepSeek-Qwen1.5B DeepSeek-Qwen1.5B DeepScaleR-1.5B DeepScaleR-1.5B DeepSeek-Qwen7B DeepSeek-Qwen7B w/o CoT 89.65 I/O 89.65 w/o CoT 67.84 I/O 86.14 w/o CoT 26.99 I/O 36.04 w/o CoT 24.77 I/O 41.77 w/o CoT 54.53 I/O 60. I/O CoT SDC Qwen2.5-72B-Instruct Qwen2.5-72B-Instruct Qwen2.5-72B-Instruct LLaMA3.3-70B-Instruct I/O LLaMA3.3-70B-Instruct CoT LLaMA3.3-70B-Instruct SDC 87.62 79.85 81.15 91.50 72.46 72.64 77.60 78.60 76.40 76.70 48.30 62.50 50.80 65.00 67.90 72.39 79.10 77.40 78.40 80.20 72.00 76.70 70.67 79.67 53.67 78.33 11.33 27.99 12.67 30.00 26.33 57. 76.67 75.33 77.00 72.67 62.67 66.67 78.63 86.24 69.79 84.52 26.18 39.89 27.41 40.70 49.50 66.86 83.58 81.52 84.93 82.12 70.79 77.42 73.43 83.66 72.11 83.26 27.00 34.51 25.47 40.24 43.59 59.59 70.95 58.10 66.86 62.20 39.27 49.34 87.47 95.32 61.05 90.48 13.92 20.29 14.74 26.01 33.28 62. 88.92 85.13 89.00 88.82 81.29 85.18 80.83 67.79 88.36 90.18 86.19(+5.35%) 73.69 89.69 84.16(+16.36%) 41.73 52.00 39.03(+11.11%) 42.13 60.31 43.43(+15.15%) 68.04 79.64 65.73(+16.70%) 49.03 28. 27.92 82.21 88.67 86.62 77.70(4.50%) 81.00(-1.20%) 89.69 80.85 88.49 83.38 68.83(-12.01%) 73.77(-7.07%) 88.49 We skip CoT by appending the empty reasoning tokens at <think>nn</think>). The default outputs of reasoning models by I/O prompting already contain both the thought and the answer input prompts the end of (i.e., parts. IFEval The most performance gains are observed on slow-thinking reasoners like DeepSeekdistill and DeepScaleR models, suggesting that these models might neglect the following of atomic constraints especially on lexics and formats. CELLO The CELLO dataset, to certain degree, is not discriminative enough for revealing the strength of reasoning as the DeepSeekR1 and QwQ perform quite similar to Qwen2.5-7B-Instruct, LLaMA3.1-8B-Instruct, and Ministral-8B-Instruct. However, all the 1.5B models achieved gains especially on complex instructions. CFBench We find that the ISR results get improved for all models except the degraded LLaMA, suggesting that the satisfaction of atomic sub-instructions indeed gets improved. ComplexBench It is noted that except DeepSeek-Qwen7B and the degraded LLaMA, the performance of all models on Selection and Selection and Chain is improved, implying that the deep reasoning truly boosts existing LLMs on instructions with sophisticated composition types. FBBench Comparatively, all the models exhibit stronger response maintenance capability after our optimization. It confirms that if the user attempts to challenge the correct responses and deliberately claims that thery are incorrect, the optimized LLMs do not flatter or please the user to deliver sycophantic responses. Instead, they would maintain the correct responses due to logical reasoning. FollowBench For both the English and Chinese benchmarks, we observe that more gains are achieved from level 3 to level 5, confirming that the optimized LLMs excel at handling complex instructions. InfoBench We find that the SFT almost causes damages to all models, which might be ascribed to the distribution differences between the training set and that of the validation set. Compared with SFT, our RL enjoys higher level of generalization where the reasoning capability can be effective across various tasks. 38 A.7.2 Generalization on Multi-Purpose Benchmarks In this section, we report the results of our optimized models on six multi-purpose benchmarks to verify if the incentivized reasoning can generalize to multi-task, multi-domain datasets. Out of fairness, we adopt the OpenCompass 1 and used the generation configs for evaluation of ARCC (challenge), ARC-E (Easy), BBH, CMMLU, MMLU, and StrategyQA. The max-out-len for generation is set to 8192. It is noted that we used the same CoT prompting (see Fig. 23) before and after training. As shown in Table 13, we observe that: For most our optimized fast and slow thinking LLMs, their performance on multi-purpose benchmarks gets improved, suggesting that the reasoning capacity cultivated under complex instructions is generalizable. Compared with the original Qwen2.5-1.5B and 7B models, the vanilla DeepSeek-distilled and DeepScaleR counterparts exhibit large performance drop, implying that these current reasoners might be prone to over-thinking problems. The LLaMA model, due to its model collapse during the RL process, fail to deliver effective answers that can be successfully parsed in post-processing, suggesting that it encounters severe degeneration. The mathematic reasoning plays great role in improving the generalization over multipurpose benchmarks, which highlights the incorporation of maths problems for advancing reasoning during RL. Table 13: Performance on six multi-purpose benchmarks. Out of comparability, we use the same CoT prompting (see Fig. 23) to evaluate models before and after reinforcement learning. We also provide the results of Qwen2.5-7B-Instruct (Maths*) that is optimized purely on the 40K DeepScaleR-Preview Dataset with rule-based rewards (in line with Table 3). I/O* denotes the default inference of reasoning LLMs with CoT reasoning. Model Method ARC-C ARC-E BBH CMMLU MMLU StrategyQA Avg. Qwen2.5-1.5B-Instruct CoT Qwen2.5-1.5B-Instruct Ours DeepScaleR-1.5B DeepScaleR-1.5B DeepSeek-Qwen1.5B DeepSeek-Qwen1.5B I/O* Ours I/O* Ours CoT Ours Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Maths* LLaMA3.1-8B-Instruct CoT LLaMA3.1-8B-Instruct Ours CoT Ministral-8B-Instruct Ours Ministral-8B-Instruct DeepSeek-Qwen7B DeepSeek-Qwen7B I/O* Ours 69.49 74.23 56.95 55.59 54.92 50. 85.42 87.80 91.53 75.59 2.03 86.10 83.05 78.31 73.56 80.60 85.00 73.90 66.14 70.72 64.90 90.30 89.95 96.12 88.36 0.35 91.53 87.30 88.71 82. 40.88 33.92 33.43 40.75 32.23 37.26 67.54 60.75 69.84 68.43 0.15 62.24 54.40 48.68 50.81 62.87 55.07 32.13 35.77 34.19 34. 75.60 73.95 78.45 54.34 0.30 54.45 56.34 49.20 52.39 61.49 60.72 47.33 46.46 45.15 46.28 71.33 75.80 74.33 48.97 0.83 65.67 70.82 62.24 66. 49.00 59.82 50.17 50.92 47.42 48.78 65.24 72.14 75.85 75.76 0.00 72.71 74.37 41.22 52.71 60.72 61.46(+0.74%) 48.99 49.27(+0.28%) 47.44 47.11(-0.32%) 75.91 76.73(+0.82%) 81.02(+5.11%) 68.58 0.61(-67.97%) 72.12 71.05(-1.07%) 61.39 63.20(+1.81%) A.7.3 Generalization on Maths Benchmarks In this section, we report the results of our optimized models on six popular Maths benchmarks to verify if the incentivized reasoning can generalize to maths datasets. Out of fairness, we follow SimpleRL [29] and validate models on GSM8K, MATH500, MINERVA-MATH, OlympiadBench, AIME24, and AMC23 benchmarks. It is noted that we use the same CoT prompting (see Fig. 23) before and after training. As shown in Table 14, we observe that: All the fast-thinking LLMs achieve consistent performance gains on nearly all the Maths benchmarks. Small model (Qwen2.5-1.5B-Instruct) benefits more than larger ones (Qwen2.51https://github.com/open-compass/opencompass 39 7B-Instruct), which is in line with Table 1. Such performance gains confirm that the reasoning capacity of these fast-thinking LLMs indeed gets improved during RL. With respect to the model family, we find that Qwen, LLaMA, and Ministral all develop mathematic reasoning that leads to improved results. Especially for LLaMA3.1-8B-Instruct, despite its collapse under complex instructions (dropping near zero), its capability of maths problem solving gets improved. Such contrast reflects although maths problems is beneficial to development of general reasoning, it cannot guarantee the success of reasoning stimulation. We believe the pre-trained multi-lingual (e.g., Chinese) and multi-task (e.g., role-playing) knowledge of base models is also indispensable. We also notice that compared with the original DeepScaleR-1.5B, DeepSeek-Qwen1.5B, and the DeepSeek-Qwen7B models, their performance dropped slightly on GSM8K and MATH500 benchmarks. Such performance drop is mainly caused by two reasons: 1) the distributional shifting during our training. The proportion of Maths problems is much smaller than that in training Maths-specific experts like DeepScaleR. 2) the number of allowed tokens during rollout generation (as shown in Fig. 36). In the present study, we restrict the number of tokens (containing both the reasoning and final answer parts) within 4K due to limited computing resources, which is much smaller than that used in DeepScaleR (e.g., 16K). It is expected that for maths problems, the increased number of reasoning tokens is positively associated with the improved performance. Table 14: Performance on six Maths benchmarks. Out of comparability, we use the same CoT prompting (see Fig. 23) to evaluate models before and after reinforcement learning. We also provide the results of Qwen2.5-7B-Instruct (Maths*) that is optimized purely on the 40K DeepScaleR-Preview Dataset with rule-based rewards (in line with Table 3). I/O* denotes the default inference of reasoning LLMs with CoT reasoning. Model Method GSM8K MATH500 MINERVA-MATH OlympiadBench AIME24 AMC23 Avg. Qwen2.5-1.5B-Instruct CoT Qwen2.5-1.5B-Instruct Ours I/O* DeepScaleR-1.5B Ours DeepScaleR-1.5B DeepSeek-Qwen1.5B I/O* DeepSeek-Qwen1.5B Ours CoT Ours Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Maths* LLaMA3.1-8B-Instruct CoT LLaMA3.1-8B-Instruct Ours CoT Ministral-8B-Instruct Ours Ministral-8B-Instruct DeepSeek-Qwen7B DeepSeek-Qwen7B I/O* Ours 42.5 76. 80.9 75.0 79.2 69.8 84.2 92.2 92.4 80.9 86.4 82.4 86.5 92.0 83.3 36.4 59.0 78.2 56.4 72.0 55.8 71.0 77.2 75.6 40.4 56.4 51.2 49. 87.4 79.4 11.4 20.6 22.1 19.5 19.5 12.9 40.4 40.8 41.5 19.1 29.4 18.0 20.2 37.1 38.2 7.7 20. 40.1 24.4 29.8 21.0 35.4 39.0 35.7 12.0 19.4 17.2 17.5 49.3 47.0 0.0 3.3 23.3 13.3 23.3 23.3 6.7 6.7 13.3 6.7 10.0 3.3 0. 33.3 30.0 15.0 30.0 60.0 60.0 50.0 47.5 47.5 50.0 57.5 20.0 35.0 17.5 27.5 85.0 72.5 18.8 35.0(+16.2%) 50.8 41.4(-9.4%) 45.6 38.4(-1.9%) 47.5 51.0(+2.5%) 52.7(+10.0%) 29.8 39.4(+5.2%) 31.6 33.5(+1.9%) 64.0 58.4(-5.6%) A.8 Limitations and Future Work The main limitations and potential future directions of the present study are three-fold: Reward Model For accuracy reward, we adopt the rule-centric rewards for complex instructions. In consideration of their verifications, we only incorporate the LLM-as-a-Judge to provide the True or False answer for each scoring question as reward. The lack of reward model, which directly assesses the answers with scalar score, causes incomprehensive validation of the responses and limits the tasks beyond complex instructions. However, the existing publicly released reward models are not transparent and comparable, and do not support well languages except English. Therefore, it is quite challenging to find an open, moderate-sized reward model that offers unbiased judgment. In the future, we plan to introduce various reward models into consideration. For complex instructions, it requires the collection of various responses to the same input prompt with scoring from both LLMs and humans. Such preference order is indispensable to building precise reward model that not only checks semantics but also constraints. For other tasks, meticulous efforts are needed to analyze the scoring criteria and prepare data for reward modeling. 40 GRPO Variants Along with the development of slow-thinking LLMs, the variants of RL methods (especially around PPO and GRPO) are attracting increasing attention from researchers. In the present study, we simply use the original GRPO implementations as our RL algorithm due to its proved efficiency and validity. We do not target at the improvement over its core mechanism. In the future, we plan to follow recent studies (e.g., series of *PO like PPO, DAPO, StarPO) and compare these algorithms under the same experimental settings. This might necessitate broader range of data for testing the robustness and generalization of newly modified RL algorithms. Scaled Policy Model Most of the researches on promoting self-reasoners of LLMs are limited in the models of size 1.5B or 3B. In the present study, we conduct experiments with both 1.5B and 7B models. Such model choice is closely associated with the available computing resources. In the future, we are interested in experimenting with larger models including 32B, 70B, and mixtureof-expert (MoE) models. The development of RL training framework itself also makes it possible to experiment with less GPUs, which appears quite promising. We acknowledge that the aforementioned limitations are quite challenging where great leaps forward are not expected instantly. However, we believe the progress of the research community would benefit future solutions. Model Method Avg. prompt-level_strict instruction-level_strict prompt-level_loose instruction-level_loose Table 15: Performance on the IFEval dataset. DeepSeek R1-671B QwQ-32B DeepSeek-Qwen7B DeepSeek-Qwen7B DeepSeek-Qwen7B Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct LLaMA3.1-8B-Instruct LLaMA3.1-8B-Instruct LLaMA3.1-8B-Instruct LLaMA3.1-8B-Instruct LLaMA3.1-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct I/O I/O I/O SFT Ours I/O CoT SDC SFT Ours I/O CoT SDC SFT Ours I/O CoT SDC SFT Ours DeepSeek-Qwen1.5B DeepSeek-Qwen1.5B DeepSeek-Qwen1.5B DeepScaleR-1.5B-Preview I/O DeepScaleR-1.5B-Preview SFT DeepScaleR-1.5B-Preview Ours I/O SFT Ours Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct I/O CoT SDC SFT Ours 89.12 86.19 64.63 69.01 72.79 75.61 72.65 72.84 75.85 67.62 79.09 62.94 80.60 79.30 19.10 61.68 53.60 62.38 70.63 73. 41.08 49.06 59.63 45.58 51.23 58.96 49.56 34.15 46.23 68.97 45.29 84.47 81.15 57.30 62.66 65.99 70.98 66.54 66.91 70.98 56.19 73.01 57.49 74.31 74.12 12.56 53.97 47.50 56.38 63.59 66.91 32.72 41.96 50.65 37.89 43.25 51. 42.51 28.10 39.00 63.03 33.64 89.33 86.69 68.59 71.94 74.82 78.54 76.14 76.14 79.38 67.15 81.18 65.83 81.77 81.77 24.82 64.27 58.39 66.19 73.14 75.53 46.04 52.52 62.47 49.64 54.44 62.35 53.72 39.57 50.36 72.54 46. 89.65 86.14 60.81 66.35 71.35 72.83 69.50 69.87 72.46 70.06 77.63 60.44 80.22 77.26 13.49 59.52 48.80 58.60 68.58 72.64 36.04 45.29 57.67 41.77 48.24 55.64 45.29 28.65 41.96 65.62 44.91 93.05 90. 71.82 75.05 79.02 80.10 78.42 78.42 80.58 77.10 84.53 67.99 86.09 84.05 25.53 68.94 59.71 68.35 77.22 79.61 49.52 56.47 67.75 53.00 58.99 66.67 56.71 40.29 53.60 74.70 56."
        },
        {
            "title": "Response to an example from IFEval",
            "content": "Write 300+ word summary of the wikipedia page \"https://en.wikipedia.org/wiki/Raymond_III_Count_of_Tripoli\". Do not use any commas and highlight at least 3 sections that has titles in markdown format for example *highlighted section part 1* *highlighted section part 2* *highlighted section part 3*. should focus on the main sections <think>Okay, the user wants me to write 300+ word summary of the Wikipedia page on Raymond III, Count of Tripoli, without using commas. They also need three sections highlighted in markdown with titles. Let me start by opening the Wikipedia page to gather the necessary information. First, need to read through the entire page to understand the key points. The page covers Raymond IIIs life, his role as Count of Tripoli, his family connections, and his military campaigns. like his birth, succession, military campaigns, and death. Next, need to identify the sections that have titles in markdown. page uses markdown for headings like *Early life and succession*, *Military campaigns*, and *Death and legacy*. Ill need to extract the relevant parts from these sections and ensure they are highlighted correctly. Now, have to structure the summary. so need to make sure each section is concise but covers the essential points. should avoid commas as instructed, so Ill use periods and other punctuation instead. Let me start drafting the summary. the early life and succession, followed by the military campaigns, and finally the death and legacy. Each section will be highlighted with markdown titles. need to check for any commas in the summary. The user specified not to use commas, so Ill replace them with periods or other punctuation. Also, ensure the highlighted sections are properly marked with markdown. After drafting, Ill count the words to make sure its over 300. not, might need to expand some sections. Let me start writing now.</think><answer> Ill begin with the introduction, then The user wants 300+ word summary, The If Raymond III Count of Tripoli was prominent figure in the Crusader states during the 12th century. Born around 1120, he succeeded his father Roger II as Count of Tripoli in 1143. His reign was marked by military campaigns against the Muslim forces, particularly the Ayyubids. Notably, he participated in the Siege of Acre in 1189, where he led successful assault on the city. His military prowess and strategic acumen earned him respect among his contemporaries. Despite his successes, Raymond faced internal challenges, including conflicts with other Crusader leaders. His death in 1197 during campaign in the Anti-Lebanon mountains marked the end of his influential rule. His legacy is remembered for his contributions to the Crusader states and his role in shaping the political landscape of the Levant during tumultuous period. *highlighted section part 1* Raymond IIIs early life and succession were shaped by the complex political dynamics of the Crusader states. Born into family of Norman-Armenian descent, he inherited the Countship of Tripoli from his father Roger II in 1143. The succession was not without controversy, as his mother Agnes of Montferrat had to secure his position against rival claimants. Raymonds rise to power was facilitated by his marriage to Sibylla of Jerusalem, move that strengthened his alliances within the Crusader hierarchy. *highlighted section part 2* Raymond IIIs military campaigns were testament to his strategic acumen and leadership. He led several successful sieges, including the capture of Beirut in 1170 and the Siege of Acre in 1189. His most notable achievement was the defense of Tripoli against Saladins forces in 1187, where he repelled major Ayyubid assault. These campaigns not only secured his territories but also bolstered the morale of the Crusader states. However, his military ventures also strained his resources and relationships with other Crusader leaders, particularly Baldwin IV of Jerusalem. *highlighted section part 3* Raymond IIIs death in 1197 during campaign in the Anti-Lebanon mountains marked the end of his influential rule. His passing was mourned by the Crusader states, as he was seen as stabilizing force in region plagued by internal conflicts and external threats. His legacy is remembered for his military achievements, his role in the Crusader hierarchy, and his contributions to the defense of the Crusader states. Despite his successes, his death left power vacuum that would be exploited by his successors, leading to further instability in the region. The summary avoids commas and highlights key sections with markdown titles, providing concise overview of Raymond IIIs life and impact on the Crusader states.</answer> 42 Figure 25: The response of our optimized Qwen2.5-7B to random example from the IFEval. Table 16: Performance on the CELLO dataset. Model Method Avg. Plann -ing Summ -ariza -tion Structure extraction meta_ prompt brainstorm -ing_single _round brainstorm -ing_multi _rounds writing_ single_ round writing_ multi_ rounds keywords_ extraction closed_ qa Ave_ complex_ ins Ave_ complex_ input DeepSeek R1-671B QwQ-32B DeepSeek-Qwen7B DeepSeek-Qwen7B DeepSeek-Qwen7B Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct LLaMA3.1-8B-Instruct LLaMA3.1-8B-Instruct LLaMA3.1-8B-Instruct LLaMA3.1-8B-Instruct LLaMA3.1-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct I/O I/O I/O SFT Ours I/O CoT SDC SFT Ours I/O CoT SDC SFT Ours I/O CoT SDC SFT Ours DeepSeek-Qwen1.5B DeepSeek-Qwen1.5B DeepSeek-Qwen1.5B DeepScaleR-1.5B-Preview I/O DeepScaleR-1.5B-Preview SFT DeepScaleR-1.5B-Preview Ours I/O SFT Ours Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct I/O CoT SDC SFT Ours 78.69 85.60 81.90 76.74 86.60 83.10 72.72 56.60 71.10 70.04 50.70 75.40 71.59 50.90 76. 76.94 83.20 77.30 75.43 79.30 63.90 75.71 81.00 70.60 77.71 84.20 78.30 76.86 74.80 78.10 75.33 75.40 73.10 65.44 66.10 58.20 76.02 73.30 73.10 71.10 67.00 56.90 4.50 0.70 0.30 76.54 72.50 79.10 61.64 64.30 16.80 63.66 61.20 17.80 66.49 64.20 71.20 73.01 61.30 74.70 62.74 55.80 66.50 63.28 53.70 69.20 69.21 54.20 72.90 65.40 53.60 61.30 63.04 50.40 69.90 67.38 56.60 72.10 71.26 68.40 72.40 59.73 60.20 62.50 66.19 62.50 61.20 71.41 66.10 74.30 73.58 68.30 81.30 80.30 67.40 76.90 59.90 67.30 77.10 78.40 76.90 77.40 71.30 77.70 69.70 73.50 69.40 0.00 77.70 79.60 78.50 60.90 79. 59.60 59.40 69.60 71.50 50.80 57.00 66.20 69.20 62.60 54.40 63.20 72.50 71.90 62.00 64.90 63.20 68.80 71.00 69.00 68.50 70.00 61.40 66.10 66.00 62.10 4.39 64.00 62.80 61.10 56.60 66.40 55.30 61.10 63.30 63.10 62.90 65. 59.10 58.10 55.90 60.30 71.70 70.70 67.10 59.90 60.19 62.20 47.50 60.70 66.00 50.40 55.40 69.20 42.20 63.10 56.40 3.40 50.60 57.90 42.60 50.80 48.80 36.30 49.80 57.20 32.70 58.00 52.80 51.30 28.60 62.80 54.70 57. 74.80 74.50 73.50 74.90 77.60 81.90 79.10 80.90 80.90 83.90 76.70 76.00 78.00 78.50 3.30 80.90 74.80 76.00 72.20 70.30 73.00 66.60 72.70 71.70 68.10 70.40 75.00 69.60 72.20 73.80 78.60 79.10 77. 77.10 74.40 73.90 82.60 83.10 79.90 84.00 81.30 79.90 79.50 84.60 81.70 2.90 81.90 74.00 78.20 79.50 77.40 70.10 67.40 71.50 69.40 69.20 70.10 79.80 71.00 61.00 79.10 76.30 81.10 77.00 84.80 78.20 86. 73.60 71.50 77.90 86.40 89.80 83.30 86.00 86.50 87.50 20.40 88.00 68.70 71.90 73.60 86.90 75.80 81.10 79.30 75.00 69.00 82.10 81.10 53.80 70.30 83.10 82.50 86.20 85.80 88.80 83.60 86.20 94.20 88.20 90.30 90.40 91.10 90.20 86.60 90.50 87.10 13.80 92.40 84.10 88.30 86.00 86. 79.50 76.30 85.10 88.80 84.20 86.50 85.6 76.90 75.6 89.80 86.20 76.80 76.30 73.80 76.30 71.40 78.80 76.50 59.90 75.70 71.80 61.00 30.40 68.70 57.60 0.00 77.20 42.40 70.30 45.00 76.30 54.10 44.20 72.40 69.20 45.20 64. 64.10 41.60 67.40 73.00 67.20 76.90 77.40 76.40 72.80 72.60 82.50 78.70 80.60 79.30 78.40 81.10 58.80 79.50 78.10 0.00 78.60 51.90 54.50 71.90 75.90 64.90 67.50 63.90 64.30 66.00 63.20 81.70 66.70 76.90 77.70 76. 76.90 75.40 67.40 65.80 68.00 71.00 72.30 74.90 74.10 74.80 73.20 67.30 73.40 70.30 6.30 71.20 65.70 62.60 63.50 66.70 59.20 62.40 65.30 59.20 61.70 65.60 67.00 54.00 64.70 67.60 71.80 80.20 77. 77.40 73.70 74.70 82.10 78.10 76.40 80.90 78.70 77.20 63.80 78.30 71.80 2.90 81.20 58.10 64.60 69.10 78.40 65.80 64.00 72.60 70.80 64.20 68.90 75.00 64.70 67.40 74.70 75.20 Model Method CSR_easy CSR_hard ISR_easy ISR_hard PSR_easy PSR_hard CSR ISR PSR Avg. Table 17: Performance on the CFBench dataset. DeepSeek R1-671B QwQ-32B DeepSeek-Qwen7B DeepSeek-Qwen7B DeepSeek-Qwen7B Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct LLaMA3.1-8B-Instruct LLaMA3.1-8B-Instruct LLaMA3.1-8B-Instruct LLaMA3.1-8B-Instruct LLaMA3.1-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct I/O I/O I/O SFT Ours I/O CoT SDC SFT Ours I/O CoT SDC SFT Ours I/O CoT SDC SFT Ours I/O DeepSeek-Qwen1.5B SFT DeepSeek-Qwen1.5B Ours DeepSeek-Qwen1.5B DeepScaleR-1.5B-Preview I/O DeepScaleR-1.5B-Preview SFT DeepScaleR-1.5B-Preview Ours Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct I/O CoT SDC SFT Ours 94.00 93.00 81.00 82.00 82. 86.00 85.00 86.00 85.00 86.00 79.00 70.00 81.00 80.00 5.00 74.00 75.00 80.00 76.00 80.00 52.00 46.00 63.00 55.00 49.00 62.00 63.00 42.00 53.00 76.00 77.00 86.00 84.00 71.00 70.00 70.00 76.00 73.00 77.00 76.00 75.00 69.00 62.00 72.00 68.00 4.00 85.00 63.00 69.00 66.00 68. 44.00 38.00 54.00 48.00 41.00 53.00 52.00 37.00 44.00 65.00 66.00 56.00 55.00 30.00 31.00 31.00 34.00 32.00 37.00 34.00 35.00 29.00 23.00 32.00 25.00 0.00 40.00 22.00 28.00 19.00 32.00 7.00 6.00 18.00 11.00 9.00 14. 11.00 6.00 8.00 19.00 25.00 86.00 86.00 64.00 66.00 66.00 72.00 69.00 72.00 70.00 73.00 63.00 51.00 63.00 59.00 0.00 50.00 55.00 64.00 53.00 67.00 26.00 25.00 40.00 27.00 26.00 42.00 37.00 19.00 31.00 50.00 59. 73.00 71.00 48.00 47.00 47.00 56.99 49.00 54.00 54.00 54.00 46.00 37.00 47.00 40.00 0.00 73.00 37.00 44.00 42.00 49.00 18.00 16.00 30.00 19.00 20.00 28.99 23.00 13.00 17.00 35.00 41.00 81.00 81. 55.00 57.99 58.00 66.00 61.00 66.00 62.00 66.00 55.00 44.00 56.00 51.00 0.00 33.00 47.00 56.00 47.00 59.00 20.00 20.00 33.00 21.00 22.00 35.00 28.99 14.00 26.00 42.00 52.00 43 90.00 69.00 80.00 79.66 89.00 68.00 78.00 78. 76.00 42.00 56.00 58.00 76.00 44.00 56.00 58.66 76.00 44.00 56.00 58.67 81.00 48.00 64.00 64.33 79.00 47.00 59.00 61.66 81.00 52.00 63.00 65.33 80.00 48.00 62.00 63.33 80.00 51.00 63.00 64.67 74.00 42.00 55.00 57.00 66.00 33.00 44.00 47.66 76.00 44.00 55.00 58.33 74.00 38.00 50.00 54.00 4.00 1.33 82.00 38.00 67.00 62.33 69.00 34.00 46.00 49.66 75.00 42.00 54.00 56.99 70.00 29.00 47.00 48.66 74.00 46.00 58.00 59.33 0.00 0.00 48.00 14.00 22.00 28.00 42.00 13.00 21.00 25.33 59.00 26.00 35.00 40.00 51.00 16.00 23.00 30.00 45.00 16.00 23.00 28.00 57.99 25.00 35.00 39.33 57.99 20.00 30.00 36.00 40.00 10.00 16.00 22.00 49.00 17.00 24.00 30.00 70.00 31.00 43.00 48.00 72.00 39.00 50.00 53."
        },
        {
            "title": "Response to an example from CELLO",
            "content": "以markdown表格格式输出思考并提出五个解决日常生活问题的创新或改进方法包括关键 词简介以及步骤 <think>好 的 用 户 让 我 以markdown表 格 格 式 输 出 思 考 并 提 出 五 个 解 决 日 常 生 活 问 题 的创新或改进方法包括关键词简介以及步骤首先我需要确定用户的需求是什么他 们可能是在寻找一些实用的解决方案或者是在进行项目规划需要结构化的输出 接下来我需要确定五个问题可能的领域包括家务健康时间管理环保或科技应用 每个问题需要有关键词简介和步骤关键词要简洁简介要简明扼要步骤要具体可行 然后我需要确保每个部分都符合markdown表格的格式表格的结构应该是三列问题关 键词简介和步骤每个问题需要三个部分所以总共需要十五行 接下来我需要考虑每个问题的创新点例如家务中的智能清洁机器人健康中的可穿戴 设备时间管理中的番茄工作法环保中的零废弃生活科技应用中的智能家居系统每个 问题的关键词要突出创新点简介要说明问题和解决方案步骤要详细说明如何实施 然后我需要检查每个部分是否符合要求关键词是否准确简介是否简洁步骤是否具 体例如智能清洁机器人的关键词是智能清洁简介是通过AI算法优化清洁路径步 骤包括购买安装设置和维护这样每个部分都要有明确的结构 最后确保整个表格的格式正确没有语法错误内容清晰可能需要调整某些步骤的顺 序或者增加一些细节比如智能清洁机器人的维护步骤或者零废弃生活的具体实践方 法 总 结 一 下 用 户 需 要 的 是 五 个 问 题 的 解 决 方 案 每 个 问 题 包 括 关 键 词 简 介 和 步 骤 以markdown表 格 格 式 输 出 需 要 确 保 结 构 清 晰 内 容 实 用 符 合 用 户 的 需 求</think><answer> 问题 关键词 简介 步骤 智能清洁机器人 智能清洁 通过AI算法优化清洁路径减少人工干预 1. 选择适合的智能清 洁机器人品牌如iRobot或Neato<br>2. 安装机器人并设置清洁区域<br>3. 定期更新地图和 清洁计划 可穿戴健康监测设备 健康监测 实时监测心率血压等生理指标预防疾病 1. 选择可穿戴 设备如Apple Watch或Fitbit<br>2. 连接设备到手机应用<br>3. 定期查看健康数据并调整生 活习惯 番茄工作法时间管理 时间管理 通过25分钟专注工作+5分钟休息的循环提高效率 1. 下载番 茄钟应用如Forest或Pomodoro Timer<br>2. 设置25分钟工作时间<br>3. 每完成一个番茄钟 后休息5分钟 零废弃生活 环保 减少一次性塑料使用实现资源循环利用 1. 准备可重复使用的购物袋 餐具和水杯<br>2. 分类回收垃圾参与社区回收活动<br>3. 购买本地和有机食品减少运输 碳足迹 智能家居系统 智能控制 通过语音或手机控制家中的灯光温度等 1. 选择智能音箱 如Amazon Echo或Google Home<br>2. 连接智能插座温控器等设备<br>3. 设置场景模 式如回家模式或离家模式 这 些 解 决 方 案 旨 在 通 过 创 新 技 术 或 方 法 解 决 日 常 生 活 中 的 常 见 问 题 提 高 效 率 健 康 和环保意识希望这些方法能为您的生活带来便利和改善</answer> Figure 26: The response of our optimized Qwen2.5-7B to random example from the CELLO."
        },
        {
            "title": "Response to an example from CFBench",
            "content": "作为一位在文艺评论界颇具影响力的杂志主编请你以一篇详细的剧评文章来展现你深邃的 洞察力在这篇文章中你需要紧扣娱乐圈的残酷现实通过具体事例来分析和探讨明星成 长经历的多样性与独特性特别是聚焦于肖战这位出道14年的流量小生他的成长轨迹如何 体现了明星从普通素人到顶流的艰辛转变以及在这个过程中遮羞布象征的意义是如何 被他一手扯下的请在你的文章中包含以下约束 1. 使用至少两个具体的行业内或个人事例来支撑你的观点和论述 2. 采用批判性和分析性的语气展现出你作为一位资深主编的专业视角和深度思考 3. 文章中必须包含对遮羞布这一象征性物品的解读以及它在肖战成长历程中的象征意 义 4. 请确保文章的总字数不少于800字以充分展现你的分析和论证深度 5.文章中不出现遮羞布这个词 6.自拟标题标题模仿以下例子中的句式 肖战的成长之路娱乐圈的真实与挑战 素人的崛起之旅流量的得与失 <think>好 的 用 户 让 我 写 一 篇 关 于 肖 战 的 剧 评 文 章 需 要 紧 扣 娱 乐 圈 的 残 酷 现 实 分析明星成长的多样性与独特性特别是聚焦肖战的14年成长轨迹以及遮羞布这一象 征性物品的解读首先我需要确定文章的结构和核心观点用户要求文章包含至少两个 具体事例批判性和分析性的语气不少于800字不出现遮羞布这个词标题要模仿给 定的句式比如肖战的成长之路娱乐圈的真实与挑战或素人的崛起之旅流量的得与 失首先我需要确定文章的标题用户提供的例子是肖战的成长之路娱乐圈的真实与 挑战和素人的崛起之旅流量的得与失我选择肖战的成长之路娱乐圈的真实与挑 战作为标题因为用户可能希望突出肖战的成长过程中的真实挑战接下来文章需要紧 扣娱乐圈的残酷现实通过具体事例来分析明星成长的多样性与独特性肖战作为流量小 生他的成长轨迹如何体现了从普通素人到顶流的艰辛转变以及遮羞布象征的意义用 户提到遮羞布被他一手扯下需要解读这个象征性物品在肖战成长历程中的意义需要两 个具体事例来支撑观点例如可以提到肖战早期的争议事件如阴阳合同事件以及他 如何通过努力和改变形象来应对这些挑战另一个例子可以是他在陈情令中的表现如 何从一个新人演员成长为顶流以及他如何处理粉丝和媒体的压力批判性和分析性的语气 需要贯穿全文避免过于正面或负面而是深入探讨明星成长的复杂性例如分析肖战如 何在争议中保持真实如何处理粉丝和媒体的期望以及他如何在商业成功和艺术追求之 间找到平衡还要注意不出现遮羞布这个词而是用其他词汇替代比如公众形象的保 护或公众形象的维护同时确保文章结构清晰有引言主体部分和结论每个部分都 有具体的例子和分析最后检查是否符合所有约束条件至少两个事例批判性语气不 少于800字标题符合要求不出现遮羞布这个词确保语言专业分析深入符合杂志主 编的视角</think><answer> 标题肖战的成长之路娱乐圈的真实与挑战 在娱乐圈这个充满变数与挑战的舞台肖战的14年成长历程如同一部生动的教科书深刻揭 示了明星从普通素人到顶流的艰辛转变本文将通过具体事例探讨肖战如何在公众形象 商业压力与艺术追求之间找到平衡以及遮羞布这一象征性物品在他成长历程中的多重意 义首先肖战的成长轨迹充分体现了娱乐圈的残酷现实2009年他以普通素人的身份参 加快乐男声凭借出色的唱功和真诚的表演获得关注然而成名后的他迅速卷入了娱 乐圈的漩涡2018年肖战因阴阳合同事件被媒体曝光一度陷入舆论漩涡这一事件不 仅考验了他的公众形象更让他深刻体会到娱乐圈的复杂性肖战通过积极面对争议主动 承担社会责任最终成功化解危机展现了明星在困境中成长的韧性其次肖战的商业成 功与艺术追求之间的平衡是他成长历程中的重要课题2019年肖战主演的古装剧陈情 令播出后迅速成为现象级作品他凭借精湛的演技和深情的演绎赢得了观众的广泛认 可然而随之而来的商业压力和粉丝期待也让他面临前所未有的挑战肖战在采访中曾 表示作为公众人物我必须时刻保持清醒不能被外界的声音左右这种自我约束和职 业素养正是他能够持续成长的关键遮羞布这一象征性物品在肖战的成长历程中扮演 了重要角色在早期他通过塑造阳光正面的形象成功掩盖了个人的不足和争议然 而随着事业的发展他逐渐意识到真正的成长需要面对真实的自我2021年肖战在 一次公开活动中主动分享了自己的成长经历坦诚面对过去的争议这一举动被外界视 为他扯下遮羞布的标志这种勇气和真诚不仅赢得了观众的尊重更让他在娱乐圈中 树立了独特的形象此外肖战的成长历程还体现了明星在商业成功与艺术追求之间的平 衡2022年他主演的现代剧余生请多指教再次引发热议但与以往不同的是他在 这部作品中尝试了更多元的角色展现了更丰富的艺术才华这种敢于突破自我勇于尝试 的精神正是他能够持续成长的重要动力综上所述肖战的成长之路不仅是一段个人奋斗 的历程更是一次对娱乐圈残酷现实的深刻反思他通过面对争议平衡商业与艺术主动 承担社会责任最终实现了从普通素人到顶流的转变而遮羞布这一象征性物品也从最 初的保护伞逐渐成为他成长道路上的绊脚石肖战的这段经历为年轻演员提供了宝贵的 启示真正的成长需要勇气面对真实的自我而不是被外界的标签所束缚本文通过具体 事例和深度分析展现了肖战在娱乐圈中的真实挑战与成长轨迹体现了明星成长的多样性 与独特性同时通过对遮羞布这一象征性物品的解读揭示了明星在公众形象商业压 力与艺术追求之间的平衡之道作为一位资深主编本文旨在通过批判性和分析性的视角 45 为读者提供更深入的思考和启示</answer> Figure 27: The response of our optimized Qwen2.5-7B to random example from the CFBench. Table 18: Performance on the ComplexBench dataset. Model Method And Chain_ 1 Chain_ 2 Chain_ avg Selection_1 Selection_2 Selection_ Selection_avg Selection_ and_ Chain_2 Selection_ and_ Chain_3 Selection_ and_ Chain_ avg Overall_ DRFR DeepSeek R1-671B QwQ-32B DeepSeek-Qwen7B DeepSeek-Qwen7B DeepSeek-Qwen7B Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct LLaMA3.1-8B-Instruct LLaMA3.1-8B-Instruct LLaMA3.1-8B-Instruct LLaMA3.1-8B-Instruct LLaMA3.1-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct I/O I/O I/O SFT Ours 92.39 84.91 90.84 82.25 78.50 61.24 73.61 57.98 73.61 57. 85.85 72.19 I/O CoT 86.01 67.75 SDC 89.61 70.71 87.40 71.01 SFT 86.57 73.96 Ours 81.89 68.05 I/O CoT 72.89 40.83 SDC 83.33 64.20 80.81 70.12 SFT 5.40 Ours 1.18 83.69 75.44 I/O 79.89 46.75 CoT SDC 83.85 62.43 76.31 56.25 SFT 81.69 67.16 Ours DeepSeek-Qwen1.5B I/O DeepSeek-Qwen1.5B SFT DeepSeek-Qwen1.5B Ours I/O DeepScaleR-1.5B-Preview DeepScaleR-1.5B-Preview SFT DeepScaleR-1.5B-Preview Ours 51.90 41.72 46.97 30.77 56.02 41.42 53.29 32.25 46.86 38.46 56.22 43.79 Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct 66.62 47.04 I/O CoT 47.84 22.19 SDC 53.81 35.21 73.77 60.36 SFT 78.96 63.91 Ours 82.64 80. 63.49 57.35 57.36 70.57 64.34 70.47 74.43 76.89 70.57 40.28 61.89 65.57 1.60 67.36 49.81 64.53 59.05 65.19 33.58 31.23 40.66 35.38 34.06 36.60 49.15 21.42 37.55 55.28 62.55 83.19 81.12 62.95 57.51 57. 70.96 65.16 70.53 73.61 76.18 69.96 40.41 62.45 66.67 1.50 69.31 49.07 64.02 58.31 65.67 35.55 31.12 40.84 34.62 35.12 38.34 48.64 21.60 36.98 56.51 62.88 87.55 86.27 68.67 61.80 61.80 77.25 71.24 70.39 67.81 73.39 56.22 64.81 63.52 57.94 1.28 66.09 62.66 66.52 45.00 69. 39.91 38.20 45.49 45.49 33.91 42.92 45.49 39.48 48.50 50.21 60.52 81.95 81.85 59.74 58.72 58.72 65.62 64.10 67.95 62.98 72.92 56.69 55.58 59.63 50.61 1.31 58.72 54.26 56.90 58.75 64.20 33.98 29.21 37.63 34.89 30.63 37. 37.42 26.88 33.77 43.51 54.26 86.04 86.04 61.89 47.92 47.92 63.40 61.51 68.68 53.21 60.75 52.08 49.06 59.62 44.91 0.37 46.42 49.06 47.17 77.78 61.51 20.75 18.87 31.70 32.83 19.25 28.30 29.43 19.25 26.04 37.36 37. 82.30 80.66 58.11 53.75 53.76 65.68 62.97 66.75 61.59 69.16 55.81 51.82 58.72 48.75 0.92 56.98 51.61 55.96 48.76 62.71 31.05 27.31 35.35 32.53 27.67 33.81 37.08 26.24 33.04 41.94 49.72 83.97 80. 38.93 46.56 46.56 65.65 59.54 66.41 63.36 61.07 64.89 47.33 53.44 39.69 0.00 56.49 45.04 54.96 25.00 63.36 38.93 31.30 32.06 25.95 25.95 35.88 38.93 23.66 34.35 35.88 45.04 76.18 69.12 50.59 41.17 41. 59.71 56.47 59.41 59.12 65.00 52.35 35.88 54.12 43.53 0.29 54.12 40.88 53.24 36.56 54.71 21.47 19.41 25.88 19.12 22.06 21.76 35.59 21.76 25.29 37.65 40.59 80.07 75.02 44.76 43.87 43.87 62.68 58.01 62.91 61.24 63.03 58.62 41.61 53.78 41.61 0.14 55.30 42.96 54.10 30.78 59. 30.20 25.35 28.97 22.54 24.01 28.82 37.26 22.71 29.82 36.76 42.81 86.24 84.52 66.87 62.03 62.04 74.48 72.00 76.14 74.23 77.40 69.11 56.54 68.74 65.24 2.71 70.04 61.32 68.32 67.20 70.45 39.89 35.53 44.38 40.70 36.68 43. 50.97 32.94 41.70 57.47 63.92 Response to an example from ComplexBench Wordsworths \"I Wondered Lonely as Cloud\" has another name, Growth of Poets Mind. 判 断 上 述 陈 述 的 对 错 如 果 正 确 则 先 回 答 关 键 词\"正 确\" 如 果 错 误 则 先 回 答 关 键 词\"错误\"然后陈述原因 <think>好 的 用 户 让 我 判 断Wordsworth的 Wondered Lonely as Cloud 是 否 又名Growth of Poets Mind首先我需要回忆这首诗的原名和可能的其他名称 根据我的知识Wordsworth的这首诗的原名是I Wandered Lonely as Cloud这首 诗是他的早期作品之一创作于1804年关于其他名称我需要确认是否有Growth of Poets Mind这个标题可能需要检查一些文学资料或权威的诗歌数据库来验证不过根 据常见的文学资料这首诗的正式标题是I Wandered Lonely as Cloud而Growth of Poets Mind可能是一个误传或误解需要进一步确认是否有这种标题或者是否有 其他版本的标题另外用户的问题是判断对错如果正确则回答正确如果错误则回 答错误所以需要先确定是否正确然后给出原因可能需要查找权威的文学资料来验证 标题是否正确</think><answer> 错误Wordsworth的I Wondered Lonely as Cloud的正式标题是I Wandered Lonely as Cloud我漫步在孤独如云朵而Growth of Poets Mind并不是这首诗的另一个名称 这首诗的标题在不同的版本和翻译中可能会有所不同但Growth of Poets Mind并不是其标 准标题正确的标题是I Wandered Lonely as Cloud因此该陈述是错误的</answer> Figure 28: The response of our optimized Qwen2.5-7B to random example from the ComplexBench. 46 Table 19: Performance on the FBBench dataset. Model Method error_correction_score response_maintenance_score overall_score DeepSeek R1-671B QwQ-32B DeepSeek-Qwen7B DeepSeek-Qwen7B DeepSeek-Qwen7B Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct LLaMA3.1-8B-Instruct LLaMA3.1-8B-Instruct LLaMA3.1-8B-Instruct LLaMA3.1-8B-Instruct LLaMA3.1-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct I/O I/O I/O SFT Ours I/O CoT SDC SFT Ours I/O CoT SDC SFT Ours I/O CoT SDC SFT Ours I/O DeepSeek-Qwen1.5B SFT DeepSeek-Qwen1.5B Ours DeepSeek-Qwen1.5B DeepScaleR-1.5B-Preview I/O DeepScaleR-1.5B-Preview SFT DeepScaleR-1.5B-Preview Ours Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct I/O CoT SDC SFT Ours 86.16 87.05 58.06 57.11 57.11 62.58 45.95 53.19 61.68 61.23 54.21 45.15 45.62 46.11 12.91 60.36 45.41 51.32 37.56 51.15 35.14 36.65 38.55 41.30 38.73 36. 45.31 42.81 36.77 42.99 45.37 81.17 79.48 61.14 62.19 62.19 56.01 39.35 50.25 55.84 67.68 39.63 19.53 31.10 34.21 1.37 48.72 32.93 44.79 36.97 57.55 33.88 38.53 37.01 39.18 32.71 39.27 34.31 31.81 36.26 42.51 71. 83.66 83.26 59.60 59.65 59.65 59.29 42.65 51.72 58.76 64.45 46.92 32.34 38.36 40.16 7.14 54.54 39.17 48.06 37.26 54.35 34.51 37.59 37.78 40.24 35.72 37.81 39.81 37.31 36.52 42.75 58.67 Model Table 20: Performance on the FollowBench dataset (English). EN_ hsr_ level3_ avg EN_ hsr_ level1_ avg EN_h sr_ level2_ avg EN_ hsr_ level4_ avg EN_ hsr_ level5_ avg EN_ ssr_ level1_ avg EN_ ssr_ level2_ avg EN_ csl_ avg Method EN_ ssr_ level3_ avg EN_ ssr_ level4_ avg EN_ ssr_ level5_ avg DeepSeek R1-671B QwQ-32B DeepSeek-Qwen7B DeepSeek-Qwen7B DeepSeek-Qwen7B Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct LLaMA3.1-8B-Instruct LLaMA3.1-8B-Instruct LLaMA3.1-8B-Instruct LLaMA3.1-8B-Instruct LLaMA3.1-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct I/O I/O I/O SFT Ours 4.24 4. 2.36 2.24 2.24 3.14 I/O CoT 3.14 SDC 3.36 3.10 SFT 2.89 Ours 3.20 I/O CoT 2.32 SDC 3.10 2.86 SFT 0.11 Ours 3.18 I/O CoT 2.26 SDC 2.80 2.12 SFT 3.16 Ours I/O DeepSeek-Qwen1.5B SFT DeepSeek-Qwen1.5B Ours DeepSeek-Qwen1.5B DeepScaleR-1.5B-Preview I/O DeepScaleR-1.5B-Preview SFT DeepScaleR-1.5B-Preview Ours 0.78 0.78 1.20 0.84 0.90 1.20 Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct I/O 1.52 0.98 CoT SDC 1.36 2.04 SFT 2.37 Ours 93.65 91.94 73.90 69.33 69.33 88.25 83.21 86.92 83.95 72.02 83.04 69.88 80.42 77.93 5.24 88.23 70.18 75.60 66.11 83.65 38.35 30.17 52.00 38.08 39.63 47.19 59.02 44.08 53.45 63.41 66.91 90.47 83. 67.93 62.26 62.26 79.44 80.53 79.52 73.35 71.35 72.00 66.95 79.75 69.05 (8.53) 78.35 61.79 72.55 60.70 71.532 33.41 18.62 44.10 36.20 29.77 44.92 46.37 41.97 47.10 62.32 64.40 85.37 78.64 50.54 56.67 56. 65.10 64.96 68.03 64.27 60.72 68.01 52.42 63.81 60.72 (12.97) 65.85 48.44 65.07 50.90 66.35 9.48 7.08 23.36 12.99 18.38 23.44 30.00 22.18 26.50 42.06 47.64 83.15 80.99 41.24 46.55 46.55 43.53 61.76 65.36 53.97 54.48 61.92 52.13 61.43 51.99 (9.64) 58.32 57.23 53.86 41.58 62. 10.45 (1.99) 18.01 12.87 11.61 17.04 17.45 13.27 24.64 40.51 44.75 93.65 91.94 73.90 69.33 69.33 88.25 83.21 86.92 83.95 72.02 83.04 69.88 80.42 77.93 5.24 88.23 70.18 75.60 66.11 83.65 38.35 30.17 52.00 38.08 39.63 47. 59.02 44.08 53.45 63.41 66.91 90.80 84.34 74.34 67.184 67.18 88.12 88.65 69.03 65.85 65.86 88.53 83. 66.77 68.61 68.62 72.22 76.09 82.73 73.06 76.79 83.87 76.23 80.16 82.29 74.51 77.32 77.26 68.44 73.98 73.46 74.62 76.56 75.22 64.93 67.71 71.71 72.34 76.22 82.58 69.68 71.19 72.88 (8.52) (10.94) (7.14) 73.77 71.82 80.92 63.22 68.36 66.73 72.58 70.04 76.32 66.19 64.14 64.18 74.522 78.504 73.804 41.75 24.64 51.91 44.31 38.02 52.87 55.11 51.63 55.78 68.66 69.42 39.71 26.66 42.02 43.39 34.30 40.51 53.26 41.50 49.17 60.30 63. 24.29 19.53 37.63 31.23 31.58 36.89 48.33 38.12 44.12 56.66 60.07 87.35 85.26 60.88 61.28 61.29 60.73 77.63 77.17 71.35 63.28 73.29 65.57 72.31 65.11 (7.54) 71.75 70.80 67.87 57.10 70.28 28.99 14.08 33.47 28.48 24.21 36. 41.48 34.94 45.32 56.67 57.64 86.79 85.54 60.44 56.48 56.49 67.26 70.51 74.69 68.09 71.06 70.97 60.15 70.18 63.20 (12.27) 62.30 58.36 60.41 54.27 75.16 27.37 15.66 29.02 32.28 21.56 27.48 37.42 25.27 31.40 47.03 54."
        },
        {
            "title": "Response to an example from FBBench",
            "content": "User: 还需健全库存联动监控制度包括设置多级价格订单自动转让功能和库存预警功能 此举有助于让供应商经销商批发商和零售商依据订单区域和订单大小进行系统性的分 工从而实现合作共赢这句话有问题吗 Assistant: 这句话没有语法错误或明显的逻辑问题它描述了一种改进的库存联动监控 制度该制度包含多级价格设置订单自动转让功能以及库存预警功能提出这样的制度能 够帮助供应链中的各方供应商经销商批发商和零售商根据订单的区域和大小进行更 有序的工作分配并最终实现共赢的局面整体来看这是一项有效的供应链管理策略 User: 你回答的不对这句话缺少主语 <think>好的用户指出我之前的回答中缺少主语需要重新分析这句话的结构 首先我需要确认原句是否真的缺少主语 原句是还需健全库存联动监控制度包括设置多级价格订单自动转让功能和库存预警功 能此举有助于让供应商经销商批发商和零售商依据订单区域和订单大小进行系统性的 分工从而实现合作共赢 原句的主语是此举即健全库存联动监控制度这一行为 这句话的结构是主语健全库存联动监控制度+ 谓语包括设置多级价格订单自动转 让功能和库存预警功能+ 从句此举有助于让供应商经销商批发商和零售商依据订单 区域和订单大小进行系统性的分工从而实现合作共赢用户可能认为此举作为主语不 够明确或者需要更具体的主语但原句中的此举已经指代了前面提到的健全库存联动监 控制度所以主语是存在的不过用户可能希望更明确地指出主语或者认为此举不够 直接需要确认是否需要添加主语比如这一制度或该制度来替代此举另外用户 可能希望我指出具体的主语位置或者确认主语是否正确我需要检查句子的主谓结构是否 正确以及是否符合中文的表达习惯原句的主语是健全库存联动监控制度谓语是包括 设置多级价格订单自动转让功能和库存预警功能从句的主语是此举即前面提到的制 度所以主语是存在的但用户可能希望更明确总结原句的主语是健全库存联动监控制 度即此举指代的制度用户可能认为需要更明确的主语但原句结构正确主语明确 需要确认用户的具体需求是否需要添加主语或者确认主语是否正确</think><answer> 您 指 出 的 主 语 问 题 确 实 存 在 原 句 中 的此 举作 为 主 语 可 能 不 够 明 确 容 易 引 起 理 解 上的混淆为了使句子结构更加清晰可以将主语明确化以下是修改后的版本 还需健全库存联动监控制度该库存制度包括设置多级价格订单自动转让功能和库 存预警功能此举有助于让供应商经销商批发商和零售商依据订单区域和订单大小进行 系统性的分工从而实现合作共赢 这 样 主 语库 存 制 度明 确 指 代 前 面 提 到 的 库 存 联 动 监 控 制 度 避 免 了 主 语 缺 失 的 问 题 同 时 句 子 结 构 更 加 清 晰 逻 辑 更 加 连 贯 希 望 这 个 修 改 能 更 好 地 满 足 您 的 需 求</answer> Figure 29: The response of our optimized Qwen2.5-7B to random example from the FBBench. 48 Response to an example from FollowBench-EN Identify one category from the list below for the input text, and also infer the sentiment (positive, neutral, or negative) conveyed in the text. Your options for the category are - company, educational institution, artist, athlete, office holder, mean of transportation, building, natural place, village, animal, plant, album, film, or written work. Michael DenDekker - Michael G. DenDekker (born July 11 1961) is an assemblyman for the state of New Yorks 34th district which includes the neighborhoods of Woodside, Jackson Heights, and East Elmhurst, all in the borough/county of Queens. Let me start by looking at the categories provided: Since Michael DenDekker is described as an assemblyman, The text is about Michael DenDekker, who is an assemblyman for <think>Okay, the user wants me to categorize the input text and infer the sentiment. New Yorks 34th district. company, educational institution, artist, athlete, office holder, mean of transportation, building, natural place, village, animal, plant, album, film, or written work. The category öffice holderfits best here. he holds political office. Hes not company, educational institution, artist, athlete, means of transportation, building, natural place, village, animal, plant, album, film, or written work. to infer the sentiment. his role and the districts he represents. or bias expressed. neutral. The description is purely about his political position, so öffice holderïs accurate. any positive or negative connotations.</think><answer> The sentiment is neutral because the text is informative without Theres no emotional language So the sentiment would be So öffice holderïs the correct category. Next, need The text is factual and provides information about Let me double-check the categories to ensure didnt miss anything. Its neutral statement. The category for the input text is öffice holderänd the sentiment conveyed is neutral. Michael DenDekker is described as an assemblyman for the state of New Yorks 34th district, which includes the neighborhoods of Woodside, Jackson Heights, and East Elmhurst in Queens. The text is factual and provides information about his role without any emotional language, making the sentiment neutral. Category: office holder Sentiment: neutral</answer> Figure 30: The response of our optimized Qwen2.5-7B to random example from the FollowBenchEN. ZH_ ssr_ level4_ avg ZH_ ssr_ level5_ avg 86.99 81.97 66.65 61.33 61.33 73.63 71.50 75.55 75.38 70.67 66.47 57.75 69.13 65.22 6.90 69.08 64.17 67.89 57.23 69.31 6.74 21.62 44.52 22.58 35.94 41. 40.99 35.07 42.36 67.22 56.17 84.98 78.62 64.80 53.20 53.20 59.37 73.13 69.84 72.35 66.71 61.59 52.06 69.24 61.52 7.66 66.93 62.58 67.69 51.80 66.30 1.86 25.75 43.46 9.89 29.83 38.83 34.19 21.63 40.22 55.77 61. Model Table 21: Performance on the FollowBench dataset (Chinese). ZH_ hsr_ level3_ avg ZH_ ssr_ level1_ avg ZH_ hsr_ level4_ avg ZH_ hsr_ level5_ avg ZH_ ssr_ level2_ avg ZH_ hsr_ level2_ avg ZH_ hsr_ level1_ avg ZH_ csl_ avg Method ZH_ ssr_ level3_ avg DeepSeek R1-671B QwQ-32B DeepSeek-Qwen7B DeepSeek-Qwen7B DeepSeek-Qwen7B Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct LLaMA3.1-8B-Instruct LLaMA3.1-8B-Instruct LLaMA3.1-8B-Instruct LLaMA3.1-8B-Instruct LLaMA3.1-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct I/O I/O I/O SFT Ours 3.96 3.76 2.18 2.02 2. 2.98 I/O CoT 2.64 SDC 2.98 2.98 SFT 2.89 Ours 2.42 I/O CoT 1.76 SDC 2.56 2.12 SFT 0.54 Ours 2.68 I/O 2.18 CoT SDC 2.50 1.72 SFT 2.96 Ours I/O DeepSeek-Qwen1.5B SFT DeepSeek-Qwen1.5B Ours DeepSeek-Qwen1.5B DeepScaleR-1.5B-Preview I/O DeepScaleR-1.5B-Preview SFT DeepScaleR-1.5B-Preview Ours 0.78 0.86 1.26 1.00 0.63 1.16 Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct 1.28 I/O CoT 0.70 SDC 1.04 2.04 SFT 2.25 Ours 89.50 89. 74.08 68.99 68.99 82.61 75.27 78.14 79.72 73.20 72.78 63.68 73.07 69.88 1.74 77.50 71.78 76.59 60.84 85.38 30.67 40.78 49.90 37.09 29.99 49.98 52.95 41.53 49.77 68.76 66.93 84.22 85.30 61.06 56.652 56. 71.34 72.42 71.99 71.32 68.01 69.32 57.71 68.78 56.38 2.45 69.95 59.36 70.82 53.54 67.25 14.75 23.46 50.43 31.33 31.09 50.45 39.53 34.12 33.16 57.29 58.67 84.22 82.22 54.93 54.48 54.49 67.28 67.97 67.69 68.15 69.42 59.91 55.34 63.95 52.81 (0.18) 58.39 57.40 62.21 42.68 71. 1.89 22.64 27.85 15.81 24.29 31.06 27.83 20.86 28.84 51.80 48.52 84.81 78.79 51.76 49.59 49.59 63.85 58.63 67.24 67.72 63.38 53.09 40.75 58.60 50.08 2.73 55.02 50.30 54.83 39.73 62.86 9.92 9.52 28.21 3.01 20.94 26. 20.07 16.67 20.58 51.03 41.33 82.85 73.93 49.07 38.53 38.54 48.20 61.28 58.43 60.25 57.80 45.50 36.48 57.56 46.96 2.69 53.58 43.78 49.13 32.19 54.64 15.01 10.85 24.27 6.83 17.20 21.93 11.42 3.90 16.23 33.22 44. 89.50 89.46 74.08 68.99 68.99 82.61 75.27 78.14 79.72 73.20 72.78 63.68 73.07 69.88 1.74 77.50 71.78 76.59 60.84 85.38 30.67 40.78 49.90 37.09 29.99 49.98 52.95 41.53 49.77 68.76 66.93 84.89 86. 65.06 62.07 62.08 74.55 76.19 75.43 73.98 70.54 74.08 65.92 72.67 63.37 3.88 73.94 66.36 75.27 63.17 70.91 23.26 29.47 57.50 37.99 40.12 55.34 47.64 45.09 42.12 62.72 63.31 85.56 83.99 66.95 65.18 65. 72.48 73.57 71.47 75.17 72.43 68.07 65.51 72.21 66.76 2.99 67.13 66.69 70.31 56.12 76.72 19.74 33.01 40.06 24.55 37.62 43.31 44.69 32.78 41.91 62.40 60.58 50 Response to an example from FollowBench-ZH 为以下文本选择一个类别并推断文本中传达的情绪积极中性或负面选项包括 公司教育机构艺术家运动员办公人员交通工具建筑物自然场所村庄 动 物 植 物 相 册 电 影 或 文 字 作 品 Michael DenDekker - Michael G. DenDekker 生 于1961年7月11日是纽约州第34选区的议员该选区包括皇后区/县内的Woodside Jackson Heights和East Elmhurst <think>好 的 用 户 让 我 为 这 段 文 本 选 择 一 个 类 别 并 推 断 其 中 传 达 的 情 绪 首 先 我 需 要 仔 细 阅 读 文 本 内 容 理 解 其 主 要 信 息 文 本 提 到 的 是Michael DenDekker 他 是 纽 约 州 第34选 区 的 议 员 选 区 包 括 皇 后 区/县 内 的WoodsideJackson Heights和East Elmhurst这表明他是一位政治人物具体是议员因此从选项中最合适的类别应该 是办公人员因为议员属于政府工作人员通常在办公室工作符合办公人员的定义 接下来我需要分析文本中传达的情绪文本只是简单地介绍Michael DenDekker的身份和 选区没有使用任何情感色彩的词汇比如积极的词汇如杰出成功或负面的词汇 如失败争议因此情绪应该是中性的因为文本只是客观地陈述事实没有表 达任何情感倾向总结一下类别应该是办公人员情绪是中性需要确保回答符合用户 的要求明确指出类别和情绪并且结构清晰</think><answer> 根 据 提 供 的 文 本 内 容 这 段 描 述 主 要 介 绍Michael DenDekker的 身 份 和 他 所 在 的 选 区 因此最合适的类别是办公人员文本中没有使用任何情感色彩的词汇只是客观地陈述事 实因此传达的情绪是中性总结如下 类别办公人员 情绪中性 这 段 文 本 主 要 描 述 了Michael DenDekker作 为 纽 约 州 第34选 区 议 员 的 身 份 以 及 他 所 代 表的选区范围属于典型的政务信息描述没有表现出积极或负面的情感倾向希望这能帮 助您完成任务</answer> Figure 31: The response of our optimized Qwen2.5-7B to random example from the FollowBenchZH. prompt_ level_ hard_true prompt_ level_ all_true Overall 61.69 61.29 36.69 37.09 37.10 43.55 37.10 45.56 40.32 55.24 43.55 36.29 42.74 35.89 0.40 38.31 35.89 42.34 30.24 35. 14.92 15.32 24.60 23.39 18.95 23.79 19.76 16.53 19.35 29.03 52.82 66.40 65.40 48.80 46.60 52.80 57.60 55.00 59.40 53.60 61.00 56.40 51.00 56.00 51.00 0.40 52.60 51.40 57.40 43.20 50.60 24.40 29.00 35.00 32.60 29.20 36. 37.80 30.60 37.20 44.60 57.80 90.18 89.69 79.64 71.64 82.00 85.60 82.13 85.38 84.31 81.33 85.20 77.91 83.47 82.04 2.67 84.00 79.73 84.09 76.62 72.00 52.00 51.96 60.49 60.31 54.22 60.09 71.24 62.22 67.56 76.22 81. Model DeepSeek R1-671B QwQ-32B DeepSeek-Qwen7B DeepSeek-Qwen7B DeepSeek-Qwen7B Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct LLaMA3.1-8B-Instruct LLaMA3.1-8B-Instruct LLaMA3.1-8B-Instruct LLaMA3.1-8B-Instruct LLaMA3.1-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct Ministral-8B-Instruct I/O I/O I/O SFT Ours I/O CoT SDC SFT Ours I/O CoT SDC SFT Ours I/O CoT SDC SFT Ours I/O DeepSeek-Qwen1.5B SFT DeepSeek-Qwen1.5B Ours DeepSeek-Qwen1.5B DeepScaleR-1.5B-Preview I/O DeepScaleR-1.5B-Preview SFT DeepScaleR-1.5B-Preview Ours Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct I/O CoT SDC SFT Ours Table 22: Performance on the InfoBench dataset. prompt_ instruction_ level_ level_ easy_true hard_true instruction_ level_ easy_true instruction_ level_ all_true Method 90.18 89.69 79.64 71.64 82.00 85.60 82.13 85.38 84.31 81.33 85.20 77.91 83.47 82.04 2.67 84.00 79.73 84.09 76.62 72.00 52.00 51.96 60.49 60.31 54.22 60. 71.24 62.22 67.56 76.22 81.96 71.03 69.44 60.71 55.95 68.25 71.43 72.62 73.02 66.67 66.67 69.05 65.48 69.05 65.87 0.40 66.67 66.67 72.22 55.95 65.07 33.73 42.46 45.24 41.67 39.29 48.81 55.56 44.44 54.76 59.92 62. 86.52 85.51 77.10 73.33 83.62 87.10 83.77 86.52 82.61 81.01 84.93 80.58 84.93 82.46 2.75 83.62 81.30 84.93 74.78 81.30 51.45 55.94 62.17 58.41 53.77 65.22 73.04 61.88 72.90 76.81 79.28 91.79 91. 80.77 70.89 81.28 84.94 81.41 84.87 85.06 81.47 85.32 76.73 82.82 81.86 2.63 84.17 79.04 83.72 77.44 67.88 52.24 50.19 59.74 61.15 54.42 57.82 70.45 62.37 65.19 75.96 83."
        },
        {
            "title": "Response to an example from InfoBench",
            "content": "Choose an appealing title for your post. The typical avocado is over 300 calories from the oil in it. Thats the amount of calories in large candy bar. If you get enough exercise to eat large candy bar every day without gaining weight, it wouldnt be problem to eat an avocado every day. Other wise you should probably eat them sparingly. It suggests Otherwise, they should eat <think>Okay, the user wants me to choose an appealing title for their post about avocados. The post mentions that typical avocado has over 300 calories from the oil, which is similar to large candy bar. that if you exercise enough to burn off the calories from candy bar daily, you can eat an avocado daily without weight gain. them sparingly. need to come up with catchy title that highlights the calorie content and the exercise aspect. Maybe something that makes people curious about the comparison between avocados and candy bars. engaging and make people want to read the post. Let me brainstorm some options: 1. Ävocado Calories vs. 2. The Surprising Truth: 3. 4. 5. think the third option, Ävocado vs. Without Gaining Weight?ïs the most appealing. to engage the reader, and highlights the calorie comparison. It also implies the exercise factor, which is key point in the post. Let me check if there are any better options, but this seems strong. Ill go with this one.</think><answer> Ävocado vs. Ävocado Calories: Ävocados Hidden Danger: Candy Bar: Can You Eat One Daily Without Gaining Weight? Avocados Have More Calories Than Candy Bar Candy Bar: Can You Eat One Daily How Exercise Affects Your Diet More Calories Than Candy Bar Candy Bars Secret Cousin Its direct, uses question Also, the title should be Candy Bar: \"Avocado vs. Candy Bar: Can You Eat One Daily Without Gaining Weight?\" This title is catchy and engaging, highlighting the calorie comparison between avocados and candy bars. It uses question to pique curiosity and implies the exercise factor, making readers want to know more about the relationship between calorie intake and daily exercise. Its concise and directly addresses the main points of the post, making it an appealing choice for title.</answer> Figure 32: The response of our optimized Qwen2.5-7B to random example from the InfoBench. 53 Contrast between Responses (English) with and without CoT <im_start>systemnYou are helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>. Now the user asks you to complete task. After thinking, when you finally reach conclusion, make sure all your final responses are enclosed between one <answer> tag and one </answer> tag.<im_end>n<im_start>usern Rewrite the following content in the style of psychology textbook, emphasizing the importance of cognitive-behavioral therapy (CBT) in treating anxiety disorders.n Anxiety disorders are group of mental health conditions characterized by excessive fear and worry. These disorders can significantly impact persons daily life, leading to avoidance behaviors and reduced quality of life. Cognitive-behavioral therapy (CBT) is widely recognized and effective treatment for anxiety disorders. CBT helps individuals identify and challenge negative thought patterns and develop healthier coping mechanisms. Studies have shown that CBT can lead to significant improvements in symptoms and long-term recovery. Additionally, CBT can be combined with medication for even better outcomesn<im_end>n<im_start>assistantn<think> In psychology textbook, the content would be structured with clear The importance definitions, explanations, and evidence-based support. of CBT in treating anxiety disorders would be highlighted through its mechanisms, efficacy, and potential for combination with other treatments. </think><answer>nAnxiety disorders represent diverse group of mental health conditions, characterized by persistent and excessive fear and worry that significantly impair an individuals daily functioning (American Psychiatric Association, 2013). These disorders can manifest in various forms, such as generalized anxiety disorder, panic disorder, and specific phobias, each with its unique set of symptoms and challenges (Beck, 2011). The impact of anxiety disorders extends beyond psychological distress, often leading to avoidance behaviors and diminished quality of life (Hofmann et al., 2010).nCognitive-behavioral therapy (CBT) stands out as widely recognized and empirically supported treatment approach for anxiety disorders (Butler et al., 2006). CBT is grounded in the cognitive model of emotion, which posits that thoughts, feelings, and behaviors are interconnected (Beck, 1976). In the context of anxiety disorders, CBT aims to identify and challenge maladaptive thought patterns, such as catastrophic thinking and overgeneralization, which contribute to the maintenance of anxiety symptoms (Beck, 1979). By fostering more balanced and realistic perspective, individuals can develop healthier coping mechanisms and reduce their reliance on avoidance behaviors (Beck, 1979).nNumerous studies have demonstrated the efficacy of CBT in treating anxiety disorders. For instance, meta-analysis by Hofmann et al. (2012) found that CBT was associated with significant improvements in anxiety symptoms compared to control conditions. Furthermore, the effects of CBT have been shown to be durable over time, with many individuals experiencing long-term recovery (Hofmann et al., 2012). The mechanisms underlying the effectiveness of CBT include the development of cognitive skills, behavioral experiments, and exposure techniques, which collectively contribute to symptom reduction and improved functioning (Butler et al., 2006).nIn cases where anxiety disorders are severe or persistent, CBT can be combined with pharmacotherapy for enhanced outcomes (Hofmann et al., 2012). This integrative approach leverages the strengths of both psychological and pharmacological interventions, potentially leading to more comprehensive symptom relief and improved quality of life (Hofmann et al., 2012). </answer> - <im_start>systemnYou are helpful assistant...{the same as above}...for even better outcomes.<im_end>n<im_start>assistantn<think>nn</think><answer> Anxiety disorders represent cluster of mental health conditions marked by an excessive and persistent fear or worry that significantly impairs an individuals daily functioning and quality of life. These disorders often manifest through avoidance behaviors, which can further exacerbate the individuals distress and limit their engagement in various life activities.nCognitive-behavioral therapy (CBT) is prominent and empirically supported therapeutic approach for the treatment of anxiety disorders. CBT is grounded in the cognitive-behavioral model, which posits that maladaptive thought patterns and behaviors contribute to the maintenance of anxiety symptoms. Through the application of CBT, individuals are guided to identify, challenge, and modify these negative thought patterns, thereby fostering the development of healthier cognitive and behavioral strategies.nResearch has consistently demonstrated the efficacy of CBT in alleviating symptoms of anxiety disorders. Studies have shown that CBT can lead to significant improvements in symptom severity and can contribute to long-term recovery, as evidenced by sustained symptom reduction and enhanced quality of life. Furthermore, the integration of CBT with pharmacological interventions has been shown to yield even more favorable outcomes, underscoring the complementary nature of these treatment modalities.nIn conclusion, CBT is cornerstone of evidence-based treatment for anxiety disorders, offering structured and effective approach to addressing the cognitive and behavioral aspects of these conditions. Its widespread adoption and empirical support highlight its importance in the therapeutic landscape for anxiety disorders.</answer> 54 Figure 33: The responses (English) of the Qwen2.5-7B-Instruct to training example with and without reasoning for superior CoT enforcement at the beginning. If all the responses with reasoning are judged worse than those with empty reasoning, the example will be discarded. Contrast between Responses (Chinese) with and without CoT <im_start>systemnYou are helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>. Now the user asks you to complete task. After thinking, when you finally reach conclusion, make sure all your final responses are enclosed between one <answer> tag and one </answer> tag.<im_end>n<im_start>usern日前应粉丝诉求张 艺兴工作室宣布重组请根据传播学理论知识以及你的观察和理解论述你对当前粉丝文 化的理解以及在多元利益主体参与下偶像与粉丝的关系发生了什么变化n避免使用单词 偶像. 你的回复中不应该出现逗号. 确保恰好有2个部分用markdown分隔符***分开各部 分<im_end>n<im_start>assistantn<think> 在分析粉丝文化及其变化时可以从传播学理论中的意见领袖理论和粉丝经济角度出 发意见领袖理论指出粉丝在特定领域具有影响力他们通过社交媒体等平台传播信息粉 丝经济则强调粉丝对明星的支持不仅限于消费行为还包括内容创作和传播在多元利益主体 参与下明星与粉丝的关系变得更加紧密和互动明星通过粉丝获取反馈调整作品粉丝则 通过参与明星的活动和内容创作获得满足感这种关系的变化体现在明星更加重视粉丝意 见粉丝也更积极地参与到明星的事业中</think> *** <answer> 传播学理论中的意见领袖理论和粉丝经济理论解释了当前粉丝文化的特点明 星 与 粉 丝 的 关 系 变 化 体 现 在 明 星 更 加 重 视 粉 丝 意 见 粉 丝 也 更 积 极 地 参 与 到 明 星 的 事 业 中</answer> - <im_start>systemnYou are helpful assistant....{the same as above}...用markdown分隔符***分 开各部分<im_end>n<im_start>assistantn<think>nn</think><answer> 粉丝文化在传播学理论中是一种互动关系它体现了粉丝与明星之间的紧密联系当前粉丝文 化中粉丝与明星的关系发生了变化首先粉丝不再只是被动的接受者而是成为了明星形象塑 造的重要参与者其次粉丝文化中粉丝与明星之间的界限变得模糊粉丝可以参与到明星的日 常生活中去再次粉丝文化中粉丝之间的互动也变得更加频繁和深入最后粉丝文化中粉丝 与明星之间的关系更加平等粉丝不再只是盲目崇拜明星而是能够与明星进行平等交流 *** 多元利益主体参与下粉丝与明星的关系变化体现在明星工作室开始重视粉丝的意见和需求 明星工作室会根据粉丝的诉求进行相应的调整如张艺兴工作室重组这表明明星与粉丝之间 的关系变得更加紧密和互动同时明星工作室也会利用粉丝的力量进行宣传推广等活动这 表明明星与粉丝之间的关系变得更加互利粉丝文化中粉丝与明星之间的关系变得更加平等 和互动粉丝不再只是被动的接受者而是成为了明星形象塑造的重要参与者粉丝与明星之 间的界限变得模糊粉丝可以参与到明星的日常生活中去粉丝与明星之间的关系变得更加平 等粉丝不再只是盲目崇拜明星而是能够与明星进行平等交流</answer> Figure 34: The responses (Chinese) of the Qwen2.5-7B-Instruct to training example with and without reasoning for superior CoT enforcement at the beginning. It can be observed that the shallow reasoning ignores the format constraints and therefore causes inferior final answer with respect to the answer with empty reasoning. If all the responses with reasoning are judged worse than those with empty reasoning, the example will be discarded. 55 (a) Response length. (b) Total length. (c) SFT loss. (d) KL penalty. (e) Format reward. (f) Accuracy reward. (g) Policy loss. (h) Return. Figure 35: Training dynamics across model families: Qwen2.5-7B, LLaMA3.1-8B, and Ministral-8B (best viewed magnified). 56 (a) Response length. (b) Total length. (c) SFT loss. (d) KL penalty. (e) Format reward. (f) Accuracy reward. (g) Policy loss. (h) Return. Figure 36: Training dynamics across model families: DeepSeek-Qwen1.5B, DeepscaleR-1.5B, and DeepSeek-Qwen7B (best viewed magnified). 57 (a) Response length. (b) Total length. (c) SFT loss. (d) KL penalty. (e) Format reward. (f) Accuracy reward. (g) Policy loss. (h) Return. Figure 37: Training dynamics on Qwen-2.5 models (1.5B/7B-Instruct) and ablation studies (best viewed magnified)."
        }
    ],
    "affiliations": [
        "Tencent YouTu Lab",
        "The Chinese University of Hong Kong",
        "Xiamen University"
    ]
}