{
    "paper_title": "AIM 2024 Sparse Neural Rendering Challenge: Dataset and Benchmark",
    "authors": [
        "Michal Nazarczuk",
        "Thomas Tanay",
        "Sibi Catley-Chandar",
        "Richard Shaw",
        "Radu Timofte",
        "Eduardo Pérez-Pellitero"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent developments in differentiable and neural rendering have made impressive breakthroughs in a variety of 2D and 3D tasks, e.g. novel view synthesis, 3D reconstruction. Typically, differentiable rendering relies on a dense viewpoint coverage of the scene, such that the geometry can be disambiguated from appearance observations alone. Several challenges arise when only a few input views are available, often referred to as sparse or few-shot neural rendering. As this is an underconstrained problem, most existing approaches introduce the use of regularisation, together with a diversity of learnt and hand-crafted priors. A recurring problem in sparse rendering literature is the lack of an homogeneous, up-to-date, dataset and evaluation protocol. While high-resolution datasets are standard in dense reconstruction literature, sparse rendering methods often evaluate with low-resolution images. Additionally, data splits are inconsistent across different manuscripts, and testing ground-truth images are often publicly available, which may lead to over-fitting. In this work, we propose the Sparse Rendering (SpaRe) dataset and benchmark. We introduce a new dataset that follows the setup of the DTU MVS dataset. The dataset is composed of 97 new scenes based on synthetic, high-quality assets. Each scene has up to 64 camera views and 7 lighting configurations, rendered at 1600x1200 resolution. We release a training split of 82 scenes to foster generalizable approaches, and provide an online evaluation platform for the validation and test sets, whose ground-truth images remain hidden. We propose two different sparse configurations (3 and 9 input images respectively). This provides a powerful and convenient tool for reproducible evaluation, and enable researchers easy access to a public leaderboard with the state-of-the-art performance scores. Available at: https://sparebenchmark.github.io/"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 2 ] . [ 1 1 4 0 5 1 . 9 0 4 2 : r AIM 2024 Sparse Neural Rendering Challenge: Dataset and Benchmark Michal Nazarczuk1, Thomas Tanay1, Sibi Catley-Chandar1, Richard Shaw1, Radu Timofte2, and Eduardo Pérez-Pellitero1 1 Huawei Noahs Ark Lab, London, United Kingdom {michal.nazarczuk1, thomas.tanay, sibi.catley.chandar, richard.shaw1, e.perez.pellitero}@huawei.com 2 University of Würzburg, Germany radu.timofte@uni-wuerzburg.de Abstract. Recent developments in differentiable and neural rendering have made impressive breakthroughs in variety of 2D and 3D tasks, e.g. novel view synthesis, 3D reconstruction. Typically, differentiable rendering relies on dense viewpoint coverage of the scene, such that the geometry can be disambiguated from appearance observations alone. Several challenges arise when only few input views are available, often referred to as sparse or few-shot neural rendering. As this is an underconstrained problem, most existing approaches introduce the use of regularisation, together with diversity of learnt and hand-crafted priors. recurring problem in sparse rendering literature is the lack of an homogeneous, up-to-date, dataset and evaluation protocol. While high-resolution datasets [3] are standard in dense reconstruction literature, sparse rendering methods often evaluate with low-resolution images. Additionally, data splits are inconsistent across different manuscripts, and testing ground-truth images are often publicly available, which may lead to overfitting. In this work, we propose the Sparse Rendering (SpaRe) dataset and benchmark. We introduce new dataset that follows the set-up of the DTU MVS dataset [2]. The dataset is composed of 97 new scenes based on synthetic, high-quality assets. Each scene has up to 64 camera views and 7 lighting configurations, rendered at 1600 1200 resolution. We release training split of 82 scenes to foster generalizable approaches, and provide an online evaluation platform for the validation and test sets, whose ground-truth images remain hidden. We propose two different sparse configurations (3 and 9 input images respectively). This provides powerful and convenient tool for reproducible evaluation, and enable researchers easy access to public leaderboard with the state-of-the-art performance scores. Available at https://sparebenchmark.github.io."
        },
        {
            "title": "Introduction",
            "content": "The recent advancements of neural rendering techniques have catalyzed significant progress in wide array of computer vision tasks, from novel view synthesis to 3D reconstruction. However, most research in this domain has predominantly 2 M. Nazarczuk et al. focused on dense-view input data, often overlooking the challenges and opportunities posed by sparse data configurations. To address this gap, we introduce SpaRe the Sparse Rendering dataset and benchmark novel dataset explicitly designed to benchmark and advance the state-of-the-art in sparse-view neural rendering. SpaRe was developed as the core dataset for the AIM 2024 Sparse Neural Rendering Challenge [29], which aimed to spur innovation in the field of few-shot neural rendering. Sparse-view rendering presents unique challenges, as it requires robust methods capable of generating high-fidelity reconstructions from minimal set of input views, often with little overlap between neighbouring images. While existing datasets have primarily catered to dense, well-sampled scenarios, SpaRe intentionally shifts the focus to scenarios where only sparse input data is available, simulating practical conditions in areas like autonomous driving, augmented reality, and resource-constrained environments. Current neural rendering datasets face several limitations that impact their effectiveness in training and evaluating models such as Neural Radiance Fields (NeRF) [28] for the task of sparse-view rendering. key limitation is often the quality of the ground-truth data, with many datasets, e.g. [3, 13, 27, 44, 48, 59], containing potentially imprecise camera poses typically estimated via COLMAP [34]. Inaccurate calibration can lead to errors in model training, which is especially detrimental in the sparse-view setting where precise camera poses are crucial. The NVS-RGBD dataset [44] introduced with SparseNeRF, contains real-world scenes with ground-truth depth maps captured using depth sensors, however, the resulting depths are often of lower resolution, inaccurate and noisy, while the camera poses are estimated using COLMAP, impeding reliable evaluation. SpaRe addresses these limitations with synthetically generated data with precise camera poses and accurately rendered high-resolution ground-truth images. Another limitation with using existing datasets for benchmarking the performance of sparse rendering methods is that they are usually either entirely real [3, 27] or entirely synthetic [6, 28]. Synthetic-only datasets might not generalize well to real-world scenarios, where lighting, reflections, and textures are more complex, whereas, as mentioned previously, real-only datasets often lack precise ground-truth information. Moreover, the difference between synthetic and real-world datasets can cause models trained on synthetic datasets to perform poorly when applied to real-world scenes. Therefore, the SpaRe benchmark contains both synthetically generated scenes and real-world scenes carefully selected from the DTU dataset [2]. The SpaRe synthetic scenes are specially designed to minimize the domain gap between synthetic and real data by closely reproducing the DTU capture setup (camera poses and light positions). Currently, the evaluation protocols for sparse-view rendering using the DTU dataset are inconsistent across the literature, often relying on outdated setups, such as evaluating performance at lower resolution. This approach can lead to inflated PSNR scores, as low-resolution novel view synthesis (NVS) is typically easier than high resolution, and may obscure the true computational challenges SpaRe Dataset and Benchmark 3 involved in rendering at higher resolutions. Additionally, different manuscripts use varying DTU versions with different test splits and image interpolation methods, further affecting PSNR. The pixelNeRF [56] protocol, which uses the same camera views for training and testing for all objects, risks overfitting of generalizable models. To address these issues, the SpaRe online benchmark platform standardizes evaluation by using full-resolution images and hidden groundtruth test set, ensuring reproducibility and reliability. The SpaRe dataset consists of 102 meticulously curated synthetic scenes based on high-quality photorealistic 3D assets, each rendered from sparse set of camera viewpoints for testing (3 or 9 input images), while dense view coverage is provided in the training set. Each scene has up to 64 camera views centred on single 3D object, with 16 point lights providing 7 different lighting configurations, rendered at 1600 1200 resolution. The dataset includes diverse range of scenes varying in complexity, materials, textures and occlusions, providing comprehensive testbed for evaluating the performance of sparse neural rendering algorithms, examples of which are shown in Fig. 3. This paper details the construction of the SpaRe dataset, including its design principles, data generation processes, evaluation protocol, and the associated challenge tasks. We further discuss the performance of top-performing methods from the AIM 2024 Sparse Neural Rendering Challenge, providing insights into current capabilities and areas ripe for future research in sparse neural rendering. Through SpaRe, we aim to provide the research community with valuable resource that drives the development of more efficient and effective neural rendering methods in sparse data environments."
        },
        {
            "title": "2.1 Datasets",
            "content": "A number of datasets have been used in the context of sparse novel view synthesis before. We briefly review them here in chronological order. The DTU dataset [2] was originally introduced for multiview stereo evaluation. It consists of 124 scenes/objects placed on table and photographed 49 or 64 times under 7 different illuminations, using camera placed on an industrial robot arm encaged in black box. This dataset has been used extensively for evaluating sparse neural rendering methods [810, 14, 17, 18, 20, 23,30,35,40,4244,49,50,52,5557], following various protocols introduced in PixelNeRF [56], RegNeRF [30] or MVSNeRF [8]. We discuss the limitations of these protocols in Section 2.3. The RealEstate10K dataset [59] was introduced with generalisable multiplane image model for stereo magnification. It consists of large number of camera poses corresponding to 10 million frames derived from about 80,000 video clips, gathered from about 10,000 YouTube videos. Using it requires to extract the corresponding video frames from YouTube. This is large but not very diverse dataset, and the quality of the images is relatively low. 4 M. Nazarczuk et al. The Spaces [15] was introduced with DeepView, another generalisable multiplane image model. It consists of 100 indoor and outdoor scenes captured 5 to 10 times each using 16-camera rig translated by small amounts. ShapeNet [6] is large-scale repository of simple shapes represented by 3D CAD models of objects. It has been used for the evaluation of some early sparse neural rendering approaches [38, 56]. The LLFF dataset [27] was introduced with the generalisable multiplane image model of the same name. It consists of 8 forward facing scenes captured with phone and camera poses were estimated using COLMAP [34]. This dataset has also been used extensively for the evaluation of sparse neural rendering approaches [8, 14, 16, 30, 35, 42, 44, 49]. The Blender dataset [28], introduced with NeRF [28], consists of 8 synthetic scenes viewed from half-sphere. The IBRNet dataset [45] contains 67 forward-facing scenes captured in setup similar to LLFF. These additional scenes allow the training of generalisable models. The Shiny dataset [48] was introduced with the NeX multiplane image model. It consists of 8 scenes containing diverse reflective surfaces, and was collected with focus on evaluating complex non-Lambertian effects. The Common Objects in 3D dataset (CO3D) [32] is large dataset of real object-centric scenes. It contains total of 1.5 million frames from nearly 19,000 videos capturing objects from 50 MS-COCO categories. The Mip-NeRF360 dataset [3] consists of 9 scenes with 5 outdoors and 4 indoors, each containing complex central object or area with detailed background. The Objaverse dataset [13] and its follow-up XL version [12] is very large dataset of 10M+ 3D Objects. It has recently enabled rapid progress in the development of large object-centric 3D generative models [16, 25, 26, 37]."
        },
        {
            "title": "2.2 Sparse View Rendering",
            "content": "In the sparse regime, novel view synthesis is highly under-constrained: small number of observations can in principle be explained by infinitely many underlying 5D radiance fields. In recent years, large number of methods have focused on this sparse regime; we present an overview in Table 1. These methods can all be interpreted as enforcing specific scene prior (or set of scene priors) to help regularize the problem, and they can be classified into two main categories: scene-specific and generalisable approaches. Generalisable approaches are well suited for the sparse regime by design, because they learn scene prior during training by being exposed to large number of scenes, and because they typically rely on small number of input views (2 to 16). They can be divided into 4 main categories: methods that predict multiplane image representation [15, 27, 59], methods that predict radiance field representation [8, 10, 18, 45, 52, 56], methods that predict representation with 3D Gaussians [7,9,47] and methods that directly predict rendered pixel colors [11,4042]. For all these methods, multi-view consistency is typically enforced SpaRe Dataset and Benchmark 5 Table 1: Overview of sparse neural rendering methods, sorted chronologically. NeRF 3DGS Method Stereo Mag. [59] LLFF [27] DeepView [15] SRN [38] PixelNeRF [56] IBRNet [45] SRF [10] MVSNeRF [8] DietNeRF [17] RegNeRF [30] InfoNeRF [20] DDP [33] DS-NeRF [14] GeoNeRF [18] GPNR [40] GNT [41] FreeNeRF [55] SPARF [43] DiffusioNeRF [50] GeCoNeRF [22] FlipNeRF [35] SparseNeRF [44] GNT-MOVE [11] DaRF [39] CombiNeRF [5] ConvGLR [42] pixelSplat [7] MuRF [52] DNGaussian [23] ZeroRF [36] ReVoRF [53] Mi-MLP [60] ReconFusion [49] MVSplat [9] latentSplat [47] CoherentGS [31] FSGS [61] CoR-GS [57] CAT3D [16] Prior Learned (generalisable) Learned (generalisable) Learned (generalisable) Learned (generalisable) Learned (generalisable) Learned (generalisable) Learned (generalisable) Learned (generalisable) CLIP-based semantic consistency loss Entropy constraint on rays SfM pointcloud with depth completion SfM pointcloud Learned (generalisable) Learned (generalisable) Learned (generalisable) Frequency regularization, near penalty MV consistency, depth consistency Loss from diffusion model on RGBD patches Depth-based pseudo-GT at feature level Flipped rays, near penalty, MV consistency Depth smoothness, mono-depth supervision Learned (generalisable) Mono-depth supervision Depth smoothness, normalizing flow likelihood Depth smoothness, near penalty, Lipschitz reg. Learned (generalisable) Learned (generalisable) Learned (generalisable) Mono-depth supervision TensoRF with randomly-initialized generator Depth-based pseudo-GT, depth smoothing Input rerouting, background/sampling reg. CLIP/PixelNeRF/image diffusion supervision Learned (generalisable) Learned (generalisable) Depth smoothness, MV consistency Gaussian unpooling from SfM point cloud Disagreement penalty across 3DGS models Multi-view diffusion supervision 6 M. Nazarczuk et al. implicitly through epipolar constraints [7, 10, 11, 40, 41, 45, 47, 56], or through the construction of cost volumes [8, 9, 18] or plane sweep volumes [15, 27, 42, 52, 59]. Scene-specific approaches, on the other hand, are not particularly well suited for the sparse regime: vanilla NeRF [28] and 3DGS [19] both produce degenerate outputs when trained with few images. In order to produce realistic renderings, these approaches require the addition of explicit scene priors. One common approach consists in regularizing training or target views in some way. For instance, some methods regularize the appearance of unseen patches with the help of an auxiliary network, such as CLIP model [17], normalizing flow model [30] or diffusion model [49, 50]. Some methods regularize the geometry of training or target patches, by enforcing depth smoothness [5,30,31,44,53] or multi-view consistency [22, 31, 35, 43]. Since 3D scenes typically consist of mostly empty space, it is also possible to enforce an emptiness prior [20], especially near the camera [5, 35, 55, 60], or where uncertainty is high [57]. Another common approach is to augment the training data in some way. For instance, synthetic rays can be generated by flipping training rays [35] or pseudo ground-truth images can be generated by reprojecting training images using predicted depth maps [53]. Additional training signal can be extracted from SfM pointclouds [14, 33, 61] or distilled from depth maps predicted by pre-trained monocular depth estimators [23,39,44]. Recently, state-of-the-art results have been obtained by generating pseudo ground-truth images with multi-view diffusion model [16]. Finally, set of methods report significant improvements in the sparse regime through optimization or architectural changes. This includes masking the high-frequency components of the inputs [55], rerouting the inputs at different levels of the network [60], parameterizing TensoRF with randomly-initialized generator [36] or applying Lipschitz regularization [5]."
        },
        {
            "title": "2.3 Current Protocol Overview",
            "content": "The DTU dataset has become cornerstone of sparse neural rendering evaluation and benchmarking: it is the dataset that is most commonly used among all the methods listed in Table 1 [9,10,14,17,18,20,23,30,35,40,4244,49,50,52,5557]. Unfortunately, existing protocols are somewhat outdated and prone to error. Firstly, there exists multiple versions of the dataset available online, each preprocessed in different way. The PixelNeRF [56] protocol uses version that has been downsampled 4 using an interpolation method producing blurry images. The MVSNeRF [8] protocol uses version that has been downsampled 2 and center-cropped. The RegNeRF [30] protocol uses the original version of the dataset and downsamples the images 4 on the fly using bilinear interpolation (in an earlier version of the codebase) or area interpolotation (in the current version of the codebase). The comparison between the images used in the evaluation is shown in Figure 1. These differences (in resolution, interpolation method and cropping), result in differences during evaluation in the metric scores, e.g. PSNR, SSIM or LPIPS, which makes comparisons across methods unreliable. To avoid these issues, we believe that the quality performance should be evaluated against the original images, hence, in full resolution and without cropping. SpaRe Dataset and Benchmark 7 Moreover, with recent developments in novel view synthesis, including the emergence and advances of Gaussian Splatting [19], the cost of training and evaluating in high resolution was drastically reduced. Furthermore, we note that the evaluation protocols used by pixelNeRF [56] and RegNeRF [30] evaluate with the same selected training views and the same selected test views across all the objects. We believe that with the development of generalisable solutions [7,9,40,42,45,56,59], this leads to unintended, and undetectable overfitting. PixelNeRF MVSNeRF Original Fig. 1: Three versions of the same image extracted from different dataset processing pipelines."
        },
        {
            "title": "3 Dataset",
            "content": "SpaRe extends the DTU dataset [2]. We believe that robust and fair evaluation protocol requires larger sample of objects and images, including subset where the test views are not publicly released. To this end, we replicate DTU-alike setup in synthetic Blender [4] environment."
        },
        {
            "title": "3.1 Scene Composition",
            "content": "In an effort to replicate the setup of DTU, we place all objects on white platform and place them in black box. We manually collected 102 assets from BlenderKit online library [1]. All the collected models are high-quality 3D assets belonging to range of categories, including toys, cars and houses miniatures, home appliances and equipment, technology items, tools, sports equipment, plants, decorations etc. breakdown of categories seen in SpaRe is presented in Figure 2. The scenes span large range of objects sized between 8cm and 50cm. We include items varying in texture and specularity (e.g. plushy elephant, glass lamp, or shiny plastic childrens toy). Selected examples from our datasets are presented in Figure 3."
        },
        {
            "title": "3.2 Generation",
            "content": "All images in the SpaRe dataset are rendered with Blender, using high-quality rendering engine Cycles included in Blender. We render all images in 1600x1200 8 M. Nazarczuk et al. Fig. 2: Composition of high-level categories of objects in SpaRe. resolution. We set the virtual camera to mimic the camera used in DTU capture. To this end, we use the same focal length and the same sensor size. In our data, we eliminate the principal point shift, introduced in DTU during the process of point cloud reconstruction and image undistortion. Finally, all images are rendered together with the segmentation map used to extract object masks which are used in our evaluation protocol."
        },
        {
            "title": "3.3 Camera Positioning",
            "content": "DTU collects the data with setup that included table and robotic arm with camera mounted on top. The camera poses in DTU are provided with respect to an arbitrary coordinate system. We exactly mimic the relative camera arrangement of DTU. However, to unify and simplify the poses, we place all the objects with the middle of their bottom side in (0, 0, 0). Therefore, our dataset consists of 64 camera views, 49 placed on smaller sphere, and 15 on larger, concentric sphere. Furthermore, number of cameras in DTU capture remain unused due to shadow cast from the robotic arm, whereas SpaRe by nature of synthetic generation does not have this limitation."
        },
        {
            "title": "3.4 Lighting Setup",
            "content": "We recreate the lighting setup used by DTU. We use 16 point lights mimicking LED sources. Following DTU, for each object we provide 7 different captures corresponding to varying lighting conditions. This relates to the object being illuminated from either side, with one capture including all lights on. An example of an object rendered under varying illumination is shown in Figure 4. SpaRe Dataset and Benchmark 9 Fig. 3: Examples of objects included in the SpaRe dataset. We provide objects from diverse categories with scene placement similar to that of the DTU dataset. 10 M. Nazarczuk et al. Fig. 4: Varying illumination in the SpaRe dataset capture for three different scenes."
        },
        {
            "title": "4 Evaluation Protocol",
            "content": "SpaRe includes benchmark for novel view synthesis under sparse input image set-up. In this section, we propose and describe an evaluation protocol which we believe provides fair comparison of sparse neural rendering methods."
        },
        {
            "title": "4.1 Data Splits",
            "content": "We propose benchmark that includes two tracks corresponding to common setups of input views. Namely: Track 1 - 3 input views, Track 2 - 9 input views. We propose to split the SpaRe data as follows: 82 training scenes (for generalisable approaches), 6 validation scenes, 9 test scenes (including 4 scenes common to 3 and 9 views benchmarks, and 5 unique scenes each). Note that the test split comprises some scenes shared between the two tracks and some unique to each track. This allows for comparisons between 3 and 9 input setups while preserving the ability to detect cross-contamination between input views in Track 1 and 2. 4."
        },
        {
            "title": "Input Views",
            "content": "Motivated by the shortcomings of current uses of DTU dataset in sparse view rendering evaluations as described in Sec. 2.3, we propose to evaluate SpaRe in full resolution. Further, we randomly select 3 or 9 camera views to serve as the input for evaluation on validation and test split. Additionally, we curate the views, especially in the 3-view scenario to include variety of camera distances. An example of input camera selection can be seen in Fig. 5. Similarly, we select 10 random views from non-input camera poses for testing. We believe this allows for detailed evaluation while keeping reasonable rendering time. SpaRe Dataset and Benchmark 11 Fig. 5: diagram marking the selection of input views among 49 available cameras. Input views are presented for the following scenes left to right: Rings, Recorder, Jenga. Selected input views are marked in blue."
        },
        {
            "title": "4.3 Sparse Neural Rendering Challenge",
            "content": "The SpaRe dataset was used in the AIM 2024 Sparse Neural Rendering Challenge. The participants were asked to develop approaches for novel view synthesis under sparse input constraints. The data used in the challenge consisted in part of the synthetic data in splits as aforementioned, and in part of DTU data. SpaRe data used in the challenge was evaluated as described in Sec. 4.1 and Sec. 4.2. DTU data was prepared in similar manner to ensure fair benchmark. To this end, we used previously proposed [56] 15 evaluation scenes and used 6 of them in the validation phase, and 9 in the testing phase. For each of those scenes, random selection of 3/9 views was used as the inputs, and random 10 from the remaining ones was used as the target (having previously discarded views where the robotic arm casts shadow onto the object)."
        },
        {
            "title": "4.4 Benchmark Platform",
            "content": "With SpaRe, we do not release the test views publicly, instead, we provide researchers with the platform for results evaluation. The benchmark can be accessed at https://sparebenchmark.github.io. We use Codabench [54] as the evaluation platform. For each track and split, the users can submit the rendered views which are then evaluated against the ground truth. In an effort to allow evaluation on SpaRe and preserve the AIM 2024 Sparse Neural Rendering Challenge continuity, we enable users to upload either SpaRe data results, DTU results, or exact challenge setup (both datasets) results. The benchmark automatically returns the corresponding scores. Given the submitted results, SpaRe returns the following metrics: Masked PSNR (PSNR-M) - PSNR calculated with the object mask, PSNR - calculated in the whole image, including background reconstruction, SSIM-M - SSIM [46] calculated withing the tight bounding box around the object, SSIM - calculated in the whole image, M. Nazarczuk et al. LPIPS-M - LPIPS [58] calculated withing the tight bounding box around the object, using AlexNet [21] as backbone, LPIPS - calculated in the whole image. The user can access detailed report for the submission that includes the aforementioned scores for every evaluated image as well as the average for each scene."
        },
        {
            "title": "5 Baseline Experiments",
            "content": "We perform baseline experiments in the SpaRe benchmark setting we propose. We select two methods that optimise the underlying representation for each scene separately, namely RegNeRF [30] and FreeNeRF [55]. Further, we conduct an experiment with generalisable method that utilises pretaining step, i.e. ConvGLR [42]. Finally, for each Track 1 and Track 2, we include the results of the AIM 2024 Sparse Neural Rendering Challenge [29] for the test set. The results of the experiment for Track 1 are presented in Table 2, and for Track 2 - in Table 3. Table 2: Quantitative results of baseline methods on Track 1 - 3 views on validation and test splits of SpaRe and DTU. winner, and runner-up of the AIM 2024 Sparse Neural Rendering Challenge Track 1. Method PSNR-M PSNR SSIM-M LPIPS-M Avg DTU Syn Avg DTU Syn Avg DTU Syn Avg DTU Syn RegNeRF [30] FreeNeRF [55] ConvGLR [42] 18.17 19.08 17.27 16.64 14.87 18.41 0.591 0.580 0.601 0.590 0.587 0.592 17.55 18.53 16.57 17.17 15.48 18.85 0.585 0.568 0.601 0.535 0.559 0.510 20.81 21.86 19.75 23.32 21.80 24.83 0.672 0.667 0.676 0.549 0.564 0.534 Validation 17.47 17.76 17.17 16.24 15.84 16.64 0.629 0.553 0.704 0.533 0.562 0.504 RegNeRF [30] 17.93 16.61 19.26 17.62 15.93 19.30 0.633 0.542 0.723 0.462 0.553 0.371 FreeNeRF [55] ConvGLR [42] 20.11 20.36 19.87 22.60 20.87 24.33 0.710 0.644 0.776 0.469 0.534 0.405 FrameNeRF [51] 18.67 18.50 18.83 17.98 16.73 19.23 0.665 0.591 0.740 0.395 0.420 0.369 SCNeRF [24] 18.30 18.16 18.43 18.18 17.00 19.36 0.654 0.584 0.725 0.515 0.584 0. Test In the quantitative comparison, we report the results as masked PSNR (PSNRM), PSNR calculated in the whole image, and SSIM and LPIPS calculated within tight bounding box around the object (SSIM-M and LPIPS-M)."
        },
        {
            "title": "5.1 Track 1",
            "content": "The quantitative results for Track 1 (3 input views), for both validation and test splits of SpaRe benchmark (including SpaRe and DTU datasets) are presented SpaRe Dataset and Benchmark 13 Table 3: Quantitative results of baseline methods on Track 2 - 9 views on validation and test splits of SpaRe and DTU. winner, and runner-up of AIM 2024 Sparse Neural Rendering Challenge - Track 2. Method PSNR-M PSNR SSIM-M LPIPS-M Avg DTU Syn Avg DTU Syn Avg DTU Syn Avg DTU Syn RegNeRF [30] FreeNeRF [55] ConvGLR [42] 23.74 25.50 21.98 22.51 22.16 22.87 0.697 0.694 0.701 0.477 0.504 0.450 25.15 27.03 23.26 25.15 25.48 24.82 0.746 0.736 0.757 0.328 0.386 0.271 23.42 25.33 21.51 26.61 25.81 27.40 0.712 0.709 0.715 0.479 0.505 0. Validation 23.22 23.24 23.21 21.29 20.95 21.62 0.723 0.688 0.758 0.415 0.451 0.379 RegNeRF [30] 24.27 24.24 24.30 23.51 23.20 23.83 0.759 0.731 0.786 0.293 0.336 0.251 FreeNeRF [55] ConvGLR [42] 22.62 22.87 22.37 24.59 23.80 25.37 0.731 0.697 0.765 0.419 0.459 0.379 FrameNeRF [51] 24.51 24.56 24.46 23.87 23.79 23.94 0.784 0.759 0.808 0.262 0.267 0.257 Thirteen 21.59 20.14 23.04 21.45 19.73 23.16 0.649 0.549 0.749 0.516 0.628 0.403 Test in Table 2. Figure 6 shows the visualisation of the baseline models evaluated on the SpaRe synthetic dataset. Note that ground truth images are withheld to preserve the integrity of the benchmark (the evaluation platform with detailed results is publicly available). Similarly, Figure 7 presents visualisations of novel views on DTU data in the setting proposed by SpaRe benchmark. We observe the superior performance of ConvGLR in 3 input views scenario across the majority of provided metrics (PSNR-M, PSNR, SSIM-M). Given single scene, the task of reconstructing 3D representation is highly underconstrained. We believe that in the very sparse scenario, the method pretrained on large dataset is able to learn stronger geometry priors than models that optimise the representation on per-scene basis. Notably, we observe the largest performance gap in PSNR calculated on the whole image, namely, 4.42dB higher than the runner-up. This can be attributed to ConvGLR having seen the background throughout the training data, and being able to reconstruct its content in the test set. Interestingly, we see that the winner of the AIM 2024 Sparse Neural Rendering Challenge performing better in one of the perceptual metrics (LPIPS-M) than ConvGLR. In the qualitative examples in Figure 6 we observe corresponding relations. Across all the images, we can see that ConvGLR is the only method that reconstructs both the object and the background without artefacts. In contrast, per-scene optimisation methods include artefacts as result of 3 views not constraining the geometry enough, e.g. see the tip of the Hanoi Tower toy. We observe similar behaviour on the DTU dataset shown in Figure 7. Note Papa Smurfs head artefact for FreeNeRF, or blurred object silhouettes in the grocery scene. On the other hand, we observed good performance of FreeNeRF in perceptual metric - LPIPS-M. This can be observed in e.g. the sharp texture of the cactus from the SpaRe dataset  (Fig. 6)  . M. Nazarczuk et al. Fig. 6: Test set results on the SpaRe synthetic dataset for Track 1. Ground truth images are omitted to preserve benchmark integrity. Fig. 7: Test set results on the DTU dataset for Track 1. SpaRe Dataset and Benchmark 15 In summary, we observe that in the scenario where only 3 input views are available, the generalisable method provides very strong object priors capable of regularising the underlying geometry, whereas per-scene optimisation may recover some higher frequency details suffering, however, from artefact induced by sparse supervision."
        },
        {
            "title": "5.2 Track 2",
            "content": "The quantitative results for Track 2 (9 input views), for both validation and test splits of SpaRe benchmark (including SpaRe and DTU datasets) are presented in Table 3. Figure 8 shows the visualisation of the baseline models evaluated on the SpaRe synthetic dataset. Figure 9 presents visualisations of novel views on DTU data in the setting proposed by SpaRe benchmark. Fig. 8: Test set results on the synthetic dataset for Track 2. Ground truth images are omitted to preserve benchmark integrity. In Track 2, we observe results contrasting to those of Track 1. We notice that the methods optimising the representation separately for each scene perform better than the generalisable one (see PSNR-M for FrameNeRF and FreeNeRF 24.51dB and 24.27dB in contrast to ConvGLR 22.62dB). This leads us to believe that current per-scene optimisation methods can utilise priors from 9 input views to better extent than generalisable prior obtained in pertaining. However, we still observe performance gap in favour of ConvGLR when PSNR is 16 M. Nazarczuk et al. Fig. 9: Test set results on the DTU dataset for Track 2. calculated over the whole image. This affirms the belief that generalisable methods having seen significantly more examples of the background can reconstruct it better. In Figure 8 we present qualitative results of the Track 2 test set from SpaRe data. We observe similarities to the corresponding results in Track 1. RegNeRF and FreeNeRF seem to provide sharper high-frequency details, e.g. plush texture of the elephant, the writing on the snow truck, or the wood boards on the house model. Similarly, ConvGLR provides smoother results around the edges and exhibits fewer artefacts. However, in Track 2 the artefacts of per-scene optimisation models are much smaller than in Track 1 which is reflected in high scores in performance metrics for these algorithms. similar visual comparison of novel views in DTU dataset is shown in Figure 9. Here, the observations are similar to SpaRe dataset. We notice smoother reconstruction around the edges for ConvGLR, whereas FreeNeRF and RegNeRF seem to provide sharper highfrequency details, e.g. barcode on the wooden board, or bunny statue texture."
        },
        {
            "title": "5.3 Ablation Study",
            "content": "In this section we experiment with different training data setups used within the generalisable method ConvGLR. In Table 4 we show the results of training ConvGLR on SpaRe training set, DTU training set, and mix of all data. We perform experiments consistent with Track 2 of our benchmark i.e. 9 input views scenario. SpaRe Dataset and Benchmark 17 Table 4: Quantitative results of generalisable method - ConvGLR on Track 2 when trained with different data. Training data PSNR-M PSNR SSIM-M LPIPS-M Avg DTU Syn Avg DTU Syn Avg DTU Syn Avg DTU Syn DTU Synthetic DTU + Synth 22.76 25.38 20.14 22.18 26.22 18.14 0.709 0.721 0.697 0.488 0.482 0.494 20.76 20.24 21.29 22.11 16.63 27.60 0.665 0.618 0.711 0.531 0.602 0.460 23.42 25.33 21.51 26.61 25.81 27.40 0.712 0.709 0.715 0.479 0.505 0. Validation DTU Synthetic DTU + Synth 21.80 23.17 20.42 21.15 24.29 18.00 0.723 0.712 0.735 0.437 0.440 0.434 20.33 18.51 22.16 20.82 15.95 25.69 0.684 0.606 0.763 0.475 0.563 0.386 22.62 22.87 22.37 24.59 23.80 25.37 0.731 0.697 0.765 0.419 0.459 0.379 Test We observe that both datasets present similar performance in generalisation capability, performing the best on the data trained with the corresponding dataset. We can see that the best average performance is achieved when mix of training data is used. Notably, we observe high impact of the background on the performance of the model e.g. there is large gap in PSNR calculated on the whole image between datasets when the model has seen only one. This indicates that the inclusion of higher-variety data is beneficial for the generalisable approaches."
        },
        {
            "title": "6 Conclusions",
            "content": "This paper introduces new benchmark and dataset for Sparse Neural Rendering - SpaRe. In this work, we identify shortcomings of current benchmark protocols based on the DTU dataset. This includes but is not limited to low-resolution evaluation, diverse pre-processing step, and using the same camera position as testing views across all the scenes. To alleviate these issues, we propose new dataset, SpaRe, composed by high-quality renderings in setup that reproduces that of the DTU capture. Further, we suggest new benchmarking protocol for the SpaRe dataset and DTU dataset, that introduces more variety in the poses of the input camera views. To facilitate unified and fair evaluation, we propose to keep the ground-truth images of target scenes and views secret. To this end, we provide an online platform for submitting the results and their evaluation. This tool fosters reproducible evaluation, and enables researchers easy access to public leaderboard with the state-of-the-art performance scores that can be updated on rolling basis. In the experimental section, we investigate several state-of-the-art baselines, emphasising the advantages and drawbacks of perscene optimisation, and generalisable approaches. When training generalisable methods, using SpaRe and the DTU dataset yields the best possible results for both synthetic and real data. 18 M. Nazarczuk et al."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was partially supported by the Humboldt Foundation. We thank the AIM 2024 sponsors: Meta Reality Labs, KuaiShou, Huawei, Sony Interactive Entertainment and University of Würzburg (Computer Vision Lab)."
        },
        {
            "title": "References",
            "content": "1. BlenderKit, https://www.blenderkit.com/, accessed on 29th August 2024 2. Aanæs, H., Jensen, R.R., Vogiatzis, G., Tola, E., Dahl, A.B.: Large-Scale Data for Multiple-View Stereopsis. International Journal of Computer Vision pp. 116 (2016) 3. Barron, J.T., Mildenhall, B., Verbin, D., Srinivasan, P.P., Hedman, P.: Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields. In: IEEE Conference on Computer Vision and Pattern Recognition (2022) 4. Blender Online Community: Blender, https://www.blender.org/, accessed on 29th August 2024 5. Bonotto, M., Sarrocco, L., Evangelista, D., Imperoli, M., Pretto, A.: CombiNeRF: Combination of Regularization Techniques for Few-Shot Neural Radiance Field View Synthesis. In: International Conference on 3D Vision (2024) 6. Chang, A.X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., et al.: ShapeNet: An Information-Rich 3D Model Repository. arXiv:1512.03012 (2015) 7. Charatan, D., Li, S., Tagliasacchi, A., Sitzmann, V.: pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction. In: IEEE Conference on Computer Vision and Pattern Recognition (2024) 8. Chen, A., Xu, Z., Zhao, F., Zhang, X., Xiang, F., Yu, J., Su, H.: MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo. In: International Conference on Computer Vision (2021) 9. Chen, Y., Xu, H., Zheng, C., Zhuang, B., Pollefeys, M., Geiger, A., Cham, T.J., Cai, J.: MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images. In: European Conference on Computer Vision (2024) 10. Chibane, J., Bansal, A., Lazova, V., Pons-Moll, G.: Stereo Radiance Fields (SRF): Learning View Synthesis for Sparse Views of Novel Scenes. In: IEEE Conference on Computer Vision and Pattern Recognition (2021) 11. Cong, W., Liang, H., Wang, P., Fan, Z., Chen, T., Varma, M., Wang, Y., Wang, Z.: Enhancing NeRF akin to Enhancing LLMs: Generalizable NeRF Transformer with Mixture-of-View-Experts. In: International Conference on Computer Vision (2023) 12. Deitke, M., Liu, R., Wallingford, M., Ngo, H., Michel, O., Kusupati, A., Fan, A., Laforte, C., Voleti, V., Gadre, S.Y., et al.: Objaverse-XL: Universe of 10M+ 3D Objects. In: Advances in Neural Information Processing Systems (2024) 13. Deitke, M., Schwenk, D., Salvador, J., Weihs, L., Michel, O., VanderBilt, E., Schmidt, L., Ehsani, K., Kembhavi, A., Farhadi, A.: Objaverse: Universe of Annotated 3D Objects. In: IEEE Conference on Computer Vision and Pattern Recognition (2023) 14. Deng, K., Liu, A., Zhu, J.Y., Ramanan, D.: Depth-supervised NeRF: Fewer Views and Faster Training for Free. In: IEEE Conference on Computer Vision and Pattern Recognition (2022) SpaRe Dataset and Benchmark 19 15. Flynn, J., Broxton, M., Debevec, P., DuVall, M., Fyffe, G., Overbeck, R., Snavely, N., Tucker, R.: Deepview: View synthesis with learned gradient descent. In: IEEE Conference on Computer Vision and Pattern Recognition (2019) 16. Gao*, R., Holynski*, A., Henzler, P., Brussee, A., Martin-Brualla, R., Srinivasan, P.P., Barron, J.T., Poole*, B.: CAT3D: Create Anything in 3D with Multi-View Diffusion Models. arXiv:2405.10314 (2024) 17. Jain, A., Tancik, M., Abbeel, P.: Putting NeRF on Diet: Semantically Consistent Few-Shot View Synthesis. In: International Conference on Computer Vision (2021) 18. Johari, M.M., Lepoittevin, Y., Fleuret, F.: GeoNeRF: Generalizing NeRF with Geometry Priors. In: IEEE Conference on Computer Vision and Pattern Recognition (2022) 19. Kerbl, B., Kopanas, G., Leimkühler, T., Drettakis, G.: 3D Gaussian Splatting for Real-Time Radiance Field Rendering. ACM Transactions on Graphics 42(4) (July 2023) 20. Kim, M., Seo, S., Han, B.: InfoNeRF: Ray Entropy Minimization for Few-Shot Neural Volume Rendering. In: IEEE Conference on Computer Vision and Pattern Recognition (2022) 21. Krizhevsky, A., Sutskever, I., Hinton, G.E.: ImageNet Classification with Deep Convolutional Neural Networks. In: Advances in Neural Information Processing Systems (2012) 22. Kwak, M., Song, J., Kim, S.: GeCoNeRF: Few-shot Neural Radiance Fields via Geometric Consistency. In: International Conference on Machine Learning (2023) 23. Li, J., Zhang, J., Bai, X., Zheng, J., Ning, X., Zhou, J., Gu, L.: DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with Global-Local Depth Normalization. In: IEEE Conference on Computer Vision and Pattern Recognition (2024) 24. Li, J., Zhou, Y., Mok, P.Y.: SCNeRF: Feature-Guided Neural Radiance Field from Sparse Inputs. In: Neural Rendering Intelligence Workshop (2024) 25. Liu, M., Shi, R., Chen, L., Zhang, Z., Xu, C., Wei, X., Chen, H., Zeng, C., Gu, J., Su, H.: One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. In: IEEE Conference on Computer Vision and Pattern Recognition (2024) 26. Liu, R., Wu, R., Van Hoorick, B., Tokmakov, P., Zakharov, S., Vondrick, C.: Zero1-to-3: Zero-shot one image to 3d object. In: International Conference on Computer Vision (2023) 27. Mildenhall, B., Srinivasan, P.P., Ortiz-Cayon, R., Kalantari, N.K., Ramamoorthi, R., Ng, R., Kar, A.: Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines. ACM Transactions on Graphics (2019) 28. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. In: European Conference on Computer Vision (2020) 29. Nazarczuk, M., Catley-Chandar, S., Tanay, T., Shaw, R., Pérez-Pellitero, E., Timofte, R., Yan, X., Wang, P., Guo, Y., Wu, Y., Cai, Y., Yang, Y., Li, J., Zhou, Y., Mok, P.Y., He, Z., Xiao, Z., Chan, K.C., Goshu, H.L., Yang, C., Dong, R., Xiao, J., Lam, K.M., Hao, J., Gao, Q., Zu, Y., Zhang, J., Jiao, L., Liu, X., Purohit, K.: AIM 2024 Sparse Neural Rendering Challenge: Methods and Results. In: Proceedings of the European Conference on Computer Vision (ECCV) Workshops (2024) 30. Niemeyer, M., Barron, J.T., Mildenhall, B., Sajjadi, M.S.M., Geiger, A., Radwan, N.: RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs. In: IEEE Conference on Computer Vision and Pattern Recognition (2022) 20 M. Nazarczuk et al. 31. Paliwal, A., Ye, W., Xiong, J., Kotovenko, D., Ranjan, R., Chandra, V., Kalantari, N.K.: CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians. In: European Conference on Computer Vision (2024) 32. Reizenstein, J., Shapovalov, R., Henzler, P., Sbordone, L., Labatut, P., Novotny, D.: Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In: International Conference on Computer Vision (2021) 33. Roessle, B., Barron, J.T., Mildenhall, B., Srinivasan, P.P., Nießner, M.: Dense Depth Priors for Neural Radiance Fields from Sparse Input Views. In: IEEE Conference on Computer Vision and Pattern Recognition (2022) 34. Schönberger, J.L., Frahm, J.M.: Structure-from-Motion Revisited. In: IEEE Conference on Computer Vision and Pattern Recognition (2016) 35. Seo, S., Chang, Y., Kwak, N.: FlipNeRF: Flipped Reflection Rays for Few-shot Novel View Synthesis. In: International Conference on Computer Vision (2023) 36. Shi, R., Wei, X., Wang, C., Su, H.: ZeroRF: Fast Sparse View 360 Reconstruction with Zero Pretraining. In: IEEE Conference on Computer Vision and Pattern Recognition (2024) 37. Shi, Y., Wang, P., Ye, J., Mai, L., Li, K., Yang, X.: Mvdream: Multi-view diffusion for 3d generation. In: International Conference on Learning Representations (2023) 38. Sitzmann, V., Zollhöfer, M., Wetzstein, G.: Scene representation networks: Continuous 3d-structure-aware neural scene representations. Advances in Neural Information Processing Systems (2019) 39. Song, J., Park, S., An, H., Cho, S., Kwak, M.S., Cho, S., Kim, S.: DäRF: Boosting Radiance Fields from Sparse Inputs with Monocular Depth Adaptation. In: Advances in Neural Information Processing Systems (2023) 40. Suhail, M., Esteves, C., Sigal, L., Makadia, A.: Generalizable Patch-Based Neural Rendering. In: European Conference on Computer Vision (2022) 41. T, M.V., Wang, P., Chen, X., Chen, T., Venugopalan, S., Wang, Z.: Is Attention All That NeRF Needs? In: International Conference on Learning Representations (2023) 42. Tanay, T., Maggioni, M.: Global Latent Neural Rendering. In: IEEE Conference on Computer Vision and Pattern Recognition (2024) 43. Truong, P., Rakotosaona, M.J., Manhardt, F., Tombari, F.: SPARF: Neural Radiance Fields from Sparse and Noisy Poses. In: IEEE Conference on Computer Vision and Pattern Recognition (2023) 44. Wang, G., Chen, Z., Loy, C.C., Liu, Z.: SparseNeRF: Distilling Depth Ranking for Few-shot Novel View Synthesis. In: International Conference on Computer Vision (2023) 45. Wang, Q., Wang, Z., Genova, K., Srinivasan, P., Zhou, H., Barron, J.T., MartinBrualla, R., Snavely, N., Funkhouser, T.: IBRNet: Learning Multi-View ImageBased Rendering. In: IEEE Conference on Computer Vision and Pattern Recognition (2021) 46. Wang, Z., Bovik, A., Sheikh, H., Simoncelli, E.: Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing 13(4) (2004) 47. Wewer, C., Raj, K., Ilg, E., Schiele, B., Lenssen, J.E.: latentSplat: Autoencoding Variational Gaussians for Fast Generalizable 3D Reconstruction. In: European Conference on Computer Vision (2024) 48. Wizadwongsa, S., Phongthawee, P., Yenphraphai, J., Suwajanakorn, S.: NeX: Realtime View Synthesis with Neural Basis Expansion. In: IEEE Conference on Computer Vision and Pattern Recognition (2021) SpaRe Dataset and Benchmark 21 49. Wu, R., Mildenhall, B., Henzler, P., Park, K., Gao, R., Watson, D., Srinivasan, P.P., Verbin, D., Barron, J.T., Poole, B., Holynski, A.: ReconFusion: 3D Reconstruction with Diffusion Priors. In: IEEE Conference on Computer Vision and Pattern Recognition (2024) 50. Wynn, J., Turmukhambetov, D.: DiffusioNeRF: Regularizing Neural Radiance Fields with Denoising Diffusion Models. In: IEEE Conference on Computer Vision and Pattern Recognition (2023) 51. Xing, Y., Wang, P., Liu, L., Li, D., Zhang, L.: FrameNeRF: Simple and Efficient Framework for Few-shot Novel View Synthesis. arXiv preprint arXiv:2402.14586 (2024) 52. Xu, H., Chen, A., Chen, Y., Sakaridis, C., Zhang, Y., Pollefeys, M., Geiger, A., Yu, F.: MuRF: Multi-Baseline Radiance Fields. In: IEEE Conference on Computer Vision and Pattern Recognition (2024) 53. Xu, Y., Liu, B., Tang, H., Deng, B., He, S.: Learning with Unreliability: Fast Few-shot Voxel Radiance Fields with Relative Geometric Consistency. In: IEEE Conference on Computer Vision and Pattern Recognition (2024) 54. Xu, Z., Escalera, S., Pavão, A., Richard, M., Tu, W.W., Yao, Q., Zhao, H., Guyon, I.: Codabench: Flexible, easy-to-use, and reproducible meta-benchmark platform. Patterns 3(7) (2022) 55. Yang, J., Pavone, M., Wang, Y.: FreeNeRF: Improving Few-shot Neural Rendering with Free Frequency Regularization. In: IEEE Conference on Computer Vision and Pattern Recognition (2023) 56. Yu, A., Ye, V., Tancik, M., Kanazawa, A.: pixelNeRF: Neural Radiance Fields from One or Few Images. In: IEEE Conference on Computer Vision and Pattern Recognition (2021) 57. Zhang, J., Li, J., Yu, X., Huang, L., Gu, L., Zheng, J., Bai, X.: CoR-GS: SparseView 3D Gaussian Splatting via Co-Regularization. In: European Conference on Computer Vision (2024) 58. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The Unreasonable Effectiveness of Deep Features as Perceptual Metric. In: IEEE Conference on Computer Vision and Pattern Recognition (2018) 59. Zhou, T., Tucker, R., Flynn, J., Fyffe, G., Snavely, N.: Stereo magnification: Learning view synthesis using multiplane images. ACM Transactions on Graphics (2018) 60. Zhu, H., He, T., Li, X., Li, B., Chen, Z.: Is vanilla mlp in neural radiance field enough for few-shot view synthesis? In: IEEE Conference on Computer Vision and Pattern Recognition (2024) 61. Zhu, Z., Fan, Z., Jiang, Y., Wang, Z.: FSGS: Real-Time Few-Shot View Synthesis using Gaussian Splatting. In: European Conference on Computer Vision (2024)"
        }
    ],
    "affiliations": [
        "Huawei Noahs Ark Lab, London, United Kingdom",
        "University of Würzburg, Germany"
    ]
}