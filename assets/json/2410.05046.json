{
    "paper_title": "Named Clinical Entity Recognition Benchmark",
    "authors": [
        "Wadood M Abdul",
        "Marco AF Pimentel",
        "Muhammad Umar Salman",
        "Tathagata Raha",
        "Clément Christophe",
        "Praveen K Kanithi",
        "Nasir Hayat",
        "Ronnie Rajan",
        "Shadab Khan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This technical report introduces a Named Clinical Entity Recognition Benchmark for evaluating language models in healthcare, addressing the crucial natural language processing (NLP) task of extracting structured information from clinical narratives to support applications like automated coding, clinical trial cohort identification, and clinical decision support. The leaderboard provides a standardized platform for assessing diverse language models, including encoder and decoder architectures, on their ability to identify and classify clinical entities across multiple medical domains. A curated collection of openly available clinical datasets is utilized, encompassing entities such as diseases, symptoms, medications, procedures, and laboratory measurements. Importantly, these entities are standardized according to the Observational Medical Outcomes Partnership (OMOP) Common Data Model, ensuring consistency and interoperability across different healthcare systems and datasets, and a comprehensive evaluation of model performance. Performance of models is primarily assessed using the F1-score, and it is complemented by various assessment modes to provide comprehensive insights into model performance. The report also includes a brief analysis of models evaluated to date, highlighting observed trends and limitations. By establishing this benchmarking framework, the leaderboard aims to promote transparency, facilitate comparative analyses, and drive innovation in clinical entity recognition tasks, addressing the need for robust evaluation methods in healthcare NLP."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 ] . [ 1 6 4 0 5 0 . 0 1 4 2 : r Wadood Abdul, Marco AF Pimentel, Muhammad Umar Salman, Tathagata Raha, Clément Christophe, Praveen Kanithi, Nasir Hayat, Ronnie Rajan, Shadab Khan M42 Abu Dhabi, UAE {wabdul, mpimentel, musalman, traha, cchristophe, pkanithi, nhayat, rrajan, skhan}@m42.ae"
        },
        {
            "title": "ABSTRACT",
            "content": "This technical report introduces Named Clinical Entity Recognition Benchmark for evaluating language models in healthcare, addressing the crucial natural language processing (NLP) task of extracting structured information from clinical narratives to support applications like automated coding, clinical trial cohort identification, and clinical decision support. The leaderboard provides standardized platform for assessing diverse language models, including encoder and decoder architectures, on their ability to identify and classify clinical entities across multiple medical domains. curated collection of openly available clinical datasets is utilized, encompassing entities such as diseases, symptoms, medications, procedures, and laboratory measurements. Importantly, these entities are standardized according to the Observational Medical Outcomes Partnership (OMOP) Common Data Model, ensuring consistency and interoperability across different healthcare systems and datasets, and comprehensive evaluation of model performance. Performance of models is primarily assessed using the F1-score, and it is complemented by various assessment modes to provide comprehensive insights into model performance. The report also includes brief analysis of models evaluated to date, highlighting observed trends and limitations. By establishing this benchmarking framework, the leaderboard aims to promote transparency, facilitate comparative analyses, and drive innovation in clinical entity recognition tasks, addressing the need for robust evaluation methods in healthcare NLP. Leaderboard available at https://huggingface.co/m42-health/clinical_ner_leaderboard."
        },
        {
            "title": "INTRODUCTION",
            "content": "Named Entity Recognition (NER) in the clinical domain is fundamental task in medical natural language processing (NLP), playing crucial role in extracting structured information from unstructured clinical narratives. The ability to identify and classify entities such as diseases, symptoms, medications, and procedures within clinical texts is essential for wide range of downstream applications (Pradhan et al., 2015; Stubbs et al., 2015). These applications include clinical decision support systems, where identified entities can trigger relevant alerts and/or recommendations; automated coding for billing and administrative purposes; and cohort identification for clinical trials, enabling rapid patient recruitment based on specific clinical criteria (Savova et al., 2010). Additionally, as the volume of electronic health records (EHRs) continues to grow, efficient and accurate extraction of clinically relevant information becomes increasingly vital for both patient care and medical research (Hossain et al., 2023). Accurate NER systems can significantly improve"
        },
        {
            "title": "Technical Report",
            "content": "the quality of data available for clinical research, facilitate the development of precision medicine approaches, and enhance the overall efficiency of healthcare delivery (Shivade et al., 2014; Hossain et al., 2023). Assessing the performance of NER tasks in the clinical domain, however, presents several challenges (Kundeti et al., 2016). The inherent complexity and variability of medical terminology, coupled with the highly context-dependent nature of clinical language, make it difficult to develop universally effective NER models. Moreover, the quality of annotations in available datasets can vary significantly, affecting the reliability of performance evaluations (Kundeti et al., 2016; Menasalvas et al., 2016; Wu et al., 2020). The scarcity of large, diverse, and well-annotated clinical datasets further complicates the assessment process, as models may perform inconsistently across different medical subdomains or institution-specific terminologies (Névéol et al., 2018; Wu et al., 2020; Niero et al., 2023). Recent advancements in language models, particularly Large Language Models (LLMs), have shown promising results in various NLP tasks, including clinical NER (Sun et al., 2021; Chen et al., 2023; Zhang et al., 2024). However, the lack of standardized evaluation framework makes it challenging to compare the performance of these models objectively and consistently across different studies and datasets (Peng et al., 2019; Wu et al., 2020; Gu et al., 2022). To address these challenges, we present comprehensive Named Clinical Entity Recognition (Clinical NER) Leaderboard. This leaderboard provides standardized platform for evaluating and benchmarking the performance of various language models on clinical NER tasks. By utilizing curated collection of openly available clinical datasets and implementing consistent evaluation metrics, our leaderboard aims to foster transparency, facilitate comparative analysis, and drive innovation in the field of clinical NER. The key contributions of this work are summarized as follows: Standardized evaluation framework: We introduce comprehensive Clinical NER Leaderboard, which provides consistent and transparent platform for evaluating and benchmarking the performance of various language models (encoder, decoder & gliner) on clinical NER tasks. Curated dataset collection with common standards: The leaderboard makes use of curated collection of openly-available clinical datasets, where entity standardization was performed using the OMOP Common Data Model standard, which ensuring that the evaluation is robust, consistent, and reflective of the diverse and context-dependent nature of clinical language. Consistent evaluation metrics: We implement standardized evaluation metrics, allowing for objective and comparable assessments of NER models across different studies and datasets. Comparative analysis: By providing centralized and transparent platform, our leaderboard enables researchers to conduct comparative analyses, promoting innovation and driving progress in clinical NER research. These contributions, ultimately, aim to advance the field of clinical NER by addressing existing challenges and promoting the development of more accurate, reliable, and universally applicable models in healthcare applications."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Unlike general domains, where benchmarks like GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) are well-established, the biomedical field lacks equivalent resources (Kanithi et al., 2024). Over the years, the field of biomedical NLP has seen the development and release of numerous datasets, often stemming from shared tasks such as BioCreative (Li et al., 2016b), BioNLP (Demner-Fushman et al., 2024), and SemEval (Ojha et al., 2024). While the focus of these datasets has evolved from simple tasks like NER to other tasks such as relation extraction and question answering, there remains significant gap in the availability of benchmarks and leaderboards for medical and clinical NLP."
        },
        {
            "title": "Technical Report",
            "content": "Researchers have extensively explored the use of shared language representations to capture the semantics of biomedical text, often applying these models across range of tasks in the field (Peng et al., 2019). common approach involves transfer learning, where models are pretrained on extensive biomedical corpora and then fine-tuned for specific tasks like NER and relation extraction. BioBERT (Lee et al., 2019) and BioELMo (Jin et al., 2019) are notable examples of these approaches. These efforts have typically involved individual models evaluated in isolation, without the benefit of standardized benchmarks or leaderboards to facilitate broader comparison and validation across different approaches in medical and clinical NLP. BLURB (Biomedical Language Understanding and Reasoning Benchmark) is one of the few benchmarks in the biomedical field, which spans multiple tasks beyond NER (Gu et al., 2022). Peng et al. (2019) also introduced the Biomedical Language Understanding Evaluation (BLUE) benchmark consisting of six tasks that cover both biomedical and clinical texts with different datasets. While these benchmarks provide broad coverage of tasks, the methods and metrics used for NER tasks are not clearly detailed, and the number of domains and entities covered in the datasets is limited. Additionally, more recent approaches, such as generative models, are not included in the benchmark, indicating gap in its ability to fully assess the latest advancements in the field (Chen et al., 2023). While comprehensive evaluation frameworks like MEDIC (Kanithi et al., 2024) assess broad range of clinical NLP tasks, in this paper we focus exclusively on NER tasks, allowing for more detailed examination of how models are assessed and performance metrics computed. By narrowing our scope to NER, we can delve deeper into the intricacies of model evaluation, ensuring that the metrics used provide comprehensive understanding of models capability to accurately identify and classify entities within the medical and clinical domains. This focused approach also enables us to explore the latest trends in utilizing large language models for diverse NER tasks, providing platform to compare the performance of different model architectures. Finally, this work also emphasizes the importance of standardizing entities across models and datasets according to widely accepted standards, critical aspect that has been insufficiently addressed in previous works. Overall, we aim to highlight the strengths and limitations of various models, offering insights into how these models perform in specialized tasks that are crucial for advancing biomedical NLP."
        },
        {
            "title": "3 THE CLINICAL NER BENCHMARK",
            "content": "To address the challenges in evaluating clinical NER models, we have developed benchmark that provides standardized platform for assessing performance. This benchmark consists of the following key components: it contains common evaluation methodology that employs well-established evaluation metrics, primarily focusing on the F1-score; it employs terminology standardization of the clinical entities included in our evaluation, which ensures consistency and interoperability; and it includes curated collection of openly available medical benchmark datasets, encompassing broad spectrum of medical entities. In the subsections below, we first elucidate the problem and then elaborate on the components in the following subsections. 3.1 NAMED-ENTITY RECOGNITION TASK NER is crucial task in biomedical NLP that aims to identify and classify medical entities in unstructured clinical text. Mathematically, we can formulate the NER task as follows. Given an input sequence of tokens = (x1, x2, . . . , xn), where each xi represents token (a word or sub-word) in clinical text, the goal is to assign corresponding sequence of labels = (y1, y2, . . . , yn), where each yi belongs to predefined set of clinical entity types {O}, with representing the Outside label for tokens that are not part of any medical entity. Formally, we can express this as function : , where is the space of all possible input sequences of text, and is the space of all possible clinical label sequences. set of The {DIS, PROC, DRUG, . . .}, where, for example: clinical entity types typically includes categories such as = DIS corresponds to medical conditions or disorders, PROC includes medical procedures or interventions,"
        },
        {
            "title": "Technical Report",
            "content": "DRUG relates to medications. The NER task can be viewed as sequence labeling problem, where we aim to maximize the conditional probability (Y X), i.e., arg maxY (Y X). This probability can be modeled using various approaches, such as Conditional Random Fields (CRFs), or neural network architectures like Bidirectional Long Short-Term Memory (BiLSTM) networks or Transformer-based models fine-tuned on clinical corpora (Wu et al., 2020). To illustrate the clinical NER task, consider the following example: Patient presents with acute myocardial infarction [DIS] and is prescribed aspirin [DRUG] until angioplasty [PROC] is performed. The input sequence is: Patient presents with acute myocardial infarction and is prescribed aspirin until angioplasty is performed The corresponding label sequence (assuming each word is token): B-DIS I-DIS I-DIS B-DRUG B-PROC Where B-* indicates the beginning of an entity, I-* indicates the continuation (inside) of an entity, and indicates tokens outside of clinical entities of interest. This example demonstrates how the clinical NER task assigns labels to each token in the input sequence, identifying acute myocardial infarction as disease, aspirin as drug, and angioplasty as procedure. 3.2 EVALUATION METRICS The performance of clinical NER models, which aim to optimize (Y X) as shown in equation (3), is evaluated using two types of metrics: token-based and span-based. Both types utilize precision, recall, and F1-score, but they differ in how they define true positives (TP), false positives (FP), and false negatives (FN). 3.2.1 TOKEN-BASED METRICS Token-based metrics evaluate the models performance at the individual token level. For each token xi in the input sequence X, we compare the predicted label ˆyi with the true label yi. Let Pt, Pt, and Nt represent token-level true positives, false positives, and false negatives, respectively. Then: Precisiont = Recallt = Pt Pt + Pt Pt Pt + Nt F1-scoret = 2 Precisiont Recallt Precisiont + Recallt (1) (2) (3) The above metrics can be calculated either globally or on per entity type basis, thus giving us two possible metrics: Micro Average: The Pt, Pt, and Nt values are calculated globally to get the final precision, recall and F1 values. Macro Average: The precision, recall and F1 are calculated for each entity type and then averaged without any weightage. With this token-based approach, we have broad idea of the performance of the model at the token level. However, it may misrepresent the performance at the entity level when the entity includes"
        },
        {
            "title": "Technical Report",
            "content": "Table 1: Exact and partial span metric calculations. Each predicted span can be attributed to each class depending on exact or partial matches. Span Class Exact Partial Correct Incorrect Missed Spurious The predicted and true spans boundary and label match exactly There is mismatch in either the boundary or label between the predicted and true span For given True span, there is no predicted span that has overlap with it For given predicted span, there is no true span that has an exact overlap with it label The predicted and true spans matches exactly and the boundary has some overlap There is an overlap in the boundary of predicted and true span but mismatch in the label For given True span, there is no predicted span that has overlap with it For given predicted span, there is no true span that has any overlap with it more than 1 token (which may be more relevant for certain applications). In addition, depending on the annotations of certain datasets, we may not want to penalize model for \"partial\" match with certain entity. 3.2.2 SPAN-BASED METRICS Span-based metrics evaluate the models performance at the entity level, considering full or partial matches. These metrics are particularly important in clinical NER, as they reflect the models ability to identify complete medical entities. Let Ps, Ps, and Ns represent span-level true positives, false positives, and false negatives, respectively. We define: Exact Match: The predicted entity spans exactly match the true entity spans boundary and label. Partial Match: The predicted entity spans overlap with the true entity spans boundary and exactly matches the label. Based on the criteria above, each predicted or true span can be classified as Correct, Incorrect, issed, Spurious (see Table 1). Using the above classifications, we have Ps = Incorrect + Spurious Ns = Incorrect + issed Then, we calculate: Precisions = Recalls = Ps Ps + Ps Ps Ps + Ns F1-scores = 2 Precisions Recalls Precisions + Recalls (4) (5) (6) (7) (8) Strict span based evaluation may be more applicable in applications like de-identifying PII, where as partial span based evaluation is desirable when we have leading/following words that do not change the entitys meaning. 3.2.3 WORKING EXAMPLE Consider the following example, with the following entities (i.e., true labels):"
        },
        {
            "title": "Technical Report",
            "content": "The patients chest X-ray [PROC] showed pneumonia [DIS] , and blood cultures [LAB] were ordered to rule out sepsis [DIS] . Patient has no diabetes [DIS] . Levofloxacin [DRUG] was prescribed for treatment.\" Assume the predicted labels are as follows: The patients chest X-ray [PROC] showed pneumonia [DIS] , and blood cultures [LAB] were ordered to rule out sepsis. Patient has no diabetes [DIS] . Levofloxacin [DRUG] was prescribed for treatment [PROC] .\" Token-based evaluation (Micro Average): TPt = 6 (X-ray, pneumonia, blood, cultures, diabetes, Levofloxacin) FPt = 1 (treatment) FNt = 2 (chest, sepsis) F1-scoret = 0.80 Token-based evaluation (Macro Average): TPt = PROC: 1, DIS: 2, DRUG: 1, LAB: 2 FPt = PROC: 1, DIS: 0, DRUG: 0, LAB: 0 FNt = PROC: 1, DIS: 1, DRUG: 0, LAB: 0 Precisiont = PROC: 0.5, DIS: 1, DRUG: 1, LAB: 1 Recallt = PROC: 0.5, DIS: 0.66, DRUG: 1, LAB: 1 F1t = PROC: 0.5, DIS: 0.8, DRUG: 1, LAB: 1 Final F1-scoret = 0.82 Span-based evaluation (Exact Match): TPs = 4 (pneumonia, blood cultures, diabetes, Levofloxacin) FPs = 2 (chest X-ray, treatment) FNs = 2 (chest X-ray, sepsis) F1-scores = 0.66 Span-based evaluation (Partial Match): TPs = 5 (chest X-ray, pneumonia, blood cultures, diabetes, Levofloxacin) FPs = 1 (treatment) FNs = 1 (sepsis) F1-scores = 0.83 This example demonstrates how token-based and span-based metrics can provide different perspectives on model performance. Span-based metrics, in particular, reveal issues with entity boundary detection, particularly for the procedure entity. The partial match evaluation shows better performance than the exact match, indicating that the model is generally identifying the correct entities but sometimes struggles with precise boundaries. For our evaluation framework we consider the Macro Average token-based metrics and the Partial Match for our span-based metrics. The variety of entity types demonstrated in this example (procedure, disease, lab test, drug) highlights the complexity of clinical NER tasks. To ensure consistency across different NER systems and to facilitate interoperability in clinical applications, it is crucial to establish standardized terminology for entity types. This standardization not only aids in the accurate evaluation of NER models"
        },
        {
            "title": "Technical Report",
            "content": "Table 2: Standard clinical entities. Brief description of the OMOP domains used in the Clinical NER Benchmark. Entity Type Description Examples Conditions Medical diagnoses, symptoms, or clinical findings Procedures Medical, surgical, or diagnostic interventions Drugs Measurements Genes Gene Variants therapeutic agents, or substances Medications, used for treatment Laboratory tests, vital signs, or other quantifiable clinical observations Specific genes or genetic loci relevant to clinical contexts Specific alterations or mutations in genes Pneumonia, Hypertension, Chest pain Appendectomy, MRI Blood transfusion Aspirin, Insulin, Amoxicillin scan, Blood glucose level, Body temperature, Serum creatinine BRCA1, TP53, EGFR BRAF V600E, EGFR T790M, KRAS G12D but also enhances the utility of extracted information in downstream tasks such as clinical decision support systems. The following section delves into the importance and implementation of common terminologies in clinical NER. 3.3 COMMON TERMINOLOGY Standardization of medical terminology is critical requirement for the effective development and deployment of clinical NLP systems. In the medical field, the proliferation of institution-specific vocabularies, coding systems, and ontologies has long posed significant challenge for data integration, interoperability, and the generalization of NLP models across different healthcare settings (Iroju et al., 2015). To address this issue, the Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM) has emerged as widely adopted standard for harmonizing clinical data (Observational Health Data Sciences & Informatics, 2021). The OMOP CDM provides standardized framework for organizing and representing wide range of medical concepts, including diagnoses, procedures, medications, laboratory tests, and demographic information. By mapping diverse source terminologies to the common OMOP concepts and vocabularies, the model enables seamless integration and analysis of data from multiple institutions and data sources. The importance of terminology standardization is particularly evident in the context of clinical NER, where the accurate identification and classification of medical entities are crucial for downstream applications such as clinical decision support, automated coding, and cohort identification. Inconsistent or ambiguous representations of these entities can lead to significant errors and performance degradation in NER models (Kundeti et al., 2016; Klug et al., 2024). In the development of our Clinical NER Benchmark, we have leveraged the OMOP Common Data Model to standardize the medical entities included in the evaluation datasets. By aligning the entities to the OMOP standard vocabularies, we ensure that the benchmark provides consistent and interoperable representation of clinical concepts, facilitating fair comparisons of NER model performance across diverse datasets and healthcare settings. Furthermore, we propose two additional domains - genes and gene variants - to cover genomic data, aligning with the OMOP CDM extension for storing genetic information, thus enhancing the benchmarks applicability to precision medicine and genomics research (Shin et al., 2019). Table 2 provides an overview of these domains, including brief descriptions and examples for each entity type. By incorporating these OMOP domains, our Clinical NER Benchmark provides comprehensive framework for evaluating NER models across diverse range of clinical entities. This approach not only ensures broad coverage of medically relevant concepts but also facilitates the benchmarks applicability to various clinical specialties and research areas, including oncology, pharmacogenomics, and rare genetic disorders. Importantly, the use of the OMOP CDM as our standardization framework ensures the scalability and future-proofing of our benchmark. Additional entity types or domains can be seamlessly integrated into the benchmark in the future, following careful mapping"
        },
        {
            "title": "Technical Report",
            "content": "Table 3: Summary of publicly available datasets. The standard entities that are included in each dataset is also shown here. For detailed entity type mapping refer 5 Dataset # samples # annotations Entity types Corpus NCBI CHIA BC5CDR BIORED 100 194 500 100 960 3,981 9,928 3,535 Condition Condition, Procedure, Measurement, Drug Clinical Trials Condition, Drug Condition, Drug, Gene, Gene variant PubMed PubMed PubMed process to align with OMOP standards. This extensibility allows our benchmark to evolve alongside advancements in medical knowledge and changing clinical information needs, maintaining its relevance and comprehensiveness over time."
        },
        {
            "title": "3.4 DATASETS",
            "content": "Four publicly-available datasets have been included in our benchmark. They are summarized in Table 3. NCBI The NCBI Disease corpus includes mention and concept level annotations on 100 PubMed abstracts (Dogan et al., 2014). It covers annotations of diseases. CHIA This is large, annotated corpus of patient eligibility criteria extracted from 194 registered clinical trials (Kury et al., 2020). Annotations cover 15 entity types (according to OMOP domains), including conditions, drugs, procedures, and measurements. BC5CDR The BC5CDR corpus contains PubMed articles with human annotations of all chemicals and diseases (Li et al., 2016a). BIORED The BIORED corpus includes set of PubMed abstracts with annotations of multiple entity types, including genes/proteins, diseases, and chemicals (Luo et al., 2022). The above datasets were adapted to align with our evaluation framework by mapping the annotations to clinically relevant entity types, as defined by the OMOP CDM. Entity types not included in the framework were omitted due to the limited availability of datasets with sufficient annotations for those entities. To ensure consistency, the retained clinical entity types were standardized across all datasets, resulting in final set of six clinical entity types, as detailed in Table 2."
        },
        {
            "title": "4 RESULTS AND ANALYSIS",
            "content": "We performed an analysis of the performance of various models evaluated on the proposed benchmarks and included on our leaderboard, showcasing the outcomes of the models assessed to date, with additional models planned to be incorporated in future iterations. 4.1 MODEL DIVERSITY The analysis encompassed diverse range of model architectures, including encoder-only, decoderonly, and the recently proposed GLiNER models (Zaratiana et al., 2023). These models varied in size, pre-training data, and whether they underwent fine-tuning for the NER task. Table 4 provides summary of the models evaluated in this study, highlighting their architectural differences and key characteristics. The different model architectures included in the leaderboard are: Encoder: The standard token classification model built on top of transformer encoder architecture. Decoder: Autoregressive token generation models based on the transformer decoder architecture."
        },
        {
            "title": "Technical Report",
            "content": "Table 4: Current Models on the Leaderboard. Models varying in architecture, training data scope and sizes are currently included on the leaderboard. Model Architecture Type #Params (M) Decoder Decoder GLiNER Encoder GLiNER Encoder GLiNER Encoder Decoder Decoder Universal-NER/UniNER-7B-type-sup Universal-NER/UniNER-7B-all knowledgator/gliner-multitask-large-v0.5 gliner-community/gliner_large-v2.5 urchade/gliner_large_bio-v0.1 Universal-NER/UniNER-7B-type openai/gpt-4o-2024-05-13 EmergentMethods/gliner_large_news-v2.1 GLiNER Encoder GLiNER Encoder urchade/gliner_large-v2.1 Decoder openai/gpt-4o-mini-2024-07-18 GLiNER Encoder numind/NuNER_Zero GLiNER Encoder numind/NuNER_Zero-span Decoder meta-llama/Meta-Llama-3.1-8B-Instruct Decoder meta-llama/Meta-Llama-3-8B-Instruct Decoder meta-llama/Meta-Llama-3-70B-Instruct Encoder alvaroalon2/biobert_diseases_ner Encoder bioformers/bioformer-8L-ncbi-disease Decoder mistralai/Mixtral-8x7B-Instruct-v0.1 fine-tuned fine-tuned zero-shot zero-shot zero-shot zero-shot zero-shot zero-shot zero-shot zero-shot zero-shot zero-shot zero-shot zero-shot zero-shot fine-tuned fine-tuned zero-shot 7000 7000 304 304 304 7000 - 304 304 - 304 304 8030 8030 70000 110 43 45000 GLiNER Encoder: An enhancement on the transformer encoder architecture that uses similarity between span and entity embeddings. The models also vary in the scope of training data used. The models that have been exposed to any of the training data on the benchmark have been categorised as Type: fine-tuned and the models with no exposure to the training data from the benchmark have been categorised as Type:zero-shot1 . The inclusion of this diverse set of models allows for comprehensive evaluation of different approaches to clinical NER, spanning from general-purpose language models (e.g., LLMs) to those specifically designed for token classification tasks. 4.2 ENTITY-SPECIFIC PERFORMANCE Figure 1 shows the overall performance of all models for each entity type using both span-based and token-based metrics. notable observation from this analysis is the higher performance (F1-score) for condition and drug entities compared to other entity types, which is observed for both span-based and token-based approaches. This trend may be attributed to the prevalence and consistency of these entity types in clinical texts, as well as their potentially more standardized representation in medical terminology. This is also reflected in figure 6 that shows the span counts for each entity type present on the leaderboard. Interestingly, when examining the performance for single entity type (condition) across different datasets (Figure 8), we observe relatively consistent performance. This suggests that the models ability to recognize Condition entity type (for example) may be generalizable across various clinical contexts and data sources. 4. IMPACT OF MODEL SIZE AND ARCHITECTURE Figure 2 illustrates the performance of models according to their size and architecture. key finding from this analysis is that LLMs models (i.e., decoder-only architectures) generally do not perform as well as the specialized encoder-based GLiNER architecture for the clinical NER task. This disparity in performance may be attributed to the inherent strengths of encoder-based architec1Note: Some of the zero-shot models may have exposure to the benchmarks clinical entities by being trained on open source or synthetically generated datasets that have similar entities."
        },
        {
            "title": "Technical Report",
            "content": "(a) Token-based (b) Span-based Figure 1: Overall performance of models across six clinical entities. Box plots represent F1-scores of various models across the clinical entity types for each metric approach: token-based (left), and span-based. Each dots represent the performance of model. tures in token classification tasks, which align closely with the requirements of NER. GLiNER was designed specifically for token classification tasks, utilizing span and label embeddings similarity, this likely contributes to its strong performance in this task. Decoder models on the other hand generate tokens in an auto-regressive manner, this limits its ability to extract accurate span information, task which is extractive in nature. 4.3.1 IMPACT OF FINETUNING Figure 3 depicts the performance across clinical entities of fine-tuned and zero-shot models . Only the decoder architecture subset is used for this comparison as architectures like GLiNER do not have supervised variant at the time of writing the paper. We note that the best performance is obtained by supervised models, which is an expected result. Among the zero-shot models, in lead are Meta-Llama-3-70B-Instruct which is much larger in size and UniNER-7B-type which has been trained on task specific synthetically generated data. 4.4 TOKEN-BASED VS. SPAN-BASED EVALUATION We have also compared token-based and span-based performance metrics for the evaluated models. While the core messages and trends derived from both evaluation approaches remain consistent, we observed differences in the absolute performance values and relative rankings of models between the two metrics (as shown in Figure 4). Token-based and span-based F1-scores reveal clear ranking distinctions between models. The figure compares the overall (average) token-based and span-based F1-scores for each model, highlighting the ranking of models according to each metric and providing insight into model performance across different evaluation approaches. These differences highlight the importance of considering both evaluation methodologies in clinical NER tasks. Token-based metrics provide insights into the models ability to correctly classify individual tokens, while span-based metrics offer more holistic view of entity recognition. The disparity between these metrics underscores the complexity of clinical NER and the need for comprehensive evaluation approaches to fully understand model performance."
        },
        {
            "title": "Technical Report",
            "content": "(a) Size Comparision (b) Architecture Comparision Figure 2: Performance across model sizes and architectures. Both the plots represent the span based F1 scores. For size comparision (left) the average F1 score across clinical entities is used. For architecture comparision (right) only decoder and GLiNER encoder models are used. Additionally, closed source models are filtered out."
        },
        {
            "title": "5 DISCUSSION AND CONCLUSIONS",
            "content": "In this work, we introduce Clinical NER Benchmark, providing standardized framework for evaluating language models for NER tasks. Our work addresses some critical challenges in clinical NLP and offers valuable insights into model performance across various clinical domains. key strength of this work lies in its comprehensive approach to addressing persistent challenges in clinical NLP. First, our leaderboard tackles the issue of non-standardized medical data formats through terminology standardization. By leveraging the OMOP CDM for entity standardization, we promote consistency and interoperability across diverse healthcare systems and datasets. This standardization not only facilitates more meaningful comparisons between models but also enhances the potential for collaborative research and development in clinical NLP. Second, we have processed set of benchmark datasets that cover various entity types and clinical domains. This diverse collection ensures robust evaluation of model performance across different aspects of clinical narratives, providing more comprehensive assessment of models capabilities in real-world healthcare scenarios. Third, our methodology for evaluation includes different criteria for computing standard metrics such as precision, recall, and F1-score, this allows for direct comparisons with existing litFigure 3: Effect of Training. Span based metrics of the open-source decoder models from the leaderboard are used here."
        },
        {
            "title": "Technical Report",
            "content": "Figure 4: Model rankings according to token-based and span-based F1-scores. The overall (average) token (left) and span-based metrics for each model are shown. The top-4 performing models, according to the span-based F1-score, are highlighted in orange, and the performance of GPT-4o is shown in teal. Models with overall performances below 20% are not shown. erature while offering comprehensive insights into model performance, addressing the multifaceted nature of entity recognition tasks. Our evaluation of various models included on the leaderboard (to date) has yielded some important insights. GLiNER-based models have demonstrated superior performance across multiple datasets and entity types. In contrast, decoder-only architectures, used by LLMs such as Llama-3 and GPT4o, have shown comparatively lower performance. similar trend has been observed in other studies (Chen et al., 2023; Soroush et al., 2024). Furthermore, our analysis revealed that the choice of evaluation strategytoken-based or span-basedcan significantly impact the ranking of models, highlighting the importance of comprehensive assessment approaches in clinical NER tasks. With the establishment of this leaderboard, we aim to drive significant advancements in clinical NLP, with particular focus on NER. By providing standardized platform for evaluating diverse language models, including LLMs, we enable researchers and practitioners to benchmark their approaches against state-of-the-art performance. This transparency and comparability are crucial for driving innovation and improving the accuracy of clinical entity recognition tasks, which have farreaching implications for applications such as clinical decision support, automated coding, and cohort identification for clinical trials. Although our current evaluation metrics focus on traditional measures such as precision, recall, and F1-score, we recognize the potential for more refined assessment approaches. For instance, Fu et al. (2020) proposed an alternative methodology that defines explainable attributes of data (e.g., entity density, label consistency, token frequency) and evaluates models on distinct buckets based on these attributes. This granular approach allows for more detailed understanding of model performance, identifying specific areas of strength and weakness. Incorporating such methodologies into future iterations of our leaderboard could provide even more actionable insights for researchers and developers, guiding targeted improvements in model architectures and training strategies. It is important to acknowledge significant limitation in the field of clinical NER, which is also reflected in our leaderboard: the issue of label imbalance. Clinical datasets, such as those used in this work, often exhibit skewed distribution of entity types, with some categories being far more prevalent than others. This imbalance can lead to reporting biased model performances, where accuracy on common entities (such as conditions) may overshadow poor performance on less prevalent (annotated) clinical entities. Future work on this leaderboard and in the broader field of clinical NER should address this limitation through the development of more balanced benchmark datasets."
        },
        {
            "title": "Technical Report",
            "content": "We are actively working to expand the scope and utility of the Clinical NER Leaderboard2. Additional internal datasets are in the process of being included, which will further enhance the robustness and generalizability of model evaluations. Moreover, we enthusiastically welcome contributions from the broader research community. Whether in the form of new datasets, innovative model architectures, or improvements to the clinical NER Benchmark codebase3, external contributions will play crucial role in the continued evolution and relevance of this resource. To facilitate engagement, we have implemented an automatic submission form, streamlining the process for researchers to add their models to the leaderboard. In conclusion, by addressing key challenges in data standardization and providing platform for transparent comparison, we aim to accelerate progress in this critical domain of healthcare informatics. As we continue to refine and expand this resource, we look forward to the insights and innovations it will foster within the research community, ultimately contributing to more accurate and efficient processing of clinical narratives."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "This work was supported by M42."
        },
        {
            "title": "REFERENCES",
            "content": "Qijie Chen, Haotong Sun, Haoyang Liu, Yinghui Jiang, Ting Ran, Xurui Jin, Xianglu Xiao, Zhimin Lin, Hongming Chen, and Zhangmin Niu. An extensive benchmark study on biomedical text generation and mining with ChatGPT. Bioinformatics, 39(9), September 2023. Dina Demner-Fushman, Sophia Ananiadou, Makoto Miwa, Kirk Roberts, and Junichi Tsujii (eds.). Proceedings of the 23rd Workshop on Biomedical Natural Language Processing, Bangkok, Thailand, August 2024. Association for Computational Linguistics. Rezarta Islamaj Dogan, Robert Leaman, and Zhiyong Lu. Ncbi disease corpus: resource for disease name recognition and concept normalization. Journal of biomedical informatics, 47:1 10, 2014. Jinlan Fu, Pengfei Liu, and Graham Neubig. Interpretable multi-dataset evaluation for named entity recognition. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 60586069, Stroudsburg, PA, USA, November 2020. Association for Computational Linguistics. Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. Domain-specific language model pretraining for biomedical natural language processing. ACM Trans. Comput. Healthc., 3(1):123, January 2022. Elias Hossain, Rajib Rana, Niall Higgins, Jeffrey Soar, Prabal Datta Barua, Anthony Pisani, and Kathryn Turner. Natural language processing in electronic health records in relation to healthcare decision-making: systematic review. Comput. Biol. Med., 155(106649):106649, March 2023. Olaronke Iroju, Department of Computer Science, Adeyemi College of Education, Ondo, Nigeria, and Janet Olaleke. systematic review of natural language processing in healthcare. Int. J. Inf. Technol. Comput. Sci., 7(8):4450, July 2015. Qiao Jin, Bhuwan Dhingra, William Cohen, and Xinghua Lu. Probing biomedical embeddings from language models. In Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP, pp. 8289, Stroudsburg, PA, USA, 2019. Association for Computational Linguistics. Praveen Kanithi, Clément Christophe, Marco Pimentel, Tathagata Raha, Nada Saadi, Hamza Javed, Svetlana Maslenkova, Nasir Hayat, Ronnie Rajan, and Shadab Khan. MEDIC: Towards comprehensive framework for evaluating LLMs in clinical applications. arXiv [cs.CL], September 2024. 2https://huggingface.co/spaces/m42-health/clinical_ner_leaderboard 3https://github.com/WadoodAbdul/clinical_ner_benchmark"
        },
        {
            "title": "Technical Report",
            "content": "Katrin Klug, Katharina Beckh, Dario Antweiler, Nilesh Chakraborty, Giulia Baldini, Katharina Laue, René Hosch, Felix Nensa, Martin Schuler, and Sven Giesselbach. From admission to discharge: systematic review of clinical natural language processing along the patient journey. BMC Med. Inform. Decis. Mak., 24(1):113, August 2024. Srinivasa Rao Kundeti, Vijayananda, Srikanth Mujjiga, and Kalyan. Clinical named entity recognition: Challenges and opportunities. In 2016 IEEE International Conference on Big Data (Big Data), pp. 19371945. IEEE, December 2016. Fabrıcio Kury, Alex Butler, Chi Yuan, Li-heng Fu, Yingcheng Sun, Hao Liu, Ida Sim, Simona Carini, and Chunhua Weng. Chia, large annotated corpus of clinical trial eligibility criteria. Scientific data, 7(1):111, 2020. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. BioBERT: pre-trained biomedical language representation model for biomedical text mining. arXiv [cs.CL], January 2019. Jiao Li, Yueping Sun, Robin J. Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman, Allan Peter Davis, Carolyn J. Mattingly, Thomas C. Wiegers, and Zhiyong Lu. Biocreative CDR task corpus: resource for chemical disease relation extraction. Database J. Biol. Databases Curation, 2016, 2016a. doi: 10.1093/database/baw068. URL https://doi.org/10.1093/ database/baw068. Jiao Li, Yueping Sun, Robin Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman, Allan Peter Davis, Carolyn Mattingly, Thomas Wiegers, and Zhiyong Lu. BioCreative CDR task corpus: resource for chemical disease relation extraction. Database (Oxford), 2016:baw068, May 2016b. Ling Luo, Po-Ting Lai, Chih-Hsuan Wei, Cecilia N. Arighi, and Zhiyong Lu. Biored: comprehensive biomedical relation extraction dataset. CoRR, abs/2204.04263, 2022. doi: 10.48550/arXiv.2204.04263. URL https://doi.org/10.48550/arXiv.2204.04263. Ernestina Menasalvas, Alejandro Rodriguez-Gonzalez, Roberto Costumero, Hector Ambit, and Consuelo Gonzalo. Clinical narrative analytics challenges. In Rough Sets, Lecture notes in computer science, pp. 2332. Springer International Publishing, Cham, 2016. Luiz Henrique Pereira Niero, João Vitor Andrioli de Souza, Luciana Martins Gomes da Silva, Yohan Bonescki Gumiel, Nícolas Henrique Borges, Gustavo Henrique Munhoz Piotto, Gustavo Giavarini, and Lucas Emanuel Silva Oliveira. Challenges and issues on extracting named entities from oncology clinical notes. J. Health Inform., 15(Especial), July 2023. Aurélie Névéol, Hercules Dalianis, Sumithra Velupillai, Guergana Savova, and Pierre Zweigenbaum. Clinical natural language processing in languages other than english: opportunities and challenges. J. Biomed. Semantics, 9(1):12, March 2018. Observational Health Data Sciences and Informatics. The Book of OHDSI: Chapter 4 The Common Data Model. https://ohdsi.github.io/TheBookOfOhdsi/, January 2021. Accessed: 2024-9-2. Atul Kr Ojha, Seza Dogruöz, Harish Tayyar Madabushi, Giovanni Da San Martino, Sara Rosenthal, and Aiala Rosá (eds.). Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024), Mexico City, Mexico, June 2024. Association for Computational Linguistics. Yifan Peng, Shankai Yan, and Zhiyong Lu. Transfer learning in biomedical natural language processing: An evaluation of BERT and ELMo on ten benchmarking datasets. arXiv [cs.CL], June 2019. Sameer Pradhan, Noémie Elhadad, Brett South, David Martinez, Lee Christensen, Amy Vogel, Hanna Suominen, Wendy Chapman, and Guergana Savova. Evaluating the state of the art in disorder recognition and normalization of the clinical narrative. J. Am. Med. Inform. Assoc., 22 (1):143154, January 2015."
        },
        {
            "title": "Technical Report",
            "content": "Guergana Savova, James Masanz, Philip Ogren, Jiaping Zheng, Sunghwan Sohn, Karin Kipper-Schuler, and Christopher Chute. Mayo clinical text analysis and knowledge extraction system (cTAKES): architecture, component evaluation and applications. J. Am. Med. Inform. Assoc., 17(5):507513, September 2010. Seo Jeong Shin, Seng Chan You, Yu Rang Park, Jin Roh, Jang-Hee Kim, Seokjin Haam, Christian Reich, Clair Blacketer, Dae-Soon Son, Seungbin Oh, and Rae Woong Park. Genomic common data model for seamless interoperation of biomedical data in clinical practice: Retrospective study. J. Med. Internet Res., 21(3):e13249, March 2019. Chaitanya Shivade, Preethi Raghavan, Eric Fosler-Lussier, Peter Embi, Noemie Elhadad, Stephen Johnson, and Albert Lai. review of approaches to identifying patient phenotype cohorts using electronic health records. J. Am. Med. Inform. Assoc., 21(2):221230, March 2014. Ali Soroush, Benjamin Glicksberg, Eyal Zimlichman, Yiftach Barash, Robert Freeman, Alexander Charney, Girish Nadkarni, and Eyal Klang. Large language models are poor medical coders benchmarking of medical code querying. NEJM AI, 1(5), April 2024. Amber Stubbs, Christopher Kotfila, and Özlem Uzuner. Automated systems for the de-identification of longitudinal clinical narratives: Overview of 2014 i2b2/UTHealth shared task track 1. J. Biomed. Inform., 58 Suppl(Suppl):S11S19, December 2015. Cong Sun, Zhihao Yang, Lei Wang, Yin Zhang, Hongfei Lin, and Jian Wang. Biomedical named entity recognition using BERT in the machine reading comprehension framework. J. Biomed. Inform., 118(103799):103799, June 2021. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353355, Stroudsburg, PA, USA, November 2018. Association for Computational Linguistics. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. SuperGLUE: stickier benchmark for general-purpose language understanding systems. Neural Inf Process Syst, abs/1905.00537:32663280, May 2019. Stephen Wu, Kirk Roberts, Surabhi Datta, Jingcheng Du, Zongcheng Ji, Yuqi Si, Sarvesh Soni, Qiong Wang, Qiang Wei, Yang Xiang, Bo Zhao, and Hua Xu. Deep learning in clinical natural language processing: methodical review. J. Am. Med. Inform. Assoc., 27(3):457470, March 2020. Urchade Zaratiana, Nadi Tomeh, Pierre Holat, and Thierry Charnois. Gliner: Generalist model for named entity recognition using bidirectional transformer, 2023. Zhen Zhang, Yuhua Zhao, Hang Gao, and Mengting Hu. LinkNER: Linking local named entity recognition models to large language models using uncertainty. In Proceedings of the ACM Web Conference 2024, pp. 40474058, New York, NY, USA, May 2024. ACM."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 DECODER MODEL EVALUATION Evaluating encoder models, such as BERT, for token classification tasks (e.g., NER) is straightforward given that these models process the entire input sequence simultaneously. This allows them to output token-level classifications by leveraging bidirectional context, facilitating direct comparison of predicted tags against the gold standard labels for each token in the input sequence. In contrast, decoder-only models, like GPT models, generate responses sequentially, predicting one token at time based on the preceding context. Evaluating the performance of these models for token classification tasks requires different approach. First, we prompt the decoder-only LLM with specific task of tagging the different entity types within given text. This task is clearly defined to the model, ensuring it understands which types of entities to identify (i.e., conditions, drugs, procedures, etc). An example of the task prompt is shown below. (cid:44) (cid:44) ## Instruction Your task is to generate an HTML version of an input text, marking up specific entities related to healthcare. The entities to be (cid:44) identified are: symptom, disorder. Use HTML <span > tags to highlight these entities. Each <span > should have class attribute indicating the type of the entity. Do NOT provide further examples and just consider the input provided below. Do NOT provide an explanation nor notes about the reasoning. Do NOT reformat nor summarize the input text. Follow the instruction and the format of the example below. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) ## Entity markup guide Use <span class='symptom' > to denote symptom. Use <span class='disorder' > to denote disorder. To ensure deterministic and consistent outputs, the temperature for generation is kept at 0.0. The model then generates sequential response that includes the tagged entities, as shown in the example below. ## Input: He had been diagnosed with osteoarthritis of the knees and had (cid:44) undergone arthroscopy years prior to admission. ## Output: He had been diagnosed with <span class=\"disease\" >osteoarthritis (cid:44) of the knees</span >and had undergone <span class=\"procedure\" >arthroscopy</span >years prior to admission. (cid:44) After the tagged output is generated, it is parsed to extract the tagged entities. The parsed data are then compared against the gold standard labels, and performance metrics are computed as above. This evaluation method ensures consistent and objective assessment of decoder-only LLMs performance in NER tasks, despite the differences in their architecture compared to encoder models. The Universal-NER decoder models series were trained on specific prompt template the same was used for these to achieve the best performance. This is shown in the example below. provided text. virtual assistant answers questions from user based on the (cid:44) USER: Text: {{text}} ASSISTANT: I've read this text. USER: What describes {{entity}} in the text? ASSISTANT:"
        },
        {
            "title": "Technical Report",
            "content": "For the GPT4o model, the above html span based prompt template was benchmarked. However to achieve better results, separate prompt inspired by the universal-ner prompt was used. The scores from this new prompt was used for GPT4o in the benchmark. The prompt used is shown below. from the input text. {%- if is_system_instruction == True -%} You are helpful medical LLM that identifies medical entities (cid:44) {%- endif -%} {%- if is_user_instruction == True -%} From given Text, find the entities that describe {{entity}} and (cid:44) Only output python list. Do not output anything else like (cid:44) For entity spans like 'breast and lung cancer',i.e, entities combined with 'and', output the whole string as single (cid:44) disease. comment or suggestion or note. return them in list of strings. (cid:44) Ouptut an empty list if there is no relevant entity. An example output is: '['entity_text_1', 'entity_text_2']' Text: {{ text }} {%- endif -%} This was then used to separately query for different entities, which were combined to get the final NER output. Details of the prompting method can be found in our opensource clinical ner benchmark codebase. A.2 COMMON TERMINOLOGY LABEL MAPPING The datasets used for the benchmark have numerous entity types. However, the entity labels for the same semantic entities vary across datasets. These entity labels are standardized across datasets using the mapping shown in 5. This mapping was derived by Referring to the guidelines used while dataset creation Randomly sampling example entity spans to understand the entity type An important aspect while evaluating models using the mapped entities is that datapoints within datasets like NCBI can also have drug entities which may not have been marked in the ground truth. Therefore, only the existing entity types within dataset should be used for evaluation. A.3 ERRORS OF TOP MODELS Figure 7 shows the confusion matrices of the top performing models and gpt-4o-mini. The predicted token counts were normalized by the number of token in ground truth(using each models tokenizer) to obtain the percentage of errors. A.4 DETAILED RESULTS We present the span and token based results of the leaderboard as of Oct 2024 in table6 and table7 respectively. These tables only contain the results on entity types, for dataset resuts, please refer to the leaderboard. Figure 8 shows the consistency of the entity, Condition, across different datasets. Table 8 shows the effect of metric type on ranking."
        },
        {
            "title": "Gene",
            "content": "Gene Variant Dropped Table 5: Mapping used to standardized dataset entities."
        },
        {
            "title": "BIORED",
            "content": "CompositeMention, DiseaseClass, Modifier, SpecificDisease"
        },
        {
            "title": "Condition",
            "content": "DiseaseOrPhenotypicFeature BC5CDR"
        },
        {
            "title": "GeneOrGeneProduct",
            "content": "SequenceVariant OrganismTaxon, CellLine"
        },
        {
            "title": "Measurement",
            "content": "Device, Mood, Temporal, Negation, Observation, Qualifier, Scope, Reference_point, Person, Value, Multiplier, Visit Figure 5: Data Distribution of Clinical Entities Figure 6: Span counts of different entities. These are the number of entity spans present in the test split of the benchmark datasets"
        },
        {
            "title": "Technical Report",
            "content": "(a) gliner-multitask-large-v0.5 (b) UniNER-7B-type-sup Figure 7: Confusion Matrices of Top Models. The numbers represent the percentage of tokens that have been classified/misclassified. (c) gpt-4o-mini"
        },
        {
            "title": "Technical Report",
            "content": "Figure 8: Models performance across the various datasets for identifying conditions. Box plots represent F1-scores of various models across the datasets for condition entities determined using the span-based approach. Table 6: Results on the Leaderboard. Span metric results of clinical entity types, from the leaderboard as of Oct 2024. Model CON. MEAS. DRUG PROC. GENE GENE V. Avg. knowledgator/gliner-multitask-large-v0.5 Universal-NER/UniNER-7B-type-sup gliner-community/gliner_large-v2.5 Universal-NER/UniNER-7B-all urchade/gliner_large_bio-v0.1 EmergentMethods/gliner_large_news-v2.1 external_services/gpt-4o-mini-2024-07-18 external_services/gpt-4o-2024-05-13 urchade/gliner_large-v2.1 numind/NuNER_Zero-span Universal-NER/UniNER-7B-type alvaroalon2/biobert_diseases_ner meta-llama/Meta-Llama-3-70B-Instruct bioformers/bioformer-8L-ncbi-disease meta-llama/Meta-Llama-3.1-8B-Instruct meta-llama/Meta-Llama-3-8B-Instruct numind/NuNER_Zero mistralai/Mixtral-8x7B-Instruct-v0. 77.05 76.92 78.25 76.39 73.83 74.24 72.50 75.99 71.00 67.88 68.38 89.14 69.17 86.05 57.09 59.05 46.10 39.30 66.89 43.69 47.60 40.74 53.62 26.84 34.12 34.51 44.93 31.56 43.57 0.00 34.13 0.00 33.38 26.35 28.83 28.72 76.00 75.13 75.74 74.10 75.01 75.00 71.00 72.74 73.37 76.33 69.18 0.00 59.05 0.00 62.63 57.09 61.28 39.92 58.84 41.30 44.67 37.26 45.30 51.06 43.94 37.73 50.00 47.94 38.10 0.00 46.98 0.00 35.91 28.16 31.69 30.97 62.23 59.88 62.08 62.20 68.38 62.80 63.64 49.05 58.32 70.54 55.55 0.00 47.65 0.00 41.65 49.46 59.22 26.45 52.99 76.72 50.20 61.96 62.74 62.30 58.85 50.08 63.55 45.40 39.39 0.00 39.42 0.00 27.10 27.36 33.90 23. 65.67 62.27 59.76 58.78 63.15 58.71 57.34 53.35 60.20 56.61 52.36 14.86 49.40 14.34 42.96 41.24 43.50 31."
        },
        {
            "title": "Technical Report",
            "content": "Table 7: Results on the Leaderboard. Token metric results of clinical entity types, from the leaderboard as of Oct 2024. Model CON. MEAS. DRUG PROC. GENE GENE V. Avg. Universal-NER/UniNER-7B-type-sup Universal-NER/UniNER-7B-all knowledgator/gliner-multitask-large-v0.5 gliner-community/gliner_large-v2.5 urchade/gliner_large_bio-v0.1 Universal-NER/UniNER-7B-type external_services/gpt-4o-2024-05-13 EmergentMethods/gliner_large_news-v2.1 urchade/gliner_large-v2.1 external_services/gpt-4o-mini-2024-07-18 numind/NuNER_Zero numind/NuNER_Zero-span meta-llama/Meta-Llama-3.1-8B-Instruct meta-llama/Meta-Llama-3-8B-Instruct meta-llama/Meta-Llama-3-70B-Instruct alvaroalon2/biobert_diseases_ner bioformers/bioformer-8L-ncbi-disease mistralai/Mixtral-8x7B-Instruct-v0.1 77.43 77.15 74.83 74.88 69.99 70.39 73.94 70.63 66.92 68.68 64.21 62.30 60.05 63.99 61.72 87.87 81.79 32. 41.85 40.65 59.86 50.68 48.08 36.09 20.87 23.84 38.42 25.55 44.15 36.62 43.15 32.43 27.82 0.00 0.00 21.40 76.81 75.99 69.68 67.75 69.93 72.45 66.80 67.26 66.20 61.47 68.73 66.48 68.99 65.42 51.95 0.00 0.00 25.80 46.36 42.14 57.21 44.03 48.28 46.81 38.86 49.58 48.55 44.46 47.10 47.33 50.49 40.71 46.17 0.00 0.00 22.22 68.00 66.74 56.67 63.57 67.42 60.91 58.72 64.07 55.93 63.15 68.37 71.95 46.91 52.20 47.36 0.00 0.00 23.10 75.59 72.22 58.73 43.96 54.23 49.10 57.45 55.59 56.00 53.34 50.90 46.46 6.28 9.25 35.15 0.00 0.00 20.46 64.34 62.48 62.83 57.48 59.66 55.96 52.77 55.16 55.34 52.78 57.24 55.19 45.98 44.00 45.03 14.65 13.63 24. Table 8: Effect of metrics on Ranking. The rank is based on average score of clinical entities score. Delta signifies the change in rank on choosing token metric over span metric. Model Architecture Type Span Rank Token Rank Delta GLiNER Encoder knowledgator/gliner-multitask-large-v0.5 Decoder Universal-NER/UniNER-7B-type-sup GLiNER Encoder gliner-community/gliner_large-v2.5 Decoder Universal-NER/UniNER-7B-all urchade/gliner_large_bio-v0.1 GLiNER Encoder EmergentMethods/gliner_large_news-v2.1 GLiNER Encoder external_services/gpt-4o-mini-2024-07-18 Decoder Decoder external_services/gpt-4o-2024-05-13 GLiNER Encoder urchade/gliner_large-v2.1 GLiNER Encoder numind/NuNER_Zero-span Decoder Universal-NER/UniNER-7B-type Encoder alvaroalon2/biobert_diseases_ner Decoder meta-llama/Meta-Llama-3-70B-Instruct Encoder bioformers/bioformer-8L-ncbi-disease Decoder meta-llama/Meta-Llama-3.1-8B-Instruct Decoder meta-llama/Meta-Llama-3-8B-Instruct GLiNER Encoder numind/NuNER_Zero Decoder mistralai/Mixtral-8x7B-Instruct-v0.1 zero-shot fine-tuned zero-shot fine-tuned zero-shot zero-shot zero-shot zero-shot zero-shot zero-shot zero-shot fine-tuned zero-shot fine-tuned zero-shot zero-shot zero-shot zero-shot 1 3 5 6 2 7 8 10 4 9 11 17 12 18 14 15 13 16 2 1 5 3 4 10 11 12 8 9 7 17 14 18 13 15 6 16 -1 2 0 3 -2 -3 -3 -2 -4 0 4 0 -2 0 1 0 7"
        }
    ],
    "affiliations": [
        "M42 Abu Dhabi, UAE"
    ]
}