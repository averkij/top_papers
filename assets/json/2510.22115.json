{
    "paper_title": "Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language Foundation",
    "authors": [
        "Ling Team",
        "Ang Li",
        "Ben Liu",
        "Binbin Hu",
        "Bing Li",
        "Bingwei Zeng",
        "Borui Ye",
        "Caizhi Tang",
        "Changxin Tian",
        "Chao Huang",
        "Chao Zhang",
        "Chen Qian",
        "Chenchen Ju",
        "Chenchen Li",
        "Chengfu Tang",
        "Chilin Fu",
        "Chunshao Ren",
        "Chunwei Wu",
        "Cong Zhang",
        "Cunyin Peng",
        "Dafeng Xu",
        "Daixin Wang",
        "Dalong Zhang",
        "Dingnan Jin",
        "Dingyuan Zhu",
        "Dongke Hu",
        "Fangzheng Zhao",
        "Feifan Wu",
        "Feng Zhu",
        "Gangshan Wang",
        "Haitao Zhang",
        "Hailin Zhao",
        "Hanxiao Zhang",
        "Hanzi Wang",
        "Hao Qian",
        "Haoyi Yu",
        "Heng Zhang",
        "Hongliang Zhang",
        "Hongzhi Luan",
        "Huirong Dong",
        "Huizhong Li",
        "Jia Li",
        "Jia Liu",
        "Jialong Zhu",
        "Jian Sha",
        "Jianping Wei",
        "Jiaolong Yang",
        "Jieyue Ma",
        "Jiewei Wu",
        "Jinjing Huang",
        "Jingyun Tian",
        "Jingyuan Zhang",
        "Jinquan Sun",
        "Juanhui Tu",
        "Jun Liu",
        "Jun Xu",
        "Jun Zhou",
        "Junjie Ou",
        "Junpeng Fang",
        "Kaihong Zhang",
        "Kaiqin Hu",
        "Ke Shi",
        "Kun Tang",
        "Kunlong Chen",
        "Lanyin Mei",
        "Lei Liang",
        "Lei Xu",
        "Libo Zhang",
        "Lin Ju",
        "Lin Yuan",
        "Ling Zhong",
        "Lintao Ma",
        "Lu Liu",
        "Lu Yu",
        "Lun Cai",
        "Meiqi Zhu",
        "Mengying Li",
        "Min Chen",
        "Minghao Xue",
        "Minghong Cai",
        "Mingming Yin",
        "Peijie Jiang",
        "Peilong Zhao",
        "Pingping Liu",
        "Qian Zhao",
        "Qing Cui",
        "Qingxiang Huang",
        "Qingyuan Yang",
        "Quankun Yu",
        "Shaowei Wei",
        "Shijie Lian",
        "Shoujian Zheng",
        "Shun Song",
        "Shungen Zhang",
        "Shuo Zhang",
        "Siyuan Li",
        "Song Liu",
        "Ting Guo",
        "Tong Zhao",
        "Wanli Gu",
        "Weichang Wu",
        "Weiguang Han",
        "Wenjing Fang",
        "Wubin Wang",
        "Xiang Shu",
        "Xiao Shi",
        "Xiaoshun Lan",
        "Xiaolu Zhang",
        "Xiaqing Sun",
        "Xin Zhao",
        "Xingyu Lu",
        "Xiong Xu",
        "Xudong Wang",
        "Xudong Wang",
        "Xuemin Yang",
        "Yajie Yang",
        "Yang Xiang",
        "Yanzhe Li",
        "Yi Zhang",
        "Yilong Wang",
        "Yingxue Li",
        "Yongzhen Guo",
        "Yuzhuo Fu",
        "Yuanyuan Wang",
        "Yue Yang",
        "Yue Yu",
        "Yufeng Deng",
        "Yun Zhang",
        "Yunfei Yu",
        "Yuqi Zhang",
        "Yuxiao He",
        "Zengke Gui",
        "Zhaoxin Huan",
        "Zhaoyang Wang",
        "Zhibo Zhu",
        "Zhihao Wang",
        "Zhiqiang Zhang",
        "Zhoufei Wang",
        "Zihang Zeng",
        "Ziqi Liu",
        "Zitao Xuan",
        "Zuoli Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Ling 2.0, a series reasoning-oriented language foundation built upon the principle that every activation boosts reasoning capability. Designed to scale from tens of billions to one trillion parameters under a unified Mixture-of-Experts (MoE) paradigm, Ling 2.0 emphasizes high sparsity, cross-scale consistency, and efficiency guided by empirical scaling laws. The series includes three non-thinking (instruct) models - Ling-mini-2.0, Ling-flash-2.0, and Ling-1T - ranging from 16B to 1T total parameters and achieving up to 7-fold active-compute efficiency compared with dense counterparts. Ling 2.0 integrates coordinated innovations across model architecture, pre-training, post-training, and infrastructure: a high-sparsity MoE with MTP for efficient reasoning, reasoning-oriented data and mid-training CoT activation, reinforcement-based fine-tuning (DFT, Evo-CoT), and full-scale FP8 training with fine-grained heterogeneous pipelines. At the trillion scale, Ling-1T establishes a new Pareto frontier of reasoning accuracy versus computational efficiency, demonstrating that sparse activation, when properly aligned with reasoning objectives, enables scalable and efficient intelligence. Collectively, Ling 2.0 provides a coherent, open, and efficient foundation for advancing future reasoning and thinking models, including the Ring series built upon the same base."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 2 5 1 1 2 2 . 0 1 5 2 : r Ling 2.0 Technical Report Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language Foundation Ling Team, Inclusion AI See Contributions section (Sec. 7) for full author list. We introduce Ling 2.0, series reasoning-oriented language foundation built upon the principle that every activation boosts reasoning capability. Designed to scale from tens of billions to one trillion parameters under unified Mixture-of-Experts (MoE) paradigm, Ling 2.0 emphasizes high sparsity, cross-scale consistency, and efficiency guided by empirical scaling laws. The series includes three non-thinking (instruct) modelsLing-mini-2.0, Ling-flash-2.0, and Ling-1Tranging from 16B to 1T total parameters and achieving up to 7 active-compute efficiency compared with dense counterparts. Ling 2.0 integrates coordinated innovations across model architecture, pre-training, post-training, and infrastructure: high-sparsity MoE with MTP for efficient reasoning, reasoning-oriented data and midtraining CoT activation, reinforcement-based fine-tuning (DFT, Evo-CoT), and full-scale FP8 training with fine-grained heterogeneous pipelines. At the trillion scale, Ling-1T establishes new Pareto frontier of reasoning accuracy versus computational efficiency, demonstrating that sparse activation, when properly aligned with reasoning objectives, enables scalable and efficient intelligence. Collectively, Ling 2.0 provides coherent, open, and efficient foundation for advancing future reasoning and thinking models, including the Ring series built upon the same base. Date: Oct 24, 2025 Code: https://github.com/inclusionAI/Ling-V2 Model: https://huggingface.co/collections/inclusionAI/ling-v"
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) such as GPT-5 (OpenAI, 2025), Gemini-2.5 (Comanici et al., 2025), Qwen-3 (Yang et al., 2025a), and DeepSeek-V3 (DeepSeek-AI, 2024) have evolved into the core infrastructure of modern AI. Yet as scaling reaches hundreds of billions of parameters, performance gains increasingly depend on models ability to reasonto decompose problems, infer hidden relations, and make consistent multi-step deductions. We believe that reasoning capability is the essence of intelligence and the foundation for building general-purpose agents that can understand, decide, and act autonomously. Recent open models highlight this trend. Kimi-K2 (Moonshot-AI, 2025), an open trillion-scale model, focuses primarily on enhancing agentic capability, while DeepSeek-V3 (DeepSeek-AI, 2024), though smaller at 671B parameters, achieves outstanding reasoning performance under efficient sparse scaling. Ling 2.0 is designed to push beyond: scaling trillion-parameter reasoning-oriented foundation model that maximizes reasoning accuracy and efficiency under sparse activation, establishing scalable blueprint for next-generation open intelligent systems. 1 Scaling general reasoning capability to the trillion-parameter level is central challenge in the evolution of LLMs. The key difficulty lies in achieving both efficient scalingmaintaining computational efficiency, stability, and predictability under extreme scaleand reasoning enhancementensuring that expanded capacity leads to more consistent and reliable reasoning. From the scaling perspective, dense architectures incur prohibitive cost, motivating high-sparsity designs that preserve expressiveness while reducing computation. Reliable scaling prediction becomes essential to anticipate trillion-scale performance (beyond 1e25 FLOPs) from smaller-scale experiments. In addition, effective algorithm-infrastructure co-design is required to align precision, parallelism, and communication for efficient large-scale execution. From the reasoning perspective, maintaining improvement across pre-training, mid-training, and post-training remains difficult. Constructing reasoning-centric corpora is resource-intensive, while transferring learned reasoning behaviors across these stages can introduce instability. Achieving sustained progress thus requires innovations in both data and training pipeline to balance reasoning accuracy and efficiency. To address the intertwined challenges of efficient scaling and sustained reasoning enhancement, Ling 2.0 introduces systematic innovations across four dimensions: model architecture, pre-training, post-training, and infrastructure. Model Architecture. Ling Scaling Laws. Our unified Ling Scaling Laws, derived from over thousand experiments, guide the hyperparameter and architectural design for trillion-parameter models, ensuring stable and near-optimal training. Crucially, the framework establishes wind tunnel for low-cost, high-fidelity extrapolation from small-scale trials to trillion-parameter models, cutting validation costs to under 1% of full training run and greatly accelerating innovation cycle. High-Sparsity MoE with MTP. Ling 2.0 scales our high-sparsity, fine-grained architecture from 16B to 1T parameters. All models use 256 routed experts, activating 8 experts plus one shared expert per token ( 3.5% activation), realizing 7 efficiency leverage per the Ling Scaling Law. With aux-loss-free load balancing and MTP, Ling 2.0 maintains high training efficiency while improving logical reasoning, leading to significant math and coding performance gains. Pre-Training. Reasoning-oriented Data Composition. Our pre-training corpus prioritizes the Ling Math and Ling Code datasets, which are tailored for mathematical reasoning and code generation, respectively, yielding 5-8% average gain on reasoning benchmarks. Throughout the 20Ttoken pre-training process, we progressively increase the proportion of reasoning data from 32% to 46%, establishing Ling 2.0s inherent reasoning strengths. Reasoning Pre-Activation in Mid-Training. In the mid-training phase, we extend the effective context window and introduce Chain-of-Thought (CoT) data to pre-activate reasoning abilities. This strategy raises the ceiling on reasoning performance, and provides more stable foundation for subsequent fine-tuning and reinforcement learning (RL). Warmup-Stable-Merge (WSM) Scheduler. To enable more flexible and effective pre-training process, the Ling 2.0 series adopts the novel WSM (warmup-stable-merge) scheduler, which replaces learning-rate decay with checkpoint merging and delivers 1-2% average gains across benchmarks. Notably, this advantage persists through subsequent post-training stages. 2 Post-Training. DFT Initialization with Progressive Reasoning Evolution. Through Decoupled Fine-Tuning (DFT) with differentiated system prompts, we establish diverse, reasoning-focused initialization. Building on this foundation, the Evolutionary Chain-of-Thought (Evo-CoT) paradigm progressively deepens reasoning capabilitiesenabling Ling 2.0 to surpass state-of-the-art models on competition-level mathematical reasoning benchmark, while requiring 25% fewer training tokens to reach comparable or better performance. Sentence-Level Policy Optimization. Introduces Linguistic-unit Policy Optimization (LPO), treating sentences as the fundamental action units for RL updates. This fine-grained optimization strategy shows higher training stability and delivers around 10% improvements on complex reasoning benchmarks compared to token-level and sequence-level baselines. Group-Based Human Preference Alignment. The Group Arena Reward (GAR) mechanism ensures precise intra-group preference alignment in RLHF, better reflecting nuanced human judgments, yielding 2-10% higher consistency scores in open-ended evaluations. Infrastructure. Full-scale FP8 training. Ling 2.0 represents the largest open-source model trained entirely in FP8 precision. Fine-grained quantization (activations/gradients [1,128]; weights [128,128]) achieves near-lossless accuracy ( 0.25 % gap to BF16 after 900 tokens) while improving utilization and reducing memory use by over 15 %. Heterogeneous fine-grained pipeline. Interleaved 1F1B scheduling with partial recomputation mitigates pipeline bubbles from heterogeneous modules such as MTP and First-K-Dense, improving throughput by around 40 %. Software Engineering for Foundation LLMs. Guiding software-engineering-oriented LLMs framework with the 4C (Correct, Consistent, Complete, and Co-Design) principle, incorporating efficient automated iteration, algorithm-system co-design and cross-platform reproducibility to jointly ensures robust trillion-scale development. Based on the above innovations, we release three models of different scales in the Ling 2.0 family: Ling-mini-2.0: 16B total parameters with 1.4B activated. Ling-flash-2.0: 103B total parameters with 6.1B activated. Ling-1T: 1 trillion total parameters with 51B activated. Ling 2.0 is comprehensively evaluated across wide range of benchmarks spanning mathematics, coding, reasoning, knowledge, alignment, and agentic tasks. The results exhibit consistent scaling trajectory: as model capacity expands from Ling-mini-2.0 to Ling-flash-2.0 and Ling-1T, performance across all tasks improve steadily in accordance with the Ling Scaling Law. At smaller scales, Ling-mini-2.0 achieves performance on par with or exceeding dense models below 10B parameters, while Ling-flash-2.0 matches or surpasses dense models below 40B. These findings confirm that Ling 2.0 provides an approximate 7 efficiency leverage, delivering dense-level capability with substantially lower active computation. At the trillion-parameter scale, Ling-1T establishes new Pareto frontier of reasoning accuracy versus efficiency, demonstrating efficient thinking and precise reasoning on competition-level benchmarks such as AIME 2025. Collectively, these results validate that Ling 2.0 effectively scales reasoning capability with both architectural efficiency and algorithmic alignment, advancing the frontier of open-source language foundation models. This report focuses on three reflex-grade non-thinking (instruct) models in the Ling 2.0 familyLing-mini-2.0, Ling-flash-2.0, and Ling-1T. These models emphasize general reasoning and instruction-following capability, while the Ring series (Ling-Team, 2025), built upon the same Ling 2.0 base, extends toward deep thinking models. The remainder of this report introduces the core model architecture, pre-training and post-training methodology, as well as the infrastructure optimizations of Ling 2.0."
        },
        {
            "title": "2 Architecture",
            "content": "To maximize performance within constrained resources, Ling 2.0 series uniformly adopts MoE architecture (Shazeer et al., 2017; DeepSeek-AI, 2024). It integrates aux-loss-free load balancing strategy (DeepSeek-AI, 2024) and Multi-Token Prediction (MTP) (Gloeckle et al., 2024; DeepSeek-AI, 2024) to optimize the training process. Furthermore, our architectural decisions are grounded in systematic scaling law experiments (Tian et al., 2025a) that verify the reliable extrapolation of key architectural details, thus enabling efficient architecture iteration and principled design choices."
        },
        {
            "title": "2.1 Basic Architecture",
            "content": "The Ling 2.0 series comprises three MoE models of varying scales: Ling-mini-2.0, Ling-flash-2.0, and Ling-1T, covering total parameter counts from 16B up to 1T. Key architectural details of the models are summarized in Table 1. Ling 2.0 models adopt unified high-sparsity, fine-grained design: each model is configured with 256 routed experts, activates 8 experts plus 1 shared expert, yielding an overall activation ratio of approximately 3.5%. Our scaling laws analysis (Tian et al., 2025a) indicates that continuously increasing sparsity yields significant performance gains (Moonshot-AI, 2025). Concurrently, the fine-grained setting of activating 8 experts presents superior balance between training speed and model performance, while the inclusion of one shared expert was identified as an optimal design heuristic through our extensive experiments. Additionally, we designate the initial 1, 1, and 4 layers of the three models, respectively, as dense layers. This approach reduces the total parameter count while maintaining equivalent model performance and improving routing balance. In the attention layers, Ling 2.0 models employ standard grouped-query attention (GQA) (Ainslie et al., 2023) with 8, 16, or 32 key-value heads to reduce KV cache size during decoding; it employs SwiGLU and RMSNorm with pre-normalization to improve representational efficiency and stability. We further introduce QKNorm (Henry et al., 2020) to enhance training robustness, which we verify to significantly improve stability under low-precision training. Furthermore, we implement Partial RoPE (Su et al., 2024), applying rotary position embeddings only to the first 64 dimensions of the attention heads, to bolster the models length extrapolation capabilities. Ling 2.0 extends the Ling 1.5 vocabulary and uses byte-level byte-pair encoding, BBPE (Shibata et al., 1999; Sennrich et al., 2015), with 156K token vocabulary to enhance multilingual performance."
        },
        {
            "title": "2.2 Model Optimization",
            "content": "To further improve the training efficiency and final performance of Ling 2.0, we incorporate the aux-loss-free load balancing strategy and Multi-Token Prediction (MTP). 4 Table 1 Key architectural configurations and training hyperparameters of the Ling 2.0 series. Ling-mini-2.0 Ling-flash-2.0 Ling-1T # Layers # Experts (total) # Experts Active per Token # Shared Experts # Attention Heads # Dense Layers Hidden Size Intermediate Size Expert Intermediate Size Total Parameters (B) Activated Parameters (B) Learning Rate Batch Size 20 256 8 1 16 1 2,048 5,120 512 16 1.4 3.36 104 4,400 32 256 8 1 32 1 4,096 9,216 1,024 103 6.1 2.61 104 8,352 80 256 8 1 64 4 8,192 18,432 2,048 1000 51.0 1.86 104 18,144 Load Balancing Strategy. Based on systematic experiments, Ling 2.0s routing balance strategy follows design similar to DeepSeek-V3 (DeepSeek-AI, 2024). We choose an aux-loss-free balance strategy to jointly encourage expert specialization and load balancing, and we apply router gate scaling to improve training stability. The scaling factor is set to 2.5 to stabilize the root mean square of the gate outputs. We slightly modify the bias update strategy, keeping the bias centered around zero (Liu et al., 2025a). Concretely, the aux-free bias is updated as: bi = bi + (cid:0)sign(ei) mean(sign(e))(cid:1), where is the update rate, bi is the bias of the i-th expert, and ei is that experts violation error. In addition, we adopt dropless routing strategy to ensure model performance, alongside group routing to improve training efficiency without any performance degradation. Multi Token Prediction. To enhance model performance and inference efficiency, Ling 2.0 natively integrates MTP (Gloeckle et al., 2024; DeepSeek-AI, 2024) as an auxiliary training objective. Through rigorous validation of its effectiveness and extrapolability, we found that MTP consistently improves performance on code and math tasks across different model scales. Considering the scaling trends of MTP hyperparameters and training efficiency across various model sizes, we introduce one MTP layer for each model scale and set the MTP loss weight to 0.1. To address the additional computational overhead introduced by MTP, we performed detailed performance analysis and implemented fine-grained Pipeline Parallelism (PP) partitioning for the MTP module within the Megatron training framework. This optimization significantly mitigates the performance overhead from MTP, ensuring high training throughput (see Section 5 for details)."
        },
        {
            "title": "2.3 Ling Scaling Laws",
            "content": "Ling 2.0 series was conceived from the outset with the long-term goal of training trillion-parameter foundation models. To this end, we establish the Ling Scaling Laws (Tian et al., 2025a) to guide hyperparameter and architecture choices. The framework also provides the foundation for standardized experimental pipeline, ensuring reliable extrapolation of findings to computational scales over 100x larger. Specifically, the Ling Scaling Laws serve two critical functions: Principled Design for Trillion-Parameter Models: The laws determine the hyperparameters and architectural settings for Ling 2.0, ensuring near-optimal architectural efficiency. Efficient Innovation at Minimal Cost: standardized pipeline are provided to validate novel ideas and emerging technologies for Ling 2.0 at just 1% of the full training compute cost. 5 (a) Scaling laws for optimal hyperparameters (b) Scaling laws for optimal model-data allocation Figure 1 Scaling laws for optimal hyperparameters and optimal model-data allocation. Blue and red lines represent the fitted laws for MoE and dense models, respectively, derived on the same training dataset. Gray circles are the experimental data points used for fitting."
        },
        {
            "title": "2.3.1 Scaling Laws for Optimal Hyper-parameters",
            "content": "To ensure that the Ling 2.0 series can be trained stably under appropriate hyperparameters, we first derived scaling laws for optimal MoE hyperparameters. Previous studies (Bi et al., 2024; Ling-Team et al., 2025) has shown that the optimal learning rate (η) and batch size (B) are primarily determined by the total compute budget (C). Accordingly, we conducted hyperparameter searches over nearly thousand experiments across compute scales up to 3e20 FLOPs, using WarmupStableDecay (WSD) scheduler (Hu et al., 2024). To simplify analysis, we initially fixed the MoE architecture to 64 experts (4 active) plus 1 shared expert. After removing outliers, we selected optimal and near-optimal1 configurations for fitting. From these data, we fit power-law relationships between compute and the optimal batch size Bopt and learning rate ηopt, and verified that the resulting laws remain near-optimal under different activation ratios. The fitting process and fitted parameters is shown in Figure 1a. Our analysis reveals key difference between MoE and dense models in hyperparameter selection: at larger compute scales, MoEs tend to use larger batch sizes and relatively lower learning rate. We attribute this phenomenon to MoEs sparse gradient updates: since only subset of tokens in each batch contributes to the gradient update for any given expert, larger batch size is necessary to ensure stable and effective training. These validated scaling laws provided reliable foundation, enabling the efficient training of the Ling 2.0 models with near-optimal hyperparameters. Furthermore, to gain deeper insight into the differing training dynamics of MoE and dense models, we analyzed the optimal allocation for training data (D) and model parameters (M, i.e., FLOPs per token) under different compute budgets (C = D). As shown in Figure 1b, our findings indicate that for any given compute budget, the optimal MoE model has fewer parameters (Mopt) but is trained on more data (Dopt) compared to its optimal dense counterpart. This conclusion suggests that MoE architectures possess larger effective capacity, enabling them to efficiently process more training data with fewer parameters, which offers significant efficiency advantage in real-world scenarios where data is abundant but computational resources are limited."
        },
        {
            "title": "2.3.2 Scaling Laws for MoE Architectural Efficiency",
            "content": "To guide the architectural design of the Ling 2.0, we systematically derived scaling laws for MoE architectural efficiency. We introduce efficiency leverage (EL) as our primary metric, defined as the ratio of computational cost required for dense model to that of an MoE model to reach an 1near-optimal is defined as configurations whose loss is within 0.25% of the minimum at given compute budget. 6 (a) IsoFLOPs curves for varying activation ratio and expert granularity. (b) Estimated efficiency leverage (EL) Figure 2 Impact of the MoE architectural configuration on loss and efficiency leverage (EL). equivalent performance level (e.g., identical validation loss). Our investigation systematically analyzes the influence of key architectural dimensions on EL, including the expert activation ratio, expert granularity, the proportion of shared experts, and others. We then integrate the empirical findings into unified scaling law that predicts EL as function of the MoE configuration, offering practical framework for designing efficient MoEs. This large-scale empirical study, based on over 300 models with up to 28B parameters, reveals several core principles governing MoE efficiency: 1. Activation ratio is the primary driver of efficiency. EL is predominantly determined by the expert activation ratio, following robust power law: efficiency gains increase as sparsity increases (i.e., as the activation ratio decreases). Illustrated in Figure 2a (left), this relationship remains consistent and quantifiable even at extremely low activation ratios, such as 1/128. 2. Expert granularity acts as nonlinear modulator. Beyond the dominant activation effect, expert granularity induces log-polynomial adjustment to EL that is largely independent of the total compute budget, implying stable optimal range for the number of activated experts. Our experiments identify this optimal range as 812, as shown in Figure 2a (right). 3. Compute budget has an amplification effect. Crucially, EL for given MoE architecture is not fixed; it scales with the training compute budget following another power law. This highlights the substantial potential of MoE in large-scale pretraining: as compute investment increases, the efficiency advantage becomes increasingly pronounced. 4. Other architectural factors have secondary effects. Factors such as the arrangement of shared expert or MoE layers have relatively minor effects. These factors typically admit broadly applicable, near-optimal settings and do not require fine-grained tuning across scenarios. Combining these insights, we derive unified EL scaling law that integrates the effects of the compute budget (C), activation ratio (A), and expert granularity (G): EL(A, G, C) = ˆAα+γ(log G)2+β log G, (1) where ˆA is saturating transformation of the activation ratio A, as defined in Clark et al. (2022). The exponent α = + log models the compute-dependent scaling. Here, > 0 quantifies the amplification of EL at larger compute scales, while represents the baseline scaling exponent. The parameters β and γ define the log-polynomial modulation from expert granularity G, capturing the observed optimal range. We fit Eq. 1 using Huber loss and BFGS optimization (Hoffmann et al., 2022), and experimentally validated the scaling on Ling-mini-2.0. As an example, Figure 2b presents the predicted EL landscape at 1e22 FLOPs, highlighting the optimal architectural region. 7 (a) Comparison Ling wind tunnel experiments and traditional experimental setting. (b) Loss scaling curves derived by Ling wind tunnel experiments. Figure 3 Illustration of the Ling Wind Tunnels experimental design (a) and an example analysis (b). Based on these results, all Ling 2.0 models adopt high-sparsity, fine-granularity design: 256 routing experts with 8 activated per token plus one shared expert, yielding 3.5% overall activation ratio. The Ling scaling law predicts over 7 efficiency leverage for this architectural configuration, which we empirically confirm on the Ling 2.0 series."
        },
        {
            "title": "2.3.3 Ling Wind Tunnel Experiments for Efficient Innovation",
            "content": "The Ling Scaling Laws not only dictate the specific training and architectural parameters but, more importantly, guide the experimental and iterative paradigm of the Ling project with longtermism. To facilitate efficient innovation at minimal cost, we design the Ling Wind Tunnel Experiments system based on these scaling laws. As depicted by the green points in Figure 3a, this system comprises five experiments with models ranging from 500M to 8B parameters, whose sizes are distributed according to power law. The entire experimental process is highly standardized: 1) Model Architecture: The specific architecture and size of each model are determined by the scaling Laws for MoE architectural efficiency in Secation 2.3.2. 2) Training Resources: Each model is trained to FLOPs count corresponding to its optimal compute allocation. The specific number of training tokens are determined by the scaling laws for optimal model-data allocation (Section 2.3.1, Figure 1b). 3) Training Hyperparameters: The core training hyperparameters (i.e., learning rate and batch size) are set according to the target FLOPs, based on the scaling laws for optimal hyperparameters (Section 2.3.1, Figure 1a). Our experiments demonstrate that by strictly adhering to these scaling laws for hyperparameters and data allocation, we can reduce training uncertainty and accurately predict the final training loss to within an error of 0.01. This allows the Ling Wind Tunnel system to provide automated and standardized experimental judgments, enabling us to fairly evaluate the scaling capability of any given feature. As an example shown in Figure 3b, the wind tunnel results clearly illustrate the loss difference of candidate feature relative to the baseline across various compute budget. This provided the empirical evidence for our decisions in training the 1T foundation model. Consequently, we employ this system to identify design elements that perform well at massive scales and then extrapolate these findings 100x to guide the design of Ling-1T. Compared to traditional ablation studies (e.g., training single Ling-mini-2.0 model on 400B tokens, shown as the black point in Figure 3a), the Ling Wind Tunnel is more cost-effective. Despite involving more individual runs, its overall computational cost is merely 35% of the traditional method. More importantly, it enables us to precisely assess the scaling potential of technology. The 8 conclusions drawn from these multi-scale observations are significantly more stable and reliable than those derived from single experimental slice. This methodology profoundly reflects our design philosophy for developing trillion-scale foundation models."
        },
        {
            "title": "3 Pre-training",
            "content": "In this section, we will present two key components of pre-training: data and recipe, separately."
        },
        {
            "title": "3.1 Pre-training Data",
            "content": "During the preparation of the pre-training data for Ling 2.0 models, we primarily focus on building an efficient data processing infrastructure and curating corpus that broadly covers high-quality universal data including but not limited general knowledge, code, math, multilingual content etc."
        },
        {
            "title": "3.1.1 General Knowledge Data",
            "content": "Data Cleaning from Raw Sources. LLMs gain general knowledge from large, diverse datasets like web pages, books, papers, and Wikipedia (Soldaini et al., 2024), which often suffer quality issues. We created specialized cleaning pipelines combining rules and models tailored per data type. For web data, we extract content using the trafilatura parser2 and apply sampling-based checks to identify common low-quality patterns. Targeted cleaning removes ads, embedded URLs, symbol-heavy texts, fixes Markdown and table parsing. HTML/PDF parsers are continuously improved to enhance extraction accuracy. Detection and Remediation of New Low-Quality Data. Iterative sampling reveals new lowquality data, addressed with an automated detection and rule-generation pipeline involving: 1) Multi-channel Recall: Using classifiers, lightweight LLM scoring, and perplexity (PPL) to flag suspect samples; 2) Issue Analysis: LLMs categorize issues as known or new rule cases; 3) Rule Generation: LLMs create cleaning rules based on issue context and quality-issue database; 4) Rule Generalization: Grouping similar cases for LLM-driven abstraction to broaden rule applicability. New rules undergo human review before integration, speeding detection and remediation. High-Quality Filtering and Knowledge Text Rewriting. Despite cleaning, datasets remain massive. To improve training, we develop High-Quality Filtering pipeline. Inspired by FineWeb-Edu (Penedo et al., 2024), we train feature models by data type (e.g., Chinese/English web, books, papers) to assess quality, education level, knowledge density, and domain. Iterative experiments identify optimal subsets; for instance, our English web subset is 5 larger than FineWeb-Edu and outperforms it on knowledge benchmarks. Models struggle with complex or rare knowledge in raw text. We use recall-rewrite: (i) select candidate texts by knowledge density, STEM domain, and QA features; (ii) apply semi-synthetic rewriting like Wikipedia-style structure, QA conversion, and concise summaries. Ablation experiments show consistent gains on MMLU (Hendrycks et al., 2021a), CMMLU (Li et al., 2024a), and CEval (Huang et al., 2023) benchmarks."
        },
        {
            "title": "3.1.2 Reasoning Data",
            "content": "We aim to endow Ling 2.0 with powerful general reasoning capabilities, which primarily encompass programming and mathematical skills. To this end, we optimize our reasoning data from multiple perspectives, including scale, diversity, and quality. 2https://trafilatura.readthedocs.io/en/latest/ 9 (a) Overall performance of Ling Code Corpus on 1B models. (b) Overall performance of Ling Math Corpus on 1B models. (c) Comparison of Ling Math Corpus with open-source mathematical web data. Figure 4 Performance of Ling Code Corpus (a) and Ling Math Corpus (b, c)."
        },
        {
            "title": "3.1.2.1 Ling Code Corpus\nTo support the training of high-performance coding-oriented LLMs, we constructed a diverse,\nlarge-scale, and quality-stratified Ling Code Corpus that integrates multiple data sources, covering\nsource code, code-related natural language data, and synthetic instructional data. Our curation\npipeline emphasizes both breadth of programming language and domain coverage, and the depth\nof quality control.",
            "content": "We collected raw source code from Github repositories. We use multilingual fine-grained cleaning rules tailored to the syntax and conventions of each language. We apply Lint-based3 syntactic validation to remove files with compilation or structural errors. This yields our source code corpus covering 660 programming languages. We further conduct 1) quality stratification according to code style/readability, norm adherence, and complexity/difficulty; 2) code rephrasing and paraphrasing techniques, to generate additional high quality augmented code data. In addition to github repositories, we 1) reconstructed commit data from GHArchive4 by replaying event sequences (e.g., pull requests, issues, merges) at the repository level; 2) iteratively optimize our code-oriented htmlparsers and cleaning operators to curate code-related pages, tutorials, developers discussions from Common Crawl and Web; 3) curated large collection of programming-competition data consist of problem statements from diverse platforms, user submissions, and related user discussions and commentary threads. Evaluating the Ling Code Corpus. We designed lightweight verification strategy, i.e., training small-sized coding models (e.g., 1B size) from scratch to measure the performance of our code data. Experiments show that from-scratch training on single-type code data provides reliable proxy for full-scale performance. This finding enables efficient early-stage validation of architecture and training recipes before scaling to tens or hundreds of billions of parameters. We show our results on 1B models (Ling-coder-1B) compared with Qwen2.5-Coder-1.5B-Base (Hui et al., 2024) and Qwen3-1.7B-Base (Yang et al., 2025a) in Figure 4a. The results are promising that we have equivalent or even better results on mainstream benchmarks compared with Qwen2.5-Coder-1.5B-Base. This is achieved by consuming only 2T tokens of our code data from scratch, with an additional 300B anealing phase. More details can be found in Appendix B.1.1 3https://en.wikipedia.org/wiki/Lint_(software) 4https://www.gharchive.org/"
        },
        {
            "title": "3.1.2.2 Ling Math Corpus\nTo train Ling 2.0 models of varying scales, we assembled a mathematics corpus drawn from web\npages, textbooks, research papers, code repositories, problem banks, and synthetic sources. A multi-\nstage processing pipeline—comprising parsing, recall, filtering, rewriting, and synthesis—was\ndesigned to curate this corpus.",
            "content": "We iteratively improved the PDF and HTML parser to ensure the completeness of mathematical content. We build fastText classifiers to recall math data from huge candidate pool. We then fine-tune small language models to develop LLM-Filter and LLM-Refiner that can filter and refine data that contain mathematical knowledge or step-by-step problem solving process. In addition, we employ synthetic data generation to create diverse range of mathematical question-answer (Q&A) pairs, varying in difficulty and incorporating step-by-step reasoning processes. This includes 1) Q&A pairs extraction from web and book; 2) development of sophisticated question generator for high quality and realistic mathematical problems; 3) the build of large-scale mathematical concept graph (Chen et al., 2025) to extend the knowledge boundaries of our model. Evaluating the Ling Math Corpus. To empirically validate the efficacy of our mathematical corpus, we use continual-training then annealing strategy with only math corpus on pre-trained Ling-coder-1B model introduced in Section 3.1.2.1 for over 1.8T tokens, in which the last 300B is used for annealing training. Due to the space limit, we only present the performance results on the average value of benchmarks. As shown in Figure 4b, the resulting Ling-math-1B model exhibited performance superior to the competitive Qwen2.5-Math-1.5B-Base (Yang et al., 2024b) and Qwen31.7B-Base (Yang et al., 2025a) on mainstream mathematical benchmarks (e.g. GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), CollegeMath (Tang et al., 2024), OlympiadBench (He et al., 2024a), CMATH (Wei et al., 2023), MathBench (Liu et al., 2024) etc.). Furthermore, specific comparative analysis was conducted to evaluate the contribution of our curated mathematical web data. Using the same 1B-model training paradigm, we benchmarked our proprietary web data against suite of well-regarded open-source datasets, namely Infi-mmmath (Han et al., 2024), finemath-3plus (Allal et al., 2025), megamath (Zhou et al., 2025), and nemotron-cc (Mahabadi et al., 2025). The Ling-math-web-1B model trained on our web data demonstrated markedly superior performance shown in Figure 4c. This finding validates the effectiveness of our specialized web data acquisition and refinement pipeline, critical factor contributing to the high quality of our pre-training data (detailed in Appendix B.1.2)."
        },
        {
            "title": "3.1.3 Multilingual Data",
            "content": "To enhance multilingual capabilities, we expand the tokenizer vocabulary from 128K in Ling 1.5 to 156K, with targeted additions of multilingual tokens. For multilingual corpus, we curate approximately 2TB of high-quality multilingual data from open web sources and parallel corpora. The data undergoes rigorous preprocessing, including language identification, filtering, cleaning, and deduplication, to ensure linguistic diversity and data integrity. The corpus spans broad range of about 30 languages and diverse domains, including web text, code, mathematics, Wikipedia, and parallel sentence pairs, supporting robust cross-lingual understanding. Furthermore, multilingual data constitutes 4% of the total pre-training data. Through experimentation, we determined an optimal distribution that significantly improves minor language performance while maintaining Chinese and English capabilities. Our findings indicate that data from Romance and Germanic languages have less negative impact on core languages, whereas data from certain other language families requires more careful balancing. More details can be found in Appendix B.2."
        },
        {
            "title": "3.1.4 Long-Context Data",
            "content": "To build long-context ability we implement retrievesynthesizevalidate pipeline over heterogeneous sources (web pages, books/novels, scientific articles, software docs, etc.). Quality controls include: Linguistic hygiene: Combination of rule checking and model recognition to identify and repair issues such as paragraph duplication, language mixing, and content truncation. Semantic consistency checks: Using model-aided detection and small amount of manual observation to detect logical contradictions within the text to filter data, and optimize relevant recall/synthesis logic. Long-range quality scoring: We eliminate low-quality long text content using the PPL gap between longand short-window evaluations, combined with auxiliary scores. This pipeline yields 1.2 high-quality long-text tokens."
        },
        {
            "title": "3.1.5 Data Infrastructure",
            "content": "Training large-scale language models presents major challenges in data infrastructure efficiency, scalability, and governance. To tackle issues like inefficient collaboration, opaque lineage, and slow iteration, we built next-generation infrastructure based on two core principles: Data-as-Code and Unified Data Lakehouse. Data-as-Code: Automating CI/CD workflows. We codify the entire data pipeline and manage it via version control (e.g., Git) to enable automated, reproducible workflows. This aligns with top ML platforms that standardize workflows through code-driven orchestration (Baylor et al., 2017). We developed unified AIDataOps library with 50+ data operators across modalities, integrated into an automated CI/CD system. Benefits include transparent, traceable end-to-end data lineage and fully automated feature development, cutting R&D iteration cycles from months to days. Unified Data Lakehouse and Wide-Table Architecture. To overcome data silos from hundreds of scattered datasets, we implemented unified lakehouse (Zaharia et al., 2021) with wide logical table aggregating major domains like web pages and code. This central hub simplifies discovery and analysis, supports elastic scalability without full-table rebuilds, and achieves over 20 TB/hour I/O throughput, removing data processing bottlenecks for large-scale training. Combining these principles, we created powerful data engine essential for building the Ling 2.0 corpus. This enabled constructing trillion-record web-wide table and processing 30 billion trainable data points in two days, accelerating model development and enabling complex future data exploration. More information can be found in Appendix B."
        },
        {
            "title": "3.2 Pre-training Recipe",
            "content": "Ling 2.0 pre-training adopts multi-stage strategy with stage-tailored data mixes, and uses WSM (warmup-stable-merge) scheduler (Tian et al., 2025b) that replaces LR decay with checkpoint merging for greater flexibility and effectiveness. Next, we detail the training recipe of Ling 2.0."
        },
        {
            "title": "3.2.1 Hyper-Parameters",
            "content": "Model Hyper-Parameters. Based on deep analysis of scaling laws in Section 2.3.2, Ling 2.0 employs high-sparsity, fine-grained MoE architecture. Each MoE layer comprises one shared and 256 routed experts, activating 8 experts per token. For stability, the first several layers are dense 12 Figure 5 Pre-training and mid-training stages of Ling 2.0. We adopt multi-stage training strategy that progressively expands the context window from 4K to 128K and introduces reasoning and CoT data in advance to pre-activate the models reasoning ability. layers. The attention head dimension is fixed at 128 across all model sizes. We use Multi-Token Prediction (MTP) with depth 1. All parameters are randomly initialized with standard deviation 0.006. Other architectural parameters scale with model size; see Table 1 for details. Training Hyper-Parameters. We use AdamW (Loshchilov and Hutter, 2017) with β1=0.9, β2=0.95, weight decay 0.1, and gradient-norm clipping 1.0. Pre-training uses 4K context window for the first 20T tokens, followed by 150B tokens with 32K contexts. We set the bias-update rate γ=0.001 for the auxiliary-loss-free load-balancing term and an MTP loss weight of 0.1. After context extension, the bias-update rate is set to 0.0001 for the rest of training. Guided by the Ling scaling laws in Section 2.3.1, we determined the learning rate and batch size for Ling 2.0 and summarize them in Table 1. For the batch size, we apply batch-size ramp for the first 500B tokens (e.g., from 3,024 to the peak), then keep it in the remaining training. For the learning rate, we use the novel WSM (warmup-stable-merge) scheduler: linear warmup for the first 2,000 steps to peak LR, then constant LR until training ends; the final annealing is achieved by checkpoint merging instead of LR decay (see Section 3.2.3 for details)."
        },
        {
            "title": "3.2.2 Multi-Stage Training",
            "content": "Ling 2.0 adopts multi-stage pretraining strategy comprising: (1) general pre-training on largescale general corpus; and (2) mid-training on medium-scale, task-specific corpus."
        },
        {
            "title": "3.2.2.2 Mid-training\nAfter general pretraining, we perform a mid-training stage to extend the context length to 128K\nand pre-activate the model’s reasoning ability by introducing chain-of-thought (CoT) data.",
            "content": "Long Context Extension. During the first 150B tokens of mid-training, we sample 20% 32Klength long-text sequences, maintaining data mixture similar to the previous stage. This process 13 expands the models effective context window from 4K to 32K. Throughout this process, the models performance on short-context benchmarks remains stable, while its performance on longcontext benchmarks (e.g., L-Eval (An et al., 2023), LongBench (Bai et al., 2023)) shows continuous improvement. Using the YaRN (Peng et al., 2023) method, we extend Lings context window to 128K. As Figure 6 shows, after supervised fine-tuning, Ling-mini-2.0 demonstrates strong performance on the Needle in Haystack (NIAH) test at 128K context length. Figure 6 Evaluation results on the Needle In Haystack (NIAH) tests. Following supervised fine-tuning, Ling-mini-2.0 performs well across all context window lengths up to 128K. Reasoning Ability Pre-Activation. In the following 600B tokens of mid-training, we maintain high proportion of reasoning data and introduce additional high-quality chain-of-thought (CoT) corpora. We continue training on this high-quality data at higher learning rate and achieve robust performance by merging mid-training checkpoints. We find that the early introduction of CoT data during the latter pretraining phase effectively pre-activates the models reasoning capabilities. This provides higher ceiling for reasoning performance and more stable foundation for subsequent fine-tuning and reinforcement learning stages. We further demonstrate the efficacy of this strategy in enhancing the models reasoning abilities in Section 3.3.2."
        },
        {
            "title": "3.2.3 WSM Scheduler",
            "content": "Learning-rate (LR) decay has long been viewed as essential for effective LLM mid-training, but it restricts flexibility and increases tuning overhead. To enable more flexible and effective process, the Ling 2.0 series adopts the novel WSM (warmup-stable-merge) scheduler (Tian et al., 2025b), which replaces LR decay with checkpoint merging and delivers superior performance. Theoretical Connection Between LR Decay and Checkpoint Merging. We first establish the theoretical equivalence between checkpoint merging and LR decay. The merging process combines sequence of checkpoints, [θn, θn+1, . . . , θn+k], into single model, ˆθn+k, via weighted average. For analytical tractability, we assume the gradient updates between checkpoints are independent. By re-expressing each checkpoint θn+j in terms of base checkpoint θn and the subsequent gradient updates (g), the derivation shows that the merging operation is mathematically equivalent to re-weighting the past gradients accumulated after the base checkpoint: ˆθn+k = j=0 cjθn+j = θn i= wign+i1 (2) 14 Figure 7 Comprehensive performance comparison between our WSM scheduler (via checkpoint merging) and standard WSD scheduler (via LR decay). Both approaches are initialized from the same pre-trained checkpoint. Notably, while WSD requires predetermined decay strategy (e.g., decay over 400B tokens in this study), WSM eliminates such constraints, enabling seamless training continuation (gray regions) and flexible decay behavior approximation. Here, the effective gradient weights, wi, are determined by the original checkpoint merge weights, cj. This equivalence demonstrates that checkpoint merging effectively simulates post-hoc LR decay schedule, achieving an annealing effect without modifying the learning rate during the training phase itself. Conversely, this relationship is invertible. Given target LR decay schedule, represented by desired sequence of monotonically non-increasing gradient decay coefficients {wi}k i=1 (where 1 w1 w2 wk 0), we can uniquely determine the non-negative checkpoint weights {cj}k j=0 that satisfy Equation 2: ck = wk cj = wj wj+1, c0 = 1 j=1 cj = 1 w1 for [1, 1] (3) This establishes bidirectional conversion between LR decay and checkpoint merging, demonstrating that any LR decay schedule can be replicated through an appropriate merging strategy. Overall Performance and Heuristic Improvements. comprehensive comparison reveals that the proposed WSM scheduler consistently outperforms the strong WSD baseline (Hu et al., 2024) across the majority of evaluated tasks (Figure 7). Specifically, WSM yields an average improvement of +1 to +2 points on leaderboard scores across all benchmark categories. Crucially, WSM requires no prior choices about when to start LR decay or how long the decay phase should last (i.e., the decay data budget), offering greater flexibility and scalability than WSD. Moreover, it produces models with more balanced capability profiles. To validate robustness, we further applied supervised fine-tuning for 5 epochs on checkpoints from both schedulers under identical settings, confirming that WSMs advantage persists beyond post-training. As practical heuristic to further improve stability, we select the top-N checkpoints in the final stage of mid-training based on validation performance and average their parameters. For the final Ling 2.0 model, we set = 32."
        },
        {
            "title": "3.3 Pre-training Evaluation",
            "content": "To evaluate the pre-training of Ling 2.0, we focus on both the final models benchmark performance and the performance dynamics throughout pretraining."
        },
        {
            "title": "3.3.1 Evaluation of Pre-training Dynamics",
            "content": "Selecting and Adapting Benchmarks for Pre-training. During pre-training, base models often exhibit limited instruction-following ability, which can lead to misleading evaluations. To mitigate this, we propose framework for selecting and adapting benchmarks. Specifically, we score candidate benchmarks by (i) their stability over the course of training and (ii) their consistency with post-training performance, quantified via Kendalls rank correlation. Only benchmarks that satisfy both criteria are retained to monitor the base model throughout training. For benchmarks that fail to meet these criteria, we adapt them to improve stability via in-context, light-instruction prompts or fill-in-the-blank formats (Luan et al., 2025). Optimizing Evaluation Methods During Pre-training. Beyond benchmark design, the evaluation process itself can suffer from instability. We systematically diagnose this instability, attributing it to two distinct sources: parameter instability, arising from training stochasticity, and evaluation instability, caused by noisy measurement protocols. To counteract these issues, we employ two-pronged approach (Wang et al., 2025). First, we use checkpoint merging to mitigate parameter instability by averaging the weights of recent checkpoints, thereby smoothing the models trajectory in the parameter space. Second, we adopt the Pass@k metric to address evaluation instability, as it offers more robust, low-variance statistical estimate of base models true capability. Extensive experiments demonstrate that this combined approach yields significantly smoother performance curves, providing more reliable and faithful lens for observing training dynamics."
        },
        {
            "title": "3.3.2 Evaluation of Ling 2.0 Base Models",
            "content": "Benchmarks and Configurations. The suite spans mathematics, coding, reasoning, knowledge, and multilingual ability. Unless noted, we report EM/Acc or Pass@1 with standardized prompting and decontamination. The evaluation datasets for pre-trained base models includes 33 benchmarks, which are categorized as follows: Math Tasks: CMath (Wei et al., 2023) (3-shot, CoT), MATH (Hendrycks et al., 2021b) (0-shot, CoT), CollegeMath (Tang et al., 2024) (4-shot, CoT), MinervaMath (Lewkowycz et al., 2022) (4-shot, CoT), FinanceReasoning (Tang et al., 2025b) (3-shot, CoT), OlympiadBench (He et al., 2024a) (3-shot, CoT), TheoremQA (Chen et al., 2023) (5-shot), OmniMath (Gao et al., 2025) (3-shot, CoT), AIME25 (MAA, 2025) (0-shot, CoT). Coding Tasks: HumanEval (Chen et al., 2021) (0-shot), HumanEval-cn (Peng et al., 2024a) (0-shot), HumanEval-Plus (Liu et al., 2023) (0-shot), CruxEval (Gu et al., 2024) (1-shot, CoT), MultiPL-E (Cassano et al., 2023) (0-shot), LiveCodeBench5 (Jain et al., 2025) (0-shot), BigCodeBench (Zhuo et al., 2025) (0-shot), BIRD-SQL (Li et al., 2023a) (0-shot), CodeCriticBench (Zhang et al., 2025a) (2-shot), CodeForces (Penedo et al., 2025) (0-shot, CoT). General Reasoning Tasks: CommonSenseQA (Talmor et al., 2018) (5-shot), WorldSense (Hong et al., 2025) (0-shot), Multi-LogiEval (Patel et al., 2024) (2-shot, CoT), AutoLogi (Zhu et al., 2025) (3-shot, CoT), ProntoQA (Saparov and He, 2023) (1-shot, CoT). 5LiveCodeBench contains 454 problems released between Aug 2024 and May 2025. Knowledge Tasks: ARC (Bhakthavatsalam et al., 2021) (0-shot), MMLU (Hendrycks et al., 2021a) (5-shot), MMLU-Pro (Wang et al., 2024) (5-shot), C-Eval (Huang et al., 2023) (5-shot), CMMLU (Li et al., 2024a) (5-shot). Multilingual Tasks: MMMLU6 (OpenAI, 2024) (0-shot), mARC (Dac Lai et al., 2023) (0-shot), MultiGSM (Shi et al., 2023) (4-shot, CoT), HumanEvalXL (Peng et al., 2024b) (0-shot). We compare Ling 2.0 base models against the base models of Qwen2.5 (Yang et al., 2024a) and Qwen3 (Yang et al., 2025a) series, as well as other leading open-source models, including Hunyuan7B (Tencent-Hunyuan, 2024), Seed-OSS-36B (ByteDance-Seed, 2025), DeepSeek-V3.1 (DeepSeek-AI, 2024) and Kimi-K2 (Moonshot-AI, 2025). Evaluation Results. Table 2, 3 and 4 present the evaluation results for the Ling 2.0 base models. All models are evaluated using our unified internal evaluation framework to ensure fair and consistent comparison. As introduced in Section 3.2.2.2, we specifically compare model versions with and without the integration of high-quality Chain-of-Thought (CoT) data to demonstrate the efficacy of this strategy. The key findings are as follows: Verified 7 Efficiency Leverage: Both our Ling-mini-2.0-base, Ling-flash-2.0-base, and Ling-1T-base achieve performance comparable or superior to other state-of-the-art opensource models of similar scale. In particular, Ling-mini-2.0-base and Ling-flash-2.0-base achieve overall performance comparable to the dense Qwen3 8B base and Seed-OSS-36B base, while using less than one-seventh of their non-embedding activated parameters, confirming the 7 efficiency leverage claimed at the outset of Ling 2.0. Exceptional Math and Code Capabilities: Notably, the Ling 2.0 series exhibits significant advantage in mathematics and coding tasks, indicating strong capabilities in structured reasoning, algorithmic thinking, and programming. For example, Ling-1T achieves superior results on benchmarks such as MathBench, CollegeMath, MinervaMath, OmniMath, HumanEval-Plus, CruxEval, MultiPL-E, etc. Effective Reasoning Pre-activation via CoT Data: Integrating high-quality CoT data during mid-training effectively pre-activates the models reasoning abilities. This leads to substantial gains on reasoning-intensive benchmarks like MATH, AIME and LiveCodeBench, while maintaining performance on other benchmarks. Crucially, this pre-activated advantage persists through subsequent SFT and RL phases (as shown in Figure 12), significantly enhancing their effectiveness. 6MMMLU language coverage may differ across baselines. 17 Table 2 Comparison among Ling-mini-2.0-base and other representative open-source base models. Benchmark Hunyuan-7B Base Qwen3-8B Base Ling-mini-2.0 Base w/o CoT Data Ling-mini-2.0 Base w/ CoT Data 92.81 76.01 69.84 23.85 37.25 24.40 61.96 2.08 81.71 73.17 75.61 60.56 65.31 44.30 26.17 47.18 13. 80.18 57.61 76.00 97.35 90.17 74.21 47.36 83.57 81.29 64.46 51.28 66.6 0 68.28 92.08 76.06 72.50 24.30 39.00 24.20 82.52 43.75 83.54 77.44 76.22 66.44 65.94 43.68 26.08 42.50 34.47 81.08 59.09 81. 97.00 90.51 74.26 47.70 80.41 79.98 65.33 50.14 67.87 65.31 CMath (Acc.) MathBench (Acc.) CollegeMath (Acc.) OlympiadBench (Acc.) TheoremQA (Acc.) OmniMath (Acc.) MATH (Acc.) AIME25 (Pass@1) HumanEval (Pass@1) HumanEval-cn (Pass@1) HumanEval-Plus (Pass@1) CruxEval (Pass@1) MultiPL-E (Pass@1) BigCodeBench (Pass@1) BIRD-SQL (Acc.) CodeForces (Pass@1) LiveCodeBench (Pass@1) CommonSenseQA (EM) WorldSense (EM) ProntoQA (EM) ARC-e (EM) ARC-c (EM) MMLU (EM) MMLU-Pro (EM) C-Eval (EM) CMMLU (EM) mARC (EM) MMMLU (EM) MultiGSM (Acc.) HumanEvalXL (Pass@1) Math Code 88.16 74.21 66.00 22.22 35.00 20.20 76.98 13.54 84.76 73.78 75.61 61.56 57.58 40.70 13.07 18.22 14.10 92.26 73.19 70.62 20.44 31.00 20.10 65.10 14. 64.02 72.56 51.22 63.69 54.97 41.67 22.75 26.91 20.15 General Reasoning 80.59 59.39 72.50 96.47 89.49 79.95 61.22 83.90 82.22 48.34 42.36 53.67 58.59 83.78 57.83 79. Knowledge 97.00 91.86 78.62 50.83 83.19 81.31 Multilingual 80.70 60.02 77.60 69.53 18 Table 3 Comparison among Ling-flash-2.0-base and other representative open-source base models. Benchmark Qwen2.5-72B Base Seed-OSS-36B Base Ling-flash-2.0 Base w/o CoT Data Ling-flash-2.0 Base w/ CoT Data MathBench (Acc.) FinanceReasoning (Acc.) TheoremQA (Acc.) OmniMath (Acc.) MATH HumanEval (Pass@1) HumanEval-cn (Pass@1) HumanEval-Plus (Pass@1) CruxEval (Pass@1) MultiPL-E (Pass@1) BigCodeBench (Pass@1) CodeCriticBench (Acc.) CodeForces (Pass@1) CommonSenseQA (EM) Multi-LogiEval (EM) AutoLogi (Acc.) ARC-e (EM) ARC-c (EM) MMLU (EM) MMLU-Pro (EM) C-Eval (EM) CMMLU (EM) MMMLU (EM) mARC (EM) MultiGSM (Acc.) HumanEvalXL (Pass@1) Math 79.70 74.98 44.25 20.40 88. Code 85.37 80.49 78.66 73.12 67.04 52.89 65.12 19.57 General Reasoning 75.02 81.72 57.36 Knowledge 98.06 94.58 84.99 60.64 88.59 87. Multilingual 70.57 85.21 85.2 73.12 76.65 74.60 39.00 18.40 76.46 82.32 78.66 73.78 63.10 60.00 41.18 67.94 17.81 88.12 74.23 58.29 98.06 96.27 86.29 61.41 88.14 89. 72.70 88.84 82.87 76.25 80.18 74.43 46.25 27.30 66.26 89.02 84.15 81.10 69.50 69.33 50.88 70.40 36.86 86.73 75.51 58.54 97.53 95.59 82.67 59.43 88.64 87.41 63.83 81.87 80.33 75. 77.69 76.44 43.50 28.30 79.54 89.63 82.32 83.54 77.38 69.70 52.37 70.93 47.54 87.71 74.67 61.10 98.24 95.93 82.98 60.73 89.06 87.90 62.76 82.07 80.07 71.88 Table 4 Comparison among Ling-1T-base and other representative open-source base models. DeepSeek-V3.1 Base Kimi-K2 Base Ling-1T Base w/o CoT Data Ling-1T Base w/ CoT Data 81.27 75.02 50.00 44.88 35.70 67. 89.63 84.76 84.15 74.88 70.70 71.56 55.32 89.60 67.43 63.21 97.71 96.61 85.91 66.70 91.41 90.18 70.13 86.64 81.87 81.72 82.11 75.48 62.87 46.62 33.60 82.78 89.63 85.37 83.54 80.88 69.94 66.09 55. 89.76 66.99 65.76 98.59 97.63 86.03 67.91 90.75 90.26 68.68 86.68 85.40 80.62 Benchmark MathBench (Acc.) CollegeMath (Acc.) MinervaMath (Acc.) TheoremQA (Acc.) OmniMath (Acc.) MATH (Acc.) HumanEval (Pass@1) HumanEval-cn (Pass@1) HumanEval-Plus (Pass@1) CruxEval (Pass@1) MultiPL-E (Pass@1) CodeCriticBench (Acc.) CodeForces (Pass@1) CommonSenseQA (EM) WorldSense (EM) AutoLogi (Acc.) ARC-e (EM) ARC-c (EM) MMLU (EM) MMLU-Pro (EM) C-Eval (EM) CMMLU (EM) Math Code 80.26 70.69 55.88 47.50 29.90 76.40 89.63 85.37 84.15 78.25 64.15 70.88 24. 73.30 63.88 48.90 43.75 21.10 35.64 74.39 72.56 65.85 69.81 59.50 67.72 45.64 General Reasoning 85.83 57.73 63.02 97.18 92.88 88.44 67.75 90.67 88.19 Knowledge 85.42 64.02 63.60 98.77 95.59 88.32 67.50 91.72 90.35 MMMLU (EM) mARC (EM) MultiGSM (Acc.) HumanEvalXL (Pass@1) Multilingual 69.46 83.62 82.20 75.00 72.91 88.40 86.87 80."
        },
        {
            "title": "4 Post-Training",
            "content": "The post-training phase of Ling 2.0 is engineered to forge powerful and versatile foundation modelcapable of strong reasoning in complex scenarios while maintaining high efficiency for everyday queries. As illustrated in Figure 8, the process employs structured three-stage methodology supported by scalable, high-throughput reward computation infrastructure. Figure 8 Post-training pipeline of Ling 2.0 series models."
        },
        {
            "title": "4.1 Supervised Fine-Tuning with Decoupled Training",
            "content": "To create strong starting point for reinforcement learning (RL), we introduce Decoupled Fine-Tuning (DFT)a supervised approach that constructs training data via differentiated system prompts. As illustrated in Stage 1 of Figure 8, DFT defines two modes: Instant Response (System Prompt 1) and In-Depth Reasoning (System Prompt 2), with details provided in Table 5. This prompt-guided decoupling enables the model to establish dedicated deep-reasoning mode, providing robust foundation for subsequent RL to further enhance reasoning performance. Table 5 Response modes guided by system prompts in Decoupled Fine-Tuning (DFT)."
        },
        {
            "title": "Instant Response",
            "content": "In-Depth Reasoning"
        },
        {
            "title": "System Prompt",
            "content": "detailed think off detailed think on <think> {long-cot} </think> <answer> {response} </answer> {response} 21 Balanced, High-Quality SFT Data. balanced capability profile is achieved through carefully structured SFT dataset integrating multiple task domains under the dual-mode prompt framework. The dataset composition adheres to three principles: Reasoning: mathematical problem solving, stem and logic reasoning, code generation, operations research, and scientific inquiry, ensuring precise logic and analytical depth. General: creative writing, empathetic dialogue, and socio-philosophical discussion, enhancing linguistic richness and social intelligence. Industrial: domain-specific tasks in finance, medical and health, production planning, supply chain orchestration, and transportation optimization, embedding end-to-end workflows under real-world constraints. This integrated design prevents skill imbalance and supports fluent transitions between abstract reasoning and practical problem solving. RL-Potential-Oriented Evaluation. Since DFT suppresses explicit chain-of-thought, standard accuracy metrics may undervalue its RL potential. We therefore employ ApexEval to gauge latent reasoning ability by testing whether problems are solvable under optimal prompting, emphasizing knowledge and reasoning over format-bound performance. It identifies checkpoints along the stabilityimprovability frontier to start RL from models that retain responsiveness while maximizing reasoning gains (see Section 4.5)."
        },
        {
            "title": "4.2 Evolutionary Reasoning Reinforcement Learning",
            "content": "Building on the DFT-initialized policy, we propose Evolutionary Chain-of-Thought (Evo-CoT), training paradigm designed to instill adaptive reasoning in reflex-grade non-thinking models, enabling them to scale their reasoning depth according to problem complexity. Formally, Evo-CoT starts from DFT-initialized policy π in instant-respsonse mode with system prompt sinstant and evolves its reasoning depth. Given user query x, the policy generates response π( xinst) accordingly, where xinst denotes the concatenation of (sinstant, x). At step t, we optimize the policy π with parameters θ via: πt+1 = arg max π xD (cid:2)J (R(x, y), θ) β KL(cid:0)πθ( xinst) (cid:13) (cid:13) πref( xinst(cid:1)(cid:3), where R() is composite reward function, () denotes the RL policy update algorithm detailed in Section 4.2.2, and β controls deviation from the base policy. The reward consists of: Accuracy Rcorrectness: +1 if the final answer matches ground truth else 0. Dynamic Length control Rlength: Penalizes exceeding difficulty-specific length limit with stage-wise coefficient α that decreases for harder tasks, allowing more elaborate reasoning when needed. Formatting Rformat: if explicit reasoning markers <think> appear, reward 0.5. Task-specific rewards Rtask-specific,k: optional signals tailored for specific domains (e.g. visual reward for front-end engineering). Taken together, Evo-CoT sustains strong reasoning under complex scenarios while upholding high efficiency for general tasks."
        },
        {
            "title": "4.2.1 Tasks-Specific Rewards",
            "content": "To cater to different domains, we construct multi-task reward framework that supports adaptive reasoning across wide spectrum of tasks. Mathematical, STEM, and Logical Reasoning. Our reward policy is guided by core principle: think more about hard problems, respond quickly to easy ones. To implement this principle, we employ the dynamic length control term Rlength inspired by Kimi-Team et al. (2025) that encourages brevity for straightforward tasks, while allowing elaborate reasoning for complex ones. Formally, we define the length preference function: where ˆRlength = p(l), min(cid:0)p(l), 0(cid:1), if racc = 1, if racc = 0, p(l) = (cid:18) 0.5 ℓmin ℓmax ℓmin + 109 (cid:19) . Here, lk denotes the length (e.g., token count) of the k-th sampled response to input x, ℓmin = mink lk and ℓmax = maxk lk represent the shortest and longest responses among the samples, respectively, and racc {0, 1} indicates correctness. To modulate the influence of the length preference relative to correctness, we introduce coefficient α > 0. larger α is used for easier tasks, strongly promoting concise outputs; conversely, smaller α is applied to harder tasks, thereby encouraging more extensive reasoning. In practice, the final scoring function is: Rlength = α ˆRlength. This formulation ensures that: For correct answers, the reward reflects how well the response length aligns with the preferred range. For incorrect answers, excessively long responses are penalized more, and any positive length-based reward is suppressed. Overall, this design achieves balance between output accuracy, clarity, and efficiency, while still promoting richer reasoning on challenging problems. Code Reasoning. Code reasoning emphasizes functional correctness. We employ unified reward framework based on test-case execution for code completion, editing, software engineering, and SQL tasks, ensuring reliable functional validation. Front-end Generation. For complex front-end engineering tasks, we propose the Visually Augmented Reward (VAR) systemat the core of SyntaxFunctionAesthetic triple-filter positive-feedback loop. As shown in Figure 9, VAR renders generated code into live interface via headless browser, then uses multimodal model to evaluate the screenshot based on aesthetic and usability criteria, yielding perceptually-aligned reward signal."
        },
        {
            "title": "4.2.2 Linguistic-unit Policy Optimization (LPO)",
            "content": "We propose Linguistic-unit Policy Optimization (LPO), novel policy gradient algorithm drived by the Evolutionary Chain-of-Thought (Evo-CoT) paradigm. LPOs core mechanism is to perform 23 Figure 9 practical example of Visually-Augmented reward. Prompt: \"Create an interactive Halloween page with animated background, costume contest upload, candy collection game, and festive visual effects. Generate clean executable code with smooth interactions.\" importance sampling and clipping at the sentence level, defining linguistic sentence as the fundamental action unit for policy updates. Specifically, Let {yi}G i=1 denote group of candidate ( xinst). For response yi, let Nsent(yi) denotes the total responses sampled from the old policy πθold number of sentences in yi, si,k the k-th sentence in yi segmented by common pause punctuation marks after detokenization, and denote the token length. The objective function of LPO is formulated as follows: JLPO(R, θ) = {yi}G i=1 πθold (cid:34) 1 i=1 yi i= Nsent(yi) k=1 (xinst) si,k min (cid:0)ri,k(θ) ˆAi, clip(ri,k(θ), 1 ε, 1 + ε) ˆAi (cid:1) (cid:35) , where ri,k(θ) = exp 1 si,k ttokens(si,k) log πθ(yi,t xinst, yi,<t) (yi,t xinst, yi,<t) πθold , ˆAi = R(x, yi) mean (cid:16) (cid:17) R(x, yi)G i=1 (cid:17) (cid:16) std R(x, yi)G i= LPO performs sentence-level policy updates with the following design choices: Sentence granularity Importance Sampling: Each sentence si,k in yi is treated as an independent action unit, with its importance ratio ri,k(θ) applied uniformly to all tokens in that sentence. Token-level Normalization: Group-based advantage estimation ˆAi are averaged over the total token length yi, ensuring scale invariance across examples. Clipping strategy: Ratios are clipped within [1 ε, 1 + ε] before multiplication, preventing unstable updates while preserving finer granularity than whole-sequence clipping. In our training setting, ε = 0.03 . This structure aligns the optimization step with the natural semantic boundaries of reasoning, resolving the mismatch in granularity found in conventional token-level and sequence-level methods. It attains stability without sacrificing data efficiency, making LPO natural fit within the Evo-CoT training paradigm. Empirically, as shown in Figure 10, LPO delivers smoother reward curves and markedly greater stability than GRPO (Shao et al., 2024), GSPO (Zheng et al., 2025), and the GSPO (Token Mean) variant. It avoids plateaus and collapse, converges faster, and generalizes better. On the challenging AIME 2025 test set, LPO-trained models achieve substantially higher accuracy, demonstrating that 24 stabilizing updates at the sentence level not only improves optimization but also guides the policy toward more robust reasoning strategies. Figure 10 Reward curves of LPO in RL training for the latest Ling 2.0 model. Left: reward progression on training data, showing smoother growth and greater stability compared to GRPO (Shao et al., 2024), GSPO (Zheng et al., 2025), and the GSPO (Token Mean) baseline, with no severe plateaus or collapses. Right: reward curves on the AIME 2025 test set, illustrating faster convergence and improved generalization due to sentence-level policy updates."
        },
        {
            "title": "4.3 Group Arena Reward for Human Preference Alignment",
            "content": "In the RLHF post-training stage for open-ended, subjective tasks, two central objectives emerge: (1) mitigating reward noise inherent in ambiguous evaluation criteria, and (2) aligning model outputs more precisely with nuanced human preferences. To this end, we design the Group Arena Reward (GAR) mechanisman intra-group comparative evaluation strategyand RubriX (Rubrics for eXtended domains), fine-grained, multi-dimensional reward guideline framework. Together, they improve stability in subjective task optimization and enable generation that is both technically accurate and naturally aligned with user intent."
        },
        {
            "title": "4.3.1 Group Arena Reward",
            "content": "For open-ended tasks, conventional reward mechanisms often struggle with quantifying subjective quality and suffer from high-variance scoring. As illustrated in Figure 11, GAR addresses these challenges by replacing independent absolute scoring with relative, tournament-style comparisons. Multiple responses from the same policy are placed into an arena; generative reward model acts as referee, performing pairwise comparisons in round-robin fashion. The cumulative results of these head-to-head contests form the final reward for each response. This relative ranking structure effectively reduces variance and reward noise, producing more reliable advantage estimates for policy updates."
        },
        {
            "title": "4.3.2 Fine-grained Multi-Dimensional RubriX",
            "content": "To complement GAR with precise preference modeling, we propose RubriXa domain-extended set of reward evaluation rubrics tailored for subjective general tasks. RubriX spans multiple dimensions, including clarity, coherence, creativity, emotional resonance, instruction adherence, and domain-specific accuracy, with instantiations for writing, translation, long-form QA, emotional dialogue, multi-turn conversation, and other instruction-following tasks. These structured rubrics guide the reward model to capture subtle aspects of user intent, encouraging responses that are both more natural in flow and more aligned with complex preference criteria. 25 Figure 11 Illustration of the Group Arena Reward (GAR) mechanism applied to open-ended subjective tasks."
        },
        {
            "title": "4.4 Reward Model System",
            "content": "To flexibly support reward computation and reward policy orchestration across diverse reasoning tasks within RL training pipelines, we propose unified scalable reward model system. This system concurrently accommodates rule-based, model-based, and multi-programming-language-based reward verification, scaling to 40K concurrent heterogeneous reward requests with sustained success rates exceeding 99.9%. The system architecture comprises three core modules: (1) highly available sandboxing environment integrating multi-programming-language sandboxes, general reward models inference, visual reward evaluators, and complex environment sandboxes (software engineering, database operations, browser interaction, etc.); (2) preemptive task scheduling mechanism employing highperformance bounded queues to mitigate timeout-induced failures arising from computational heterogeneity under peak concurrency, achieving 39% improvement in system throughput; and (3) an asynchronous reward computation framework that decouples RL training iterations from reward computation latency, yielding empirically measured training time reduction of up to 30%."
        },
        {
            "title": "4.5 ApexEval: Searching for Checkpoint with Highest Potential",
            "content": "In post-training, RL is used to unlock the models reasoning potential. To initialize RL effectively, we must identify the SFT checkpoint with the highest potential. However, conventional methods fall short: 1) They rely on greedy or average pass@k scores, which reflect average performance rather than the best potential; 2) Checkpoints lack strong instruction-following ability, leading to misjudgment of correct responses that deviate from fixed formats. To address the above two issues with conventional evaluation methods, we propose ApexEval to get the best checkpoint initialization for RL training. The method includes: Instead of greedy or average pass@k, we use the highest score of pass@k to estimate the probability of producing at least one correct response in multiple attempts, effectively capturing the models potential upper bound. To reduce the impact of answer formatting, we use LLM-based intelligent judges (e.g., Math26 Verify, XVerify) for tasks with explicit answers like mathematics, knowledge, and logic. These judges assess answer validity based on model predictions, minimizing misjudgment caused by pattern variability. For coding tasks, we evaluate valid code snippets via test-case execution to fairly assess actual capabilities. Find High-potential Checkpoint. ApexEval is designed to assess models true capability and potential for further improvement. This enables the identification of promising checkpoints for subsequent instruction tuning or RL optimization. During the Ling 2.0 pretraining phase, we introduce portion of instant-response and in-depth reasoning data. From post-training perspective, this inclusion raises the reasoning performance ceiling when applying Evolutionary Reasoning Reinforcement Learning (ERL). As shown in Figure 12, we compare Decoupled Fine-Tuning (DFT) with and without Chain-ofThought (CoT) data during pretraining. The performance ceiling is evaluated using both ApexEval and high-value pass@k metrics. In all settings, pretraining with CoT data consistently yields higher ceiling under the same DFT configuration.We use ApexEval as the criterion for selecting the initial model for Reasoning Reinforcement Learning. The DFT model pretrained with CoT data exhibits stronger AIME performance at ERL step450 compared to the model without CoT data, indicating faster performance gain trajectory. Figure 12 ApexEval-based checkpoint selection experiment on the Ling 2.0 mini model. Left: ApexEval results for DFT models pretrained with and without CoT-style data. Right: Performance of the two DFT variants after applying ERL, showing the pretraining-with-CoT model achieves higher AIME scores and faster improvement."
        },
        {
            "title": "4.6 Evaluation Results",
            "content": "Benchmarks and Configurations. The suite spans mathematics, coding, reasoning, knowledge, agent, instruction following and alignment ability. Unless noted, we report EM/Acc or Pass@1 with 0-shot prompting and decontamination. For fill-in-the-blank benchmarks, we employ LLM-asa-Judge to improve the accuracy of the evaluation. The evaluation datasets for post-trained models include 36 benchmarks, which are categorized as follows: Coding Tasks: MultiPL-E (Cassano et al., 2022), MBPP (Austin et al., 2021)(MBPP Sanitized), LiveCodeBench (Jain et al., 2025)(questions from August 2024 to May 2025), CodeForces (Quan et al., 2025)(ratings from CodeElo), BIRD-SQL (Li et al., 2023b), ArtifactsBench (Zhang et al., 2025b), FullStack Bench (Cheng et al., 2024), Aider-Edit (Aider, 2025). Math Tasks: CNMO 2024 (Liu et al., 2025b), AIME24 (MAA, 2024), AIME25 (MAA, 2025), UGMathBench (Xu et al., 2025), Omni-MATH (Gao et al., 2025), HMMT25 (Balunovic et al., 27 2025), FinanceReasoning (Tang et al., 2025a), Optibench (Yang et al., 2025b), OptMATH (Lu et al., 2025). For the Omni-MATH benchmark, instead of the original rule-based evaluation method, we rely on LLM to perform the assessment. Reasoning Tasks: BBEH (Kazemi et al., 2025), KOR-Bench (Ma et al., 2025), ARC-AGI1 (Chollet, 2019), ZebraLogic (Lin et al., 2025), HLE (Phan et al., 2025). For BBEH, we employ LLM-as-a-Judge for evaluation. For ARC-AGI-1, ZebraLogic and HLE, we repeat each query 4 times and report the Pass@1 score. Knowledge Tasks: C-Eval (Huang et al., 2023), MMLU-Redux (Hendrycks et al., 2021a), MMLU-Pro (Wang et al., 2024), GPQA-Diamond (Rein et al., 2023), MMLU-Pro-Stem (Wang et al., 2024), OlympiadBench-Stem (He et al., 2024b), MedXpertQA (Zuo et al., 2025). For GPQA-Diamond, we repeat 16 times for each query and report the Pass@1 score. For MMLUPro-Stem, we selected subset of the mmlu-pro evaluation set belonging to the STEM category, consist of math, physics, chemistry, engineering, biology, computer science, and calculated their average score. For the OlympiadBench-Stem evaluation set, we selected the physics subset from OlympiadBench (He et al., 2024b) that is suitable for evaluating language models, excluding subsets containing images. Alignment Tasks: Arena Hard v2.0 (Li et al., 2024b; Li* et al., 2024), Writing Bench (Wu et al., 2025), Creative Writing v3 (Paech, 2025), Multi-Challenge (Deshpande et al., 2025). Agent&Instruction Following Tasks: BFCL-V3 (Yan et al., 2024), IFEval(Prompt Strict) (Zhou et al., 2023). Table 6, Table 7 and Table 8 provide comprehensive comparisons of Ling-mini-2.0 , Ling-flash-2.0 and Ling-1T against leading models. As shown in Table 8, Ling-1T demonstrates superiority over leading models across multiple domains, including coding, math, reasoning, alignment and multiturn dialogues on the majority of benchmarks. Current results of the Ling-1T align well with the scaling law. Moreover, we have the following findings: Reasoning Capability. Benefits from the In-depth Reasoning during the Decoupled Fine-tuning phase and evolutionary CoT training during RL, the reasoning capability of the model significantly improve. Respectively, the in-depth Reasoning in SFT employs prompts in Table.5 to establish dedicated deep-reasoning mode, providing robust foundation for RL, while the evolutionary CoT in subsequent RL instill adaptive reasoning in reflex-grade non-thinking models, enabling them to scale their reasoning depth according to problem complexity. As shown in Table 6, Table 7 and Table 8, the Ling-mini-2.0, Ling-flash-2.0 and Ling-1T outperform most of the leading industry models in various benchmarks that require reasoning capability, involving coding tasks e.g. LiveCodeBench, MBPP Sanitized and CodeForces, math tasks e.g. CNMO 2024, Omni-MATH and OptMATH, and reasoning tasks e.g. BBEH, KOR-Bench and ZebraLogic. Better and Cheaper. We analyze the overall performance of Ling-1T in terms of reasoning accuracy and efficiency. As illustrated in Figure 13, taking the competition-level mathematics benchmark AIME 25 as an example, Ling-1T showcase its advantage in \"efficient thinking and precise reasoning.\" The optimal balance between efficient thinking and precise reasoning benefits from the evolutionary CoT. It progressively activates the models reasoning ability from shallow to deep, while enabling precise control over reasoning costs. We believe that for reflexive non-thinking models, this approachgradually activating reasoning capability from pre-training to post-trainingcan continuously push the Pareto frontier of reasoning accuracy and average reasoning depth. 28 Figure 13 Model Performance vs. Average Tokens (AIME-25) 29 Table 6 Comparison between Ling-mini-2.0 and other representative models."
        },
        {
            "title": "Benchmark",
            "content": "Ling-mini-2.0 Qwen3-4B -Instruct 2507 Qwen3-8B (Non-thinking) Ernie-4.5-21B -A3B-PT gpt-oss-20B (low thinking) MBPP Sanitized (Pass@1) LiveCodeBench (Pass@1) CodeForces (Rating) BIRD-SQL (Acc.) ArtifactsBench MultiPL-E (Pass@1) FullStack Bench (Pass@1) CNMO 2024 (Pass@1) AIME24 (Pass@1) AIME25 (Pass@1) UGMathBench (Acc.) Omni-MATH (Acc.) HMMT25 (Pass@1) FinanceReasoning (Acc.) OptMATH (Pass@1) Optibench (Pass@1) KOR-Bench (Acc.) ARC-AGI-1 (Pass@1) HLE (Pass@1) ZebraLogic (Pass@1) GPQA-Diamond (Pass@1) C-Eval (Acc.) MMLU-Redux (Acc.) MMLU-Pro (Acc.) MMLU-Pro-Stem (Acc.) OlympiadBench-Stem (Acc.) BFCL-V3 (Function Call)1 82.99 41.69 1410 39.60 29.94 70.82 43.45 72.66 65.62 46.72 66.83 60.30 35.83 69.64 12.20 61. 62.00 10.25 6.01 80.20 58.74 83.31 81.55 65.11 72.14 70.43 53."
        },
        {
            "title": "Coding",
            "content": "85.54 34.03 1224 45.40 36.61 72.03 40."
        },
        {
            "title": "Math",
            "content": "68.49 64.53 47.81 67.14 60.25 29.79 74.17 10.39 28."
        },
        {
            "title": "Reasoning",
            "content": "65.12 15.38 4.55 79."
        },
        {
            "title": "Knowledge",
            "content": "44.82 81.71 84.24 62.38 69.90 77."
        },
        {
            "title": "Agent",
            "content": "61.16 79.45 26.10 624 36.80 31.00 67.37 39.24 34.38 27.97 24.01 59.62 41.71 11.46 69.52 10.39 41.65 54.40 4.06 4.00 36.05 48.64 80.06 80.83 52.54 57.62 59.37 59."
        },
        {
            "title": "Instruction Following",
            "content": "85.36 26.10 480 29.86 31.44 71.92 43.02 42.71 24.43 15.68 56.31 38.71 6.88 70.55 1.51 31.90 48.48 0.75 5.11 46.98 77.27 85.38 82.59 65.46 72.98 62.17 IFEval (Prompt Strict) 1 The Ernie-4.5-21B-A3B-PT model lacks function call capability, so BFCL-V3 (Function Call) score is not available for this model. 83.92 75.05 77.74 84.47 89.40 46.64 1481 36.15 45.90 61.10 49.91 45.31 45.68 38.59 61.57 50.70 20.05 77.31 2.71 37. 66.00 3.56 4.69 44.10 55.71 64.41 83.50 65.59 72.63 63.02 36.22 72.50 30 Table 7 Comparison between Ling-flash-2.0 and other representative models. Benchmark Ling-flash-2.0 Qwen3-32B (Non-thinking) Hunyuan-A13B Seed-OSS-36B GPT-OSS-120B -Instruct (low think) -Instruct GPT-4.1 mini 91.01 45.54 1309 39.77 73.79 56.31 74.44 56.68 51.82 49.64 65.65 57.32 27.86 84.45 34.49 40.17 70.40 7.62 5.10 46.50 66.67 76.90 89.80 77.74 82.98 72.17 94.58 42.68 1519 38.49 33.25 46.83 69. 63.72 57.55 50.83 67.58 60.39 33.54 83.84 26.96 59.01 73.12 10.69 5.33 68.40 63.42 70.94 88.50 74.14 80.74 73.04 58.34 56.94 73. 57.34 81.19 79.09 85.50 37.00 87.21 49.10 42.77 74.35 72.17 35.90 MBPP Sanitized (Pass@1) LiveCodeBench (Pass@1) CodeForces (Rating) BIRD-SQL (Acc.) MultiPL-E (Pass@1) FullStack Bench (Pass@1) Aider-Edit (Acc.) CNMO 2024 (Pass@1) AIME24 (Pass@1) AIME25 (Pass@1) UGMathBench (Acc.) Omni-MATH (Acc.) HMMT25 (Pass@1) FinanceReasoning (Acc.) OptMATH (Pass@1) Optibench (Pass@1) KOR-Bench (Acc.) ARC-AGI-1 (Pass@1) HLE (Pass@1) ZebraLogic (Pass@1) GPQA-Diamond (Pass@1) C-Eval (Acc.) MMLU-Redux (Acc.) MMLU-Pro (Acc.) MMLU-Pro-Stem (Acc.) OlympiadBench-Stem (Acc.) BFCL-V3 (Function Call) IFEval (Prompt Strict) Arena Hard v2.0 (Style-Control) Arena Hard v2.0 (Win-Rate) Creative Writing v3 Writing Bench Multi-Challenge 94.17 51.38 1600 47.65 75.82 47.01 71.24 74.48 69.95 55.83 71.90 66.64 39.58 81.59 39.76 68. 68.80 24.56 5.05 86.80 68.12 87.89 89.34 77.07 84.64 87.83 59.14 81.52 49.12 61.33 85.17 87.22 42.12 85.42 30.73 679 39.47 69.00 45.82 68. 39.58 23.18 14.9 61.87 37.35 8.33 78.14 14.61 55.37 44.24 4.38 5.15 46.40 51.96 90.00 86.56 73.16 77.44 76.52 39.17 81.52 33.58 37.07 82.17 81.64 28. Coding Math Reasoning Knowledge Agent 84.78 31.50 696 37.65 70.79 48.19 77. 37.41 29.90 22.5 64.10 43.81 10.42 78.51 15.51 54.38 56.96 3.31 4.47 33.80 56.16 87.69 86.88 69.24 73.19 72.17 63.79 82.82 25.77 569 30.05 68.68 50.21 43.80 43.84 32.66 21.46 52.10 50.11 8.54 64.27 2.86 29. 47.60 0.06 5.68 33.20 52.15 76.11 76.50 65.00 71.82 63.48 54.86 Instruction Following Aligment 83. 28.44 34.44 77.57 74.97 30.62 79.11 7.21 8.56 59.69 65.10 17.66 31 Table 8 Comparison between Ling-1T and other representative models. Benchmark Ling-1T DeepSeek-V3.1-Teminus (Non-thinking) Kimi-K2 -Instruct-0905 GPT-5-main Gemini 2.5 Pro (lowthink) MBPP Sanitized (Pass@1) LiveCodeBench (Pass@1) CodeForces (Rating)1 BIRD-SQL (Acc.) MultiPL-E (Pass@1)2 ArtifactsBench FullStack Bench (Pass@1) Aider-Edit (Acc.) CNMO 2024 (Pass@1) AIME24 (Pass@1) AIME25 (Pass@1) UGMathBench (Acc.) Omni-MATH (Acc.) HMMT25 (Pass@1) FinanceReasoning (Acc.) Optibench (Pass@1) OptMATH (Pass@1) BBEH (Acc.) KOR-Bench (Acc.) ARC-AGI-1 (Pass@1) ZebraLogic (Pass@1) HLE (Pass@1) GPQA-Diamond (Pass@1) C-Eval (Acc.) MMLU-Redux (Acc.) MMLU-Pro (Acc.) MMLU-Pro-Stem (Acc.) OlympiadBench-Stem (Acc.) MedXpertQA (Acc.) 96.87 61.68 1901 52.38 77.91 59.31 56.55 83.65 79.25 80.21 70.42 74.95 74.46 47.08 87.45 74.71 57.68 47.34 76.00 43.81 90.80 7. 72.98 92.19 92.25 82.04 88.50 91.30 22.33 BFCL-V3 (Function Call) 69.64 IFEval (Prompt Strict) 86.11 Coding 90.69 48.02 1582 44.88 77.68 43.29 55.48 88.16 Math 73.78 71.67 55.21 72.70 64.77 41.25 86.44 64.30 35.99 Reasoning 42.86 73.76 14.69 81.60 10.38 Knowlwdge 76.23 91.76 92.37 83.25 87.91 87.83 31.14 Agent 52.67 Instruction Following 86.32 Alignment 89.96 48.95 1574 46.45 73.54 44.87 54.00 85.34 68.92 67.24 50.16 69.97 62.42 38.80 84.83 60.83 35.84 34.83 73.20 22.19 85.50 7.29 73.93 91.12 91.58 81.03 85.30 79.13 20.61 91.72 48.57 1120 43.97 76.66 41.04 50.92 84.40 63.11 67.60 59.43 67.27 61.09 36.98 86.28 40.66 39. 39.75 70.56 14.06 57.30 7.33 71.31 83.59 92.75 81.94 73.45 78.26 17.59 91.01 45.43 1675 54.76 71.48 60.28 48.19 89.85 74.65 77.50 70.10 70.10 72.02 60.73 86.65 68.76 42.77 29.08 59.68 18.94 70.20 12.07 71.81 88.77 94.67 82.13 88.60 89.57 44. 71.05 50.27 63.31 90.99 85.11 87. Arena Hard v2.0 (Style-Control)3 65.37 54.09 74.46 63.24 Arena Hard v2.0 (Win-Rate) 80.53 80.95 Writing Bench 84.99 85.18 Creative Writing v3 Multi-Challenge 51.28 42.49 1 CodeForces is composed of problems from 14 Div.2 contests along with expert-crafted test cases, while 2209 representing the highest rating attainable on it. 2 In MultiPL-E, we choose six programming languages: Python, C++, Java, JavaScript, TypeScript, and PHP. 3 Arena Hard Style-controlled score following LMSYSs Arena Hard Auto protocol: https://lmsys.org/blog/2024-08-28/style-control/ . 76.95 69.88 87.59 87.01 48.72 68.37 65.06 77.07 80.93 48.72 76.26 75.83 89.40 89.24 58.24 32 Figure 14 The overview of the optimization tasks we implemented for Ling-1T during the pretrain stage."
        },
        {
            "title": "Infrastructure",
            "content": "The algorithmic architecture of the Ling 2.0 model theoretically provides technical roadmap for low-cost scaling, while also ensuring the upper limits of training and inference efficiency. However, algorithm design alone is insufficient to achieve our objectives. Without any engineering optimizations, this highly sparse MoE architecture offers no performance advantage over dense models. Therefore, we require matching infrastructure capabilities to support efficient training and scale the model to the trillion-parameter level at minimal cost. Despite steady progress in LLMs training technologies, building systems that can support efficient trillion-parameter training still presents numerous significant challenges: FP8 Training. To reduce the training cost of the Ling 2.0 model and enhance its efficiency in both training and inference, all models are trained entirely in FP8 precision (DeepSeek-AI, 2024), which presents challenges in both precision and stability. We provide an advanced FP8 training framework that achieves near-lossless model performance, while simultaneously reducing computation and memory consumption. Heterogeneous TransformerBlock. The MTP block and the First-K-Dense strategy shift the Pipeline Parallelism (PP) scheduling units from homogeneous to heterogeneous, implying that the forward and backward computational latencies as well as memory consumption may differ across blocks, which can significantly increase pipeline bubbles without careful design. Increased Number of Experts and Higher Sparsity. These lead to higher communication costs in Expert Parallelism (EP) and increased CPU overhead. Larger Overall Model Size. Scaling in model size entails greater computational and memory demands, and increased distributed training overhead. Co-Design of Algorithms and Systems. Advances in model architectures necessitate effective codesign during the model development phase to ensure maximal utilization of hardware resources. To address the new challenges, we upgrade our infrastructure, which helps the Ling 2.0 models achieve optimal training and inference efficiency. Figure 14 summarizes the optimizations, and the specific results are obtained from the Ling-1T training. Baseline performance is measured under the following configuration: modified version of Megatron 0.11, the FP8 training strategy adapted to Ling 2.0 with MTP support, running on 2016 Hopper GPUs. Other key distributed training settings include: TP1, EP8, PP21, VPP2, sequence length 4K, and fully recomputation."
        },
        {
            "title": "5.1 FP8 Training",
            "content": "Ling 2.0 employs fine-grained block-wise FP8 quantization strategy: activations and gradients are quantized in blocks of [1,128] elements, while weights are quantized in blocks of [128,128] elements. During forward and backward passes of most linear layers, the original BF16 tensor is quantized 33 Figure 15 BF16 v.s. FP8 loss diff on Ling-1T. into FP8 E4M3 format along with FP32 scaling factors. After FP8 GEMM computation, the output of BF16 is obtained. Our quantization strategy significantly mitigates the impact of outliers on global quantization errors, making it feasible to train LLMs in FP8. To further ensure FP8 training stability, we use QKNorm introduced in Section 2.1 to prevent the layer-by-layer diffusion of outliers that amplifies quantization errors. Simultaneously, our FP8 Training Safeguard System tracks risk coefficients for each operation across all layers in real-time, greatly facilitating timely anomaly detection and intervention. Validated on the Ling-1T model, the proposed FP8 mixed-precision framework maintained numerical stability throughout 900B-token training, with relative loss difference within 0.25% (averaging around 0.1%) compared to the BF16 baseline. (as shown in Figure 15), with no significant variance on benchmark leaderboards. On the efficiency side, we reduce CPU overhead through optimizations such as Padding Routing Map, and by employing the FP8 On-Demand Transpose Weight technique to trade time for space, we achieve higher acceleration ratios. Ultimately, FP8 training delivers roughly +15% MFU gain for Ling-1T."
        },
        {
            "title": "5.1.1 Training Percision Tracking and Assurance",
            "content": "FP8 Training Safeguard System. Benefiting from the FP32 accumulate operation that effectively reduces precision errors in FP8 GEMM computations, we attribute the precision deviations in our current FP8 training scheme to two primary sources: 1) FP8 Quantization Underflow, defined as the proportion of matrix elements that become zero after quantization. 2) FP8 Quantization Distortion, measure of information loss calculated as the cosine similarity between the original and reconstructed (quantized then de-quantized) matrix. Through error profiling via high-precision recomputation, our FP8 training safeguard system monitors all operations across layers in real time and reports their health status. For the first time, we quantify low-precision training safety as measurable indicators, ensuring continuous protection for the training of Ling 2.0 models. Figure 16 shows the trend of FP8 underflow and distortion metrics during the Ling-mini-2.0 training process. Under the fine-grained FP8 quantization scheme, both activations and gradients maintain healthy precision states, ensuring reliability in forward computations and x calculations during backpropagation. However, monitoring reveals elevated quantization errors in tail layers during gradient transpose computations for W in backpropagation. Through joint analysis with high-precision recomputation metrics and experimental validation, we conclude these errors have 34 Figure 16 FP8 quantization error during pre-training phase of Ling-mini-2.0. For the metrics, lower underflow is better and higher distortion is better. negligible impact on model training. As its quantization errors do not accumulate layer by layer. resides at leaf nodes in the backward propagation path, QKNorm to Mitigate Outliers and Reduce FP8 Precision Loss. During our early experiments, severe outlier phenomena were observed in both activations and the y gradient within the attention.linear_qkv layer. These outliers amplify progressively as layer depth increases, directly causing substantial quantization precision errors in FP8 computations for this layer. To address this, we introduced QKNorm, which not only suppresses outliers to enhance training stability but also demonstrably reduces precision errors across all FP8 modules in the network."
        },
        {
            "title": "5.1.2 Toward Even Greater Training Efficiency",
            "content": "The computational efficiency of FP8 delivers direct performance gains for end-to-end training. Furthermore, FP8s memory advantages unlock greater flexibility in micro batch size (mbs), parallelization strategies, and recomputation techniques, thereby boosting overall training throughput. To maximize these benefits: CPU Overhead Optimization: We increase FP8 computation ratio through multiple CPU overhead optimizations (e.g., replacing FP8 padding/unpadding layers with FP8 padding routing map7, removing redundant assert checks). Time-Space Tradeoff: We trade time for VRAM by introducing FP8 on-demand transpose weight 8 together with the optimizer (Ling-mini-2.0 only). Furthermore, our fine-grained FP8 quantization keeps LLM training stable, allowing most tensors to be compressed and decompressed at FP8 with negligible error, opening up further ways to reclaim VRAM. By adopting the above techniques, Ling-1T achieves +15% MFU improvement over BF16 training. On 8/16/32 80GB GPUs, Ling-mini-2.0 delivers 30-60% throughput improvement over LLaMA 7https://github.com/NVIDIA/Megatron-LM/commit/92d68dae89af0baab2d4eee092f884902dca4db0 8https://github.com/inclusionAI/linghe Figure 17 Computation flow before and after the MTP splitting. The left figure shows the original MTP computation flow. The right figure illustrates the computation flow after MTP splitting. In the latter, we divide MTP into individual Transformer layers and loss computation layer, thereby enabling support for more fine-grained scheduling scheme. 3.1 8B and Qwen3 8B when MTP is enabled, and 90-120% throughput improvement without MTP. FP8 On-Demand Transpose Weight. Due to the low efficiency of FP8 tensor transposition, the Transformer Engine implementation caches an additional pre-transposed weight matrix (weight.T) to accelerate backpropagation. However, this optimization failed to reduce overall memory consumption because the weight are still stored in two copies of FP8 tensors, including the original and transposed forms. To address this, we introduce high-performance on-demand transpose kernel that eliminates the need for persistent transposed-weight storage, reducing the memory footprint of weight tensors by exactly 50% without compromising computational correctness. FP8 Padding Routing Map. The FP8 GEMM kernel mandates 16-element alignment for matrix dimensions, requirement inherently incompatible with dynamic token allocation per expert in Mixture-of-Experts (MoE) architectures. The Megatron implementation addresses this through explicit padding operations, incurring non-negligible CPU overhead. To eliminate this latency, we strategically adjust routing map prior to expert assignment, ensuring resultant tensor dimensions satisfy kernel alignment constraints. As modifications are limited exclusively to zero-probability routing regions, strict mathematical equivalence is preserved, thereby enhancing training throughput without computational side effects."
        },
        {
            "title": "5.2 Heterogeneous Fine-grained Pipeline Parallelism",
            "content": "To reduce the bubble ratio in PP, Shoeybi et al. (2019) proposed the interleaved 1F1B pipeline strategy, and further support features such as non-uniform layer partitioning. These enhancements aim to alleviate bottlenecks in the first and last stages caused by the presence of Embedding and loss computation layers, which often constrain overall pipeline throughput. Nevertheless, when applying this approach to train the Ling 2.0 series models, we still faced the following challenges: In addition to the Embedding and loss computation layers, the First-K-Dense strategy and the MTP layers introduced in Ling 2.0 differ significantly from normal MoE layers in both computational and memory consumption, necessitating more refined PP partitioning strategy. When employing the interleaved 1F1B pipeline strategy, it is necessary to apply non-uniform partitioning across different PP ranks and VPP stages. This ensures more balanced workload 36 Figure 18 Example of 1F1B and Heterogeneous Pipeline scheduling for 5 PP ranks. Compared with the baseline, our approach substantially reduces pipeline bubbles, thereby significantly lowering the overall training cost. distribution throughout the pipeline and prevents blocking between stages. The MTP layer contains Transformer layers and loss computation block, both of which incur higher computational and memory cost than single MoE layer. Furthermore, under the original 1F1B strategy, the MTP layer must be grouped within the same VPP stage alongside another Transformer layer and loss computation layer, causing this stage to become bottleneck. To address these issues, we modified the PP framework to support the following new features: Configurable Transformer Layer Allocation per VPP Stage: Enabled flexible configuration of the number of transformer layers per VPP stage, including support for empty stages. Scheduling MTP as Standalone Layer: The MTP layer no longer needs to be grouped with other MoE layers or bound to the loss computation layer during scheduling. Partial Recomputation For MTP: During the backward pass, only the Transformer layer portion within MTP is recomputed, while the logits computation part is not. This approach effectively trades additional memory consumption for improved computation speed. Fine-Grained Partition Strategy For MTP: Support for partition the MoE layer and the loss computation layer within MTP into two separate layers for scheduling. Figure 17 illustrates the computation flow before and after the MTP splitting operation. Figure 18 illustrates the differences in portion of the forward pass of several micro-batches within the interleaved 1F1B strategy, before and after applying the aforementioned optimizations. For simplicity, the figure only combines selected segments of the forward step and omits the backward step. The sample model comprises 3 dense layers, 15 MoE layers, and employs 1 MTP layer during training, with PP size of 5. In Ling 2.0 training, we observed that the computation cost of MTP layer is approximately 1.7 that of standard MoE layer. Based on this observation, we progressively refined the PP partition strategy, ultimately achieving 40% relative end-to-end improvement. Furthermore, for MoE models with balanced routing, we can increase virtual pipeline stages from VPP2 to VPP4 to reduce pipeline bubbles, yielding an additional 5% gain. However, in other cases where VPP stage contains only single MoE layer, the inter-stage blocking becomes more sensitive to imbalanced MoE routing. In such cases, our strategy may not provide end-to-end performance gains."
        },
        {
            "title": "5.3 Distributed Training Framework",
            "content": "In addition to FP8 training and heterogeneous scheduling, we also implement meticulous engineering optimizations on distributed training framework to enhance both performance and stability in Ling 2.0 training. 37 Figure 19 Illustration of the fast expert recomputation flow. 5.3.1 Intra-Node DeepEP DeepEP (DeepSeek-AI, 2024) was designed to optimize EP communication performance across nodes, which reduce significant communication overhead. During the training of the Ling 2.0 models, we do not involve cross-node EP communication, however, deploying DeepEP for intranode operations still yields substantial performance gains. Taking Ling-1T training as an example, the reduction in communication redundancy resulted in 2% end-to-end speedup, while operator fusion contributed to an additional 13% end-to-end performance improvement."
        },
        {
            "title": "5.3.2 Fused Kernels",
            "content": "During the training of Ling 2.0 models, wide range of fused operators was introduced, including RoPE Fusion, Router Fusion, and Upgrading GroupGemm (Shoeybi et al., 2019), and others. Our observations indicate that, in addition to enhancing speed by reducing memory-bound bottlenecks, these fused operators also effectively address substantial CPU overhead encountered during training, which contributes to notable improvement in end-to-end training performance."
        },
        {
            "title": "5.3.3 Fast Expert Full-Recomputation",
            "content": "To support larger models, we use full recomputation to save GPU memory, enabling larger training within fixed resources at 25% compute cost. Thanks to several recent advances from the community (Shoeybi et al., 2019; Ascend, 2023), we have identified the potential to reduce recomputation latency by half without incurring any additional cost: Recent works show that the weighted-sum computation of expert probabilities in the MoE layer can be moved forward to occur within the activation. This eliminates the dependency of the linear_fc2 and unpermute operations on the earlier results in the computation graph. As illustrated in Figure 19, unlike standard full-recomputation, we discard the recompute flow before the linear_fc2. We then run the backward pass with custom function for linear_fc2 and unpermute, and propagate its gradients back to activation function to complete the standard backward process. In Ling-2.0 training, smaller models saw end-to-end performance gains of up to 10%, while larger models achieved approximately 7%. The drop is likely due to persistent pipeline bottlenecks in complex partitioning, with some recomputation gains overlapped by pipeline bubbles."
        },
        {
            "title": "5.3.4 Long-Context Training",
            "content": "For LLMs training, long-context training is crucial. In addition to fundamental long-context training techniques, the training process of the Ling 2.0 models has been thoroughly optimized for efficiency in handling long contexts: 38 Long-Context Training with MTP: We resolved correctness issues related to loss and gradient misalignment when applying Tensor Parallelism (TP) and Context Parallelism (CP) to MTP. This fix enables long-sequence training for Ling 2.0 models with MTP. Support for Cross-Sample Attention Mask: During long-sequence training, we identified NaN issues in the cross-sample attention mask mechanism, primarily caused by the all-padding issue introduced in the CP implementation. We mitigated these issues in FusedAttention by setting cu_seqlens_padded to cu_seqlens. Performance Degradation in RoPE Fusion: In long-context training, varying sub-sequence counts per sample lead to unstable RoPE performance. We mitigate this by limiting sub-sequences and allocating resources based on the actual maximum sequence length per micro-batch, avoiding RoPE performance degradation and fluctuations."
        },
        {
            "title": "5.3.5 Framework Optimization",
            "content": "Beyond MFU optimization, daily token throughput under fixed resources also depends on the Effective Training Time Ratio (ETTR). We address this with the following framework improvements: Optimizing the Storage Latency of Distributed Checkpoints: In Distributed Checkpoint (DCP) saving, GPU Rank0 generates and verifies metadata, which is time-consuming bottleneck. Since metadata depends solely on the model architecture, we introduced metadata cache to avoid redundant computation. For Ling-1T, checkpoint save time dropped from 269s to 30s, and its share of total training time from 2.43% to 0.82%. Startup Time Optimization: To reduce the latency in the job startup phase, we construct smallscale batch prior to the first forward pass of training. This batch is passed once through both the forward and backward of the model, allowing all GPU ranks to perform warm-up computation without storing weights or updating gradients, which reduces the time required for the first training step by approximately 30%. Optimal Failover Strategy: To address unrecoverable training failures, checkpoints are periodically saved so that the latest checkpoint can be loaded after task restart. shorter checkpoint interval reduces failover loss, but the saving process incurs non-negligible overhead, making interval configuration critical. In Ling-1T training, we configure the checkpoint saving interval to be 48 minutes, which is calculated with simple strategy, and we will discuss it in Appendix A. Loss Spike Handling: Loss spikes can have significant negative impacts on both training stability and model performance. To mitigate these issues, we continued to employ the same methodology utilized in our previous work (Ling-Team et al., 2025), monitoring the training state from both the gradient and loss perspectives, and preventing the occurrence of loss spikes."
        },
        {
            "title": "5.4 Software Engineering for Foundation LLMs",
            "content": "During the training of the Ling 2.0 model and the development of the distributed framework, framework development frequently became bottleneck for model training, and in severe cases could even compromise the training outcomes. Compared with traditional software engineering, we identified the following underlying causes: Unpredictability of Outcomes: In LLMs development, whether in algorithm or engineering, it is far less predictable than in traditional software. Extensive experiments are needed to improve 39 reliability, but actual testing is often infeasible due to resource limits. Many defects only emerge late in release. Thus, enhancing outcome predictability and early risk detection is essential. Trade-offs Between Algorithms and Engineering: The results from DeepSeek-V3 indicate that only tight integration of algorithms with software and hardware systems can improve the overall ROI of projects. This requires comprehensive trade-offs in the early stages of model design for certain features, which also increases the complexity of the development process. Diversified Software Deployment Environments: In both training and inference scenarios, maximizing resource use often involves deployment across heterogeneous hardware. These platforms differ in precision and performance, making alignment of model behavior and efficiency an important research challenge. It is evident that the development process of foundation LLMs involves substantial costs and involves considerable complexity. Therefore, we propose adapting fundamental principles of software engineering to the context of foundation LLMs development, forming domain we refer to as Foundation LLMs Software Engineering. We consider Foundation LLMs Software Engineering to be research domain worthy of in-depth exploration, and further introduce the 4C (Correct, Consistent, Complete, and Co-Design) principle. Its objective is to enhance the efficiency and delivery quality of foundational model development while reducing associated costs. Based on the 4C principle, we conducted preliminary explorations into several key aspects of foundation LLMs software engineering during the training of Ling 2.0."
        },
        {
            "title": "5.4.1 Training Efficiency Optimization and Numerical Integrity Assurance",
            "content": "The LLMs training cycle typically lasts for several months. Throughout this period, continuous development and iteration of the training framework is needed to improve training efficiency. In addition, we occasionally extend the framework with new functionalities to accommodate algorithmic characteristics or to improve training stability. Throughout the development process, it is essential to ensure the consistency and correctness of model training following these updates. However, it is nearly impossible to accurately predict the ultimate impact of planned optimization once deployed to task running on large number of GPUs in parallel. To address this, we have established workflow for the iterative upgrade of large language model training frameworks, structured as cycle: Progressive Estimation Release Approval Task Monitoring and Sampling Analysis Experience Accumulation. During the entire training cycle of model, experiments are conducted under varying resource configurations, utilizing up to approximately 3% of the actual training resources. In each iteration, we validate the performance estimation results and only iterations that meet the established standards are approved for release. For correctness verification, we developed set of precision alignment and verification tools, which are applied during the development and testing phases. Finally, through continuous observation and analysis of new features, we summarize the corresponding insights and apply them to improve subsequent iterations."
        },
        {
            "title": "5.4.2 Co-design of Algorithms and Systems",
            "content": "In the development of the Ling 2.0 models, we implemented the following measures to achieve better trade-off between algorithm performance and system efficiency: Infrastructure-Aware Architecture Design: Taking the Norm Head strategy (Ling-Team et al., 2025) as an example, we found that its usage leads to performance improvement of less than 1%, 40 Figure 20 The operator efficiency comparison between the dense architecture, Ling 1.0 and Ling 2.0. while significantly increasing computation and memory consumption within single PP stage. This substantial overhead made it challenging to tune the distributed training strategy to an optimal state. Therefore, we did not employ this technique in the training of Ling 2.0 models. In addition, DeepEP sends token to up to 4 RDMA and 8 NVLink nodes. To better exploit this feature and prepare for larger-scale EP training in the future, we employ the Group Router algorithm in the MoE layer to divide all experts into 8 groups, and each token is routed within the top 4 scoring groups to maximize intra/inter-node communication efficiency. Operator Efficiency Analysis: During the design of Ling 2.0, we performed an operator efficiency comparison between the new architecture and Ling 1.0, as shown in Figure 20. Efficiency in Ling-2.0 is more centered in the mid-range, consistent with its wider and shallower design. No operators show exceptionally low efficiency, which we attribute to the First-K-Dense strategy mitigating imbalance in shallow MoE layer and improving overall computational efficiency. Parameter Design Integrated with Distributed Architecture: In the parameter design of Ling 2.0, we carried out detailed parameter configuration tailored to various heterogeneous modules. For example, in the design of Ling-1T, the computation consumption ratio between dense layers and MoE layers was set at 1:2. This design facilitates achieving uniform computation time across all PP stages, allowing us to minimize pipeline bubbles to the greatest extent possible."
        },
        {
            "title": "5.4.3 Cross-Platform Alignment",
            "content": "During the training process, in addition to using the standard Hopper architecture, we occasionally have the need to train on other heterogeneous GPU. Throughout this process, we adhere to the 4C principle to align algorithmic logic and results as closely as possible across different platforms. Figure 21 shows the alignment results based on Ling-flash-2.0. Throughout the training process, the differences in loss values consistently oscillate around zero. This indicates that although variations in operator implementations across different GPU architectures introduce floating-point precision deviations, the mean value of these errors remains within one-thousandth9. We consider such deviations insufficient to affect the accuracy of model training convergence, thus ensuring the validity of Ling 2.0 series model training on heterogeneous platforms. 9The increase in loss curve during the first 1,000 steps was caused by switch in the training data and the fact that the optimizer state of the model was not loaded. 41 Figure 21 The loss comparison of Ling-flash-2.0 on Hopper vs. Non-Hopper GPUs."
        },
        {
            "title": "5.5 Evaluation Pipeline",
            "content": "Model evaluation provides critical insights into model quality and informs continuous algorithmic and engineering optimizations based on feedback. However, at the trillion-parameter scale, traditional evaluation methods face significant challenges in stability, speed, and precision, severely constraining the development and training of Ling 2.0 models. To overcome these issues, we redesigned the entire evaluation pipeline based on OpenCompass (Contributors, 2023) to support large-scale, distributed, and incremental benchmarking. The system now integrates on-the-fly checkpoint evaluation, dynamic resource scheduling, and prompt caching to minimize redundant computation. Compared with the original OpenCompass, the total evaluation time per checkpoint was reduced by more than two-thirds. Multi-Node Inference Optimization. For large models that cannot fit on single GPU node, we extended OpenCompass to support distributed evaluation across Ray Clusters. Each evaluation task dynamically allocates multi-node multi-instance resources and interfaces with the SGLang (Zheng et al., 2024) inference backend for high-throughput serving. This allows us to handle trillion-parameter models with balanced network and GPU utilization. Prompt Caching For Repeated Prefixes. In benchmark settings that involve repeated prompts (e.g., identical few-shot templates or shared prefixes across options), recomputing log-probabilities (PPL) for each variant is wasteful. We implemented prefix-level caching that reuses the shared prompt embedding across samples, improving evaluation throughput by more than 30%. Batch Parallelization and Async Execution. The original OpenCompass implementation handled prompt preprocessing, inference, and postprocessing serially. We parallelized these steps and introduced asynchronous inference scheduling. Small requests are automatically batched into larger groups to increase GPU saturation, improving overall service efficiency and stability. These optimizations collectively make evaluation continuous feedback component of training rather than separate phase. By integrating distributed inference, prompt reuse, and asynchronous batching, we achieved significant improvements in evaluation speed, resource efficiency, and iteration velocity, ensuring that model checkpoints can be validated within hours rather than days."
        },
        {
            "title": "5.6 A Bitter Lesson of Computation-Communication Overlapping",
            "content": "Studies on large MoE models, such as DualPipe and interleaved 1F1B with A2A overlap (DeepSeekAI, 2024; NVIDIA, 2024), improve training efficiency by overlapping expert computation in one micro-batch with A2A communication in another through modified PP scheduling. We applied these methods in Ling 2.0 training, and resolving several performance issues, such as streaming multiprocessors (SMs) computationcommunication contention and CPU synchronization bottlenecks. However, the end-to-end acceleration remained limited. We have analyzed the reasons for the lack of significant end-to-end training acceleration despite these fixes. Key factors include: Overlapping Strategy Need Large EP Configuration: With fixed resources and global batch size, larger EP size assigns more tokens per expert, boosting expert-layer matrix efficiency while masking added communication overhead. However, EP group time is gated by the slowest rank. The larger EP size reduces experts per rank, exacerbating this bottleneck effect. Additionally, the performance of DeepEP itself is influenced by the balance of token routing. Imbalanced Routing in Shallow MoE Layers: Routing in the shallow layers of MoE models tends to be more imbalanced, which makes the PP rank containing these layers more prone to OOM errors. This forced us to reconsider the PP partitioning strategy to alleviate the issue, and the new approach incurred an overall performance penalty due to these constraints. In summary, large EP-based optimizations are more sensitive to routing imbalance than smaller configurations. Although Ling 2.0 training saw limited gains, we view computationcommunication overlap as key avenue for improving large-scale MoE performance and plan to jointly optimize routing and related components to better realize its potential benefits."
        },
        {
            "title": "6 Conclusion",
            "content": "Conclusion. Ling 2.0 demonstrates that large-scale sparse language foundations can advance both reasoning capability and computational efficiency through coordinated innovations in architecture, training, and infrastructure. With its high-sparsity Mixture-of-Experts design, reasoning-oriented data pipeline, multi-stage alignment strategy, and FP8-based trillion-scale infrastructure, Ling 2.0 establishes scalable foundation for general reasoning models. The three released modelsLing-mini-2.0, Ling-flash-2.0, and Ling-1Tconsistently follow the Ling Scaling Law and collectively define new Pareto frontier between reasoning accuracy and computational cost, illustrating the effectiveness of the every activation boosts principle. Despite these advances, Ling 2.0 still faces several open challenges. First, its current grouped-query attention (GQA) architecture constrains efficiency in long-context scenarios; ongoing work explores linear and sparse-attention designs to further improve scalability. Second, while Ling 2.0 achieves strong reasoning precision and efficiency, the effective reasoning length and depth still have room for enhancement. Finally, complex instruction following and agentic behaviors remain under development. Building upon Ling 2.0s strong reasoning foundation, future work will extend toward more general, autonomous, and interactive capabilities. Together, these directions mark the next step in scaling general intelligencetoward models that not only think more efficiently, but also act more generally."
        },
        {
            "title": "7 Contributors",
            "content": "Authors are listed alphabetically by the first name."
        },
        {
            "title": "Ling Team\nAng Li\nBen Liu\nBinbin Hu\nBing Li\nBingwei Zeng\nBorui Ye\nCaizhi Tang\nChangxin Tian\nChao Huang\nChao Zhang\nChen Qian\nChenchen Ju\nChenchen Li\nChengfu Tang\nChilin Fu\nChunshao Ren\nChunwei Wu\nCong Zhang\nCunyin Peng\nDafeng Xu\nDaixin Wang\nDalong Zhang\nDingnan Jin\nDingyuan Zhu\nDongke Hu\nFangzheng Zhao\nFeifan Wu\nFeng Zhu\nGangshan Wang\nHailin Zhao\nHaitao Zhang\nHanxiao Zhang\nHanzi Wang\nHao Qian\nHaoyi Yu\nHeng Zhang\nHongliang Zhang\nHongzhi Luan\nHuirong Dong\nHuizhong Li\nJia Li\nJia Liu",
            "content": "Jialong Zhu Jian Sha Jianping Wei Jiaolong Yang Jiewei Wu Jieyue Ma Jingyuan Zhang Jingyun Tian Jinjing Huang Jinquan Sun Juanhui Tu Jun Liu Jun Xu Jun Zhou Junjie Ou Junpeng Fang Kaihong Zhang Kaiqin Hu Ke Shi Kun Tang Kunlong Chen Lanyin Mei Lei Liang Lei Xu Libo Zhang Lin Ju Lin Yuan Ling Zhong Lintao Ma Lu Liu Lu Yu Lun Cai Meiqi Zhu Mengying Li Min Chen Minghao Xue Minghong Cai Mingming Yin Peijie Jiang Peilong Zhao Pingping Liu Qian Zhao Qing Cui 44 Qingxiang Huang Qingyuan Yang Quankun Yu Shaowei Wei Shijie Lian Shoujian Zheng Shun Song Shungen Zhang Shuo Zhang Siyuan Li Song Liu Ting Guo Tong Zhao Wanli Gu Weichang Wu Weiguang Han Wenjing Fang Wubin Wang Xiang Shu Xiao Shi Xiaolu Zhang Xiaoshun Lan Xiaqing Sun Xin Zhao Xingyu Lu Xiong Xu Xudong Wang Xudong(Logan) Wang Xuemin Yang Yajie Yang Yang Xiang Yanzhe Li Yi Zhang Yilong Wang Yingxue Li Yongzhen Guo Yuanyuan Wang Yue Yang Yue Yu Yufeng Deng Yun Zhang Yunfei Yu Yuqi Zhang"
        },
        {
            "title": "Yuxiao He\nYuzhuo Fu\nZengke Gui\nZhaoxin Huan\nZhaoyang Wang\nZheng Wang",
            "content": "Zhibo Zhu Zhihao Wang Zhiqiang Zhang Zhongfang Jia Zhoufei Wang Zihang Zeng denotes corresponding authors."
        },
        {
            "title": "Ziqi Liu\nZitao Xuan\nZuoli Tang",
            "content": ""
        },
        {
            "title": "References",
            "content": "Aider. Ai pair programming in your terminal, 2025. https://github.com/Aider-AI/aider. Joshua Ainslie, James Lee-Thorp, Michiel De Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Guilherme Penedo, Lewis Tunstall, Andrés Marafioti, Hynek Kydlíˇcek, Agustín Piqueres Lajarín, Vaibhav Srivastav, Joshua Lochner, Caleb Fahlgren, Xuan-Son Nguyen, Clémentine Fourrier, Ben Burtenshaw, Hugo Larcher, Haojun Zhao, Cyril Zakka, Mathieu Morlon, Colin Raffel, Leandro von Werra, and Thomas Wolf. Smollm2: When smol goes big data-centric training of small language model, 2025. https://arxiv.org/abs/2502.02737. Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. L-eval: Instituting standardized evaluation for long context language models, 2023. Ascend. Mindspeed llm, 2023. https://gitee.com/ascend/MindSpeed-LLM/. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. corr abs/2108.07732 (2021). arXiv preprint arXiv:2108.07732, 2021. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. Mislav Balunovic, Jasper Dekoninck, Ivo Petrov, Nikola Jovanovic, and Martin Vechev. Matharena: Evaluating llms on uncontaminated math competitions, February 2025. https://matharena.ai/. Denis Baylor, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo, Zakaria Haque, Salem Haykal, Mustafa Ispir, Vihan Jain, Levent Koc, Chiu Yuen Koo, Lukasz Lew, Clemens Mewald, Akshay Naresh Modi, Neoklis Polyzotis, Sukriti Ramesh, Sudip Roy, Steven Euijong Whang, Martin Wicke, Jarek Wilkiewicz, Xin Zhang, and Martin Zinkevich. Tfx: tensorflow-based production-scale machine learning platform. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, page 13871395. Association for Computing Machinery, 2017. ISBN 9781450348874. Sumithra Bhakthavatsalam, Daniel Khashabi, Tushar Khot, Bhavana Dalvi Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, and Peter Clark. Think you have solved direct-answer question answering? try arc-da, the direct-answer AI2 reasoning challenge. CoRR, abs/2102.03315, 2021. https://arxiv.org/abs/2102.03315. Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024. ByteDance-Seed. Seed-oss open-source models. https://github.com/ByteDance-Seed/seed-oss, 2025. Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Feldman, et al. Multipl-e: scalable and extensible approach to benchmarking neural code generation. arXiv preprint arXiv:2208.08227, 2022. Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q. Feldman, Arjun Guha, Michael Greenberg, and Abhinav Jangda. Multipl-e: scalable and polyglot approach to benchmarking neural code generation. IEEE Trans. Software Eng., 49(7): 36753691, 2023. doi: 10.1109/TSE.2023.3267446. https://doi.org/10.1109/TSE.2023.3267446. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. https://arxiv.org/abs/2107.03374. Sirui Chen, Changxin Tian, Binbin Hu, Kunlong Chen, Ziqi Liu, Zhiqiang Zhang, and Jun Zhou. Arrows of math reasoning data synthesis for large language models: Diversity, complexity and correctness. arXiv preprint arXiv:2508.18824, 2025. Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. Theoremqa: theorem-driven question answering dataset. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 78897901. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.489. https://doi.org/10.18653/v1/2023.emnlp-main.489. Yao Cheng, Jianfeng Chen, Jie Chen, Li Chen, Liyu Chen, Wentao Chen, Zhengyu Chen, Shijie Geng, Aoyan Li, Bo Li, et al. Fullstack bench: Evaluating llms as full stack coders. arXiv preprint arXiv:2412.00535, 2024. François Chollet. Abstraction and reasoning corpus for artificial general intelligence (arc-agi), 2019. https://github. com/fchollet/ARC-AGI. Aidan Clark, Diego de Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified scaling laws for routed language models. In International conference on machine learning, pages 40574086. PMLR, 2022. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. https://arxiv.org/abs/2110.14168. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. OpenCompass Contributors. Opencompass: universal evaluation platform for foundation models. https://github. com/open-compass/opencompass, 2023. Viet Dac Lai, Chien Van Nguyen, Nghia Trung Ngo, Thuat Nguyen, Franck Dernoncourt, Ryan Rossi, and Thien Huu Nguyen. Okapi: Instruction-tuned large language models in multiple languages with reinforcement learning from human feedback. arXiv e-prints, pages arXiv2307, 2023. DeepSeek-AI. Deepseek-v3 technical report, 2024. https://arxiv.org/abs/2412.19437. Kaustubh Deshpande, Ved Sirdeshmukh, Johannes Baptist Mols, Lifeng Jin, Ed-Yeremai Hernandez-Cardona, Dean Lee, Jeremy Kritz, Willow E. Primack, Summer Yue, and Chen Xing. MultiChallenge: realistic multi-turn conversation evaluation benchmark challenging to frontier LLMs. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Findings of the Association for Computational Linguistics: ACL 2025, pages 18632 18702, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-256-5. doi: 10.18653/ v1/2025.findings-acl.958. https://aclanthology.org/2025.findings-acl.958/. Andreas Eisele and Yu Chen. MultiUN: multilingual corpus from united nation documents. In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC10), Valletta, Malta, May 2010. European Language Resources Association (ELRA). http://www.lrec-conf.org/proceedings/lrec2010/pdf/686_Paper.pdf. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. Omni-math: universal olympiad level mathematic benchmark for large language models. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. https://openreview.net/forum?id=yaqPf0KAlN. Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozière, David Lopez-Paz, and Gabriel Synnaeve. Better & faster large language models via multi-token prediction. arXiv preprint arXiv:2404.19737, 2024. Alex Gu, Baptiste Rozière, Hugh James Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida Wang. Cruxeval: benchmark for code reasoning, understanding and execution. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. https://openreview.net/forum?id=Ffpg52swvg. Xiaotian Han, Yiren Jian, Xuefeng Hu, Haogeng Liu, Yiqi Wang, Qihang Fan, Yuang Ai, Huaibo Huang, Ran He, Zhenheng Yang, and Quanzeng You. Infimm-webmath-40b: Advancing multimodal pre-training for enhanced mathematical reasoning, 2024. https://arxiv.org/abs/2409.12568. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Lun-Wei Ku, Andre Martins, and 47 Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 38283850. Association for Computational Linguistics, 2024a. doi: 10.18653/V1/2024.ACL-LONG.211. https://doi.org/10.18653/v1/2024.acl-long.211. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. CoRR, 2024b. Conghui He, Zhenjiang Jin, Chao Xu, Jiantao Qiu, Bin Wang, Wei Li, Hang Yan, Jiaqi Wang, and Dahua Lin. Wanjuan: comprehensive multimodal dataset for advancing english and chinese large models. arXiv preprint arXiv:2308.10755, 2023. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021a. https://openreview.net/forum?id=d7KBjmI3GmQ. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Joaquin Vanschoren and Sai-Kit Yeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021b. https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/ hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html. Alex Henry, Prudhvi Raj Dachapally, Shubham Pawar, and Yuxuan Chen. Query-key normalization for transformers. arXiv preprint arXiv:2010.04245, 2020. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Jack Hong, Shilin Yan, Jiayin Cai, Xiaolong Jiang, Yao Hu, and Weidi Xie. Worldsense: Evaluating real-world omnimodal understanding for multimodal llms. CoRR, abs/2502.04326, 2025. doi: 10.48550/ARXIV.2502.04326. https://doi.org/ 10.48550/arXiv.2502.04326. Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. http://papers.nips.cc/paper_files/ paper/2023/hash/c6ec1844bec96d6d32ae95ae694e23d8-Abstract-Datasets_and_Benchmarks.html. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. https://openreview.net/forum?id=chfJJYC3iL. Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, Sanket Vaibhav Mehta, Lalit Jain, Virginia Aglietti, Disha Jindal, Peter Chen, et al. Big-bench extra hard. arXiv preprint arXiv:2502.19187, 2025. Kimi-Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in neural information processing systems, 35:38433857, 2022. Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. CMMLU: In Lun-Wei Ku, Andre Martins, and Vivek measuring massive multitask language understanding in chinese. Srikumar, editors, Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 1126011285. Association for Computational Linguistics, 2024a. doi: 10.18653/V1/2024.FINDINGS-ACL.671. https://doi.org/10.18653/v1/2024.findings-acl.671. Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, Xuanhe Zhou, Chenhao Ma, Guoliang Li, Kevin Chen-Chuan Chang, Fei Huang, Reynold Cheng, and Yongbin Li. Can LLM already serve as database interface? big bench for large-scale database grounded text-to-sqls. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023a. http://papers.nips.cc/paper_files/paper/2023/hash/ 83fc8fab1710363050bbd1d4b8cc0021-Abstract-Datasets_and_Benchmarks.html. Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, et al. Can llm already serve as database interface? big bench for large-scale database grounded text-to-sqls. Advances in Neural Information Processing Systems, 36:4233042357, 2023b. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024b. Tianle Li*, Wei-Lin Chiang*, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. From live data to high-quality benchmarks: The arena-hard pipeline, April 2024. https://lmsys.org/blog/2024-04-19-arena-hard/. Bill Yuchen Lin, Ronan Le Bras, Kyle Richardson, Ashish Sabharwal, Radha Poovendran, Peter Clark, and Yejin Choi. Zebralogic: On the scaling limits of LLMs for logical reasoning. In Forty-second International Conference on Machine Learning, 2025. https://openreview.net/forum?id=sTAJ9QyA6l. Ling-Team. Every step evolves: Scaling reinforcement learning for trillion-scale thinking model, 2025. https://arxiv. org/abs/2510.18855. Ling-Team, Binwei Zeng, Chao Huang, Chao Zhang, Changxin Tian, Cong Chen, Dingnan Jin, Feng Yu, Feng Zhu, Feng Yuan, et al. Every flop counts: Scaling 300b mixture-of-experts ling llm without premium gpus. arXiv preprint arXiv:2503.05139, 2025. Hongwei Liu, Zilong Zheng, Yuxuan Qiao, Haodong Duan, Zhiwei Fei, Fengzhe Zhou, Wenwei Zhang, Songyang Zhang, Dahua Lin, and Kai Chen. Mathbench: Evaluating the theory and application proficiency of llms with In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings hierarchical mathematics benchmark. of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 68846915. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.FINDINGS-ACL.411. https://doi.org/10.18653/v1/2024.findings-acl.411. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. rigorous evaluation of large language models for code generation. Is your code generated by chatgpt reIn Alice Oh, Tristan Naually correct? mann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, http://papers.nips.cc/paper_files/paper/2023/hash/ New Orleans, LA, USA, December 10 - 16, 2023, 2023. 43e9d647ccd3e4b7b5baab53f0368686-Abstract-Conference.html. Jingyuan Liu, Jianlin Su, Xingcheng Yao, Zhejun Jiang, Guokun Lai, Yulun Du, Yidao Qin, Weixin Xu, Enzhe Lu, Junjie Yan, et al. Muon is scalable for llm training. arXiv preprint arXiv:2502.16982, 2025a. Junnan Liu, Hongwei Liu, Linchen Xiao, Ziyi Wang, Kuikun Liu, Songyang Gao, Wenwei Zhang, Songyang Zhang, and Kai Chen. Are your LLMs capable of stable reasoning? pages 1759417632, Vienna, Austria, 2025b. Association for Computational Linguistics. https://aclanthology.org/2025.findings-acl.905/. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Hongliang Lu, Zhonglin Xie, Yaoyu Wu, Can Ren, Yuxuan Chen, and Zaiwen Wen. OptMATH: scalable bidirectional data synthesis framework for optimization modeling. In Forty-second International Conference on Machine Learning, 2025. https://openreview.net/forum?id=9P5e6iE4WK. Hongzhi Luan, Changxin Tian, Zhaoxin Huan, Xiaolu Zhang, Kunlong Chen, Zhiqiang Zhang, and Jun Zhou. Bose: systematic evaluation method optimized for base models. arXiv preprint arXiv:2503.00812, 2025. Kaijing Ma, Xeron Du, Yunran Wang, Haoran Zhang, Zhoufutu Wen, Xingwei Qu, Jian Yang, Jiaheng Liu, Minghao Liu, Xiang Yue, Wenhao Huang, and Ge Zhang. Kor-bench: Benchmarking language models on knowledge-orthogonal 49 reasoning tasks. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. https://openreview.net/forum?id=SVRRQ8goQo. MAA. American mathematics artofproblemsolving.com/wiki/index.php/American_Invitational_Mathematics_Examination?srsltid= AfmBOoqiDCiaGTLQrsRTKsZui8RFnjOZqM4qIqY3yGB3sBaqOaxwf_Xt. examination invitational 2024. 2024, MAA. American mathematics artofproblemsolving.com/wiki/index.php/American_Invitational_Mathematics_Examination?srsltid= AfmBOoqiDCiaGTLQrsRTKsZui8RFnjOZqM4qIqY3yGB3sBaqOaxwf_Xt. examination invitational 2025. 2025, https:// https:// Rabeeh Karimi Mahabadi, Sanjeev Satheesh, Shrimai Prabhumoye, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Nemotron-cc-math: 133 billion-token-scale high quality math pretraining dataset. arXiv preprint arXiv:2508.15096, 2025. Moonshot-AI. Kimi k2: Open agentic intelligence, 2025. https://moonshotai.github.io/Kimi-K2/. Thuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A. Rossi, and Thien Huu Nguyen. CulturaX: cleaned, enormous, and multilingual dataset for large language models in 167 languages. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 42264237, Torino, Italia, May 2024. ELRA and ICCL. https://aclanthology. org/2024.lrec-main.377. NVIDIA. Nvidia nemo framework, 2024. https://github.com/NVIDIA-NeMo/NeMo/. OpenAI. Multilingual massive multitask language understanding, 2024. https://huggingface.co/datasets/openai/ MMMLU. OpenAI. Gpt-5 system card. Technical report, OpenAI, Aug 2025. https://cdn.openai.com/gpt-5-system-card.pdf. Technical report. Samuel Paech. Eq-bench creative writing benchmark v3. https://github.com/EQ-bench/creative-writing-bench, 2025. Nisarg Patel, Mohith Kulkarni, Mihir Parmar, Aashna Budhiraja, Mutsumi Nakamura, Neeraj Varshney, and Chitta Baral. Multi-logieval: Towards evaluating multi-step logical reasoning ability of large language models. arXiv preprint arXiv:2406.17169, 2024. Guilherme Penedo, Hynek Kydlíˇcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the finest text data at scale, 2024. https: //arxiv.org/abs/2406.17557. Guilherme Penedo, Anton Lozhkov, Hynek Kydlíˇcek, Loubna Ben Allal, Edward Beeching, Agustín Piqueres Lajarín, Quentin Gallouédec, Nathan Habib, Lewis Tunstall, and Leandro von Werra. Codeforces. https://huggingface.co/ datasets/open-r1/codeforces, 2025. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023. Qiwei Peng, Yekun Chai, and Xuhong Li. Humaneval-xl: multilingual code generation benchmark for cross-lingual natural language generalization. In Nicoletta Calzolari, Min-Yen Kan, Véronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue, editors, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024, 20-25 May, 2024, Torino, Italy, pages 83838394. ELRA and ICCL, 2024a. https://aclanthology.org/2024.lrec-main.735. Qiwei Peng, Yekun Chai, and Xuhong Li. Humaneval-xl: multilingual code generation benchmark for cross-lingual natural language generalization. In Nicoletta Calzolari, Min-Yen Kan, Véronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue, editors, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024, 20-25 May, 2024, Torino, Italy, pages 83838394. ELRA and ICCL, 2024b. https://aclanthology.org/2024.lrec-main.735. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanitys last exam. arXiv preprint arXiv:2501.14249, 2025. Shanghaoran Quan, Jiaxi Yang, Bowen Yu, Bo Zheng, Dayiheng Liu, An Yang, Xuancheng Ren, Bofei Gao, Yibo Miao, Yunlong Feng, et al. Codeelo: Benchmarking competition-level code generation of llms with human-comparable elo ratings. CoRR, 2025. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level google-proof q&a benchmark. CoRR, abs/2311.12022, 2023. doi: 10.48550/ARXIV.2311.12022. https://doi.org/10.48550/arXiv.2311.12022. Abulhair Saparov and He He. Language models are greedy reasoners: systematic formal analysis of chain-of-thought. In The Eleventh International Conference on Learning Representations, 2023. https://openreview.net/forum?id=qFVVBzXxR2V. Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Language models are multilingual chain-of-thought reasoners. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. https://openreview.net/forum?id=fR3wGCk-IXp. Yusuxke Shibata, Takuya Kida, Shuichi Fukamachi, Masayuki Takeda, Ayumi Shinohara, Takeshi Shinohara, and Setsuo Arikawa. Byte pair encoding: text compression scheme that accelerates pattern matching. 1999. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an open corpus of three trillion tokens for language model pretraining research, 2024. https: //arxiv.org/abs/2402.00159. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018. Zhengyang Tang, Xingxing Zhang, Benyou Wang, and Furu Wei. Mathscale: Scaling instruction tuning for mathematical reasoning. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. https://openreview.net/forum?id=Kjww7ZN47M. Zichen Tang, Haihong E, Ziyan Ma, Haoyang He, Jiacheng Liu, Zhongjun Yang, Zihua Rong, Rongjin Li, Kun Ji, Qing Huang, Xinyang Hu, Yang Liu, and Qianhe Zheng. Financereasoning: Benchmarking financial numerical reasoning more credible, comprehensive and challenging. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), page 1572115749. Association for Computational Linguistics, 2025a. doi: 10.18653/v1/2025.acl-long.766. http://dx.doi.org/10.18653/v1/2025.acl-long.766. Zichen Tang, Haihong E, Ziyan Ma, Haoyang He, Jiacheng Liu, Zhongjun Yang, Zihua Rong, Rongjin Li, Kun Ji, Qing Huang, Xinyang Hu, Yang Liu, and Qianhe Zheng. Financereasoning: Benchmarking financial numerical reasoning more credible, comprehensive and challenging. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 1572115749. Association for Computational Linguistics, 2025b. https://aclanthology.org/2025.acl-long.766/. Tencent-Hunyuan. Hunyuan-7b, 2024. https://github.com/Tencent-Hunyuan/Hunyuan-7B. Changxin Tian, Kunlong Chen, Jia Liu, Ziqi Liu, Zhiqiang Zhang, and Jun Zhou. Towards greater leverage: Scaling laws for efficient mixture-of-experts language models, 2025a. https://arxiv.org/abs/2507.17702. Changxin Tian, Jiapeng Wang, Qian Zhao, Kunlong Chen, Jia Liu, Ziqi Liu, Jiaxin Mao, Wayne Xin Zhao, Zhiqiang Zhang, and Jun Zhou. Wsm: Decay-free learning rate schedule via checkpoint merging for llm pre-training, 2025b. https://arxiv.org/abs/2507.17634. 51 Jörg Tiedemann. Parallel data, tools and interfaces in OPUS. In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Mehmet gur Do gan, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC12), pages 22142218, Istanbul, Turkey, May 2012. European Language Resources Association (ELRA). http://www.lrec-conf.org/proceedings/ lrec2012/pdf/463_Paper.pdf. Jiapeng Wang, Changxin Tian, Kunlong Chen, Ziqi Liu, Jiaxin Mao, Wayne Xin Zhao, Zhiqiang Zhang, and Jun Zhou. Map: unified framework for reliable evaluation of pre-training dynamics. 2025. https://arxiv.org/abs/2510.09295. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. Mmlupro: more robust and challenging multi-task language understanding benchmark. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. http://papers.nips.cc/paper_files/paper/2024/hash/ ad236edc564f3e3156e1b2feafb99a24-Abstract-Datasets_and_Benchmarks_Track.html. Tianwen Wei, Jian Luan, Wei Liu, Shuang Dong, and Bin Wang. CMATH: can your language model pass chinese elementary school math test? CoRR, abs/2306.16636, 2023. doi: 10.48550/ARXIV.2306.16636. https://doi.org/10. 48550/arXiv.2306.16636. Yuning Wu, Jiahao Mei, Ming Yan, Chenliang Li, Shaopeng Lai, Yuran Ren, Zijia Wang, Ji Zhang, Mengyue Wu, Qin Jin, and Fei Huang. Writingbench: comprehensive benchmark for generative writing, 2025. https://arxiv.org/abs/ 2503.05244. Xin Xu, Jiaxin Zhang, Tianhao Chen, Zitong Chao, Jishan Hu, and Can Yang. Ugmathbench: diverse and dynamic benchmark for undergraduate-level mathematical reasoning with large language models. arXiv preprint arXiv:2501.13766, 2025. Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. Berkeley function calling leaderboard. https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_ leaderboard.html, 2024. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. CoRR, abs/2412.15115, 2024a. doi: 10.48550/ARXIV.2412.15115. https://doi.org/10.48550/arXiv.2412.15115. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024b. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jian Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. CoRR, abs/2505.09388, 2025a. doi: 10.48550/ARXIV.2505.09388. https://doi.org/10.48550/arXiv.2505.09388. Zhicheng Yang, Yiwei Wang, Yinya Huang, Zhijiang Guo, Wei Shi, Xiongwei Han, Liang Feng, Linqi Song, Xiaodan Liang, and Jing Tang. Optibench meets resocratic: Measure and improve LLMs for optimization modeling. In The Thirteenth International Conference on Learning Representations, 2025b. https://openreview.net/forum?id=fsDZwS49uY. Matei Zaharia, Ali Ghodsi, Reynold Xin, and Michael Armbrust. Lakehouse: new generation of open platforms that unify data warehousing and advanced analytics. In 11th Conference on Innovative Data Systems Research, CIDR 2021, Virtual Event, January 11-15, 2021, Online Proceedings. www.cidrdb.org, 2021. http://cidrdb.org/cidr2021/papers/ cidr2021_paper17.pdf. Alexander Zhang, Marcus Dong, Jiaheng Liu, Wei Zhang, Yejie Wang, Jian Yang, Ge Zhang, Tianyu Liu, Zhongyuan Peng, Yingshui Tan, Yuanxing Zhang, Zhexu Wang, Weixun Wang, Yancheng He, Ken Deng, Wangchunshu Zhou, Wenhao 52 Huang, and Zhaoxiang Zhang. Codecriticbench: holistic code critique benchmark for large language models. CoRR, abs/2502.16614, 2025a. doi: 10.48550/ARXIV.2502.16614. https://doi.org/10.48550/arXiv.2502.16614. Chenchen Zhang, Yuhang Li, Can Xu, Jiaheng Liu, Ao Liu, Shihui Hu, Dengpeng Wu, Guanhua Huang, Kejiao Li, Qi Yi, Ruibin Xiong, Haotian Zhu, Yuanxing Zhang, Yuhao Jiang, Yue Zhang, Zenan Xu, Bohui Zhai, Guoxiang He, Hebin Li, Jie Zhao, Le Zhang, Lingyun Tan, Pengyu Guo, Xianshu Pang, Yang Ruan, Zhifeng Zhang, Zhonghu Wang, Ziyan Xu, Zuopu Yin, Wiggin Zhou, Chayse Zhou, and Fengzong Lian. Artifactsbench: Bridging the visual-interactive gap in llm code generation evaluation, 2025b. https://arxiv.org/abs/2507.04952. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. SGLang: Efficient execution of structured language model programs. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. https://openreview. net/forum?id=VqkAKQibpq. Fan Zhou, Zengzhi Wang, Nikhil Ranjan, Zhoujun Cheng, Liping Tang, Guowei He, Zhengzhong Liu, and Eric Xing. Megamath: Pushing the limits of open math corpora. arXiv preprint arXiv:2504.02807, 2025. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. Qin Zhu, Fei Huang, Runyu Peng, Keming Lu, Bowen Yu, Qinyuan Cheng, Xipeng Qiu, Xuanjing Huang, and Junyang Lin. Autologi: Automated generation of logic puzzles for evaluating reasoning abilities of large language models. CoRR, abs/2502.16906, 2025. doi: 10.48550/ARXIV.2502.16906. https://doi.org/10.48550/arXiv.2502.16906. Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen Gong, James Hoang, Armel Randy Zebaze, Xiaoheng Hong, Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, Prateek Yadav, and et al. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. https://openreview.net/forum?id= YrycTjllL0. Yuxin Zuo, Shang Qu, Yifei Li, Zhang-Ren Chen, Xuekai Zhu, Ermo Hua, Kaiyan Zhang, Ning Ding, and Bowen Zhou. Medxpertqa: Benchmarking expert-level medical reasoning and understanding. In Forty-second International Conference on Machine Learning, 2025."
        },
        {
            "title": "A The Method to Compute Save Interval",
            "content": "To determine the optimal checkpoint saving interval, we devised simple and easy-tounderstand strategy. First, the impact of daily checkpoint saving and failover rollbacks on the ETTR can be expressed as: = 1440 + 2 + (4) where represents the number of failover events per day, is the time cost of each failover, denotes the storage overhead incurred by each checkpoint save, is the checkpoint saving interval, and will be constant value. By removing the constant terms and continuing the derivation, we can further obtain: = 1440 + >= 2 (cid:114) 1440 2 = 2 720 54CF (5) It is evident that when 1440C , which corresponds to the minimal impact of failover on the ETTR. In Ling-1T training, we configure the checkpoint saving interval to be 48 minutes 2 we can obtain the optimal value of E, i.e., = = Fs 2880C (cid:113) 53 Pre-training Data Details B.1 Reasoning Data B.1.1 Ling Code Corpus To support the training of high-performance coding-oriented large language models, we constructed diverse, large-scale, and quality-stratified Ling Code Corpus that integrates multiple data sources, covering source code, code-related natural language data, and synthetic instructional data. Our curation pipeline emphasizes both breadth of programming language and domain coverage, and the depth of quality control. Source Code. We collected raw source code from GitHub repositories, and conduct multistage data curation pipeline consists of: Multilingual fine-grained cleaning rules tailored to the syntax and conventions of each language. We also apply Lint-based10 syntactic validation to remove files with compilation or structural errors. This stage results in approximately 2.7 tokens of source code after deduplication, and covers 660 programming languages. We further conduct quality stratification along three dimensions as below. Code style and readability, Norm adherence and structure, and Complexity and difficulty. This stage results in 600 tokens of top-quality subset of curated code. To further enhance linguistic diversity and naturalness, we applied code rephrasing and paraphrasing techniques, generating an additional 300 tokens of augmented code data. Common CrawlBased Code Related Data. To complement GitHub-sourced material, we iteratively optimize our code-oriented html-parsers and cleaning operators to curate data from Common Crawl and Web. We conducted two-stage recall (broad recall followed by fine recall), targeting code-related pages, tutorials, and developers discussions. This process yielded approximately 700 tokens of code-related Common Crawl data, from which we further extracted 140 B+ tokens of high-quality refined corpus after rigorous filtering and normalization. CodeNLP Data. We reconstructed commit data from GHArchive11 by replaying event sequences (e.g., pull requests, issues, merges) at the repository level. This reconstruction produced rich dataset of 73 tokens of commit-level records, capturing real developer intent, revision rationale, and contextual discussions. Besides, we also include other types of code-nlp data such as notebooks. Code Contest Data. To improve problem-solving and reasoning ability, we curated large collection of programming-competition data. This includes: 1) problem statements from diverse platforms; 2) user submissions representing various solution strategies; 3) related user discussions and commentary threads. 10https://en.wikipedia.org/wiki/Lint_(software) 11https://www.gharchive.org/ 54 Figure 22 Experimental results to show the detailed performance of complete code corpus with 1B models. Synthetic Data. In addition, we incorporated small but diverse portion of synthetic data. Seed sources were drawn from programming platforms, library reference documentation, and programming concepts. We used compositional augmentation to cover broader coding concepts/topics. Evaluating the Ling Code Corpus. We designed lightweight verification strategy, i.e., training small-sized coding models (e.g., 1B size) from scratch to measure the performance of our code data. Experiments show that from-scratch training on single-type code data provides reliable proxy for full-scale performance: the resulting base models exhibit strong task competence and consistent behavioral correlation with larger-scale models. This finding enables efficient early-stage validation of architecture and training recipes before scaling to tens or hundreds of billions of parameters. We show our results on 1B models (Ling-coder-1B) compared with Qwen2.5-Coder-1.5B-Base (Hui et al., 2024) and Qwen3-1.7B-Base (Yang et al., 2025a) in Figure 22. The results are promising that we have equivalent or even better results on mainstream benchmarks compared with Qwen2.5Coder-1.5B-Base. This is achieved by consuming only 2T tokens of our code data from scratch, with an additional 300B anealing phase. B.1.2 Ling Math Corpus The mathematical proficiency of language models hinges on high-quality, diverse corpora. To train Ling 2.0 models of varying scales, we assembled mathematics corpus exceeding 1.8T tokens, drawn from web pages, textbooks, research papers, code repositories, problem banks, and synthetic sources. multi-stage processing pipelinecomprising parsing, recall, filtering, rewriting, and synthesiswas designed to curate this corpus. After careful balancing, the refined data constitutes the final pre-training mixture for our models. General Math Data. Prior to construction of Ling Math Corpus, we iteratively improved the PDF and HTML parser to ensure the completeness of mathematical content. After that, we develop multi-stage pipeline to recall highly relevant math data from diverse sources including the web (e.g. Common Crawl), book, paper, and source code. First of all, we iteratively build fastText classifier with high recall ratio to locate math data inside much smaller candidate pool. Next, we fine-tune small language models to develop 55 LLM-Filter and LLM-Refiner with 4B parameters to filter out and refine data that contain mathematical knowledge or step-by-step problem solving process. Upon applying the recall pipeline to diverse data sources along with employing deduplication technologies (e.g. MD5, MinHash), we collect substantial volume of mathematical data comprising of web, book, paper etc. Synthetic Math Data. In addition, we employ synthetic data generation to create diverse range of mathematical question-answer (Q&A) pairs, varying in difficulty and incorporating step-by-step reasoning processes. This is performed on high-quality recalled corpus sourced from multiple reputable origins. In parallel, we actively extract existing Q&A pairs from web and book corpora. Furthermore, we synthesize entirely new math problems from scratch using large-scale mathematical concept graph (Chen et al., 2025), which contains thousands of nodes and millions of edges. This approach significantly expands the knowledge boundaries of our model. To complement this, we have also developed sophisticated question generator designed to pose higher-quality and more realistic mathematical problems to the model. Evaluating the Ling Math Corpus. To empirically validate the efficacy of our mathematical corpus, we use continual-training then annealing strategy with only math corpus on pre-trained Ling-coder-1B model introduced in Section B.1.1 for over 1.8T tokens, in which the last 300B is used for annealing training. Due to the space limit, we only present the performance results on the average value of benchmarks. As shown in Figure 4b, the resulting Ling-math-1B model exhibited performance superior to the competitive Qwen2.5Math-1.5B-Base (Yang et al., 2024b) and Qwen3-1.7B-Base (Yang et al., 2025a) on mainstream mathematical benchmarks (e.g. GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), CollegeMath (Tang et al., 2024), OlympiadBench (He et al., 2024a), CMATH (Wei et al., 2023), MathBench (Liu et al., 2024) etc.). This outcome substantiates the high quality of the integrated corpus. Furthermore, specific comparative analysis was conducted to evaluate the contribution of our curated mathematical web data. Using the same 1B-model training paradigm, we benchmarked our proprietary web data against suite of well-regarded open-source datasets, namely Infi-mm-math (Han et al., 2024), finemath-3plus (Allal et al., 2025), megamath (Zhou et al., 2025), and nemotron-cc (Mahabadi et al., 2025). The Ling-math-web-1B model trained on our web data demonstrated markedly superior performance shown in figure 24. This finding empirically validates the effectiveness of our specialized web data acquisition and refinement pipeline, which constitutes critical factor in the overall strength of our pre-training data. B.2 Multilingual Data The 2TB of multilingual data is mainly from open-source web datasets, such as CulturaX (Nguyen et al., 2024) and WanJuan (He et al., 2023), and we also involve several classical parallel corpora (OPUS (Tiedemann, 2012), MultiUN (Eisele and Chen, 2010) etc.) to strengthen the cross-lingual alignment. In terms of language distribution, our multilingual corpus covers about 30 languages in different domains such as web pages, code, mathematics, Wikipedia, parallel corpora, and small amount of synthetic translation data. We first specifically include data from 18 individual languages, among which are 56 Figure 23 Experimental results of LingMathCorpus on representative benchmarks. Figure 24 Complete comparison with open-source mathematical web data. Germanic languages: German, Dutch, Swedish, Norwegian, Danish. Romance languages: Spanish, Portuguese, French, Italian, Romanian. Slavic languages: Russian, Polish, Ukrainian, Czech. Others: Vietnamese, Thai, Korean, Indonesian. Furthermore, some corpora contain mix of other languages, such as Japanese, Arabic, Hindi, Turkish, Finnish, etc. are also used. Multilingual data accounts for 4% of the pre-training corpus. The proportion by language family is about: Romance languages 50%, Germanic languages 10%, Slavic languages 57 3%, and other languages the rest. The experiments find that this distribution maintains performance in Chinese and English, while significantly improving performance for minor languages. Data from Romance and Germanic languages have less negative effect on English and Chinese benchmarks, whereas lower-quality data from Slavic and other languages, especially Arabic or Japanese, can have considerable negative effect. B.3 Data Infrastructure Training large-scale language models poses significant challenges to the efficiency, scalability, and governance of data infrastructure. To address the pain points of traditional workflows, such as inefficient collaboration, opaque lineage, and slow iteration, we built next-generation data infrastructure based on two core principles: Data-as-Code and Unified Data Lakehouse. Data-as-Code: From Manual Operations to Automated CI / CD. Our first principle is to codify the entire data processing pipeline and manage it within version control system (e.g., Git) to achieve an automated and reproducible workflow. This methodology aligns with the design principles of the leading industry ML platforms, which aim to standardize and modulize the workflows for data processing and model training, managing them through code-driven orchestration (Baylor et al., 2017). Therefore, we developed unified library for better managing AI Data Operators (AIDataOps), which centralizes over 50 data processing operators across multiple modalities, and integrated it into our automated CI/CD system. This transformation yields significant benefits: First, it provides an an end-to-end transparent and reproducible data lineage, making the origin of any data point clearly traceable. Second, it fully automates the development and backfilling of new features, dramatically increasing R&D agility by reducing the iteration cycle from months to days. Unified Data Lakehouse and Wide-Table Architecture. The second principle is the implementation of unified lakehouse architecture to consolidate disparate data sources (Zaharia et al., 2021). One of the biggest challenges for large-scale pretraining data management is that the data was scattered across hundreds of independent datasets, leading to severe data silos and inefficient experimentation. To solve this issue, we designed and implemented unified logical wide table for major domains like web pages and code. This architecture acts as the central hub for all raw, processed, and trainable data. It not only simplifies data discovery and analysis through unified view but also features elastic scalability, allowing new features to be added without full-table rebuilds. The system has been deeply optimized for large-scale training, achieving high-performance I/O of over 20 TB/hour and ensuring data processing is no longer bottleneck. By combining these two principles, we have created powerful and efficient data engine. This infrastructure was instrumental in building the Ling 2.0 corpus. For instance, it enabled us to construct wide table for all web data with trillions of records and to process 30 billion trainable data points in just two days. This system not only accelerates our current model development but also provides solid foundation for more complex data exploration in the future."
        }
    ],
    "affiliations": [
        "Inclusion AI"
    ]
}