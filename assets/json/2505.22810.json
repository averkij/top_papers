{
    "paper_title": "VidText: Towards Comprehensive Evaluation for Video Text Understanding",
    "authors": [
        "Zhoufaran Yang",
        "Yan Shu",
        "Zhifei Yang",
        "Yan Zhang",
        "Yu Li",
        "Keyang Lu",
        "Gangyan Zeng",
        "Shaohui Liu",
        "Yu Zhou",
        "Nicu Sebe"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual texts embedded in videos carry rich semantic information, which is crucial for both holistic video understanding and fine-grained reasoning about local human actions. However, existing video understanding benchmarks largely overlook textual information, while OCR-specific benchmarks are constrained to static images, limiting their ability to capture the interaction between text and dynamic visual contexts. To address this gap, we propose VidText, a new benchmark designed for comprehensive and in-depth evaluation of video text understanding. VidText offers the following key features: 1) It covers a wide range of real-world scenarios and supports multilingual content, encompassing diverse settings where video text naturally appears. 2) It introduces a hierarchical evaluation framework with video-level, clip-level, and instance-level tasks, enabling assessment of both global summarization and local retrieval capabilities. 3) The benchmark also introduces a set of paired perception reasoning tasks, ranging from visual text perception to cross-modal reasoning between textual and visual information. Extensive experiments on 18 state-of-the-art Large Multimodal Models (LMMs) reveal that current models struggle across most tasks, with significant room for improvement. Further analysis highlights the impact of both model-intrinsic factors, such as input resolution and OCR capability, and external factors, including the use of auxiliary information and Chain-of-Thought reasoning strategies. We hope VidText will fill the current gap in video understanding benchmarks and serve as a foundation for future research on multimodal reasoning with video text in dynamic environments."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 0 1 8 2 2 . 5 0 5 2 : r VidText: Towards Comprehensive Evaluation for Video Text Understanding Zhoufaran Yang2,* Yan Shu1,* Zhifei Yang3 Yan Zhang4,5 Yu Li2 Shaohui Liu2 Yu Zhou8 Nicu Sebe1 Keyang Lu6 Gangyan Zeng7 1UNITN 2HIT 3PKU 4IIE, CAS 5UCAS Equal contribution. https://github.com/shuyansy/VidText 6BUAA 7NJUST 8NKU"
        },
        {
            "title": "Abstract",
            "content": "Visual texts embedded in videos carry rich semantic information, which is crucial for both holistic video understanding and fine-grained reasoning about local human actions. However, existing video understanding benchmarks largely overlook textual information, while OCR-specific benchmarks are constrained to static images, limiting their ability to capture the interaction between text and dynamic visual contexts. To address this gap, we propose VidText, new benchmark designed for comprehensive and in-depth evaluation of video text understanding. VidText offers the following key features: 1) It covers wide range of real-world scenarios and supports multilingual content, encompassing diverse settings where video text naturally appears. 2) It introduces hierarchical evaluation framework with video-level, clip-level, and instance-level tasks, enabling assessment of both global summarization and local retrieval capabilities. 3) The benchmark also introduces set of paired perception reasoning tasks, ranging from visual text perception to cross-modal reasoning between textual and visual information. Extensive experiments on 18 state-of-the-art Large Multimodal Models (LMMs) reveal that current models struggle across most tasks, with significant room for improvement. Further analysis highlights the impact of both model-intrinsic factors, such as input resolution and OCR capability, and external factors, including the use of auxiliary information and Chain-of-Thought reasoning strategies. We hope VidText will fill the current gap in video understanding benchmarks and serve as foundation for future research on multimodal reasoning with video text in dynamic environments."
        },
        {
            "title": "Introduction",
            "content": "Large Multimodal Models (LMMs) [15] are rapidly emerging as general-purpose solutions for wide range of vision-language tasks, demonstrating impressive perception and cognitive capabilities across various multimodal benchmarks. Building on this success, there is growing interest in extending LMMs to video understanding [610], including video captioning, question-answering and retrieval. To support this development, number of video benchmarks [1115] have recently been proposed to enable more comprehensive evaluations of LMMs in dynamic visual environments. However, existing video understanding evaluations primarily focus on major events, character actions, and interpersonal relationships, while largely overlooking video text. As self-descriptive visual component, text in videos plays crucial role in visual understanding [1618]. On one hand, it provides explicit perceptual cues, such as street signs, storefronts, or subtitles, that help identify key elements and clarify the scene. On the other hand, text also enables contextual reasoning, rePreprint. Under review. Table 1: Comparison of video understanding benchmarks. Vid, Cli and Ins denote videolevel, clip-level and instance-level tasks. T, and C\" mean temporal, spatial and causal dimensions. MC and OE represent multiple-choice and open-ended questions. TQ: the percentage of questions containing visual text instances. Benchmark Video QA TQ % Multi Lingual Multi Source Multi Granularity Perception Reasoning 5,440 4,000 1,000 900 1,730 General Video Understanding Datasets 52,044 NExT-QA [21] 4,000 MVBench [13] 13,000 MovieChat-1K [9] 2,700 Video-MME [11] MLVU [12] 3,102 Video Text Datasets BovText [22] RoadText1k [23] M4ViteVQA [17] RoadTextVQA [24] EgoTextVQA [25] Ours 2,103 1,052 7,064 2,857 2,000 1000 680 329 1,507 939 40 60 52 65 Vid+Cli Vid+Cli Vid+Cli Vid+Cli+Ins Vid+Cli+Ins Ins Ins Cli+Ins Cli+Ins Cli+Ins Vid+Cli+Ins C Paired Tasks TaskType MC+OE MC MC+OE MC MC+OE OE OE MC+OE MC OE MC+OE vealing underlying motivations or causal relationships. For example, SALE sign in shop may explain why people are gathering, which is not readily apparent from visual cues alone. Compared to images, perceiving dynamic video text and modeling its interaction with evolving visual contexts in videos is significantly more challenging. It requires not only fine-grained localization at the instance level, but also temporal tracking and spotting at the clip level, as well as holistic understanding at the video level. Furthermore, video text appears in wide range of scenarios and across multiple languages, which further increases the complexity of recognition and reasoning. Based on these insights, we propose VidText, comprehensive benchmark for video text understanding, which introduces the following key advantages: It encompasses wide variety of video genres, including media, entertainment, ego-centric, sports, life record and knowledge, with 27 fine-grained categories covering diverse scenarios rich in visual text, such as scene text and subtitles. Moreover, it includes multilingual content, covering English, Chinese, Korean, Japanese, and German. It supports multi-granularity evaluation, including video-level, clip-level, and instance-level tasks. Video-level tasks involve holistic OCR understanding and reasoning over global video content. Clip-level tasks are designed to require localized comprehension based on specific temporal segments. Instance-level tasks demand fine-grained temporal and spatial grounding of individual text instances to support precise question answering. It spans from visual text perception to cross-modal reasoning with visual context. Building upon the meticulously annotated video text data, we produce video text-centric Chain-of-Thought (CoT) annotations, explicitly capturing the reasoning process between video descriptions and embedded texts, including spatial relationships with surrounding objects and temporal dependencies related to actions or events. In this way, we extend video text perception tasks into their corresponding reasoning counterparts, forming comprehensive paired perceptionreasoning framework that spans eight tasks covering multiple levels of understanding. Tab. 1 shows that VidText enables more comprehensive evaluation of video text understanding compared to both general video understanding benchmarks and video text-specific benchmarks. We conduct extensive evaluations on 18 popular LMMs, revealing several important insights. First, video text understanding remains technically challenging task for existing models. Although Gemini 1.5 Pro [19] achieves the highest performance, it only reaches an average score of 46.8%, and all models perform poorly on multi-granularity tasks, which is far below estimated human-level performance. Second, several concurrent open-source models [6, 20] demonstrate competitive performance, narrowing the gap with proprietary systems. Third, our empirical findings highlight several influential factors in video text understanding, including the models image OCR capability, input resolution, the integration of auxiliary information, and the role of Chain-of-Thought reasoning strategies. Therefore, we believe VidText serves as valuable complement to existing general video understanding benchmarks, while also providing new insights for the OCR and multimodal reasoning communities."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Video Large Language Models With the rapid advancement of large language models (LLMs), series of video large language models (Video LLMs) have emerged [3, 2628], leveraging LLMs as backbones to enhance video reasoning capabilities. Early Video LLMs primarily relied on sparsely sampled frames and temporal modeling mechanisms [5, 29], such as Q-Former and temporal pooling, to facilitate video captioning and question answering. Building upon these designs, subsequent models [1, 2, 6, 30 35] have focused on addressing key challenges in video understanding, including fine-grained semantic alignment, temporal representation, and long-duration video comprehension. For instance, Qwen-VL 2.5 [1] introduces dynamic resolution processing and absolute temporal encoding to handle variable-resolution videos. Video-LLaMA3 [6] applies frame compression strategy based on frame similarity to reduce the number of visual tokens, resulting in more compact and precise video representations. To handle extremely long videos, LongVA [32] extends the context length of the LLM backbone and transfers its long-context capability to the video domain. Video-XL [33] leverages the inherent key-value sparsification mechanism of LLMs to efficiently condense visual inputs. VideoChatFlash [30] proposes hierarchical compression strategy, reducing token redundancy in both the video and language modules. 2.2 Video Understanding Benchmarks With the growing interest in video LLMs, the development of dedicated benchmarks has become increasingly emphasized. Existing benchmarks have been designed for variety of video understanding tasks, including action reasoning, spatio-temporal inference, video captioning, and longvideo comprehension [1114, 21, 3638]. For example, NeXT-QA [21] evaluates temporal reasoning abilities by testing models on the relationships between human actions. VideoChatGPT-Bench [38] focuses on open-ended video conversation, constructing captioning and dialogue tasks to assess generative and interactive capabilities. TempCompass [37] introduces fine-grained temporal perturbations to assess whether models can answer questions based on temporal changes within the video. To support comprehensive video question answering, MVBench [13] proposes large-scale benchmark covering 20 distinct subtasks, spanning multiple perception and reasoning dimensions. For long-video understanding, VideoMME [11], MLVU [12], LVBench [39] and LongVideoBench [14] curate diverse and extended-duration videos to evaluate multi-level abilities across extended temporal contexts. As text carries rich and structured information in videos, several benchmarks have been proposed to evaluate video text understanding [17, 22, 23, 25, 40], including tasks such as text tracking, spotting, and reasoning. Specifically, RoadTextVQA [23] focuses on autonomous driving scenarios, while EgoTextVQA [25] targets egocentric perspectives in daily life settings. In addition, M4-ViteVQA [17] collects videos from nine diverse real-world scenarios, such as shopping, traveling, and movies, to evaluate the generalization capabilities of video-language models. However, these benchmarks exhibit two notable limitations. First, their task types are relatively simple, and therefore insufficient for comprehensively evaluating the diverse capabilities of modern video LLMs. Second, their video categories and language coverage remain limited, often constrained to specific application domains."
        },
        {
            "title": "3 Dataset Construction",
            "content": "In this section, we describe the dataset construction process for VidText. We begin by illustrating how the source videos are collected (Sec. 3.1), followed by detailed explanation of the annotation pipeline (Sec. 3.2). Finally, we describe the task taxonomy of our benchmark (Sec. 3.3). 3.1 Video Collection In VidText, we aim to evaluate video text understanding across diverse scenarios, including both video category variety and language diversity. While several existing datasets [17, 22, 23, 40] provide detailed text annotations, they all suffer from several key limitations: (1) Limited scenario diversity: Most datasets focus on specific domains such as indoor scenes or egocentric videos, lacking coverage of richer, more interactive contexts such as sports events, livestreaming games, or daily 3 Figure 1: Statistical overview of our VidText. (Left) Video genres included in VidText. (Top Right) Visual Text Instance Distribution. (Bottom Right) Hierarchical Task type settings. vlogs. (2) Lack of language diversity: Nearly all existing datasets contain only English, failing to reflect the multilingual nature of real-world video text. (3) Short video duration: Many videos are only 1015 seconds long, which limits their suitability for tasks involving cross-temporal reasoning or holistic understanding. Therefore, in addition to incorporating existing datasets, we further collect video data from comprehensive long-form video benchmarks [11, 12] and public platforms such as YouTube, in order to enhance the scenario diversity, temporal richness, and linguistic coverage of VidText. For the manually collected videos, we leverage expert models to construct an effective selection pipeline. First, we ensure the presence of visual text in each video by using Gomatching [41], video text detection tool, to assess text density. Second, we filter out low-quality videos containing blur, watermarks, or low resolution, using existing video quality assessment models [42, 43]. Third, we enforce minimum duration threshold of 3 minutes to guarantee sufficient temporal content. As result, we collect total of 939 high-quality videos, each annotated with one of 27 predefined scene categories. Additionally, we record metadata for each video, including language type, resolution, frame rate, and text density. Fig. 1 presents basic statistics of VidText. More detailed statistics across multiple dimensions are provided in the Supplementary Materials. 3.2 Annotations Generation To support evaluation at both the perception and reasoning levels, VidText provides meticulously constructed annotations tailored to the requirements of each task. Perception. For each qualified video, we adopt bottom-up strategy to construct multi-granularity annotations, including instance-level, clip-level, and video-level information. First, annotators are instructed to track at least three clear visual text instances throughout the video. For each instance, we conduct frame-by-frame fine-grained annotation until it disappears, generating sequence of annotations that include bounding boxes, transcriptions, and unique track IDs. Second, the video is segmented into multiple intervals based on its duration (i.e., longer videos are divided into more segments). For each segment, we check the presence of visual text using instance-level annotations and record clip-level labels, including the temporal span (start and end timestamps) and associated transcriptions. Third, separate group of annotators performs video-level annotations, which involve recording all distinct transcriptions that appear across the entire video. Specifically, for Chinese, we use text lines as the basic annotation unit, while for other languages, annotations are performed at the word level. Reasoning. Since the multi-granularity annotations constructed for perception address the question of what texts appear in the video or clip, we further investigate what actions or events are linked to these texts. To this end, we design video text-centric Chain-of-Thought (CoT) annotation pipeline. First, for each video or clip (as defined by the time span annotations), we apply an adapFigure 2: Examples from VidText. The benchmark includes eight tasks, featuring paired perception and reasoning components designed to evaluate the video-level, clip-level, and instance-level capabilities of LMMs. Given the video input and textual prompt, models are required to solve the tasks, with ground-truth answers highlighted in green. 5 tive sampling strategy to extract key frames. Then, we utilize the powerful vision-language model Aria [44] to generate high-quality frame-level captions, capturing both intra-frame and inter-frame contextual information. Based on the paired OCR transcripts and the multimodal descriptions, human annotators are instructed to design QA pairs that focus on the semantic or causal relationships between visual text and surrounding visual content. To ensure the quality of reasoning QA pairs, we enforce two post-validation principles: (1) Mask the visual texts and verify whether the question can be answered using only the visual content; (2) Mask the visual frames and check whether the question can be answered using only the textual information. 3.3 Task Taxonomy Based on the detailed annotations encompassing perception and reasoning, we further define 8 hierarchical tasks which are demonstrated on Fig. 2. Holistic OCR & Holistic Reasoning. Holistic OCR requires the model to recognize all visual texts appearing throughout the entire video. Redundant entries are removed, and the remaining text instances are sorted in chronological order. We evaluate this task using the F1-score, which is calculated based on instance-level precision and recall. Holistic Reasoning assesses the models ability to understand the overall topic of the video by integrating recognized textual information with global semantic context. The task is formulated as multi-label selection problem, where the model is asked to choose three correct answers from seven candidate options. Performance is measured by top-3 accuracy. Local OCR & Local Reasoning. In contrast to holistic tasks, Local OCR and Local Reasoning focus on the models ability to spot and interpret visual text within user-specified video segments. Local OCR requires recognizing all visual texts that appear within given segment and is evaluated using the F1-score based on instance-level matching. Local Reasoning assesses the models ability to infer local semantic meaning or intent from the text. It is formulated as multiple-choice question, and performance is measured by answer accuracy. Text Localization & Temporal Causal Reasoning. Similar to temporal grounding tasks, Text Localization requires the model to accurately predict the temporal interval during which specific text appears in the video. The task is evaluated using Mean Intersection-over-Union (mIoU) based on ground-truth temporal spans. The corresponding reasoning task, Temporal Causal Reasoning, extends beyond localization to assess whether the model can infer causal relationships between identified texts and subsequent multimodal events or actions. Standard evaluation is conducted in multiple-choice format, with accuracy as the performance metric. Text Tracking & Spatial Reasoning. Given target text instance, Text Tracking requires the model to predict its spatial bounding box locations at its first and last appearance within the video. Spatial Reasoning extends this task by asking the model to infer spatial relationships between the textual instance and surrounding visual elements at specified timestamp. To enable standardized evaluation with LMMs, both tasks are formatted as multiple-choice questions."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Settings We conduct comprehensive evaluation of 18 large multimodal models (LMMs) using our VidText benchmark, encompassing both open-source and proprietary models. For proprietary models, we evaluate the Gemini series [19] and GPT series [45, 46], using their official multi-image evaluation APIs. For open-source models, we select current state-of-the-art video LMMs with diverse architectures and LLM sizes, enabling broad assessment of video text understanding capabilities. All evaluations are conducted in zero-shot manner. More details about the evaluation settings are provided in the Supplementary Materials. 4.2 Main Results The overall evaluation results for all investigated LMMs in the VidText are shown in Tab. 2. Individual performances are reported for each task, while average performances are provided. From the results, we derive three primary conclusions: 6 Table 2: The overall performance on VidText. HoliOCR: Holistic OCR; HoliRea.: Holistic Reasoning; LocalOCR: Local OCR; LocalRea.: Local Reasoning; TextLocal.: Text Localization; TempCauRea.: Temporal Causal Reasoning; TextTrac.: Text Tracking; SpaRea.: Spatial Reasoning; Avg.: the average performance of the eight tasks. The best Accuracy / Score results are highlighted. Method Size Avg. HoliOCR HoliRea. LocalOCR LocalRea. TextLocal. TempCauRea. TextTrac. SpaRea. Human proprietary LMMs GPT-4-Turbo [45] Gemini 1.5 Flash [19] GPT-4o [46] Gemini 1.5 Pro [19] Open-source LMMs LongVU [47] Qwen2.5-VL [1] Video-XL-Pro [34] LongVA [32] MiniCPM-V2.6 [48] VideoChatFlash [30] Qwen2-VL[49] Qwen2.5-VL [1] VideoLLaMA3 [6] ShareGPT4Video [50] Oryx-1.5 [29] LLava-OV[51] Qwen2.5-VL [1] InternVL2.5 [20] 89.5 29.7 34.7 40.2 45.3 17.0 3B 21.1 3B 22.5 3B 19.2 7B 26.5 7B 29.2 7B 30.3 7B 31.9 7B 39.9 7B 8B 16.4 32B 35.4 72B 36.1 72B 38.5 78B 39.8 92.8 22.9 26.3 29.5 34. 5.8 11.4 10.9 4.8 29.2 13.6 27.0 35.9 23.5 2.5 35.3 20.1 40.1 40.2 96.0 28.7 34.0 38.9 43.6 20.4 23.2 22.9 5.6 21.2 13.3 34.0 36.0 31.5 2.6 33.9 28.1 49.3 37.4 94.3 36.7 40.2 46.0 50. 15.4 28.5 30.4 3.2 11.4 1.0 37.5 37.0 39.2 0.8 30.8 41.3 35.9 29.0 95.7 36.5 42.4 43.3 50.1 17.0 17.8 15.6 46.9 42.9 50.1 23.7 26.5 41.2 43.5 48.5 49.4 28.2 50.4 81.3 15.8 28.9 45.5 48. 15.6 18.7 18.7 4.5 13.3 45.1 11.2 26.5 47.3 0.0 26.7 9.9 28.7 30.5 88.6 39.4 40.0 42.5 47.0 15.9 15.4 27.9 28.3 30.3 42.4 42.4 35.4 55.6 27.3 45.2 54.6 52.5 48.5 80.3 24.3 30.7 36.2 40. 15.4 18.3 20.9 29.6 20.5 23.3 24.6 22.4 31.1 28.0 26.0 31.8 31.1 29.9 87.3 33.6 35.4 39.8 47.9 30.5 35.3 32.9 30.5 43.2 44.3 42.1 35.2 50.0 26.1 36.4 53.4 42.1 52.3 1) Gemini 1.5 Pro [19] achieves the best performance on our benchmark. It significantly outperforms other models on video-text-based perception and reasoning tasks. 2) Proprietary models typically perform better than open-source models. However, some opensource models deliver surprisingly strong results on specific tasks. For example, VideoLLaMA3 [6] achieves the highest performance on both Temporal Causal Reasoning and Spatial Reasoning. 3) Video text understanding remains an overwhelming challenge for current video LMMs. First, even the best models still fall far short of human-level performance. Second, most LMMs show limited ability in fundamental video OCR tasks, where specialized video OCR models often perform better. Third, multimodal reasoning based on visual text cues in videos is significantly more difficult than in images: all video multiple-choice reasoning tasks yield accuracies below 60%, falling far behind the performance on similar image-based tasks. (ST-VQA [52] and Text-VQA [53]) Beyond the primary conclusions on overall performance, we further analyze model behaviors across individual tasks. 4) Among multi-granular tasks, video-level and instance-level tasks are more challenging than clip-level tasks, across both perception and reasoning settings. We hypothesize that this is due to the limited capabilities of current LMMs in two aspects: video-level tasks require global information aggregation, while instance-level tasks demand fine-grained retrieval and grounding, both of which remain weak points for existing models. 5) For video-level and instance-level tasks, the performance of perception and reasoning shows strong correlation, while the two appear relatively independent in clip-level tasks. This may be because certain clip-level perception tasks, such as text localization, require accurate temporal grounding based on fine-grained visual cues. However, the corresponding reasoning tasks, such as temporal reasoning, can often be solved using local visual clues from sparsely sampled frames, allowing models to bypass the need for precise perception outputs. 6) Scaling up the size of LLMs leads to more significant performance gains on reasoning tasks compared to perception tasks. This suggests that video text perception cannot be effectively improved by model scale alone, and instead requires careful architectural design, specialized training data, and other task-specific considerations."
        },
        {
            "title": "5 Ablation Studies",
            "content": "This section begins with an investigation into the effectiveness of our hierarchical task design, including the multi-granularity structure and the extension from perception to reasoning. We also 7 Figure 3: Ablation studies on the multi-granularity design of VidText. Figure 4: Ablation studies on the joint reasoning of video texts and video contents. HR, LR and SR denote Holistic Reasoning, Local Reasoning and Spatial Reasoning, respectively. We visualize Video content masking and Video Text masking in the right part. explore critical factors that affect model performance in video text understanding through series of ablation studies. 5. Investigating the effectiveness of VidText Design Multi-granularity design. VidText includes multi-granular tasks spanning video-level, clip-level, and instance-level. To verify that tasks at different levels require correspondingly different levels of contextual information, we conduct ablation studies using VideoLLaMA3 [6] and Qwen2.5-VL [1]. Specifically, for holistic tasks, we randomly extract 50% of the video duration as segment and evaluate performance on Holistic Reasoning. For clip-level and instance-level tasks, we select key clips based on their original task annotations. As shown in Fig. 3, clip-level and instance-level tasks benefit significantly from segment-based evaluation, as key frames provide concentrated visual text information. In contrast, Holistic Reasoning performance declines, as the task requires global information aggregation, which is lost when only partial segments are used. Joint video text and multimodal contexts reasoning. VidText successfully extends perceptionlevel tasks into reasoning tasks, which require the joint modeling of video texts and their multimodal contextual information. To validate this, we perform an ablation study by selectively masking either the visual text regions or the surrounding video content at varying random ratios. As shown in Fig. 4, the performance on all reasoning tasks consistently drops as the masking ratio increases, confirming that both textual and visual cues are essential for reasoning under our task design. 5.2 Exploring Crucial Factors of Video Text Understanding Model-intrinsic Factors. As shown in Tab. 3, we conduct ablation studies on several influential factors. First, we examine the impact of input resolution using two representative models, Oryx-1.5 [29] and InternVL2.5 [2], both of which support adjustable input sizes. Increasing the resolution significantly improves video text understanding performance, especially in InternVL2.5 [2], where the input images are divided into sub-patches, where higher resolution allows better preservation of text details. Second, to assess the role of OCR capability, we refer to each models performance on standard OCR benchmarks such as OCRBench [54]. The results show that models video text unImpact of input resolution Impact of OCR ability Impact of LLM Model Oryx-1.5 InternVL Model LLaVA-OV resolution Avg 35.4 38.63.2 VideoLLaMA3 39.8 GPT4V 44.85.0 GPT-4o 4482 8962 4482 8962 OCRBench Avg 36.1 39.93.8 29.7 40.210.5 621 828 645 822 Model LLaVA-OV LLaVA-Next LLM Avg Qwen2-7B 22.1 Qwen2-72B 36.114.0 LLaMA3-8B 15.3 Qwen2-7B 20.8 5.2 Table 3: Detailed analysis about the impact of input resolution, image OCR ability, and LLM Backbone. LLaVA-Next: LLaVA-Next-Video [31]. LR TR SR Method Method Qwen2.5-VL Qwen2.5-VL + Audio Qwen2.5-VL + Text Qwen2.5-VL + Audio + Text HR 36.0 36.3 37.2 37.6 26.5 26.6 28.3 29.5 35.4 35.2 37.9 38.0 35.2 35.4 38.1 39.5 Qwen2.5-VL Qwen2.5-VL + CoT VideoLLaMA3 31.5 VideoLLaMA3 + CoT 33. HR 36.0 40.5 LR TR SR 26.5 28. 41.2 44.6 35.4 37.2 55.6 56.2 35.2 40.9 50.0 53.8 Table 4: Ablations about auxiliary information (Left) and CoT strategy (Right) to video text understanding.HR, LR. TR and SR mean Holistic Reasoning, Local Reasoning, Temporal Causal Reasoning and Spatial Reasoning. derstanding performance generally aligns with its fundamental OCR accuracy. Finally, we compare different LLM backbones and find that certain architectures (e.g., Qwen2.5) exhibit stronger performance in multilingual scenarios, often outperforming LLaMA-based variants. These observations collectively indicate that video text understanding is influenced by combination of input fidelity, OCR capacity, and language modeling strength. External Factors. As demonstrated in Tab. 4, we first investigate whether external auxiliary information can enhance video text understanding, particularly for reasoning tasks. In this study, we consider audio transcripts and video text (e.g., subtitles or OCR outputs), both of which can be extracted using specialized tools. We convert these modalities into textual sequences and append them to the original query as contextual subtitles. As shown in our experiments, both sources contribute positively to performance. Video text provides stronger gains in global tasks that require long-range context, while audio transcripts are more beneficial for local tasks, possibly due to their alignment with short-term actions or events. Second, we propose video text-centric Chain-ofThought (CoT) reasoning strategy, which decomposes complex reasoning processes into structured sub-steps. Specifically, the video is uniformly segmented into multiple clips. For each clip, the model is prompted to: (1) spot all visible texts, (2) generate detailed description of the clip, and (3) infer whether any visual texts are semantically related to the description and answer the reasoning question accordingly. This CoT-based prompting strategy yields consistent improvements across all reasoning tasks, highlighting the potential of test-time reasoning augmentation for video-language models."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper presents VidText, novel benchmark for evaluating video text understanding in large multimodal models (LMMs). With three key innovations, including broad scenario and multilingual coverage, multi-granular evaluation framework, and paired perceptionreasoning tasks, VidText enables comprehensive and in-depth analysis of LMM performance on video text understanding. Empirical studies reveal that current LMMs still face significant challenges in both perceiving and reasoning over video texts. Future progress in this area will require the joint optimization of multiple complex factors, including model-intrinsic aspects (such as input resolution, OCR capability, and LLM backbone) and external strategies (such as auxiliary modality integration and Chain-ofThought prompting). We hope VidText will serve as valuable resource for advancing research in the OCR and video understanding communities."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [2] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198, 2024. [3] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [4] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:2371623736, 2022. [5] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 1973019742. PMLR, 2329 Jul 2023. [6] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025. [7] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. [8] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. arXiv preprint arXiv:2311.17043, 2023. [9] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1822118232, 2024. [10] Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Deyao Zhu, Jian Ding, and Mohamed Elhoseiny. Minigpt4-video: Advancing multimodal llms for video understanding with interleaved visual-textual tokens. arXiv preprint arXiv:2404.03413, 2024. [11] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comarXiv preprint prehensive evaluation benchmark of multi-modal llms in video analysis. arXiv:2405.21075, 2024. [12] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. [13] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2219522206, 2024. [14] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for longcontext interleaved video-language understanding. Advances in Neural Information Processing Systems, 37:2882828857, 2024. 10 [15] Keshigeyan Chandrasegaran, Agrim Gupta, Lea Hadzic, Taran Kota, Jimming He, Cristóbal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, and Fei-Fei Li. Hourvideo: 1-hour videolanguage understanding. Advances in Neural Information Processing Systems, 37:53168 53197, 2024. [16] Yan Zhang, Gangyan Zeng, Huawen Shen, Daiqing Wu, Yu Zhou, and Can Ma. Track the answer: Extending textvqa from image to video with spatio-temporal clues. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1027510283, 2025. [17] Minyi Zhao, Bingjia Li, Jie Wang, Wanqing Li, Wenjing Zhou, Lan Zhang, Shijie Xuyang, Zhihang Yu, Xinkun Yu, Guangze Li, et al. Towards video text visual question answering: Benchmark and baseline. Advances in Neural Information Processing Systems, 35:35549 35562, 2022. [18] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. Videoclip: Contrastive pre-training for zeroshot video-text understanding. arXiv preprint arXiv:2109.14084, 2021. [19] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [20] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of arXiv preprint open-source multimodal models with model, data, and test-time scaling. arXiv:2412.05271, 2024. [21] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of questionIn Proceedings of the IEEE/CVF conference on answering to explaining temporal actions. computer vision and pattern recognition, pages 97779786, 2021. [22] Weijia Wu, Yuanqiang Cai, Debing Zhang, Sibo Wang, Zhuang Li, Jiahong Li, Yejun Tang, and Hong Zhou. bilingual, openworld video text dataset and end-to-end video text spotter with transformer. arXiv preprint arXiv:2112.04888, 2021. [23] Sangeeth Reddy, Minesh Mathew, Lluis Gomez, Marçal Rusinol, Dimosthenis Karatzas, and CV Jawahar. Roadtext-1k: Text detection & recognition dataset for driving videos. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 1107411080. IEEE, 2020. [24] George Tom, Minesh Mathew, Sergi Garcia-Bordils, Dimosthenis Karatzas, and CV Jawahar. Reading between the lanes: Text videoqa on the road. In International Conference on Document Analysis and Recognition, pages 137154. Springer, 2023. [25] Sheng Zhou, Junbin Xiao, Qingyun Li, Yicong Li, Xun Yang, Dan Guo, Meng Wang, TatSeng Chua, and Angela Yao. Egotextvqa: Towards egocentric scene-text aware video question answering. arXiv preprint arXiv:2502.07411, 2025. [26] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. [27] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [28] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences, 67(12):220101, 2024. [29] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx arXiv preprint mllm: On-demand spatial-temporal understanding at arbitrary resolution. arXiv:2409.12961, 2024. [30] Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, et al. Videochat-flash: Hierarchical compression for long-context video modeling. arXiv preprint arXiv:2501.00574, 2024. [31] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. [32] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. [33] Yan Shu, Zheng Liu, Peitian Zhang, Minghao Qin, Junjie Zhou, Zhengyang Liang, Tiejun Huang, and Bo Zhao. Video-xl: Extra-long vision language model for hour-scale video understanding. arXiv preprint arXiv:2409.14485, 2024. [34] Xiangrui Liu, Yan Shu, Zheng Liu, Ao Li, Yang Tian, and Bo Zhao. Video-xl-pro: RearXiv preprint constructive token compression for extremely long video understanding. arXiv:2503.18478, 2025. [35] Huaying Yuan, Zheng Liu, Minhao Qin, Hongjin Qian, Shu, Zhicheng Dou, and Ji-Rong Wen. Memory-enhanced retrieval augmentation for long video understanding. arXiv preprint arXiv:2503.09149, 2025. [36] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception test: diagnostic benchmark for multimodal video models. Advances in Neural Information Processing Systems, 36:4274842761, 2023. [37] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, arXiv preprint and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv:2403.00476, 2024. [38] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. [39] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao Dong, et al. Lvbench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024. [40] Weijia Wu, Yuzhong Zhao, Zhuang Li, Jiahong Li, Mike Zheng Shou, Umapada Pal, Dimosthenis Karatzas, and Xiang Bai. Icdar 2023 competition on video text reading for dense and small text. In International Conference on Document Analysis and Recognition, pages 405 419. Springer, 2023. [41] Haibin He, Maoyuan Ye, Jing Zhang, Juhua Liu, Bo Du, and Dacheng Tao. Gomatching: simple baseline for video text spotting via long and short term matching. arXiv preprint arXiv:2401.07080, 2024. [42] Wen Wen, Mu Li, Yabin Zhang, Yiting Liao, Junlin Li, Li Zhang, and Kede Ma. Modular blind video quality assessment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 27632772, 2024. [43] Yachun Mi, Yan Shu, Yu Li, Chen Hui, Puchao Zhou, and Shaohui Liu. Clif-vqa: Enhancing video quality assessment by incorporating high-level semantic information related to human In Proceedings of the 32nd ACM International Conference on Multimedia, pages feelings. 99899998, 2024. [44] Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Fan Zhou, Chengen Huang, Yanpeng Li, et al. Aria: An open multimodal native mixture-of-experts model. arXiv preprint arXiv:2410.05993, 2024. 12 [45] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [46] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [47] Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, et al. Longvu: Spatiotemporal adaptive compression for long video-language understanding. arXiv preprint arXiv:2410.17434, 2024. [48] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. [49] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [50] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Zhenyu Tang, Li Yuan, et al. Sharegpt4video: Improving video understanding and generation with better captions. Advances in Neural Information Processing Systems, 37:1947219495, 2024. [51] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [52] Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Marçal Rusinol, Ernest Valveny, CV Jawahar, and Dimosthenis Karatzas. Scene text visual question answering. In Proceedings of the IEEE/CVF international conference on computer vision, pages 42914301, 2019. [53] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi In Proceedings of the Parikh, and Marcus Rohrbach. Towards vqa models that can read. IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. [54] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12):220102, 2024. [55] Yan Shu, Weichao Zeng, Fangmin Zhao, Zeyu Chen, Zhenhang Li, Xiaomeng Yang, Yu Zhou, Paolo Rota, Xiang Bai, Lianwen Jin, et al. Visual text processing: comprehensive review and unified evaluation. arXiv preprint arXiv:2504.21682, 2025. [56] Zhenhang Li, Yan Shu, Weichao Zeng, Dongbao Yang, and Yu Zhou. First creating backarXiv preprint grounds then rendering texts: new paradigm for visual text blending. arXiv:2410.10168, 2024. [57] Weichao Zeng, Yan Shu, Zhenhang Li, Dongbao Yang, and Yu Zhou. Textctrl: Diffusion-based scene text editing with prior guidance control. Advances in Neural Information Processing Systems, 37:138569138594, 2024. [58] Shangbang Long, Xin He, and Cong Yao. Scene text detection and recognition: The deep learning era. International Journal of Computer Vision, 129(1):161184, 2021. [59] Yingying Zhu, Cong Yao, and Xiang Bai. Scene text detection and recognition: Recent advances and future trends. Frontiers of Computer Science, 10:1936, 2016. [60] Mingxin Huang, Yuliang Liu, Zhenghao Peng, Chongyu Liu, Dahua Lin, Shenggao Zhu, Nicholas Yuan, Kai Ding, and Lianwen Jin. Swintextspotter: Scene text spotting via better synergy between text detection and text recognition. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 45934603, 2022. 13 [61] Xinyu Zhou, Cong Yao, He Wen, Yuzhi Wang, Shuchang Zhou, Weiran He, and Jiajun Liang. East: an efficient and accurate scene text detector. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 55515560, 2017. [62] Minghui Liao, Zhaoyi Wan, Cong Yao, Kai Chen, and Xiang Bai. Real-time scene text detection with differentiable binarization. In Proceedings of the AAAI conference on artificial intelligence, pages 1147411481, 2020. [63] Yan Shu, Wei Wang, Yu Zhou, Shaohui Liu, Aoting Zhang, Dongbao Yang, and Weipinng Wang. Perceiving ambiguity and semantics without recognition: an efficient and effective ambiguous scene text detector. In Proceedings of the 31st ACM International Conference on Multimedia, pages 18511862, 2023."
        },
        {
            "title": "A Overview of Appendix",
            "content": "B: Limitations. C: Broader Impact. D: Collecting Details of VidText. E: Details of Annotation. F: Detailed Experimental Results. G: Model Prompt. H: More Visualization Results. Checklist"
        },
        {
            "title": "B Limitation",
            "content": "We summarize the limitations of our work as follows: Limited scenario coverage: Although VidText includes 27 fine-grained video categories, it still lacks representation of long-tail or high-risk domains such as medical emergencies, industrial workflows, or disaster scenarios. Imbalanced language distribution: The majority of samples are in English and Chinese, with significantly fewer examples in other languages such as German, Korean, and Japanese. This imbalance prevents thorough evaluation of multilingual OCR and reasoning capabilities. Scarcity of challenging text instances: VidText contains relatively few examples involving difficult text conditions such as severe occlusion, low resolution, motion blur, unusual fonts, or multi-line arrangements. This limits the benchmarks ability to fully assess model robustness under real-world noise and distortion. B.1 Discussion These dataset and model limitations are mutually reinforcing. Dataset gaps may conceal important weaknesses in current models, while existing models deficiencies highlight the need for broader and more diverse benchmarks. Future efforts should focus on expanding long-tail scene and language coverage in VidText, while also improving LMM architectures with better multilingual OCR, noise robustness, and cross-modal reasoning abilities. Furthermore, we also summarize three insights as follows: Weak cross-domain transfer: Most LMMs are pretrained on image-based OCR tasks and struggle to generalize to unseen video scenes, such as sports broadcasts or livestream interfaces, where text appearance and context are highly dynamic. Insufficient multilingual alignment: Current models show limited ability in detecting, transcribing, and semantically linking non-English texts to the visual context, resulting in degraded performance on multilingual content. Low robustness to visual noise: Models often fail when confronted with noisy, blurry, or occluded text, particularly in tasks requiring instance-level grounding. This degrades downstream reasoning performance and reflects need for stronger visual resilience."
        },
        {
            "title": "C Broader Impact",
            "content": "The VidText benchmark is poised to make significant contribution to both the OCR and video understanding communities by bridging the gap between low-level text perception [5557] and highlevel semantic reasoning [58, 59] in video contexts. For the OCR community, VidText offers valuable opportunity to move beyond traditional imagebased text detection and recognition [6063]. By shifting the focus to temporal and contextual dynamics in videos, it promotes the development of algorithms that can track, ground, and interpret visual texts over time. 15 For the video understanding community, VidText introduces the underexplored yet semantically rich modality of scene text into the landscape of video-language research. By incorporating fine-grained text perception tasks and their paired reasoning counterparts, VidText pushes video-language models to integrate visual texts with multimodal contextual cues, fostering more explainable, interpretable, and grounded video understanding."
        },
        {
            "title": "D Collecting Details of VIDTEXT",
            "content": "This section outlines the procedures for sourcing, filtering, and analyzing the video content in VIDTEXT. Sources. To ensure broad coverage of video scenarios and textual styles, VIDTEXT integrates data from six public datasets: BOVText [22] Multi-scene videos suitable for holistic OCR tasks. RoadText-1K [23] Dense road-text detection in driving scenarios. DSText [40] Subtitles from indoor instructional videos. M4-ViteVQA [17] Clipand instance-level multimodal QA videos. Video-MME/MLVU [11, 12] Long-form videos with strong temporal reasoning demands. YouTube Supplementation. To supplement long-form data, we collect additional videos from YouTube, focusing on the following categories: Sports highlights: NBA, FIFA World Cup, and related competitions. Gaming commentary: live streams and post-game analysis. TV shows and variety entertainment. Retrieval and Filtering Criteria. Candidate videos were retrieved using targeted keyword queries such as \"match subtitles\", \"game commentary\", and \"captioned recap\". We applied the following filtering rules: Minimum duration: 3 minutes for YouTube, >30 minutes for Video-MME. Scene-text richness: We use the latest detector Gomatching [41] to calculate the proportion of frames containing text. Density thresholds: Videos must meet minimum ratio of text-bearing frames: 20% for YouTube videos and 10% for Video-MME. Metadata Statistics. We also collect metadata such as video length, resolution, and frame rate to ensure coverage diversity across temporal and visual characteristics. Scene and Language Distributions. Fig. 5 illustrates the distribution of visual text quantity across six video scene categories. The largest number of text instances appears in Entertainment and Sports-related content, while knowledge and Media are less dense in text content. Video Duration Distribution. VIDTEXT exhibits wide range of video durations, with an average length of 108.2 seconds. As shown in Fig. 6, this highlights the multi-duration characteristic of VIDTEXT, ensuring the temporal diversity needed to support both short-form and long-form video understanding tasks. Semantic Content Word Cloud. To visualize the semantic richness and diversity of video-text interactions, we construct word cloud using all questions and answers in VIDTEXT. As shown in Fig. 7, high-frequency words such as text, video, content, and EXIT reflect strong alignment between text and semantic reasoning. The co-existence of spatial keywords (e.g., LEFT, RIGHT), functional terms (e.g., score, speed), and contextual references (e.g., player, talent) highlights the multi-granular reasoning needs of the dataset. 16 Figure 5: Text quantity distribution across six scene categories. Figure 6: Video duration distribution in VIDTEXT."
        },
        {
            "title": "E Details of Annotation",
            "content": "D.1 Instance Annotation Each video underwent two-stage text annotation process. In the first stage, annotators drew tight bounding boxes around visible text lines and assigned each to category: ClearText or Illegible. created. tracking tool automatically propagated bounding boxes across frames using consistent Track IDs.More Details are shown in Fig. 8. D.2 Clip-Level Annotation Videos shorter than 1 minute were split into 5-second clips; longer ones into 20-second clips. For each clip, annotators recorded all visible, legible text and its temporal span. Repeated instances within clip were marked only once. Illegible or heavily blurred texts were ignored. More Details shown in Fig. 9. D.3 Video-Level Text Collection separate annotation team reviewed the OCR predictions from our model. Annotators removed hallucinated content and added missing instances. Chinese was annotated by full lines; other lan17 Figure 7: Word cloud of all questions and answers in VIDTEXT. Figure 8: Instance-Level Annotation Guidelines. guages (e.g., English, German) were annotated by words. Each unique string was listed once in the final inventory. More Details shown in Fig. 9. D.4 Holistic Reasoning Annotators watched the full video and consulted the video-level text inventory to write one multilabel question per video (see Fig. 10). Each question included seven options describing high-level semantics such as scene, role, topic, or sponsor. D.5 Local Reasoning For every clip (as defined in D.2), annotators created one four-option multiple-choice question requiring reasoning between localized text and visual context (e.g., subtitle character behavior). The 18 Figure 9: CLip&Video-Level Annotation Guidelines. Figure 10: HolisticReasoning Annotation Guidelines. question must require multimodal reasoning and not be solvable using text or image alone. More Details shown in Fig. 11. D.6 Temporal Causal Reasoning (TCR) Given reference text (e.g., scoreboard or subtitle), annotators identified the timestamp of its appearance, observed the following 330 seconds, and formulated causal reasoning question. The answer was single factual sentence describing the resulting action. Each QA pair was anchored to the cues timestamp. More Details shown in Fig. 12. 19 Figure 11: LocalReasoning Annotation Guidelines. Figure 12: TemporalCausalReasoning Annotation Guidelines. D.7 Spatial Reasoning (SR) As shown in Fig. 13, at given timestamp, annotators located reference text or entity and constructed question requiring reasoning over its spatial relation to nearby visual elements (e.g., direction, proximity, interaction). Quality Control All annotations underwent double review. Each item was cross-validated by second annotator, and disagreements were resolved by expert adjudication. On random sample of 200 items, we achieved an average inter-annotator agreement of 0.81 (Cohens ), indicating high reliability. 20 Figure 13: SpatialReasoning Annotation Guidelines."
        },
        {
            "title": "F Details of Experimental Settings",
            "content": "E.1 Model Configuration In this section, we outline the primary baselines evaluated on our VidText.To ensure fair comparison across both openand closed-source models, we explicitly standardize frame sampling and spatial resolution for each baseline as summarized in Tab. 5. For proprietary models such as GPT-4o, Gemini 1.5 (Pro and Flash), and GPT-4-Turbo, we follow their official or API-supported settings. GPT-4o models support up to approximately 500 images inputs, for which we adopt uniform sampling rate of 0.5 fps with an input resolution of 512 512 to accommodate most of our videos. GPT-4-Turbo is restricted to 16 frames, uniformly sampled across the video, and resized to the same resolution. For open-source models, we align each configuration with their original public implementations. VideoChat-Flash, Qwen2-VL (7B), and All Qwen2.5-VL variants (3B/7B/72B) operate under 1 fps sampling strategy, with maximum of 768 frames extracted per video. Models that support extended temporal contextssuch as VideoLLaMA 3, InternVL 2.5, and LLaVA-OVare provided with 64 uniformly sampled frames, resized to 336 336. ShareGPT4Video also uses 64 frames, but with reduced spatial resolution of 224 224. LongVU and LongVA, are evaluated with sparse and extended frame settings. LongVU uses 1 fps sampling, while LongVA accepts up to 128 uniformly distributed frames. MiniCPM-V2.6 applies fixed 64-frame sliding window, following its official implementation. E.2 Human Performance Study To assess the upper-bound of performance on VIDTEXT, we conducted controlled human evaluation across all tasks in our benchmark. Three annotators with experience in video analysis and text recognition were recruited to answer representative subset of questions spanning all eight task types. Each participant was given access to the full video content and instructed to answer using their best judgment, without time constraints. The average human accuracy across all tasks reaches 89.5%, substantially outperforming all evaluated models. In particular, humans demonstrated nearperfect scores in holistic and local OCR, reasoning, and spatial understanding tasks, highlighting the gap between human-level comprehension and the capabilities of current multimodal large models. These results serve as reference ceiling for future model development and underline the complexity 21 Table 5: Framesampling and input-resolution settings for baselines. Model Size Sampling Resolution Proprietary MLLMs GPT-4-Turbo Gemini 1.5 Flash GPT-4o Gemini 1.5 Pro Open-source MLLMs LongVU Qwen2.5-VL Video-XL-Pro LongVA MiniCPM-V2.6 VideoChat-Flash Qwen2-VL Qwen2.5-VL VideoLLaMA 3 ShareGPT4Video Oryx-1.5 LLaVA-OV Qwen2.5-VL InternVL 2.5 3 3 7 7 7 7 7 7 7 8 32 72 72 78 16 frames 1 fps 0.5 fps 1 fps 1 fps 1 fps 1 fps 128 frames 64 frames 1 fps 1 fps 1 fps 64 frames 64 frames 64 frames 64 frames 1 fps 64 frames 5122 5122 5122 5122 4482 4482 4482 4482 4482 4482 4482 3362 2242 3362 3362 4482 3362 and nuance of the video-text understanding challenges posed by VIDTEXT.More details are shown in Tab. 2. E.3 Experiment Environment. All experiments are conducted on server equipped with 4NVIDIA A100 GPUs (80GB each). Model inference and evaluation are implemented in PyTorch with mixed-precision support."
        },
        {
            "title": "G Model Prompts",
            "content": "Fig. 14 shows the prompt template used to obtain detailed frame-level captions from the Aria model. The prompt includes instructions to describe the scene, detect visible text, summarize actions, and relate them spatially and semantically. Tab. 6 lists the standardized prompt templates used for each task in VidText. 22 Figure 14: Prompt template used for Aria to generate frame-level captions. 23 Table 6: Prompt templates used for VidText tasks. Task Prompt Template Holistic OCR Holistic Reasoning Local OCR Local Reasoning Text Localization \"Recognize all visual texts in the video. If the text is not in English, do not provide an English translation. Do not include any descriptions, narrative, or context. Output only the extracted text lines, each on new line.\" \"Watch the video carefully and select the correct three answers. Question: {question} Options: {options} Please output your answer in the format: Correct Answers: A, B, C\" \"Watch the video and answer the following question based on its content. Question: {question} Please output only the texts that appear in the specified time interval as JSON array of strings, with each element representing one piece of text. Do not include any additional description or translation.\" \"Watch the video and answer the following multiple-choice question based on its content. Question: {question} Options: Option A: {text} Option B: {text} ... Please select the correct option.\" \"Watch the video and answer the following question based on its content. Please provide the time interval (in seconds, precise to 0.1s) during which the text appears in the video. Output your answer in JSON format with keys start and end. For example: {\"start\": Do not include any extra commentary.\" 0.0, \"end\": 30.0}. Temporal Causal Reasoning \"Watch the video and answer the following multiple-choice question based on its content. Question: {question} Options: Option A: {text} Option B: {text} ... Please select the correct option.\" Text Tracking (Same prompt as Spatial Reasoning) Spatial Reasoning \"Watch the video and answer the following multiple-choice question based on its content. Question: {question} Options: Option A: {text} Option B: {text} ... Please select the correct option.\" 24 Figure 15: (Top) More examples of HolisticOCR. (Middle) More examples of HolisticReasoning. (Bottom) More examples of LocalOCR. 25 Figure 16: (Top) More examples of LocalReasoning. (Middle) More examples of TextLocalization. (Bottom) More examples of TemporalCausalReasoning. Figure 17: (Top) More examples of TextTracking. (Bottom) More examples of SpatialReasoning."
        },
        {
            "title": "H More Visualization Results",
            "content": "We present additional visualizations of our VidText annotation examples in Fig 15, 16, and 17."
        }
    ],
    "affiliations": [
        "BUAA",
        "HIT",
        "IIE, CAS",
        "NJUST",
        "NKU",
        "PKU",
        "UCAS",
        "UNITN"
    ]
}