{
    "paper_title": "Vivid4D: Improving 4D Reconstruction from Monocular Video by Video Inpainting",
    "authors": [
        "Jiaxin Huang",
        "Sheng Miao",
        "BangBnag Yang",
        "Yuewen Ma",
        "Yiyi Liao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reconstructing 4D dynamic scenes from casually captured monocular videos is valuable but highly challenging, as each timestamp is observed from a single viewpoint. We introduce Vivid4D, a novel approach that enhances 4D monocular video synthesis by augmenting observation views - synthesizing multi-view videos from a monocular input. Unlike existing methods that either solely leverage geometric priors for supervision or use generative priors while overlooking geometry, we integrate both. This reformulates view augmentation as a video inpainting task, where observed views are warped into new viewpoints based on monocular depth priors. To achieve this, we train a video inpainting model on unposed web videos with synthetically generated masks that mimic warping occlusions, ensuring spatially and temporally consistent completion of missing regions. To further mitigate inaccuracies in monocular depth priors, we introduce an iterative view augmentation strategy and a robust reconstruction loss. Experiments demonstrate that our method effectively improves monocular 4D scene reconstruction and completion."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 2 9 0 1 1 . 4 0 5 2 : r Vivid4D: Improving 4D Reconstruction from Monocular Video by Video Inpainting Jiaxin Huang1, Sheng Miao1, BangBnag Yang2, Yuewen Ma2, Yiyi Liao1(cid:66) 1 Zhejiang University 2 ByteDance PICO Project Page: https://xdimlab.github.io/Vivid4D/ Figure 1. Vivid4D. We improve dynamic scene reconstruction from casually captured monocular videos by synthesizing augmented views. Our approach integrates both geometric and generative priors to reformulate the video augmentation as video inpainting task. This enables our method to effectively complete invisible regions in the scene and enhance reconstruction quality."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Reconstructing 4D dynamic scenes from casually captured monocular videos is valuable but highly challenging, as each timestamp is observed from single viewpoint. We introduce Vivid4D, novel approach that enhances 4D monocular video synthesis by augmenting observation views synthesizing multi-view videos from monocular input. Unlike existing methods that either solely leverage geometric priors for supervision or use generative priors while overlooking geometry, we integrate both. This reformulates view augmentation as video inpainting task, where observed views are warped into new viewpoints based on monocular depth priors. To achieve this, we train video inpainting model on unposed web videos with synthetically generated masks that mimic warping occlusions, ensuring spatially and temporally consistent completion of missing regions. To further mitigate inaccuracies in monocular depth priors, we introduce an iterative view augmentation strategy and robust reconstruction loss. Experiments demonstrate that our method effectively improves monocular 4D scene reconstruction and completion. Reconstructing dynamic 3D scenes from convenient monocular video captures is highly valuable task in computer vision and graphics, yet remains profound challenge. Recent advances in 4D reconstruction [9, 17, 21, 23, 44, 50, 61, 83, 93] have made significant progress in modeling dynamic scenes, but they typically rely on synchronized multiview cameras. Given the sparse observations of monocular videos where each timestamp is observed from single viewpoint, existing methods attempt to address this challenging task by incorporating additional priors. One line of studies has explored various geometric priors for monocular 4D reconstruction, including geometric constraints [57, 58], optical flow [41, 42, 81], depth estimation [24, 47], and tracking [38, 69, 78]. However, existing methods yield suboptimal results for two main reasons. First, these auxiliary supervisions are not consistently reliable and may not correlate linearly with rendering performance; for example, small depth error may lead to significant color shifts in regions with texture changes. Second, these priors, which are derived solely from input views, provide limited guidVivid4D to improve 4D reconstruction while maintaining geometric consistency and temporal coherence. The main contributions of our work can be summarized as follows: 1) We propose Vivid4D, novel method for monocular 4D scene reconstruction that leverages multiview supervision from both video diffusion models and depth-guided warping. 2) We reformulate view augmentation as video inpainting task, enabling efficient training on unposed web videos through synthesized occlusion masks based on 2D tracking. 3) We design an iterative view augmentation strategy to progressively expand the observation viewpoint of the scene, enabling robust reconstruction and novel view synthesis. 4) We conduct extensive experiments across various dynamic scenes and show that our method improves the quality of novel view synthesis compared to existing approaches on different types of motion and scene complexity. 2. Related Work 3D Novel View Synthesis. Recent advances in neural rendering, from NeRF [35, 53, 54, 77] to 3D-GS [10, 28, 32, 34, 49, 88, 98, 106], have revolutionized static scene reconstruction and novel view synthesis. To improve reconstruction quality, especially in challenging scenarios such as sparse inputs, researchers have explored two main diintegrating traditional geometric reconstruction rections: techniques [14, 20, 101] and applying geometric regularizations [35, 39, 56, 61, 75, 90]. Recently, generative models have been leveraged for additional visual supervision. For instance, several methods [12, 87, 107] employ score distillation sampling (SDS) [60] to distill knowledge from pre-trained diffusion models, optimizing the 3D representation to align with learned priors. Another line of work [46, 52, 59, 67, 85, 95, 97, 99] projects dense scene representations [34, 54, 67, 80] and projects them into novel viewpoints, guiding reconstruction with geometrically consistent views generated by diffusion models. Meanwhile, [45] incorporates 3D structure conditioning into video diffusion for 3D-consistent video generation, and [26] generates multi-view images conditioned on input views and target camera poses to provide additional supervision. While these methods achieve impressive results, they are typically limited to static scenes and struggle with dynamic objects and temporal consistency. 4D Monocular Novel View Synthesis. Many methods have extended neural rendering to dynamic scenes, typically based on canonical space modeling with time-dependent deformation fields [21, 44, 47, 50, 61, 83] or 4D timevarying representations [9, 17, 23, 62, 86, 93]. However, these methods typically require multi-view synchronized and calibrated videos, making them ill-posed when multiview data is limited. Figure 2. Video Inpainting for 4D Reconstruction. To train the video inpainting model, we use 2D tracking to generate masked training pairs from unposed web videos. During 4D reconstruction, we warp the monocular video to novel viewpoints, creating masked videos that our inpainting diffusion model then completes. ance for regions occluded or unobserved in the input views. More recently, video diffusion models have emerged as another type of prior for such tasks, owing to their impressive ability to generate realistic videos [7, 36, 48, 94]. These models can generate plausible RGB images for unknown viewpoints, overcoming the disadvantages of using geometry priors alone. However, existing methods are either limited to static scenes [46, 52, 85, 97, 99], or require large amount of posed images for training [84, 105], while collecting such training data is highly challenging as dynamic scenes camera poses are difficult to obtain. In this work, we propose Vivid4D, novel approach for 4D monocular reconstruction that integrates geometric priors from input videos with the generative capabilities of video diffusion models. Instead of directly generating novel views, we reformulate view augmentation as video inpainting task: first, observed views are warped to unseen viewpoints based on geometric priors; then, video diffusion model inpaints missing regions to ensure spatial-temporal consistency. This allows us to train the video inpainting model using unposed web videos by mimicking warping occlusions based on 2D trackings. Specifically, our method includes two key steps as shown in Fig. 2. First, we train video inpainting model tailored for our view augmentation task that learns to in-paint occluded regions in spatial-temporally coherent manner (Fig. 2 top). This model is trained on unposed web videos, using 2D trackings to synthesize occlusion masks. Next, we use the trained video inpainting model for view augmentation in monocular-video-based 4D reconstruction, leveraging monocular depth priors to warp the video to unseen viewpoints, which our inpainting model then completes (Fig. 2 bottom). To maximize scene coverage while compensating for inaccurate monocular depth priors, we propose progressive view expansion strategy that iteratively generates augmented views, effectively accumulating comprehensive multi-view observations. When further combined with robust reconstruction loss, the augmented views enables For monocular video reconstruction, some methods leverage data-driven priors like depth [68, 91], tracks [89], and geometric constraints [38, 42, 47, 58, 69, 78, 81] for initialization or supervision, but these priors may not be fully accurate and do not always correlate linearly with RGB performance. More recently, diffusion models have been introduced for 4D reconstruction. For example, [15] utilizes pre-trained diffusion models [64] for scene completion, though the visual quality still requires improvement. Concurrent work [108] uses LGM [71] to generate pseudo novel views as additional supervision, but it is primarily applied at object level. Other methods [70, 84, 105] generate spatial-temporally consistent frames to optimize 4D scenes, but they require temporally and spatially decoupled data to learn disentangled control over camera motion and time. Additionally, [84, 105] rely on camera motion as input for training, which can be challenging to obtain for dyIn contrast, our approach only renamic video datasets. quires unposed web videos with simulated warping occlusion masks, enabling training on wider range of casual video datasets without camera pose information. Moreover, [70] lacks precise camera control during inference, while our method implicitly incorporates precise pose information through warping. Video Diffusion Models. Video diffusion models have shown remarkable capabilities in video generation tasks. Recent works focued on video in-painting [27, 37, 40, 103, 109] and out-painting [11, 19, 74] have demonstrated the effectiveness of diffusion models in generating temporally consistent content for incomplete videos. However, these methods mainly deal with single video without perspective changes. Other generative methods [1, 73] take camera poses as input to generate synchronized videos from novel viewpoints, but their quality needs improvement, and they are constrained by static input-output camera viewpoints. Some works [6, 104] attempt to overcome viewpoint constraints by inpainting videos after perspective changes. These methods utilize geometric constraints [63, 96, 104] or pseudo 4D Gaussian fields [6] to obtain masked videos after warping, and fine-tune video diffusion models [7, 30, 94] for multi-view synthesis. While these approaches show the potential of diffusion models in generating consistent content for partially observed videos, they rely on expensive training data from video warping or 4D Gaussian field creation. Moreover, they focus primarily on cameracontrolled video generation [2, 29] rather than 4D reconstruction, which does not guarantee consistency or efficiency for novel view playback like Gaussian Splatting. In contrast, our method simply uses pre-trained tracking model [33] to generate masked training data and fine-tunes the model to provide additional supervision for 4D monocular reconstruction through iterative view expansion. Note that [52, 97] also employ diffusion models to inpaint warped videos, but they are limited to static scenes. 3. Method We present novel approach for improving monocular 4D scene reconstruction through view augmentation using monocular depth priors and video diffusion models. The key to enhancing the 4D monocular video synthesis is to warp the monocular video to augmented views based on the depth prior, followed by video inpainting model to fill occluded regions. Our key components include video inpainting model tailored for the task of view augmentation from single monocular video (Sec. 3.1), and an iterative augmented view generation strategy (Sec. 3.2). The original and the augmented videos are then used for 4D reconstruction (Sec. 3.3). Fig. 3 illustrates our 4D reconstruction based on view augmentation. 3.1. Video Inpainting Diffusion Model We aim to learn video inpainting diffusion model to inpaint the occluded or invisible areas in masked video while ensuring spatial-temporal consistency. To align with our core objective of 4D reconstruction, it is essential to provide training data in which the regions requiring inpainting primarily result from camera motion. Unlike previous methods, which require known camera poses [84, 97], or apply random mask to input videos [52], our method leverages pre-trained 2D tracking model [33] for generating the training data for the video inpainting model, allowing us to leverage casual, unposed videos for training. t)T 1}N t=1}N Training Data Preparation: Given an unposed input video sequence = {It}T t=1 with frames, we first employ pre-trained 2D tracking model [33] to establish point correspondences across frames. Let P1 = {pi i=1 denote the set of pixel points sampled from the first frame I1. The tracking model generates trajectories = {(pi i=1 that record the positions of these initial points throughout the video sequence. For each subsequent frame It (t > 1), we identify pixels that are not associated with any tracked point from P1. These untracked pixels form our mask region Mt, as they likely correspond to areas that were previously occluded or newly revealed in the scene. Formally, we define the mask for frame as: i=1} Mt = {q It / {pi t}N where {pi t. The masked frame can be obtained as: i=1 represents the set of tracked points in frame t}N = {I t }T t=1, = It Mt where indicates element-wise multiplication. This approach naturally captures regions that become visible due to object motion or camera movement, providing ideal training data for our video inpainting task. Figure 3. 4D reconstruction based on view augmentation. Given an input monocular video, we first perform sparse reconstruction to obtain camera poses and align monocular depth to metric scale, forming an initial data buffer D0. In each iterative view augmentation step, we select frames at each timestamp from the previous buffer Dj1 and warp them to novel viewpoints using pre-defined camera poses T, creating new perspective videos with continuous invisible region masks. These masked videos, along with binary masks and an anchor video, are fed into our pre-trained anchor-conditioned video inpainting diffusion model to produce completed novel-view videos. We update the buffer Dj with these enhanced videos, their metric depths and poses. Finally, both the original monocular video and all synthesized multi-view videos are used to supervise 4D scene reconstruction. Anchor-Conditioned Video Inpainting Diffusion Model: Given the masked video and corresponding binary mask M, we introduce video diffusion model that inpaints occluded or invisible regions with spatial-temporal consistency while incorporating priors from unwarped videos. To leverage pre-trained video diffusion models effectively, we fine-tune an existing model [51, 76] and adapt its architecture for video inpainting in view augmentation. Our modifications include processing the masked input and conditioning the model on unwarped videos to utilize full observed frames for context. Specifically, we modify the standard video diffusion model in two ways. First, we extend the input processing to handle masked videos by encoding the incomplete video sequence through VAE encoder, producing 4-channel latent representation zm RT HW 4 that is concatenated with the corresponding resized binary mask RT HW 1 [64] and the noisy latent zt RT HW 4. Second, we introduce an anchor video mechanism, where frames sampled from the same video clip but at different timestamps serve as additional spatial-temporal context. Note that in our later 4D reconstruction stage, we use the original, unwarped monocular video as the anchor video. The anchor video is similarly encoded through the VAE into latent space za RT HW 4. During training, our modified U-Net receives latent input = [zt; zm; M; za], where these maps are concatenated along the channel dimension. While za are not spatially aligned with other maps using this simple concatenation, our 3D U-Net can effectively learn spatio-temporal correspondences through its hierarchical operations. The model is trained with the following objective: = Ex,t,ϵN (0,1)ϵ ϵθ(zt, t, zm, M, za)2 2 where zt = αtx + σtϵ is the noised latent of the ground truth video x, and is the diffusion timestep. 3.2. Diffusion-based View Augmentation Given the trained video inpainting model, we augment the monocular input used for 4D reconstruction to generate multi-view supervision, as illustrated in Fig. 3. Depth Estimation and Scale Alignment: Given monocular video = {It}T t=1 with frames, we first utilize COLMAP [65, 66] to obtain its camera intrinsics K, camera t=1, and sparse point cloud {Xi}M poses {Tt}T i=1, where each point Xi R3 denotes the 3D coordinates reconstructed in the world coordinate system. For depth estimation, we adopt two-step approach that combines learning-based prediction with geometric refinement, as presented in Fig. 3 (left). We first employ an offthe-shelf monocular depth estimation model (e.g., [92]) to obtain initial depth maps { ˆDt}T t=1. Next, we align them with the metric scale using the sparse point cloud {Xi}M i=1 from COLMAP via Least Square and RANSAC [22], obtaining the metric depth maps {Dt}T t=1. Please refer to supplementary ( Sec. 6.3) for details of the scale alignment strategy. Iterative View Augmentation: Using the scale-aligned Figure 4. Difference between Direct Warping and Iterative Warping. and denote spatial and temporal dimensions, with being the input monocular video (yellow dots indicating camera poses in 4D space). Left: With = 1, we directly warp the input video to new perspectives (green dots) in single iteration. Warped frames are ranked by warping distance per timestamp and organized into video (purple underline). Right: With = 2, the video is first warped the closest pre-defined poses (green dots, = 1), then additional perspectives (red dots, = 2) are generated by selecting frames with minimal warping angles from existing frames. This iterative approach minimizes distortion and floaters caused by depth inaccuracies in large-angle warping, enhancing reconstruction quality. metric depth, we can warp the observed frames = {It}T t=1 to unseen viewpoints for synthesizing multi-view RHW 3 is observations, where each warped frame {0, 1}HW . Howaccompanied by binary mask ever, due to inaccuracies in the depth priors (e.g., bleeding edges), directly warping the videos by large angle introduces noticeable artifacts, which degrade the performance of 4D reconstruction. To address this, we propose an iterative view augmentation strategy, as illustrated in Fig. 3 (middle), leveraging the fact that artifacts like bleeding edges are less severe when the warping angle is small. Fig. 4 shows the difference between direct warping and iterative warping. Specifically, we first pre-compute set of target unseen camera viewpoints and perform iterative view augmentation over iterations. As presented in Fig. 4, when = 1 (left), we directly warp the original video to all target frames in single step. In contrast, when > 1 (right), we progressively expand the warped angles over multiple iterations. We start with data buffer D0 containing the input monocular video 0, its aligned depths D0 and camera poses T0, formally expressed as D0 = (V 0, D0, T0). At each iteration (1 ), we select frames with the smallest warping angles from the data buffer Dj1 at each timestamp and warp them to target poses Tj, generating multiple new videos of frames. The warped videos and corresponding masks, together with the anchor video, are then fed into our diffusion model to synthesize completed videos ˆV j. Next, we estimate the metric depth Dj using our scale alignment strategy and update the data buffer as follows: Dj = Dj1 ( ˆV j, Dj, Tj). This process is repeated until all pre-defined target poses have been covered by each frame. Furthermore, to avoid the inconsistency in 4D reconstruction supervisions caused by repeated inpainting across different iterations, we introduce supervision mask Sj {0, 1}HW for each augmented frame, in which pixel value 1 indicates valid for supervision. Please refer to supplementary material ( Sec. 6.5) for more details. 3.3. 4D Reconstruction 4D Scene Representation: Our method can effectively produce accurate, consistent, and high-fidelity novel view videos from monocular input, offering rich multi-view supervision for 4D scene reconstruction. is compatible with various 4D reconstruction approaches, including those extending static representations with temporal dimensions [93] and those modeling dynamic scenes via deformation or motion fields [78, 83]. In our experiments, we adopt motion field-based 3DGS representation [78] to capture both geometric structure and temporal dynamics effectively. It Loss Function: We rely solely on the multi-view constraints from our synthesized novel views to guide reconstruction. To mitigate distortions from depth estimation errors and video diffusion artifacts, we propose to use IV RGB loss, robust pixel-wise loss inspired by [18], as the supervisory signal for our augmented views. Specifically, we compute the L1 loss between each pixel in the rendered image and the closest 3 3 pixels in the corresponding aug,mented supervision image, backpropagating only through the pixel with the minimum error. This loss, denoted as LIV, improves reconstruction robustness by reducing sensitivity to slight misalignments. For the original frames, we use standard L1, SSIM and LPIPS loss to supervise the entire image. For our augmented frames, we use the loss function as follows: = (cid:88) (cid:88) j=1 t=1 (cid:16) λrLt,j IV + λsLt,j ssim + λlLt,j lpips (cid:17) where each loss term is computed only in regions where the supervision mask equals 1. Specifically, for time step in iteration j: Lt,j IV = (cid:88) j (p) IV(I (p), ˆI (p)) j Lt,j Lt,j ) ) ssim = SSIM(I lpips = LPIPS(I j Here, denotes pixel locations, are the rendered and our augmented supervisions respectively, and indicates element-wise multiplication. The hyperparameters λr, λs, and λl balance the contribution of each loss term. , ˆI , ˆI and ˆI 4. Experiments In this section, we begin by comparing our method with state-of-the-art 4D scene reconstruction methods in Sec. 4.1. After that, we conduct ablation studies to evaluate our design choices in Sec. 4.2. We highlight the best , 4D GS CoCoCo Figure 5. Qualitative comparison of dynamic scene reconstruction on iPhone dataset and HyperNeRF dataset. The black holes in 4D GS and the white areas in Shape of Motion indicate regions where the input video lacks visibility. In contrast, our method effectively fills these invisible areas within the scenes, leveraging multi-view constraints and spatiotemporal priors to enhance reconstruction quality. Shape of Motion Ground Truth Ours second-best , and third-best scores achieved on any metrics. Please refer to supplementary ( Sec. 6.2) for more training details. 4.1. Monocular 4D Reconstruction Dataset and Metrics: We evaluate dynamic scene reconstruction from monocular video using the iPhone [25] and HyperNeRF [58] datasets. Our test sets comprise 5 iPhone and 3 HyperNeRF scenes, each with synchronized static cameras for novel-view synthesis evaluation. From the original video sequences, we select continuous clips with few dozen frames, chosen for their large perspective differences from the test view to emphasize our methods efficacy. We assess reconstruction quality using mPSNR, mSSIM, and mLPIPS [25] for dynamic foreground, static background, and the overall scene, employing co-visible mask to specify visible areas for reconstruction. Comparison Baselines: We compare our method against 4D GS [93], Shape of Motion [78], CoCoCo [109], StereoCrafter [104], and ViewCrafter [97]. CoCoCo and StereoCrafter are designed for video inpainting, while ViewCrafter focuses on 3D-aware inpainting. We integrate their diffusion models into our pipeline for warped video inpainting to validate the necessity of our anchor-conditioned model. For fairness, we provide CoCoCo with the same text prompt for each scene and use the first frame of our anchor video as the image conditioning input for ViewCrafter. Additionally, we compare against the state-of-the-art monocCondition PSNR SSIM LPIPS FVD w/o anchor anchor (ours) 25.34 27.22 0.8053 0. 0.1056 0.0801 18.99 14.30 Table 1. Quantitative ablation study on anchor video condition on our processed 5K videos from OpenVid-1M. ular reconstruction method Shape of Motion [78] and the general 4D reconstruction approach 4D GS [93]. Results: The results are shown in Fig. 5 and Tab. 2. It can be observed that our pipeline significantly improves reconstruction quality over 4D GS and Shape of Motion, which exhibit outliers or over-sharpening artifacts. In contrast, our method not only produces smoother results but also better preserves dynamic foreground details. Additionally, it fills missing regions invisible in the input video, addressing the inner holes seen in 4D GS and Shape of Motion. Compared to CoCoCo, which also achieves high-quality reconstruction, our method offers greater overall clarity, finer detailsespecially in dynamic areasand more contextually consistent inpainting, thanks to our anchor-based video inpainting model. Quantitatively, our approach outperforms all baselines across all metrics, further demonstrating its effectiveness in 4D reconstruction. 4.2. Ablation Study We ablate various components of our method on the iPhone dataset and HyperNeRF dataset for 4D reconstruction to Dataset Method iPhone Dataset 4D GS Shape of Motion CoCoCo StereoCrafter ViewCrafter Ours HyperNeRF Dataset 4D GS Shape of Motion CoCoCo StereoCrafter ViewCrafter Ours Dynamic Part Static Part All mPSNR mSSIM mLPIPS mPSNR mSSIM mLPIPS mPSNR mSSIM mLPIPS 12.28 11.87 12.20 12.09 12.13 12. 17.93 17.70 18.47 18.51 18.53 18.85 0.8757 0.8894 0.8964 0.8989 0.8985 0.8990 0.9556 0.9609 0.9648 0.9660 0.9658 0.9666 0.5249 0.4778 0.4650 0.4811 0.5053 0.4516 0.3819 0.3027 0.3048 0.3343 0.3228 0.3020 14.54 15.42 15.85 15.60 15.82 16. 18.32 19.12 19.11 18.98 18.98 19.57 0.4870 0.5744 0.5809 0.5989 0.5946 0.6088 0.4737 0.5659 0.5599 0.5677 0.5691 0.5910 0.5594 0.5082 0.5130 0.5593 0.5315 0.4726 0.4409 0.4520 0.4627 0.5171 0.4860 0.4407 14.01 14.56 14.99 14.85 14.94 15. 18.24 18.82 19.00 18.86 18.91 19.45 0.3877 0.4570 0.4701 0.4945 0.4888 0.5004 0.4200 0.5176 0.5147 0.5231 0.5259 0.5446 0.5939 0.5292 0.5280 0.5676 0.5772 0.4930 0.4450 0.4589 0.4692 0.5181 0.4888 0.4449 Table 2. Quantitative comparison of dynamic scene reconstruction on iPhone dataset and HyperNeRF dataset. Masked Sequence w/o anchor anchor (ours) (a) (b) (c) (d) Figure 6. Quantitative ablation study on anchor video condition on our processed 5K videos from OpenVid-1M. verify design choices of our approach. Effect of Anchor condition: We explored the impact of anchor video condition on the video inpaint results of our diffusion model, as presented in Tab. 1 and Fig. 6. Without the anchor condition, the models filling capability weakens, resulting in noticeable artifacts. In contrast, after incorporating the anchor video, the filled areas become more consistent with the surrounding content, significantly reducing artifacts. This demonstrates that our model effectively leverages spatiotemporal priors from the anchor video. Effect of View Warping and Video inpainting: To assess the effectiveness of our view augmentation, which includes view warping and video inpainting, and to compare it with direct depth supervision loss LD, we compare our full method against: (a) no view augmentation or depth supervision, (b) direct depth supervision without view augmentation, and (c) iterative view warping without video inpaintFigure 7. Qualitative ablation study on view warping and video inpainting on HyperNeRF dataset. (a) w/o warping, w/o inpainting, w/o depth; (b) w/o warping, w/o inpainting, w/ depth; (c) w/ warping, w/o inpainting; (d) w/ warping, w/ inpainting (ours). ing. For fairness, all experiments use the same Gaussian initialization, and our view augmentation is set to 6 iterations. The results are presented in Tab. 3 and Fig. 7. It can be observed that directly leveraging the depth for supervision (b) only leads to minor improvement compared with (a). In contrast, using the same depth to synthesize augmented training views leads to larger improvement even without inpainting (c). This verifies that directly adopting geometric priors for supervision may not linearly correlate with the reconstruction performance in the RGB space. Moreover, with our inpainting model to complete the warped views (d), reconstruction further improves, correctly recovering previously invisible areas and reducing flying artifacts. This demonstrates that leveraging depth priors via warping benefits reconstruction, while integrating diffusion priors further enhances completion and quality in Method mPSNR mSSIM mLPIPS Model mPSNR mSSIM mLPIPS (a) w/o warp, w/o inpaint, w/o LD (b) w/o warp, w/o inpaint, w/ LD (c) w/ warp, w/o inpaint, w/o LD (d) w/ warp, w/ inpaint, w/o LD (ours) 16.04 16.12 16.44 16.80 0.4617 0.4699 0.4913 0.5170 0.5249 0.5084 0.4876 0.4750 DepthCrafter Depth Pro Video Depth Anything Depth Anything V2 (ours) 16.76 16.79 16.79 16. 0.5135 0.5121 0.5159 0.5170 0.4835 0.4807 0.4875 0.4750 Table 3. Quantitative ablation study on view warping and video inpainting on iPhone dataset and HyperNeRF dataset. Table 5. Quantitative ablation study on various depth estimation models on iPhone dataset. Iterations mPSNR mSSIM mLPIPS L1 IV (ours) w/o view aug., w/o depth = 1, = 1 = 6, = 2 = 6, = 4 = 6, = 6 (ours) 16.04 15.80 16.41 16.56 16.80 0.4617 0.4711 0.5054 0.5104 0.5170 0.5249 0.4959 0.4964 0.4868 0. Table 4. Quantitative ablation study on iterations on iPhone dataset and HyperNeRF dataset. w/o view aug. = 1, = 1 = 6, = 2 Figure 8. Qualitative ablation study on iterations on HyperNeRF dataset. = 6, = 6 = 6, = 4 4D reconstruction. Effect of Iterations: We investigated the impact of iteration count on reconstruction results, as shown in Tab. 4 and Fig. 8. When no iterations are performed (w/o view aug), meaning only single monocular video is used for supervision, the rendered image exhibits holes and noticeable artifacts at object edges. After applying view augmentation, we obtain more comprehensive observations of the scene. However, directly warping the initial video into multiple views at once (N = 1, = 1) results in poor reconstruction due to large-angle warping, which introduces severe artifacts when transforming to distant viewpoints (e.g., bleeding artifacts are more noticeable at larger viewing angles). In contrast, our iterative strategy gradually enhances viewpoints, reducing warping angles and minimizing the impact of depth inaccuracies. Even with small number of iterations (e.g., 2 or 4), the scene is better filled, and multiview supervision helps mitigate floaters. As iterations increase, more comprehensive views of the scene are obtained for each timestamp, leading to more robust reconstruction. Although the overall scene coverage remains the same for = 1, = 1 and = 6, = 6, additional iterations reduce warping distortion and improve reconstruction fidelity. Quantitative metrics further confirm that increasing the number of iterations leads to progressively better reconstruction results. Effect of Depth Estimation Methods: Next, we evaluate Loss mPSNR mSSIM mLPIPS L1 IV (ours) 16.50 16. 0.5118 0.5170 0.5097 0.4750 Figure 9. Qualitative ablation on loss function on HyperNeRF dataset. Table 6. Quantitative ablation study on loss function on iPhone dataset and HyperNeRF dataset. various depth estimation methods [8, 13, 31, 92] to assess their impact, as shown in Tab. 5. We observe that different methods yield similar results, indicating our methods robustness against monocular depth estimation variations. This may be attributed to aligning monocular depth predictions with COLMAPs scale, along with our interactive view warping and robust RGB loss. By default, we use Depth Anything V2 [92], as it achieves slightly better performance than the others. Effect of Loss Function: Lastly, we also evaluated the effectiveness of IV RGB loss on the robustness of the results. Instead of using LIV for RGB supervision, we directly applied the L1 loss. The results, shown in Tab. 6 and Fig. 9 (best viewed zoomed in), reveal that L1 loss leads to blurrier renderings due to misaligned supervision. In contrast, our method backpropagates errors only to the pixel with the minimum difference within adjacent patches, effectively reducing blurring caused by distortion. 5. Conclusion We introduced Vivid4D, novel approach for 4D monocular reconstruction that combines geometric priors with video diffusion models. By reformulating view augmentation as video inpainting task, our method leverages unposed web videos for training and employs an iterative view expansion strategy with robust loss to minimize artifacts from depth errors. Experiments demonstrate that Vivid4D consistently outperforms existing methods across dynamic scenes, producing more coherent and detailed novel view synthesis results while effectively filling unobserved regions. Our approach bridges geometric-based methods and generative models, enabling high-quality 4D reconstruction from casual monocular videos, with applications in virtual reality and content creation. Future work will focus on enabling plausible scene extrapolation beyond observed regions, and improving robustness to complex camera motions and scene dynamics."
        },
        {
            "title": "References",
            "content": "[1] Jianhong Bai, Menghan Xia, Xintao Wang, Ziyang Yuan, Xiao Fu, Zuozhu Liu, Haoji Hu, Pengfei Wan, and Di Zhang. Syncammaster: Synchronizing multi-camera video generation from diverse viewpoints. arXiv preprint arXiv:2412.07760, 2024. 3 [2] Jianhong Bai, Menghan Xia, Xiao Fu, Xintao Wang, Lianrui Mu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Pengfei Wan, et al. Recammaster: Camera-controlled generative rendering from single video. arXiv preprint arXiv:2503.11647, 2025. 3 [3] Jonathan Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul Srinivasan. Mip-nerf: multiscale representation for anti-aliasing neural radiance fields. In Proceedings of the IEEE/CVF international conference on computer vision, pages 58555864, 2021. 2 [4] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded In Proceedings of anti-aliased neural radiance fields. the IEEE/CVF conference on computer vision and pattern recognition, pages 54705479, 2022. [5] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Zip-nerf: Anti-aliased In Proceedings of the grid-based neural radiance fields. IEEE/CVF International Conference on Computer Vision, pages 1969719705, 2023. 2 [6] Weikang Bian, Zhaoyang Huang, Xiaoyu Shi, Yijin Li, FuYun Wang, and Hongsheng Li. Gs-dit: Advancing video generation with pseudo 4d gaussian fields through efficient dense 3d point tracking. arXiv preprint arXiv:2501.02690, 2025. [7] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2, 3, 1 [8] Aleksei Bochkovskii, Amael Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan Richter, and Vladlen Koltun. Depth pro: Sharp monocular metric depth in less than second. arXiv preprint arXiv:2410.02073, 2024. 8 [9] Ang Cao and Justin Johnson. Hexplane: fast representation for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 130141, 2023. 1, 2 [10] David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1945719467, 2024. 2 [11] Qihua Chen, Yue Ma, Hongfa Wang, Junkun Yuan, Wenzhe Zhao, Qi Tian, Hongmei Wang, Shaobo Min, Qifeng Chen, and Wei Liu. Follow-your-canvas: Higher-resolution video outpainting with extensive content generation. arXiv preprint arXiv:2409.01055, 2024. [12] Shen Chen, Jiale Zhou, and Lei Li. Optimizing 3d gaussian splatting for sparse viewpoint scene reconstruction. arXiv preprint arXiv:2409.03213, 2024. 2 [13] Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, and Bingyi Kang. Video depth anything: Consistent depth estimation for super-long videos. arXiv:2501.12375, 2025. 8 [14] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatarXiv preprint ting from sparse multi-view images. arXiv:2403.14627, 2024. 2 [15] Wen-Hsuan Chu, Lei Ke, and Katerina Fragkiadaki. Dreamscene4d: Dynamic multi-object scene generation from monocular videos, 2024. 3 [16] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Luciddreamer: Domain-free generation of 3d gaussian splatting scenes. arXiv preprint arXiv:2311.13384, 2023. 2 [17] Yuanxing Duan, Fangyin Wei, Qiyu Dai, Yuhang He, Wenzheng Chen, and Baoquan Chen. 4d gaussian splatting: Towards efficient novel view synthesis for dynamic scenes. arXiv preprint arXiv:2402.03307, 2024. 1, [18] Adrian Dziembowski, Dawid Mieloch, Jakub Stankowski, and Adam Grzelka. Iv-psnrthe objective quality metric IEEE Transactions on for immersive video applications. Circuits and Systems for Video Technology, 32(11):7575 7591, 2022. 5 [19] Fanda Fan, Chaoxu Guo, Litong Gong, Biao Wang, Tiezheng Ge, Yuning Jiang, Chunjie Luo, and Jianfeng Zhan. Hierarchical masked 3d diffusion model for video outpainting. In Proceedings of the 31st ACM International Conference on Multimedia, pages 78907900, 2023. 3 [20] Zhiwen Fan, Wenyan Cong, Kairun Wen, Kevin Wang, Jian Zhang, Xinghao Ding, Danfei Xu, Boris Ivanovic, Marco Pavone, Georgios Pavlakos, et al. Instantsplat: Unbounded sparse-view pose-free gaussian splatting in 40 seconds. arXiv preprint arXiv:2403.20309, 2024. 2 [21] Jiemin Fang, Taoran Yi, Xinggang Wang, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Matthias Nießner, and Qi Tian. Fast dynamic radiance fields with time-aware neural voxIn SIGGRAPH Asia 2022 Conference Papers, pages els. 19, 2022. 1, 2 [22] Martin Fischler and Robert Bolles. Random sample consensus: paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM, 24(6):381395, 1981. 4, 1 [23] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, and Angjoo Kanazawa. Kplanes: Explicit radiance fields in space, time, and appearance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1247912488, 2023. 1, 2 [24] Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang. Dynamic view synthesis from dynamic monocular video. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 57125721, 2021. 1 [25] Hang Gao, Ruilong Li, Shubham Tulsiani, Bryan Russell, and Angjoo Kanazawa. Monocular dynamic view synthesis: reality check, 2022. 6, 4 [26] Ruiqi Gao, Aleksander Holynski, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul Srinivasan, Jonathan Barron, and Ben Poole. Cat3d: Create anything in 3d with multi-view diffusion models. arXiv preprint arXiv:2405.10314, 2024. 2 [27] Bohai Gu, Hao Luo, Song Guo, and Peiran Dong. Advanced video inpainting using optical flow-guided efficient diffusion. arXiv preprint arXiv:2412.00857, 2024. 3 [28] Antoine Guedon and Vincent Lepetit. Sugar: Surfacealigned gaussian splatting for efficient 3d mesh reconstrucIn Proceedings of tion and high-quality mesh rendering. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 53545363, 2024. 2 [29] Hao He, Ceyuan Yang, Shanchuan Lin, Yinghao Xu, Meng Wei, Liangke Gui, Qi Zhao, Gordon Wetzstein, Lu Jiang, and Hongsheng Li. Cameractrl ii: Dynamic scene exploration via camera-controlled video diffusion models. arXiv preprint arXiv:2503.10592, 2025. 3 [30] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. [31] Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying Shan. Depthcrafter: Generating consistent long depth sequences for open-world videos. arXiv preprint arXiv:2409.02095, 2024. 8 [32] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically acIn ACM SIGGRAPH 2024 confercurate radiance fields. ence papers, pages 111, 2024. 2 [33] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker: It is better to track together. In European Conference on Computer Vision, pages 1835. Springer, 2024. 3 [34] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 2 [35] Hamed Razavi Khosroshahi, Jaime Sancho, Gun Bang, Gauthier Lafruit, Eduardo Juarez, and Mehrdad Teratani. Da4nerf: Depth-aware augmentation technique for neural radiance fields. Journal of Visual Communication and Image Representation, 107:104365, 2025. 2 [36] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [37] Minhyeok Lee, Suhwan Cho, Chajin Shin, Jungho Lee, Video diffusion arXiv preprint Sunghun Yang, and Sangyoun Lee. models are strong video inpainter. arXiv:2408.11402, 2024. 3 [38] Jiahui Lei, Yijia Weng, Adam Harley, Leonidas Guibas, and Kostas Daniilidis. Mosca: Dynamic gaussian fusion from casual videos via 4d motion scaffolds. arXiv preprint arXiv:2405.17421, 2024. 1, 3 [39] Jiahe Li, Jiawei Zhang, Xiao Bai, Jin Zheng, Xin Ning, Jun Zhou, and Lin Gu. Dngaussian: Optimizing sparse-view 3d gaussian radiance fields with global-local depth normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2077520785, 2024. 2 [40] Xiaowen Li, Haolan Xue, Peiran Ren, and Liefeng Bo. Diffueraser: diffusion model for video inpainting. arXiv preprint arXiv:2501.10018, 2025. 3 [41] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-time view synthesis of dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 64986508, 2021. [42] Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker, and Noah Snavely. Dynibar: Neural dynamic image-based rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 42734284, 2023. 1, 3 [43] Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, and Noah Snavely. Megasam: Accurate, fast, and robust structure and motion from casual dynamic videos. arXiv preprint arXiv:2412.04463, 2024. 5 [44] Youtian Lin, Zuozhuo Dai, Siyu Zhu, and Yao Yao. Gaussian-flow: 4d reconstruction with dynamic 3d gaussian particle. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21136 21145, 2024. 1, 2 [45] Fangfu Liu, Wenqiang Sun, Hanyang Wang, Yikai Wang, Haowen Sun, Junliang Ye, Jun Zhang, and Yueqi Duan. Reconx: Reconstruct any scene from sparse views with video diffusion model. arXiv preprint arXiv:2408.16767, 2024. 2 [46] Kunhao Liu, Ling Shao, and Shijian Lu. Novel view extrapolation with video diffusion priors. arXiv preprint arXiv:2411.14208, 2024. 2 [47] Qingming Liu, Yuan Liu, Jiepeng Wang, Xianqiang Lv, Peng Wang, Wenping Wang, and Junhui Hou. Modgs: Dynamic gaussian splatting from causually-captured monocular videos. arXiv preprint arXiv:2406.00434, 2024. 1, 2, 3 [48] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. Sora: review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024. [49] Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, and Bo Dai. Scaffold-gs: Structured 3d gaussians for view-adaptive rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2065420664, 2024. 2 [50] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Tracking Dynamic 3d gaussians: Deva Ramanan. by persistent dynamic view synthesis. arXiv:2308.09713, 2023. 1, 2 arXiv preprint Large 4d gaussian reconstruction model. arXiv preprint arXiv:2406.10324, 2024. 2 [51] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tieniu Tan. Videofusion: Decomposed diffusion modIn Proceedings of els for high-quality video generation. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 4, 1 [52] Baorui Ma, Huachen Gao, Haoge Deng, Zhengxiong Luo, Tiejun Huang, Lulu Tang, and Xinlong Wang. You see it, you got it: Learning 3d creation on pose-free videos at scale. arXiv preprint arXiv:2412.06699, 2024. 2, 3 [53] Sheng Miao, Jiaxin Huang, Dongfeng Bai, Weichao Qiu, Bingbing Liu, Andreas Geiger, and Yiyi Liao. Efficient depth-guided urban view synthesis. arXiv preprint arXiv:2407.12395, 2024. 2 [54] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 2 [55] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. Openvid-1m: large-scale high-quality dataset for text-tovideo generation. arXiv preprint arXiv:2407.02371, 2024. 1, 4 [56] Michael Niemeyer, Jonathan Barron, Ben Mildenhall, Mehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 54805490, 2022. [57] Keunhong Park, Utkarsh Sinha, Jonathan Barron, Sofien Bouaziz, Dan Goldman, Steven Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 58655874, 2021. 1 [58] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan Goldman, Ricardo MartinBrualla, and Steven M. Seitz. Hypernerf: higherdimensional representation for topologically varying neural radiance fields, 2021. 1, 3, 6, 4 [59] Soumava Paul, Christopher Wewer, Bernt Schiele, and Jan Eric Lenssen. Sp2360: Sparse-view 360 scene reconstruction using cascaded 2d diffusion priors. In ECCV 2024 Workshop on Wild 3D: 3D Modeling, Reconstruction, and Generation in the Wild, 2024. 2 [60] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 2 [61] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1031810327, 2021. 1, 2 [62] Jiawei Ren, Kevin Xie, Ashkan Mirzaei, Hanxue Liang, Xiaohui Zeng, Karsten Kreis, Ziwei Liu, Antonio TorL4gm: ralba, Sanja Fidler, Seung Wook Kim, et al. [63] Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin Nimier-David, Thomas Muller, Alexander Keller, Sanja Fidler, and Jun Gao. Gen3c: 3d-informed world-consistent video generation with precise camera control. arXiv preprint arXiv:2503.03751, 2025. 3 [64] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 3, 4 [65] Johannes Lutz Schonberger and Jan-Michael Frahm. In Conference on ComStructure-from-motion revisited. puter Vision and Pattern Recognition (CVPR), 2016. 4, 1 [66] Johannes Lutz Schonberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise view selection for unstructured multi-view stereo. In European Conference on Computer Vision (ECCV), 2016. 4, 1 [67] Junyoung Seo, Kazumi Fukuda, Takashi Shibuya, Takuya Narihira, Naoki Murata, Shoukang Hu, Chieh-Hsin Lai, Seungryong Kim, and Yuki Mitsufuji. Genwarp: Single image to novel views with semantic-preserving generative warping. Advances in Neural Information Processing Systems, 37:8022080243, 2025. [68] Jiahao Shao, Yuanbo Yang, Hongyu Zhou, Youmin Zhang, Yujun Shen, Vitor Guizilini, Yue Wang, Matteo Poggi, and Yiyi Liao. Learning temporally consistent video depth from video diffusion priors, 2024. 3 [69] Colton Stearns, Adam Harley, Mikaela Uy, Florian Dubost, Federico Tombari, Gordon Wetzstein, and Leonidas Dynamic gaussian marbles for novel view Guibas. arXiv preprint synthesis of casual monocular videos. arXiv:2406.18717, 2024. 1, 3 [70] Wenqiang Sun, Shuo Chen, Fangfu Liu, Zilong Chen, Yueqi Duan, Jun Zhang, and Yikai Wang. Dimensionx: Create any 3d and 4d scenes from single image with controllable video diffusion. arXiv preprint arXiv:2411.04928, 2024. 3 [71] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multiview gaussian model for high-resolution 3d content creation. In European Conference on Computer Vision, pages 118. Springer, 2024. 3 [72] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric and challenges, 2019. 4 [73] Basile Van Hoorick, Rundi Wu, Ege Ozguroglu, Kyle Sargent, Ruoshi Liu, Pavel Tokmakov, Achal Dave, Changxi Zheng, and Carl Vondrick. Generative camera dolly: ExIn Eutreme monocular dynamic novel view synthesis. ropean Conference on Computer Vision, pages 313331. Springer, 2024. [74] Fu-Yun Wang, Xiaoshi Wu, Zhaoyang Huang, Xiaoyu Shi, Dazhong Shen, Guanglu Song, Yu Liu, and Hongsheng Li. Be-your-outpainter: Mastering video outpainting through input-specific adaptation. In European Conference on Computer Vision, pages 153168. Springer, 2024. 3 [75] Guangcong Wang, Zhaoxi Chen, Chen Change Loy, and Ziwei Liu. Sparsenerf: Distilling depth ranking for fewshot novel view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9065 9076, 2023. 2 [76] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 4, 1 [77] Peng Wang, Yuan Liu, Zhaoxi Chen, Lingjie Liu, Ziwei Liu, Taku Komura, Christian Theobalt, and Wenping Wang. F2-nerf: Fast neural radiance field training with free camera trajectories. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4150 4159, 2023. 2 [78] Qianqian Wang, Vickie Ye, Hang Gao, Jake Austin, Zhengqi Li, and Angjoo Kanazawa. Shape of motion: arXiv preprint 4d reconstruction from single video. arXiv:2407.13764, 2024. 1, 3, 5, [79] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei A. Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state, 2025. 5 [80] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2069720709, 2024. 2 [81] Shizun Wang, Xingyi Yang, Qiuhong Shen, Zhenxiang Jiang, and Xinchao Wang. Gflow: Recovering 4d world from monocular video. arXiv preprint arXiv:2405.18426, 2024. 1, 3 [82] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4): 600612, 2004. 4 [83] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2031020320, 2024. 1, 2, 5 [84] Rundi Wu, Ruiqi Gao, Ben Poole, Alex Trevithick, Changxi Zheng, Jonathan Barron, and Aleksander Holynski. Cat4d: Create anything in 4d with multi-view video diffusion models. arXiv preprint arXiv:2411.18613, 2024. 2, [85] Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel Watson, Pratul Srinivasan, Dor Verbin, Jonathan Barron, Ben Poole, et al. Reconfusion: 3d reconstruction with diffusion priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2155121561, 2024. 2 [86] Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil Kim. Space-time neural irradiance fields for free-viewpoint In Proceedings of the IEEE/CVF conference on video. computer vision and pattern recognition, pages 94219431, 2021. 2 [87] Haolin Xiong. Sparsegs: Real-time 360 sparse view synthesis using gaussian splatting. Masters thesis, University of California, Los Angeles, 2024. 2 [88] Yunzhi Yan, Haotong Lin, Chenxu Zhou, Weijie Wang, Haiyang Sun, Kun Zhan, Xianpeng Lang, Xiaowei Zhou, and Sida Peng. Street gaussians for modeling dynamic urban scenes. arXiv preprint arXiv:2401.01339, 2024. 2 [89] Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything: Segment anything meets videos. arXiv preprint arXiv:2304.11968, 2023. 3 [90] Jiawei Yang, Marco Pavone, and Yue Wang. Freenerf: Improving few-shot neural rendering with free frequency regularization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 82548263, 2023. [91] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1037110381, 2024. 3 [92] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. Advances in Neural Information Processing Systems, 37:2187521911, 2025. 4, 8 [93] Zeyu Yang, Hongye Yang, Zijie Pan, Xiatian Zhu, and Li Zhang. Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting. arXiv preprint arXiv:2310.10642, 2023. 1, 2, 5, 6 [94] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-tovideo diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 3 [95] Hanyang Yu, Xiaoxiao Long, and Ping Tan. Lm-gaussian: Boost sparse-view 3d gaussian splatting with large model priors. arXiv preprint arXiv:2409.03456, 2024. 2 [96] Mark YU, Wenbo Hu, Jinbo Xing, and Ying Shan. Trajectorycrafter: Redirecting camera trajectory for monocarXiv preprint ular videos via diffusion models. arXiv:2503.05638, 2025. 3 [97] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. 2, 3, 6, [98] Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. Mip-splatting: Alias-free 3d gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19447 19456, 2024. 2 [99] Zhongrui Yu, Haoran Wang, Jinze Yang, Hanzhang Wang, Zeke Xie, Yunfeng Cai, Jiale Cao, Zhong Ji, and Mingming Sun. Sgd: Street view synthesis with gaussian splatting and diffusion prior. arXiv preprint arXiv:2403.20079, 2024. 2 [100] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-Hsuan Yang. Monst3r: simple approach for estimating geometry in the presence of motion. arXiv preprint arXiv:2410.03825, 2024. 5 [101] Jiawei Zhang, Jiahe Li, Xiaohan Yu, Lei Huang, Lin Gu, Jin Zheng, and Xiao Bai. Cor-gs: Sparse-view 3d arXiv preprint gaussian splatting via co-regularization. arXiv:2405.12110, 2024. 2 [102] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric, 2018. 4 [103] Zhixing Zhang, Bichen Wu, Xiaoyan Wang, Yaqiao Luo, Luxin Zhang, Yinan Zhao, Peter Vajda, Dimitris Metaxas, and Licheng Yu. Avid: Any-length video inpainting with diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 71627172, 2024. [104] Sijie Zhao, Wenbo Hu, Xiaodong Cun, Yong Zhang, Xiaoyu Li, Zhe Kong, Xiangjun Gao, Muyao Niu, and Ying Shan. Stereocrafter: Diffusion-based generation of long and high-fidelity stereoscopic 3d from monocular videos. arXiv preprint arXiv:2409.07447, 2024. 3, 6, 4 [105] Yuyang Zhao, Chung-Ching Lin, Kevin Lin, Zhiwen Yan, Linjie Li, Zhengyuan Yang, Jianfeng Wang, Gim Hee Lee, and Lijuan Wang. Genxd: Generating any 3d and 4d scenes. arXiv preprint arXiv:2411.02319, 2024. 2, 3 [106] Hongyu Zhou, Jiahao Shao, Lu Xu, Dongfeng Bai, Weichao Qiu, Bingbing Liu, Yue Wang, Andreas Geiger, and Yiyi Liao. Hugs: Holistic urban 3d scene understanding via gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2133621345, 2024. 2 [107] Zhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Distilling view-conditioned diffusion for 3d reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1258812597, 2023. 2 [108] Hanxin Zhu, Tianyu He, Xiqian Yu, Junliang Guo, Zhibo Chen, and Jiang Bian. Ar4d: Autoregressive 4d generation from monocular videos. arXiv preprint arXiv:2501.01722, 2025. 3 [109] Bojia Zi, Shihao Zhao, Xianbiao Qi, Jianan Wang, Yukai Shi, Qianyu Chen, Bin Liang, Kam-Fai Wong, and Lei Zhang. Cococo: Improving text-guided video inpainting for better consistency, controllability and compatibility. arXiv preprint arXiv:2403.12035, 2024. 3, 6, Vivid4D: Improving 4D Reconstruction from Monocular Video by Video Inpainting"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Implemenentation Details 6.1. Training Data Curation Based on our proposed pipeline that utilizes pre-trained 2D tracking model to generate masked videos, we curated our training dataset from OpenVid-1M [55]. To ensure the quality and effectiveness of our training data, we applied two filtering criteria. First, we removed videos with fixed viewpoints using the camera motion annotations provided by OpenVid-1M, as they do not offer the viewpoint variation needed for robust view synthesis and 3D understanding. Second, we filtered out sequences with insufficient dynamic content, despite having camera motion. Specifically, we computed the ratio of masked regions to the total video area: Rv = 1 (cid:88) t=1 Mt It where Mt represents the number of pixels in the mask region of frame t, and It is the total number of pixels in frame t. Videos with ratio Rv below threshold τ (τ = 0.98) were excluded from our training set, as they indicate limited scene dynamics and thus provide insufficient novel content for learning inpainting patterns. Through this filtering process, our final dataset comprises 150K video clips totaling 7.5M frames, each containing rich viewpoint variations and dynamic scene content. The simplicity and automation of our curation pipeline makes it particularly scalable, enabling continuous dataset expansion as new video content becomes available online. 6.2. Training Details of Diffusion Model We fine-tuned our video inpainting diffusion model based on text-to-video diffusion model [7, 51, 76]. The training data is selected from OpenVid-1M [55], which provides detailed text captions for each video. We center-crop the input training frames to resolution of 512 512, with frame length of 16, and randomly select an anchor video as 16frame slice from the video. We expand the input channels of the original U-Net from 4 to 13, initializing the additional convolutional layer parameters to zero. The entire U-Net was trained for 450K iterations with learning rate of 105 and batch size of 2. During inference, we process input videos exceeding 16 frames in length using multiple overlapping slices. To maintain temporal consistency, each subsequent slice (after the first) incorporates the last 4 frames from the previous slice, concatenated with the next 12 frames as input. This overlapping strategy ensures smooth transitions between slices and mitigates temporal discontinuities in the reconstructed video. 6.3. Depth Scale Alignment & View Warping Details Depth Scale Alignment: To align the monocular depth predicted by the model to the metric scale, we utilize the sparse point cloud {Xi}M i=1 from COLMAP [65, 66] and apply least squares algorithm combined with the RANSAC algorithm [22] to achieve more robust and accurate depth map, effectively reducing the impact of outliers and improving alignment reliability. = K[Tt Xi]1:3, Specifically, given video = {It}T t=1 with frames and its camera intrinsics K, camera poses {Tt}T t=1, we first project the sparse 3D points onto the image plane for each frame t: Pi = [Pi di where Xi is the homogeneous coordinate of each point in world frame, [] extracts the corresponding component of the coordinate. We determine valid points located in the image plane based on pixel coordinates, and then compute an optimal scale factor αt and shift factor βt using RANSAC combined with least squares optimization: = [Pi pi t/di t]1:2 t]3, For each frame t, we perform RANSAC for = 100 iterations. In each iteration: 1. Randomly sample 10 valid projected points {pj with their corresponding metric depths {dj and shift β(k) 2. Compute candidate scale α(k) ing the least square error: t }10 j=1 }10 j=1. by minimizα(k) , β(k) = argmin α,β 10 (cid:88) (cid:16) α ˆDt(pj ) + β dj (cid:17)2 j=1 3. Apply the computed scale and shift to align the predicted depth map: D(k) = α(k) ˆDt + β(k) . 4. Back-project all valid pixels using the aligned depth to . 5. Compute the Chamfer distance between this point cloud obtain point cloud (k) and the corresponding COLMAP sparse points: CD(k) = 1 (k) (cid:88) pP (k) min qXt q2 2+ 1 Xt (cid:88) qXt min pP (k) p2 2 where Xt represents the COLMAP sparse points visible Start Frame End Frame Num Frames Dataset Scene iPhone Dataset apple block paper windmill spin teddy HyperNeRF Dataset 3dprinter broom chicken 290 240 56 10 40 106 40 321 303 136 90 265 103 169 103 32 64 81 81 81 64 64 64 Table 7. Details of the datasets we test for 4D Reconstrcution."
        },
        {
            "title": "Backbone",
            "content": "mPSNR mSSIM mLPIPS 4D GS Motion Field (ours) 16.16 16.80 0.4676 0.5170 0.5164 0.4750 Table 8. Quantitative ablation study on reconstruction backbones on iPhone dataset and HyperNeRF dataset. in frame t. Finally, we select the optimal scale and shift parameters that yield the minimum Chamfer distance: αt, βt = α(k) , β(k) where = argmin CD(k) The final metric-aligned depth maps are obtained as Dt = αt ˆDt + βt. View Warping: The view warping stage transforms each frame to desired novel viewpoint using the aligned depth maps and camera poses independently. For target camera pose t, we perform two-step projection process. First, we back-project each pixel pm from frame It into 3D space using its depth value: pm cam,t = Dt(pm pm where represents the homogeneous coordinate. These 3D points are then projected onto the target view by perspective projection π(): ) = T1 )K1 pm X(pm cam,t, pm = π( X(pm ), t, K) The warping process inevitably creates regions without valid projections due to occlusions. This results in masked video sequence = {I t}T t=1, where each warped frame RHW 3 is accompanied by binary mask {0, 1}HW indicating valid pixels (1) and masked regions (0). 6.4. Iterative View Augmentation Following recent advances in 3D reconstruction [16, 26], we pre-compute set of target camera viewpoints in 4D space. Specifically, we start with data buffer D0 with input"
        },
        {
            "title": "Method",
            "content": "PSNR SSIM LPIPS FVD"
        },
        {
            "title": "CoCoCo\nStereoCrafter\nViewCrafter\nOurs",
            "content": "24.98 23.44 16.95 27.22 0.8213 0.8006 0.5822 0.8223 0.0987 0.1498 0.2560 0.0801 12.58 52.63 52.07 14.30 Table 9. Quantitative comparison of novel view-aware video in-painting on our processed 5K videos from OpenVid-1M. monocular video 0 along with its aligned depths D0 and poses T0, namely D0 = (V 0, D0, T0). We define novel camera poses {Ti}H i=1 distributed around the scene and set the total number of iteration to , resulting in the synthesis of = H/N novel videos in each iteration. During iteration j(1 ), we select frames with minimal warping angles from data buffer Dj1 at each timestamp and warp them to target poses Tj,l by our depth-guided warping pipeline, where {1, 2, ..., h}. Specifically, we first identify the two most peripheral frames from the data buffer Dj1 at each timestamp t, denoted as It,1 and It,2 with poses Tt,1 and Tt,2. We then select total of closest unvisited target poses from the set of remaining target poses, in which each target pose is closest to either of the peripheral frames: , ..., Tj,h {Tj,1 min(d(Ti, Tt,1), d(Ti, Tt,2)) } = argmin TiUt where Ut is the set of unvisited poses for frame t, and d(, ) measures the pose distance. For each selected target pose Tj,l where {1, 2, ..., h}, we find the closer peripheral frame and warp it to the target pose using our depth-guided warping pipeline. Next, we organize the masked frames temporally based on warp distance rank, meaning that the l-th least distant masked frames from each timestamp are grouped together into video. This results in novel video sequences = {V j,1, ..., j,h} where j,l = {I j,l t=1, along with corresponding masks Mj = {Mj,1, ..., Mj,h} with consistent occluded regions. }T To complete the novel view synthesis for each of the videos, we employ our trained video inpainting diffusion model F, which utilizes the input monocular video 0 as an anchor along with text description of the scene: ˆV j,l = F(V j,l, Mj,l, 0) }T where ˆV j,l = { ˆI j,l sequence for the l-th novel video in iteration j. t=1 represents the final inpainted video After each successful inpainting iteration, we estimate the metric depth Dj,l for each of the novel synthesized videos and update the data buffer Dj = Dj1 {( ˆV j,l, Dj,l, Tj,l)}h t=1. We repeat this process until all pre-defined poses are traversed by each frame. l=1, where Tj,l = {Tj,l }T Above all, we progressively build set of multi-view observations of the scene. Since our pre-defined target poses"
        },
        {
            "title": "Ground Truth",
            "content": "Figure 10. Qualitative comparison of dynamic scene reconstruction on iPhone dataset and HyperNeRF dataset. cover the entire observation space with smooth transitions between adjacent poses, this iterative approach allows us to gradually expand viewpoint coverage, hence maintaining geometric accuracy. This strategy also helps mitigate floaters that typically arise from inaccurate depth estimates when warping at large angles. By the final iteration, we obtain data buffer DN , which provides rich supervision signals for robust 4D scene reconstruction. 6.5. Details for Obtaining Supervision Masks During iterative view augmentation, we introduce supervision mask to avoid the inconsistency of 4D reconstruction supervision signals caused by repeated inpainting across different iterations. To maintain this mask, we utilize global point cloud = {Pi}K i=1 merged from all input monocular frames to track the inpainting history. For each inpainted frame, t where j,l , where Mj,l we first render the point cloud to the corresponding camera pose, generating visibility mask j,l (p) = 1 indicates visible regions and j,l (p) = 0 indicates invisible regions. When masked frame is inpainted, we check the visibility of each filled pixel Mj,l is the binary mask corresponding to the masked frame. If the pixel is not yet visible (V j,l (p) = 0), we back-project it into the global point cloud as Pp R3 (making this region visible in future renderings) and set its supervision mask value Sj,l (p) = 1. If the pixel is already visible (V j,l (p) = 1), we set Sj,l (p) = 0. Note that the supervision mask for the initially visible areas of the scene is always set to 1. This mechanism ensures that only the first inpainted result for each region is used as supervision signal, preventing conflicting supervision from repeated inpainting of the same region. t"
        },
        {
            "title": "Ground Truth",
            "content": "Figure 11. Qualitative comparison of novel view-aware video in-painting on our processed 5K videos from OpenVid-1M. 7. Additional Experimental Results 7.1. Monocular 4D Reconstruction Datasets: We select continuous clip containing few dozen frames from each test scene in the iPhone [25] and HyperNeRF [58] datasets, rather than using all available observations. The perspectives used for reconstruction exhibit significant parallax compared to the test viewpoints in these cases. The specific scenes, along with their start and end frames, are detailed in Tab. 7. Results: We presented some of our 4D reconstruction results in the main paper ( Sec. 4.1). Here, we further provide qualitative comparison with StereoCrafter [104] and ViewCrafter [97] in Fig. 10 to illustrate our superiority in reconstruction quality. It can be observed that StereoCrafter produces results with noticeable color differences, which is primarily due to limitations in its video inpainting model when handling large missing areas. ViewCrafter, while avoiding color discrepancies, also suffers from blurry outputs, as the input masked videos contain dynamic objects. In contrast, our method generates significantly clearer results with minimal deviation from the ground truth, demonstrating the superior capability of our video inpainting model in generating warped multi-view videos. 7.2. Novel View-Aware Video inpainting Dataset and Metrics: We compare the novel view-aware video inpainting task on our processed non-training 5K masked videos from OpenVid [55], where an anchor video is paired with each masked video. We employ PSNR, SSIM [82], LPIPS [102] and FVD [72] as the evaluation metrics for assessing inpainting quality. Comparison Baselines: We choose CoCoCo [109], StereoCrafter [104] and ViewCrafter [97] as our baselines. CoCoCo is text-guided video inpainting diffusion model with strong consistency and controllability. StereoCrafter trains video diffusion model for stereo video generation. Although ViewCrafter is 3D-aware video inpainting method, we include it as strong baseline due to its state-of-theart performance in geometry-aware scene completion. For StereoCrafter, we provide the masked video and corresponding mask as input. For CoCoCo, we additionally supply text prompt to guide the inpainting process, following its original setup. For ViewCrafter, we use the first frame from our anchor video as the image condition. Results: The results are presented in Tab. 9 and Fig. 11. Among the baseline methods, CoCoCo demonstrates superior inpainting performance due to its integration with base image generation models. However, this approach tend to generates content that deviates from the scene context. In regions with weak contextual constraints, the results exhibit significant discrepancies from the ground truth, suggesting tendency toward generation rather than faithful reconstruction. This observation is further supported by our methods higher PSNR/SSIM/LPIPS scores in quantitative evaluations. StereoCrafter exhibits limitations in handling large-scale holes due to its task orientation and training data constraints. Viewcrafter, originally designed for Figure 13. Failure Cases. Our method degenerates when the depth of the input video is not correctly obtained. vision of multiview synchronized video, the results of 4D GS [93] have improved significantly but still do not reach the same level as motion field [78]. This discrepancy may be attributed to differences in their representations. The combination of the canonical GS with motion field may offer greater robustness than extending 3D GS with an additional time dimension when using generated videos as supervision. 8. Limitations We present some failure cases in Fig. 13. The main limitation of our proposed method is its dependency on the accuracy of poses and metric depth estimation. Although we utilize robust depth scale alignment to improve depth accuracy, as well as iterative view augmentation and IV loss to mitigate the impact of depth noise on warping and reconstruction, our results may still degrade in scenes where the monocular depth prediction is highly inaccurate. In future work, we could combine our method with some recent dense reconstruction methods [43, 79, 100] to obtain more accurate scene geometry, as well as improving our diffusion models by improving model architecture to better synthesize fine details during the inpainting process. Additionally, extending our approach to handle more extreme camera motions and complex scene dynamics would further enhance its applicability to wider range of scenarios. 4D GS Motion Field (ours) Figure 12. Qualitative ablation study on reconstruction backbones on iPhone dataset. 3D inpainting, shows notable distortions and color inconsistencies when handling videos with dynamic objects. In contrast, our method produces results that are most consistent with the ground truth, demonstrating that our model effectively learns the spatiotemporal priors embedded in the anchor video. Additionally, leveraging an anchor video as constraint provides crucial spatiotemporal priors and partial observations of occluded regions (due to dynamic foreground objects). This additional source of information enables more accurate inpainting and reconstruction, making our approach particularly well-suited for 4D pseudo-ground-truth generation. By incorporating natural geometric and temporal constraints, our method ensures that the inpainted results closely match the original video and maintain consistency across different views. Furthermore, subsequent 4D reconstruction experiments validate the necessity of our model design. 7.3. Ablation Study Effect of 4D Reconstruction Backbones: We also evaluate the reconstruction backbone of our method. The results are illustrated in Tab. 8 and Fig. 12. With the super-"
        }
    ],
    "affiliations": [
        "ByteDance PICO",
        "Zhejiang University"
    ]
}