{
    "paper_title": "Interpretable non-linear dimensionality reduction using gaussian weighted linear transformation",
    "authors": [
        "Erik Bergh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Dimensionality reduction techniques are fundamental for analyzing and visualizing high-dimensional data. With established methods like t-SNE and PCA presenting a trade-off between representational power and interpretability. This paper introduces a novel approach that bridges this gap by combining the interpretability of linear methods with the expressiveness of non-linear transformations. The proposed algorithm constructs a non-linear mapping between high-dimensional and low-dimensional spaces through a combination of linear transformations, each weighted by Gaussian functions. This architecture enables complex non-linear transformations while preserving the interpretability advantages of linear methods, as each transformation can be analyzed independently. The resulting model provides both powerful dimensionality reduction and transparent insights into the transformed space. Techniques for interpreting the learned transformations are presented, including methods for identifying suppressed dimensions and how space is expanded and contracted. These tools enable practitioners to understand how the algorithm preserves and modifies geometric relationships during dimensionality reduction. To ensure the practical utility of this algorithm, the creation of user-friendly software packages is emphasized, facilitating its adoption in both academia and industry."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 1 0 6 7 1 . 4 0 5 2 : r Interpretable non-linear dimensionality reduction using gaussian weighted linear transformation Erik Bergh M.Sc er.bergh@gmail.com April 25, 2025 Abstract Dimensionality reduction techniques are fundamental for analyzing and visualizing high-dimensional data. With established methods like t-SNE and PCA presenting trade-off between representational power and interpretability. This paper introduces novel approach that bridges this gap by combining the interpretability of linear methods with the expressiveness of non-linear transformations. The proposed algorithm constructs non-linear mapping between high-dimensional and low-dimensional spaces through combination of linear transformations, each weighted by Gaussian functions. This architecture enables complex non-linear transformations while preserving the interpretability advantages of linear methods, as each transformation can be analyzed independently. The resulting model provides both powerful dimensionality reduction and transparent insights into the transformed space. Techniques for interpreting the learned transformations are presented, including methods for identifying suppressed dimensions and how space is expanded and contracted. These tools enable practitioners to understand how the algorithm preserves and modifies geometric relationships during dimensionality reduction. To ensure the practical utility of this algorithm, the creation of userfriendly software packages is emphasized, facilitating its adoption in both academia and industry."
        },
        {
            "title": "Introduction",
            "content": "Dimensionality reduction is fundamental task in data analysis and maIts objective is to transform high-dimensional data into chine learning. more compact and meaningful representation. This process addresses critical challenges by reducing computational demands, improving visualization, and highlighting essential structures while filtering out noise. By focusing on the most relevant patterns, dimensionality reduction facilitates efficient computation and enhances the understanding of high-dimensional datasets. Over the years, numerous techniques have been developed for dimensionality reduction, ranging from classical linear approaches to advanced nonlinear methods. Principal Component Analysis (PCA) [Hotelling, 1933], one of the earliest and most influential techniques, employs linear transformation to project data onto orthogonal axes that maximize variance. PCA is valued for its computational efficiency, scalability, and ease of interpretation, as the principal components are linear combinations of the original features. However, its reliance on linearity often limits its ability to capture non-linear relationships. To overcome these limitations, non-linear methods have been developed to capture complex patterns in high-dimensional data. t-Distributed Stochastic Neighbor Embedding (t-SNE) [Van der Maaten and Hinton, 2008], for example, is extensively used for visualizing high-dimensional data. By optimizing model that preserves local similarities, t-SNE generates informative embeddings. Uniform Manifold Approximation and Projection (UMAP) [McInnes et al., 2018] leverages manifold learning to preserve both local and global data structures effectively. Other notable methods, including Isomap [Tenenbaum et al., 2000] and Locally Linear Embedding (LLE) [Roweis and Saul, 2000], demonstrate strong representational capacity, revealing complex patterns in data. However, these methods often lack interpretability, and some, like tSNE, cannot extend their transformations to new data without retraining. While UMAP, Isomap, and LLE can extend transformations, they come with varying degrees of computational overhead. Deep learning have further expanded the toolkit for dimensionality reduction, with autoencoders [Hinton and Salakhutdinov, 2006] representing common approach. Autoencoders, neural network architectures designed to encode data into compressed latent space and decode it back to the original space, offer remarkable representational capacity by capturing complex relationships within data. However, they often require substantial amounts of data and lack interpretability. Despite these advancements, gap remains in developing methods that combine the representational power of non-linear approaches with the inter2 pretability of linear techniques, motivating the need for new solutions."
        },
        {
            "title": "2.1 Mathematical Framework",
            "content": "2.1.1 Core Transformation Construct non-linear transformation from Rd1 to Rd2 (where d2 < d1) by combining multiple linear transformations through Gaussian weighting. Each linear transformation is assigned Gaussian function. For an input vector Rd1, the transformation is defined as: (x) = (cid:88) i=1 wi(x)Ti(x) (1) where is the number of linear transformations with corresponding weight functions, wi(x) are weights, and Ti : Rd1 Rd2 are linear transformations. 2.1.2 Gaussian Weight Computation The weight wi(x) for each transformation is computed using Gaussian function: (cid:18) gi(x) = exp (cid:19) µi2 σ2 These weights are then normalized to sum to 1: wi(x) = gi(x) j=1 gj(x) + ϵ (cid:80)m where: (2) (3) σi, the standard deviation, is optimized during training. ϵ is small constant added for numerical stability. µi Rd1 represents the center of the i-th Gaussian function, initialized through random sampling from the input dataset . By default, these centers remain fixed during optimization. 3 2.1.3 Linear Transformations Each Ti is linear transformation represented by matrix Mi Rd1d2. The transformation of point by Ti is computed as: Ti(x) = Mix (4)"
        },
        {
            "title": "2.2 Optimization Process",
            "content": "2.2.1 Objective Function The algorithm minimizes the difference between pairwise distances in the original and transformed spaces. For dataset = {x1, ..., xn}, the loss function is: = 1 (cid:88) i,j (xi xj (xi) (xj))2 (5) where is the number of considered pairs. 2.2.2 Training Procedure The optimization process consists of two phases: Initialization Phase The Gaussian centers µi are initialized through random sampling from the input dataset. The standard deviations, σi, are initialized to unity. Transformation matrices Mi are initialized using random distribution. Compute pairwise distances between all points in the original space. Optimization Phase For each iteration: 1. Forward pass: compute transformed points (xi) for all data points. 2. Compute pairwise distances in the transformed space. 3. Update parameters using gradient descent, such as the Adam optimizer [Kingma and Ba, 2014]. Standard deviations σi. Transformation matrices Mi. Optionally, Gaussian centers µi if enabled. 2.2.3 Distance Computation Optimization To improve computational efficiency, the algorithm considers only the nearest neighbors during loss function computation: Lk = 1 (cid:88) (cid:88) jNk(i) (xi xj (xi) (xj))2 (6) where Nk(i) represents the nearest neighbors of point xi in the original space. The optimization process stops based on the chosen termination criterion, such as exceeding predefined patience threshold or reaching the maximum number of epochs."
        },
        {
            "title": "Interpretability",
            "content": "To showcase the interpretability of trained model, the algorithm is applied to 3-dimensional S-shaped dataset created using Scikit-learns make curve function. total of 1,000 data points are generated, as illustrated in Figure 1. For details, refer to https://scikit-learn.org/stable/modules/ generated/sklearn.datasets.make_s_curve.html. This section does not present an exhaustive list of possible interpretability techniques for the proposed algorithm. Instead, it presents those techniques implemented at https: //github.com/erikbergh/interpretable_dim_reduction, as of the time of writing. The algorithm employs 100 Gaussian functions and accounts for all pairwise distances. The output dimensionality is reduced to two, and the training is performed over 2,000 epochs. The resulting dimensionality reduction is depicted in Figure 2. The reconstruction error, which measures how well the distances are preserved, is given by: error = (cid:80) i,j xi xj2 (xi) (xj)2 i,j xi xj2 (cid:80) (7) The reduction shown in Figure 2 has reconstruction error of 0.45 (rounded to two decimal places), where value of zero indicates perfect preservation of distances. 5 Figure 1: Visualization of dataset generated using Scikit-learns make curve. 3."
        },
        {
            "title": "Influence by dimension",
            "content": "By comparing Figure 1 and Figure 2, it is evident that the y-axis contributes less prominently in the reduced dimensions. Such comparisons are challenging for most datasets. The influence of each dimension can be calculated as follows: = 1 (cid:88) (cid:33) (cid:32) (cid:80) (cid:80) Mijk j,k Mijk (8) Here, denotes the index of the linear transformations, represents the column index, and corresponds to the row index. For this dimensionality reduction, = [0.40, 0.25, 0.35] (rounded to two decimal places), confirming that the y-dimension is less represented in the reduction. Due to the non-linear nature of the dimensionality reduction, these values vary across the space. Understanding how the influence of each dimension changes spatially can provide additional insights. By calculating: w(p) = 1 (cid:88) (cid:32) (cid:80) (cid:80) w(p)iMijk j,k w(p)iMijk (cid:33) (9) 6 Figure 2: The data points visualized in the reduced 2D space. where represents point in the reduced-dimensional space, w(p) is evaluated for points forming mesh grid over the reduced space. The values of w(p)j=2, corresponding to the original y-dimension, are shown as the background in Figure 3, with the data points in the reduced space plotted on top. The color of the points corresponds to their original y-values. Figure 3 illustrates that the influence of the original y-axis is minimal at the center of the S-shape. In contrast, other regions show greater influence from the original y-axis, although this influence remains less pronounced compared to the contributions of the and axes. 3."
        },
        {
            "title": "Influence Skewness",
            "content": "It is of interest to know if the reduced space is skewed to represent or underrepresent one or more original dimensions. By calculating the variance of w(p) in Equation (9) for each point p, we obtain the variance of influence across the different dimensions. In Figure 4, this variance is plotted as the background, with the data 7 Figure 3: The values w(p)j=2, corresponding to the original y-dimension, are shown as the background. The data points in the reduced space are plotted on top, with the point colors corresponding to their original y-values. points in the reduced space shown on top. As shown in Figure 4, the maximum skewness occurs at the center of the shape. This is directly related to the minimal influence of the original dimension y, as illustrated in Figure 3."
        },
        {
            "title": "3.3 Expansion and Contraction of Space",
            "content": "It is useful to determine whether the reduced space is expanded or contracted at given point. This analysis helps compare the relative distances between neighboring points, such as nearby pair versus distant pair. The expansion or contraction at point is quantified using: (p) = (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:88) w(p)iMi (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2 , (10) where (p) represents the norm of the transformation at point p. value of (p) > 1 indicates that the space is expanded at p, while (p) < 1 indicates Figure 4: The variance ar( w(p)) over the mesh grid is plotted as the background. Data points in the reduced space are plotted on top. contraction. To visualize this, (p) is computed over set of points forming mesh grid in the reduced space. In Figure 5, (p) is shown as the background, with data points in the reduced space plotted on top. As shown in Figure 5, the background indicates that the space is contracted everywhere. The contraction is minimal at the center of the S-shape."
        },
        {
            "title": "4 Discussion and Conclusion",
            "content": "The proposed algorithm demonstrates both representational power and interpretability, addressing key limitations of traditional dimensionality reduction techniques. Unlike t-SNE, which is constrained to the dataset on which it is trained, the proposed algorithm can extend its transformations to new data points with no additional computational cost compared to processing data point from the training set. Furthermore, the algorithms representational capacity enables it to capture complex patterns and relationships within the data, offering richer representations than linear methods like PCA. 9 Figure 5: The values of (p) calculated over the mesh grid are shown as the background. Data points in the reduced space are plotted on top. However, the algorithm presents several challenges. Like t-SNE, it requires substantial computational resources for training, limiting its scalability for very large datasets or resource-constrained environments. Additionally, the risk of converging to local minima can affect result quality and stability. The algorithms robustness remains an open question, particularly regarding whether the risk of suboptimal representation increases with dataset complexity. Future work should examine its computational complexity on large and complex datasets and compare performance with methods such as t-SNE. Another key area is benchmarking its representational power against other algorithms. Additionally, improving model interpretability is essential. While the algorithm shows promise in this regard, systematic methods for extracting insights and articulating relationships in user-friendly manner are needed. This is especially crucial for high-dimensional datasets where in-depth investigation of each dimension is unfeasible. Developing intuitive yet effective interpretation techniques without sacrificing insights would significantly enhance the algorithms usability, making it valuable tool for analyzing complex data. In conclusion, this algorithm combines representational power with interpretability, bridging the gap between popular techniques like t-SNE and PCA. By creating intuitive and meaningful software packages for interpretation, the algorithm has the potential to become widely used tool in both academic and industrial settings."
        },
        {
            "title": "5 Resources",
            "content": "A Python implementation of the proposed method is available at https:// github.com/erikbergh/interpretable_dim_reduction. The repository includes the algorithm, minimal working example, and interpretability demonstrations."
        },
        {
            "title": "6 Conflict of interest",
            "content": "There are no conflicts of interest known to the author."
        },
        {
            "title": "References",
            "content": "Harold Hotelling. Analysis of complex of statistical variables into principal components. Journal of Educational Psychology, 24(6):417441, 1933. Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11):25792605, 2008. Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018. Joshua Tenenbaum, Vin De Silva, and John Langford. global geometric framework for nonlinear dimensionality reduction. Science, 290(5500): 23192323, 2000. Sam Roweis and Lawrence Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):23232326, 2000. Geoffrey Hinton and Ruslan Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504507, 2006. D.P. Kingma and J.B. Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014."
        }
    ],
    "affiliations": []
}