{
    "paper_title": "TRAIL: Trace Reasoning and Agentic Issue Localization",
    "authors": [
        "Darshan Deshpande",
        "Varun Gangal",
        "Hersh Mehta",
        "Jitin Krishnan",
        "Anand Kannappan",
        "Rebecca Qian"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The increasing adoption of agentic workflows across diverse domains brings a critical need to scalably and systematically evaluate the complex traces these systems generate. Current evaluation methods depend on manual, domain-specific human analysis of lengthy workflow traces - an approach that does not scale with the growing complexity and volume of agentic outputs. Error analysis in these settings is further complicated by the interplay of external tool outputs and language model reasoning, making it more challenging than traditional software debugging. In this work, we (1) articulate the need for robust and dynamic evaluation methods for agentic workflow traces, (2) introduce a formal taxonomy of error types encountered in agentic systems, and (3) present a set of 148 large human-annotated traces (TRAIL) constructed using this taxonomy and grounded in established agentic benchmarks. To ensure ecological validity, we curate traces from both single and multi-agent systems, focusing on real-world applications such as software engineering and open-world information retrieval. Our evaluations reveal that modern long context LLMs perform poorly at trace debugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our dataset and code are made publicly available to support and accelerate future research in scalable evaluation for agentic workflows."
        },
        {
            "title": "Start",
            "content": "TRAIL: Trace Reasoning and Agentic Issue Localization Darshan Deshpande Varun Gangal Hersh Mehta Jitin Krishnan Anand Kannappan Rebecca Qian Patronus AI {darshan, varun.gangal, hersh, jitin, anand, rebecca}@patronus.ai 5 2 0 2 3 1 ] . [ 1 8 3 6 8 0 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The increasing adoption of agentic workflows across diverse domains brings critical need to scalably and systematically evaluate the complex traces these systems generate. Current evaluation methods depend on manual, domainspecific human analysis of lengthy workflow tracesan approach that does not scale with the growing complexity and volume of agentic outputs. Error analysis in these settings is further complicated by the interplay of external tool outputs and language model reasoning, making it more challenging than traditional software debugging. In this work, we (1) articulate the need for robust and dynamic evaluation methods for agentic workflow traces, (2) introduce formal taxonomy of error types encountered in agentic systems, and (3) present set of 148 large human-annotated traces (TRAIL) constructed using this taxonomy and grounded in established agentic benchmarks. To ensure ecological validity, we curate traces from both single and multi-agent systems, focusing on real-world applications such as software engineering and open-world information retrieval. Our evaluations reveal that modern long context LLMs perform poorly at trace debugging, with the best GEMINI-2.5-PRO model scoring mere 11% on TRAIL. Our dataset and code are made publicly available to support and accelerate future research in scalable evaluation for agentic workflows1."
        },
        {
            "title": "Introduction",
            "content": "The rapid advancement of large language models (LLMs) has catalyzed the development of complex agentic systems capable of automating difficult, multi-step tasks across various domains such as software engineering and multi-hop IR (Ma et al., 2023; OpenAI, 2024; Nguyen et al., 2024; Wang et al., 2025a). Unlike traditional generative models, 1https://huggingface.co/datasets/PatronusAI/ TRAIL Figure 1: Illustration of the TRAIL taxonomy of errors agents can interact with diverse tools and dynamically navigate uncertain environments, often with minimal human supervision (Wang et al., 2024a). This escalation of system complexity has leads to proposals for more challenging and multifaceted evaluation processes (Nasim, 2025) and has led to the adoption of LLMs as evaluators and critics of these agentic systems (Zheng et al., 2023; Chen et al., 2024; Kim et al., 2024; Zhu et al., 2025; Deshpande et al., 2024a). However, as multi-agent systems become integral to real-world workflows, evaluating and debugging their performance remains significant challenge. Agentic non-determinism (Laban et al., 2025; Patronus AI, 2025) and multistep task solving (Mialon et al., 2023; Yao et al., 2024) demand greater observability than the simple end-to-end evaluations offered by existing benchmarks (Kapoor et al., 2024a; Zhuge et al., 2024; Moshkovich et al., 2025; Cemri et al., 2025). Such complex environments require granular taxonomies and well-annotated traces that can serve as references for debugging and root-cause analysis of agent behaviors (Cemri et al., 2025). When creating taxonomies and benchmarks to test and improve agents, we must ensure these are grounded in real-world applications, following principles established in prior work (Bowman and Dahl, 2021; Liu et al., 2024b). Previous agent trace analysis frameworks have primarily focused on parsed traces containing unstructured text (Cemri et al., 2025), which do not adequately represent common agent framework outputs that generate structured traces logged in standardized formats like opentelemetry (OpenTelemetry, 2025). We have observed that handling structured data remains challenging for LLMs (Guo et al., 2023; Sui et al., 2024), an observation corroborated by previous research on automated software engineering trace analysis (Roy et al., 2024a; Ma et al., 2024b). These limitations highlight the need for new approaches specifically designed for structured agentic traces. To address these challenges and facilitate the analysis and evaluation of structured agentic executions, we propose formal error taxonomy, shown in Figure 3, that promotes granular failure diagnosis. We also present carefully curated, turnlevel annotated trace dataset called TRAIL (Trace Reasoning and Agentic Issue Localization), which demonstrates the validity and practical utility of our proposed taxonomy. In our work, we utilize and build on SWE-Bench (Jimenez et al., 2024; Aleithan et al., 2024) and GAIA (Mialon et al., 2023) while addressing three major shortcomings inherent to previous automatic agent evaluation paradigms. Firstly, we aim to replace end to end analysis of agents (Mialon et al., 2023; Wang et al., 2025a; Jimenez et al., 2024; Huang et al., 2024) with benchmark containing step-level analysis of traced agentic workflows. Secondly, we address the need for grounding in real scenarios by producing opentelemetry-based structured traces that span beyond present model context length limits. Finally, as compared to benchmarks focused only on agentic reasoning and coordination (Cemri et al., 2025; Kokel et al., 2025), TRAIL displays its validity through addition of finer, more aligned system execution failures and planning error categories such as API errors and Task Orchestration Errors to our taxonomy. Such categories are not only relevant to model developers but also to users and engineers optimizing single and multi-agent AI applications. The contributions of our work are as follows: 1. We introduce formal taxonomy (Figure 1) that defines, fine-grained agentic error categories spanning across three key areas: reasoning, planning, and execution. 2. Based on this taxonomy, we present TRAIL, an ecologically grounded execution trace benchmark comprising 148 meticulously curated traces (totaling 1987 open telemetry spans, of which 575 exhibit at least one error) drawn from the GAIA (Mialon et al., 2023) and SWE-Bench (Jimenez et al., 2024) datasets and covering wide range of tasks. 3. We show that TRAIL is non-trivially difficult benchmark for LLMs on many fronts (a) Current SOTA LLM families such as O3, CLAUDE-3.7-SONNET and GEMINI2.5-PRO perform modestly at best on TRAIL, both in terms of predicting error categories and their location. With GEMINI-2.5-PRO the best performing model, achieving only 11% combined joint accuracy on both splits. (b) Solving TRAIL requires significant fraction of the maximum input length of LLMs (or exceeds it), as well as requires generating significant fraction of their maximum output (See Table Table 2) (c) Models benchmarked on TRAIL benefit from both the presence and greater extent of reasoning chains (5.1.4, 5.1.5), highlighting the need for improvement in exploration capabilities of LLMs. 4. TRAIL is fully open-source (MIT License), will be accompanied by HuggingFace leaderboard, and serves as foundation for future research on evaluating agentic workflows."
        },
        {
            "title": "2 Relevant Work",
            "content": "of LLM-as-a-Judge Shortcomings conventional metrics such as ROUGE, BLEU, and BERTScore (Schluter, 2017; Freitag et al., 2020; Hanna and Bojar, 2021) has led to the wide adoption of LLMs as evaluators and critics of other AI systems (Zheng et al., 2023; Zhu et al., 2025; Chen et al., 2025, 2024; Kim et al., 2024). Recent approaches have enhanced LLM judges reasoning capabilities through techniques like unconstrained evaluation plan and specialized training methods that enable more robust evaluation performance across diverse scenarios (Lightman et al., 2023; Wang et al., 2024e; Trivedi et al., 2024; Saha The evaluation landscape has et al., 2025). evolved significantly with the introduction of frameworks like FLASK (Ye et al., 2024b) which decompose coarse-level scoring into skill set-level evaluations for each instruction, demonstrating high correlation between model-based and humanbased evaluations. The Prometheus models (Kim et al., 2023, 2024, 2025) established significant benchmark by creating judge models that surpass GPT-4 in ranking for subjective evaluation criteria. Their research also examined how performance deteriorates as subjectivity increases. More recently, several studies have enhanced judge model performance through external augmentations and checklists, highlighting the importance of incorporating high-quality reasoning chains and human guidance in model training (Lee et al., 2025; Deshpande et al., 2024b,a; Chen et al., 2025; Wang et al., 2025b). Despite promising advancements, LLM judges have shown issues with propagation of biases and lack of robustness to longer inputs (Ye et al., 2024a; Hu et al., 2024b; Wei et al., 2024; Zhou et al., 2025). Since trace evaluation requires robust reasoning over large contexts (Tian et al., 2024), LLM judges have not seen wide application in this sector yet. Agentic Evaluation LLM-powered agents have gained significant traction for their capacity to manage intricate, sequential tasks while adaptively engaging with varied environments, rendering them particularly valuable for practical real-world applications such as software engineering and multi-hop IR (Ma et al., 2023; OpenAI, 2024; Nguyen et al., 2024; Wang et al., 2025a; Jimenez et al., 2024; Qian et al., 2024; Wang et al., 2024d; Patil et al., 2024). However, the performance gains of multiagent frameworks remain minimal compared to their single-agent counterparts (Xia et al., 2024; Kapoor et al., 2024b). As these agentic systems become more prevalent, evaluation frameworks (as compared to LLM evaluation) must offer greater customization and granularity to effectively assess the complex and sometimes unpredictable interactions between multiple agents, enabling users to precisely identify and diagnose errors at each step of the process (Roy et al., 2024b; Akhtar et al., 2025; Jiang et al., 2025; Zhuge et al., 2024; OpenManus, 2024). Agent Benchmarks Software engineering domain has become fertile testbed for LLM-based collaborative problem solving for real-world use cases and to evaluate agents ability to handle realistic coding tasks. SWE-Bench (Jimenez et al., 2024; Aleithan et al., 2024; Pan et al., 2024) was introduced as grounded benchmark asking whether LLMs can resolve real-world GitHub issues. Similarly, GAIA (Mialon et al., 2023) is benchmark for General AI Assistants featuring real-world questions requiring reasoning, tool use, and multimodality. AssistantBench (Yoran et al., 2024) introduces challenging benchmark of realistic, timeconsuming web tasks to evaluate web agents. For agents, it is key to distinguish input sample failures from the judge models own internal reasoning failures. Highlighting spans can help models focus and avoid losing context while also providing additional explainability and performance improvements (Lv et al., 2024; Li et al., 2024). Other core benchmarks include DevAI (Zhuge et al., 2024), MLE-bench (Chan et al., 2024), HumanEval (Du et al., 2024), and MBPP (Odena et al., 2021). Traces and Error Taxonomies Emerging work has emphasized the need for better observability in the agent execution traces to diagnose and manage the non-deterministic nature of agentic systems (Kapoor et al., 2024a; Zhuge et al., 2024; Moshkovich et al., 2025; Cemri et al., 2025). For instance, Roy et al. (2024a) explores using LLMbased agents to dynamically collect diagnostic information from logs and metrics using retrieval tools for root cause analysis of cloud system incidents. Akhtar et al. (2025) surveys how LLMs are being applied to automate even log analysis in security contexts. Jiang et al. (2025) is log analysis framework for diagnosing large-scale LLM failures based on studying real-world training failures. Ma et al. (2024c) explores the potential for log parsing by proposing an LLMParser delivering comprehensive evaluations in various settings. Once the trace errors are found, to serve as references for users to debug or conduct root cause analysis of agent behaviors, these errors require granular taxonomy (Cemri et al., 2025; Kokel et al., 2025; Bai et al., 2024a). MAST (Cemri et al., 2025) presents an empirically grounded failure mode taxonomy but focusing only on agentic reasoning and coordination. ACPBench (Kokel et al., 2025), using synthetic dataset, focuses on atomic reasoning about action and is designed to evaluate LLMs core planning skills. Other related work includes taxonomies to evaluate multi-turn conversations (Bai et al., 2024a) and designing LLM agent framework to identify and quantify complex evaluation criteria (Arabzadeh et al., 2024; Epperson et al., 2025). Thus, TRAIL distinguishes itself through its ecological validity while comprehensively addressing both single and multi-turn systems with its granular taxonomy, particularly emphasizing critical execution and planning failure patterns."
        },
        {
            "title": "3 Agentic Error Taxonomy",
            "content": "LLM reasoning, while having advanced significantly , remains critical source of failures in agentic workflows (Costarelli et al., 2024). These errors span several dimensions, from flawed information generation to problematic decision-making and output production (Cemri et al., 2025). In this section, we define comprehensive taxonomy (as summarized in Figure 3) of agentic errors spanning three key areas of failures: reasoning, planning and coordination, and system execution."
        },
        {
            "title": "3.1 Reasoning Errors",
            "content": "Hallucinations The generation of factually incorrect or nonsensical content is pervasive issue with LLMs that extends to agents (Huang et al., 2025; Ji et al., 2023). Text-only hallucinations manifest as deviations from factual reality or fabricated textual elements, such as ungrounded statements misaligned with established world knowledge (Ji et al., 2023). On the other hand, tool-related hallucinations occur when agents fabricate tool outputs or misunderstand tool capabilities (Zhang et al., 2024b). This can involve inventing results supposedly produced by tool or claiming non-existent functionalities (Xu et al., 2024) Information Processing Recent years have observed rise in retrieval augmented generation, process through which relevant data is retrieved based on similarity to query and reasoned over (Hu and Lu, 2024; Gao et al., 2025). In their work Xu et al. (2025); Su et al. (2025) show that LLMs are poor at reasoning over retrieved instances for different tasks . The problems in information processing can be categorized into two subcategories: poor information retrieval and misinterpretation of reasoned outputs. Poor information retrieval (Wu et al., 2024) is critical problem since incorrect or irrelevant queries lead to redundancy in agent systems (Wu et al., 2024) and can potentially lead to content overloading problems as observed with chain-of-thought reasoning models (Stechly et al., 2024). Similarly, misinterpretation of retrieved context (Tool output Misinterpretation) (Karpinska et al., 2024; Wang et al., 2024b) may lead to incorrectness of smaller, local task which propagates through the multiple steps of agent reasoning, leading to incorrectness or inefficiencies. Decision Making At step-level, task misunderstanding can stem from ambiguity in the input prompt, unclear instructions, or from the LLMs inability to differentiate the instructions in the data from the instructions in its prompt (Zverev et al., 2024). Detecting if an agent has misunderstood problem (Incorrect Problem ID) involves analyzing agents path trajectory (Yuan et al., 2024) which, when combined with large contexts, makes error localization difficult. Hence, reliably detecting misinterpretation is essential for agent improvement. Another important aspect of decision making in agent workflows involves selection of correct tools at every step (Qin et al., 2023). Since planoptimality leads to cost minimization and efficiency (Yehudai et al., 2025), selection of the correct tool is essential. However, this selection is highly dependent on the task, available tools and contextual knowledge about the system. Hence, we include Tool Selection Error as sub-category under Decision Making. Output Generation LLMs have been shown to incorrectly format structured outputs (Shorten et al., 2024; Liu et al., 2024a) and since most tool calls are made either via JSON responses or through code, correct formatting of outputs becomes necessary. To address this, we add the Formatting Errors subcategory to our taxonomy. On the other hand, works such as White et al. (2024); Heo et al. (2024) show that LLMs are poor at instruction following when provided with complex or ambiguous instructions. To address such non-compliant outputs, we add the Instruction Non-compliance subcategory to our taxonomy."
        },
        {
            "title": "3.2 System Execution Errors",
            "content": "Configuration Issues Incorrect configuration of the agentic environment can lead to failures and restriction of agentic capabilities (Hu et al., 2024a). core example of an agentic configuration issue is Incorrect Tool Definition. In their recent work, Fu et al. (2024) show that agents can be tricked into using tools based on incorrect definitions or obfuscations in prompt which makes this subcategory concern for both security and reliability of the system. Furthermore, poor configuration of environment variables (Environment Setup Errors) such as missing API keys, incorrect file access permissions can lead to unexpected failures in following correct reasoning paths generated by the model. API and System Issues Since agentic systems combine LLMs with software tools, issues with tool use or incorrect tool implementation can show up as errors during the workflow. Recently, agentic frameworks have seen increased adoption of remote tool access through protocols like Model Context Protocol (Anthropic, 2025) which highlights the need for capturing and categorizing API failures so that failures can be quickly reported to engineers building these remote tools (Shen, 2024). Additionally, Milev et al. (2025) also show that runtime errors with agentic tools are an underexplored problem. To capture these runtime errors with API tools, we add the most common errors (in addendum to (Liu et al., 2023a)): Rate Limiting (such as 429 error), Authentication Errors (such as 401 and 403 errors), Service Errors (such as 500 errors) and Resource Not Found Errors (such as 404 error). Resource Management Resource management is key for agents that have access to operating system tools like python interpreters or terminal access tools. In such scenarios, poor task understanding and planning in agents can surface vulnerabilities in systems. One prime example of such vulnerability is the exhaustion of resources allocated to the agent (Ge et al., 2023) (Resource Exhaustion) or infinite loops (Zhang et al., 2024a) (Timeout Issues) which can lead to cases of compute memory overflow or system overloads. Early detection of such errors is essential to prevent infrastructure collapse."
        },
        {
            "title": "3.3 Planning and Coordination Errors",
            "content": "Context Management With increased adoption of planning and reasoning stages in agentic workflows (Yao et al., 2023; Ke et al., 2025), reasoning over long contexts is an essential task for agents. In such scenarios, maintaining episodic and semantic (Zhang et al., 2024c) context for information becomes necessary for an agent to improve. For the creation of this taxonomy, we label the context and instruction retention errors as Context Handling Failures. Tool call repetition (Kokane et al., 2024) (Resource Abuse) is another clear failure of planning, context management and tool understanding in agents which we capture in our taxonomy. Task Management Environmental misconfigurations or LLM hallucinations can act as distractions in agentic systems. Poor recoverability from such distractions can lead to failures in task completion and goal deviation (Ma et al., 2024a). This error is exacerbated with multi-agent systems and introduction of sub-tasks, making proper task orchestration an important aspect of agentic success. Hence, we include Goal Deviation and Task Orchestration Errors in our taxonomy."
        },
        {
            "title": "4 TRAIL Benchmark",
            "content": "While existing agent trace evaluation benchmarks focus on parsed traces and unstructured text (Cemri et al., 2025), applications utilizing agentic frameworks 2,3,4 tend to produce structured traces that are logged using standardized opentelemetry formats such as openinference (Arize AI, 2025; Moshkovich et al., 2025). TRAIL, following this standardization, is benchmark aimed to evaluate LLM capabilities to analyze and evaluate these long, structured agentic executions. TRAIL follows our fine grained taxonomy and contains total of 148 carefully annotated agent execution traces. TRAIL uses text-only data instances from the GAIA (Mialon et al., 2023) and SWE Bench Lite (Jimenez et al., 2024) datasets, spanning multiple information retrieval and software engineering bug fixing tasks. The TRAIL dataset contains total of 841 annotated errors, averaging at 5.68 errors per trace."
        },
        {
            "title": "4.1 Goals and Design Choices",
            "content": "Core Agent Task We aim to showcase realistic agentic workflows and so we target two widely adopted agentic datasets, the GAIA benchmark (Mialon et al., 2023), an open world search task, and the SWE-Bench-Lite (Jimenez et al., 2024) dataset, for locating problems in Github repositories and creating fixes for them. We select these datasets due to their challenging nature, necessity for environment, search space exploration, and good alignment with the taxonomy. Agent Orchestration Liu et al. (2023b) first presented standardized hierarchical method of orchestrating agents, derivatives of which are actively adopted by several works (Zhao et al., 2024, 2025). We closely follow this hierarchical structure 2https://github.com/huggingface/smolagents 3https://github.com/pydantic/pydantic-ai 4https://github.com/langchain-ai/langchain GAIA SWE Bench Model Cat. F1 Loc. Acc. Joint ρ Cat. F1 Loc. Acc. Joint ρ LLAMA-4-SCOUT-17B-16E-INSTRUCT LLAMA-4-MAVERICK-17B-128E-INSTRUCT GPT-4.1 OPEN AI O1* OPEN AI O3* ANTHROPIC CLAUDE-3.7-SONNET* GEMINI-2.5-PRO-PREVIEW-05-06* GEMINI-2.5-FLASH-PREVIEW-04-17* 0.041 0.122 0.218 0.138 0.296 0.254 0.389 0.337 0.000 0.023 0.107 0.040 0.535 0.204 0.546 0.372 0.000 0.134 0.000 0.338 0.028 0.411 0.013 0.450 0.092 0.449 0.047 0.738 0.183 0.462 0.100 0.550 0.050 0.191 0.166 CLE CLE CLE 0.148 0.213 0.000 0.083 0.000 CLE CLE CLE 0.238 0. 0.000 0.264 0.000 -0.273 0.153 0.000 CLE CLE CLE CLE CLE CLE 0.817 0.050 0.292 0.000 Table 1: Performance on GAIA and SWE Bench. Models marked with * have reasoning set to \"high\"; indicates 1M+ token context window. Insufficient context length is indicated by CLE. Pearson correlation between overall human and generated scores are represented in the ρ column. and adopt the Hugging Face OpenDeepResearch agent (Hugging Face, 2024) for creating traces for the GAIA benchmark. We select the state-of-theart o3-mini-2025-01-31 (OpenAI, 2025d) and assign it as the backbone model for the manager and search agents respectively because of its strong tool use and planning ability as showcased by Phan et al. (2025). For more information, refer to Appendix subsection A.6. On the other hand, to explore single-agent planning errors and elicit context handling errors for the SWE-Bench split, we use CodeAct agent (Wang et al., 2024c) and provide it access to sandboxed environment and python interpreter and the gitingest5 library. We select claude-3-7-sonnet-20250219 as the backbone model due to its strong performance on software engineering tasks (Anthropic, 2025). To further organically introduce errors into this agent pipeline, we add instructional constraints such as output text length limits and force exploration through prompts. The complete prompt can be found in the Appendix subsection A.7. Workflow Tracing To ensure compatibility of this dataset with real world tracing and observability software, all traces are collected with the help of opentelemetry (OpenTelemetry, 2025), specifically, its most widely adopted open-source derivative compatible with agents, the openinference standard (Arize AI, 2025) following Moshkovich et al. (2025)."
        },
        {
            "title": "4.2 Data Annotation and Validation",
            "content": "We select four expert annotators with background in software engineering and log debugging to annotate our agent traces. Furthermore, to ensure 5https://github.com/cyclotruc/gitingest thoroughness and completeness of the annotation, we assign separate set of 63 traces to calculate and analyze agreement. Since these traces are large and sometimes extend well beyond the maximum context length of most LLMs (as we detail in 5.1.1), we perform four independent rounds of verification with set of four ML researchers to ensure high quality. During curation, annotators are asked to first iterate over each LLM and tool span and annotate each one individually and in context to the previous spans based on our taxonomy. For each span, the annotators mark the span ID, error category type, evidence, description and impact level (Low/Medium/High) associated with the error if it exists. At the end, the annotators are asked to rate the overall trace based on instruction adherence, plan optimality, security and reliability (definitions in the Appendix A.3.1). On an average, single trace of GAIA takes approximately 30 minutes to annotate whereas SWE Bench takes close to 40 minutes each. The additional verification step takes approximately 20 minutes for each trace, making the overall annotation time for each trace close to 110 minutes for GAIA and 120 minutes for SWE-Bench-Lite6. For the SWE Bench split, we reviewed 30 traces containing total of 444 annotated spans, of which 25 spans (5.63%) were modified by reviewers. The top three error categories revised were Resource Abuse (33.33%), Languageonly Hallucinations (20.83%) and Tool-related Hallucinations (12.5%). Alternately, we reviewed 33 traces with total of 697 annotated spans and modified 37 spans (5.31%) after revision. The most commonly revised error types were Language-only 6We did not explore and verify information (web-based or otherwise) from contents external to the trace because our baseline models are not expected to do so. Verifying such information will add more time to this estimate. Hallucinations (23.08%), Resource Abuse(19.23%) and Poor Information Retrieval(19.23%). This displays high level of interannotator agreement during data curation. 4.3 Dataset Analysis Figure 2: Distribution of Errors per Trace As result of the post annotation review, we observed that total of 144 traces were found to contain errors and 114 traces in GAIA and 30 from the SWE Bench contained at least one error. Our annotators identified total of 841 unique errors in all traces with an average of 5.68 and median of 5 errors per trace (see Figure 2). Further breakdown in Figure 3 shows that errors span wide range of categories, with most errors belonging to the Output Generation category. Formatting Errors or Instruction Non-compliance errors in particular account for 353 of the 841 errors (or nearly 42%). Meanwhile, there do not appear to be many instances of System Execution Errors. Although this categorical imbalance stems directly from the type of traces we use as input, we believe it highlights two key considerations when evaluating modern agentic pipelines. Firstly, the high concentration of Output Generation errors indicates that despite best efforts at prompt-engineering, LLM systems have hard time with higher-level reasoning and understanding the parameters of the task they are given. Secondly, the large number of categories with very few errors are several times, catastrophic for the progress of the system. For example, recoverability is severely hindered for cases where an API fails as opposed to when systems deviate from goals or misinterpret tool outputs. Despite appearing infrequently, these error categories are the most important to identify and evaluate for practical agentic applications. Most of the errors in our dataset fall in either the high or medium categories (Figure 6a). Model hallucinations and resource management issues have high impact on the agent behavior, whereas nearly 44% of the Output Generation errors are classified as Low impact (Figure 6b). This further emphasizes the importance of classification schema which is able to capture \"high-value\" infrequent or rare behaviors. We view the ability to identify and classify each of the samples in the long-tail of high-errors into specific category as key feature of our taxonomy. 4.4 Evaluation Setup To show the effectiveness of TRAIL as benchmark for evaluating LLM-as-judge models, we select state-of-the-art closed and open source models. For closed source models, we select OpenAIs O1, O3 and GPT-4.1 models (OpenAI, 2025b,c,a), Anthropics CLAUDE 3.7 SONNET (Anthropic, 2025) and Googles GEMINI-2.5 PRO and FLASH models (DeepMind, 2025) due to their strong reasoning and agentic capabilities. For open source alternatives, we select the Llama-4 suite of models, specifically LLAMA-4 SCOUT and MAVERICK (Meta AI, 2025) due to their long context length and good reasoning support. We use Together AI as the provider for testing Llama-4 models. We separate these open and closed models according to support for reasoning tokens and large context windows (1M+ tokens) respectively in Table 1. The generation temperature and top were set to 0 and 1 to maximize reproducibility for non-reasoning tests whereas we used API defaults for reasoning models."
        },
        {
            "title": "5 Results",
            "content": "In the following subsections, we analyze the research questions below: 1. How is TRAIL performance affected by long context ability of the LLM? How many of the input instances exhaust the LLMs context window? How does it vary with length of the trace? We answer these in 5.1.1 5.1.2, and 5.1.3. 2. Does TRAIL benefit from more reasoning? We answer this question in 5.1.4 and 5.1.5. 3. Which error categories are easier to predict for models when predicting TRAIL over traces? On which particular ones is performance substantively worse for non-reasoning models? We answer these questions in 5.1.6 (a) Category Annotations (b) Parent Category Annotations (c) Root Category Annotations Figure 3: Error Category Distributions in TRAIL as per different levels of the taxonomy 5.1 Qualitative and Quantitative Analysis 5.1.1 Task Difficulty - Context Length and Generation Horizon As we can see in Table 2, the distribution of input token lengths that need to be ingested in the form of raw trace jsons to perform our task cuts close to the input context limit of several LLMs - with the maximum input trace length always being twice longer than the input length limit, and even the mean itself sometimes going over. Furthermore, even the typical output token length horizon the LLMs need to generate for the task exceeds the 1K token mark on average, with the maximum being 3.7K at the least. Besides being significant % of the maximum output token length, this indicates the challengingly long generation horizon generating for TRAIL entails."
        },
        {
            "title": "Model Performance",
            "content": "We compare how the models in Table 1 rank based on their aggregate performance on TRAIL vis-avis the relative ranking of the subsets of these models that occur on the freshly [per May 2025] updated long-context benchmark leaderboards Longbenchv2 and fiction.lives LongContextBench (Bai et al., 2024b; Ficlive, 2025), and notice this differs for only one model (o3 being third best rather than best on the latter). We defer the complete detail of the rankings read off these leaderboards to A.1.1."
        },
        {
            "title": "Length",
            "content": "We find all performance metrics to be anticorrelated with input token length, as detailed in Table 3. This supports the hypothesis that longer input raw traces increase the difficulty of TRAIL for models. 5.1.4 Reasoning vs Non-Reasoning Models From Table 1, we see all post-2024 reasoning models (i.e. save O1) outperforming non-reasoning ones on both Error category F1 and Location Accuracy. On Joint Accuracy i.e. getting both error category and location right, the gap between the two families is larger. Reasoning models other than o1 perform at 1.5-8 times the best performing non-reasoning model in terms of joint accuracy."
        },
        {
            "title": "5.1.5 Does Reasoning Effort Matter?\nTo ablate for the effect of extent of reasoning in\na controlled fashion, we run experiments for the\nsame model (O3) with different levels of effort -\n\"high\", \"medium\" and \"low\", as supported by Ope-\nnAI’s API through its reasoning.effort parameter.\nWe observe a steady decrease in all three metrics\nwith decrease in the extent of reasoning. For in-\nstance, Category F1 goes from 0.296 for \"high\" →\n0.277 for \"medium\" → 0.264 for \"low\".",
            "content": "These observations empirically validate the hypothesis that TRAIL benefits from the model being able to expend reasoning effort at test time, and the superior numbers reasoning models we observe on TRAIL (noted in 5.1.4) are not merely courtesy better post-training or pre-training. We defer the complete reasoning effort ablation results to Appendix A.2."
        },
        {
            "title": "5.1.6 Performance Across Categories",
            "content": "1. Hard-to-Predict Categories Context Handling Failures is one of the most challenging categories, with most models showing an F1 score of 0.00, suggesting this type of error requires sophisticated reasoning. The only model that performs passably atleast is CLAUDE3.7-SONNET at 0.18 Tool Selection Errors has poor performance across most models (0.000.08 range), except for GEMINI-2.5-PRO (a) GAIA (b) SWEBench Figure 4: Input Token Length Distributions (plotted in logscale) across TRAIL tasks w.r.t two different models for raw trace json inputs. We see that significant part of the distribution for each model crosses the maximum input context length, which is dashed vertical line. Moreover, even mean lengths (dot-dashed line) fills significant % of the context window. Task Tokenizer Input Output Limit Limit Input Context Lengths Min Max Mean StdDev Min Output Token Lengths Max Mean StdDev GAIA GAIA GAIA SWEBench SWEBench SWEBench gpt-4.1 (=o3) gemini-2.5 claude-3.7 gpt-4.1 (=o3) gemini-2.5 claude-3.7 20.94K 7.50M 286.85K 768.85K 0.11K 4.47K 1.11K 0.69K 32.77K 1M 23.09K 8.25M 313.49K 843.53K 0.13K 4.95K 1.20K 0.75K 1M 8.19K 23.67K 2.66M 262.67K 456.64K 0.12K 5.37K 1.23K 0.78K 200K 128K 32.77K 120.40K 2.05M 616.92K 473.05K 0.11K 3.71K 1.71K 0.75K 1M 134.88K 2.21M 698.09K 552.34K 0.13K 4.09K 1.88K 0.83K 1M 8.19K 140.16K 2.43M 727.75K 557.86K 0.12K 4.17K 1.93K 0.87K 200K 128K Table 2: Input Context Lengths and Human-Annotated Output Token Lengths Across both GAIA and SWEBench Tasks and various SOTA models and their tokenizers. Input Length aggregates that exceed the limit are highlighted. Corr. Pearson (r) Spearman (ρ) Location Acc -0.379 -0.508 Joint Acc Categ. -0.291 -0.349 -0.296 -0.225 Table 3: Correlations between Input Length & Performance (0.26), CLAUDE-3.7-SONNET (0.27) and O3 (0.53), indicating this complex error type is difficult to detect. Task Orchestration has poor performance across most models (0.00-0.08 range) apart from GEMINI-2.5-FLASH which does significantly better on it with F1 of 0.47 2. Interesting Performance Divergence Goal Deviation shows vivid performance differences, with GEMINI2.5-PRO/FLASH doing relatively best (0.70/0.41), CLAUDE-3.7-SONNET and O3 performing moderately (0.31,0.24); and lastly O1 and the other nonreasoning models performing the lowest ( 0.05) For Poor Information Retrieval, the two gemini models GEMINI-2.5-PRO/FLASH are distinctively better (0.50/0.53) than the others which have F1 below 0.30. This points to an advantage these models might have at diagnosing incorrect conditioning on the context as whole. 3. Other Surprising Patterns Language-Only (a type of Hallucination) errors are relatively well-detected across all models (0.14-0.59), suggesting that predicting this error category is more accessible even without reasoning. Formatting Errors shows some interesting non-monotonicity and performance bands w.r.t - GPT-4.1 (0.43) and GEMINI2.5 models (0.44-0.57) perform well, while O1, O3 and CLAUDE-3.7-SONNET occupy lower band (0.23-0.31). Surprisingly, both GPT-4.1 and O1 outdo O3 here inspite of being non-reasoning and Figure 5: Heatmap visualizing Error Category F1 across models; categories are ordered left to right based on their support an earlier generation respectively."
        },
        {
            "title": "6 Conclusion",
            "content": "4. Model-Specific Observations GEMINI-2.5-PRO is clearly the top performer, with particularly strong relative performance on: Goal Deviation (0.70), Poor Information Retrieval (0.50) and Tool Output Misinterpretation (0.67) and Environment Setup Errors (0.57) GPT-4.1s performance varies highly sensitive to category, doing very well or moderately on some (Instruction Noncompliance, Language-only, Formatting Errors, Resource Abuse), while going below the 0.10 mark and even hitting complete zero on some others (Goal Deviation, Tool Selection Errors, Task Orchestration, Tool-Related Hallucinations and Context Handling Failures). In this work, we devised TRAIL, new taxonomy for agentic error classification. We accompanied this taxonomy with an expert curated dataset of 148 agentic problem instances consisting of 841 unique error derived from two publicly available datasets: GAIA and SWE Bench. We demonstrated the inability of SOTA models to act as LLM Judges, with the best performing model on our dataset (Gemini 2.5-pro) only able to achieve 18% joint accuracy on GAIA, and 5% joint accuracy on SWE Bench. Meanwhile, three of the eight models we test are unable to handle the complexity of TRAIL and fail to solve the task due to context-length limitations. The benchmark performance of the SOTA models on our dataset shows that these models are unable to systematically evaluate complex traces generated by agentic systems. This problem stems from the fundamental complexity of large agentic systems and finite context limitations of LLMs. We require new framework in order to systematically and scalably evaluate agentic workflows. Limitations & Future Work The TRAIL dataset and taxonomy are primarily focused on text-only inputs and outputs but recent advancements in multi-modal agentic systems require careful extension of the taxonomy to handle multimodal errors arising from newer categories such as multimodal tool use. One additional limitation of TRAIL is the large number of tail categories with very few examples. It is important to ensure correctness of LLM-Judges on these categories due to the high-impact nature of the failures. Future research work can look into synthetic data generation for high-impact, low-occurrence categories by systematically modifying existing traces to induce catastrophic irrecoverable failures within the LLM context."
        },
        {
            "title": "Ethics Statement",
            "content": "While curating this dataset, we ensure that annotators are only selected based on their age (18+) and their expertise in the computer science field. Annotator selection was not based on nationality, language, gender or any other characteristic apart from these two criteria. We pay annotators total of $12.66 per trace where each trace takes 30-40 minutes to annotate. We ensure that the traces do not contain any PII or any explicit or biased content by manually verifying traces before forwarding these to annotators."
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to acknowledge industry AI practicioners: Sam Yang, Mark Klein, Pasha Rayan and Pennie Li for their feedback on our error taxonomy."
        },
        {
            "title": "References",
            "content": "Siraaj Akhtar, Saad Khan, and Simon Parkinson. 2025. Llm-based event log analysis techniques: survey. arXiv preprint arXiv:2502.00677. Reem Aleithan, Haoran Xue, Mohammad Mahdi Mohajer, Elijah Nnorom, Gias Uddin, and Song Wang. 2024. Swe-bench+: Enhanced coding benchmark for llms. arXiv preprint arXiv:2410.06992. Anthropic. 2025. Claude 3.7 sonnet. https://www. anthropic.com/news/claude-3-7-sonnet. Accessed: May 9, 2025. Anthropic. 2025. Model context protocol: Transparency and control for ai inputs and outputs. Accessed: 2025-05-08. Negar Arabzadeh, Siqing Huo, Nikhil Mehta, Qingyun Wu, Chi Wang, Ahmed Hassan Awadallah, Charles L. A. Clarke, and Julia Kiseleva. 2024. Assessing and verifying task utility in LLM-powered applications. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2186821888, Miami, Florida, USA. Association for Computational Linguistics. Arize AI. 2025. Openinference. https://github. com/Arize-ai/openinference. Accessed: May 9, 2025. Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, and Wanli Ouyang. 2024a. MT-bench-101: fine-grained benchmark for evaluating large language models in multi-turn dialogues. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 74217454, Bangkok, Thailand. Association for Computational Linguistics. Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, et al. 2024b. Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks. arXiv preprint arXiv:2412.15204. Samuel Bowman and George Dahl. 2021. What will it take to fix benchmarking in natural language understanding? arXiv preprint arXiv:2104.02145. Mert Cemri, Melissa Pan, Shuyi Yang, Lakshya Agrawal, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Dan Klein, Kannan Ramchandran, et al. 2025. Why do multi-agent llm systems fail? arXiv preprint arXiv:2503.13657. Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal A. Patwardhan, Lil ian Weng, and Aleksander Mkadry. 2024. Mlebench: Evaluating machine learning agents on machine learning engineering. ArXiv, abs/2410.07095. Dongping Chen, Ruoxi Chen, Shilin Zhang, Yaochen Wang, Yinuo Liu, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. 2024. Mllm-asa-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. In Proceedings of the 41st International Conference on Machine Learning (ICML). Nuo Chen, Zhiyuan Hu, Qingyun Zou, Jiaying Wu, Qian Wang, Bryan Hooi, and Bingsheng He. 2025. Judgelrm: Large reasoning models as judge. Anthony Costarelli, Mat Allen, Roman Hauksson, Grace Sodunke, Suhas Hariharan, Carlson Cheng, Wenjie Li, Joshua Clymer, and Arjun Yadav. 2024. Gamebench: Evaluating strategic reasoning abilities of llm agents. arXiv preprint arXiv:2406.06613. Google DeepMind. 2025. thinking updates: March 2025. //blog.google/technology/google-deepmind/ gemini-model-thinking-updates-march-2025/. Accessed: May 11, 2025. Gemini model https: Darshan Deshpande, Selvan Sunitha Ravi, Sky CH Wang, Bartosz Mielczarek, Anand Kannappan, and Rebecca Qian. 2024a. Glider: Grading llm interactions and decisions using explainable ranking. arXiv preprint arXiv:2412.14140. Darshan Deshpande, Zhivar Sourati, Filip Ilievski, and Fred Morstatter. 2024b. Contextualizing argument quality assessment with relevant knowledge. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers), pages 316326, Mexico City, Mexico. Association for Computational Linguistics. Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng, and Yiling Lou. 2024. Evaluating large language models in class-level code generation. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering, ICSE 24, New York, NY, USA. Association for Computing Machinery. Will Epperson, Gagan Bansal, Victor Dibia, Adam Fourney, Jack Gerrits, Erkang Zhu, and Saleema Amershi. 2025. Interactive debugging and steering of multiagent ai systems. arXiv preprint arXiv:2503.02068. 2025. 2025). Ficlive. 6, stories/Fiction-livebench-April6-2025/ oQdzQvKHw8JyXbN87. Accessed: 2025-05-12. Fiction.livebench (april https://fiction.live/ Markus Freitag, David Grangier, and Isaac Caswell. 2020. BLEU might be guilty but references are not innocent. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6171, Online. Association for Computational Linguistics. Xiaohan Fu, Shuheng Li, Zihan Wang, Yihao Liu, Rajesh Gupta, Taylor Berg-Kirkpatrick, and Earlence Fernandes. 2024. Imprompter: Tricking llm agents into improper tool use. arXiv preprint arXiv:2410.14923. Yunfan Gao, Yun Xiong, Yijie Zhong, Yuxi Bi, Ming Xue, and Haofen Wang. 2025. Synergizing rag and reasoning: systematic review. Yingqiang Ge, Yujie Ren, Wenyue Hua, Shuyuan Xu, Juntao Tan, and Yongfeng Zhang. 2023. Llm as os, agents as apps: Envisioning aios, agents and the aiosagent ecosystem. arXiv preprint arXiv:2312.03815. Jiayan Guo, Lun Du, Hengyu Liu, Mengyu Zhou, Xinyi He, and Shi Han. 2023. Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking. arXiv preprint arXiv:2305.15066. Michael Hanna and Ondˇrej Bojar. 2021. fine-grained analysis of BERTScore. In Proceedings of the Sixth Conference on Machine Translation, pages 507517, Online. Association for Computational Linguistics. Juyeon Heo, Miao Xiong, Christina Heinze-Deml, and Jaya Narain. 2024. Do llms estimate uncertainty well in instruction-following? arXiv preprint arXiv:2410.14582. Mengkang Hu, Pu Zhao, Can Xu, Qingfeng Sun, Jianguang Lou, Qingwei Lin, Ping Luo, and Saravan Rajmohan. 2024a. Agentgen: Enhancing planning abilities for large language model based agent via environment and task generation. arXiv preprint arXiv:2408.00764. Yucheng Hu and Yuxing Lu. 2024. Rag and rau: survey on retrieval-augmented language model in natural language processing. Zhengyu Hu, Linxin Song, Jieyu Zhang, Zheyuan Xiao, Jingang Wang, Zhenyu Chen, Jieyu Zhao, and Hui Xiong. 2024b. Rethinking llm-based preference evaluation. arXiv e-prints, pages arXiv2407. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2025. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Trans. Inf. Syst., 43(2). Shijue Huang, Wanjun Zhong, Jianqiao Lu, Qi Zhu, Jiahui Gao, Weiwen Liu, Yutai Hou, Xingshan Zeng, Yasheng Wang, Lifeng Shang, et al. 2024. Planning, creation, usage: Benchmarking llms for comprehensive tool utilization in real-world complex scenarios. arXiv preprint arXiv:2401.17167. Hugging Face. 2024. open deep research: An open-source replication of openais deep research https://github.com/huggingface/ agent. smolagents/tree/main/examples/open_deep_ research. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Comput. Surv., 55(12). Zhihan Jiang, Junjie Huang, Zhuangbin Chen, Yichen Li, Guangba Yu, Cong Feng, Yongqiang Yang, Zengyin Yang, and Michael R. Lyu. 2025. L4: Diagnosing large-scale llm training failures via automated log analysis. arXiv preprint arXiv:2503.20263. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2024. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations. Sayash Kapoor, Benedikt Stroebl, Zachary S. Siegel, Nitya Nadgir, and Arvind Narayanan. 2024a. Ai agents that matter. arXiv preprint arXiv:2407.01502. Philippe Laban, Hiroaki Hayashi, Yingbo Zhou, and Jennifer Neville. 2025. Llms get lost in multi-turn conversation. arXiv preprint arXiv:2505.06120. Sayash Kapoor, Benedikt Stroebl, Zachary S. Siegel, Nitya Nadgir, and Arvind Narayanan. 2024b. Ai agents that matter. arXiv preprint arXiv:2407.01502. Marzena Karpinska, Katherine Thai, Kyle Lo, Tanya Goyal, and Mohit Iyyer. 2024. One thousand and one pairs: A\" novel\" challenge for long-context language models. arXiv preprint arXiv:2406.16264. Zixuan Ke, Fangkai Jiao, Yifei Ming, Xuan-Phi Nguyen, Austin Xu, Do Xuan Long, Minzhi Li, Chengwei Qin, Peifeng Wang, Silvio Savarese, et al. 2025. survey of frontiers in llm reasoning: Inference scaling, learning to reason, and agentic systems. arXiv preprint arXiv:2504.09037. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, and Minjoon Seo. 2023. Prometheus: Inducing finegrained evaluation capability in language models. arXiv preprint arXiv:2310.08491. Seungone Kim, Juyoung Suk, Ji Yong Cho, Shayne Longpre, Chaeeun Kim, Dongkeun Yoon, Guijin Son, Yejin Cho, Sheikh Shafayat, Jinheon Baek, Sue Hyun Park, Hyeonbin Hwang, Jinkyung Jo, Hyowon Cho, Haebin Shin, Seongyun Lee, Hanseok Oh, Noah Lee, Namgyu Ho, Se June Joo, Miyoung Ko, Yoonjoo Lee, Hyungjoo Chae, Jamin Shin, Joel Jang, Seonghyeon Ye, Bill Yuchen Lin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. 2025. The BiGGen bench: principled benchmark for finegrained evaluation of language models with language models. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 5877 5919, Albuquerque, New Mexico. Association for Computational Linguistics. Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. 2024. Prometheus 2: An open source language model specialized in evaluating other language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 43344353, Miami, Florida, USA. Association for Computational Linguistics. Shirley Kokane, Ming Zhu, Tulika Awalgaonkar, Jianguo Zhang, Thai Hoang, Akshara Prabhakar, Zuxin Liu, Tian Lan, Liangwei Yang, Juntao Tan, et al. 2024. Spectool: benchmark for characterizing errors in tool-use llms. arXiv preprint arXiv:2411.13547. Harsha Kokel, Michael Katz, Kavitha Srinivas, and Shirin Sohrabi. 2025. Acpbench: Reasoning about action, change, and planning. In AAAI. AAAI Press. Yukyung Lee, Joonghoon Kim, Jaehee Kim, Hyowon Cho, Jaewook Kang, Pilsung Kang, and Najoung Kim. 2025. Checkeval: reliable llm-as-a-judge framework for evaluating text generation using checklists. arXiv preprint arXiv:2403.18771. Yafu Li, Zhilin Wang, Leyang Cui, Wei Bi, Shuming Shi, and Yue Zhang. 2024. Spotting AIs touch: Identifying LLM-paraphrased spans in text. In Findings of the Association for Computational Linguistics: ACL 2024, pages 70887107, Bangkok, Thailand. Association for Computational Linguistics. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets verify step by step. Michael Xieyang Liu, Frederick Liu, Alexander Fiannaca, Terry Koo, Lucas Dixon, Michael Terry, and Carrie Cai. 2024a. \" we need structured output\": Towards user-centered constraints on large language model output. In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, pages 19. Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. 2023a. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688. Yu Lu Liu, Su Lin Blodgett, Jackie Chi Kit Cheung, Vera Liao, Alexandra Olteanu, and Ziang Xiao. 2024b. Ecbd: Evidence-centered benchmark design for nlp. arXiv preprint arXiv:2406.08723. Zhiwei Liu, Yutong Liu, Yuxuan Zhang, Jiaxin Zhang, Xiaotian Liu, Zhen Wang, Jun Huang, and Yaliang Wang. 2023b. Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents. arXiv preprint arXiv:2308.05960. Qitan Lv, Jie Wang, Hanzhu Chen, Bin Li, Yongdong Zhang, and Feng Wu. 2024. Coarse-to-fine highlighting: Reducing knowledge hallucination in large language models. In International Conference on Machine Learning (ICML). Kaixin Ma, Hongming Zhang, Hongwei Wang, Xiaoman Pan, Wenhao Yu, and Dong Yu. 2023. Laser: Llm agent with state-space exploration for web navigation. arXiv preprint arXiv:2309.08172. Xinbei Ma, Yiting Wang, Yao Yao, Tongxin Yuan, Aston Zhang, Zhuosheng Zhang, and Hai Zhao. 2024a. Caution for the environment: Multimodal agents are susceptible to environmental distractions. arXiv preprint arXiv:2408.02544. Zeyang Ma, An Ran Chen, Dong Jae Kim, Tse-Hsun Chen, and Shaowei Wang. 2024b. Llmparser: An exploratory study on using large language models for log parsing. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering, pages 113. Zeyang Ma, An Ran Chen, Dong Jae Kim, Tse-Husn Chen, and Shaowei Wang. 2024c. Llmparser: An exploratory study on using large language models for log parsing. 2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE), pages 12091221. Meta AI. 2025. Llama 4: Advancing multimodal intelligence. https://ai.meta.com/blog/ llama-4-multimodal-intelligence/. Accessed: May 11, 2025. Gregoire Mialon, Clementine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom. 2023. Gaia: benchmark for general ai assistants. arXiv preprint arXiv:2311.12983. Ivan Milev, Mislav Balunovic, Maximilian Baader, and Martin Vechev. 2025. Toolfuzzautomated agent tool testing. arXiv preprint arXiv:2503.04479. Dany Moshkovich, Hadar Mulian, Sergey Zeltyn, Natti Eder, Inna Skarbovsky, and Roy Abitbol. 2025. Beyond black-box benchmarking: Observability, analytics, and optimization of agentic systems. arXiv preprint arXiv:2503.06745. Imran Nasim. 2025. Governance in agentic workflows: Leveraging llms as oversight agents. In AAAI 2025 Workshop on AI Governance: Alignment, Morality, and Law. Dang Nguyen, Viet Dac Lai, Seunghyun Yoon, Ryan A. Rossi, Handong Zhao, Ruiyi Zhang, Puneet Mathur, Nedim Lipka, Yu Wang, Trung Bui, Franck Dernoncourt, and Tianyi Zhou. 2024. Dynasaur: Large language agents beyond predefined actions. arXiv preprint arXiv:2411.01747. Augustus Odena, Charles Sutton, David Martin Dohan, Ellen Jiang, Henryk Michalewski, Jacob Austin, Maarten Paul Bosma, Maxwell Nye, Michael Terry, and Quoc V. Le. 2021. Program synthesis with large language models. In n/a, page n/a, n/a. N/a. OpenAI. 2024. Introducing deep research. OpenAI Blog. Accessed: 2025-05-12. OpenAI. 2025a. https:// openai.com/index/gpt-4-1/. Accessed: May 11, 2025. Introducing GPT-4.1. OpenAI. 2025b. Introducing O1: state-of-the-art multimodal ai model. https://openai.com/o1/. Accessed: May 11, 2025. 2025c. OpenAI. mini. introducing-o3-and-o4-mini/. May 11, 2025. o4https://openai.com/index/ Accessed: Introducing and o3 OpenAI. 2025d. Introducing o3-mini: smaller, faster and more cost-effective model. https://openai. com/index/openai-o3-mini/. Accessed: May 9, 2025. OpenManus. 2024. Openmanus-rl: An open-source rl environment for evaluating multimodal llms on scientific reasoning. https://github.com/OpenManus/ OpenManus-RL. OpenTelemetry. 2025. OpenTelemetry openteleme- [Accessed try.io. https://opentelemetry.io/. 07-05-2025]. Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. 2024. Training software engineering agents and verifiers with swe-gym. arXiv preprint arXiv:2412.21139. Shishir Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. 2024. Gorilla: Large language model connected with massive APIs. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Patronus AI. 2025. Modeling statistical risk in ai https://www.patronus.ai/blog/ products. modeling-statistical-risk-in-ai-products. Blog post. Long Phan et al. 2025. Humanitys last exam. Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2024. ChatDev: Communicative In Proceedings agents for software development. of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1517415186, Bangkok, Thailand. Association for Computational Linguistics. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789. Devjeet Roy, Xuchao Zhang, Rashi Bhave, Chetan Bansal, Pedro Las-Casas, Rodrigo Fonseca, and Saravan Rajmohan. 2024a. Exploring llm-based agents for root cause analysis. In Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering, pages 208 219. Devjeet Roy, Xuchao Zhang, Rashi Bhave, Chetan Bansal, Pedro Las-Casas, Rodrigo Fonseca, and Saravan Rajmohan. 2024b. Exploring llm-based agents for root cause analysis. In Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering, FSE 2024, page 208219, New York, NY, USA. Association for Computing Machinery. Swarnadeep Saha, Xian Li, Marjan Ghazvininejad, Jason Weston, and Tianlu Wang. 2025. Learning to plan & reason for evaluation with thinking-llm-as-ajudge. Natalie Schluter. 2017. The limits of automatic summarisation according to ROUGE. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 4145, Valencia, Spain. Association for Computational Linguistics. Zhuocheng Shen. 2024. Llm with tools: survey. arXiv preprint arXiv:2409.18807. Minzheng Wang, Longze Chen, Fu Cheng, Shengyi Liao, Xinghua Zhang, Bingli Wu, Haiyang Yu, Nan Xu, Lei Zhang, Run Luo, Yunshui Li, Min Yang, Fei Huang, and Yongbin Li. 2024b. Leave no document behind: Benchmarking long-context LLMs with extended multi-doc QA. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 56275646, Miami, Florida, USA. Association for Computational Linguistics. Sky CH Wang, Darshan Deshpande, Smaranda Muresan, Anand Kannappan, and Rebecca Qian. 2025a. Browsing lost unformed recollections: benchmark for tip-of-the-tongue search and reasoning. arXiv preprint arXiv:2503.19193. Connor Shorten, Charles Pierse, Thomas Benjamin Smith, Erika Cardenas, Akanksha Sharma, John Trengrove, and Bob van Luijt. 2024. Structuredrag: Json response formatting with large language models. arXiv preprint arXiv:2408.11061. Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. 2024c. Executable code actions elicit better llm agents. In Proceedings of the 41st International Conference on Machine Learning (ICML 2024). Kaya Stechly, Karthik Valmeekam, and Subbarao Kambhampati. 2024. Chain of thoughtlessness? an analysis of cot in planning. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han yu Wang, Haisu Liu, Quan Shi, Zachary S. Siegel, Michael Tang, Ruoxi Sun, Jinsung Yoon, Sercan O. Arik, Danqi Chen, and Tao Yu. 2025. Bright: realistic and challenging benchmark for reasoning-intensive retrieval. Yuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, and Dongmei Zhang. 2024. Table meets llm: Can large language models understand structured table data? benchmark and empirical study. In WSDM 24: Proceedings of the 17th ACM International Conference on Web Search and Data Mining, pages 16201629. Association for Computing Machinery. Runchu Tian, Yining Ye, Yujia Qin, Xin Cong, Yankai Lin, Yinxu Pan, Yesai Wu, Hui Haotian, Liu Weichuan, Zhiyuan Liu, and Maosong Sun. 2024. DebugBench: Evaluating debugging capability of large language models. In Findings of the Association for Computational Linguistics: ACL 2024, pages 4173 4198, Bangkok, Thailand. Association for Computational Linguistics. Prapti Trivedi, Aditya Gulati, Oliver Molenschot, Meghana Arakkal Rajeev, Rajkumar Ramamurthy, Keith Stevens, Tanveesh Singh Chaudhery, Jahnavi Jambholkar, James Zou, and Nazneen Rajani. 2024. Self-rationalization improves llm as fine-grained judge. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2024a. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345. Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. 2024d. OpenHands: An Open Platform for AI Software Developers as Generalist Agents. Yutong Wang, Pengliang Ji, Chaoqun Yang, Kaixin Li, Ming Hu, Jiaoyang Li, and Guillaume Sartoretti. 2025b. Mcts-judge: Test-time scaling in llm-as-ajudge for code correctness evaluation. arXiv preprint arXiv:2502.12468. Zhilin Wang, Alexander Bukharin, Olivier Delalleau, Daniel Egert, Gerald Shen, Jiaqi Zeng, Oleksii Kuchaiev, and Yi Dong. 2024e. Helpsteer2preference: Complementing ratings with preferences. Hui Wei, Shenghua He, Tian Xia, Fei Liu, Andy Wong, Jingyang Lin, and Mei Han. 2024. Systematic evaluation of llm-as-a-judge in llm alignment tasks: Explainable metrics and diverse prompt templates. arXiv preprint arXiv:2408.13006. Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, et al. 2024. Livebench: challenging, contamination-free llm benchmark. arXiv preprint arXiv:2406.19314. Shirley Wu, Shiyu Zhao, Michihiro Yasunaga, Kexin Huang, Kaidi Cao, Qian Huang, Vassilis Ioannidis, Karthik Subbian, James Zou, and Jure Leskovec. 2024. Stark: Benchmarking llm retrieval on textual and relational knowledge bases. Advances in Neural Information Processing Systems, 37:127129127153. Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. 2024. Agentless: Demystifying llm-based software engineering agents. arXiv preprint arXiv:2407.01489. Austin Xu, Srijan Bansal, Yifei Ming, Semih Yavuz, and Shafiq Joty. 2025. Does context matter? contextualjudgebench for evaluating llm-based judges in contextual settings. Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and JiRong Wen. 2024c. survey on the memory mechanism of large language model based agents. Hongshen Xu, Zichen Zhu, Lei Pan, Zihan Wang, Su Zhu, Da Ma, Ruisheng Cao, Lu Chen, and Kai Yu. 2024. Reducing tool hallucination via reliability alignment. arXiv preprint arXiv:2412.04141. Qi Zhao, Haotian Fu, Chen Sun, and George Konidaris. 2024. Epo: Hierarchical llm agents with environment preference optimization. arXiv preprint arXiv:2408.16090. Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. 2024. τ -bench: benchmark for toolagent-user interaction in real-world domains. arXiv preprint arXiv:2406.12045. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR). Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, Pin-Yu Chen, et al. 2024a. Justice or prejudice? quantifying biases in llm-as-a-judge. arXiv preprint arXiv:2410.02736. Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, and Minjoon Seo. 2024b. Flask: Fine-grained language model evaluation based on alignment skill sets. In International Conference on Learning Representations (ICLR). Asaf Yehudai, Lilach Eden, Alan Li, Guy Uziel, Yilun Zhao, Roy Bar-Haim, Arman Cohan, and Michal Shmueli-Scheuer. 2025. Survey on evaluation of llmbased agents. arXiv preprint arXiv:2503.16416. Ori Yoran, Samuel Joseph Amouyal, Chaitanya Malaviya, Ben Bogin, Ofir Press, and Jonathan Berant. 2024. AssistantBench: Can web agents solve realistic and time-consuming tasks? In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 89388968, Miami, Florida, USA. Association for Computational Linguistics. Chenhan Yuan, Qianqian Xie, Jimin Huang, and Sophia Ananiadou. 2024. Back to the future: Towards explainable temporal reasoning with large language models. In Proceedings of the ACM Web Conference 2024, pages 19631974. Boyang Zhang, Yicong Tan, Yun Shen, Ahmed Salem, Michael Backes, Savvas Zannettou, and Yang Zhang. 2024a. Breaking agents: Compromising autonomous llm agents through malfunction amplification. arXiv preprint arXiv:2407.20859. Yuxiang Zhang, Jing Chen, Junjie Wang, Yaxin Liu, Cheng Yang, Chufan Shi, Xinyu Zhu, Zihao Lin, Hanwen Wan, Yujiu Yang, Tetsuya Sakai, Tian Feng, and Hayato Yamana. 2024b. Toolbehonest: multilevel hallucination diagnostic benchmark for toolaugmented large language models. Yong Zhao, Kai Xu, Zhengqiu Zhu, Yue Hu, Zhiheng Zheng, Yingfeng Chen, Yatai Ji, Chen Gao, Yong Li, and Jincai Huang. 2025. Cityeqa: hierarchical llm agent on embodied question answering benchmark in city space. arXiv preprint arXiv:2502.12532. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. In Advances in Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track. Yilun Zhou, Austin Xu, Peifeng Wang, Caiming Xiong, and Shafiq Joty. 2025. Evaluating judges as evaluators: The jetts benchmark of llm-as-judges as testtime scaling evaluators. Lianghui Zhu, Xinggang Wang, and Xinlong Wang. 2025. Judgelm: Fine-tuned large language models In Proceedings of the Interare scalable judges. national Conference on Learning Representations (ICLR). Spotlight. Mingchen Zhuge, Changsheng Zhao, Dylan Ashley, Wenyi Wang, Dmitrii Khizbullin, Yunyang Xiong, Zechun Liu, Ernie Chang, Raghuraman Krishnamoorthi, Yuandong Tian, Yangyang Shi, Vikas Chandra, and Jürgen Schmidhuber. 2024. Agent-as-ajudge: Evaluate agents with agents. arXiv preprint arXiv:2410.10934. Egor Zverev, Sahar Abdelnabi, Soroush Tabesh, Mario Fritz, and Christoph Lampert. 2024. Can llms separate instructions from data? and what do we even mean by that? arXiv preprint arXiv:2403.06833."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Prompt Structure A.1.1 Long Context Leaderboard Rankings vs TRAIL From LongBenchv2, the rank-order GEMINI-2.5PRO > GEMINI-2.5-FLASH > O1 is observed, which exactly matches the ranking we observe for these models in Table 1. From fiction.lives LongContextBench, the rank order O3 > GEMINI-2.5-PRO > GEMINI-2.5-FLASH > CLAUDE-3.7-SONNET > GPT-4.1 > O1 > LLAMA4-MAVERICK > LLAMA4SCOUT can be read out. Apart from the exception of O3 being worse off than GEMINI-2.5-PRO and GEMINI-2.5-FLASH in our case, the ranking of models for TRAIL matches this entirely. A.2 Reasoning Effort Ablations In Table 4 we detail the performance metrics achieved by O3 on the GAIA split of TRAIL with different levels of reasoning effort ranging from \"low\" to \"high\", using the corresponding API parameter provided by OpenAI. A.3 Span Statistics This section details the variation in the number of input spans across TRAIL, both the overall spans found in the raw input trace open telemetry json files as well as the number out of these that are marked by annotators to exhibit an error. A.3.1 Annotation Rubric Reliability score (1-5 Likert scale): How reliable is the agent at doing its task? Does it reliably call the necessary tools without abusing resources to successfully achieve the desired goal? 1 - Unreliable: Consistently fails to utilize necessary tools, often leading to incomplete or incorrect task outcomes. 2 - Erratic: Frequently misuses resources and struggles to achieve goals consistently. 3 - Moderate: Sometimes meets task requirements but may inefficiently use tools. 4 - Reliable: Generally effective in completing tasks with appropriate tool use. 5 - Highly Reliable: Consistently achieves goals efficiently with optimal tool utilization. Security score (1-5 Likert scale): Did the agent make any unsafe code executions or did it try to use or access API or anything that can cause harm to the users personal, financial or other reputation 1 - Very Insecure: Regularly executes unsafe code and accesses harmful APIs, posing significant risks. 2 - Insecure: Often attempts unsafe operations or API accesses that could be harmful. 3 - Moderately Secure: Occasionally risky actions, but generally avoids harmful operations. 4 - Secure: Rarely engages in unsafe behaviors, minimizing potential risks. 5 - Very Secure: Consistently avoids unsafe code and harmful API accesses, ensuring user safety. Instruction adherence (1-5 Likert scale): How well was the agent able to adhere to the original task/guidelines defined by the user (first message)? Did the agent successfully complete the task that the user wanted the agent to perform? 1 - Poor: Regularly deviates from instructions and fails to complete the desired task. 2 - Inconsistent: Often struggles to follow guidelines and achieve the intended outcome. 3 - Moderate: Sometimes adheres to instructions, but task completion is inconsistent. 4 - Good: Generally follows guidelines well and completes the task successfully. 5 - Excellent: Consistently adheres to instructions and successfully completes the task as intended. Plan Optimality (1-5 Likert scale): How well did the agent plan the task? Was it able to execute all tasks appropriately? Did it handle system errors effectively by choosing the best alternative option to get to the answer? 1 - Poor: Fails to plan effectively, often executing tasks improperly and mishandling errors. 2 - Suboptimal: Frequently overlooks better options, struggling with task execution and error management. 3 - Fair: Adequately plans tasks with occasional missteps, sometimes handles errors. 4 - Good: Plans tasks well with proper execution and effective error handling. 5 - Excellent: Consistently optimal planning with efficient task execution and exemplary error management. GAIA Model o3 + \"high\" o3 + \"medium\" o3 + \"low\" Cat. F1 Loc. Acc. Joint 0.296 0.277 0.264 0.535 0.373 0.331 0.092 0.104 0.071 Table 4: Variation in performance on GAIA and SWE Bench with variation in reasoning effort Table 5: Span and Error Annotation Statistics for GAIA and SWEBench Datasets Dataset Total Traces Total Spans Total Errors Unique Error Spans Error Span Total GAIA SWEBench 118 977 (mean 8.28) 1,010 (32.58) 579 256 383 (3.33) 192 (6.19) 115 31 Model Reliability Security Instruction Adherence Plan Optimality LLAMA-4-SCOUT-17B-16E-INSTRUCT LLAMA-4-MAVERICK-17B-128E-INSTRUCT GPT-4.1 OPEN AI O1* OPEN AI O3* ANTHROPIC CLAUDE-3.7-SONNET* GEMINI-2.5-PRO-PREVIEW-05-06* GEMINI-2.5-FLASH-PREVIEW-04-17* 0.09/0.25 0.37/0.20 0.41/0.03 0.50/CLE 0.52/CLE 0.79/CLE 0.59/1.00 0.58/0.61 1.00/1.00 1.00/1.00 1.00/1.00 1.00/CLE 1.00/CLE 1.00/CLE 1.00/1.00 1.00/1. 0.075/0.08 0.14/-0.22 0.21/0.09 0.24/CLE 0.26/CLE 0.53/CLE 0.41/1.00 0.39/0.12 0.19/0.20 0.33/ -0.39 0.43/0.22 0.40/CLE 0.44/CLE 0.59/CLE 0.15/1.00 0.29/0.00 Table 6: Pearson correlation scores (GAIA/SWE Bench) between human annotators and model scores. Insufficient model context length is represented by CLE . A.4 Correlation scores for Rubrics As observed in Table 6, CLAUDE-3.7-SONNET receives the best scores (average of 0.738) for the GAIA subset whereas GEMINI-2.5-PRO achieves the highest correlation with human judgment on the SWE Bench split of TRAIL (average of 0.817). A.5 Distribution of Impact Levels in TRAIL instances The distribution of impact levels can be found in Figure 6b A.6 Agent Orchestrations for TRAIL Figure 7 shows the agent orchestration that produces the GAIA traces. This subsection describes the agents and tools used along with their descriptions. Search Agent Description The manager agent receives the following description for the search agent: team member that will search the internet to answer your question. Ask him for all your questions that require Provide him as much browsing the web. context as possible, in particular if you need to search on specific timeframe! And dont hesitate to provide him with like finding complex search task, difference between two webpages.Your request must be real sentence, not google search! Like \"Find me this information (...)\" rather than few keywords."
        },
        {
            "title": "Additional information that is provided to the",
            "content": "search agent: You can navigate to .txt online files. If non-html page is in another format, especially .pdf or Youtube video, use tool inspect_file_as_text to inspect it. Additionally, if after some searching you find out that you need more information you can use to answer the question, for request your final_answer clarification as argument to request for more information. with Google Search Tool description = \"\"\"Performs google web search for your query then returns name = \"web_search\" (a) Error Impact Levels (b) Impact Level of Errors for each Category Figure 7: Search agent orchestration for GAIA dataset \"type\": string of the top search results.\"\"\" inputs = \"query\": \"description\": to perform.\", \"integer\",\"description\": restrict results to certain year\" output_type = \"string\" \"string\", query \"type\": \"Optionally \"filter_year\": search \"The name = \"visit_page\" Visit Page Tool description = \"Visit webpage at given URL and return its text. Given url to YouTube video, this returns the transcript.\" inputs \"string\", \"description\": \"The relative or absolute url of the webpage to visit.\" output_type = \"string\" \"type\": \"url\": = name = \"page_up\" Page Up Tool description = \"Scroll the viewport UP one page-length in the current webpage and return the new viewport content.\" inputs = # This means it takes no inputs - programatically this means you call this tool as page_up() - this is not an empty dictionary output_type = \"string\" name = \"page_down\" Page Down Tool description = (\"Scroll the viewport DOWN one page-length in the current webpage and return the new viewport content.\") inputs = # This means it takes no inputs - programatically this means you call this tool as page_down() - this is not an empty dictionary output_type = \"string\" name = \"find_on_page_ctrl_f\" Finder Tool description = \"Scroll the viewport to the first occurrence of the search string. This is equivalent to Ctrl+F.\" \"type\": \"search_string\": inputs \"string\", \"description\": \"The string to search for on the page. This search string supports wildcards like *\", output_type = \"string\" = name = \"find_next\" Find Next Tool description = \"Scroll the viewport to next occurrence of the search string. This is equivalent to finding the next match in Ctrl+F search.\" inputs = output_type = \"string\" # The tool takes no inputs = = name Tool Search \"url\": Archive \"find_archived_url\" description = \"Given url, searches the Wayback Machine and returns the archived version of the url thats closest in time to the desired date.\" inputs \"string\", \"description\": need the \"type\": \"The date that \"string\",\"description\": you want to find the archive for. Give this date in the format YYYYMMDD, for instance 27 June 2008 is written as 20080627.\" output_type = \"string\" \"type\": url \"date\": archive for.\", \"The you = = name \"file_path\": Tool Inspector Text \"inspect_file_as_text\" description = \"\"\"You cannot load files yourself: instead call this tool to read file as markdown text and ask questions about it. This tool handles the following file extensions: [\".html\", \".htm\", \".xlsx\", \".pptx\", \".wav\", \".mp3\", \".m4a\", \".flac\", \".pdf\", \".docx\"], and all other types of text files. IT DOES NOT HANDLE IMAGES.\"\"\" \"description\": inputs \"The path to the file you want to read Must be .something file, as text. If it is an image, use like .pdf. DO NOT the visualizer tool instead! use this tool for an HTML webpage: use the web_search tool instead!\", \"type\": \"string\",, \"description\": \"[Optional]: Your question, as natural Provide language much context as possible. Do not pass this parameter if you just want to directly return the content of the file.\", \"type\": \"string\", \"nullable\": True, output_type = \"string\" \"question\": sentence. as name = \"visualizer\" Visualizer Tool description = \"A tool that can answer questions about attached images.\" inputs = \"image_path\": \"type\": \"string\", \"The path to the image \"description\": on which to answer the question. This should be local path to downloaded image.\", \"question\": \"type\": \"string\", \"description\": \"The question to answer.\" output_type = \"string\" A.7 Prompt for SWE Bench Data Curation A.7.1 System prompt You are an expert assistant who can solve any task using code blobs. You will be given task to solve as best you can. To do so, you have been given access to list of tools: these tools are basically Python functions which you can call with code. To solve the task, you must plan forward to proceed in series of steps, in cycle of Thought:, Code:, and Observation: sequences. At each step, in the Thought: sequence, you should first explain your reasoning towards solving the task and the tools that you want to use. Then in the Code: sequence, you should write the code in simple Python. The code sequence must end with <end_code> sequence. During each intermediate step, you can use print() to save whatever important information you will then need. These print outputs will then appear in the Observation: field, which will be available as input for the next step. In the end you have to return final answer using the final_answer tool. Here are few examples using notional tools: --- Task: \"Generate an image of the oldest person in this document.\" Thought: will proceed step by step and use the following tools: document_qa to find the oldest person in the document, then image_generator to generate an image according to the answer. Code: py answer = document_qa(document=document, question =\"Who is the oldest person mentioned?\") print(answer) <end_code> Observation: \"The oldest person in the document is John Doe, 55 year old lumberjack living in Newfoundland.\" Thought: will now generate an image showcasing the oldest person. Code: py image = image_generator(\"A portrait of John Doe, 55-year-old man living in Canada.\") final_answer(image) <end_code> --- Task: \"What is the result of the following operation: 5 + 3 + 1294.678?\" Thought: will use python code to compute the result of the operation and then return the final answer using the final_answer tool Code: py result = 5 + 3 + 1294.678 final_answer(result) <end_code> --- Task: \"Answer the question in the variable question about the image stored in the variable image. The question is in French. You have been provided with these additional arguments, that you can access using the keys as variables in your python code: {question: Quel est lanimal sur limage?, image: path/to/image.jpg}\" Thought: will use the following tools: translator to translate the question into English and then image_qa to answer the question on the input image. Code: py translated_question = translator(question= question, src_lang=\"French\", tgt_lang=\" English\") print(f\"The translated question is { translated_question}.\") answer = image_qa(image=image, question= translated_question) final_answer(f\"The answer is {answer}\") <end_code> --- Task: In 1979 interview, Stanislaus Ulam discusses with Martin Sherwin about other great physicists of his time, including Oppenheimer. What does he say was the consequence of Einstein learning too much math on his creativity, in one word? Thought: need to find and read the interview of Stanislaus Ulam with Martin Sherwin. Code: py pages = search(query=\"1979 interview Stanislaus Ulam Martin Sherwin physicists Einstein\") print(pages) <end_code> Observation: No result found for query \"1979 interview Stanislaus Ulam Martin Sherwin physicists Einstein\". Thought: The query was maybe too restrictive and did not find any results. Lets try again with broader query. Code: py pages = search(query=\"1979 interview Stanislaus Ulam\") print(pages) <end_code> Observation: Found 6 pages: [Stanislaus Ulam 1979 interview](https://ahf. nuclearmuseum.org/voices/oral-histories/ stanislaus-ulams-interview-1979/) py final_answer(\"Shanghai\") <end_code> [Ulam discusses Manhattan Project](https://ahf. nuclearmuseum.org/manhattan-project/ulammanhattan-project/) (truncated) Thought: will read the first 2 pages to know more. Code: py for url in [\"https://ahf.nuclearmuseum.org/ voices/oral-histories/stanislaus-ulamsinterview-1979/\", \"https://ahf.nuclearmuseum .org/manhattan-project/ulam-manhattanproject/\"]: whole_page = visit_webpage(url) print(whole_page) print(\"n\" + \"=\"*80 + \"n\") # Print separator between pages <end_code> Observation: Manhattan Project Locations: Los Alamos, NM Stanislaus Ulam was Polish-American mathematician. He worked on the Manhattan Project at Los Alamos and later helped design the hydrogen bomb. In this interview, he discusses his work at (truncated) Thought: now have the final answer: from the webpages visited, Stanislaus Ulam says of Einstein: \"He learned too much mathematics and sort of diminished, it seems to me personally, it seems to me his purely physics creativity.\" Lets answer in one word. Code: py final_answer(\"diminished\") <end_code> --- Task: \"Which city has the highest population: Guangzhou or Shanghai?\" Thought: need to get the populations for both cities and compare them: will use the tool search to get the population of both cities. Code: py for city in [\"Guangzhou\", \"Shanghai\"]: --- Task: \"What is the current age of the pope, raised to the power 0.36?\" Thought: will use the tool wiki to get the age of the pope, and confirm that with web search. Code: py pope_age_wiki = wiki(query=\"current pope age\") print(\"Pope age as per wikipedia:\", pope_age_wiki) pope_age_search = web_search(query=\"current pope age\") print(\"Pope age as per google search:\", pope_age_search) <end_code> Observation: Pope age: \"The pope Francis is currently 88 years old.\" Thought: know that the pope is 88 years old. Lets compute the result using python code. Code: py pope_current_age = 88 ** 0.36 final_answer(pope_current_age) <end_code> Above example were using notional tools that might not exist for you. On top of performing computations in the Python code snippets that you create, you only have access to these tools: - final_answer: Provides final answer to the given problem. Takes inputs: {answer: {type: any, description: The final answer to the problem}} Returns an output of type: any Here are the rules you should always follow to solve your task: 1. Always provide Thought: sequence, and Code:npy sequence ending with < end_code> sequence, else you will fail. 2. Use only variables that you have defined! 3. Always use the right arguments for the tools. DO NOT pass the arguments as dict as in answer = wiki({query: \"What is the place where James Bond lives?\"}), but use the arguments directly as in answer = wiki( query=\"What is the place where James Bond lives?\"). print(f\"Population {city}:\", search(f\"{city} 4. Take care to not chain too many sequential population\") <end_code> Observation: Population Guangzhou: [Guangzhou has population of 15 million inhabitants as of 2021.] Population Shanghai: 26 million (2019) tool calls in the same code block, especially when the output format is unpredictable. For instance, call to search has an unpredictable return format, so do not have another tool call that depends on its output in the same block: rather output results with print() to use them in the next block. Thought: Now know that Shanghai has the 5. Call tool only when needed, and never re-do highest population. Code: tool call that you previously did with the exact same parameters. 6. Dont name any new variable with the same name as tool: for instance dont name variable final_answer. 7. Never create any notional variables in our code, as having these in your logs will derail you from the true variables. 8. You can use imports in your code, but only from the following list of modules: [ asyncio, collections, csv, datetime, gitingest, io, itertools, json, math, os, pandas, queue, random, re, requests, stat, statistics, sys , time, unicodedata] 9. The state persists between code executions: so if in one step youve created variables or imported modules, these will all persist. 10. Dont give up! Youre in charge of solving the task, not providing directions to solve it. Now Begin! If you solve the task correctly, you will receive reward of $1,000,000. A.7.2 Task prompt New task: You will be provided with partial code base and an issue statement explaining problem to resolve. <issue> {INSERT ISSUE HERE} </issue> <repo> {INSERT REPO HERE} </repo> <base_commit> {BASE COMMIT} </base_commit> Here is an example of patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. single patch file can contain changes to multiple files. <patch> --- a/file.py +++ b/file.py @@ -1,27 +1,35 @@ def euclidean(a, b): - while b: - a, = b, % - return + if == 0: + return + return euclidean(b, % b) def bresenham(x0, y0, x1, y1): points = [] dx = abs(x1 - x0) dy = abs(y1 - y0) - sx = 1 if x0 < x1 else -1 - sy = 1 if y0 < y1 else -1 - err = dx - dy + x, = x0, y0 + sx = -1 if x0 > x1 else 1 + sy = -1 if y0 > y1 else 1 - while True: - points.append((x0, y0)) - if x0 == x1 and y0 == y1: - break - e2 = 2 * err - if e2 > -dy: + if dx > dy: + err = dx / 2.0 + while != x1: + points.append((x, y)) err -= dy - x0 += sx - if e2 < dx: - err += dx - y0 += sy + if err < 0: + += sy + err += dx + += sx + else: + err = dy / 2.0 + while != y1: + points.append((x, y)) + err -= dx + if err < 0: + += sx + err += dy + += sy + points.append((x, y)) return points </patch> need you to solve the provided issue by generating single patch file that can apply directly to this repository using git apply. Please respond with single patch file in the format shown above. To solve this, you must first use gitingest as follows (you can use this as many times as you want): from gitingest import ingest_async import asyncio summary, tree, content = asyncio.run( ingest_async(\"https://github.com/pydicom/ pydicom/commit/49 a3da4a3d9c24d7e8427a25048a1c7d5c4f7724\", max_file_size=1*1024*1024)) # filters out files greater than 1MB in size You must then carefully analyze the tree structure of the repository and its summary to understand the code and the directory structure. The content variable is huge string (cannot be printed or processed directly). The structure of the string is as follows: =============== File: README.md ================== [Contents of the README.md file here] ================ File: directory/file.py ================= A.8 Sample TRAIL Trace Data [Contents of the directory/file.py file here] ... You must parse this string in-memory by writing the appropriate regex code to extract the contents of the required file accordingly. Do not attempt to read the full string at any cost and always write regex to parse or search the content string for suitable files and contents. sample regex function to extract the content of the README.md, you would: def extract_readme_content(text): pattern = r=(2,)s* File: README.mds* =(2,)s* (.*?)(?=s* =(2,)s* File:Z) match = re.search(pattern, text, re.DOTALL) if match: return match.group(1).strip() return \"README.md content not found\" Remember that you can read the summary and tree variables directly but do not attempt to read entire content string since it might be too large to keep in memory. You must find suitable method to read and understand these code files. There is possibility that the content of the file (for example content of directory/file. py in the example above) might be too large to read as well so you must only read it in chunks or perform regex searches over the extracted file string. Never read the entire contents of the content variable or the specific content file directly. DO NOT try to use git commands and only use the gitingest import for reading and understanding the file system to generate suitable patch file. DO NOT print file contents to the terminal for analysis at all costs. If you want to analyze file string contents, make sure to do it 500 characters at time. Figure 8: Sample SWE Bench Trace and Error Labels"
        }
    ],
    "affiliations": [
        "Patronus AI"
    ]
}