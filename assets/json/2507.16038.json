{
    "paper_title": "Discovering and using Spelke segments",
    "authors": [
        "Rahul Venkatesh",
        "Klemen Kotar",
        "Lilian Naing Chen",
        "Seungwoo Kim",
        "Luca Thomas Wheeler",
        "Jared Watrous",
        "Ashley Xu",
        "Gia Ancone",
        "Wanhee Lee",
        "Honglin Chen",
        "Daniel Bear",
        "Stefan Stojanov",
        "Daniel Yamins"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Segments in computer vision are often defined by semantic considerations and are highly dependent on category-specific conventions. In contrast, developmental psychology suggests that humans perceive the world in terms of Spelke objects--groupings of physical things that reliably move together when acted on by physical forces. Spelke objects thus operate on category-agnostic causal motion relationships which potentially better support tasks like manipulation and planning. In this paper, we first benchmark the Spelke object concept, introducing the SpelkeBench dataset that contains a wide variety of well-defined Spelke segments in natural images. Next, to extract Spelke segments from images algorithmically, we build SpelkeNet, a class of visual world models trained to predict distributions over future motions. SpelkeNet supports estimation of two key concepts for Spelke object discovery: (1) the motion affordance map, identifying regions likely to move under a poke, and (2) the expected-displacement map, capturing how the rest of the scene will move. These concepts are used for \"statistical counterfactual probing\", where diverse \"virtual pokes\" are applied on regions of high motion-affordance, and the resultant expected displacement maps are used define Spelke segments as statistical aggregates of correlated motion statistics. We find that SpelkeNet outperforms supervised baselines like SegmentAnything (SAM) on SpelkeBench. Finally, we show that the Spelke concept is practically useful for downstream applications, yielding superior performance on the 3DEditBench benchmark for physical object manipulation when used in a variety of off-the-shelf object manipulation models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 8 3 0 6 1 . 7 0 5 2 : r Stanford NeuroAI Lab"
        },
        {
            "title": "Discovering and using Spelke segments",
            "content": "Rahul Venkatesh,1,, Klemen Kotar,1, Lilian Naing Chen,1, Seungwoo Kim1, Luca Thomas Wheeler1, Jared Watrous1, Ashley Xu1, Gia Ancone1, Wanhee Lee1, Honglin Chen2, Daniel Bear3, Stefan Stojanov1, Daniel Yamins1, 1Stanford University, 2OpenAI, 3Noetik Inc. Abstract. Segments in computer vision are often defined by semantic considerations and are highly dependent on category-specific conventions. In contrast, developmental psychology suggests that humans perceive the world in terms of Spelke objectsgroupings of physical things that reliably move together when acted on by physical forces. Spelke objects thus operate on category-agnostic causal motion relationships which potentially better support tasks like manipulation and planning. In this paper, we first benchmark the Spelke object concept, introducing the SpelkeBench dataset that contains wide variety of well-defined Spelke segments in natural images. Next, to extract Spelke segments from images algorithmically, we build SpelkeNet, class of visual world models trained to predict distributions over future motions. SpelkeNet supports estimation of two key concepts for Spelke object discovery: (1) the motion affordance map, identifying regions likely to move under poke, and (2) the expected-displacement map, capturing how the rest of the scene will move. These concepts are used for statistical counterfactual probing, where diverse virtual pokes are applied on regions of high motion-affordance, and the resultant expected displacement maps are used to define Spelke segments as statistical aggregates of correlated motion statistics. We find that SpelkeNet outperforms supervised baselines like SegmentAnything (SAM) on SpelkeBench. Finally, we show that the Spelke concept is practically useful for downstream applications, yielding superior performance on the 3DEditBench benchmark for physical object manipulation when used in variety of off-the-shelf object manipulation models. Project page: https://neuroailab.github.io/spelke_net. Corresponding authors: rahulvenkk@gmail.com, dyamins@gmail.com."
        },
        {
            "title": "1 Introduction",
            "content": "As the work of developmental psychologist Elizabeth Spelke and others has shown, children in their early months of life already possess notions of objecthood based on motion, segmenting the visual world into bounded units that move and interact as cohesive wholes [1]. These early capabilities enable infants to track and predict how objects behave under physical forces, laying the cognitive groundwork for intuitive physical reasoning [2, 3, 4]. While the idea of such motion-based Spelke objects is cognitively natural, typical ontologies of segmentation in the computer vision literature diverge substantially from the Spelke concept. Standard segmentation datasets like COCO [5] and ADE20K [6] define segments based on semantic or instance-level labels such as car, tree, or amorphous categories like stuff [7]. Although these are useful for recognition tasks, the resulting masks often do not reflect how objects move or interact in the real world. This highlights core challenge in segmentation: conventional definitions may not reflect the causal structure required for physical reasoning. In contrast, Spelke segments provide category-agnostic alternative by grouping regions based on their responses to forces, resulting in more functional notion of segmentation, which can serve as useful foundation for vision systems in robotics tasks like manipulation and planning (see Figure 2). In this work, we introduce self-supervised procedure for extracting Spelke segments from real-world *Equal contribution. Author order randomly decided. 1 SpelkeNet Stanford NeuroAI Lab Figure 1: Overview of SpelkeNets capabilities. Our model first predicts probability of motion map, indicating regions likely to undergo movement independent of camera motioni.e. candidate movable objects. We sample point from this map and apply virtual poke. Conditioned on this intervention, our model completes the flow field. From this, we extract grouping of pixels, or segment corresponding to an entity that would move as cohesive whole under the application of external forces (i.e. Spelke object). On the right, we illustrate how these discovered segments can be used in physical object editing pipeline to precisely define the object we desire to manipulateensuring that edits are applied to groups of pixels that would move together in the real world as opposed to segments defined based on appearance or semantics. We show in this paper that the Spelke segments we discover enable more physically plausible object manipulation. static images, and show how these Spelke segments can be effectively used as the basis for physically grounded object manipulation tasks. We first benchmark the Spelke segment concept by introducing SpelkeBench: an evaluation dataset designed to assess whether segmentation algorithms can identify Spelke segments. This benchmark allows us to systematically measure how well models output align with the Spelke concept. While collecting such dataset on scale suitable for evaluation is feasible, collecting it on scale suitable for supervised training is prohibitively expensive. Capturing annotations that respect Spelkes core principlescohesion, continuity, solidity, and contactrequires nuanced human judgment and possibly some form of physical interaction, making it impractical to scale through conventional labeling pipelines. To sidestep this bottleneck, we turn to self-supervised visual world models. Specifically, we build on the recently proposed Local Random Access Sequence Modeling (LRAS) [9], probabilistic SpelkeNet Stanford NeuroAI Lab framework that models sequences of locally quantized tokens. Here, we introduce SpelkeNet as specific instance of LRAS trained to predict distribution of plausible future flow fields given an input image. By virtue of being trained on large-scale internet videos, SpelkeNet acquires an implicit understanding of what moves together in natural scenes without ever being given explicit segmentation labels. The autoregressive structure and locality properties of the LRAS framework are natural architectural foundation for SpelkeNet: Spelke segments can be discovered by applying localized virtual pokes simply by appending optical flow tokens to the input sequence and having the model complete the flow field to discover motion correlation patterns. In contrast, diffusion-based models [10] rely on Figure 2: Benchmarking Spelke Segmentation: comparing Spelke segments with conventional segmentation definitions. SAM [8] produces fine-grained segments but often includes regions that do not typically move independently when forces are appliedsuch as logos on bottles, shadows, or sub-parts of objects like camera lensesreflecting its focus on visual distinctiveness over physical structure. Entity segmentation [7] more closely approximates the Spelke notion of segmentation but still includes non-movable elements like walls, streets, and fixed shelves. Our SpelkeBench benchmark is constructed by manually filtering out such segments (as described in Section 3.1), retaining only those that correspond to physically grounded entities defined by correlated motion in response to applied forces. SpelkeNet Stanford NeuroAI Lab dense, global conditioning, making such targeted interventions hard. To discover Spelke segments, we introduce statistical counterfactual probing procedure on SpelkeNeta model analog of the physical act of poking multiple times at location in static image and observing, in the short (imagined) videos that would result, what else in the scene is likely to move in response. To execute this procedure, we must first determine which regions in the image are capable of moving when acted upon by external forces. For example, chair or car might respond to push, while static elements like the sky or ground would not. Once these candidate regions are identified, we simulate virtual poke at specific pixel and ask the model: what else in the scene would move as result? To realize this query, we extract two intermediate representations from SpelkeNet: (1) motion affordance map, indicating regions that are likely to move when external forces are applied, and (2) an expected displacement map, predicting how the rest of the scene would move in response to virtual poke. We sample poke locations from high-affordance regions (as shown in Figure 1) and analyze resulting displacement fields from diverse virtual pokes to estimate patterns of correlated motionyielding partitions of pixels (or segments) that describe co-moving entities. This statistical counterfactual probing framework not only enables object discovery but also holds promise for general-purpose robotics, where understanding movable parts of an environment and the distribution of likely responses to hypothetical applied forces is essential for planning and control. On our SpelkeBench benchmark, SpelkeNet outperforms both self-supervised approaches like DINO [11] and supervised methods like SAM [8]. We then explore how these discovered Spelke objects can be used in practice, focusing on the task of 3D physical object manipulation. In current practice, the typical pipeline for 3D object manipulation begins by identifying an object to manipulate using an off-the-shelf segmentation model, followed by applying transformation to the selected segment to produce an edited image. While various techniques exist for performing these edits, most assume predefined segment that corresponds to an object in the physical world. However, since standard segmentation models often reflect semantic categories or texture similarity (as in SAM), but not entities that move together, the edits can often appear unrealisticaffecting parts of multiple objects or fragmenting single object. In contrast, as shown on the right side of Figure 1, we find that using segments extracted using SpelkeNet leads to edits that are significantly more physically plausible and align better with human expectations. We evaluate SpelkeNet on the 3DEditBench benchmark [9], which assesses the realism and physical consistency of 3D object manipulations. Our approach outperforms supervised baselines, including SAM, demonstrating the importance of using segments defined based on what moves together in the world (i.e. Spelke segments) for tasks involving physical manipulation. In this way, our work addresses three key questions: (a) How can we benchmark segmentation models ability to capture the notion of pixel co-movement (i.e Spelke segments)? (b) How can such segments be defined using visual world model and discovered in self-supervised way? and (c) Are Spelke segments practically useful definition, i.e. can they be used effectively in downstream physical manipulation tasks?"
        },
        {
            "title": "2 Related Works",
            "content": "Evaluation datasets for segmentation: Segmentation in computer vision is typically framed as semantic [6, 5], instance [5, 12], or panoptic segmentation [13], and segments are labeled by annotators based on category. They will search through the image for objects matching given category description and label the segment accordingly. However, these definitions often conflict with the physical structure of scenes: masks may merge independently movable objects, split objects 4 SpelkeNet Stanford NeuroAI Lab into parts, or include amorphous stuff categories like sky or terrain [7]. Models trained and evaluated on such datasets, including recent efforts like SAM [8], may produce segments that diverge from the demands of downstream tasks like interaction and control [14]. In contrast, our work introduces new segmentation benchmark called SpelkeBench which is aligned with the notion of Spelke objects [1]a category-agnostic definition of segments as bounded regions that move as physical units, which allows for more physically grounded evaluation of segmentation models. Object segmentation models: Numerous approaches to object segmentation based on supervised learning have been proposed in prior work [15, 16, 17, 18, 19, 20]. While these models achieve strong performance, they depend on large amounts of labeled data, which can be expensive to obtain. To reduce reliance on annotations, recent unsupervised and self-supervised methods attempt to extract object masks from unlabeled data by clustering attention maps from pre-trained contrastive learning models [21, 22, 23, 24, 25, 26]. Methods like CutLER [27] and ProMerge [28] use these clusters as pseudo-masks for distillation. However, these methods often struggle in scenes with multiple objects of the same category, as contrastive learning tends to produce similar representations for such instances, making them harder to distinguish. Here, we introduce SpelkeNet, self-supervised world model that extracts segments based on what moves together in the physical world. We show that this approach is well-suited for downstream tasks such as object manipulation, compared to other ways of defining segments. Emergent visual structures in world models: Object-centric world models aim to decompose scenes into discrete entities by imposing inductive biases that encourage low-dimensional, disentangled object-centric representations to emerge. These models route information through fixed number of competing latent slots, often using soft attention bottlenecks and minimizing either future prediction objectives (models such as MONet [29], Slot Attention [30], SAVi [31]) or contrastive objectives like C-SWM [32]. These models do not scale to complex real-world datasets due to architectural constraintstoo many slots lead to degenerate solutions with minimal structure, while too few make the reconstruction task ill-posed. In contrast, SpelkeNet defines predictive world model that enables the extraction of segments grounded in motion causality without having to bake in architectural constraints. Counterfactual World Models (CWM) [33, 34] are class of world models that reveal object-level structure by prompting regression-based video predictor with targeted interventions. Specifically, CWM simulates object motion by copying an RGB patch from the input frame to different location in an otherwise fully masked target frame, and asking the model to reconstruct the target frame. The optical flow between the predicted and original frames reveals sets of pixels that tend to move together. However, this RGB-based intervention has important limitations. The copied pixel values often fail to reflect how that region would appear if the object truly moveddue to changes in lighting, occlusion, etc.resulting in degraded predictions. Moreover, as CWM models are deterministic, when multiple future motions are plausible (e.g. for articulated bodies like humans), they produce blurred reconstructions that might result in inaccurate segments. SpelkeNet resolves both these issues by a) specifying interventions via sparse flows that more meaningfully indicate intended motion and b) using autoregressive generative modeling to estimate the marginals of and sample over the distribution of plausible futures, avoiding averaging artifacts and enabling more accurate computation of correlated motion statistics for object discovery. Text-guided vision foundation models: Diffusion-based generative models [10] have shown impressive capabilities in text-guided image generation and editing, and have an implicit understanding of objects and causal relationships. However, reliance on iterative global denoising makes it hard to probe these models with localized physical interventions, which is required for answering questions 5 SpelkeNet Stanford NeuroAI Lab about causalitysuch as the effects of force application and object interactions. Similarly, while vision-language models (VLMs) such as CLIP [35] and BLIP [36] excel at grounding global semantics in images, text-based prompts have proven to be sub-optimal control surface for fine-grained spatial reasoning [37, 38]. SpelkeNet leverages the LRAS autoregressive modeling framework to provide robust control surface for spatially localized prompting, allowing sparse flow interventions to be specified simply by appending few flow tokens to the input sequence. Object manipulation: Object manipulation involves applying transformations to objects to generate novel scenes, and is core task in computer vision. Most modern methods rely on spatial segmentation masks to define the set of pixels to be edited [39, 40, 41, 42, 43]. For physically plausible edits, it is critical that these segments correspond to parts of the scene that move together in the real world. However, commonly used models like SAM [8] often produce masks that capture subparts of objects that often do not move independently of the rest of the scene, leading to implausible image edits. In contrast, SpelkeNet produces segments aligned with real-world physical motion, enabling more accurate and realistic object manipulations for wide range of physical object manipulation models."
        },
        {
            "title": "3 Methods",
            "content": "3.1 Benchmarking Spelke segments: The SpelkeBench Benchmark To benchmark Spelke object discovery, we introduce the SpelkeBench benchmarka curated set of 500 images with ground-truth Spelke segment annotations. These annotations follow the definition proposed by developmental psychologist Liz Spelke [1]Spelke segments are groups of pixels that move together as unit under variety of virtual pokes applied to the object. Existing benchmarks like COCO [5] and ADE20K [6] prioritize semantic or instance-level distinctions, often producing segments that merge independently movable objects, split them into parts, or include amorphous background regions. As illustrated in Figure 2, models like SAM frequently produce segments that diverge from Spelke criteria. Since such models are evaluated using existing benchmarks, it is hard to quantify their utility for tasks like robotic manipulation [14], which require an understanding of which parts of scene move together in the physical world. To address this gap, we introduce method to construct meaningful benchmark that tests whether models understand the pixel co-movement/Spelke object concept. We curate dataset of segmented objects from two complementary sources: the EntitySeg benchmark [7] and the OpenX-Embodiment robotics dataset [44]. These datasets differ in their collection paradigms: EntitySeg is designed for high-resolution internet imagery with dense segmentation annotations, whereas OpenX consists of real-world, egocentric robot interactions. This contrast allows us to evaluate segmentation models in both unconstrained image domains and physically grounded robotics environments. Since OpenX does not provide segment labels, we manually annotate Spelke-consistent segments for subset of 50 images. These annotations reflect the types of objects relevant for physical interaction and manipulation tasks that are central to robot learning. For EntitySeg, we extract high-quality subset of 500 images using three-stage filtering pipeline to filter out the annotated segments in the dataset which do not align with Spelkes principles: Stage 1: Removal of amorphous background regions. We exclude all regions labeled as stuffsuch as sky, ground, or terrainbased on the standard stuff-vs-things taxonomy [7]. These regions lack the individuated, cohesive properties associated with Spelke objects and SpelkeNet Stanford NeuroAI Lab are typically not physically manipulable entities. Stage 2: Filtering non-movable object categories. Despite being labeled as things, certain objects like kitchen sinks, traffic signs, or large fixtures are functionally immovable in real-world settings. We identify and remove such regions through manual inspection. Stage 3: Final curation of diverse, high-quality scenes. From the filtered pool, we select 500 images that contain only Spelke-consistent regions. We also ensure that this set is diverse in terms of object types, spatial arrangements, and scene complexity. 3.2 Discovering Spelke segments Local Random Access Sequence Modeling (LRAS) [9], is sequence modeling framework inspired by large language models (LLMs) that causally predicts locally quantized image (i.e. RGB) and optical flow patches. In this section, we describe the LRAS architecture and provide details about how some of its properties make it strong candidate for our goal of Spelke object discovery. The LRAS framework operates on unified vocabulary comprising RGB and flow content tokens and set of pointer tokens for each modality that specifies one of spatial locations in the image gridresulting in vocabulary that can be partitioned into four disjoint sets of integers: (rgb): RGB pointer tokens [0, l) : RGB content tokens [l, + ) (flow): Flow pointer tokens [l + , 2l + ) F: Flow content tokens [2l + , 2l + + F) When constructing sequences, each content token (i.e. RGB or flow) is paired with corresponding pointer token that specifies its spatial location. This (pointer, content) pairing allows sequences to be arranged in arbitrary spatial order. Additionally, since the pointer tokens are modality-specific, they serve as way of asking the model to decode desired modality. For example, pointer token from (rgb) prompts the model to decode an RGB token at given location, while one from (flow) can query for flow token at that same location. Token sequences are denoted as1 = [(i(rgb) 1 , x1), . . . , (i(rgb) , xN )], = [(i(flow) , f1), . . . , (i(flow) , fM )] xt , ft F, i(rgb) (rgb), i(flow) (flow) special camera pose token representing the relative camera motion between frames can optionally be included to form the final sequence, = [c] . Here, denotes concatenation. The model is trained like an LLMit learns to predict the next token, conditioned on all preceding tokens. More concretely, the model outputs categorical distribution over the unified vocabulary V, and is trained to minimize cross-entropy loss between this distribution and the target next token. Since the tokens appear in random spatial order, there is no need to learn the ordering, so the prediction of the pointer tokens is not supervised [9]. During inference, the sequence model can accept sequence composed of any subset of the combined sequence z. In prior work [9], the LRAS framework was used to generate complete flow field by 1In practice, for efficiency purposes, we reduce the number of pointer tokens by grouping each pointer with patch of content tokenseach pointer token is followed by four content tokens as illustrated in Figure 3. 7 SpelkeNet Stanford NeuroAI Lab Figure 3: SpelkeNet Architecture. The left panel illustrates SpelkeNetan instance of the LRAS [9] framework applied to the task of optical flow completion for Spelke object discovery. The input consists of tokenized RGB image ({xk, I}) and sparse virtual poke indicated by flow token, . Each token is paired with pointer token indicating its spatial location, forming 1D sequence of (pointer, content) pairs. The model accepts this sequence and predicts categorical distribution D[ij] over the flow token vocabulary for every spatial location ij in the image. The right panel shows that autoregressively sampling from these distributions yields complete flow field in pixel spaceat each step we randomly select an undecoded location ij, and sample flow token ˆf D[ij] from the distribution predicted by the model. We then append the pair (ij, ˆf ) to the input sequence, which is fed back into the model to generate new distribution D, and the process repeats. In this way, the input sequence grows over time, progressively completing the flow field, representing how the scene responds to the virtual poke. We discover Spelke segments by analyzing the motion correlation patterns of these resulting flow fields. conditioning only on RGB tokens and the camera pose (i.e. = [c]), achieving state-of-the-art performance on tasks like monocular depth estimation and novel view synthesis [9]. SpelkeNet: an instance of LRAS for Spelke object discovery. Here, our goal is to discover pixel co-movement in natural images, i.e. what moves together when external forces are applied. We propose to discover such causal relationships by injecting localized virtual pokes and using world model to infer what else in the scene moves. Among existing generative world modeling techniques, the LRAS paradigm is particularly well-suited for this task. Unlike diffusion models [10], which require dense, global conditioning, the autoregressive structure of LRAS supports composable input sequences. Our model, SpelkeNet is specific instance of LRAS that leverages its flexible sequence design properties to apply sparse, localized interventions simply by appending to the input sequence flow token fk, representing the motion to be applied and pointer token ik, indicating the spatial location of the pokeand discovers Spelke objects by completing the flow field which indicates how the rest of the scene will respond to the poke. Disentangling object motion from camera motion. In natural videos, pixel motion can arise either due to external forces acting on objects or due to camera movement. However, for discovering Spelke objects, we are specifically interested in motion caused by external perturbations to objects, not from camera-induced motion. If we provide the model with sparse flow input (a virtual poke) without additional context, it has no way of knowing whether the input motion arose from forces 8 SpelkeNet Stanford NeuroAI Lab acting on objects or camera movement. Consequently, the model might complete the flow field in ways that conflate both sources of motion, making it difficult to isolate responses that are effects of the applied poke. To ensure that the predicted motion is attributed solely to the virtual poke, we must explicitly condition the model on static camera. With the LRAS paradigm, this form of controlled probing is simple to implement: we simply append zero camera pose token to the input sequence, guiding the model to interpret any input motion as arising exclusively from external forces and not from camera displacement. We formalize this input as: zf = [c = 0] [(ik, fk)], Decoding strategies. Given the input sequence, SpelkeNet predicts for every spatial location, ik (flow), categorical distribution D[ik] over V.2 In practice, D[ik] is obtained by querying the model by appending pointer token to the end of the sequence: Ψ(zf [ik]) 7 D[ik] SpelkeNet can be thought of as composite function Ψ that returns set of flow distributions, one for each spatial location: Ψ(zf ) = D[ik] ik (flow)o To infer what else in the scene will move as result of the poke fk, we can use Ψ to complete the flow field, either in parallel or autoregressively. In parallel decoding, all spatial locations are sampled independently, ˆf (par) ]kI. We denote this method as Ψpar flow: D[ik], resulting in the spatially completed flow field, ˆf (par) = [ ˆf (par) ˆf (par) = Ψpar flow(zf ). In contrast, sequential decoding starts from an initial sequence z0 = zf . We then iteratively select an undecoded, random location ik, query the current model distribution D0 = Ψ(z0), sample token: ˆf (seq) D0[ik], and append it to the sequence: zq+1 zq {(ik, ˆf (seq) )} This process continues until the entire flow field is decoded, with each prediction step refining the models estimate of distribution: Dq+1 = Ψ(zq+1). This sequential decoding process results in the dense flow field, ˆf (seq) = [ ˆf (seq) ]kI. We denote this method as Ψseq flow: ˆf (seq) = Ψseq flow(zf ; seed = t) Sequential decoding is especially valuable when modeling objects with many degrees of freedom, such as articulated bodies like humans or mechanical tools, and deformable materials like cloth or paper, where different parts are causally linked and must move in coordinated way. For example, in human motion, movement of one partsay, the lower handimposes constraints on how other parts, like the upper arm or torso, can respond. Decoding tokens sequentially allows the model to respect these causal dependencies, as each token is generated in the context of previously decoded 2Although the model predicts distribution over the entire unified vocabulary, not just the flow token subset F, sampling from this distribution yields flow token D[ik] because the model is trained to produce flow token whenever it encounters preceding flow pointer token. 9 SpelkeNet Stanford NeuroAI Lab ones (such as the motion of the lower hand in this example), resulting in globally consistent motion. In contrast, parallel decoding offers faster inference but can yield locally plausible yet globally inconsistent flow fieldse.g. the upper arm moving independently of the lower handleading to physically implausible outcomes. Defining Spelke objects using SpelkeNet. Our approach builds on the idea of counterfactual probing, introduced in CWM [34], where Spelke objects are discovered by simulating localized virtual pokes through local patch motion interventions and analyzing the outcome of the intervention. However, since CWM is regression-based, it produces single deterministic outputan important limitation, because in the physical world, responses to pokes are often multimodal. Consider simple example of moving persons hand. In the physical world, one of two things can plausibly happen: either the hand moves independently while the rest of the body remains fixed, or the entire person translates, causing the hand to move along with it. Both are physically valid outcomes. But as CWM is deterministic, it is forced to average over these distinct possibilities, leading to blurry or ambiguous motion completions that fail to reveal which parts of the scene tend to move together. To address this limitation, we propose more expressive definition of Spelke objects using generative world model like SpelkeNet, which generates multiple plausible future motions of scene. We operationalize Spelke objects as groups of pixels that consistently move together across multiple plausible outcomes of world model, under different virtual pokes. This requires modeling the distribution of possible responses to external forces. We implement this using statistical counterfactual probing on SpelkeNet, stochastic extension of the original CWM counterfactual procedure. Instead of generating single prediction like CWM, we use SpelkeNet to produce diverse set of imagined flow completions for various virtual pokes at candidate spatial location. Diversity arises from two sources of randomness: 1. Sampling flow tokens from the learned distribution D[ik]: For fixed index ik, we draw multiple flows fk D[ik] to explore the local responses the model deems plausible. For example, in the human motion scenario discussed above, different samples can make the same body part move in distinct yet physically feasible ways. 2. Varying the decoding order of spatial indices ik: Because SpelkeNet is sequence model, tokens decoded earlier condition those decoded later. Shuffling the order therefore changes how motion propagates through the objecte.g. decoding the torso before the leg yields different global outcome than decoding the leg first. Computing what is effectively marginal (i.e. probability-weighted integral) over these diverse generations, allows us to obtain robust definition of Spelke objects using SpelkeNet. We now describe few useful structure extractions from SpelkeNet that support statistical counterfactual probes. Motion affordance maps. To discover Spelke objects, we must first identify the candidate locations where virtual pokes can be appliedwhich pixels in the scene lie on regions that are likely to move under external forces (i.e. movable entities). We refer to this notion as the probability of motion affordance map, denoted pmotion. Such motion-centric affordance maps are especially useful in robotics applications where we need to identify high motion affordance regions that are likely to move under interaction (e.g. cup or plate). Regions that typically do not move upon external forces (e.g. sky, walls, and flooring) would have low motion affordance. To compute pmotion, we define set of flow tokens that correspond to motion greater than some threshold τ , and then sum their estimated probabilities. As the flow tokens, fj, are by themselves 10 SpelkeNet Stanford NeuroAI Lab Figure 4: Spelke object discovery using statistical counterfactual probing. To discover movable objects, multiple virtual pokes are applied at location sampled on the pmotion map that indicates which regions are likely to move under the application of external forces. The average dot product of the poke vector with the expected displacement maps isolates the desired segment. not interpretable, we map each flow token to 2D flow vector, vj, through an epigraphy on the flow vocabulary3 and define the token set corresponding to motion as: Fmotion = {fj vj2 > τ } , where τ is threshold Next, given sequence of RGB tokens x, as we are only interested in finding regions that are likely to move under external forces, we concatenate the sequence with token indicating zero camera motion to discount it (i.e. = [c = 0]) and obtain the predicted flow token distributions D[ik] = Ψ(z)[ik], ik I. Using these distributions, the probability of motion at each spatial location ik, is computed by summing over the token set Fmotion: pmotion[ik] = D[ik, j] fj Fmotion In this way, pmotion : [0, 1] is 2D heatmap of the regions likely to move under external forces. Figures 4 and 1 illustrate some examples of these maps. Expected Displacement Maps. Having identified regions of high motion-affordance, we can sample candidate locations and condition the model on virtual pokes in those regions. To discover Spelke objects, we introduce useful quantity called the expected displacement map, which is the estimate of the likely flow at each location conditioned on the poke. In robotics settings, this map can provide valuable guidance about how objects might move if interacted with, even before the robot makes contact with objects in the scene. 3Flow token epigraphy: SpelkeNet uses learnt local patch quantization to produce flow tokens, but relies on global decoder to generate coherent, high-quality flow fields. As result, tokens cannot be interpreted by decoding them in isolationtheir meaning emerges only in the context of the full sequence. However, since the tokenizer is local, we can find which continuous flow vectors map to it by performing kind of token space epigraphyby assigning meaning to discrete flow tokens through statistical aggregation of typical input flow fields that produced them: fj 7 vj = 1 Sj uSj u, where Sj = (cid:8)u R2 tokenizer(u) = fj (cid:9) . 11 SpelkeNet Stanford NeuroAI Lab We use our flow model to apply virtual poke represented with the flow token, fk, at location ik I, construct an input sequence, = [c = 0] [(ik, fk)], and obtain the predicted distribution, D[ik] = Ψ(z)[ik], I. We then compute the expected displacement as the probability-weighted average of flow vectors vj, where each vj maps to token fj as defined by flow token epigraphy3: disp[ik] = Epar D[ik, j] vj, ik The result is dense 2D vector field over spatial locations, decoded in parallel: Epar denote this method using the function Ψpar disp: disp : R2. We disp = Ψpar Epar disp(z) To obtain more faithful estimate of Epar disp, we can also average predictions over multiple stochastic generations (i.e rollouts) of the model. Denoting ˆft flow(z, seed = t) as the set of sequentially decoded flows in the tth rollout, where each rollout is one sequential completion of the flow field conditioned on z, the expected displacement map, computed in sequential mode can be written as: = Ψseq (seq) Eseq disp[ik] = 1 t=1 (seq) ˆft [ik] vj. We denote this method using the function Ψseq disp: disp = Ψseq Eseq Some examples of these maps are shown in Figure 4. To simplify the notation going forward, unless the superscript (seq/par) is specified, well assume either sequential or parallel modes can be used. disp(z) Statistical counterfactual probing for Spelke object discovery. Using these structure extractions, we first sample location that is likely to move: such that pmotion(k) > τp. Then, to discover Spelke objects we will identify regions that consistently move together under various virtual pokes applied at k. We use our flow model to apply diverse virtual pokes {f (r)}R r=1, at k. For each direction (r), we compute the expected displacement field, given the input sequence: E(r) disp = Ψdisp(x = 0 [ik, (r)]). To discover co-moving entities (i.e. Spelke objects), we computed the expected motion correlation, dot[u] at each location I, by averaging across various pokes, the dot product between the poke vector (r) and the expected displacement map E(r) disp[u]: dot[u] = 1 r= (r), E(r) disp[u] . Finally, Otsu thresholding [45] of dot yields our desired Spelke segment. Refer to Figure 4 for more detailed illustration of this procedure. In practice, we find that using the sequential model to aggregate over multiple stochastic generations of the model is more effective. This is especially the case for objects with many degrees of freedom, like humans and deformable objects (see above), 12 SpelkeNet Stanford NeuroAI Lab Figure 5: Automatic discovery of Spelke segments. We extract probability of motion maps from an image, and use it to sample candidate poke points (top left). We apply an optical flow vector poke to the image at the sampled points and obtain dense flow fields conditioned on the poke (top right) which are used to compute affinity maps. As shown in the bottom panel, these maps enable the extraction of segments using iterative clustering (see Section 3.2). although more expensive to compute. However, we also find that reasonable results can be achieved in parallel mode as well. Automatically discovering every Spelke object in scene. So far, we have shown how Spelke segments can be extracted from point prompts. However, in many real-world settings, especially in robotics, it is advantageous to automatically discover every independently movable segment/Spelke object in scene without requiring manual point-prompting. For example, household robot tasked with clearing dining table must infer that plate and its contents will move as unit, while napkin resting on the plate is an independent entity, so it can plan appropriate grasps and avoid unintended collisions. We now describe method to extract the full set of Spelke segments in scene automatically. Our approach consists of two steps. First, we compute dense pixel-to-pixel affinity matrix that captures the likelihood that pair of pixels will move together under virtual force. In essence, this process recovers the pairwise causal structure of the scene, revealing which regions are causally entangled in motion space. An iterative clustering algorithm is then applied to this matrix to isolate complete set of independently movable entities. Computing the affinity matrix. We begin by sampling locations from the motion affordance map. These points are where we poke to collect flows. = { k1, k2, . . . , kN } I, pmotion[ki] > τp. We then build motion descriptor for each pixel using the following procedure: For each = 1, . . . , , choose poke-directions {f (r) }R r=1. For each (n, r) and each of = 1, . . . , 13 SpelkeNet Stanford NeuroAI Lab random seeds, compute the flow completion given the input image tokens x, (n,r) ˆft = Ψseq flow(x, = 0, [ikn, (r) ], seed = t) Then for each the motion descriptor, φ[u] = (cid:2) ˆf1 (1,1) , . . . , ˆft (n,r) (u)(cid:3) R2 . Finally, the affinity matrix can be described as the pairwise dot product of motion descriptors: A[u, v] = φ[u] φ[v], u, I. For simplicity, we denote A[u] to be the affinity of the pixel with the rest of the image. Clustering the affinity matrix to extract segments. Given the precomputed affinity matrix A, we extract segments in an iterative selectthresholdrefine loop. At each step, we choose the most confident probe center ki, defined as the one whose affinity-row A[ki] has the highest mean over all pixelsindicative of strong binding to the other pixels that make up the object. We apply Otsus method to threshold this row, yielding an initial mask (0). We then gather all remaining poke points kj that lie within (0) and average their affinity-rows to form: Aavg = 1 {j : kj (0)} A[kj] kj (0) We threshold Aavg via Otsus method to obtain refined mask (t), for = 0. All centers contained in (t) are then removed from consideration, and the loop repeats on the remaining set of poke points. Once no poke points remain, the algorithm returns the complete set of extracted segments {M (1), . . . , (T )}. Figure 5 illustrates this procedure using an example. 3.3 Using Spelke segments for physically plausible object manipulation Now that we have described how Spelke segments can be discovered given point samples, we discuss how they can be used in practical applications that require an understanding of pixel co-movement. We consider the standard task of object manipulation shown in Figure 6 : an input image is given along with user-defined edit prompt specifying the desired 2D/3D transformation, and an object mask tells the editing model which parts of the scene to apply the transformation on. Successful manipulation relies on having physically meaningful object segment as downstream object transformations can suffer if segment corresponds to region that is not independently movable. Spelke segments are grounded in physical principles: they group pixels based on correlated motion under virtual forces. This makes them more suitable primitive for physically plausible editinghere, we demonstrate that the choice of segmentation method significantly affects the realism of the edit. In this way, we show that Spelke segments are not just theoretical concept, but have practical utility in downstream tasks. The LRAS framework enables both image editing and segmentation via flexible sequence In this paper, we leverage the LRAS framework to build SpelkeNeta flow completion design. model for discovering Spelke segments. To recap, SpelkeNet is trained to complete flow fields conditioned on an input sequence comprising RGB tokens and sparse virtual poke . The input sequence is denoted as: = , 14 SpelkeNet Stanford NeuroAI Lab Figure 6: Standard pipeline for object editing using segmentation masks: We substitute SAM segments for Spelke objects predicted by SpelkeNet and find that they yield more intuitive and physically plausible edits. The model predicts complete flow field representing how the rest of the scene will move as result of the poke. By analyzing the resulting flow field, we discover Spelke segments. The same underlying LRAS framework can also be used to define an image editing model in pixel space. Prior work [9] demonstrates this by building LRAS-3Dan instance of the LRAS framework that is conditioned on input RGB tokens and dense flow tokens dense that specify desired object transformation to be applied: = dense. Given this sequence, the model predicts distribution over RGB tokens: Drgb[ik] = Ψedit(z)[ik], ik I. To construct an dense that represents transformation targeting particular object in the scene, segmentation mask must be specified. Prior work relies on off-the-shelf methods such as SAM [8] to define these masks, which often represent regions which do not move as unified whole, resulting in implausible edits. Here, we show that the LRAS framework itself can be used to define segmentation model, removing the reliance on external segmentation methods. By simple modification of the sequence structuree.g., prompting with sparse pokes and analyzing the flow responsewe obtain segments better aligned with the demands of physical reasoning and manipulation. In this way, we demonstrate in this paper that the LRAS framework unifies both segmentation and image editing within single, token-based autoregressive modeling paradigm."
        },
        {
            "title": "4 Results",
            "content": "4.1 Point-prompted segmentation Task & Dataset. To evaluate our models ability to extract Spelke objects, we formalize the task as point-promoted segmentation: given point on an object, the goal is to recover the region that would move together if virtual force were applied at that point. To evaluate Spelke object discovery, we use SpelkeBench: our 500-image benchmark described in Section 3.1. Baselines. We compare against several strong baselines. For supervised segmentation, we use SAM2 (heira-large) [8], state-of-the-art point prompt-based method. For self-supervised baselines, 15 SpelkeNet Stanford NeuroAI Lab we evaluate DINOv1 [21] and DINOv2 [11], which reveal semantic object structure via attention mapssegments are obtained by thresholding attention maps at prompted location. We also compare to Counterfactual World Models (CWM) [33, 34], which segment objects by generating local patch motion interventions and thresholding the estimated optical flow (using RAFT [46]) between the outcome of the intervention and the original image. Evaluation details & metrics. For each ground truth segment, we generate point prompt using the centroid or, if outside the mask, the point farthest from the boundary. We run 8 poke directions and 3 autoregressive flow completions per prompt with the procedure described in Section 3.2, and we use the same setup for CWM. We use Average Recall (AR) and mean intersection-over-union Figure 7: Qualitative results for point-promoted segmentation across models. SpelkeNet yields sharper segments, better aligned with Spelkes definition of grouping pixels based on comovement, compared to SAM2, DINO, and CWM. More results are provided in the attached supplementry. 16 SpelkeNet Stanford NeuroAI Lab Table 1: Quantitative evaluation of point-prompted segmentation accuracy across models. We report Average Recall (AR) and mean Intersection over Union (mIoU) for various segmentation methods. SpelkeNet outperforms both self-supervised baselines (DINO, CWM) and the supervised SAM2 model. SAM2 DINOv1-B/8 DINOv2-L/14 DINOv2-G/14 CWM SpelkeNet AR 0.4816 mIoU 0.6225 0.2708 0.4990 0.2524 0.4931 0.2254 0. 0.3271 0.4807 0.5411 0.6811 (i.e. mIoU) to measure performance. AR is defined as the fraction of GT segments that the model detects. Here, GT segment is classified as detected if the predicted segment obtains an IoU less than some threshold τ . In practice, we compute the average AR across multiple IoU thresholds (0.5 - 0.99). Intuitively, we can think of AR as measuring how likely it is that the GT segments are detected by the model and the mIoU metric as measuring how precisely each segment boundary is predicted. Qualitative and Quantitative Comparisons. SpelkeNet outperforms all baselines across both Average Recall (AR) and mean IoU (mIoU), surpassing self-supervised methods like DINO and CWM, as well as the supervised baseline SAM, as shown in Table 1. Qualitatively, as depicted in Figure 7, although SAM performs well in many cases, it often segments non-movable regions such as textures, printed designs, or object subparts such as human skin, relying on appearance rather than physical coherence. This suggests that semantic and texture-driven segmentation can be misaligned with the goal of identifying physically grounded, movable objects. Meanwhile, contrastive learning methods like DINO exhibit different failure mode, merging Figure 8: CWM Segmentation failure modes in complex scenes. Each row shows challenging example where CWM struggles. The first column shows the input image with the patch motion prompt (red arrow). The second column displays the counterfactual prediction generated by CWM. The third column shows the RAFT-predicted flow field between the input and counterfactual image. The final column presents the resulting segment obtained by thresholding the flow magnitude. Compared to SpelkeNet, CWM often produces diffuse motion fields due to blurry RGB reconstruction and inaccurate object boundaries. 17 SpelkeNet Stanford NeuroAI Lab same-category instances, as the contrastive learning objective brings representations of instances of the same object closer. These observations highlight fundamental limitation of such models for the task of discovering Spelke segments. CWM, while stronger than other self-supervised methods, often merges nearby objects. This happens because the model often generates blurry reconstructions, as its RGB pixel regression objective during training does not account for uncertainty. As result, the flow estimation may produce diffuse or extended motion fields, causing nearby objects to be grouped together, as illustrated in Figure 7 and 8. In contrast, SpelkeNet yields sharp, high-quality segments closely aligned with the Spelke definition. This can be largely attributed to the ability to prompt the model with local cues and to the probabilistic flow completion architecture that explicitly accounts for uncertainty in visual scenes. 4.2 Automatic discovery of Spelke segments Task. So far, we have quantified how well our method discovers Spelke segments given point prompts. However, as we discussed in Section 3.2, it is often desirable to automatically discover every Spelke segment in the scene. We show illustrative examples of discovered segments using the auto-discovery method proposed in Section 3.2 in Figure 9 and evaluate performance quantitatively in Table 2, on our SpelkeBench benchmark. Evaluation Metrics. To evaluate segmentation quality, we compute Average Precision (AP), Average Recall (AR), F1-Score, and mIoU. Unlike point-prompted segmentation, where point on ground-truth object is provided and the model predicts segment associated with that specific point, auto-segmentation outputs set of segments without indicating which ground-truth objects they represent. To determine which predicted segment should be matched to which ground-truth segment, we compute the pairwise IoU matrix and apply the Hungarian method [47] to find the best one-to-one matching, which is necessary first step before we can compute our metrics. We then compute how many segments the model successfully detects by counting the number of Figure 9: Illustration of unprompted Spelke segment discovery using SpelkeNet. The corresponding discovered segments are highlighted, demonstrating the ability of SpelkeNet to automatically identify every physically coherent, movable entity in the scene without manual prompts. 18 SpelkeNet Stanford NeuroAI Lab Table 2: Quantitative evaluation of unprompted automatic segmentation across models on SpelkeBench. We find that SpelkeNet obtains competitive performance compared to existing self-supervised methods. SAM2 [8] CutLER [27] ProMerge [28] SpelkeNet AP AR mIoU F1-score 0.11 0.62 0.68 0.17 0.41 0.32 0.42 0. 0.42 0.34 0.43 0.36 0.35 0.46 0.57 0.38 predicted segments that are matched to ground-truth segments (i.e those that have an IoU greater than some threshold τ ). Given these detected segments, Average Precision (AP) measures the fraction of predicted segments that end up being matched and detected (i.e those that are in-fact Spelke objects). Average Recall (AR), by contrast, measures the fraction of ground-truth segments that are successfully detected by the model4. Intuitively, model that predicts only few high quality segments may achieve high precision but low recall as it may miss many segments, while model that over-segments may boost recall at the cost of precision. The F1-Score balances these two metrics by computing their harmonic mean, providing an aggregate measure of segmentation performance. Finally, to assess how accurately the model predicts the boundaries of objects at the pixel-level, we use mIoU. For each GT segment G, where is the set of all GT segments, we use the IoU with its matched prediction (if any), or assign 0 if unmatched: mIoU = 1 X gG IoU(g, matched(g)) Results. Overall, we find that SpelkeNet outperforms other self-supervised methods such as CutLER [27] and ProMerge [28] on most evaluation metrics. ProMerge slightly exceeds SpelkeNet in AP due to its tendency to predict fewer segments than those in the GTsome of which align well with ground truth and thus boost precisionat the cost of lower recall, as some objects are missed. For fairness, we report numbers only from the segment extraction stage for both CutLER and ProMerge, and not from their final distilled models. Compared to supervised methods like SAM2 [8], SpelkeNet achieves higher F1 score, although its AR and mIoU are lower. This is largely expected: SAM often over-segments scenes based on texture and semantic cues, leading to multiple masks for single Spelke object. While this increases the likelihood that at least one segment aligns with ground truth (improving recall), it reduces interpretability for physical reasoning, as these segments do not always capture true pixel co-movement. This makes them less useful for downstream robotic applications which could benefit from such structure extractions. Interestingly, while our method outperforms SAM on point-prompted segmentation metrics, this advantage does not fully translate to the automatic setting. This suggests that when given the right point prompts, SpelkeNet is capable of producing high-quality segments. However, in their absence, performance may degrade due to occasional poor flow rollouts from sub-optimal point samples or to limitations in the current clustering strategy. This points towards promising future direction: distilling our automatically discovered segments into segmentation architecture like SAM could 4Both AP and AR are averaged over multiple IoU thresholds in the range τ = (0.5, 0.99) 19 SpelkeNet Stanford NeuroAI Lab Figure 10: Qualitative comparisons of scene edits using SAM masks versus SpelkeNet segments. Each row shows the original image, the user click location, and the resulting edited image using segments from different methods. Note that while the results shown here are computed using the state-of-the-art LRAS-3D model for object editing, our segments are agnostic of the specific editing method used and can improve the results of any editing model as shown in Table 3. Additional results are reported in the attached supplementary material. potentially factor out noise and improve performance, in the same spirit as distillation pipelines used in prior work [27, 24, 28]. 4.3 Using Spelke segments for object manipulation Task. We consider the task of object-centric scene editing, where user clicks point on an object and provides an edit prompt specifying 2D or 3D transformation. The object mask is generated from this point selection using segmentation model. Here, we will present evidence that realistic edits require masks that reflect physically movable entities such as Spelke segments. Dataset. To evaluate the utility of SpelkeNet segments for object manipulation, we use 3DEditBench, recently introduced in [9]. The benchmark contains 100 real world images with associated point prompts, 3D transformation and resulting ground truth edited images with the transformation applied. It comprises of diverse range of object types undergoing physical changes such as rotations, translations, and inter object occlusions. Baselines. We evaluate our segments within several widely used image editing pipelines, including Lightning Drag [40], DiffusionHandles [41], and the recently introduced Diffusion-as-Shader model [43], which demonstrated impressive performance on object manipulation tasks. We also evaluate the LRAS-3D model [9], state-of-the-art model for image editing, built on the LRAS framework. For each method, we compare edits using SAM masks versus our SpelkeNet segments to isolate the effect of segmentation quality on edit realism and physical plausibility. Metrics. While standard metrics like PSNR, SSIM, and LPIPS capture image quality, prior work [41] has shown they often fail to reflect edit accuracy. To address this, they introduced the Edit Adherence (EA) metric, which measures how well the transformed object aligns with ground truth by computing the IoU between ground truth and predicted segments in the edited image. We report both this metric as well as standard image quality metrics. 20 SpelkeNet Stanford NeuroAI Lab Table 3: Quantitative evaluation of edit quality across segmentation methods and editing pipelines. We report results for edits generated using SAM versus SpelkeNet segments across four editing models. Lower is better, higher is better. Method Segment MSE PSNR LPIPS SSIM EA LRAS-3D [9] LightningDrag [40] DiffusionHandles [41] DiffusionAsShader [43]"
        },
        {
            "title": "SpelkeNet\nSAM",
            "content": "0.009 0.013 0.017 0.020 0.024 0.031 0.015 0.019 21.64 20.17 19.16 18. 17.42 16.15 19.29 18.20 0.213 0.255 0.195 0.241 0.364 0.419 0.194 0. 0.698 0.685 0.672 0.658 0.555 0.526 0.707 0.682 0.776 0.633 0.679 0. 0.576 0.495 0.640 0.503 Qualitative and Quantitative Comparisons. We find that SpelkeNet segments consistently outperforms SAM, yielding physically grounded segments that improve realism across diverse image editing models (see Table 3). In contrast, SAM-generated masks capture only sub-parts of objects, resulting in fragmented or implausible edits (see Figure 10)."
        },
        {
            "title": "5 Conclusion & Future Work",
            "content": "In this paper, we show how class of self-supervised visual world modelstrained to predict plausible motion from input RGB imagescan be used to discover motion-defined Spelke object entities from static images via zero-shot statistical counterfactual probing. To evaluate this approach, we introduce new benchmark, SpelkeBench, which measures this capability, and find that our model, SpelkeNet achieves superior results in comparison to both supervised and self-supervised segmentation methods on this benchmark. While Spelke segments have largely been explored in cognitive science, we show in this paper that they align well with the kinds of abstractions needed for physically grounded computer vision and robotics tasks, such as selecting and manipulating coherent parts of scene. When applied to the Figure 11: Support relationship understanding capabilities. When applying virtual poke to an object, the extracted Spelke segment includes both the directly contacted object and all the objects it physically supports, which implies an implicit understanding of the support hierarchy within scene. 21 SpelkeNet Stanford NeuroAI Lab Figure 12: Emergent material property understanding capabilities. We find that motion probability maps are uniform in case of rigid objects, but more localized near the virtual poke for deformable objects. This can potentially enable the discovery of material properties. 3DEditBench object manipulation benchmark, Spelke segments enabled more physically plausible editing as they reflect what truly moves together in the scene. On the other hand, we found that models like SAM often split up or combine objects in ways that are inconsistent with how they move, resulting in segments that may be less useful when the goal is to physically manipulate objects. Looking ahead, we observe that causal probe-based structure extractions from SpelkeNet may also offer pathway to inferring other properties of the scene beyond segmentation. We find that segments from SpelkeNet can reveal support relationships between objects. As shown in Figure 11, when virtually poking an object at the bottom of stacked structure, the extracted segment includes every entity that the object physically supports. Additionally, as illustrated in Figure 12, the pmotion maps produced can be used to infer physical attributes such as rigidity or material type. For instance, rigid objects like laptops and cardboard boxes tend to exhibit uniform probability across the segment, while deformable objects such as cloth and plastic covers often show more localized motion responses near the poke point. Exploring this connection between motion response patterns and physical properties is promising direction for future work. Though our focus in this paper has been on human-centric macroscopic physical scenes, the underlying philosophy of using predictive models to uncover causal and structural patterns through probing could open new avenues for data-driven discovery in other domains where humans have less direct intuition about the nature of objecthood. For example, in medical imaging, model trained on time-lapse microscopy might help identify cohesive intra-cellular structures or track morphological changes, while in astrophysics, models trained on galaxy evolution data could be probed to discover gravitationally bound systems. 22 SpelkeNet Stanford NeuroAI Lab"
        },
        {
            "title": "Acknowledgements",
            "content": "This work was supported by the following awards: Simons Foundation grant SFI-AN-NC-GBCulmination-00002986-05, National Science Foundation CAREER grant 1844724, National Science Foundation Grant NCS-FR 2123963, Office of Naval Research grant N00014-20-1-2589, ONR MURI N00014-21-1-2801, ONR MURI N00014-24-1-2748, and ONR MURI N00014-22-1-2740. We also thank Stanford HAI, Stanford Data Science, the Marlowe team, and the Google TPU Research Cloud team for providing computing support. Supplementary Materials In this supplementary material, we provide additional qualitative results and further details about our architecture. This document is organized as follows: Section 6: Additional Qualitative results. More illustrations showing probability of motion maps (i.e pmotion) and expected direction of motion maps (i.e Edisp) on real-world images and how they are used for Spelke segment discovery (Section 6.1). Additional point-prompted segmentation results (Section 6.2). More examples of downstream object manipulation using our Spelke segments (Section 6.3). Section 7: Further Architectural Details"
        },
        {
            "title": "6 Additional Qualitative Results",
            "content": "6.1 Additional Examples illustrating our Spelke Segment Discovery Algorithm In Section 3.2 and Figure 4 of the main paper, we outlined our algorithm for discovering Spelke segments by simulating virtual pokes and aggregating directional flow responses. Here, in Figure 13 we provide additional examples visualizing the full processfrom poke point sampling to segment extractionhighlighting the emergence of coherent, manipulable object masks. These visualizations further support the robustness and consistency of our flow-based grouping method across diverse range of objects and scenes. These results also include more examples of probability of motion maps and expected direction of motion maps. However, unlike Figure 4 of the main paper which only showed these maps generated in parallel mode, here we depict the expected direction of motion maps computed using multiple autoregressive rollouts (i.e. Eseq) 6.2 Point-Prompted Segmentation Results In Section 4.1 of the main paper, we evaluated segmentation quality under point-prompted settings on our SpelkeEntitySeg benchmark SpelkeBench. Here, we present additional qualitative results comparing our method to baselines including SAM2 [8], DINOv2 [11], and CWM [33]. As shown in Figure 14, our method consistently produces cohesive and physically plausible segments, in contrast to alternatives that often fragment objects or include extraneous background. 23 SpelkeNet Stanford NeuroAI Lab 6.3 Object Manipulation Using Predicted Segments We previously demonstrated in Section 4.3 of the main paper, the importance of physically grounded segmentation for object manipulation. Here, in Figure 15 we include further qualitative comparisons of edits generated using our predicted segments versus those from SAM [8]. As illustrated, segments aligned with Spelke objecthood significantly improve edit realism, spatial coherence, and transformation consistency across multiple 3DEditBench [9] examples."
        },
        {
            "title": "7 SpelkeNet Specifications",
            "content": "7.1 Model Architecture LRAS [9] is generative visual world model that predicts plausible optical flow fields conditioned on an RGB frame. It is 7 billion-parameter autoregressive transformer (standard LLaMA architecture [48], 32 layers, 4096 embed dimensions, 32 attention heads) that operates over local patch tokens and can generate the predicted flow field in any order, sequentially or in parallel. 7.2 RGB and Flow Quantization The LRAS pipeline starts with lightweight convolutional auto-encoder which quantizes each 4 4 pixel patch into an independent 16-bit code, yielding 65,536-token vocabulary for RGB images (a second, similar quantizer is used to quantize optical-flow patches). 7.3 Enabling both locality and random access During serialization of the RGB and flow tokens into 1D causal sequence, the model inserts special pointer tokens that tell the decoder which patch location to fill next, letting it generate images in an arbitrary or explicitly user-defined order instead of the usual raster scan. This locality plus random-access design promotes compositionality and gives every patch equal causal power. It also allows for fully parallel decoding, illustrating the current best estimate of the models prediction at any step during the decoding (as described in Section 3.2 in the main paper). 7.4 Dataset preparation The model is pre-trained on BVD (Big Video Dataset [9]) - 7k hour dataset of diverse Internet videos mixed with standard 3-D vision datasets such as ScanNet++ [49], CO3D [50], RealEstate-10K [51] and standard video datasets such as Kinetics [52], SomethingSomethingv2 [53] and OpenX embodiment [44]. Camera pose information is provided to the model whenever available in the dataset, and optical flow for every frame pair is computed with the SeaRAFT [46] model and quantized. 7.5 Training details The model is trained with sequence lengths of 4096 and batch size of 512 for 200k steps with next-token cross-entropy loss. The quantizers themselves are first trained on Kinetics-400 frames with simple L2 reconstruction loss. The model was trained on 64 H100 GPUs for approximately 14 days. 24 SpelkeNet Stanford NeuroAI Lab Figure 13: More illustrations of our Spelke segment discovery algorithm. To discover movable objects, we apply multiple virtual pokes at locations sampled from the pmotion map (column 2). While the model consistently propagates flow across the poked object (column 3), it also generates unprompted flow on other objects. However, since this unprompted flow varies across pokes and typically diverges in direction from the input poke, it gets suppressed when averaging the dot product (column 4) and helps us isolate independently movable entities as shown in the last column. Note that we average across 5 pokes, but only show two rows here for brevity. 25 SpelkeNet Stanford NeuroAI Lab Figure 14: Additional qualitative results for point-promoted segmentation across models. SpelkeNet yields sharper, more Spelke-like segments compared to SAM2, DINO, and CWM. 26 SpelkeNet Stanford NeuroAI Lab Figure 15: Additional qualitative comparisons of scene edits using SAM masks versus SpelkeNet segments. Each row shows the original image, the user click location, and the resulting edited image using different segmentation methods. 27 SpelkeNet"
        },
        {
            "title": "References",
            "content": "Stanford NeuroAI Lab [1] Elizabeth Spelke. Principles of object perception. Cognitive science, 14(1):2956, 1990. [2] Scott Johnson. Development of perceptual completion in infancy. Psychological Science, 15 (11):769775, 2004. [3] Jacob Feldman. The role of objects in perceptual grouping. Acta Psychologica, 102(2-3): 137163, 1999. [4] Thomas Brox and Jitendra Malik. Object segmentation by long term analysis of point trajectories. In European conference on computer vision, pages 282295. Springer, 2010. [5] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. Microsoft coco: Common objects in context, 2015. URL https://arxiv.org/abs/1405.0312. [6] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset, 2018. URL https: //arxiv.org/abs/1608.05442. [7] Lu Qi, Jason Kuen, Tiancheng Shen, Jiuxiang Gu, Weidong Guo, Jiaya Jia, Zhe Lin, and Ming-Hsuan Yang. High-quality entity segmentation. In International Conference on Computer Vision (ICCV), October 2023. [8] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollár, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. URL https://arxiv.org/abs/2408.00714. [9] Wanhee Lee, Klemen Kotar, Rahul Mysore Venkatesh, Jared Watrous, Honglin Chen, Khai Loong Aw, and Daniel LK Yamins. 3d scene understanding through local random access sequence modeling. arXiv preprint arXiv:2504.03875, 2025. [10] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [11] Maxime Oquab, Timothée Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. [12] Agrim Gupta, Piotr Dollar, and Ross Girshick. LVIS: dataset for large vocabulary instance In Proceedings of the IEEE Conference on Computer Vision and Pattern segmentation. Recognition, 2019. [13] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollár. Panoptic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 94049413, 2019. SpelkeNet Stanford NeuroAI Lab [14] Oliver Kroemer, Scott Niekum, and George Konidaris. review of robot learning for manipulation: Challenges, representations, and algorithms. Journal of machine learning research, 22 (30):182, 2021. [15] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn, 2017. [16] Bowen Cheng, Alexander G. Schwing, and Alexander Kirillov. Per-pixel classification is not all you need for semantic segmentation. In Proceedings of the conference on Neural Information Processing Systems, 2021. [17] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. Segment anything, 2023. [18] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Wang, Lijuan Wang, Jianfeng Gao, and Yong Jae Lee. Segment everything everywhere all at once, 2023. URL https://arxiv.org/abs/2304.06718. [19] Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, and Humphrey Shi. OneIn Proceedings of the Former: One Transformer to Rule Universal Image Segmentation. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [20] Timo Lüddecke and Alexander Ecker. Image segmentation using text and image prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 70867096, June 2022. [21] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the International Conference on Computer Vision (ICCV), 2021. [22] Oriane Siméoni, Gilles Puy, Huy V. Vo, Simon Roburin, Spyros Gidaris, Andrei Bursuc, Patrick Pérez, Renaud Marlet, and Jean Ponce. Localizing objects with self-supervised transformers and no labels, 2021. URL https://arxiv.org/abs/2109.14279. [23] Yangtao Wang, Xi Shen, Yuan Yuan, Yuming Du, Maomao Li, Shell Xu Hu, James Crowley, and Dominique Vaufreydaz. Tokencut: Segmenting objects in images and videos with selfsupervised transformer and normalized cut, 2023. URL https://arxiv.org/abs/2209.00383. [24] Mark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah Snavely, and William T. Freeman. Unsupervised semantic segmentation by distilling feature correspondences, 2022. URL https: //arxiv.org/abs/2203.08414. [25] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit: Semantic segmentation emerges from text supervision, 2022. URL https://arxiv.org/abs/2202.11094. [26] Wouter Van Gansbeke, Simon Vandenhende, and Luc Van Gool. Discovering object masks with transformers for unsupervised semantic segmentation, 2022. URL https://arxiv.org/ abs/2206.06363. [27] Xudong Wang, Rohit Girdhar, Stella Yu, and Ishan Misra. Cut and learn for unsupervised object detection and instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 31243134, 2023. SpelkeNet Stanford NeuroAI Lab [28] Dylan Li and Gyungin Shin. Promerge: Prompt and merge for unsupervised instance segmentation. In European Conference on Computer Vision (ECCV), 2024. [29] Christopher Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt Botvinick, and Alexander Lerchner. Monet: Unsupervised scene decomposition and representation. arXiv preprint arXiv:1901.11390, 2019. [30] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention. Advances in neural information processing systems, 33:1152511538, 2020. [31] Thomas Kipf, Gamaleldin Elsayed, Aravindh Mahendran, Austin Stone, Sara Sabour, Georg Heigold, Rico Jonschkowski, Alexey Dosovitskiy, and Klaus Greff. Conditional object-centric learning from video. arXiv preprint arXiv:2111.12594, 2021. [32] Thomas Kipf, Elise Van der Pol, and Max Welling. Contrastive learning of structured world models. arXiv preprint arXiv:1911.12247, 2019. [33] Daniel M. Bear, Kevin Feigelis, Honglin Chen, Wanhee Lee, Rahul Venkatesh, Klemen Kotar, Alex Durango, and Daniel L. K. Yamins. Unifying (machine) vision via counterfactual world modeling, 2023. URL https://arxiv.org/abs/2306.01828. [34] Rahul Venkatesh, Honglin Chen, Kevin Feigelis, Daniel Bear, Khaled Jedoui, Klemen Kotar, Felix Binder, Wanhee Lee, Sherry Liu, Kevin Smith, et al. Understanding physical dynamics with counterfactual world modeling. In European Conference on Computer Vision, pages 368387. Springer, 2024. [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [36] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation. In International conference on machine learning, pages 1288812900. PMLR, 2022. [37] Aneesh Komanduri, Karuna Bhaila, and Xintao Wu. Causalvlbench: Benchmarking visual causal reasoning in large vision-language models. arXiv preprint arXiv:2506.11034, 2025. [38] Meiqi Chen, Bo Peng, Yan Zhang, and Chaochao Lu. Cello: Causal evaluation of large vision-language models. arXiv preprint arXiv:2406.19131, 2024. [39] Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting Gao, and Di Zhang. Draganything: Motion control for anything using entity representation, 2024. [40] Yujun Shi, Jun Hao Liew, Hanshu Yan, Vincent Y. F. Tan, and Jiashi Feng. Lightningdrag: Lightning fast and accurate drag-based image editing emerging from videos, 2024. URL https://arxiv.org/abs/2405.13722. [41] Karran Pandey, Paul Guerrero, Metheus Gadelha, Yannick Hold-Geoffroy, Karan Singh, and Niloy J. Mitra. Diffusion handles: Enabling 3d edits for diffusion models by lifting activations to 3d. CVPR, 2024. 30 SpelkeNet Stanford NeuroAI Lab [42] Jiange Yang, Wenhui Tan, Chuhao Jin, Keling Yao, Bei Liu, Jianlong Fu, Ruihua Song, Gangshan Wu, and Limin Wang. Transferring foundation models for generalizable robotic manipulation, 2025. URL https://arxiv.org/abs/2306.05716. [43] Zekai Gu, Rui Yan, Jiahao Lu, Peng Li, Zhiyang Dou, Chenyang Si, Zhen Dong, Qifeng Liu, Cheng Lin, Ziwei Liu, Wenping Wang, and Yuan Liu. Diffusion as shader: 3d-aware video diffusion for versatile video generation control, 2025. URL https://arxiv.org/abs/2501.03847. [44] Open X-Embodiment Collaboration, Abby ONeill, Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Andrey Kolobov, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Felipe Vieira Frujeri, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guangwen Yang, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik Christensen, Hiroki Furuta, Homanga Bharadhwaj, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jay Vakil, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi \"Jim\" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Muhammad Zubair Irshad, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag Sanketi, Patrick \"Tree\" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martin-Martin, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shubham Tulsiani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, 31 SpelkeNet Stanford NeuroAI Lab Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vikash Kumar, Vincent Vanhoucke, Vitor Guizilini, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiangyu Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yansong Pang, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yongqiang Dou, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, and Zipeng Lin. Open X-Embodiment: Robotic learning datasets and RT-X models. https://arxiv.org/abs/2310.08864, 2023. [45] Nobuyuki Otsu. threshold selection method from gray-level histograms. IEEE Transactions on Systems, Man, and Cybernetics, 9(1):6266, 1979. doi: 10.1109/TSMC.1979.4310076. [46] Yihan Wang, Lahav Lipson, and Jia Deng. SEA-RAFT: Simple, efficient, accurate raft for optical flow. In Proceedings of the European Conference on Computer Vision (ECCV), 2024. URL https://arxiv.org/abs/2405.14793. [47] Harold Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):8397, 1955. [48] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. URL https://arxiv.org/abs/2302.13971. [49] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. ScanNet++: high-fidelity dataset of 3d indoor scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. URL https://arxiv.org/abs/2308.11417. Dataset: ScanNet++. [50] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. URL https://arxiv.org/abs/2109.00512. Dataset: CO3D. [51] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. In ACM SIGGRAPH Conference Proceedings, 2018. URL https://arxiv.org/abs/1805.09817. Dataset: RealEstate-10K. [52] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and Andrew Zisserman. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017. URL https://arxiv.org/abs/1705.06950. Dataset: Kinetics-400. [53] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz MuellerFreitag, Florian Hoppe, Christian Thurau, Ingo Bax, and Roland Memisevic. The Something Something Video Database for Learning and Evaluating Visual Common Sense. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 58435851, 2017. URL https: //20bn.com/datasets/something-something/v2. Dataset: 20BNSomethingSomething V2."
        }
    ],
    "affiliations": [
        "Noetik Inc.",
        "OpenAI",
        "Stanford University"
    ]
}