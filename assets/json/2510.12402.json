{
    "paper_title": "Cautious Weight Decay",
    "authors": [
        "Lizhang Chen",
        "Jonathan Li",
        "Kaizhao Liang",
        "Baiyu Su",
        "Cong Xie",
        "Nuo Wang Pierse",
        "Chen Liang",
        "Ni Lao",
        "Qiang Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Cautious Weight Decay (CWD), a one-line, optimizer-agnostic modification that applies weight decay only to parameter coordinates whose signs align with the optimizer update. Unlike standard decoupled decay, which implicitly optimizes a regularized or constrained objective, CWD preserves the original loss and admits a bilevel interpretation: it induces sliding-mode behavior upon reaching the stationary manifold, allowing it to search for locally Pareto-optimal stationary points of the unmodified objective. In practice, CWD is a drop-in change for optimizers such as AdamW, Lion, and Muon, requiring no new hyperparameters or additional tuning. For language model pre-training and ImageNet classification, CWD consistently improves final loss and accuracy at million- to billion-parameter scales."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 2 0 4 2 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Cautious Weight Decay",
            "content": "Lizhang Chen Jonathan Li Kaizhao Liang Baiyu Su Cong Xie Nuo Wang Pierse Chen Liang Ni Lao Qiang Liu Abstract We introduce Cautious Weight Decay (CWD), one-line, optimizer-agnostic modification that applies weight decay only to parameter coordinates whose signs align with the optimizer update. Unlike standard decoupled decay, which implicitly optimizes regularized or constrained objective, CWD preserves the original loss and admits bilevel interpretation: it induces sliding-mode behavior upon reaching the stationary manifold, allowing it to search for locally Pareto-optimal stationary points of the unmodified objective. In practice, CWD is drop-in change for optimizers such as AdamW, Lion, and Muon, requiring no new hyperparameters or additional tuning. For language model pre-training and ImageNet classification, CWD consistently improves final loss and accuracy at millionto billion-parameter scales."
        },
        {
            "title": "Introduction",
            "content": "Algorithm 1 Cautious Weight Decay (CWD) given parameters xt, optimizer update ut, learning rates ηt > 0, weight decay coefficient λ 0 xt+1 xt ηt ut + λI(utxt 0)xt (cid:16) (cid:17) entrywise multiplication Optimization algorithms lie at the core of modern deep learning, shaping not only convergence speed but also training stability and generalization ability across domains such as natural language processing and computer vision. As models and datasets scale, traditional methods such as stochastic gradient descent (SGD) and SGD with momentum [SMDH13] encounter limitations, including slow convergence in non-convex landscapes, sensitivity to learning rate schedules, and poor robustness to sparse or noisy gradients [SM20, ZMB+25]. In response, wide range of alternatives have emerged, including adaptive gradient methods [DHS11, KB15], approximate second-order approaches [MG15, GKS18, YGS+21, LLH+24, NCLL24, WHML25], and specialized algorithms for extreme training regimes [LLCL24, LYL24, XZL+24, HZJ+25, ZCL+25]. Among these advances, decoupled weight decay [LH19] has proven especially influential. In its general form, decoupled weight decay augments any optimizer update ut with decay term applied directly to the parameters, i.e. xt+1 xt ηt(ut + λxt), ut = OptimizerUpdate(xt). Equal contribution by LC and JL. Correspondence: lzchen, jli@cs.utexas.edu University of Texas at Austin Google Figure 1: Final validation loss vs. weight decay coefficient λ for 338M models trained on C4 under Chinchilla scaling. Our approach (red) achieves lower final loss than standard weight decay (blue) while preserving the optimizer-specific optimum in λ. For each optimizer (AdamW, Lion, Muon), both methods use the same hyperparameters. This technique improves training stability and generalization by preventing the adaptive learning rates from interfering with regularization, as exemplified by the success of AdamW in large model training [BMR+20, DBK+21, TMS+23] and the subsequent development of state-of-the-art optimizers such as Lion [CLH+23], Lion-K [CLLL24], and Muon [JJB+24, LSY+25]. However, decoupled weight decay remains agnostic to the directional alignment between the optimizer update and the parameters, which may hurt performance when they conflict. Intuitively, when the update ut and parameters xt point in the same direction for given dimension, weight decay acts as regularizer that improves stability; however, when their directions differ, applying decay actively resists beneficial movement toward the optimum. Furthermore, decoupled weight decay has been shown to implicitly impose regularization terms on the objective function [CLLL24, XL24], which corresponds to parameter norm constraints for AdamW, Lion, and Muon. In light of these limitations, we propose simple refinement: cautious weight decay (CWD), in which decay is applied only in dimensions where the update and parameter signs align (Algorithm 1). Our main contributions are as follows. We introduce cautious weight decay, sign-selective extension of decoupled decay that applies weight decay only when the parameters and update align. Our technique can be implemented as one-line modification without introducing additional hyperparameters compared to standard decoupled decay. We use Lyapunov analysis to show that standard optimizers (SGD(M), Lion-K, Adam) with cautious weight decay are asymptotically stable and unbiased, in the sense that they optimize the original loss rather than regularized surrogate. The regularization effect of cautious weight decay instead becomes bilevel objective of finding locally Pareto-optimal points within the stationary manifold (Figure 2). Furthermore, we show that discrete-time Adam with cautious weight decay attains standard convergence rate in the 2 Figure 2: Trajectories of Adam, AdamW, and Adam + CWD on toy example. Adam halts at minimizer, while AdamW minimizes the objective within constrained region (green). In contrast, Adam + CWD exhibits sliding mode dynamics within the minimizer manifold. smooth nonconvex setting. In language modeling [OWS+25, KFP+25] and ImageNet classification [DDS+09], we observe that cautious weight decay generally accelerates convergence and lowers final validation loss for AdamW, Lion, and Muon (e.g., Figure 1). These improvements translate into higher zero-shot accuracy on standard benchmarks from 338M to 2B parameters and across architectures without retuning baseline settings (20,000 NVIDIA H100 HBM3-80GB GPU hours for all experiments)."
        },
        {
            "title": "2.1 Decoupled weight decay",
            "content": "Gradient-based optimizers with decoupled weight decay can be characterized by the update rule xt+1 = (1 ηtλ)xt ηtut, (1) where ut := U(xt, g1, . . . , gt, t) is an adaptive, often sign-normalized update vector constructed from first and second-moment estimates (e.g., momentum buffers, diagonal preconditioners), ηt > 0 is the learning rate, and λ 0 is the decoupled weight decay coefficient. This framework encapsulates wide range of standard optimizers for machine learning, including AdamW and Lion-K. AdamW. The update vector is given by ut = D1 (cid:98)mt is bias-corrected first-moment estimate. Explicitly, (cid:98)mt, where Dt is diagonal preconditioner and (cid:98)mt = β1mt1 + (1 β1)gt 1 βt 1 , (cid:98)vt = β2vt1 + (1 β2)g2 1 βt 2 , Dt = diag (cid:16)(cid:112) (cid:17) , (cid:98)vt + ϵ1 where β1 and β2 are momentum coefficients and ϵ is numerical stability constant. Lion-K. Given convex function K, the update vector ut is momentum-filtered step that is preconditioned using subgradient, i.e. mt = β2mt1 (1 β2)gt, (cid:101)mt = β1mt1 (1 β1)gt, ut = K( (cid:101)mt), where β1 and β2 are momentum coefficients and is subgradient of K. Examples include Lion when = 1 and Muon when = tr, where tr denotes the trace norm when the parameters are treated as matrix. 2."
        },
        {
            "title": "Implicit regularization effects of weight decay",
            "content": "In general, the application of decoupled weight decay imposes certain regularization or constraint effect on the objective function, where the specific effect depends on the choice of ut. For example, SGD with decoupled weight decay is exactly SGD on an ℓ2-regularized objective. To see the equiv2 x2 alence, let : Rd be differentiable and consider the regularized variant (cid:98)f (x) := (x) + λ 2 . single SGD step on (cid:98)f with learning rate ηt > 0 yields the update xt+1 = xt ηt(f (xt) + λxt) = (1 ηtλ)xt ηtf (xt), which is precisely the decoupled weight decay update given by (1). 3 Given convex function with subgradient and convex conjugate K, suppose the iterates of Lion-K converge to fixed point (x, m, (cid:101)m). Then the moment estimators stabilize so that = (cid:101)m = (x), and the fixed-point condition yields K(f (x))+λx = 0. Rearranging and using the identity (K)1 = K, we obtain (x) + K(λx) = 0, where the left-hand side is the gradient of the function (cid:98)f (x) := (x) + 1 λ K(λx). This suggests that Lion-K optimizes the regularized objective (cid:98)f , an observation made by [CLLL24]. In the special cases of Lion and Muon, is the 0indicator function of dual norm ball, corresponding to the constrained optimization problems min xRd (x) s.t. 1 λ and min XRnm (X) s.t. Xop 1 λ , respectively, where op is the spectral norm when the parameters are treated as matrix. similar analysis for AdamW suggests that it solves the box-constrained problem of minimizing (x) such that 1 λ , but convergence cannot be established due to the lack of Lyapunov function. For more discussion, see Appendix and [XL24]. While AdamW and Lion-K are practically strong, they implicitly optimize regularized surrogate that is dependent on the weight decay coefficient λ. This motivates the development of mechanism that maintains the beneficial effects of decoupled weight decay (e.g. regularization, training acceleration) while optimizing the original objective."
        },
        {
            "title": "3 Cautious Weight Decay",
            "content": "Cautious weight decay (CWD) modifies the update rule (1) as xt+1 = xt ηt(ut + λI(ut xt 0) xt), where denotes entrywise multiplication.1 As one-line modification, cautious weight decay is implementation-trivial and universally compatible with gradient-based optimization algorithms. Theoretically, cautious weight decay also exhibits the following behavior. Unbiased optimization, in the sense that every accumulation point of the trajectory satisfies (x) = 0 under the same convergence conditions required of the base optimizer without weight decay. In over-parameterized deep models, the set of stationary points is typically union of connected submanifolds rather than isolated points. Consequently, the ω-limit set of the trajectory is contained in some stationary manifold, and the iterates eventually remain arbitrarily close to it. Sliding mode dynamics within the stationary manifold, where cautious weight decay allows the trajectory to traverse along the manifold until it cannot decrease the parameter magnitudes in every coordinate. In other words, cautious weight decay steers the trajectory towards local Pareto front of the stationary manifold under the ordering that prioritizes smaller parameter magnitudes. 1Throughout the paper, when it is clear from context, we also drop and write = vx for simplicity."
        },
        {
            "title": "3.1 Convergence to the stationary manifold",
            "content": "We construct Lyapunov functions for the continuous-time limits of several standard optimizers equipped with cautious weight decay. Lyapunov function is lower bounded function with nonpositive derivative that is used to certify the stability of systems of differential equations. Consider the continuous-time dynamics of SGD with cautious weight decay xt = (xt) λI(f (xt)xt 0)xt. This ODE has the Lyapunov function H(x) = (x), since is lower bounded and dH dt = (xt), (xt) λI(f (xt)xt 0)xt = (xt) 2 λ (cid:13) (cid:13)(f (xt)xt)+(cid:13) (cid:13)1 0, where ()+ := max(0, ). LaSalles invariance principle [LaS60] states that the accumulation points of any trajectory lie within the union of trajectories zt that satisfy dt H(zt) = 0 for all 0. Consequently, we conclude that SGD with cautious weight decay produces trajectories that approach the stationary set {x (x) = 0} of the original loss. This holds because cautious weight decay is applied only in secondary fashion and is automatically deactivated whenever it conflicts with the main objective, thereby ensuring that the loss landscape remains unbiased. Beyond the simple case of SGD, the same Lyapunov-type argument can be extended to more sophisticated algorithms such as SGDM, Lion-K, and Adam. In each case, cautious weight decay still minimizes the original objective without introducing explicit bias, but key difficulty lies in constructing appropriate Lyapunov functions. Table 1 summarizes the Lyapunov functions of several major optimizers with cautious weight decay, and detailed derivations are provided in Appendix D. By applying LaSalles invariance principle, we can show that the momentum-based algorithms in Table 1 converge to the stationary set of the original objective, together with vanishing momentum: {(x, m) (x) = 0, = 0}."
        },
        {
            "title": "3.2 Sliding mode dynamics",
            "content": "Although both standard optimization (with no weight decay) and cautious weight decay are unbiased with respect to the original objective, their behaviors diverge within the stationary manifold. In the former, the dynamics halt as the momentum decays to zero, while, in contrast, the cautious weight decay dynamics induce sliding mode, continuing to move along the manifold while reducing the parameter magnitudes as much as possible. Consequently, the algorithm converges to subset of the stationary manifold where further simultaneous reduction of all coordinates of is no longer possible. Equivalently, it converges to locally Pareto-optimal stationary point under preference for smaller parameter magnitudes. To provide mathematical background, consider possibly time-varying discontinuous ODE zt = ft(zt), zt Rd. Due to the discontinuity of ft, the solution may not be well defined in the classical or Carathéodory sense, especially across switching surfaces. We therefore interpret solutions in the Filippov sense Table 1: Comparison of the continuous-time dynamics of different optimizers. SGDM represents SGD with momentum. Lion-K includes Lion (K = 1) and Muon (K = tr) as special cases. : Rd is assumed to be differentiable and lower bounded by . Optimizer Continuous-time dynamics Lyapunov function SGD + CWD xt = (xt) λI(f (xt)xt 0)xt H(x) = (x) SGDM + CWD Lion-K + CWD xt = mt λI(mtxt 0)xt mt = β(f (xt) mt) xt = K(mt) λI(mtxt 0)xt mt = αf (xt) γmt Adam + CWD λI(mtxt 0)xt xt = αtmt ht mt = α(f (xt) mt) vt = γ(f (xt)2 vt) H(x, m) = βf (x) + 1 2 m2 2 + λ (mx)+1 H(x, m) = αf (x) + K(m) + λ (mx)+ Ht(x, m, h) = αf (x) + (cid:13) (cid:13) (cid:13) (cid:13) αtm2 2h (cid:13) (cid:13) (cid:13) (cid:13)1 + λ (cid:13) (cid:13)(mx)+(cid:13) (cid:13) Notation. We drop for simplicity. αt := (1 exp(αt))1, γt := (1 exp(γt))1, ht := γtvt + ϵ1. [Fil88], where discontinuous ODE is formally differential inclusion that specifies that zt belongs to the closed convex envelope of the discontinuous vector field, i.e. zt F[ft](zt) := (cid:92) (cid:92) δ>0 µ(S)=0 co(ft(B(zt, δ) S)), where µ denotes the Lebesgue measure, B(z, δ) is the δ-ball centered at z, and co denotes the closed convex envelope. This construction captures all possible limiting directions of the vector field near discontinuities, ensuring well-defined dynamics even when ft is not continuous. The key idea is that the values of zt must be determined by the behavior of ft in neighborhood around zt, rather than at the point itself. The inclusion, therefore, defines range of admissible velocities consistent with the nearby values of the vector field. In particular, whenever ft contains coordinatewise indicators such as I(g(zt) 0), the Filippov set replaces them by selectors st [0, 1]d on the switching set {[g(zt)]i = 0}: [st]i {1} {0} [0, 1] [g(zt)]i > 0, [g(zt)]i < 0, [g(zt)]i = 0. Recalling the Lyapunov analysis in Section 3.1, the continuous-time dynamics of standard optimizers with cautious weight decay converge to the stationary manifold := {x (x) = 0}, with the momentum mt also decaying to 0 for momentum-based methods. Consequently, once the trajectory enters the stationary manifold, the residual dynamics reduce to xt = λst xt, st [0, 1]d. (2) 6 Figure 3: Toy objectives and trajectories. Left: (x, y) = ((y 3)2 (x 3)2 1)2. Right: (x, y) = (y 3 (x 3)2)2. We compare Adam, AdamW, and Adam + CWD; AdamW and CWD use the same weight decay λ, and all other hyperparameters (η, β1, β2, ϵ) are identical. For both objectives, Adam converges to generic point on the minimizer manifold, whereas AdamW converges to solution of the box-constrained problem minx,y (x, y) subject to max{x, y} 1 λ . In contrast, Adam + CWD converges to the Pareto front of the minimizer manifold. Moreover, since the Lyapunov function confines the dynamics to the stationary set, the selectors st must be chosen such that the trajectory remains within the manifold. Differentiating the stationarity condition yields dt (xt) = λ2f (xt)(st xt) = 0, st [0, 1]d. This relation allows us to solve for admissible choices of st that guarantee invariance of the manifold. In general, the solution for st need not be unique, and the actual value realized in practice may be implicitly determined by the discretization scheme employed. Effectively, cautious weight decay decreases parameter magnitudes along each coordinate while staying within the stationary manifold, pushing toward the local Pareto front of the manifold := {x δ > 0 (B(x, δ) M) {x}, x} , where the tangent space no longer allows nonzero st in (2). In other words, stationary point is locally Pareto-optimal if it has neighborhood in the stationary manifold that contains no other point with smaller or equal magnitude in every coordinate. This argument shows that cautious weight decay dynamics converge to P. Since may not be singleton, the exact limit point depends intricately on initialization and the discretization of the continuous-time dynamics. Figure 3 illustrates this behavior on two toy problems."
        },
        {
            "title": "4 Discrete-Time Analysis",
            "content": "Algorithm 2 Adam with cautious weight decay 1: given learning rates {ηt}tN R>0, momentum coefficients 0 β1 β2 < 1, numerical stability constant ϵ 0, weight decay coefficient λ > 0 gt StochasticGradient(xt) 2: initialize time step 1, parameters x1 Rd, first moment m0 0, second moment v0 0 3: repeat 4: 5: mt β1mt1 + (1 β1)gt 6: 7: 8: vt β2vt1 + (1 β2)g2 (cid:98)mt (1 βt (cid:98)vt (1 βt xt+1 xt ηt 1)1mt 2)1vt (cid:18) (cid:99)mt (cid:98)vt+ϵ1 9: + 1 10: 11: until stopping criterion is met 12: return optimized parameters xt +λI(mtxt 0)xt (cid:19) entrywise multiplication entrywise operations Leveraging the Lyapunov functions in Section 3, it is possible to extend our analysis to the discretetime dynamics of various optimizers with cautious weight decay. In this section, we use Adam with cautious weight decay (Algorithm 2) as an example, showing that in the smooth nonconvex setting, Algorithm 2 achieves standard convergence rate of O(T 1 2 ) on the squared gradient norm and an additional stationarity measure. We make the following assumptions, which are mild and often used in the analysis of stochastic gradient algorithms [GL13, BB21, DBBU22, ACD+23]. Assumption 1. is coercive and L-smooth. This implies that attains minimum value, which we denote as , and that the iterates of Algorithm 2 are bounded. Assumption 2 (Bounded variance). The stochastic gradient gt satisfies E[gt] = (xt) and Var(gt) = (cid:104) gt (xt)2 2 (cid:105) σ2 nbatch , where σ is constant and nbatch denotes the batch size. Theorem 1. Under Assumptions 1 and 2, let 0 β1 β2 < 1, λ 0, ϵ > 0, and ηt = η > 0, and suppose xt is updated using Algorithm 2. Then for all N, 1 (cid:88) t[T ] (cid:104) (xt)2 2 + λ (cid:13) (cid:13)(f (xt)xt)+(cid:13) (cid:13)1 (cid:105) K1 ηT + K2 + K3η + K4σ nbatch , where K1, K2, K3, and K4 are constants. Proof sketch. We follow the standard approach of first proving descent lemma. The full proof is deferred to Appendix E. Remark 1. The first term on the left-hand side, (xt)2 2, reflects how much is optimized, while the second term, (f (xt)xt)+1, reflects the degree of conflict between the objective and the parameter magnitudes. If (xt)xt 0, then there is room to jointly decrease both and 8 Figure 4: Evaluation loss across scales. 33 grid for 338M, 986M, and 2B Transformer models trained with AdamW, Lion, and Muon on C4 dataset. All panels show zoom into the final 40% of training steps to highlight late-stage behavior. Baseline curves (dashed blue) use standard weight decay with tuned hyperparameters (learning rate schedule, βs, weight decay, etc.; see Appendix F). Our method (solid red) follows Algorithm 1 and reuses the baseline hyperparameters without additional tuning. Full (non-zoomed) curves are in Figures 8, 9 and 10 in Appendix G. the magnitudes. Thus, small value of (f (xt)xt)+1 indicates that the optimizer has reached state where it is difficult to further decrease and shrink the magnitudes simultaneously. This corresponds to convergence toward Pareto front, where trade-offs between the two objectives become unavoidable. Remark 2. In the setting of Theorem 1, let N, η = Θ , and nbatch = Θ(T ). Then (cid:16) 1 (cid:17) 1 (cid:88) t[T ]"
        },
        {
            "title": "5 Experiments",
            "content": "(cid:104) (xt)2 2 + λ (cid:13) (cid:13)(f (xt)xt)+(cid:13) (cid:13)1 (cid:105) = (cid:18) 1 (cid:19) . Overview. We evaluate CWD against three standard optimizersAdamW, Lion, and Muonon autoregressive language modeling and ImageNet classification. For Transformer models with similar 9 Table 2: Ablation study of selective weight decay strategies on OLMo-1B (100B tokens). We compare our momentum-based selection against alternative masking approaches. Baseline: standard weight decay (λ tuned). Ours: update-based mask I(ux 0) using baselines λ without retuning. Random: time-varying Bernoulli mask matching our methods sparsity ratio (see Figure 6 in Appendix G). Gradient: uses I(gx 0) instead. No WD: λ = 0. Lower validation loss is better. Weight Decay Active Ablated Masks Disabled Optimizer Baseline AdamW Muon 2.65 2. Ours 2.56 2.42 Random Gradient No WD 2.82 2. 2.75 2.74 2.70 2.62 Table 3: ImageNet validation accuracy (%) across architectures and optimizers. All models train for 300 epochs with standard augmentation. Base: optimizer with tuned weight decay. Ours: cautious weight decay using the same coefficient as baseline (no retuning). AdamW Lion Muon Model Params ViT-S/16 ResNet-50 ViT-B/16 22.05M 25.56M 86.57M Base 78.84 76.30 80. Ours 79.45 76.68 80.71 Base 79.29 76.41 80.76 Ours 79.82 76.75 80. Base 79.35 76.47 80.83 Ours 79.91 76.83 81.04 architecture to Gemma [KFP+25] with 338M, 986M, and 2B parameters in the Simply [LHY+25] codebase, we follow the Chinchilla compute-optimal scaling rule20 tokens per parameter (TPP) [HBM+22] and train on C4 [RSR+20]. For each size, we grid-search batch size, learning rate, weight decay, warmup ratio, and optimizer-specific hyperparameters for the baselines (AdamW, Lion, Muon); we then reuse the selected baseline settings for CWD without retuning (details in Appendix F). Under matched settings, CWD lowers final validation loss and improves zero-shot accuracy. On the OLMo codebase [OWS+25], we further study an over-training regimeOLMo-1B trained on 100B tokens (100 TPP) from Dolma [SKB+24]. Under matched settings, CWD lowers final validation loss and improves zero-shot accuracy  (Table 4)  . We also observe similar gains on ImageNet [DDS+09] across ViT [DBK+21] and ResNet [HZRS16]. Ablations of weight decay. Figure 1 sweeps the weightdecay coefficient λ for 338M model on C4: λ [0, 0.4] for Muon and AdamW, and λ [0, 3.0] for Lion. Two patterns are consistent across runs: (i) at fixed λ, CWD attains lower final loss than the corresponding baseline with decoupled weight decay; (ii) the minimizing value λ is essentially unchanged when replacing the baseline with CWD. In practice, one can swap in CWD at an already tuned λ and obtain improvements without additional sweeps. Ablations on masking. Table 2 tests whether the benefits arise from the amount of decay applied or from CWDs structure. Replacing our mask with time-matched Bernoulli random mask substantially degrades performance (e.g., 2.56 2.82 for AdamW, 2.42 2.73 for Muon), showing that simply reducing the frequency of decay is insufficient. Substituting the indicator with the gradient-based I(gx 0) also underperforms. Finally, λ = 0 remains worse than tuned decay, illustrating that explicit regularization is helpful and CWD leverages it more effectively. Training dynamics. On 1B models trained for 100B tokens, we observe that CWD tends to improve the loss trajectory relative to tuned AdamW and Muon, rather than only the final value (Figure 5). similar pattern appears at 986M: Figure 7 in Appendix shows evaluation/training loss and RMS 10 Optimizer Hellaswag ARC-Easy ARC-C PIQA MMLU ComQA acc_norm acc_norm acc_norm acc_norm acc AdamW AdamW+CWD Muon Muon+CWD 0.38 0.40 0.39 0.41 0.50 0. 0.51 0.51 0.25 0.27 0.26 0.28 0.67 0.69 0.68 0.71 0.23 0. 0.24 0.26 acc 0.29 0.31 0.30 0.33 Table 4: Downstream accuracy across diverse reasoning benchmarks. All runs use the OLMo codebase with 1B-parameter models trained for 100B tokens under an over-training regime. Here ARC-C=ARC-Challenge and ComQA=CommonsenseQA. Figure 5 shows the corresponding loss curves. Figure 5: Training loss of OLMo 1B on 100B tokens. Left: AdamW. Right: Muon. parameter norm over time. CWD generally achieves lower loss while ending with an intermediate norm. In contrast, removing decay entirely (λ = 0) descends faster mid-training but plateaus earlier, finishing at higher loss and the largest norm; tuned AdamW with λ > 0 yields the smallest norm. Overall, these results suggest that the gains come from more selective application of regularization rather than from disabling it. CWD outperforms standard decay across optimizers and scales. Under the common setup across 338M, 986M, and 2B parameters, CWD consistently lowers eval loss for AdamW, Lion, and Muon (see Figure 4 and Figures 810 in Appendix G) and increases downstream accuracy  (Table 4)  . CWD yields lower gradient norms than standard decay. Across model sizes, CWD produces lower RMS-normalized gradient norms than the corresponding baselines (see Figure 11 in Appendix G). This coincides with the lower end-of-training loss in Figure 5 and the accuracy gains in Table 4."
        },
        {
            "title": "6 Related Work",
            "content": "Weight decay. Weight decay originated as an ℓ2 penalty for ill-posed problems and ridge regression [Tik63, HK70] and was introduced to neural networks as generalization tool to mitigate overfitting [HP88, WRH90, KH91]. [LH19] showed that, for adaptive methods, weight decay and ℓ2 are not equivalent, motivating the decoupled formulation in AdamW; subsequent work established decoupled decay as standard feature of modern optimizers [CLH+23, CLLL24, LSY+25]. Recent analyses suggest that in contemporary networks, weight decay functions more as training 11 accelerator and stabilizer than as explicit regularization [KSH17, HBM+22, PC23, DAVF24]. Interactions with normalization layers and learning rate schedules have also been clarified [Def25], and architectural designs can obviate explicit decay [LHSG25]. Weight decay variants. Various efforts have been made to develop different adaptive variants of weight decay. For example, [XXZ+23] found that weight decay can lead to large gradient norms at the final phase of training and proposed Scheduled Weight Decay (SWD) to dynamically adjust weight decay strength based on gradient norms. [KMJ24] investigate how weight decay affects individual neuron updates, revealing rotational equilibrium states that balance learning across layers and neurons. [GSA23] introduce adaptive weight decay that automatically tunes the hyperparameter during training based on classification and regularization loss gradients, achieving significant improvements in adversarial robustness. Masked or conditional updates. Several works have explored the sign-based conditioning of optimizer updates. [RB93] introduced Rprop, which adjusted step sizes based on current gradient and past gradient sign agreement. [LCLL24] propose the cautious optimizer, which restricts updates to dimensions where the proposed update and current gradient share the same sign. [WLX+24] apply similar mask to Adam to improve robustness in online learning. Our work is the first to integrate the intuition behind these approaches with decoupled weight decay, showing that selective decay is not merely compatible with adaptive methods but can enhance them significantly. Constrained and bilevel optimization. Decoupled weight decay can be interpreted through the lens of FrankWolfe algorithms for constrained optimization [FW56, Jag13, SW25, PXA+25]. This connection suggests that optimizers with decoupled weight decay implicitly solve constrained optimization problems, which was shown to be the case for Lion [CLLL24, SW25, PXA+25], AdamW [XL24, BN24], and Muon [CLL25, SW25, LLS25]. In contrast, optimizers with cautious weight decay perform bilevel optimization, framework from classical optimization [Sol07a, Sol07b, SS17] that has been recently explored in machine learning [GLL21, PMA24]."
        },
        {
            "title": "7 Conclusion",
            "content": "We introduce cautious weight decay and formalize it as simple, optimizer-agnostic modification of decoupled weight decay that preserves the optimization objective while retaining the practical benefits of weight decay. For standard optimizers (SGD, Adam, and Lion-K), we show the bilevel optimization structure of cautious weight decay and establish convergence guarantees in both continuousand discrete-time regimes. Across diverse tasks and benchmarks, cautious weight decay consistently improves training dynamics compared to no decay and traditional decoupled decay, yielding faster loss reduction and more stable trajectories without changes to hyperparameters or model architectures. Our results indicate that cautious weight decay is theoretically principled and empirically effective technique that retains the benefits of weight decay while addressing its fundamental limitations. References [ACD+23] Yossi Arjevani, Yair Carmon, John C. Duchi, Dylan J. Foster, Nathan Srebro, and Blake E. Woodworth. Lower bounds for non-convex stochastic optimization. Math. 12 Program., 199(1):165214, 2023. [BB21] Anas Barakat and Pascal Bianchi. Convergence and dynamical behavior of the ADAM algorithm for nonconvex stochastic optimization. SIAM J. Optim., 31(1):244274, 2021. [BC99] Andrea Bacciotti and Francesca Ceragioli. Stability and stabilization of discontinuous systems and nonsmooth lyapunov functions. ESAIM: Control, Optimisation and Calculus of Variations, 4:361376, 1999. [BMR+20] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, 2020. [BN24] Jeremy Bernstein and Laker Newhouse. Old optimizer, new norm: An anthology. CoRR, abs/2409.20325, 2024. [BWAA18] Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar. SIGNSGD: compressed optimisation for non-convex problems. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, volume 80 of Proceedings of Machine Learning Research, pages 559568, 2018. [CLH+23] Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, and Quoc V. Le. Symbolic discovery of optimization algorithms. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, 2023. [CLL25] Lizhang Chen, Jonathan Li, and Qiang Liu. Muon optimizes under spectral norm constraints. CoRR, abs/2506.15054, 2025. [CLLL24] Lizhang Chen, Bo Liu, Kaizhao Liang, and Qiang Liu. Lion secretly solves constrained optimization: As lyapunov predicts. In The Twelfth International Conference on Learning Representations, ICLR 2024, 2024. [DAVF24] Francesco DAngelo, Maksym Andriushchenko, Aditya Vardhan Varre, and Nicolas Flammarion. Why do we need weight decay in modern deep learning? In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, 2024. [DBBU22] Alexandre Défossez, Léon Bottou, Francis R. Bach, and Nicolas Usunier. simple convergence proof of adam and adagrad. Trans. Mach. Learn. Res., 2022, 2022. [DBK+21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Trans13 formers for image recognition at scale. Representations, ICLR 2021, 2021. In 9th International Conference on Learning [DDS+09] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009), pages 248255, 2009. [Def25] Aaron Defazio. Why gradients rapidly increase near the end of training. CoRR, abs/2506.02285, 2025. [DHS11] John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. J. Mach. Learn. Res., 12:21212159, 2011. [Fil88] Aleksej F. Filippov. Differential Equations with Discontinuous Righthand Sides, volume 18 of Mathematics and Its Applications. Springer, 1988. [FW56] Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Nav. Res. Logist. Q., 3(12):95110, 1956. [GBB+21] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling. CoRR, abs/2101.00027, 2021. [GKS18] Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Preconditioned stochastic tensor optimization. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, volume 80 of Proceedings of Machine Learning Research, pages 18371845, 2018. [GL13] Saeed Ghadimi and Guanghui Lan. Stochastic firstand zeroth-order methods for nonconvex stochastic programming. SIAM J. Optim., 23(4):23412368, 2013. [GLL21] Chengyue Gong, Xingchao Liu, and Qiang Liu. Automatic and harmless regularization with constrained and lexicographic optimization: dynamic barrier approach. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, pages 2963029642, 2021. [GSA23] Amin Ghiasi, Ali Shafahi, and Reza Ardekani. Improving robustness with adaptive weight decay. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, 2023. [HBM+22] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack W. Rae, and Laurent Sifre. An empirical analysis of compute-optimal large language model training. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, 2022. 14 [HK70] Arthur E. Hoerl and Robert W. Kennard. Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12(1):5567, 1970. [HP88] Stephen Jose Hanson and Lorien Y. Pratt. Comparing biases for minimal network In Advances in Neural Information Processing construction with back-propagation. Systems 1, NIPS Conference, pages 177185, 1988. [HZJ+25] Tianjin Huang, Ziquan Zhu, Gaojie Jin, Lu Liu, Zhangyang Wang, and Shiwei Liu. SPAM: spike-aware adam with momentum reset for stable LLM training. In The Thirteenth International Conference on Learning Representations, ICLR 2025, 2025. [HZRS16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning In 2016 IEEE Conference on Computer Vision and Pattern for image recognition. Recognition, CVPR 2016, pages 770778, 2016. [Jag13] Martin Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, volume 28 of JMLR Workshop and Conference Proceedings, pages 427435, 2013. [JJB+24] Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein. Muon: An optimizer for hidden layers in neural networks, 2024. [KB15] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, 2015. [KFP+25] Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean-Bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Gaël Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Róbert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen, Abhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, András György, André Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson, Ashish Shenoy, Ayan Chakrabarti, Bilal Piot, Bo Wu, Bobak Shahriari, Bryce Petrini, Charlie Chen, Charline Le Lan, Christopher A. ChoquetteChoo, CJ Carey, Cormac Brick, Daniel Deutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree Shivakumar Sreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eugene Kharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna Klimczak-Plucinska, Harman Singh, Harsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian Ballantyne, Idan Szpektor, Ivan Nardini, Jean Pouget-Abadie, Jetha Chan, Joe Stanton, John Wieting, Jonathan Lai, Jordi Orbay, Joseph Fernandez, Josh Newlan, Ju-yeong Ji, Jyotinder Singh, Kat Black, Kathy Yu, Kevin Hui, Kiran Vodrahalli, Klaus Greff, Linhai Qiu, Marcella Valentine, Marina Coelho, Marvin Ritter, Matt Hoffman, Matthew Watson, Mayank Chaturvedi, Michael Moynihan, Min Ma, Nabila Babar, Natasha Noy, Nathan 15 Byrd, Nick Roy, Nikola Momchev, Nilay Chauhan, Oskar Bunyan, Pankil Botarda, Paul Caron, Paul Kishan Rubenstein, Phil Culliton, Philipp Schmid, Pier Giuseppe Sessa, Pingmei Xu, Piotr Stanczyk, Pouya Tafti, Rakesh Shivanna, Renjie Wu, Renke Pan, Reza Rokni, Rob Willoughby, Rohith Vallu, Ryan Mullins, Sammy Jerome, Sara Smoot, Sertan Girgin, Shariq Iqbal, Shashir Reddy, Shruti Sheth, Siim Põder, Sijal Bhatnagar, Sindhu Raghuram Panyam, Sivan Eiger, Susan Zhang, Tianqi Liu, Trevor Yacovone, Tyler Liechty, Uday Kalra, Utku Evci, Vedant Misra, Vincent Roseberry, Vlad Feinberg, Vlad Kolesnikov, Woohyun Han, Woosuk Kwon, Xi Chen, Yinlam Chow, Yuvein Zhu, Zichuan Wei, Zoltan Egyed, Victor Cotruta, Minh Giang, Phoebe Kirk, Anand Rao, Jessica Lo, Erica Moreira, Luiz Gustavo Martins, Omar Sanseviero, Lucas Gonzalez, Zach Gleicher, Tris Warkentin, Vahab Mirrokni, Evan Senter, Eli Collins, Joelle K. Barral, Zoubin Ghahramani, Raia Hadsell, Yossi Matias, D. Sculley, Slav Petrov, Noah Fiedel, Noam Shazeer, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clément Farabet, Elena Buchatskaya, Jean-Baptiste Alayrac, Rohan Anil, Dmitry (Dima) Lepikhin, Sebastian Borgeaud, Olivier Bachem, Armand Joulin, Alek Andreev, Cassidy Hardin, Robert Dadashi, and Léonard Hussenot. Gemma 3 technical report. CoRR, abs/2503.19786, 2025. [KH91] Anders Krogh and John A. Hertz. simple weight decay can improve generalization. In Advances in Neural Information Processing Systems 4, NIPS Conference, pages 950 957, 1991. [KMJ24] Atli Kosson, Bettina Messmer, and Martin Jaggi. Rotational equilibrium: How weight decay balances learning across neural networks. In Forty-first International Conference on Machine Learning, ICML 2024, 2024. [KSH17] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. Commun. ACM, 60(6):8490, 2017. [LaS60] Joseph P. LaSalle. Some extensions of liapunovs second method. IRE Transactions on Circuit Theory, 7(4):520527, 1960. [LCLL24] Kaizhao Liang, Lizhang Chen, Bo Liu, and Qiang Liu. Cautious optimizers: Improving training with one line of code. CoRR, abs/2411.16085, 2024. [LH19] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, 2019. [LHSG25] Ilya Loshchilov, Cheng-Ping Hsieh, Simeng Sun, and Boris Ginsburg. ngpt: Normalized transformer with representation learning on the hypersphere. In The Thirteenth International Conference on Learning Representations, ICLR 2025, 2025. [LHY+25] Chen Liang, Da Huang, Chengrun Yang, Xiaomeng Yang, Andrew Li, Xinchen Yan, and Simply Contributors. Simply: an experiment to accelerate and automate AI research. GitHub repository, 2025. [LLCL24] Kaizhao Liang, Bo Liu, Lizhang Chen, and Qiang Liu. Memory-efficient LLM training with online subspace descent. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in 16 Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, 2024. [LLH+24] Hong Liu, Zhiyuan Li, David Leo Wright Hall, Percy Liang, and Tengyu Ma. Sophia: scalable stochastic second-order optimizer for language model pre-training. In The Twelfth International Conference on Learning Representations, ICLR 2024, 2024. [LLS25] Tim Tsz-Kit Lau, Qi Long, and Weijie Su. Polargrad: class of matrix-gradient optimizers from unifying preconditioning perspective. CoRR, abs/2505.21799, 2025. [LSY+25] Jingyuan Liu, Jianlin Su, Xingcheng Yao, Zhejun Jiang, Guokun Lai, Yulun Du, Yidao Qin, Weixin Xu, Enzhe Lu, Junjie Yan, Yanru Chen, Huabin Zheng, Yibo Liu, Shaowei Liu, Bohong Yin, Weiran He, Han Zhu, Yuzhi Wang, Jianzhou Wang, Mengnan Dong, Zheng Zhang, Yongsheng Kang, Hao Zhang, Xinran Xu, Yutao Zhang, Yuxin Wu, Xinyu Zhou, and Zhilin Yang. Muon is scalable for LLM training. CoRR, abs/2502.16982, 2025. [LYL24] Qijun Luo, Hengxu Yu, and Xiao Li. Badam: memory efficient full parameter optimization method for large language models. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, 2024. [MG15] James Martens and Roger B. Grosse. Optimizing neural networks with kroneckerfactored approximate curvature. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, volume 37 of JMLR Workshop and Conference Proceedings, pages 24082417, 2015. [NCLL24] Son Nguyen, Lizhang Chen, Bo Liu, and Qiang Liu. H-fac: Memory-efficient optimization with factorized hamiltonian descent. CoRR, abs/2406.09958, 2024. [OWS+25] Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, and Hannaneh Hajishirzi. 2 olmo 2 furious. CoRR, abs/2501.00656, 2025. [PC23] Leyan Pan and Xinyuan Cao. Towards understanding neural collapse: The effects of batch normalization and weight decay. CoRR, abs/2309.04644, 2023. [PMA24] Ieva Petrulionyte, Julien Mairal, and Michael Arbel. Functional bilevel optimization for machine learning. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, 2024. [PXA+25] Thomas Pethick, Wanyun Xie, Kimon Antonakopoulos, Zhenyu Zhu, Antonio SilvetiFalls, and Volkan Cevher. Training deep learning models with norm-constrained lmos. In Forty-second International Conference on Machine Learning, ICML 2025, 2025. 17 [RB93] Martin A. Riedmiller and Heinrich Braun. direct adaptive method for faster backpropagation learning: the RPROP algorithm. In Proceedings of International Conference on Neural Networks (ICNN88), San Francisco, CA, USA, March 28 - April 1, 1993, pages 586591. IEEE, 1993. [RSR+20] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. J. Mach. Learn. Res., 21(140):167, 2020. [SKB+24] Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Raghavi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an open corpus of three trillion tokens for language model pretraining research. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, pages 1572515788, 2024. [SM20] Kevin Scaman and Cédric Malherbe. Robustness analysis of non-convex stochastic gradient descent using biased expectations. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, 2020. [SMDH13] Ilya Sutskever, James Martens, George E. Dahl, and Geoffrey E. Hinton. On the importance of initialization and momentum in deep learning. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, volume 28 of JMLR Workshop and Conference Proceedings, pages 11391147, 2013. [Sol07a] Mikhail V. Solodov. bundle method for class of bilevel nonsmooth convex minimization problems. SIAM J. Optim., 18(1):242259, 2007. [Sol07b] Mikhail V. Solodov. An explicit descent method for bilevel convex optimization. J. Convex Anal., 14(2):227238, 2007. [SP94] Daniel W. Shevitz and Brad Paden. Lyapunov stability theory of nonsmooth systems. IEEE Trans. Autom. Control., 39(9):19101914, 1994. [SS17] Shoham Sabach and Shimrit Shtern. first order method for solving convex bilevel optimization problems. SIAM J. Optim., 27(2):640660, 2017. [SW25] Maria-Eleni Sfyraki and Jun-Kun Wang. Lions and muons: Optimization via stochastic frank-wolfe. CoRR, abs/2506.04192, 2025. [Tik63] Andrey Tikhonov. On the solution of ill-posed problems and the method of regularization. Dokl. Akad. Nauk SSSR, 151(3):501504, 1963. [TMS+23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David 18 Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023. [WHML25] Kaiyue Wen, David Hall, Tengyu Ma, and Percy Liang. Fantastic pretraining optimizers and where to find them. CoRR, abs/2509.02046, 2025. [WLX+24] Shaowen Wang, Anan Liu, Jian Xiao, Huan Liu, Yuekui Yang, Cong Xu, Qianqian Pu, Suncong Zheng, Wei Zhang, and Jian Li. Cadam: Confidence-based optimization for online learning. CoRR, abs/2411.19647, 2024. [WRH90] Andreas S. Weigend, David E. Rumelhart, and Bernardo A. Huberman. Generalization by weight-elimination with application to forecasting. In Advances in Neural Information Processing Systems 3, NIPS Conference, pages 875882, 1990. [XL24] Shuo Xie and Zhiyuan Li. Implicit bias of adamw: ℓ-norm constrained optimization. In Forty-first International Conference on Machine Learning, ICML 2024, 2024. [XXZ+23] Zeke Xie, Zhiqiang Xu, Jingzhao Zhang, Issei Sato, and Masashi Sugiyama. On the overlooked pitfalls of weight decay and how to mitigate them: gradient-norm perspective. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, 2023. [XZL+24] Xingyu Xie, Pan Zhou, Huan Li, Zhouchen Lin, and Shuicheng Yan. Adan: Adaptive nesterov momentum algorithm for faster optimizing deep models. IEEE Trans. Pattern Anal. Mach. Intell., 46(12):95089520, 2024. [YGS+21] Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, and Michael W. Mahoney. ADAHESSIAN: an adaptive second order optimizer for maIn Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI chine learning. 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, pages 1066510673, 2021. [YLY+25] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jian Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng 19 Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. CoRR, abs/2505.09388, 2025. [ZCL+25] Yushun Zhang, Congliang Chen, Ziniu Li, Tian Ding, Chenwei Wu, Diederik P. Kingma, Yinyu Ye, Zhi-Quan Luo, and Ruoyu Sun. Adam-mini: Use fewer learning rates to gain more. In The Thirteenth International Conference on Learning Representations, ICLR 2025, 2025. [ZMB+25] Rosie Zhao, Depen Morwani, David Brandfonbrener, Nikhil Vyas, and Sham M. Kakade. Deconstructing what makes good optimizer for autoregressive language models. In The Thirteenth International Conference on Learning Representations, ICLR 2025, 2025."
        },
        {
            "title": "A Notation and Definitions",
            "content": "N := {1, 2, 3, . . . } denotes the natural numbers. For N, [n] denotes the set {1, 2, . . . , n}. Vectors are denoted in lowercase boldface, and matrices are denoted in capital boldface. 0 and 1 denote the all-zeros and all-ones tensors of appropriate dimension, respectively. Scalar operations and functions, e.g. multiplication, division, and square roots, are understood to be performed entrywise when applied to vectors. We also use to explicitly denote the entrywise product. x+ denotes the positive part of x, i.e. x+ := max(0, x) = (cid:40) if > 0 0 otherwise . denotes the ℓp norm for [1, ]. , denotes the standard inner product on Rd. [x]i denotes the ith entry of vector x. diag (x) denotes the diagonal matrix with diagonal entries given by x. I(x 0) denotes the indicator tensor that is 1 in coordinate if is nonnegative in that coordinate and 0 otherwise. If : Rd is convex, we let K(x) denote the set of subgradients of at and overload K(x) to denote an element of K(x). Definition 1 (L-smoothness). function : Rd is L-smooth if it is differentiable and (y) (x)2 x2 for all x, Rd. If is L-smooth, then (y) (x) + (x), + x2 2 for all x, Rd. Definition 2 (Coerciveness). function : Rd is coercive if (x) as ."
        },
        {
            "title": "B Pseudocode of Optimizers with CWD",
            "content": "B.1 SGD with momentum Algorithm 3 SGD with momentum and cautious weight decay gt StochasticGradient(xt) 1: given learning rates {ηt}tN R>0, momentum coefficient β [0, 1), weight decay coefficient λ > 0 2: initialize time step 1, parameters x1 Rd, first moment m0 0 3: repeat 4: 5: mt βmt1 + (1 β)gt 6: 7: 8: until stopping criterion is met 9: return optimized parameters xt xt+1 xt ηt + 1 mt +λI(mtxt 0)xt (cid:16) (cid:17) entrywise multiplication B.2 Lion-K Algorithm 4 Lion-K with cautious weight decay 1: given learning rates {ηt}tN R>0, momentum coefficients β1, β2 [0, 1), convex : Rd with subgradient K, weight decay coefficient λ > 0 gt StochasticGradient(xt) 2: initialize time step 1, parameters x1 Rd, first moment m1 0 3: repeat 4: 5: mt+1 β2mt (1 β2)gt (cid:101)mt+1 β1mt (1 β1)gt 6: xt+1 xt + ηt + 1 K( (cid:101)mt+1) λI(K( (cid:101)mt+1)xt 0)xt (cid:16) (cid:17) 7: 8: 9: until stopping criterion is met 10: return optimized parameters xt B.3 Lion Algorithm 5 Lion with cautious weight decay 1: given learning rates {ηt}tN R>0, momentum coefficients β1, β2 [0, 1), weight decay coefficient λ > 0 2: initialize time step 1, parameters x1 Rd, first moment m0 0 3: repeat 4: 5: (cid:16) gt StochasticGradient(xt) (cid:101)mt β1mt1 + (1 β1)gt xt+1 xt ηt 6: 7: mt β2mt1 + (1 β2)gt 8: 9: until stopping criterion is met 10: return optimized parameters xt + 1 sgn( (cid:101)mt) +λI( (cid:101)mtxt 0)xt (cid:17) entrywise multiplication entrywise sgn and multiplication 21 B.4 Muon Algorithm 6 Muon with cautious weight decay 1: given learning rates {ηt}tN R>0, momentum coefficient β [0, 1), weight decay coefficient λ > 0 2: initialize time step 1, parameters X1 Rnm, first moment M0 0 3: repeat 4: Gt StochasticGradient(Xt) 5: Mt βMt1 + Gt 6: Ot NewtonSchulz(Mt) 7: Xt+1 Xt ηt 8: + 1 9: until stopping criterion is met 10: return optimized parameters Xt (cid:16) Ot +λI(OtXt 0)Xt (cid:17) approximation of matrix sign entrywise matrix multiplication Fixed-Point Analysis Revisiting the fixed-point analysis in Section 2.2 for AdamW, suppose the trajectory of AdamW converges to fixed point (x, (cid:98)m, (cid:98)v), so that (cid:98)m = (x) and (cid:98)v = (x)2. Passing to the limit ϵ 0, the fixed-point condition gives (x) (x) + ϵ1 + λx sgn(f (x)) + λx = 0. Taking inner products with (x) yields (x)1 + λx, (x) = 0, which shows that is KarushKuhnTucker (KKT) point of the constrained optimization problem min xRd (x) s.t. 1 λ (3) by Lemma 3.8 of [XL24]. Intuitively, AdamW normalizes the gradient to its coordinatewise sign at stationarity and then balances it against the linear pull of the decoupled weight decay, which enforces box constraint on the parameters. [XL24] formalize this intuition and show that whenever the iterates of AdamW converge, the limit point is KKT point of the box-constrained problem (3). However, this guarantee holds only under the assumption of convergence, and AdamW is not known to converge in general. We remark that we can adapt this argument for another, more heuristic insight into why optimizers with cautious weight decay perform unbiased optimization. Suppose Adam with cautious weight decay reaches fixed point, so that (x) (x) + ϵ = λI(f (x)x 0)x. For fixed point of Lion-K with cautious weight decay, we have K(f (x)) = λI(K(f (x))x 0)x. In either situation, casework on the signs of the update and shows that both sides must be 0. It follows that (x) = 0 for Adam and K(f (x)) = 0 for Lion-K, and if is convex function that achieves unique minimum at 0 (e.g. norm), then this condition becomes (x) = 0 as well. Hence, the fixed-point analysis suggests that Adam and Lion-K with cautious weight decay find stationary point of the original objective ."
        },
        {
            "title": "D Lyapunov Functions",
            "content": "Throughout this section, vector variables are implicitly dependent on when clear from context, and we drop the subscript for notational simplicity. D.1 SGD SGD with cautious weight decay admits the continuous-time dynamics = (x) λI(f (x)x 0)x, which has Lyapunov function H(x) = (x), since dH dt = (x), (x) λI(f (x)x 0)x = (x)2 2 λ (cid:13) (cid:13)(f (x)x)+(cid:13) (cid:13)1 0. D.2 SGD with momentum When SGD is equipped with momentum [SMDH13] and cautious weight decay, the continuous-time dynamics becomes = λI(mx 0)x = β(f (x) m), which has Lyapunov function H(x, m) = βf (x) + 1 2 m2 2 + λ (cid:13) (cid:13)(mx)+(cid:13) (cid:13)1 , since dH dt = βf (x) + λI(mx 0)m, λI(mx 0)x + + λI(mx 0)x, β(f (x) m) = (cid:10)λI(mx 0) + β1, m2(cid:11) λ(β + λ) (cid:13) (cid:13)(mx)+(cid:13) (cid:13)1 0. D.3 Lion-K We assume that is convex and satisfies sgn(K(m)) = sgn(m) for all Rd. This assumption is mild and that holds for every example of given by [CLLL24]. The continuous-time dynamics of Lion-K without gradient enhancement is given by = K(m) λx = αf (x) γm. (4) [CLLL24] showed that this system has Lyapunov function H(x, m) = αf (x) + γ λ K(λx) + K(λx) + K(m) m, λx , 23 thereby elucidating the origin of the K(λx) regularization term. However, when equipped with cautious weight decay, (4) becomes = K(m) λI(mx 0)x = αf (x) γm and admits Lyapunov function H(x, m) = αf (x) + K(m) + λ (cid:13) (cid:13)(mx)+(cid:13) (cid:13)1 , (5) (6) which corresponds to optimizing the original objective . To see that (6) is Lyapunov function for (5), note that dH dt = αf (x) λI(mx 0)m, K(m) λI(mx 0)x + K(m) λI(mx 0)x, αf (x) γm = K(m) λI(mx 0)x, (λI(mx 0) + γ1)m = λI(mx 0) + γ1, K(m)m λ(λ + γ) (cid:13) (cid:13)(mx)+(cid:13) (cid:13)1 0. D.4 Adam The continuous-time limit of Adam with cautious weight decay yields the system of ordinary differential equations (cf. [BB21]) = (1 exp(αt))1m (cid:112)(1 exp(γt))1v + ϵ1 λI(mx 0)x = α(f (x) m) = γ(f (x)2 v). (7) We assume that 0 < γ 4α, which is satisfied by standard implementations of Adam in practice. This system admits the Lyapunov function H(x, m, v, t) = αf (x) + (cid:13) (cid:13) (cid:13) (cid:13) αtm2 γtv + ϵ1) 2( (cid:13) (cid:13) (cid:13) (cid:13)1 + λ (cid:13) (cid:13)(mx)+(cid:13) (cid:13)1 , (8) where αt := (1 exp(αt))1 and γt := (1 exp(γt))1. To see that (8) is Lyapunov function for (7), note that is lower bounded by αf and = xH, + mH, + vH, + t αf (x) + λI(mx 0)m, αtm γtv + ϵ1 λI(mx 0)x (cid:29) dH dt (cid:28) = + (cid:28) αtm γtv + ϵ1 + λI(mx 0)x, α(f (x) m) 24 (cid:42) (cid:29) αt (cid:0) γtm2 γtv + ϵ1(cid:1)2 , γ(f (x)2 v) (cid:43) 4 γ exp(γt)γt γtv (cid:43) , 1 + λ(α + λ)(mx)+ + 2α exp(αt)( m2 2 (cid:42) (cid:42) = (α1 + λI(mx 0)) (cid:42) + αtγm2 4 (cid:0) γtv γtv + ϵ1(cid:1)2 , (cid:43) γtv + ϵ1)(cid:1)2 γtv + ϵ1) α1 2 (cid:0)α1 ( αtm2 γtv + ϵ1 (cid:42) m2 2α exp(αt)( (cid:17) α 1 λI(mx 0), α α 2(exp(αt) 1) + = (cid:28)(cid:16) γ 4 (cid:28)(cid:18) γ 0, (cid:29) αtm2 γtv + ϵ1 γ 4(exp(γt) 1) (cid:19) αtγ (cid:0) 4 γtv + ϵ1) α1 2 (cid:0)α1 ( (cid:43) γtm2f (x)2 γtv + ϵ1(cid:1)2 , 1 γ exp(γt)γt γtv + ϵ1)(cid:1)2 (cid:28) αt(2αtα exp(αt) γtγ exp(γt))m2 γtv 4( γtv + ϵ1) αtm2 γtv + ϵ1 (cid:29) 1 λI(mx 0), (cid:43) , 1 (cid:29) , 1 where the first inequality drops some nonpositive terms and uses inequality uses γ α α 2(exp(αt) 1) + γ 4(exp(γt) 1) 0 γtv γtv + ϵ1 and the second for 0 < γ 4α and > 0. Remark 3. Cautious weight decay can be seen as an attempt to fix the asymptotic instability of AdamW via Lyapunov function. Consider the simplified continuous-time AdamW dynamics (9) , (x)2 (cid:29) = λx = (x) = (x)2 and the function H(x, m, v) = (x) + (cid:13) (cid:13) (cid:13) (cid:13) m2 2 (cid:13) (cid:13) (cid:13) (cid:13)1 + m, λx . By straightforward computation, dH dt (cid:28) = (x) + λm, (cid:29) λx + + λx, (x) (cid:28) m2f (x)2 (cid:29) (cid:28) + m2 3 2 4v (cid:18) (cid:29) , 1 3 4 (cid:28)(cid:18) λ + λ + = = 3 2 (cid:13) (cid:13) (cid:13) (cid:13) 4v 1 4 m2f (x)2 + λ(λ + 1)mx + 3 4 (cid:19) (cid:13) (cid:13) (cid:13) (cid:13) λ(λ + 1) m, (cid:19) m2 (cid:13) m2 (cid:13) (cid:13) (cid:13)1 Note that is not guaranteed to be lower bounded and dH is not guaranteed to be nonnegative, dt since m, has unknown sign. This motivates the introduction of mask I(mx 0) to the weight decay term and slight adjustment to so that the result is Lyapunov function for (9). Remark 4. For expositional clarity, we treat the ODEs and Lyapunov candidates in this section as smooth, even though the dynamics include the discontinuous indicator function I(ux 0). fully rigorous analysis can be developed by interpreting the systems in the sense of differential inclusions, specifically, using Filippovs framework [Fil88], and by applying specialized tools from nonsmooth Lyapunov stability theory to obtain convergence guarantees [SP94, BC99]. (cid:13) (cid:13) (cid:13) (cid:13)1 3 2 ."
        },
        {
            "title": "E Deferred Proofs",
            "content": "We assume the setting of Theorem 1. Lemma 1. For all N, (cid:13) (cid:13) (cid:13) (cid:13) (cid:98)mt (cid:98)vt + ϵ1 (cid:13) (cid:13) (cid:13) (cid:13) (cid:115) 1 β1 1 β2 =: C. Proof. It suffices to work in an arbitrary coordinate i. Let := [ (cid:98)mt]i, := [(cid:98)vt]i, and gt := [gt]i. By expanding the update rule for and v, we obtain = 1 β1 1 βt (cid:88) k[t] βtk 1 gk and = 1 β2 1 βt (cid:88) k[t] βtk 2 g2 k. Now by CauchySchwarz, m2 = (1 β1)2 1)2 (1 βt (1 β1)2 1)2 (1 βt 1 βt 2 1 β2 1 βt 2 1 β2 k[t] 1 βt 1 1 β1 (cid:88) (cid:18) β2 1 β2 (cid:19)tk (1 β1)2 1)2 (1 βt 1 βt 2 1 β2 (cid:88) k[t] βtk = 1 β1 1 β2 1 βt 2 1 βt 1 1 β1 1 β . The conclusion follows from + ϵ (cid:115) 1 β1 1 β2 . Lemma 2. For all N, xt R, (cid:98)mt G, and (cid:98)vt G2 for some constants and G. We can choose 1 without loss of generality. Proof. Since the iterates are bounded and is L-smooth by Assumption 1, there exist constants and such that xt and (xt) for all N. It follows that (cid:98)mt and (cid:98)vt G2. Fact 1 (Lemma F.1, [BWAA18]). For all N, [d], and α1, α2, . . . , αt R, (cid:88) k[t] 2 αk([gk]i [f (xk)]i) σ2 nbatch α2 k. (cid:88) k[t] Lemma 3. For all N, E[f (xt) mt1] βt 1Gd + β1ηLd(C + λR) 1 β1 + σd (cid:112)nbatch(1 + β1) . Proof. Note that mt (xt) = βt 1f (x1) + (cid:88) βtk 1 k[t1] (f (xk) (xk+1)) + (1 β1) βtk 1 (gk (xk)). (cid:88) k[t] (10) By smoothness, Lemma 1, and Lemma 2, we have (xk) (xk+1)1 By Jensens inequality and Fact 1, (xk) (xk+1)2 xk+1 xk2 ηLd(C + λR). (11) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:88) k[t] βtk 1 (cid:12) (cid:12) (cid:12) ([gk]i [f (xk)]i) (cid:12) (cid:12) (cid:12) (cid:118) (cid:117) (cid:117) (cid:117) (cid:116)E (cid:88) βtk 1 ([gk]i [f (xk)]i) 2 k[t] (cid:118) (cid:117) (cid:117) (cid:116) σ2 nbatch (cid:88) (β2 1)tk k[t] σ (cid:112)nbatch(1 β2 1) . (12) Taking E[1] of (10) and applying (11) and (12), E[f (xt) mt1] βt 1 (x1)1 + β1ηLd(C + λR) 1 β1 + (1 β1)E βt 1Gd + β1ηLd(C + λR) 1 β1 + σd (cid:112)nbatch(1 + β1) , as desired. Lemma 4. For all N, (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:88) βtk k[t] (gk (xk)) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)1 (cid:20) (cid:28) (xt), mt (cid:98)vt + ϵ (cid:29)(cid:21) (cid:104) (cid:105) (xt)2 2 + ϵ + 1G2d βt ϵ + β1ηGLd(C + λR) (1 β1)ϵ + σGd ϵ(cid:112)nbatch(1 + β1) . Proof. We have (cid:28) (xt), (cid:29) = mt (cid:98)vt + ϵ1 (cid:28) (xt) (cid:98)vt + ϵ1 1 + ϵ 1 + ϵ , (xt) mt (xt) (cid:29) (xt)2 2 + (xt) 2 + (cid:28) (xt) (cid:98)vt + ϵ1 (cid:13) (xt) (cid:13) (cid:13) (cid:13) (cid:98)vt + ϵ1 (cid:13) (cid:13) (cid:13) (cid:13) (cid:29) , (xt) mt (xt) mt1 The result follows by (cid:13) (cid:13) (cid:13) Lemma 5. For all m, g, R, (xt) (cid:98)vt+ϵ1 (cid:13) (cid:13) (cid:13) ϵ and Lemma 3 . (I(mx 0) I(gx 0))x I(mg 0)x. Proof. If = 0, then the inequality is trivially valid, so suppose = 0. We proceed by casework on the sign of mg. If mg > 0, then and have the same sign, and the conditions mx 0 and gx 0 are equivalent. Thus I(mx 0) I(gx 0) = 0, and the inequality holds. If mg 0, then I(mg 0) = 1. It remains to show (I(mx 0) I(gx 0))x x, which follows upon realizing I(mx 0) I(gx 0) {1, 0, 1}. Lemma 6. For all N, E[ (xt), I(mtxt 0)xt] (cid:13) (cid:13)(f (xt)xt)+(cid:13) (cid:13)1+βt 1GRd+ β1ηLRd(C + λR) 1 β + σRd (cid:112)nbatch(1 + β1) . Proof. We have (xt), I(mtxt 0)xt = (xt), I(xtf (xt) 0)xt + (I(mtxt 0) I(xtf (xt) 0))xt = (xt), (I(xtf (xt) 0) I(mtxt 0))xt (cid:13) (xt), (I(xtf (xt) 0) I(mtxt 0))xt (cid:13) (xt), I(mtf (xt) 0)xt (cid:13) (cid:13)1 , (cid:13)(f (xt)xt)+(cid:13) (cid:13)(f (xt)xt)+(cid:13) (cid:13)1 (cid:13)(f (xt)xt)+(cid:13) (cid:13)1 (13) where the fourth line uses Lemma 5. Taking the expectation of (13) conditioned on xt and expanding the inner product, E[f (xt), I(mtf (xt) 0)xt xt] = (xt), E[I(mtf (xt) 0) xt]xt = = (cid:88) i[d] (cid:88) i[d] (cid:88) i[d] (cid:88) i[d] [f (xt)]i[xt]i E[I([mt]i[f (xt)]i 0) xt] [f (xt)]i[xt]i Pr([mt]i[f (xt)]i 0 xt) [f (xt)]i[xt]i Pr([f (xt)]i [mt]i [f (xt)]i xt) [xt]i E[[f (xt)]i [mt]i xt] (14) where the fifth line uses Markovs inequality. Taking the expectation of (14) and applying Lemma 3, E[f (xt) mt1 xt], E[ (xt), I(mtxt 0)xt] (cid:13) (cid:13)(f (xt)xt)+(cid:13) (cid:13)1+βt 1GRd+ β1ηLRd(C + λR) 1 β + σRd (cid:112)nbatch(1 + β1) , as desired. Theorem 1. Under Assumptions 1 and 2, let 0 β1 β2 < 1, λ 0, ϵ > 0, and ηt = η > 0, and suppose xt is updated using Algorithm 2. Then for all N, 1 (cid:88) t[T ] (cid:104) (xt)2 2 + λ (cid:13) (cid:13)(f (xt)xt)+(cid:13) (cid:13) (cid:105) K1 ηT + K2 + K3η + K4σ nbatch , where K1, K2, K3, and K4 are constants. Proof. Let := (xt+1) (xt) and δt := (cid:98)mt (cid:98)vt + ϵ1 + λI(mtxt 0)xt. 28 By smoothness, f (xt), xt+1 xt + xt+1 xt2 2 2 δt2 2 (cid:29) η2L 2 = η (xt), δt + (cid:98)mt (xt), = η (cid:28) (cid:98)vt + ϵ1 (cid:28) = η 1 βt 1 (xt), mt (cid:98)vt + ϵ1 ηλ (xt), I(mtxt 0)xt + η2L 2 δt2 2 (15) (cid:29) ηλ (xt), I(mtxt 0)xt + η2L 2 δt2 2 . Taking the expectation of (15) and applying Lemmas 1, 2, 4, and 6, E[t] η 1 βt 1 (cid:32) + ηλ (cid:104) (cid:105) (xt)2 2 + ϵ + 1G2d βt ϵ + β1ηGLd(C + λR) (1 β1)ϵ + (cid:13) (cid:13)(f (xt)xt)+(cid:13) (cid:13)1 + βt 1GRd + β1ηLRd(C + λR) 1 β + σGd ϵ(cid:112)nbatch(1 + β1) (cid:33) σRd (cid:112)nbatch(1 + β1) (16) (C2d + λ2R2d). + η2L 2 Rearranging (16), using 1 βt by gives 1 1 and 1, summing over iterations, and dividing both sides 1 (cid:88) t[T ] E[S(xt)] + ϵ ηT (f (x1) ) + + ϵ (cid:88) 1G2d βt ϵ + β1ηGLd(C + λR)(G + ϵ) (1 β1)ϵ t[T ] λ(G + ϵ) + + + σGd(G + ϵ) ϵ(cid:112)nbatch(1 + β1) β1ηλLRd(C + λR)(G + ϵ) 1 β1 t[T ] ηL(G + ϵ) 2 + (cid:88) βt 1GRd + λσRd(G + ϵ) (cid:112)nbatch(1 + β1) (C2d + λ2R2d) K1 ηT + K2 + K3η + K4σ nbatch , where the fourth line uses (cid:80) t[T ] βt 1 β1 1β1 and S(xt) := (xt)2 2 + λ (cid:13) (cid:13)(f (xt)xt)+(cid:13) (cid:13)1 K1 := (G + ϵ)(f (x1) ) β1Gd(G + ϵ) 1 β (cid:18) ϵ K2 := (cid:19) + λR K3 := K4 := β1Ld(C + λR)(G + ϵ) 1 β1 (cid:18) ϵ d(G + ϵ) 1 + β1 + λR (cid:19) . (cid:19) + λR + (cid:18) ϵ 1 2 Ld(C2 + λ2R2)(G + ϵ) Table 5: Hyperparameter configurations for the different model sizes. All models use an expansion factor of 8 and vocabulary size of 100,864. Hyperparameter 2.3B Model 986M Model 338M Model 111M Model Model Architecture Total Parameters Model Dimension Number of Layers Number of Heads Per Head Dimension Sequence Length Validation Setup Evaluation Batch Size Number of Eval Steps Evaluation Interval 2,321.38M 2048 18 8 256 1024 2 1000 steps 985.89M 1536 12 8 256 2048 512 4 1000 steps 338.44M 1024 8 8 128 2048 128 4 500 steps 110.55M 512 8 8 64 256 8 500 steps Model & Experiment Configurations We evaluate cautious weight decay (CWD) across two experimental setups: (1) transformer models ranging from 111M to 2.3B parameters, and (2) the OLMo-1B architecture. All models employ SwiGLU activations and rotary position embeddings (RoPE). To ensure fair comparison, we conduct extensive grid searches to optimize hyperparameters for each baseline optimizer (AdamW, Lion, and Muon) before applying CWD with identical settings. Table 5 details the scaled model configurations, Table 6 presents the OLMo-1B architecture, and the following subsection describes our hyperparameter search methodology. We conducted an extensive grid search to determine optimal hyperparameters for AdamW, Lion, and Muon optimizers. Our learning rate search employed quasi-logarithmic grid spanning four orders of magnitude from 1 105 to 1 101, with denser sampling in the critical 104 to 102 range where transformer models typically achieve optimal performance. The grid included standard decade values (e.g., 0.001, 0.01) as well as intermediate points within each logarithmic interval (e.g., 0.2, 0.3, 0.5, 0.8 scaled to each decade) to capture potential performance peaks between order-ofmagnitude boundaries, totaling 24 distinct learning rate values. For the learning rate schedule, we systematically evaluated warmup ratios of {0, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5}, corresponding to 0% to 50% of total training steps dedicated to linear warmup, followed by cosine annealing decay. For AdamW, we additionally performed grid search over the momentum parameters β1 and β2, evaluating combinations of β1 {0.85, 0.9, 0.95} and β2 {0.95, 0.98, 0.99, 0.995, 0.999}. Our experiments identified β1 = 0.9 and β2 = 0.95 as the optimal configuration. For Lion, we swept β1 {0.85, 0.9, 0.95} and β2 {0.95, 0.98, 0.99}, finding β1 = 0.9 and β2 = 0.95 to be optimal. For Muon, we similarly swept momentum coefficients and confirmed 0.95 as optimal. 30 Table 6: Model Architecture Configuration for OLMo-1B Hyperparameter Value Architecture Hidden dimension (dmodel) Number of attention heads Number of layers MLP ratio Vocabulary size Embedding size Max sequence length Attention Mechanism Positional encoding Flash attention Multi-query attention ALiBi Attention dropout Attention layer norm Model Components Activation function Block type Weight tying Include bias Layer norm type Layer norm with affine Residual dropout Embedding dropout Initialization Initialization method Initialization device 2048 16 16 8 50,280 50,304 2048 RoPE 0.0 SwiGLU Sequential Default 0.0 0.0 Mitchell CUDA"
        },
        {
            "title": "G Additional Experiment Results",
            "content": "This section provides supplementary experimental analyses that further characterize the behavior of cautious weight decay (CWD) across different optimizers and training dynamics. We present detailed visualizations of the mask activation patterns (Figure 6), showing how the fraction of parameters receiving weight decay evolves during training for both AdamW and Muon optimizers. Additionally, we include comprehensive loss and accuracy curves for all three optimizers (AdamW, Lion, and Muon) across model scales from 111M to 2.3B parameters (Figures 810), demonstrating consistent improvements with CWD. Finally, Figure 12 tracks the evolution of parameter norms throughout training, revealing that CWD maintains stable regularization comparable to standard weight decay while achieving superior performance. These results collectively illustrate that CWDs selective application of weight decay leads to more effective optimization without compromising 31 Figure 6: Masked weight-decay activation ratio rt := I(utxt>0)1 , i.e., the fraction of parameters for which the sign-selective mask is active at step (d = number of parameters). Left: AdamW; right: Muon. Insets zoom into the first 2.5k steps to highlight early-training behavior. Model: Qwen-0.6B [YLY+25] trained on The Pile [GBB+21]. training stability. Table 7: Final evaluation accuracy (higher is better) and loss (lower is better) comparisons across different model sizes, expanded to the full text width. Our proposed method is benchmarked against three baseline optimizers: AdamW, Lion, and Muon. The best result in each pair is bolded. GPT Model Size 338M 986M 2B GPT Model Size 338M 986M 2B Accuracy (higher is better) AdamW Lion Muon Ours 0.4232 0.4566 0. Base 0.4221 0.4556 0.4831 Ours 0.4230 0.4552 0.4839 Base 0.4211 0.4545 0. Ours 0.4256 0.4589 0.4873 Loss (lower is better) AdamW Lion Muon Ours 3.0059 2.7053 2.4881 Base 3.0136 2.7142 2.4973 Ours 3.0012 2.7171 2. Base 3.0121 2.7231 2.5012 Ours 2.9851 2.6873 2.4703 Base 0.4252 0.4575 0. Base 2.9896 2.6968 2.4803 32 Figure 7: Training dynamics for the 986M-parameter Gemma model. (a) 338M parameters (b) 986M parameters (c) 2B parameters Figure 8: Training dynamics across model scales with Muon optimizer. Baseline Muon (dashed) vs. Muon with CWD (solid). 33 (a) 338M parameters (b) 986M parameters (c) 2B parameters Figure 9: Training dynamics across model scales with AdamW optimizer. We compare baseline AdamW (dashed) against AdamW with CWD (solid) on models ranging from 338M to 2B parameters. (a) 338M parameters (b) 986M parameters (c) 2B parameters Figure 10: Training dynamics across model scales with Lion optimizer. Baseline Lion (dashed) vs. Lion with CWD (solid). Figure 11: Comparison of gradient norms using RMS normalization across four model sizes: 111M, 338M, 986M, and 2B. All models are trained under Chinchilla settings. CWD achieves lower gradient norms across all configurations. 35 Figure 12: Evolution of parameter norm (RMS) during training for 986M parameter model. We compare three optimization strategies: AdamW with weight decay 0.1 (orange), our proposed method (blue), and Adam without weight decay (green). Our method maintains stable parameter norms comparable to AdamW while achieving improved performance."
        }
    ],
    "affiliations": [
        "Google",
        "University of Texas at Austin"
    ]
}