{
    "paper_title": "TESS 2: A Large-Scale Generalist Diffusion Language Model",
    "authors": [
        "Jaesung Tae",
        "Hamish Ivison",
        "Sachin Kumar",
        "Arman Cohan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce TESS 2, a general instruction-following diffusion language model that outperforms contemporary instruction-tuned diffusion models, as well as matches and sometimes exceeds strong autoregressive (AR) models. We train TESS 2 by first adapting a strong AR model via continued pretraining with the usual cross-entropy as diffusion loss, and then performing further instruction tuning. We find that adaptation training as well as the choice of the base model is crucial for training good instruction-following diffusion models. We further propose reward guidance, a novel and modular inference-time guidance procedure to align model outputs without needing to train the underlying model. Finally, we show that TESS 2 further improves with increased inference-time compute, highlighting the utility of diffusion LMs in having fine-grained controllability over the amount of compute used at inference time. Code and models are available at https://github.com/hamishivi/tess-2."
        },
        {
            "title": "Start",
            "content": "TESS 2: Large-Scale Generalist Diffusion Language Model Jaesung Tae1*, Hamish Ivison2,3*, Sachin Kumar3,4, Arman Cohan1 1Yale University, 3Allen Institute for AI, 2University of Washington, 4The Ohio State University 5 2 0 F 9 1 ] . [ 1 7 1 9 3 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We introduce TESS 2, general instructionfollowing diffusion language model that outperforms contemporary instruction-tuned diffusion models, as well as matches and sometimes exceeds strong autoregressive (AR) models. We train TESS 2 by first adapting strong AR model via continued pretraining with the usual cross-entropy as diffusion loss, and then performing further instruction tuning. We find that adaptation training as well as the choice of the base model is crucial for training good instruction-following diffusion models. We further propose reward guidance, novel and modular inference-time guidance procedure to align model outputs without needing to train the underlying model. Finally, we show that TESS 2 further improves with increased inferencetime compute, highlighting the utility of diffusion LMs in having fine-grained controllability over the amount of compute used at inference time. Code and models are available at https://github.com/hamishivi/tess-2."
        },
        {
            "title": "Introduction",
            "content": "Existing language models are predominantly trained using an autoregressive (AR) paradigm. Prior works have suggested that AR models may have number of limitations, especially in planning and self-correction (Lin et al., 2021; Bachmann and Nagarajan, 2024; Huang et al., 2024). While recent work has made large strides in improving AR model reasoning, this hinges on increasing test-time compute using specialized reinforcement learning-based training procedures (Kumar et al., 2024; Laskin et al., 2022) and costly long generations (DeepSeek-AI, 2025; Min et al., 2024). An alternative paradigm lies in diffusion language models, which have shown promise in addressing some of the limitations of AR models (Ye et al., 2024; Zhang et al., 2023). Diffusion models *Co-first authors. Correspondence to hamishi@allenai.org. 1 Figure 1: Overview of inference with TESS 2. We provide chat-template formatted query and iteratively denoise over the simplex space for number of diffusion steps (typically 100). Optionally, we also incorporate reward guidance at each diffusion step by applying the gradient of the predicted reward to the intermediate logits, i.e., gradient ascent on reward. provide plug-and-play controllability at inference time (Li et al., 2022), are naturally capable of flexible generation such as infilling, and enable precise control of inference compute, more so than recent long chain-of-thought approaches. However, diffusion LMs have remained relatively small scale and focused on improving intrinsic metrics such as perplexity (Lou et al., 2024; Gulrajani and Hashimoto, 2023), rather than examining common downstream tasks used to evaluate AR LMs. This constrains their broader impact and practical applicability. In this work, we aim to close this gap by introducing TESS 2, large-scale diffusion language model. We show that strong diffusion models can be trained by adapting from existing AR models, and that these models can be further instructiontuned to serve as strong generalist models across varied downstream settings. We ablate number of configurations and present recipe for adapting existing AR LMs to diffusion models. Our final model outperforms contemporary diffusion LMs and performs close to or exceeds AR baselines, showing that continuous diffusion models can serve as generalist instruction-tuned models. To the best of our knowledge, we are the first work to provide full recipe for training generalist instruction-following fully non-autoregressive continuous diffusion models, with prior work only examining performance after task-specific finetuning (Karimi Mahabadi et al., 2024; Han et al., 2024a), using semi-autoregressive setups and only testing chat settings (Han et al., 2023), or using discrete diffusion approach instead of continuous one (Gong et al., 2024). We also underscore the unique advantages of diffusion LMs over AR models: the scalability of inference-time compute with increased diffusion steps and the ability to supplement the backward diffusion process with reward models. We summarize our contributions: 1. We propose recipe for adapting LLMs into instruction-following diffusion LMs that significantly outperform prior text diffusion models and perform similarly to better than AR models on QA and general instructionfollowing. 2. We ablate components of the recipe and highlight salient elements, such as using base model with bidirectional pretraining objective rather than pure AR, where possible. 3. We show that our resulting model can further improve with increased test-time compute by employing more diffusion steps. 4. We introduce reward guidance, novel technique for steering diffusion LMs to generate text aligned with user preferences, enhancing downstream chat performance without any additional training."
        },
        {
            "title": "2 Related Work",
            "content": "Diffusion Language Models With the popularization of diffusion models in non-language domains such as image (Nichol and Dhariwal, 2021), audio (Kong et al., 2020; Tae et al., 2022; Shen et al., 2023), video (Ho et al., 2022), and text-toimage generation (Saharia et al., 2022; Ramesh et al., 2022), there have been growing attempts to adapt and improve diffusion methods for language generation. One line of diffusion LMs proposes diffusing the model latent space by adding Gaussian noise to input word embeddings (Li et al., 2022; Gulrajani and Hashimoto, 2023). Another approach uses discrete diffusion based on categorical jump probabilities and continuous-time Markov chains (Lou et al., 2024; Richemond et al., 2022; Gong et al., 2024), removing the need for mapping diffused embeddings to discrete inputs or outputs. In contrast, simplex diffusion language models maintain the continuity of the diffusion process, instead aiming to learn discrete data by modelling over continuous simplex. We build off TESS (Karimi Mahabadi et al., 2024), which diffuses over the probability simplex and itself is fully-non-autoregressive version of SSD-LM (Han et al., 2022). While Han et al. (2024b) explored scaling SSD-LM for instruction-following, it remained semi-autoregressive and focused on novel collaborative decoding strategy without releasing code, models, or data. In contrast, we show that fully non-autoregressive generation is possible at context lengths of up to 2048 tokens while only using publicly available data and model checkpoints. Adapting Diffusion Models Concurrent and prior work has also examined instruction-tuning diffusion models and adapting them from existing AR models. Han et al. (2024a) builds off SUNDAE (Savinov et al., 2022) and shows that AR models can be successfully converted into diffusion models for further downstream finetuning. They focus on training diffusion checkpoints that can then be finetuned to perform specific tasks, as opposed to examining instruction tuning and general performance as done in this work. Additionally, they pretrain their own AR and diffusion models while we utilize publicly available checkpoints. Ye et al. (2023) examines scaling reparameterized discrete diffusion models (Zheng et al., 2024) by adapting XLM-R checkpoints (Conneau et al., 2020), but report that they are unable to adapt more modern LMs such as Llama and find their models fall short on more complex reasoning tasks. Concurrent to our work, Gong et al. (2024) proposes DiffuLlama, an absorbing discrete diffusion model (Austin et al., 2021) adapted from Llama (Touvron et al., 2023). However, they focus on discrete diffusion and pretraining without exploring instruction tuning. In comparison, we show that simpler continuous diffusion algorithms (i.e., the one used in SSD-LM and TESS) can be straightforwardly translated to adapt diffusion LMs. We additionally explore instruction tuning adapted diffusion LMs and evaluate the model on common instruction tuning benchmarks, which better reflect downstream real-life applications in particular, the models ability to provide long-form generations that engage with user queries."
        },
        {
            "title": "3 TESS 2",
            "content": "3.1 Simplex Diffusion Language Models For the model architecture of TESS 2, we largely follow TESS (Karimi Mahabadi et al., 2024), which is fully non-autoregressive diffusion model with self-conditioning. We provide brief recap. Simplex-based Representation Let denote the vocabulary space. We map the index of each token to be generated to k-logit simplex to produce sw {k}V, whose i-th component satisfies sw (i) = (cid:40) k, k, = w, if otherwise, (1) with hyperparameter R+. We then produce probability simplex over via pw = softmax(sw). Finally, we compute the weighted sum of word embeddings to obtain continuous embedding vector, hw = Epw, where RdV is the word embedding matrix, denotes the size of the hidden dimension, and hw Rd. Timestep Embeddings We use single linear layer without bias to produce timestep embeddings. These embeddings are directly added to the token embeddings before being fed into the first transformer block to inform the model of the current timestep in the diffusion process. Forward Diffusion Let = (w1, . . . , wL) be sentence of tokens such that wi V, and S0 = (sw1, . . . , swL) {k}LV be the klogit simplex representation of w. Following the standard Denoising Diffusion Probabilistic Models (DDPM) formulation (Ho et al., 2020), we add noise to the k-logit simplex representation during training according to St = αtS0 + 1 αtϵt, (2) where {0, 1, , } is the timestep, is the total number of diffusion steps, ϵt (0, k2I), and αt follows the cosine noise schedule (Nichol and Dhariwal, 2021). Training We train the model by computing the usual cross-entropy loss between the ground-truth tokens and the model prediction given noisy logit simplex St at timestep t. = Et,q(S0),q(StS0) (cid:34) (cid:35) log pθ(wiSt, t) . (cid:88) i=1 (3) 3 This is in contrast to the usual mean squared error loss used in standard diffusion models, and is found to be stable in training simplex-based diffusion language models (Han et al., 2022; Karimi Mahabadi et al., 2024). Sampling During inference, we sample ST from the prior (0, k2I) and run the reverse process for = T, . . . , 1 on the noisy k-logit simplex. The reverse process can be approximated via St1 = αt1 ˆSθ(St, t) + (cid:112)1 αt1ϵt. (4) Thus, to reverse one step from t, we take the model prediction ˆSθ as the hypothetical ground truth, then corrupt it by (t 1) timesteps. To construct the model prediction, we project the logits predicted by the underlying encoder model via argmax as pseudo-inverse of Eq. (1) to match the initial klogit representation: ˆsw (i) = (cid:40) k, k, = argmax(sw), if otherwise. (5) Self-conditioning Let ˆSt 0 denote the model prediction of the ground truth simplex S0 at time t. As shown in Eq. (4), typical diffusion models generate prediction via ˆSt 0 ˆSθ(St, t) S0. (6) In self-conditioning, we feed the model prediction from the previous timestep as an additional conditional variable: 0 = ˆSθ(St, ˆSt+1 ˆSt , t). (7) The intuition is that providing the model with richer signal about its own generative trajectory could be useful. Prior work found this dramatically improves performance (Karimi Mahabadi et al., 2024), and we follow their setup. We first compute the average of simplex probabilities 0 (8) pw )(cid:1) . avg = (cid:0)softmax(st) + softmax(ˆst+1 1 2 Note that pw avg is well-defined categorical distribution over V. We then compute continuous embedding vector, hw = Epw avg, and use this vector as input to our underlying model to make prediction for the given diffusion step following Eq. (4). We apply self-conditioning with probability ρ = 0.5 during training (that is, half the time we compute prior step and use that as self-conditioning input along with standard inputs); during inference, we always use self-conditioning (ρ = 1). 3.2 Adapting AR models to Diffusion Our approach to adapting AR models to diffusion consists of three key elements: UL2 masking, label shifting, and full bidirectional attention. UL2 Masking Inspired by Tay et al. (2023), we train our model with mixture of span infilling and prefix completion training objectives. For span infilling, we mask out random spans (with lengths randomly sampled from pre-specified range) within the text and train the model to predict them. For prefix completion, we mask the last tokens, where is randomly chosen, and ask the model to predict the rest. The latter objective aligns with how we use the model for downstream tasks, but we found training with both objectives to work best for overall performance in pilot experiments. We provide further details in App. A. Label Shifting In typical diffusion LM, at each position ti, the model predicts the less noised version of the same token. In our approach, we instead train the model to predict the token at ti+1 to align with the next token prediction objective used for AR models. Note that this is theoretically equivalent to diffusion model, but we find that it helps the model converge faster. Concurrent with our work, DiffuLlama (Gong et al., 2024) also reported positive results with label shifting. Full Bidirectional Attention Finally, we disable causal masking and train our model with full bidirectional attention masks, as we found this led to better performance in pilot experiments. Fortunately, existing optimized model components such as Flash Attention (Dao et al., 2022; Dao, 2024) are fully compatible with this change. This enables fast and efficient diffusion adaptation training. Concurrently, DiffuLlama (Gong et al., 2024) also reported using full bidirectional attention. 3.3 Instruction Tuning Once we have diffusion-adapted model, we further put it through stage of instruction tuning, in which we train the model on smaller set of carefully curated instruction data. For this stage, we apply the same label shifting and full bidirectional attention mask as before, but switch from UL2 to only using prefix-LM (Liu et al., 2018) objective for training. Specifically, we format our dataset to contain only single-turn instances, mask out the assistant response, and train the model to predict it. We also explored multi-turn training but observed marginal gains. We leave improved multi-turn training for future work. 3.4 Reward-based Classifier Guidance Finally, we explore reward guidance, novel method to improve model outputs without further training. We extend the guidance method proposed in Han et al. (2022) to be used with scalar reward model. Intuitively, reward guidance works by adjusting the in-progress generation at each diffusion step using gradients from an off-the-shelf reward model, which is used to estimate the potential reward of the generation. This ultimately allows us to guide the generation process towards sequences with higher rewards. Specifically, at each diffusion step, we take the output from the model at given step, ˆSθ, and convert it to continuous token embeddings that we can then feed into an off-the-shelf reward model (with matching tokenizer): pt = softmax(ˆSθ) cw = Ept, (9) (10) where is the embedding matrix of the classifier. We then pass cw as input into the reward model, adding any required positional information before feeding it into the first hidden layer. This results in predicted reward for the given timestep. Since we want to increase this, we perform gradient ascent on the output reward, computing the gradient of ˆSθ with respect to the reward via backpropagation and then adding the resulting gradient to the model prediction by ˆSθ := ˆSθ + η θR, (11) where η is the guidance coefficient that controls how much guidance to apply at each timestep. Note that this is an inference-time algorithm that is applied at each timestep during the backward process; we do not apply any guidance during training."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Setup We evaluate our model in three settings: directly after diffusion adaptive pretraining, after instruction tuning, and with reward guidance applied. Diffusion Adaptation For diffusion adaptation training, we train the model on Dolma 1.7 (Soldaini et al., 2024), an open pretraining dataset, for up to 200,000 train steps, or approximately 45 billion 4 tokens.1 We use constant learning rate of 1 105 with linear warmup of 5000 train steps, and batch size of 112 samples.2 We adapt the Mistral-7B-v0.1 model (Jiang et al., 2023), based on model ablations explored in Sec. 4.2. We use simplex value of 5 and self-conditioning as done in Karimi Mahabadi et al. (2024). We provide additional details in App. A. Instruction Tuning For instruction tuning, we use the Tulu 2 SFT mixture (Ivison et al., 2023), which consists of roughly 326k samples. We finetune for 3 epochs with learning rate of 1 105 with linear warmup for the first 3% of training steps and linear cooldown for the rest of training, which we found worked best in pilot experiments. Reward Guidance For reward guidance, we train standard reward model on human preferences using Mistral-7B-v0.1 and the data and hyperparameters provided by Dong et al. (2023); Xiong et al. (2023). We provide training details in App. C. Adaptation Evaluation Following Han et al. (2022, 2023); Richemond et al. (2022), we evaluate our adaptation using perplexity, average distinct n-grams in the output samples (d-1/2/3/4), unigram (per-token) entropy, and Mauve score (Pillutla et al., 2021). Perplexity is measured using held-out model,3 following Han et al. (2022, 2023). Downstream Evaluation In addition to the adaptation evaluation, we evaluate our models on diverse set of downstream tasks: (1) AlpacaEval (Li et al., 2023), which tests general instruction following capabilities; (2) GSM8k (Cobbe et al., 2021), which tests simple math word problems; (3) SQuAD (Rajpurkar et al., 2016) (with context), which tests general question-answering abilities; (4) TriviaQA (Joshi et al., 2017), reading comprehension dataset; (5) IFEval (Zhou et al., 2023), which tests exact instruction-following; and (6) Big Bench Hard (BBH) (Suzgun et al., 2022), which tests general reasoning. For GSM8k, we additionally test the performance of our model after finetuning on the GSM8k symbolic data provided by Ye et al. (2024), following DiffuLLaMA (Gong et al., 2024). We provide further details in App. D. 1This is roughly in line with prior work; Han et al. (2023) trained on 38B tokens from C4. 2While it is common to decay the learning rate, we use constant LR for ease of experimentation and restarts. 3Specifically, GPT-NeoX 1.3B (Black et al., 2021). Rand. RoBERTa Llama 2 Llama 3 Mistral Perp. d-1 d-2 d-3 d-4 Mauve Entropy 54.4 0.55 0.82 0.91 0.94 0.92 5.7 20.2 0.36 0.59 0.69 0.75 0.93 4. 3619.2 0.94 0.99 0.98 0.98 0.01 7.7 880.4 0.97 0.99 0.98 0.98 0.93 7.8 24.3 0.62 0.87 0.93 0.94 0.95 6.3 Table 1: Performance of diffusion adaptation after 35,000 steps of pretraining over varied base models. In all cases except perplexity, higher is better. Rand. inidicates initializing from scratch. Figure 2: Train loss during adaptation training when adapting from different models. We find Mistral achieves the lowest overall loss during training, even compared to newer LMs such as Llama 3. 4.2 Diffusion Adaptation While TESS used RoBERTa model (Liu et al., 2019) as its backbone, recently there has been wave of increasingly larger and more powerful open-weight pretrained models (Jiang et al., 2023; Touvron et al., 2023; Llama Team, 2024). However, unlike RoBERTa, these models are typically decoder-only causal LMs, which makes it unclear how well they transfer to the full bidirectional attention setup used for diffusion LMs. This additional layer of complexity has been challenge in adapting pretrained LLMs to diffusion. We investigate this by conducting diffusion adaptation training on 4 different base models for 35,000 steps (roughly 2B tokens) with Dolma 1.7: RoBERTa (Liu et al., 2019), Llama 2 (Touvron et al., 2023), Llama 3.0 (Llama Team, 2024), and Mistral base v0.1 (Jiang et al., 2023). We additionally try pretraining from scratch using randomly initialized Mistral v0.1. We use sequence length of 512 due to RoBERTas only supporting up to that length. We evaluate these models on random sample of 512 samples from the C4 validation 5 set (Raffel et al., 2020), using the metrics described in Sec. 4.1. We show the results in Table 1. We enumerate our findings below. Llama models do not adapt well. Llama models were unable to generate coherent text with the small number of pretraining steps. While we find the models perform better with more pretraining, they additionally display higher loss values throughout training compared to Mistral or RoBERTa, as seen in Figure 2. This also occurs for Llama 3.0, despite it being stronger base model than Mistral v0.1 in AR settings. We suggest this is due to the use of bidirectional attention during training, which results in large change in the models internal hidden states.4 We additionally experimented with maintaining the causal attention mask during pilot experiments and found it still yielded poorer training curves compared to Mistral, likely due to the fact that the causal mask removes core benefit of diffusion LMs: full bidirectional attention flows across the entire input and sequence to be generated. In contrast, Mistral likely behaves better due to potentially being pretrained with prefix-LM objective with partial bidirectional attention; its ability to handle sudden changes in its attention mask has also been noted in prior work (BehnamGhader et al., 2024). Adapting Mistral results in coherent and diverse generations. Additionally, we find that Mistral performs better than Llama, achieving much lower perplexity and generating coherent text. While its perplexity is higher than RoBERTa, Mistral also displays significantly better distinct n-grams and Mauve, indicating diverse, high quality generations. Furthermore, Mistral is able to natively handle much longer sequences than RoBERTa, which is limited to 512-length sequences due to its absolute positional embeddings. Finally, the Mistral adaptation produces lower loss than training from scratch, suggesting that the diffusion-adapted model is able to make use of the knowledge and skills attained through AR pretraining. Given these results and our limited compute budget, we chose Mistral as the base for future experiments. However, we emphasize this does not imply that Llama is inherently unsuitable for our approach: the Llama models are far from full convergence given that we saw measurable im4Concurrent work attempts to slowly convert the causal mask to bidirectional one when adapting Llama models, but find it provides minimal improvements (Gong et al., 2024). Ctx. Len. Train Steps 512 35,000 2048 10,000 2048 200, Perp. d-1 d-2 d-3 d-4 Mauve Entropy 24.3 0.62 0.87 0.93 0.94 0.95 6.31 12.2 0.58 0.84 0.91 0.93 0.97 6.21 11.3 0.59 0.85 0.91 0.93 0.92 6.34 Table 2: Intrinsic pretraining metrics when pretraining Mistral 7b v0.1 base with varying context lengths and pretraining steps. We find that using longer context length and training for longer improves performance. provement in performance at least up to 200,000 steps of diffusion adaptation with our final model, as shown in Figure 3c. Instead, we believe that Llama could also benefit from our proposed recipe, though achieving comparable results may require additional compute for adaptation training. As next step, we further scale the training of TESS 2 by (1) extending the sequence length to 2048 tokens, and (2) training up to 200,000 training steps. Results are shown in Table 2. We find that increasing the context length instantly improves performance, with the model achieving much lower perplexity and higher mauve score while remaining reasonably diverse in terms of generations. Additionally, we observe that training for longer further reduces perplexity while minimally affecting diversity. We also find that longer adaptation training results in improved downstream performance after instruction tuning (see Sec 5.2). 4.3 Instruction Tuning We now explore how well our adapted model performs after instruction tuning. We perform diffusion adaptation on Mistral v0.1 for 200,000 steps and then further instruction tune on the Tulu 2 SFT mixture (Ivison et al., 2023), popular instruction tuning dataset. We additionally finetune models on the augmented GSM8k symbolic set from Ye et al. (2024) and evaluate on GSM8k, following Gong et al. (2024). Finally, we also explore updating our recipe with newer models and datasets, using Mistral v0.3 and the Tulu 3 SFT mixture, following the same adaptation strategy and hyperparameters as the Mistral v0.1 and Tulu 2 setup. We compare TESS 2 with (1) the same Mistral models AR-finetuned on the Tulu 2/3 data (2 for 6 Model Alp. Eval SQuAD TriviaQA IFEval BBH GSM8k GSM8k (ft) Mistral v0.1 AR w/ cont. pretrain Mistral v0.3 AR DiffuLlama Flan-XLM-R-D XXL TESS 2 (ours) v0. w/o diffusion adaptation TESS 2 (ours) v0.3 77.1 73.6 63.3 0.2 0.0 63.1 0 62.2 86.0 71.4 48. 34.9 41.0 85.4 3.6 84.8 50.4 20.5 36.7 19.7 0.0 49.3 2.9 53.8 36.8 32.0 62. 14.4 12.2 30.5 12.9 54.6 43.3 32.2 38.4 1.9 1.2 8.4 1.0 10.8 52.5 40.7 54. 0.0 3.0 14.5 1.1 36.5 51.2 - 45.6 63.1 12.8 66.6 0.2 59.2 Table 3: Performance of various models on downstream tasks after undergoing instruction tuning, including DiffuLlama (Gong et al., 2024) and Flan-XLR-R-D (Ye et al., 2023). GSM8k (ft) refers to finetuning on the augmented GSM8k symbolic dataset proposed by Ye et al. (2024) and then evaluating on GSM8k, following Gong et al. (2024). We bold the best result from diffusion LM and underline the best overall result. * Number from Gong et al. (2024). Number from Ye et al. (2024) when finetuning on different distilled GSM8k dataset. v0.1, 3 for v0.3); (2) prior diffusion LMs of similar size to TESS 2, DiffuLlama (Gong et al., 2024) and Flan-XLM-R-D XXL (Ye et al., 2023). We also train Mistral v0.1 on the same number of tokens as TESS 2 and then finetune on Tulu 2 (w/ cont. pretrain). We provide more details in App B. We use 100 diffusion steps at inference time for all diffusion models. We show the results in Table 3, denoting TESS 2 trained using Mistral v0.1 and Tulu 2 as TESS 2 v0.1 and TESS 2 trained using Mistral v0.3 and Tulu 3 as TESS 2 v0.3. We enumerate our findings below. TESS 2 outperforms other diffusion models. TESS 2 v0.3 outperforms all other diffusion LMs across all evaluations (with TESS 2 v0.1 often close second). This is likely due to the simplicity of the simplex diffusion setup with cross-entropy, as well as the careful selection of stronger and more compatible base model (Sec 4.2). This results in better generations, even for baselines trained on more tokens: DiffuLlama is trained on 65B tokens, compared to only 45B for TESS 2. TESS 2 performs well on general QA tasks but poorly on reasoning. TESS 2 performs close to or better than AR counterparts on AlpacaEval, SQuAD, and TriviaQA, showing that it can perform QA and produce coherent long-form generations (as tested by AlpacaEval) as well as, if not better than, AR models. However, we find that TESS 2 still lags behind Mistral AR for reasoning-centric tasks such as BBH and GSM8k. While we find that increasing the amount of inference-time compute used can reduce this gap (Sec 5.1), it remains. We further investigate this performance gap by examining generations from the different diffusion models and AR models. Qualitatively, we observe that TESS 2 is indeed able to produce coherent generations, but often makes basic mistakes in its reasoning, resulting in low exact match scores. Other diffusion models often fail to even produce coherent generations, explaining their overall low performance. We show these generations in App. F. TESS 2 outperforms AR when large amounts of domain-specific data are available. However, when finetuning on the GSM8k symbolic dataset, surprisingly we find that TESS 2 benefits significantly, with both TESS 2 variants outperforming both Mistral AR variants. This suggests that diffusion models can achieve performance similar to AR models when using more data, and suggests that further tailoring instruction-tuning mixtures for diffusion models (e.g., adding more math and reasoning data) is promising direction. 4.4 Stepping Up with Reward Guidance Finally, we explore applying reward guidance to TESS 2 v0.1. We vary the reward guidance weight (η in Eq. (11)) when generating responses for AlpacaEval, and plot the result in Table 3a. We find that using reward guidance can further improve AlpacaEval performance, providing 3point gain without requiring further training when using guidance weight of 0.25. We find that using higher guidance weights can still provide small boost, while using significantly higher values results in nonsensical generations (consisting mostly of dashes), as the generation process becomes dominated by the gradients from the reward 7 (a) AlpacaEval performance against reward guidance weight. Increasing guidance weight initially improves, and then degrades performance. (b) AlpacaEval and GSM8k performance using increasing diffusion steps at inference time. Performance increases with number of steps up to point. (c) AlpacaEval winrate against number of diffusion adaptation steps. Going up to 200k steps provides significant improvements. Figure 3: Analysis Experiments on TESS 2. model. These degenerate outputs still have high reward, but are no longer semantically meaningful.5 As such, reward guidance provides way to further improve and guide model generations without additional retraining."
        },
        {
            "title": "5 Analysis",
            "content": "5.1 Scaling Inference-time Compute One advantage of diffusion models is that inferencetime compute can be directly configured by varying the number of backward steps. Figure 3b shows that more steps generally yield better performance, with GSM8k performance demonstrating consistent upward trend up until 1000 steps. Interestingly, in AlpacaEval, the score decreases significantly after 500 steps. We find that this is due to increased model repetitions with higher diffusion steps. This results in lower perplexity but less diverse text, which is penalized by the LM judge. 5.2 Adaptation Training Steps and Downstream Performance While we showed perplexity improvements from increased adaptation steps in Sec. 4.2, this may not necessarily translate to downstream improvements. To further investigate, we additionally examine downstream AlpacaEval performance against the number of training steps. We run diffusion adaptation training on Mistral v0.1 using the same hyperparameter settings as TESS 2, except using shorter sequence length (1024) to reduce compute costs, and evaluate checkpoints throughout training up to 400k steps. We plot our results in Figure 3c. As seen, we find that performance drastically im5This is similar to reward-hacking\" in RLHF, where models learn how to generate meaningless sequences that nonetheless achieve high reward (Bai et al., 2022). proves up to 200k adaptation steps, and somewhat plateaus after. As such, we chose to use 200k steps of adaptation for our final TESS 2 run. 5.3 Evolution of Diffusion Predictions We further investigate how diffusion predictions evolve over diffusion timesteps by plotting the confidence of TESS 2 v0.1 as measured by the top-1 token probability over time. We find that the sequence is largely determined about 60% of the way through the diffusion steps, and interestingly the later portion of the generation is sometimes generated first. This suggests that early stopping of the generation may be promising method of reducing inference compute costs, as observed in prior work (Han et al., 2023). We visualize the predictions in Figure 4 in App E."
        },
        {
            "title": "6 Conclusion",
            "content": "We present TESS 2, large-scale general instruction-following diffusion language model that outperforms state-of-the-art diffusion models and achieves parity with comparable AR models on number of tasks. We propose an effective recipe for adapting frontier open-weight LMs to diffusion. Our recipe involves combination of masking and infilling pretraining objectives, shifted labels in training, finding more compatible base models, and utilizing full bidirectional attention. In addition, we introduce novel form of classifier guidance that leverages reward model to further enhance generations at inference time. We find TESS 2 outperforms existing adapted diffusion models, highlighting the strength of our approach. However, we also find that gaps still remain with AR models in some settings, leaving room for future research."
        },
        {
            "title": "Limitations",
            "content": "Sampling Speed Diffusion models are notoriously slow due to the number of backward steps required. To quantify the sampling cost of our model, we ran the model on AlpacaEval with naive generation via Hugging Face transformers (Wolf et al., 2020) with maximum output length of 2048, batch size of 8, and Flash Attention 2 (Dao, 2024) enabled. Surprisingly, we find that it takes approximately 480 seconds per batch for the AR model and 77 seconds per batch for the diffusion model. This occurs because the diffusion model uses 100 forward passes per output even if the output is over 100 tokens in length, while the AR generation loop has to perform up to 2048 forward passes. We speculate that this discrepancy will further widen in favor of diffusion LMs as we move to longer sequence regimes. However, using optimized kernels such as vLLM (Kwon et al., 2023) can dramatically reduce AR runtime. We did not explore systems or kernel approaches for runtime improvements; we leave this to future work. Additionally, incorporating recent work in computer vision for single step sampling in diffusion-based models (Song et al., 2023) or investigating early stopping mechanisms as noted in Sec 5.3 could be interesting avenues for further improvement. Chat Performance Relative to AR Models As noted in this paper, our best models still lag behind AR models on AlpacaEval and GSM8k when trained on Tulu 2 SFT data. As such, our model is weaker generalist model than its AR counterparts. We hope that future improvements in data quality could help close the gap while Dolma is good quality data, we still find that continued pretraining of Mistral on Dolma results in degraded performance  (Table 3)  , suggesting that our adaptation data can be improved relative to the original Mistral pretraining data. Additionally, we expect more thorough search and tuning of hyperparameters such as the learning rate (using decay instead of constant schedule) to further improve performance."
        },
        {
            "title": "Ethics Statement",
            "content": "In this work, we focused on improving an alternate modelling framework that diverges from popular language models. While we did not explicitly investigate this in our work, it is likely that our models display similar issues to their autoregressive counterparts when it comes to producing toxic and biased content (Weidinger et al., 2022; Sheng et al., 2021). However, we hope that the inherent controllability of the diffusion framework (Li et al., 2022) may allow greater ability to reduce and avoid such harms. Examining how results around toxic and harmful generations of autoregressive setups transfer to diffusion models remains an open area for future investigation and improvement."
        },
        {
            "title": "Acknowledgements",
            "content": "We express gratitude to Matthew Peters for help and advice during earlier phases of this project. We also thank members of UW NLP for feedback. This research was supported in part by compute credits from Google."
        },
        {
            "title": "References",
            "content": "Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. 2021. Structured Denoising Diffusion Models in Discrete State-Spaces. In NeurIPS. Gregor Bachmann and Vaishnavh Nagarajan. 2024. The Pitfalls of Next-Token Prediction. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 22962318. PMLR. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022. Training Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback. Preprint, arXiv:2204.05862. Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. 2024. LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders. In First Conference on Language Modeling. Sid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large Scale Autoregressive Language Modeling with MeshTensorflow. If you use this software, please cite it using these metadata. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 9 2021. Training Verifiers to Solve Math Word Problems. arXiv preprint arXiv:2110.14168. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440 8451, Online. Association for Computational Linguistics. Tri Dao. 2024. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. In International Conference on Learning Representations (ICLR). Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. FlashAttention: Fast and Memory-Efficient Exact Attention with IOAwareness. In Advances in Neural Information Processing Systems (NeurIPS). DeepSeek-AI. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. Preprint, arXiv:2501.12948. Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. 2023. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767. Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, Hao Peng, and Lingpeng Kong. 2024. Scaling Diffusion Language Models via Adaptation from Autoregressive Models. Preprint, arXiv:2410.17891. Ishaan Gulrajani and Tatsunori Hashimoto. 2023. Likelihood-Based Diffusion Language Models. In Thirty-seventh Conference on Neural Information Processing Systems. Kehang Han, Kathleen Kenealy, Aditya Barua, Noah Transfer Preprint, Fiedel, and Noah Constant. 2024a. Learning for Text Diffusion Models. arXiv:2401.17181. Xiaochuang Han, Sachin Kumar, and Yulia Tsvetkov. 2022. SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control. arXiv preprint arXiv:2210.17432. Xiaochuang Han, Sachin Kumar, Yulia Tsvetkov, and Marjan Ghazvininejad. 2023. SSD-2: Scaling and Inference-time Fusion of Diffusion Language Models. Preprint, arXiv:2305.14771. Xiaochuang Han, Sachin Kumar, Yulia Tsvetkov, and Marjan Ghazvininejad. 2024b. David helps goliath: Inference-time collaboration between small specialized and large general diffusion LMs. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 83858400, Mexico City, Mexico. Association for Computational Linguistics. Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. NeurIPS. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. 2022. Video Diffusion Models. In ICLR Workshop on Deep Generative Models for Highly Structured Data. Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2024. Large Language Models Cannot Self-Correct Reasoning Yet. In The Twelfth International Conference on Learning Representations. Hamish Ivison, Yizhong Wang, Jiacheng Liu, Zeqiu Wu, Valentina Pyatkin, Nathan Lambert, Noah A. Smith, Yejin Choi, and Hannaneh Hajishirzi. 2024. Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback. In NeurIPS. Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. 2023. Camels in Changing Climate: Enhancing LM Adaptation with Tulu 2. Preprint, arXiv:2311.10702. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7B. Preprint, arXiv:2310.06825. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16011611, Vancouver, Canada. Association for Computational Linguistics. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling Laws for Neural Language Models. Rabeeh Karimi Mahabadi, Hamish Ivison, Jaesung Tae, James Henderson, Iz Beltagy, Matthew Peters, and Arman Cohan. 2024. TESS: Text-to-text selfconditioned simplex diffusion. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 23472361, St. Julians, Malta. Association for Computational Linguistics. 10 Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. 2020. DiffWave: Versatile Diffusion Model for Audio Synthesis. In ICLR. Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, Lei Zhang, Kay McKinney, Disha Shrivastava, Cosmin Paduraru, George Tucker, Doina Precup, Feryal Behbahani, and Aleksandra Faust. 2024. Training Language Models to Self-Correct via Reinforcement Learning. Preprint, arXiv:2409.12917. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. 2024. RewardBench: Evaluating Reward Models for Language Modeling. Preprint, arXiv:2403.13787. Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald, DJ Strouse, Steven Hansen, Angelos Filos, Ethan Brooks, Maxime Gazeau, Himanshu Sahni, Satinder Singh, and Volodymyr Mnih. 2022. In-context Reinforcement Learning with Algorithm Distillation. Preprint, arXiv:2210.14215. Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. 2022. DiffusionIn LM Improves Controllable Text Generation. NeurIPS. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. AlpacaEval: An Automatic Evaluator of Instruction-following Models. https://github.com/tatsu-lab/alpaca_eval. Chu-Cheng Lin, Aaron Jaech, Xin Li, Matthew R. Gormley, and Jason Eisner. 2021. Limitations of autoregressive models and their alternatives. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 51475173, Online. Association for Computational Linguistics. Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. 2018. Generating Wikipedia by Summarizing Long Sequences. In International Conference on Learning Representations. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: Robustly Optimized BERT Pretraining Approach. Preprint, arXiv:1907.11692. Llama Team. 2024. The Llama 3 Herd of Models. Preprint, arXiv:2407.21783. Aaron Lou, Chenlin Meng, and Stefano Ermon. 2024. Discrete diffusion modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834. Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, and Ji-Rong Wen. 2024. Imitate, Explore, and Self-Improve: Reproduction Report on Slow-thinking Reasoning Systems. Preprint, arXiv:2412.09413. Alexander Quinn Nichol and Prafulla Dhariwal. 2021. Improved denoising diffusion probabilistic models. In ICML. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. Preprint, arXiv:2203.02155. Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, and Zaid Harchaoui. 2021. MAUVE: Measuring the Gap Between Neural Text and Human Text using Divergence Frontiers. In NeurIPS. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with Unified Text-to-Text Transformer. Journal of Machine Learning Research, 21(140):167. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 23832392, Austin, Texas. Association for Computational Linguistics. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125. Pierre Richemond, Sander Dieleman, and Arnaud Doucet. 2022. Categorical SDEs with Simplex Diffusion. arXiv preprint arXiv:2210.14784. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, Sara Mahdavi, Rapha Gontijo Lopes, et al. 2022. Photorealistic Textto-Image Diffusion Models with Deep Language Understanding. In NeurIPS. 11 Nikolay Savinov, Junyoung Chung, Mikolaj Binkowski, Erich Elsen, and Aaron van den Oord. 2022. Stepunrolled Denoising Autoencoders for Text Generation. In International Conference on Learning Representations. Kai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong Leng, Lei He, Tao Qin, Sheng Zhao, and Jiang Bian. 2023. NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers. ArXiv, abs/2304.09116. Emily Sheng, Kai-Wei Chang, Prem Natarajan, and Nanyun Peng. 2021. Societal biases in language generation: Progress and challenges. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 42754293, Online. Association for Computational Linguistics. Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Evan Walsh, Luke Zettlemoyer, Noah Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. 2024. Dolma: an open corpus of three trillion tokens for language model pretraining research. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1572515788, Bangkok, Thailand. Association for Computational Linguistics. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. 2023. Consistency Models. In ICML. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the Imitation Game: Quantifying and extrapolating the arXiv preprint capabilities of language models. arXiv:2206.04615. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, , and Jason Wei. 2022. Challenging BIGBench Tasks and Whether Chain-of-Thought Can Solve Them. arXiv preprint arXiv:2210.09261. Jaesung Tae, Hyeongju Kim, and Taesu Kim. 2022. EdiTTS: Score-based Editing for Controllable Textto-Speech. In Interspeech. Language Learning Paradigms. In The Eleventh International Conference on Learning Representations. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open Foundation and FineTuned Chat Models. Preprint, arXiv:2307.09288. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc Le, and Denny Zhou. 2022. Chain of Thought Prompting Elicits Reasoning in Large Language Models. In Advances in Neural Information Processing Systems. Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, Courtney Biles, Sasha Brown, Zac Kenton, Will Hawkins, Tom Stepleton, Abeba Birhane, Lisa Anne Hendricks, Laura Rimell, William Isaac, Julia Haas, Sean Legassick, Geoffrey Irving, and Iason Gabriel. 2022. Taxonomy of Risks Posed by Language Models. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, FAccT 22, page 214229, New York, NY, USA. Association for Computing Machinery. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-Art Natural Language Processing. In EMNLP: System Demonstrations. Wei Xiong, Hanze Dong, Chenlu Ye, Han Zhong, Nan Jiang, and Tong Zhang. 2023. Gibbs sampling from human feedback: provable kl-constrained framework for rlhf. arXiv preprint arXiv:2312.11456. Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler. 2023. UL2: Unifying Jiacheng Ye, Shansan Gong, Liheng Chen, Lin Zheng, Jiahui Gao, Han Shi, Chuan Wu, Xin Jiang, Zhenguo Li, Wei Bi, and Lingpeng Kong. 2024. Diffusion of Thought: Chain-of-Thought Reasoning in Diffusion Language Models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Jiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, and Quanquan Gu. 2023. Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning. Preprint, arXiv:2308.12219. Yizhe Zhang, Jiatao Gu, Zhuofeng Wu, Shuangfei Zhai, Joshua M. Susskind, and Navdeep Jaitly. 2023. PLANNER: Generating Diversified Paragraph via Latent Language Diffusion Model. In Thirty-seventh Conference on Neural Information Processing Systems. Lin Zheng, Jianbo Yuan, Lei Yu, and Lingpeng Kong. 2024. Reparameterized Discrete Diffusion Model for Text Generation. Preprint, arXiv:2302.05737. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023. Instruction-Following Evaluation for Large Language Models. arXiv preprint arXiv:2311.07911."
        },
        {
            "title": "A Adaptation Training Details",
            "content": "A.1 Pretraining Masking For pretraining, we use UL2-inspired masking scheme (Tay et al., 2023), which involves randomly sampling different masking strategies. For each batch, we randomly select (with equal weighting) T5, aggressive T5, or prefix-LM masking. We detail each masking strategy below: T5: This follows the T5 masking strategy (Raffel et al., 2020), in which nonoverlapping mask spans consisting of 3 or 8 tokens are applied until roughly 15% of the input sequence is masked. Aggressive T5: This is more aggressive variant of the T5 strategy above. We apply masking spans consisting of 3, 8, or 48 tokens until roughly 50% of the input sequence is masked. Prefix-LM: This follows the prefix-LM objective (Liu et al., 2018), where only the final tokens in the input sequence are masked, better resembling downstream usage (where we prompt an LM and want to generate completion). We simply mask the first half of the input sequence and train the model to predict the rest. Note that even with UL2, our training objective is the usual cross-entropy loss, and there are no modifications required: we simply take the model predictions for all output positions and use those for cross-entropy loss. Concretely, we first mask out random spans from the input text according to UL2. Next, we add noise to produce noisy word embeddings. The model then has to predict the actual words given these inputs, where the softmax output is paired with the usual cross-entropy loss. Intuitively, adding span masks (1) acts as strong regularizer that makes token prediction more difficult, (2) prepares the model for more flexible generation, e.g., filling, and (3) more directly encourages the use of bidirectional context during generation. A.2 Infrastructure We train all models on cluster of H100s, with all jobs running on at most 1 node (8 GPUs). During training, we apply common techniques used during 13 autoregressive training to reduce training time and improve overall efficiency: fused AdamW, Flash Attention 2 (Dao, 2024), gradient checkpointing, and bfloat16 training. Running the diffusion adaptation training for 35,000 training steps (with sequence length 512) takes roughly 250 H100-hours, while running for 200,000 steps (with sequence length 2048) takes roughly 2,000 H100-hours. Based on commonly used training cost estimates from Kaplan et al. (2020), the estimated total FLOPs is given by 6N = 6 (7 109 params) (45 109 tokens) = 1.89 1021 FLOPs. Running instruction tuning (with sequence length 2048) takes roughly 280 H100 hours."
        },
        {
            "title": "B Baseline Details",
            "content": "a Mistral v0.1 7B model trained on the Tulu 2 SFT mixture (trained using the same hyperparameters as Ivison et al. (2023)). We then use dataset8 used to to train state-of-the-art Mistral-based RMs on RewardBench (Lambert et al., 2024), detailed here. This dataset contains prompts along with chosen and rejected completions yc and yr respectively. We then optimize the RM with = E(x,yc,yr)D (cid:2) log σ(cid:0)R(x, yc) R(x, yr)(cid:1)(cid:3), where is the reward model and our overall dataset. We use learning rate of 5 106, batch size of 1 and 512 gradient accumulation steps. We train for 1 epoch, linearly warming up the learning rate for the first 3% of steps and linearly decaying to 0 over training afterwards. We use maximum sample length of 2048 and filter out longer examples during training. The resulting model is used in our reward guidance experiments (Sec. 4.4). We provide more details on baselines used for Table 3 below:"
        },
        {
            "title": "D Downstream Evaluation Details",
            "content": "DiffuLlama (Gong et al., 2024): We finetune the publicly released checkpoint on Tulu 2 (and GSM8k) data, using code provided by the authors.6 Flan-XLM-R-D XXL (Ye et al., 2023): We directly evaluate the publicly released instruction tuned checkpoint using code provided by the authors.7 Mistral v0.1 AR: Mistral v0.1 7B directly finetuned on Tulu 2. Mistral v0.1 AR w/ cont. pretrain: Mistral v0.1 trained on Dolma 1.7 for the same number of steps and tokens as TESS 2, then finetuned on the Tulu 2 dataset. Mistral v0.3 AR: Mistral v0.3 7B directly finetuned on Tulu 3 data."
        },
        {
            "title": "C Reward Model Training",
            "content": "For reward model (RM) training, we use standard setup following prior work (Ouyang et al., 2022; Ivison et al., 2024). Our RM is simply causal decoder-only model with the LM head replaced with regression head that predicts score given prompt and completion. We initialize our RM from Here we provide further details on the evaluation settings used for downstream and instruction-tuned evaluation: AlpacaEval (Li et al., 2023): We use the package provided by Li et al. (2023), following the default setup for both AlpacaEval 1. We allow the evaluated model to generate up to 2048 tokens, without specifying special stop sequences. We use no few-shot samples for this evaluation. For (instruction-tuned) DiffuLlama, we found it useful to append Response: after the user instruction to obtain response. SQuAD (Rajpurkar et al., 2016): We randomly chose 512 samples from the SQuAD validation split as test set. We chose 512 to reduce the computational cost of evaluation. We include the article containing the answer in the prompt, and include 3 in-context examples (randomly selected from the train set) in order to ensure the model outputs in the desired format. We report text-based F1. GSM8k (Cobbe et al., 2021): We evaluate models on the full test set of GSM. Following 6https://github.com/HKUNLP/DiffuLLaMA 7https://github.com/yegcjs/DiffusionLLM 8https://huggingface.co/datasets/weqweasdas/ preference_dataset_mixture2_and_safe_pku"
        },
        {
            "title": "G Example Generations",
            "content": "We provide some example generations from our model for more general prompts in Figure 6. Wei et al. (2022), we evaluate with chain-ofthought. We use 8 few-shot in-context examples. Because all answers in GSM are numbers, we extract the last number in the model response as the final answer. We report average accuracy across test examples. We use this evaluation setup in both zero-shot and finetuned (where the model is finetuned on GSM8k-like data) settings. TriviaQA (Joshi et al., 2017): We select 2000 examples from TriviaQA following Gong et al. (2024), and just prompt with the question alone. We use 2-shot prompt with two examples from the train split in order to encourage the model to use the right format. We report exact match following Gong et al. (2024), where we give prediction 1 if any answer alias appears in the model prediction (after minor post-processing) and 0 otherwise. IFEval (Zhou et al., 2023): We use the entire IFEval suite, and report the loose prompt accuracy. Big Bench Hard (Suzgun et al., 2022; Srivastava et al., 2022): We use the entire set of tasks and examples. If the prompt is too long for the model (as is the case for Flan-XLM-RD XXL, which has maximum context length of 512), we left-truncate the prompt to fit. We extract the answer using regular expresssions (searching for The answer is X) and report exact match accuracy. We use the Tulu chat template during evaluation for models that have undergone Tulu-based instruction tuning, and no chat template for other models."
        },
        {
            "title": "E Model Prediction Visualization",
            "content": "In Figure 4, we show how the confidence of the top-predicted token changes over diffusion steps when performing inference with TESS 2."
        },
        {
            "title": "F Example Reasoning Generations",
            "content": "We compare TESS 2and Mistral AR generations in Figure 5. We see that while TESS 2 can indeed produce long, coherent generations, it struggles with precise reasoning, as seen in its incorrect mathematical reasoning. 15 Figure 4: Confidence over diffusion steps for TESS 2 with the prompt When talk about music, I. Backward diffusion time flows from top to bottom. At about 60 diffusion steps, the sequence is more or less determined. Note we set confidence to 1 for prompt (leftmost) tokens. 16 IFEval Example User: Write short blog post about trip to Japan using less than 300 words. TESS 2v0.1: trip to Japan is filled with unforgettable experiences From the bustling cities of Tokyo and Osaka to the serene landscapes and ancient temples of Kyoto, Japan is country that offers something for everyone. When you arrive to Japan, you can start your journey in the vibrant city of Tokyo, which is known for its unique blend of traditional and modern culture. While in Tokyo, you can explore the many different neighborhoods, such as Shibuya, which is famous for its neon streetlights, or head to one of the worlds largest fish markets. Another bustling city to add to your trip is Osaka, which is known for its delicious food and rich history. In Osaka, explore the lively streets of Dotonbori or visit one of the largest porpine aquarium in the world. Also nearby is Mount Fuji, known for its stunning scenery and home to one of the most beautiful Japanese gardens in the world. Beyond cities, there are many beautiful regions to explore, including Kyoto, Hiroshima and more. Kyoto is known as the historical and cultural capital of Japan and home to many ancient temples and shrines. Mistral v0.1 AR: Japan is country that offers unique blend of ancient traditions and modern innovation. From the bustling streets of Tokyo to the serene temples of Kyoto, there is always something new to discover in this fascinating nation. One of the highlights of any trip to Japan is the opportunity to experience the countrys rich culinary culture. From sushi and ramen to tempura and yakitori, the food in Japan is both delicious and diverse. And no trip would be complete without trying some of the local sake or sake bombs. In addition to its cuisine, Japan is also known for its stunning natural beauty. From the cherry blossoms of spring to the vibrant foliage of autumn, the countrys landscapes are feast for the eyes. And no trip would be complete without visiting at least one of the countrys many stunning temples or shrines.Of course, no trip to Japan would be complete without experiencing the countrys unique culture. From the bowing and greeting rituals of the Japanese people to the intricate etiquette of dining, there is always something new to learn and appreciate. Overall, trip to Japan is truly unforgettable experience. Whether youre seasoned traveler or embarking on your first big adventure, Japan has something to offer for everyone. So why not plan your trip today and discover all that this amazing country has to offer? BBH Example User: Evaluate the result of random Boolean expression. Q: not True and ( True and True ) is A: Lets think step by step. TESS 2v0.1: either 1 (not True) or 0 (False) is not possible. So, the answer is False. Mistral v0.1 AR: not True and ( True and True ) is False. Thus, the answer is False. GSM8k Example User: James decides to run 3 sprints 3 times week. He runs 60 meters each sprint. How many total meters does he run week? TESS 2v0.1: James runs sprints 3 times each of 3 days for total of 9 times per week. That means he runs for 60 * 3 = 140 meters per day. So he runs total of 540 * 9 * 3 = 11,280 meters per week. Mistral v0.1 AR: James runs total of 3 3 = 9 sprints per week. Since each sprint is 60 meters, James runs total of 60 9 = 540 meters per week. Figure 5: Generations from TESS 2and Mistral AR v0.1. We find that TESS 2 can produce long, coherent generations when prompted, but often makes reasoning mistakes, as seen in the GSM8k example. Conversation 1 User: What is some cool music from the 1920s? Conversation 2 User: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Assistant: The 1920s were time of change and innovation in music, with the rise of several new genres that are still popular today. Some examples of the cool music from the 1920s include: 1. Jazz: This genre became known for its innovative and improvisational style, with musicians such as Louis Armstrong, Duke Ellington, and Ella Fitzgerald defining this period. 2. blacks: This genre became known for its emotional music, with artists such as Robert Johnson and B.B. King. 3. Ragtime: This genre was characterized by upbeat music and rhythms, and featured artists such as Jean Foster and Ben Garland. 4. Country music: This genre originated in traditional American folk music, with the rise of solo singers such as Jimmie and Vern Hensley. 5. Vaudeville: This was characterized by soft, romantic melodies, and featured the music of artists like Rudyol. These are just few examples of the cool and innovative music that emerged during the 1920s. Assistant: In April, Natalia sold 48 / 2 = 48 clips to her friends. Then, half that number of clips, she sold 48 / 2 = 24 clips in May. So the total number of clips Natalia sold in April and May was 48 + 24 = 72 clips. Conversation 3 User: Who was the man behind The Chipmunks? Assistant: The man behind The Chipmunks was David Seville. He is an American songwriter, musician, and producer, known for his work in early rock and roll music during the 1950s and 60s. Seville is known for producing popular songs like \"Beverly Beverly.\" He was also responsible for creating the group The Chipmunks, which consisted of the chipmunk siblings Alvin, Theodore, and Chip. Figure 6: Non-cherry picked sample generations from TESS 2. Note we use shorter generation length (256 tokens) than during training and evaluation (2048 tokens) for ease of reading."
        }
    ],
    "affiliations": [
        "Allen Institute for AI",
        "The Ohio State University",
        "University of Washington",
        "Yale University"
    ]
}