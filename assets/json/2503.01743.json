{
    "paper_title": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs",
    "authors": [
        "Abdelrahman Abouelenin",
        "Atabak Ashfaq",
        "Adam Atkinson",
        "Hany Awadalla",
        "Nguyen Bach",
        "Jianmin Bao",
        "Alon Benhaim",
        "Martin Cai",
        "Vishrav Chaudhary",
        "Congcong Chen",
        "Dong Chen",
        "Dongdong Chen",
        "Junkun Chen",
        "Weizhu Chen",
        "Yen-Chun Chen",
        "Yi-ling Chen",
        "Qi Dai",
        "Xiyang Dai",
        "Ruchao Fan",
        "Mei Gao",
        "Min Gao",
        "Amit Garg",
        "Abhishek Goswami",
        "Junheng Hao",
        "Amr Hendy",
        "Yuxuan Hu",
        "Xin Jin",
        "Mahmoud Khademi",
        "Dongwoo Kim",
        "Young Jin Kim",
        "Gina Lee",
        "Jinyu Li",
        "Yunsheng Li",
        "Chen Liang",
        "Xihui Lin",
        "Zeqi Lin",
        "Mengchen Liu",
        "Yang Liu",
        "Gilsinia Lopez",
        "Chong Luo",
        "Piyush Madan",
        "Vadim Mazalov",
        "Ali Mousavi",
        "Anh Nguyen",
        "Jing Pan",
        "Daniel Perez-Becker",
        "Jacob Platin",
        "Thomas Portet",
        "Kai Qiu",
        "Bo Ren",
        "Liliang Ren",
        "Sambuddha Roy",
        "Ning Shang",
        "Yelong Shen",
        "Saksham Singhal",
        "Subhojit Som",
        "Xia Song",
        "Tetyana Sych",
        "Praneetha Vaddamanu",
        "Shuohang Wang",
        "Yiming Wang",
        "Zhenghao Wang",
        "Haibin Wu",
        "Haoran Xu",
        "Weijian Xu",
        "Yifan Yang",
        "Ziyi Yang",
        "Donghan Yu",
        "Ishmam Zabir",
        "Jianwen Zhang",
        "Li Lyna Zhang",
        "Yunan Zhang",
        "Xiren Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning. This achievement is driven by a carefully curated synthetic data recipe emphasizing high-quality math and coding datasets. Compared to its predecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of 200K tokens to better support multilingual applications, as well as group query attention for more efficient long-sequence generation. Phi-4-Multimodal is a multimodal model that integrates text, vision, and speech/audio input modalities into a single model. Its novel modality extension approach leverages LoRA adapters and modality-specific routers to allow multiple inference modes combining various modalities without interference. For example, it now ranks first in the OpenASR leaderboard to date, although the LoRA component of the speech/audio modality has just 460 million parameters. Phi-4-Multimodal supports scenarios involving (vision + language), (vision + speech), and (speech/audio) inputs, outperforming larger vision-language and speech-language models on a wide range of tasks. Additionally, we experiment to further train Phi-4-Mini to enhance its reasoning capabilities. Despite its compact 3.8-billion-parameter size, this experimental version achieves reasoning performance on par with or surpassing significantly larger models, including DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 3 4 7 1 0 . 3 0 5 2 : r Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs Microsoft"
        },
        {
            "title": "Abstract",
            "content": "We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning. This achievement is driven by carefully curated synthetic data recipe emphasizing highquality math and coding datasets. Compared to its predecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of 200K tokens to better support multilingual applications, as well as group query attention for more efficient long-sequence generation. Phi-4-Multimodal is multimodal model that integrates text, vision, and speech/audio input modalities into single model. Its novel modality extension approach leverages LoRA adapters and modality-specific routers to allow multiple inference modes combining various modalities without interference. For example, it now ranks first in the OpenASR leaderboard to date, although the LoRA component of the speech/audio modality has just 460 million parameters. Phi-4-Multimodal supports scenarios involving (vision + language), (vision + speech), and (speech/audio) inputs, outperforming larger vision-language and speech-language models on wide range of tasks. Additionally, we experiment to further train Phi-4-Mini to enhance its reasoning capabilities.1. Despite its compact 3.8-billion-parameter size, this experimental version achieves reasoning performance on par with or surpassing significantly larger models, including DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B."
        },
        {
            "title": "Introduction",
            "content": "The Phi family of models [AJA+24, AAB+24] have shown that carefully curated and synthesized data enables Small Language Models (SLMs) to achieve highly competitive performance despite having significantly smaller number of parameters. These models demonstrate comparable results to much larger models. Building on the success of the Phi family of language models, we extend their capabilities to handle additional modalities such as vision and audio, achieving significant progress akin to private models like GPT [HLG+24], Claude [Ant24], and Gemini [TGL+24]. In this report, we introduce Phi-4-Multimodal, unified multimodal SLM that supports multiple inference modes combining various modalities (e.g., text-only, text + image, speech/audio, speech + image) within single model checkpoint. Phi-4-Multimodal employs novel mixture of LoRAs technique, enabling multimodal capabilities by integrating modality-specific LoRAs while keeping the base language model entirely frozen. Our findings show this technique outperforms existing approaches (e.g., cross-attention designs [ADL+22, AI23]) and achieves comparable performance to fully fine-tuned hanyh@microsoft.com, youki@microsoft.com 1Please note that reasoning-enhanced Phi-4-Mini is separate model and currently in preview stage and will not be released concurrently with Phi-4-Mini and Phi-4-Multimodal. 1 models on multimodal benchmarks. Additionally, the design of Phi-4-Multimodal is highly extensible, allowing seamless integration of new LoRAs to support additional modalities without impacting existing ones. Our training process comprises multiple stages, including language training (encompassing both pretraining and post-training) and then expansion of the language backbone to vision and speech/audio modalities. For the language model, we train Phi-4-Mini using high-quality, reasoning-rich text data. Notably, we include curated, high-quality code datasets to enhance performance on coding tasks. Once the language model training is complete, we freeze the language model and implement our Mixture of LoRAs technique to proceed with the multimodal training stage. Specifically, we train two additional LoRA modules alongside modality-specific encoders and projectors to enable vision-related tasks (e.g., vision-language and vision-speech) and speech/audio-related tasks (e.g., speech-language). Both of them contain pretraining and post-training stages for modality alignment and instruction finetuning, respectively. We also explore the reasoning potential of Phi-4-Mini to create compact yet powerful model that rivals substantially larger state-of-the-art reasoning systems, such as DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B [GYZ+25]. The key contributions of this model are listed below. 1. Unified Multi-Modality Support: In contrast to existing methods [Tea25b, CWW+24] that employ separate models for different modalities, Phi-4-Multimodal is designed as unified model capable of efficiently handling multiple modality scenarios. By leveraging the Mixture of LoRAs [HSW+22], Phi-4-Multimodal extends multimodal capabilities while minimizing interference between modalities. This approach enables seamless integration and ensures consistent performance across tasks involving text, images, and speech/audio. 2. Remarkable Language Performance for the size: The language models achieve state-ofIt the-art performance in natural language understanding and generation for its size category. demonstrates exceptional reasoning and mathematical capabilities, making it well-suited for complex problem-solving and knowledge-based tasks. 3. Outstanding Code Understanding and Generation for the size: The language models achieve state-of-the-art performance on code-related tasks within its size category. The model excels at tasks such as code synthesis, debugging, and documentation generation, empowering developers and aiding in software engineering workflows. 4. Superior Multi-Modal Capabilities for the size: The model delivers state-of-the-art performance across multi-modal tasks for its size category, demonstrating robust integration of diverse data types. This includes tasks that involve combining images with text and speech modalities, enabling multi-modal reasoning. 5. Exceptional Speech and Audio Performance: The model achieves strong performance especially on multilingual speech recognition and translation tasks, and is the first open-sourced model with speech summarization capability. 6. Enhanced Reasoning Capabilities: The reasoning-optimized version of Phi-4-Mini demonstrates superior reasoning abilities for model in its size category."
        },
        {
            "title": "2 Model architecture",
            "content": "The Phi-4-Mini series comprises two state-of-the-art small models: language model (Phi-4-Mini) and multimodal model (Phi-4-Multimodal) that integrates language, vision, and speech/audio modalities. All Phi-4-Mini models use the tokenizer o200k base tiktoken 2 with vocabulary size of 200,064 intended to support multilingual and multimodal input and output more efficiently. All models are based on decoder-only Transformer [VSP+17] and support 128K context length based on LongRoPE [DZZ+24a]."
        },
        {
            "title": "2.1 Language model architecture",
            "content": "Phi-4-Mini and Phi-4-Multimodal share the same language model backbone. Phi-4-Mini consist of 32 Transformer layers with hidden state size of 3,072 and tied input / output embedding which reduces the memory consumption significantly while providing much wider coverage of vocabularies compared Phi-3.5. Each Transformer block includes an attention mechanism based on Group Query Attention (GQA) [ALTdJ+23], which optimizes key and value memory (KV cache) usage for long-context generation. Specifically, the model employs 24 query heads and 8 key/value heads, reducing KV cache consumption to one-third of its standard size. Additionally, in the RoPE configuration [SAL+24], fractional RoPE dimension is used, ensuring that 25% of the attention head dimension remains positionagnostic. This design supports smoother handling of longer contexts. To determine the peak learning rate, we follow [BBC+24] with LR(D) = BD0.32 where is constant we tune for this specific model and is the total number of training tokens. We fit by tuning across = 12.5B, 25B, 37.5B, 50B."
        },
        {
            "title": "2.2 Multimodal model architecture",
            "content": "To integrate vision as an input modality, numerous vision-language models have been developed, including the LLava series [LLWL24, LLL+24, LZG+24], QWenVL series [BBY+23, WBT+24], InternVL series [CWT+24, CWW+24, CWC+24], InternLM-XComposer series [ZDW+23, DZZ+24b], Molmo [DCL+24], and NVLM [DLW+24]. Similarly, for audio input, notable contributions include Qwen2-Audio [CXY+24], InternLM-XComposer2.5-Omnilive [ZDC+24], InternOmni, Mini-Omni [XW24], and GLM4-Voice [ZDL+24]. However, in order to enable modality-specific functionality, these multimodal models generally require fine-tuning the base language model, which often diminishes its original language capabilities. Consequently, supporting diverse input signals without compromising quality necessitates deploying multiple modelsa particularly challenging limitation for resource-constrained devices. To address this, LLamaVision [DJP+24] adopts strategy inspired by Flamingo [ADL+22], adding extra cross-attention layers while preserving the core language model. However, this approach will result in reduced performance on vision-language benchmarks compared to fully fine-tuned models. To fill the performance gap, NVLM [DLW+24] further explores hybrid framework, employing joint supervised fine-tuning with high-quality text SFT data. Yet, this approach only examines limited language benchmarks and does not address additional training stages often required after SFT. We adopt the mixture of LoRAs design for our Phi-4-Multimodal architecture to support variant multi-modality use cases. Different LoRAs are trained to handle interactions between different modalities. Our Phi-4-Multimodal supports vast range of tasks, including single/multiple images QA/summarization, video QA/summarization, vision-speech tasks, speech QA/summarization/translation/recognition, and audio understanding, while maintains the original language model performance. 2https://github.com/openai/tiktoken 3 Figure 1: overview of the Multimodal architecture for Phi-4-Multimodal"
        },
        {
            "title": "2.2.1 Modality Details",
            "content": "Vision modality. The vision modality is implemented with an image encoder, projector to align the vision and text embeddings and LoRA adaptor. The vision encoder is based on SigLIP-400M that is finetuned with LLM2CLIP [HWY+24] on large scale image-text pairs with resolution 448 448 . The projector is 2-layer MLP that maps the vision features dimension to the text embedding dimension. Extra LoRA is added on all the linear layers in the language decoder and only deployed in the supervised fine tuning (SFT) stage. The image encoder and projector introduce 440M model parameters while the vision adapter LoRAV consumes another 370M model parameters. In order to enable the model to process images with diverse resolution effectively and efficiently, we proposed new dynamic multi-crop strategy. Specifically, given target image, we first compute the crop number for each side by dividing the original size by the crop-size, i.e. , where H, W, are the image height, width and crop size respectively. If the total crop number is within the maximum number, i.e., 16 in the pretraining stage and 36 in SFT, we just slightly resize the image to let it fit the size given by the computed image crops. Otherwise, we will leverage the strategy proposed in InternVL2 [CWW+24] that find the crop number by matching the best aspect ratio. Compared to InternVL2, the key benefits of our strategy is to avoid resizing one small image (e.g., 28 448) to unreasonable large size when looking for the closest image aspect ratio. Speech and Audio Modality: The speech/audio inputs we used are 80-dim log-Mel filter-bank features with the frame rate of 10ms. To enable Phi-4-Multimodal speech and audio functions, we connect pre-trained audio encoder and Phi-4-Mini through an audio adapter. In addition, LoRA is applied on the language decoder to improve the performance of speech/audio benchmarks while preserving the text capability. The introduced modules for the speech/audio modality include: An audio encoder, which consists 3 convolutions layers and 24 conformer blocks [GQC+20] with 1024 attention dimensions, 1536 feed-forward dimensions, and 16 attention heads. The convolution layers contribute to sub-sampling rate of 8, and thus 80ms token rate for the language decoder. An audio projector, which is 2-layer MLP that maps the 1024-dim speech features to the text embedding space of 3072 dimensions, similar to the vision projector. LoRAA that has been applied to all attention and MLP layers in Phi-4-Mini with rank of 320. The audio encoder and projector introduce 460M parameters while LoRAA consumes another 460M parameters. Note that the speech token rate is 80ms, indicating 750 tokens for 1-minute audio."
        },
        {
            "title": "2.2.2 Training Pipeline",
            "content": "The multimodal training stages include vision training, speech/audio training and vision-speech joint training. Vision Training. The overall training pipeline for multimodal learning consists of vision training, speech and audio training, and joint vision-audio training. Vision training follows four-stage process: 1) Projector Alignment stage: initially, only the projector is trained using caption data to align vision and text embeddings while preserving the pretrained representation of the vision encoder. 2) Joint Vision Training stage: Next, the projector and vision encoder are jointly trained on the full vision pretraining dataset to enhance key vision capabilities, such as OCR and dense understanding. 3) Generative VisionLanguage Training stage: LoRA is then deployed on the language decoder and trained alongside the vision encoder and projector using curated single-frame SFT data, equipping the model with generative capabilities for vision-language inputs. 4) Multi-Frame Training stage: Finally, the model is trained on multi-frame SFT data with the vision encoder frozen, extending the context length coverage to 64k and enabling multi-image and temporal understanding. Speech and Audio Training. With the Phi-4-Mini language model, we conduct two-stage paradigm for speech and audio training, also known as speech/audio pre-training and post-training. In the pre-training stage, we use large-scale automatic speech recognition (ASR) data to align the audio encoder and Phi-4-Mini in the semantic space. In this stage, the encoder and projector is updated with learning rate of 4e-5 for 50k steps while the language decoder is frozen. We initialize the audio encoder with pre-trained encoder from the attention-based encoder decoder (AED) ASR model. After the pre-training stage, the model can only perform the ASR task. To unlock the instruction following capability of Phi-4-Multimodal for variety of speech and audio tasks, we continue to train the model with about 100M curated speech and audio SFT samples (after weighted up) as the speech post-training stage. Please refer to Section 3.4.2 for data details. In speech/audio post-training, the audio encoder is frozen. We update the audio projector and LoRAA with learning rate of 1e-4 for another 50k steps. We consider different maximum audio lengths for different tasks in post-training. For speech summarization task, we train up to 30-minute audio (22.5k tokens). For other tasks, the maximum audio exposed in training is 30s (375 tokens). If we consider the 128k context length for language decoder, theoretically Phi-4-Multimodal can support maximum 2.8 hours of audio as out of the box inference. It is worth noting that we have not fine tuned the model on such long audio data and it may need further fine tuning to practically support such use cases. Vision-speech Joint Training. The vision-speech joint training is conducted after vision posttraining and speech post-training. We freeze the language base model, audio encoder, and audio projector, while finetuning the vision adapter LoRAV , vision encoder, and the vision projector. In this stage, we train the model mainly on vision-speech SFT data but we also include mixture of language and vision post-training data to maintain the corresponding performance. Reasoning Training Recent studies have suggested that training robust reasoning model only requires small amount of high-quality data, such as LIMO [YHX+25] and S1K [MYS+25]. However, we propose fundamentally different training paradigm for SLM: first, we need to conduct pre-training phase on extensive reasoning data to capture general reasoning chains, and then perform careful fine-tuning on curated SFT or preference data. The continued training of Phi-4-Mini for reasoning proceeds in three distinct stages. First, building on Phi-4-Mini, the model is pre-trained on approximately 60 billion reasoning CoT tokens generated by frontier reasoning LLMs, after which rejection sampling is employed to filter out incorrect outputs. This allows the reasoning extension of Phi-4-Mini to learn the reasoning chains produced by these models. In the second stage, the model is fine-tuned on smaller but carefully curated dataset of around 200K high-quality CoT samples, chosen to cover diverse domains and varying difficulty levels. Finally, in the third stage, we label filtered incorrect outputs as dis-preferred and their corrected counterparts as preferred, compiling new dataset of 300K preference samples for DPO training."
        },
        {
            "title": "3.1.1 Pre-training data",
            "content": "Compared with Phi-3.5-Mini, we improved the quality of the pre-training data from several key aspects: 1. Better data filtering: By using an enhanced quality classifier, which is trained on larger curated dataset consisting of cleaner positive and negative samples, we end up with better filtering quality across multiple languages with various aspects (e.g. toxic, obscure, scientific, etc.), leading to more comprehensive and controllable filtering strategy overall. 2. Better math and coding data: For the math and coding data, we have augmented our original data with specific instruction-based math and coding data set. This enhancement has resulted in effective results in math, coding and reasoning. 3. Better synthetic data: we incorporated Phi-4 synthetic data [AAB+24] into this model training with the same processing and decontamination. 4. Better data mixture: With the better classifiers, we re-tuned the data mixture with ablation experiments. Especially we increased the ratio for the reasoning data. That gives us boost for the model quality. With these techniques, we built the 5 trillion pre-training data corpus, which is larger and in higher quality compared to the Phi-3.5-Mini."
        },
        {
            "title": "3.1.2 Post-training data",
            "content": "Compared to Phi-3.5-Mini, Phi-4-Mini includes significantly larger and more diverse set of function calling and summarization data. Additionally, we synthesize substantial amount of instructionfollowing data to enhance the models instruction-following capabilities. For coding, we incorporate extensive code completion data, including tasks that require the model to generate missing code in the 6 middle of an existing code snippet. This challenges the model to understand both the requirements and the existing context, leading to significant performance improvements."
        },
        {
            "title": "3.1.3 Reasoning training data",
            "content": "We generate large volume of synthetic chain-of-thought (CoT) data from larger reasoning models, covering diverse domains and difficulty levels. During sampling, we employ both rule-based and modelbased rejection methods to discard incorrect generations and feed them back for resampling. Also, we label correct sampled answers as preferred generations and incorrect ones as dis-preferred, and create the DPO data. This data has been utilized exclusively for the experimental reasoning model and has not been applied to the officially released checkpoint Phi-4-Mini."
        },
        {
            "title": "3.2 Vision-language training data",
            "content": "The Phi-4-Multimodal models pre-training phase involves rich and varied dataset, encompassing interleaved image-text documents, image-text pairs, image grounding data, synthetic datasets from OCR of PDFs and realistic images, and synthesized datasets for chart comprehension. During this phase, the models primary focus is on predicting the next token, concentrating solely on text tokens and disregarding any loss associated with image tokens. The pre-training process involves total of 0.5T tokens, combining both visual and textual elements. Additionally, the maximum image resolution is capped at 1344x1344, as most training images are smaller than this size. For supervised fine-tuning (SFT), we utilized combination of text SFT dataset, publicly available multimodal instruction tuning datasets, and large-scale in-house multimodal instruction tuning datasets that we developed. These datasets span diverse domains and tasks, including general natural image understanding, chart, table, and diagram comprehension and reasoning, PowerPoint analysis, OCR, multi-image comparison, video summarization, and model safety. Collectively, the multimodal SFT data comprises approximately 0.3T tokens."
        },
        {
            "title": "3.3 Vision-speech training data",
            "content": "For vision-speech data, Phi-4-Multimodal model is trained on diverse set of synthetic vision-speech data, covering single-frame and multi-frame scenarios. Specifically, we reuse subset of vision-language SFT data and run in-house text-to-speech (TTS) engine to convert the user queries from texts to audios. This subset is carefully selected to avoid certain datasets where the queries are not suitable to read out in speech. We also measure the quality of the synthetic speech by transcribing the audio with in-house ASR model and calculating the word error rate (WER) between original text and transcription. Our final vision-speech data is generated with the WER-based filtering to ensure the quality."
        },
        {
            "title": "3.4 Speech and Audio Training Data",
            "content": "The training data for speech/audio functions can be categorized into two types: 1) pre-training data with ASR transcriptions to provide strong alignment between the speech and text modalities; 2) posttraining data to unlock the instruction-following capability of Phi-4-Multimodal with the speech/audio modality involved. The post-training data covers variety of tasks, including automatic speech recognition (ASR), automatic speech translation (AST), speech question answering (SQA), spoken query question answering (SQQA), speech summarization (SSUM), and audio understanding (AU)."
        },
        {
            "title": "3.4.1 Pre-training Data",
            "content": "Despite that the audio encoder is initialized from well-trained ASR model as mentioned in Sec. 2.2, the speech and text latent spaces differ. To pre-train the adapter and reduce the modality gap between the speech and text sequences, we curate dataset of approximately 2M hours of anonymized in-house speech-text pairs with strong/weak ASR supervisions, covering the eight supported languages 3."
        },
        {
            "title": "3.4.2 Post-training Data",
            "content": "Following language post-training paradigm, we curate SFT data for speech/audio post-training, aiming for unlocking the instruction-following capability with speech/audio as query or context. We use both the real and synthetic speech/audio data during speech post-training, covering the majority of speech and audio understanding tasks. All the SFT data are formatted as: < user >< audio > {task prompt} < end >< assistant > {label} < end > where task prompt is to describe each task in the natural language description and it is null for the SQQA task. Speech Recognition Data. ASR training data contains about 20k hours anonymized in-house, and 20k hours selected public transcribed speech recordings that span eight languages. The weighted ASR training data contributes to 28M SFT examples. Speech Translation Data. AST training data contains about 30K hours of anonymized in-house and public speech data with translations in two directions: from 7 languages to English and from English to 7 languages. This data contains both supervised and synthetic translation from machine translation model. The AST data is created with two formats: direct ST and ASR + translation in Chain-ofthoughts (CoT) manner, contributing to 28M weighted training examples in post-training. Speech and Spoken Query Question Answering Data. SQA and SQQA training data contain synthetic QA pairs from real speech and synthetic audio from text SFT data. Synthetic QA pairs for SQA: To enable SQA capability, we reuse the speech-transcript pairs in the ASR training data and prompt the language model to generate multiple text QA pairs for each transcript. The low-quality QA pairs are filtered during training. Synthetic spoken query (audio) for SQQA: SQA is tasked to respond speech context plus text query. Responding to spoken query directly is also an important capability for Phi-4-Multimodal. Consequently, We sample from the language post-training data and convert the text query to audio query using our internal zero-shot TTS system. Synthetic LM response for SQQA: Similar to [FWL+24], we synthetically generate responses for speech prompts by prompting the language model with the ASR transcripts of those prompts. The LM response data can improve the SQQA robustness of Phi-4-Multimodal in real scenarios because of more diverse spoken queries sampled from the ASR training data. The total SQA and SQAQA data contribute to 26M weighted SFT examples. 3The speech interface supports the following 8 languages: Chinese, English, French, German, Italian, Japanese, Portuguese, and Spanish. 8 Speech Summarization Data. The summarization training data is assembled from anonymized audio recordings paired with their transcripts. The audio consists of multi-speaker conversational speech that spans range of topics. Rather than dividing the audio into shorter segments, we maintain its full length up to maximum of 30 minutes. To construct query-summary pairs for each audio clip, we use GPT-4 to generate variety of queries and their respective summaries based on the transcripts. For each audio clip, the summarization queries address specific or general aspects of the conversation and vary in format, including length (number of words or sentences) and structure (summaries formatted as bullet points, JSON, or email). The weighted dataset contributes to 1M SFT examples with English speech only. Audio Understanding Data. The audio understanding data contributes to around 17M weighted SFT examples sourced from public. The dataset is created in the form of (audio, question, answer) tuples, where audio contains speech, audio, and music inputs. Similar to [GLL+23], the question and answer pairs are generated from GPT4 based on audio transcripts and/or meta information. In addition the task-specific data, we also include audio safety data in speech/audio post-training. Please refer to Sec. 5.2 for the details of audio safety data. For all the public data, we utilize our Azure PII Detector4 to identify and handle Personally Identifiable Information (PII). The training examples with PII detected are removed for privacy concerns."
        },
        {
            "title": "4.1.1 Vision Benchmarks",
            "content": "We report in Table 1 the evaluation results of Phi-4-Multimodal on 13 open-source academic singleimage vision-language benchmarks, 2 open-source multi-image/video vision-language benchmarks, and 4 vision-speech benchmarks. Additionally, we compare Phi-4-Multimodal with multiple state-ofthe-art open-source models: our previous Phi-3.5-Vision [AJA+24], Qwen2.5-VL-3B & 7B [Tea25b], InternVL2.5-4B & 8B [CWC+24], and close-sourced multimodal models Gemini [TAB+23], Claude3.5 [Ant24]5, and GPT-4o [HLG+24]6. For most benchmarks, we used the same internal evaluation pipeline as in Phi-3.5-Vision [AJA+24] to ensure fair comparisons across all baseline methods. For benchmarks (e.g., DocVQA and InfoVQA) requiring submission to an evaluation server, we directly utilized results reported in previous papers for baseline methods and submitted our own evaluations to the server to obtain results for Phi-4-Multimodal. For single-image vision-language benchmarks, the evaluations assess reasoning and perceptual capabilities across various domains, including but not limited to science, charts, OCR, and general knowledge. For multi-image/video vision-language benchmarks, we used one multi-image benchmark (BLINK [FHL+24]) and one video benchmark (VideoMME [FDL+24]). In the case of VideoMME, the evaluation setup is same as the one used in Phi-3.5-Vision [AJA+24], where 16 frames are extracted from each video by sampling frames at rate ensuring uniform time coverage. These benchmarks evaluate perceptual capabilities across multiple images/frames and text, covering scenarios such as art and style recognition, forensic detection, and video understanding. For vision-speech benchmarks, we adopted four existing 4https://learn.microsoft.com/en-us/azure/ai-services/language-service/ personally-identifiable-information/overview 5Claude-3.5-Sonnet-2024-10-22 6GPT-4o-2024-11-20 and GPT-4o-mini-2024-07-18 MMMU (val) [YNZ+23] MMMUPro (standard/vision) [YZN+24] ScienceQA (test) [LMX+22] MathVista (testmini) [LBX+24] Inter-GPS (test) [LGJ+21] MMBench (dev-en) [LDZ+24] POPE (test) [LDZ+23] AI2D (test) [KSK+16] ChartQA (test) [MLT+22] TextVQA (test) [SNS+19] DocVQA (test) [MKJ21] InfoVQA (test) [MBT+22] OCRBench [LLH+24] BLINK (test) [FHL+25] VideoMME-16Frame (test) [FDL+24] Phi-4-Multimodal 5.6B Phi-3.5-Vision 4.2B Qwen2.5-VL-3B 3.8B InternVL2.5-4B 3.7B Qwen2.5-VL-7B 8.3B InternVL2.5-8B 8.1B Gemini-2.0-FlashLite-prv-02-05 Gemini-2.0Flash Claude-3.5 -Sonnet GPT-4o -mini GPT-4o - 55.1 38.5 (39.7/37.3) 43.0 21.8 (25.5/18.0) 47. 29.9 (31.8/28.0) 48.3 32.4 (36.1/28.6) 51.8 38.7 (39.5/37.9) 50. 34.4 (39.0/29.8) 54.1 64.7 55.8 52.1 61. 45.1 (45.8/44.3) 54.4 (57.1/51.6) 54.3 (56.5/52.1) 40.8 (44.0/37.7) 53.0 (55.3/50.7) 97. 62.4 48.6 86.7 85.6 82.3 81. 75.6 93.2 72.7 84.4 61.3 55. 91.3 43.9 36.3 81.9 86.1 78. 81.8 72.0 69.3 36.6 63.8 57. 50.8 79.4 60.8 48.3 84.3 87. 78.4 80.0 76.8 93.9 77.1 82. 48.1 56.5 96.2 51.2 53.7 86. 89.4 80.0 79.1 70.9 91.6 72. 71.6 51.2 57.3 87.7 67.8 52. 87.8 87.5 82.6 85.0 77.7 95. 82.6 87.7 55.3 58.2 97.3 56. 54.1 88.2 89.1 83.0 81.0 74. 93.0 77.6 74.8 52.5 58.7 85. 57.6 57.9 85.0 87.5 77.6 73. 72.9 91.2 73.0 75.7 59.3 58. 88.3 47.2 65.4 90.0 88.0 82. 79.0 74.4 92.1 77.8 81.0 64. 65.5 81.2 56.9 47.1 86.7 82. 70.6 78.4 58.6 95.2 74.3 77. 56.9 60.2 84.0 38.8 39.9 83. 83.6 75.2 54.5 70.9 84.2 59. 77.1 51.9 61.2 88.2 56.1 49. 89.0 86.5 83.8 75.1 73.1 90. 71.9 77.7 62.4 68.2 Average 72. 60.9 68.7 68.8 73.3 71.1 70. 74.3 69.1 63.8 72.4 Table 1: Comparison results on public vision-language benchmarks. All the reported numbers are produced with the exact same internal pipeline to ensure that the numbers are comparable. These numbers might differ from other published numbers due to slightly different prompts. Note that for MathVista number of Gemini-2.0Flash, we find the low performance is because its output sometimes cannot follow the format defined in the input instruction and the evaluation script cannot parse the answer easily. Phi-4-Multimodal 5.6B InternOmni 8.7B Gemini-2.0-FlashLite-prv-02-05 Gemini-2.0Flash ShareGPT4o AI2D[CWT+24] ShareGPT4o ChartQA[CWT+24] ShareGPT4o DocVQA[CWT+24] ShareGPT4o InfoVQA[CWT+24]"
        },
        {
            "title": "Average",
            "content": "68.9 69.0 87.3 63.7 72.2 53.9 56.1 79.9 60.3 62.6 62.0 35.5 76.0 59.4 58. 69.4 51.3 80.3 63.6 66.2 Table 2: Comparison results on public vision-speech benchmarks. All the reported numbers are produced with the exact same internal pipeline to ensure that the numbers are comparable. benchmarks from InternOmni [CWT+24], which convert text prompts into speech format for evaluation on four vision-language benchmarks. Since Claude and GPT-4o endpoints do not support audio input along with images, we do not report their numbers here. For Gemini models, promting with only image and speech input will generate free-form responses that are difficult to extract and evaluate. Therefore, we add the corresponding text instructions to prompt the model to respond with one of ABCD options or single-word-or-phrase answers for the respective ShareGPT4o benchmark. From the results in Table 1 and Table 2, we summarize Phi-4-Multimodals performance as follows: On vision-language benchmarks, Phi-4-Multimodal demonstrates significant improvements over our previous model Phi-3.5-Vision [AJA+24] and outperforms baseline models of similar sizes overall. Notably, in chart understanding and science reasoning tasks, Phi-4-Multimodal even surpasses some close-sourced models like Gemini and GPT-4o. We have shown one demo case in Figure 2. On vision-speech benchmarks, Phi-4-Multimodal significantly outperforms InternOmni and 10 Figure 2: One demo case to show the vision-language understanding and reasoning capability of Phi-4-Multimodal. Gemini-2.0-Flash by large margin, which have larger model sizes. For instance, on ShareGPT4o AI2D and ShareGPT4o ChartQA benchmarks, Phi-4-Multimodal achieves more than 10 points higher performance than InternOmni. Unlike other open-source vision-language models that fully fine-tune their base language models (often resulting in performance degradation on pure language benchmarks), Phi-4-Multimodal keeps the language model entirely frozen by only incorporating additional fine-tunable LoRA modules. This approach ensures that language performance remains unchanged for pure text inputs. While some academic efforts aim to enable multimodal capabilities while preserving original language performance (e.g., through added cross-attention layers [AI23, DLW+24]), these techniques often lead to performance drops on vision-language benchmarks compared to fully fine-tuned large language models. Our proposed LoRA solution provides novel perspective for the community, maintaining language capabilities while achieving minimal performance loss on multimodal benchmarks compared to fully fine-tuned baselines."
        },
        {
            "title": "4.1.2 Speech and Audio Benchmarks",
            "content": "We evaluate the speech and audio capabilities of Phi-4-Multimodal on variety of understanding tasks. The performance of Phi-4-Multimodal is compared with several state-of-the-art opensourced models for the speech and audio understanding, including WhisperV3 [RKX+23], SeamlessM4Tv2 [BCM+23], Qwen2-audio [CXY+24]. We also include the performance of close-sourced multi-modal models (GPT-4o [HLG+24] and Gemini [TAB+23]) for comparisons 7. The results are obtained through 7Speech evaluations for closed models are done through Azure cloud API. 11 Table 3: Main Results on the speech benchmarks. All results are obtained with 0-shot evaluations except additional CoT evaluations on the AST task, where CoT refers to chain-of-thoughts decoding with transcription plus translation in generation. MT-Bench results are averaged scores over two-turn SQA conversations. SSUM evaluation is with the overall numbers covering the adherence and hallucination scores. The scores in the table are judged by GPT-4-0613. N/A indicates the model does not have such capability. Phi-4-Multimodal WhisperV3 Task Metric Dataset ASR WER CV15 FLEURS OpenASR 5.6B 6.80 4.00 6.14 AST BLEU Inference Type CoVoST2 X-EN CoVoST2 EN-X FLEURS X-EN FLEURS EN-X (0-shot, CoT) (39.33, 40.76) (37.82, 38.73) (29.86, 32.35) (32.15, 33.56) SQQA Score 1-10 ACC MT-Bench MMMLU SSUM Score 1-7 Golden3 AMI AU Score 1-10 ACC AirBench-chat MMAU 7.05 38.50 6.28 6.29 6.98 55.56 1.5B 8.13 4.58 7.44 0-shot 33.26 N/A 25.76 N/A N/A N/A N/A N/A N/A N/A SeamlessM4T-V2 2.3B Qwen2-audio 8B Gemini2.0-Flash GPT-4o - 8.46 7.34 20.70 0-shot 37.54 32.84 28.87 30.44 N/A N/A N/A N/A N/A N/A 8.55 8.28 7.43 0-shot 34.80 34.04 23.72 23.24 4.92 15.53 2.25 1. 6.93 52.50 9.29 4.73 8.56 0-shot 36.62 35.93 30.69 37.33 8.07 72.31 6.29 5.97 6.68 61. 18.14 5.42 15.76 0-shot 37.09 37.19 32.61 36.78 8.11 72.56 6.76 6.53 6.54 53.29 evaluation on the exact same test data version without further clarifications. We sample the top-1 token at each generation step during inference. The main results on the speech benchmark are presented in Table 3. We summarize the performance of Phi-4-Multimodal as listed: Phi-4-Multimodal achieves very strong ASR and AST performance, surpassing the expert ASR model, WhisperV3, and expert AST model, SeamlessM4T-large-v2, on CommonVoice [ABD+20], FLEURS [CMK+23], OpenASR [SMK+23], and CoVoST2 [WWGP21] test sets. Phi-4-Multimodal is 5.5% relatively better in WER than the best model on the Huggingface OpenASR leaderboard8 and now ranks No.1 on the leaderboard as of 1/14/2025. Phi-4-Multimodal is the first open-sourced model with speech summarization capability. The summarization quality is close to that of GPT-4o in the sense of adherence and low hallucinations. Phi-4-Multimodal is the smallest open-sourced multi-modal LLM that behaves better than the open-sourced Qwen2-audio [CXY+24] with 2x in size. We should notice in Table 3 that Phi-4-Multimodal is optimized for speech and audio understanding tasks while Gemini and GPT-4o might be optimized towards chat experience. That may be the reason why Phi-4-Multimodal outperforms Gemini-2.0-Flash and GPT-4o on ASR and AST tasks while lags behind on the SQQA tasks. We describe the benchmark and evaluation details for each task below. Automatic Speech Recognition. We evaluate the ASR performance on three public benchmarks: CommonVoice [ABD+20], FLEURS [CMK+23], and OpenASR [SMK+23]. CommonVoice is an open-source, multilingual speech dataset developed by Mozilla. The test set of CommonVoice version 15.0 (CV15) is adopted in our evaluation, in which the data is collected before 9/13/2023. We conduct the evaluations on the eight supported languages. 8https://huggingface.co/spaces/hf-audio/open_asr_leaderboard 12 Table 4: Detailed results on ASR benchmarks. We compute CER () for JA and ZH, and WER () for other languages. nvidia/canary-1B model is the best performing model on the Huggingface OpenASR leaderboard to date. The results of canary and WhisperV3 are from the official report while others are obtained through internal evaluation on the same test data version. Dataset Sub-Category Phi-4-Multimodal 5.6B CV FLEURS OpenASR EN DE ES FR IT JA PT ZH Average EN DE ES FR IT JA PT ZH Average AMI Earnings22 Gigaspeech Spgispeech Tedlium LS-clean LS-other Voxpopuli Average 7.61 5.13 4.47 8.08 3.78 10.98 6.97 7.35 6. 3.38 3.96 3.02 4.35 1.98 4.50 3.98 6.83 4.00 11.69 10.16 9.78 3.13 2.90 1.68 3.83 5.91 6.14 nvidia/canary WhisperV3 1B N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 13.90 12.19 10.12 2.06 3.56 1.48 2.93 5.79 6.50 1.5B 9.30 5.70 4.70 10.80 5.50 10.30 5.90 12.80 8.13 4.10 4.90 2.80 5.30 3.00 4.80 4.00 7.70 4.58 15.95 11.29 10.02 2.01 3.91 2.94 3.86 9.54 7.44 SeamlessM4T-V2 2.3B Qwen2-audio 8B Gemini2.0-Flash GPT-4o - 7.65 6.43 5.42 9.75 5.50 12.37 9.19 11.36 8.46 6.54 6.95 5.39 7.40 4.70 11.47 7.67 8.6 7.34 56.1 37.18 26.22 12.04 19.26 2.60 4.86 7.37 20. 8.68 7.61 5.71 9.57 6.78 13.55 10.03 6.47 8.55 5.27 8.77 6.90 9.00 5.78 12.68 10.59 7.21 8.28 15.24 14.09 10.26 3.00 4.05 1.74 4.03 7.05 7.43 11.21 6.2 4.81 10.45 4.88 13.46 7.4 15.87 9.29 3.96 4.06 2.61 5.06 1.86 4.94 3.57 11.74 4.73 21.58 13.13 10.71 3.82 3.01 2.49 5.84 7.89 8. 21.48 10.91 11.24 17.63 13.84 19.36 23.07 27.55 18.14 6.52 4.17 3.69 6.42 3.28 5.18 6.33 7.77 5.42 57.76 20.94 13.64 5.66 5.79 3.48 7.97 10.83 15.76 FLEURS multilingual speech dataset designed for evaluating speech recognition and speech-totext translation models across wide range of languages. The models are evaluated on the test sets of the eight supported languages for ASR. OpenASR Leaderboard on Hugging Face is designed for benchmarking and evaluating the robustness of ASR models on English. The datasets in the leaderboard cover diverse speech domains including reading speech, conversations, meetings, and so on. The ASR prompt for Phi-4-Multimodal is Transcribe the audio clip into text., which is language agnostic. We notice that the model can learn to recognize in the target language perfectly without providing language information, while Qwen2-audio and Gemini-2.0-Flash require the language information in the prompt to obtain the optimal ASR performance. For example, the ASR prompt for Gemini-2.0-Flash is Transcribe the audio clip into {tgt-lang}. Please ignore background noise. We compute the Character Error Rate (CER) for Japanese and Chinese language and Word Error Rate (WER) for other six languages. The detailed ASR results on the three benchmarks are summarized in Table 4. Overall, we achieve the new SOTA multi-lingual ASR performance on the eight supported languages, surpassing the expert ASR models like WhisperV3. Noticeably, Phi-4-Multimodal beats the best performing model, nvidia/canary-1b, by 5.5% relative WER on the Huggingface OpenASR leaderboard and now ranks No.1 in the leaderboard to date. Phi-4-Multimodal is also better than the open-sourced Qwen2-audio with doubled model size. Note that GPT-4o is very sensitive to ASR prompt. We tried many ASR prompts and present the one with the best overall ASR results we can obtain on the test sets. The ASR prompt we finally use is Capture the speech in written format in the language spoken, please. Dont include any information outside of the spoken content in your response. Remove any hesitation words like um, uh. Support mixed language. Your response should be formatted as follows: Spoken Content: <transcribed text here>.. 13 Automatic Speech Translation. We evaluate the AST performance on two public benchmarks: CoVoST2 [WWGP21] and FLEURS [CMK+23]. CoVoST2 is multilingual speech-to-text translation dataset derived from Mozillas Common Voice project. It is one of the largest open datasets available for speech translation, providing support for both X-to-English (X-En) and English-to-X (En-X) translation tasks. We evaluate the directions with supported languages on the test sets. We use the same FLEURS test audios as those in ASR evaluation but replacing the ASR transcription with the translations. We evaluate EN-X and X-EN directions with supported languages on the test sets. The AST prompts for 0-shot and CoT evaluation are Translate the audio to {tgt-lang}. and Transcribe the audio to text, and then translate the audio to {tgt-lang}. Use < sep > as separator between the original transcript and the translation., respectively. We compute BLEU score between the reference and text translations. For CoT evaluation, the text after < sep > is regarded as the translation. The detailed AST results on each translation direction are shown in Table 5. As we can see from the table, CoT inference can largely benefit the translation quality, improving 1-2 BLUE score on various test sets. Phi-4-Multimodal achieves the best AST performance among the evaluated models on CoVoST2 benchmark, including Gemini-2.0-Flash and GPT-4o. On FLEURS, Phi-4-Multimodal is better than the expert model SeamlessM4T-large-V2 and the performance is on par with GPT-4o, the size of which is much larger than Phi-4-Multimodal. We dont apply CoT evaluation to other models since either the model does not support CoT decoding, or it is hard to find good CoT prompt for the model to respond to each test sample correctly. Similar to ASR, Phi-4-Multimodal does not require source language information in the AST prompt. Spoken Query Question Answering. We evaluate the SQQA performance on two language benchmarks with synthetic audio query: MT-Bench [ZCS+23] and MMMLU [HBB+20]. The text query is synthesized into the audio query with the internal zero-shot TTS system. MT-Bench (Multi-turn Benchmark) is specifically designed to evaluate the conversational and instruction-following abilities of AI models in multi-turn question-answering (QA) scenarios. MMMLU (Multilingual Massive Multitask Language Understanding) is an extensive benchmark designed to evaluate the general knowledge and reasoning capabilities of AI models across wide array of subjects. We evaluate the model on the eight supported languages for this test set. The task prompt is null for the spoken query QA task. The metrics are different for the two benchmarks. The answer for MT-bench is open-ended, so we use GPT-4 as judge to score the model outputs from 1 to 10. We evaluate the model outputs in the first two turns for MT-bench. Please refer to Appendix for the judege prompts for GPT-4. MMMLU is QA task with multiple-choice questions. We use the accuracy to measure the model quality. We summarize the SQQA results in Table 6. It can be seen from the table that Phi-4-Multimodal outperforms Qwen2-audio with doubled model size on MT-bench. However, the performance lags far behind than the Gemini-2.0-Flash and GPT-4o, which show strong SQQA capability. The results on SQQA show that Phi-4-Multimodal is more good at conversational chat rather than general knowledge and reasoning chat (less gap to closed-source models on MT-bench than that on MMMLU). The reason might be that we weighed more conversational SQQA data in the speech/audio post-training stage. 14 Table 5: Detailed results on AST benchmarks with BLEU () score reported. We use zh, ja-mecab, and 13a tokenizer in Sacrebleu [Pos18] to compute BLUE scores for Chinese, Japanese, and other six languages, respectively. All results are obtained through our internal evaluation. Phi-4-Multimodal (+CoT) WhisperV3 Dataset Sub-Category 5.6B CoVoST2 X-EN CoVoST2 EN-X FLEURS X-EN FLEURS EN-X DE ES FR IT JA PT ZH Average DE JA ZH Average DE ES FR IT JA PT ZH Average DE ES FR IT JA PT ZH Average 39.81 43.60 42.24 41.42 30.54 55.28 22.39 39.33 34.22 32.93 46.30 37. 37.71 25.33 35.10 26.06 21.62 40.80 22.37 29.86 34.44 23.66 37.92 23.44 30.67 37.79 37.10 32.15 1.5B 34.17 39.21 35.43 35.82 23.59 50.22 14.36 33.26 N/A N/A N/A N/A 33.49 22.68 30.98 23.00 16.63 37.50 16.07 25. N/A N/A N/A N/A N/A N/A N/A N/A 40.83 44.84 43.42 42.45 31.87 56.25 25.64 40.76 34.87 34.04 47.28 38.73 39.43 27.56 37.42 28.45 25.22 42.85 25.49 32.35 35.94 25.09 40.12 24.85 30.81 38.94 39.19 33.56 SeamlessM4T-V2 2.3B Qwen2-audio 8B Gemini2.0-Flash GPT-4o - 39.90 42.90 42.18 39.85 22.18 53.82 21.92 37.54 37.16 24.94 36.41 32.84 36.80 25.67 33.78 26.80 18.63 37.61 22.78 28. 32.35 23.37 42.08 24.55 20.46 42.36 27.93 30.44 34.99 39.91 38.31 36.35 22.98 47.79 23.27 34.8 29.72 27.30 45.09 34.04 32.88 22.40 30.82 22.12 4.49 35.38 17.95 23.72 23.60 19.47 27.71 19.61 12.38 32.52 27.38 23.24 38.34 41.74 38.96 37.76 28.04 50.81 20.69 36. 34.32 32.56 40.91 35.93 38.48 26.51 35.18 25.02 23.89 41.51 24.27 30.69 37.15 26.40 46.51 29.04 35.51 45.34 41.36 37.33 39.29 41.49 38.56 37.33 30.46 50.60 21.93 37.09 34.38 32.98 44.22 37.19 41.03 29.10 37.98 28.51 24.17 43.33 24.12 32. 36.68 25.99 44.26 28.59 33.99 45.82 42.16 36.78 Speech Summarization. We evaluate the speech summarization performance on an in-house (Golden3) and public (AMI [CAB+05]) benchmark. Golden3 is real-world meeting dataset, containing 108 meeting recordings with corresponding transcripts, averaging 6 minutes each. The dataset is primarily in English, covering wide range of topics. There are in total 1071 queries for the entire dataset, averaging 9.9 instructions for each conversation. The AMI (Augmented Multi-Party Interaction) dataset is comprehensive collection of meeting recordings, encompassing approximately 100 hours of data. These recordings feature synchronized audio and video streams, including close-talking and far-field microphones, individual and roomview cameras, and outputs from devices like slide projectors and electronic whiteboards. The dataset is primarily in English and includes contributions from both native and non-native speakers, captured in various rooms with distinct acoustic properties. The test split contains 20 meeting recordings with average duration of 32 minutes. We test on close-talking version of audio. There are 10 instructions generated for each conversation, summing up to 200 for the dataset. To generate the summarization instructions for the test data, GPT-4 is employed being asked to summarize partial or the entire conversation or control the output style/length/structure. An example prompt could be Summarize the ideas shared for making the remote control suitable for older generations. or Summarize in bullet points the key product specifications discussed. The summarization instructions are regarded as task prompt for multi-model LLM inference. During evaluation, we use GPT4 to score the response corresponding to each instruction in 3 criteria: overall quality, hallucination, and instruction adherence. The overall quality, scaled 1 to 7, measures accuracy in capturing details, coherence, and writing style. The hallucination score is binary flag that measures whether any Table 6: Result details on speech QA/summarization/audio understanding tasks for multi-modal models. The scores are obtained using GPT-4-0613 as judge. Task Metric Dataset Sub-Category Phi-4-Multimodal 5.6B Qwen2-audio 8B Gemini2.0-Flash GPT-4o - Score 1-10 MT-Bench SQQA ACC MMMLU turn-1 turn-2 AVG EN DE ES FR IT JA PT ZH AVG SSUM Score 1-7 Golden3 AMI Hallucination Instruction adherence Overall Hallucination Instruction adherence Overall Score 1-10 AirBench-chat AU ACC MMAU mixed music sound speech AVG music sound speech AVG 7.42 6.67 7. 54.25 39.50 42.25 38.50 35.00 30.00 34.00 34.50 38.50 0.14 5.87 6.28 0.13 6.50 6.29 6.78 6.67 7.00 7.47 6.98 52.87 60.97 52.83 55.56 5.07 4.76 4. 16.00 10.50 25.00 19.25 18.50 14.25 11.25 9.50 15.53 0.51 2.64 2.25 0.96 1.40 1.34 6.77 6.79 6.99 7.18 6.93 53.26 58.34 45.90 52.50 8.08 8.06 8. 74.00 78.75 75.75 74.25 70.50 68.75 70.50 66.00 72.31 0.20 6.25 6.29 0.28 6.25 5.97 6.84 6.33 5.62 7.92 6.68 58.33 62.60 62.77 61.23 8.27 7.94 8. 78.75 73.70 78.32 76.21 71.84 67.40 70.48 63.77 72.56 0.09 6.73 6.76 0.10 6.83 6.53 6.00 5.55 7.45 7.17 6.54 55.27 48.30 56.30 53.29 part of the summary is fabricated or in consistent with the source content (0 represents no hallucination and vice versa). The adherence score, scaled 1 to 7, measures how well the response follows the instruction in specific format, content or length requirements. During scoring, GPT4 has access to the ground truth transcripts of each recording. Please refer to Appendix for the GPT4 scoring prompt. We average the scores of all responses belong to the same dataset for each criteria. The detailed summarization scores are presented in Table 6. Qwen2-Audio has 30-second cut-off for audio input, therefore it couldnt deal with long-form audio input and appears incompetent in this task. Phi-4-Multimodal instead can naturally encode long-form audio in one-shot and perform speech understanding. It exhibits competitive performance on both Golden3 and AMI test sets, compared with Gemini-2.0-Flash and GPT-4o. Considering that speech summarization data contributes only 1% of the data in speech post-training, the gap can be reduced easily with finetuning on more summarization data. Audio Understanding. We evaluate the audio understanding capability on two benchmarks: AIRBenchchat [YXL+24] and MMAU [STK+24]. AIR-Bench (Audio Instruction and Response Benchmark) is comprehensive evaluation framework designed to test the capabilities of large audio language models. It includes both foundation and chat benchmarks. The chat benchmark is selected for the open-end QA style evaluation. The chat benchmark includes the categories of music, sound, speech and mixed domain. The MMAU (Massive Multi-Task Audio Understanding) benchmark is comprehensive dataset designed to evaluate the capabilities of multi-modal models in audio-based understanding and reasoning tasks. The test sets are in the form of multiple-choices QA, covering the categories of music, sound, and speech. Similar to open-ended QA as MT-bench, we use GPT-4-0613 as judge to score the model outputs. Figure 3: An example to showcase the understanding capabilities for Phi-4-Multimodal, including audio understanding, summarization, ASR, and AST. Please refer to the Appendix for the GPT4 scoring prompt. The accuracy is used to measure the model quality on MMAU. The detailed results on each category for multi-model models are presented in Table 6. Although we freeze the audio encoder in post-training, Phi-4-Multimodal achieves strong speech, audio, and music understanding capability on the two evaluated benchmarks, surpassing the open-sourced Qwen2audio. The GPT-4o does not perform well on the audio and music understanding tasks because the models may not respond to the audio/music inputs for some test samples. In other words, GPT-4o is either sensitive to the prompts for audio and music understanding tasks . We showcase an example for strong speech understanding capabilities of Phi-4-Multimodal in Figure 3."
        },
        {
            "title": "4.2.1 Language",
            "content": "We have conducted benchmarks on various different academic datasets. We compare the scores with the latest open-source models - Qwen 2.5 [YYZ+24], Llama-3.2 [DJP+24], Ministral [Mis24] and Gemma2 [TRP+24] series. Overall, we observe Phi-4-Mini shows very strong performance across different benchmarks as shown in Table 7. 1. Overall performance: Across different language understanding benchmarks, Phi-4-Mini outperforms similar size models size models and on-par with the models with 2 times larger. Especially, Phi-4-Mini outperforms most of the larger models except for Qwen2.5 7B with large margins as well as similar sized models. 2. Strong Math and Reasoning capabilities: Phi-4-Mini excels on math and reasoning related benchmarks thanks to the reasoning-rich synthetic data its trained on. For the Math benchmark, 17 Phi-4-Mini 3.8b Phi-3.5-Mini 3.8b Llama-3.2-Ins 3B Ministral 3B Qwen2.5-Ins 3B Qwne2.5-Ins 7B Ministral-2410 8B Llama-3.1 8B Llama-3.1 Tulu-3 8B Gemma2-It 9B BigBench-Hard (0-Shot; CoT) [SRR+22, SSS+22] MMLU (5-Shot) [HBK+21a] MMLU-Pro (0-Shot; CoT) [WMZ+24] Arc-C (10-Shot) [CCE+18] BoolQ (2-Shot) [CLC+19] GPQA (0-Shot; CoT) [RHS+23] HellaSwag (5-Shot) [ZHB+19] OpenBookQA (10-Shot) [MCKS18] PIQA (5-Shot) [BZGC19] SociQA (5-Shot) [SRC+19] TruthfulQA (10-Shot; MC2) [LHE22] WinoGrande (5-Shot) [SLBBC19] Multilingual-MMLU (5-Shot) [HBK+21a, DLVNN+23] MGSM (0-Shot; CoT) [CKB+21, SSF+22] GSM-8K (8-Shot; CoT) [CKB+21] MATH (0-Shot; CoT) [HBK+21b] Qasper (0-shot) [DLB+21] SQuALITY (0-shot) [WPC+22] IFEval (0-shot) [ZLM+23] BFCL (0-shot) [YMJ+24] HumanEval (0-Shot) [CTJ+21] MBPP (3-Shot) [AON+21] 70.4 67.3 52.8 83. 81.2 30.4 69.1 79.2 77.6 72. 66.4 67.0 49.3 63.9 88.6 64. 40.4 22.8 70.1 70.3 74.4 65. 63.1 65.5 47.4 84.6 77.7 25. 72.2 81.2 78.2 75.1 65.6 72. 55.4 47.9 86.2 48.5 41.9 25. 50.6 66.1 70.1 70.0 55.4 61. 39.2 76.1 71.4 26.6 69.0 72. 68.2 68.3 59.2 53.2 48.1 49. 75.6 46.7 33.4 25.7 68.0 78. 62.8 67.2 51.2 60.8 35.3 80. 79.4 24.3 77.2 79.8 78.3 73. 62.9 59.8 46.4 44.6 80.1 41. 35.3 25.5 47.5 61.4 72.0 65. 56.2 65.0 44.7 82.6 65.4 24. 74.6 77.6 77.2 75.3 64.3 63. 55.9 53.5 80.6 61.7 32.1 25. 59.0 74.2 72.0 65.3 72.4 72. 56.2 90.1 80.0 30.6 80.1 86. 80.8 75.3 69.4 71.1 64.4 64. 88.7 60.4 38.1 10.0 69.5 81. 75.0 76.3 53.3 63.0 36.6 82. 80.5 26.3 80.9 80.2 76.2 77. 63.0 63.1 53.7 58.3 81.9 41. 37.4 24.9 52.5 74.0 70.7 68. 63.4 68.1 44.0 83.1 82.8 26. 73.5 84.8 81.2 71.8 69.2 64. 56.2 56.7 82.4 47.6 37.2 26. 55.5 65.0 40.9 79.4 79.0 29. 72.8 79.8 83.2 73.4 64.1 65. 54.5 58.6 84.3 46.1 35.4 26. 74.1 77.3 77.0 59.4 66.5 69. 62.8 63.9 65.7 71.3 50.1 89. 85.7 31.0 80.9 89.6 83.7 74. 76.6 74.0 63.8 75.1 84.9 51. 13.9 23.6 73.2 59.9 63.4 69."
        },
        {
            "title": "Average",
            "content": "64.9 62.3 58.0 58.3 61.4 67. 61.2 63.9 61.7 66.0 Table 7: Phi-4-Mini language benchmark scores in comparison with Llama 3.2, Llama 3.1-8B, Qwen 2.5, Ministral and Gemma series. Phi-4-Mini 3.8b Phi-3.5-Mini 3.8b Llama-3.2-Ins 3B Ministral 3B Qwen2.5-Ins 3B Qwne2.5-Ins 7B Ministral-2410 8B Llama-3.1 8B Llama-3.1 Tulu-3 8B Gemma2-It 9B 43.0 40. 25.7 50.0 33.8 43.4 47.4 34. 30.4 40.6 33.8 74.4 68.3 19. 30.5 65.3 63.8 42.2 14.3 70. 62.8 15.7 18.3 70.0 63.8 47. 18.6 62.8 51.8 9.9 14.8 67. 52.9 51.5 33.8 72.0 67.5 7. 14.8 65.1 60.8 42.1 25.0 72. 64.6 14.7 22.7 65.3 60.6 24. 33.5 75.0 68.9 19.9 38.3 76. 65.9 48.2 35.6 70.7 70.7 16. 25.0 68.9 61.6 22.1 34.8 28. 66.5 57.3 16.8 18.8 69.4 11. 61.6 62.8 50.0 17.8 22.7 63. 55.3 43.4 33.6 63.4 54.3 14. 23.4 69.6 63.5 44."
        },
        {
            "title": "BigCodeBench\nCompletion",
            "content": "(0-Shot) [ZVC+24] BigCodeBench instruct (0-Shot) [ZVC+24] HumanEval (0-Shot) [CTJ+21] HumanEval+ (0-Shot) [LXWZ23] LCB (05-09-2024) [JHG+24] LiveBench (code task) [WDR+24] MBPP (3-Shot) [AON+21] MBPP+ (3-Shot) [LXWZ23] Spider (4-Shot) [YZY+18]"
        },
        {
            "title": "Average",
            "content": "49.0 44.7 39.5 45.9 42.6 52. 46.5 41.2 41.6 45.3 Table 8: Phi-4-Mini coding performance comparison with Llama 3.2, Llama 3.1-8B, Qwen 2.5, Ministral and Gemma models. the model outperforms similar sized model with large margins sometimes more 20 points. It even outperforms two times larger models scores. 3. Excellent instruction following and function calling performance: Compared to the predecessor Phi-3.5-Mini, Phi-4-Mini shows significantly improved performance on instruction following and function calling thanks to the curated data and improved post-training. 4. Strong coding performance: Phi-4-Minis strong reasoning capabilities are also shown on the coding tasks thanks to the curated organic and synthetic data. In the HumanEval benchmark, Phi-4-Mini outperforms most of the similar sized and two times larger sized models."
        },
        {
            "title": "4.2.2 Coding",
            "content": "In Phi-4-Mini training, we have put special emphasis on the coding capability. We have collected high quality code data and generated various code related data. As result, Phi-4-Mini shows very strong performance on coding tasks as shown in the Table 8. Across 9 different coding benchmarks, Phi-4-Mini outperforms all 3B sized model and 8B sized model except for Qwen2.5 on the average score."
        },
        {
            "title": "4.2.3 CoT Reasoning",
            "content": "We evaluate the reasoning performance of reasoning-enhanced model that we have trained over Phi-4-Mini. We show results on AIME 2024 [MAA24], MATH-500 [LKB+23], and GPQA Diamond [RHS+], comparing it against OpenAI reasoning models and several recent, larger reasoning models from Deepseek and others. Despite having only 3.8B parameters, Phi-4-Mini reasoning-enhanced model outperforms DeepSeek-R1-Distill-Llama-8B [GYZ+25], Bespoke-Stratos-7B [Lab25], OpenThinker-7B [Tea25a], and achieves performance comparable to DeepSeek-R1-Distill-Qwen-7B as shown in the Table 9."
        },
        {
            "title": "Model",
            "content": "AIME MATH-500 GPQA Diamond o1-mini* DeepSeek-R1-Distill-Qwen-7B DeepSeek-R1-Distill-Llama-8B Bespoke-Stratos-7B* OpenThinker-7B* Llama-3.2-3B-Instruct Phi-4-Mini Phi-4-Mini (reasoning trained) (3.8B) 63.6 53.3 43.3 20.0 31.3 6.7 10.0 50.0 90.0 91.4 86.9 82.0 83.0 44. 71.8 90.4 60.0 49.5 47.3 37.8 42.4 25.3 36.9 49.0 Table 9: CoT Reasoning results of reasoning-enhanced Phi-4-Mini compared with larger 7B reasoning models and OpenAI models. An asterisk (*) indicates results taken directly from the published reports, while the remaining results were reproduced in our work."
        },
        {
            "title": "5 Safety",
            "content": "Phi-4-Mini and Phi-4-Multimodal were developed in accordance with Microsofts responsible AI principles. The overall approach consisted of safety alignment in post-training, red-teaming, automated testing and evaluations across dozens of RAI harm categories."
        },
        {
            "title": "5.1 Text safety",
            "content": "Our approach was almost identical to the one described in the Phi-3 Technical Report [AJA+24]. Further details can be found in the Phi-3 Safety Paper [Mic24]. The main improvement was to extend our Safety post-training datasets to all Tier 1 languages by performing (and verifying) machine translation with GPT-4o-mini model. Helpfulness and harmlessness preference datasets [BJN+22, JLD+23] with modifications inspired by [BSA+24] and multiple in-house generated datasets were leveraged to address the RAI harm categories in safety post-training. An independent red team at Microsoft iteratively examined Phi-4-Mini to further identify areas of improvement during the post-training process. Based on their feedback, we curated additional datasets tailored to address their insights, thereby refining the post-training dataset. Systematic Safety evaluations were carried out as described in the Phi-3 Safety Paper [Mic24]. The main difference lied with our evaluations for Harmful Content, which now leverage Microsofts Azure AI Evaluation SDK. We used GPT-4o to simulate adversarial conversations with our model, and to evaluate the models responses toxicity along four harm categories: Violence, Sexual Content, Self-Harm, and Hateful Content. We then computed Defect Rate for each category - the fraction of responses that did contain harmful content. Table 10 shows that our models are on par with other models of similar size, and with our previously released Phi-3.5-mini (which is not surprising, due to the similar approach for safety alignment). To assess the vulnerability of the model to jailbreaks (JBs), we repeated the previous evaluation while prepending the simulated user prompts with known JBs. The results shown in table 11 allow us to draw 2 conclusions. First, our latest Phi models are more robust to jailbreaks than our previously released Phi-3.5-mini, and than other models of similar size. Second, our models seem to manage to detect the presence of JBs, and in such cases are even less likely to comply with prompts eliciting harmful responses. This can be seen from the Defect Rates being smaller than the ones obtained without JBs shown in table 10. Defect Rate Violence Sexual Self-Harm Hateful Average Phi-4-Mini Phi-4-Multimodal Phi-3.5-mini GPT-4o-mini Llama-3.2-3B Qwen-2.5-3B 6% 6% 0% 3% 3.75% 7% 6% 0% 3% 4% 7% 7% 0% 2% 4% 6% 7% 1% 3% 4.25% 8% 8% 1% 3% 5% 7% 6% 1% 3% 4.25% Table 10: RAI benchmark results for Phi-4-Mini, Phi-4-Multimodal, Phi-3.5-mini, and other models of similar size. The Defect Rate denotes the fraction of model responses containing harmful content. The last row shows the average Defect Rates across all 4 harm categories. JB Defect Rate Violence Sexual Self-Harm Hateful Average Phi-4-Mini 2% 1% 0% 2% 1.25% Phi-4-Multimodal Phi-3.5-mini GPT-4o-mini Llama-3.2-3B Qwen-2.5-3B 4% 3% 0% 2% 2.25% 11% 8% 1% 10% 7.5% 7% 7% 1% 6% 5.25% 11% 8% 1% 12% 8% 20% 14% 3% 19% 14% Table 11: RAI benchmark results for Phi-4-Mini, Phi-4-Multimodal, Phi-3.5-mini, and other models of similar size. The Defect Rate denotes the fraction of model responses containing harmful content, when known jailbreaks are included in the user prompts. The last row shows the average Defect Rates across all 4 harm categories. To assess the ability of the model to refuse answering harmful prompts without sacrificing the ability to answer seemingly harmful but innocuous prompts, we leveraged the XSTest framework described in [RKV+23]. Two metrics were computed: the Refusal Rate to harmful prompts (IPRR a.k.a. Inappropriate Prompt Refusal Rate - the higher, the better) and the Refusal Rate to innocuous prompts (VPRR a.k.a. Valid Prompt Refusal Rate - the lower, the better). Results for Phi models and some competitor models are shown in table 12. Two conclusions can be drawn. First, both Phi-4-Mini and Phi-4-Multimodal are very good at refusing to answer harmful prompts. Second, when it comes to answering innocuous prompts, Phi-4-Multimodal errs little more on the safety side compared to the rest of the field. Refusal Rate IPRR VPRR Phi-4-Mini Phi-4-Multimodal Phi-3.5-mini Llama-3.2-3B Qwen-2.5-3B 93.5% 20.8% 92% 26.4% 87% 21.2% 92.5% 15.6% 92% 25.6% Table 12: XSTest benchmark results for Phi-4-Mini, Phi-4-Multimodal, Phi-3.5-mini, and other models of similar size. IPRR is the Refusal Rate to inappropriate prompts (e.g. How can kill my neighbor?) - the higher, the better. VPRR is the Refusal Rate to valid prompts (e.g. How can kill python process?) - the lower, the better. We also evaluated the models tendency to produce harmful content across all Tier 1 languages by following the approach described above, that leverages the Azure AI Evaluation SDK. Defect Rates can be seen in table 13, where the numbers were averaged over the 4 harm categories (Violence, Sexual Content, Self-Harm, and Hateful Content) to produce single number per language. These results show an improvement for both Phi-4-Mini and Phi-4-Multimodal compared to Phi-3.5-mini, and show comparable performance to other competitor models."
        },
        {
            "title": "Language\nGerman\nFrench\nSpanish\nItalian\nPortuguese\nChinese\nJapanese\nAverage",
            "content": "Phi-4-Mini 3.25% 3.25% 3% 2.25% 4.5% 6.25% 5% 3.91% Phi-4-Multimodal Phi-3.5-mini GPT-4o-mini Llama-3.2-3B Qwen-2.5-3B 4.5% 5% 4.5% 4.75% 5.5% 6.5% 5.75% 5.06% 6.75% 6% 6.25% 6.25% 6% 8.5% 6.75% 6.31% 3.75% 4.25% 4.25% 3.75% 5.25% 4.5% 3% 4.13% 4.5% 4.25% 4.25% 4.25% 4.25% 4.75% 5.75% 4.63% 7% 5.5% 5.5% 5.5% 5.25% 6.5% 5.75% 5.66% Table 13: Defect Rates for production of harmful content for Phi-4-Mini, Phi-4-Multimodal, Phi-3.5-mini, and other models. The lower the value, the better. The last row shows the average across all Tier 1 languages (including English numbers from table 10)."
        },
        {
            "title": "5.2 Audio safety",
            "content": "For the audio safety alignment of Phi-4-Multimodal, we followed an approach analogous to that of text safety alignment described above. Our audio safety datasets were obtained by performing TTS (Text-To-Speech) synthesis on our text safety datasets. We want to acknowledge two limitations of this approach. 1. Our audio safety datasets are voice-only. No other types of sounds (non-speech) were included. 2. We did not train against audio-specific jailbreaks. For audio safety evaluations, we carried out three families of automated evaluations. First, like we did with text inputs, we leveraged Microsofts Azure AI Evaluation SDK to detect the presence of harmful content in the models responses to speech prompts. The Defect Rates are shown in table 14. Although somewhat higher than those obtained with GPT-4o (a model of much bigger size), they are comparable to those shown in table 10 for text inputs. Defect Rate Violence Sexual Self-Harm Hateful Average Phi-4-Multimodal GPT-4o 4% 4% 1% 4% 3.25% 2% 1% 1% 0% 1% Table 14: RAI benchmark results for Phi-4-Multimodal and GPT-4o. The Defect Rate denotes the fraction of model responses containing harmful content, when the input prompt was an audio trace. The last row shows the average Defect Rates across all 4 harm categories. Second, we ran Microsofts Speech Fairness evaluation to verify that Speech-To-Text transcription worked well across variety of demographics - as measured by the WER metric. The audio samples were spread across 2 gender sub-groups, and 3 age sub-groups (17-30, 31-45, and 46-65). The following locales (corresponding to Tier 1 languages) were considered: it-IT, fr-FR, ja-JP, es-MX, pt-BR, es-ES, zh-CN, en-US, en-GB, and de-DE. No sub-group with egregiously worse performance than the overall population was found. Some sub-groups did have slightly better/worse performance than the overall population in their given locale. The sub-groups with slightly better performance than the overall population were: it-IT 17-30, es-MX 46-65, es-ES 17-30, en-US Female, en-US 46-65, and de-DE 46-65. The sub-groups with slightly worse performance than the overall population were: en-US Male and es-MX 17-30. Third, we implemented custom evaluation to assess whether the model would infer Sensitive Attributes (SAs) from the voice of user - ideally, it should not. The 12 SAs were: Race, Sexual Orientation, Political Orientation, Religious Beliefs, Trade Union Membership, Personality Characteristics, Age, Gender, Medical Conditions, Country or Region of Origin, Social Economic Status, and Profession). We used variety of voices and prepared hundreds of audio prompts containing prompt seed, and an explicit ask for the model to infer the SA. Prompt seeds were either generic truth (e.g. Fire is hot.) or first-person statement about the user that had no obvious relation to the SA (e.g. am 6 feet tall.). We then used GPT-4 to determine whether the model responses did contain an inference of the SAs. The results were as follows. Without any additional mitigation measure, Phi-4-Multimodal performed the inference of SA (ISA) on 27% of our test prompts less frequently than Qwen2-Audio (49%). For both models, Personality Characteristics and Country or Region of Origin were the SAs most likely to be inferred. ISA can be very well mitigated for Phi-4-Multimodal by using system prompt, which brings down the Defect Rate to 0.4% - comparable to the 2% we measured for GTP-4o deployed to real-time audio endpoint that uses Microsofts meta prompt to prevent ISA. In addition to these automated evaluations, extensive red teaming was performed by an independent group within Microsoft. The red teaming effort focused on the following safety areas: harmful content, self-injury risks, and exploits. Phi-4-Multimodal was found to be more susceptible to providing undesirable outputs when attacked with context manipulation or persuasive techniques. These findings apply to all languages, with the susceptibility to persuasive techniques mostly affecting French and Italian."
        },
        {
            "title": "5.3 Vision safety",
            "content": "To assess model safety in scenarios involving both text and images, we utilized Microsofts Azure AI Evaluation SDK. This tool enables the simulation of single-turn conversations with the target model by providing prompt text and images specifically designed to elicit harmful responses. The target models responses are then evaluated by fine-tuned GPT-4o model across multiple harm categories, including violence, sexual content, self-harm, hateful or unfair content. Each response is assigned severity score based on the level of harm identified. We compared the vision safety evaluation of Phi-4-Multimodal with those of Phi-3.5-Vision, open-source models of comparable size, as well as OpenAI models. In addition, we ran both an internal and the public RTVLM [LLY+24] and VLGuard [ZBY+24] multimodal (text & vision) RAI benchmarks. In table 15, we compare vision safety metrics of Phi-4-Multimodal with Phi-3.5-Vision, the open-source models Llava-1.6 [LLL+24] and Qwen-VL-Chat [BBY+23], as well as GPT4-V [Ope23]. Text & Vision Safety Evaluation Internal (private) RTVLM (public) VLGuard (public) Phi-4-Multimodal Phi-3.5-Vision Llava-1.6 Vicuna Qwen-VL-Chat GPT4-V 7.96 6.39 8.91 8.16 5.44 9. 5.44 3.86 5.62 7.27 4.78 8.33 8.55 6.81 8.90 Table 15: Model safety evaluation for vision and text scenarios using public and private multi-modal RAI benchmarks. Note that all metrics in the table are bound between [0,10], with higher values indicating safer models."
        },
        {
            "title": "6 Weaknesses and limitations",
            "content": "Due to the model size limitation, the model could not remember some specific facts such as information of Olympic games results. Also, multilingual capability is limited by the number of model parameters. As we emphasize more on the coding data, multilingual data ratio went down. This results in worse performance on other languages than English. Like every other model, both Phi-4-Mini and Phi-4-Multimodal can sometimes output undesirable content. We stress the importance for developers to implement application-level measures to further mitigate the impact of harmful responses. Mitigation strategies include (but are not limited to) system prompts, content filters, etc. Phi-4-Multimodal is not designed or intended to be used as biometric categorization system to categorize individuals based on their biometric data to deduce or infer their race, political opinions, trade union membership, religious or philosophical beliefs, sex life, or sexual orientation."
        },
        {
            "title": "References",
            "content": "[AAB+24] Marah Abdin, Jyoti Aneja, Harkirat Behl, Sebastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905, 2024. [ABD+20] [ADL+22] Rosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber. Common voice: massively-multilingual speech corpus. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 42184222, Marseille, France, May 2020. European Language Resources Association. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. [AI23] Meta AI. Introducing meta llama 3: The most capable openly available llm to date, 2023. [AJA+24] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. [ALTdJ+23] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multihead checkpoints. arXiv preprint arXiv:2305.13245, 2023. [Ant24] [AON+21] AI Anthropic. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card, 2024. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. 24 [BBC+24] [BBY+23] [BCM+23] [BJN+22] Johan Bjorck, Alon Benhaim, Vishrav Chaudhary, Furu Wei, and Xia Song. Scaling optimal lr across token horizons, 2024. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. Loc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, PaulAmbroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, et al. Seamlessm4t-massively multilingual & multimodal machine translation. arXiv preprint arXiv:2308.11596, 2023. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac HatfieldDodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training helpful and harmless assistant with reinforcement learning from human feedback, 2022. [BSA+24] Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul Rottger, Dan Jurafsky, Tatsunori Hashimoto, and James Zou. Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions, 2024. [BZGC19] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. arXiv preprint arXiv:1911.11641, 2019. [CAB+05] [CCE+18] [CKB+21] [CLC+19] [CMK+23] Jean Carletta, Simone Ashby, Sebastien Bourban, Matthew Flynn, Mael Guillemot, Thomas Hain, Jaroslav Kadlec, Vasilis Karaiskos, Wessel Kraaij, Melissa Kronenthal, et al. The ami meeting corpus: pre-announcement. In International Workshop on Machine Learning for Multimodal Interaction, pages 2839. Springer, 2005. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 29242936, 2019. Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth Dalmia, Jason Riesa, Clara Rivera, and Ankur Bapna. Fleurs: Few-shot learning evaluation of universal representations of speech. In 2022 IEEE Spoken Language Technology Workshop (SLT), pages 798805. IEEE, 2023. 25 [CTJ+21] [CWC+24] [CWT+24] [CWW+24] [CXY+24] [DCL+24] [DJP+24] [DLB+21] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Internvl: Scaling up vision foundation Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, et al. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759, 2024. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. dataset of information-seeking questions and answers anchored in research papers. 2021. [DLVNN+23] Viet Dac Lai, Chien Van Nguyen, Nghia Trung Ngo, Thuat Nguyen, Franck Dernoncourt, Ryan Rossi, and Thien Huu Nguyen. Okapi: Instruction-tuned large language models in multiple languages with reinforcement learning from human feedback. arXiv e-prints, pages arXiv2307, 2023. 26 [DLW+24] Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nvlm: Open frontierclass multimodal llms. arXiv preprint arXiv:2409.11402, 2024. [DZZ+24a] Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang. Longrope: Extending llm context window beyond 2 million tokens, 2024. [DZZ+24b] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et al. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420, 2024. [FDL+24] [FHL+24] [FHL+25] [FWL+24] [GLL+23] [GQC+20] [GYZ+25] [HBB+20] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390, 2024. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In European Conference on Computer Vision, pages 148166. Springer, 2025. Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Ke Li, Junteng Jia, Yuan Shangguan, Jay Mahadeokar, Ozlem Kalinli, Christian Fuegen, and Mike Seltzer. Audiochatllama: Towards general-purpose speech abilities for llms. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 55225532, 2024. Yuan Gong, Alexander Liu, Hongyin Luo, Leonid Karlinsky, and James Glass. Joint audio and speech understanding. In 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2023. Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. Conformer: Convolution-augmented transformer for speech recognition. In 21st Annual Conference of the International Speech Communication Association, Interspeech 2020, Virtual Event, Shanghai, China, October 25-29, 2020, pages 50365040. ISCA, 2020. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. 27 [HBK+21a] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset, 2021. [HBK+21b] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. [HLG+24] [HSW+22] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. [HWY+24] Weiquan Huang, Aoqi Wu, Yifan Yang, Xufang Luo, Yuqing Yang, Liang Hu, Qi Dai, Xiyang Dai, Dongdong Chen, Chong Luo, et al. Llm2clip: Powerful language model unlock richer visual representation. arXiv preprint arXiv:2411.04997, 2024. [JHG+24] [JLD+23] [KSK+16] [Lab25] [LBX+24] [LDZ+23] [LDZ+24] [LGJ+21] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Chi Zhang, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. Beavertails: Towards improved safety alignment of llm via human-preference dataset, 2023. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images, 2016. Bespoke Labs. Bespoke-stratos: The unreasonable effectiveness of reasoning distillation. https://www.bespokelabs.ai/blog/bespoke-stratos-the-unreasonable-effectiveness-ofreasoning-distillation, 2025. Accessed: 2025-01-22. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts, 2024. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models, 2023. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player?, 2024. Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and SongChun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning, 2021. 28 [LHE22] [LKB+23] [LLH+24] [LLL+24] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods, 2022. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12):220102, 2024. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. [LLWL24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. [LLY+24] [LMX+22] [LXWZ23] [LZG+24] Mukai Li, Lei Li, Yuwei Yin, Masood Ahmed, Zhenguang Liu, and Qi Liu. Red teaming visual language models. arXiv preprint arXiv:2401.12915, 2024. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. arXiv preprint arXiv:2305.01210, 2023. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [MAA24] MAA. American invitational mathematics examinationaime. In American Invitational Mathematics ExaminationAIME 2024, February 2024. [MBT+22] Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 16971706, 2022. [MCKS18] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering, 2018. [Mic24] Microsoft. Phi-3 safety post-training: Aligning language models with break-fix cycle. arXiv preprint arXiv:2407.13833, 2024. [Mis24] AI Mistral. Un ministral, des ministraux. Ministral, 2024. [MKJ21] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. 29 [MLT+22] [MYS+25] [Ope23] [Pos18] [RHS+] [RHS+23] [RKV+23] [RKX+23] Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: In benchmark for question answering about charts with visual and logical reasoning. Findings of the Association for Computational Linguistics: ACL 2022, pages 22632279, Dublin, Ireland, May 2022. Association for Computational Linguistics. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. OpenAI. Gpt-4v(ision) system card, 2023. https://cdn.openai.com/papers/ GPTV_System_Card.pdf. Matt Post. call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186191, Brussels, Belgium, October 2018. Association for Computational Linguistics. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level googleproof q&a benchmark. In First Conference on Language Modeling. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: graduate-level googleproof q&a benchmark, 2023. Paul Rottger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, and Dirk Hovy. Xstest: test suite for identifying exaggerated safety behaviours in large language models. 2023. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202, pages 2849228518. PMLR, 2023. [SAL+24] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [SLBBC19] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. arXiv preprint arXiv:1907.10641, 2019. [SMK+23] [SNS+19] [SRC+19] Vaibhav Srivastav, Somshubra Majumdar, Nithin Koluguri, Adel Moumen, Sanchit Gandhi, Hugging Face Team, Nvidia NeMo Team, and SpeechBrain Team. Open automatic speech recognition leaderboard. urlhttps://huggingface.co/spaces/huggingface.co/spaces/open-asrleaderboard/leaderboard, 2023. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read, 2019. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019. 30 [SRR+22] [SSF+22] [SSS+22] [STK+24] [TAB+23] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adri`a Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Language models are multilingual chain-of-thought reasoners, 2022. Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them, 2022. Sakshi, Utkarsh Tyagi, Sonal Kumar, Ashish Seth, Ramaneswaran Selvakumar, Oriol Nieto, Ramani Duraiswami, Sreyan Ghosh, and Dinesh Manocha. Mmau: massive multitask audio understanding and reasoning benchmark. arXiv preprint arXiv:2410.19168, 2024. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [Tea25a] OpenThoughts Team. Open Thoughts. https://open-thoughts.ai, January 2025. [Tea25b] Qwen Team. Qwen2.5-vl, January 2025. [TGL+24] [TRP+24] [VSP+17] [WBT+24] [WDR+24] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Leonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Rame, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, (cid:32)L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30, 2017. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, Chinmay Hegde, Yann LeCun, Tom Goldstein, Willie Neiswanger, and Micah Goldblum. Livebench: challenging, contamination-free llm benchmark. 2024. 31 [WMZ+24] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [WPC+22] Alex Wang, Richard Yuanzhe Pang, Angelica Chen, Jason Phang, and Samuel R. Bowman. SQuALITY: Building long-document summarization dataset the hard way. arXiv preprint 2205.11465, 2022. [WWGP21] Changhan Wang, Anne Wu, Jiatao Gu, and Juan Pino. Covost 2 and massively multilingual speech translation. In Proceedings of Interspeech 2021, pages 22472251, 2021. [XW24] [YHX+25] [YMJ+24] [YNZ+23] [YXL+24] [YYZ+24] [YZN+24] [YZY+18] [ZBY+24] Zhifei Xie and Changqiao Wu. Mini-omni2: Towards open-source gpt-4o with vision, speech and duplex capabilities. arXiv preprint arXiv:2410.11190, 2024. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387, 2025. Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. Berkeley function calling leaderboard. 2024. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi, 2023. Qian Yang, Jin Xu, Wenrui Liu, Yunfei Chu, Ziyue Jiang, Xiaohuan Zhou, Yichong Leng, Yuanjun Lv, Zhou Zhao, Chang Zhou, and Jingren Zhou. AIR-bench: Benchmarking large audio-language models via generative comprehension. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 19791998. Association for Computational Linguistics, August 2024. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, et al. Mmmu-pro: more robust multidiscipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, et al. Spider: large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. arXiv preprint arXiv:1809.08887, 2018. Yongshuo Zong, Ondrej Bohdal, Tingyang Yu, Yongxin Yang, and Timothy Hospedales. Safety fine-tuning at (almost) no cost: baseline for vision large language models. arXiv preprint arXiv:2402.02207, 2024. 32 [ZCS+23] [ZDC+24] [ZDL+24] [ZDW+23] [ZHB+19] [ZLM+23] [ZVC+24] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mtbench and chatbot arena. Advances in Neural Information Processing Systems, 36:46595 46623, 2023. Pan Zhang, Xiaoyi Dong, Yuhang Cao, Yuhang Zang, Rui Qian, Xilin Wei, Lin Chen, Yifei Li, Junbo Niu, Shuangrui Ding, et al. Internlm-xcomposer2. 5-omnilive: comprehensive multimodal system for long-term streaming video and audio interactions. arXiv preprint arXiv:2412.09596, 2024. Aohan Zeng, Zhengxiao Du, Mingdao Liu, Kedong Wang, Shengmin Jiang, Lei Zhao, Yuxiao Dong, and Jie Tang. Glm-4-voice: Towards intelligent and human-like end-to-end spoken chatbot. arXiv preprint arXiv:2412.02612, 2024. Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Haodong Duan, Songyang Zhang, Shuangrui Ding, et al. Internlm-xcomposer: visionlanguage large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112, 2023. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 47914800, 2019. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models, 2023. Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877, 2024. 33 Prompt for GPT-4 as Judge on speech benchmarks We use GPT-4-0613 as judge model for speech benchmarks, including synthetic MT-Bench, AirBenchChat, and Summarization tasks as shown in Table 3. Here are the scoring prompts used for different evaluation sets: Listing 1: GPT-4 Scoring Prompt for MT-Bench turn1 (default) 1 { 2 3 5 \"sys_template\": \"You are helpful assistant.\", \"user_template\": \" [Instruction] Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. Begin your evaluation by providing short explanation. Be as objective as possible. After providing your explanation, you must rate the response on scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\". 6 7 9 10 11 12 \" 13 14 } [Question] {question placeholder} [The Start of Assistant's Answer] {answer placeholder} [The End of Assistant's Answer] 1 { 2 3 5 6 7 8 9 11 12 13 14 Listing 2: GPT-4 Scoring Prompt for MT-Bench turn-1 (math and code) \"sys_template\": \"You are helpful assistant.\", \"user_template\": \" [Instruction] Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider correctness and helpfulness. You will be given reference answer and the assistant's answer. Begin your evaluation by comparing the assistant's answer with the reference answer. Identify and correct any mistakes. Be as objective as possible. After providing your explanation, you must rate the response on scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\". [Question] {question placeholder} [The Start of Reference Answer] {ref_answer placeholder} [The End of Reference Answer] [The Start of Assistant's Answer] 34 {answer placeholder} [The End of Assistant's Answer] 15 16 17 18 } \" Listing 3: GPT-4 Scoring Prompt for MT-Bench turn-2 (default) \"sys_template\": \" Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. You evaluation should focus on the assistant's answer to the second user question. Begin your evaluation by providing short explanation. Be as objective as possible. After providing your explanation, you must rate the response on scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\". \", \"user_template\": \" The Start of Assistant A's Conversation with User ### User: {question_1} ### Assistant A: {answer_1} ### User: {question_2} ### Assistant A: {answer_2} The End of Assistant A's Conversation with User 1 { 2 3 4 5 6 8 9 10 11 12 14 15 16 17 18 20 \" 21 22 } 1 { 2 Listing 4: GPT-4 Scoring Prompt for MT-Bench turn-2 (math and code) \"sys_template\": \" Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question. Your evaluation should consider correctness and helpfulness. You will be given reference answer and the assistant's answer. You evaluation should focus on the assistant's answer to the second question. Begin your evaluation by comparing the assistant's answer with the reference answer. Identify and correct any mistakes. Be as objective as possible. After providing your explanation, you must rate the response on scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\". 5 \", \"user_template\": \" 35 6 7 9 10 11 12 13 15 16 17 18 19 21 22 23 24 25 27 28 29 30 31 33 34 35 36 37 \" 38 39 } 1 { The Start of Reference Answer ### User: {question_1} ### Reference answer: {ref_answer_1} ### User: {question_2} ### Reference answer: {ref_answer_2} The End of Reference Answer The Start of Assistant A's Conversation with User ### User: {question_1} ### Assistant A: {answer_1} ### User: {question_2} ### Assistant A: {answer_2} The End of Assistant A's Conversation with User Listing 5: GPT-4 Scoring Prompt for AirBench-Chat 2 3 5 6 7 8 9 11 12 13 14 15 \"user_template\": \""
        },
        {
            "title": "You are a helpful and precise assistant for checking the quality of the",
            "content": "answer. [Detailed Audio Description] {meta_info} [Question] {question} [The Start of Assistant 1s Answer] {reference} [The End of Assistant 1s Answer] [The Start of Assistant 2s Answer] {ai_response} [The End of Assistant 2s Answer] [System] We would like to request your feedback on the performance of two AI assistants in response to the user question and audio description 36 displayed above. AI assistants are provided with detailed audio descriptions and questions. Please rate the helpfulness, relevance, accuracy, and comprehensiveness of their responses. Each assistant receives an overall score on scale of 1 to 10, where higher score indicates better overall performance. Please output single line containing only two values indicating the scores for Assistant 1 and 2, respectively. The two scores are separated by space. \" 17 18 } Listing 6: GPT-4 Scoring Prompt for Speech Summarization-Overall Score 1 You are skilled evaluator for summaries generated based on user-provided instructions. prominent organization has enlisted your help to assess the overall quality of summary by focusing on how effectively it adheres to the user's specific instructions. Rate the summary on scale of 1 to 7 based on the following criteria: 2 3 1. If the summary fulfills the user's instructions comprehensively, accurately captures the required details, excludes any explicitly prohibited information, maintains the correct level of detail, adheres to the requested structure (e.g ., bullet points, paragraphs), and is both fluent and coherent, assign score of 7. The summary should read naturally, resembling human-written summary. Coherence means ideas are logical and well-connected, with smooth transitions. 4 5 2. If the summary mostly fulfills the user instructions but has minor issues, such as slight deviations in structure, missing small details, or minor readability issues, assign score of 5-6, depending on the severity of the deviation. Consider whether the issues are easy to fix and whether they affect the summary 's usability. 6 7 3. If the summary fulfills the majority of the instructions but includes unimportant or extra information, omits key details specified by the user, or diverges slightly in structure or emphasis, assign score of 4-5, depending on the significance of the issues. Weigh the importance of missing or extraneous content against the clarity and adherence to instructions. 8 9 4. If the summary partially adheres to the instructions, capturing some of the requested details but introducing inconsistencies, hallucinations, or irrelevant content, assign score of 2-4, depending on the extent of the deviations and errors. Penalize for any explicitly prohibited content that has been included. 10 11 5. If the summary minimally adheres to the instructions, misses most of the required details, includes significant irrelevant or hallucinated content, or ignores the specified structure or tone, assign score of 1-3, depending on the severity of the shortcomings. 12 13 6. If the summary fails to follow the user's instructions altogether, missing all critical requirements or containing high proportion of irrelevant or fabricated content, assign score of 1. This includes summaries that fail to meet any formatting, detail, or exclusion criteria. 14 37 15 Here is the input document, user instruction and the corresponding summary. 16 Source: ``` 17 18 {src} ``` 19 20 User Instruction: ``` 21 22 {instruction} ``` 23 24 Summary ``` 25 26 {tgt} ``` 27 28 Note: It is helpful to read the summary first, before reading the source document."
        },
        {
            "title": "This will allow you to judge whether you understand the main contents of the",
            "content": "source document through the summary alone. Afterward, you can assess to what extent the summary accurately reflects the source document. 29 30 Note: Based on the above criteria and assign overall score of summary in the scale 1-7. If the summary is not provided for evaluation, return \"N/A\". Besides the score, you should also provide **brief** explanation. 31 32 Note: Use the following json format for easy downstream consumption. 33 34 {{ 35 36 37 }} \"explanation\": \"judge the summary based on the given criteria and explain your reasoning for the score you are going to give in the next field.\", \"score\": THE_SCORE_VALUE Authors (alphabetical) Abdelrahman Abouelenin Atabak Ashfaq Adam Atkinson Hany Awadalla Nguyen Bach Jianmin Bao Alon Benhaim Martin Cai Vishrav Chaudhary Congcong Chen Dong Chen Dongdong Chen Junkun Chen Weizhu Chen Yen-Chun Chen Yi-ling Chen Qi Dai Xiyang Dai Ruchao Fan Mei Gao Min Gao Amit Garg Abhishek Goswami Junheng Hao Amr Hendy Yuxuan Hu Xin Jin Mahmoud Khademi Dongwoo Kim Young Jin Kim Gina Lee Jinyu Li Yunsheng Li Chen Liang Xihui Lin Zeqi Lin Mengchen Liu Yang Liu Gilsinia Lopez Chong Luo Piyush Madan Vadim Mazalov Ali Mousavi Anh Nguyen Jing Pan Daniel Perez-Becker Jacob Platin Thomas Portet Kai Qiu"
        },
        {
            "title": "Bo Ren\nLiliang Ren\nSambuddha Roy\nNing Shang\nYelong Shen\nSaksham Singhal\nSubhojit Som\nXia Song\nTetyana Sych\nPraneetha Vaddamanu\nShuohang Wang\nYiming Wang\nZhenghao Wang\nHaibin Wu\nHaoran Xu\nWeijian Xu\nYifan Yang\nZiyi Yang\nDonghan Yu\nIshmam Zabir\nJianwen Zhang\nLi Lyna Zhang\nYunan Zhang\nXiren Zhou",
            "content": ""
        }
    ],
    "affiliations": [
        "Microsoft"
    ]
}