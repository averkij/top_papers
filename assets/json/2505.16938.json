{
    "paper_title": "NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification",
    "authors": [
        "NovelSeek Team",
        "Bo Zhang",
        "Shiyang Feng",
        "Xiangchao Yan",
        "Jiakang Yuan",
        "Zhiyin Yu",
        "Xiaohan He",
        "Songtao Huang",
        "Shaowei Hou",
        "Zheng Nie",
        "Zhilong Wang",
        "Jinyao Liu",
        "Runmin Ma",
        "Tianshuo Peng",
        "Peng Ye",
        "Dongzhan Zhou",
        "Shufei Zhang",
        "Xiaosong Wang",
        "Yilan Zhang",
        "Meng Li",
        "Zhongying Tu",
        "Xiangyu Yue",
        "Wangli Ouyang",
        "Bowen Zhou",
        "Lei Bai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Artificial Intelligence (AI) is accelerating the transformation of scientific research paradigms, not only enhancing research efficiency but also driving innovation. We introduce NovelSeek, a unified closed-loop multi-agent framework to conduct Autonomous Scientific Research (ASR) across various scientific research fields, enabling researchers to tackle complicated problems in these fields with unprecedented speed and precision. NovelSeek highlights three key advantages: 1) Scalability: NovelSeek has demonstrated its versatility across 12 scientific research tasks, capable of generating innovative ideas to enhance the performance of baseline code. 2) Interactivity: NovelSeek provides an interface for human expert feedback and multi-agent interaction in automated end-to-end processes, allowing for the seamless integration of domain expert knowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in several scientific fields with significantly less time cost compared to human efforts. For instance, in reaction yield prediction, it increased from 27.6% to 35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from 0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation, precision advanced from 78.8% to 81.0% in a mere 30 hours."
        },
        {
            "title": "Start",
            "content": "NovelSeek: Starting Point of Innovation NOVELSEEK: When Agent Becomes the Scientist Building Closed-Loop System from Hypothesis to Verification NovelSeek Team, Shanghai Artificial Intelligence Laboratory https://alpha-innovator.github.io/NovelSeek-project-page https://github.com/Alpha-Innovator/NovelSeek https://huggingface.co/U4R/NovelSeek 5 2 0 2 2 ] . [ 1 8 3 9 6 1 . 5 0 5 2 : r Figure 1: NOVELSEEK can support 12 types of scientific research tasks ranging from the AI field to the science field, including reaction yield prediction, molecular dynamics, power flow estimation, time series forecasting, transcription prediction, enhancer activity prediction, sentiment classification, 2D image classification, 3D point classification, 2D semantic segmentation, 3D autonomous driving, large vision-language model fine-tuning. 1 NovelSeek: Starting Point of Innovation"
        },
        {
            "title": "Abstract",
            "content": "Artificial Intelligence (AI) is accelerating the transformation of scientific research paradigms, not only enhancing research efficiency but also driving innovation. We introduce NOVELSEEK, unified closed-loop multi-agent framework to conduct Autonomous Scientific Research (ASR) across various scientific research fields, enabling researchers to tackle complicated problems in these fields with unprecedented speed and precision. NOVELSEEK highlights three key advantages: 1) Scalability: NOVELSEEK has demonstrated its versatility across 12 scientific research tasks, capable of generating innovative ideas to enhance the performance of baseline code. 2) Interactivity: NOVELSEEK provides an interface for human expert feedback and multi-agent interaction in automated end-to-end processes, allowing for the seamless integration of domain expert knowledge. 3) Efficiency: NOVELSEEK has achieved promising performance gains in several scientific fields with significantly less time cost compared to human efforts. For instance, in reaction yield prediction, it increased from 27.6% to 35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from 0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation, precision advanced from 78.8% to 81.0% in mere 30 hours."
        },
        {
            "title": "Introduction",
            "content": "Autonomous Scientific Discovery (ASD) refers to the use of Large Language Models (LLMs) (Yang et al., 2024; Chen et al., 2024; Wang et al., 2024a; Guo et al., 2025) and robotics to independently perform scientific research without direct human intervention (Yuan et al., 2025; Yan et al., 2025; Gottweis et al., 2025; Yamada et al., 2025; Lu et al., 2024). This approach holds transformative potential for accelerating the pace of discovery across various scientific domains. By automating tasks such as data analysis, hypothesis generation, experiment design, and result interpretation, automated systems (Yuan et al., 2025; Lu et al., 2024) can efficiently process vast amounts of information and uncover patterns or insights that may be difficult for human researchers to detect. ASR, while promising, faces significant challenges in generating effective and novel proposals, as well as achieving closed-loop feedback for the experimental validation of these proposals: First, generating proposals that are both effective and novel is complicated task. Autonomous systems must identify research gaps and generate hypotheses that are not only innovative but also scientifically valid. This requires balancing creativity and rigor, which is difficult for AI models that rely on existing data and patterns. Additionally, ensuring the novelty of proposals often demands deep understanding of the broader scientific context, which can be challenging for models limited by the quality and scope of their training data. Second, implementing closed-loop feedback for end-to-end experimental validation is another major hurdle. Autonomous systems need to design experiments, execute them, analyze results, and iteratively refine their hypotheses in seamless loop. This requires integration across multiple domains, such as robotics for experiment execution and advanced analytics for result interpretation. Furthermore, real-world experiments often come with unexpected variables and noise, making it challenging for autonomous systems to adapt and learn effectively. Achieving truly closedloop system demands robust coordination, adaptability, and the ability to handle uncertainty, which remain technical and conceptual barriers. To further facilitate the advancement of ASR, we propose the NOVELSEEK, an end-to-end auto-research pipeline, which covers four main modules: self-evolving idea generation, human-interactive feedback, idea-to-methodology construction, and multi-round experiment planning and execution. With the help of the Self-Evolving Human-interactive Idea NovelSeek: Starting Point of Innovation Generation and Idea-to-Methodology Construction, NOVELSEEK can transform rough proposal into detailed and easily implementable method, which further increases the success rate of code implementation process and enhances the efficiency of closed-loop experiments. Besides, by leveraging multi-round experiment planning and execution, NOVELSEEK is capable of designing experimental plans and decomposing the experimental process according to the NOVELSEEK-proposed modules, thereby validating the effectiveness of each NOVELSEEK-generated module through experimentation. As shown in Fig. 1, NOVELSEEK has been validated across 12 scientific research tasks, and we are excited to see that the experimental results demonstrate the significant value of NOVELSEEK in the entire process from hypothesis generation to experimental validation. For instance, in the Reaction Yield Prediction task, the baseline model only achieved performance of 24.2% 4.2, while our model improved it to 34.8% 1.1 in just 12 hours. In contrast, human researchers typically require several months to achieve similar level of performance improvement. Another example of performance improvement is the Enhancer Activity Prediction task. The baseline model, DeepSTARR, achieved result of 0.52. By utilizing NOVELSEEK to search relevant domain literature, automatically generate code, and conduct validation, the performance can be improved to 0.79, representing promising enhancement. In addition, NOVELSEEK also supports complex project-level modifications and debugging, which consist of multiple code files. These results clearly indicate that NOVELSEEK can autonomously generate ideas and design algorithms, effectively reducing the dependence on human effort in scientific research. To facilitate reproducibility, we have open-sourced both the baselines and the codes generated by NOVELSEEK used in all involved scientific research tasks at https://github.com/Alpha-Innovator/NovelSeek. Furthermore, the contributions of this paper are summarized below: Unified Multi-agent Framework for Diverse Scientific Research Tasks: We present NOVELSEEK, unified closed-loop scientific research framework that can automate the entire research cycle, including idea generation, idea-to-methodology transformation, experiment execution, and result feedback. This unified framework can be directly applied to various scientific research scenarios and fields. Interactive Interfaces for Enhanced Cooperative Research: NOVELSEEK offers interactive interfaces for human-machine collaboration within the idea generation module and across the entire system. By selecting collaboration modes, such as leveraging AI or human experts, it provides evaluations of idea generation effectiveness and facilitates the assessment, reflection, and documentation of experimental results. Comprehensive Experimental Validation and Human Studies: We conducted extensive human studies centered around NOVELSEEK, including inviting domain experts to evaluate and score the novelty of ideas generated by NOVELSEEK, and comparing the research efficiency between human researchers and NOVELSEEK. These experiments and human studies are crucial for gaining insights into the capabilities of multi-agent systems in conducting scientific research tasks in open-ended environments. We observed many promising phenomena, while also identifying certain technical modules that require improvement."
        },
        {
            "title": "2 NOVELSEEK",
            "content": "As unified closed-loop multi-agent framework for ASR, NOVELSEEK is designed to facilitate innovative research across diverse scientific domains, as illustrated in Fig. 2. It incorporates three primary capabilities: self-evolving idea generation with human-interactive feedback (Sec. 2.1), comprehensive idea-to-methodology construction (Sec. 2.2), and multiround automated experiment execution (Sec. 2.3). Each capability is realized through the collaboration of specialized agents, allowing for seamless integration of different processes to enhance scientific discoveries. 3 NovelSeek: Starting Point of Innovation Figure 2: NOVELSEEK covers three main capabilities: 1) Self-evolving Idea Generation with Human-interactive Feedback, 2) Idea-to-Methodology Construction, and 3) Evolutionary Experimental Planning and Execution. 2.1 Self-Evolving Idea Generation with Human-interactive Feedback The self-evolving idea generation capability is central to NOVELSEEK, enabling the framework to autonomously generate and refine innovative research ideas. This process involves several specialized agents, each contributing to different stages of idea development and refinement. Survey Agent. The Survey Agent is designed to meet the diverse needs of various scientific research tasks by adaptively aligning with user-specified requirements and the necessary depth of detail for exploring existing methodologies. This adaptability is crucial for effectively generating new ideas across different research contexts, and the primary responsibility of the Survey Agent is to search for scientific papers, offering two distinct modes to address the varying needs for depth and breadth in literature research during the scientific discovery process: 1) the literature review mode and 2) the deep research mode. In the literature review mode, the Survey Agent deconstructs the research task into multiple keyword combinations, enabling broad search across various academic databases. It collects scientific literature from these sources and evaluates the relevance of each document by analyzing abstracts in relation to the task at hand. Denote the keyword generation process by the function : K, where represents the descriptions of research tasks and is the set of generated keyword combinations. The relevance evaluation of each document can be represented by the function: : Labs [0, 1], (1) where Labs is the abstract of retrieved literature L, and R(r, t) measures the relevance of literature to the task as floating-point score between 0 and 1, with higher scores indicating greater relevance. 4 NovelSeek: Starting Point of Innovation In the deep research mode, following the initial literature survey, the Survey Agent downloads and thoroughly examines the full texts of relevant scientific papers. This deeper analysis allows it to generate new keyword combinations, facilitating further rounds of literature exploration. The process of generating new keywords can be denoted by the function: : K, (2) where is the expanded set of keyword combinations generated from the detailed analysis of full texts. By dynamically adjusting its search strategies based on the context of the research stage, the Survey Agent ensures comprehensive and nuanced understanding of the research landscape. This capability not only supports the generation of innovative ideas but also ensures that the NOVELSEEK framework remains at the cutting edge of scientific discovery. Code Review Agent. The Code Review Agent is crucial for understanding baseline codes for different research tasks, serving as foundation for innovation by identifying improvements and developing new methodologies. It provides detailed analyses of code structures, dependencies, and functionalities, enabling NOVELSEEK to fully comprehend existing code-bases and identify potential enhancements to advance research objectives. Moreover, the agents ability to document and summarize complex code-bases ensures efficient navigation and utilization of existing methods. The agent manages two scenarios: 1) reviewing user-provided code or 2) searching for relevant code-bases. For user-uploaded code, it conducts comprehensive review of the structure, logic, and functionality. Alternatively, in the absence of user-uploaded code, it searches public repositories like GitHub to find relevant code-bases, performing thorough analyses at both the repository and file levels to understand inter-dependencies and assess logic, efficiency, and correctness. Furthermore, the agent employs static code analysis using Pythons ast module to parse and understand code structure without execution, while the LLM generates human-readable descriptions and summaries, transforming technical details into structured documentation. By using parallel processing with Pythons multiprocessing module, the agent enhances efficiency and scalability for large code-bases. Overall, the Code Review Agent offers detailed documentation that deepens the understanding of code repositories and supports innovation in scientific research. Idea Innovation Agent. The Idea Innovation Agent is an integral part of NOVELSEEK, designed to enhance the creative and iterative processes of scientific research. This agent plays crucial role by automating the generation and evolution of ideas, thereby addressing the limitations of traditional research works (Yuan et al., 2025; Yamada et al., 2025; Lu et al., 2024), which often rely on time-consuming manual efforts and are constrained by human cognitive biases. The agents dual responsibilities, idea generation and idea evolution, are specifically designed to address the diverse needs of various scientific disciplines. In the context of idea generation, the agent utilizes general LLM configured with higher temperature setting. This configuration encourages the generation of more diverse and creative outputs. This enables the agent to identify patterns and insights that might be overlooked in traditional research, generating novel hypotheses and strategies based on task definitions, baseline methods, and current scientific knowledge. The process can be represented by the function: : (T , B, L) I, (3) where denotes analysis of baseline methods and is the set of generated ideas. The LLM facilitates the exploration of broader spectrum of possibilities, accelerating the pace of discovery and innovation by leveraging its comprehensive understanding of language and context. Idea evolution leverages the capabilities of an LLM to improve existing ideas. The process involves analyzing the content of these ideas, incorporating reflections, which include evaluations of novelty, feasibility, and scientific validity, and integrating insights from related literature. This approach enables the generation of refined and innovative ideas by addressing the inherent limitations of initial concepts. The process can be represented by the same function: : (I, C, L) , (4) 5 NovelSeek: Starting Point of Innovation where is the initial set of ideas, denotes the critique, and is the set of evolved ideas. Overall, the Idea Innovation Agent enhances scientific ideas into viable and creative solutions by synthesizing and contextualizing information. It critically examines current ideas and employs feedback loops with human experts and other NOVELSEEK agents for continuous improvement. This iterative process balances novelty, feasibility, and ethical considerations, producing impactful and well-rounded ideas. Assessment Agent. The Assessment Agent is vital component of NOVELSEEK, designed to ensure the quality and viability of generated ideas through rigorous evaluation process. In the rapidly evolving landscape of scientific research, the systematic and objective assessment of ideas is essential. Traditional methods often suffer from subjectivity and lack comprehensive coverage of all relevant dimensions, which can lead to promising ideas being overlooked(Qiu et al., 2025; Si et al., 2024). Therefore, the Assessment Agent addresses these challenges by providing structured and multidimensional evaluation process, which in turn enhances the reliability and effectiveness of idea selection. The primary responsibility of the Assessment Agent is to critically evaluate ideas using multidimensional scoring. Each idea is analyzed across key dimensions: coherence, credibility, verifiability, novelty, and alignment. Coherence checks the logical consistency and structure of the idea, while credibility assesses its trustworthiness based on existing knowledge. Verifiability examines the ideas testability through empirical methods. Novelty measures originality, and alignment ensures consistency with research goals. Moreover, for each dimension, the Assessment Agent provides detailed evaluation narrative to explain its reasoning. It assigns scores from 0 to 10, which are combined using weighted summation to produce an overall score for each idea, aiding in the ranking process. By utilizing advanced LLMs, the agent can accurately process and evaluate complex scientific concepts. This capability allows comprehensive assessment that includes both qualitative and quantitative aspects, ensuring the evaluation is thorough and well-rounded. Furthermore, the Assessment Agent possesses the ability to ensure diversity among topranked ideas. This capability prevents high-scoring ideas from being overly similar or derived from the same original concept. By promoting varied pool of ideas, the agent encourages the exploration of diverse pathways in the research process. This is crucial for maintaining balance between innovation and practicality, ensuring that the most promising ideas are both high-quality and distinct from each other. In summary, by employing LLMs for multidimensional scoring and leveraging its ability to promote diversity among ideas, the Assessment Agent ensures that only the most viable and innovative concepts are selected for further development. This process not only enhances the efficiency of the research cycle but also fosters more dynamic and diverse research environment. Human-interactive Feedback. In the context of multi-agent systems, human-interactive feedback is crucial component for effectively managing and solving complex tasks. This integration of human insights enables agents to navigate dynamic environments more effectively, aligning their outputs with complex user requirements and ensuring practical applicability. human-interactive feedback automatically generated by agent. feedback mechanism of NOVELSEEK categoThe feedback directly provided by humans and rized into two primary types: 2) Human-provided feedback can address one or multiple ideas, offering insights and critiques that lead to further refinement and adjustment of these ideas based on the feedback received. This iterative process facilitates the continuous improvement of ideas, ensuring they are honed to meet specific objectives and challenges. 1) is For example, in scenario involving medical image segmentation, an LLM multi-agent system might initially propose broad idea focused on developing more advanced segmentation algorithms. However, human feedback can refine this idea by directing attention specifically to the medical domain. Human experts can provide insights that encourage the 6 NovelSeek: Starting Point of Innovation Figure 3: NOVELSEEK Self-evolutionary path of ideas for reaction yield prediction task. development of adaptive solutions tailored to the unique challenges of medical imaging, such as handling diverse tissue types and ensuring high accuracy in identifying critical structures. This targeted feedback not only sharpens the focus of the idea but also ensures it aligns with the specific needs and priorities of medical research, enhancing its practical applicability and impact. Orchestration Agent. The Orchestration Agent coordinates all other agents within the system, facilitating collaboration by synchronizing tasks and managing data flow. This ensures the process remains efficient, coherent, and aligned with research objectives, allowing the framework to function as an effective research tool. Central to the Orchestration Agents role is designing and managing workflows among agents like the Survey Agent, Code Review Agent, Idea Innovation Agent, and Assessment Agent. It also oversees the timing of human feedback, especially for high-scoring ideas. This requires understanding each agents capabilities and their interactions to optimize task execution and completion. For example, the Survey Agent conducts adaptive literature exploration, providing insights that the Idea Innovation Agent uses to generate novel hypotheses. The Orchestration Agent ensures these findings are communicated effectively. Similarly, it synchronizes the Code Review Agents analyses to enhance idea evaluation and development. Furthermore, the Orchestration Agent manages the Assessment Agents evaluation process, ensuring timely and relevant outputs. This helps guide the development of diverse top ideas. Additionally, it determines optimal points for human feedback, integrating expert insights after identifying high-scoring ideas to refine and adapt them, aligning outputs with user requirements. In summary, as illustrated in Fig. 3, by managing multi-agent collaboration and integrating human feedback, the Orchestration Agent enables NOVELSEEK to operate as cohesive and innovative research tool, driving scientific discovery forward. 2.2 Comprehensive Idea-to-Methodology Construction The idea-to-methodology construction process systematically bridges the gap between concise research ideas and concrete, implementable methodologies, ensuring that the AINovelSeek: Starting Point of Innovation generated ideas could be realized and their validity verified. This process is orchestrated by the Methodology Development Agent, which collaborates closely with other agents and integrates both automated processes and human-interactive feedback loops to ensure that methodological development is rigorous, traceable, and practically relevant. Specifically, to develop comprehensive method corresponding to the concise research idea, the Method Development Agent possesses two core capabilities: 1) Methodology Initialization: which involves constructing the basic structure and content of method by integrating the idea with baseline codes and the methodology content of relevant literature; 2) Methodology Refinement: which iteratively enhances the basic method structure for the purpose of rigor and completeness, ensuring more detailed and robust methodology."
        },
        {
            "title": "2.2.1 Methodology Initialization",
            "content": "To convert concise research ideas into detailed methodological frameworks, the Method Development Agent uses its Methodology Initialization capability. The process begins by extracting core objectives and hypotheses from research ideas, identifying key variables, and understanding their interrelationships to construct coherent framework. The agent uses multiple resources: task descriptions provide context and constraints; baseline implementations offer adaptable methods; and relevant literature integrates existing knowledge and ensures that the framework aligns with current research. By formalizing mechanisms that require empirical investigation, the agent details processes and conditions for conducting research and specifies methods for data collection and analysis. The outcome is methodological framework that is both theoretically sound and practically executable. The transformation function is represented as: : M, (5) where denotes research ideas, includes task descriptions, represents baseline methods, is the literature corpus, and is the resulting methodological framework. Overall, through Methodology Initialization, the Method Development Agent effectively turns initial ideas into detailed, actionable methods, ready for further refinement. 2.2.2 Methodology Refinement After the initialization, the Methodology Development Agent leverages its refinement capability to critically evaluate and iteratively improve the methodological framework. The agent conducts comprehensive analysis of the initial methodology M, incorporating structured critiques C, which include both automated assessments and expert human feedback. Additionally, it synthesizes insights from the latest scientific literature L. The refinement process is formally defined as: : M, (6) where represents the initial methodology, denotes the critique space, potentially including human feedback and automated assessments, is the literature corpus, and is the refined methodological framework. During both initialization and refinement, the Methodology Development Agent collaborates closely with other agents, such as the Assessment Agent for multidimensional evaluation and the Orchestration Agent for workflow coordination. This collaboration ensures that each methodological step benefits from comprehensive feedback and current domain knowledge. The integrated, multi-agent approach guarantees that the transformation from idea to methodology is systematic and adaptable, supporting the continuous evolution and optimization of scientific research within the NOVELSEEK framework. 2.3 Evolutionary Experimental Planning and Execution 2.3.1 Exception-Guided Debugging Framework Converting theoretical concepts into functional code is challenging. To this end, we developed an exception-guided debugging framework that systematically converts abstract 8 NovelSeek: Starting Point of Innovation methodological text descriptions into executable implementation codes. This framework operates by systematically capturing runtime exceptions during execution attempts, analyzing error contexts, and formulating targeted fixes through reasoning of the large language model. Our coder module employs dual-strategy approach according to the complexity of given baseline code. For single-file or limited-scope implementation tasks, we use the Aider coding assistant (Gauthier & Contributors, 2023), which facilitates localized code modifications with minimal overhead. For complex repository-level codes requiring comprehensive structural understanding across different functions, we deploy OpenHands framework (Wang et al., 2024b), which enables thorough codebase analysis and coordinated multi-file modifications while maintaining the integrity of the overall code architecture. Once the initial code implementation is completed, the framework transitions to systematic debugging phase to ensure functionality and robustness. The debugging process follows systematic cycle: (1) execution attempt, (2) exception capture and traceback analysis, (3) contextual code structure understanding, (4) debugging strategy formulation, and (5) targeted implementation. This cycle continues iteratively until successful execution or reaching predefined iteration threshold. 2.3.2 Experimental Planning and Adaptive Evolution After establishing basic functionality through debugging, we transition to implementation planning focused on identifying critical structures and integration points. Our planning process first determines which core modules require modification, then develops step-bystep implementation strategy with clear priorities and dependencies. Implementation planning operates at multiple abstraction levels: architectural modifications for methodological alignment, algorithmic transformations for core functionality, and optimization adjustments for performance characteristics. This approach aims to provide structure when implementing methodological improvements across interconnected components in AI systems, which helps guide development efforts. Rather than employing single-pass implementation strategy, we designed an adaptive evolution approach for our implementation process. This approach involves structured iterations where each implementation attempt is followed by performance assessment and potential refinement. We maintain records of implementation decisions across iterations, which helps track changes and their corresponding effects. This directed adaptation process enables the gradual refinement of complex implementations based on empirical results rather than theoretical assumptions alone."
        },
        {
            "title": "3 Experiments",
            "content": "In this section, we evaluate the effectiveness of NOVELSEEK in conducting autonomous research and accelerating scientific discovery. We begin by providing brief overview of the selected multi-domain tasks and detailing the experimental implementation in Sec. 3.1. Subsequently, we present the quantitative results across various tasks in Sec. 3.2 and conduct an analysis of the different modules within NOVELSEEK in Sec. 3.3. 3.1 Experimental Setup 3.1.1 Task Description We select 12 distinct tasks to demonstrate NOVELSEEKs capability in conduct Autonomous Scientific Research (ASR). These tasks span multiple modalities, including science (e.g., reaction yield prediction, molecular dynamics), time series (e.g., time series forecasting), natural language (e.g., sentiment classification), image (e.g., semantic segmentation), and point cloud (e.g., 3D object detection), which cover both discriminative and generative tasks. We believe that experiments ranging from fundamental tasks to complex multi-modal 9 NovelSeek: Starting Point of Innovation tasks can comprehensively illustrate the effectiveness of NOVELSEEK. Below, we detail the datasets, the base code repositories, and the experimental settings for each task. Reaction Yield Prediction (AutoRYP). We conduct experiments on the widely-used Suzuki-Miyaura reaction dataset (Perera et al., 2018), which contains 5,760 reaction data. Each data point includes structured chemical reaction information, such as reactants, products, reaction types, reaction conditions (solvent, catalyst, ligand, and base), functional group, and yield values. We use the LoRA-finetuned LLaMA3-8B as our baseline, an embedding model that converts chemical reaction texts into high-dimensional vector representations, which are subsequently fed into fully connected prediction network predictor to perform chemical yield prediction. Molecular Dynamics (AutoMD). We conduct experiments on the widely-used MD17 dataset (Chmiela et al., 2017), which contains energy and force calculation results for seven small organic molecules: aspirin, ethanol, malonaldehyde, naphthalene, salicylic acid, toluene, and uracil. We use VisNet (Wang et al., 2024d) as our baseline, an equivariant geometry-enhanced graph neural network that achieves excellent chemical property prediction. Power Flow Estimation (AutoPower). We conduct experiments on the IEEE 39-Bus dataset (Zimmerman et al., 2010), which is medium-scale benchmark based on the New England power system, comprising 39 buses, 10 synchronous generators, 19 load buses and 46 transmission lines, and providing AC power flow snapshots under variety of load conditions. We use SenseFlow (Zhao et al., 2024) as our baseline, novel physics-informed, self-ensembling power flow estimation model that has demonstrated state-of-the-art accuracy on standard IEEE test systems consistently outperforming both traditional state-estimation techniques and recent data-driven approaches in voltage and power-flow recovery tasks. Time Series Forecasting (AutoTSF). We conduct experiments on the ETTh1 dataset, which is 1-hour-level subset of the Electricity Transformer Temperature (ETT) benchmark (Zhou et al., 2021). This dataset comprises two years of hourly multivariate time series, including the target oil temperature and six power-load covariates, collected from transformer stations in two Chinese counties. We use DLinear (Zeng et al., 2023) as our baseline, an MLP-based forecasting model that decomposes each series into trend and seasonality and employs simple linear layers, outperforming Transformer-based methods on multiple time series benchmarks. We report the average results of 96, 192, 336, and 720 prediction length. Transcription Prediction for Perturbation Response (AutoTPPR). We conduct experiments on the Perturb-seq dataset (Norman et al., 2019), which contains singlecell gene expression data measuring the transcriptional responses of cells to various perturbations. We use GEARS (Generative Energy-based Autoencoder for scRNAseq) (Roohani et al., 2024) as our baseline, framework based on Graph Neural Networks (GNNs) and Multi-Layer Perceptrons (MLPs), designed to learn joint representations of single-cell multi-omics data. Enhancer Activity Prediction (AutoEAP). We conduct experiments on the UMISTARR-seq dataset (Arnold et al., 2013), which contains genome-wide, highresolution quantitative activity maps of developmental and housekeeping enhancers in Drosophila S2 cells. We use DeepSTARR (de Almeida et al., 2022) as our baseline, deep learning model that excels at quantitatively predicting enhancer activity from DNA sequences. Sentiment Analysis (AutoSenCls). We conduct experiments on the Stanford Sentiment Treebank (SST-2) dataset (Socher et al., 2013), binary sentiment classification dataset consisting of movie reviews with approximately 67,000 training samples. We use BERT-base (Devlin et al., 2019) as our baseline, Transformer-based pretrained language model that has shown excellent performance on various NLP tasks. 2D Image Classification (Auto2DCls). We conduct experiments on the widelyused CIFAR-100 dataset (Krizhevsky et al., 2009), which contains 60,000 3232 color images in 100 classes, with 500 training images and 100 testing images per 10 NovelSeek: Starting Point of Innovation class. We use Wide Residual Networks (WRN) (Zagoruyko, 2016) as our baseline, which improves performance by increasing the width rather than the depth of convolutional networks. 3D Point Cloud Classification (Auto3DCls). We conduct experiments on the ModelNet40 dataset (Wu et al., 2015), which contains 12,311 CAD models across 40 common object categories and is widely used for 3D shape classification tasks. We use PointNet (Qi et al., 2017) as our baseline, pioneering deep learning architecture that directly processes point cloud data. 2D Semantic Segmentation (Auto2DSeg). We conduct experiments on the widelyused Pascal VOC 2012 dataset (Everingham et al., 2012), which includes 20 object classes and background class for semantic segmentation tasks. The dataset contains 1,464 images for training and 1,449 for validation. We use DeepLabV3Plus (Chen et al., 2018) as our baseline method, which enhances segmentation performance by employing atrous convolution and more refined encoderdecoder structure to capture multiscale contextual information effectively. 3D Point Cloud Autonomous Driving (AutoPCDet). We conduct experiments on the widely-used dataset ONCE (Mao et al., 2021) and use CenterPoint (Yin et al., 2021) as our baseline. Our code is based on OpenPCDet (Team, 2020) and we filter out all code irrelevant to the baseline model to avoid knowledge leakage. Large Vision-Language Model Fine-tuning (AutoVLM).We conduct experiments on filtered geometry subset of the URSA dataset (Luo et al., 2025), comprising manually curated multimodal QA pairs and CoT process. Natural images were excluded, and data were downsampled to control experimental budgets, enabling training completion within 20 hours on 8 A800 GPUs.We use LLaVA-Onevision (Li et al., 2024a) as our baseline, robust multimodal alignment framework using simple MLP to align visual encoders with LLMs, forming an effective LMM with strong scalability on vision-language tasks. We take SigLIP (Zhai et al., 2023) and Qwen2.5-Math-7B-Instruct(Yang et al.) as the visual and language modules, respectively. 3.1.2 Evaluation Metric Since our NOVELSEEK has been validated across wide range of scientific research fields, the evaluation metrics used for tasks in each field are not consistent. Therefore, in this part, we provide detailed introduction to the evaluation metrics used for each scientific research task. AutoRYP. For Reaction Yield Prediction, we evaluate model performance using the coefficient of determination (RÂ²), which quantifies the proportion of variance in the actual reaction yields that is predictable from the models predictions. AutoMD. Our method is evaluated on the MD17 dataset, molecular chemical property prediction task. The performance is measured using Force-MAE, representing the mean absolute error between the true and predicted forces of molecules. AutoPower. For Power Flow Estimation, we use Root Mean Square Error (RMSE) on PQ node to evaluate the estimation performance on IEEE 39-Bus datasets, representing the root mean square error between the true and predicted voltage magnitudes and phase angles. AutoTSF. For Time Series Forecasting, we use Mean Absolute Error (MAE) to evaluate the prediction performance on ETTh1 dataset. The performance is calculated by taking the average of the four prediction steps of {96, 192, 336, 720}. AutoTPPR. For Transcription Prediction for Perturbation Response, we employ the Top 20 DE MSE as the evaluation metric, calculating the mean squared error between the predicted and actual expression levels of the top 20 most differentially expressed genes under each perturbation condition. AutoEAP. For Enhancer Activity Prediction, we use Housekeeper Pearson Correlation Coefficient (HK-PCC) as the metric, which quantifies the correlation between the true enhancer activities and the predicted values. NovelSeek: Starting Point of Innovation AutoSenCls. We evaluate our method on the SST-2 dataset, which is binary sentiment classification task. The performance is measured using accuracy (Acc), which represents the percentage of correctly classified samples. Auto2DCls. For 2D image classification, we conduct experiments on CIFAR-100 dataset, which contains 100 classes. The performance is measured using classification accuracy (Acc), representing the percentage of correctly classified images. Auto3DCls. For the task of 3D point cloud classification, we use the widely adopted ModelNet40 benchmark, which comprises 40 distinct object categories. We report the Overall Accuracy (OA) as our primary evaluation metric, which calculates the proportion of correctly classified instances in the entire test set. Auto2DSeg. For 2D semantic segmentation, we conduct experiments on the Pascal VOC 2012 dataset, which includes 20 object classes and background class. The performance is measured using the mean Intersection over Union (MIoU), which quantifies the average overlap between the predicted segmentation and the ground truth across all classes, providing comprehensive assessment of the models segmentation accuracy. AutoPCD. Following ONCE official evaluation metric, we merge the car, bus and truck class into super-class (i.e., vehicle). AP3D is used to evaluate the performance of the ONCE dataset, we report Mean average precision (mAP) which is the average of the scores of the three categories. AutoVLM. We evaluated our model on the geometry subset of MathVista (Lu et al., 2023), widely adopted multimodal mathematical benchmark. Models answers to questions were extracted using GPT-4o and compared against the ground truth to calculate accuracy. 3.1.3 Implementation Details In the self-evolving idea generation process, the survey agent, code review agent, generation agent, self-evolving agent, and orchestration agent are based on GPT-4o (Hurst et al., 2024). The survey agent searches and reviews 50 papers to provide domain knowledge for the subsequent idea generation agent, and then the idea generation agent generates 15 ideas. The self-evolving agent evolves each idea into 3 ideas and then selects the top 5 ideas for the next evolving process until the maximum number of evolutions (i.e., 4) is reached. In the idea-to-methodology process, each idea is initialized and refined once by the method development agent. In the evolutionary experimental planning and execution process. We use Claude-3.7-Sonnet to generate codes and debug. We set the max debug attempt to 4. The max run number is set to 5 for Aider (Gauthier & Contributors, 2023) and 3 for OpenHands (Wang et al., 2024c). 3.2 Experimental Results To comprehensively evaluate the effectiveness of NOVELSEEK in accelerating scientific discovery, we first provide quantitative experimental results as shown in Tab. 1, Tab. 2, Tab. 3, and Tab. 4. Extensive results demonstrate that NOVELSEEK excels in the following aspects: Outperforming existing auto-research systems on multiple tasks. We first compare NOVELSEEK with existing auto-research system (i.e., DOLPHIN (Yuan et al., 2025)) on single-file tasks. Tab. 1 and Tab. 2 show the max performance and average performance (i.e., the average performance across experiments with performance gains) achieved by NOVELSEEK and DOLPHIN. It can be observed that NOVELSEEK consistently improves the performance compared to the baseline and outperforms DOLPHIN across all tasks including both generative and discriminative tasks. This suggests that NOVELSEEK can generate better ideas on each specific domain benefiting from the self-evolving idea generation process and automatically implement them. For example, in AutoRYP, methods proposed by NOVELSEEK can largely 12 NovelSeek: Starting Point of Innovation Table 1: Performance comparison across six types of scientific research tasks. We conduct experiments using 10 NOVESEEK generated ideas for each task. Tasks Method AutoRYP AutoMD AutoPower AutoTSF AutoTPPR AutoEAP R2 Forces-MAE RMSE MAE MSE HK-PCC Max Performance 27.6 Baseline Dolphin 31.8 (+4.2) NOVELSEEK 35.4 (+7.8) 0.158 0.152 0.148 0.00473 0.00455 0. Average Performance 27.6 Baseline Dolphin 31.3 (+3.7) NOVELSEEK 33.5 (+5.9) 0.158 0.155 0.152 0.00473 0.00459 0.00447 0.4382 0.4627 0.4331 0.4382 - 0. 0.197 0.173 0.146 0.197 0.179 0.170 0.52 0.76 0.79 0.52 0.73 0.77 Table 2: Performance comparison for six types of scientific research tasks. We conduct experiments using 10 NOVESEEK generated ideas for each task, where baseline codes for Auto2DSeg, AutoPCDet, and AutoVLM are project-level, consisting of multiple code files with complex call relation between functions. Therefore, the coder in Dolphin (Yuan et al., 2025) does not support modifying this type of baseline codes. Tasks Method AutoSenCls Auto2DCls Auto3DCls Auto2DSeg AutoPCDet AutoVLM Acc Top-1 Acc OA mIoU mAP QA Max Performance 91.0 Baseline DOLPHIN 92.5 (+1.5) NOVELSEEK 93.5 (+2.5) 81.2 82.0 (+0.8) 83.3 (+2.1) 91.0 93.9 (+2.9) 95.5 (+4.5) 78.8 - 81.0 (+2.2) 65.0 - 65.9 (+0.9) 67.1 - 67.6 (+0.5) Average Performance 91.0 Baseline DOLPHIN 91.8 (+0.8) NOVELSEEK 92.5 (+1.5) 81.2 81.8 (+0.6) 82.2 (+1.0) 91.0 92.0 (+1.0) 93.4 (+2.4) 78.8 - 80.1 (+1.3) 65.0 - 65.7 (+0.7) 67.1 - 67.6 (+0.5) outperform those proposed by DOLPHIN (i.e., +3.6 on max performance). We highlight that NOVELSEEK can achieve SoTA performance on some tasks such as 3D point cloud classification (i.e., 95.5% overall accuracy without pre-training achieved by NOVELSEEK compared to 95.3% overall accuracy with pre-training achieved by human experts). Table 3: Experiments statistics across different tasks. Each cell shows the number of ideas that improved performance, the number of ideas that successfully ran, and the total number of ideas tested (format: improved / successful / tested). For all the tasks, we conduct experiments with 10 ideas. Research Task Method AutoRYP AutoMD AutoPower AutoTSF AutoTPPR AutoEAP Dolphin NOVELSEEK 2/3/10 4/6/10 2/4/10 4/8/ 2/4/10 5/6/10 0/3/10 3/7/10 2/3/10 5/5/10 2/4/10 8/8/10 13 NovelSeek: Starting Point of Innovation Table 4: Experiments statistics across different tasks. Each cell shows the number of ideas that improved performance, the number of ideas that successfully ran, and the total number of ideas tested (format: improved / successful / tested). For all the tasks, we conduct experiments with 10 ideas. Method Auto2DCls Auto3DCls AutoSenCls Auto2DSeg AutoPCDet AutoVLM Dolphin NOVELSEEK 2/4/10 5/7/10 2/5/10 3/6/ 4/7/10 9/9/10 - 6/9/10 - 2/5/10 - 1/5/10 Research Task Table 5: Computational and financial cost analysis for all tasks. Training time is measured using A100 GPU hours, while idea generation and code debugging costs are measured in USD using gpt-4o and claude-sonnet-3.7 models respectively. Cost Metric AutoRYP AutoMD AutoPower AutoTSF AutoTPPR AutoEAP Training time (A100 hours) Idea-Gen cost (gpt-4o) ($) Coder-Debug cost (claude-sonnet-3.7) ($) 6.0 0.6 0.7 10.0 0.6 0.5 5.0 0.6 1. 0.1 0.6 0.4 1.0 0.6 0.9 1.0 0.6 0.6 Besides, Tab. 3 and Tab. 4 report the percentage of experiments with performance gains and executable experiments out of the total number of experiments. First, results show that even on complex tasks such as AutoPCDet (i.e., 50%) and Auto2DSeg (i.e., 90%), NOVELSEEK can still ensure reasonable execution success rate which is due to the carefully designed idea-to-methodology process, enabling the coder to auto-implement based on detailed methodologies. Second, NOVELSEEK demonstrates higher performance improvement rate compared to DOLPHIN. This improvement is mainly attributed to the idea-to-methodology feature of NOVELSEEK, which enables the concretization of high-level ideas. Additionally, through the process of multi-round experimental planning and execution, the submodules of the AI-generated methodology are progressively integrated into the baseline code. Covering wide range of tasks including the scientific research tasks and AI tasks. Further, NOVELSEEK exhibits strong generalization capability across wide range of tasks, enabling it to handle tasks from the AI domain (e.g., Auto2DSeg) to the scientific domain (e.g., AutoMD). As shown in Tab. 1 and Tab. 2, NOVELSEEK can support 12 different tasks ranging from simple classification tasks to complex multimodal and cross-disciplinary tasks. This is because the survey agent in NOVELSEEK can auto-search task-related literature on academic websites such as arXiv and review the literature to understand each task. Besides, NOVELSEEK is highly extensible, as it can support new tasks with just task description and reference codes. This capability not only assists AI researchers in automatically updating algorithms, but also empowers researchers in scientific domains to utilize AI tools at lower cost, thereby accelerating the pace of scientific discovery. Support repo-level experiments. Most of existing auto-research systems such as DOLPHIN (Yuan et al., 2025) only support single-file experiments. On more complex tasks, researchers are required to manually consolidate complex task codes into single file, which is highly time-consuming and limits their ability to conduct experiments on complex tasks. In contrast, NOVELSEEK can support repo-level tasks such as AutoPCDet, AutoVLM, AutoTPPR, and so on, and achieve better performance on these repo-level tasks compared to their baselines. For example, on Auto2DSeg, NOVELSEEK pipeline can improve the DeepLabV3Plus baseline (Chen et al., 2018) from the original 78.80% to 81.0%. This is attributed to the detailed methodology, code comprehension achieved by the code review agent, and the auto-exploration ability of the coder agent. Runtime Statistics. We further provide the runtime statistics of NOVELSEEK on all 12 tasks including the training costs (i.e., GPU hours) and monetary costs in the idea generation stage (including self-evolving idea generation and idea-to-methodology) and code execution and 14 NovelSeek: Starting Point of Innovation Table 6: Computational and financial cost analysis for all tasks. Training time is measured using A100 GPU hours, while idea generation and code debugging costs are measured in USD using gpt-4o and claude-sonnet-3.7 models respectively. Cost Metric Auto2DCls Auto3DCls AutoSenCls Auto2DSeg AutoPCDet AutoVLM Training time (A100 hours) Idea-Gen cost (gpt-4o) ($) Coder-Debug cost (claude-sonnet-3.7) ($) 2.0 0.6 0.7 0.8 0.6 0.6 0.3 0.6 0.7 30.0 0.6 1.1 9.0 0.6 1. 192.0 0.6 1.0 Table 7: To compare the performance of the baseline and NOVELSEEK-generated code, we adopted few-shot training setup for the yield prediction task. Due to the large variance in experimental results under this setting, we report the outcomes of 5 independent repeated experiments. Epoch=300 Repeat=1 Repeat=2 Repeat=3 Repeat=4 Repeat=5 AVG/VAR Baseline (train-set=60) GAT (ours, train-set=60) ADAGT (ours, train-set=60) Baseline (train-set=100) GAT (ours, train-set=100) ADAGT (ours, train-set=100) 20.0 34.7 35.4 38.8 36.9 38.5 26.2 34.8 35.2 30.6 39.1 38.0 27.6 33.9 34.5 34.8 34.4 38. 26.6 32.7 35.2 39.0 41.4 37.9 20.1 34.2 33.7 34.5 35.0 40.4 24.24.2 34.11.4 34.81.1 35.54.9 37.44.0 38.71. debug stage. As shown in Tab. 5 and Tab. 6. As mentioned in Sec. 3.1.3, we select top 5 ideas in each idea generation process and then generate detailed methodology for the selected ideas. Therefore, we report the average cost of 5 ideas as the idea generation cost. It can be seen that the idea generation cost of each idea is about $0.6 using GPT-4o which is cost-efficient. The coder-debug cost denote the cost of each run, for example, if running for 5 times for single idea as mentioned in Sec. 2.3, we calculate the average cost of 5 runs. It can be seen from the table that the coder-debug cost varies between the file-level codes and repo-level codes and repo-level codes generally need more cost for high complexity of codes. For example, for single-file code such as Auto2DCls, the cost is below $1 for each run and for more complex AutoPCDet, the cost is about $1.2 using claude-sonnet-3.7. Generally, NOVELSEEK is cost-efficient auto-research framework that can generate ideas and execute codes at reasonable cost. 3.3 Insightful Analyses Analysis on Survey Agent. As mentioned in Sec. 2.1, survey agent mainly have two modes (i.e., the literature review mode and the deep research mode). As shown in Fig. 4 (a), under the literature review mode, the survey agent can search for domain-related papers and automatically select the most relevant literature to read and extract task-related information. For example, the agent can identify works such as \"Multimodal Transformer-based Model for BuchwaldHartwig and Suzuki-Miyaura Reaction Yield Prediction\" or \"ReacLLaMA: Merging chemical and textual information in chemical reactivity AI models\" to quickly gather foundational studies in the field. Such process is essential for idea generation process since the used agent may not have relevant domain knowledge, especially in emerging fields. Besides, under deep research mode, the survey agent needs to search for literature related to specific technical terms used in generated ideas. As shown in Fig. 4 (b), the agent updates its queries based on generated technical terms and retrieves papers like \"Large Language Models to Accelerate Organic Chemistry Synthesis\" which are closely aligned with these refined research directions. This process is highly similar to human researchers, they initially perform comprehensive review of the relevant field to build foundational knowledge, and then search for articles focused on specific techniques to further refine the research direction. Analysis on Idea Innovation Agent. Idea innovation agent can first generate ideas and then evolve the generated ideas in an iterative manner. We take the idea evolution tree as an example to show the iterative process of polishing ideas. As shown in Fig. 3, the root node NovelSeek: Starting Point of Innovation Table 8: Ablation Study on Adaptive Evolution (AE). Ideas (i/s/t) shows the number of ideas that improved performance, the number of ideas that successfully ran, and the total number of ideas tested (format: improved / successful / tested). Method AutoRYP Auto2DCls AutoSenCls Max R2 Avg R2 Ideas (i/s/t) Max Acc Avg Acc Ideas (i/s/t) Max Acc Avg Acc Ideas (i/s/t) Baseline NOVELSEEK (w/o AE) NOVELSEEK 27.6 34.7 35. 27.6 33.0 33.5 - 2/5/10 4/6/10 81.2 81.6 83.3 81.2 81.5 82.2 - 2/5/10 5/7/10 91.0 92.4 93. 91.0 91.9 92.5 - 6/8/10 9/9/10 Figure 4: Analysis of two modes on survey agent. (i.e., Init Idea 0) denotes an initially generated idea and the child nodes are evolved from the parent node. As ideas continue to evolve, more external knowledge sourced from the survey agent is incorporated into ideas, which enriches the content and enhances the practicality of the ideas. For example, starting with basic idea such as \"adding graph-derived reaction descriptor as precondition for attention scores in the transformer architecture\", the agent refines and evolves the ideas in an iterative manner. At each step, the evolved idea shows improvements over its predecessor in terms of technical sophistication, novelty, or practical applicability. As illustrated in Fig. 3, the process can involve incorporating more specific chemical descriptors, introducing cross-modality attention mechanisms, or leveraging hierarchical architectures, with each evolution step guided by additional insights from literature or domain knowledge, thus ensuring continuous advancement of the ideas. Analysis on Idea-to-Methodology Phase. The correspondence between an idea and its final code implementation plays crucial role in assessing the effectiveness of the idea since the idea can be verified once the experiments have been conducted. The goal of the idea-tomethodology process is to generate detailed methodologies so that code can be written based on these comprehensive method descriptions (e.g., method-level descriptions in research NovelSeek: Starting Point of Innovation Table 9: Comparison with AI-Scientist-V2 and AI-Researcher on AutoRYP and Auto2DCls task. Total cost means the cost of the whole session. For each task, we conduct 10 experiments. AI-Scientist-V2 and AI-Researcher demonstrate relatively weak baseline improvement capabilities, with AI-Scientist-V2 in particular struggling to write code that runs correctly. The primary reason lies in the fact that AI-Scientist-V2s pipeline utilizes limited task-related information (e.g. task formulation and type, relevant papers, and commonly used code) when generating new ideas or coding. As result, their generated ideas tend to be more divergent and difficult to implement. Method AutoRYP Auto2DCls Max R2 Avg R2 Total Cost Max Acc Avg Acc Total cost Baseline AI-Scientist-V2 (Yamada et al., 2025) AI-Researcher (Lab, 2025) NOVELSEEK 27.6 - 12.3 35.4 27.6 - - 33.5 - 15$ 25$ 3$ 81.2 - 80.3 83.3 81.2 - - 82. - 10$ 32$ 3$ papers). As illustrated in Fig. 5, our idea-to-method approach enables the generation of fine-grained methodologies, which facilitates accurate and faithful code implementation. Analysis on Evolutionary Experimental Planning and Execution. To verify the effectiveness of the adaptive evolution (AE), we conduct ablation studies on three tasks, ranging from AI tasks to scientific tasks including AutoRYP, Auto2DCls, and AutoSenCls. As shown in Tab. 8, the performance can be further improved on multiple tasks with adaptive evolution. For example, on the image classification task, both the max accuracy and the mean accuracy can be improved by 1.7% and 0.7%, compared to the setting without AE. This is because our coder agent can automatically analyze the previous results and baseline results and further re-plan the following experiments. Besides, the successful execution rate and the percentage of performance gains will also improve (e.g., on AutoRYP, the percentage of performance gains is 40% compared to 20% without AE). This is due to with AE, the coder will implement the idea step by step and analyze the experimental phenomena after each stage of the experiments. Improving Baseline in Multi-Dimension. NOVELSEEK not only can improve the performance on different tasks, but it also enhance the quality of ideas in other dimensions. For example, as shown in Tab. 7, on few-shot yield prediction task. We find that the results of baseline methods are unstable, for the results of multiple repeated experiments tend to exhibit large variance (e.g., 24.24.2 when train-set=60). In contrast, the methods proposed by NOVELSEEK can improve both the performance and the stability of the results. For example, when the train-set=60, ADAGT proposed by NOVELSEEK achieves 34.8 average R2 across 5 repeated experiments compared to 24.2 achieved by the baseline method. Besides, the variance of the results achieved by ADAGT (i.e., 1.1) is much lower than baseline methods (i.e., 4.2). This phenomenon further shows the quality of ideas and code implementation of NOVELSEEK. Comparison with AI-Researcher. We evaluate the performance and cost of NOVELSEEK and AI-Researcher (Lab, 2025) on AutoRYP and Auto2DCls research tasks. To ensure fair comparison, we supplied AI-Researcher with the same code templates that NOVELSEEK uses. Both systems employ GPT-4o-2024-08-06 for idea generation and Claude-3-7-Sonnet20250219 for code generation. As shown in Tab. 9, NOVELSEEK outperforms both the baseline methods and AI-Researcher across both tasks, whereas AI-Researcher is unable to improve the provided baselines. One important reason for NOVELSEEKs outstanding performance is its ability to generate novel ideas through extensive search and reflection. Moreover, NOVELSEEK has complete experimental planning and adaptive evolution mechanism, thereby enabling it to achieve better performance. In contrast, the idea generated by AI-Researcher is more dependent on user-provided reference papers, limiting its novelty. Moreover, AI-Researcher often ignore the prior information of the existing codebases, which further hinders its performance. In terms of cost, NOVELSEEK is significantly more economical than AI-Researcher. For an instance, the economic cost required for NOVELSEEK is approximately one-sixth that of AI-Researcher on the AutoRYP task. This lower cost allows NovelSeek: Starting Point of Innovation Figure 5: Visual Examples of AutoRYP Task. NOVELSEEK to conduct broader scientific experiments, thereby accelerating the exploration and validation of innovative research ideas."
        },
        {
            "title": "4 Case Studies",
            "content": "4.1 Qualitative Results In this section, we present the results from various tasks implemented using NOVELSEEK. 4.1.1 Visual Examples of Various Tasks We present showcases for three distinct tasks: AutoRYP, AutoMD, and AutoPower, to highlight the innovative methodologies discovered and their applications. These showcases are illustrated in Figs. 5, 6, and 9, respectively. Each task demonstrates unique approach to solving complex problems, showcasing the potential impact of NOVELSEEK across different scientific domains. In the AutoRYP task illustrated in Fig. 5, NOVELSEEK autonomously discovered an innovative approach called \"Adaptive Dual-Attention Graph-Transformer with Dynamic Freezing\" for predicting chemical reaction yields. This method effectively integrates SMILES and graph-derived descriptors using hybrid graph-transformer network, incorporating hierarchical attention mechanisms to enhance accuracy while minimizing overfitting. The approach features Dual-Attention Fusion Mechanism (DAFM) that systematically comNovelSeek: Starting Point of Innovation Figure 6: Visual Examples of AutoMD Task. bines token and graph embeddings with reaction conditions, ensuring effective information flow across different representations. Furthermore, dynamic layer freezing mechanism, based on gradient magnitudes, optimizes which layers are trained, thereby enhancing generalization in low-data scenarios. Implemented with self-attention and cross-modality attention modules, this system not only combines 1D and 2D molecular representations effectively but also improves prediction accuracy and model adaptability. Consequently, this showcase underscores the methods potential for advancing research in complex chemical tasks using deep learning. In the AutoMD task illustrated in Fig. 6, novel framework called \"Hierarchical Equivariant Directional Graph Encoder\" (HEDGE-Net) has been autonomously discovered for predicting molecular energy and forces. This approach utilizes SE(3)-equivariant graph neural networks with hierarchical geometric self-attention and multi-hop message enrichment. By integrating angular and directional features into aggregated substructures, the method captures interacting atomic patterns and propagates dynamic weight updates, aligning with both local and global molecular geometries. The core of this method, the GeometryEnhanced Directional Attention (GEDA) mechanism, ensures SE(3)-equivariance, enabling precise predictions for complex molecular systems at both atomic and substructural scales. Implemented with advanced message passing techniques, HEDGE-Net effectively combines directional and substructural information, enhancing scalability and precision in molecular modeling. This showcases the methods potential to advance research in complex molecular tasks using deep learning techniques. 19 NovelSeek: Starting Point of Innovation Figure 7: Visual Examples of Experimental Planning and Adaptive Evolution on Auto3DCls task. 4.1.2 Visual Examples of Experimental Planning and Adaptive Evolution To further illustrate the practical utility of our experimental planning and adaptive evolution framework as described in Sec. 2.3.2, we present some concrete examples of its application in the development and optimization for 3D point cloud classification and transcription prediction for perturbation response. Fig. 7 and Fig. 8 visually summarize the stepwise experimental planning and adaptive evolution process that guided the implementation and refinement of our method. 4.2 Human Evaluation Table 10 compares the novelty of ideas generated by our NOVELSEEK and AI-ScientistV2 (Yamada et al., 2025), across various research tasks. Each task involves generating 20 ideas, which are evaluated by five qualified reviewers. The assessments focus on four NovelSeek: Starting Point of Innovation Table 10: From the perspectives of soundness, contribution, and overall, we compare the novelty of ideas generated by NOVELSEEK and AI-Scientist-V2 (Yamada et al., 2025). For each research task, we generate 20 ideas. Each idea is scored by 5 qualified reviewers, and the final score for each task is reported as the average score of all 20 ideas. The detailed scores for each idea can be found in the Appendix B.1. Research Task Idea-gen Method Soundness Contribution Overall Confidence Reaction Yield Prediction Reaction Yield Prediction 2D Semantic Segmentation 2D Semantic Segmentation 2D Image Classification 2D Image Classification AI-Scientist-V2 NOVELSEEK AI-Scientist-V2 NOVELSEEK AI-Scientist-V2 NOVELSEEK Point Cloud Autonomous Driving AI-Scientist-V2 Point Cloud Autonomous Driving NOVELSEEK 1.42 3.09 1.84 2.41 2.78 3.15 2.15 2.75 1.45 2.66 2.07 2. 2.82 3.10 2.47 2.95 3.50 4.35 2.95 4.05 4.40 5.85 3.10 5. 3.50 4.00 3.64 3.48 3.87 3.32 3.94 4.10 criteria: soundness, contribution, overall rating, and confidence. For each research task, the average scores of the 20 ideas are reported. In the Reaction Yield Prediction task, NOVELSEEK outperforms AI-Scientist-V2 in all aspects, especially in overall rating and soundness. Similarly, for 2D Semantic Segmentation, NOVELSEEK shows better idea generation ability, particularly in soundness and overall rating. In 2D Image Classification and Point Cloud Autonomous Driving, NOVELSEEK scores higher across all criteria, indicating consistent advantage over AI-Scientist-V2 in generating more effective and novel ideas."
        },
        {
            "title": "5 Related Works",
            "content": "Recent advances in Large Language Models (LLMs) and agent-based systems have demonstrated significant potential in the field of Autonomous Scientific Research (ASR), enabling progress from creative idea generation to end-to-end research automation. Some studies (Li et al., 2024b; Wang et al., 2023; Zhou et al., 2024) have shown that LLMs are capable of generating novel research ideas, which has sparked widespread discussion in the academic community. For example, Li et al. (2024b) introduce method that derives research ideas through the analysis of interconnected scholarly works. Beyond idea generation, several studies have examined the use of LLMs for hypothesis formulation, such as extracting hypotheses from large-scale web data (Yang et al., 2023) or scientific literature (Wang et al., 2023; Zhou et al., 2024). However, most of these efforts remain at the stage of idea or hypothesis generation, lacking systematic empirical validation of their practical effectiveness. In terms of end-to-end research automation, Lu et al. (2024) introduced the AI Scientist framework, which was among the first to achieve fully automated pipeline in the machine learning domain, covering problem definition, experimental execution, and result reporting. The subsequent AI Scientist-V2 (Yamada et al., 2025) further enhanced the framework by incorporating agent tree search, vision-language model feedback, and parallelized experiment execution, leading to the first workshop paper fully generated and peer-reviewed by AI. Similarly, systems such as AI-Researcher (Lab, 2025) and Dolphin (Yuan et al., 2025) have proposed closed-loop, LLM-driven frameworks that automate the entire research process on range of simple tasks. Human-AI collaboration is gaining traction in ASR. Systems like Agent Laboratory (Schmidgall et al., 2025) integrate human feedback into multi-stage LLM agent workflows, automating literature review, experiment execution, and report writing, while allowing user input at each step to enhance research quality. AgentRxiv (Schmidgall & Moor, 2025) addresses the collaborative nature of scientific discovery by enabling LLM agent laboratories to communicate and build upon each others work via shared preprint server, thus facilitating knowledge sharing and collective innovation. Experimental results demonstrate that agent laboratories utilizing AgentRxiv for collaboration achieve greater performance 21 NovelSeek: Starting Point of Innovation improvements compared to isolated settings. Similarly, AI Co-Scientist (Gottweis et al., 2025), based on Gemini 2.0, employs multi-agent system with \"generate-debate-evolve\" strategy for hypothesis generation, and has demonstrated effectiveness in biomedical domains such as drug repurposing, novel target identification, and interpretation of bacterial evolution, with several hypotheses validated through experiments. Despite these advances, most current systems are still evaluated primarily on relatively simple tasks or within narrow scientific domains. However, when applied to more complex, system-level scientific challenges, these approaches often face significant limitations. Key challenges include generating truly novel and scientifically sound research ideas, establishing robust closed-loop feedback between experiments and idea generation, and developing systematic evaluation standards to rigorously assess the effectiveness and real-world value of autonomous research systems."
        },
        {
            "title": "6 Conclusion and Future Works",
            "content": "Summary. We have introduced closed-loop multi-agent framework for the first time, which supports 12 types of scientific research tasks. It has been validated to generate novel ideas and transform these ideas into code that can effectively improve performance. NOVELSEEK refines the initially generated ideas through human-interactive feedback enriched with self-evolutionary path of ideas. It facilitates the transformation from coarse-grained proposals to fine-grained methodologies via an idea-to-methodology construction process. Furthermore, by leveraging multi-round experimental planning and execution, it implements the corresponding theoretical methods, thereby completing the closed-loop process in scientific researchfrom hypothesis generation to verification. Future Outlook. NOVELSEEK faces several key technical challenges that need to be addressed in the future: Knowledge Retrieval: This involves establishing connections and relationships between papers, utilizing deep research techniques to conduct extensive searches across wide range of papers, and performing meta-analyses on the search results. Additionally, it requires transforming the papers into structured representations such as triples, and utilizing graph networks to uncover relationships between papers, including associations in paper ideas, methodologies, experimental conditions, and experimental results. Moreover, retrieval-augmented generation will be employed to alleviate the hallucination issues of LLMs when generating viewpoints or citing references. Knowledge Understanding and Representation: This involves utilizing VLM/LLM to accurately analyze relevant academic papers, aiming to understand the significance of their core concepts, methodologies, and research findings, while also refining knowledge and formulating hypotheses. Additionally, it focuses on extracting valuable knowledge from large number of papers, identifying common patterns, trends, and connections, thereby advancing the understanding and representation of knowledge in the field. Agent Capability Enhancement: This focuses on improving the ability of AI systems to autonomously perform complex tasks in scientific research. The strength of agents lies in their ability to dynamically adapt, rather than solely relying on historical records to determine subsequent actions. Through self-modification, they can flexibly redefine their initial goals and planning strategies while utilizing feedback, as well as communication logs between agents or between humans and agents, to train and improve themselves. This mechanism should focus on improving their ability to gather feedback from three key sources: the environment, interactions with other agents, and human experts. Scientific Discovery-related Benchmark Construction: This involves evaluating the value that an idea can bring, rather than simply evaluating its novelty. It also includes evaluating whether the methods proposed by AI align with their correspond22 NovelSeek: Starting Point of Innovation ing code implementations and determining whether NOVELSEEK demonstrates certain level of generalization ability in broader scientific scenarios."
        },
        {
            "title": "References",
            "content": "Cosmas Arnold, Daniel Gerlach, Christoph Stelzer, Åukasz Bory n, Martina Rath, and Alexander Stark. Genome-wide quantitative enhancer activity maps identified by starr-seq. Science, 339(6123):10741077, 2013. Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In Proceedings of the European conference on computer vision (ECCV), pp. 801818, 2018. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences, 67(12):220101, 2024. Stefan Chmiela, Alexandre Tkatchenko, Huziel Sauceda, Igor Poltavsky, Kristof SchÃ¼tt, and Klaus-Robert MÃ¼ller. Machine learning of accurate energy-conserving molecular force fields. Science advances, 3(5):e1603015, 2017. Bernardo de Almeida, Franziska Reiter, Michaela Pagani, and Alexander Stark. Deepstarr predicts enhancer activity from dna sequence and enables the de novo design of synthetic enhancers. Nature genetics, 54(5):613624, 2022. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics, pp. 41714186, 2019. M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results. http://www.pascalnetwork.org/challenges/VOC/voc2012/workshop/index.html, 2012. Paul Gauthier and Aider-AI Contributors. Aider: Ai pair programming in your terminal. https://github.com/Aider-AI/aider, 2023. URL https://github.com/Aider-AI/aider. Accessed: 2025-05-07. Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, et al. Towards an ai co-scientist. arXiv preprint arXiv:2502.18864, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. HKU Data Intelligence Lab. Ai-researcher: Fully-automated scientific discovery with llm agents. https://github.com/HKUDS/AI-Researcher, 2025. URL https://github.com/ HKUDS/AI-Researcher. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024a. 23 NovelSeek: Starting Point of Innovation Long Li, Weiwen Xu, Jiayan Guo, Ruochen Zhao, Xinxuan Li, Yuqian Yuan, Boqiang Zhang, Yuming Jiang, Yifei Xin, Ronghao Dang, et al. Chain of ideas: Revolutionizing research in novel idea development with llm agents. arXiv preprint arXiv:2410.13185, 2024b. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292, 2024. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Ruilin Luo, Zhuofan Zheng, Yifan Wang, Yiyao Yu, Xinzhe Ni, Zicheng Lin, Jin Zeng, and Yujiu Yang. Ursa: Understanding and verifying chain-of-thought reasoning in multimodal mathematics. arXiv preprint arXiv:2501.04686, 2025. Jiageng Mao, Minzhe Niu, Chenhan Jiang, Hanxue Liang, Jingheng Chen, Xiaodan Liang, Yamin Li, Chaoqiang Ye, Wei Zhang, Zhenguo Li, et al. One million scenes for autonomous driving: Once dataset. arXiv preprint arXiv:2106.11037, 2021. Thomas Norman, Max Horlbeck, Joseph Replogle, Alex Ge, Albert Xu, Marco Jost, Luke Gilbert, and Jonathan Weissman. Exploring genetic interaction manifolds constructed from rich single-cell phenotypes. Science, 365(6455):786793, 2019. Damith Perera, Joseph Tucker, Shalini Brahmbhatt, Christopher Helal, Ashley Chong, William Farrell, Paul Richardson, and Neal Sach. platform for automated nanomolescale reaction screening and micromole-scale synthesis in flow. Science, 359(6374):429434, 2018. Charles Qi, Hao Su, Kaichun Mo, and Leonidas Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 652660, 2017. Yansheng Qiu, Haoquan Zhang, Zhaopan Xu, Ming Li, Diping Song, Zheng Wang, and Kaipeng Zhang. Ai idea bench 2025: Ai research idea generation benchmark. arXiv preprint arXiv:2504.14191, 2025. Yusuf Roohani, Kexin Huang, and Jure Leskovec. Predicting transcriptional outcomes of novel multigene perturbations with gears. Nature Biotechnology, 42(6):927935, 2024. Samuel Schmidgall and Michael Moor. Agentrxiv: Towards collaborative autonomous research. arXiv preprint arXiv:2503.18102, 2025. Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, and Emad Barsoum. Agent laboratory: Using llm agents as research assistants. arXiv preprint arXiv:2501.04227, 2025. Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. Can llms generate novel research ideas? large-scale human study with 100+ nlp researchers. arXiv preprint arXiv:2409.04109, 2024. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 16311642, 2013. OpenPCDet Development Team. Openpcdet: An open-source toolbox for 3d object detection from point clouds. https://github.com/open-mmlab/OpenPCDet, 2020. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a. NovelSeek: Starting Point of Innovation Qingyun Wang, Doug Downey, Heng Ji, and Tom Hope. Scimon: Scientific inspiration machines optimized for novelty. arXiv preprint arXiv:2305.14259, 2023. Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. OpenHands: An Open Platform for AI Software Developers as Generalist Agents, 2024b. URL https: //arxiv.org/abs/2407.16741. Xingyao Wang, Boxuan Li, Yufan Song, Frank Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Openhands: An open platform for ai software developers as generalist agents. In The Thirteenth International Conference on Learning Representations, 2024c. Yusong Wang, Tong Wang, Shaoning Li, Xinheng He, Mingyu Li, Zun Wang, Nanning Zheng, Bin Shao, and Tie-Yan Liu. Enhancing geometric representations for molecules with equivariant vector-scalar interactive message passing. Nature Communications, 15(1): 313, 2024d. Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: deep representation for volumetric shapes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 19121920, 2015. Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. arXiv preprint arXiv:2504.08066, 2025. Xiangchao Yan, Shiyang Feng, Jiakang Yuan, Renqiu Xia, Bin Wang, Bo Zhang, and Lei Bai. Surveyforge: On the outline heuristics, memory-driven generation, and multidimensional evaluation for automated survey writing. arXiv preprint arXiv:2503.04629, 2025. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement, 2024b. URL https://arxiv. org/abs/2409.12122. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, and Erik Cambria. Large language models for automated open-domain scientific hypotheses discovery. arXiv preprint arXiv:2309.02726, 2023. Tianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl. Center-based 3d object detection and tracking. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1178411793, 2021. Jiakang Yuan, Xiangchao Yan, Botian Shi, Tao Chen, Wanli Ouyang, Bo Zhang, Lei Bai, Yu Qiao, and Bowen Zhou. Dolphin: Closed-loop open-ended auto-research through thinking, practice, and feedback. arXiv preprint arXiv:2501.03916, 2025. Sergey Zagoruyko. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016. Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? In Proceedings of the AAAI conference on artificial intelligence, volume 37, pp. 1112111128, 2023. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1197511986, 2023. NovelSeek: Starting Point of Innovation Zhen Zhao, Zhen Huang, Zicheng Wang, Wenqi Huang, and LEI BAI. Senseflow: physicsinformed and self-ensembling iterative framework for power flow estimation, 2024. URL https://openreview.net/forum?id=UKiCFpwcqY. Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In The Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Virtual Conference, volume 35, pp. 1110611115. AAAI Press, 2021. Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, and Chenhao Tan. Hypothesis generation with large language models. arXiv preprint arXiv:2404.04326, 2024. Ray Daniel Zimmerman, Carlos Edmundo Murillo-SÃ¡nchez, and Robert John Thomas. Matpower: Steady-state operations, planning, and analysis tools for power systems research and education. IEEE Transactions on power systems, 26(1):1219, 2010. NovelSeek: Starting Point of Innovation"
        },
        {
            "title": "A Contributions and Acknowledgments",
            "content": "Enhancer Activity Prediction: Zhiyin Yu VLM finetuning: Tianshuo Peng 4. Software Development Zheng Nie, Zhilong Wang, Runmin Ma, Jinyao Liu, Shiyang Feng, Xiangchao Yan 5. Manuscript Preparation Shiyang Feng, Bo Zhang, Xiangchao Yan, Jiakang Yuan, Zhiyin Yu, Songtao Huang, Lei Bai, Xiaohan He, Tianshuo Peng 6. Idea Human Evaluation Specialist Bo Zhang, Peng Ye, Shufei Zhang, Dongzhan Zhou, Xiaosong Wang, Lei Bai 7. Project Management and Product Yilan Zhang, Meng Li, Shaowei Hou, Zhongying Tu 8. Advising Bowen Zhou, Wanli Ouyang, Xiangyu Yue 9. Project Co-lead Lei Bai, bailei@pjlab.org.cn Bo Zhang, zhangbo@pjlab.org.cn 1. Core Contributors Bo Zhang Shiyang Feng Xiangchao Yan Jiakang Yuan 2. Multi-agent System Algorithm Design Self-Evolving Idea: Shiyang Feng Idea-to-Methodology: Xiangchao Yan Coder: Shiyang Feng, Jiakang Yuan, Xiangchao Yan Multi-round Experimental Plan: Jiakang Yuan, Bo Zhang, Xiangchao Yan, Shiyang Feng 3. Scientific Research Task Integration and Enhancement Reaction Yield Prediction: Xiaohan He, Zhiyin Yu Molecular Dynamics: Zhiyin Yu Power Flow Estimation: Songtao Huang Time Series Forecasting: Songtao Huang Sentiment CLS: Xiangchao Yan, Jiakang Yuan 3D Point CLS: Xiangchao Yan, Jiakang Yuan 2D Image CLS: Xiangchao Yan, Jiakang Yuan 2D Semantic Segmentation: Shiyang Feng 3D Autonomous Driving: Jiakang Yuan Transcription Prediction: Xiaohan He, Peng Ye"
        },
        {
            "title": "B Evaluation Details",
            "content": "B.1 Scoring Criteria for Idea Review In Table 10 of the main text, we conducted human evaluation to assess the novelty of ideas generated by AI-Scientist-V2 (Yamada et al., 2025) and our NOVELSEEK. The evaluation was carried out across four dimensions: soundness, contribution, overall rating, and confidence. Specifically, considering the evaluation cost, we opted to evaluate four types of research tasks: reaction yield prediction, 2D semantic segmentation, 2D image classification, and point cloud autonomous driving. Each invited researcher was required to have peer-review qualifications for top-tier journals or conferences in the relevant field. For each research task, we generated 20 ideas using both AI-Scientist-V2 and NOVELSEEK, and five experienced researchers were invited to score each idea. In this part, we provide detailed description of the scoring criteria for each reviewer, as outlined below: Soundness: 4 excellent NovelSeek: Starting Point of Innovation 3 good 2 fair 1 poor Contribution: 4 excellent 3 good 2 fair 1 poor Overall: Rating: 10: Award quality: Technically flawless paper with groundbreaking impact on one or more areas of AI, with exceptionally strong evaluation, reproducibility, and resources, and no unaddressed ethical considerations. Rating: 9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI and excellent impact on multiple areas of AI, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations. Rating: 8: Strong Accept: Technically strong paper with, with novel ideas, excellent impact on at least one area of AI or high-to-excellent impact on multiple areas of AI, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations. Rating: 7: Accept: Technically solid paper, with high impact on at least one subarea of AI or moderate-to-high impact on more than one area of AI, with goodto-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations. Rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. Rating: 5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly. Rating: 4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly. Rating: 3: Reject: For instance, paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations. Rating: 2: Strong Reject: For instance, paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations. Rating: 1: Very Strong Reject: For instance, paper with trivial results or unaddressed ethical considerations Confidence: Confidence: 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully. Confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked. 28 NovelSeek: Starting Point of Innovation Confidence: 2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked. Confidence: 1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked. B.2 Information on the Expert Review Process Qualifications for Human Evaluators: Evaluators must hold Ph.D. or be Ph.D. candidates with reviewing experience in top-tier AI conferences such as ICLR, ICML, NeurIPS, CVPR, ICCV, and ACL. Steps for Expert Evaluation and Validation: Before the evaluation begins, evaluators are required to carefully read the scoring guidelines, as outlined in Appendix B.1. Each evaluator is assigned 20 ideas generated by NOVELSEEK and 20 ideas generated by AI-Scientist-V2 (Yamada et al., 2025). For each idea, evaluators must carefully review the generated content and provide final scores across four dimensions: Soundness, Contribution, Overall, and Confidence. Qualified evaluators are required to spend at least 10 minutes reading each idea. During the evaluation process, they are allowed to conduct relevant literature searches and verify idea redundancy to ensure that the scoring results are objective and representative."
        },
        {
            "title": "C NOVELSEEK Software Development",
            "content": "Fig. 13 shows the front-end interface of the current NOVELSEEK software platform. Overall, NOVELSEEK software platform employs frontend-backend separation design pattern, building highly scalable distributed service platform. The frontend layer is developed based on the React framework, featuring an advanced visual interaction system. Key innovations include an infinite canvas rendering engine supporting multi-node topology, collaborative mind mapping component driven by state synchronization, code editor supporting multiple formats, and real-time training metrics visualization dashboard. The backend leverages cloud-native technology stack, utilizing dynamic container orchestration engine for elastic resource scheduling, distributed asynchronous task queue for high concurrency support, and cross-cloud storage gateway for data synchronization across heterogeneous cloud environments. Additionally, microservice governance system is established using Service Mesh architecture. The entire system is delivered through containerization, with Kubernetes cluster management enabling self-healing and intelligent scaling, ensuring business continuity while significantly improving resource utilization efficiency."
        },
        {
            "title": "D Visualization Results",
            "content": "We further conducted detailed visualization and analysis around the multi-round experimental planning and execution  (Fig. 8)  in NOVELSEEK, as well as the automated scientific research tasks (Figs. 9, 10, 11 and 12) supported by NOVELSEEK. Fig. 8 illustrates the process of Experimental Planning and Adaptive Evolution on AutoTPPR. Each block in the figure represents step in the multi-round experiment planning process, where the complete NOVELSEEK-generated method is decomposed into multiple logical 29 NovelSeek: Starting Point of Innovation Figure 8: Visual Examples of Experimental Planning and Adaptive Evolution on AutoTPPR task. steps. This allows for task decomposition during the experimental validation phase, thereby facilitating more significant benchmark results. Furthermore, in the AutoPower task illustrated in Fig. 9, the \"Adaptive Hierarchical Graph Transformer\" (AHGT) introduces significant advancements for power flow estimation in energy systems. This approach features two key innovations: the Enhanced Edge-Node Hierarchical Pooling (EENHPool) mechanism, which integrates global and local features to retain crucial graph structures while reducing ambiguities, and the Stability-Regularized Temporal Graph Transformer (SRT-GT), designed to capture temporal dynamics while maintaining training stability. These components together enhance the models robustness and accuracy, validated on IEEE benchmarks under scenarios involving renewable energy and grid perturbations. The AHGT method outputs precise voltage magnitude and angle predictions, assessed using metrics such as MAE, RMSE, and the Graph Perturbation Robustness Index (GPRI). This approach showcases the potential for improved power system modeling through advanced graph transformer techniques. 30 NovelSeek: Starting Point of Innovation Figure 9: Visual Examples of AutoPower Task Figure 10: Visual Examples of AutoSenCls Task NovelSeek: Starting Point of Innovation Figure 11: Visual Examples of Auto3DCls Task 32 NovelSeek: Starting Point of Innovation Figure 12: Visual Examples of Auto2DCls Task Figure 13: NOVELSEEK software platform includes features such as the user entry interface, task selection interface, idea-tree visualization and human-computer interaction interface, code generation, and auto-debug interface. In the near future, we plan to support additional functionalities, including custom dataset uploads and academic idea thinking modes."
        }
    ],
    "affiliations": [
        "Shanghai Artificial Intelligence Laboratory"
    ]
}