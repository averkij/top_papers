{
    "paper_title": "WritingBench: A Comprehensive Benchmark for Generative Writing",
    "authors": [
        "Yuning Wu",
        "Jiahao Mei",
        "Ming Yan",
        "Chenliang Li",
        "SHaopeng Lai",
        "Yuran Ren",
        "Zijia Wang",
        "Ji Zhang",
        "Mengyue Wu",
        "Qin Jin",
        "Fei Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in large language models (LLMs) have significantly enhanced text generation capabilities, yet evaluating their performance in generative writing remains a challenge. Existing benchmarks primarily focus on generic text generation or limited in writing tasks, failing to capture the diverse requirements of high-quality written contents across various domains. To bridge this gap, we present WritingBench, a comprehensive benchmark designed to evaluate LLMs across 6 core writing domains and 100 subdomains, encompassing creative, persuasive, informative, and technical writing. We further propose a query-dependent evaluation framework that empowers LLMs to dynamically generate instance-specific assessment criteria. This framework is complemented by a fine-tuned critic model for criteria-aware scoring, enabling evaluations in style, format and length. The framework's validity is further demonstrated by its data curation capability, which enables 7B-parameter models to approach state-of-the-art (SOTA) performance. We open-source the benchmark, along with evaluation tools and modular framework components, to advance the development of LLMs in writing."
        },
        {
            "title": "Start",
            "content": "WritingBench: Comprehensive Benchmark for Generative Writing Yuning Wu1, Jiahao Mei1,3, Ming Yan1*, Chenliang Li1, Shaopeng Lai1, Yuran Ren2, Zijia Wang2, Ji Zhang1, Mengyue Wu3, Qin Jin2*, Fei Huang1 1Alibaba Group, 2Renmin University of China, 3Shanghai Jiao Tong University {yuningwu, ym119608}@alibaba-inc.com, qjin@ruc.edu.cn 5 2 0 2 ] . [ 1 4 4 2 5 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in large language models (LLMs) have significantly enhanced text generation capabilities, yet evaluating their performance in generative writing remains challenge. Existing benchmarks primarily focus on generic text generation or limited in writing tasks, failing to capture the diverse requirements of high-quality written contents across various domains. To bridge this gap, we present WritingBench, comprehensive benchmark designed to evaluate LLMs across 6 core writing domains and 100 subdomains, encompassing creative, persuasive, informative, and technical writing. We further propose querydependent evaluation framework that empowers LLMs to dynamically generate instancespecific assessment criteria. This framework is complemented by fine-tuned critic model for criteria-aware scoring, enabling evaluations in style, format and length. The frameworks validity is further demonstrated by its data curation capability, which enables 7B-parameter models to approach state-of-the-art (SOTA) performance. We open-source the benchmark, along with evaluation tools and modular framework components, to advance the development of LLMs in writing."
        },
        {
            "title": "Introduction",
            "content": "In recent years, LLMs (DeepSeek-AI et al., 2025; Hurst et al., 2024) have revolutionized text generation, demonstrating impressive performance across diverse applications, from generating creative content (Mirowski et al., 2023; Lei et al., 2024) and assisting in education (Wang et al., 2024a; Li et al., 2024b) to enhancing professional workflows (Shao et al., 2024; Li et al., 2024a). However, generative writing, which requires high levels of coherence, creativity, logical reasoning, and stylistic precision, poses unique challenge that existing evaluation methods fail to address adequately. *Corresponding authors Figure 1: WritingBench query example with colorcoded requirements. The three black-bordered categories highlight essential requirements analyzed in follow-up assessments. Red phrases correlate with grayshaded writing support materials. Current evaluation benchmarks for generative writing suffer from two major limitations: 1) Limited scope and diversity in task formulation; and 2) Inadequate evaluation metrics for complex writing tasks. First, there is significant lack of specialized benchmarks covering broad range of writing tasks. Most existing writing-oriented benchmarks are restricted to single domains, such as fictions (Karpinska et al., 2024; Gómez-Rodríguez and Williams, 2023), and their task formulations tend to be simplisticoften relying on singlesentence queries (Bai et al., 2024) or small set of instruction templates (Paech, 2023; Que et al., 2024). Additionally, many benchmarks use homogeneous input materials (Que et al., 2024; Karpinska et al., 2024), limiting their ability to accommodate the complex and customized requirements inherent in real-world writing scenarios. As result, they fail to capture the diversity and intricacies of practical writing tasks (see Figure 1). Second, Figure 2: Construction pipeline of WritingBench queries. The refinement pool contains five writing requirements (three core competencies with black borders) and one expression type (purple). Checked strategies refine initial queries into multi-requirement prompts (color-coded text) with red phrases referencing gray materials. Implementation details in Section 3.1. current automatic evaluation metrics lack the robustness needed for comprehensive and nuanced assessment of writing quality. While LLM-based evaluation methods show promise in capturing semantic meanings (Shao et al., 2024; Que et al., 2024; Bai et al., 2024), they typically rely on narrow set of predefined criteria (e.g., fluency and coherence). As LLMs continue to evolve with increasingly sophisticated writing capabilities, these static evaluation criteria and frameworks are inadequate for assessing the complex, multi-dimensional nature of writing, including creativity, argumentation strength, and domain-specific adherence. To address these challenges, we introduce WritingBench, comprehensive benchmark and robust framework for evaluating general-purpose writing. Our approach begins with carefully designed secondary domain categorization, grounded in realworld writing needs. We develop four-stage query construction pipeline (illustrated in Figure 2), where LLMs first generate and diversify writing queries, followed by human-driven material collection and optimization. This process ensures diverse set of writing tasks with broad domain coverage, varied requirements, and integration of heterogeneous source of materials. To enable more nuanced evaluation, we propose query-dependent evaluation framework that dynamically generates five instance-specific criteria using LLMs, which are then scored by fine-tuned critic model. Finally, we integrate the framework to filter writing-specific data and train small-scale model to verify its ability in identifying high-quality writing samples. Our primary contributions are as follows: We present WritingBench, an open-source writing benchmark comprising 1,239 queries across 6 primary domains and 100 subdomains,featuring style, format and length requirements. WritingBench supports extended-context generation with input ranging from tens to thousands of words, addressing real-world diversity. It facilitates systematic evaluation to identify improvement areas and highlights the potential of chain-of-thought (CoT) processes in creative tasks. We propose query-dependent evaluation framework that integrates instance-specific criteria generation with criteria-aware scoring model. It achieves 83% human alignment, significantly surpassing static-criteria baselines (65%, 59%). The effectiveness is further evidenced by its data curation capability - models trained with frameworkfiltered data match SOTA performance. We publicly release WritingBench, including its evaluation protocols, criteria generation tools with an integrated critic model, and to foster further rewriting-enhanced models, search. Available at: https://github.com/ X-PLUG/WritingBench."
        },
        {
            "title": "2.1 Writing Benchmarks",
            "content": "Existing evaluation benchmarks suffer from significant limitations in domain coverage and task granularity. For instance, EQ-Bench encompasses story-related quires using templated queries (Paech, 2023), while LongBench-Write incorporates length constraints in 120 queries (Bai et al., 2024); however, they both lack hierarchical domain taxonomies and multi-dimensional requirement specifications (e.g., style and format). Furthermore, most current benchmarks rely on fixed instruction templates, short contexts or materials predominantly from single source (Que et al., 2024; Karpinska et al., 2024; Marco et al., 2024), rendering them insufficient for addressing the complexity of real-world data needs. In contrast, our proposed benchmark fills these gaps by introducing 1,239 free-form queries distributed across 6 primary domains and 100 subdomains, with potential controls over style, format, and length, paired with inputs ranging from tens to thousands of words."
        },
        {
            "title": "2.2 Evaluation Methods",
            "content": "Using LLMs as evaluators has become prevalent approach for evaluating the quality of generated responses. Typically, researchers pre-define fixed set of evaluation dimensions applicable across all test instances (Gómez-Rodríguez and Williams, 2023; Marco et al., 2024). For example, SuperCLUE (Xu et al., 2023) employs three dimensions, whereas LongBench-Write (Bai et al., 2024) adopts six dimensions (e.g., relevance and accuracy). HelloBench (Que et al., 2024) introduces task-specific dimensions, but the dimensions remain consistent across all queries of given task. Although the LLM-as-a-judge approach enhances scalability, static evaluation dimensions often fail to accommodate the diversity of writing styles and specifications. To address this limitation, recent work (Liang et al., 2024) trains model to dynamically generate evaluation dimensions for individual queries. However, the dimensions remains confined to small predefined set. In contrast, our query-dependent evaluation framework leverages LLMs to generate diverse and instance-specific criteria while fine-tuning dedicated critic model to perform the evaluation."
        },
        {
            "title": "2.3 Writing-Enhanced Models",
            "content": "Although existing LLMs demonstrate exceptional writing capabilities, researchers strive to further enhance their overall writing proficiency. Recent models, such as Weaver (Wang et al., 2024b) and Suri (Pham et al., 2024), have exhibited notable domain-specific strengths. For instance, Weaver benefits from over 200B parameter pretraining, supporting four distinct writing domains, while LongWriter (Bai et al., 2024) specializes in length constraints. However, these models experience substantial performance degradation in crossdomain scenarios and multi-constraint tasks. In this work, leveraging the effectiveness of our evaluation framework, we introduce writing-enhanced models trained on high-quality data, achieving high performance across various tasks."
        },
        {
            "title": "3 WritingBench",
            "content": "Figure 3: Domain categories in WritingBench. Inner represents the 6 primary domains and outer depicts 100 secondary subdomains (Sub = subdomains per category, Num = dataset entries). In this section, we will mainly introduce the construction process of our WritingBench, and the query-dependent evaluation framework with the accompanied critic model for criteria-aware evaluation. Additionally, we train writing-enhanced model to demonstrate the effectiveness of our frameworks data curation capability."
        },
        {
            "title": "3.1 Benchmark Construction",
            "content": "WritingBench is constructed following delicate pipeline (see Figure 2) that combines modelgenerated query refinement and human annotation, ensuring diversity and real-world relevance. The two-phrase construction process is illustrated below."
        },
        {
            "title": "3.1.1 Model-Augmented Query Generation\nThis phase leverages LLMs to generate an initial\nset of writing queries, which are enriched and diver-\nsified through systematic guidance, with material\nsuggestions provided as needed.",
            "content": "Phase 1: Initial Query Generation We begin by constructing two-tiered domain pool grounded in real-world writing scenarios, consisting of 6 primary domains and 100 secondary subdomains (see Figure 3 and Appendix for detailed descriptions). The selected domains are designed to capture both traditional and emerging user needs for AI-assisted writing, encompassing categories such as Academic & Engineering, Finance & Business, Politics & Law, Literature & Art, Education, Advertising & Marketing. Leveraging the domain and subdomain tags, we prompt two Benchmark EQ-Bench LongBench-Write HelloBench WritingBench (Ours) Num 241 120 647 1,239 Domains Requirement Input Token Primary Secondary Style Format Length Avg Max Free Query-Form Diverse Material-Source 1 7 5 6 / / 38 130 87 1,210 1,546 213 684 7,766 19,361 / / Table 1: Comparison of existing writing benchmarks. Category Num Avg Token Max Token Domain Academic & Engineering Finance & Business Politics & Law Literature & Arts Education Advertising & Marketing 187 238 226 242 151 195 1,915 1,762 2,274 1,133 1,173 886 15,534 19,361 18,317 9,973 10,737 6,504 Style Format Length <1K 1K-3K 3K-5K 5K+ Requirement 395 342 214 1,404 1,591 1,226 18,197 18,197 14,097 Length 727 341 94 77 443 1,808 3,804 8,042 994 2,991 4,966 19,361 Table 2: Data statistics for WritingBench categorized by domain, requirement, and length. different LLMs (ChatGPT-4o-latest and Claude3.5-Sonnet) to generate initial writing queries that simulate realistic user requests to maximize diversity (see Appendix B.1). Phase 2: Query Diversification To improve the diversity and practical applicability of queries, while taking into account real-world needs, we design set of query diversification strategies (see Appendix B.2) inspired by Xu et al. (2024), include three core requirements and three auxiliary dimensions: Stylistic adjustments (e.g., Use friendly and simple tone that kids can easily understand.) Format specifications (e.g., Follow the IEEE conference template) Length constraints (e.g., Generate 500-word executive summary) Personalization (e.g., From the viewpoint of an educator with two decades of experience.) Content specificity (e.g., Detail the 2023 Q3 financial metrics) Expression (e.g., Modify the query expression to be shorter.) The first three categories represent typical writing capabilities, which we will evaluate through specialized assessments. While refining the queries, the LLM simultaneously provides material requirements (e.g., requesting financial reports as input for market analysis queries) (see Appendix B.3). This approach results in enriched queries accompanied by corresponding recommended reference materials."
        },
        {
            "title": "3.1.2 Human-in-the-Loop Refinement",
            "content": "This phase incorporates human expertise to verify model-generated queries and supplement material requirements, thereby ensuring their alignment with real-world applications and avoiding harmful content. Phase 1: Material Collection At this stage, we engage 30 trained annotators, each compensated at $18 per hour and possessing specialized expertise proven through domainknowledge tests. Their role is to collect necessary open-source materials (e.g., public financial statements or legal templates), guided by material requirements generated by LLMs. To minimize errors arising from parsing documents in diverse formats, the annotators carefully extract and verify the most pertinent text segments. Phase 2: Expert Screening & Optimization Next, five experts with LLM experience or industry expertise perform data screening. The experts conduct delicate two-stage filtering process: (1) query adaptation: ambiguous or unrealistic queries are revised to better align with the provided materials and practical scenarios (e.g., adjusting legal opinion query to reference specific clauses from the supplied statutes), (2) material pruning: redundant or irrelevant content is eliminated from the collected materials, ensuring that the context provided for writing tasks remained focused and relevant while guaranteeing the information is free from harmful content. Finally, we construct WritingBench, benchmark comprising 1,239 queries categorized using Figure 4: Example of dynamically generating criteria for writing query in WritingBench. Different background colors represent various types of requirements. two-tiered taxonomy. In comparison to existing writing benchmarks summarized in Table 1, WritingBench exhibits notable advantages in terms of the number of instances, domain diversity, requirement coverage, and variability in input lengths. The detailed statistical distribution of WritingBench is shown in Table 2."
        },
        {
            "title": "3.2 Query-Dependent Evaluation Framework",
            "content": "Traditional LLM-as-a-judge evaluations typically rely on fixed evaluation criteria derived from general writing assessment conventions (Shao et al., 2024; Bai et al., 2024; Que et al., 2024). However, such static criteria exhibit three critical limitations: (1) Domain exhaustiveness: fixed criteria are inadequate in adapting to specialized domains, failing to address unique characteristics found in areas like technical documentation or creative writing; (2) Requirement specificity: such criteria lack the flexibility to encompass specific requirements related to style, format, or length control; and (3) Material dependency: they are insufficient to verify whether responses appropriately utilize the provided reference materials, such as incorporating data points or maintaining narrative continuity. To address these challenges, we propose querydependent evaluation framework that enables dynamic adaptation to diverse writing scenarios. Our approach comprises two phases: Phase 1: Dynamic Criteria Generation As illustrated in Figure 4, given query in the WritingBench, the LLM is prompted to automatically generate set of five evaluation criteria, Cq = {c1, . . . , c5} (opting for five criteria reflects common approach observed in many contemporary evaluation contexts (Shao et al., 2024; Ma et al., 2024; Marco et al., 2024)), using carefully designed instruction to ensure structural guidance during criteria specification (see Appendix B.4). Each criterion comprises three components: concise name summarizing the criterion, an extended description elaborating on the evaluation focus, and detailed scoring rubrics, which provide finegrained quality levels for the respective evaluation dimensions. Phase 2: Rubric-based Scoring For each criterion ci Cq, the evaluator independently assigns score on 10-point scale to response r, providing both score and justification. The final overall score is computed by averaging scores across all criteria. Detailed prompt is provided in Appendix B.5."
        },
        {
            "title": "3.3 Critic Model",
            "content": "To alleviate the computational overhead with LLMbased evaluation, we develop dedicated critic model, M, designed to implement our rubric-based scoring framework. Specifically, this model performs the mapping Mc : (q, r, Ci) (cid:55) [1, 10] , where the output consists of numerical score and corresponding justification text, , both in accordance with the predefined evaluation rubric. We fine-tune the critic model on dataset comprising 50K instances, which are collected using LLMs in our experiments. The dataset encompasses diverse queries, evaluation criteria, and Models Proprietary LLMs ChatGPT-4o-latest o1-Preview Claude-3.5-Sonnet Gemini-1.5-Pro Qwen-Max Open-source LLMs Deepseek-R1 Deepseek-V3 Mistral-Large-Instruct Qwen-2.5-72B-Instruct Qwen-2.5-7B-Instruct Llama-3.3-70B-Instruct Llama-3.1-8B-Instruct Avg 8.16 8.15 7.71 7.78 8.37 8.55 7.95 7.64 7.90 7.43 7.01 6.35 Capability-enhanced LLMs Suri LongWriter Qwen-2.5-7B-filtered Llama-3.1-8B-filtered 4.97 7.91 8.49 8.49 Languages Domains Requirements ZH EN D1 D2 D3 D4 D5 D6 R1 R2 R3 8.3 8.1 7.7 7.8 8.4 8.7 8.0 7.6 8.0 7.3 6.7 5.7 4.4 7.9 8.6 8.6 8.1 8.2 7.7 7.7 8.3 8.5 7.9 7.7 7.9 7.5 7.3 6.9 5.5 7.9 8.4 8. 8.1 8.0 7.6 7.7 8.3 8.5 7.9 7.7 8.0 7.7 7.0 6.6 5.6 8.0 8.4 8.5 8.1 8.1 7.5 7.5 8.3 8.5 7.8 7.6 7.8 7.4 6.9 6.4 5.3 8.1 8.4 8. 8.2 8.2 7.6 7.8 8.4 8.6 8.0 7.8 8.1 7.6 7.0 6.1 5.0 8.1 8.6 8.6 8.1 8.2 7.7 7.9 8.4 8.6 7.8 7.3 7.7 6.9 6.8 6.0 4.1 7.7 8.4 8. 8.4 8.4 7.9 8.0 8.5 8.7 8.2 7.9 8.2 7.8 7.3 6.7 5.0 8.1 8.6 8.6 8.1 8.1 8.0 7.9 8.4 8.6 8.0 7.6 7.8 7.3 7.3 6.6 5.1 7.6 8.5 8. 8.3 8.2 7.9 7.9 8.5 8.7 8.1 7.7 8.0 7.5 7.1 6.4 4.8 7.9 8.6 8.6 8.7 8.6 8.5 8.6 8.7 8.9 8.6 8.2 8.3 7.9 7.8 7.0 5.2 8.2 8.8 8. 8.2 8.2 7.7 7.9 8.4 8.6 8.0 7.7 8.0 7.6 7.1 6.4 5.0 8.1 8.5 8.5 8.9 8.8 8.5 8.8 9.0 9.0 8.9 8.7 8.8 8.6 8.2 7.6 5.4 8.8 9.0 8. 8.2 8.2 7.9 7.9 8.4 8.6 8.0 7.7 7.9 7.4 7.0 6.3 4.5 7.7 8.5 8.5 8.3 8.2 8.0 8.0 8.5 8.7 8.2 7.9 8.0 7.5 7.2 6.4 4.0 7.7 8.6 8. Table 3: WritingBench performance of different LLMs across 6 domains and 3 writing requirements evaluated with our critic model (scale: 1-10). The six domains include: (D1) Academic & Engineering, (D2) Finance & Business, (D3) Politics & Law, (D4) Literature & Art, (D5) Education, and (D6) Advertising & Marketing. The three writing requirements assessed are: (R1) Style, (R2) Format, and (R3) Length. Here, C\" indicates category-specific scores. model responses to enhance the robustness of evaluation. The experiments presented in Section 4.3 validate the consistency of the critic model."
        },
        {
            "title": "Writing Enhancement",
            "content": "The query-dependent evaluation framework enables systematic data curation across diverse writing tasks through two-phase filtering. First, applying instance-specific criteria generation to initial 24K SFT samples. Subsequently, the critic model filters 50% samples, yielding 12K frameworkcurated samples with higher quality. Fine-tuning experiments are conducted using the llama-3.1-8binstruct and qwen-2.5-7b-instruct models. Evaluations are performed on two writing benchmarks to validate the frameworks effectiveness in identifying high-quality writing samples."
        },
        {
            "title": "4.1 Experiment Settings",
            "content": "This section outlines the settings employed in our experiments using the WritingBench framework. The evaluation protocol, and the model training configurations are as follows: Evaluation Protocol: Uniform generation settings are applied to model responses on WritingBench, allowing up to 16,000 tokens. We set the temperature at 0.7, with top-k of 20 and top-p of 0.8, ensuring balance of creativity. The critic model uses the same parameters, with input length capped at 2,048 tokens for scoring stability. Model Training: The critic model is fine-tuned on the Qwen-2.5-7B-Instruct base model, using the AdamW optimizer with 7e-6 learning rate. It is trained on 50K SFT data for 3 epochs across 8xA100 GPUs (batch size 64, 8-step accumulation). The writing models, trained on both Qwen-2.5-7BInstruct and Llama-3.1-8B-Instruct for 5 epochs on 32xA100 GPUs, achieving total batch size of 128 with 4-step gradient accumulation."
        },
        {
            "title": "4.2 Comparison between LLMs",
            "content": "We evaluate 16 LLMs: ChatGPT-4o-latest (Hurst et al., 2024), o1-Preview (Jaech et al., 2024), Claude-3.5-Sonnet (Anthropic, 2024), Gemini1.5-Pro (Reid et al., 2024), Qwen-Max (Team, 2025), Deepseek-R1 (DeepSeek-AI et al., 2025), Deepseek-V3 (DeepSeek-AI et al., 2024), MistralLarge-Instruct (MistralAI, 2024), Qwen-2.5-72BInstruct and Qwen-2.5-7B-Instruct (Yang et al., 2024), Llama-3.3-70B-Instruct and Llama-3.1-8BInstruct (Dubey et al., 2024), Suri (Pham et al., 2024), LongWriter (Bai et al., 2024), and our writFigure 5: Scores of different models across different subdomains in WritingBench. Red indicates higher score and blue refers to lower score. The figures are the average score of each subdomain for different models. ing model, Qwen-2.5-7B fine-tuned model and Llama-3.1-8B fine-tuned model, on WritingBench. Each query is assessed by 5 instance-specific criteria using 10-point scale, with subdomain-specific subcategory heatmaps revealing detailed variations. Key Insights from Domain Scores: From Table 3, we observe that Education (D5) always yields high performance among models, followed by Academic & Engineering (D1). Literature & Art (D5) is the lowest-scoring domain, showing notable variance. CoT-capable models like Deepseek-R1 and o1-preview outperform their non-CoT counterparts models, indicating CoTs potential in handling narrative and creative content. Figure 5 identifies challenges in niche areas such as novel continuations, tender proposals, and white papers where models generally score lower. These tasks demand higher level of knowledge, long-text generation capabilities, and adherence to contextual consistency, pinpointing areas for further enhancement. Key Insights from Requirement Scores: Table 3 presents scores for three requirement subsets: columns showing overall scores and columns showing requirement-specific averages. For example, if style query includes two relevant criteria (as confirmed by human annotation), the column averages scores for those criteria only. Style (R1) and Format (R2) dimensions reveal model distinctions , with Deepseek-R1 and Qwen-Max often excels others. Nevertheless, length requirements (C of R3) remains challenging, especially in sectionspecific constraints (see Appendix 16 for examples) and extended text production (see Section 4.5). The overall analysis of WritingBench experiment highlights: Deepseek-R1 consistently leads across both domain and requirement dimensions, showcasing its versatility and strong language capabilities. There is significant performance variance within creative content domains, with CoT models showing notable promisea hypothesis further confirmed by our comparison (see Appendix C.1, where the model incorporating CoT mechanisms outperform the one without CoT on creative writing tasks). Additionally, we observe that advanced models often achieve higher specialized scores (C column) for the three common requirements than their overall scores (R column). The criteria outside these specialized sets tend to emphasize content aspects, such as integration with source materials and writing depth, highlighting the need to improve content quality. This analysis benchmarks existing model capabilities and underscores specific areas needing improvement for future development."
        },
        {
            "title": "4.3 Human Consistency",
            "content": "To validate the alignment between automated evaluation and human judgment, we conducted human evaluation on 300 queries, covering all 100 subdomains. Five professionally trained annotators with linguistic backgrounds perform pairwise comparisons of model responses. For each query, two responses are randomly selected from different models. Annotators select the preferred response or declared equivalence based on the querys requirements. We compare our dynamic query-dependent criteria against two baselines: static globally uniform criteria and static domain-specific customized criteria (static criteria are designed by domain experts) on two LLM judges: ChatGPT-4o-latest and Claude-3.5-Sonnet. As shown in Table 4, our dynamic querydependent criteria achieve superior human alignment compared to static, both globally uniform or domain-specific customized criteria. Notably, Evaluation Metric Judge Score Models WritingBench Benchmark2 Static Global Static Domain-Specific Dynamic Query-Dependent ChatGPT ChatGPT ChatGPTt Claude Static Global Static Domain-Specific Claude Dynamic Query-Dependent Claude Dynamic Query-Dependent Critic Model 69% 40% 79% 65% 59% 87% 83% Deepseek-R1 Qwen-2.5-7B-Instruct Llama-3.1-8B-Instruct Qwen-2.5-7B-all Qwen-2.5-7B-filtered Llama-3.1-8B-all Llama-3.1-8B-filtered 8.55 7.43 6.35 8.46 8.49 8.45 8. 4.79 4.39 3.12 4.69 4.70 4.65 4.65 Table 4: Comparison of human consistency scores across different criteria generation methods. ChatGPT corresponds to ChatGPT-4o-latest, Claude corresponds to Claude-3.5-Sonnet. Table 5: Performance evaluation of our writing models on two benchmarks.-filtered indicates models trained with filtered data, while -all refers to those trained with the full dataset. domain-specific criteria underperform despite customization, since our queries involve highly diverse tasks and varied sources. These findings confirm that context-sensitive query-dependent evaluation better captures real-world writing complexity compared to conventional static approaches. Furthermore, the critic model attains 83% agreement, confirming its practical viability."
        },
        {
            "title": "4.4 Ablation of Data Curation for\nWriting-Enhanced Models",
            "content": "Leveraging the query-dependent evaluation framework described in Section 3.2, we perform instancespecific criteria generation an initial 24K SFT dataset spanning various domains, including longinput queries enriched with supplemental materials. Our critic model then apply query-dependent evaluation to filter this dataset down to high-quality 12K subset. We conduct fine-tuning experiments using the Llama-3.1-8B-Instruct and Qwen-2.57B-Instruct models. As shown in 5, both models, trained on the filtered data, demonstrate significant performance improvements over their previous versions. Notably, they outperform models trained on the full dataset and even approached the capabilities of SOTA models. Further validation on an alternative benchmark, LongBench-Write (following the settings of quality evaluation in (Bai et al., 2024)), confirm these findings, with our models surpassing baseline performance. These results validate the robustness of our query-dependent evaluation strategy and the utility of our critic model for curating high-quality writing samples. This integrated approach enables smaller models to compete with, and even surpass, larger models across diverse writing tasks."
        },
        {
            "title": "4.5 Ablation of Length",
            "content": "We assess the performance of LLMs across varying input and output lengths, with statistical validity ensured by excluding intervals containing fewer than 5 samples.As demonstrated in Appendix 6, experiments reveal that most SOTA models generally maintain consistent performance regardless of input length variations, attributable to their advanced long-context comprehension capabilities. However, analysis in Appendix 7 shows that most models exhibit inherent limitations in response generation length, typically producing outputs constrained to approximately 3,000 tokens. While output quality remains relatively stable within this range, smaller models suffer more performance degradation characterized by repetitive outputs. Notably, only two models, Qwen-Max and LongWriter, which incorporate architectural optimizations for long-form generation, effectively support extended response lengths. It underscores the importance of enhancing long-output capabilities and length optimization in writing."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduce WritingBench, comprehensive benchmark designed to evaluate LLMs in generative writing across diverse domains. It includes 1,239 queries spanning 6 primary domains and 100 subdomains, providing evaluation dimensions for style, format, and length requirements. Our query-dependent evaluation framework, supported by critic model, achieves high human alignment. Evaluation efficiency is further demonstrated by compact models trained on curated data, achieving performance comparable to SOTA models. By making WritingBench and its resources publicly available, we aim to foster further research and advancements in LLM writing capabilities."
        },
        {
            "title": "Limitations",
            "content": "This work faces three primary limitations that warrant consideration. First, both the writing model and critic model are primarily trained using conventional SFT approaches, omitting systematic exploration of enhanced optimization strategies. While we demonstrate the partial efficacy of the CoT mechanisms in creative domains, their potential remains unexplored compared to established successes in mathematical reasoning tasks. Second, our evaluation framework exhibits diminished precision in handling complex multidimensional length requirements, including temporal sequencing constraints and section-specific word counts. This limitation underscores the necessity for enhanced scoring methodologies that integrate learned metrics with structured rule-based evaluations to better regulate output specifications. Third, inherent challenges persist in obtaining reliable pairwise preference annotations for compositional tasks. Despite rigorous annotation protocols, human evaluators inevitably introduce subjective biases when assessing two fair-well responses, particularly regarding narrative preferences and contextual interpretations. While our consensusbuilding procedures mitigate some variability, absolute alignment with diverse user preferences remains theoretically unattainable. Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948."
        },
        {
            "title": "References",
            "content": "Anthropic. 2024. Introducing claude 3.5 sonnet. Accessed: 2024-10-22. Yushi Bai, Jiajie Zhang, Xin Lv, Linzhi Zheng, Siqi Zhu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2024. Longwriter: Unleashing 10,000+ word generation from long context llms. CoRR, abs/2408.07055. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, and Wangding Zeng. 2024. Deepseek-v3 technical report. CoRR, abs/2412.19437. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. 2024. The llama 3 herd of models. CoRR, abs/2407.21783. Carlos Gómez-Rodríguez and Paul Williams. 2023. confederacy of models: comprehensive evaluation of llms on creative writing. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 14504 14528. Association for Computational Linguistics. Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander Madry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll L. Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, and Dane Sherburn. 2024. Gpt-4o system card. CoRR, abs/2410.21276. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian Osband, Ignasi Clavera Gilaberte, and Ilge Akkaya. 2024. Openai o1 system card. CoRR, abs/2412.16720. Marzena Karpinska, Katherine Thai, Kyle Lo, Tanya Goyal, and Mohit Iyyer. 2024. One thousand and one pairs: \"novel\" challenge for long-context language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages 1704817085. Association for Computational Linguistics. Huang Lei, Jiaming Guo, Guanhua He, Xishan Zhang, Rui Zhang, Shaohui Peng, Shaoli Liu, and Tianshi Chen. 2024. Ex3: Automatic novel writing by extracting, excelsior and expanding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 91259146. Association for Computational Linguistics. Miao Li, Jey Han Lau, and Eduard H. Hovy. 2024a. sentiment consolidation framework for meta-review generation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1015810177. Association for Computational Linguistics. Qingyao Li, Lingyue Fu, Weiming Zhang, Xianyu Chen, Jingwei Yu, Wei Xia, Weinan Zhang, Ruiming Tang, and Yong Yu. 2024b. Adapting large language models for education: Foundational capabilities, potentials, and challenges. CoRR, abs/2401.08664. Xiaobo Liang, Haoke Zhang, Helan hu, Juntao Li, Jun Xu, and Min Zhang. 2024. Fennec: Finegrained language model evaluation and correction extended through branching and bridging. CoRR, abs/2405.12163. Yan Ma, Yu Qiao, and Pengfei Liu. 2024. Mops: Modular story premise synthesis for open-ended automatic In Proceedings of the 62nd Anstory generation. nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 2135 2169. Association for Computational Linguistics. Guillermo Marco, Julio Gonzalo, María Teresa Mateo Girona, and Ramón Santos. 2024. Pron vs prompt: Can large language models already challenge worldclass fiction author at creative text writing? In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages 1965419670. Association for Computational Linguistics. Piotr Mirowski, Kory W. Mathewson, Jaylen Pittman, and Richard Evans. 2023. Co-writing screenplays and theatre scripts with language models: Evaluation by industry professionals. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, CHI 2023, Hamburg, Germany, April 23-28, 2023, pages 355:1355:34. ACM. MistralAI. 2024. Large enough. Samuel J. Paech. 2023. Eq-bench: An emotional intelligence benchmark for large language models. CoRR, abs/2312.06281. Chau Pham, Simeng Sun, and Mohit Iyyer. 2024. Suri: Multi-constraint instruction following in long-form text generation. In Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, pages 1722 1753. Association for Computational Linguistics. Haoran Que, Feiyu Duan, Liqun He, Yutao Mou, Wangchunshu Zhou, Jiaheng Liu, Wenge Rong, Zekun Moore Wang, Jian Yang, Ge Zhang, Junran Peng, Zhaoxiang Zhang, Songyang Zhang, and Kai Chen. 2024. Hellobench: Evaluating long text generation capabilities of large language models. CoRR, abs/2409.16191. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy P. Lillicrap, Jean-Baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud, Andrew M. Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James Molloy, Jilin Chen, Michael Isard, Paul Barham, Tom Hennigan, Ross McIlroy, Melvin Johnson, Johan Schalkwyk, Eli Collins, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Clemens Meyer, Gregory Thornton, Zhen Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher, Ankesh Anand, Richard Ives, James Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri, Pranav Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezener, and et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. CoRR, abs/2403.05530. Yijia Shao, Yucheng Jiang, Theodore A. Kanell, Peter Xu, Omar Khattab, and Monica S. Lam. 2024. Assisting in writing wikipedia-like articles from scratch with large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 62526278. Association for Computational Linguistics. Qwen Team. 2025. Qwen2.5-max: Exploring the intelligence of large-scale moe model. Accessed: 202502-15. Shen Wang, Tianlong Xu, Hang Li, Chaoli Zhang, Joleen Liang, Jiliang Tang, Philip S. Yu, and Qingsong Wen. 2024a. Large language models for education: survey and outlook. CoRR, abs/2403.18105. Tiannan Wang, Jiamin Chen, Qingrui Jia, Shuai Wang, Ruoyu Fang, Huilin Wang, Zhaowei Gao, Chunzhao Xie, Chuou Xu, Jihong Dai, Yibin Liu, Jialong Wu, Shengwei Ding, Long Li, Zhiwei Huang, Xinle Deng, Teng Yu, Gangan Ma, Han Xiao, Zixin Chen, Danjun Xiang, Yunxia Wang, Yuanyuan Zhu, Yi Xiao, Jing Wang, Yiru Wang, Siran Ding, Jiayang Huang, Jiayi Xu, Yilihamu Tayier, Zhenyu Hu, Yuan Gao, Chengfeng Zheng, Yueshu Ye, Yihang Li, Lei Wan, Xinyue Jiang, Yujie Wang, Siyu Cheng, Zhule Song, Xiangru Tang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, Yuchen Eleanor Jiang, and Wangchunshu Zhou. 2024b. Weaver: Foundation models for creative writing. CoRR, abs/2401.17268. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, and Daxin Jiang. 2024. Wizardlm: Empowering large pre-trained language models to follow complex instructions. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Liang Xu, Anqi Li, Lei Zhu, Hang Xue, Changtai Zhu, Kangkang Zhao, Haonan He, Xuanwei Zhang, Qiyue Kang, and Zhenzhong Lan. 2023. Superclue: comprehensive chinese large language model benchmark. CoRR, abs/2307.15020. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2024. Qwen2.5 technical report. CoRR, abs/2412.15115."
        },
        {
            "title": "A Benchmark Statistics",
            "content": "A.1 Overview of Six Primary Domains 1. Academic & Engineering: This domain encompasses the structured and formalized nature of academic writing workflows, focusing on clarity, precision, and adherence to rigorous standards. includes the creation of paper outlines, abstracts, literature reviews, experiment reports, and technical documents such as patents and test reports. The writing prioritizes logical argumentation, thorough analysis, and the integration of empirical evidence. 2. Finance & Business: Writing in this domain is analytical and strategic, aimed at informing decision-making and promoting corporate objectives. It includes wide range of documentation such as contracts, market analyses, investment reports, strategic plans, and operational materials like product specifications and sales reports. The emphasis is on clarity and conciseness, with focus on financial acumen and strategic insights. 3. Politics & Law: This domain demands an authoritative and formal tone, as it involves the composition of government documents, legal writings, and political communications. These materials require careful balance between clarity and formality, often employing complex and structured language. The aim is to clearly convey policy positions, legal arguments, or political messages while strictly adhering to legal and procedural standards. 4. Literature & Art: This domain covers the creative and expressive realms of writing, including novels, poetry, scripts, artistic designs, and critiques of books and movies. Writers explore thematic and emotional depths, crafting works that connect with audiences on human level. The language is rich and evocative, allowing for personal exploration of ideas that engage and move the reader. 5. Education: This domain involves pedagogical materials and educational communication, including lesson plans, course designs, feedback, assignments, and institutional communications like admissions promotions and parent-teacher meeting scripts. The writing prioritizes clarity, accessibility, and instructional effectiveness, using an approachable tone to facilitate learning and engagement. 6. Advertising & Marketing: Writing in this domain is vibrant and persuasive, designed to captivate and influence target audiences across various digital platforms. It includes social media scripts, advertising copy, brand narratives, and multimedia campaign materials. The writing is dynamic and strategic, with creative twist, necessitating deep understanding of audience psychology and trend dynamics to ensure that the content is appealing and strategically effective. A.2 Overview of 100 Secondary Subdomains See Table 7, Table 8 and Table 9."
        },
        {
            "title": "B Prompts",
            "content": "B."
        },
        {
            "title": "Initial Query Generation Prompt",
            "content": "Generate 10 different writing requests under {domain2} within the context of {domain1}. Ensure the requests are as detailed and specific as possible, and reflect realistic user tone and needs. Please return in the following JSON format, and do not include anything outside of JSON: [ \"Writing request 1\", \"Writing request 2\", . . . ] Table 6: Initial query generation prompt introduced in Section 3.1.1. Subdomain Description Academic & Engineering Paper Outline Hierarchical organization of research components and logical flow Acknowledgments Formal recognition of institutional and individual support Limitations Systematic identification of methodological constraints and scope boundaries Defense Presentation Presentation supporting materials, such as slides Research Proposal Investigation blueprint with validation road map Technical Documentation Implementation specifications and system interface protocols Experiments Introduction Conclusion Test Report Contributions Internship Report Literature Review Defense Script Abstract Parameterized validation framework with controlled variable analysis Contextual foundation establishing research gaps and significance synthesize the main findings of the research or project Evaluations of testing activities and performance Novel aspects differentiating the work from prior research Chronological documentation of practical work placement Critical gap analysis through scholarly works taxonomy Oral presentations and responses for research defense. Summary of research objectives, methods, results, and significance Engineering Report Technical analysis on tasks, methodologies, and outcomes Patent Legal-technical specification of novel implementable claims Finance & Business Meeting Minutes User Research Concise documentation of key discussion points, decisions, and action items Insight collection on user needs and behaviors to inform product or service design Business Correspondence Formal communication with internal or external stakeholders for business purposes Human Resource Management Strategies and processes for managing workforce effectively Recruitment Briefing Event Planning Market Research Market Analysis Strategies for attracting, selecting, and onboarding suitable candidates Summarized information provided to stakeholders ahead of task or meeting Coordinated organization of logistics and activities for event execution Systematic collection and analysis about market and consumer Evaluation of market trends, size, competitors, and dynamics Risk Management Identification, assessment, and prioritization of risks with mitigation strategies Sales Report Pitch Deck Contract Summary of sales activities, performance, and revenue figures over given period Visual presentation designed to communicate business ideas or proposals to investors Legally binding agreement outlining the terms and conditions for business transactions Tender Document Formal proposal request containing project specifications and bidding instructions Investment Analysis Evaluation of financial investments to determine potential returns and risks Product Proposal Strategic Planning Financial Reports Detailed plan outlining the development, features, and potential of new products Business goal setting with actionable strategies for desired outcomes Comprehensive statements reflecting the financial performance and status Requirements Specification Documentation detailing functional and non-functional requirements for project Bid Proposal Formal offer to provide goods or services at specified price, addressing client needs Table 7: Subdomains in Academic & Engineering and Finance & Business. Domain Description Politics & Law Legal Opinion Authoritative assessment and guidance on legal matters or questions Government Speech Formal address delivered by government officials outlining policies or positions Judgment Document Official written decision or order issued by court Legal Agreement Binding contract setting out terms and obligations between parties Case Study Case Bulletin In-depth analysis of legal case for educational or professional purposes Summary and update on ongoing or concluded legal cases Legal Consultation Professional advice provided on legal rights, responsibilities, or strategies Regulatory Analysis Examination of rules and regulations affecting compliance and enforcement Meeting Summary Ideological Report Brief overview of discussions, decisions, and outcomes from meeting Analysis or commentary on political or ideological trends and perspectives Policy Interpretation Explanation or clarification for public or organizational guidance Official Document Formal written record issued by government entities or officials Legal Awareness Campaign Initiative to educate the public on legal rights and responsibilities Defense Plea Formal written argument submitted by the defense in legal proceeding Party Membership Application Form and process for joining political party Policy Advocacy Work Report Efforts to influence or promote specific policy changes or implementations Detailed account of activities, achievements, and challenges within specific period Deed Achievement Record highlighting significant accomplishments and contributions Litigation Documents Legal filings and paperwork submitted in the course of lawsuit White Paper Authoritative report providing information or proposals on an issue Character Design Greeting Message Host Script Novel Outline Podcast Script Literature & Art Creation and development of detailed characters for stories or visual media Friendly or formal introductory statement used for various occasions Guided narration and dialogue for presenter during an event or show Structured plan for the plot, characters, and settings of novel Written content outlining the dialogue and segments for podcast episodes Derivative Work Creative work based on or inspired by an existing piece Reading Reflection Personal thoughts and analysis on piece of literature Video Script Book Review Game Design Lyric Writing Brainstorm Script detailing dialogue and action for video content creation Critical evaluation and summary of books content and impact Creation of game mechanics, stories, and interfaces for interactive entertainment Crafting of words for songs with rhyme and meter considerations Rough ideas and notes generated during creative thinking session Plot Development Process of mapping out the storyline and narrative structure Prose Screenplay Written or spoken language in its ordinary form, without metrical structure Scripted blueprint for film or television with dialogue and directions Novel Manuscript Complete text of novel prepared for publication Biography Detailed account of persons life experiences and achievements Film/TV Review Analytical critique of film or television shows content and effectiveness Poetry Fan Fiction Artistic composition using rhythmic and metaphorical language Amateur stories written by enthusiasts featuring characters from existing media Table 8: Subdomains in Politics & Law and Literature & Art. Domain Description Education Training Reflection Personal assessment of training experiences and learned insights Class Activity Planned exercises or tasks designed to engage students in learning Parent-Teacher Meeting Formal discussion between educators and parents about student progress Lesson Plan Structured outline of educational objectives and teaching methods for class Teaching Materials Resources used to aid in presenting information to students Assignment Grading Evaluation and scoring of student work based on specific criteria Curriculum Design Development of educational content, structure, and delivery methods Educational Report Analysis or summary of educational outcomes and performance Coursework Academic work assigned to students as part of course Evaluation Comments Feedback provided on student performance and areas of improvement Educational Consulting Professional guidance on educational strategies and systems Admissions Promotion Strategies and activities aimed at encouraging enrollment in educational institutions Advertising & Marketing Sales Letter Persuasive written communication intended to motivate potential buyers Product Description Detailed overview of products features, benefits, and uses Social Media Content Engaging text, images, or videos crafted for online platforms Multimedia Script Planned screenplay integrating various forms of media for marketing Promotional Copy Compelling text written to boost interest and sales of products Promotional Voiceover Recorded narration to accompany marketing visuals or ads Travel Guide Brand Story Personal Blog Informative content offering insights and tips for travelers Narrative that outlines the history, values, and mission of brand Individual commentary or stories shared in an informal online format Marketing Commentary Analytical thoughts on marketing trends and strategies Slogans Catchy and memorable phrases designed to convey brand identity Table 9: Subdomains in Education and Advertising & Marketing. B.2 Guidance Pool B.4 Criteria Generation Prompt"
        },
        {
            "title": "Query Refinement Guidance Pool",
            "content": "[ \"Add requirement for generating specific lengths.\", \"Include format adherence requirements, such as writing according to prescribed outline or outputting in specific format.\", \"Add style requirements, like drafting speech suitable for particular occasion or adopting the style suitable for specific audience or mimicking particular tone.\", \"Incorporate user personalization needs, such as considering the users identity or integrating personal experiences.\", \"Include more specific content requirements, like details about particular event or focusing on specific content.\", \"Express concisely in one sentence.\" ] Table 10: Query refinement guidance pool introduced in Section B.5. B.3 Query Refine Prompt"
        },
        {
            "title": "Query Refinement Prompt",
            "content": "Please refine and enhance the original writing requirements in the context of generating content in {domain2} from {domain1} based on the provided guidance. Include as many details as possible and indicate whether additional writing materials are needed. ** Original Writing Requirements ** {query} ** Guidance for Modification ** {guidance} ** Output Requirements ** Return the result strictly in the following JSON format, with no additional content outside the JSON: { \"query\": \"Modified writing requirements\", \"material\": \"Whether additional reference materials are needed to supplement the writing requirements. If needed, provide suggestions for the materials; if not needed, return\" }"
        },
        {
            "title": "System Evaluation Prompt",
            "content": "You are an expert evaluator with extensive experience in evaluating the response of given query. Table 12: Evaluation system prompt introduced in Section 3.2."
        },
        {
            "title": "Criteria Generation Prompt",
            "content": "Please generate five strict evaluation criteria for assessing the response given the following query. Each criterion should include the following fields: name, criteria_description, 1-2, 3-4, 5-6, 7-8, 9-10. The criteria should be designed to emphasize detailed assessment and distinguish subtle differences in quality. Ensure that the criteria can discern issues such as relevance, coherence, depth, specificity, and adherence to the query context. Do not include any additional text. Only output the criteria in the specified JSON format. ** Query ** {query} ** Output format ** [ {\"name\": \"first_criteria_name\", \"criteria_description\": \"Description for the first criteria, emphasizing detailed and critical assessment.\", \"1-2\": \"Low score description: Clearly deficient in this aspect, with significant issues.\", \"3-4\": \"Below average score description: Lacking in several important areas, with noticeable problems.\", \"5-6\": \"Average score description: Adequate but not exemplary, meets basic expectations with some minor issues.\", \"7-8\": \"Above average score description: Generally strong but with minor shortcomings.\", \"9-10\": \"High score description: Outstanding in this aspect, with no noticeable issues.\" }, ... ] Table 11: Query refinement prompt introduced in Section 3.1.1. Table 13: Criteria generation prompt introduced in Section 3.2. B.5 Rubric-based Scoring Prompt Models WritingBench-D4 EQBench Deepseek-R1 Qwen-2.5-32B-Instruct Qwen-2.5-32B-CoT -w/o CoT 8.55 7.34 8.66 8.49 84.99 48. 82.48 79.43 Table 15: Ablation of CoT in creative content on two benchmarks. C.2 Case Study"
        },
        {
            "title": "Query",
            "content": "Create advertising slogans for pet grooming salon, including: - Main slogan (10-15 words) - Sub-slogan (15-20 words) - Service tagline (8-12 words) - Brand promise (10-15 words) The slogans should reflect professional pet grooming and care services while emphasizing thoughtful care for pet owners. The slogans should be catchy, memorable, with warm and friendly wording, and light, cheerful tone."
        },
        {
            "title": "Response",
            "content": "**Main Slogan:** Pamper Your Pet, Delight Your Heart. **Sub-Slogan:** Expert grooming and loving care, making your furry friend feel fabulous, inside and out. **Service Tagline:** Gentle paws, happy pets, stress-free smiles. **Brand Promise:** Quality grooming, compassionate care, every time. Table 16: Bad case of length requirement. Query from WritingBench with index 539, response generated by Gemini-1.5-Pro."
        },
        {
            "title": "Scoring Prompt",
            "content": "Evaluate the Response based on the Query and criteria provided. ** Criteria ** {criteria} ** Query ** {query} ** Response ** {response} Provide your evaluation based on the criteria: {criteria} Provide reasons for each score, indicating where and why any strengths or deficiencies occur within the Response. Reference specific passages or elements from the text to support your justification. Ensure that each reason is concrete, with explicit references to the text that align with the criteria requirements. Scoring Range: Assign an integer score between 1 to 10 Output format: Return the results in the following JSON format, Only output this JSON format and nothing else: { \"score\": an integer score between 1 to 10, \"reason\": \"Specific and detailed justification for the score using text elements.\" } Table 14: Scoring prompt introduced in Section 3.2."
        },
        {
            "title": "C Experiment Results",
            "content": "C.1 Ablation of CoT in Creative content To validate the impact of CoT reasoning on creative content generation, we conduct knowledge distillation experiments using DeepSeek-R1. Two variants of Qwen-2.5-32B-Instruct are developed: 1) Base model trained with CoT-formatted instructions, 2) ablated version (-w/o CoT) using direct response generation. These models are evaluated on the Literature & Art (D5) subset of WritingBench and EQBench, specialized benchmark for fiction evaluation, to thoroughly examine their capabilities in generating creative content. As shown in Table 15, the CoT-enhanced model achieves 8.66 (vs. 8.49) on WritingBench-D5 and 82.48 (vs. 79.43) on EQBench, showing CoTs effectiveness in generating creative content. Figure 6: Scores of different models across various input lengths on the WritingBench. Figure 7: Scores of different models across various output lengths on the WritingBench."
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "Renmin University of China",
        "Shanghai Jiao Tong University"
    ]
}