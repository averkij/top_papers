{
    "paper_title": "Unfolding Spatial Cognition: Evaluating Multimodal Models on Visual Simulations",
    "authors": [
        "Linjie Li",
        "Mahtab Bigverdi",
        "Jiawei Gu",
        "Zixian Ma",
        "Yinuo Yang",
        "Ziang Li",
        "Yejin Choi",
        "Ranjay Krishna"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Spatial cognition is essential for human intelligence, enabling problem-solving through visual simulations rather than solely relying on verbal reasoning. However, existing AI benchmarks primarily assess verbal reasoning, neglecting the complexities of non-verbal, multi-step visual simulation. We introduce STARE(Spatial Transformations and Reasoning Evaluation), a benchmark designed to rigorously evaluate multimodal large language models on tasks better solved through multi-step visual simulation. STARE features 4K tasks spanning foundational geometric transformations (2D and 3D), integrated spatial reasoning (cube net folding and tangram puzzles), and real-world spatial reasoning (perspective and temporal reasoning), reflecting practical cognitive challenges like object assembly, mechanical diagram interpretation, and everyday spatial navigation. Our evaluations show that models excel at reasoning over simpler 2D transformations, but perform close to random chance on more complex tasks like 3D cube net folding and tangram puzzles that require multi-step visual simulations. Humans achieve near-perfect accuracy but take considerable time (up to 28.9s) on complex tasks, significantly speeding up (down by 7.5 seconds on average) with intermediate visual simulations. In contrast, models exhibit inconsistent performance gains from visual simulations, improving on most tasks but declining in specific cases like tangram puzzles (GPT-4o, o1) and cube net folding (Claude-3.5, Gemini-2.0 Flash), indicating that models may not know how to effectively leverage intermediate visual information."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 3 3 6 4 0 . 6 0 5 2 : r Unfolding Spatial Cognition:"
        },
        {
            "title": "Evaluating Multimodal Models on Visual Simulations",
            "content": "Linjie Li 1, Mahtab Bigverdi 1, Jiawei Gu 2, Zixian Ma 1, Yinuo Yang 1, Ziang Li 1, Yejin Choi 3, Ranjay Krishna 1 1 University of Washington 2 Sun Yat-sen University 3 Stanford University Figure 1: Visual simulations play crucial role in real-world tasks, from assembling complex structures to interpreting mechanical diagrams and predicting spatial interactions. Different from how humans would approach cube net folding problem, existing multimodal models rely heavily on textual simulation, which is not sufficient for reaching human-level spatial cognition. The above example shows how textual simulations of GPT-4o make obvious errors when we simulate the steps in 3D space."
        },
        {
            "title": "Abstract",
            "content": "Spatial cognition is essential for human intelligence, enabling problem-solving through visual simulations rather than solely relying on verbal reasoning. However, existing AI benchmarks primarily assess verbal reasoning, neglecting the complexities of non-verbal, multi-step visual simulation. We introduce STARE (Spatial Transformations and Reasoning Evaluation), benchmark designed to rigorously evaluate multimodal large language models on tasks better solved through multi-step visual simulation. STARE features 4K tasks spanning foundational geometric transformations (2D and 3D), integrated spatial reasoning (cube net folding and tangram puzzles), and real-world spatial reasoning (perspective and temporal reasoning), reflecting practical cognitive challenges like object assembly, mechanical diagram interpretation, and everyday spatial navigation. Our evaluations show that models excel at reasoning over simpler 2D transformations, but perform close to random chance on more complex tasks like 3D cube net folding and tangram puzzles that require multi-step visual simulations. Humans achieve near-perfect accuracy but take considerable time (up to 28.9s) on complex tasks, significantly speeding up (down by 7.5 seconds on average) with intermediate visual simulations. In contrast, models exhibit inconsistent performance gains from visual simulations, improving on most tasks but declining in specific cases like tangram puzzles (GPT-4o, o1) and cube net folding (Claude3.5, Gemini-2.0 Flash), indicating that models may not know how to effectively leverage intermediate visual information.1 1STARE is available at https://github.com/STARE-bench/STARE Preprint."
        },
        {
            "title": "Introduction",
            "content": "Spatial reasoning is not merely subset of human cognitive abilities but rather the fundamental underpinnings of intellectual processes [1]. Reasoning with space enables individuals to solve complex tasks through visually simulating transformations of objects in the mind, anticipating how their actions would physically manipulate other artifacts. Cognitive psychologists have found ample evidence that humans simulate 2D and 3D transformations to reason about spatial problems [25]. Shepard and Metzler [6] found that the time taken by subject to recognize two perspective drawings as the same 3D shape increases linearly with their angular difference in orientation, suggesting an analog mental rotation process. Hegarty [7] found that humans employ mental animation, incrementally simulating the movement of parts to understand mechanical diagrams. Such abilities enable everyday tasks like assembling furniture, reading maps or instructional diagrams, navigating new environments, and are strongly correlated with success in STEM disciplines [810]. Despite their prevalence in real-world applicationsfrom arranging furniture in house to molecular docking for drug discoverydynamic visual simulations are still under-represented when evaluating multimodal large language models (MLLMs). Existing datasets largely target static recognition or problems that can be re-phrased as linguistic reasoning [1116]. In contrast, humans frequently solve spatial challengessuch as folding 2D net into 3D object, assembling tangram, or taking another visual perspectiveby running internal, step-wise visual simulations (Figure 1), which have long pedigree in the cognitive science studying human spatial reasoning [1721]. To bridge this gap, we introduce STARE (Spatial Transformations and Reasoning Evaluation), benchmark focused on spatial reasoning tasks that can be better solved through multi-step visual simulations. STARE evaluates whether MLLMs can perform complex visual reasoning akin to the visual simulations humans perform. It spans spectrum of spatial cognition challenges (Figure 2), structured in increasing complexity: Foundational geometric transformations: Tasks involving basic planar (2D) and volumetric (3D) transformations, such as rotations, translations, and reflections. Integrated spatial reasoning: Cube net folding, requiring understanding how 2D patterns fold into 3D objects, and tangram puzzles, assessing sequential assembly and spatial positioning. Real-world spatial reasoning: Tasks demanding reasoning about perspective changes and temporal frame sequences, simulating realistic spatial cognition scenarios encountered in daily life. In the first two categories, each transformation or operation (e.g., folding face) can be explicitly visualized step by step, and indeed humans often draw or imagine intermediate states when solving them. The last category demands higher-level visual simulation skills without always having clear intermediate visual cues (e.g., perspective reasoning) [22, 23]. We carefully curate 4K total instances across these categories, controlling difficulty via distractor similarity and number of simulation steps, to push models beyond superficial pattern-matching. Our experiments show that models find reasoning over simple 2D transformations relatively easy but struggle with 3D cube net folding and tangram puzzles, performing near random chance due to the need for multi-step simulations. Humans, though nearly perfect in accuracy, took significantly longerup to 28.9 secondsto solve some tasks but sped up considerably (down by 7.5 seconds on average) when given intermediate steps. Meanwhile, when models receive intermediate visual steps, their performance varies: e.g., GPT-4o, Gemini-2.0 Flash Thinking and o1 improve while Gemini-2.0 Flash and Claude worsen on cube net folding, suggesting that not all models effectively utilize visual guidance. In general, models lag behind human performance significantly. To better understand these gaps, we conduct detailed error analyses, pinpointing specific reasons for model failures, such as difficulties in accurately interpreting 3D spatial relationships, inadequate of imagining in space, and struggles with extended visual contexts even when providing explicit visual simulations. Fundamentally, models cannot effectively perform visual simulation. Overall, STARE aims to comprehensively test MLLMs ability to perform sequential visual simulations as opposed to pure textual reasoning. By evaluating models on tasks grounded in cognitive phenomena like mental imagery, we aim to reveal whether current MLLMs can approach the flexible spatial problem-solving of humans. 2 Figure 2: Overview of STARE. STARE consists of 3 levels of tasks, 2D Transformation and 3D Transformation for foundational spatial reasoning skills, tangram puzzle and cube net folding for integrated spatial reasoning, temporal frame inference and perspective reasoning to mimic real-world scenarios. The intermediate steps for completing tasks in the first two levels can be explicitly simulated, while the more real-word spatial reasoning tasks requires more abstract and implict mental simulations."
        },
        {
            "title": "2 The STARE benchmark",
            "content": "STARE is designed to evaluate multimodal models abilities in spatial cognition and visual reasoning, focusing specifically on tasks that humans solve non-linguistically, through visual simulation. Current perception-focused multimodal benchmarks still rely heavily on linguistic reasoning [24 26] or static visual recognition [2729], failing to measure models abilities in sequential visual problem-solving. Parallel work in spatial cognition [30, 12, 31, 16, 32] probes analogy making and pattern induction, yet simulation is optional and intermediate visual states are seldom provided because of annotation cost. VSI-Bench [33] underscores the role of mental imagery in spatial reasoning, but focuses on spatial memory and estimation from video rather than explicit step-by-step simulation. STARE closes the gap by testing multimodal models across diverse spatial tasks that require step-by-step visual simulations with or without explicit linguistic guidance. We describe the overall design of STARE (2.1), highlighting key differences compared to existing benchmarks. We then provide detailed descriptions of each task, discussing how the data was curated (2.2). 2.1 Overview of STARE STARE is structured to comprehensively cover spatial reasoning at multiple complexity levels, from basic geometric transformations (2D and 3D) to more integrated tasks (cube net folding and tangram puzzles) and real-world spatial reasoning scenarios (temporal frame and perspective reasoning). Each task is presented as multiple-choice or yes/no question using carefully designed visual and In total, the dataset contains 4K instances across different evaluation setups textual prompts. (Figure 3). Detailed statistics of STARE are provided in Appendix Figure 7. STARE separates tasks that can be visually simulated, i.e., where each transformation step is visually observable, from tasks demanding more abstract and implicit mental simulations, such as perspective reasoning. To support more fine-grained evaluation, we synthesize the tasks that humans can mentally picture or even explicitly draw the intermediate steps, including 2D transformations, 3D transformations, cube net folding and tangram puzzle. Additionally, STARE tasks are intentionally crafted to closely reflect real-world scenarios such as assembling objects (e.g., tangram puzzles), interpreting mechanical diagrams (e.g., cube net folding) and navigating environments (e.g., perspective reasoning). These scenarios can potentially shed lights on models abilities in practical, everyday spatial cognition, providing meaningful assessments aligned with common human chal3 Figure 3: The different variants in the Tangram Puzzle task. We provide visualizations of the complete interleaved inputs for all three types in Appendix F.2. lenges. detailed discussion about related works in human visual reasoning and MLLM benchmarks are provided in Appendix D. 2.2 Data curation 2D transformations: We design two types of tasks assessing spatial reasoning through twodimensional shape transformations: visual analogy, and instruction-based tasks. In visual analogy tasks, shape is shown to transform visually into shape A, after which shape is provided with candidate shapes for applying the same transformation sequence to B. Instruction-based tasks explicitly describe transformations (e.g., Rotate 90 degrees clockwise, then make it bigger) and require selecting the correctly transformed shape from 4 answer choices. Transformations include rotations, translations, uniform scaling, reflection and shearing, with clearly defined parameters. Each task is created with three difficulty levels: easy (with two distractors out of three clearly different in appearance), medium (one obvious distractor), and hard (all distractors visually similar, forcing the model to pay attention to the transformation itself). In addition, we synthesize samples with 1/2/3 transformation steps to facilitate evaluations in multi-turn visual transformations. We programmatically generate all shapes and their transformed version using Matplotlib [34]. Visualization of different variants of 2D transformation samples is shown in Figure 8 of the Appendix. We develop two experimental setups: (1) question + transformation steps, where the transformation steps are shown either verbally (for instruction-based tasks) or visually (for visual analogy tasks); and (2) question + transformation steps + intermediate visual simulations, showing all intermediate visualizations of shape B, excluding the final step. We synthesize total of 1000 instances, 600 of which are without intermediate visual simulations. 3D transformations: We extend the 2D transformation tasks to three dimensions, creating similar tasks using 3D shapes. Reflection is omitted in 3D because the mirror plane isnt obviously recognizable to human evaluators. The transformations include rotations around arbitrary axes, translations in 3D space, scaling, and shearing. Tasks, difficulty levels, and experimental setups mirror those of the 2D tasks, with total of 1000 instances. Following [35], we create abstract 3D shapes as detailed meshes and use Blender [36] to render realistic and consistent visuals. Tangram puzzles: Tangram puzzles test spatial reasoning about how individual pieces fit together to form complete shape. Each puzzle provides target grid and pieces, and the task is to determine whether the pieces can exactly fill the grid. Valid puzzles were generated by randomly dividing small grids (3x3 or 4x4) into rectangular or square shapes, then randomly rotated. Irregular variants were also created by merging adjacent rectangles. Invalid puzzles were constructed by adding or removing pieces, altering piece sizes, or giving incorrect placement instructions. We create three setups for evaluation: (1) question-only, which presents the initial puzzle configuration with query about solvability; (2) question + assembly steps, adding descriptive instructions of each assembly step without visual aids; and (3) question + assembly steps + intermediate visual simulations, providing both descriptive annotations and intermediate visualizations of the assembly 4 process, excluding the final visualization indicating success or failure. This task comprises 800 puzzles, evenly divided into solvable and unsolvable instances. Cube net folding: This task evaluates the models capacity to mentally fold flat 2D patterns into 3D cubes. We provide examples comprising both valid nets (correctly folding into cube) and invalid nets (leading to overlapping or disconnected faces). Each cube net has explicitly labeled faces. To generate these examples, we implement step-by-step algorithm that simulates the folding process by designating stationary base face and sequentially folding the connected faces. During each folding step, we detect and annotate errors, such as overlaps or disconnected faces, and generate corresponding visualizations using Matplotlib, clearly delineating face boundaries. Similar to tangram puzzles, we evaluate models in three setups, including (1) question-only, (2) question + folding steps, and (3) question + folding steps + intermediate visual simulations. The final cube net folding task contains 320 samples, balanced between valid and invalid configurations. Temporal frame reasoning: This task evaluates models ability to infer missing sequential visual information. Each example consists of four consecutive frames from video, with one frame hidden. The model must identify the missing frame from set of three options, relying on temporal consistency and logical scene progression. We construct 471 examples from the Objectron [37] dataset, which contains short, object-centric videos with camera pose annotations. To create meaningful sequences, we extract the longest continuous segment where the camera moves only in one direction (left or right), divide it into four equal intervals, and select frame from the central portion of each interval. One of these frames is hidden, and the model must identify it from three choices: the correct missing frame and two distractor frames sampled from different, non-overlapping parts of the video. Perspective reasoning: This task assesses models ability to understand how scenes appear from different viewpoints. Each example consists of top-down map that indicates an agents position and orientation, represented by an arrow showing the agents viewing direction. The model must then select the correct first-person view from four choices, emphasizing spatial perspective reasoning and spatial relationships in various indoor environments. We construct 250 samples using the HM3D dataset [38], large collection of 3D indoor spaces derived from real-world environments. To generate each example, we use the Habitat simulator [39 41] to place an agent at random position on the floor while ensuring the surrounding scene contains enough visual cues, such as objects and structures, rather than just walls. top-down view of the agents position is then captured, and random viewing direction is assigned (forward, right, left, or backward). The four answer choices correspond to these fixed 90-degree viewpoints, ensuring clear distinctions between them. To improve dataset quality, we conduct human filtering to remove ambiguous cases and low-resolution images."
        },
        {
            "title": "3 Experiments",
            "content": "In this section, we describe our experimental setup in detail, present comprehensive results, and provide an in-depth analysis of common model errors and limitations. 3.1 Experimental Setup For synthetic tasks involving explicit simulations (2D transformations, 3D transformations, cube net folding, tangram puzzles), we explore two evaluation settings: Without Visual Simulations: Models receive only an initial image with or without step-by-step textual instructions and had to mentally infer the subsequent transformations without visual guidance, thereby testing their internal mental simulation capabilities. With Visual Simulations: Models were provided with step-by-step visualizations clearly illustrating each transformation step before the final result, enabling explicit visual reasoning. Instead of collating the complex step-by-step visualizations into single image, we provide the model with interleaved image and text query for evaluation. For real-world reasoning tasks, including temporal frame and perspective reasoning, we evaluate models under the standard single image setting without providing explicit intermediate visual steps. 5 Model Random 2D Trans. 3D Trans. Cube Net Tangram VSim VSim VSim VSim VSim VSim VSim VSim 25.0 25.0 25.0 25.0 50. 50.5 50.5 49.1 TempPersoral pective 33.3 25.0 Closed-source Models 71.2 82.7 ( 11.5) 65.5 68.4 ( 2.9) 50.3 GPT-4o 51.5 57.8 ( 6.3) 52.3 65.9 Claude-3.5 Sonnet 56.1 59.3 ( 1.6) 37.7 Gemini-2.0 Flash 69.5 49.5 56.1 ( 6.6) 48.3 Gemini-2.0 Flash Think 60.6 67.9 71.6 ( 3.7) 51.3 81.8 o1 71.4 ( 5.5) 75.2 ( 5.7) 62.8 ( 2.2) 87.7 ( 5.9) 52.2 ( 1.9) 51.6 ( 0.7) 35.6 ( 2.1) 50.7 ( 2.4) 53.4 ( 2.1) 51.5 ( 1.0) 39.0 52.5 67.6 ( 8.6) 54.0 59.0 65.5 ( 0.5) 38.6 65.0 39.8 62.8 ( 23.0) 45.0 53.2 ( 2.1) 45.0 55.3 Open-source Models LLaVA-OneVision-72B 32.9 47.5 InternVL2.5-78B 16.6 Qwen2.5-VL-3B 35.4 Qwen2.5-VL-7B 45.2 Qwen2.5-VL-72B 32.2 ( 0.7) 50.1 ( 2.6) 20.0 ( 3.4) 32.4 ( 3.0) 48.5 ( 3.2) 39.8 ( 9.5) 35.7 27.0 30.6 ( 3.6) 28.5 30.3 60.7 48.2 ( 12.5) 31.4 38.1 36.5 ( 1.6) 37.1 42.7 ( 7.4) 33.3 29.1 31.4 ( 2.3) 43.5 50.1 52.9 ( 1.6) 36.5 28.8 31.7 ( 2.9) 40.7 54.5 56.9 ( 4.3) 31.4 43.0 49.1 ( 6.1) 35.2 53.4 ( 18.2) 61.2 34.2 ( 3.7) 37.3 ( 0.2) 41.0 ( 2.5) 44.9 ( 4.2) Human Performance 38.7 26.1 37.2 32.7 36.8 24.8 26.0 23.3 23.2 26. Overall 34.8 53.9 53.1 51.3 48.8 57.2 31.4 39.2 32.3 36.7 42.3 95.0 Accuracy Response Time (s) 17.1 (Best Model, Human) -13.2 97.0 ( 2.0) 11.1 ( 6.0) -9. 96.0 97.5 ( 1.5) 99.0 14.4 11.8 ( 2.6) 15.4 -46.7 -25.9 -28.1 99.0 ( - ) 6.0 ( 9.4) -45.6 94.0 ( 6.5) 98.1 87.5 28.9 17.1 ( 11.8) 10.8 -26.4 -22.5 98.4 23.9 -44.1 -59.7 96.5 15.9 -39.3 Table 1: Model Performance With or Without Visual Simulation (VSim) Across Tasks in STARE. Even the top performer, o1, achieves just under 60% accuracy. Humans, in contrast, get near perfect scores. Green (Red) arrows indicate performance improvements (degradations) with visual simulation. Evaluation Metrics. We report accuracy for multiple-choice questions in 2D/3D transformations, temporal frame, and perspective reasoning tasks. For cube net folding and tangram puzzles, which involve binary yes/no questions, we report the F1 score. We report macro-average performance across tasks as the overall evaluation metric. Models. We consider the following models: (1) Closed-source models: GPT-4o [42], Claude-3.5 Sonnet [43], Gemini2.0 Flash [44], and the reasoning-focused Gemini2.0 Flash Thinking [45] and o1 [46]. (2) Open-source models: InternVL2.5-78B [47], LLaVA-OneVision-72B [48], Qwen2.5VL-3B, Qwen2.5-VL-7B, and Qwen2.5-VL-72B [49]. Additionally, we invite two undergraduate students to complete the same tasks as the models. The averaged performance and response time are recorded to benchmark model capabilities against human-level spatial reasoning. 3.2 Main Results The results present in Table 1 show notable variations in model performance across different spatial reasoning tasks in the STARE benchmark. Models achieve the highest accuracy (up to 87.7%) on simpler 2D transformation tasks, significantly surpassing random chance (25%). Accuracy decreases by roughly 5% on average for more complex 3D transformations. Tasks involving intricate multi-step reasoning, such as cube net folding and tangram puzzles, resulted in even worse model performance, closer to random chance (50%). Additionally, temporal frame reasoning and perspective reasoning, which require interpreting sequential visual contexts and viewpoint changes, posed considerable difficulties, with most models performing similarly to random chance. The use of visual simulations (VisSim) enhances model performance in most cases, but not all. GPT-4o exhibits notable improvement of 11.5% accuracy on 2D transformations with visual simulations, and Claude-3.5 Sonnet shows significant gains (+8.6%) on tangram puzzles. However, visual simulations did not uniformly benefit model performance; certain models like Gemini-2.0 Flash experienced slight performance declines (e.g., 2.1% decrease on F1 for cube net tasks), indicating that models can not always effectively leverage intermediate visual information. The reasoning-focused o1 model outperforms all other models with visual simulations, except for the tangram puzzles. Overall, it improves over GPT-4o by 3.3% on average, but still lag behind human performance. Despite the large improvement observed for Gemini-2.0 Flash Thinking from adding visual simulation on tangram puzzles (+23.0%), it notably underperforms its non-reasoning counterpart (Gemini-2.0 Flash) across tasks like 2D/3D transformations, tangram puzzles, and perspective reasoning with or without visual simulations, suggesting that optimizing for linguistic reasoning does not always benefit spatial reasoning. 6 Open-source models generally exhibite lower accuracy compared to closed-source counterparts, highlighting significant performance gap. Larger models like InternVL2.5-78B and Qwen2.5-VL72B performe relatively better, suggesting benefits from scale, but their results with visual simulations also varied. For instance, InternVL2.5-78Bs performance decreases significantly in tangram tasks (-12.5%), whereas Qwen2.5-VL-72B improves notably (18.2%) in cube net tasks. Human performance consistently surpasses that of models, achieving high accuracy across all STARE tasks, and further improved by intermediate visual simulations. However, these tasks were cognitively demanding even for humans, reflected by relatively long response times (e.g., 28.9 seconds on tangram puzzles without visual simulations). Although intermediate visual simulations significantly reduces cognitive load and response time, humans still require more than 5 seconds to mentally manipulate and reason through these problems and complete the last step. Thus, STARE tasks clearly involve complex, multi-step spatial reasoning beyond simple recognition tasks solvable at glance [29]. These findings underscore humans superior spatial reasoning capabilities, particularly when aided by visual simulations. Moreover, to study whether gains on abstract, synthetic spatial tasks translate to real-world tasks, we computed model-level correlations between the two domains. Concretely, for each model, we average its performance across with or without visual simulation on the 4 synthetic tasks and contrast that with its mean accuracy on the two real-world tasks. This yields strong overall Pearson correlation (r 0.88, 5e4) across all 11 models. Counting in human performance, further increase the correlation to (r 0.97, 1e7). 3.3 Detailed Analysis To gain deeper insights into model limitations and identify specific reasoning challenges, we structure our detailed analysis around several targeted questions. We focus our discussion below on the GPT-4o model, given that it achieves the best performance among the non-thinking models. Analysis on all the other models can be found in Appendix G. Q1: How well do models understand individual transformation types in 2D and 3D? We evaluate model accuracy on individual transformation operationsrotation, translation, scaling, reflection, and shearingfor both 2D and 3D tasks, comparing performance with and without visual simulation (Figure 4). For 2D tasks, scaling achieves the highest accuracy (approximately 90% without visual simulation), improving further with visual simulation. Shearing was the most challenging in 2D (around 54%), showing minimal improvement from visual aids. Reflection, rotation, and translation significantly benefits from visual simulation, improving roughly 10 percentage points each. In 3D tasks, translation had the highest accuracy (about 76% without visual simulation), although it slightly declines with visual simulation. However, shearing, scaling, and rotation notably improve with visual simulation by about 38 percentage points. Overall, visual simulation substantially enhances performance for complex transformations, especially in 2D, though the added complexity of 3D transformations continues to present significant challenges. Q2: How does model accuracy change as task complexity increases? (1) Performance vs. Difficulty-level: The left sub-figure in Figure 6 shows model accuracy decreased as tasks became harder. For 2D tasks, GPT-4o performed best on easy tasks (86% with visual simulation), with accuracy declining notably for medium and hard tasks, especially without visual simulation (dropping to 66% for hard tasks). For 3D tasks, overall accuracy was lower, decreasing from easy tasks (72% without visual simulation) to hard tasks (60% without). Visual simulation generally improved accuracy but was less effective or even slightly detrimental for the hardest 3D tasks (60.5% without, 57.4% with). (2) Performance vs. Number of Turns: The right sub-figure in Figure 6 shows that how model performance varies with the number of transformation steps (N = 1, 2, 3). Without visual simulation, accuracy for both 2D and 3D tasks initially increases from = 1 to = 2, and then decreases at = 3. The observed peak at = 2 likely occurs because two-step transformations combine simpler transformations (e.g., scaling) with more challenging ones (e.g., shearing), allowing models to leverage the simpler transformations to determine the correct answer. In contrast, one-step transformations are evenly distributed across all transformation types, while at = 3, the increased complexity from multiple transformations compounds cognitive demands, reducing overall model accuracy. With visual simulation, accuracy remains consistently high across 2 and 3 steps in 2D tasks and shows stable or slightly improved performance at = 3 in 3D tasks. Performance at = 1 with visual simulation is not shown because there is no intermediate step. 7 Figure 4: GPT-4o performance on individual 2D/3D transformation types, with and without Visual Simulation (VisSim). Figure 5: perception error from Claude-3.5 Sonnet. Refer to Appendix F.5 for more case study. Figure 6: GPT-4o performance vs. task complexity (left: difficulty levels and right: number of transformation steps) with or without Visual Simulation (VSim). Q3: Do model failures originate from basic visual perception errors? To determine if model failures originate from fundamental visual perception rather than higher-level reasoning limitations, we design straightforward probing experiment. Specifically, we simplify the task by directly presenting the model with the final, fully simulated outcomes, reducing the problem to visually matching these outcomes to the correct candidate answers. Under these conditions, accuracy improves by 4.2% (from 82.7% to 86.9%) on 2D transformations and 2.8% (from 68.4% to 71.2%) on 3D transformations, indicating only modest improvement when eliminating intermediate steps. However, for more structured tasks like cube net folding and tangram puzzles, providing the fully completed final form drastically raises accuracy to 100% and 91.6%, respectively, highlighting that models can solve these tasks when the perceptual complexity is minimized. To further isolate the nature of perceptual errors in cube net folding, we create targeted tasks to test both 2D perception (color recognition and face connectivity) and 3D perception (identifying if face has been folded). Results from these tasks  (Table 2)  reveal perfect color recognition but notable decrease in accuracy for face connectivity (94.1%) and particularly low accuracy in correctly identifying folded faces (57.4%). Figure 5 illustrates an example of perception error on connectivity misalignments from Claude-3.5 Sonnet. Moreover, these specific perceptual errors in folding explain the limited benefits from visual simulations observed in Table 1 for GPT-4o. Overall, while some errors indeed stem from basic visual perception deficits, particularly in more complex 3D scenarios, the results suggest higher-level reasoning likely plays larger role in overall model failures. Q4: How well do models reason spatially in text? To evaluate how well models reason spatially from text alone, we translate each visual task into clear, concise descriptions. For 2D and 3D transformation tasks, each object is described by stating its shape, color, position, size and etc.for instance, red square at position (3,4) with size 2. In the cube-net folding task, the unfolded cube is represented by numbering each face and arranging these numbers in grid matching the cube nets visual layout. For example, 123456\" represents all six faces in single row. Lastly, for the tangram puzzle task, each piece is labeled (e.g., Piece A) and represented by compact grid indicating occupied cells marked by 1. For instance, square piece might be shown as two rows of 11. Examples of text representations of each task are provided in Appendix F.4. 8 Model 2D Perception 3D Perception Input 2D Trans. 3D Trans. Cube Nets Tangram Color Connectivity Folded? GPT-4o 100.0 94. 57.4 Text-only Image-only Image+Text 87.5 75.1 90.8 64.7 67.7 70.0 57.0 56.0 62.1 72.6 62.5 Table 2: 2D and 3D perception accuracy in cube-net folding. Table 3: GPT-4o performance without visual simulation under different input representations. Input Cube Nets Tangram Question-only Question+Steps 50.2 50.4 62.4 34.7 Simulation State 2D Trans. 3D Trans. Cube Nets Tangram Partial All Last 72.1 68.4 68. 51.3 52.2 35.2 43.5 51.5 43.4 86.8 82.7 89.4 Table 4: GPT-4o performance with question-only vs. explicit reasoning steps. Table 5: GPT-4o performance with different intermediate visualsimulation states. As shown in Table 3, providing the model with text representation removes much of the perception challenge, yet accuracy remains well below human performanceabout 57% on cube-net folding, 65% on 3D transformations, and roughly 73% on tangram puzzles, suggesting that the model still lacks the ability to mentally simulate the steps to solve each task. Text helps most on 2D spatial reasoning: accuracy on 2D transformations rises from 75% with images alone to 87% with text, and tangram performance climb from 63% to 73%. For tasks involving 3D spatial reasoning, however, text gives little benefit, partly because the simple text description about shape, color, material, center, and size, cannot capture all the depth and adjacency cues in 3D spatial reasoning. Q5: How well do models verbally simulate without visual simulation? We evaluate how effectively models verbally simulate spatial reasoning without intermediate visual simulations by comparing performance when provided only the question (Question-only) versus explicit verbal reasoning steps (Question+Steps). Table 4 shows minimal improvement in cube net folding (50.2% to 50.4%), indicating limited benefit from verbal reasoning alone. Conversely, tangram performance notably decreases (62.4% to 34.7%), suggesting models adopt shortcuts like summing piece areas rather than genuine spatial simulation. This result partially reflects bias in our question-only set: models can achieve 75% accuracy by checking the total areas of available pieces. Q6: How well do models integrate textual context with isolated visual simulations? We compared accuracy when presenting models with complete visual sequences versus only the final or most relevant visual state  (Table 5)  . Easier tasks like 2D and 3D transformations showed improved or comparable accuracy when presented only the final state (e.g., 82.7% for complete vs. 89.4% for last), suggesting that for these tasks, the final visual state closely resembles the initial state, reducing cognitive load. However, in complex tasks such as cube net folding (52.2% complete vs. 35.2% last) and tangram puzzles (51.5% complete vs. 43.4% last), the final state becomes more disconnected from the initial configuration, requiring deeper understanding of preceding verbal steps. This disconnection introduces significant challenges for models, aligning with earlier findings (Q4) and underscoring their difficulties in integrating complex visual sequences during multi-step reasoning."
        },
        {
            "title": "4 Conclusion",
            "content": "In this paper, we introduced STARE, novel benchmark specifically designed to evaluate multimodal models on diverse spatial cognition tasks involving complex visual reasoning and mental simulations. STARE uniquely assesses model capabilities across foundational geometric transformations, integrated spatial reasoning tasks, and real-world scenarios requiring temporal and perspective reasoning. Our extensive experiments reveal significant performance variations among multimodal models, highlighting substantial challenges, especially in complex, multi-step reasoning scenarios. Visual simulations notably enhance performance on simpler tasks but yield mixed results for more sophisticated tasks. The substantial gap in performance between closed-source and open-source models further emphasizes the necessity for advancements in multimodal reasoning. Overall, STARE sets critical benchmark to guide future research towards human-level spatial reasoning capabilities in AI."
        },
        {
            "title": "References",
            "content": "[1] Barbara Tversky and Masaki Suwa. Thinking with sketches. 2009. 1 [2] Alex Mitko and Jason Fischer. When it all falls down: The relationship between intuitive physics and spatial cognition. Cognitive research: principles and implications, 5:113, 2020. 1 [3] Jiafei Duan, Samson Yu, Soujanya Poria, Bihan Wen, and Cheston Tan. Pip: Physical interaction prediction via mental simulation with span selection. In European Conference on Computer Vision, pages 405421. Springer, 2022. [4] Jonathan Wai, David Lubinski, and Camilla Benbow. Spatial ability for stem domains: Aligning over 50 years of cumulative psychological knowledge solidifies its importance. Journal of Educational Psychology, 101(4):817, 2009. [5] Peter Battaglia, Jessica Hamrick, and Joshua Tenenbaum. Simulation as an engine of physical scene understanding. Proceedings of the National Academy of Sciences, 110(45):1832718332. 1 [6] Roger Shepard and Jacqueline Metzler. Mental rotation of three-dimensional objects. Science, 171 (3972):701703, 1971. 1, [7] Mary Hegarty. Mental animation: Inferring motion from static displays of mechanical systems. Journal of Experimental Psychology: Learning, Memory, and Cognition, 18(5):10841102, 1992. 1 [8] Nicholas Judd and Torkel Klingberg. Training spatial cognition enhances mathematical learning in randomized study of 17,000 children. Nature Human Behaviour, 5(11):15481554, 2021. 1 [9] Bo Christensen and Christian Schunn. The role and impact of mental simulation in design. Applied Cognitive Psychology: The Official Journal of the Society for Applied Research in Memory and Cognition, 23(3):327344, 2009. [10] Mary Hegarty. Mechanical reasoning by mental simulation. Trends in Cognitive Sciences, 8(6):280 ISSN 1364-6613. doi: https://doi.org/10.1016/j.tics.2004.04.001. URL https://www. 285, 2004. sciencedirect.com/science/article/pii/S1364661304001007. 1 [11] et al. Johnson, Justin. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 29012910, 2017. 1, [12] C. et al. Zhang. Raven: dataset for relational and analogical visual reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 53175327, 2019. 2, [13] et al. Ji, W. Abstract visual reasoning with tangram shapes. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 23502360, 2022. [14] Jiafei Duan, Samson Yu, and Cheston Tan. Space: simulator for physical interactions and causal learning in 3d environments. In Proceedings of the ieee/cvf international conference on computer vision, pages 20582063, 2021. [15] François Chollet. On the measure of intelligence. arXiv preprint arXiv:1911.01547, 2019. [16] S. K. Ramakrishnan, E. Wijmans, P. Krahenbuhl, and V. Koltun. Does spatial cognition emerge in frontier models? arXiv preprint arXiv:2410.06468, 2024. 1, 2, [17] Janellen Huttenlocher and Clark Presson. Mental rotation and the perspective problem. Cognitive Psychology, 4(2):277299, 1973. ISSN 0010-0285. doi: https://doi.org/10.1016/0010-0285(73)90015-7. URL https://www.sciencedirect.com/science/article/pii/0010028573900157. 1 [18] Peri Gunalp, Tara Moossaian, and Mary Hegarty. Spatial perspective taking: Effects of social, directional, and interactive cues. Memory & cognition, 47:10311043, 2019. [19] Roger N. Shepard and Christine Feng. chronometric study of mental paper folding. Cognitive Psychology, 3(2):228243, 1972. ISSN 0010-0285. doi: https://doi.org/10.1016/0010-0285(72)90005-9. URL https://www.sciencedirect.com/science/article/pii/0010028572900059. [20] Kai Preuss, Christopher Hilton, Klaus Gramann, and Nele Russwinkel. Identifying cognitive processes and neural substrates of spatial transformation in mental folding task with cognitive modeling. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 46, 2024. 10 [21] Hasan Ayaz, Patricia Shewokis, Meltem Izzetoglu, Murat Çakır, and Banu Onaral. Tangram solved? In 2012 Annual International prefrontal cortex activation analysis during geometric problem solving. Conference of the IEEE Engineering in Medicine and Biology Society, pages 47244727. IEEE, 2012. 1 [22] Ilona Bass, Kevin A. Smith, Elizabeth Bonawitz, and Tomer D. Ullman. Partial mental simulation explains fallacies in physical reasoning. Cognitive Neuropsychology, 2022. 1, [23] Tony Chen, Kelsey R. Allen, Samuel J. Cheyette, Joshua B. Tenenbaum, and Kevin A. Smith. Just in timerepresentations for mental simulation in intuitive physics. In Proceedings of the 45th Annual Meeting of the Cognitive Science Society (CogSci), 2023. 1, [24] Deqing Fu, Ghazal Khalighinejad, Ollie Liu, Bhuwan Dhingra, Dani Yogatama, Robin Jia, and Willie IsoBench: Benchmarking multimodal foundation models on isomorphic representations, Neiswanger. 2024. 2, [25] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Intergps: Interpretable geometry problem solving with formal language and symbolic reasoning. In The 59th Annual Meeting of the Association for Computational Linguistics (ACL), 2021. [26] Baiqi Li, Zhiqiu Lin, Wenxuan Peng, Jean de Dieu Nyandwi, Daniel Jiang, Zixian Ma, Simran Khanuja, Ranjay Krishna, Graham Neubig, and Deva Ramanan. Naturalbench: Evaluating vision-language models on natural adversarial samples. European Conference on Computer Vision, 2024. 2 [27] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. arXiv preprint arXiv:2401.06209, 2024. 2 [28] Penghao Wu and Saining Xie. V*: Guided visual search as core mechanism in multimodal llms. ArXiv, abs/2312.14135, 2023. URL https://api.semanticscholar.org/CorpusID:266436019. [29] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390, 2024. 2, 3.2, [30] Eunice Yiu, Maan Qraitem, Charlie Wong, Anisa Noor Majhi, Yutong Bai, Shiry Ginosar, Alison Gopnik, and Kate Saenko. Kiva: Kid-inspired visual analogies for testing large multimodal models. arXiv preprint arXiv:2407.17773, 2024. 2, [31] Sheng Hu, Yuqing Ma, Xianglong Liu, Yanlu Wei, and Shihao Bai. Stratified rule-aware network for abstract visual reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 15671574, 2021. 2, [32] Sina Rismanchian, Yasaman Razeghi, Sameer Singh, and Shayan Doroudi. Turtlebench: visual programming benchmark in turtle geometry. arXiv preprint arXiv:2411.00264, 2024. 2, [33] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces. arXiv preprint arXiv:2412.14171, 2024. 2, [34] Matplotlib. Matplotlib: Visualization with python. https://matplotlib.org/, 2012. 2. [35] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross Girshick. CLEVR: diagnostic dataset for compositional language and elementary visual reasoning. In CVPR, 2017. 2.2 [36] Blender. Blender is free software. https://www.blender.org/. 2.2 [37] Adel Ahmadyan, Liangkai Zhang, Artsiom Ablavatski, Jianing Wei, and Matthias Grundmann. Objectron: large scale dataset of object-centric videos in the wild with pose annotations. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2021. 2.2 [38] Santhosh Kumar Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alexander Clegg, John Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel Chang, Manolis Savva, Yili Zhao, and Dhruv Batra. Habitat-matterport 3d dataset (HM3d): 1000 large-scale 3d environments for embodied AI. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2021. URL https://arxiv.org/abs/2109.08238. 2.2 [39] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra. Habitat: Platform for Embodied AI Research. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019. 2.2 [40] Andrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimir Vondrus, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva, and Dhruv Batra. Habitat 2.0: Training home assistants to rearrange their habitat. In Advances in Neural Information Processing Systems (NeurIPS), 2021. [41] Xavi Puig, Eric Undersander, Andrew Szot, Mikael Dallaire Cote, Ruslan Partsey, Jimmy Yang, Ruta Desai, Alexander William Clegg, Michal Hlavac, Tiffany Min, Theo Gervet, Vladimír Vondruš, VincentPierre Berges, John Turner, Oleksandr Maksymets, Zsolt Kira, Mrinal Kalakrishnan, Jitendra Malik, Devendra Singh Chaplot, Unnat Jain, Dhruv Batra, Akshara Rai, and Roozbeh Mottaghi. Habitat 3.0: co-habitat for humans, avatars and robots, 2023. 2.2 [42] OpenAI. Hello gpt-4o. https://openai.com/index/hello-gpt-4o/. 3.1 [43] Anthropic. Claude 3.5 sonnet. https://www.anthropic.com/news/claude-3-5-sonnet. 3.1 [44] Google Deepmind. Introducing gemini 2.0: our new ai model for the agentic era. https://blog. google/technology/google-deepmind/google-gemini-ai-update-december-2024//, . 3.1 [45] Google Deepmind. Gemini 2.0 flash thinking mode. https://ai.google.dev/gemini-api/docs/ thinking-mode, . 3.1 [46] OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian Osband, Ignasi Clavera Gilaberte, Ilge Akkaya, Ilya Kostrikov, Ilya Sutskever, Irina Kofman, Jakub Pachocki, James Lennon, Jason Wei, Jean Harb, Jerry Twore, Jiacheng Feng, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joaquin Quiñonero Candela, Joe Palermo, Joel Parish, Johannes Heidecke, John Hallman, John Rizzo, Jonathan Gordon, Jonathan Uesato, Jonathan Ward, Joost Huizinga, Julie Wang, Kai Chen, Kai Xiao, Karan Singhal, Karina Nguyen, Karl Cobbe, Katy Shi, Kayla Wood, Kendra Rimbach, Keren Gu-Lemberg, Kevin Liu, Kevin Lu, Kevin Stone, Kevin Yu, Lama Ahmad, Lauren Yang, Leo Liu, Leon Maksin, Leyton Ho, Liam Fedus, Lilian Weng, Linden Li, Lindsay McCallum, Lindsey Held, Lorenz Kuhn, Lukas Kondraciuk, Lukasz Kaiser, Luke Metz, Madelaine Boyd, Maja Trebacz, Manas Joglekar, Mark Chen, Marko Tintor, Mason Meyer, Matt Jones, Matt Kaufer, Max Schwarzer, Meghan Shah, Mehmet Yatbaz, Melody Y. Guan, Mengyuan Xu, Mengyuan Yan, Mia Glaese, Mianna Chen, Michael Lampe, Michael Malek, Michele Wang, Michelle Fradin, Mike McClay, Mikhail Pavlov, Miles Wang, Mingxuan Wang, Mira Murati, Mo Bavarian, Mostafa Rohaninejad, Nat McAleese, Neil Chowdhury, Neil Chowdhury, Nick Ryder, Nikolas Tezak, Noam Brown, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivia Watkins, Patrick Chao, Paul Ashbourne, Pavel Izmailov, Peter Zhokhov, Rachel Dias, Rahul Arora, Randall Lin, Rapha Gontijo Lopes, Raz Gaon, Reah Miyara, Reimar Leike, Renny Hwang, Rhythm Garg, Robin Brown, Roshan James, Rui Shu, Ryan Cheu, Ryan Greene, Saachi Jain, Sam Altman, Sam Toizer, Sam Toyer, Samuel Miserendino, Sandhini Agarwal, Santiago Hernandez, Sasha Baker, Scott McKinney, Scottie Yan, Shengjia Zhao, Shengli Hu, Shibani Santurkar, Shraman Ray Chaudhuri, Shuyuan Zhang, Siyuan Fu, Spencer Papay, Steph Lin, Suchir Balaji, Suvansh Sanjeev, Szymon Sidor, Tal Broda, Aidan Clark, Tao Wang, Taylor Gordon, Ted Sanders, Tejal Patwardhan, Thibault Sottiaux, Thomas Degry, Thomas Dimson, Tianhao Zheng, Timur Garipov, Tom Stasi, Trapit Bansal, Trevor Creech, Troy Peterson, Tyna Eloundou, Valerie Qi, Vineet Kosaraju, Vinnie Monaco, Vitchyr Pong, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wes McCabe, Wojciech Zaremba, Yann Dubois, Yinghai Lu, Yining Chen, Young Cha, Yu Bai, Yuchen He, Yuchen Zhang, Yunyun Wang, Zheng Shao, and Zhuohan Li. Openai o1 system card, 2024. URL https://arxiv.org/abs/2412.16720. 3.1 [47] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. 3.1 [48] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 3.1 [49] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. URL https://arxiv.org/ abs/2502.13923. 3.1 [50] Dedre Gentner. Structure-mapping: theoretical framework for analogy. Cognitive Science, 7(2):155 170, 1983. [51] P.A. Carpenter, M.A. Just, and P. Shell. What one intelligence test measures: theoretical account of the processing in the raven progressive matrices test. Psychological Review, 97(3):404431, 1990. [52] A. Lovett and K. Forbus. Modeling visual problem solving as analogical reasoning. Psychological Review, 124(1):6090, 2017. [53] Nicholas Ichien, Qing Liu, Shuhao Fu, Keith J. Holyoak, Alan Yuille, and Hongjing Lu. Visual analogy: Deep learning versus compositional models. In Proceedings of the 43rd Annual Meeting of the Cognitive Science Society (CogSci), 2021. [54] Taylor W. Webb, Shuhao Fu, Trevor Bihl, Keith J. Holyoak, and Hongjing Lu. Zero-shot visual reasoning through probabilistic analogical mapping. arXiv preprint arXiv:2209.15087, 2022. [55] N. Ichien, Q. Liu, S. Fu, K.J. Holyoak, A. Yuille, and H. Lu. Two computational approaches to visual analogy: Task-specific models versus domain-general mapping. Cognitive Science, 47(4):e13347, 2023. [56] M. Hegarty. Mechanical reasoning by mental simulation. Trends in Cognitive Sciences, 8(6):280285, 2004. [57] L.W. Barsalou. Grounded cognition. Annual Review of Psychology, 59:617645, 2008. [58] P.W. Battaglia, J.B. Hamrick, and J.B. Tenenbaum. Simulation as an engine of physical scene understanding. Proceedings of the National Academy of Sciences, 110(45):1832718332, 2013. [59] J.B. Tenenbaum, T.L. Griffiths, and C. Kemp. Theory-based bayesian models of inductive learning and reasoning. Trends in Cognitive Sciences, 10(7):309318, 2006. [60] T.D. Ullman, E.S. Spelke, P. Battaglia, and J.B. Tenenbaum. Mind games: Game engines as an architecture for intuitive physics. Trends in Cognitive Sciences, 21(9):649665, 2017. [61] Lingxiao Yang, Hongzhi You, Zonglei Zhen, Dahui Wang, Xiaohong Wan, Xiaohua Xie, and Ru-Yuan Zhang. Neural prediction errors enable analogical visual reasoning in human standard intelligence tests. In Proceedings of the 40th International Conference on Machine Learning (ICML), 2023. [62] Luis S. Piloto, Ari Weinstein, Peter Battaglia, and Matthew Botvinick. Intuitive physics learning in deep-learning model inspired by developmental psychology. Nature Human Behaviour, 6(9):12571267, 2022. [63] Daniel M. Bear, Elias Wang, Damian Mrowca, Felix J. Binder, Hsiao-Yu F. Tung, R. T. Pramod, Cameron Holdaway, Sirui Tao, Kevin A. Smith, Fan-Yun Sun, Li Fei-Fei, Nancy Kanwisher, Joshua B. Tenenbaum, Daniel L. K. Yamins, and Judith E. Fan. Physion: Evaluating physical prediction from vision in humans and machines. arXiv preprint arXiv:2106.08261, 2022. [64] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. VQA: Visual question answering. In ICCV, 2015. [65] Xiyao Yue, Yifan Ni, Kai Zhang, Tao Zheng, Ruixuan Liu, Wen Chen, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023. [66] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, Ryan Kim, Richard Ren, Jason Hausenloy, Oliver Zhang, Mantas Mazeika, Tung Nguyen, Daron Anderson, Imad Ali Shah, Mikhail Doroshenko, Alun Cennyth Stokes, Mobeen Mahmood, Jaeho Lee, Oleksandr Pokutnyi, Oleg Iskra, Jessica P. Wang, Robert Gerbicz, John-Clark Levin, Serguei Popov, Fiona 13 Feng, Steven Y. Feng, Haoran Zhao, Michael Yu, Varun Gangal, Chelsea Zou, Zihan Wang, Mstyslav Kazakov, Geoff Galgon, Johannes Schmitt, Alvaro Sanchez, Yongki Lee, Will Yeadon, Scott Sauers, Marc Roth, Chidozie Agu, Søren Riis, Fabian Giska, Saiteja Utpala, Antrell Cheatom, Zachary Giboney, Gashaw M. Goshu, Sarah-Jane Crowson, Mohinder Maheshbhai Naiya, Noah Burns, Lennart Finke, Zerui Cheng, Hyunwoo Park, Francesco Fournier-Facio, Jennifer Zampese, John Wydallis, John B. Wydallis, Ryan G. Hoerr, Mark Nandor, Tim Gehrunger, Jiaqi Cai, Ben McCarty, Jungbae Nam, Edwin Taylor, Jun Jin, Gautier Abou Loume, Hangrui Cao, Alexis Garretson, Damien Sileo, Qiuyu Ren, Doru Cojoc, Pavel Arkhipov, Usman Qazi, Aras Bacho, Lianghui Li, Sumeet Motwani, Christian Schroeder de Witt, Alexei Kopylov, Johannes Veith, Eric Singer, Paolo Rissone, Jaehyeok Jin, Jack Wei Lun Shi, Chris G. Willcocks, Ameya Prabhu, Longke Tang, Kevin Zhou, Emily de Oliveira Santos, Andrey Pupasov Maksimov, Edward Vendrow, Kengo Zenitani, Joshua Robinson, Aleksandar Mikov, Julien Guillod, Yuqi Li, Ben Pageler, Joshua Vendrow, Vladyslav Kuchkin, Pierre Marion, Denis Efremov, Jayson Lynch, Kaiqu Liang, Andrew Gritsevskiy, Dakotah Martinez, Nick Crispino, Dimitri Zvonkine, Natanael Wildner Fraga, Saeed Soori, Ori Press, Henry Tang, Julian Salazar, Sean R. Green, Lina Brüssel, Moon Twayana, Aymeric Dieuleveut, T. Ryan Rogers, Wenjin Zhang, Ross Finocchio, Bikun Li, Jinzhou Yang, Arun Rao, Gabriel Loiseau, Mikhail Kalinin, Marco Lukas, Ciprian Manolescu, Nate Stambaugh, Subrata Mishra, Ariel Ghislain Kemogne Kamdoum, Tad Hogg, Alvin Jin, Carlo Bosio, Gongbo Sun, Brian Coppola, Haline Heidinger, Rafael Sayous, Stefan Ivanov, Joseph Cavanagh, Jiawei Shen, Joseph Marvin Imperial, Philippe Schwaller, Shaipranesh Senthilkuma, Andres Bran, Andres Algaba, Brecht Verbeken, Kelsey Van den Houte, Lynn Van Der Sypt, David Noever, Lisa Schut, Ilia Sucholutsky, Evgenii Zheltonozhskii, Qiaochu Yuan, Derek Lim, Richard Stanley, Shankar Sivarajan, Tong Yang, John Maar, Julian Wykowski, Martí Oller, Jennifer Sandlin, Anmol Sahu, Cesare Giulio Ardito, Yuzheng Hu, Felipe Meneguitti Dias, Tobias Kreiman, Kaivalya Rawal, Tobias Garcia Vilchis, Yuexuan Zu, Martin Lackner, James Koppel, Jeremy Nguyen, Daniil S. Antonenko, Steffi Chern, Bingchen Zhao, Pierrot Arsene, Sergey Ivanov, Rafał Poswiata, Chenguang Wang, Daofeng Li, Donato Crisostomi, Ali Dehghan, Andrea Achilleos, John Arnold Ambay, Benjamin Myklebust, Archan Sen, David Perrella, Nurdin Kaparov, Mark Inlow, Allen Zang, Kalyan Ramakrishnan, Daniil Orel, Vladislav Poritski, Shalev BenDavid, Zachary Berger, Parker Whitfill, Michael Foster, Daniel Munro, Linh Ho, Dan Bar Hava, Aleksey Kuchkin, Robert Lauff, David Holmes, Frank Sommerhage, Anji Zhang, Richard Moat, Keith Schneider, Daniel Pyda, Zakayo Kazibwe, Mukhwinder Singh, Don Clarke, Dae Hyun Kim, Sara Fish, Veit Elser, Victor Efren Guadarrama Vilchis, Immo Klose, Christoph Demian, Ujjwala Anantheswaran, Adam Zweiger, Guglielmo Albani, Jeffery Li, Nicolas Daans, Maksim Radionov, Václav Rozhoˇn, Vincent Ginis, Ziqiao Ma, Christian Stump, Jacob Platnick, Volodymyr Nevirkovets, Luke Basler, Marco Piccardo, Niv Cohen, Virendra Singh, Josef Tkadlec, Paul Rosu, Alan Goldfarb, Piotr Padlewski, Stanislaw Barzowski, Kyle Montgomery, Aline Menezes, Arkil Patel, Zixuan Wang, Jamie Tucker-Foltz, Jack Stade, Declan Grabb, Tom Goertzen, Fereshteh Kazemi, Jeremiah Milbauer, Abhishek Shukla, Hossam Elgnainy, Yan Carlos Leyva Labrador, Hao He, Ling Zhang, Alan Givré, Hew Wolff, Gözdenur Demir, Muhammad Fayez Aziz, Younesse Kaddar, Ivar Ängquist, Yanxu Chen, Elliott Thornley, Robin Zhang, Jiayi Pan, Antonio Terpin, Niklas Muennighoff, Hailey Schoelkopf, Eric Zheng, Avishy Carmi, Jainam Shah, Ethan D. L. Brown, Kelin Zhu, Max Bartolo, Richard Wheeler, Andrew Ho, Shaul Barkan, Jiaqi Wang, Martin Stehberger, Egor Kretov, Peter Bradshaw, JP Heimonen, Kaustubh Sridhar, Zaki Hossain, Ido Akov, Yury Makarychev, Joanna Tam, Hieu Hoang, David M. Cunningham, Vladimir Goryachev, Demosthenes Patramanis, Michael Krause, Andrew Redenti, David Aldous, Jesyin Lai, Shannon Coleman, Jiangnan Xu, Sangwon Lee, Ilias Magoulas, Sandy Zhao, Ning Tang, Michael K. Cohen, Micah Carroll, Orr Paradise, Jan Hendrik Kirchner, Stefan Steinerberger, Maksym Ovchynnikov, Jason O. Matos, Adithya Shenoy, Michael Wang, Yuzhou Nie, Paolo Giordano, Philipp Petersen, Anna Sztyber-Betley, Paolo Faraboschi, Robin Riblet, Jonathan Crozier, Shiv Halasyamani, Antonella Pinto, Shreyas Verma, Prashant Joshi, Eli Meril, Zheng-Xin Yong, Allison Tee, Jérémy Andréoletti, Orion Weller, Raghav Singhal, Gang Zhang, Alexander Ivanov, Seri Khoury, Nils Gustafsson, Hamid Mostaghimi, Kunvar Thaman, Qijia Chen, Tran Quoc Khánh, Jacob Loader, Stefano Cavalleri, Hannah Szlyk, Zachary Brown, Himanshu Narayan, Jonathan Roberts, William Alley, Kunyang Sun, Ryan Stendall, Max Lamparth, Anka Reuel, Ting Wang, Hanmeng Xu, Pablo Hernández-Cámara, Freddie Martin, Thomas Preu, Tomek Korbak, Marcus Abramovitch, Dominic Williamson, Ida Bosio, Ziye Chen, Biró Bálint, Eve J. Y. Lo, Maria Inês S. Nunes, Yibo Jiang, Saiful Bari, Peyman Kassani, Zihao Wang, Behzad Ansarinejad, Yewen Sun, Stephane Durand, Guillaume Douville, Daniel Tordera, George Balabanian, Earth Anderson, Lynna Kvistad, Alejandro José Moyano, Hsiaoyun Milliron, Ahmad Sakor, Murat Eron, Isaac C. McAlister, Andrew Favre D. O., Shailesh Shah, Xiaoxiang Zhou, Firuz Kamalov, Ronald Clark, Sherwin Abdoli, Tim Santens, Harrison Wang, Evan Chen, Alessandro Tomasiello, G. Bruno De Luca, Shi-Zhuo Looi, VinhKha Le, Noam Kolt, Niels Mündler, Avi Semler, Emma Rodman, Jacob Drori, Carl Fossum, Luk Gloor, Milind Jagota, Ronak Pradeep, Honglu Fan, Tej Shah, Jonathan Eicher, Michael Chen, Kushal Thaman, William Merrill, Moritz Firsching, Carter Harris, Stefan Ciobâca, Jason Gross, Rohan Pandey, Ilya Gusev, Adam Jones, Shashank Agnihotri, Pavel Zhelnov, Siranut Usawasutsakorn, Mohammadreza Mofayezi, Alexander Piperski, Marc Carauleanu, David K. Zhang, Kostiantyn Dobarskyi, Dylan Ler, Roman Leventov, Ignat Soroko, Thorben Jansen, Scott Creighton, Pascal Lauer, Joshua Duersch, Vage Taamazyan, Dario Bezzi, Wiktor Morak, Wenjie Ma, William Held, Tran Ðuc Huy, Ruicheng Xian, Armel Randy Ze14 baze, Mohanad Mohamed, Julian Noah Leser, Michelle Yuan, Laila Yacar, Johannes Lengler, Katarzyna Olszewska, Hossein Shahrtash, Edson Oliveira, Joseph W. Jackson, Daniel Espinosa Gonzalez, Andy Zou, Muthu Chidambaram, Timothy Manik, Hector Haffenden, Dashiell Stander, Ali Dasouqi, Alexander Shen, Emilien Duc, Bita Golshani, David Stap, Mikalai Uzhou, Alina Borisovna Zhidkovskaya, Lukas Lewark, Miguel Orbegozo Rodriguez, Mátyás Vincze, Dustin Wehr, Colin Tang, Shaun Phillips, Fortuna Samuele, Jiang Muzhen, Fredrik Ekström, Angela Hammon, Oam Patel, Faraz Farhidi, George Medley, Forough Mohammadzadeh, Madellene Peñaflor, Haile Kassahun, Alena Friedrich, Claire Sparrow, Rayner Hernandez Perez, Taom Sakal, Omkar Dhamane, Ali Khajegili Mirabadi, Eric Hallman, Kenchi Okutsu, Mike Battaglia, Mohammad Maghsoudimehrabani, Alon Amit, Dave Hulbert, Roberto Pereira, Simon Weber, Handoko, Anton Peristyy, Stephen Malina, Samuel Albanie, Will Cai, Mustafa Mehkary, Rami Aly, Frank Reidegeld, Anna-Katharina Dick, Cary Friday, Jasdeep Sidhu, Hassan Shapourian, Wanyoung Kim, Mariana Costa, Hubeyb Gurdogan, Brian Weber, Harsh Kumar, Tong Jiang, Arunim Agarwal, Chiara Ceconello, Warren S. Vaz, Chao Zhuang, Haon Park, Andrew R. Tawfeek, Daattavya Aggarwal, Michael Kirchhof, Linjie Dai, Evan Kim, Johan Ferret, Yuzhou Wang, Minghao Yan, Krzysztof Burdzy, Lixin Zhang, Antonio Franca, Diana T. Pham, Kang Yong Loh, Joshua Robinson, Abram Jackson, Shreen Gul, Gunjan Chhablani, Zhehang Du, Adrian Cosma, Jesus Colino, Colin White, Jacob Votava, Vladimir Vinnikov, Ethan Delaney, Petr Spelda, Vit Stritecky, Syed M. Shahid, Jean-Christophe Mourrat, Lavr Vetoshkin, Koen Sponselee, Renas Bacho, Florencia de la Rosa, Xiuyu Li, Guillaume Malod, Leon Lang, Julien Laurendeau, Dmitry Kazakov, Fatimah Adesanya, Julien Portier, Lawrence Hollom, Victor Souza, Yuchen Anna Zhou, Julien Degorre, Yigit Yalın, Gbenga Daniel Obikoya, Luca Arnaboldi, Rai, Filippo Bigi, M. C. Boscá, Oleg Shumar, Kaniuar Bacho, Pierre Clavier, Gabriel Recchia, Mara Popescu, Nikita Shulga, Ngefor Mildred Tanwie, Denis Peskoff, Thomas C. H. Lux, Ben Rank, Colin Ni, Matthew Brooks, Alesia Yakimchyk, Huanxu, Liu, Olle Häggström, Emil Verkama, Hans Gundlach, Leonor BritoSantana, Brian Amaro, Vivek Vajipey, Rynaa Grover, Yiyang Fan, Gabriel Poesia Reis Silva, Linwei Xin, Yosi Kratish, Jakub Łucki, Wen-Ding Li, Sivakanth Gopi, Andrea Caciolai, Justin Xu, Kevin Joseph Scaria, Freddie Vargus, Farzad Habibi, Long, Lian, Emanuele Rodolà, Jules Robins, Vincent Cheng, Tony Fruhauff, Brad Raynor, Hao Qi, Xi Jiang, Ben Segev, Jingxuan Fan, Sarah Martinson, Erik Y. Wang, Kaylie Hausknecht, Michael P. Brenner, Mao Mao, Xinyu Zhang, David Avagian, Eshawn Jessica Scipio, Alon Ragoler, Justin Tan, Blake Sims, Rebeka Plecnik, Aaron Kirtland, Omer Faruk Bodur, D. P. Shinde, Zahra Adoul, Mohamed Zekry, Ali Karakoc, Tania C. B. Santos, Samir Shamseldeen, Loukmane Karim, Anna Liakhovitskaia, Nate Resman, Nicholas Farina, Juan Carlos Gonzalez, Gabe Maayan, Sarah Hoback, Rodrigo De Oliveira Pena, Glen Sherman, Elizabeth Kelley, Hodjat Mariji, Rasoul Pouriamanesh, Wentao Wu, Sandra Mendoza, Ismail Alarab, Joshua Cole, Danyelle Ferreira, Bryan Johnson, Mohammad Safdari, Liangti Dai, Siriphan Arthornthurasuk, Alexey Pronin, Jing Fan, Angel RamirezTrinidad, Ashley Cartwright, Daphiny Pottmaier, Omid Taheri, David Outevsky, Stanley Stepanic, Samuel Perry, Luke Askew, Raúl Adrián Huerta Rodríguez, Ali M. R. Minissi, Sam Ali, Ricardo Lorena, Krishnamurthy Iyer, Arshad Anil Fasiludeen, Sk Md Salauddin, Murat Islam, Juan Gonzalez, Josh Ducey, Maja Somrak, Vasilios Mavroudis, Eric Vergo, Juehang Qin, Benjámin Borbás, Eric Chu, Jack Lindsey, Anil Radhakrishnan, Antoine Jallon, I. M. J. McInnis, Pawan Kumar, Laxman Prasad Goswami, Daniel Bugas, Nasser Heydari, Ferenc Jeanplong, Archimedes Apronti, Abdallah Galal, Ng Ze-An, Ankit Singh, Joan of Arc Xavier, Kanu Priya Agarwal, Mohammed Berkani, Benedito Alves de Oliveira Junior, Dmitry Malishev, Nicolas Remy, Taylor D. Hartman, Tim Tarver, Stephen Mensah, Javier Gimenez, Roselynn Grace Montecillo, Russell Campbell, Asankhaya Sharma, Khalida Meer, Xavier Alapont, Deepakkumar Patil, Rajat Maheshwari, Abdelkader Dendane, Priti Shukla, Sergei Bogdanov, Sören Möller, Muhammad Rehan Siddiqi, Prajvi Saxena, Himanshu Gupta, Innocent Enyekwe, Ragavendran V, Zienab EL-Wasif, Aleksandr Maksapetyan, Vivien Rossbach, Chris Harjadi, Mohsen Bahaloohoreh, Song Bian, John Lai, Justine Leon Uro, Greg Bateman, Mohamed Sayed, Ahmed Menshawy, Darling Duclosel, Yashaswini Jain, Ashley Aaron, Murat Tiryakioglu, Sheeshram Siddh, Keith Krenek, Alex Hoover, Joseph McGowan, Tejal Patwardhan, Summer Yue, Alexandr Wang, and Dan Hendrycks. Humanitys last exam, 2025. URL https://arxiv.org/abs/2501.14249. [67] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, et al. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024. [68] Wenxuan Zhang, Sharifah M. Aljunied, Chang Gao, Yew Ken Chia, and Lidong Bing. M3exam: multilingual, multimodal, multilevel benchmark for examining large language models. In Advances in Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks, 2023. [69] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. MME: comprehensive evaluation benchmark for multimodal large language models. In arXiv preprint arXiv:2306.13394, 2023. [70] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. MMBench: Is your multi-modal model an all-around player? In Proceedings of the European Conference on Computer Vision (ECCV), 2024. 15 [71] Mahtab Bigverdi, Zelun Luo, Cheng-Yu Hsieh, Ethan Shen, Dongping Chen, Linda Shapiro, and Ranjay Krishna. Perception tokens enhance visual reasoning in multimodal language models. arXiv preprint arXiv:2412.03548, 2024. [72] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. F.1 [73] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020.emnlp-demos.6. F."
        },
        {
            "title": "A Overview of the Appendix",
            "content": "This Appendix is organized as follows: Section and discuss the limitations and broader impact of STARE. Section presents an extended discussion about related works. Section details the statistics of STARE and the design spaces for all synthetic tasks, including 2D transformations, 3D transformations, cube net folding, and tangram puzzles. Section describes the experimental setup, covering the prompoints used, model configurations, hyperparameter settings, and presents full visualizations of different experimental settings (e.g., evaluation settings with or without visual simulations, perception probing questions). Section provides experimental results on additional models for analysis conducted in Section 3."
        },
        {
            "title": "B Limitations",
            "content": "Although STARE provides valuable insights, it still has several limitations. First, it uses simplified synthetic images that do not fully represent real-world complexity; future versions could include realistic or dynamic scenes with clutter and occlusion. Second, it focuses only on rigid shape transformations; adding tasks involving flexible shapes, articulated objects, or additional sensory cues (such as audio or depth) would cover wider range of spatial reasoning skills. Lastly, multiple-choice scoring hides intermediate reasoning steps; extending evaluations with explanations, step-by-step checks, or open-ended responses would give more detailed insights. Still, STARE current design has clear strengths. The simplified images isolate spatial reasoning from general object recognition tasks. Its structured variety of tasks helps pinpoint specific model difficulties. Automatic scoring ensures consistent and easy-to-scale evaluations. Modular task presentations (image-only, text-only, image+text prompts) let researchers analyze individual modality contributions. Additionally, synthetic data makes STARE easily reproducible, accessible, and extensible. Overall, STARE is strong first step toward measuring multimodal spatial reasoning, with clear pathways toward more realistic and comprehensive future benchmarks."
        },
        {
            "title": "C Broader Impact",
            "content": "STARE provides standardized way to measure AI capabilities in spatial reasoning tasks, potentially guiding research toward AI systems that can better support robotics, autonomous driving, augmented reality, and education. However, improved spatial reasoning could also lead to negative societal impacts if misused, such as enhanced surveillance or military applications. Additionally, the synthetic nature of STARE may introduce biases toward simplified or artificial scenarios, limiting direct applicability to real-world conditions. Future versions should aim to include more realistic, diverse datasets and consider ethical guidelines to minimize risks and ensure fair, positive societal outcomes."
        },
        {
            "title": "D Related work",
            "content": "Human visual reasoning. Human visual reasoning relies on two complementary faculties: relational analogymapping abstract structures across scenesand mental simulationpredicting future states through incremental transformations. StructureMapping Theory [50] and analyses of Ravens Progressive Matrices [51] first showed that success in visual problem-solving hinges on aligning relations rather than surface features. Computational accounts echo this claim: explicit relational models reproduce human-like performance [52], whereas modern deep networks still struggle with visual analogy tasks [5355]. Mental simulation complements analogy-making. Classic work on mental rotation [6] and mechanical reasoning [56] demonstrates that people mentally run transformations, consistent with grounded-cognition theories [57]. Intuitive-physics studies cast the mind as noisy physics engine 17 that combines object-centric structure with probabilistic dynamics [5860]. Object-based predictivecoding models such as PLATO extend these ideas, achieving human-like physical prediction and developmental trajectories [61, 62]. Simulations are also selective: people allocate attention just in time, focusing on the most diagnostic elements instead of exhaustively modeling the entire scene [22, 63, 23]. Together, these findings suggest that effective problem-solving hinges on the ability to carry out step-by-step visual simulations; our benchmark therefore probes whether multimodal models can effectively leverage or even produce such simulations and exhibit human-like visual reasoning on sequential, compositional tasks. Multimodal evaluation benchmarks. Recent advances in evaluating multimodal large language models have led to the development of benchmarks targeting diverse aspects of visual reasoning. Early benchmarks such as VQA [64] and CLEVR [11] focus on compositional reasoning and general visual question answering. However, more challenging benchmarks, such as MMMU [65] and Humanitys Last Exam (HLE) [66], assess expert-level, domain-specific reasoning using complex multimodal inputs, where state-of-the-art models achieve only around 60% on MMMU-pro [67] and below 20% on HLE. In response to the growing demand for robust evaluation, several new benchmarks [2428] have been introduced. For example, M3Exam repurposes multilingual professional-license questions [68]. MME [69] and MMBench [70] separate low-level perception from higher-level cognition. BLINK [29] departs from pure linguistic reasoning tasks to include tasks grounded in core computer vision capabilities, including relative depth estimation, semantic correspondence, visual similarity assessment, inpainting, etc. Improvements on BLINK require the use of perception tokens [71], which generate latent intrinsic images to reason, demonstrating for the first time, that reasoning doesnt have to be linguistic. In this work, we build upon this finding, targeting primarily visual reasoning that can be better solved with visual cues. The most relevant benchmarks to ours are perhaps KiVA [30], RAVEN/I-RAVEN [12, 31], SPACE [16], and TurtleBench [32], which mostly evaluate static analogy or pattern induction, often step-wise visual simulations is optional and curating the intermediate visual simulations is either not feasible or requiring extensive human efforts. VSI-Bench [33] underscores the role of mental imagery in spatial reasoning, but focuses on spatial memory and estimation from video rather than explicit step-by-step simulation. STARE bridges this gap with programmatically generated puzzles2D/3D transformations, cube-net folding, and tangram assemblythat isolate models capacity to benefit from explicit visual simulations. We further extend the benchmark with perspective taking and temporal frame reasoning tasks that mirror real-world scenarios."
        },
        {
            "title": "E Data Curation Details",
            "content": "Figure 7 presents the overall composition of STARE. Table 6 details the number of instances for each task in STARE, further broken down by whether the input contains an explicit intermediate visual simulations. Task category Without visual simulation With visual simulation Total Foundational Geometric Transformations 2D transformations 3D transformations Cube net folding Tangram puzzle 639 612 Integrated Spatial Reasoning 193 532 Real-world Spatial Reasoning Perspective reasoning Temporal frame reasoning Total 250 471 2,697 423 408 1,062 1,020 120 289 313 821 1, 250 471 3,937 Table 6: Dataset statistics grouped by task category and by the presence of full intermediate visual simulation. 18 Figure 7: Data Statistics of STARE. Below, we summarize the design space of data curation for synthetic tasks, including (1) 2D Transformations (E.1); (2) 3D Transformations (E.2); (3) Cube Net Folding (E.3); and (4) Tangram Puzzles (E.4); E.1 2D Transformations Shape generation. Shapes are selected from fixed set and assigned properties as follows: Types: Circle, Square, Rectangle, Triangle, Ellipse, Hexagon, Pentagon. Colors: Face color is random RGB tuple (r, g, [0, 1]); edge color is fixed (black). Center & Size: All shapes are centered at (0, 0). For circles, squares, triangles, hexagons, and pentagons, size is scalar drawn from [30, 35]; for rectangles and ellipses, size is tuple (width in [30, 35], height in [20, 25]). Transformations. sequence of randomly sampled operations is applied to the shapes: Rotate: Squares: 30, 60 (avoiding 90). Hexagons: 30, 90. Others: 30, 60, or 90. Rotation is applied w.r.t the shapes center. Flip: Horizontal (about = 0) or vertical (about = 0); not applied when the shape is centered at (0, 0) for symmetric shapes such as square, circle and etc. Translate: (dx, dy) with dx, dy {30, 10, 0, 10, 30} with constraints to ensure nonzero translation. Scale: Factors chosen from {0.5, 2.0}, ensuring the resultant size is within roughly [10, 40]. Shear: Parameters (shearx, sheary) are drawn from approximately [1, 1], with constraints to ensure perceptible skew. Shear is excluded for 2D text instructed transformation tasks, as human participants find it hard to describe the degree of shear such that they can differentiate among the answer candidates. Number of Transformation Steps. The final dataset contains instances with 1, 2, or 3 transformation steps. 19 Figure 8: Design space of 2D Transformations (1). 20 Figure 9: Design space of 2D Transformations (2). E.2 3D Transformations Shape generation. 3D objects are loaded from external blend files and instantiated with random properties defined in JSON file. Their attributes include: Types: Various 3D models such as cube, sphere, cone, cylinder, torus, pyramid, etc. Colors & Materials: Colors are sampled from predefined set, and materials are selected from external files. Size & Location: Objects are assigned size scalar (from the JSON-specified values) and an initial 3D location (typically near the origin), with adjustments to ensure they remain above the ground plane. 21 Transformations. sequence of randomly sampled operations is applied to the objects in 3D space: Translate: Axis selection: Randomly choose one or more axes from x, y, and (e.g., x, xy, xz, yz). Displacement: Translations are applied with discrete displacements: along and by 2 units and along by 1 unit, with constraints to keep the object above the ground (z 0). Rotate: Axis: single rotation axis is chosen randomly from x, y, or z. Angle: The rotation angle is drawn from discrete set (typically 30, 60, or 90), with the range sometimes adjusted for specific shapes (e.g., cubes or pyramids). Rotation is applied about the objects center. Shear: Plane: The shear operation is applied along one of three directional pairs: xy, xz, or yz. Factors: Two shear factors are sampled uniformly from the interval [0.2, 1.0], with an enforced minimum difference (approximately 0.4) to ensure perceptible skew. Scale: Factor: uniform scaling factor is chosen from 0.5, 2.0, either reducing or enlarging the object while keeping its final size within acceptable bounds. Flip: Direction: The object is reflected along principal axisflipped horizontally (reflection across the x-axis) or vertically (reflection across the y-axis). All transformation operations are applied sequentially, updating the objects 3D coordinates (including its bounding box and center) to reflect the cumulative effects. Number of Transformation Steps. Instances are generated with transformation sequences comprising 1, 2, or 3 steps, where each step randomly selects one of the available operations. This multi-step approach enables diverse design space of 3D transformations, as the operations can compound in various orders and combinations. E.3 Cube Net Folding Net Representation. Cube nets are represented as collections of faces, where each face is defined by its vertices in 3D space. Additional attributes include: Face Geometry: Each face is polygon (typically quadrilateral) with vertex coordinates stored as NumPy arrays. Connectivity: mapping of face connections identifies which faces share common edges, serving as potential hinges. Visual Attributes: Faces are rendered with colors (sampled from colormap) and labeled with their keys for easy identification. Folding Operations. The folding process simulates converting 2D cube net into 3D cube via sequence of rotation operations: Shared Edge Detection: The algorithm locates the common edge between candidate face and an already folded face. tolerance is used to robustly identify two shared vertices. Rotation Calculation: Using the shared edge as hinge, rotation is computed with fixed magnitude of 90 (i.e. π/2 radians). The sign of the angle is chosen by comparing the candidate faces center (projected onto the hinges perpendicular plane) with the desired direction toward the cubes center, which is derived from the base face. 22 Recursive Propagation: The rotation is applied not only to the candidate face but also recursively to all connected faces that have not been folded yet, ensuring that the entire net adjusts consistently. Folding Sequence and Visualization. The design space supports iterative, step-by-step folding, with each step comprising: Candidate Selection: Among the faces not yet folded, the algorithm picks one that is connected to an already folded face. Folding Parameters: It computes the rotation axis (the shared edge) and the appropriate 90 rotation (with correct sign) to fold the face into its 3D position. Instruction Generation: Each fold is described in natural language (e.g., Fold face 2 upwards towards face 3) based on changes in the faces center relative to the cubes base. 3D Rendering: After each step, the current state of the net is visualized using 3D plot (with Poly3DCollection) and saved as an image. Perturbation and Validity. To enrich the design space and introduce challenge: Perturbations: Selected folding steps can be intentionally altered by inverting the rotation angle or modifying the rotation axis. This simulates errors or variations, yielding nets that might fold incorrectly. Validity Checks: Functions are provided to verify that folded faces do not overlap, that shared edges are consistently maintained, and that face connections remain intact. These checks ensure that the final folded cube is geometrically valid. Dataset Generation and Perception Tasks. Beyond simulating the folding process, the design space incorporates mechanisms to create annotated datasets: Instructional Sequences: Detailed, step-by-step folding instructions (with corresponding images) are generated, supporting tasks that require understanding the folding procedure. Perception Variants: Additional tasks query the observers perceptionsuch as verifying if particular face has been folded or determining the connectivity between facesusing intermediate folding images. Randomness and Parameter Control. Stochastic elements pervade the folding simulation: Random seeds govern the selection of candidate faces, the decision to perturb folding step, and the choice of rotation adjustments. This randomness ensures that diverse range of cube nets and folding sequences are produced, which is crucial for generating robust datasets and for studying perception and reasoning in 3D folding tasks. E.4 Tangram Puzzle Segmentation. The puzzle begins with an iterative segmentation algorithm that splits full rectangular board into smaller pieces. The process is governed by minimum piece size and maximum number of pieces. At each segmentation step, the algorithm: Selects splittable rectangle based on its area. Chooses split direction (horizontal if the height is greater or vertical otherwise) and split line ensuring both resulting pieces exceed the minimum size. Records each split as an action with details (original rectangle, split line, and direction) that form the basis for later textual instructions. Piece Generation & Attributes. Each tangram piece is defined by its board coordinates (e.g., (r0, r1, c0, c1)) and derived properties such as area and dimensions. Additionally: Colors: Pieces are assigned unique, randomly generated colors. 23 Visualization: Grid lines and labels are overlaid on each piece to indicate its boundaries and area, facilitating clear visualization during reassembly. Scrambling and Transformation. Once segmented, pieces are scrambled to increase puzzle complexity. This involves applying series of random transformation operations: Rotation: Each piece is rotated by discrete angle chosen from 0, 30, 60, 90. Translation: Pieces are repositioned into non-overlapping cells on larger canvas. Flip: In some reassembly variants, horizontal or vertical flips are applied to further randomize the piece orientations."
        },
        {
            "title": "F Experimental Details",
            "content": "F.1 Models and Settings To expedite response generation, we use the vLLM [72] library, an open-source tool for fast LLM inference and serving. For all other cases, we load models directly using the Transformers [73] library. All model sources are official and listed in Table 7. When evaluating different models, we use default hyperparameter values unless otherwise specified, with detailed parameter settings provided in Table 7. For all models, we explicitly prompt it with Think step-by-step, and then put your final answer in \"boxed{}\". to encourage chain-of-thought reasoning and for easier answer parsing. Model GPT-4o Parameter Setting Source URL temperature = 0.0 chatgpt-4o-latest https://platform.openai.com Claude 3.5 Sonnet temperature = 0. claude-3-5-sonnet https://www.anthropic.com/ Gemini 2.0 Flash temperature = 0.0 gemini-2.0-flash-exp https://ai.google.dev/ Gemini 2.0 Flash Thinking temperature = 0.0 gemini-2.0-flashthinking-exp-1219 https://ai.google.dev/ OpenAI o1 temperature = 0. o1-2024-12-17 https://platform.openai.com Qwen2.5-VL-3B Qwen2.5-VL-7B Qwen2.5-VL-72B LLaVA-Onevision-72B do sample=True, temperature = 0.7 do sample=True, temperature = 0.7 do sample=True, temperature = 0.7 do sample=True, temperature = 0.7 local checkpoint https://huggingface.co/Qwen/ Qwen2.5-VL-3B-Instruct local checkpoint https://huggingface.co/Qwen/ Qwen2.5-VL-7B-Instruct local checkpoint https://huggingface.co/Qwen/ Qwen2.5-VL-72B-Instruct local checkpoint https://huggingface. co/llava-hf/ llava-onevision-qwen2-72b-ov-hf InternVL2.5-78B https://huggingface.co/ OpenGVLab/InternVL2_5-78B Table 7: The sources of models used in the experiments and the hyperparameters configuration. local checkpoint do sample=True, temperature = 0.7 F.2 Visualization of Evaluation Settings Figures 1011 provide full visualizations of evaluation settings illustrated in Figure 3. In addition, we show an example of how real-world spatial reasoning task temporal frame reasoning is evaluated without visual simulation in Figure 12. 24 Figure 10: Examples of Tangram Puzzle under without Visual Simulations\" Evaluation Setting (top: questiononly, bottom: question+assembly steps). Figure 11: Example of Tangram Puzzle under with Visual Simulations\" Evaluation Setting. Figure 12: Examples of Temporal Frame Reasoning under without Visual Simulations\" Evaluation Setting. 25 F.3 Visualizations of Perception Probing Questions In Figure 5, Claude demonstrates perceptual error: while it correctly identifies all face colors, it incorrectly perceives face 6 to be positioned beneath face 4, when it is actually located beneath face 5. Such errors prompt an important question regarding task performance: for challenging tasks like cube net folding, to what extent does the low performance stem from perceptual inaccuracies rather than deficiencies in simulation capabilities or an inability to correctly interpret simulation outcomes? We design probing questions to evaluate model performance 2D and 3D perception on cube nets (Figure 13), which reveals that model fail substantially on 3D perception  (Table 2)  , which may be the main bottleneck in understanding intermediate visualizations in cube net folding  (Table 1)  . Figure 13: Exemplary questions on cube nets to probe model performance on 2D and 3D perception. F.4 Visualizations of STARE Task in Different Representations Figures 1417 provide concrete examples of the input modalities evaluated in STARE. For every task family we visualize the image-only variant (the original format in STARE), the text-only variant (compact symbolic description that can be consumed without vision), andwhere applicablethe combined image+text variant that concatenates the two. 2D and 3D transformations. In the text-only panels, each object is serialized as <shape>, <color>, <x,y>, <size>, with attributes separated by commas (e.g., square, red, (3, 4), 2). The image+text panels place the same textual description beneath the image, so that language and vision can be attended to jointly. Cube-net folding. We flatten the cube into 2D grid and enumerate its faces from 1 to 6. The text-only representation thus becomes short digit string (e.g., 123456) or block array that mirrors the spatial arrangement of the net. Tangram puzzle. Because rotations in the image cannot be expressed succinctly in the image+text setting, we show only image-only and text-only variants. Each piece is labeled alphabetically and encoded by binary occupancy gridrows of 1 indicate filled cells, yielding representation that is both human-readable and unambiguous for MLLMs. Together, these examples clarify the correspondence between the natural visual stimuli and the stripped-down symbolic forms used in our text-only experiments, as introduced in Section 3.3. 26 Figure 14: Visualizations of 2D transformations (w/ text instructions) in different representations (upper left: image-only, lower left: text-only, right: image+text). Figure 15: Visualizations of 3D transformations (w/ text instructions) in different representations (upper left: image-only, lower left: text-only, right: image+text). Figure 16: Visualizations of cube net folding in different representations (upper left: image-only, lower left: text-only, right: image+text). Figure 17: Visualizations of tangram in different representations (left: image-only, right: text-only). F.5 Case Study Figure 18 presents error cases on 2D transformation tasks from o1 and GPT-4o. The o1 explanation correctly identifies that regular hexagon rotated 30 counter-clockwise will have vertex pointing straight up, so it chooses panel B, but its justification overlooks potential differences in size, color, or shape variants among the answer choices. GPT-4o, meanwhile, mistakenly claims the hexagon looks exactly the same after 30 rotation (confusing 60 symmetry with 30) and vacillates between panels and C, revealing it doesnt fully grasp the visual outcome of the specified rotation. 28 Figure 18: Error cases of o1 and GPT-4o on 2D transformation. Figure 19 presents error cases on 3D transformation tasks from o1 and GPT-4o. The o1 response accurately places the torus up and left of the origin and selects panel A, but it overlooks that material/appearance differences could also rule out that choice. GPT-4o correctly sees that both panels and share the required displacement, yet it arbitrarily favors D, giving justification (clearer displacement) unrelated to the stated transformation criteria, so its final selection is wrong despite partially sound reasoning. Figure 19: Error cases of o1 and GPT-4o on 3D transformation. 29 On cube net folding task, besides the perception error from Claude in Figure 5 and the text simulation error from GPT-4o in Figure 1, when provided with intermediate visual simulation, models like GPT4o still struggles with understanding the intermediate visual cues. In Figure 20, GPT-4o asserts that face 1 becomes the cubes top surface, whereas face 1 is actually enclosed between faces 6 and 3. This misrepresentation of face adjacency highlights GPT-4os difficulty in maintaining accurate 3D spactial cognition, which led to the wrong final result. This observation aligns with the conclusion from the perception probing test on cube nets in Section 3.3, where GPT-4o fails substantially on 3D perception than 2D perception. Figure 20: error case from GPT-4o on cube net folding, where the model fails to understand the intermediate visual simulation. For tangram puzzles, the left example of Figure 21 shows how Claude takes shortcut by conducting an area-based feasibility check, correctly noting that the 4 4 board contains 16 unit squares, but it erroneously counts the L-shaped piece as 5 rather than its actual 7 squares. This miscalculation reduces the summed piece area to 14 instead of the correct 16, leading to concludeincorrectlythat the puzzle cannot be completed. The right example of Figure 21 shows how Claude fails to reason about the piece positions when given clear step-by-step instructions. Claude correctly interprets the coordinate instructions and board dimensions, yet its spatial reasoning falters when projecting how the rotated pieces occupy the grid. It underestimates the extents of pieces and D, inventing overlaps and gaps that never arise, and therefore erroneously concludes the puzzle cannot be completed. This error stems from an inaccurate mental simulation of spatial relationships. 30 Figure 21: Left: perception error case from Claude on tangram puzzle. Right: An error case from Claude on tangram puzzle, which failed to simulate the intermediate steps even when step-by-step instructions are given. Figure 22 presents two error cases from Claude on temporal frame reasoning. In the left example, Claude correctly inferred the cameras left-to-right movement across the given frames, yet it mis-evaluated the viewpoints depicted in the answer choices and consequently selected the wrong completion frame. In the right example, the model erred even earlier, misconstruing the direction of camera motion itself; this foundational misinterpretation then led to an incorrect choice despite seemingly systematic rationale. 31 Figure 22: Error case on temporal frame reasoning. Figure 23 presents an error for perspective reasoning from GPT-4o. GPT-4o misprojects the agents top-down pose into egocentric space: it assumes the arrowed direction is pointing towards curved seating area and therefore selects option A. correct geometric mapping should conclude that the agent would instead stand next to the curved seating area. This error underscores the models difficulty in reason about perspective changing in 3D space. 32 Figure 23: Error case on perspective reasoning."
        },
        {
            "title": "G Detailed Analysis Results",
            "content": "Correlation Analysis between Synthetic tasks and Real tasks. In Section 3.2, we briefly discussed the correlation between averaged model performance on synthetic tasks (including 2D transformation, 3D transformation, cube net folding and tangram puzzle) and that on real-world tasks (including temporal frame reasoning and perspective reasoning). Figure 24 shows the averaged model performance on synthetic and real-world tasks across 11 models and the fitted line with correlation coefficient 0.88. Figure 24: Correlation between model performance on synthetic tasks and that on real-world tasks. 33 Note that for open-source models, the real-world task performance is close to random guessing (29%). Removing the open-source models, the correlation coefficient decreased to 0.58, still showing weak but positive correlation between sythetic task performance and real-world task performance. Model Performance on 2D/3D Individual Transformation Types. Table 8 presents model accuracy across 2D visual analogy and text instruction tasks. Across the nine subtasks, adding visual simulation lifted accuracy for every model except in few narrow cases, and the size of the gain correlates strongly with baseline capability. Closed-source leaders that were already solid on the raw pixel taskso1 ( +3 points overall) and GPT-4o ( +8 points)were pushed into the mid-80 and low-90 s, effectively reaching ceiling on the text-instruction variants, where gains were biggest (e.g., GPT-4o jumps +25 points on Reflection and +18 points on both Rotation and Translation). Mid-tier proprietary models such as Gemini 2.0 Flash ( +5 points) and its Flash Thinking mode ( +5.5 points) benefited even more on instructions than on analogies, narrowing the gap to GPT-4-class systems. Open-source vision-language models lag full generation behindthe best of them (InternVL 2.5-78B) still sits below 55% on average after simulationbut they, too, record healthy boosts of 612 points, chiefly on the analogy side. The lone regression is GPT-4os 5 pt dip on Reflection analogies, suggesting that simulation may occasionally overwrite correct latent heuristic. Overall, the pattern indicates that visual simulation chiefly helps models convert verbal transformation instructions into precise spatial operations, while stronger base perception/reasoning models harvest the largest absolute improvements and approach human-like proficiency. Table 9 presents model accuracy across 3D visual analogy and text instruction tasks. Visual simulation gives 3D spatial reasoning measurablebut more unevenboost than in 2D: averaged over all eight subtasks, every proprietary model gains between +2 points (GPT-4o, o1) and +6 points (Claude-3.5 Sonnet, Gemini-Flash Thinking), while the open-source field improves by +47 pointsexcept InternVL, which slips point. Gains concentrate in the conceptually harder operations: across models, Shearing (both analogy +6.6 points and instruction +6.6 points) and Rotation-instruction ( +6.4 points) see the largest lifts, whereas Translation under visual analogy actually falls slightly (0.9 points), echoing smaller 2D reflection dip. Even after simulation, closed-source leaders plateau in the high-60s to mid-70s on most 3D subtasksroughly 15 points below their 2D ceilingsindicating that depth-aware transformations remain major bottleneck. Open-source VL models still trail full generation (45% average), but their sharper relative gains suggest they, too, leverage synthetic roll-outs to bridge language and geometry. Model 2D Transformations w/ Visual Analogy 2D Transformations w/ Text Instruction Reflection Rotation Shearing Scaling Translation Reflection Rotation Scaling Translation Without Visual Simulation GPT-4o Claude-3.5 Sonnet Gemini2.0 Flash Gemini2.0 Flash Thinking o1 LLaVA-OneVision Qwen2.5-VL-72B InternVL2.5-78B 82.1 75.0 85.7 52.4 92.9 7.1 57.1 35. 69.8 60.9 63.8 48.9 70.7 25.9 38.8 41.4 88.5 87.4 84.4 71.9 83.3 24.4 64.4 45.6 53.7 55.8 51.0 46.9 59.2 32.7 34.7 34.7 With Visual Simulation 72.0 71.2 71.4 55.1 84.0 25.4 42.3 36.6 65.8 63.8 65.8 63.2 89.5 31.7 29.3 41.5 67.8 58.9 62.3 60.6 78.1 33.1 49.6 51. 90.6 85.9 88.4 83.0 92.2 51.0 62.5 75.0 73.3 66.5 70.3 67.8 92.2 34.6 38.8 51.9 GPT-4o Claude-3.5 Sonnet Gemini2.0 Flash Gemini2.0 Flash Thinking o1 LLaVA-OneVision Qwen2.5-VL-72B InternVL2.5-78B 91.9 85.5 85.5 71.0 87.1 30.6 71.0 59.7 Table 8: Model Performance With or Without Visual Simulation across 2D Transformation types in Visual Analogy and Text Instruction Tasks. 54.8 50.0 59.5 40.5 54.8 31.0 35.7 33.3 93.2 83.8 90.5 89.2 94.6 48.6 60.8 73. 86.0 72.9 74.8 68.2 93.5 41.1 40.2 53.3 91.5 73.9 78.2 73.9 97.6 33.9 39.4 53.9 72.8 70.9 70.9 68.2 80.6 30.1 56.3 43.7 76.9 73.1 73.1 61.5 80.8 15.4 65.4 69.2 80.0 73.9 74.5 56.4 84.2 24.8 57.0 47.3 91.2 55.9 79.4 70.6 100 20.6 41.2 50. 34 Model 3D Transformations w/ Text Instruction 3D Transformations w/ Visual Analogy Rotation Shearing Scaling Translation Rotation Shearing Scaling Translation GPT-4o Claude-3.5 Sonnet Gemini2.0 Flash Gemini2.0 Flash Thinking o1 LLaVA-OneVision Qwen2.5-VL-72B InternVL2.5-78B GPT-4o Claude-3.5 Sonnet Gemini2.0 Flash Gemini2.0 Flash Thinking o1 LLaVA-OneVision Qwen2.5-VL-72B InternVL2.5-78B 60.7 50.0 54.2 42.4 65.6 18.8 36.5 31. 64.3 51.2 46.4 50.0 63.1 27.4 46.4 31.0 Without Visual Simulation 55.7 46.2 53.9 43.6 58.1 29.1 40.2 30.8 64.3 59.5 64.3 47.6 63.1 28.6 54.8 28.6 76.0 63.3 63.3 61.5 76.7 28.9 61.1 51.1 80.1 62.6 73.0 63.8 85.6 25.3 46.6 37. With Visual Simulation 78.2 69.2 62.8 60.3 76.9 32.1 69.2 43.6 76.0 59.7 68.2 66.7 79.8 29.5 55.0 32.6 60.1 45.9 55.86 37.8 61.3 27.0 36.9 37.8 62.6 55.6 60.9 48.5 69.7 27.3 45.5 43.4 46.9 40.4 44.44 32.7 46.3 19.4 33.3 32. 54.7 48.0 49.5 46.7 50.7 26.7 34.7 37.3 71.1 55.6 61.90 52.5 70.5 41.0 47.6 60.0 75.3 64.5 64.9 59.1 79.6 45.2 48.4 57.0 71.2 53.4 51.63 55.7 73.9 30.7 45.1 40.5 68.5 59.3 56.1 59.3 74.1 35.2 46.3 40.7 Table 9: Model Performance With or Without Visual Simulation across 3D Transformation types in Visual Analogy and Text Instruction Tasks. Task complexity vs. performance. Table 11 presents model performance across different task difficulties for 2D and 3D transformations. Adding visual simulation helps most when tasks get tougher, but the effect differs by setting. For 2D text instructions tasks, we observe big boost closed-source models jump about 10-20 points on medium and hard tasks, often hitting 90%+. For 2D visual analogy tasks, we observe smaller liftseveral points on easy, up to 10 points on medium/hard. For 3D tasks, only few-point gain, and some models slip on the hardest visual analogy tasks, showing 3D reasoning is still hard. Open-source MLLMs stay well behind; their scores move up and down unpredictably, meaning they havent yet learned to use the simulated views well. Table 10 presents model performance across different number of transformation steps for 2D and 3D transformations. Models struggle more as the number of transformation steps grows, and visual simulation mainly fixes that. Without simulation, accuracy often peaks at one or two steps and drops at threeespecially in 3D visual-analogy, where GPT-4o falls from 73% (N = 2) to 49% (N = 3). When simulation is added, scores for the multi-step cases (N = 23) jump 1015 points for the top proprietary systems and few points for open-source ones, erasing most of the earlier decline in 2D tasks and cutting the 3D drop roughly in half. Single-step problems were already easy for the best models and see little change. Overall, simulation is most useful for longer, instruction-driven chains of transforms, while depth-heavy 3D sequences remain the hardest setting. 35 Model 2D Visual Analogy 2D Text Instruction 3D Visual Analogy 3D Text Instruction N=1 N=2 N=3 N=1 N=2 N=3 N=1 N=2 N=3 N=1 N=2 N=3 Without Visual Simulation GPT-4o 60.46 74.84 73.86 67.27 77.56 73.55 62.75 73.37 48.69 63.07 63.40 60.78 Claude-3.5 Sonnet 63.73 75.82 65.69 65.17 65.02 60.61 45.10 57.35 57.35 50.98 55.23 45.75 64.71 73.53 68.53 63.96 76.24 70.25 61.76 60.78 63.73 46.08 56.86 56.86 Gemini2.0 Flash Gemini2.0 Flash Thinking 54.58 52.94 55.56 61.71 67.33 71.07 47.71 53.92 57.19 45.59 50.00 20.59 66.7 o1 66.67 72.55 77.45 61.76 66.67 62.75 30.39 26.47 24.51 49.57 33.70 31.53 25.49 28.43 22.55 30.39 30.39 24.51 LLaVA-OneVision 40.2 43.14 34.31 42.16 61.74 52.17 50.45 InternVL2.5-78B 50.00 45.10 41.18 55.65 36.96 40.54 48.04 42.16 45.10 38.24 43.14 41.18 Qwen2.5-VL-72B 40.2 29.41 36.27 34.31 48.04 89.3 82.0 81.4 89. 82.4 With Visual Simulation GPT-4o Claude-3.5 Sonnet Gemini2.0 Flash Gemini2.0 Flash Thinking o1 LLaVA-OneVision InternVL2.5-78B Qwen2.5-VL-72B - - - - - - - - 78.43 73.53 70.59 70.59 73.5 69.6 46.08 58.82 73.4 85.3 30.39 25.49 39.22 51.96 51.96 58.82 - - - - - - - - 88.04 90.57 71.74 72.64 80.43 77.40 79.35 67.92 94.6 97.2 38.04 34.91 56.52 52.83 43.48 41.51 - - - - - - - - 70.59 72.55 56.86 57.84 61.76 59.80 55.88 60.78 70.6 75.5 28.43 28.43 25.49 35.29 49.02 58.82 - - - - - - - - 61.76 68.63 65.69 50.98 61.76 53.92 53.92 53.92 70.6 69.6 36.27 29.41 46.08 39.22 47.06 43.14 Table 10: Model Performance With or Without Visual Simulation across number of transformation steps (N) in 2D/3D Visual Analogy and Text Instruction Tasks. Model 2D Visual Analogy 2D Text Instruction 3D Visual Analogy 3D Text Instruction easy medium hard easy medium hard easy medium hard easy medium hard Without Visual Simulation 80.4 GPT-4o 76.5 Claude-3.5 Sonnet Gemini 2.0 Flash 78.4 Gemini 2.0 Flash Think 66.3 83.3 o1 22.6 LLaVA-OneVision 45.1 InternVL 2.5-78B 57.8 Qwen 2.5-VL-72B 67.3 66.7 63.7 52.3 77.5 32.4 40.2 40.2 61.4 76.2 62.1 68.7 64.7 75.0 44.4 65.5 69.6 90.6 26.5 39.5 34.3 63.2 38.2 50. 70.4 61.8 67.2 69.4 81.1 46.3 50.9 41.7 71.3 74.2 59.4 54.9 67.3 67.6 65.4 54.6 89.1 78.4 29.2 25.5 50.0 32.4 41.7 55.9 64.9 54.4 59.8 53.9 70.6 20.6 34.3 40.2 65.7 69.0 50.5 55.6 58.8 56.9 50.3 48.0 67.7 69.6 30.4 31.4 39.2 48.0 39.2 42.2 63.1 52.0 52.9 44.7 64.7 28.4 37.3 38.2 55.2 44.4 50.0 46.1 56.9 25.5 37.3 42. With Visual Simulation 80.9 GPT-4o 76.5 Claude-3.5 Sonnet Gemini 2.0 Flash 79.4 Gemini 2.0 Flash Think 54.4 80.9 o1 36.8 LLaVA-OneVision 57.4 InternVL 2.5-78B 72.1 Qwen 2.5-VL-72B 89.4 65.2 86.4 74.2 98.5 34.9 48.5 30.3 Table 11: Model Performance With or Without Visual Simulation across different difficulty levels in 2D/3D Visual Analogy and Text Instruction Tasks. 86.9 80.9 72.1 67.7 67.2 64.7 68.9 72.1 95.1 85.3 34.4 26.5 49.2 23.5 36.1 63.2 67.7 91.6 63.2 78.9 63.2 81.7 47.1 76.1 75.0 94.4 27.9 39.4 35.3 64.8 44.1 59.2 58.8 75.0 51.5 66.2 58.8 57.4 48.5 63.2 64.7 73.5 38.2 45.6 39.7 55.9 48.5 47. 55.9 51.5 60.3 50.0 61.8 27.9 44.1 44.1 79.4 72.1 72.1 55.9 82.4 19.1 44.1 50.0 75.0 52.9 58.8 54.4 69.1 20.6 27.9 50.0 64.7 57.4 55.9 48.5 75.0 25.0 27.9 44.1 2D and 3D Perception Probing with Cube Nets. Table 12 presents model performance on 2D and 3D perception probing questions about cube nets, in comparison to the success rate on cube net folding task. The results show that success on cube-net folding is driven by models 3D perception, not its 2D eyesight. All closed-source systems (and several open-source ones) already read colors and 2D face connectivity at or near ceiling, yet their cube-net scores diverge sharply. When we compare cube accuracy ( VSim column) with each perceptual measure, the strongest linear relationship is with the 3D Folded? test (Pearson 0.89), while 2D connectivity (r 0.68) and color (r 0.72) are weaker. Gemini Flash illustrates the pattern: it pairs the top 3D perception score (69%) with the best cube-net performance (65%), whereas GPT-4o and InternVL match its 2D vision but lag 10-20 points on both 3D perception and cube folding. In short, being able to judge how faces come together in depthrather than recognizing colors or flat adjacencieslargely determines how well model can reason about folded cubes. Model Random GPT-4o Gemini-2.0-Flash Gemini-2.0-Flash-Thinking LLaVA-OneVision InternVL 2.5-78B Qwen 2.5-VL-72B Color 25. 100.0 100.0 99.0 88.0 92.0 96.0 2D Perception Connectivity 50.0 3D Perception Folded? 50.0 Cube Net Performance VSim 50. Vsim 50.5 Closed-source Models 94.1 84.9 49.4 Open-source Models 10.0 86.0 81.7 57.4 68.8 54. 22.0 40.2 42.1 52.5 65.0 39.8 28.5 43.5 35.2 49.1 65.5 62.8 34.2 41.0 53.4 Table 12: 2D and 3D perception performance in cube net folding. Question-only vs. Question+Steps As shown in Table 13, adding explicit reasoning steps (Q + Steps\") has opposite effects on cube-net tasks for the two model groups: open-source models gain, while closed-source ones do not. The three open-source VL models jump mean + 20 points on cube nets (driven by LLaVAs + 40 pts), whereas the five proprietary models average small decline (-1 pt, with mixed signs). On tangram puzzles, however, the pattern converges: every modelopen or closeddrops sharply once reasoning steps are included, with average losses of about -24 pts for closed-source and -19 pts for open-source models. Again, the trivial solution on tangram puzzles would be comparing the total areas of all available pieces and the grid area, which can easily lead to 75% performance. This result suggest that the models cannot leverage explicit text reasoning steps. Model Q-only Cube Nets Q+Steps Closed-source Models Tangram Puzzles Q+Steps Q-only GPT-4o Claude-3.5 Sonnet Gemini-2.0 Flash Gemini-2.0 Flash Thinking o1 LLaVA-OneVision InternVL 2.5-78B Qwen 2.5-VL-72B 50.2 51.5 47.4 47.2 56.0 0.0 33.2 29.0 50.4 46.4 51.5 49.6 47. +0.2 -5.1 +4.1 +2.4 -7.0 Open-source Models 40.5 41.4 41.6 +40.5 +8.2 +12.6 62.4 71.1 72.8 42.9 73.5 30.3 69.5 72. 34.7 41.9 59.8 35.3 29.6 14.6 51.7 47.7 -27.7 -29.2 -13.0 -7.6 -43.9 -15.7 -17.8 -24.6 Table 13: Model performance on question-only prompts versus prompts that include explicit reasoning steps (Q+Steps). values are Q+Steps performance - Q-only performance. Intermediate Visual Simulation States vs. Performance Table 14 summarizes extended results on varying the slice of intermediate visual simulation presented to the model across different tasks. Across models, which slice of the simulation you show matters, and the best slice shifts with task type. For 2D transformations, most closed-source models and the stronger open-source one (InternVL) peak when they see only the last intermediate state, gaining 26 points over the full roll-out; showing every intermediate frame (all) often drags accuracy down few points. For 3D transformations, the pattern flipsaccuracy is usually highest with all states ( +24 points over partial), while the last-only view tends to erase that gain, especially for GPT-4o, Gemini Flash, and o1. For cube nets, no single view helps every model. Scores barely change with all frames, and last-only often hurts closed-source models (-8 points on average) yet uniquely rescues LLaVA (+11 points). For Tangram puzzles, seeing all steps is consistently best: every model but LLaVA jumps 724 points versus the partial view, whereas last-only falls back toor belowthe partial baseline. Overall, for more complex tasks, models struggle to leverage intermediate visual states effectively. 37 Model 2D Transformation Last All Partial GPT-4o Claude-3.5-Sonnet Gemini-2.0-Flash 86.8 67.8 75.4 89.3 82.8 71.4 75.2 87.7 89.4 70.7 79.3 93.4 3D Transformation All Last Partial Closed-source Models 68.4 68.4 72.1 55.9 57.8 54.9 57.8 59.3 61.0 70.1 65.2 71.6 Open-source Models Cube Nets All Partial 51.3 58.7 40.5 54.4 52.2 51.6 35.6 53.4 Last 35.2 46.8 41.5 45.4 Tangram Puzzles All Partial Last 43.5 43.5 63.8 34.8 51.5 67.6 65.5 53.2 43.4 43.3 58.2 46.0 39.8 41.8 44. LLaVA-OneVision InternVL 2.5-78B Qwen 2.5-VL-72B 30.6 36.5 49.1 Table 14: Model performance with partial, all, and last intermediate visual simulations. 44.9 54.3 49.0 34.2 37.3 53.4 45.6 37.8 42.3 29.4 40.2 43. 40.2 34.7 41.9 31.8 56.6 44.4 25.5 32.3 48.7 32.2 54.5 48.5 28.3 48.3 44.4 40.2 48.2 56."
        }
    ],
    "affiliations": [
        "Stanford University",
        "Sun Yat-sen University",
        "University of Washington"
    ]
}