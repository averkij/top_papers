{
    "paper_title": "WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation",
    "authors": [
        "Zezhong Qian",
        "Xiaowei Chi",
        "Yuming Li",
        "Shizun Wang",
        "Zhiyuan Qin",
        "Xiaozhu Ju",
        "Sirui Han",
        "Shanghang Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Wrist-view observations are crucial for VLA models as they capture fine-grained hand-object interactions that directly enhance manipulation performance. Yet large-scale datasets rarely include such recordings, resulting in a substantial gap between abundant anchor views and scarce wrist views. Existing world models cannot bridge this gap, as they require a wrist-view first frame and thus fail to generate wrist-view videos from anchor views alone. Amid this gap, recent visual geometry models such as VGGT emerge with geometric and cross-view priors that make it possible to address extreme viewpoint shifts. Inspired by these insights, we propose WristWorld, the first 4D world model that generates wrist-view videos solely from anchor views. WristWorld operates in two stages: (i) Reconstruction, which extends VGGT and incorporates our Spatial Projection Consistency (SPC) Loss to estimate geometrically consistent wrist-view poses and 4D point clouds; (ii) Generation, which employs our video generation model to synthesize temporally coherent wrist-view videos from the reconstructed perspective. Experiments on Droid, Calvin, and Franka Panda demonstrate state-of-the-art video generation with superior spatial consistency, while also improving VLA performance, raising the average task completion length on Calvin by 3.81% and closing 42.4% of the anchor-wrist view gap."
        },
        {
            "title": "Start",
            "content": "WRISTWORLD: GENERATING WRIST-VIEWS VIA 4D WORLD MODELS FOR ROBOTIC MANIPULATION Yuming Li1 Sirui Han2,(cid:12) Zezhong Qian1,* Xiaowei Chi2,*, Xiaozhu Ju4 1 State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University 2 Hong Kong University of Science and Technology 3 National University of Singapore 4 Beijing Innovation Center of Humanoid Robotics Shizun Wang3 Zhiyuan Qin4 Shanghang Zhang 1,(cid:12)"
        },
        {
            "title": "ABSTRACT",
            "content": "Wrist-view observations are crucial for VLA models as they capture fine-grained handobject interactions that directly enhance manipulation performance. Yet large-scale datasets rarely include such recordings, resulting in substantial gap between abundant anchor views and scarce wrist views. Existing world models cannot bridge this gap, as they require wrist-view first frame and thus fail to generate wrist-view videos from anchor views alone. Amid this gap, recent visual geometry models such as VGGT emerge with precisely the geometric and cross-view priors that make it possible to address such extreme viewpoint shifts. Inspired by these insights, we propose WristWorld, the first 4D world model generates wrist-view videos solely from anchor views. WristWorld operates in two stages: (i) Reconstruction, which extends VGGT and incorporates our Spatial Projection Consistency (SPC) Loss to estimate geometrically consistent wrist-view poses and 4D point clouds; (ii) Generation, which employs our designed video generation model to synthesize temporally coherent wrist-view videos from the reconstructed perspective. Experiments on Droid, Calvin, and Franka Panda demonstrate stateof-the-art video generation with superior spatial consistency, while also improving VLA performance, raising the average task completion length on Calvin by 3.81% and closing 42.4% of the anchor-wrist view gap. (cid:128) Project Page Code 5 2 0 2 8 ] . [ 1 3 1 3 7 0 . 0 1 5 2 : r Figure 1: We present WristWorld, framework that synthesizes realistic wrist-view videos from anchor views through two-stage process: reconstruction stage for estimating wrist-view projections, and generation stage for producing coherent wrist-view videos. The generated wrist observations effectively expanding training data to novel view and lead to significant performance improvements for downstream VLA models across various tasks. * Equal contribution, Project leader"
        },
        {
            "title": "INTRODUCTION",
            "content": "Wrist-view observations play central role in visionlanguageaction (VLA) models because they directly capture the fine-grained handobject interactions that underlie precise manipulation. However, most large-scale robotic datasets provide only limited wrist-view coverage (Ebert et al., 2024; Mandlekar et al., 2018; Collaboration et al., 2025), producing substantial and practically important gap between abundant anchor (third-person) views and scarce wrist-centric recordings. Models pretrained on external perspectives therefore often underperform when tasks require detailed wristcentric perception or control. Collecting wrist-view data at scale is expensive: it demands extra sensors, careful calibration, and specialized recording setups. While world models have been proposed for data completion and synthesis (Wang et al., 2025c; Yang et al., 2023; Zhen et al., 2025), existing approaches are not designed to close the anchor-to-wrist gap in realistic, dynamic manipulation settings. Many of them (Liao et al., 2025; Liu et al., 2024) require wrist-view first frame as condition and thus cannot generate wrist-view sequences from anchor views alone. This raises natural question: can we enrich existing third-person datasets with automatically generated, geometrically consistent wrist-view sequences that support both perception and control? However, bridging anchor views to wrist views is highly challenging: First, scenes are dynamic and dominated by articulated arms and manipulators that cause severe, time-varying occlusions. Second, the target wrist perspective is often not seen during training of current viewpoint transfer methods. Third, geometric reconstructions from anchor views are typically sparse and temporally inconsistent, making naïve view-synthesis prone to spatial or temporal artifacts (Liu et al., 2024). To address these challenges, we propose WristWorld, the first 4D world model that synthesizes wrist-view videos solely from anchor views. Motivated by recent advances in visual geometric modeling (Wang et al., 2025b) and diffusion-based video synthesis (Blattmann et al., 2023), WristWorld features two-stage pipeline that explicitly enforces both geometric and temporal consistency. In the Reconstruction stage, we extend VGGT with dedicated wrist head that encodes the extreme viewpoint transform and estimates geometrically consistent 4D point clouds and wrist-view camera poses. novel Spatial Projection Consistency (SPC) loss is proposed to enforce alignment between 2D correspondences and the reconstructed 3D/4D geometry, improving spatial fidelity. In the Generation stage, diffusion-based video generator conditioned on the reconstructed wrist projections and CLIP-encoded anchor-view features synthesizes temporally coherent wrist-view videos that respect the recovered geometry and scene semantics. We validate WristWorld on Droid (Khazatsky et al., 2024), Calvin (Mees et al., 2022), and Franka Panda setups. Results show state-of-the-art wrist-view video generation with superior spatial consistency, and practical downstream gains for VLA: on Calvin we increase average task completion length by 3.81% and close 42.4% of the anchorwrist performance gap. Importantly, WristWorld can be used as plug-in to extend existing single-view world models with multi-view capability without requiring new wrist-view data collection. Our contributions are three-fold: novel two-stage framework for anchor-view to wrist-view video generation that achieves both temporal Consistency and geometric consistency. Leveraging wrist head, SPC loss, and CLIP-encoded anchor-view features to synthesize consistent wrist-view sequences from anchor views. Experiments showing that our approach improves VLA performance and can be applied in plugand-play manner to extend single-view world models into multi-view settings."
        },
        {
            "title": "2 RELATED WORK",
            "content": "3D Reconstruction for Robotic Perception. Accurate multi-view 3D reconstruction and camera pose are key for manipulation, yet many frameworks assume fixed calibration or static views. Recent work injects geometry into policies; e.g., GNFactor jointly optimizes NeRF scene model and manipulation policy, sharing one 3D representation for multi-task learning (Ze et al., 2023). Transformer-based vision models have also been explored; for instance, VGGT encodes multi-view 2 observations into fused geometric features for 3D prediction (Wang et al., 2025b). Still, moving wrist-camera pose is rarely modeled, and large datasets rely on manual calibration instead of online wrist-centric pose prediction. Cross-view consistency remains crucial in dynamic 3D scene reconstruction (Wang et al., 2025d; Hu et al., 2025), with MTVCrafter introducing 4D motion tokens to enforce coherence (Ding et al., 2025). Video Generation Models for Manipulation. Diffusion-based video generators let planners imagine robot futures. Web-scale text-to-video models look realistic but struggle with novel objectaction combinations. RoboDreamer improves compositionality by factorizing generation via language parsing (Zhou et al., 2024). This&That adds gesture conditioning for controllable plans beyond text-only inputs (Wang et al., 2025a). Coupling prediction with control, VideoAgent iteratively self-refines diffused plans to reduce hallucinations (Soni et al., 2025), and action-conditioned diffusion in generative predictive control approximates dynamics for policy improvement (Qi et al., 2025). Synthetic data routes like DreamGen synthesize diverse dream trajectories for stronger generalization (Jang et al., 2025). For spatial consistency, EnerVerse combines multi-view diffusion with 4D reconstruction (Gaussian splatting) to produce geometry-consistent futures and better long-horizon planning (Huang et al., 2025; Li et al., 2025). Large frameworks such as UniPi use text-guided video generation to learn universal multi-task policies (Du et al., 2023; Chi et al., 2025a). VisionLanguageAction (VLA) Robotics Models. VLA policies learn directly from paired visual and linguistic inputs without constructing an explicit world model. GR-1, GPT-style videoconditioned policy, is pretrained on large human video corpora and fine-tuned to achieve state-ofthe-art multi-task performance on CALVIN (88.9% to 94.9%) with zero-shot generalization to novel scenes (Wu et al., 2023; Mees et al., 2022). GR-2 scales training to 38M videotext pairs, producing generalist agent capable of executing 100+ manipulation tasks by grounding instructions in action sequences (Cheang et al., 2024). Beyond internet video pretraining, Vid2Robot maps human video demonstrations to robot policies via cross-attention (Jain et al., 2024), and MimicPlay derives hierarchical plans from unstructured human play (Wang et al., 2023). Label-free alignment has also been explored by grounding frozen video generator into continuous actions through goal-conditioned self-exploration (Luo & Du, 2024). Conversely, human-in-the-loop fine-tuning attains high dexterity yet forgoes an explicit visual world model (Luo et al., 2025; Chi et al., 2025b)."
        },
        {
            "title": "3 METHOD",
            "content": "Our method is two-stage 4D Generative World Model designed to synthesize geometrically consistent wrist-view videos from third-person observations. The first reconstruction stage estimates wrist poses and generates condition maps via point cloud projection. The second generation stage synthesizes temporally coherent wrist-view sequences conditioned on these maps and enriched by semantic guidance. The overall framework is illustrated in Figure 2. 3.1 PRELIMINARY Video Diffusion Models. Recent advances in video synthesis are largely driven by diffusionbased generative models. video = {xt}T t=1 is first compressed into latent representation t=1 RT CHW using video VAE, which reduces spatial and temporal resolution Z0 = {zt}T while preserving content semantics. The diffusion framework then defines forward noising process that gradually perturbs Z0 into Gaussian noise, and learned denoising model ϵθ that reverses this process. At training time, the objective is to predict the added noise given the noisy latent Zτ at step τ and the conditioning signal S: Ldiff = EZ0, ϵ, τ ϵ ϵθ(Zτ , τ c)2 2 , where ϵ (0, I) and Zτ is obtained by variance-scheduled corruption of Z0. In Diffusion Transformer (DiT), conditioning is typically realized as text embeddings, which are projected into conditioning tokens and injected into the transformer blocks, thereby guiding the denoising process. Visual Geometry Models. To capture multi-view geometry and establish dense cross-view correspondences, we build on VGGT (Wang et al., 2025b), large Transformer that encodes multicamera observations into fused features and predicts 3D quantities such as point clouds and correspondences. Given query point uj in image Iq, the matching head predicts corresponding points Figure 2: Overview of our method. We introduce two-stage 4D Generative World Model. In the reconstruction stage, VGGT Wang et al. (2025b) is extended with wrist head to regress wrist pose, guided by Spatial Projection Consistency Loss that supervises directly from RGB without depth or extrinsics. The predicted pose projects point clouds into the wrist view. In the generation stage, these projections, combined with external-view CLIP embeddings, condition video generator to synthesize wrist-view sequences. Without first-frame guidance, the model produces additional wrist views for VLA datasets, yielding substantial performance gains. }N {ˆuj query points. The resulting correspondences are i=1 in other views {Ii}, where is the number of anchor views and the number of sampled = {(uj q, ˆuj )}j=1,...,M i=1,...,N , providing dense pixel-level matching across anchor and wrist views. We adopt the pinhole camera model, where 3D point ˆy R3 projects to pixel R2 via camera intrinsics K, extrinsics (R, T), and projection function Π(): = Π(K, R, T, ˆy). 3.2 RECONSTRUCTION STAGE Wrist Head Design. To estimate the wrist-mounted viewpoint, we extend VGGT with specialized wrist head. Based on the aggregated multi-view features F, we introduce set of learnable wrist queries that attend to these tokens through Transformer decoder. The wrist head regresses the wrist camera extrinsics, denoted as rotation Rw SO(3) and translation Tw R3: (Rw, Tw) = WristHead(F, qw), where qw are the wrist queries. This design allows the model to capture hand-centered motion and implicit camera pose even when wrist-view data is unavailable. Spatial Projection Consistency Loss. Direct supervision of wrist extrinsics or depth maps is often missing. To address this, we propose Spatial Projection Consistency (SPC) loss that enforces 4 geometric consistency from RGB correspondences alone. As shown in Figure 3, given dense 2D 2D correspondences = {(uj j=1 between an anchor view Iq and the wrist view Iw, and reconstructed point cloud = {ˆyk} from anchor views, we associate each anchor pixel uj with its corresponding 3D point ˆyj Y. This yields set of 3D2D pairs w)}M q, ˆuj = {(ˆyj, ˆuj w)}M j=1, linking reconstructed world points to wrist-view pixels. For each pair (ˆyj, ˆuj by uj positive depth values and Sback for negative ones. The SPC loss consists of two terms: w), the projection of ˆyj under the predicted wrist pose (Rw, Tw) is denoted = Π(K, Rw, Tw, ˆyj), where is fixed by the dataset. We then divide points into Sfront for Lu = 1 Sfront (cid:88) ˆyj Sfront MSE(uj w, ˆuj w), Ldepth = 1 Sback (cid:88) zj. ˆyj Sback where zj is the depth value of point ˆyj in the wrist camera coordinate frame. Finally, the projection loss is defined as Lproj = λuLu + λdepthLdepth, where λu and λdepth control the balance between reprojection consistency and depth feasibility. Condition Map Generation. With the estimated wrist poses across frames, reconstructed 3D scenes are projected into the wrist-view image plane to form temporally aligned sequence of condition maps. These maps provide frameconsistent structural guidance for the subsequent video generation stage. 3.3 GENERATION STAGE Video Generation Model. We adopt DiT (Peebles & Xie, 2022) for video synthesis and introduce two targeted modifications. First, the conditioning comprises third-person CLIP features together with text embeddings to modulate the DiT. Second, we modify the patch embedding to ingest the concatenated latent stream t=1 RT 2CHW , exZ0 = {[ zt panding the standard input from (T, C, H, ) to (T, 2C, H, ). w; zt ]}T Spatial Projection Consistency Figure 3: (SPC) loss. We first establish anchorwrist 2D point matching and then lift the matched pixels to 2D3D correspondences using the reconstructed point cloud. The 3D points are subsequently projected into the wrist view with the predicted wrist pose, after which the SPC loss is computed to enforce geometric consistency. Wrist-View-Projection-Guided Generation. Since the projected condition maps are geometrically aligned with the wrist viewpoint, they directly provide spatial structure. We encode each wrist view projection Ct into latent representation zt using VAE, and concatenate it with the noisy wrist latent zt w: zt = [ zt ; zt ], which is then processed by the video generation model. This integration allows the model to synthesize temporally coherent wrist-view sequences that remain consistent with 3D geometry. CLIP-Encoded Anchor-View Semantics. Because condition maps may miss global semantics (e.g., small or blurred objects from point-cloud projection), we introduce an external semantic pathway. Each third-person frame from anchor views is encoded by CLIP image encoder to obtain per-frame, per-view features: Eclip = (cid:8) et,i (cid:9) i=1,...,N ; t=1,...,T R(N )dc, where et,i denotes the CLIP embedding of the t-th frame from the i-th external view. text prompt is also encoded, and both modalities are projected into shared conditioning space: Eclip = Wc Eclip, Etext = Wt Etext. 5 Figure 4: Visualization of our generation result. As illustrated in the figure, we compare our generated condition maps against the 3D Base (VGGT without the SPC Loss), where our approach demonstrates superior viewpoint consistency. Furthermore, in comparison to the WoW 14B (Chi et al., 2025c) baseline which based on Wan 14B (Wan et al., 2025), our method achieves both higher generation quality and improved viewpoint alignment accuracy. These results highlight the effectiveness of our framework and underscore its potential to serve as training data for downstream VLA models. Method Wrist RGB First Frame VGGT (Wang et al., 2025b) Pix2Pix (Isola et al., 2017) WoW 1.3B (Chi et al., 2025c) SVD (Blattmann et al., 2023) Cosmos-Predict2 (NVIDIA, 2025) WoW 14B (Chi et al., 2025c) Ours Droid Franka Panda FVD LPIPS SSIM PSNR FVD LPIPS SSIM PSNR - - 1142.15 2005.44 1990.72 935.03 421.10 0.74 0.55 0.61 0.56 0.51 0. 0.39 0.28 0.58 0.46 0.50 0.56 0.54 0.64 9.56 12.81 10.08 11.12 12.74 11.98 14.78 - - 1944.59 1354.56 1156.69 985. 231.43 0.73 0.58 0.72 0.60 0.65 0.59 0.33 0.49 0.71 0.53 0.68 0.67 0.68 0.78 12.05 15.60 10.45 14.10 12.59 13. 17.84 Table 1: Quantitative comparison on Droid and our Franka Panda. Green rows denote methods without wrist-view input, yellow rows require wrist-view first frame, and red highlights our method. Our method achieves the best performance across all metrics without first-frame guidance. We then form the conditioning tokens by concatenating along the token dimension: = (cid:2) Eclip + p1:T temporal are temporal embeddings, p1:N temporal + p1:N where p1:T view are view-identity embeddings distinguishing the anchor views, which are both learnable parameters. And ptext is text-token positional embedding. The resulting injects global semantics into the video generation process. view ; Etext + ptext (cid:3),"
        },
        {
            "title": "4 EXPERIMENT",
            "content": "4.1 IMPLEMENTATION DETAILS Dataset. We conduct experiments on three sources of data: 1. Droid (Khazatsky et al., 2024). The Droid dataset is large-scale robotics video corpus with about 76k videos covering 59 diverse manipulation tasks. Each video is recorded at 1280720 resolution from multiple static viewpoints, including ext1, ext2, and wrist-mounted camera. For pretraining, we sample 10k subset and use two anchor views as model inputs. For evaluation, we additionally hold out 100 videos as validation set. 6 Method Inputs MDT (Reuss et al., 2024) HULC++ (Mees et al., 2023) VPP (Hu et al., 2024) Static RGB + Gripper RGB Static RGB + Gripper RGB Static RGB + Gripper RGB SuSIE (Black et al., 2023) TaKSIE (Kang & Kuo, 2025) VPP + VGGT (Wang et al., 2025b) VPP VPP + Ours Static RGB Static RGB Static RGB Static RGB Static RGB 1/5 93.7% 93% 94.9% 87.7% 90.4% 91.8% 91.2% 2/5 84.5% 79% 86.8% 67.4% 73.9% 79.9% 82.2% 3/5 74.1% 64% 80.4% 49.8% 61.7% 65.4% 73.2% 4/5 64.4% 52% 72.9% 41.9% 51.2% 54.3% 65.2% 5/5 Avg. Len. 55.6% 40% 65.4% 33.7% 40.8% 44.1% 55.4% 3.72 3.30 4. 2.80 3.18 3.33 3.67 92.9% 1.7 84.2% 2.0 75.4% 2.2 67.6% 2.4 60.4% 5. 3.81 0.14 Table 2: VLA performance on the Calvin (Mees et al., 2022) benchmark with and without wristview generation. We use the Video Prediction Policy (VPP) (Hu et al., 2024) as our VLA. In Calvin, each episode consists of five sequential tasks and terminates once failure occurs. The columns 1/55/5 report success rates of completing at least tasks in sequence, while Avg. Len. denotes the mean number of tasks completed per episode. Inputs Anchor + Wrist Anchor Anchor + Ours Gen Wrist Close the upper drawer Pick bread and place into drawer Pick up the milk 80.0% 60.0% 73.3% 13.3 73.3% 40.0% 53.3% 13. 46.7% 13.3% 33.3% 20.0 Mean 66.7% 37.8% 53.3% 15.5 Table 3: VLA performance on our Franka Panda dataset with and without wrist-view generation. 2. Calvin (Mees et al., 2022). To benchmark vision-language-action learning in simulation, we adopt the Calvin environment. Calvin provides multi-view demonstrations across multiple task splits; in this work we focus on the split and use 10% of the data for training. For downstream VLA models, we follow the standard Task configuration for training and evaluation. 3. Franka Panda. Beyond simulation, we collect 1700 demonstrations on real Franka Panda manipulator. Our setup includes three static cameras (left, right, top) and one wrist-view camera. Videos are captured at 30 fps and downsampled temporally by factor of three. For evaluation, we hold out 100 videos from this collection. Training. Our framework is trained in two stages: reconstruction and video generation. During pretraining on the Droid dataset, the reconstruction stage is optimized on 8A800 GPUs for 12 hours with batch size of 4 and resolution of 640480, followed by the video generation stage on 8A800 GPUs for 24 hours using condition token length of 512. Building upon this, we perform cross-view fine-tuning with the Franka demonstrations. The reconstruction stage fine-tuning requires 8 GPUs for 6 hours, while the generation stage is trained on 8 GPUs for 12 hours under the same batch size, resolution, and token length settings. Model. Our framework consists of two stages: reconstruction and generation. In the reconstruction stage, VGGT backbone encodes multi-view features, and dedicated wrist head predicts wrist camera parameters through attention-based token fusion and transformer refinement, supervised by projection and optional L1 losses. In the generation stage, We use the VAE (Kingma & Welling, 2013) compresses frames into latent sequences, while the DiT applies spatio-temporal attention on tokenized latents. Conditioned jointly on text and visual features, the DiT generates geometrically consistent and temporally coherent videos. 4.2 VIDEO GENERATION QUANTITATIVE EVALUATION We compare against recent baselines on Droid and Franka-Panda (Tab. 1). Our method surpasses prior work across all metrics (FVD (Unterthiner et al., 2019), LPIPS (Zhang et al., 2018), SSIM (Wang et al., 2004), PSNR) without first-frame guidance, achieving large FVD gains (temporal coherence) and improved perceptual and structural fidelity. Even on the challenging Franka-Panda dataset with viewpoint variation, our framework consistently outperforms all baselines. Qualitative results are shown in Fig. 4, Fig. 5, and Fig. 6. In Fig. 4, our wrist view projection yield more viewpoint-aligned generations than both the 3D Base and WoW 14B. On Calvin  (Fig. 5)  , our approach improves spatial/viewpoint consistency over SVD. On Franka Panda data  (Fig. 6)  , our wrist-view generations closely match ground truth, showing strong third-to-wrist generalization. 7 Wrist view Projection Ext view clip embedding SPC Loss FVD LPIPS SSIM PSNR 3091.74 790.10 474.32 421.10 0.74 0.59 0.44 0.39 0.55 0.47 0. 0.64 10.42 10.75 13.67 14.78 Table 4: Ablation on wrist view projection, clip embeddings, and SPC loss. Figure 5: Visualization on the Calvin (Mees et al., 2022) benchmark. We compare our generated wrist-view videos (bottom row) with the ground truth (second row) and baseline method (third row, Stable Video Diffusion (Blattmann et al., 2023)). Our approach achieves better spatial and viewpoint consistency than the baseline, while also producing more faithful wrist-view frames. These results highlight the effectiveness of our method in bridging anchor-view and wrist-view perspectives. Together, these qualitative and quantitative results highlight that our generated videos not only serve as high-quality reconstructions but also provide valuable data for downstream VLA models. 4.3 DATA-DRIVEN VLA ENHANCEMENT We evaluate whether synthesized wrist-view videos improve visionlanguageaction (VLA) policies. Our framework generates wrist-view sequences from anchor-view rollouts and augments the training data of an unchanged VLA model (Video Prediction Policy, VPP (Hu et al., 2024)), without adding demonstrations, losses, or architectural changes. On Calvin, this augmentation increases average task completion length by 3.81%, narrows the anchorwrist gap by 42.4%, and improves full five-task completion by 5%. These results show that wrist-view generation provides effective supervisory signals and yields measurable VLA gains without extra data collection. Similarly, on Franka-Panda demonstrations (Tab. 3), generated wrist views consistently improve task performance, confirming that the synthetic data is realistic and beneficial. Overall, wrist-view generation enriches robot datasets and yields measurable VLA gains. 4.4 PLUG-AND-PLAY EXTENSION TO SINGLE-VIEW WORLD MODELS Our framework serves as plug-and-play add-on to an existing single-view world model (SVWM) while leaving the SVWM unchanged. In the baseline, the SVWM takes an anchor-view first frame and predicts an anchor-view rollout; with our module, that rollout is then converted into temporally 8 Figure 6: Visualization on Franka real-robot data. Multiple input anchor views (left) are used to generate wrist-view sequences (top right), which are compared with ground-truth wrist observations (bottom right). Our approach yields highly consistent predictions that closely match real data, demonstrating strong generalization from third-view to wrist-view perspectives. Method Ours Ours Input Left, right, top view videos Left view videos FVD 231.43 234.30 LPIPS SSIM PSNR 0.33 0. 0.78 0.78 17.84 18.13 Cosmos WoW 14B Ours + Cosmos Ours + WoW 14B Wrist view first frame Wrist view first frame Left view first frame Left view first frame 1156.69 985.99 467.19 689.50 455.57 530.42 0.65 0.59 0.58 0.07 0.57 0. 0.67 0.68 0.70 0.03 0.71 0.03 12.59 13.93 14.66 2.07 14.60 0.67 Table 5: Plug-and-play extension to single-view world models. Our framework enhances models trained solely on external viewpoints by synthesizing virtual wrist-view videos. We adopt CosmosPredict2 (NVIDIA, 2025) as the baseline Single-view World Model(SWM) and observe substantial gains. This plug-and-play design improves spatial consistency and perceptual quality across different baselines, while still delivering high-quality results with fewer anchor views. aligned wrist-view video without wrist first frame required. This post-hoc wrist synthesis resolves the core hurdle for multi-view extension, namely cross-view consistency without wrist initialization, while enriching the observation space without additional data. As shown in Tab. 5, integrating our module with WoW (Chi et al., 2025c) improves spatial and perceptual fidelity, and the gains persis 4.5 ABLATION STUDY We conduct an ablation study to disentangle the contribution of different components, as summarized in Tab. 4. The results show that the wrist view projection plays the most critical role: removing it leads to drastic drop in video quality, as reflected by large increase in FVD and degraded perceptual metrics. Moreover, the SPC loss proves essential for ensuring that the condition map carries accurate guidance information; without it, the model struggles to align wrist-view synthesis with external observations. The combination of condition map, external-view embeddings, and tracking loss yields the best performance across all metrics. Although experiments without projection embeddings are still in progress, we estimate that their performance will be similarly poor, further reinforcing the necessity of projection-based conditioning for coherent multi-view generation."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we introduced WristWorld, two-stage 4D framework for synthesizing geometrically and temporally consistent wrist-view videos from anchor-view inputs. In the reconstruction stage, 9 wrist head and SPC loss augment geometric transformer to estimate poses and generate condition maps without explicit wrist supervision. These maps are then fused with CLIP semantics and text guidance in diffusion transformer, producing high-fidelity wrist-view sequences aligned with geometry and task semantics. Experiments on Calvin, Droid, and Franka Panda validate our framework: WristWorld achieves strong video generation across metrics and visual quality, while the synthesized wrist-view videos significantly boost downstream VLA learning. By augmenting datasets with wrist-centric views, WristWorld bridges the exocentricegocentric gap and provides scalable, data-driven solution for robotic training."
        },
        {
            "title": "REFERENCES",
            "content": "Kevin Black, Mitsuhiko Nakamoto, Pranav Atreya, Homer Walke, Chelsea Finn, Aviral Kumar, and Sergey Levine. Zero-shot robotic manipulation with pretrained image-editing diffusion models. arXiv preprint arXiv:2310.10639, 2023. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets, 2023. Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, et al. Gr-2: generative video-language-action model with webscale knowledge for robot manipulation. arXiv preprint arXiv:2410.06158, 2024. Xiaowei Chi, Chun-Kai Fan, Hengyuan Zhang, Xingqun Qi, Rongyu Zhang, Anthony Chen, ChiMin Chan, Wei Xue, Qifeng Liu, Shanghang Zhang, et al. Empowering world models with reflection for embodied video prediction. In Forty-second International Conference on Machine Learning, 2025a. Xiaowei Chi, Kuangzhi Ge, Jiaming Liu, Siyuan Zhou, Peidong Jia, Zichen He, Yuzhen Liu, Tingguang Li, Lei Han, Sirui Han, Shanghang Zhang, and Yike Guo. Mind: Learning dualsystem world model for real-time planning and implicit risk analysis, 2025b. URL https: //arxiv.org/abs/2506.18897. Xiaowei Chi, Peidong Jia, Chun-Kai Fan, Xiaozhu Ju, Weishi Mi, Kevin Zhang, Zhiyuan Qin, Wanxin Tian, Kuangzhi Ge, Hao Li, et al. Wow: Towards world omniscient world model through embodied interaction. arXiv preprint arXiv:2509.22642, 2025c. Embodiment Collaboration, Abby ONeill, Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Andrey Kolobov, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Felipe Vieira Frujeri, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guangwen Yang, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik Christensen, Hiroki Furuta, Homanga Bharadhwaj, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jay Vakil, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin 10 Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi \"Jim\" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Muhammad Zubair Irshad, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag Sanketi, Patrick \"Tree\" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto MartínMartín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shubham Tulsiani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vikash Kumar, Vincent Vanhoucke, Vitor Guizilini, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiangyu Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yansong Pang, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yongqiang Dou, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, and Zipeng Lin. Open x-embodiment: Robotic learning datasets and rt-x models, 2025. Yanbo Ding, Xirui Hu, Zhizhi Guo, Chi Zhang, and Yali Wang. Mtvcrafter: 4d motion tokenization for open-world human image animation. arXiv preprint arXiv:2505.10238, 2025. Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation. Advances in neural information processing systems, 36:91569172, 2023. F. Ebert, C. Finn, A. X. Lee, and S. Levine. BAIR robot pushing dataset. Dataset, 2024. URL https://doi.org/10.57702/hpe85xky. Self-supervised visual planning with temporal skip connections. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance, 2022. URL https://arxiv. org/abs/2207.12598. Jie Hu, Shizun Wang, and Xinchao Wang. Pe3r: Perception-efficient 3d reconstruction. arXiv preprint arXiv:2503.07507, 2025. Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, and Jianyu Chen. Video prediction policy: generalist robot policy with predictive visual representations. arXiv preprint arXiv:2412.14803, 2024. Siyuan Huang, Liliang Chen, Pengfei Zhou, Shengcong Chen, Zhengkai Jiang, Yue Hu, Yue Liao, Peng Gao, Hongsheng Li, Maoqing Yao, et al. Enerverse: Envisioning embodied future space for robotics manipulation. arXiv preprint arXiv:2501.01895, 2025. Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei Efros. Image-to-image translation with conditional adversarial networks. CVPR, 2017. Vidhi Jain, Maria Attarian, Nikhil Joshi, Ayzaan Wahid, Danny Driess, Quan Vuong, Pannag Sanketi, Pierre Sermanet, Stefan Welker, Christine Chan, et al. Vid2robot: End-to-end videoconditioned policy learning with cross-attention transformers. arXiv preprint arXiv:2403.12943, 2024. Joel Jang, Seonghyeon Ye, Zongyu Lin, Jiannan Xiang, Johan Bjorck, Yu Fang, Fengyuan Hu, Spencer Huang, Kaushil Kundalia, Yen-Chen Lin, et al. Dreamgen: Unlocking generalization in robot learning through neural trajectories. arXiv e-prints, pp. arXiv2505, 2025. Xuhui Kang and Yen-Ling Kuo. Incorporating task progress knowledge for subgoal generation in robotic manipulation through image edits. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pp. 74907499. IEEE, 2025. Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty ElarXiv preprint lis, et al. Droid: large-scale in-the-wild robot manipulation dataset. arXiv:2403.12945, 2024. Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Ying Li, Xiaobao Wei, Xiaowei Chi, Yuming Li, Zhongyu Zhao, Hao Wang, Ningning Ma, Ming Lu, and Shanghang Zhang. Manipdreamer3d : Synthesizing plausible robotic manipulation video with occupancy-aware 3d trajectory, 2025. URL https://arxiv.org/abs/2509.05314. Yue Liao, Pengfei Zhou, Siyuan Huang, Donglin Yang, Shengcong Chen, Yuxin Jiang, Yue Hu, Jingbin Cai, Si Liu, Jianlan Luo, Liliang Chen, Shuicheng Yan, Maoqing Yao, and Guanghui Ren. Genie envisioner: unified world foundation platform for robotic manipulation, 2025. URL https://arxiv.org/abs/2508.05635. Jia-Wei Liu, Weijia Mao, Zhongcong Xu, Jussi Keppo, and Mike Zheng Shou. Exocentric-toegocentric video generation. Advances in Neural Information Processing Systems, 37:136149 136172, 2024. Jianlan Luo, Charles Xu, Jeffrey Wu, and Sergey Levine. Precise and dexterous robotic manipulation via human-in-the-loop reinforcement learning. Science Robotics, 10(105):eads5033, 2025. Yunhao Luo and Yilun Du. Grounding video models to actions through goal conditioned exploration. arXiv preprint arXiv:2411.07223, 2024. Ajay Mandlekar, Yuke Zhu, Animesh Garg, Jonathan Booher, Max Spero, Albert Tung, Julian Gao, John Emmons, Anchit Gupta, Emre Orbay, Silvio Savarese, and Li Fei-Fei. Roboturk: crowdsourcing platform for robotic skill learning through imitation. CoRR, abs/1811.02790, 2018. URL http://arxiv.org/abs/1811.02790. Oier Mees, Lukas Hermann, Erick Rosete-Beas, and Wolfram Burgard. Calvin: benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks. IEEE Robotics and Automation Letters, 7(3):73277334, 2022. Oier Mees, Jessica Borja-Diaz, and Wolfram Burgard. Grounding language with visual affordances over unstructured data. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), London, UK, 2023. NVIDIA. Cosmos-predict2: World foundation models for physical ai. GitHub repository, 2025. URL https://github.com/nvidia-cosmos/cosmos-predict2. William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. Han Qi, Haocheng Yin, Aris Zhu, Yilun Du, and Heng Yang. Strengthening generative robot policies through predictive world modeling. arXiv preprint arXiv:2502.00622, 2025. Moritz Reuss, Ömer Erdinç Yagmurlu, Fabian Wenzel, and Rudolf Lioutikov. Multimodal arXiv preprint diffusion transformer: Learning versatile behavior from multimodal goals. arXiv:2407.05996, 2024. Achint Soni, Sreyas Venkataraman, Abhranil Chandra, Sebastian Fischmeister, Percy Liang, Bo Dai, and Sherry Yang. Videoagent: Self-improving video generation for embodied planning. In Workshop on Reinforcement Learning Beyond Rewards@ Reinforcement Learning Conference 2025, 2025. 12 Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges, 2019. URL https://arxiv.org/abs/1812.01717. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models, 2025. Boyang Wang, Nikhil Sridhar, Chao Feng, Mark Van der Merwe, Adam Fishman, Nima Fazeli, and Jeong Joon Park. This&that: Language-gesture controlled video generation for robot planning. In 2025 IEEE International Conference on Robotics and Automation (ICRA), pp. 1284212849. IEEE, 2025a. Chen Wang, Linxi Fan, Jiankai Sun, Ruohan Zhang, Li Fei-Fei, Danfei Xu, Yuke Zhu, and Anima Anandkumar. Mimicplay: Long-horizon imitation learning by watching human play. arXiv preprint arXiv:2302.12422, 2023. Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025b. Jiayu Wang, Aws Albarghouthi, and Frederic Sala. Cosmos: Predictable and cost-effective adaptation of llms, 2025c. URL https://arxiv.org/abs/2505.01449. Shizun Wang, Xingyi Yang, Qiuhong Shen, Zhenxiang Jiang, and Xinchao Wang. Gflow: Recovering 4d world from monocular video. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 78627870, 2025d. Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600612, 2004. doi: 10.1109/TIP.2003.819861. Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, and Tao Kong. Unleashing large-scale video generative pre-training for visual robot manipulation. arXiv preprint arXiv:2312.13139, 2023. Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. Yanjie Ze, Ge Yan, Yueh-Hua Wu, Annabella Macaluso, Yuying Ge, Jianglong Ye, Nicklas Hansen, Li Erran Li, and Xiaolong Wang. Multi-task real robot learning with generalizable neural feature fields. CoRL, 2023. Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric, 2018. URL https://arxiv.org/abs/ 1801.03924. Haoyu Zhen, Qiao Sun, Hongxin Zhang, Junyan Li, Siyuan Zhou, Yilun Du, and Chuang Gan. Tesseract: Learning 4d embodied world models. arXiv preprint arXiv:2504.20995, 2025. URL https://arxiv.org/abs/2504.20995. Preprint. Siyuan Zhou, Yilun Du, Jiaben Chen, Yandong Li, Dit-Yan Yeung, and Chuang Gan. Robodreamer: Learning compositional world models for robot imagination. arXiv preprint arXiv:2404.12377, 2024."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 DATASETS Calvin. The Calvin benchmark (Mees et al., 2022) (Composing Actions from Language and Vision) is simulated environment designed for studying long-horizon, language-conditioned robotic manipulation. It defines set of five core manipulation tasksopening and closing drawers, switching lights, placing objects in containers, pushing blocks, and rotating knobswhich can be composed into multi-step instructions. Each demonstration is collected via teleoperation in simulation and includes synchronized RGB observations from multiple static cameras, robot states, low-level actions, and the corresponding language command. The benchmark further provides different environment variations, enabling controlled evaluations of generalization to new objects, layouts, and task compositions. CALVIN thus serves as standardized testbed to validate whether synthesized wrist-view videos can enhance task-conditioned learning in controlled simulation setting. Droid. The Droid dataset (Khazatsky et al., 2024) (Distributed Robot Interaction Dataset) is one of the largest collections of real-world robotic demonstrations. It contains approximately 76k trajectories spanning 350 hours, collected across 564 unique scenes and 86 manipulation tasks by over 50 contributors from multiple institutions. To ensure consistency, each robot setup follows unified configuration: Franka Panda 7-DoF manipulator, two ZED 2 stereo cameras for external anchor views, and ZED Mini wrist-mounted camera for egocentric observations, along with standardized calibration. Each trajectory records synchronized multi-view RGB-D streams, camera intrinsics and extrinsics, joint states, and control commands. The scale and diversity of DROID make it challenging but rich source of anchor-view data, while the presence of dedicated wrist camera offers an opportunity to validate generated views against ground-truth egocentric observations. Franka Panda demonstrations. To further evaluate our method on controlled real-robot setup, we collected dataset of 1.7 demonstrations using Franka Panda manipulator. The hardware setup mirrors that of DROID, with multiple fixed anchor cameras and wrist-mounted camera, but data collection was conducted entirely in-house. Demonstrations span diverse manipulation skills such as grasping, transporting, and placing objects under natural occlusions from the robot arm. For each sequence, we log synchronized anchor-view and wrist-view videos, as well as proprioceptive states and control commands. While smaller in scale than DROID, this dataset captures calibration imperfections and actuation variability, making it particularly valuable for fine-tuning and validating wrist-view generation under true physical dynamics. A.2 TRAINING AND IMPLEMENTATION DETAILS This section provides the full setup for both stages of WristWorld in the 1.3B configuration, with emphasis on novel components and implementation details. We also include short explanations of key terms to aid readers unfamiliar with video generation or geometric reconstruction. Implementation Details. The wrist head is implemented as lightweight transformer decoder with 3 layers, 8 attention heads per layer, and an embedding dimension of 1024. It attends to aggregated multi-view tokens from VGGT and directly regresses wrist camera extrinsics (Rw, Tw). The Spatial Projection Consistency (SPC) loss is computed using dense 2D2D correspondences. For each 3D point ˆyj, we project it into the wrist view as uj = Π(K, Rw, Tw, ˆyj), where is the dataset-provided intrinsics. We then split the projected points into Sfront = {ˆyj zj > 0} and Sback = {ˆyj zj < 0}, with zj denoting the depth value in the wrist frame. The loss is defined as Lu = 1 Sfront (cid:88) ˆyj Sfront and Lproj = λuLu + λdepthLdepth. MSE(uj w, ˆuj w), Ldepth = 1 Sback (cid:88) zj, ˆyj Sback For video generation, we use wrist-view projections as conditioning inputs. Each frame is encoded by VAE into latents zt and concatenated with the noisy wrist-view latents zt w. The patch embedding of the DiT is modified from standard 2 2 convolution, expanding input channels to 32 to accommodate concatenated latents. CLIP features from anchor views are projected from 512 dimensions into the DiT token space, while text embeddings from T5 are 4096-dimensional. Temporal and view embeddings are added to capture sequence alignment and camera identity. Reconstruction stage (VGGT + Wrist Head). We adopt VGGT-1B with point, depth, and camera heads (frozen) and train new wrist head. Training images are resized to 518518, and intrinsics are scaled accordingly. Two anchor views (ext1, ext2) are inputs, and the wrist pose is predicted relative to ext1. SPC loss enforces consistency: visible points are supervised with normalized reprojection error, while back-facing points are penalized if their predicted depth is negative. Component Setting VGGT-1B 518 518 Backbone Image size Token aggregation Attention-based 3 layers, 8 heads Wrist decoder AdamW (wd = 0.05) Optimizer 2 105 (cosine decay) Learning rate 4 per GPU, grad accum = 3 Batch size 8 A800 GPUs Hardware 12h pretrain, 6h finetune Training time Table 6: Reconstruction stage hyperparameters. Generation stage (Video DiT). We build on Wan 1.3B DiT, which is diffusion transformer pretrained for text-to-video. Condition maps are encoded with VAE and concatenated with noisy wrist-view latents. CLIP embeddings from anchor views are projected into the text space and added as pseudo tokens, together with temporal and view embeddings. maximum of 512 conditioning tokens is used. Classifier-Free Guidance (CFG) is set to 5.0. Component Setting Backbone Resolution Patch in-channels LoRA config Condition tokens CFG Optimizer Batch size Hardware Training time Wan 1.3B DiT 640 480 (latent scale 1/8) Expanded to 32 (for concat latents) Rank 4, α = 4, targets {q,k,v,o,ffn} 512 total (CLIP + text + temporal/view) 5.0 AdamW, lr = 1 105 4 per GPU 8 A800 GPUs 24h pretrain, 12h finetune Table 7: Generation stage hyperparameters. A.3 VISUALIZATION To assess appearance fidelity and viewpoint stability under realistic manipulation, we run inference on Franka Panda platform for all compared video generation methods, including ours, using identical input trajectories and pre-processing. For each generated clip, we visualize the middle frame so as to emphasize long-horizon behavior and to minimize the bias introduced by the initial condition. The compared models comprise Pix2Pix (Isola et al., 2017), SVD (Blattmann et al., 2023), Wan 1.3B (Wan et al., 2025), Wan 14B, Cosmos-Predict2 (NVIDIA, 2025), and Ours. Following common practice for controllable synthesis, SVD, Wan 14B, and Cosmos-Predict2 are evaluated CFG balances diversity and fidelity in diffusion sampling by mixing conditional and unconditional generations. higher value yields sharper but less diverse outputs (Ho & Salimans, 2022). Figure 7: Qualitative visualization on Franka Panda. For each method, we generate videos and visualize the middle frame of each sequence to probe long-horizon stability. Rows denote methods (the top row shows ground-truth wrist-camera frames), and columns denote different manipulation scenes from the Franka Panda setup. Our approach maintains superior geometric consistency (crisp boundaries, coherent occlusions, stable perspective/scale) and wrist-following behavior (viewpoint motion aligned with end-effector motion and consistent object-relative poses) compared with prior models. with first-frame guidance, while the remaining methods use their public default settings. Qualitative results are shown in Fig. 7. Across diverse scenes, our method exhibits the strongest geometric consistency and wristfollowing capability. By geometric consistency we refer to coherent object shapes and boundaries, stable perspective and scale, and physically plausible occlusions (e.g., handobject and objecttable contacts). By wrist-following we mean that the synthesized wrist-camera view remains aligned with the end-effector motion, yielding stable object-relative poses and camera parallax over time. In contrast, baselines frequently suffer from accumulated drift in the middle of the sequence: textures smear or dissolve, straight edges warp, object scale fluctuates, and the rendered viewpoint decouples from the manipulator pose, producing noticeable misalignment between the camera motion and the underlying action. Methods that rely on first-frame guidance preserve appearance early on but tend to exhibit background and pose drift as the sequence proceeds; methods without guidance avoid overfitting to the first frame but often display ghosting and fine-detail loss under fast motions or partial occlusions. While our approach may still blur very small, fast-moving details in rare cases, its overall spatial coherence and cameramotion coupling are substantially more reliable, which agrees with the improvements observed in video quantitative metrics. A.4 FRANKA PANDA SETTING As shown in Figure. 8, we employ Franka Emika Panda manipulator in our experimental setup, augmented with multiple Intel RealSense cameras to provide diverse visual perspectives. Specifically, wrist-mounted camera enables close-up views of the manipulation workspace, while topmounted camera captures overhead information. In addition, left and right side cameras provide complementary viewpoints for robust perception. This multi-view sensing arrangement facilitates both accurate 3D reconstruction and reliable visual feedback for manipulation tasks. Figure 8: Experimental setup with the Franka Panda manipulator. The system is equipped with multiple Intel RealSense cameras: wrist-mounted, top, left, and right. This configuration enables multi-view visual input for robust perception and manipulation."
        }
    ],
    "affiliations": [
        "Beijing Innovation Center of Humanoid Robotics",
        "Hong Kong University of Science and Technology",
        "National University of Singapore",
        "State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University"
    ]
}