{
    "paper_title": "Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models",
    "authors": [
        "Ziwei Luo",
        "Ziqi Jin",
        "Lei Wang",
        "Lidong Bing",
        "Thomas B. Schön"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This work presents self-rewarding sequential Monte Carlo (SMC), an inference-time scaling algorithm enabling effective sampling of masked diffusion language models (MDLMs). Our algorithm stems from the observation that most existing MDLMs rely on a confidence-based sampling strategy, where only tokens with the highest prediction confidence are preserved at each step. This restricts the generation to a noise-sensitive, greedy decoding paradigm, resulting in an inevitable collapse in the diversity of possible paths. We address this problem by launching multiple interacting diffusion processes in parallel, referred to as particles, for trajectory exploration. Importantly, we introduce the trajectory-level confidence as a self-rewarding signal for assigning particle importance weights. During sampling, particles are iteratively weighted and resampled to systematically steer generation towards globally confident, high-quality samples. Our self-rewarding SMC is verified on various masked diffusion language models and benchmarks, achieving significant improvement without extra training or reward guidance, while effectively converting parallel inference capacity into improved sampling quality. Our code is available at https://github.com/Algolzw/self-rewarding-smc."
        },
        {
            "title": "Start",
            "content": "Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models Ziwei Luo 1 Ziqi Jin 2 3 Lei Wang 2 Lidong Bing 2 Thomas B. Schon 1 6 2 0 2 2 ] . [ 1 9 4 8 1 0 . 2 0 6 2 : r Abstract This work presents self-rewarding sequential Monte Carlo (SMC), an inference-time scaling algorithm enabling effective sampling of masked diffusion language models (MDLMs). Our algorithm stems from the observation that most existing MDLMs rely on confidence-based sampling strategy, where only tokens with the highest prediction confidence are preserved at each step. This restricts the generation to noise-sensitive, greedy decoding paradigm, resulting in an inevitable collapse in the diversity of possible paths. We address this problem by launching multiple interacting diffusion processes in parallel, referred to as particles, for trajectory exploration. Importantly, we introduce the trajectory-level confidence as self-rewarding signal for assigning particle importance weights. During sampling, particles are iteratively weighted and resampled to systematically steer generation towards globally confident, high-quality samples. Our self-rewarding SMC is verified on various masked diffusion language models and benchmarks, achieving significant improvement without extra training or reward guidance, while effectively converting parallel inference capacity into improved sampling quality. Our code is available at https://github. com/Algolzw/self-rewarding-smc. 1. Introduction Generative modeling with diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) has led to remarkable advances across wide range of applications, including image generation (Dhariwal & Nichol, 2021; Rombach et al., 2022; Peebles & Xie, 2023), text-to-image generation (Saharia et al., 2022; Zhang et al., 2023; Gu et al., 2022), and video synthesis (Ho et al., 2022; Blattmann et al., 2023). More 1Uppsala University, Sweden 2MiroMind AI, Singapore 3Nanyang Technological University, Singapore. Correspondence to: Lei Wang <lei.wang@miromind.ai>. Preprint. February 3, 2026. 1 recently, diffusion models have further shown strong potential for discrete data generation, particularly for text (Sahoo et al., 2024; Arriola et al., 2025; Nie et al., 2025; Ye et al., 2025), by modeling forward masking process and iteratively predicting masked tokens at inference (Lou et al., 2024; Chang et al., 2022). Despite this progress, existing masked diffusion language models (MDLMs) adopt greedy, confidence-based sampling strategy (Sahoo et al., 2024), in which only tokens with the highest prediction probability are preserved at each step, leading to myopic trajectory exploration and suboptimal generation performance. Inference-time scaling methods have been proposed to improve MDLMs sample diversity and quality without modifying pretrained models (Dang et al., 2025; Singhal et al., 2025). Typically, they leverage human preference guidance, such as promoting fluency, enforcing structured format, or controlling toxicity (Dathathri et al., 2019; Rafailov et al., 2023; Loula et al., 2025), to tilt the generation trajectories towards high-reward target distributions (Dang et al., 2025; Uehara et al., 2025). While their results are impressive, such methods rely on external reward signals that are often taskspecific and require hand-crafted tuning, which restricts their applicability to general MDLM-based generation. In this work, we revisit the confidence-based sampling of masked diffusion and propose self-rewarding sequential Monte Carlo (SMC) algorithm for inference-time scaling on general tasks. Specifically, we maintain set of interacting diffusion processes, referred to as particles, to explore multiple trajectories in parallel. These particles still follow the low-confidence remasking strategy of MDLMs but will be resampled based on their trajectory-level confidence, which is serving as an implicit reward signal to assign importance weights to each particle. Conceptually, our algorithm runs multiple generation processes, periodically duplicating promising candidates and discarding less confident ones based on their accumulated likelihood. The resulting selfrewarding SMC enables principled trajectory exploration and steers the sampling process towards stable, globally confident, and high-quality outputs, without extra reward models or task-specific design as guidance. Our method is verified on various masked diffusion language models including MDLM (Sahoo et al., 2024) and Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models BD3-LMs (Arriola et al., 2025), and diffusion large language models (dLLMs) including LLaDA 1.5 (Zhu et al., 2025) and Dream (Ye et al., 2025). The results show that the proposed self-rewarding SMC consistently improves the baseline models across multiple benchmarks. In summary, we make the follow contributions: inference, the reverse unmasking process is parametrized as pθ(xs xt, ˆx0), where ˆx0 Cat(x0; pθ(xt)) is sampled from the model predictive distribution. To simplify the notation, we only consider sampling from the diffusion reverse process and denote := t(i) as the discrete time step throughout the work. 1. We propose general, self-rewarding sequential Monte Carlo algorithm for masked diffusion language models. The proposed method improves sampling without extra training or reward signals. 2. We unify the sampling and remasking strategy of MDLMs from the probabilistic perspective, and theoretically show that the trajectory-level confidence is naturally self-rewarding signal for SMC. 3. Extensive experiments and analysis demonstrate our self-rewarding SMC improves sample quality on different pretrained models and benchmarks. 2. Background Notation We consider variables = {x1, . . . , xL} as sequence of tokens, where each token xℓ = x(ℓ) is one-hot column vector with categories in the space {x {0, 1}V : (cid:80)V i=1 x(i) = 1} for the simplex . We let the th category denote special [MASK] token, where is its one-hot vector. Moreover, we define Cat(; p) as categorical distribution with probability . 2.1. Masked Diffusion Models Given prior distribution Cat(; m), masked diffusion models (MDMs) (Austin et al., 2021; Arriola et al., 2025; Chang et al., 2022; Sahoo et al., 2024) are characterized by parametric models pθ trained to reverse forward masking process for new data sampling from full masked sequence. For finite-time process, we let be the number of diffusion steps and t(i) = [0, 1] be the continuous time. The marginal distribution of xt(i) conditioned on target data x0 is as follows (Sahoo et al., 2024): p(xt x0) = Cat(xt; αtx0 + (1 αt)m), (1) where αt denotes monotonically decreasing schedule satisfying α0 1 and α1 0, such that x1 Cat(; m). Let s(i) = i1 be the time step directly preceding t(i), the time-reversal conditional posterior for all [MASK] tokens i.e., xt = m, can be obtained by p(xs xt, x0) = Cat(xs; αs αt 1 αt x0 + 1 αs 1 αt xt). (2) Notably, unmasked tokens i.e., xt = m, remain unchanged in the reverse process. Since x0 is not available during 2.2. Importance Sampling and Sequential Monte Carlo Assume we want to estimate expectations under trajectory target distribution π(xt:T ), such as Eπ[f (xt:T )], where () is test function. Sampling from π is generally intractable. Importance sampling (IS) (Robert et al., 1999) introduces proposal distribution q(xt:T ) that allows the expectation to be rewritten as Eπ[f (xt:T )] = Eq[ π(xt:T ) q(xt:T ) (xt:T )]"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) wi tf (xi t:T ), i=1 (3) where xi t:T q(xt:T ) and π is the unnormalized density. = wi Moreover, wi is the normalized importance t:T )/q(xi weight, where wi t:T ) gauges the discrepancy between the target distribution and the proposal distribution. While conceptually simple, importance sampling often suffers from unfavorably high variance. t/(cid:80)N = π(xi j=1 wj Sequential Monte Carlo (SMC) (Del Moral et al., 2006) improves upon IS by introducing sequence of intermediate unnormalized path measures πt(xt:T ), whose terminal distribution coincides with the desired trajectory target distribution. SMC incorporates resampling and sequential weighting techniques across the trajectory, thereby reducing variance in practice. We begain by defining the incremental importance weights as wt1(xt1:T ) = πt1(xt1:T ) πt(xt:T ) qt1(xt1 xt) , (4) where qt1(xt1 xt) is Markovian sequential proposal operating in reverse time (see Appendix A.1 for more details). During sampling, we initialize set of particles xi qT (xT ), each representing trajectory distribution, with weights wi )/qT (xi ). At each iteration e.g., from to 1, SMC takes the follows three steps: = πT (xi i) Resample: resample ancestor {xi i=1; weights {wi t}N t}N i=1 according to the ii) Propagate: sample new particles from proposal distribution xi t1 qt1(xt1 xi t); iii) Re-weight: compute and accumulate the incremental weights in Eq. (4), and normalize wi t1 = t1 wi j=1 wj (cid:80)N t1 . The resulting collection of weighted particles provides an asymptotically consistent approximation of the trajectory target distribution. 2 Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models Figure 1. Illustrative example of text generation using (a) masked diffusion models and (b) our self-rewarding SMC framework. Here, represents [MASK] tokens and Resa. denotes resampling. SMC maintains multiple diffusion processes, called particles, to explore the sampling trajectories in parallel. At each iteration, we take three steps: resample, propagate, and re-weight, to perform as an interactive optimization process. Importantly, traditional diffusion sampling only considers token-level confidence, while our algorithm uses the trajectory-level confidence as importance weights, calculated using Eq. (13), to select globally confident outputs. 3. Self-Rewarding Sequential Monte Carlo 3.1. Reformulate the Sampling of MDMs We consider sampling from pretrained masked diffusion model pθ(xt). Given mask set Mt {ℓ : xt(ℓ) = [MASK]} at time t, recall that the learned posterior pθ(xt1 xt, ˆx0) in Eq. (2) is only applied for mask tokens i.e., xt1(j) pθ(xt1(j) xt, ˆx0), Mt, (5) function of t, or be more flexible by explicitly defining ρt as confidence threshold (Wu et al., 2025b), such that thr = { Mt : ct(j) ρt }, (8) which enables faster sampling while preserving performance when the model is confident in its predictions. In summary, the reverse transition distribution of each token xt(j) can be formulated by where ˆx0 Cat(x0; pθ(xt)) is sampled from the model predictive distribution. For each token xt1(j), we directly define its confidence as the model probability on j, as (cid:0)ˆx0(j) xt ct(j) := pθ Mt. (cid:1), (6) At each iteration, MDMs update subset St Mt following predefined policy, such as the low-confidence remasking strategy (Chang et al., 2022; Nie et al., 2025), to ensure an iterative unmasking process from xT to x0. Low-confidence remasking has been widely used in diffusion large language models as an efficient strategy for sequence generation. Typically, by defining schedule ρt to specify the number of tokens to be unmasked at step t, we introduce the following policy: top-k = { Mt : Top-ρt{ct(j)} }, (7) which indicates only the highest probability tokens are preserved at each step. Note that the schedule ρt can be scalar pθ(xt1(j) xt) = pθ(xt1(j) xt, ˆx0), Cat(xt1(j); m), Cat(xt1(j); xt), St, Mt St, / Mt, (9) where Cat(xt1(j); xt) is like Dirac delta distribution concentrated at xt(j). Accordingly, the reverse transition kernel over the full sequence xt is Kt(xt, xt1) = (cid:89) j=1 pθ(xt1(j) xt). (10) This transition kernel deterministically preserves unmasked tokens, remasks low-confidence tokens, and samples newly accepted tokens according to the model prediction. One problem of sampling from Eq. (9) is that only step-wise confidence i.e., ct in Eq. (6), is utilized for remasking. This often bias generation towards locally optimal tokens, inducing noise-sensitive, myopic exploration of the sequence trajectory, as illustrated in Figure 1. Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models 3.2. Confidence-based Sequential Monte Carlo Algorithm 1 Self-Rewarding SMC (SR-SMC) Assume that we have diffusion sampling processes, called particles, to generate sequences in parallel. To tilt sampling towards globally confident sequence generation, we define FeynmanKac model (Del Moral, 2004) with potential Gt1(xt, xt1) = (cid:89) jSt pθ(xt1(j) xt), (11) which is the joint probability of accepted tokens within the set St. Intuitively, the potential denotes how confident the model is in the tokens at step t, performing as selfrewarding signal for SMC update. In addition, we define the intermediate unnormalized path measures πt(xt:T ) to satisfy the following recursion: πt1(xt1:T ) = πt(xt:T ) Kt(xt, xt1) Gt1(xt, xt1), (12) where Kt(xt, xt1) is the reverse diffusion transition kernel in Eq. (10). Recall that SMC defines an incremental importance weight for each particle (see Eq. (4)). By letting its proposal equal the transition kernel in Eq. (12), we obtain the following result: Proposition 3.1. Given pretrained diffusion model pθ, let {πt(xt:T )}T t=0 denote the unnormalized path measures defined by the recursion in Eq. (12). If the sequential proposal in SMC is chosen to be the diffusion transition kernel, i.e., qt1(xt1 xt) = Kt(xt, xt1), then the incremental importance weights at step 1 is given by wt1(xt1:T ) = (cid:89) jSt ct(j), (13) (cid:1) is the token confidence and where ct(j) := pθ St denotes the selected mask subset to be updated at step t. (cid:0)ˆx0(j) xt The proof is provided in Appendix A.2. Under our SMC framework, Eq. (13) defines trajectory-level confidencebased weight, since it accumulates confidence scores across sampling steps until all tokens are unmasked. Require: Pretrained diffusion model pθ, sampling steps , number of particles , remasking policy SELECT(). Ensure: Generated sequence ˆx0. 1: Initialize particles i.e., sequences {xi set to [MASK], and weights wi = 1/N for all i. }N i=1 with all tokens sample xi 2: for = T, . . . , 1 do Resample {xi t}N 3: Propagate with mask set Mi 4: 5: 6: 7: 8: 9: end for 10: return ˆx0 xi 0 pθ(xi i) ii) select update set iii) sample xi t1 Kt(xi Re-weight by computing wi i=1 according to weights {wi t}N i=1. for all i: t) and compute confidence ci t. SELECT(ci t, Mi t). t1) using Eq. (9). t, xi t1 using Eq. (13). 0 where = arg maxi wi 0. resample, propagate, and re-weight as standard SMC described in Sec. 2.2. Specifically, the diffusion sampling with local confidence-based remasking is performed during propagation, denoted by the transition kernel in Eq. (9) and Eq. (10). Then each particle is reweighted according to Eq. (13), which forms trajectory-level confidence score for resampling. The final output is selected from the resulting particle set with the maximum weight. In addition, we use effective sample size (ESS) (Zheng et al., 2024) and Gumbel-Max trick (Zheng et al., 2024) to further improve the sample efficiency. Adaptive resampling. In practice, resampling at every diffusion step might be unnecessary in particular when the variance of weights wt is low. We therefore adopt an adaptive resampling strategy based on the effective sample size, which is defined as ESS = 1 i=1(wi t)2 (cid:80)N (14) We follow common practical setting (Doucet et al., 2001) to let resample be triggered only when ESS falls below N/2, which indicates significant weight degeneracy. Moreover, we note that this choice of proposal corresponds to bootstrap SMC scheme (Doucet et al., 2001), further showing that reweighting particles by trajectory-level confidence is not heuristic choice but follows naturally from the underlying diffusion-based formulation. Gumbel-Max sampling. Following Zheng et al. (2024), we employ the Gumbel-Max trick to sample discrete tokens from controlled categorical distribution for masked diffusion models. Particularly, given logits zℓ over the vocabulary, token sampling is performed as 3.3. Practical Sampling with Self-Rewarding SMC We now describe how Eq. (13) is incorporated into the sampling procedure of masked diffusion models via sequential Monte Carlo. As shown in Figure 1 and Algorithm 1, we begin by initializing particles that are fully masked sequences, with uniform weights. At each step, we perform = arg max ℓ (cid:0)zℓ/τ + gℓ (cid:1), gℓ G(0, 1), (15) where τ is temperature and denotes the Gumbel distribution. This Gumbel-Max trick approximates sampling from Cat(; softmax(z)). Note that τ = 0 means argmax and τ = 1 recovers the standard categorical sampling. 4 Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models 4. Experiment Our self-rewarding SMC is evaluated across multiple benchmarks to demonstrate its ability to improve sampling of pretrained diffusion language models. 4.1. Experimental Setup Pretrained models We investigate two kinds of pretrained models: 1) masked diffusion language models, including MDLM (Sahoo et al., 2024) and BD3-LMs (Arriola et al., 2025) pretrained on the OpenWebText (OWT) dataset (Gokaslan & Cohen, 2019) for sample quality evaluation; and 2) diffusion large language models including LLaDA 1.5 (Zhu et al., 2025) and Dream-7b (Ye et al., 2025). All of them are pretrained and finetuned on >2T tokens for general task evaluations. Both settings adopt semi-autoregressive (Semi-AR) generation structure for higher-quality sequence generation. In addition, all models except MDLM employ block-wise generation policy for more efficient sampling. Implementation For our self-rewarding SMC, the adaptive sample strategy is used in all experiments. More specifically we set the resample frequency to 128 for MDLM and BD3-LMs, and to per-block for all dLLMs. The default number of particles is set to 4. Moreover, we use temperature τ = 1 and identical decoding settings for all comparisons. For dLLMs experiments, we follow Wu et al. (2025b) to enable KV cache and parallel decoding (i.e., with threshold-based policy, see Eq. (8)) for inference acceleration. We test MDLM and BD3-LMs on single NVIDIA H200 GPU and evaluate all dLLMs experiments on 8 NVIDIA A800 GPUs with single batch size. 4.2. Sample Quality Evaluation We first investigate our self-rewarding SMC on pretrained masked diffusion language models (MDLM (Sahoo et al., 2024) and BD3-LMs (Arriola et al., 2025)) for text sample quality evaluation. The other baselines include Autoregressive (AR), SEDD (Lou et al., 2024), and SSD-LM (Han et al., 2023). In Table 1 and Figure 2, we generate sequences of lengths = 1024, 2048 and measure their generative perplexity under GPT2-Large. The results show that by scaling the inference compute with our SR-SMC, we significantly improve the sample quality for both diffusion and block diffusion baselines. The block diffusion variant of BD3-LMs with size = 4, 8 achieves generative perplexity below 20, substantially narrowing the performance gap between diffusion-based models and autoregressive baselines. To further assess the impact on sample diversity, we also report the corresponding entropy results of the generated texts in the Appendix  (Table 6)  . These results indicate that SR-SMC improves text quality while maintaining high output diverTable 1. Generative perplexity (Gen. PPL; ) and the number of function evaluations (NFEs; ) of 300 samples of lengths = 1024, 2048. All models are trained on the OWT dataset. For BD3-LMs (Arriola et al., 2025) and its SR-SMC implementation, we also report results with different block sizes L. We set the resample frequency to 128 for all our SR-SMC variants. Model Gen. PPL() NFEs Gen. PPL() NFEs = 1024 = 2048 Autoregressive Diffusion SEDD MDLM MDLM w/ SR-SMC Block Diffusion SSD-LM = 25 = 25 BD3-LMs = 16 = 8 = 4 BD3-LMs w/ SR-SMC = 16 = 8 = 4 14. 52.0 46.8 25.8 37.2 281.3 33.4 30.4 25.7 21.1 18.9 16.1 1K 1K 1K 4K 40K 1K 1K 1K 1K 4K 4K 4K 13.2 41.3 25.9 35.3 281. 31.5 28.2 23.6 20.2 17.3 15.1 2K 2K 8K 80K 2K 2K 2K 2K 8K 8K 8K Figure 2. Generative perplexity () comparison of our selfrewarding SMC and the corresponding baselines. sity, rather than collapsing generation toward low-entropy or overly greedy solutions. 4.3. Results on Diffusion Large Language Models To further evaluate the effectiveness and generalizability of our SR-SMC, we conduct experiments on two representative diffusion large language models: LLaDA-1.5 (Zhu et al., 2025) and Dream-7B (Ye et al., 2025). Following Wu et al. (2025b), we evaluate these models across four challenging benchmarks: GSM8K and MATH for mathematical reasoning, and HumanEval and MBPP for code generation. The performance is measured using two different generation lengths (L {256, 512}) with block size of 32. The results are summarized in Table 2. Overall, incorporating SR-SMC in sampling consistently improves the results across all benchmarks, model architectures, and generation lengths. Typically, our algorithm achieves average performance gains of 2.8+ and 4.5+ for LLaDA-1.5 and 5 Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models Figure 3. Comparison results of LLaDA 1.5 and Dream-7B using different numbers of particles on four tasks. Each marker denotes the empirical result while the dashed curves indicate first-order polynomial fits used solely to illustrating overall trends as increases. Table 2. Performance of Self-Rewarding SMC (SR-SMC) on Diffusion Large Language Models. Results compare baselines versus SR-SMC variants using block decoding with block size = 32 with KV cache and parallel decoding enabled. Benchmark Length LLaDA-1.5 w/ SR-SMC Dream-7B w/ SR-SMC GSM8K (5-shot) MATH (4-shot) HumanEval (0-shot) MBPP (3-shot) Average 256 512 256 256 512 256 512 256 512 79.8 80.4 38.2 41.4 38.7 35. 40.4 40.2 49.3 49.4 80.7 82.0 41.8 45.4 41.5 41.5 44.2 43. 52.1 53.0 76.2 78.0 41.6 44.6 47.9 45.1 42.0 39.6 51.9 51. 78.0 78.0 45.2 47.6 53.7 53.7 48.6 46.4 56.4 56.4 Table 3. Ablation results of scaling the number of particles for LLaDA-1.5 and Dream-7B across four benchmarks (L = 256). Model Benchmark = 1 = 2 = 3 = 4 LLaDA-1.5 Dream-7B GSM8K (5-shot) MATH (4-shot) HumanEval (0-shot) MBPP (3-shot) Average GSM8K (5-shot) MATH (4-shot) HumanEval (0-shot) MBPP (3-shot) Average 79.8 38.2 38.7 40.4 49.3 76.2 41.6 47.9 42. 51.9 81.7 41.8 39.0 44.2 51.7 76.3 43.2 52.4 43.6 53.9 80.7 41.8 39.0 44. 51.4 77.9 45.6 54.6 44.2 55.6 80.7 41.8 41.5 44.2 52.1 78.0 45.2 53.7 48. 56.4 Dream-7B, respectively. This indicates strong and consistent benefit of inference-time scaling through particle-based sampling. Moreover, the improvements on both mathematical and coding tasks suggest that SR-SMC generalizes well across different task domains. These gains are also sustained when generating much longer sequences, indicating that our trajectory-level resampling effectively mitigates error accumulation in diffusion-based generation. Collectively, these findings suggest that SR-SMC can serve as an effective and robust solution to computational scaling in dLLMs. 5. Discussion and Analysis Scaling the Number of Particles As the number of particles increases, we observe clear and consistent improvement in performance across both LLaDA-1.5 and Dream-7B on all four benchmarks, as shown in Table 3 and Figure 3. When = 1, the models reduce to standard parallel decoding, which serves as lower bound on performance across all benchmarks. Scaling the particles to = 2, 3, 4 steadily improves performance (see Figure 3), with the strongest gains typically achieved at = 3 or = 4. On average, scaling particles to = 4 improves LLaDA-1.5 from 49.3 to 52.1 and Dream-7B from 51.9 to 56.4. These results suggest that increasing the number of particles effectively expands the search space over generation trajectories, allowing the model to recover from locally suboptimal token choices and accumulate higher trajectory-level confidence. Interestingly, even modest increase to = 2 yields significant gains over the baseline, highlighting SR-SMC as practical and scalable inference-time method to masked diffusion language models. Analysis of Particle Overtake To investigate the mechanical advantage of maintaining multiple interacting trajectories, we quantify the occurrence of overtake events during the block-wise generation process. We define an overtake event as case where the particle with the highest probability at the end of block was not the dominant particle (i.e., highest probability) at the start of that same block. As shown in Table 4, we analyzed 640 blocks per benchmark using LLaDA-1.5. We observe that overtake events occur in approximately 24% to 31% of all blocks across different tasks. This empirical evidence indicates that the particles in SR-SMC are not merely performing idle exploration around greedy trajectories. Instead, the resampling mechanism preserves initially non-dominant but high-potential trajectories, allowing them to eventually outperform the local greedy choices that standard decoding methods would otherwise lock into. Notably, such behavior cannot be captured by single-trajectory decoding or purely greedy confidencebased remasking, which irreversible commit to the locally 6 Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models Qualitative Comparison: Mathematical Reasoning (MATH) Problem: In the land of Ink, the money system is unique. One Trinket is equal to 4 Blinkets, and 3 Blinkets are equal to 7 Drinkets. In Trinkets, what is the value of 56 Drinkets? Reference Answer: 6 Greedy Decoding (τ = 0) w/ SR-SMC (τ = 1, = 4) To find the value of 56 Drinkets in Trinkets, we need to convert all units to Trinkets. We know the following conversion rates: 1. 1 Trinket = 4 Blinkets 2. 3 Blinkets = 7 Blinkets (Local Consistency Error) First, lets convert 3 Blinkets to Trinkets: 3 Blinkets = 3 4 Trinkets. Next, lets convert 7 Blinkets to Trinkets: 7 Blinkets = 7 3 Now, lets convert 56 Drinkets to Trinkets: 56 Drinkets = 56 5.25 Trinkets = 294 Trinkets. 4 Trinkets = 5.25 Trinkets. Final Answer: 294 To find the value of 56 Drinkets in Trinkets, we need to convert Drinkets to Blinkets first and then from Blinkets to Trinkets. 1. Convert Drinkets to Blinkets: - We know that 3 Blinkets are equal to 7 Drinkets. - Therefore, 1 Drinket is equal to 3 - So, 56 Drinkets is equal to 56 3 7 Blinkets. 7 = 24 Blinkets. 2. Convert Blinkets to Trinkets: - We know that 1 Trinket is equal to 4 Blinkets. - Therefore, 1 Trinket is equal to 1 - So, 24 Blinkets is equal to 24 1 4 Trinkets. 4 = 6 Trinkets. Therefore, the value of 56 Drinkets in Trinkets is 6 Trinkets. Final Answer: 6 Figure 4. qualitative comparison of reasoning trajectories. Greedy decoding focus on step-wise confidence (Sahoo et al., 2024) leads to hallucinated identity (3 Blinkets = 7 Blinkets) that persists through the chain. SR-SMC utilizes trajectory-level confidence to explore multiple trajectories in parallel and successfully recovers the correct multi-step conversion. Table 4. Analysis of particle Overtake. An overtake occurs when the particle with the highest probability at the end of block was not the dominant particle at the start of that same block. Benchmark # Overtake Percentage GSM8K (5-shot) MBPP (3-shot) MATH (4-shot) HumanEval (0-shot) Average 160 / 640 154 / 640 197 / 640 196 / 640 - 25.0% 24.1% 30.8% 30.6% 27.6% dominant particle at the beginning of each block. Analysis of Gumbel Noise We investigate the impact of sampling stochasticity on the performance of masked diffusion language models. Traditional sampling strategies for MDLMs often rely on confidence-based greedy decoding, which can lead to myopic trajectory exploration and lack of generation diversity. To examine how our proposed self-rewarding SMC interacts with different levels of randomness, we provide ablation experiments to evaluate the performance of LLaDA-1.5 and Dream-7B across range of Gumbel noise temperatures τ [0, 1.0] during sampling. As illustrated in Figure 5, the baseline performance using standard parallel decoding is highly sensitive to the sampling temperature. For the Dream-7B model, we observe significant performance collapse at low temperatures (starting from τ = 0.1) on both MBPP and MATH benchmarks. This failure is primarily attributed to excessive token repetition during the deterministic decoding process, which traps the model in suboptimal generative trajectories. In contrast, SR-SMC with = 4 particles demonstrates superior robustness across the entire temperature range. By maintaining multiple interacting particles and performing resampling based on trajectory-level confidence, SR-SMC effectively explores the generative space and steers the sampling process away from repetitive or low-confidence regions. For LLaDA-1.5, although the baseline does not exhibit the same catastrophic collapse at low temperatures, SR-SMC still consistently outperforms the baseline across most temperature settings. The results show that SR-SMC provides the most significant gains at moderate to high temperatures, where it can better leverage the diverse candidates generated by the stochastic diffusion process. These findings highlight that SR-SMC is not only an effective inference-time scaling method but also principled framework that enhances the stability and reliability of diffusion-based language generation regardless of the chosen sampling temperature. Figure 4 further illustrates qualitative comparison of decoding trajectories with different sampling strategies. Zero-Shot Evaluation To further evaluate the generalizability of our method across different prompting configurations, we conduct zero-shot evaluation on the GSM8K and MATH benchmarks. We note that the results for HumanEval presented in our main experiments  (Table 2)  are inherently zero-shot. For the MBPP benchmark, however, we observed that both base models failed to produce code in the requisite format for automated post-processing with7 Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models Figure 5. Effect of sampling temperature τ on model performance across different benchmarks. We report the accuracy of LLaDA-1.5 and Dream-7B on MBPP and MATH datasets as the temperature varies uniformly from 0.0 to 1.0. The blue circles represent the baseline with standard parallel decoding, while the red stars denote the results using our SR-SMC with = 4 particles. SR-SMC consistently demonstrates better robustness across the entire temperature range. Notably, while the baseline performance of Dream-7B collapses at low temperatures (startig from 0.1) due to repetition, SR-SMC maintains stable and high accuracy by effectively exploring the generative space through particle re-weighting and resampling. Table 5. Zero-shot performance of LLaDA-1.5 and DREAM7B variants on GSM8K and MATH benchmarks across different generation lengths. Benchmark Length LLaDA-1.5 w/ SR-SMC Dream-7B w/ SR-SMC GSM8K (0-shot) MATH (0-shot) 256 256 512 75.4 80.4 35.2 39.0 76.3 82.0 36.6 44.8 77.9 78. 44.2 48.8 81.1 78.0 49.2 51.6 out few-shot exemples. Unlike few-shot settings, where in-context demonstrations provide template for the output, zero-shot generation relies exclusively on the models intrinsic reasoning capabilities and the effectiveness of the inference-time exploration. As illustrated in Table 5, SR-SMC consistently enhances the performance of both LLaDA-1.5 and Dream-7B in these zero-shot settings. These results demonstrate that trajectorylevel confidence serves as robust implicit reward signal, enabling the model to effectively navigate wider exploration space and steer away from low-quality outputs even in the absence of prompting demonstrations. 6. Related Work As D3PM (Austin et al., 2021) introduced the state absorbing with mask token for discrete diffusion models, masked diffusion language models have attracted increasing attention as promising alternative to auto-regressive (AR) models (Sahoo et al., 2024; Lou et al., 2024; Schiff et al., 2024; Shi et al., 2024; Arriola et al., 2025). Built upon MDLMs, diffusion large language models such as LLaDAs (Nie et al., 2025; Zhu et al., 2025; Bie et al., 2025) Dream (Ye et al., 2025), and DiffuLLaMA (Gong et al., 2024) have demonstrated strong scalability and achieved competitive performance when compared to similarly sized AR models. Subsequently, dLLM-Cache (Liu et al., 2025) and Fast-dLLM (Wu et al., 2025b) introduced caching and parallel decoding to MDLMs, further improving inference efficiency and their potential for real-world applications. Despite these advances, existing MDLMs primarily improve performance through model scaling (Nie et al., 2024; 2025), architectural modifications (Wu et al., 2025a; Bie et al., 2025), or training-time interventions (Schiff et al., 2024; Hersche et al., 2025), while the role of inference-time scaling remains largely unexplored. As primary work, Ma et al. (2025) first proposed scaling test-time compute for diffusion models, substantially improving the performance beyond simply scaling diffusion sampling steps. Singhal et al. (2025) illustrated this idea on MDLMs using the sequential Monte Carlo framework. Dang et al. (2025) further extend it with particle Gibbs sampling that enables generation refinement via an external task-specific reward guidance. In parallel, ReMDM (Wang et al., 2025) introduces principled remasking strategy to improve text quality with more sampling steps. In contrast, our proposed algorithm is self-rewarding and can be used for general tasks with arbitrary pretrained models and remasking strategies. 7. Conclusion In this paper, we propose novel inference-time scaling algorithm for masked diffusion language models (MDLMs). Our algorithm is sequential Monte Carlo (SMC) method where the trajectory confidence is used as importance weights. This results in self-rewarding SMC framework that promotes globally confident generation trajectories, without requiring additional training or external reward guidance. Extensive experiments and ablation studies across multiple benchmarks and model families demonstrate that our self-rewarding SMC significantly improves pretrained MDLMs in terms of both sample quality and diversity, unlocking an effective and principled inference-time scaling dimension for diffusion-based language generation. Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models"
        },
        {
            "title": "Acknowledgements",
            "content": "This research was supported by Kjell & Marta Beijer Foundation and by the project Deep Probabilistic Regression New Models and Learning Algorithms (contract number: 2021-04301) funded by the Swedish Research Council. Some of the computations were enabled by resources provided by the National Academic Infrastructure for Supercomputing in Sweden (NAISS), partially funded by the Swedish Research Council through grant agreement no. 2022-06725."
        },
        {
            "title": "References",
            "content": "Arriola, M., Sahoo, S. S., Gokaslan, A., Yang, Z., Qi, Z., Han, J., Chiu, J. T., and Kuleshov, V. Block Interpolating between autoregressive and diffusion: In The Thirteenth Indiffusion language models. ternational Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=tyEyYT267x. 1, 2, 5, 8 Austin, J., Johnson, D. D., Ho, J., Tarlow, D., and Van Den Berg, R. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021. 2, 8 Bie, T., Cao, M., Chen, K., Du, L., Gong, M., Gong, Z., Gu, Y., Hu, J., Huang, Z., Lan, Z., et al. Llada2. 0: Scaling up diffusion language models to 100b. arXiv preprint arXiv:2512.15745, 2025. 8 Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., Levi, Y., English, Z., Voleti, V., Letts, A., et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 1 Chang, H., Zhang, H., Jiang, L., Liu, C., and Freeman, W. T. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1131511325, 2022. 1, 2, 3 Dang, M., Han, J., Xu, M., Xu, K., Srivastava, A., and Ermon, S. Inference-time scaling of diffusion language models with particle gibbs sampling. arXiv preprint arXiv:2507.08390, 2025. 1, Dathathri, S., Madotto, A., Lan, J., Hung, J., Frank, E., Molino, P., Yosinski, J., and Liu, R. Plug and play language models: simple approach to controlled text generation. arXiv preprint arXiv:1912.02164, 2019. 1 Del Moral, P. Feynman-kac formulae. In Feynman-Kac Formulae: Genealogical and Interacting Particle Systems with Applications, pp. 4793. Springer, 2004. 4 Del Moral, P., Doucet, A., and Jasra, A. Sequential monte carlo samplers. Journal of the Royal Statistical Society Series B: Statistical Methodology, 68(3):411436, 2006. 2 Dhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 1 Doucet, A., De Freitas, N., and Gordon, N. An introduction to sequential monte carlo methods. In Sequential Monte Carlo methods in practice, pp. 314. Springer, 2001. 4 Gokaslan, A. and Cohen, V. Openwebtext corhttp://Skylion007.github.io/ pus. OpenWebTextCorpus, 2019. 5 Gong, S., Agarwal, S., Zhang, Y., Ye, J., Zheng, L., Li, M., An, C., Zhao, P., Bi, W., Han, J., et al. Scaling diffusion language models via adaptation from autoregressive models. arXiv preprint arXiv:2410.17891, 2024. 8 Gu, S., Chen, D., Bao, J., Wen, F., Zhang, B., Chen, D., Yuan, L., and Guo, B. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1069610706, 2022. 1 Han, X., Kumar, S., and Tsvetkov, Y. Ssd-lm: Semiautoregressive simplex-based diffusion language model In Proceedfor text generation and modular control. ings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1157511596, 2023. 5 Hersche, M., Moor-Smith, S., Hofmann, T., and Rahimi, A. Soft-masked diffusion language models. arXiv preprint arXiv:2510.17206, 2025. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1 Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., and Fleet, D. J. Video diffusion models. Advances in neural information processing systems, 35:86338646, 2022. 1 Liu, Z., Yang, Y., Zhang, Y., Chen, J., Zou, C., Wei, Q., Wang, S., and Zhang, L. dllm-cache: Accelerating diffusion large language models with adaptive caching. arXiv preprint arXiv:2506.06295, 2025. 8 Lou, A., Meng, C., and Ermon, S. Discrete diffusion modeling by estimating the ratios of the data distribution. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/ forum?id=CNicRIVIPA. 1, 5, 8 9 Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models Loula, J., LeBrun, B., Du, L., Lipkin, B., Pasti, C., Grand, G., Liu, T., Emara, Y., Freedman, M., Eisner, J., et al. Syntactic and semantic control of large language models via sequential monte carlo. arXiv preprint arXiv:2504.13139, 2025. 1 Ma, N., Tong, S., Jia, H., Hu, H., Su, Y.-C., Zhang, M., Yang, X., Li, Y., Jaakkola, T., Jia, X., et al. Inference-time scaling for diffusion models beyond scaling denoising steps. arXiv preprint arXiv:2501.09732, 2025. 8 Nie, S., Zhu, F., Du, C., Pang, T., Liu, Q., Zeng, G., Lin, M., and Li, C. Scaling up masked diffusion models on text. arXiv preprint arXiv:2410.18514, 2024. 8 Nie, S., Zhu, F., You, Z., Zhang, X., Ou, J., Hu, J., Zhou, J., Lin, Y., Wen, J.-R., and Li, C. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. 1, 3, 8 Peebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. 1 Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36: 5372853741, 2023. Robert, C. P., Casella, G., and Casella, G. Monte Carlo statistical methods, volume 2. Springer, 1999. 2 Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. 1 Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35: 3647936494, 2022. 1 Shi, J., Han, K., Wang, Z., Doucet, A., and Titsias, M. Simplified and generalized masked diffusion for discrete data. Advances in neural information processing systems, 37:103131103167, 2024. 8 Singhal, R., Horvitz, Z., Teehan, R., Ren, M., Yu, Z., McKeown, K., and Ranganath, R. general framework for inference-time scaling and steering of diffusion models. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/ forum?id=Jp988ELppQ. 1, Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265. pmlr, 2015. 1 Uehara, M., Zhao, Y., Wang, C., Li, X., Regev, A., Levine, S., and Biancalani, T. Inference-time alignment in diffusion models with reward-guided generation: Tutorial and review. arXiv preprint arXiv:2501.09685, 2025. 1 Wang, G., Schiff, Y., Sahoo, S. S., and Kuleshov, V. Remasking discrete diffusion models with inference-time scaling. arXiv preprint arXiv:2503.00307, 2025. 8, 12 Wu, C., Zhang, H., Xue, S., Diao, S., Fu, Y., Liu, Z., Molchanov, P., Luo, P., Han, S., and Xie, E. Fastdllm v2: Efficient block-diffusion llm. arXiv preprint arXiv:2509.26328, 2025a. 8 Wu, C., Zhang, H., Xue, S., Liu, Z., Diao, S., Zhu, L., Luo, P., Han, S., and Xie, E. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding. arXiv preprint arXiv:2505.22618, 2025b. 3, 5, 8 Ye, J., Xie, Z., Zheng, L., Gao, J., Wu, Z., Jiang, X., Li, Z., and Kong, L. Dream 7b: Diffusion large language models. arXiv preprint arXiv:2508.15487, 2025. 1, 2, 5, 8, Zhang, L., Rao, A., and Agrawala, M. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 38363847, 2023. 1 Sahoo, S., Arriola, M., Schiff, Y., Gokaslan, A., Marroquin, E., Chiu, J., Rush, A., and Kuleshov, V. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37:130136 130184, 2024. 1, 2, 5, 7, 8 Zheng, K., Chen, Y., Mao, H., Liu, M.-Y., Zhu, J., and Zhang, Q. Masked diffusion models are secretly timeagnostic masked models and exploit inaccurate categorical sampling. arXiv preprint arXiv:2409.02908, 2024. 4 Schiff, Y., Sahoo, S. S., Phung, H., Wang, G., Boshar, S., Dalla-torre, H., de Almeida, B. P., Rush, A., Pierrot, T., and Kuleshov, V. Simple guidance mechanisms for discrete diffusion models. arXiv preprint arXiv:2412.10193, 2024. 8 Zhu, F., Wang, R., Nie, S., Zhang, X., Wu, C., Hu, J., Zhou, J., Chen, J., Lin, Y., Wen, J.-R., et al. Llada 1.5: Variancereduced preference optimization for large language diffusion models. arXiv preprint arXiv:2505.19223, 2025. 2, 5, 8, 12 Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models A. Sequential Monte Carlo for Diffusion Reverse Process A.1. Incremental Importance Weights Consider sequence of tokens xT , . . . , x0 in the diffusion reverse path, with unnormalized intermediate target distribution πt(xt:T ). We define the following Markov sequential proposal qt(xt:T ) = qT (xT ) 1 (cid:89) k=t qk(xk xt+1), so that it satisfies standard factorization: qt1(xt1:T ) = qt(xt:T ) qt1(xt1 xt). The importance weight of trajectory xt:T (from the full-mask tokens to partial masked state) at time is given by Wt(xt:T ) = πt(xt:T ) qt(xt:T ) . Likewise, at next step, we could write Wt1(xt1:T ) = πt1(xt1:T ) qt1(xt1:T ) = = πt1(xt1:T ) qt(xt:T ) qt1(xt1 xt) πt1(xt1:T ) πt(xt:T ) qt1(xt1 xt) (cid:125) (cid:123)(cid:122) (cid:124) = wt1(xt1:T ) , πt(xt:T ) qt(xt:T ) (cid:124) (cid:125) (cid:123)(cid:122) = Wt(xt:T ) (16) (17) (18) (19) (20) (21) where Wt(xt:T ) is the previous weight as in Eq. (18), and wt1(xt1:T ) is the incremental importance weights in Eq. (4). Conceptually, wt1(xt1:T ) is the local ratio that updates the global path-wise importance weight when extending trajectory with more unmasked tokens i.e., from xt:T to xt1:T . In practice, SMC maintains particle weights recursively via Eq. (21) and performs resampling using normalized version of Wt1(xt:T ). A.2. Proof for Confidence-based Sequential Monte Carlo Proposition 3.1. Given pretrained diffusion model pθ, let {πt(xt:T )}T t=0 denote the unnormalized path measures defined by the recursion in Eq. (12). If the sequential proposal in SMC is chosen to be the diffusion transition kernel, i.e., qt1(xt1 xt) = Kt(xt, xt1), then the incremental importance weights at step 1 is given by wt1(xt1:T ) = (cid:89) jSt ct(j), (22) where ct(j) := pθ (cid:0)ˆx0(j) xt (cid:1) is the token confidence and St denotes the selected mask subset to be updated at step t. Proof. Recall that in sequential Monte Carlo, the incremental importance weight at step 1 is defined as Substituting the path recursion in Eq. (12) into the numerator yields wt1(xt1:T ) = πt1(xt1:T ) πt(xt:T ) qt1(xt1 xt) , wt1(xt1:T ) = πt(xt:T ) Kt(xt, xt1) Gt1(xt, xt1) πt(xt:T ) qt1(xt1 xt) . Removing πt(xt:T ) in both numerator and denominator, we obtain wt1(xt1:T ) = Kt(xt, xt1) Gt1(xt, xt1) qt1(xt1 xt) . 11 (23) (24) (25) Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models When the proposal distribution qt1(xt1 xt) is selected to be the same as the diffusion transition kernel, i.e, qt1(xt1 xt) = Kt(xt, xt1), the transition kernel disappear, and wt1(xt1:T ) = Gt1(xt, xt1). (26) And recall that we define the potential as the joint probability of accepted tokens within set St, i.e., Gt1(xt, xt1) = (cid:81) (cid:1), we finally have the following pθ(xt1(j) xt), and that ct(j) := pθ (cid:0)ˆx0(j) xt jSt wt1(xt1:T ) = = (cid:89) jSt (cid:89) jSt pθ(xt1(j) xt) ct(j), (27) (28) which completes the proof. B. Limitation and Future Work While the proposed self-rewarding SMC provides principled framework for trajectory-level confidence-guided sampling, there are several worth-noting limitations. First, our method increases inference-time computation by running multiple diffusion processes in parallel. This trade-off is inherent to inference-time scaling methods and can be controllable by adjusting the number of particles. Second, the proposed trajectory confidence relies solely on model likelihood. While this choice is generic and task-agnostic, it does not explicitly optimize for downstream objectives such as reasoning correctness or human preference. In our future work, we plan to explore more informed proposals, such as look-ahead or twisted diffusion transitions, to further improve sampling efficiency and quality. C. Additional Experiment C.1. Entropy Results of Text Generation We also report the entropy values of text generation with our self-rewarding SMC in Table 6. Note that the generative perplexity and entropy of the original data are 14.8 and 5.44, respectively, as reported in Wang et al. (2025), which demonstrate that our method improves sample quality while preserving text diversity. Table 6. Generative perplexity (Gen. PPL; ), entropy, and the number of function evaluations (NFEs; ) of 300 samples of lengths = 1024, 2048. All models are trained on OWT dataset. For reference, the Gen. PPL and entropy of the original data are 14.8 and 5.44, respectively, as reported by Wang et al. (2025). Model MDLM w/ SR-SMC BD3-LMs w/ SR-SMC = 1024 = 2048 Gen. PPL() Entropy() NFEs Gen. PPL() Entropy() NFEs = 16 = 8 = 25.8 21.1 18.9 16.1 5.15 5.19 5.18 5.20 4K 4K 4K 4K 25.9 20.2 17.3 15.1 5.41 5.46 5.45 5.49 8K 8K 8K 8K C.2. Detailed Results of Inference with Gumbel Noise We provide the detailed results of diffusion sampling with different Gumbel noise temperatures τ ranging from 0.0 to 1.0, as shown in Table 7. We observe that our self-reward SMC (SR-SMC) consistently outperforms the baseline models LLaDA-1.5 (Zhu et al., 2025) and Dream-7B (Ye et al., 2025) over wide range of noise temperatures across both MBPP and MATH benckmarks. Notably, Dream-7B baseline is highly sensitive to noise temperatures, as its performance degrades severely when slightly increase τ to 0.1 and 0.2. While our method significantly improves its robustness over all noise temperatures. This behavior underscores the brittleness of traditional diffusion sampling strategies , showing great potential of our SR-SMC that mitigates this issue through particle-based exploration and resampling. C.3. Additional Examples We include more examples of comparison of the paths between greedy decoding and our self-rewarding SMC in Figure 6 and 7. These qualitative results further illustrate how standard greedy decoding is prone to local consistency errors and 12 Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models calculation hallucinations, whereas SR-SMC maintains global coherence through its particle resampling mechanism. Table 7. Effect of Gumbel noise temperature τ on MBPP and MATH. Benchmark Method τ = 0.0 τ = 0.1 τ = 0. τ = 0.3 τ = 0.4 τ = 0.5 τ = 0.6 τ = 0.7 τ = 0. τ = 0.9 τ = 1.0 MBPP MATH LLaDA-1.5 LLaDA-1.5 w/ SR-SMC Dream-7B Dream-7B w/ SR-SMC LLaDA-1.5 LLaDA-1.5 w/ SR-SMC Dream-7B Dream-7B w/ SR-SMC 42.6 43.8 48.2 49.6 39.8 39.2 42.4 46.8 42.2 43.2 1.80 43.4 39.8 39.2 2.40 42.0 42.8 43.0 5.60 46.8 38.0 41.0 10.4 44. 42.8 42.6 14.2 47.4 37.6 42.0 22.2 42.4 41.0 43.4 28.0 48.4 40.0 40.0 29.2 44.2 42.0 45.2 34.0 48.6 38.4 39.4 38.0 45. 40.2 43.2 39.0 47.4 38.8 39.2 39.8 43.0 42.6 44.0 41.2 48.4 37.2 41.4 38.0 43.8 40.6 43.6 41.4 48.6 38.8 39.6 41.0 44. 41.6 43.4 41.8 48.0 38.2 40.6 42.4 46.2 40.4 44.2 42.0 48.6 38.2 41.8 41.6 45.2 Qualitative Comparison: More Examples on Arithmetic Reasoning Problem: Simplify 1 Reference Answer: 8 5 8 7 12 20 . Greedy Decoding (τ = 0) w/ SR-SMC (τ = 1, = 4) First, simplify the multiplication of the fractions: 1 8 7 = 8 7 (Calculation Hallucination: ignored 1 5 ) Next, simplify the division of the fractions. Recall that dividing by fraction is equivalent to multiplying by its reciprocal: 8 7 12 20 = 8 7 20 12 Now, multiply the numerators and the denominators: 8 20 7 12 = 160 84 Simplify the fraction by finding the greatest common divisor (GCD) of 160 and 84. The GCD of 160 and 84 is 40: 160 40 84 40 = 4 7 Final Answer: 4 7 To simplify the expression 1 5 8 7 12 20 , we can follow these steps: 1. Multiply the fractions first: 1 8 7 = 1 8 5 7 = 8 2. Divide the fractions: 8 35 12 20 = 8 20 12 = 8 20 35 12 = 160 3. Simplify the fraction: 160 420 = 8 21 Therefore, the simplified form of the expression is: Final Answer: 8 21 Figure 6. More qualitative comparison of arithmetic reasoning. 13 Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models Qualitative Comparison: Distance Calculation Problem: At 50 miles per hour, how far would car travel in 2 3 Reference Answer: 137 1 2 4 hours? Express your answer as mixed number. Greedy Decoding (τ = 0) w/ SR-SMC (τ = 1, = 4) To determine how far car would travel in 2 3 50 miles per hour, we can use the formula for distance: 4 hours at speed of To determine how far car would travel in 2 3 50 miles per hour, we use the formula: 4 hours at speed of Distance = Speed Time Distance = Speed Time First, convert the mixed number 2 3 4 to an improper fraction: First, convert the time from mixed number to an improper fraction: 2 3 4 = 1 4 (Severe Conversion Error) Next, multiply the speed by the time: Distance = 50 1 4 = 12.5 miles 2 3 = 8 4 + 3 4 = 11 Now, multiply the speed by the time: Distance = 50 11 4 = 550 4 = 137. Finally, convert 12.5 miles to mixed number: Convert 137.5 to mixed number: 12.5 = 12 1 2 137.5 = 137 1 Final Answer: 12 1 2 Final Answer: 137 1 2 Figure 7. More qualitative comparison of physical reasoning."
        }
    ],
    "affiliations": [
        "MiroMind AI, Singapore",
        "Nanyang Technological University, Singapore",
        "Uppsala University, Sweden"
    ]
}