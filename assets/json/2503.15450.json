{
    "paper_title": "SkyLadder: Better and Faster Pretraining via Context Window Scheduling",
    "authors": [
        "Tongyao Zhu",
        "Qian Liu",
        "Haonan Wang",
        "Shiqi Chen",
        "Xiangming Gu",
        "Tianyu Pang",
        "Min-Yen Kan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in LLM pretraining have featured ever-expanding context windows to process longer sequences. However, our pilot study reveals that models pretrained with shorter context windows consistently outperform their long-context counterparts under a fixed token budget. This finding motivates us to explore an optimal context window scheduling strategy to better balance long-context capability with pretraining efficiency. To this end, we propose SkyLadder, a simple yet effective approach that implements a short-to-long context window transition. SkyLadder preserves strong standard benchmark performance, while matching or exceeding baseline results on long context tasks. Through extensive experiments, we pre-train 1B-parameter models (up to 32K context) and 3B-parameter models (8K context) on 100B tokens, demonstrating that SkyLadder yields consistent gains of up to 3.7% on common benchmarks, while achieving up to 22% faster training speeds compared to baselines. The code is at https://github.com/sail-sg/SkyLadder."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 0 5 4 5 1 . 3 0 5 2 : r Preprint SKYLADDER: BETTER AND FASTER PRETRAINING"
        },
        {
            "title": "VIA CONTEXT WINDOW SCHEDULING",
            "content": "Tongyao Zhu ,S Qian LiuS Haonan Wang Shiqi Chen Xiangming Gu Tianyu Pang Min-Yen Kan National University of Singapore SSea AI Lab CCity University of Hong Kong tongyao.zhu@u.nus.edu; liuqian.sea@gmail.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advancements in LLM pretraining have featured ever-expanding context windows to process longer sequences. However, our pilot study reveals that models pretrained with shorter context windows consistently outperform their long-context counterparts under fixed token budget. This finding motivates us to explore an optimal context window scheduling strategy to better balance long-context capability with pretraining efficiency. To this end, we propose SkyLadder, simple yet effective approach that implements short-to-long context window transition. SkyLadder preserves strong standard benchmark performance, while matching or exceeding baseline results on long-context tasks. Through extensive experiments, we pre-train 1B-parameter models (up to 32K context) and 3B-parameter models (8K context) on 100B tokens, demonstrating that SkyLadder yields consistent gains of up to 3.7% on common benchmarks, while achieving up to 22% faster training speeds compared to baselines."
        },
        {
            "title": "INTRODUCTION",
            "content": "The evolution of language models has been marked by consistent expansion in context window sizes (Figure 1 left). While early models like GPT (Radford, 2018) and BERT (Kenton & Toutanova, 2019) were limited to context windows of 512 tokens, subsequent models have pushed these boundaries significantly. GPT-2 (Radford et al., 2019) doubled this capacity to 1024 tokens, and with the advent of Large Language Models (LLMs) exceeding 1B parameters, the progression continued: Llama (Touvron et al., 2023a) implemented 2048-token window, Llama-2 (Touvron et al., 2023b) extended it to 4096, and Llama-3 (Dubey et al., 2024) further expanded to 8192 tokens. The push to expand the context window is motivated by the need for models to handle longer sequences during inference. The development is also driven by widespread belief that models pretrained with longer context windows should perform comparably to, or even surpass, their shorter context counterparts, as extended windows reduce document truncation and preserve coherence (Ding et al., 2024). We question whether the common belief that larger context windows does actually improve performance. Close inspection of previous work reveals that there has yet to be fair experimental setup for comparing models across different context windows while adhering to fixed token budget. Using tightly controlled experiments, we test how changing only the context window size during pretraining impacts their performance. As shown in Figure 1 (right), our results indicate that models pretrained using shorter contexts always outperform long-context models, when assessed by their average performance of over popular benchmarks. In addition, we verify that the performance gap is not eliminated by using advanced document packing strategies (Dubey et al., 2024; Ding et al., 2024; Shi et al., 2024). To ensure the model can ultimately process long sequences, the model does need to be exposed to long sequences. However, given the finding that shorter context windows enhance performance on downstream tasks, we face trade-off between long-context capability and pre-training effectiveness. We propose SkyLadder, simple yet effective context window scheduling strategy designed to Corresponding author. 1The code is available at https://github.com/sail-sg/SkyLadder. 1 Preprint Figure 1: Left: Pretraining context window of LLMs grows over time. Right: Average performance across nine downstream tasks for 1B-parameter models with varying pretrained context window sizes. Increasing the context window size degrades the overall performance. balance both objectives. SkyLadder does this by progressively expanding the size of the context window during pretraining, beginning pretraining with minimal short context window (e.g., 8 tokens) and progressively expanding it to the long target context window (e.g., 32,768 tokens). Empirical results on 1B-parameter models (up to 32K context window) and 3B-parameter models (up to 8K context window) on 100B tokens demonstrate that SkyLadder outperforms naive longcontext pretraining baselines, in both shortand long-context evaluation tasks. For example, models trained with SkyLadder demonstrate significantly higher accuracy on standard benchmarks (e.g., HellaSwag), and reading comprehension tasks (e.g., HotpotQA), while still maintaining competitive performance on long-context evaluations like RULER. We further investigate the mechanisms behind the superior performance by observing the training dynamics, and discover that SkyLadder exhibits more concentrated and effective attention patterns. Overall, we suggest that the length of the context window is an important dimension in pretraining and should be scheduled over the course of training. We recommend progressive approach that begins with small context of 8 tokens and gradually increases according to linear function of training steps. Given target context window (e.g., 32K), we suggest that allocating approximately 60% of the total training tokens to this expansion phase leads to stronger downstream performance compared to baselines. This scheduling strategy optimally enhances both training efficiency and model capability, offering practical recipe for improving pretraining in language models."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Context Window Scheduling. Early work explored gradually increasing the context window, in smaller models like BERT and GPT-2, to improve training stability and efficiency (Nagatsuka et al., 2021; Li et al., 2022; Jin et al., 2023). Notably, Li et al. (2022) proposed length warmup for more stable training but did not show clear performance gains, and Jin et al. (2023) focused on training acceleration in 400Mparameter models. We extend these findings by demonstrating, for the first time, that context window scheduling significantly boosts both efficiency and performance at much larger scales (up to 3B parameters). parallel approach from Pouransari et al. (2024) segments training documents by length, but Fu et al. (2024) caution that such segmentation can introduce domain biases, particularly since longer texts often cluster in specific domains, such as books. Recent developments in continual pretraining with long context windows (Peng et al., 2024; Wang et al., 2024; Gao et al., 2024b), can also be viewed through the lens of context window scheduling with different strategies (illustrated in Figure 2). Our work represents the first Figure 2: Schematic comparison of training-time context window scheduling. 2 Preprint Figure 3: An illustration of the workflow for pretraining data preparation highlights several critical decisions. Key considerations include the method of data packing, the type of attention mask to employ (causal mask and intra-doc mask), and determining the appropriate context window length denoted as L. demonstration of both the effectiveness and the efficiency of context window scheduling, providing empirical evidence of its benefits in both standard and long-context benchmarks. Long-Context Language Models. Long-context language models have received lot of attention due to their ability to capture extended dependencies across large textual windows. Most existing approaches follow continual pretraining paradigm (Fu et al., 2024; Xiong et al., 2023), which extends pretrained backbone model to longer contexts through specialized fine-tuning or additional training. Several works propose to intervene in the positional embeddings to accommodate longer sequences (An et al., 2024; LocalLLaMA, 2023; Peng et al., 2024; Chen et al., 2023; Jin et al., 2024), while others perform extended pretraining on longer-sequence corpora (Gao et al., 2024b; Wang et al., 2024; Lu et al., 2024; Zhao et al., 2024a). Our approach differs from previous methods as we train native long-context models from scratch, rather than modifying pretrained model in post-training. Compared with naive long-context pretraining baseline with constant schedule, our approach delivers substantial gains on multiple long-sequence tasks, underscoring the benefits of training from scratch. These findings suggest that our method can be promising direction for future research on building language models with longer context windows."
        },
        {
            "title": "3 HOW CONTEXT WINDOW AFFECTS PRETRAINING",
            "content": "How does context window affect pretraining? To investigate this, we train models from scratch with context windows ranging from 512 to 16,384 tokens under fixed computational budgets, evaluating via perplexity and downstream task benchmarks. We investigate whether longer contexts improve model quality, analyzing how data packing strategies interact with context window size. 3.1 PACKING, MASKING AND CONTEXT WINDOW Most modern LLMs are based on decoder-only transformer architecture (Vaswani et al., 2017) with fixed context window size denoted by L. In contrast, the pretraining corpus, = {d1, d2, d3, . . . , dn}, consists of documents with varying lengths different from L. Therefore, key step before pretraining is to pack the documents into sequences of length L. Formally, packed sequence Ci is constructed as Ci = Trunc(di,1) di,2 di,n1 Trunc(di,n) , where represents concatenation, and Trunc() denotes truncation of documents to ensure len(Ci) = L. Following previous works (Shi et al., 2024; Zhao et al., 2024b), document boundaries within Ci are explicitly marked using end-of-sequence ([EOS]) tokens. After the sequences are packed, the inputs are passed into transformer layers for next-token prediction training. crucial component of these layers is the attention mechanism, which can be formulated as Ai,j = kj, and then Attn(X) = Softmax(A + ). In decoder-only models, mask is applied to introduce constraints during training. common approach is to use causal mask, which ensures that each position can only attend to previous tokens by masking out (setting to ) attention scores 3 Preprint corresponding to future positions: Mi,j = (cid:26)0 if otherwise recently proposed masking scheme, known as intra-doc mask (Zhao et al., 2024b; Dubey et al., 2024), introduces extra constraints among documents. Let each document have start index sd and end index ed, the masking can be denoted as: Intra i,j = (cid:26)0 if such that sd i, ed and otherwise The models are then trained with the standard cross-entropy loss on the packed sequences of length L. The overall workflow for data packing process is illustrated in Figure 3. 3.2 PRELIMINARY STUDY ON CONTEXT WINDOW SIZE As per Section 1, we initiate our study by investigating the impact of context window size on model performance through controlled experiment. Specifically, we pretrain language models with varying context window sizes, while preserving all other experimental settings. This isolates the effects of context window size, enabling our analysis of its influence on model performance. Through this analysis, we aim to understand whether longer context windows inherently lead to better or worse model performance. Key Variables. The context window size determines the number of tokens included in the context for each packed sequence. However, as discussed earlier, several additional factors influence the content within the context window: (1) Packing methods determine which documents constitute the context window, and different packing strategies can significantly alter the composition of the sequences used for pretraining; (2) Masking methods decide whether cross-document attention is enabled within the same context window. The choice of masking strategy affects how the information from different documents interacts during training. Packing and Masking. To study the impact of packing, we employ two widely adopted strategies: random packing and semantic packing. For random packing, documents are randomly concatenated without specific ordering. For semantic packing, inspired by Shi et al. (2024), we retrieve and concatenate semantically relevant documents from the corpus, aiming to keep them within the same context window. After experimenting with both dense retriever (Izacard et al., 2021) used in the paper, and typical retriever BM25, we found that BM25 gives stronger performance and chose it as our primary focus. For masking, the baseline approach is causal masking. where each token can attend to all preceding tokens within the same context window, regardless of document boundaries. Conversely, recent studies (Zhao et al., 2024b; Ding et al., 2024) have demonstrated that disabling cross-document attention, thereby enabling intra-document attention, can lead to improved performance. For clarity in subsequent discussions, we denote random packing with causal masking as Random, BM25 packing with causal masking as BM25 and random packing with intra-document masking as IntraDoc. Training. We pretrain models from scratch using the TinyLlama codebase (Zhang et al., 2024a), and experiments on various model sizes including 120M, 360M and 1B parameters. Given the substantial computational cost associated with retrieval in semantic packing, we randomly select around 30B tokens from the CommonCrawl (CC) subset of the SlimPajama dataset (Zhang et al., 2020) as the pretraining corpus. All models undergo training for up to 100B tokens (3.3 epochs). To ensure consistency across experiments, we maintain strict control over all other experimental settings, retaining the same batch size and learning rate schedule for all context windows. All models also incorporate Rotary Positional Encoding (RoPE; Su et al., 2024) to encode positional information. Appendix A.1 and A.2 gives further model architecture details and training settings. Evaluation. For all model sizes, we use perplexity (PPL) on validation documents from the original dataset as key metric, in line with established practices (Fu et al., 2024; Kaplan et al., 2020; Hoffmann et al., 2022). Note that when comparing models across different context windows (e.g., 4 Preprint Figure 4: Ablation studies of different factors on different context window sizes. Note that the validation PPL is obtained on the validation documents with sliding window size of 512 tokens. 2K-context model and an 8K-context model), we must ensure the evaluation sequence fits within the shorter models context window to maintain fair comparison. We also evaluate our 1B models on their performance on downstream tasks using standard benchmarks: HellaSwag (Zellers et al., 2019), ARC-Easy and ARC-Challenge (Clark et al., 2018), Winogrande (Sakaguchi et al., 2021), CommonsenseQA (Talmor et al., 2019), OpenBookQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2020), Social-QA (Sap et al., 2019), and MMLU (Hendrycks et al., 2021). We employ the OLMES suite (Gu et al., 2024b) for the evaluation, as it has been shown to provide reliable and stable results with its provided 5-shot demonstrations (Gao et al., 2024b). 3.3 EXPERIMENTAL RESULTS Figure 1 presents the main experimental results, obtained using the Random setting with 1B-parameter models. The results indicate that context window size significantly influences the performance of LLMs, with shorter contexts generally lead to better performance. To further investigate the factors contributing to the observation, we conduct comprehensive analysis, examining potential variables that may affect the conclusion. The results are in Figure 4, from which we derive four key findings: Findings: (1) The advantage of shorter contexts is consistent across model sizes; (2) This advantage is independent of the packing and masking methods employed; (3) It is also unrelated to the use of positional encoding; (4) The best packing and masking strategy is IntraDoc, which outperforms others possibly as it introduces more short contexts. Findings (1) and (2). As shown in Figure 4, regardless of the model size (a) or packing and masking methods (b), shorter context window for pretraining generally results in higher average performance on benchmarks. The finding on benchmarks is consistent with the trend of validation PPL, where shorter context windows always yield lower PPL. Findings (3). When using shorter context windows, one might hypothesize that the model learns positional encoding patterns for nearer positions more frequently, leading to better performance on standard benchmarks. To test the hypothesis, we systematically ablate RoPE by completely excluding it during pretraining, following prior work (Kazemnejad et al., 2023). As shown in Figure 4 (c), models trained with short-context windows exhibit superior performance compared to their longcontext counterparts, even in the absence of positional encoding. This suggests that the advantages of shorter context windows are independent of positional encoding. Findings (4). From Figure 4 (b), we observe that IntraDoc achieves the best validation PPL across all context window sizes compared to Random and BM25, alongside consistently higher performance on standard benchmarks (c.f. Appendix A.4.1). This raises the question: why does IntraDoc excel? We attribute the advantage to the context window size distribution of IntraDoc, which implicitly increases the prevalence of shorter contexts. As illustrated in Figure 4 (d), despite the sequence Preprint length of 8K, fewer than 1% of context windows actually reach this limit. While prior work links the success of IntraDoc to reduced contextual noise (Zhao et al., 2024b), we identify complementary factor reduced average context window size as key factor in its strong performance. That is, we hypothesize that the effectiveness of IntraDoc may also be closely tied to short context windows."
        },
        {
            "title": "4 SKYLADDER: CONTEXT WINDOW SCHEDULING",
            "content": "We now present SkyLadder for progressively expanding the context window during pretraining. 4.1 METHOD Inspired by learning rate scheduling, we explore whether dynamically scheduling the context window from short to long during pretraining could lead to performance improvements. This method can be implemented by applying multiple local mini causal masks to long, packed sequence. We illustrate this masking strategy in Figure 5. Formally, we define local window length w. The associated mask Mw is defined as follows: (cid:26)0 Mi,j = if otherwise. i where w calculates the largest multiple of that is less than or equal to i, effectively defining block-wise attention mask for the query token at position i. We linearly adjust the size upwards by constant factor per training step. w(t) = min(we, ws + αt) where we and ws represent the ending and starting context window sizes, respectively. Here, α denotes the rate of expansion, and corresponds to the training step. As the training progresses, when the dynamic context window size w(t) eventually reaches the desired (long) context window size = we, it remains fixed at that value. At this point, the attention mask is equivalent to full causal mask. Notably, this method modifies the effective context window through masking, independent of how the sequences are packed. As such, this mask Mw can be integrated with Intra, which maintains the attention boundaries between documents; it can be seamlessly combined with most packing and masking strategies. 4.2 EXPERIMENTAL SETUP We follow the same setup in Section 3.2 to pretrain language models with 8K context on 100B tokens. We set ws = 32 and α = 1/8 by default, which means that model roughly needs 64K steps (around 64B tokens) to reach the final desired context window of = 8192. We fix all other hyperparameters, such as the learning rate schedule, batch size, etc., for fair comparison. Due to resource constraints, we do not perform extensive hyper-parameter search to obtain the best combinations for w(t), α, and ws. In our ablation study, we demonstrate that the selection of these hyper-parameters has negligible impact on performance, provided that they fall within reasonable range. Figure 5: An illustration of Random and IntraDoc along with SkyLadder. The example shows packed sequence (length L) consisting of two documents. For SkyLadder, the context window starts from small value and dynamically adjusts during training, eventually converging to the masking patterns of Random and IntraDoc, respectively. For evaluation, we use the same evaluation suite mentioned in Section 3.2 with standard benchmarks. To evaluate the performance of long-context question answering within an 8k token length, we utilize the 30-document setting from the Multi-Document QA (MDQA) benchmark (Liu et al., 2023). This setting contains tasks with an average length of approximately 6K tokens and is widely adopted in prior work (Pouransari et al., 2024; Zhao et al., 2024b). We also select synthetic tasks within RULER (Hsieh et al., 2024),as defined by Yen et al. (2024). We choose the setup of the task that fills up the models target context window L. 6 Preprint 4.3 EXPERIMENTAL RESULTS Method Random + SkyLadder IntraDoc + SkyLadder Avg. 46.3 50.0 (+3.7) 47.4 49.3 (+1.9) Standard Benchmarks Long Benchmarks ARC-E ARC-C CSQA HS OBQA PIQA SIQA WG MMLU Avg. MDQA RULER 58.0 65.4 61.8 64.8 32.7 35.6 33.4 33.8 49.6 56.8 52.7 55.4 43.0 47.0 45.6 47.9 40.2 42.8 38.0 39. 64.8 64.8 64.3 66.1 46.4 48.9 45.7 48.0 51.9 56.0 54.8 56.4 29.9 32.4 30.5 31.8 15.3 14.3 13.0 13.2 17.7 18.3 15.3 15. 12.8 10.3 10.6 10.7 Table 1: Performance (accuracy in %) of 1B models pretrained on CC with different methods on standard and long benchmarks. Tables 1 and 2 present the main results, highlighting significant improvements achieved by SkyLadder across both standard benchmarks and reading comprehension tasks. For instance, compared to the Random baseline, integrating SkyLadder yields notable performance gains on standard tasks such as MMLU (+2.5%), ARC-E (+7.4%), and HellaSwag (+4%). This suggests that models with SkyLadder excel at learning common knowledge during pretraining. Additionally, our method further improves the performance of the strong baseline IntraDoc across all evaluated benchmarks. Meanwhile, for realistic long-context benchmarks like MDQA, our approach consistently matches or exceeds baseline performance. To address potential concerns that the benefits observed in short contexts might stem from the high level of noise in the CommonCrawl corpus, we conducted additional experiments using the FineWeb-Pro dataset (Zhou et al., 2024), heavily cleaned and carefully curated high-quality dataset containing 100B tokens. As shown in Table 3, the improved data quality indeed leads to substantial performance gains across most benchmarks. However, our key findings remain consistent: the IntraDoc approach continues to outperform the Random approach, and SkyLadder consistently delivers significant improvements over both baselines. This demonstrates that our method generalizes well across different types of corpora, regardless of their quality. 4.4 SCALABILITY EXPERIMENTS We now examine whether SkyLadders improvements persist as we scale up the model parameters and extend the context window size. Model Size. We conduct experiments across three model sizes: 120M, 360M, and 3B parameters on the Fineweb-Pro dataset. Table 4 demonstrates that models utilizing SkyLadder consistently achieve better standard benchmark performance on all model sizes. For long context tasks, our method does not benefit 120M models, possibly due to their limited capacity in processing long sequences. However, performance gain on 3B models is still prominent. We observe positive scaling trend: as the model size grows, the performance improvement also increases. This reveals the potential of applying our method to even larger models. Context Window Size. To examine whether SkyLadder can effectively scale to longer context windows, we trained 1B models with 32K context window on the FineWeb-Pro dataset, which contains 100B tokens. As the target context window increases, we adjusted α to 1/2 to ensure that the final context window expands to 32K before the end of pretraining. As shown in Table 6, Method Random + SkyLadder IntraDoc + SkyLadder Avg. 25.5 30.2 (+4.7) 28.7 29.1 (+0.4) Reading Comprehension Benchmarks HotpotQA SQuAD NQ TriviaQA RACE-h 6.5 12.4 11.4 11.0 37.0 40.2 39.0 38.5 15.8 20.4 18.2 20.4 37.7 43.0 42.3 41.5 30.7 35.0 32.3 34.3 Table 2: Performance (accuracy in %) of 1B models pretrained on the CC corpus with different methods on reading comprehension benchmarks. Detailed setup is in Appendix A.4.3. 7 Preprint Figure 6: Validation PPL on 512 and 8k contexts of models with different expansion rate α (left) and initial window length ws (right). our model demonstrates strong performance on both standard and long benchmarks. In addition, the performance difference of SkyLadder (0.9%) between the 8K and 32K models is largely reduced compared with the baseline approach (1.8%), which alleviates the performance degradation described in our earlier study. Notably, compared to the baseline Random approach, SkyLadder trains the model on progressively shorter contexts during earlier stages. This reveals counterintuitive insight: naively training model with long context window is not always optimal, and strategic scheduling of the context window during pretraining can yield better results. Coding To examine whether SkyLadder is generalizable to different types of pretraining besides natural language tasks, following Ding et al. (2024), we pretrain 1B code models from scratch on 100B Python code tokenized by the Starcoder tokenizer (Li et al., 2023). We observe lower training loss ( 0.9) for code pretraining compared to natural language pretraining ( 2.1), suggesting that the structure in code makes it easier for the model to learn. However, as shown in Table 5, we still observe significant improvement when applying SkyLadder under both greedy decoding and sampling setups, especially when the target context length is 32K. This demonstrates the potential of SkyLadder to coding and reasoning tasks beyond natural language modelling. 4.5 ABLATION STUDY We now examine the impact of different hyperparameters in the scheduling of SkyLadder. To manage computational costs, we adopt default setup where 120M model is pretrained on 100B tokens from CommonCrawl with an 8k context window. Method Standard Long Random + SkyLadder IntraDoc + SkyLadder 50.7 54.3 (+3.6) 54.0 54.9 (+0.9) 9.7 13.5 (+3.8) 13.0 14.4 (+1.4) Expansion Rate. We investigate the impact of the expansion rate α in Figure 6 (left). We choose different α ranging from slowest (1/12) to fastest (1). Our findings reveal that, for short contexts, performance generally improves as the expansion rate slows down. However, selecting an excessively slow expansion rate (e.g., 1/12) can negatively affect long-context performance, as the model receives insufficient exposure to longer contexts during pretraining. Therefore, we recommend setting α to 1/8 for good balance. Table 6: Performance (average accuracy in %) of 1B models trained on the FineWeb-Pro dataset with 32K context window. Method Standard Long Random + SkyLadder IntraDoc + SkyLadder 52.5 55.2 (+2.7) 54.3 54.8 (+0.5) 11.1 12.3 (+1.2) 12.7 13.9 (+1.2) Size Method Standard Long 120M 360M 3B Random + SkyLadder Random + SkyLadder Random + SkyLadder 40.1 41.2 (+1.1) 47.2 49.6 (+2.4) 57.0 60.5 (+3.5) 5.8 5.1 (-0.7) 8.9 8.9 15.8 19.3 (+3.5) Table 3: Performance (average over tasks, in %) of 1B models pretrained on FineWeb-Pro with different methods using an 8K context window. Table 4: Experimental results (average accuracy in %) for models trained on the FineWeb-Pro dataset across varying model sizes. 8 Preprint HumanEval BigCodeBench Greedy Sampling (t = 0.8) Greedy Sampling (t = 0.8) Model Pass@1 Pass@10 Pass@100 Pass@1 Pass@10 Pass@ 32K Random 8K + SkyLadder Random + SkyLadder 17.7 21.3 22.0 23.2 32.4 37.7 37.2 38.2 51.8 59.8 61.0 63. 9.0 9.4 9.9 11.3 16.1 20.6 19.3 20.0 19.7 24.3 23.6 24.1 Table 5: Performance (in %) of 1B models pretrained on 100B Python code data. We follow the protocol of Huang et al. (2024) to evaluate on HumanEval (Chen et al., 2021) and BigCodeBench (Zhuo et al., 2024). is the temperature of sampling. SkyLadder shows consistent improvement, especially for 32K-context models. Method Long Standard Constant Long (32k) Linear (3232k, default) Stepwise Linear (3232k) Sinusoidal (3232k) Exponential (3232k) Cont. Pretraining (4k32k) 9.7 13.5 13.3 14.2 11.5 2.7 50. 54.3 55.3 54.2 54.7 52.9 Table 7: Comparison of 1B models trained with 32K context window using different scheduling methods. Numbers are average accuracy (in %). Context Model Relative Time (%) 8k 8k 32k 32k Random + SkyLadder 100.0% 86.9% (-13.1%) Random + SkyLadder 100.0% 77.8% (-22.2%) Table 8: Comparison of training time efficiency for 1B Models with different context window sizes. larger context window leads to more efficiency gain. Initial Context Window. As the final context window length we is fixed to L, the sole remaining hyper-parameter is ws. Intuitively, setting ws to an excessively large value (e.g. close to L) leaves little room for scheduling, resulting in sub-optimal performance. In Figure 6 (right), we demonstrate that when ws is set to relatively small value (e.g., 8), great performance can be achieved for both short and long contexts. This suggests that there is still potential for further improvement in our default setup. Therefore, we recommend starting with small context window, such as 8 tokens. Scheduling Type. The default scheduling method in SkyLadder is linear scheduling. We evaluate different context window scheduling types (more details in Table 15 and Figure 11 in Appendix A.4.4): (1) Stepwise Linear rounds window size (ws) to multiples of 1K, resulting in step function; (2) Sinusoidal increases quickly early then slows down; (3) Exponential starts slow but accelerates sharply; (4) Continual pretraining setup trains with 4K context windows for 97B tokens, then switches to 32K context for the final 3B tokens. Results in Table 7 show that linear and sinusoidal scheduling outperforms the exponential variant on long benchmarks, likely because the exponential scheduling, with an extended period of short-context at the beginning of training, fails to adequately train on long contexts. Last, the most commonly used continual pretraining setup performs poorly overall, suggesting abrupt context changes harm both short and long task performance. These findings suggest that context window scheduling is superior to both constant long-context pretraining and continual pretraining. Overall, we conclude that the schedule should start from small ws and the expansion should be gradual. We leave it to future work to study more advanced schedules and discover optimal configurations. For instance, it is possible that the schedule needs to be adjusted for various model sizes. More ablations for long-to-short scheduling, combination with BM25, cyclic schedules and scheduling under computing budget can be found in Appendix A.4.4. 4.6 TRAINING EFFICIENCY We observe significant reduction in training time when employing our method. Table 8 illustrates the relative training time efficiency for models with different context window sizes. On 8K models, SkyLadder accelerates training by 13% due to the reduced context window in calculating attention. 9 Preprint When we increase the context window size to 32K, the efficiency gain becomes even more pronounced: our method saves 22% of training time while achieving better performance. 4.7 ANALYSIS We next investigate why SkyLadder, despite being trained on short contexts overall, consistently outperforms the baseline. As language models rely on attention mechanisms to encode context information, we first study how attention patterns change as SkyLadder adjusts its context window. Specifically, during pretraining, we monitor the dynamics of (i) attention entropy (solid lines in Figure 7), where lower entropy is associated with better downstream performance (Zhang et al., 2024b); (ii) attention sink (Xiao et al., 2024), where the initial token in the context receives disproportionately high attention. We also utilize the metric in Gu et al. (2024a) to quantitatively measure the amplitude of attention sink. As shown in Figure 7 (dashed lines), compared with the baseline Random, SkyLadder demonstrates reduced attention entropy, suggesting more concentrated attention pattern. However, slower emergence and lower amplitude of attention sink are simultaneously observed. This suggests that SkyLadders attention is concentrated on the key information in the context instead of the initial token, which explains the performance gain. Figure 7: Dynamics of attention sink and entropy during pretraining 1B models with an 8K context. SkyLadder delays the emergence of attention sink while lowering the overall entropy, indicating more effective attention pattern. 4.8 COMPARISON WITH RELATED WORK We compare our method with another effective approach for improving pretraining in Table 9. As discussed in Section 2, Pouransari et al. (2024) proposed Dataset Decomposition (DD) by segmenting document into sequences of varying lengths and using curriculum during pretraining. However, this approach evitably introduces data bias, as the document lengths in different domains are different (Fu et al., 2024). This explains why DD with only one cycle fails to outperform the IntraDoc baseline. To mitigate this, the authors suggested iterating through multiple cycles of long and short data, which does improve performance substantially. In contrast, our method achieves better performance by avoiding such biases by not altering the order of data based on length. In Appendix A.4.4, we experimented with various cyclic schedules but did not observe any improvements. In fact, we noticed loss spikes between cycles, indicating potential issues with domain shifts. This further supports that our method is safer since it does not disrupt the natural ordering and distribution of the data. Model Standard Avg. Long Avg. IntraDoc +SkyLadder +DD (1 cycle) +DD (8 cycles) 54.3 54.8 (+0.5) 53.9 (-0.4) 54.5 (+0.2) 12.7 13.9 (+1.2) 12.3 (-0.4) 13.5 (+0.8) Table 9: Comparison between SkyLadder and Dataset Decomposition (DD) on 1B models trained with 100B FineWeb-Pro tokens."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We conduct comprehensive controlled study of the impact of context window on pretraining, revealing that shorter context window is more beneficial to the models performance on standard benchmarks. We therefore propose SkyLadder to schedule the context window over the course of training, which gives substantial improvement in downstream performance and compute efficiency. We conclude that context window scheduling is an important dimension for pretraining. In the future, we plan to explore more dynamic and performant scheduling strategies that adapt to model size or data distribution. 10 Preprint"
        },
        {
            "title": "REFERENCES",
            "content": "Chenxin An, Jun Zhang, Ming Zhong, Lei Li, Shansan Gong, Yao Luo, Jingjing Xu, and Lingpeng Kong. Why does the effective context length of llms fall short?, 2024. URL https://arxiv. org/abs/2410.18745. Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. URL https://arxiv.org/abs/2107.03374. Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try ARC, the AI2 reasoning challenge. arXiv:1803.05457v1, 2018. Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. Hantian Ding, Zijian Wang, Giovanni Paolini, Varun Kumar, Anoop Deoras, Dan Roth, and Stefano Soatto. Fewer truncations improve language modeling, 2024. URL https://arxiv.org/ abs/2404.10830. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pp. arXiv2407, 2024. Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128k context, 2024. URL https: //arxiv.org/abs/2402.10171. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 07 2024a. URL https://zenodo.org/records/12608602. Tianyu Gao, Alexander Wettig, Howard Yen, and Danqi Chen. How to train long-context language models (effectively). arXiv preprint arXiv:2410.02660, 2024b. Xiangming Gu, Tianyu Pang, Chao Du, Qian Liu, Fengzhuo Zhang, Cunxiao Du, Ye Wang, and Min Lin. When attention sink emerges in language models: An empirical view, 2024a. URL https://arxiv.org/abs/2410.10781. Yuling Gu, Oyvind Tafjord, Bailey Kuehl, Dany Haddad, Jesse Dodge, and Hanna Hajishirzi. OLMES: standard for language model evaluations. ArXiv, abs/2406.08446, 2024b. URL https://api.semanticscholar.org/CorpusID:270391754. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. 11 Preprint Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. RULER: Whats the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024. Siming Huang, Tianhao Cheng, J. K. Liu, Jiaran Hao, Liuyihan Song, Yang Xu, J. Yang, J. H. Liu, Chenchen Zhang, Linzheng Chai, Ruifeng Yuan, Zhaoxiang Zhang, Jie Fu, Qian Liu, Ge Zhang, Zili Wang, Yuan Qi, Yinghui Xu, and Wei Chu. Opencoder: The open cookbook for top-tier code large language models, 2024. URL https://arxiv.org/abs/2411.04905. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning, 2021. URL https://arxiv.org/abs/2112.09118. Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Chia-Yuan Chang, and Xia Hu. Growlength: Accelerating LLMs pretraining by progressively growing training length, 2023. URL https://arxiv.org/abs/2310.00576. Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, and Xia Hu. LLM maybe longlm: Selfextend LLM context window without tuning. In Forty-first International Conference on Machine Learning, 2024. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: large scale distantly supervised challenge dataset for reading comprehension. In Regina Barzilay and Min-Yen Kan (eds.), Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16011611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https: //aclanthology.org/P17-1147/. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers, 2023. URL https://arxiv.org/abs/2305.19466. Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of naacL-HLT, volume 1, pp. 2. Minneapolis, Minnesota, 2019. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466, 2019. doi: 10.1162/tacl 00276. URL https://aclanthology.org/Q19-1026/. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale ReAding comprehension dataset from examinations. In Martha Palmer, Rebecca Hwa, and Sebastian Riedel (eds.), Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 785794, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1082. URL https://aclanthology.org/D17-1082/. Conglong Li, Minjia Zhang, and Yuxiong He. The stability-efficiency dilemma: Investigating sequence length warmup for training gpt models, 2022. URL https://arxiv.org/abs/ 2108.06084. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023. 12 Preprint Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts, 2023. arXiv:2307.03172. LocalLLaMA. NTK-aware scaled rope allows llama models to have extended (8k+) URL context size without any fine-tuning and minimal perplexity degration, 2023. https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_ scaled_rope_allows_llama_models_to_have/. Yi Lu, Jing Nathan Yan, Songlin Yang, Justin Chiu, Siyu Ren, Fei Yuan, Wenting Zhao, Zhiyong Wu, and Alexander Rush. controlled study on long context extension and generalization in llms. arXiv preprint arXiv:2409.12181, 2024. Xin Men, Mingyu Xu, Bingning Wang, Qingyu Zhang, Hongyu Lin, Xianpei Han, and Weipeng Chen. Base of RoPE bounds context length, 2024. URL https://arxiv.org/abs/2405.14591. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In EMNLP, 2018. Koichi Nagatsuka, Clifford Broni-Bediako, and Masayasu Atsumi. Pre-training BERT with curriculum learning by increasing block-size of input text. In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021), pp. 989996, 2021. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=wHBfxhZu1u. Hadi Pouransari, Chun-Liang Li, Jen-Hao Rick Chang, Pavan Kumar Anasosalu Vasu, Cem Koc, Vaishaal Shankar, and Oncel Tuzel. Dataset decomposition: Faster llm training with variable sequence length curriculum. arXiv preprint arXiv:2405.13226, 2024. URL https://arxiv. org/abs/2405.13226. Alec Radford. Improving language understanding by generative pre-training. OpenAI blog, 2018. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 2019. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Jian Su, Kevin Duh, and Xavier Carreras (eds.), Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 23832392, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/ D16-1264. URL https://aclanthology.org/D16-1264/. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Commonsense reasoning about social interactions. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 44634473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1454. URL https://aclanthology.org/D19-1454/. Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Xi Victoria Lin, Noah Smith, Luke Zettlemoyer, Wen-tau Yih, and Mike Lewis. In-context pretraining: Language modeling beyond document boundaries. In The Twelfth International Conference on Learning Representations, 2024. Leslie Smith. Cyclical learning rates for training neural networks. In 2017 IEEE winter conference on applications of computer vision (WACV), pp. 464472. IEEE, 2017. 13 Preprint Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41494158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1421. URL https: //aclanthology.org/N19-1421. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and Efficient Foundation Language Models. arXiv e-prints, pp. arXiv2302, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023b. doi: 10.48550/ARXIV.2307.09288. URL https://doi.org/10. 48550/arXiv.2307.09288. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2017/ 2017. file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. Haonan Wang, Qian Liu, Chao Du, Tongyao Zhu, Cunxiao Du, Kenji Kawaguchi, and Tianyu Pang. When precision meets position: BFloat16 breaks down RoPE in long-context training. arXiv preprint arXiv:2411.13476, 2024. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming In The Twelfth International Conference on Learning language models with attention sinks. Representations, 2024. URL https://openreview.net/forum?id=NG7sS51zVF. Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, In North Sinong Wang, and Hao Ma. Effective long-context scaling of foundation models. American Chapter of the Association for Computational Linguistics, 2023. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 23692380, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259/. Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, and Danqi Chen. Helmet: How to evaluate long-context language models effectively and thoroughly, 2024. URL https://arxiv.org/abs/2410.02694. 14 Preprint Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 47914800, 2019. Hao Zhang, Jae Ro, and Richard William Sproat. Semi-supervised url segmentation with recurrent neural networks pre-trained on knowledge graph entities. In The 28th International Conference on Computational Linguistics (COLING 2020), 2020. Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model, 2024a. URL https://arxiv.org/abs/2401.02385. Zhisong Zhang, Yan Wang, Xinting Huang, Tianqing Fang, Hongming Zhang, Chenlong Deng, Shuaiyi Li, and Dong Yu. Attention entropy is key factor: An analysis of parallel context encoding with full-attention-based pre-trained language models, 2024b. URL https://arxiv. org/abs/2412.16545. Liang Zhao, Tianwen Wei, Liang Zeng, Cheng Cheng, Liu Yang, Peng Cheng, Lijie Wang, Chenxia Li, Xuejie Wu, Bo Zhu, et al. Longskywork: training recipe for efficiently extending context length in large language models. arXiv preprint arXiv:2406.00605, 2024a. Yu Zhao, Yuanbin Qu, Konrad Staniszewski, Szymon Tworkowski, Wei Liu, Piotr Miłos, Yuxiang Wu, and Pasquale Minervini. Analysing the impact of sequence composition on language model pre-training. arXiv preprint arXiv:2402.13991, 2024b. Fan Zhou, Zengzhi Wang, Qian Liu, Junlong Li, and Pengfei Liu. Programming every example: Lifting pre-training data quality like experts at scale. arXiv preprint arXiv:2409.17115, 2024. Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877, 2024. 15 Preprint Model Tinyllama 1b Tinyllama 120M Tinyllama 360M Llama3.2 3b Vocab Size Layers Heads Embedding Dim Intermediate Size Normalization Normalization ϵ Query Groups Bias RoPE θ 32000 22 32 2048 5632 RMSNorm 1 105 4 No 10000 if = 8k 1000000 if = 32k 32000 12 12 768 2048 RMSNorm 1 105 1 No 10000 32000 18 16 1024 4096 RMSNorm 1 105 16 No 10000 32000 28 24 3072 8192 RMSNorm 1 105 8 No 100000 Table 10: Model Configuration Parameter Optimizer AdamW-β1 AdamW-β2 Learning Rate Schedule Peak Learning Rate Minimum Learning Rate Warmup Steps Gradient Norm Clipping Total Steps Global Batch Size Weight Decay Value AdamW 0.9 0.95 Cosine 4e-4 4e-5 2000 1 100,000 1,048,576 (220) tokens 0.1 Table 11: Training Configuration"
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 MODEL ARCHITECTURE In Table 10, we list the architecture choices of the models trained, including the 120M, 360M, and 1B models based on the TinyLlama architecture (Zhang et al., 2024a). The 3B model is based on Llama3.2 architecture (Dubey et al., 2024). A.2 TRAINING CONFIGURATIONS We include details of the training configurations in Table 11. All models, irrespective of size or context window length, are trained on this same set of hyperparameters. For most of the hyperparameter values, we follow the TinyLlama (Zhang et al., 2024a) project, therefore our results are highly reproducible. A.3 IMPLEMENTATION We provide the pseudocode for implementing SkyLadder with Flash Attention 2 (Dao, 2024). The only change is to apply local causal masking with size w, and combine them with the original document boundaries under the IntraDoc scenario. It can easily be integrated into any model before calculating attention. The rest of the training pipeline remains unchanged. 16 Preprint SkyLadder with Flash Attention 2 # u : # , , : r , key , and u # b d e : n i f u t # n d : t # i g p : # : a n n r i t s e l t and a r t t o x window g ( RoPE l ) ( EOS e s o ) t d m o t = min ( , c e a e h ( i g p ) ) k n i = np . n (w, , w) # [w , 2w , 3w , . . . ] s r c : s u r = np . o 1 ( k n i , b d e ) # max g e e two n i a q = m e n ( k n i , ) # u i e n e h , u l = c q s ( k n i , ) u d by s t i t = s t r f ( , , , e n , s e , s = e ) A.4 ADDITIONAL RESULTS A.4.1 CONTEXT WINDOW STUDY Figure 8: Validation perplexity (evaluated on sliding window of 512) on models with different context lengths. Figure 9: Left: Evaluation perplexity of models with different packing or masking strategies. Right: Downstream performance over 9 tasks of different models. Figure 10: Validation perplexity v.s. training tokens with different context windows and base of RoPE, θ. Evaluation is done on sliding window of varying length (x-axis) on the validation documents. In Figure 8. We plot the validation perplexity of models with different context windows under the Random, IntraDoc, and BM25 settings. We observe consistent trend that shorter-context model has lower evaluation perplexity on shorter sequence under all settings. 17 Preprint In Figure 9, we plot the evaluation perplexity and downstream performance of models with different packing or masking strategies. We conclude that overall IntraDoc achieves the best performance, with consistently lower PPL and higher downstream accuracy. We think that this is partially due to the shorter context window that the IntraDoc model is trained on. A.4.2 ABLATIONS FOR CONTEXT WINDOW STUDY Base of RoPE. It has been shown that the value of RoPE may have significant impact on the models long context performance, and longer context requires larger base (Men et al., 2024). Therefore, we increase the RoPE base to 100,000, which is sufficiently large according to Men et al. (2024). In Figure 10, we observe an improvement for long-context models on long-context evaluation. However, the large gap between shorter and longer model still remains, therefore rejecting the hypothesis that the RoPE base is the key contributing factor to the superior performance of short-context models. A.4.3 SKYLADDER EVALUATION Reading Comprehension For reading comprehension, we evaluate the following benchmarks: Hotpot QA (2-shot) (Yang et al., 2018), SQuAD (4-shot) (Rajpurkar et al., 2016), NaturalQuestions (NQ) (2-shot) (Kwiatkowski et al., 2019), TriviaQA (2-shot) (Joshi et al., 2017), and RACE-high (0-shot) (Lai et al., 2017). We follow the setup by Zhao et al. (2024b), where NQ and TriviaQA use retrieved documents as contexts. For RACE, we use lm-evaluation-harness (Gao et al., 2024a) to compare the PPL between options. Closed-book QA NQ TriviaQA Average Random +SkyLadder IntraDoc +SkyLadder 6.1 9.0 7.8 8.2 11.9 17.5 14.7 17. 9.0 13.2 11.3 12.8 Table 12: 1B model (trained on CC) performance on closed-book QA tasks. Performance are reported in Exact Match (%) using the evaluation script of Zhao et al. (2024b). Closed-book QA We additionally evaluate the closed-book QA performance of our models without access to any document. In Table 12, we notice significant improvement in our methods compared to the baselines for answering closed-book questions. This is consistent with the results that our models show improvements on standard benchmarks that contain commonsense knowledge. A.4.4 SKYLADDER ABLATIONS Long-to-Short Schedule possibility that SkyLadder works better than baseline on standard benchmarks, which are typically short, might be that the training data mix has more short-context data after applying the mask. To study the effect of pure data distribution, we conduct an ablation of reversing the original short-to-long schedule and name it as long-to-short schedule. This schedule spends the same number of tokens (64B) in the changing phase, before the constant training phase in = 8K for another 36B tokens. In Table 13, we show that the long-to-short schedule is not helpful to the models performance in both short and long evaluation tasks. This highlights that the context window needs to be scheduled, rather than simply having data mixture of long and short contexts. Standard Avg Long Avg. Standard Avg. Long Avg. No Scheduling Short-to-Long Long-to-Short 52.5 55.2 (+2.7) 52.6 (+0.1) 11.1 12.3 (+1.2) 10.7(-0.4) Random BM25 +SkyLadder 46.3 47.5 (+1.2) 49.8 (+3.5) 15.3 16.4 (+1.1) 17.0 (+1.7) Table 13: Performance (%) of 1B models with different schedule types. All models are trained on the same 100B FineWeb-Pro tokens with final context length of 8K. Short-to-long scheduling is consistently better than long-toshort scheduling. Table 14: Performance (%) of 1B models with different schedule types. All models are trained on the same 100B CommonCrawl tokens with final context length of 8K. BM25 packing, when combined with SkyLadder, significantly boosts performance on long tasks. Preprint Schedule Function Constant Linear Stepwise Sinusoidal we ws + (we ws) αx wews (cid:107) ) max(ws, (cid:16) (cid:106) L(x) ws + (we ws) sin Exponential ws (cid:17) αx wews (cid:16) we ws (cid:17) απx 2(wews) Table 15: Functions for different context window schedule types. We set ws = 32 and we = 32768 in our experiments. The for rounding is set to 1024. Figure 11: Plot of various scheduling types. Combination with BM25 Packing As SkyLadder only changes the context length via masking without altering the underlying data, it is orthogonal to any advanced data packing method such as Shi et al. (2024); Ding et al. (2024). In Table 14, we combine the SkyLadder with the BM25 packing method. We show that the model achieves even better performance on both short and long context evaluation than BM25 without scheduling, which is also better than the Random baseline. This reveals that our methods can be combined with more advanced packing techniques to further boost pretraining performance. Alternative Schedule Types We explore various types of short-to-long scheduling following different functions as mentioned in Section 4.5. Table 15 shows the details of the schedule as function of t, and Figure 11 shows an illustration of the different schedules types. In Table 7, we show that smoother increase following the sinusoidal schedule works the best for long-context evaluation, while also achieving strong performance on standard benchmarks. Figure 12: An illustration of the effect of different α. Dashed lines represent the current context window for each step, and solid lines are the loss evaluated at 8K length. Figure 13: An illustration of the cyclic schedules with gradual increases or jumps. Dashed lines represent the context length for each step, and solid lines are the loss evaluated at 8K length. represents the number of cycles. Expansion Rate We illustrate the effect of the rate of expansion α in Figure 12. As the evaluation is done on 8K contexts, models with lower rate (and shorter context window) will have higher loss as the evaluation length is out-of-distribution. However, eventually, all models loss converges to low level after the schedule reaches 8K. The detailed numbers of validation loss after pretraining can be found in Table 16. Following previous work (Fu et al., 2024; Hoffmann et al., 2022; Kaplan et al., 2020), we consider loss difference larger than 0.01 as significant. We conclude that setting reasonable rate of 1/8 balances both short and long-context loss. 19 Preprint Rate (1/α) Tokens to Reach 8k (B) 1 2 4 8 9 10 11 12 8 16 32 64 72 80 88 96 Baseline (Constant) Le = 512 Le = 4k Le = 8k 2.751 2.741 2.740 2.732 2.731 2.732 2.730 2.729 2.780 2.563 2.551 2.551 2.553 2.553 2.555 2.554 2.557 2.590 2.522 2.514 2.515 2.519 2.519 2.522 2.521 2.526 2.549 Table 16: Validation loss with different expansion rates. box is colored red if it is significantly worse (difference > 0.01) than the best of the column. Le is the evaluation context length. All models are of size 120M and trained on 100B tokens. Type Random +SkyLadder Gradual Jump Gradual Jump Gradual Jump Number of Cycles Tokens per Cycle (B) Le = 512 Le = 8k 4.5 9 2.5 5 1.5 3 16 8 32 16 64 2.780 2.732 2.743 2.744 2.732 2.733 2.728 2.727 2.549 2.519 2.530 2.532 2.521 2.521 2.524 2.522 Table 17: Validation loss with cyclic schedules. Le represents the evaluation context length. All models are of size 120M and trained on 100B tokens. Cyclic Schedule Inspired by the cyclic schedule learning rate (Smith, 2017), we also wonder if cycles are helpful in the schedule. In Figure 13, we show two cyclic schedules. In the Jump schedule, w(t) will decrease to ws immediately after reaching L. One the other hand, the Gradual schedule means an shape alternating between we and ws. Notably, in the discontinuous Jump schedule, we notice significant increase in long-context perplexity when we train on only short contexts for an extended period. However, as long as increases back to L, the performance will return. In Table 17, we show that these schedules have no major impact on the final performance. This highlights that the method does not introduce additional bias in data selection: different from existing methods such as Pouransari et al. (2024) that proposes to train on short data first, followed by long data, we do not assume such curriculum on data. We argue that the context window size should be independent of the data lengths to avoid bias in training only on certain domains of data. Initial Window Length We show the effect of having different ws, the initial window length when the training starts. In Table 18, we show that the optimal starting length is 8 tokens. The trend is the same across both α = 1/4 and α = 1/8. This suggests that the starting length should be sufficiently small, irrespective of the expansion rate. It also reveals that prior studies, such as Jin et al. (2023) and Pouransari et al. (2024) that start with an initial length of 256 could be suboptimal. Compute Budget We show that when the total number of tokens is limited, our method can still improve language model performance. In Table 19, we choose 12.5B, 25B, and 50B total tokens as the computing budget, and vary the expansion rate so that reaches at the same point during training. We observe that under different token budgets, the performance trend is the best: gradually expanding the context window gives better performance than rapid increase. 20 Preprint ws Le = 512 Le = 4k Le = 8k 4 8 16 32 64 128 256 4 8 16 32 64 128 256 8192 α = 1/4 2.731 2.730 2.733 2.740 2.742 2.748 2. 2.546 2.545 2.551 2.551 2.557 2.564 2.566 α = 1/8 2.727 2.725 2.729 2.732 2.735 2.743 2.748 2.780 2.549 2.545 2.550 2.553 2.553 2.564 2.567 2. 2.510 2.508 2.513 2.515 2.520 2.528 2.527 2.515 2.510 2.516 2.519 2.519 2.530 2.531 2.549 Table 18: Final validation loss after training 120M models on 100B tokens with different ws when α = 1/4 and α = 1/8. Le represents the context length of evaluation. cell is colored red if its loss has difference larger than 0.01 than the columns best. ws = 8192 equals to no scheduling. α Tokens to (B) % of Token Budget Le = 512 Le = 4096 Le = 1 2 4 8 1/2 1 2 4 1/4 1/2 1 2 8 4 2 1 16 8 4 2 32 16 8 Baseline Baseline Baseline Token Budget = 12.5B 2.912 64% 2.933 32% 2.958 16% 2.976 8% 3.008 Token Budget = 25B 64% 32% 16% 8% 2.829 2.841 2.851 2.873 2. Token Budget = 50B 64% 32% 16% 8% 2.771 2.781 2.789 2.795 2.839 2.732 2.746 2.767 2.782 2.823 2.650 2.656 2.665 2.683 2.734 2.590 2.596 2.603 2.607 2.652 2.698 2.709 2.729 2.743 2. 2.617 2.619 2.626 2.645 2.700 2.556 2.560 2.564 2.567 2.616 Table 19: Final validation loss under different training token budgets and expansion rate α with 120M models. Le represents the context length used for evaluation. % of Token Budget means how many tokens are spent in the expansion phase with w(t) increasing. Under all token budgets, we observe consistent improvement when we spend around 64% in expansion, and 36% in the stable phase. Sliding Window Expansion possible alternative to SkyLadder (using local causal masks) is to use sliding window attention with window size of w(t) that changes with the training time. Formally, the mask becomes: Mi,j = (cid:26) if otherwise. so that each token in the context has fixed preceding context of size w. When w(t) reaches L, the mask becomes equivalent to causal mask. We compare the performance of the two in Table 20 and observe that the sliding window approach shows slightly better performance in long tasks and 21 Preprint Standard Avg. Long Avg. 52.5 Random 55.2 (+2.7) +SkyLadder w/ local causal +SkyLadder w/ sliding window 54.4 (+1.9) 11.1 12.3 (+1.2) 12.8 (+1.7) Table 20: Performance (%) of 1B models with different masking schemes. All models are trained on the same 100B FineWeb-Pro tokens with final context length of 8K. Both implementations of SkyLadder outperform the baseline, and the sliding window approach excels at long tasks with slight performance drop on standard benchmarks. worse performance in standard benchmarks. This is likely because overall there are more tokens with longer preceding contexts for the sliding window approach. In both cases, SkyLadder outperforms the Random baseline. We think that future work could further investigate the differences between SkyLadder implementations with causal and sliding window attention, such as the formation of attention sink (Gu et al., 2024a)."
        }
    ],
    "affiliations": [
        "City University of Hong Kong",
        "National University of Singapore",
        "Sea AI Lab"
    ]
}