{
    "paper_title": "Words or Vision: Do Vision-Language Models Have Blind Faith in Text?",
    "authors": [
        "Ailin Deng",
        "Tri Cao",
        "Zhirui Chen",
        "Bryan Hooi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language Models (VLMs) excel in integrating visual and textual information for vision-centric tasks, but their handling of inconsistencies between modalities is underexplored. We investigate VLMs' modality preferences when faced with visual data and varied textual inputs in vision-centered settings. By introducing textual variations to four vision-centric tasks and evaluating ten Vision-Language Models (VLMs), we discover a \\emph{``blind faith in text''} phenomenon: VLMs disproportionately trust textual data over visual data when inconsistencies arise, leading to significant performance drops under corrupted text and raising safety concerns. We analyze factors influencing this text bias, including instruction prompts, language model size, text relevance, token order, and the interplay between visual and textual certainty. While certain factors, such as scaling up the language model size, slightly mitigate text bias, others like token order can exacerbate it due to positional biases inherited from language models. To address this issue, we explore supervised fine-tuning with text augmentation and demonstrate its effectiveness in reducing text bias. Additionally, we provide a theoretical analysis suggesting that the blind faith in text phenomenon may stem from an imbalance of pure text and multi-modal data during training. Our findings highlight the need for balanced training and careful consideration of modality interactions in VLMs to enhance their robustness and reliability in handling multi-modal data inconsistencies."
        },
        {
            "title": "Start",
            "content": "Words or Vision: Do Vision-Language Models Have Blind Faith in Text? Ailin Deng Tri Cao Zhirui Chen Bryan Hooi National University of Singapore {ailin,zhiruichen}@u.nus.edu, tricao2001vn@gmail.com bhooi@comp.nus.edu.sg 5 2 0 2 4 ] . [ 1 9 9 1 2 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Vision-Language Models (VLMs) excel in integrating visual and textual information for vision-centric tasks, but their handling of inconsistencies between modalities is underexplored. We investigate VLMs modality preferences when faced with visual data and varied textual inputs in visioncentered settings. By introducing textual variations to four vision-centric tasks and evaluating ten Vision-Language Models (VLMs), we discover blind faith in text phenomenon: VLMs disproportionately trust textual data over visual data when inconsistencies arise, leading to significant performance drops under corrupted text and raising safety concerns. We analyze factors influencing this text bias, including instruction prompts, language model size, text relevance, token order, and the interplay between visual and textual certainty. While certain factors, such as scaling up the language model size, slightly mitigate text bias, others like token order can exacerbate it due to positional biases inherited from language models. To address this issue, we explore supervised fine-tuning with text augmentation and demonstrate its effectiveness in reducing text bias. Additionally, we provide theoretical analysis suggesting that the blind faith in text phenomenon may stem from an imbalance of pure text and multi-modal data during training. Our findings highlight the need for balanced training and careful consideration of modality interactions in VLMs to enhance their robustness and reliability in handling multi-modal data inconsistencies. 1. Introduction With the rise of Vision-Language Models (VLMs) [4, 9, 24], these models are increasingly applied in complex multi-modal tasks, such as retrieval-augmented generation (RAG) [41] and multi-modal agents [12, 17, 19], where they handle large, context-rich cross-modal inputs. In these practical scenarios, inconsistencies between visual and textual corresponding author Figure 1. Illustration of the Blind Faith in Text phenomenon in Vision-Language Models (VLMs). These models demonstrate strong tendency to trust textual data, when it is inconsistent with the visual data or even incorrect. inputs are common, as additional textual data may be irrelevant or even misleading [35]. Despite their strong performance on vision-centric benchmarks [15, 44, 45], VLMs capability to handle such inconsistencies remains underexplored. This gap motivates our study, as understanding and addressing VLMs tendencies under these conditions is essential for their safe and reliable application in real-world, multi-modal contexts. In this work, we explore an open but underexplored question: How do VLMs handle inconsistencies between visual and textual inputs? This question drives us to investigate the following aspects: 1. Modality Preference: What modality do VLMs prefer when there are inconsistencies between vision and language data? 2. Robustness to Text Perturbation: Can these models maintain their performance on vision-centric tasks when faced with corrupted textual data? 3. Influencing Factors: What factors affect the modality preference in VLMs? To address these questions, we construct comprehensive benchmark by introducing textual variations to four vision-centric tasks and evaluate ten VLMs, including both proprietary and open-source models. Our findings reveal phenomenon we term blind faith in text: when inconsistencies arise between visual and textual inputs, VLMs tend 1 to overly trust the textual data, even when it contradicts visual evidence. This text bias not only leads to significant performance degradation when the text is corrupted but also raises potential safety concerns in practical applications."
        },
        {
            "title": "We further investigate this issue by examining factors",
            "content": "that influence text bias: Instruction Prompts: While instructions can modestly adjust modality preference, their effectiveness is limited. Language Model Size: Scaling up the language model size slightly mitigates text bias, but the effect saturates in larger models. Text Relevance: The preference for textual data increases with text relevance. Token Order: Placing text tokens before image tokens exacerbates text bias, possibly due to positional biases inherited from language models. Uni-Modal Certainty: The interplay between visual and textual certainty influences modality preference. To mitigate text bias, we explore supervised fine-tuning with text augmentation, demonstrating its effectiveness even with limited data. Additionally, we provide theoretical analysis suggesting that the blind faith in text phenomenon may stem from an imbalance of pure text and multi-modal data during training, as VLMs are built upon large language models primarily trained on textual data. Our contributions are summarized as follows: We uncover the blind faith in text phenomenon, where VLMs prefer language data over visual data when inconsistencies occur in context. We confirm that this text bias leads to significant performance drops under text corruption, even in vision-centric tasks where VLMs typically excel. We identify key factors influencing text bias, including instruction prompts, language model size, text relevance, token order, and uni-modal certainty. We demonstrate that supervised fine-tuning with text augmentation effectively reduces text bias. We provide theoretical analysis suggesting that the imbalance of pure text and multi-modal data during training contributes to the blind faith in text phenomenon. 2. Preliminaries Given model fvlm(; θ) parameterized by θ, sample := (I, T, Q) contains an image I, textual information , and question Q, with corresponding ground truth . We can obtain the answers under three conditions: (1) only given the image; (2) only given textual information; (3) https://github.com/d-ailin/blind-faith-in-text given both modalities information: ˆYimg := fvlm(Q, I; θ), ˆYtxt := fvlm(Q, ; θ), ˆYmix := fvlm(Q, I, ; θ). Generation Certainty. The response generation certainty can be estimated based on the length-normalized predictive likelihood of the response sequence. Formally: ( ˆY X; θ) := 1 ˆY P( ˆYi X, ˆY<i; θ) , ˆY (cid:89) i= where ˆYi represents the i-th token of ˆY , and ˆY<i are the tokens generated before i-th token in ˆY . By considering this certainty in each modality separately, we can compute the Uni-Modal Certainty to quantify the generation uncertainty in each modality. Specifically, it is computed given only the image or the text: Pimg := ( ˆYimg Q, I; θ), Ptxt := ( ˆYtxt Q, ; θ). 2.1. Text Variations To comprehensively study the effect of text variations, we consider three types of variations: Match, Corruption, and Irrelevance cases given the question and the original ground-truth answer : Match. We denote Tm as matching text such that (Tm, Q) has ground-truth , indicating the text provides sufficient and relevant information, allowing for answering the question correctly when only given the text. Corruption. We denote corrupted text as Tc such that (Tc, Q) has ground-truth Yc = , indicating that the text is relevant and sufficient for answering, but leads to different answer when relying only on the text. Irrelevance. We consider text to be an irrelevant text Tirr (i.e., Tirr I, Q), indicating the textual information is unrelated to both the image and question, thus insufficient to answer when relying only on the text. Starting with the base set = {(I, Q)}, we derive three variant sets by adding each text type as context: Qm = {(I, Tm, Q)}, Qc = {(I, Tc, Q)}, and Qirr = {(I, Tirr, Q)}. The motivation for these cases is to examine how models handle different text types: matching text assesses use of relevant information, corrupted text evaluates handling of misleading information, and irrelevant text checks the models ability to ignore distractions. Including matching text also prevents models from simply rejecting all text, as might happen if only irrelevant or corrupted text were used in prior study [40]. Together, these cases provide fuller view of VLM performance across varied text inputs. 2 2.2. Model Behavior Given both vision and language information, we categorize the model behaviors into three conditions: (1) consistent with Image answer ( ˆYmix = ˆYimg); (2) consistent with Text answer ( ˆYmix = ˆYtxt); (3) Other cases ( ˆYmix / { ˆYimg, ˆYtxt}). To better understand the model behavior when inconsistency between vision and language data happens, we only consider the cases where the Image answers are different from the Text answers (under exact match) in empirical analysis (i.e., ˆYimg = ˆYtxt). Formally, for problem set {Qm, Qc, Qirr}, the proportion of the Image, Text and Other answers as pimg, ptxt and po are: pimg := ptxt := {X ˆYmix = ˆYimg} {X ˆYmix = ˆYtxt} , , po := {X ˆYmix / { ˆYimg, ˆYtxt}} , where = {X ˆYimg = ˆYtxt}, as we only consider the inconsistent cases where the Image answers are different from the Text answers. 2.3. Metrics Text Preference Ratio (TPR). We define TPR to quantify the models preference for text over image-based answers. It is calculated as: TPR := ptxt ptxt + pimg . The TPR indicates text bias by showing the likelihood of the model choosing text over visual information when they are inconsistent. higher TPR reflects stronger text bias. Accuracy. For any problem set Q, we have: Acc(fvlm; Q) = (cid:80) XQ 1[fvlm(X) = ] . Macro Accuracy. Macro(f ) is the average accuracy of model over different problem sets with text variations: Macro(fvlm) := 1 (Acc(fvlm; Qm) + Acc(fvlm; Qc) + Acc(fvlm; Qirr)). Normalized Accuracy. Norm(fvlm; Q) measures how model is affected by the text variation, compared to the Base Accuracy, i.e., its accuracy on the base problem set B. For any problem set under text variation, we calculate the corresponding normalized accuracy by Norm(fvlm; Q) = Acc(fvlm; Q) Acc(fvlm; B) . Text Construction Given the image, the question and the answer, your task is to: 1. Generate an accurate Description 1 which can be used for answering the question correctly without using the image. 2. Generate wrong description Description 2 which can be used for answering the question with completely wrong answer answer 2 without using the image. 3. Make sure both descriptions are sound and concise. 4. The wrong descriptions sentence structure should be similar to the correct description. Here are the questions and answers: Question: {question} Answer: {answer} Please output the two statements in this format: Description 1: Description 1 Description 2: Description 2 Answer 2: answer 2 Figure 2. Prompt for generating matched and corrupted text given an image, the question and the ground-truth answer. We substitute {question} and {answer} with the specific sample. 3. Empirical Analysis In this section, we aim to answer the following research questions: (Modality Preference) How are the models behaviors under different text conditions? Is there any modality preference bias in the models? (Performance Impact) To what extent can text bias affect the models performance, particularly with corrupted text in the context? (Influencing Factors) Is text bias affected by instructions, size of language models, or token position? 3.1. Setup Tasks and Datasets. We evaluate model performance on VQA datasets covering four domains, including (1) General VQA: 1,000 samples from VQAv2 [15] validation split; (2) Document VQA: 1,000 samples from DocVQA [29] validation split for chart and table understanding.; (3) Math Reasoning: 1,000 samples from the (4) Brand Recogniminitest split of MathVista [28]. tion: 2,500 samples from phishing detection dataset test split [22] using HTML text and webpage screenshots, focused on identifying websites brand. Note, each question is expanded with three types of text variations, creating total of 16,500 test samples derived from 5,500 unique questions. Our study includes 10 VLMs, covering proprietary models [2, 3] and open models [1, 10, 26, 38]. The temperature is set to 0 for deterministic generation. We include all experimental details, examples and results in the Appendix. Text Variation Construction. We use GPT-4o model to generate matching and corrupted text. Given an image I, question Q, and answer , we prompt the model to produce supporting description as the matched text Tm and contradictory description as the corrupted text Tc, allowing models to produce the correct answer and an incorrect answer without image input. The prompt used for text generation is shown in Figure 2. We extract Description 1 and Description 2 as the matched and corrupted texts, respectively. To construct irrelevant text, we randomly sample passages from the WikiText dataset [30], which contains texts from Wikipedia articles. These sampled texts serve as irrelevant cases, as they are factual but unrelated to the image and question. See Appendix for examples. Query Instruction. Humans may be uncertain about which information to trust when inconsistency arises. To reduce ambiguity, following previous works [35], we prepend sentence to the textual information, alerting the model to potential errors and encouraging cautious use of the textual information. <image> Here are some additional information which are text descriptions based on the image to assist you for answering the later question. Note, the information could be irrelevant, missing some information or inaccurate, please use it with caution: <text information> --------------------------- <question> Sanity Check. We evaluate the constructed text variations with model performance when only the text context is provided for answering questions. We expect that the matched text supplies enough information for correct answers, the corrupted text misleads the model into incorrect answers, and the irrelevant text lacks relevant information, leading the model to respond either randomly or with uncertainty (e.g., dont know). VQAv2 Model Claude Sonnet GPT-4o Molmo-7B-D Base Match Corruption 66.88 78.39 76.33 16.17 17.59 18.74 84.39 90.07 88. Irrelevance 24.39 18.67 35.40 Table 1. Text-only accuracy (%) across different models. It provides sanity check for the constructed text when matched, corrupted, or irrelevant. 3.2. Blind Faith in Text In Figures 3 and 4, we observe two key findings that illustrate the phenomenon of blind faith in text. First, when textual data is inconsistent with visual data yet is relevant, models tend to favor the text, as indicated by high text preference ratios in both match and corruption cases. For example, Claude Haiku shows text preference ratios of 87% 4 Figure 3. Model behaviors over different models when text is corrupted, matched or irrelevant. and 83% under match and corruption in VQAv2, respectively. Overall, high preference ratios are observed (usually over 50%), particularly for open models. Second, some models, such as Qwen2-VL-7B, show even higher text preference in corruption cases (29%) compared to match cases (13%), indicating tendency to rely on text even when its incorrect, thus demonstrating limited discernment between accurate and inaccurate textual information. These results underscore the blind faith in text seen across models. Open models exhibit stronger text bias compared to proprietary models. Although open models perform comparably to or even surpass proprietary models in standard VQA benchmarks, our results show that open models display much higher text preference in our benchmark, even in the presence of incorrect text. This text bias remains prominent even in the efficient versions of proprietary models (i.e., GPT-4o mini and Claude Haiku). Overall, Claude Sonnet shows the most robustness under text-based interference among the evaluated models. This issue is critical in the development of open models, particularly when deploying them in real-world, complex applications, such as multi-modal agents or online shopping platforms [19, 39]. 3.3. Performance Impact Figure 4. Text Preference Ratio (TPR) of all models under different text variations. Most models exhibit high text preference bias when the textual information is relevant even if they are incorrect, especially for open models. Among the proprietary models, Claude-Sonnet exhibits the strongest robustness to corrupted text. VQAv2 DocVQA Base Corruption Norm TPR Base Corruption Norm TPR Base Corruption Norm TPR Model 80.28 69.82 GPT-4o mini 77.42 50.08 Claude Haiku 48.98 78.39 GPT-4o 29.14 66.88 Claude Sonnet 84.19 LLaVA-NeXT-7B 79.45 80.83 LLaVA-NeXT-13B 81.02 LLaVA-NeXT-34B 82.96 67.64 80.20 75.65 Phi3.5 60.63 76.33 Molmo-7B-D 85.51 70.23 Qwen2-VL-7B 73.83 50.99 90.25 101.93 36.10 46.40 51.70 46.50 64.50 59.41 52.30 41.00 58.90 56.30 35.80 36.20 34.00 43.10 44.90 55.40 55.04 58.43 86.59 97.24 18.60 19.10 23.61 64.60 51.90 63.63 38.20 40.20 73.60 84.60 10.00 11.00 15.10 50.50 38.40 57.50 45.70 48.29 69.95 87.57 54.97 56.89 61.98 51.47 73.27 52.18 23.90 19.80 41.20 49.30 19.70 20.60 21.70 22.20 32.90 28. 51.55 25.54 70.75 68.17 28.69 37.61 42.87 35.23 49.29 50.79 52.42 82.70 27.09 9.58 85.52 74.43 67.56 74.05 59.40 29.22 52.07 47.67 17.96 3.21 87.77 86.84 82.69 40.51 57.20 37.41 69.40 68.80 85.00 87.00 53.60 57.70 64.00 78.20 74.00 90.50 MathVista Table 2. Performance (%) reported as Base Accuracy, Corruption Accuracy, Normalized Corruption Accuracy (Norm) and Text Preference Ratio (TPR) under corruption. Bold: best performance; underline: second best. Full results under all text variations are in the Appendix. The strong text bias leads to significant performance drops under corruption. Given the phenomenon of blind faith in text, it is essential to assess its impact in performance, especially with corrupted text. As shown in Table 2, performance drops sharply in the presence of corrupted text. For instance, Qwen2-VL-7B accuracy on VQAv2, DocVQA, and MathVista falls to 59%, 63%, and 52% of its original levels, an approximate 50% reduction. While proprietary models show greater stability with smaller declines, the efficient variants of these models also experience significant drops. This highlights the need for caution when deploying efficient variants of proprietary models in safety-critical applications. Brand Detection Base Corruption Norm TPR Model 7.48 88.84 GPT-4o mini 6.44 84.40 Claude Haiku 0.83 88.68 GPT-4o 90.20 0.96 Claude Sonnet 59.17 LLaVA-NeXT-7B 78.60 40.65 LLaVA-NeXT-13B 83.00 23.49 LLaVA-NeXT-34B 66.28 50.45 84.40 Phi3.5 60.40 87.44 Molmo-7B-D 2.99 89.68 Qwen2-VL-7B 95.44 93.27 101.22 100.04 70.39 72.29 80.77 71.90 47.39 96.43 84.8 78.72 89.76 90.24 55.32 60.00 53.52 60.68 41.44 86.48 Table 3. Performance on the Brand Detection dataset reported in Base Accuracy, Corruption Accuracy, Normalized Corruption Accuracy (Norm), and Text Preference Ratio (TPR). Bold: best performance; underline: second best performance. Text bias can introduce safety risks in real-world applications. Beyond general VQA tasks, we examine the safety implications of text bias in real-world context: brand recognition in webpage understanding [22]. In this task, models typically use both an HTML string and webpage screenshot to identify websites brand. However, HTML content can be easily manipulated with incorrect or misleading information; for example, phishing websites may inject targeted brand names into the HTML to evade detection systems, which is taken as corruption cases. Further details about this setting are provided in the Appendix. As shown in Table 3, under the corruption condition, most open models, such as Molmo-7B-D, show significant performance drop, with accuracy reduced by nearly 50% compared to the original performance. In contrast, proprietary models show slight resilience, likely due to their ability to use information from the HTML string while being less affected by injected content. 3.4. Influencing Factors In this section, we explore factors that contribute to text bias in VLMs and identify key influences. Unless otherwise noted, results are based on the VQAv2 dataset. Instructions can reduce text bias but with limitations. We further investigate whether text bias can be mitigated by explicitly instructing models to focus on image information and reduce reliance on text. Inspired by previous work [35], we prepend instructions to the questions to guide the modFigure 5. The effect of different factors (prompting, language model size, text relevance) on text bias. Left: Instructional prompts influence modality preference slightly; text preference drops from 16.8% to 14.2% with Focus on Image vs. Focus on Text in QwenVL-2-7B. Middle: Scaling the language models (7B, 13B, 34B) in LLaVA-NeXT models decreases text bias but only marginally. Right: Increasing text relevance to the query with BM25 retrieval, raises text bias. els on which modality to prioritize. Specifically, we compare text preference ratios in three cases: neutral, Focus on Text, and Focus on Image. In the modified prompts, we add the phrases Please focus on the text to answer the question and Please focus on the image to answer the question respectively. As shown in Figure 5 (left), the instructions influence modality preference, but the effect is In QwenVL-2-7B, the average text preference ralimited. tio only shifts from 16.8% to 14.2% when changing the instruction from Focus on Text to Focus on Image. This limited effect may also indicate weak instruction-following capabilities in cross-modal interactions. Training with larger language model can reduce text bias but saturates. Language models are essential components in current VLM architectures [9, 24]. Scaling up language models in VLMs generally enhances model capabilities [18, 36]. We thus study the impact of model size on text bias using the LLaVA-NeXT models. As shown in Figure 5 (Middle), increasing model size from 7B to 34B reduces text bias overall. The 7B model exhibits high text preference with similar ratios for both match and corruption cases (86.3% and 85.5%, respectively). When scaled to 14B, there is notable improvement, with gap of 12% between match and corruption text preferences. Further scaling to 34B continues to reduce text preference overall, however the gap between matched and corrupted text preference ratios remains stable. Relevant text is more likely to influence vision-language models. In applications like RAG, retrieved text can appear relevant to query but may ultimately be unhelpful for accurate answers. To examine how text relevance affects text bias in VLMs, we use BM25 rank retrieval [33] with the question as the query, varying top-k results to indicate relevance levels. The Top-1 result is the most relevant to the question but remains unrelated to the image, making it unhelpful for answering the question. As shown in Figure 5 (Right), text bias increases with text relevance. In the most relevant (Top-1) cases, Molmo-7B-D exhibits over 10% text preference ratio, even though the text does not aid accurate predictions. This suggests that models are less distracted by clearly irrelevant text but are influenced by seemingly relevant (yet ultimately irrelevant) text, raising concerns for applications like multi-modal RAG, where retrieved text may appear relevant yet distract the model. Text bias is related to the order of image and text tokens. Previous studies have shown that token order influences bias in LLMs during language generation [32, 49]. Since VLMs use LLMs [8, 37] as core components and are trained in an autoregressive manner, we examine whether text bias is affected by text and image token order. Notably, VLMs often include large number of image tokens from vision encoder. To test this, we compare text preference ratios by altering the order of text and image tokens in Phi3.5. As shown in Figure 6, placing text tokens before image tokens increases text bias consistently under three text variations. While previous research has suggested that generation misalignment or hallucinations in VLMs may stem from reduced attention to image tokens [11, 47], our findings indicate that the initial token modality may strongly influence modality preference, exacerbating text bias. Interplay between uni-modal certainty and model behavior. To explore when models rely on vision versus text, we explore uni-modal certainty as key factor in shaping model behavior. Specifically, we analyze the proportions of image, text, and other responses (i.e., pimg, ptxt, and po) across groups divided by uni-modal certainty quantiles. Figure 7 shows an interesting interplay effect: when text certainty Ptxt is high and image certainty Pimg is low, VQAv2 Base Match Corruption Irrelevance Macro Model LLaVA-NeXT-7B 79.45 79.45 Instruction 77.48 SFT 85.51 Qwen2-VL-7B 85.51 Instruction 84.18 SFT 92.32 92.25 87.56 92.76 92.62 87.01 28.69 34.27 71.25 50.79 54.78 82.72 79.43 78.15 77.32 83.70 82.82 84.00 66.81 68.22 78.71 75.75 76.74 84. Figure 6. Effect of token order on text bias: Placing text tokens before image tokens increases text bias in Phi3.5. Table 4. In-distribution performance comparison between original models, instruction and fine-tuned models. DocVQA Brand Detection MathVista Base Macro Base Macro Base Macro Model LLaVA-NeXT-7B 53.60 53.60 Instruction 52.20 SFT 90.50 Qwen2-VL-7B 90.50 Instruction 90.30 SFT 51.07 49.27 56.17 80.83 80.77 88. 35.80 35.80 35.30 55.40 55.40 58.50 41.03 41.20 41.63 53.87 54.10 57.17 78.60 78.60 81.36 89.68 89.68 89.44 46.44 47.36 72.29 81.85 84.48 88.75 Table 5. Performance comparison with Base and Macro accuracy based on DocVQA, MathVista, and Brand Recognition. See full results under different text conditions in Appendix. augmented samples. Seed data is from the VQAv2 validation split, separate from the benchmark evaluation data. Setup. We follow standard supervised fine-tuning procedure, using learning rate of 1.0 104 with cosine decay over 3 epochs and warmup ratio of 0.1 for stable convergence. Additionally, we apply LoRA for efficient finetuning. Experiments are conducted on the LLaVA-NeXT-7B and Qwen2-VL-7B models. In-Distribution Performance. In Table 4, we compare the performance of the original models, models with instruction, and models after supervised fine-tuning on indistribution data. The results show that supervised finetuning can better improve model accuracy compared to instruction, especially under text corruption conditions, where corruption accuracy increases from 28.69% to 71.25%, while maintaining overall performance in macro accuracy. Generalization. We further assess the generalization of the fine-tuned models by evaluating their performance on datasets beyond VQAv2. As shown in Table 5, the finetuned models exhibit some improvement across all datasets. However, improvements are smallest on MathVista, likely due to greater distribution shift from general VQA tasks to math reasoning tasks in vision. Effect of Text-Only Data. We conduct an ablation study to inspect the role of text-only and cross-modality data in fine-tuning on LLaVA-NeXT-7B. For fair comparison, the total amount of training data remains constant across experiments. As shown in Figure 8 (Left), fine-tuning reFigure 7. Effect of uni-modality certainty on model modality preference. Image/Text certainties are divided into three quantile bins, with higher values indicating higher certainty. Models favor visual data when image certainty is high and text certainty is low, and vice versa. When both certainties are low, models often produce Other answers instead of favoring one modality alone. models favor Text answers, and vice versa. When both certainties are low, models often produce Other answers, instead of favoring Text or Image alone. 4. Investigated Solutions 4.1. Instruction In Section 3.4, we observed that instructional prompts can influence the models modality preference. For example, adding the instruction Focus on the image to answer the question before the question helps reduce text bias to some extent. To explore this further, we evaluate performance with this instruction as baseline, finding slight improvement (12%) in Macro accuracy, as shown in Tables 4 and 5. 4.2. Supervised Finetuning (SFT) Data. The composition of training data is key for effective VLM training [36]. Specifically, we include both textonly and image-text samples for fine-tuning. We collect 1,000 samples evenly distributed across five data types: text-only data, original VQA data, and VQA samples under match, corruption, and irrelevance text conditions as textappr (resp. εmul εtxt appr) and εcross are the quantities that represent the approximation error of pure-text data (resp. multimodal data) and cross-modal error, respectively, and they are only dependent on the distributions Dtxt,Dmul and the hypothesis of models; Cvlm is quantity related to the covering number of the hypothesis of models. See details in Appendix A. Remark 5.2. Observe that the expected losses under pure-text data and multi-modal data are influenced by +M εcross and +M εcross, respectively. Our theoretical analysis, under specific assumptions, indicates that the tendency of blind faith in textual information may arise from the significant imbalance between and . Particularly, in most VLMs, , as these models often rely heavily on pre-trained language models, leading to the larger expected loss in multi-modal data and less in pure-text data, potentially making models favor text over image. 6. Related Work Evaluation on VLMs. Current evaluation benchmarks for VLMs include single-task benchmarks [15, 28, 29, 34] and multi-modal benchmarks [20, 27, 44, 45] designed to assess general model capabilities across diverse tasks. Some studies also evaluate specific issues, such as hallucination [11, 21], catastrophic forgetting [46], and robustness [43]. However, these benchmarks are primarily vision-centric, usually treating text as question input without additional context, which limits the evaluation of models robustness to text variations. While text can be additional hints in specific tasks like math reasoning, current datasets [28] focus on assessing reasoning skills rather than the models ability to handle varied text inputs. As result, whether VLMs can reliably handle multi-modal inconsistencies remains an open question. This gap is critical for real-world applications, such as multi-modal RAG, where models encounter variable text inputs. To this end, our work studies VLM performance under different text variations, identifying text bias that affects model reliability. Benchmarks with Input Perturbation. Text perturbations have been widely used in natural language tasks to evaluate model robustness and stability against distractions or misleading context [6, 16, 23, 31, 35, 42]. In computer vision, similar efforts focus on adding imperceptible perturbations to image inputs to assess models sensitivity to noise [14, 48]. Our work shifts focus from image perturbations to explore the effects of text variations on VLMs, which already excel in vision-centric benchmarks. Recent research [7] highlights data leakage in VLM benchmarks by studying performance with missing modalities. With different goal, we investigate how VLMs manage inconsistencies between visual and textual data in vision-centered tasks, evaluating robustness in cross-modal interactions. Figure 8. Left: The effect of text-only data in SFT. Right: The effect of data volume in SFT. duces text bias and enhances the models ability to distinguish between match and corruption cases, with gap up to 40%. Additionally, text-only data is important for maintaining core language capabilities: without it, models may reject text indiscriminately, leading to overly cautious behavior and limiting their use of helpful text. Effect of Data Volume. We study the impact of data volume in SFT, shown in Figure 8 (Right). As the amount of SFT data increases, the models reliance on text decreases significantly in corruption cases (from 58% to 25%) while remaining relatively steady in match cases. This trend indicates that scaling up SFT data can reduce dependency on corrupted or irrelevant text, while preserving the models effectiveness to match text. 5. Theoretical Analysis In this section, we present theoretical analysis to explain why the majority of VLMs exhibit an inherent tendency to have blind faith in text. Let and be the size of pure-text data and multi-modal data in the training set that are i.i.d sampled from distributions Dtxt and Dmul, respectively. Our informal results are as follows, see more details in Appendix A. (Informal; Theorem A.5 (simplified) Theorem 5.1. certain assumptions, with probability at ) Under least 1 δ the expected loss under pure-text data E(X,Y )Dtxt (cid:105) l(fvlm(X; ˆθERM), ) achieves (cid:104) (cid:32) (cid:101)O εtxt appr + + εcross + (cid:114) Cvlm/ log(1/δ) + (cid:33) , and similarly the expected loss under multi-modal data E(X,Y )Dmul (cid:105) l(fvlm(X; ˆθERM), ) achieves (cid:104) (cid:32) (cid:101)O εmul appr + + εcross + (cid:114) Cvlm/ log(1/δ) + (cid:33) . l(, ) is bounded loss function, and ˆθERM is the learned parameter(s) from Empirical Risk Minimization (ERM); 8 7. Conclusion and Discussion Revisiting our core questioncan VLMs reliably handle multi-modal inconsistencies?our findings indicate that substantial challenges remain. In this work, we observe the phenomenon of blind faith in text in VLMs, often relying on text over visual input when inconsistency arises, resulting in performance drops and potential safety risks. Our analysis showed that factors like instructions, model size, text relevance, token order, and modality certainty can influence text bias. Notably, scaling model size and prompt changes alone do not resolve this issue. While supervised fine-tuning with text augmentation helps, balancing robustness and effectiveness in cross-modal settings remains challenging. We hope this work highlights the risks of deploying VLMs in applications like multi-modal RAG, offering insights and prompting further development of more reliable and robust VLMs for cross-modal interactions."
        },
        {
            "title": "Acknowledgement",
            "content": "This research is supported by the Ministry of Education, Singapore, under the Academic Research Fund Tier 1 (FY2023) (Grant A-8001996-00-00)."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. 3 [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 3 [3] Anthropic. 2024. https : / / www - cdn . anthropic . com / de8ba9b01c9ab7cbabf5c33b80b7bbc618857627 / Model Card Claude 3.pdf. 3 3 model"
        },
        {
            "title": "Claude",
            "content": "card, [4] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An opensource framework for training large autoregressive visionlanguage models. arXiv preprint arXiv:2308.01390, 2023. 1 [5] Peter Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463482, 2002. 13 [6] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. Benchmarking large language models in retrieval-augmented generation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1775417762, 2024. 8 [7] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. 8 [8] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2023. 6 [9] Dai, Li, Li, AMH Tiong, Zhao, Wang, Li, Fung, and Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. arxiv 2023. arXiv preprint arXiv:2305.06500. 1, 6 [10] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. 3 [11] Ailin Deng, Zhirui Chen, and Bryan Hooi. Seeing is believing: Mitigating hallucination in large visionlanguage models via clip-guided decoding. arXiv preprint arXiv:2402.15300, 2024. 6, [12] Zane Durante, Qiuyuan Huang, Naoki Wake, Ran Gong, Jae Sung Park, Bidipta Sarkar, Rohan Taori, Yusuke Noda, Demetri Terzopoulos, Yejin Choi, et al. Agent ai: Surveying the horizons of multimodal interaction. arXiv preprint arXiv:2401.03568, 2024. 1 [13] Benjamin Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable creation in selfattention mechanisms. In International Conference on Machine Learning, pages 57935831. PMLR, 2022. 12, 13 [14] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. arXiv Explaining and harnessing adversarial examples. preprint arXiv:1412.6572, 2014. 8 [15] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answerIn Proceedings of the IEEE conference on computer ing. vision and pattern recognition, pages 69046913, 2017. 1, 3, 8, 19 [16] Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems. arXiv preprint arXiv:1707.07328, 2017. 8 [17] Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem Alshikh, and Ruslan Salakhutdinov. Omniact: dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web. arXiv preprint arXiv:2402.17553, 2024. [18] Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic vlms: Investigating the design space of visually-conditioned language models. arXiv preprint arXiv:2402.07865, 2024. 6 [19] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024. 1, 4 [20] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 8 [21] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large visionlanguage models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 292305, Singapore, 2023. Association for Computational Linguistics. 8 [22] Yuexin Li, Chengyu Huang, Shumin Deng, Mei Lin Lock, Tri Cao, Nay Oo, Bryan Hooi, and Hoon Wei Lim. Knowphish: Large language models meet multimodal knowledge graphs for enhancing reference-based phishing detection. arXiv preprint arXiv:2403.02253, 2024. 3, 5, 15, 16, 19 [23] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022. 8 [24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 1, [25] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 16 [26] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 3 [27] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an In European Conference on Computer all-around player? Vision, pages 216233. Springer, 2025. 8 [28] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations, 2024. 3, 8, 16, 19 [29] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. 3, 8, 19 [30] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016. 4 [31] John Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. Textattack: framework for adversarial attacks, data augmentation, and adversarial training in nlp. arXiv preprint arXiv:2005.05909, 2020. [32] Pouya Pezeshkpour and Estevam Hruschka. Large language models sensitivity to the order of options in multiple-choice questions. arXiv preprint arXiv:2308.11483, 2023. 6 [33] Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in Information Retrieval, 3(4):333389, 2009. 6 benchmark for visual question answering using world knowlIn European conference on computer vision, pages edge. 146162. Springer, 2022. 8 [35] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Scharli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. In International Conference on Machine Learning, pages 3121031227. PMLR, 2023. 1, 4, 5, 8 [36] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. 6, 7 [37] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 6 [38] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 3 [39] Chen Henry Wu, Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried, and Aditi Raghunathan. Adversarial attacks on multimodal agents. arXiv preprint arXiv:2406.12814, 2024. 4 [40] Kevin Wu, Eric Wu, and James Zou. Clasheval: Quantifying the tug-of-war between an llms internal prior and external evidence. Preprint, 2024. [41] Peng Xia, Kangyu Zhu, Haoran Li, Tianze Wang, Weijia Shi, Sheng Wang, Linjun Zhang, James Zou, and Huaxiu Yao. Mmed-rag: Versatile multimodal rag system for medical vision language models. arXiv preprint arXiv:2410.13085, 2024. 1 [42] Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. Adaptive chameleon or stubborn sloth: Revealing the behavior of large language models in knowledge conflicts. In The Twelfth International Conference on Learning Representations, 2024. 8 [43] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. arXiv preprint arXiv:2306.13549, 2023. 8 [44] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 1, 8 [45] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023. 1, 8 [46] Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee, and Yi Ma. Investigating the catastrophic forgetting in multimodal large language models. arXiv preprint arXiv:2309.10313, 2023. 8 [34] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: [47] Yi-Fan Zhang, Weichen Yu, Qingsong Wen, Xue Wang, Zhang Zhang, Liang Wang, Rong Jin, and Tieniu Tan. 10 Debiasing large visual language models. arXiv:2403.05262, 2024. 6 arXiv preprint [48] Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan Li, Ngai-Man Man Cheung, and Min Lin. On evaluating adversarial robustness of large vision-language models. Advances in Neural Information Processing Systems, 36, 2024. 8 [49] Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. Large language models are not robust multiple choice selectors. In The Twelfth International Conference on Learning Representations, 2023. 6 A. Details of Theoretical Analysis To provide rigorous foundation for our theoretical analysis, we begin by formally outlining the training process of visionlanguage model. For clarity and conciseness, the following is streamlined adaptation of the standard training process. VLM is function fvlm : Y, where := Rτ denotes the set of sequences of d-dimensional feature vector (that can represent text or image) with length τ , and denotes the output space of the model. Without loss of generalization, we assume := for simplicity. A.1. Structure Following Edelman et al. [13], we consider the form of transformer structure of fvlm with layers as follows. The parameters (cid:111) . In addition, we denote 1:i = (cid:0)W (1), . . . , i1(cid:1) to be the of is layer is denoted by (i) := parameters up to is layer. Further, we let the block of i-th layer g(i) tf-block : Rτ Rτ to be , (i) , (i) , (i) (i) (cid:110) g(i+1) tf-block g(i+1) tf-block (cid:0)X; 1:i+1(cid:1) := Πnorm (cid:0)X; 1:i+1(cid:1) := Πnorm (cid:16) (cid:16) σ (Πnorm (f (X))) (i) (cid:16) (cid:16) (cid:16) σ Πnorm g(i) tf-block (cid:17) for = 1, (cid:0)X; 1:i(cid:1) ; (i)(cid:17)(cid:17)(cid:17) (cid:17) (i) for > 1, where Rτ is the models input, and Πnorm is the layer normalization function, σ is non-linear activation function, and (Z; {WQ, WK, WV }) := Softmax (cid:16) ZWQ (ZWK)(cid:17)"
        },
        {
            "title": "ZWV",
            "content": "with Softmax() being the standard softmax function. Finally, the scalar output is defined as fvlm(X; 1:L, w) := w[g(L+1) tf-block (cid:0)X; 1:L(cid:1)]τ , for some Rd, (1) where [G]τ Rd denotes the τ -th row of the matrix Rτ d. Furthermore, we have the following assumptions within the structure. Assumption A.1. For all = 1, , L, we have Assumption A.2. For all = 1, , L, we have (cid:13) (cid:13)W (i) (cid:13) (cid:13) (cid:13)W (i) (cid:13) , (cid:13) (cid:13) (cid:13)2 (cid:13) (cid:13) (cid:13)2, (cid:13) (cid:13)W (i) (cid:13) (cid:13) (cid:13)W (i) (cid:13) , (i) (i) , (cid:13) (cid:13) (cid:13)2 (cid:13) (cid:13) (cid:13)2 (cid:13) (cid:13) (cid:13)2,1 (cid:13) (cid:13)W (i) (cid:13) (cid:13) (cid:13)W (i) (cid:13) , C2. (cid:13) (cid:13) (cid:13)2,1 C2,1. Assumption A.3. The activation function σ() is Lσ-Lipschitz in the l2 norm. Assumption A.4. The loss function l() is b-bounded and is Lloss-Lipschitz in its arguments. A.2. Training process 1 , ytxt Let txt = [(X txt text feature vector of length τ , and ytxt = txt function for the pure text data. We assume txt 1 ), , (X txt , ytxt gt (X txt 1 , , txt )] be pure-text training set with size , where txt ) is its ground-truth label with txt Rτ is sequence of the gt () denoted as the ground-true In addition, let mul = [(X mul ), , (X mul Rτ is sequence of multi-modal (e.g., text and image) feature vector of length τ , and ymul its ground-truth label with mul mul 1 Furthermore, let : be loss function. Then, we define the parameter ˆθERM Θ according to the ERM learning ) is gt () denoted as the ground-true function for the multi-modal data. Similarly, we assume are i.i.d. sampled from unknown distribution Dmul. gt (X multi are i.i.d. sampled from unknown distribution Dtxt. )] be multi-modal training set with size , where multi , , mul , ymul = mul , ymul 1 1 i process of the multi-modal paradigm as ˆθERM arg min θΘ"
        },
        {
            "title": "1\nN + M",
            "content": "(cid:32) (cid:88) i=1 (cid:0)fvlm(X txt ; θ), ytxt (cid:1) + (cid:0)fvlm(X mul ; θ), ymul (cid:1) (cid:33) (cid:88) i=1 (2) Our main theoretical result is given in the next subsection. A.3. Results We now provide the formal statement of Theorem A.5. Theorem A.5. Let Θ be the set of parameters that satisfies Assumption A.1, A.2, A.3 and A.4. For any θ Θ, let fvlm(; θ) be VLM as is defined in equation 1 with layers. With probability at least 1 δ, XDtxt inf θΘ (cid:124) (cid:2)l (cid:16) fvlm(X; ˆθERM), txt gt (X) (cid:17) (cid:3) XDtxt (cid:2)l (cid:0)fvlm(X; θ, txt (cid:123)(cid:122) approximation error gt (X)(cid:1) (cid:3) (cid:125) + + (cid:124) (cid:12) (cid:12) (cid:12) (cid:12) sup θΘ (cid:16) (cid:2)l XDmul fvlm(X; θ), mul gt (X) (cid:17) (cid:3) XDtxt (cid:123)(cid:122) cross-modal error (cid:2)l (cid:0)fvlm(X; θ), txt (cid:12) (cid:12) gt (X)(cid:1) (cid:3) (cid:12) (cid:12) (cid:125) (cid:114) + (cid:124) 1/ log(δ) + + Lloss (cid:114) Cvlm + (cid:123)(cid:122) generalization error log(1 + + Cvlm ) , (cid:125) (3) and (cid:16) (cid:2)l XDmul fvlm(X; ˆθERM), mul gt (X) (cid:17) (cid:3) inf θΘ (cid:124) (cid:16) (cid:2)l XDmul fvlm(X; θ, mul gt (X) (cid:123)(cid:122) approximation error (cid:17) (cid:3) + (cid:125) + (cid:124) (cid:12) (cid:12) (cid:12) (cid:12) sup θΘ (cid:16) (cid:2)l XDmul fvlm(X; θ), mul gt (X) (cid:17) (cid:3) XDtxt (cid:123)(cid:122) cross-modal error (cid:2)l (cid:0)fvlm(X; θ), txt (cid:12) (cid:12) gt (X)(cid:1) (cid:3) (cid:12) (cid:12) (cid:125) (cid:114) + (cid:124) 1/ log(δ) + where + Lloss (cid:114) Cvlm + (cid:123)(cid:122) generalization error log(1 + + Cvlm ) , (cid:125) (4) B2 is the constant related to the covering number of the function class of {fvlm(; θ) θ Θ}, and the notation hides global constants and logarithmic factors on quantities besides N, and τ . 2,1 log(dτ (N + )) Cvlm (C2Lσ)O(L) B2 wC 2 A.4. Proof of Theorem A.5 Before we formally prove Theorem A.5, we first present some useful Lemmas from previous works. For any real-valued (cid:0)F; ε; x(1), . . . , x(m)(cid:1) denote the converting number of with respect to the radius ε and the function class F, we let samples {x(1), . . . , x(m)}. Lemma A.6. (Adapted from Bartlett and Mendelson [5, Theorem 8] and Edelman et al. [13, Lemma A.4]) Consider (cid:0)F; ε; x(1), . . . , x(m)(cid:1) CF /ε2 for all real-valued function class such that for all and log x(1), . . . , x(m) . Let l(, ) to be loss function bounded by and is Lloss-Lipschitz in its arguments, and ggt : be ground-true function. Then for any δ > 0 and any distribution for the i.i.d samples x(1), . . . , x(m) , with probability at least 1 δ, simultaneously for all F, (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) xD [l(f (x), ggt(x))] 1 (cid:88) (cid:16) i=1 (x(i)), ggt(x(i)) (cid:17) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 4cLloss (cid:114) (cid:16)"
        },
        {
            "title": "CF\nm",
            "content": "1 + log (cid:16) A(cid:112)m/CF (cid:17)(cid:17) + 2b (cid:114) log(1/δ) 2m for some constant > 0. Lemma A.7. (Adapted from Edelman et al. [13, Theorem A.17]) Suppose [m], (cid:13) (cid:13)2, BX . Let Θ be the set of parameters that satisfies Assumption A.1, A.2, A.3 and A.4. For any θ Θ, let fvlm(; θ) is vlm model as is fined in equation 1 with layers. We have (cid:13)X (i)(cid:13) log (cid:16) {fvlm(; θ) θ Θ}; ε; (1), . . . , (m)(cid:17) (C2Lσ)O(L) B2 B2 wC 2 2,1 ε2 log(dmT ). 13 Proof of Theorem A.5. By Lemma A.6, with probability at least 1 δ we have simultaneously for all θ Θ, (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) XDtxt (cid:2)l (cid:0)fvlm(X; θ), txt gt (X)(cid:1) (cid:3)"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 4cLloss (cid:114) (cid:16)"
        },
        {
            "title": "C\nN",
            "content": "1 + log (cid:16) A(cid:112)N/C (cid:17)(cid:17) + 2b (cid:0)fvlm(X txt (cid:114) log(1/δ) 2N , ); θ), txt gt (X txt )(cid:1) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (5) where (C2Lσ)2L BX , and is constant such that for all ε > 0 and (1), . . . , (m) Rτ with (i)2, BX log (cid:16) {fvlm(; θ) θ Θ}; ε; (1), . . . , (m)(cid:17) ε2 . Similarly, with probability at least 1 δ we have simultaneously for all θ Θ, (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) XDmul (cid:2)l (cid:0)fvlm(X; θ), mul gt (X)(cid:1) (cid:3)"
        },
        {
            "title": "1\nM",
            "content": "M (cid:88) i=1 (cid:0)fvlm(X mul 4cLloss (cid:114) (cid:16)"
        },
        {
            "title": "C\nM",
            "content": "1 + log (cid:16) A(cid:112)M/C (cid:17)(cid:17) + 2b (cid:114) log(1/δ) 2M . Note that for any θ Θ we have ); θ), mul gt (X mul )(cid:1) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:32) (cid:88) (cid:0)fvlm(X txt ; θ), ytxt (cid:1) + (cid:88) i=1 (cid:0)fvlm(X mul ; θ), ymul (cid:1) (cid:33) XDtxt (cid:2)l (cid:0)fvlm(X; θ), txt (cid:12) (cid:12) gt (X)(cid:1) (cid:3) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)"
        },
        {
            "title": "1\nN + M",
            "content": "= (cid:12) (cid:12) (cid:12) (cid:12) + i=1 (cid:32)"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (cid:0)fvlm(X txt ; θ), ytxt (cid:1) XDtxt (cid:2)l (cid:0)fvlm(X; θ), txt (cid:33) gt (X)(cid:1) (cid:3) + + (cid:32)"
        },
        {
            "title": "1\nM",
            "content": "M (cid:88) i=1 (cid:0)fvlm(X mul ; θ), ymul (cid:1) XDtxt (cid:2)l (cid:0)fvlm(X; θ), txt gt (X)(cid:1) (cid:3) (cid:33) (cid:12) (cid:12) (cid:12) (cid:12) (a) + (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)"
        },
        {
            "title": "1\nN",
            "content": "+ + (cid:88) i=1 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)"
        },
        {
            "title": "1\nM",
            "content": "l (cid:0)fvlm(X txt ; θ), ytxt (cid:1) XDtxt (cid:2)l (cid:0)fvlm(X; θ), txt (cid:12) (cid:12) gt (X)(cid:1) (cid:3) (cid:12) (cid:12) (cid:12) (cid:88) i=1 (cid:0)fvlm(X mul ; θ), ymul (cid:1) XDtxt (cid:2)l (cid:0)fvlm(X; θ), txt (cid:12) (cid:12) gt (X)(cid:1) (cid:3) (cid:12) (cid:12) (cid:12) , where (a) follows from Jensens inequality. In addition, with probability at least 1 δ,we have for all θ Θ (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)"
        },
        {
            "title": "1\nM\n(cid:12)\n(cid:12)\n(cid:12)\n(cid:12)\n(cid:12)",
            "content": ""
        },
        {
            "title": "1\nM",
            "content": "+ (cid:88) i=1 (cid:12) (cid:12) (cid:12) (cid:12) XDtxt (cid:114)"
        },
        {
            "title": "C\nM",
            "content": "+ (cid:12) (cid:12) (cid:12) (cid:12) (cid:88) i=1 (cid:0)fvlm(X mul ; θ), ymul (cid:1) XDtxt (cid:2)l (cid:0)fvlm(X; θ), txt (cid:12) (cid:12) gt (X)(cid:1) (cid:3) (cid:12) (cid:12) (cid:12) (cid:0)fvlm(X mul ; θ), ymul (cid:1) XDmul (cid:2)l (cid:0)fvlm(X; θ), mul (cid:12) (cid:12) gt (X)(cid:1) (cid:3) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) gt (X)(cid:1) (cid:3) (cid:2)l (cid:0)fvlm(X; θ), mul (cid:12) (cid:12) (cid:2)l (cid:0)fvlm(X; θ), txt gt (X)(cid:1) (cid:3) XDmul (cid:114) (a) 4cLloss (cid:16) 1 + log (cid:16) A(cid:112)M/C (cid:17)(cid:17) + 2b log(1/δ) 2M XDtxt (cid:2)l (cid:0)fvlm(X; θ), txt gt (X)(cid:1) (cid:3) XDmul (cid:2)l (cid:0)fvlm(X; θ), mul (cid:12) (cid:12) gt (X)(cid:1) (cid:3) (cid:12) (cid:12) where (a) follows from equation 6. 14 (6) (7) (8) (9) Combining equation 5, equation 8 and equation 9, we get that with probability at least 1 δ, for all θ Θ, (cid:32) (cid:88) (cid:0)fvlm(X txt ; θ), ytxt (cid:1) + (cid:88) i=1 (cid:0)fvlm(X mul ; θ), ymul (cid:1) (cid:33) XDtxt (cid:2)l (cid:0)fvlm(X; θ), txt (cid:12) (cid:12) gt (X)(cid:1) (cid:3) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)"
        },
        {
            "title": "1\nN + M",
            "content": "4cLloss i=1 + (cid:16) 1 + log (cid:16) A(cid:112)M/C (cid:17)(cid:17) + 2b (cid:112)M log(1/δ)/2 + (cid:112)N log(1/δ)/2 + + 2b (cid:16) 1 + log + (cid:2)l (cid:0)fvlm(X; θ), txt + 4cLloss + (cid:12) (cid:12) (cid:12) (cid:12) XDtxt (cid:16) A(cid:112)N/C (cid:17)(cid:17) gt (X)(cid:1) (cid:3) (cid:2)l (cid:0)fvlm(X; θ), mul XDmul (cid:12) (cid:12) gt (X)(cid:1) (cid:3) (cid:12) (cid:12) . (10) By the definition of ˆθERM, equation 10 implies that with probability at least 1 δ, (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:16) fvlm(X txt ; ˆθERM), ymul 1 + ; ˆθERM), ytxt fvlm(X mul (cid:32) (cid:88) XDtxt (cid:88) (cid:33) + (cid:16) (cid:17) (cid:17) i i (cid:2)l i=1 (cid:16) fvlm(X; ˆθERM), txt gt (X) (cid:12) (cid:12) (cid:17) (cid:3) (cid:12) (cid:12) (cid:12) inf θΘ XDmul (cid:2)l (cid:16) fvlm(X; θ, mul gt (cid:17) (cid:3) + (X) + 4cLloss + (cid:12) (cid:12) (cid:12) (cid:12) XDtxt (cid:16) (cid:16) A(cid:112)M/C 1 + log + (cid:16) (cid:2)l fvlm(X; ˆθERM), txt gt (X) (cid:17)(cid:17) + 2b (cid:17) (cid:3) i=1 + sup θΘ (cid:12) (cid:12) (cid:12) (cid:12) XDmul (cid:112)M log(1/δ)/2 + (cid:2)l XDmul (cid:16) fvlm(X; ˆθERM), mul gt (cid:17) (cid:3) (X) (cid:12) (cid:12) (cid:12) (cid:12) . (cid:2)l (cid:16) fvlm(X; θ), mul gt (cid:17) (cid:3) (X) XDtxt (cid:2)l (cid:0)fvlm(X; θ), txt gt (X)(cid:1) (cid:3) + 4cLloss (cid:16) + 1 + log (cid:16) A(cid:112)N/C (cid:17)(cid:17) + 2b (cid:112)N log(1/δ)/2 + (cid:12) (cid:12) (cid:12) (cid:12) (11) Note the fact that max{N, } + 2 max{N, } . Finally, by Lemma A.7 and hiding global constants and logarithmic factors on quantities besides N, and τ , we get with probability 1 δ, (cid:2)l (cid:16) fvlm(X; ˆθERM), txt gt (X) (cid:17) (cid:3) XDtxt inf θΘ XDtxt (cid:114) (cid:2)l (cid:0)fvlm(X; θ, txt gt (X)(cid:1) (cid:3) + + (cid:12) (cid:12) (cid:12) (cid:12) sup θΘ (cid:16) (cid:2)l XDmul fvlm(X; θ), mul gt (X) (cid:17) (cid:3) XDtxt (cid:2)l (cid:0)fvlm(X; θ), txt gt (X)(cid:1) (cid:3) (cid:12) (cid:12) (cid:12) (cid:12) + 1/ log(δ) + + Lloss (cid:114) Cvlm + log(1 + + Cvlm ), (12) and similarly, (cid:16) (cid:2)l XDmul fvlm(X; ˆθERM), mul gt (X) (cid:17) (cid:3) inf θΘ XDmul (cid:114) + (cid:16) (cid:2)l fvlm(X; θ, mul gt (X) (cid:17) (cid:3) + + (cid:12) (cid:12) (cid:12) (cid:12) sup θΘ (cid:16) (cid:2)l XDmul fvlm(X; θ), mul gt (X) (cid:17) (cid:3) XDtxt (cid:2)l (cid:0)fvlm(X; θ), txt gt (X)(cid:1) (cid:3) (cid:12) (cid:12) (cid:12) (cid:12) 1/ log(δ) + + Lloss (cid:114) Cvlm + log(1 + + Cvlm ). (13) This completes the proof of Theorem A. B. Experimental Setup This section outlines the experimental setup, including examples of constructed textual variations, details of the brand detection task [22], and the evaluation protocols employed. We present examples illustrating the three types of textual variations alongside the corresponding image, original question, and ground-truth answers to provide clarity and context. B.1. Examples This subsection provides examples of matching, corrupted, and irrelevant texts across different datasets in Tables 6 to 9. B.2. Brand Recognition Brand recognition from webpage is crucial step in detecting phishing websites. Phishing webpages aim to deceive users by imitating the appearance of legitimate websites associated with well-known brands. Accurately identifying the brand 15 Q: What green veggie is on the pizza GT: pepper Match: Corruption: The pizza has green pepper slices on one of its sections. The pizza has green broccoli florets on one of its sections. Irrelevance: Beckham obtained his early education at Roseland Academy in Bardstown. In 1881 he served as page in the Kentucky House of Representatives at the age of 12. Later, he enrolled at Central University ( now Eastern Kentucky University ) in Richmond, Kentucky but was forced to quit school at the age of 17 to support his widowed mother. Two years later, he became principal of Bardstown public schools, serving from 1888 to 1893. Concurrently, he studied law at the University of Kentucky, where he earned his law degree in 1889. He was admitted to the bar and commenced practice in Bardstown in 1893. He also served as president of the Young Democrats Club of Nelson County . Table 6. Illustration of matching, corrupted, and irrelevant information in sample from VQAv2. linked to webpage allows for comparison between the input webpages URL and the official URL of the recognized brand, aiding in the detection of phishing attempts. In our experiments, we utilized phishing webpage samples from the TR-OP dataset [22]. Each sample comprises screenshot and its corresponding HTML code. Depending on the scenario, the HTML content either reflects the target brand displayed in the screenshot or is altered to assess the models robustness. We evaluated three specific scenarios: Matching: The original HTML includes information about the target brand visible in the screenshot. This scenario provides the model with consistent inputs, helping it correctly identify the brand. Corruption: In this case, we inserted fabricated brand name (e.g., The official webpage of MobrisPremier) into the HTML to mislead the model into recognizing non-existent brand. Since no corresponding URL exists for such brands, phishing detection becomes infeasible for these inputs. Irrelevance: The HTML content was replaced with randomly selected sentences from the Wiki dataset [], ensuring that the new content was unrelated to any brand. This scenario tests the models ability to handle inputs with no brand-specific information. To standardize the inputs, we preprocessed the HTML content by removing all tags and truncating it to maximum length of 5,000 characters. B.3. Evaluation We follow the evaluation protocol specified for each dataset. To reduce cases where models generate open-ended answers, which complicates evaluation, we adopt similar approach to the evaluation setting in LLaVA-1.5 [25]. For certain datasets, we append additional formatting prompts after the question, as shown in Table 10. For MathVista [28], which uses GPT-based evaluation, we do not include formatting prompts. Instead, GPT is employed directly to evaluate the outputs. C. Experimental Results To rigorously assess the performance impact of varying textual contexts, we have documented the comprehensive results across four distinct datasets. These results are quantified using several metrics: Accuracy, Normalized Accuracy, and Text Preference Ratio (TPR) for the text variations of Match, Corruption, and Irrelevance, alongside Macro Accuracy. The detailed outcomes are encapsulated in Table 11. 16 Q: What time is question and answers session? GT: 12:25 to 12:58 p.m. Match: Corruption: The Questions and Answers session is scheduled from 12:25 to 12:58 p.m. The Questions and Answers session is scheduled from 2:00 to 5:00 p.m. Irrelevance: The Americans knew of the approach of the Japanese forces from reports from native scouts and their own patrols , but did not know exactly where or when they would attack . The ridge around which Edson deployed his men consisted of three distinct hillocks . At the southern tip and surrounded on three sides by thick jungle was Hill 80 ( so named because it rose 80 ft ( 24 ) above sea level ) . Six hundred yards north was Hill 123 ( 123 ft ( 37 ) high ) , the dominant feature on the ridge . The northernmost hillock was unnamed and about 60 ft ( 18 ) high . Edson placed the five companies from the Raider battalion on the west side of the ridge and the three Parachute battalion companies on the east side , holding positions in depth from Hill 80 back to Hill 123 . Two of the five Raider companies , and , held line between the ridge , small , swampy lagoon , and the Lunga River . Machine @-@ gun teams from Company , the heavy weapons company , were scattered throughout the defenses . Edson placed his command post on Hill 123 . Table 7. Illustration of matching, corrupted, and irrelevant information in sample from DocVQA. For thorough assessment of the investigated methodologies, encompassing base models, instructional prompts, and Supervised Fine-Tuning (SFT), we present results across four datasets, measured in terms of Accuracy, Normalized Accuracy, Text Preference Ratio (TPR) under the text variations of Match, Corruption, and Irrelevance, as well as Macro Accuracy. These experiments were conducted utilizing the models LLaVA-NeXT-7B and Qwen2-VL-7B. The detailed findings are provided in Table 12. Q: Hint: Please answer the question requiring an integer answer and provide the final value, e.g., 1, 2, 3, at the end. Question: what is the total volume of the measuring cup? (Unit: g) GT: 1000 Match: Corruption: The measuring cup has markings up to 1000 grams, indicating its total volume capacity. The measuring cup has markings up to 500 grams, indicating its total volume capacity. Irrelevance: The windmill at Thelnetham was built by millwright George Bloomfield for William Button in 1819 . It replaced post mill which had been moved to Sandy Lane , Diss , Norfolk the previous year . The mill was set to work on Christmas Day 1819 . In 1832 , the mill was modernised by the fitting of cast @-@ iron windshaft , Patent sails and fantail . The new windshaft was fitted on 16 July 1832 . It was made by Aickman , the Kings Lynn millwright , and weighs 1 00be tons ( 1 @,@ 780 kg ) . new stock was fitted in September 1836 . William Button died on 11 February 1837 . The mill passed jointly to his widow Rebecca and their son Richard . Richard Button worked the mill until 1860 , at which date it was conveyed to his sons Richard and William , who sold it to Richard Peverett from Kenninghall , Norfolk in 1862. Table 8. Illustration of matching, corrupted, and irrelevant information in sample from MathVista. 18 Q: Define targeted brand as brand that webpage belongs to. Given the screenshot of webpage as the primary information for identifying the target brand and the text as additional reference, determine what the targeted brand of is. The text can be HTML from the webpage or something irrelevant. Please be careful with the text, as it may contain noise or adversarial attacks. You must output the targeted brand of even if you are not sure about it. Only output the brand name without any additional information. Input information: HTML: html Answer: GT: Adobe Adobe PDF Reader Xl This File is Protected by Adobe Security LogIn with your email and password to open this document. The official webpage of MoThe official webpage of MobrisPremier. brisPremier. Adobe PDF Reader Xl This File is Protected by Adobe Security LogIn with your email and password to open this document. Ayola began her career in the theatre , performing in number of Shakespearean plays including Twelfth Night , Midsummer Night Dream , The Tempest , The Merchant of Venice and Macbeth . She states of this : Shakespeare keeps coming my way . love the fact that get to play people who are much more articulate than ll ever be . Ayola has performed in Twelfth Night in the lead roles of both Olivia and Viola . She explains : The role of Viola didn sit that well with me for some reason but Olivia makes more sense . She has also appeared in modern performances , assuming the title role of Dido , Queen of Carthage at the Globe Theatre in London in 2003 , which she described as dream of part . She has deemed her dream role to be that of Isabella in Measure for Measure , as she once lost out on the part and would like to prove herself capable of playing it. Match: Corruption: Irrelevance: Table 9. Illustration of matching, corrupted, and irrelevant information in sample from Brand Recognition. Dataset VQAv2 [15] DocVQA [29] MathVista [28] Brand Recognition [22] Only output the brand name without any additional information. Response Formatting Prompts Please only output the answer with single word or phrase. Please only output the answer directly. Table 10. Response formatting prompts used for evaluation. 19 Model Base 69.82 GPT-4o mini 51.02 Claude Haiku 78.39 GPT-4o 66.88 Claude Sonnet LLaVA-NeXT-7B 79.45 LLaVA-NeXT-13B 81.02 LLaVA-NeXT-34B 82.96 75.65 Phi3.5 76.33 Molmo-7B-D 85.51 Qwen2-VL-7B Model Base 69.40 GPT-4o mini 69.53 Claude Haiku 85.00 GPT-4o 87.00 Claude Sonnet LLaVA-NeXT-7B 53.60 LLaVA-NeXT-13B 57.70 LLaVA-NeXT-34B 64.00 78.20 Phi3.5 74.00 Molmo-7B-D 90.50 Qwen2-VL-7B Model Base 52.30 GPT-4o mini 41.00 Claude Haiku 58.90 GPT-4o 56.30 Claude Sonnet 35.80 LLaVA-NeXT-7B LLaVA-NeXT-13B 36.20 LLaVA-NeXT-34B 34.00 43.10 Phi3.5 44.90 Molmo-7B-D 55.40 Qwen2-VL-7B Model Base 88.84 GPT-4o mini 84.40 Claude Haiku 88.68 GPT-4o 90.20 Claude Sonnet LLaVA-NeXT-7B 78.60 LLaVA-NeXT-13B 83.00 LLaVA-NeXT-34B 66.28 84.40 Phi3.5 87.44 Molmo-7B-D 89.68 Qwen2-VL-7B Match Irrelevance Corruption Accuracy Norm TPR Accuracy Norm TPR Accuracy Norm TPR 3.77 13.95 1.56 1.38 4.72 3.30 2.70 2.25 9.36 1.28 125.31 162.31 113.88 116.40 116.20 115.51 112.19 120.59 116.04 108.48 103.28 100.16 100.55 106.00 99.97 100.33 95.99 98.97 100.22 97.88 73.83 51.61 90.25 101.93 36.11 46.42 51.68 46.57 64.57 59.40 51.55 26.33 70.75 68.17 28.69 37.61 42.87 35.23 49.29 50. 89.15 86.74 69.03 49.86 86.25 86.45 79.10 79.51 88.32 13.17 52.42 82.71 27.09 9.58 85.52 74.43 67.56 74.05 59.40 29.22 87.49 82.81 89.27 77.85 92.32 93.59 93.07 91.23 88.57 92.76 72.11 51.10 78.82 70.89 79.43 81.29 79.64 74.87 76.50 83.70 (a) VQAv2 Match Irrelevance Corruption Accuracy Norm TPR Accuracy Norm TPR Accuracy Norm TPR 0.80 1.18 0.23 0.00 0.71 0.65 0.13 0.00 0.37 0.22 117.26 120.06 106.35 105.15 169.40 156.68 137.19 118.16 122.30 105.08 96.83 83.16 101.65 100.47 97.76 96.68 97.97 98.46 100.95 99.34 55.04 56.61 86.59 97.24 18.66 19.06 23.59 64.60 51.89 63.64 38.20 39.35 73.60 84.60 10.00 11.00 15.10 50.50 38.40 57. 82.74 68.77 64.75 41.18 86.92 87.82 84.62 58.01 87.54 51.97 52.07 47.67 17.96 3.21 87.77 86.84 82.69 40.51 57.20 37.41 67.20 57.82 86.40 87.41 52.40 55.80 62.70 77.00 74.70 89.90 81.40 83.45 90.40 91.53 90.80 90.40 87.80 92.40 90.30 95.10 (b) DocVQA Match Irrelevance Corruption Accuracy Norm TPR Accuracy Norm TPR Accuracy Norm TPR 20.14 23.33 13.55 7.96 38.22 37.18 20.40 13.99 27.49 8.44 141.11 195.85 125.04 120.95 273.62 257.43 200.00 171.21 152.57 140.43 84.89 96.83 90.15 98.05 104.02 96.28 94.41 95.36 100.89 99.10 23.90 19.80 41.20 49.30 19.70 20.60 21.70 22.20 32.90 28.90 45.70 48.29 69.95 87.57 54.97 56.89 61.98 51.47 73.27 52. 44.40 39.70 53.10 55.20 28.40 32.60 32.10 41.10 45.30 54.90 80.28 77.42 48.98 29.14 84.19 80.83 67.64 80.20 60.63 70.23 88.82 88.04 85.20 57.69 88.72 88.98 73.59 84.82 82.46 84.50 73.80 80.30 73.70 68.10 74.80 76.20 68.00 73.70 68.50 77.80 (c) MathVista Match Irrelevance Corruption Accuracy Norm TPR Accuracy Norm TPR Accuracy Norm TPR 0.08 0.00 0.04 0.00 70.45 79.61 10.65 79.17 27.36 15.73 95.44 93.27 101.22 100.04 79.54 40.92 80.77 71.90 47.39 96.43 97.80 98.81 100.90 100.40 98.67 95.18 102.99 99.33 99.86 99.15 99.60 97.49 100.54 100.04 20.72 14.12 79.69 19.48 69.63 78.20 84.80 78.72 89.76 90.24 62.52 33.96 53.52 60.68 41.44 86. 30.43 26.02 14.64 17.03 82.39 77.04 31.60 31.39 37.38 17.22 86.88 83.40 89.48 90.56 77.56 79.00 68.28 83.84 87.32 88.92 88.48 82.28 89.16 90.24 16.28 11.72 52.84 16.44 60.88 70.16 7.48 6.44 0.83 0.96 64.74 72.97 23.49 50.54 60.40 2.99 Macro 70.38 53.41 79.61 72.30 66.81 70.83 71.86 67.11 71.45 75. Macro 62.27 60.21 83.47 87.85 51.07 52.40 55.20 73.30 67.80 80.83 Macro 47.37 46.60 56.00 57.53 40.97 43.13 40.60 45.67 48.90 53.87 Macro 86.72 81.47 89.47 90.35 52.12 41.56 58.21 53.65 63.21 81. Table 11. Performance in Accuracy, Normalized Accuracy (Norm) and Text Preference Ratio (TPR) across four datasets under three text variations: Match, Corruption, and Irrelevance. The Macro column represents the average of Match, Corruption, and Irrelevance Accuracy for each model, calculated to be comparable to the Base accuracy. (d) Brand Detection 20 Model Base LLaVA-NeXT-7B 79.45 79.45 Instruction 77.48 SFT 85.51 Qwen2-VL-7B 85.51 Instruction 84.18 SFT Model Base LLaVA-NeXT-7B 53.60 53.60 Instruction 52.20 SFT 90.50 Qwen2-VL-7B 90.50 Instruction 90.30 SFT Model Base LLaVA-NeXT-7B 35.80 35.80 Instruction 35.30 SFT 55.40 Qwen2-VL-7B 55.40 Instruction 58.50 SFT Model Base LLaVA-NeXT-7B 78.60 78.60 Instruction 81.36 SFT 89.68 Qwen2-VL-7B 89.68 Instruction 89.44 SFT Match Irrelevance Corruption Accuracy Norm TPR Accuracy Norm TPR Accuracy Norm TPR 4.72 6.69 4.06 1.28 1.18 2. 116.20 116.12 113.01 108.48 108.32 103.36 28.69 34.27 71.25 50.79 54.78 82.72 92.32 92.25 87.56 92.76 92.62 87.01 36.11 43.13 91.94 59.40 64.07 98.26 79.43 78.15 77.32 83.70 82.82 84.00 86.25 86.46 59.73 13.17 14.42 36. 99.97 98.36 99.79 97.88 96.85 99.79 85.52 78.50 20.00 29.22 27.01 6.69 (a) VQAv2 Match Irrelevance Corruption Accuracy Norm TPR Accuracy Norm TPR Accuracy Norm TPR 0.71 1.54 0.14 0.22 0.11 0. 169.40 165.30 144.63 105.08 104.64 103.10 10.00 9.80 42.80 57.50 57.80 84.30 90.80 88.60 75.50 95.10 94.70 93.10 18.66 18.28 81.99 63.64 63.88 93.35 87.77 87.38 28.19 37.41 37.00 6.32 86.92 84.01 56.21 51.97 51.46 26. 97.76 92.16 96.17 99.34 99.23 99.11 52.40 49.40 50.20 89.90 89.80 89.50 (b) DocVQA Match Irrelevance Corruption Accuracy Norm TPR Accuracy Norm TPR Accuracy Norm TPR 34.57 32.94 10.76 8.44 8.11 5. 208.94 197.77 194.90 140.43 140.79 126.50 79.33 87.15 92.64 99.10 99.10 97.78 19.70 21.80 23.50 28.90 29.30 40.30 84.32 84.68 77.42 84.50 86.50 78.31 74.80 70.60 68.70 77.80 78.10 74.00 84.19 81.85 63.75 70.23 70.59 49. 55.03 60.89 66.57 52.17 52.88 68.89 28.40 31.20 32.70 54.90 54.90 57.20 (c) MathVista Match Irrelevance Corruption Accuracy Norm TPR Accuracy Norm TPR Accuracy Norm TPR 89.14 85.26 9.08 15.73 9.34 0. 98.67 99.70 96.26 99.15 98.71 100.72 20.72 11.30 84.92 78.20 86.77 97.72 62.52 54.84 69.48 86.48 87.12 88.76 68.30 66.57 37.18 17.22 17.50 20.32 77.56 78.36 78.32 88.92 88.52 90.08 59.17 59.63 17.92 2.99 1.94 1. 79.54 69.77 85.39 96.43 97.15 99.24 16.28 8.88 69.08 70.16 77.80 87.40 Macro 66.81 68.22 78.71 75.75 76.74 84.58 Macro 51.07 49.27 56.17 80.83 80.77 88. Macro 41.03 41.20 41.63 53.87 54.10 57.17 Macro 46.44 47.36 72.29 81.85 84.48 88.75 Table 12. Performance of investigated solutions in Accuracy, Normalized Accuracy (Norm) and Text Preference Ratio (TPR) across four datasets under three text variations: Match, Corruption, and Irrelevance. The Macro column represents the average of Match, Corruption, and Irrelevance Accuracy for each model, calculated to be comparable to the Base accuracy. (d) Brand Detection"
        }
    ],
    "affiliations": [
        "National University of Singapore"
    ]
}