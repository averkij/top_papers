{
    "paper_title": "Cost-of-Pass: An Economic Framework for Evaluating Language Models",
    "authors": [
        "Mehmet Hamza Erol",
        "Batu El",
        "Mirac Suzgun",
        "Mert Yuksekgonul",
        "James Zou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The widespread adoption of AI systems in the economy hinges on their ability to generate economic value that outweighs their inference costs. Evaluating this tradeoff requires metrics that account for both performance and costs. We propose a framework grounded in production theory for evaluating language models by combining accuracy and inference cost. We introduce \"cost-of-pass\", the expected monetary cost of generating a correct solution. We then define the \"frontier cost-of-pass\" as the minimum cost-of-pass achievable across available models or the \"human-expert, using the approximate cost of hiring an expert. Our analysis reveals distinct economic insights. First, lightweight models are most cost-effective for basic quantitative tasks, large models for knowledge-intensive ones, and reasoning models for complex quantitative problems, despite higher per-token costs. Second, tracking this frontier cost-of-pass over the past year reveals significant progress, particularly for complex quantitative tasks where the cost has roughly halved every few months. Third, to trace key innovations driving this progress, we examine counterfactual frontiers: estimates of cost-efficiency without specific model classes. We find that innovations in lightweight, large, and reasoning models have been essential for pushing the frontier in basic quantitative, knowledge-intensive, and complex quantitative tasks, respectively. Finally, we assess the cost-reductions afforded by common inference-time techniques like majority voting and self-refinement, finding that their marginal accuracy gains rarely justify their costs. Our findings underscore that complementary model-level innovations are the primary drivers of cost-efficiency, and our economic framework provides a principled tool for measuring this progress and guiding deployment."
        },
        {
            "title": "Start",
            "content": "Cost-of-Pass: An Economic Framework for Evaluating Language Models Mehmet Hamza Erol* 1 Batu El* 1 Mirac Suzgun* 1 Mert Yuksekgonul 1 James Zou"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction 5 2 0 2 7 1 ] . [ 1 9 5 3 3 1 . 4 0 5 2 : r The widespread adoption of AI systems in the economy hinges on their ability to generate economic value that outweighs their inference costs. Evaluating this tradeoff requires metrics that account for both performance and costs. We propose framework grounded in production theory for evaluating language models by combining accuracy and inference cost. We introduce cost-ofpass, the expected monetary cost of generating correct solution. We then define the frontier costof-pass as the minimum cost-of-pass achievable across available models or the human-expert, using the approximate cost of hiring an expert. Our analysis reveals distinct economic insights. First, lightweight models are most cost-effective for basic quantitative tasks, large models for knowledgeintensive ones, and reasoning models for complex quantitative problems, despite higher per-token costs. Second, tracking this frontier cost-of-pass over the past year reveals significant progress, particularly for complex quantitative tasks where the cost has roughly halved every few months. Third, to trace key innovations driving this progress, we examine counterfactual frontiersestimates of cost-efficiency without specific model classes. We find that innovations in lightweight, large, and reasoning models have been essential for pushing the frontier in basic quantitative, knowledgeintensive, and complex quantitative tasks, respectively. Finally, we assess the cost-reductions afforded by common inference-time techniques like majority voting and self-refinement, finding that their marginal accuracy gains rarely justify their costs. Our findings underscore that complementary model-level innovations are the primary drivers of cost-efficiency, and our economic framework provides principled tool for measuring this progress and guiding deployment. *Co-first authors. Co-senior authors. 1Stanford University. (cid:0) {mhamza,jamesz}@stanford.edu. https://github.com/mhamzaerol/Cost-of-Pass. 1 The recent progress in generative AI, particularly language models (LMs), has sparked significant interest in their potential to transform industries, automate cognitive tasks, and reshape economic productivity (Brynjolfsson et al., 2025; Eloundou et al., 2024; Acemoglu, 2024). The widespread adoption of these AI systems in the economy hinges on whether the economic benefits generated by the tasks they can perform outweigh the associated inference costs, and whether those inference costs are lower than the cost of equivalent human labor. Consequently, two priorities have emerged at the forefront of LM research: advancing capabilities and reducing costs. These goals, however, often involve trade-offs with more powerful models or testtime techniques that offer higher accuracy at the expense of greater computational and monetary cost (Chen et al., 2024; Parashar et al., 2025; Madaan et al., 2023; Wang et al., 2023; Kapoor et al., 2024). While standard metrics capture accuracy or other system capabilities, they fail to account for cost, leading to an incomplete picture of progress. Ultimately, what matters to the users is not just raw capability, but the value delivered relative to cost and the standard has been to interpret and report these separately. As the ecosystem of models grows, it is essential to assess new models not in isolation, but in the context of broader ecosystem, where marginal improvements may or may not justify higher costs, and do so in an easy-to-interpret manner. To systematically investigate the trade-off between cost and performance and analyze the LM ecosystem as whole, we draw insights from well-established and foundational framework from economics: production frontiers. Economists have long studied these frontiers, which map set of inputs to the maximum output attainable under given technology (Farrell, 1957). In Farrells original formulation, producer is technically efficient if no input can be reduced without lowering output, and price efficient if the input mix minimizes cost given input prices. Together, these conditions yield the lowest possible cost per unit of output. Extending this framework, Aigner et al. (1977) introduced stochastic frontier production functions, in which the relationship between inputs and output is modeled as stochastic rather than deterministic, practically accounting for potential defective outputs that do not pass evaluation criteria due to factors beyond the producers control. Cost-of-Pass: An Economic Framework for Evaluating Language Models Figure 1: Highlights of the cost-of-pass framework and empirical analyses. Core concepts (left) set foundations for: (A) Comparing the Human Expert Baseline to the frontier achieved by the single most effective LM per task category. (B) Tracking the reduction in frontier cost-of-pass over time, indicating progress driven by new model releases (color-coded by family). (C) Quantifying the essential contribution of each model family: lightweight (less than $1 per million tokens), large, and reasoning; to the current cost-efficiency frontier, measured by the percentage of each familys contribution. (D) Assessing the economic benefit (relative cost reduction) achieved by applying common inference-time techniques over the baseline model frontier (which rarely results in meaningful gains). These economic concepts are highly relevant to modern LMs, which inherently function as stochastic producers: for given input, they yield desired output (e.g., correct solution) stochastically (Brown et al., 2024). Common practices such as employing scaffolds or more computationally intensive inference techniques (Snell et al., 2024; Madaan et al., 2023; Wang et al., 2023) represent efforts to manipulate this production process. These strategies seek to increase the probability of success but typically do so at the expense of higher computational cost, directly mirroring the economic trade-offs inherent in production efficiency. Motivated by these parallels and the economic goal of minimizing cost per successful output under uncertainty, we develop quantitative framework tailored to LMs. We summarize our contributions as follows. Concepts. We introduce cost-of-pass (2.2), which quantifies the expected monetary cost to achieve successful output for given problem. Building on this concept and incorporating human-expert cost baseline, we define the frontier cost-of-pass as the minimum achievable cost-ofpass across all available options (LMs and human-expert) for that problem. We show these reveal distinct economic niches for model families (e.g., lightweight vs. reasoning models) on different tasks, which accuracy comparisons alone obscure (3.2). Tracking progress with frontier cost-of-pass. Using the cost-of-pass and frontier cost-of-pass, we analyze economic improvements across three task categories from May 2024 to February 2025. We observe an exponential decrease in frontier cost-of-pass across all tasks, though the trends vary. Notably, we observe that, over the past year, the expected cost of generating correct solution to complex quantitative problems has been cut in half every few months. We find that the frontier cost-of-pass is driven primarily by lightweight models and reasoning models (3.3). Counterfactual frontier in the absence of model families. We show that our analysis reveals the complementary roles of different model types in driving recent progress. Innovations in lightweight models have been instrumental in reducing costs on basic quantitative tasks. Large models, by contrast, have been most impactful for knowledge-based benchmarks like GPQA Diamond (Rein et al., 2024). Meanwhile, reasoning models have been central to advances on complex quantitative reasoning challenges such as AIME (MAA, 2024) and MATH (Hendrycks et al., 2021) ( 3.4). Impact of post-hoc inference time techniques. We observe that common test-time techniques such as selfrefinement (Madaan et al., 2023) and majority voting (selfconsistency; Wang et al., 2022) to improve performance offer either limited or no economic benefits, indicating that the recent reductions in frontier cost-of-pass have been mostly driven by model-level innovations ( 3.5). 2 Cost-of-Pass: An Economic Framework for Evaluating Language Models 2. Setup 2.1. Economic Theory of Production Efficiency Classical production theory examines how producers convert inputs into outputs efficiently. Given set of producers = {f0, . . . , fn1}, we are often interested in the maximum output attainable for given combination of inputs. If producing R>0 units of output requires an input vector Rk 0 (e.g., quantities of different resources), the input requirement set Pu contains all input vectors capable of producing at least units: Pu = {x max fiF fi(x) u}. (1) Based on this input requirement and vector wi Rk 0 being the prices of the inputs (incurred by each producer i), the frontier cost for producing units of output is the minimum cost required: Vu = min xPu,fiF wi x, (2) subject to fi(x) implicitly included in Pu. This Vu quantifies the lowest possible cost to achieve output given the available production technologies (F) and input prices (wi). Farrell (1957) used these core concepts to build definitions for technical and price efficiency in production ecosystem for producers. Critically, Aigner et al. (1977) extended this framework to handle stochastic production functions, where output is probabilistic for given input. Building on this economic foundation, we adapt the core concept of frontier cost (Vu) to represent the minimum achievable cost for obtaining correct solution using LMs. Recognizing that key aspect of LM behavior is its inherent stochasticity, an issue long addressed in economic production theory (Aigner et al., 1977), we incorporate this variability into our cost-efficiency metric. This enables us to align our framework with core production concepts and assess the economic impact of stochastic LM producers. 2.2. Cost-of-Pass: An Efficiency Metric for LMs Here we instantiate the economic framework for language models (LMs). Consider specific problem p, where the unit of production is correct solution. We define model as an inference pipeline using an LM, acting as stochastic producer. Two quantities characterize its efficiency on problem p: Rm(p) = Prob. of producing correct answer on p, Cm(p) = Expected cost of one inference attempt by on p. In the context of LMs, the inputs correspond to resources like prompt and generated tokens, while the input prices represent the costs per token charged by the provider. The 3 total cost of these inputs for single inference attempt by model on problem is captured by Cm(p), effectively instantiating the term wT from the theory in the previous section. Since the model output is stochastic, the expected number of attempts to obtain the first correct solution is 1/Rm(p), assuming independent trials. This yields the cost-of-pass, defined as the expected monetary cost to obtain one correct solution for problem p: v(m, p) = Cm(p) Rm(p) . (3) The cost-of-pass integrates both performance (Rm(p)) and cost (Cm(p)) into single economically interpretable metric: it quantifies how efficiently financial resources are converted into correct outputs. This formulation mirrors classical production theory, where the goal is to assess the cost of achieving specific target output (Farrell, 1957); in our case, the target is correct solution. When model cannot produce one (Rm(p) = 0), the cost-of-pass becomes infinite, appropriately signaling infeasibility. 2.3. The LM Frontier Cost-of-Pass While cost-of-pass ( 2.2) evaluates single models efficiency, understanding the overall state of LM capabilities for given problem requires assessing the collective performance of the entire available LM ecosystem. Therefore, analogous to the frontier cost Vu (Eq. 2), we define the LM frontier cost-of-pass for problem as the minimum cost-ofpass achievable using any available LM strategy from the set M: Vp(M) = min mM v(m, p). (4) Vp(M) quantifies the minimum expected cost to solve problem using the most cost-effective model currently available within the set M. If no LM in can solve (i.e., Rm(p) = 0 for all M), then Vp(M) = . 2.4. Grounding Evaluation: Estimated Human-Expert Baseline The LM frontier cost-of-pass Vp(M) reveals the best LM performance but lacks context: it does not show if LMs are economically advantageous over human labor. Moreover, the LM frontier cost-of-pass can be infinite if no LM succeeds. To address both, we introduce human-expert baseline as reference point, by considering human-expert annotator as specific strategy: mexpert. Let M0 = {mexpert} represent this baseline set. We assume experts typically achieve near-perfect correctness (Rexpert(p) 1) for tasks they are qualified for. Thus, the cost-of-pass for qualified Cost-of-Pass: An Economic Framework for Evaluating Language Models expert is approximately their labor cost per problem: We can then extend our definitions for such distribution through the following: v(expert, p) Cexpert(p). (5) The estimation of Cexpert(p) involves considering required expertise, time per problem, and appropriate compensation rates (detailed in 2.6.1). By incorporating this baseline, we define the frontier cost-of-pass for problem p, considering both LMs (M) and the human-expert alternative (M0): Vp(M M0) = min(cid:0)Vp(M), v(expert, p)(cid:1). (6) This frontier cost-of-pass represents the true minimum expected cost to obtain correct solution for problem using the best available option, whether its an LM or human. Crucially, Vp(M M0) is always finite (assuming finite human-expert cost and capability). 2.5. Measuring Progress and Value Gain To track improvements against the best available option over time, let Mt denote the total set of available strategies at time t, encompassing both the set of LM strategies released up to time and the human-expert baseline M0, that is, Mt = {mt} M0. The frontier cost-of-pass achievable at time can be calculated as: Vp(Mt) = min mMt v(m, p). (7) As new LM models {mt} are released, the set expands such that Mt = Mt1 {mt}. Consequently, the frontier costof-pass Vp(Mt) forms non-increasing sequence over time t, tracking the reduction in the minimum cost needed to solve particular problem p. To quantify the economic impact of new developments, we define the gain. When new set of models {mt} becomes available at time (often single model), the gain for problem is the reduction it causes in the frontier cost-of-pass: Gp({mt}, Mt1) = Vp(Mt1) Vp(Mt1 {mt}). (8) Note that Gp measures how much cheaper the new model(s), {mt}, make solving compared to prior best options, including humans. Hence, large Gp value indicates significant economic contribution in solving p. This notion underlies our experiments, analyzing the value generated by models relative to the human baseline and tracking the evolution of the overall frontier. Extending to distribution. Although measuring frontier cost-of-pass and value gain for individual problems can be informative, particularly through fine-grained perspective, we often care about more than single instance. Let be set of problems sampled from problem distribution D. VpD(Mt) = EpD[Vp(Mt)], GpD({mt}, Mt1) = EpD[Gp({mt}, Mt1)]. (9) (10) 2.6. Estimating the Economic Efficiency To operationalize our overall framework for any given distribution of problems, we introduce the following recipe: (1) Estimate success rates. For each model-problem pair (m, p), generate number of independent attempts to approximate Rm(p). We use the same prompt and model settings across these attempts, varying only factors necessary to ensure independence (e.g., internal sampling randomness). (2) Estimate per-attempt cost. Track the average number of tokens (prompt + generation) consumed per attempt, multiply by the current token price (which can differ by model provider or usage level), and add any extra charges (e.g., third-party API calls, external reasoning modules, etc.). This sum yields Cm(p). (3) Compute cost-of-pass. For each model m, calculate v(m, p) = Cm(p)/Rm(p). (Rm(p) = 0 yields v(m, p) = .) (4) Determine frontier cost-of-pass. Estimate humanexpert cost v(expert, p) (see below). Find Vp(M M0) for given set of strategies M. (5) Analyze over benchmarks. Aggregate Vp(M) across problems to get VpD(Mt). Track progress over time (for Mt) and compute gain GpD for new models. 2.6.1. Estimating Human-Expert Cost To estimate v(expert, p), the plausible cost of obtaining correct human-expert answer, we systematically determine the required qualifications, appropriate hourly compensation, and average time for typical problem per dataset. We determine these quantities based on hierarchy of evidence by prioritizing the datasets creation process or associated studies (e.g., reported annotation pay/time (Parrish et al., 2022)). When direct data is absent, we leverage findings from closely related work (Zhang et al., 2024) or infer parameters from the datasets context (e.g., deriving timeper-problem from contest rules (Art of Problem Solving, 2023)). Compensation rates are informed by reported study payments (Rein, 2024) or relevant market rates for comparable expertise (e.g., specialized tutoring rates (TutorCruncher, 2025; Wyzant Tutoring, 2025)).1 1The full derivation, justification, and sources for our approach are detailed in Appendix A. The resulting estimates are in Table 3. Cost-of-Pass: An Economic Framework for Evaluating Language Models Model Category Lightweight Models Llama-3.1-8B GPT-4o mini Llama-3.3-70B Large Models Llama-3.1-405B Claude Sonnet-3.5 GPT-4o Reasoning Models OpenAI o1-mini OpenAI o1 DeepSeek-R1 OpenAI o3-mini Basic Quantitative Knowledge Based Complex Quantitative 2-Digit Add. GSM8K BBQ GPQA Dia. MATH 500 AIME24 4.8e5 5.4e5 1.6e4 6.9e4 2.1e3 2.3e3 5.4e3 1.9e2 1.8e3 1.1e3 0.19 0.22 0. 0.14 0.19 0.17 0.17 0.22 0.17 0.11 2.7e2 1.3e2 7.4e3 6.7e3 6.4e3 6.2e3 1.3e2 4.3e2 1.5e2 1.1e2 18.58 25.38 18. 10.43 14.06 14.07 12.27 8.07 14.57 8.18 3.38 2.06 1.31 1.13 2.54 0.96 0.50 0.90 0.21 0.76 15.33 14.67 10. 8.67 14.67 14.01 4.80 2.85 3.41 2.03 Table 1: Frontier dollar cost-of-pass per model / dataset. Each entry is the expected dollar cost of problem with the presence of the model and human expert: VpD({m} M0). Per column, the 3 entries with the lowest value (i.e. best frontier cost-of-pass) have blue highlights. Different model families emerge as cost-effective at different task categories, highlighting the strengths of our evaluation. 3. Experiments 3.1. Models and Datasets Models. We consider three categories of models: (1) Lightweight models: We use the per-token cost as proxy and select models with cost less than $1 per million input and output tokens (see Table 4): Llama-3.18B (Grattafiori et al., 2024), GPT-4o mini (OpenAI, 2024), and Llama-3.3-70B (Meta-AI, 2024). (2) Large models: We select large general-purpose LMs: Llama-3.1-405B (Grattafiori et al., 2024), Claude Sonnet3.5 (Anthropic, 2024), and GPT-4o (Hurst et al., 2024). (3) Reasoning models: We select models with special reasoning post-training, including OpenAIs o1-mini (OpenAI et al., 2024), o1 (OpenAI et al., 2024), and o3-mini (OpenAI, 2025), as well as DeepSeek R1 (Guo et al., 2025). Within each category, we select three to four representative models released between the second half of 2024 and early 2025. To preserve the integrity of our temporal analysis, we prioritize the earliest stable releases and exclude research previews or experimental versions. Datasets. We evaluate models across three sets of tasks: (1) Basic quantitative tasks: These involve basic numerical reasoning. We include an arithmetic dataset (Two Digit Addition) to assess basic numerical computation, and GSM8K (Cobbe et al., 2021) to evaluate multi-step gradeschool level problem solving. (2) Knowledge-based tasks: These require recalling and reasoning over factual knowledge. We include scientific knowledge-intensive question answering task (GPQA-Diamond (Rein et al., 2024)) to evaluate models ability to recall and utilize complex scientific facts, and bias benchmark (BBQ (Parrish et al., 2022)) to evaluate whether models rely on stereotypical knowledge or can disambiguate factual responses from biased defaults. (3) Complex quantitative reasoning tasks: These require complex mathematical reasoning and problem solving. We use MATH-500 (Hendrycks et al., 2021; Lightman et al., 2023) to assess models on competition-level maths problems, and AIME24 (MAA, 2024) to evaluate performance on challenging problems from the 2024 American Invitational Mathematics Examination. 3.2. Frontier Cost-of-Pass with Single Model In this experiment, we aim to quantify the economic value each model generates on different distributions of problems D. For this, we take human-expert as baseline and quantify the frontier cost-of-pass of problem in the presence of the model m: VpD({m} M0). The results in Table 1, highlighting the top three costs, show that our frontier cost-of-pass effectively captures how different model families offer economic advantages across various task categories. We find that lightweight models yield the lowest frontier cost-of-pass on basic quantitative tasks, such as Two Digit Addition. This is expected, as all model families achieve high accuracy on this dataset, making the least expensive models the most cost-effective. In contrast, for knowledge-based tasks, larger models achieve lower frontier cost-of-pass compared to lightweight ones. While the reasoning models, such as o1, are priced significantly more expensively compared to both large and lightweight models, they lead to significant performance improvements, which, overall, result in reductions in the cost-of-pass mainly in complex quantitative tasks. 5 Cost-of-Pass: An Economic Framework for Evaluating Language Models Figure 2: The frontier dollar cost-of-pass (i.e. VpD(Mt) steadily decreases with new model releases, spanning models released between May 2024 and February 2025. Y-axes are normalized (divided by VpD(M0), shown in percentage (%)). In contrast, when either task performance (Rm(p D)) or cost (Cm(p D) is solely taken into account (Tables 5 and 6) such metrics tend to favor either reasoning models or lightweight models respectively due to their significant edge per criteria, without assessing the nuances in the economic impact they induce. This effectively highlights the sophistication of our metric and evaluation framework. 3.3. Tracking Frontier Cost-of-Pass with New Releases In this experiment, we track the improvements on the frontier cost-of-pass for problem. Figure 2 shows the trends of the cumulative gain per dataset (VpD(Mt)), each updated by the corresponding model release (Mt1 {mt}). We observe steady decline in the frontier cost-of-pass for complex quantitative tasks. In contrast, knowledge-based and basic quantitative tasks typically exhibit sharp initial drop in frontier cost-of-pass with the early releases of models, followed by plateau. To quantify the cost reduction trends, we empirically fit an exponential decay curve of the form: Vp(Mt) eb + c, (11) where denotes time in months since the first model release, and a, b, and are fit parameters. From this, we compute the time for the exponential component of the cost to drop by 50%: T1/2 = ln(2)/b. Using this formulation, we find that for complex quantitative tasks, between May 2024 and February 2025, the frontier cost-of-pass for MATH500 halved approximately every 2.6 months, whereas for AIME 2024, the halving time was 7.1 monthsindicating consistent cost reductions over the past year. 3.4. Essentialness of Model Families: Counterfactual Frontier Cost-of-Pass Section 3.3 showed the frontier cost-of-pass decreasing over time with new model releases. To understand which model families were most critical to this progress, we conduct counterfactual analysis that quantifies the impact of removing each family. Defining Mg as family of models (lightweight, large, or reasoning), we measure the counterfactual contribution of family on dataset by calculating the relative improvement in frontier cost-of-pass attributable to its inclusion: GpD(Mg, MT Mg) VpD(MT ) . (12) Here, MT includes all models used in our experiments. This metric represents the relative improvement in the final frontier cost-of-pass VpD(MT ) attributable to the model family Mg, with higher values indicating greater essentialness of that family for achieving the current frontier. Figure 3 illustrates our main findings, revealing distinct roles across model families. Lightweight models help reduce the frontier cost-of-pass on basic quantitative tasks, while large models drive performance on knowledge-intensive tasks. Reasoning models play key role in advancing the frontier for complex quantitative reasoning and also improve 6 Cost-of-Pass: An Economic Framework for Evaluating Language Models Figure 3: The relative improvement (%) in frontier cost-of-pass attributable to each model family g, calculated under counterfactual setting where Mg is removed. Higher values signify greater essentialness for maintaining the current frontier. Inference Time Technique Basic Quantitative Knowledge Based Complex Quantitative Two Digit Addition GSM8K BBQ GPQA Diamond MATH500 AIME24 Self-Refine Maj. Vote (k=3) Maj. Vote (k=4) 0 0 0 0 0 0 6.7 0 24.9 0 0 0 0 0 0 0 0 Table 2: Relative performance gains (%) from different inference time techniques across datasets. performance on GPQA-Diamond, as well as GSM8K, which benefits from small reasoning models like o3-mini. refinement (Madaan et al., 2023) and majority voting (a.k.a. self-consistency; Wang et al., 2023), with 3 and 4 votes. These findings highlight that progress on different task types is driven by different model paradigms. While large models have brought clear gains on knowledge-intensive tasks (e.g., GPQA), recent improvements in cost-efficiencyespecially in more quantitative domainsappear largely driven by advances in lightweight and reasoning models. Together, these results suggest that the current cost-efficiency frontier, as reflected in our framework, is shaped mainly by (i) lightweight models and (ii) reasoning models. 3.5. Impact of Inference Time Techniques on Frontier Cost-of-Pass We now assess whether common inference-time techniques provide meaningful economic benefits. Specifically, we ask: is it cost-effective to improve model performance through these techniques, compared to relying on the models baseline performance? To explore this, we focus on the set of lightweight and large models, denoted by ML. First, we determine the frontier cost-of-pass achieved by ML without any modifications. We then apply given inference-time technique uniformly across all models in ML, yielding modified set L. The gain from this technique, measured relative to the original frontier cost-of-pass, can be computed as follows: GpD(M L, ML) VpD(ML) . (13) In this study, we consider two popular techniques: selfAs shown in Table 2, self-refinement shows moderate economic benefit on knowledge-intensive tasks, with notable 24.9% improvement on GPQA Diamond. In contrast, majority votingdespite potentially enhancing raw accuracy does not offer relative economic improvement across the tested models and datasets. Collectively, these findings suggest, at least for the evaluated techniques, that the increased computational costs generally outweigh the performance benefits relative to the frontier cost-of-pass established by the baseline models. This implies that these common inference-time approaches may not be sufficient on their own to yield significant economic benefits within our evaluation framework for now. 4. Related Works Economic perspectives and broader impacts. The efficiency of LMs carries significant economic implications, as they are viewed as general-purpose technologies impacting productivity and labor (Eloundou et al., 2024; Brynjolfsson et al., 2025). Complementary economic analyses explore provider strategies regarding pricing and product design Bergemann et al. (2025), and user-side decision-making involving ROI, token costs, and success probabilities. Our cost-of-pass metric serves as crucial bridge between these technical realities of model performance and their economic consequences. By providing fundamental measure, the expected monetary cost to successfully complete 7 Cost-of-Pass: An Economic Framework for Evaluating Language Models task, it allows for quantifying the economic contribution of specific AI systems and informs rational model selection for achieving economic viability, and provides quantitative perspective on the economic evolution of the LM ecosystem. LM resource consumption, efficiency optimization and benchmarking. Research increasingly recognizes the importance of LM resource consumption and efficiency. Studies have quantified operational costs like tokens (Chen et al., 2023) and energy (Maliakel et al., 2025), revealing taskdependent performance and potential diminishing returns from high expenditure (Miserendino et al., 2025). This focus has intensified with the rise of reasoning methodologies (Sui et al., 2025) and inference-time techniques (e.g., Madaan et al. (2023); Wang et al. (2023)), which often trade increased computational cost for potential accuracy gains. Concerns like overthinking, where lengthy processing fails to improve results (Chen et al., 2024; Cuadron et al., 2025), have spurred efforts to optimize resource use through methods like dynamic token budgeting (Han et al., 2025), specialized training (Arora & Zanette, 2025), prompt engineering (Xu et al., 2025; Aytes et al., 2025) or researching optimal reasoning lengths (Wu et al., 2025; Yang et al., 2025). Concurrently, evaluation methodologies have evolved beyond pure accuracy or correctness measures. Recognizing its insufficiency, researchers have incorporated cost via fixed budgets (Wang et al., 2024), performance heuristics (McDonald et al., 2024), or non-monetary metrics like conciseness (Nayab et al., 2024). Kapoor et al. (2024) strongly advocated for using real dollar costs and accounting for stochasticityfactors central to our approach. Benchmarking efforts have also highlighted diminishing returns from simply scaling inference computation (Parashar et al., 2025). While these works underscore the need for cost-aware analysis, they often rely on specific constraints (e.g., fixed budgets) or heuristic metrics. Our cost-of-pass framework seeks to advance this by providing single, interpretable metric grounded in economic production principles, offering unified way to assess the economic viability of different models and techniques without predefined budget assumptions or proxy metrics. 5. Conclusion We introduced an economic framework designed to evaluate language models by integrating their performance with inference cost. Drawing from production theory, we conceptualize language models as stochastic producers, and assess their efficiency using our proposed cost-of-pass metric, which measures the expected cost per correct solution. Our analysis utilizes this metric alongside the frontier costof-pass, defined as the minimum achievable cost compared to an human expert baseline. This approach reveals distinct economic roles played by different model classes. For instance, retrospective and counterfactual evaluations demonstrate that lightweight models primarily drive efficiency on basic tasks, whereas reasoning models are essential for complex problem-solving. Critically, our findings show that common inference-time techniques typically increase the cost-of-pass, thus failing to provide net economic benefits when compared to the progress made by improving the underlying models themselves. In conclusion, our framework offers principled foundation for measuring language model innovation in economic terms. It serves as valuable tool for guiding model selection and aligning AI development with real-world value."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Federico Bianchi, Dan Jurafsky, Daniel E. Ho, Can Yesildere, and Semyon Lomasov for valuable comments and discussions in the early stages of this project. MHE gratefully acknowledges support from the Fulbright Foreign Student Program. BE gratefully acknowledges the support of the Stanford Knight-Hennessy Scholarship. MS gratefully acknowledges the support of an HAI-SAP Fellowship."
        },
        {
            "title": "References",
            "content": "1st grade 4th quarter expectations fast facts timed tests. Elementary School Curriculum Note (online PDF), 2021. States 2025 addition problems should be solved in 1 minute (23 sec each) (Fas, 2021). Daron Acemoglu. The Simple Macroeconomics of AI. NBER Working Papers 32487, National Bureau of Economic Research, Inc, May 2024. URL https://ideas. repec.org/p/nbr/nberwo/32487.html. Dennis Aigner, C.A.Knox Lovell, and Peter Schmidt. Formulation and estimation of stochastic frontier Journal of Econoproduction function models. metrics, 6(1):2137, 1977. doi: https://doi.org/10.1016/0304-4076(77)90052-5. URL https://www.sciencedirect.com/science/article/pii/ 0304407677900525. ISSN 0304-4076. Anthropic. Claude 3.5 sonnet announcement, 2024. URL https://www.anthropic.com/news/claude-3-5-sonnet. Accessed: 13 Feb. 2025. Daman Arora and Andrea Zanette. Training language models to reason efficiently. arXiv preprint arXiv:2502.04463, 2025. Art of Problem Solving. American Invitational Mathematics Examination (AIME) Format. AoPS Wiki (aops.com), 2023. States AIME is 15 questions in 3 hours ( 12 min 8 Cost-of-Pass: An Economic Framework for Evaluating Language Models per problem) (Art of Problem Solving, 2023). Accessed Mar 25, 2025. et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Simon Aytes, Jinheon Baek, and Sung Ju Hwang. Sketch-of-thought: Efficient llm reasoning with adaparXiv preprint tive cognitive-inspired sketching. arXiv:2503.05179, 2025. Dirk Bergemann, Alessandro Bonatti, and Alex Smolin. The economics of large language models: Token allocation, fine-tuning, and optimal pricing. arXiv preprint arXiv:2502.07736, 2025. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Re, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling, 2024. URL https://arxiv. org/abs/2407.21787. Erik Brynjolfsson, Danielle Li, and Lindsey Raymond. Generative ai at work. The Quarterly Journal of Economics, pp. qjae044, 2025. Lingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt: How to use large language models while reducing cost and improving performance. arXiv preprint arXiv:2305.05176, 2023. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Alejandro Cuadron, Dacheng Li, Wenjie Ma, Xingyao Wang, Yichuan Wang, Siyuan Zhuang, Shu Liu, Luis Gaspar Schroeder, Tian Xia, Huanzhi Mao, et al. The danger of overthinking: Examining the reasoningarXiv preprint action dilemma in agentic tasks. arXiv:2502.08235, 2025. Tyna Eloundou, Sam Manning, Pamela Mishkin, and Daniel Rock. Gpts are gpts: Labor market impact potential of llms. Science, 384(6702):13061308, 2024. Michael James Farrell. The measurement of productive efficiency. Journal of the royal statistical society: series (General), 120(3):253281, 1957. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, and Zhenyu Chen. Token-budget-aware llm reasoning, 2025. URL https://arxiv.org/abs/2412. 18547. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solvIn Thirty-fifth Confering with the MATH dataset. ence on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https: //openreview.net/forum?id=7Bywt2mQsCe. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Sayash Kapoor, Benedikt Stroebl, Zachary Siegel, Nitya Nadgir, and Arvind Narayanan. Ai agents that matter. arXiv preprint arXiv:2407.01502, 2024. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. MAA. American Examination (AIME). maa-invitational-competitions/, 2024. 2025-03-25. Invitational Mathematics https://maa.org/ Accessed: Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. Paul Joe Maliakel, Shashikant Ilager, and Ivona Brandic. Investigating energy efficiency and performance tradeoffs in llm inference across tasks and dvfs settings. arXiv preprint arXiv:2501.08219, 2025. Tyler McDonald, Anthony Colosimo, Yifeng Li, and Ali Emami. Can we afford the perfect prompt? balancing cost and accuracy with the economical prompting index. arXiv preprint arXiv:2412.01690, 2024. Cost-of-Pass: An Economic Framework for Evaluating Language Models Meta-AI. URL 3-70B-Instruct. Llama 3.3 70b instruct model, 2024. https://huggingface.co/meta-llama/Llama-3. Samuel Miserendino, Michele Wang, Tejal Patwardhan, and Johannes Heidecke. Swe-lancer: Can frontier llms earn $1 million from real-world freelance software engineering? arXiv preprint arXiv:2502.12115, 2025. Sania Nayab, Giulio Rossolini, Marco Simoni, Andrea Saracino, Giorgio Buttazzo, Nicolamaria Manes, and Fabrizio Giacomelli. Concise thoughts: Impact of output length on llm reasoning and cost. arXiv preprint arXiv:2407.19825, 2024. OpenAI. Gpt-4o mini: Advancing cost-efficient intelligence, 2024. URL https://openai.com/index/ gpt-4o-mini-advancing-cost-efficient-intelligence/. OpenAI. Openai o3-mini system card, 2025. URL https: //openai.com/index/o3-mini-system-card/. OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian Osband, Ignasi Clavera Gilaberte, Ilge Akkaya, Ilya Kostrikov, Ilya Sutskever, Irina Kofman, Jakub Pachocki, James Lennon, Jason Wei, Jean Harb, Jerry Twore, Jiacheng Feng, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joaquin Quinonero Candela, Joe Palermo, Joel Parish, Johannes Heidecke, John Hallman, John Rizzo, Jonathan Gordon, Jonathan Uesato, Jonathan Ward, Joost Huizinga, Julie Wang, Kai Chen, Kai Xiao, Karan Singhal, Karina Nguyen, Karl Cobbe, Katy Shi, Kayla Wood, Kendra Rimbach, Keren Gu-Lemberg, Kevin Liu, Kevin Lu, Kevin Stone, Kevin Yu, Lama Ahmad, Lauren Yang, Leo Liu, Leon Maksin, Leyton Ho, Liam Fedus, Lilian Weng, Linden Li, Lindsay McCallum, Lindsey Held, Lorenz Kuhn, Lukas Kondraciuk, Lukasz Kaiser, Luke Metz, Madelaine Boyd, Maja Trebacz, Manas Joglekar, Mark Chen, Marko Tintor, Mason Meyer, Matt Jones, Matt Kaufer, Max Schwarzer, Meghan Shah, Mehmet Yatbaz, Melody Y. Guan, Mengyuan Xu, Mengyuan Yan, Mia Glaese, Mianna Chen, Michael Lampe, Michael Malek, Michele Wang, Michelle Fradin, Mike McClay, Mikhail Pavlov, Miles Wang, Mingxuan Wang, Mira Murati, Mo Bavarian, Mostafa Rohaninejad, Nat McAleese, Neil Chowdhury, Neil Chowdhury, Nick Ryder, Nikolas Tezak, Noam Brown, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivia Watkins, Patrick Chao, Paul Ashbourne, Pavel Izmailov, Peter Zhokhov, Rachel Dias, Rahul Arora, Randall Lin, Rapha Gontijo Lopes, Raz Gaon, Reah Miyara, Reimar Leike, Renny Hwang, Rhythm Garg, Robin Brown, Roshan James, Rui Shu, Ryan Cheu, Ryan Greene, Saachi Jain, Sam Altman, Sam Toizer, Sam Toyer, Samuel Miserendino, Sandhini Agarwal, Santiago Hernandez, Sasha Baker, Scott McKinney, Scottie Yan, Shengjia Zhao, Shengli Hu, Shibani Santurkar, Shraman Ray Chaudhuri, Shuyuan Zhang, Siyuan Fu, Spencer Papay, Steph Lin, Suchir Balaji, Suvansh Sanjeev, Szymon Sidor, Tal Broda, Aidan Clark, Tao Wang, Taylor Gordon, Ted Sanders, Tejal Patwardhan, Thibault Sottiaux, Thomas Degry, Thomas Dimson, Tianhao Zheng, Timur Garipov, Tom Stasi, Trapit Bansal, Trevor Creech, Troy Peterson, Tyna Eloundou, Valerie Qi, Vineet Kosaraju, Vinnie Monaco, Vitchyr Pong, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wes McCabe, Wojciech Zaremba, Yann Dubois, Yinghai Lu, Yining Chen, Young Cha, Yu Bai, Yuchen He, Yuchen Zhang, Yunyun Wang, Zheng Shao, and Zhuohan Li. Openai o1 system card, 2024. URL https://arxiv.org/abs/2412.16720. Shubham Parashar, Blake Olson, Sambhav Khurana, Eric Li, Hongyi Ling, James Caverlee, and Shuiwang Ji. Inference-time computations for llm reasoning and planarXiv preprint ning: benchmark and insights. arXiv:2502.12521, 2025. Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel Bowman. BBQ: hand-built bias benchmark for question answering. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Findings of the Association for Computational Linguistics: ACL 2022, pp. 20862105, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. 10 Cost-of-Pass: An Economic Framework for Evaluating Language Models Wyzant Tutoring. New jersey math tutors cost $33 - $55 per hour on average. Wyzant.com (tutoring rate listing), 2025. Average private tutoring rates for math (K-12 and competition) (Wyzant Tutoring, 2025). Accessed Mar 25, 2025. Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. Chain of draft: Thinking faster by writing less. arXiv preprint arXiv:2502.18600, 2025. Wenkai Yang, Shuming Ma, Yankai Lin, and Furu Wei. Towards thinking-optimal scaling of test-time compute for llm reasoning. arXiv preprint arXiv:2502.18080, 2025. Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja, Charlotte Zhuang, Dylan Slack, Qin Lyu, Sean Hendryx, Russell Kaplan, Michele Lunati, and Summer Yue. careful examination of large language model performance on grade school arithmetic. In NeurIPS 2024 Datasets and Benchmarks Track, 2024. Reports human solve rate on GSM8K: 4 problems/15 min (3.7 min each) (Zhang et al., 2024). findings-acl.165. URL https://aclanthology.org/2022. findings-acl.165/. David Rein. Can good benchmarks contain mistakes? NYU Alignment Research Group Blog, May 2024. Reveals GPQA expert pay ($100/hr) and non-expert solve times (Rein, 2024). Online: wp.nyu.edu/...mistakes. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduatelevel google-proof qa benchmark. In First Conference on Language Modeling, 2024. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Hanjie Chen, Xia Hu, et al. Stop overthinking: survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419, 2025. TutorCruncher. Average tutoring rates usa: How much do tutors charge per hour? TutorCruncher Blog, Feb 2025. Reports $45$100/hr as typical range for test-prep tutoring (TutorCruncher, 2025). Upwork. Data entry specialist hourly rates (cost to hire data entry specialist). Upwork Hiring Guide, 2025. Median $13/hr for data entry freelancers; $10$20/hr typical range (Upwork, 2025). Accessed Mar 25, 2025. Junlin Wang, Siddhartha Jain, Dejiao Zhang, Baishakhi Ray, Varun Kumar, and Ben Athiwaratkun. Reasoning in token economies: Budget-aware evaluation of llm reasoning strategies. arXiv preprint arXiv:2406.06461, 2024. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=1PL1NIMMrw. Yuyang Wu, Yifei Wang, Tianqi Du, Stefanie Jegelka, and Yisen Wang. When more is less: Understanding chain-ofthought length in llms. arXiv preprint arXiv:2502.07266, 2025. 11 Cost-of-Pass: An Economic Framework for Evaluating Language Models A. Details of Human Expert Cost Estimation approximately 16 questions in one hour (Zhang et al., 2024). In this section, we introduce the detailed analysis of how the human expert costs in Table 3 are calculated per dataset. AIME (American Invitational Mathematics Examination) consists of 15 challenging math problems in 3-hour contest (administered in two separate sections: AIME & II), giving an average of about 12 minutes per problem (Art of Problem Solving, 2023). In practice, expert math tutors for competitions like AIME command high hourly fees in the range of $45$100, reflecting intensive test-preparation rates (TutorCruncher, 2025). This rate range aligns with specialized test prep tutoring in the US, which is higher than regular tutoring due to the advanced problem-solving skills required (TutorCruncher, 2025). At roughly 12 minutes per AIME question on average, solver could handle about five such problems per hour under exam conditions (Art of Problem Solving, 2023). BBQ (Bias Benchmark for QA) contains short questionanswer scenarios targeting social bias. Crowdworkers annotating BBQ have been paid around $15 per hour, rate chosen to exceed U.S. minimum wage (Parrish et al., 2022). Because each task includes multiple BBQ questions, workers were able to answer roughly 5 questions in 2 minutes (Parrish et al., 2022) i.e. 24 seconds per question, or about 0.4 minutes per question. This fast per-question time reflects the fact that BBQ items are short multiplechoice queries, allowing human annotator to complete approximately 150 BBQ questions in an hour at that pay rate (Parrish et al., 2022). GPQA Diamond consists of extremely difficult graduatelevel science questions, so human experts demand high compensation. In one case, domain experts were paid about $100 per hour to contribute and validate GPQA questions (Rein et al., 2024). These questions are Google-proof and time-consuming: skilled non-expert participants spent over 3035 minutes on average per question when attempting to solve GPQA problems with unrestricted web access (Rein et al., 2024). This long duration per question underscores GPQAs complexity at most 2 questions could be solved in an hour even by motivated annotators, which justifies the premium expert hourly rate (Rein, 2024). GSM8K contains grade-school level math word problems. Solving these is relatively time-efficient for adults: in one study, crowdworkers under time pressure managed to solve about 4.07 GSM8K problems in 15 minutes on average (Zhang et al., 2024). That corresponds to roughly 3.7 minutes per question for human solver. The required skill is comparable to general math tutoring at the K-8 level, for which typical U.S. tutor rates are about $33$55 per hour on platforms like Wyzant (Wyzant Tutoring, 2025). At such rate, paying person to solve GSM8K problems would be economical, given that proficient solver can complete MATH500 is set of 500 advanced competition math problems (drawn from the harder tier of larger MATH dataset). These problems are similar in difficulty to top-level contest questions such as late AIME or Olympiad qualifying problems. As with AIME, well-prepared human might spend on the order of 1015 minutes per problem, roughly 12 minutes on average for hard competition question (Art of Problem Solving, 2023). Tutors capable of solving and teaching such Olympiad-level problems often charge rates on the order of $50 per hour (with typical range of $35 $60 for competition math tutoring) (Wyzant Tutoring, 2025). This implies that solving roughly five MATH500 problems could cost about $50 and take around an hour, consistent with the per-question time and high skill required. Two-Digit Addition consists of simple two-digit addition problems, which are very quick for humans to solve. Early elementary students are often expected to complete about 20 25 basic addition problems in one minute in mad minute drills (Fas, 2021). This corresponds to roughly 23 seconds per addition (0.04 minutes per question). Because the task is so elementary, the labor to solve large numbers of such problems can be valued at lower hourly rate. Simple dataentry style work or basic math tasks on freelance platforms pay on the order of $10$20 per hour (Upwork, 2025). At $15/hour, for example, worker could theoretically solve several hundred 2-digit additions within the hour, given the 3-second average solution time (Fas, 2021). B. Details of Evaluation For each dataset in our evaluation, we sample up to 128 instances and run each model = 8 times to estimate the expected runtime cost and accuracy per sample. For all models except OpenAIs reasoning models, we set the temperature to 0.7 and top to 1.0. In the case of OpenAIs reasoning models, we use temperature of 1.0 and do not apply top p. Additionally, we use the default maximum token generation limits provided by each model. Per sample, we employ concise but descriptive instruction prompt for the models to follow. In our experiments, we define the pass rm(p) as whether the model obtains correct answer after single run or not (0 or 1), and the cost cm(p) as: cm(p) = nin(m, p) cin(m) + nout(m, p) cout(m) (14) where n*(m, p) denotes the number of input / output tokens consumed / generated by the model on problem p, and c*(m) denotes the dollar costs per input / output tokens consumed / generated by the model (see Table 4 for the pricing). For the expert costs, we utilize the estimations from Table 3, and set the rates to the upper-bound value to ensure the approximation of the expert accuracy being 1. 12 Cost-of-Pass: An Economic Framework for Evaluating Language Models Dataset"
        },
        {
            "title": "AIME",
            "content": "BBQ GPQA Dia. GSM8K MATH500 Qualification Requirements Hourly Rate Time per Question Est. Cost Advanced high-school contest math skills General familiarity with social biases Graduate-level domain expertise Basic arithmetic reasoning Strong competition-level solving problem- $45$100 12 minutes $9$20 $15 $100 $33$55 $35$60 0.4 minutes (24 sec) 35 minutes 3.7 minutes 12 minutes $0.10 $58 $2$3.50 $7$12 Two-Digit Add. Basic numeracy $10$20 0.04 minutes (3 sec) $0.01$0.02 Table 3: Estimated costs of hiring human expert to solve one question from each dataset, based on typical qualifications, hourly rates, and time per question. Experiment Prompt Please solve the following question. You can explain your solution before presenting the final answer. Format your final answer as: <answer> ... </answer> Instructions: - For multiple-choice: Give only the letter (e.g., (A)). - For numeric: Give only the number (e.g., 42). - For free-response: Provide the full final answer text. INPUT: {input} C. Additional Results C.1. Expected Accuracy and Inference Costs As discussed in the Section 3.2, we share the results of expected cost and accuracy per model per dataset. We can observe the skewed preference of particular model family under each metric, implying the inability of expressing economic impact of models through these metrics solely. C.2. Relative Gain per Model Release Figure 4 presents the relative improvement in temporal frontier cost-of-pass for each model release, illustrated using bar plots. Namely, we calculate: GpD({mt}, Mt1) VpD(Mt1) (15) 13 The results indicate that the reasoning models demonstrate notable advancements, particularly on complex quantitative tasks. In contrast, lightweight models exhibit marked gains on basic tasks. These findings support the observations from our experiments (Sections 3.2, 3.4). Notably, The substantial improvement observed for GPT-4o is likely due to it being the first model included in our analysis, resulting in pronounced leap relative to the baseline cost associated with human expert annotation. C.3. Counterfactual Frontier Cost-of-Pass in the Absence of Single Model In this section, following the methodology outlined in Section 3.4, we quantify the relative improvement in frontier cost-of-pass using counterfactual approach. Specifically, for each model m, we calculate the following: GpD({m}, MT {m}) VpD(MT {m}) , (16) quantifying the essentialness of the model m. The results presented in Figure 5 demonstrate that the contributions of most individual models are largely compensable by the remaining models. Furthermore, we observe similar coarselevel trend, as noted in Section 3.4, indicating that different model families provide greater benefits in specific task categories. D. Limitations of Our Framework and Future"
        },
        {
            "title": "Work Directions",
            "content": "In this section, we acknowledge the limitations of the presented framework and propose directions for future improvements and extensions. primary limitation pertains to our definitions and computations of cost (Cp(m)) and performance (Rp(m)). Specifically, our current cost computation considers only input and output token costs as proxies for the total expense incurred in obtaining correct outputs. This approach neglects indirect or overhead costs associated with generating incorrect outputs, such as subsequent verification costs. Regarding perCost-of-Pass: An Economic Framework for Evaluating Language Models Category Model Release Date Cost (per million tokens) Input Tokens Output Tokens Lightweight Models Large Models Reasoning Models Llama-3.1-8B GPT-4o Mini Llama-3.3-70B Llama-3.1-405B GPT-4o Claude Sonnet-3.5 OpenAI o1-mini OpenAI o3-mini DeepSeek-R1 OpenAI o1 7/23/2024 7/18/2024 12/6/2024 7/23/2024 5/13/2024 6/20/2024 9/12/2024 1/31/2025 1/20/2025 12/5/2024 $0.18 $0.15 $0. $3.50 $2.50 $3.00 $1.10 $1.10 $7.00 $15.00 $0.18 $0.60 $0.88 $3.50 $10.00 $15.00 $4.40 $4.40 $7.00 $60.00 Table 4: Per-token inference costs with release dates. Model Category Lightweight Models Llama-3.1-8B GPT-4o mini Llama-3.3-70B Large Models Llama-3.1-405B Claude Sonnet-3.5 GPT-4o Reasoning Models OpenAI o1-mini OpenAI o1 DeepSeek-R1 OpenAI o3-mini Basic Quantitative Knowledge Based Complex Quantitative 2-Digit Add. GSM8K BBQ GPQA Dia. MATH 500 AIME24 89.45 99.90 99.90 99.71 100.00 99.71 99.51 100.00 100.00 100. 75.78 88.57 92.09 93.95 94.43 91.99 92.58 94.04 93.36 92.77 21.48 53.32 85.06 85.74 92.58 90.04 85.74 95.02 83.69 83. 17.87 18.07 46.48 44.14 55.37 47.07 49.12 73.83 54.88 71.68 37.30 70.02 72.75 67.87 64.75 73.14 85.94 89.45 93.85 88. 12.50 14.58 33.33 31.67 15.83 14.58 53.33 72.50 60.83 77.08 Table 5: Accuracy (%) per model per dataset: Rm(p D). In each column, the 3 entries with the highest accuracy have blue highlights. Model Category Lightweight Models Llama-3.1-8B GPT-4o mini Llama-3.3-70B Large Models Llama-3.1-405B Claude Sonnet-3.5 GPT-4o Reasoning Models OpenAI o1-mini OpenAI o1 DeepSeek-R1 OpenAI o3-mini Basic Quantitative Knowledge Based Complex Quantitative 2-Digit Add. GSM8K BBQ GPQA Dia. MATH 500 AIME24 4.2e5 5.4e5 1.6e4 6.9e4 2.1e3 2.3e3 5.4e3 0.02 1.8e3 1.1e 7.4e5 1.9e4 3.3e4 5.2e5 1.0e4 3.1e4 1.4e3 3.7e3 4.5e3 1.0e3 3.0e3 2.7e3 8.4e3 0.03 5.1e3 2.1e3 7.6e3 0.04 4.6e3 2.6e 1.8e4 3.9e4 9.6e4 3.0e3 6.9e3 0.01 0.02 0.25 0.04 0.01 1.5e4 3.7e4 6.7e4 2.4e3 5.9e3 8.7e3 0.02 0.13 0.01 5.4e 2.2e4 5.6e4 1.1e3 3.7e3 7.5e3 0.01 0.07 0.52 0.04 0.02 Table 6: Dollar cost incurred per model per dataset: Cm(p D). In each column, the 3 entries with the lowest cost have blue highlights. 14 Cost-of-Pass: An Economic Framework for Evaluating Language Models Figure 4: Bar plot showing the percentage of change in frontier cost-of-pass per model release (i.e. GpD ({mt},Mt1) VpD (Mt1) ) could also enhance practical usability and interpretability. An additional limitation lies in the evaluation methodology, particularly regarding human expert cost estimation. Our framework assumes that experts can reliably solve tasks given sufficient conditions (e.g., adequate qualifications, time, compensation). However, this assumption may not hold for particularly challenging problems or datasets with inherently high uncertainty in achieving correct solutions. Future research could address this limitation by conducting rigorous human subject studies to empirically evaluate and incorporate expert performance variability into the cost estimation process. formance, the use of accuracy as binary success-or-failure metric presupposes the existence of reliable verification pipeline and practical decision mechanism, potentially oversimplifying scenarios where these assumptions do not hold. Additionally, our cost-of-pass metric, which combines cost and performance, currently does not account for variance information, limiting its practical interpretability in situations where two scenarios with similar cost-of-pass values exhibit substantially different variances. Furthermore, from practical standpoint, cost modeling could consider alternative units (e.g., latency, inference time, FLOPs), which are currently not analyzed. Nevertheless, significant strength of our framework is its abstract and modular design, facilitating extensions to address these limitations. Future work can enhance the precision of cost computations by integrating additional cost factors, such as verification overheads or indirect costs. Moreover, the framework could be adapted to alternative resource-consumption metrics like latency, inference time, or FLOPs. Regarding performance evaluation, the binary accuracy metric could be replaced or supplemented with alternative success measures tailored to specific scenarios, especially those emphasizing particular balance between performance and cost. Incorporating variance and other statistical information into cost and performance calculations 15 Cost-of-Pass: An Economic Framework for Evaluating Language Models Figure 5: The relative improvement (%) in frontier cost-of-pass under counterfactual setting, removing model from the model set MT . High values mean that the model is essential for maintaining the current frontier."
        }
    ],
    "affiliations": [
        "Stanford University"
    ]
}