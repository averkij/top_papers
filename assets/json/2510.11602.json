{
    "paper_title": "Deconstructing Attention: Investigating Design Principles for Effective Language Modeling",
    "authors": [
        "Huiyin Xue",
        "Nafise Sadat Moosavi",
        "Nikolaos Aletras"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The success of Transformer language models is widely credited to their dot-product attention mechanism, which interweaves a set of key design principles: mixing information across positions (enabling multi-token interactions), sequence-dependent activations (where attention weights adapt to each input), a specific mathematical form (dot-product similarities plus softmax weighting), and coupling of queries and keys to evolving hidden states (grounding attention in the current layer). However, the necessity of each of these principles remains largely untested. In this work, we systematically deconstruct attention by designing controlled variants that selectively relax these principles, applied both uniformly across all layers and in hybrid architectures where only some layers retain standard attention. Our empirical analysis reveals that mechanisms for mixing tokens are indispensable, as their absence collapses models to near-random behavior, while the exact mathematical form and sequence dependency can be substantially relaxed, especially when preserved in just a subset of layers. Surprisingly, even variants that fail in isolation can achieve robust performance when interleaved with standard attention, highlighting a cooperative effect. These findings deepen our understanding of what truly underpins attention's effectiveness and open new avenues for simplifying language models without sacrificing performance."
        },
        {
            "title": "Start",
            "content": "Deconstructing Attention: Investigating Design Principles for Effective Language Modeling Huiyin Xue, Nafise Sadat Moosavi and Nikolaos Aletras School of Computer Science, University of Sheffield, United Kingdom {hxue12, n.s.moosavi, n.aletras}@sheffield.ac.uk 5 2 0 2 3 1 ] . [ 1 2 0 6 1 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The success of Transformer language models is widely credited to their dot-product attention mechanism, which interweaves set of key design principles: mixing information across positions (enabling multi-token interactions), sequence-dependent activations (where attention weights adapt to each input), specific mathematical form (dot-product similarities plus softmax weighting), and coupling of queries and keys to evolving hidden states (grounding attention in the current layer). However, the necessity of each of these principles remains largely untested. In this work, we systematically deconstruct attention by designing controlled variants that selectively relax these principles, applied both uniformly across all layers and in hybrid architectures where only some layers retain standard attention. Our empirical analysis reveals that mechanisms for mixing tokens are indispensable, as their absence collapses models to near-random behavior, while the exact mathematical form and sequence dependency can be substantially relaxed, especially when preserved in just subset of layers. Surprisingly, even variants that fail in isolation can achieve robust performance when interleaved with standard attention, highlighting cooperative effect. These findings deepen our understanding of what truly underpins attentions effectiveness and open new avenues for simplifying language models without sacrificing performance."
        },
        {
            "title": "Introduction",
            "content": "The remarkable success of Transformer-based language models (Singh, 2025; Liu et al., 2024; Yang et al., 2024a, LMs) is widely attributed to the dotproduct attention mechanism (i.e. standard attention), which enables these models to weight the significance of each token in sequence by computing pairwise similarities of their contextual rep1Code is available at https://github.com/HUIYINXUE/ DeconAttn. resentations (Vaswani et al., 2017). However, this powerful mechanism comes at substantial computational cost with respect to the input sequence length (L). This has led to diverse landscape of proposed mechanisms, including processing longer context (Tay et al., 2022), token-mixing via pooling and multi-layer perceptron MLP-Mixer (Tolstikhin et al., 2021), non-parametric transformations (Yu et al., 2022; Lee-Thorp et al., 2022), optimized kernel functions (Aksenov et al., 2024; Arora et al., 2024; Qin et al., 2022; Peng et al., 2021; Kasai et al., 2021; Choromanski et al., 2021; Katharopoulos et al., 2020), and linear recurrent neural network (RNN) architectures (Siems et al., 2025; Peng et al., 2025; Dao and Gu, 2024; Yang et al., 2024b; Qin et al., 2024; Peng et al., 2024; Poli et al., 2023; Peng et al., 2023; Orvieto et al., 2023). Despite this rich body of work, most of these approaches implicitly preserve several underlying design principles inherited from standard attention. Broadly, these principles include: (1) incorporating mechanisms for mixing information across tokens (Token Mixing), enabling multi-token interactions, (2) emulating the original mathematical form of standard attention (Mathematical Form), i.e. dotproduct similarities followed by softmax weighting, (3) enforcing strict sequence-dependency in activation maps (Sequence-Dependency), where attention weights depend on the specific input sequence, and (4) deriving queries and keys from the current layers hidden states (Current QK), as opposed to other input types such as uncontextualized representations. However, the importance of each of these principles remains largely untested. Are all of these truly essential, or could relaxing some of them suffice if applied selectively? Motivated by this foundational question and guided by Occams Razor (Baker, 2022), we take diagnostic approach: we systematically relax these principles through controlled attention variants, evaluated in two settings: (1) uniform replacement across all layers, and (2) hybrid configurations that interleave standard and simplified modules. Through extensive empirical analysis across multiple model sizes, attention variants, and layer configurations, while carefully matching parameter counts of variants, we uncover set of insights that refine our understanding of key attention principles. Under uniform replacement, mechanisms enabling token mixing prove indispensable: removing them, e.g. in MLP variants, leads to nearrandom accuracy on challenging natural language understanding (NLU) tasks, though such models still capture superficial statistical patterns, as reflected in improved perplexity over trivial baselines. Retaining the dot-product structure and sequencedependent weighting contributes to stability, but these elements are not strictly necessary in every layer, provided token interactions remain strong. Notably, in hybrid configurations that interleave simpler attention mechanisms with standard layers, we uncover striking pattern: attention variants that fail in isolation can nonetheless contribute meaningfully when paired with standard attention, achieving robust performance that often matches or exceeds fully standard models. This suggests standard layers may stabilize activations, mitigate distributional drift, and foster cooperative dynamics across the network, as reflected in both predictive outcomes and structural diagnostics such as attention entropy, head diversity, and sink behaviors. While hybrid attention schemes have been explored in prior work, such as taking advantages of state space models (Glorioso et al., 2024) or augmenting feed-forward modules via mixture-ofexperts routing (Lenz et al., 2025), these are typically driven by performance or efficiency goals. By contrast, our hybrid designs serve as deliberate probes to isolate and examine the causal roles of specific attention properties. Taken together, our findings challenge the assumption that attention mechanisms must adhere rigidly to their original formulation. By identifying which components are essential and which can be simplified, we outline path toward new LM architectures that can be structurally leaner and adaptable."
        },
        {
            "title": "2 Related Work",
            "content": "Prior research attributes the success of Transformer models to their efficient token mixing mechanisms. Consequently, numerous studies explore replacing the standard dot-product attention with simpler architectural components that enable parallel training. For instance, Yu et al. (2022) demonstrate the effectiveness of pooling, MLPs, and convolution as alternatives within vision Transformers. Similarly, Lee-Thorp et al. (2022) highlight the efficiency of token mixers based on Fourier transformation and random projection in the BERT model (Devlin et al., 2019). However, these investigations focus on encoder-only Transformer architectures and may not readily adapt to causal language modeling. While Tolstikhin et al. (2021) to introduce learnable linear layer for token mixing by employing position-wise projection vectors, similar to Linformer (Wang et al., 2020), this approach encounters scalability challenges with long sequences due to its parameter count growing linearly with L. Concurrent research largely retains the standard dot-product attention mechanism as foundational principle. Efforts to reduce the computational cost of this mechanism primarily follow two strategies: weight sharing (Rajabzadeh et al., 2024; Ainslie et al., 2023; Xue and Aletras, 2023; Yan et al., 2021; Kitaev et al., 2020; Shazeer, 2019; Xiao et al., 2019) or input length shrinkage (Nawrot et al., 2023; Clark et al., 2022; Xue and Aletras, 2022). Recent work revisits linear RNNs to handle inputs of varying length (Gu and Dao, 2024; Poli et al., 2023; Peng et al., 2023; Orvieto et al., 2023; Gu et al., 2022). Follow-up research further improves performance by designing more sophisticated gating mechanisms and update rules (He et al., 2025; Lin et al., 2025; Siems et al., 2025; Peng et al., 2025; Dao and Gu, 2024; Yang et al., 2024b; Qin et al., 2024; Peng et al., 2024), with the goal of mimicking human memory, drawing inspiration from the work of Schlag et al. (2021) on fast weight programmers. Notably, such replacements can also be selectively applied to subset of attention layers or heads (Lenz et al., 2025; Ren et al., 2025; Team et al., 2024; Glorioso et al., 2024; Peng and Cao, 2024; Dong et al., 2025; Tay et al., 2019). Additionally, this work operates on the contextual representations encoded by deep networks to generate activation maps dynamically. Another line of research approximates the dotproduct computation to achieve linear complexity. These methods rely on various kernel functions that emulate the exponential function using its Taylor expansion (Aksenov et al., 2024; Arora et al., 2024; Qin et al., 2022; Peng et al., 2021; Kasai et al., 2021; Choromanski et al., 2021; Katharopoulos et al., 2020). This allows for prioritization of the key-value dot product through feature mapping. However, this line of work does not examine the necessity of the other key principles of attention mechanism identified in 1."
        },
        {
            "title": "3 Attention Variants",
            "content": "To operationalize our investigation of the four key design principles identified in 1, we design targeted variations of attention that selectively relax each property. This allows us to probe their necessity in controlled, principled framework."
        },
        {
            "title": "3.1 Standard Dot-product Attention",
            "content": "We take standard scaled dot-product attention (Vaswani et al., 2017) as our baseline, where queries (Q), keys (K), and values (V) are computed from the layer hidden states RLdm: = Att(Q, K, V) = AV (cid:112) (cid:16) = Softmax QK/ dh Q, K, = HWQ,K,V (cid:17) (1) (2) (3) This follows all principles: mixing information across positions via A, using similarity-softmax form, adapting to each input sequence, and tying Q, to the current hidden state H."
        },
        {
            "title": "3.2 Relaxing Token Mixing",
            "content": "MLP. To directly examine the necessity of crosstoken interactions, we replace attention with gated MLP layer, consisting of three fullyconnected (FC) layers (FCDn, FCGt, FCUp) for down-projection, gating and down-projection respectively. This effectively eliminates token mixing and each token is processed independently, only attending to itself. = GatedMLP(H) = FCDn(SiLU(FCGt(H)) FCUp(H)) (4) (5) We use SiLU activation (Elfwing et al., 2018) and match the parameter count of standard attention. This variant serves as minimal baseline to assess how much attentions effectiveness depends on cross-token interaction, beyond what feed-forward paths alone can provide without using any Q, and V."
        },
        {
            "title": "3.3 Relaxing the Mathematical Form",
            "content": "We assess whether attention must strictly follow the canonical dot-product plus softmax formulation. To this end, we evaluate two variants that either approximate or break this form. Approximate. Following Arora et al. (2024), we preserve the mathematical intention of similaritybased weighting, while relaxing the exact form of softmax via second-order Taylor expansion, yielding linear-time recurrent form (Appx. I): (cid:16) (cid:17) Taylor QK/ (cid:112) dh (6) Q, and are computed using Eq. 3. Non-approximate. To contrast this, we introduce new variant that discards explicit pairwise similarity altogether. Instead of computing an attention matrix via QK, it uses element-wise selfgating, multiplying and derived from the same hidden state, and normalizes the result across time steps with softmax: (cid:16) (cid:17) = Softmax (Q K)1/ (cid:112) dh (7) = SiLU HWQ(cid:17) (cid:16) ; K, = HWK,V (8) This variant follows an entirely different mathematical form to standard attention. We expect that this should make it harder for adjacent context tokens to receive large attention scores, as the denominator in the softmax computation monotonically increases (see recurrent form in Appx. I). Notably, the SiLU activation is applied element-wise and does not introduce additional complexity. We apply SiLU activation on projection to add non-linearity. This does not require additional parameters and allows parallelism during training."
        },
        {
            "title": "3.4 Relaxing Sequence Dependency",
            "content": "To test whether attention weights must be dynamically adapted to each input sequence (i.e. sequencedependent), we construct two variants where and are fixed across all inputs, inspired by MLP-Mixer (Fusco et al., 2023; Tolstikhin et al., 2021), but making the parameter count in attention blocks independent of the maximum sequence length. Relaxing sequence dependency allows attention scores for all inputs to be pre-computed and cached during inference. Random-fixed (RndEmbQK). We initialize set of random embeddings ϵ (0, σ2I) that remain constant across inputs. These are passed through the Transformer stack up to layer l: = TransformerBlock(l)(ϵ), ϵ (0, σI) (9) Q, = XWQ,K ; = HWV (10) Since and do not depend on the input, attention maps are fixed and do not adapt to context. Text-fixed (FixedSeqQK). Instead of random embeddings, we use randomly selected fixed sequence of natural language tokens ts (first 2048 tokens from FineWeb-10BT (Lozhkov et al., 2024)). These are embedded and passed through the Transformer to generate X: = TransformerBlock(l)(Emb(ts)) Q, = XWQ,K ; = HWV (11) (12) This setup also produces fixed attention maps, but grounded in natural text instead of completely randomly initialized embeddings. Compared to RndEmbQK, it may encode weak structural priors, such as grammatical patterns or token cooccurrences. These variants allow us to test whether dynamic, input-conditioned attention maps are necessary, or whether fixed maps, paired with learned value paths, are sufficient."
        },
        {
            "title": "3.5 Relaxing the Derivation of Q and K",
            "content": "StaticEmbQK. Finally, to test whether tying Q, to current layer hidden states (H or above) is essential, we compute them directly from static input embeddings corresponding to the input sequence t: Q, = eWQ,K ; = HWV ; = Emb(t) (13) This means that while attention maps are not fixed, they are computed without contextualization from the evolving hidden representations. It further allows attention scores from different layers to be computed in parallel."
        },
        {
            "title": "4.1 Data",
            "content": "We use seven zero-shot NLU tasks in English: ARC-E (Clark et al., 2018), BOOLQ (Clark et al., 2019), COPA (Roemmele et al., 2011), PIQA (Bisk et al., 2020), SCIQ (Welbl et al., 2017), RTE (Wang et al., 2019) and HELLASWAG (Zellers et al., 2019). We also experiment with two LM tasks: WIKITEXT (Merity et al., 2017) and LAMBADA OPENAI (Radford et al., 2019). attention variants, we use multi-head attention (Vaswani et al., 2017), deviating from Qwen2.5s default grouped-query attention (Ainslie et al., 2023). For tokenization, we use the 50K Englishcentric BPE (Sennrich et al., 2016) vocabulary of Pythia (Biderman et al., 2023), offering small memory footprint, and fast training. Model configurations. We pretrain models with approximately 500M parameters, using two configurations: (1) Uniform with simple attention mechanisms across all Transformer layers; (2) Hybrid that integrates simple attention mechanisms in oddnumbered layers and standard attention in evennumbered layers. To assess the contribution of the modified attention variants within the hybrid configuration, we introduce configuration where we remove the odd-numbered layers from pre-trained hybrid models (skip) and evaluate the resulting performance without additional training. We further test these three configurations by training models of 70 million and 160 million parameters (see Appx. A). We finally explore various alternative hybrid configurations such as changing the simple attention replacement ratio, the details of which are presented in 6. Specific model size details are provided in Appx. L. Meanwhile, we strictly constrain all models with different attention variants to have the same number of parameters to eliminate any effects from differences in size. Pre-training. All models are pre-trained on the SlimPajama dataset (Soboleva et al., 2023) for up to 15 billion tokens, following Chinchilla scaling laws (Hoffmann et al., 2022). We use mini-batch size of 500K tokens, aligning with the training budget outlined in Titans (Behrouz et al., 2024). To optimize pre-training efficiency, we use sequence length of 2048 tokens."
        },
        {
            "title": "4.3 Predictive Performance Evaluation",
            "content": "the LM-evaluation-harness We use toolkit v0.4.8 (Gao et al., 2024) for evaluation. We report accuracy for all NLU tasks and perplexity (PPL) for LM tasks. For LAMBADA OPENAI, we report both. 4."
        },
        {
            "title": "4.4 Attention Pattern Indicators",
            "content": "Base model. Our models are built upon Qwen2.5 (Yang et al., 2024a). However, we replace its standard attention mechanism with the alternative attention modules detailed in 3. To ensure strict parameter count match across all Looking at the performance itself may not offer comprehensive picture of the behavior of the dif2Details on hyperparameter selection is provided in Appx. J. For both pre-training and evaluation, we use single AMD Instinct MI300X accelerator. ferent attention mechanisms we test. To obtain more granular understanding of their internal workings, we investigate their attention patterns. We compute eight indicators from the attention matrices Aj RLL for each head = 1, . . . , nh in given layer. We specifically focus on attention sinks, i.e. over-attending to the initial token in sequence, and local patterns within attention matrices, i.e. prioritizing nearby tokens, following prior work (Xiao et al., 2024; Han et al., 2024).3 Entropy (H). Measures the randomness of attention scores. Higher ENTROPY indicates more uniform attention distribution across tokens, similar to mean-pooling: = (cid:80) aA log(a). Concentration (Conc). Measures the concentration of attention. higher Frobenius norm AF indicates attention is focused on limited number of tokens: Conc = AF = (cid:112)(cid:80) aA a2. Head diversity (HeadDiv). Quantifies the variability of attention patterns across different heads. Calculated as the average position-wise standard deviation across heads, higher HEADDIV suggests better use of the multi-head mechanism. HeadDiv ="
        },
        {
            "title": "2\nL(1 + L)",
            "content": "(cid:88) std({A1, . . . , Anh }) Attention sink (Sink). Detects focus on the first token. It is the average attention score assigned by all queries to the initial token. Higher Sink means stronger attention sink: Sink = (cid:80) A:,1/L. Local Focus (LocFocN). Measures the attention focus on nearby tokens. It is the average attention score for tokens at fixed relative distance (here {0, 1, 2, 3}). Higher LocFocN suggests stronger contribution from local context. LocFocN = (cid:88) ALN,LN / (L )"
        },
        {
            "title": "5 Results",
            "content": "Tbl. 1 shows the performance of all model variants (3), employing uniform, hybrid, and skip configurations across NLU and LM tasks. Results illumi3ENTROPY (H), CONC, and HEADDIV are min-max SINK and LOCFOCN use absolute values normalized. (LOCFOCN is scaled by two for visibility). High ENTROPY and low CONC suggest mean-pooling like behavior. High CONC and low ENTROPY indicate focus on few tokens. Further examination of SINK and LOCFOCN clarifies if this focus is on the first token or local tokens. Low ENTROPY and high CONC with low scores elsewhere (except HEADDIV) may point to sparse attention on mid-sequence tokens. nate the role each design principle plays in effective language modeling. Token mixing is crucial. The uniform MLP model, which lacks any cross-token interaction, performs near chance on most NLU tasks, highlighting that token mixing is essential for reasoning and understanding. Despite this, it achieves much lower perplexity on WikiText (993.5 vs. 300K for RndEmbQK ), indicating that even without explicit mixing, MLP can memorize or exploit local token statistics, likely unigram or bigram patterns. Introducing token mixing in hybrid setup substantially improves NLU performance (e.g. 9.2 average accuracy points over uniform MLP), showing that mixing in part of the network can compensate to degree. Still, the hybrid MLP variant has the highest WikiText perplexity among all hybrids, indicating that token mixing across all layers is important for fully modeling long-range dependencies. Standard mathematical form is important in uniform. When applied uniformly, variants that retain the core structure of attention (e.g. Approximate, RndEmbQK, FixedSeqQK and StaticEmbQK ) restore over 92% of the average NLU accuracy of attention. In contrast, Non-approximate, which discards this structure, performs close to random guess (39.3 vs. 39.9 on NLU Avg. accuracy). Approximate achieves the strongest results among uniform variants (8.8 higher PPL on WikiText), suggesting that preserving or closely approximating its mathematical form appears critical for maintaining predictive performance. Sequence-dependency enhances the generalization ability. To assess the role of sequencedependent attention, we compare variants that retain similar architectures but differ in whether attention scores vary across inputs. StaticEmbQK, which preserves Sequence-Dependency, consistently outperforms RndEmbQK and FixedSeqQK, which use fixed attention patterns, particularly on LAMBADA OPENAI by around 2% higher accuracy. This pattern holds across both uniform and hybrid settings. Additionally, hybrid models that preserve sequence-dependency, such as Approximate, StaticEmbQK, and Non-approximate, tend to perform better on global-context benchmarks. These results suggest that input-specific attention contributes to better generalization, even when other attention properties are simplified. Rnd. Guess Majority Standard MLP Approx. Non-apx. RndEmbQK FixedSeqQK StaticEmbQK MLP Approx. Non-apx. RndEmbQK FixedSeqQK StaticEmbQK F I Y I MLP R Approx. Non-apx. RndEmbQK FixedSeqQK StaticEmbQK ARC-E acc BoolQ COPA acc acc 25.00.0 25.70.0 41.51.0 28.50.9 40.71.0 26.80.9 39.51.0 39.41.0 39.61.0 37.51.0 39.91.0 42.31.0 40.11.0 40.51.0 39.21.0 24.40.9 26.60.9 26.60.9 27.40.9 27.20.9 25.50.9 50.00.0 62.20.0 56.60.9 37.80.8 51.50.9 37.80.8 55.30.9 59.00.9 52.90. 49.80.9 51.50.9 56.80.9 48.30.9 58.50.9 54.70.9 41.80.9 46.10.9 39.20.9 37.80.8 39.40.9 43.00.9 50.00.0 56.00.0 63.04.9 54.05.0 64.04.8 60.04.9 57.05.0 61.04.9 63.04.9 60.04.9 67.04.7 63.04.9 61.04.9 64.04.8 64.04.8 54.05.0 59.04.9 52.05.0 58.05.0 59.04.9 57.05. PiQA acc 50.00.0 50.50.0 60.91.1 54.81.2 59.91.1 53.21.2 59.81.1 59.41.1 59.41.1 60.21.1 60.41.1 61.71.1 61.21.1 61.91.1 60.91.1 52.81.2 52.81.2 51.41.2 53.31.2 52.31.2 53.11.2 SciQ acc 25.00.0 25.00.0 60.21.5 25.91.4 55.01.6 19.31.2 46.41.6 51.21.6 49.21.6 54.31.6 60.51.5 63.01.5 60.01.5 62.01.5 58.41.6 19.01.2 20.11.3 20.41.3 21.11.3 22.11.3 22.01.3 RTE acc 50.00.0 52.70.0 53.13. 52.73.0 52.33.0 52.33.0 50.93.0 52.73.0 54.23.0 52.73.0 53.43.0 54.93.0 50.93.0 52.73.0 57.43.0 46.91.7 48.03.0 46.93.0 52.73.0 48.43.0 51.63.0 HellaSwag Avg. Wiki ppl acc acc LAMBADA ppl acc 25.00.0 25.00.0 28.30.4 26.10.4 28.10.4 26.00.4 27.20.4 27.50.4 27.20.4 26.10.4 28.40.4 28.50.5 27.20.4 28.40.4 28.20.4 25.60.4 26.00.4 25.80.4 26.10.4 25.90.4 25.90. 39.9 39.9 51.9 40.0 50.2 39.3 48.0 50.0 49.4 48.7 51.6 52.9 49.8 52.6 51.8 37.8 39.8 37.5 39.5 39.2 39.7 3E+5 - 38.1 993.5 47.9 9E+4 84.8 79.1 79. 45.8 39.4 39.4 39.3 38.5 38.7 2E+5 2E+6 5E+5 2E+4 2E+5 7E+4 3E+6 - 134.1 1E+5 238.6 2E+6 6402.4 19578.1 2287.4 228.7 140.0 133.1 157.5 354.7 140.7 5E+6 1E+7 9E+6 3E+6 5E+6 5E+ 0.00.0 - 22.90.5 0.00.0 18.50.5 0.00.0 1.30.2 1.40.2 3.30.2 20.80.6 23.70.6 23.80.6 22.00.6 20.30.6 23.80.6 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 Table 1: Performance of uniform, hybrid, skip and standard models (500M). Purple (MLP), blue (Approx., Nonapx.), green (RndEmbQK, FixedSeqQK) and yellow (StaticEmbQK) denote variants that relax Token Mixing, Mathematical Form, Sequence-Dependency and Current QK, respectively. Current QK is not as essential as expected. StaticEmbQK relaxes Current QK. Though it does not match the PPL of standard across language modeling tasks, it results in PPL of 79.9 twice as high as 38.1 of standard under uniform configuration on WIKITEXT. It also greatly outperforms MLP, reducing PPL tenfold (from 993.5 on WIKITEXT), while its predictive performance is comparable to standard. Moreover, under hybrid configuration, it achieves predictive performance comparable to standard baseline across all tasks. It indicates Current QK is not as essential for strong predictive performance as initially believed. Layer collaboration matters. All hybrid models where simple attention variants are used in odd layers and standard attention in even layers achieve predictive performance comparable to Standard attention on both NLU and language modeling tasks. Surprisingly, Non-approximate attention, the worst performer in the uniform configuration, demonstrates strong performance in this hybrid setup, slightly surpassing Standard on average NLU accuracy (+1.8%) and LAMBADA OPENAI accuracy (+0.9%), while reducing PPL by 1.0. The hybrid configuration also alleviates the relatively higher uncertainty observed with RndEmbQK and FixedSeqQK, halving their WIKITEXT PPL by incorporating standard layers that aid in grounding attention to individual inputs. These findings suggest that layers exhibiting poor performance in isolation can be effective when combined with stronger layers (i.e. standard attention). Considering the residual connections, which facilitate information flow along shortcut pathway bypassing the simple attention alternatives, we further conduct an ablation study to constrain information flow solely through these residual connections. This involves skipping the non-Standard layers when pre-training hybrid models (denoted as SKIP in Tbl. 1). The results provide further support to the assumption of layer collaboration. All variants in w/ SKIP perform even slightly worse than random guessing (i.e. average accuracy lower than 39.9 on NLU) and further result in PPL explosion in language modeling compared to hybrid by margin. This indicates that the non-Standard layers, despite their simplicity or poor performance in uniform configurations, contribute positively to the overall predictive performance in hybrid architectures."
        },
        {
            "title": "6 Analysis and Discussion",
            "content": "Attention variants. Non-approximate attention that relaxes standard attentions Mathematical Form appears to be the most challenging to train in uniform configuration. Radar plots in Fig. 1 show very low ENTROPY alongside high CONC and HEADDIV, indicating that most heads place formation. Configurations. To illustrate the impact of different configurations, Fig. 1 shows the attention patterns of RndEmbQK and Non-approximate variants as representative methods for studying the behavior of different attention variants in uniform and hybrid configurations (see Fig. 8 for all layers). With uniform RndEmbQK (and uniform Standard ), the top-most layers (e.g. layer 22) exhibit high concentration (low ENTROPY and high CONC). This indicates probability mass predominated by few selective tokens. In the hybrid design, those same layers become less selective (higher ENTROPY, lower CONC), leading to decreased SINK score, suggesting that the hybrid mix alleviates firsttoken sink effects. In the Non-approximate hybrid model, odd layers keep the Non-approximate heads while even layers revert to Standard. clear division of labor emerges: even (Standard ) layers mirror the baseline, balancing token mixing and focus, while odd (Non-approximate) layers specialize, either acting as attention sinks (high SINK, low ENTROPY) or as mean-poolers (high ENTROPY, low CONC). This complementary interplay compensates for the lower expressiveness of Non-approximate heads observed in the uniform setting, explaining why the hybrid configuration trains successfully while the uniform one does not. Why hybrid works. We investigate the magnitude of raw activations (logits before softmax) within each RndEmbQK and Non-approximate layer in the hybrid configuration  (Fig. 2)  . Our analysis reveals that activations generally exhibit lower magnitudes compared to the uniform configuration for both attention variants. Notably, the uniform Non-approximate model shows activation outliers exceeding 103 in the final Transformer layers (e.g. Layer 21). In contrast, the hybrid configuration maintains activations below 101. This suggests that the Standard layers in the hybrid architecture might serve as normalization mechanism. This normalization could mitigate over-concentration and the formation of highly sparse attention matrices, which can arise from large magnitude outliers during the numerically stable softmax operation. This normalizing effect appears sufficiently strong to rescue models that are otherwise challenging to train and prone to gradient vanishing (e.g. Nonapproximate in the uniform configuration). Figure 1: Layer-wise attention indicators for Approx., Non-approx., RndEmbQK, FixedSeqQK and StaticEmbQK in uniform (top) and hybrid (bottom) configurations, and Standard (H: ENTROPY, C: CONC, HD: HEADDIV, LF : LOCFOCN , S: SINK). almost all probability mass on narrow set of mid-sequence tokens. This behavior might stem from its monotonically increasing denominators (seeEq. 18). This could make it progressively harder for later tokens in the sequence to attract attention, thereby hindering effective training in uniform configurations. StaticEmbQK relaxing Current QK coupling, generally presents active token mixing from Layer 7, however, its mid-layers exhibit high similarity. Its reliance on static embeddings for attention computation limits its adaptability to individual layers, further constraining predictive performance. Approximate and FixedSeqQK, showing attention patterns most similar to Standard across all layers. However, the performance of FixedSeqQK generally lags behind Approximate. This due to FixedSeqQK derivation of and matrices from fixed, pre-defined text sequence, which remains constant for all inputs. Consequently, the model might become prone to simulating this specific text sequence, thereby compromising its generalization ability. RndEmbQK attention faces similar issue to FixedSeqQK, but suffers additional marginal performance drops, perhaps due to its inability to encode syntactic inFigure 2: Distribution of pre-softmax activations for RndEmbQK (left) and Non-approximate (right) across two different configurations. See Fig. 6 for all layers. Figure 3: Performance of RndEmbQK and Nonapproximate across nine hybrid configurations. The vertical dotted lines represent the Standard baseline. Theoretical analysis. Li et al. (2024) connects Transformer LMs to spin glass models. They suggest standard attention matrices align with the Gibbs-Boltzmann distribution (Gibbs, 1902), implying an implicit energy minimization process Input-independent and with tokens as spins. or form deviations disrupt this. This perspective provides theoretical basis for the performance variations observed in our uniform replacement experiments. While Zhang et al. (2022) suggests full-rank attention offers maximal flexibility, causal attention can be low-rank due to stable softmax allowing zeros in diagonals with activation outliers. This supports our normalization analysis in hybrid configurations, with Neyshabur et al. (2017)s observation on unbalanced network training difficulty. Model size. We also evaluate all attention variants across models of 70M, 160M, and 500M parameters. Our main observations remain consistent across these different model sizes. See Appx. for detailed results. Hybrid configuration ablation. To investigate the impact of replacing subsets of layers with simpler attention mechanisms, we consider nine different configurations. These focus on different segments of 24-layer architecture of the 500M model: (1) even or 50% configuration, where evennumbered layers retain standard attention while odd-numbered layers are replaced; (2) odd configuration, with the reverse arrangement; (3) top configuration, where the upper layers (13-24) employ the simpler attention mechanism; (4) middle configuration, targeting the middle layers (7-18); (5) bottom configuration, focusing on the initial layers (1-6); (6) 25%, replacing layers except Layer 4,8,12,16,20,24 with simpler attention; (7) first, replacing all layers with simpler attention except the first layer; (8) last, replacing all layers with simpler attention except the last layer; (9) bilateral, replacing all layers with simpler attention except Layer 1 and 24. See Tbl. 11 in Appx. for details. Fig. 3 presents the predictive performance using these nine settings. For both RndEmbQK and Non-approximate mechanisms, the difference in performance across these hybrid configurations is marginal (e.g. all with PPL around 40.0 on WIKITEXT). However, this observation does not generalize to extreme settings, such as employing Standard attention in only the first or the last layer. For RndEmbQK attention, the predictive performance remains comparable to Standard if only the last layer (or layers at both ends) uses Standard. Nevertheless, its accuracy on LAMBADA OPENAI drops to zero in such extreme cases. For Non-approximate attention, using Standard attention mechanism only in the last layer greatly harms performance, leading to PPL exceeding 400 on WIKITEXT. This indicates that the normalization strength provided by single Standard layer is limited. Therefore, in extreme hybrid settings where we can afford only one or two Standard layers, we should choose substitute that still respects the main design principles presented in the uniform setting (i.e. stronger lightweight attention). Conversely, if the compute budget allows using even small fraction of Standard transformer layers (e.g. 25%), we can safely replace the remainder with much simpler mechanism and still maintain competitive accuracy."
        },
        {
            "title": "7 Conclusion",
            "content": "We systematically relax core design principles in controlled setting, offering the first principled framework for assessing which aspects of attention are truly foundational and which can be safely simplified in language modeling. Our findings reveal that adhering to standard attention design principles varies between uniform and hybrid architectures. Token mixing and following the mathematical form are crucial for attention alternatives when applied uniformly, but not necessary for hybrid. Strategically integrating few standard attention layers within LMs can greatly improve, even overcome, limitations of less powerful attention mechanisms. This is likely due to the inherent normalization of standard attention, fostering training stability."
        },
        {
            "title": "Limitations",
            "content": "We performed experiments using maximum model size of 500M parameters and pretraining budget of 15B tokens, using monolingual tokenizer and vocabulary, similar to Allal et al. (2025); Poli et al. (2023). While experimenting with larger models and different model families presents interesting avenues for future work, we believe that the current scope sufficiently supports our conclusions regarding the relative effectiveness of different attention designs."
        },
        {
            "title": "Acknowledgments",
            "content": "This projected made use of time on UK Tier2 HPC facility JADE@ARC, funded by EPSRC (EP/T022205/1). We would like to thank Miles Williams and Atsuki Yamaguchi for their invaluable feedback."
        },
        {
            "title": "References",
            "content": "Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. 2023. GQA: Training generalized multi-query transformer models from multi-head checkpoints. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 4895 4901, Singapore. Association for Computational Linguistics. Yaroslav Aksenov, Nikita Balagansky, Sofia Lo Cicero Vaina, Boris Shaposhnikov, Alexey Gorbatovski, and Daniil Gavrilov. 2024. Linear transformers with learnable kernel functions are better in-context models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 95849597, Bangkok, Thailand. Association for Computational Linguistics. Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Guilherme Penedo, Lewis Tunstall, Andrés Marafioti, Hynek Kydlícek, Agustín Piqueres Lajarín, Vaibhav Srivastav, and 1 others. 2025. Smollm2: When smol goes big-datacentric training of small language model. CoRR. Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, James Zou, Atri Rudra, and Christopher Re. 2024. Simple linear attention language models balance the recall-throughput tradeoff. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 17631840. PMLR. Alan Baker. 2022. Simplicity. In Edward N. Zalta, editor, The Stanford Encyclopedia of Philosophy, Summer 2022 edition. Metaphysics Research Lab, Stanford University. Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. 2024. Titans: Learning to memorize at test time. arXiv preprint arXiv:2501.00663. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, and 1 others. 2023. Pythia: suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 23972430. PMLR. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, and 1 others. 2020. PIQA: Reasoning about Physical Commonsense in Natural Language. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 74327439. Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy Colwell, and Adrian Weller. 2021. In InternaRethinking attention with performers. tional Conference on Learning Representations. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 29242936, Minneapolis, Minnesota. Association for Computational Linguistics. Jonathan H. Clark, Dan Garrette, Iulia Turc, and John Wieting. 2022. Canine: Pre-training an efficient tokenization-free encoder for language representation. Transactions of the Association for Computational Linguistics, 10:7391. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? Try arc, the AI2 reasoning challenge. arXiv preprint arXiv:1803.05457. Tri Dao and Albert Gu. 2024. Transformers are SSMs: generalized models and efficient algorithms through structured state space duality. In Proceedings of the 41st International Conference on Machine Learning, pages 1004110071. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, Minnesota. Association for Computational Linguistics. Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, ZIJIA CHEN, Ameya Sunil Mahabaleshwarkar, ShihYang Liu, Matthijs Van keirsbilck, Min-Hung Chen, Yoshi Suhara, Yingyan Celine Lin, Jan Kautz, and Pavlo Molchanov. 2025. Hymba: hybrid-head architecture for small language models. In The Thirteenth International Conference on Learning Representations. Stefan Elfwing, Eiji Uchibe, and Kenji Doya. 2018. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural networks, 107:311. Francesco Fusco, Damian Pascual, Peter Staar, and Diego Antognini. 2023. pNLP-mixer: an efficient all-MLP architecture for language. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track), pages 5360, Toronto, Canada. Association for Computational Linguistics. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, and 5 others. 2024. framework for few-shot language model evaluation. Josiah Willard Gibbs. 1902. Elementary principles in statistical mechanics: Developed with especial reference to the rational foundations of thermodynamics. C. Scribners sons. Paolo Glorioso, Quentin Anthony, Yury Tokpanov, James Whittington, Jonathan Pilault, Adam Ibrahim, Zamba: comand Beren Millidge. 2024. arXiv preprint pact 7B SSM hybrid model. arXiv:2405.16712. Albert Gu and Tri Dao. 2024. Mamba: Linear-time sequence modeling with selective state spaces. In First Conference on Language Modeling. Albert Gu, Karan Goel, and Christopher Re. 2022. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations. Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. 2024. LMinfinite: Zero-shot extreme length generalization for large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 39914008, Mexico City, Mexico. Association for Computational Linguistics. Zhihao He, Hang Yu, Zi Gong, Shizhan Liu, Jianguo Li, and Weiyao Lin. 2025. Rodimus*: Breaking the accuracy-efficiency trade-off with efficient attentions. In The Thirteenth International Conference on Learning Representations. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, and 3 others. 2022. Training compute-optimal large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA. Curran Associates Inc. Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, and Noah A. Smith. 2021. Finetuning preIn Proceedings trained transformers into RNNs. of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1063010643, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. 2020. Transformers are RNNs: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pages 51565165. PMLR. Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient transformer. In International Conference on Learning Representations. Vijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. 2023. Reducing activation recomputation in large transformer models. Proceedings of Machine Learning and Systems, 5:341353. James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. 2022. FNet: Mixing tokens with Fourier transforms. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 42964313, Seattle, United States. Association for Computational Linguistics. Barak Lenz, Opher Lieber, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, Daniel Gissin, Daniel Jannai, Dor Muhlgay, Dor Zimberg, Edden M. Gerber, Elad Dolev, Eran Krakovsky, Erez Safahi, Erez Schwartz, and 42 others. 2025. Jamba: Hybrid Transformer-Mamba language models. In The Thirteenth International Conference on Learning Representations. He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartłomiej Koptyra, and 13 others. 2023. RWKV: Reinventing RNNs for the transformer era. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 14048 14077, Singapore. Association for Computational Linguistics. Yuhao Li, Ruoran Bai, and Haiping Huang. 2024. Spin glass model of in-context learning. arXiv preprint arXiv:2408.02288. Zhixuan Lin, Evgenii Nikishin, Xu He, and Aaron Courville. 2025. Forgetting transformer: Softmax attention with forget gate. In The Thirteenth International Conference on Learning Representations. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, and 1 others. 2024. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437. Anton Lozhkov, Loubna Ben Allal, Leandro von Werra, and Thomas Wolf. 2024. FineWeb-Edu: The finest collection of educational content . Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017. Pointer sentinel mixture models. In International Conference on Learning Representations. Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, and 1 others. 2021. Efficient large-scale language model training on gpu clusters using megatron-lm. In Proceedings of the international conference for high performance computing, networking, storage and analysis, pages 115. Piotr Nawrot, Jan Chorowski, Adrian Lancucki, and Edoardo Maria Ponti. 2023. Efficient transformers with dynamic token pooling. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 64036417, Toronto, Canada. Association for Computational Linguistics. Bo Peng, Daniel Goldstein, Quentin Gregory Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Kranthi Kiran GV, Haowen Hou, Satyapriya Krishna, Ronald McClelland Jr., Niklas Muennighoff, Fares Obeid, Atsushi Saito, Guangyu Song, Haoqin Tu, Ruichong Zhang, Bingchen Zhao, and 3 others. 2024. Eagle and Finch: RWKV with matrix-valued states and dynamic recurrence. In First Conference on Language Modeling. Bo Peng, Ruichong Zhang, Daniel Goldstein, Eric Alcaide, Haowen Hou, Janna Lu, William Merrill, Guangyu Song, Kaifeng Tan, Saiteja Utpala, and 1 others. 2025. RWKV-7\" Goose\" with expressive dynamic state evolution. arXiv preprint arXiv:2503.14456. Dazhi Peng and Hangrui Cao. 2024. E-Tamba: Efficient Transformer-Mamba layer transplantation. In NeurIPS 2024 Workshop on Fine-Tuning in Modern Machine Learning: Principles and Scalability. Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. 2021. Random feature attention. In International Conference on Learning Representations. Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Ré. 2023. Hyena hierarchy: Towards larger convolutional language models. In International Conference on Machine Learning, pages 2804328078. PMLR. Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. 2022. cosFormer: Rethinking Softmax In Attention. In International Conference on Learning Representations. Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, and Nathan Srebro. 2017. Geometry of optimization and implicit regularization in deep learning. arXiv preprint arXiv:1705.03071. Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. 2024. HGRN2: Gated linear RNNs with state expansion. In First Conference on Language Modeling. Antonio Orvieto, Samuel Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. 2023. Resurrecting recurrent neural netIn International Conworks for long sequences. ference on Machine Learning, pages 2667026698. PMLR. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Gv, Xuzheng Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Hossein Rajabzadeh, Aref Jafari, Aman Sharma, Benyamin Jami, Hyock Ju Hj Kwon, Ali Ghodsi, Boxing Chen, and Mehdi Rezagholizadeh. 2024. Echoatt: Attend, copy, then adjust for more efficient large language models. In NeurIPS Efficient Natural Language and Speech Processing Workshop, pages 259269. PMLR. Weiming Ren, Wentao Ma, Huan Yang, Cong Wei, Ge Zhang, and Wenhu Chen. 2025. Vamba: Understanding hour-long videos with hybrid mambatransformers. arXiv preprint arXiv:2503.11579. Melissa Roemmele, Cosmin Adrian Bejan, and Andrew Gordon. 2011. Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning. In AAAI spring symposium: logical formalizations of commonsense reasoning, pages 9095. Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber. 2021. Linear transformers are secretly fast weight programmers. In International conference on machine learning, pages 93559366. PMLR. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 17151725, Berlin, Germany. Association for Computational Linguistics. Noam Shazeer. 2019. One write-head is all you need. arXiv:1911.02150. Fast transformer decoding: arXiv preprint Julien Siems, Timur Carstensen, Arber Zela, Frank Hutter, Massimiliano Pontil, and Riccardo Grazzi. 2025. DeltaProduct: Improving state-tracking in linear RNNs via Householder products. arXiv preprint arXiv:2502.10297. Ajit Singh. 2025. Meta Llama 4: The future of multimodal AI. Available at SSRN 5208228. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, and 1 others. 2022. Using deepspeed and megatron to train megatron-turing nlg 530b, large-scale generative language model. arXiv preprint arXiv:2201.11990. Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob Steeves, Joel Hestness, and Nolan Dey. 2023. SlimPajama: 627B token cleaned and deduplicated version of RedPajama. 2024. Jamba-1.5: Hybrid transformer-mamba models at scale. arXiv preprint arXiv:2408.12570. Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, and 1 others. 2021. MLP-Mixer: An allmlp architecture for vision. Advances in neural information processing systems, 34:2426124272. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue: stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems, 32. Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768. Johannes Welbl, Nelson F. Liu, and Matt Gardner. 2017. Crowdsourcing multiple choice science questions. In Proceedings of the 3rd Workshop on Noisy Usergenerated Text, pages 94106, Copenhagen, Denmark. Association for Computational Linguistics. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2024. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations. Tong Xiao, Yinqiao Li, Jingbo Zhu, Zhengtao Yu, and Tongran Liu. 2019. Sharing attention weights for fast transformer. arXiv preprint arXiv:1906.11024. Huiyin Xue and Nikolaos Aletras. 2022. HashFormers: Towards vocabulary-independent pre-trained transformers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 78627874, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2022. Efficient transformers: survey. ACM Computing Surveys, 55(6):128. Yi Tay, Aston Zhang, Anh Tuan Luu, Jinfeng Rao, Shuai Zhang, Shuohang Wang, Jie Fu, and Siu Cheung Hui. 2019. Lightweight and efficient neural natural language processing with quaternion networks. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1494 1503, Florence, Italy. Association for Computational Linguistics. Huiyin Xue and Nikolaos Aletras. 2023. Pit one against many: Leveraging attention-head embeddings for In Findparameter-efficient multi-head attention. ings of the Association for Computational Linguistics: EMNLP 2023, pages 1035510373, Singapore. Association for Computational Linguistics. Yu Yan, Jiusheng Chen, Weizhen Qi, Nikhil Bhendawade, Yeyun Gong, Nan Duan, and Ruofei Zhang. 2021. El-attention: Memory efficient lossless attention for generation. In International Conference on Machine Learning, pages 1164811658. PMLR. Jamba Team, Barak Lenz, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, and 1 others. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. 2025. Qwen3 technical report. arXiv:2505.09388. arXiv preprint An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, and 1 others. 2024a. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. 2024b. Parallelizing linear transformers with the delta rule over sequence length. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and Shuicheng Yan. 2022. Metaformer is actually what you need for vision. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1081910829. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 47914800, Florence, Italy. Association for Computational Linguistics. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. 2022. Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations."
        },
        {
            "title": "Sizes",
            "content": "To assess the impact of model size, we evaluate all attention mechanisms across models with approximately 70M, 160M, and 500M parameters. Fig. 4 illustrates the predictive performance of these models on the WIKITEXT, ARC-E, and SCIQ datasets. Our results indicate that the predictive performance of LMs with hybrid configuration consistently improves with increasing model size. For instance, the accuracy of the Non-approximate method on ARC-E improves from 34.3 to 42.3 when increasing the model size from 70M to 500M. Furthermore, all attention mechanisms incorporating token mixing achieve predictive performance comparable to same-sized model employing standard attention (indicated by the vertical dotted lines in Fig. 4). For RndEmbQK, such performance gap on WIKITEXT PPL is even within 1.2 across all sizes. This trend suggests that our observations may generalize to larger models. To further investigate the immediate generalizability of our findings, we further pretrain larger model (Yang et al., 2025, Qwen3-1.7b-Base) from scratch on 45 billion tokens with Standard and Figure 4: Predictive performance of 70M parameters (small dots), 160M parameters (medium dots), and 500M parameters (large dots) models with different attention mechanisms and configurations on WIKITEXT, ARC-E, and SCIQ. our proposed RndEmbQK and Non-approximate variants in both uniform and hybrid configurations. Tbl. 2 presents their performance on NLU and LM tasks. We find both RndEmbQK and Nonapx. under hybrid configuration, achieve performance comparable to Standard across all downstream tasks, which is consistent to our observation on models with modest scales. However, different to the model with 500M parameters, Nonapproximate under uniform configuration successfully converges. This is because Qwen3 incorporates RSMNorm above the queries and key in its attention module. This normalization helps to alleviate the potential for pre-softmax attention activations to explode, but it is less effective than using several standard layers, as it restricts the length of query and key vectors, narrowing the adaptable range for raw pre-softmax activations. Grouped-query Attention Ablation To confirm the generality of our main investigations, we also trained 500M parameter versions of the Standard, Non-approximate, and RndEmbQK models using the grouped-query attention configuration. These models are trained on the same 15 billion tokens, with precisely matched parameter counts. We observe that the results on downstream tasks remain consistent across both the multi-head attention and grouped-query attention configurations. Their performance on both NLU and LM tasks is detailed in Tbl. 3."
        },
        {
            "title": "C Robustness to Context Length",
            "content": "Tbl. 4 illustrates the perplexity scores of the UNIFORM, HYBRID and tandard models on WIKITEXT dataset. These models were evaluated across various contextual lengths (128, 256, 512, 1024, and ARC-E acc BoolQ COPA acc acc 25.00.0 25.70.0 44.61.0 41.01.0 44.41.0 45.01.0 45.41. 50.00.0 62.20.0 56.40.9 61.90.9 50.20.9 58.10.9 57.00.9 50.00.0 56.00.0 64.04.8 58.05.0 60.04.9 65.04.8 67.04. PiQA acc 50.00.0 50.50.0 64.01.1 59.51.2 62.41.1 63.41.1 64.51.1 SciQ acc 25.00.0 25.00.0 67.31. 56.91.6 56.31.6 66.21.5 65.51.5 RTE acc 50.00.0 52.70.0 52.73.0 52.43.0 54.93.0 53.13.0 55.23. Rnd. Guess Majority Standard . Non-apx. U"
        },
        {
            "title": "RndEmbQK",
            "content": ". Non-apx. H"
        },
        {
            "title": "RndEmbQK",
            "content": "HellaSwag Avg. Wiki ppl acc acc 25.00.0 25.00.0 30.50.5 27.80.5 28.60.5 30.20.5 30.40. 39.9 39.9 54.2 51.1 51.0 54.4 55.0 3E+5 - 27.6 67.3 54.9 29.9 28. LAMBADA ppl acc 3E+6 - 60.0 619.6 1872.8 0.00.0 - 28.90.6 8.10.4 3.80. 77.4 61.6 26.80.6 29.80.6 Table 2: Performance of uniform, hybrid and standard models (1.7B). Blue (Non-apx.) and green (RndEmbQK) denote variants that relax Mathematical Form, and Sequence-Dependency, respectively. ARC-E acc BoolQ COPA acc acc 25.00.0 25.70.0 39.41. 26.80.9 37.91.0 40.71.0 40.11.0 50.00.0 62.20.0 49.60.9 37.80.8 53.20.9 44.60.9 45.80.9 50.00.0 56.00.0 60.05.0 52.05.0 56.05.0 67.05.0 63.04.9 PiQA acc 50.00.0 50.50.0 62.21.1 52.01.2 58.31.2 61.31.1 61.31.1 SciQ acc 25.00.0 25.00.0 59.31.6 20.31.3 46.71.6 61.51.5 61.81.5 RTE acc 50.00.0 52.70.0 51.63.0 52.73.0 52.73.0 52.43.0 52.73.0 HellaSwag Avg. Wiki ppl acc acc LAMBADA ppl acc 25.00.0 25.00.0 28.10.5 25.90.4 27.20.4 28.30.5 28.30.5 39.9 39.9 50.0 38.2 47.4 50.8 50.4 3E+5 - 38. 5466.8 84.6 38.1 39.3 3E+6 - 154.0 2E+6 6462.7 133.1 138.6 0.00.0 - 22.90.6 0.00.0 12.40.2 23.40.6 23.60.6 Rnd. Guess Majority Standard"
        },
        {
            "title": "RndEmbQK",
            "content": "I . Non-apx. . Non-apx. H"
        },
        {
            "title": "RndEmbQK",
            "content": "Table 3: Performance of uniform, hybrid and standard models (500m) using grouped-query attention. Blue (Non-apx.) and green (RndEmbQK) denote variants that relax Mathematical Form, and Sequence-Dependency, respectively. PPL length Standard MLP Approx. Non-apx. RndEmbQK FixedSeqQK StaticEmbQK MLP Approx. Non-apx. RndEmbQK FixedSeqQK StaticEmbQK F U R 128 69.9 993.5 81.7 10023.7 107.1 100.5 104.8 81.3 71.8 69.0 72.4 69.0 70. 256 56.2 993.5 66.4 9476.8 95.7 89.6 92.2 66.4 57.8 56.0 58.1 56.0 56.6 512 47. 993.5 57.2 9173.5 89.6 83.6 85.5 57.0 49.3 48.0 49.4 48.0 48.4 1024 42.0 993.5 51.2 9064.8 86.3 80.6 81.7 50.3 43.4 42.5 43.4 42.3 42. 2048 38.1 993.5 47.9 9025.9 84.8 79.1 79.9 45.8 39.4 39.4 39.3 38.5 38.7 Table 4: Perplexities of uniform, hybrid and standard models (500M) on WIKITEXT across different context lengths. 2048 tokens), all while being trained on maximum sequence length of 2048 tokens. The results clearly show that models incorporating token mixing achieve lower perplexity scores with longer contexts. This indicates their ability to capture more contextual information for predicting the next token. Furthermore, under the hybrid configuration, the perplexity scores for the RndEmbQK, FixedSeqQK, StaticEmbQK, Approximate and Non-approximate attention mechanisms consistently match those of the standard model on WIKITEXT, regardless of contextual length."
        },
        {
            "title": "Attentions",
            "content": "Unlike previous work that primarily focused on reducing computational time complexity to subquadratic with respect to contextual sequence length, we define simpler attention more broadly. This encompasses mechanisms that reduce time complexity concerning any factor: inference batch size, sequence length, or hidden dimension. Below, we systematically summarize the characteristics of the different simpler attention mechanisms we investigated. RndEmbQK and FixedSeqQK. These mechanisms create global static attention graphs during inference. This approach reduces the computational time complexity and cache size within attention while enabling batched decoding (see Appx. and H). StaticEmbQK. Inspired by cross-layer attention sharing (Rajabzadeh et al., 2024; Xiao et al., 2019), this mechanism primarily captures semantic similarities between input tokens without contextualization. It establishes an upper bound for broadcasting attention matrices from initial layers to all subsequent layers by aligning its parameter count with standard attention. While StaticEmbQK attention does not explicitly reduce computational time complexity, it allows for system optimization by computing attention scores asynchronously. This enables scores to be prefetched before sequentially retrieving output hidden states from each layer. Approximate and Non-approximate. These attention mechanisms result in time complexities linear to sequence length. Their recurrent forms are detailed in Appx. I. Non-approximate can further reduce the activation memory, cache size, and floating-point operations per iteration (FLOPs/it) required for large LMs during the decode stage, offering advantages over Approximate. The details for these reductions are provided in Appendices G, H, and F, respectively."
        },
        {
            "title": "Computation",
            "content": "Tbl. 5 details the computational time complexity for single forward pass, explicitly excluding any caching mechanisms. For RndEmbQK and FixedSeqQK attention, which employ global attention scores, the floating-operations could be further reduced to through pre-computation and subsequent caching of these scores (see Appx. F). This optimization would free up computational resources, enabling further software-level enhancements such as coordinating CPUs and GPUs to pre-fetch the pre-calculated attention scores. While StaticEmbQK does not inherently offer lower computational time complexity, it provides an upper bound for pre-computing attention scores on static embeddings by aligning the If attention scores on number of parameters. static embeddings are pre-computed, the computational time complexity would be reduced by (cid:0)(l 1) (BL2d + BLd2)(cid:1) in total, where represents the total number of Transformer layers. Furthermore, an attention mechanism that supports pre-computation offers the potential to proactively evict values, which could lead to further reductions in computation, particularly if the attention matrices exhibit sparsity. Floating-point Operations per Token"
        },
        {
            "title": "Attention",
            "content": "Complexity O(.) Standard MLP Approx. Non-apx. RndEmbQK FixedSeqQK StaticEmbQK BL2d + BLd2 BLd2 BLd2 BLd2 BL2d + BLd2 BL2d + BLd2 BL2d + BLd2 Table 5: Details of time complexities for each attention across all attention variants, where denotes the number of attention heads, denotes the batch size, denotes the input sequence length, denotes the hidden dimension. We assume = dh, where is the number of attention heads and dh is the dimension of each attention head. We also ignore those low-order terms for element-wise activations and scaling factors with O(BLd) complexity. Attention Standard MLP Approx. Non-apx. RndEmbQK FixedSeqQK StaticEmbQK Prefill Decode 4BL2d + 6BLd2 6BLd2 14BLd2 6BLd2 2L2d + 2BL2d + 6BLd2 2L2d + 2BL2d + 6BLd2 4BL2d + 6BLd2 6Bd2 + 4BLd 6Bd2 10Bd2 6Bd2 2Ld + 2BLd + 6Bd2 2Ld + 2BLd + 6Bd2 6Bd2 + 4BLd Table 6: Details of floating-point operations per iteration for each attention across all attention variants, where denotes the number of attention heads, denotes the batch size, denotes the input sequence length, denotes the hidden dimension. We assume = dh, where is the number of attention heads and dh is the dimension of each attention head. Non-approximate achieves low FLOP/it, equivalent to that of the simplest MLP model, because it leverages vectors instead of the matrices employed by the Approximate method for state tracking. This structural difference significantly reduces the number of GEMMs required. Tbl. 6 details the floating-point operations per iteration (FLOP/it) for inference with the cache enabled. We focus solely on General Matrix Multiplications (GEMMs) (Narayanan et al., 2021, GEMMs), as they are the dominant contributors to the total floating-point operations. Furthermore, if RndEmbQK and FixedSeqQK are allowed to use pre-computed global attention scores, their FLOP/it can be further reduced. During the prefill stage, the operations drop to 2L2d + 2BLd2 and 2BLd + 2d2 during prefill and decode stage respectively."
        },
        {
            "title": "MLP",
            "content": "Approx. Non-apx."
        },
        {
            "title": "StaticEmbQK",
            "content": "8BLd+2BL2h 8BLd + 3Bd2 ht 11BLd 8BLd+4BLh 4BLd+8Ld+2L2h 4BLd+8Ld+2L2h 8BLd+2BL2h Table 7: Details of activation memory for each attention across all attention variants, where denotes the number of attention heads, denotes the batch size, denotes the input sequence length, denotes the hidden dimension, denotes the tensor parallel size. We assume = dh, where is the number of attention heads and dh is the dimension of each attention head. We ignore the attention dropout here."
        },
        {
            "title": "Attention Computation",
            "content": "We detail the activation memory required for halfprecision training in Tbl. 7. Unlike the full recomputation method mentioned in Smith et al. (2022), our approach incorporates sequence parallelism following Korthikanti et al. (2023). We find that RndEmbQk and FixedSqeQK are effective at reducing activation memory, particularly when using substantially large batch size. Furthermore, both Approximate and Non-approximate enhance memory efficiency for long-context processing. Nonapproximate offers superior reduction in activation memory compared to Approximate, especially for large LMs characterized by relatively large hidden state dimension."
        },
        {
            "title": "H Cache Size Required for Inference",
            "content": "Tbl. 8 presents the cache size required for halfprecision inference. Both the Approximate and Non-approximate variants allow the cache size to be independent of the context sequence length. Meanwhile, RndEmbQk and FixedSeqQK can reduce the cache size by nearly half by sharing the"
        },
        {
            "title": "Cache Size for Inference",
            "content": "Standard MLP Approx. Non-apx. RndEmbQK FixedSeqQK StaticEmbQK 4BLd 0 6Bd + 4Bd2/h 2Bd + 4Bh 2(B + 1)Ld 2(B + 1)Ld 4BLd Table 8: Details of cache size (in bytes) per layer across all attention variants required during inference, where denotes the number of attention heads, denotes the batch size, denotes the context length, denotes the hidden dimension. We assume = dh, where is the number of attention heads and dh is the dimension of each attention head. same set of keys within the same batch, provided the batch size is sufficiently large. It is also important to note that RndEmbQK and FixedSeqQk enable cache size further optimized to (2L + δ)δ. This can be achieved by using dynamic cache and prefetching the attention scores for the next δ steps into buffer, given that the attention matrices are independent of the inputs."
        },
        {
            "title": "I Recurrent Form of Linear Attentions",
            "content": "The recurrent form of the Approximate attention computation, derived from Eq. 6, is presented in Eq. 17. Similarly, Eq. 18 shows the recurrent form of the Non-approximate attention computation, originating from Eq. 7. As detailed in Tbl. 5, the Approximate attention mechanism necessitates the computation of recursions for both first-order and second-order terms in the Taylor expansion, resulting in higher time complexity compared to the Non-approximate approach. key characteristic of Oi in Eq. 18 is that its denominator strictly increases with the index i. Notably, as grows along the sequence, the attention score for the ith token, given by eqik qj vi +eqik (cid:80)i1 j=1 more challenging to increase. , becomes progressively"
        },
        {
            "title": "Hyperparameters in Pretraining",
            "content": "Maximum train steps Batch size (in total) Adam epsilon Adam β1 Adam β2 Sequence length Peak learning rate Learning rate schedule Number of cycles in scheduler Warmup steps Weight decay Max gradient norm clip value 120000 256 instances 1e-8 0.9 0.9999 2048 4e-4 (3e-4 for Qwen3-1.7B) CosineLRScheduler 0.5 2000 (1B tokens) 0.1 1.0 Table 9: Details of hyperparameters used in pre-training. oi = o0i + o1i + o2i o0i = qi o1i = (cid:80)i1 j=1 vj + vi (cid:16)(cid:80)i1 j=1 (cid:16)(cid:80)i1 (cid:17) vj + vi (cid:17) qi (cid:18) + j=1 k2 2 (cid:80)i1 j=1( )vj + ( k2 2 q2 2 (cid:18) q2 2 (cid:80)i1 j=1( k2 2 ) + ( k2 2 (cid:19) )vi (cid:19) ) o2i = oi = (cid:80)i1 j=1 eqj (cid:80)i1 j=1 eqj j vj + eqik + eqik vi (14) (15) (16) (17) (18) (a) Training loss across models with 70M parameters (b) Training loss across models with 160M parameters"
        },
        {
            "title": "J Hyperparameters",
            "content": "The hyperparameters used in pre-training are listed in Tbl. 9."
        },
        {
            "title": "Mechanisms",
            "content": "Fig. 5 presents the loss curves across all model variants and sizes, while training for 15B tokens."
        },
        {
            "title": "Sizes",
            "content": "Tbl. 10 presents the detailed configurations of models across various sizes (70M, 160M, 500M and 1.7B)."
        },
        {
            "title": "M Distribution of Raw Logits",
            "content": "Fig. 6 (the full version of Fig. 2) exhibits the magnitude of pre-softmax activations within each 24layer (500M) RndEmbQK and Non-approximate layer in the hybrid configuration. (c) Training loss across models with 500M parameters Figure 5: Training loss across all model variants with three different sizes."
        },
        {
            "title": "Layers across Attention Variants",
            "content": "Fig. 7, the full version of the left subfigure in Fig. 1), exhibits attention characterstics from all 24 layers across Standard attention and five attention Figure 6: Distribution of raw logits in the pre-softmax activations for RndEmbQK (left) and Non-approximate (right) attention mechanisms in both uniform and hybrid configurations."
        },
        {
            "title": "Model Size",
            "content": "70M 160M 500M 1.7B"
        },
        {
            "title": "Hidden Size\nIntermediate Size\nNum of Hidden Layers\nMax Window Layers\nNum of Attention Heads\nNum of Key Value Heads",
            "content": "512 2048 6 6 8 8 768 3072 12 12 12 12 896 4864 24 24 14 14 2048 6144 28 28 16 16 Table 10: Details of model configurations for different sizes."
        },
        {
            "title": "Config",
            "content": "even (50%) odd top middle bottom 25% first last bilteral"
        },
        {
            "title": "Standard Layer IDs",
            "content": "{2,4,6,8,10,12,14,16,18,20,22,24} {1,3,5,7,9,11,13,15,17,19,21,23} {1,2,3,4,5,6,7,8,9,10,11,12} {1,2,3,4,5,6,19,20,21,22,23,24} {13,14,15,16,17,18,19,20,21,22,23,24} {4,8,12,16,20,24} {1} {24} {1,24} Table 11: Details of model configurations for ablation study. variants - RndEmbQK, FixedSeqQK, StaticEmbQK, Approximate and Non-approximate."
        },
        {
            "title": "O Attention Characteristics from All\nLayers across Configurations",
            "content": "Fig. 8, the full version of the right subfigure in Fig. 1, exhibits attention characterstics from all 24 layers across Standard and two representative attention variants - Approximate and Non-approximate in both uniform and hybrid configurations."
        },
        {
            "title": "Study",
            "content": "Tbl. 11 details nine distinct hybrid architectures, as discussed in 6, for 24-layer model variants with approximately 500 million parameters. Figure 7: Visualization of attention matrix characteristics across different layers for Approximate, Non-approximate, RndEmbQK, FixedSeqQK and StaticEmbQK, and their hybrid variants, compared to Standard (H: ENTROPY, C: CONC, HD: HEADDIV, LF : LOCFOCN , S: SINK). Figure 8: Visualization of attention matrix characteristics across different layers for Non-approximate and RndEmbQK, and their hybrid variants, compared to Standard (H: ENTROPY, C: CONC, HD: HEADDIV, LF : LOCFOCN , S: SINK)."
        }
    ],
    "affiliations": [
        "School of Computer Science, University of Sheffield, United Kingdom"
    ]
}