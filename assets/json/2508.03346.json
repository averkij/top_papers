{
    "paper_title": "Compressing Chain-of-Thought in LLMs via Step Entropy",
    "authors": [
        "Zeju Li",
        "Jianyuan Zhong",
        "Ziyang Zheng",
        "Xiangyu Wen",
        "Zhijian Xu",
        "Yingying Cheng",
        "Fan Zhang",
        "Qiang Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) using Chain-of-Thought (CoT) prompting excel at complex reasoning but generate verbose thought processes with considerable redundancy, leading to increased inference costs and reduced efficiency. We introduce a novel CoT compression framework based on step entropy, a metric that quantifies the informational contribution of individual reasoning steps to identify redundancy. Through theoretical analysis and extensive empirical validation on mathematical reasoning benchmarks, we demonstrate that steps with low entropy are indeed highly redundant. Our experiments reveal that an astonishing 80\\% of low-entropy intermediate steps can be pruned with minor degradation in the final answer accuracy across DeepSeek-R1-7B, 14B and Qwen3-8B. This finding sharply contrasts with random or high-entropy pruning, which severely impairs reasoning performance. Building on this, we propose a novel two-stage training strategy combining Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO) reinforcement learning. This approach enables LLMs to autonomously learn to generate compressed COTs during inference by strategically incorporating [SKIP] tokens. Our method significantly enhances LLM inference efficiency while rigorously preserving accuracy, offering profound implications for practical LLM deployment and a deeper understanding of reasoning structures."
        },
        {
            "title": "Start",
            "content": "Compressing Chain-of-Thought in LLMs via Step Entropy Zeju Li1, Jianyuan Zhong1, Ziyang Zheng1, Xiangyu Wen1, Zhijian Xu1, Yingying Cheng2, Fan Zhang2, Qiang Xu1 * 1The Chinese University of Hong Kong, 2Huawei Technologies Co., Ltd {zjli24, jyzhong, zyzheng23, xywen22, zjxu21, qxu}@cse.cuhk.edu.hk, {cheng.yingying1, zhang.fan2}@huawei.com 5 2 0 2 5 ] . [ 1 6 4 3 3 0 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) using Chain-of-Thought (CoT) prompting excel at complex reasoning but generate verbose thought processes with considerable redundancy, leading to increased inference costs and reduced efficiency. We introduce novel CoT compression framework based on step entropy, metric that quantifies the informational contribution of individual reasoning steps to identify redundancy. Through theoretical analysis and extensive empirical validation on mathematical reasoning benchmarks, we demonstrate that steps with low entropy are indeed highly redundant. Our experiments reveal that an astonishing 80% of lowentropy intermediate steps can be pruned with minor degradation in the final answer accuracy across DeepSeek-R1-7B, 14B and Qwen3-8B. This finding sharply contrasts with random or high-entropy pruning, which severely impairs reasoning performance. Building on this, we propose novel twostage training strategy combining Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO) reinforcement learning. This approach enables LLMs to autonomously learn to generate compressed COTs during inference by strategically incorporating [SKIP] tokens. Our method significantly enhances LLM inference efficiency while rigorously preserving accuracy, offering profound implications for practical LLM deployment and deeper understanding of reasoning structures. The code and data are released in https://github.com/staymylove/COT Compresstion via Step entropy. Introduction Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when employing techniques like Chain-of-Thought (COT) (Wei et al. 2022). By generating explicit intermediate reasoning steps, often referred to as slow thinking, Large Reasoning Model (LRM) such as the DeepSeek-R1 (Guo et al. 2025) Series and Qwen3 (Yang et al. 2025) significantly enhance performance on multi-step problems in domains like mathematics, coding and symbolic logic. This process allows the model to break down complex problems into more manageable components, leading to more reliable and accurate outcomes. However, notable drawback of current slow thinking COT implementations is the considerable redundancy often *Corresponding Author present within the generated thought processes (Deng et al. 2023; Zhong et al. 2025a). These verbose reasoning paths, while thorough, lead to increased inference latency, higher computational costs, and diminished overall efficiency. As models become larger and are deployed at scale, these inefficiencies present significant bottleneck for practical applications. To mitigate this, prior research has explored several compression strategies. One prominent direction focuses on making the CoT process implicit or latent, finetuning the model to internalize reasoning steps without verbalizing them (Deng, Choi, and Shieber 2024; Hao et al. 2024) or dynamically compressing them in latent space (Tan et al. 2025). Other work has focused on compressing the reasoning chain at different granularities, from pruning tokens in the input context (Li et al. 2023), enabling controllable token-level skipping during generation (Xia et al. 2025), to chunk-based compression (Wang et al. 2025b). While these methods improve efficiency, they do not offer principled way to identify and remove entire reasoning steps that are semantically redundant. Intuitively, when humans tackle complex problems, they record only key milestones, omitting obvious thoughts. Recent work has sought to teach LLMs similar ability to skip steps (Liu et al. 2024; Jiang, Li, and Ferraro 2025) or tune for length-compressible CoTs (Ma et al. 2025b). However, fundamental question persists: how can we systematically identify which steps in reasoning chain are crucial versus superfluous? In this paper, we propose novel, entropy-based method to identify and quantify the significance of each step within an LLMs Chain-of-Thought. We introduce the concept of step entropy, metric that measures the informational contribution of individual reasoning steps by aggregating tokenlevel entropy during generation. Our core hypothesis is that steps with lower entropy represent more predictable, and therefore less informative, parts of the reasoning chain that can be safely pruned without compromising accuracy. To validate this approach, we conduct systematic empirical analysis by calculating step entropy for reasoning trajectories and investigating the impact of pruning varying proportions of steps (10% to 100%) using three strategies: lowentropy pruning, high-entropy pruning, and random pruning. Our findings, as shown in Figure 1, reveal that pruning up to 80% of low-entropy steps maintains accuracy while achieving substantial token reductions (16-45% across multiple benchmarks), whereas high-entropy step removal causes immediate performance degradation. Cross-model validation on DeepSeek-R1 (7B, 14B) and Qwen3-8B demonstrates the universality of our entropy-based approach across different architectures. To demonstrate the superiority of steplevel pruning, we compare our method with direct tokenlevel pruning in the experimental section. Building on this validation, we introduce two-stage training strategy combining Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO) (Shao et al. 2024) that enables models to autonomously generate compressed reasoning trajectories. The SFT stage teaches models to predict when to use [SKIP] tokens based on entropy-compressed training data, while GRPO optimizes composite reward function balancing accuracy, compression ratio, and response length. Our trained models achieve 3557% token reductions while maintaining or improving accuracy, demonstrating that LLMs can learn to perform efficient reasoning without sacrificing quality."
        },
        {
            "title": "The main contributions of our work are summarized as",
            "content": "follows: We introduce step entropy as principled metric for quantifying the contribution of each step in the Chainof-Thought thinking trajectory. We provide strong empirical evidence that low-entropy steps are largely redundant and can be pruned up to 80% without significant loss of accuracy. We propose two-stage training strategy that enables LLM to learn the efficient compressed reasoning policy, significantly improving inference efficiency while maintaining performance. This paper is structured as follows: Related Work reviews LLM reasoning with reinforcement learning and CoT compression techniques. Section 3 presents our entropy-based CoT compression methodology, including step entropy formulation, pruning strategy, and two-stage training for autonomous compression. Section 4 provides experimental validation across multiple benchmarks and models, establishing optimal pruning ratios and demonstrating our training approachs effectiveness."
        },
        {
            "title": "Related Work",
            "content": "LLM Reasoning with Reinforcement Learning Reinforcement Learning (RL) has emerged as powerful paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs). Recent advancements, such as those demonstrated in (Shao et al. 2024) and (Xie et al. 2025), showcase RLs efficacy in refining LLMs ability to tackle complex reasoning tasks. Furthermore, strategies involving long COT (Yeo et al. 2025) and slow thinking (Zhong et al. 2025b) (which involves extending inference time) (Comanici et al. 2025; Guo et al. 2025; OpenAI 2025) have been shown to significantly improve LLM reasoning performance by allowing for more elaborate and deliberate thought processes. However, the increased length and computational overhead associated with these verbose COTs have led to concerns regarding efficiency. Research by (Wang et al. 2025a; Cuadron et al. 2025; Sui et al. 2025) highlights the phenomenon of overthinking, where excessively long COTs can paradoxically lead to diminished efficiency without proportional gains in accuracy. This emphasizes the need for methods that can optimize the length and content of reasoning trajectories. COT Compression and Latent Reasoning To address the inefficiency of verbose reasoning, researchers have pursued two main avenues: compressing the explicit CoT and making the reasoning process entirely implicit. the finest Explicit compression methods aim to shorten the genlevel, erated text at various granularities. At some works enable controllable token-level skipping (Xia et al. 2025) or explore the information-theoretic minimum number of tokens required for solution (Lee, Che, and Peng 2025). At coarser grain, R1-Compress introduces chunk-based compression and search (Wang et al. 2025b). Other strategies use length-constrained tuning, integrating penalties into RL reward functions (Shen et al. 2025; Hou et al. 2025) or developing specific architectures for lengthcompressible CoTs like CoT-Valve (Ma et al. 2025b). Our work advances this line by proposing more semantically grounded approach: we operate at the step level, arguing it better mimics human cognition (skipping entire thoughts, not words). Furthermore, our method is guided by an explicit information-theoretic signalstep entropyteaching the model not just to be shorter, but to selectively discard what is verifiably uninformative. An alternative, more radical approach is to make reasoning implicit or latent. Methods like iCOT (Deng, Choi, and Shieber 2024) and COCONUT (Hao et al. 2024) finetune models to internalize reasoning steps, while others use knowledge distillation to embed the process in the models hidden states (Deng et al. 2023). More recently, dynamic latent compression performs reasoning entirely within these hidden states, avoiding explicit generation altogether (Tan et al. 2025). While these latent strategies offer maximum efficiency, they sacrifice the critical interpretability and verifiability of an explicit CoT. Our work carves distinct path by focusing on optimizing the explicit reasoning chain, preserving its benefits while drastically improving its efficiency. CoT Compression via Step Entropy This section details our framework for CoT compression, which is built on novel, entropy-based metric. We begin by formally defining step entropy and providing its theoretical justification as measure of reasoning steps importance. We then describe our primary contribution: the low-entropy steps pruning strategy and the process for performing LLM inference with the compressed CoT. Step Entropy The foundational premise of our work is that not all steps in CoT contribute equally to the final answer. To formalize Figure 1: Comprehensive Performance of Chain-of-Thought Compression via Step Entropy. (a) Accuracy vs. Mask Ratio on 50 samples from DeepScaleR. This plot illustrates the impact of different pruning strategies (Random, High-Entropy Steps, LowEntropy Steps) on final answer accuracy as the mask ratio of intermediate COT steps increases. Note that masking up to 80% of low-entropy steps maintains complete COT accuracy (74%), while high-entropy and random masking lead to significant performance degradation. (b) Accuracy vs. Tokens Usage Ratio on Other Benchmarks. This plot compares the accuracy and token usage ratio of the Full COT against our Compressed COT (80% low-entropy steps pruning) across Math500, AIME 2024, and AIME 2025 datasets of DeepSeek-R1-7B. this, we introduce step entropy as measure of the informational contribution of each reasoning step. We hypothesize that steps generated with high confidence (low uncertainty) by the model are more likely to be redundant. Information entropy provides natural way to quantify this uncertainty. Given CoT sequence generated by LRM, we first segment it into series of distinct steps, = (S1, S2, . . . , SN ), is sequence of tokens, Si = where each step Si (ti,1, ti,2, . . . , ti,Mi). During autoregressive generation, for each token ti,j, the model produces probability distribution p(ci,j) over its entire vocabulary , where ci,j is the context consisting of the input prompt and all previously generated tokens. The entropy of this distribution, which represents the models uncertainty at that generation step, is calculated using the standard Shannon entropy formula: H(ti,jci,j) = (cid:88) wV p(wci,j) log2 p(wci,j) (1) We define the step entropy H(SiS<i) as the sum of tokenlevel entropy across all tokens within that step: H(SiS<i) = Mi(cid:88) j=1 H(ti,jci,j) = H(ti,1, ..., ti,Mi S<i) (2) high step entropy H(Si) indicates that the model was, on average, highly uncertain when generating step Si, while low step entropy indicates the deterministic generation. Now assume that the entropy of the step Sj is low, i.e., H(SjS<j) is low, we want to explore the relation of Sj and final solution A. We consider the conditional mutual information I(Sj; Sj) = I(Sj; AS1, ..., Sj1, Sj+1, ..., SN ) = H(Sj Sj) H(Sj Sj, A) H(Sj Sj) = H(SjS<j, S>j) = H(SjS<j) I(Sj; S>jS<j) (3) (4) (5) (6) Since I(Sj; S>jS<j) 0, we have I(Sj; Sj) H(SjS<j). This result demonstrate that when the entropy of step Sj is low, the conditional mutual information of Sj and the final answer A, which implies that the relation of Sj and is minor. Now lets assume that there are steps with low entropy, = Sk0 , Sk1, ..., SK, then we have I( S; (C/ S)) = (cid:88) i=0 I(Ski; A(C/[Sk0, ..., Ski])) (7) Without loss of generality, we assume the indices ki are arranged in descending order, i.e., ki < ki1. Therefore, we could split the sequence C/[Sk0, ..., Ski] into S<ki and S>ki/[Sk0, ..., Ski1 ]. Now consider the item with ki: I(Ski;A(C/[Sk0, ..., Ski])) H(SkiC/[Sk0, ..., Ski ]) = H(SkiS<ki, (S>ki/[Sk0, ..., Ski1])) H(SkiS<ki) (8) (9) (10) special [SKIP] token, while preserving high-entropy steps in their original form. The example of this process is shown in Figure 2. 4. Inference with Compressed CoT: The compressed sequence is concatenated with the user query and the </think> delimiter to prompt the model to generate only the final answer, as illustrated in Figure 3. This approach allows us to systematically compress CoT sequences while maintaining reasoning coherence. The pruning ratio κ provides flexible control mechanism for balancing compression efficiency and answer quality, with optimal values determined empirically across different datasets and problem types. The upper bound of steps pruning ratio κ will be discussed in the experiment section."
        },
        {
            "title": "Inference with Compressed Thinking COT",
            "content": "User Query / Problem <think> S1, [SKIP], S3, [SKIP], [SKIP], . . . , SN </think> Final Answer Generation Figure 3: Inference pipeline using compressed thinking COT. The compressed CoT sequence is concatenated with the user query in the prompt context. The </think> delimiter signals the model to generate only the final answer without additional reasoning. Experiments We present comprehensive experiments validating our entropy-based CoT compression method. We first establish the optimal pruning ratio through controlled experiments, then demonstrate the effectiveness and generalizability of our approach across multiple benchmarks and model sizes. Finally, we compare our step-level compression strategy against token-level alternatives to justify our methodological choices. Finally, we propose two-stage training method to make LLM to learn to generate the compressed reasoning trajectory. Determining the Optimal Pruning Ratio To identify the safe threshold for step pruning, we conduct controlled experiment using 50 samples from DeepScaleR (Luo et al. 2025) dataset on DeepSeek-R1-7B. And we investigate the impact on final answer accuracy by pruning steps using three distinct strategies with mask ratio varying from 10% to 100% (no-thinking mode (Ma et al. 2025a)), shown in Figure 1 (left). Low-Entropy Steps Pruning: Steps with the lowest entropy scores are progressively removed. High-Entropy Steps Pruning: Steps with the highest entropy scores are progressively removed. Figure 2: The case of Low-Entropy steps pruning strategy for COT compression, and replacing each selected lowentropy step with special [SKIP] token. Therefore, we conclude that I( S; (C/ S)) = (cid:88) i=0 (cid:88) i=0 I(Ski; A(C/[Sk0 , ..., Ski])) (11) H(SkiS<ki) (12) The result denotes that steps Sk0 , Sk1, ..., SK could have minor relation to the final solution A. This observation implies that, step with low entropy suggesting the deterministic thinking, which has minor relation to the finally solution, thus denotes such step could be less informative, and potentially redundant. Low-Entropy Steps Pruning Strategy for COT Compression Based on the theoretical foundation established above, we propose practical CoT compression approach that selectively removes low-entropy steps while preserving the essential reasoning structure. Our method operates on the principle that steps with low entropy are more likely to be redundant and can be safely pruned without significantly impacting the final answer quality. 1. Generate Full CoT: For each problem instance x, we use DeepSeek-R1-Distill-Qwen-7B to generate complete CoT trajectory, the response format is <think> </think> final answer . The reasoning steps S1, S2, . . . , SN , delimited by nn, are extracted from thinking content between the <think> and </think> tags, = (S1, S2, . . . , SN ). 2. Calculate Step Entropy: For each step Si C, we compute its step entropy H(Si) using Equation 2. 3. Entropy-Based Pruning: We rank all steps in ascending order of their entropy scores and identify the κ lowest-entropy steps for pruning, where κ is the pruning ratio hyperparameter. The compressed CoT is constructed by replacing each selected low-entropy step with Method GSM8k Math500 AIME 2024 AIME 2025 ACC (%) Tokens ACC (%) Tokens ACC (%) Tokens ACC (%) Tokens 80.36 DeepSeek-R1-7B 80.82 DeepSeek-R1-7B (Our) 82.64 DeepSeek-R1-14B DeepSeek-R1-14B (Our) 84.00 94.46 Qwen3-8B 94.39 3373301 (16.2%) 91.13 2604619 (27.0%) 81.48 346007 (44.9%) 76.00 351499 (41.1%) Qwen3-8B (Our) 88.17 88.17 1302114 (29.7%) 56.67 302784 (36.3%) 35.71 344135 (37.0%) 84.37 82.16 91.37 65.52 58.62 261167 (43.5%) 51.72 325262 (39.8%) 79.31 390 545 388180 (0.6%) 374 109 367060 (1.9%) 4 027 793 1 426 865 990486 (30.6%) 3 569 1 851 917 540 003 546 097 597 073 462 445 475 628 097 63.33 58.62 35.71 76.92 Table 1: Comparing the full COT baseline with our proposed step-entropy based pruning (Our) method, which prunes 80% of the lowest-entropy steps for DeepSeek-R1-7B, 14B and Qwen-8B. We conduct experiments to get the Pass@1 Accuracy(ACC)(%) and the number of Thinking Tokens (contains the Unicode characters) during the inference on GSM8k, Math500, AIME2024 and AIME2025. Random Steps Pruning: Steps are removed at random, serving as control. The results, illustrated in Figure 1, strongly validate our hypothesis. With Low-Entropy Steps Pruning, we observe that final answer accuracy remains stable and unaffected even when up to 80% of the lowest-entropy steps are masked. Beyond this 80% threshold, accuracy begins to decline, eventually converging to the accuracy of the no-thinking mode. This provides powerful evidence that vast majority of low-entropy steps are indeed redundant. We found that with the best pruning strategy, we can prune up to 80% lowest-entropy steps (40% tokens redundancy) when not affecting the accuracy, shown in Figure 1 (right). Conversely, with High-Entropy Steps Pruning, accuracy degrades immediately upon masking even small fraction of steps. When the mask ratio exceeds 40%, performance drops below that of the no-thinking mode, indicating that removing these critical, high-information steps is more detrimental than providing no reasoning at all. The Random Steps Pruning strategys performance falls between the two, beginning to decline at 40% ratio. Based on these findings, we establish our core strategy: pruning 80% of steps with the lowest entropy (κ = 0.8), replacing them with [SKIP] tokens while preserving the remaining high-entropy steps. Moreover, we validate this strategy also work on Deepseek-R1-14B and Qwen-8B. Validating Low-Entropy Steps Pruning Strategy With the 80% threshold established, we conduct extensive experiments to validate our strategys effectiveness, generalizability, and scalability across different models and datasets. (7B and 14B) Models and Datasets. We use models from the DeepSeek-R1 series and Qwen3-8B (Yang et al. 2025), which are open-source Large Reasoning Models with strong mathematical reasoning capabilities. For generating the initial CoT trajectories and creating our training data, we use combination of DeepScaleR (40k) (Luo et al. 2025) and OpenR1-Math (90k) (OpenR1-Math the effectiveness and generalizability of 2025). To test our compression method, we evaluate performance on several standard mathematical benchmarks: GSM8k (Cobbe et al. 2021), Math500 (Lightman et al. 2023), AIME2024 (dataset card AIME 2024) and AIME2025 (dataset card AIME 2025). Performance and Generalizability on Benchmarks. To test the broader effectiveness of our strategy, we applied the 80% low-entropy steps pruning strategy to multiple benchmarks across both Deepseek-R1-7B, 14B and Qwen3-8B models. Table 1 presents comprehensive results comparing our compressed CoT method against full CoT baselines. Our approach consistently achieves substantial efficiency gains while maintaining or improving accuracy across all models. The DeepSeek-R1 series shows remarkable token reductions: 29.7-37.0% for the 7B model and 30.6-43.5% for the 14B model across mathematical benchmarks, with GSM8k showing slight accuracy improvements for both sizes. Notably, Qwen3-8B demonstrates the strongest performance with impressive token reductions of 16.2-44.9% while maintaining competitive accuracy and even achieving slight improvements on AIME 2024 (79.31%81.48%). The crossarchitecture consistencyspanning both DeepSeek-R1 and Qwen3 model familiesdemonstrates that step entropy is robust and generalizable principle for identifying redundancy, independent of model architecture, size. Scalability and Dataset Creation. To ensure our method scales beyond controlled experiments, we further validated that the 80% low-entropy pruning strategy holds at much larger scale. To validate this strategy at scale, we applied this compression pipeline to the entire DeepScaler (40k) and OpenR1-Math datasets (90k). The results in Table 2 confirm that even across these tens of thousands of examples, the accuracy of the statically compressed CoTs remains almost identical to that of the full CoTs. This large-scale validation not only proves the robustness of our method but also serves as the direct procedure for CoT Compression for Dataset Creation. The resulting dataset, consisting of (problem, compressed CoT) pairs, serves as the foundation for our training strategy. Discussion of Our Method v.s. Directly Masking Tokens crucial aspect of our methodology is the decision to prune entire reasoning steps rather than individual tokens. To jus-"
        },
        {
            "title": "Model",
            "content": "DeepScaleR OpenR1-Math"
        },
        {
            "title": "Full CoT\nCompressed COT",
            "content": "63.74 62.17 48.39 47.31 Table 2: Comparing the Accuracy (%) of Full CoT (Full Chain of Thought) against Our Compressed CoT, based DeepSeek-R1-7B on two large datasets: DeepScaleR (40K) and OpenR1-Math (90K). Stage 1: Supervised Fine-Tuning (SFT) We first train the model on (problem, compressed CoT) pairs generated using our 80% entropy-based pruning strategy. The model learns to predict compressed reasoning paths and generate [SKIP] tokens by minimizing cross-entropy loss, providing robust initialization for reinforcement learning. Stage 2: Group Relative Policy Optimization (GRPO) While SFT teaches static imitation of compressed traces, it does not explicitly optimize the accuracy-efficiency trade-off. We employ Group Relative Policy Optimization (GRPO)(Shao et al. 2024) to further optimize this behavior through reward-driven learning. For each input prompt, we sample group of completions. The models goal is to learn policy πθ that maximizes composite reward function R(C) for each generated completion C. The total reward is the sum of four components designed to balance correctness with efficiency: R(C) = [Rcorrectness, Rskip ratio, Rskip num, Rresponse length] Figure 4: Comparing the accuracy of Our Method (via stepentropy) and Directly Masking Tokens (via token-entropy) across various thinking token mask ratios, with Full COT serving as the baseline. tify this, we compared our step-based pruning approach against token-based pruning baseline, where we remove the lowest-entropy tokens from the thinking trace irrespective of the steps they belong to. The results, shown in Figure 4, are unequivocal. While our step-pruning method maintains baseline accuracy even after removing up to 40% of the total thinking tokens, the token-pruning approach leads to sharp and immediate decline in performance. Accuracy drops significantly after just 20% token mask ratio. This demonstrates that reasoning step is the correct semantic unit for compression. Removing individual low-entropy tokens (e.g., common words or operators) can break the syntactic and semantic integrity of critical reasoning step, rendering it incomprehensible to the model. In contrast, removing an entire low-entropy step preserves the structure of the remaining, more important steps, leading to much more robust compression strategy. Two-Stage Training Strategy While our entropy-based pruning strategy effectively compresses existing CoT sequences, enabling models to autonomously generate compressed reasoning trajectories during inference represents more practical advancement. Our two-stage training methodology successfully achieves this goal by teaching models to balance accuracy with efficiency through learning when to skip redundant reasoning steps. (13) Let Tthink(C) be the thinking content within the completion C. The reward components are defined as follows: 1. Correctness Reward (Rcorrectness): large positive reward for generating the correct final answer. Let Aextracted(C) be the answer extracted from completion and be the ground truth. (cid:26)2.0 0.0 if == Aextracted(C) otherwise Rcorrectness(C, A) = (14) 2. Skip Ratio Reward (Rskip ratio): tiered reward for achieving high ratio of skipped steps, encouraging compression. Let Nskip be the count of [SKIP] tokens and Nsteps be the total number of steps in Tthink(C). The skip ratio is Ratioskip = Nskip/ max(1, Nsteps). 1.0 0.5 0.0 if Ratioskip κhigh if κlow Ratioskip < κhigh otherwise Rskip ratio(C) = (15) 3. Skip Number Penalty (Rsn): Penalty -1.0 when [SKIP] tokens exceed τskip num to prevent degenerate behavior. 4. Response Length Penalty (Rrl): Penalty -1.0 for responses exceeding τlength tokens to encourage conciseness. This two-stage process trains the model to strategically decide when to perform detailed reasoning versus when to skip steps, achieving efficient reasoning while preserving accuracy. Experimental Setup We use DeepSeek-R1-Distill-Qwen- (DeepScaleR, 7B with 130k mathematical problems OpenR1-Math) preprocessed using 80% entropy-based pruning, yielding 70k training samples after filtering sequences exceeding 4096 tokens. Stage 1 (SFT): 3 epochs on 70k samples using DeepSpeed Stage 2 and LoRA (r=16, α=16). Stage 2 (GRPO): 10k samples with reward parameters τhigh = 0.8, τlow = 0.5, τskip = 100, τlength = 3500. Method GSM8k Math AIME 2024 AIME 2025 ACC (%) Tokens ACC (%) Tokens ACC (%) Tokens ACC (%) Tokens DeepSeek-R1-7B 78.54 78.47 SFT 79.15 221 655 (44%) 85.00 1 219 708 (35%) 57.14 205 181 (57%) 33.33 322 501 (41%) SFT + GRPO 1 851 917 1 388 393 497 223 767 475 303 276 942 546 097 353 154 63.33 56.67 35.71 30.00 88.17 85. Table 3: Comparison of Pass@1 Accuracy (ACC %) and Thinking Tokens across baseline (DeepSeek-R1-7B), SFT, and SFT+GRPO training results on GSM8k, Math500, AIME2024, and AIME2025 benchmarks. Training uses DeepSpeed Stage 3, LoRA (r=16, α=32), AdamW, G=14, KL=0.04 on 880GB GPUs. More details can be found in Appendix. Experimental Analysis The results of our two-stage training process, presented in Table 3, demonstrate the effectiveness of our approach in creating an efficient yet powerful reasoning model. The initial SFT stage successfully teaches the model compressed reasoning style. Compared to the baseline DeepSeek-R1-7B model, the SFT model dramatically reduces the number of thinking tokens across all benchmarks. For example, on GSM8k, it cuts token usage by 43% while maintaining nearly identical accuracy. On AIME 2024, it achieves 42% token reduction with moderate drop in accuracy. This shows that the model effectively learns to use the [SKIP] token to bypass redundant steps identified in our entropy-based data preparation."
        },
        {
            "title": "The GRPO stage further optimizes",
            "content": "this behavior through reward-driven learning. The final two-stage training achieves impressive efficiency gains: 44% reduction on GSM8k, 35% on Math500, 57% on AIME 2024, and 41% on AIME 2025. Accuracy remains stable or improves slightly (GSM8k: 78.54%79.15%), demonstrating that the model learns to skip truly redundant steps without compromising reasoning quality. The Math500 results reveal nuanced behavior where GRPO learns to be selectively less aggressive with compression to preserve accuracy, indicating adaptive reward-driven optimization rather than static imitation. Overall, the results confirm the success of our training strategy, which provide robust foundation for compressed reasoning and balance the trade-off between efficiency and task accuracy. Method GSM8k Math500 ACC (%) Tokens ACC (%) Tokens 78.54 DeepSeek-R1-7B 75.93 Rc + Rsr Rc + Rsr + Rsn 78.70 Rc + Rsr + Rsn + Rrl 79.15 390 545 88.17 1 851 917 734 913 51.00 2 041 351 604 452 85.00 1 346 954 221 655 85.00 1 219 708 Table 4: Ablation experiment for different rewards in GRPO on GSM8k and Math500 benchmarks. Reward Components Ablation Study Table 4 demonstrates the necessity of our multi-component reward design through systematic ablation. Using only correctness and skip ratio rewards (Rc + Rsr) yields catastrophic results, severely degrading performance (GSM8k: 78.5475.93%, Math500: 88.1751.00%) while paradoxically increasing token usage by 88.2% and 10.2% respectively. This indicates that naive skip optimization without constraints leads to degenerate policies generating excessive low-quality [SKIP] tokens. Adding the skip number penalty (Rsn) restores competitive accuracy but token usage remains suboptimal. The complete reward function (Rc + Rsr + Rsn + Rrl) achieves optimal balance, maintaining near-baseline accuracy while delivering substantial efficiency gains: 43.3% token reduction on GSM8k and 34.1% on Math500. These results underscore that effective CoT compression requires carefully balanced multi-objective optimization, where each component addresses specific failure modes to enable robust compression without sacrificing reasoning quality. Limitations Despite the strong performance, our entropy-based CoT compression method has limitations. The 80% pruning threshold, while effective on DeepSeek-R1 and Qwen3 models, may not transfer to other architectures due to variations in reasoning redundancy. Furthermore, the methods effectiveness is limited to mathematical reasoning tasks, necessitating validation for new application domains with potentially different optimal compression ratios. One possible way to address this challenge is to develop adaptive thresholds for task-aware and model-aware compression strategies, as discussed in extended experiments and discussions in Appendix. Conclusion We introduced novel Chain-of-Thought compression framework that uses step entropy to identify redundant reasoning steps in LLM-generated CoTs. Our empirical validation demonstrates that pruning up to 80% of low-entropy steps maintains accuracy while achieving substantial token reductions (16-57% across benchmarks). Cross-model validation on DeepSeek-R1 and Qwen3 architectures confirms the broad applicability of step entropy as generalizable principle for reasoning compression. Additionally, our twostage training strategy enables models to autonomously generate compressed reasoning trajectories during inference. This work offers significant implications for efficient LLM deployment and provides new insights into the structure of reasoning processes. card card 2025. 2024. AIME. AIME. References Cobbe, K.; Kosaraju, V.; Bavarian, M.; Chen, M.; Jun, H.; Kaiser, L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R.; et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Comanici, G.; Bieber, E.; Schaekermann, M.; Pasupat, I.; Sachdeva, N.; Dhillon, I.; Blistein, M.; Ram, O.; Zhang, D.; Rosen, E.; et al. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, arXiv preprint and next generation agentic capabilities. arXiv:2507.06261. Cuadron, A.; Li, D.; Ma, W.; Wang, X.; Wang, Y.; Zhuang, S.; Liu, S.; Schroeder, L. G.; Xia, T.; Mao, H.; et al. 2025. The danger of overthinking: Examining the reasoning-action dilemma in agentic tasks. arXiv preprint arXiv:2502.08235. dataset URL https://huggingface.co/datasets/HuggingFaceH4/aime 2024. dataset URL https://huggingface.co/datasets/opencompass/. Deng, Y.; Choi, Y.; and Shieber, S. 2024. From explicit cot to implicit cot: Learning to internalize cot step by step. arXiv preprint arXiv:2405.14838. Deng, Y.; Prasad, K.; Fernandez, R.; Smolensky, P.; ChaudImplicit chain of thought hary, V.; and Shieber, S. 2023. arXiv preprint reasoning via knowledge distillation. arXiv:2311.01460. Guo, D.; Yang, D.; Zhang, H.; Song, J.; Zhang, R.; Xu, R.; Zhu, Q.; Ma, S.; Wang, P.; Bi, X.; et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Hao, S.; Sukhbaatar, S.; Su, D.; Li, X.; Hu, Z.; Weston, J.; and Tian, Y. 2024. Training large language models arXiv preprint to reason in continuous latent space. arXiv:2412.06769. Hou, B.; Zhang, Y.; Ji, J.; Liu, Y.; Qian, K.; Andreas, J.; and Chang, S. 2025. Thinkprune: Pruning long chain-ofthought of llms via reinforcement learning. arXiv preprint arXiv:2504.01296. Hu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; Wang, L.; Chen, W.; et al. 2022. Lora: Low-rank adaptation of large language models. ICLR, 1(2): 3. Jiang, Y.; Li, D.; and Ferraro, F. 2025. DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models. arXiv:2505.13975. Lee, A.; Che, E.; and Peng, T. 2025. How Well do LLMs Compress Their Own Chain-of-Thought? Token Complexity Approach. arXiv:2503.01141. Li, Y.; Dong, B.; Lin, C.; and Guerin, F. 2023. Compressing context to enhance inference efficiency of large language models. arXiv preprint arXiv:2310.06201. Lightman, H.; Kosaraju, V.; Burda, Y.; Edwards, H.; Baker, B.; Lee, T.; Leike, J.; Schulman, J.; Sutskever, I.; and Cobbe, K. 2023. Lets Verify Step by Step. arXiv preprint arXiv:2305.20050. Liu, T.; Guo, Q.; Hu, X.; Jiayang, C.; Zhang, Y.; Qiu, X.; and Zhang, Z. 2024. Can Language Models Learn to Skip Steps? arXiv:2411.01855. Loshchilov, I.; and Hutter, F. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101. J.; Shi, X.; Tang, W.; Luo, M.; Tan, S.; Wong, J.; Zhang, T.; Li, E.; Roongta, M.; Cai, C.; Luo, Popa, R. A.; and Stoica, I. 2025. DeepScaleR: Surpassing O1-Preview with 1.5B Model by Scaling RL. https://pretty-radio-b75.notion.site/DeepScaleRSurpassing-O1-Preview-with-a-1-5B-Model-by-ScalingRL-19681902c1468005bed8ca303013a4e2. Notion Blog. Ma, W.; He, J.; Snell, C.; Griggs, T.; Min, S.; and Zaharia, M. 2025a. Reasoning models can be effective without thinking. arXiv preprint arXiv:2504.09858. Ma, X.; Wan, G.; Yu, R.; Fang, G.; and Wang, X. 2025b. CoT-Valve: Length-Compressible Chain-of-Thought Tuning. arXiv:2502.09601. OpenAI. 2025. OpenAI o3 and o4-mini System Card. OpenR1-Math, D. 2025. OpenR1-Math-220k Dataset. Shao, Z.; Wang, P.; Zhu, Q.; Xu, R.; Song, J.; Bi, X.; Zhang, H.; Zhang, M.; Li, Y.; Wu, Y.; et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Shen, Y.; Zhang, J.; Huang, J.; Shi, S.; Zhang, W.; Yan, J.; Wang, N.; Wang, K.; Liu, Z.; and Lian, S. 2025. Dast: Difficulty-adaptive slow-thinking for large reasoning models. arXiv preprint arXiv:2503.04472. Sui, Y.; Chuang, Y.-N.; Wang, G.; Zhang, J.; Zhang, T.; Yuan, J.; Liu, H.; Wen, A.; Zhong, S.; Chen, H.; et al. 2025. Stop overthinking: survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419. Tan, W.; Li, J.; Ju, J.; Luo, Z.; Luan, J.; and Song, R. 2025. Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains. arXiv:2505.16552. Wang, Y.; Liu, Q.; Xu, J.; Liang, T.; Chen, X.; He, Z.; Song, L.; Yu, D.; Li, J.; Zhang, Z.; et al. 2025a. Thoughts are all over the place: On the underthinking of o1-like llms. arXiv preprint arXiv:2501.18585. Wang, Y.; Shen, L.; Yao, H.; Huang, T.; Liu, R.; Tan, N.; Huang, J.; Zhang, K.; and Tao, D. 2025b. R1-Compress: Long Chain-of-Thought Compression via Chunk Compression and Search. arXiv:2505.16838. Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.; Chi, E.; Le, Q. V.; Zhou, D.; et al. 2022. Chain-ofthought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35: 2482424837. Xia, H.; Leong, C. T.; Wang, W.; Li, Y.; and Li, W. 2025. TokenSkip: Controllable Chain-of-Thought Compression in LLMs. arXiv:2502.12067. Xie, T.; Gao, Z.; Ren, Q.; Luo, H.; Hong, Y.; Dai, B.; Zhou, J.; Qiu, K.; Wu, Z.; and Luo, C. 2025. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768. Yang, A.; Li, A.; Yang, B.; Zhang, B.; Hui, B.; Zheng, B.; Yu, B.; Gao, C.; Huang, C.; Lv, C.; et al. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388. Yeo, E.; Tong, Y.; Niu, M.; Neubig, G.; and Yue, X. 2025. Demystifying long chain-of-thought reasoning in llms. arXiv preprint arXiv:2502.03373. Zhong, J.; Li, Z.; Xu, Z.; Wen, X.; Li, K.; and Xu, Q. 2025a. Solve-Detect-Verify: Inference-Time Scaling with Flexible Generative Verifier. arXiv preprint arXiv:2505.11966. Zhong, J.; Li, Z.; Xu, Z.; Wen, X.; and Xu, Q. 2025b. Dyve: Thinking fast and slow for dynamic process verification. arXiv preprint arXiv:2502.11157."
        },
        {
            "title": "Appendix",
            "content": "Training Details Our experiments utilize DeepSeek-R1-7B as the base Large Reasoning Model (LRM). For data preparation, we began with an initial dataset of 130k (DeepScaleR and OpenR1Math) mathematical problems, which were pre-processed by masking 80% of their low-entropy steps. After filtering out sequences exceeding 4096 tokens, we obtained refined dataset of 70k samples for the initial training stage. Stage 1: Supervised Fine-Tuning (SFT). The 70k preprocessed samples were used for SFT. This stage was conducted for 3 epochs using DeepSpeed Stage 2 for distributed training and LoRA PEFT (Hu et al. 2022) with parameters r=16 and α=16. Stage 2: Reinforcement Learning (RL). For the RL phase, subset of 10k data samples was randomly selected from the 70k SFT-trained samples. We employed Group Relative Policy Optimization (GRPO) to further train the SFTinitialized model. This stage also utilized DeepSpeed Stage 3 for distributed training. The optimization objective involved composite reward function designed to balance accuracy, [SKIP] token ratio, [SKIP] token number, and overall response length. LoRA PEFT was applied with parameters r=16 and α=32, and the AdamW optimizer (Loshchilov and Hutter 2017) was used. Key GRPO parameters included G=14 samples per input and KL coefficient of 0.04. All experiments were performed on cluster of 8 GPUs, each has 80GB RAM and over 300 TFLOPS of BF16 compute performance. Extended Experiments Model-aware Experiments To validate the generalizability and robustness of our step entropy-based compression method across different model architectures and scales, we conducted comprehensive model-aware experiments on four distinct Large Reasoning Models: DeepSeek-R1-7B, DeepSeek-R1-14B, Qwen3-8B, and QwQ-32B, across four mathematical reasoning benchmarks. The results in Table 5 demonstrate remarkable consistency across diverse model architectures, with our method showing consistent performance across three different model familiesDeepSeekR1, Qwen3, and QwQ. This cross-architecture consistency indicates that step entropy captures fundamental properties of reasoning redundancy that transcend specific architectural choices or training methodologies, making it generalizable metric for identifying redundant reasoning steps. The compression benefits scale effectively across model sizes ranging from 7B to 32B parameters, with token reduction percentages remaining relatively consistent within model families while absolute token savings increase with model scale. DeepSeek-R1 models achieve token reductions from 0.6% to 43.5% while maintaining or improving accuracy, with the 14B variant showing particularly impressive performance gains on GSM8k (82.64%84.00%). Qwen3-8B exhibits the most aggressive compression capabilities with reductions ranging from 16.2% to 44.9%, even achieving accuracy improvements on AIME 2024 (79.31%81.48%). QwQ-32B, as the largest model, demonstrates the highest compression potential with up to 55.1% token reduction on AIME 2024, indicating that larger models generate proportionally more redundant reasoning steps. Benchmarkspecific analysis reveals distinct patterns: GSM8k shows the smallest token reductions but maintains accuracy, suggesting elementary problems require fewer redundant steps; AIME benchmarks consistently show the highest compression ratios (36.3% to 55.1%) across all models, indicating complex competition-level problems generate the most redundancy; and Math500 demonstrates balanced performance with 27.0-33.2% token reductions while maintaining high accuracy. The consistency of compression patterns across fundamentally different training methodologies provides strong evidence that step entropy captures universal properties of reasoning redundancy rather than model-specific artifacts, making our compression framework broadly applicable to current Large Reasoning Models while delivering substantial computational efficiency gains for practical deployment scenarios. Domain-aware Experiments To evaluate the domain generalizability of our step entropy-based compression method beyond mathematical reasoning, we conducted experiments on MMLU (Massive Multitask Language Understanding) benchmarks, specifically focusing on College Medicine and High School History tasks. Tables 8 and 7 present results for DeepSeek-R1-7B and QwQ-32B models respectively, comparing different compression levels (80%, 90% low-entropy step pruning) against full CoT and nothinking baselines. The results reveal domain-specific compression characteristics that differ significantly from mathematical reasoning tasks, with both models showing varying degrees of compression tolerance across the two MMLU domains. our robust DeepSeek-R1-7B demonstrates on MMLU tasks with performance compression method, achieving accuracy improvements on College Medicine (61.73%62.34%) and High School History (61.74%64.32%) while reducing token usage by 18.6% and 7.1% respectively at 80% compression. Notably, the model maintains or improves accuracy even at 90% compression levels, suggesting that knowledge-based reasoning tasks contain substantial redundancy that can be effectively identified through step entropy. The dramatic performance drop in the no-thinking baseline (52.46% and Method GSM8k Math500 AIME 2024 AIME 2025 ACC (%) Tokens ACC (%) Tokens ACC (%) Tokens ACC (%) Tokens 80.36 DeepSeek-R1-7B 80.82 DeepSeek-R1-7B (Our) 82.64 DeepSeek-R1-14B DeepSeek-R1-14B (Our) 84.00 94.46 Qwen3-8B 94.39 3373301 (16.2%) 91.13 2604619 (27.0%) 81.48 346007 (44.9%) 76.00 351499 (41.1%) Qwen3-8B (Our) 94.09 QwQ-32B 93.56 2148697 (17.7%) 91.75 1989032 (33.2%) 74.07 286324 (55.1%) 65.52 403025 (43.4%) QwQ-32B (Our) 88.17 88.17 1302114 (29.7%) 56.67 302784 (36.3%) 35.71 344135 (37.0%) 84.37 82.16 91.37 65.52 58.62 261167 (43.5%) 51.72 325262 (39.8%) 79.31 390 545 388180 (0.6%) 374 109 367060 (1.9%) 4 027 793 1 426 865 990486 (30.6%) 3 569 247 1 851 2 977 694 2 610 185 540 003 462 445 475 303 546 597 073 628 097 637 318 711 348 79.13 63. 92.35 58.62 35.71 76.92 66.67 Table 5: Comparing the full COT baseline with our proposed step-entropy based pruning (Our) method, which prunes 80% of the lowest-entropy steps for DeepSeek-R1-7B, 14B, Qwen-8B and QwQ-32B. We conduct experiments to get the Pass@1 Accuracy(ACC)(%) and the number of Thinking Tokens (contains the Unicode characters) during the inference on GSM8k, Math500, AIME2024 and AIME2025. Method GSM8k Math500 AIME 2024 AIME 2025 ACC (%) Tokens ACC (%) Tokens ACC (%) Tokens ACC (%) Tokens QwQ-32B (Full COT) QwQ-32B (80%) QwQ-32B (90%) QwQ-32B (No thinking) 93.86 2 610 185 94.09 93.56 2148697 (17.7%) 91.75 1989032 (33.2%) 74.07 286324 (55.1%) 65.52 403025 (43.4%) 93.56 2010201 (23.0%) 90.95 1505681 (49.4%) 70.37 225711 (64.6%) 66.67 193792 (72.8%) 2 977 694 637 711 348 79.13 92.35 66.67 - 88. - 62.07 - 56.67 - Table 6: Comparing the full COT and No Thinking baseline with our proposed step-entropy based pruning method, which prunes 80% and 90% of the lowest-entropy steps for QwQ-32B. We conduct experiments to get the Pass@1 Accuracy(ACC)(%) and the number of Average Thinking Tokens Per Answer (contains the Unicode characters) during the inference on GSM8k, Math500, AIME2024 and AIME2025. 47.82%) emphasizes the critical importance of maintaining some reasoning structure, validating our selective pruning approach over complete reasoning elimination. QwQ-32B exhibits even stronger compression capabilities on MMLU benchmarks, maintaining perfect accuracy preservation on High School History (92.83%) across all compression levels while achieving substantial token reductions of up to 20.1% at 90% compression. On College Medicine, the model shows minimal accuracy degradation (86.13%84.97%) with significant efficiency gains (15.0-20.1% token reduction). The domain-specific patternswhere History tasks show higher compression tolerance than Medicine taskssuggest that factual recall and historical reasoning contain more redundant steps than medical reasoning, which may require more careful step-by-step analysis. These results demonstrate that our step entropy method successfully generalizes beyond mathematical domains while revealing important domain-specific characteristics that could inform adaptive compression strategies. Key Findings and Analysis Our comprehensive experimental evaluation reveals several critical insights about Chain-of-Thought compression via step entropy. The most significant finding is that 80% of low-entropy reasoning steps can be safely pruned without accuracy degradation across multiple model architectures and reasoning domains. This substantial redundancy indicates that current Large Reasoning Models generate highly verbose thought processes, with the majority of steps coninformational value to final answer tributing minimal requality. The cross-architectural consistency of our sultsspanning DeepSeek-R1 (7B, 14B), Qwen3-8B, and QwQ-32Bdemonstrates that step entropy captures fundamental properties of reasoning redundancy that transcend specific model designs. Token reductions ranging from 16.2% to 55.1% across mathematical benchmarks, combined with maintained or improved accuracy, provide strong evidence that our entropy-based metric successfully identifies genuinely redundant reasoning components rather than model-specific artifacts. Domain-specific compression patterns emerge from our MMLU experiments, revealing that factual reasoning tasks (High School History) tolerate higher compression rates than analytical reasoning tasks (College Medicine). QwQ-32B maintains perfect accuracy on History tasks while achieving 20.1% token reduction, whereas medical reasoning shows more sensitivity to aggressive compression. This suggests that different cognitive processes exhibit varying degrees of redundancy, opening avenues for adaptive, domain-aware compression strategies. Broader Impact Our work addresses the critical challenge of reasoning efficiency in practical LLM deployment, where verbose Chainof-Thought processes create significant computational bottlenecks. By providing principled method for identifying Method MMLU-College Medicine MMLU-High School History ACC (%) Avg Tokens Per Answer ACC (%) Avg Tokens Per Answer QwQ-32B (Full COT) QwQ-32B (80%) QwQ-32B (90%) QwQ-32B (No Thinking) 86.13 84.97 84.97 84.30 2912.3 2475.9 (15.0%) 2326.4 (20.1%) - 92.83 92.83 92.83 91.14 1703.9 1683.9 (1.2%) 1683.9 (1.2%) - Table 7: QwQ-32B performance on MMLU-College Medicine and MMLU-High School History datasets showing accuracy and average tokens per answer across different pruning levels. Comparing the full COT and No Thinking baseline with our proposed step-entropy based pruning method, which prunes 80% and 90% of the lowest-entropy steps of per answer thinking tokens reduction percentages. Method MMLU-College Medicine MMLU-High School History ACC (%) Avg Tokens Per Answer ACC (%) Avg Tokens Per Answer DeepSeek-R1-7B (Full COT) DeepSeek-R1-7B (80%) DeepSeek-R1-7B (90%) DeepSeek-R1-7B (No Thinking) 61.73 62.34 62.34 52.46 2612.7 2127.9 (18.6%) 2069.4 (20.8%) - 61.74 64.32 61.74 47.82 2054.3 1907.5 (7.1%) 1936.2 (5.7%) - Table 8: DeepSeek-R1-7B performance on MMLU-College Medicine and MMLU-High School History datasets showing accuracy and average tokens per answer across different pruning levels. Comparing the full COT and No Thinking baseline with our proposed step-entropy based pruning method, which prunes 80% and 90% of the lowest-entropy steps of per answer thinking tokens reduction percentages. and removing redundant reasoning steps, we enable more sustainable and accessible deployment of Large Reasoning Models. The interpretability benefits of maintaining explicit reasoning chains while achieving substantial compression offer advantages over latent reasoning approaches. Practitioners can retain the transparency and verifiability of explicit Chain-of-Thought while significantly reducing computational overhead. Our findings contribute to the theoretical understanding of reasoning structures in Large Language Models, revealing that current models generate substantial redundancy in their thought processes. This insight informs future model design and training methodologies, potentially leading to more efficient reasoning architectures."
        }
    ],
    "affiliations": [
        "Huawei Technologies Co., Ltd",
        "The Chinese University of Hong Kong"
    ]
}