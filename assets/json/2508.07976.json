{
    "paper_title": "Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL",
    "authors": [
        "Jiaxuan Gao",
        "Wei Fu",
        "Minyang Xie",
        "Shusheng Xu",
        "Chuyi He",
        "Zhiyu Mei",
        "Banghua Zhu",
        "Yi Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in LLM-based agents have demonstrated remarkable capabilities in handling complex, knowledge-intensive tasks by integrating external tools. Among diverse choices of tools, search tools play a pivotal role in accessing vast external knowledge. However, open-source agents still fall short of achieving expert-level Search Intelligence, the ability to resolve ambiguous queries, generate precise searches, analyze results, and conduct thorough exploration. Existing approaches fall short in scalability, efficiency, and data quality. For example, small turn limits in existing online RL methods, e.g. <=10, restrict complex strategy learning. This paper introduces ASearcher, an open-source project for large-scale RL training of search agents. Our key contributions include: (1) Scalable fully asynchronous RL training that enables long-horizon search while maintaining high training efficiency. (2) A prompt-based LLM agent that autonomously synthesizes high-quality and challenging QAs, creating a large-scale QA dataset. Through RL training, our prompt-based QwQ-32B agent achieves substantial improvements, with 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our agent exhibits extreme long-horizon search, with tool calls exceeding 40 turns and output tokens exceeding 150k during training time. With a simple agent design and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on xBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We open-source our models, training data, and codes in https://github.com/inclusionAI/ASearcher."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 6 7 9 7 0 . 8 0 5 2 : r Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL Jiaxuan Gao12, Wei Fu12, Minyang Xie12, Shusheng Xu2, Chuyi He2, Zhiyu Mei2, Banghua Zhu3, Yi Wu12 1 IIIS, Tsinghua University, 2 Ant Research, RL Lab 3 University of Washington samjia2000@gmail.com, jxwuyi@gmail.com"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in LLM-based agents have demonstrated remarkable capabilities in handling complex, knowledge-intensive tasks by integrating external tools. Among diverse choices of tools, search tools play pivotal role in accessing vast external knowledge. However, open-source agents still fall short of achieving expert-level Search Intelligence, the ability to resolve ambiguous queries, generate precise searches, analyze results, and conduct thorough exploration. Existing approaches fall short in scalability, efficiency, and data quality. For example, small turn limits in existing online RL methods, e.g. 10, restrict complex strategy learning. This paper introduces ASearcher, an open-source project for large-scale RL training of search agents. Our key contributions include: (1) Scalable fully asynchronous RL training that enables long-horizon search while maintaining high training efficiency. (2) prompt-based LLM agent that autonomously synthesizes high-quality and challenging QAs, creating large-scale QA dataset. Through RL training, our prompt-based QwQ-32B agent achieves substantial improvements, with 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our agent exhibits extreme long-horizon search, with tool calls exceeding 40 turns and output tokens exceeding 150k during training time. With simple agent design and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on xBench and 52.8 on GAIA, surpassing existing open-source 32B agents. 2 Figure 1: (Left) Asynchronous RL brings substantial improvements: Through RL training, our agent, ASearcher-Web-QwQ, obtains 20.8%, 46.7%, and 20.4% improvements on GAIA, xBench, and Frames, respectively. (Middle) & (Right) Through RL training, ASearcher-Web-QwQ learns to conduct long-horizon search, with tool calls exceeding 40 turns and output tokens exceeding 150k during training. The agent also learns expert-level search strategies (See case study in Sec. 2) Corresponding author 2We open-source our models, training data, and codes in https://github.com/inclusionAI/ASearcher. Preprint."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in LLM-based agents have demonstrated remarkable capabilities in solving complex, knowledge-intensive problems by leveraging single or multiple external tools [42, 45, 37]. Among these, search tools stand out as particularly critical, enabling agents to access vast external knowledge for enhanced problem-solving [26, 8, 27]. However, expert-level use of search requires advanced intelligence. For instance, consider the question As of December 31, 2024, what were the numbers of gold, silver, and bronze medals won by China in the 2012 London Olympics?.While seemingly straightforward, this query is indeed challenging due to conflicting answers online (e.g., 38 gold, 27 silver, 22 bronze vs. 39 gold, 31 silver, 22 bronze). search agent must navigate noisy and conflicting answers from diverse sources, identify the root cause of conflicts as doping test disqualifications from official reports, and ultimately determine the correct answer. Challenging real-world tasks require the agent to resolve high uncertainty in input queries, generate precise search queries, analyze and extract key insights from massive data, resolve inconsistencies, and conduct in-depth exploration. We term this advanced capability \"Search Intelligence\". Proprietary agents and models has already exhibit signs of complex search behaviors through largescale Reinforcement Learning (RL) training [1, 25]. However, open-source approaches for developing search agents still face significant limitations. series of works employ Reinforcement Learning or Supervised Fine-Tuning approaches to incentivize tool-using capabilities [11, 30, 49, 33]. On the other hand, prompt-based LLM agents supported by open-source models could perform massive tool calls without training [18, 2]. However, in practice, we find that existing online RL approaches fail to incentivize complex and effective search strategies. We also find prompt-based LLM agents could fail due to the insufficient capabilities of the LLM, such as failing to precisely extract key information from noisy webpages and unable to verify wrong conclusions. More recently, some works further build up on prompt-based LLM agents, utilizing offline RL approaches to improve the prompt-based agents [32, 19]. However, this offline RL paradigm, has been shown to underperform online RL in broader range of domains [43, 6, 31]. In reasoning tasks such as math and coding, online RL has enable the models to evolve complex behaviors through iterative refining the reasoning processes based on correctness feedback. [9, 22, 7],. This raises critical question: How could online RL methods effectively unlock Search Intelligence in open-source agents? We identify two critical obstacles hindering effective online RL training for search agents: Insufficient search turns limit complex strategy learning. Existing works, such as SearchR1 [11], artificially limit the number of search turns, e.g. 10 per trajectory, preventing the agent from exploring deeper search paths. However, complex queries often require multi-turn tool calls and multi-step reasoning, that could not be learned under strict turn limits. Lack of large-scale, high-quality question-answer (QA) pairs: RL training for reasoning tasks requires abundant, challenging, and correct QA pairs [3, 16, 46]. However, most existing open-source datasets for search agents are often outdated (e.g. HotpotQA), oversimplified, or too small, failing to stimulate complex search behaviors through RL [44, 17, 34]. To address these challenges, we introduce ASearcher, an open-source project to enable large-scale agentic RL training for search agents. Our contributions include: Long-horizon search via fully asynchronous agentic RL training. With large turn limit in batch generation RL training systems [11, 30, 21, 35], long trajectories within batch could easily lead to significant idle time, slowing down the whole training process. Building up on AReaL [7], our fully asynchronous system avoids long trajectories from blocking the training by decoupling trajectory execution from model updates. This allows relaxed turn limits (e.g., 128 turns/trajectory), enabling agents to explore deeper search paths without sacrificing training efficiency. Remarkably, our agent, ASearcher-Web-QwQ, achieves extreme long-horizon search, with tool calls exceeding 40 turns and generated tokens surpassing 150k during RL training. scalable QA synthesis agent. We design an LLM-based agent that autonomously generates challenging, uncertain, and grounded QA pairs requiring multi-turn tool use. Starting 2 Figure 2: Comparison between ASearcher and Search-R1. (Left) Search-R1 is only equipped with search tools and lacks web browsing capability. (Right) ASearcher utilizes simple agent design with two basic tools including search and browsing tools, without relying on any external LLM. ASearcher is comprehensive agent capable of both reasoning and summarizing lengthy web contents. Notably, both reasoning and summarization abilities are optimized through end-to-end RL training. from seed questions, the agent iteratively fuzzes queries by obscuring key information, or injects external facts to increase complexity. Each constructed question undergoes multistage validation to ensure quality and difficulty. From 14k seed QAs, we generate 134k high-quality samples, with 25.6k requiring external tools for resolution. Using ASearcher, we train agents equipped with search engines and browsers under two settings, RL training starting from base models (Qwen2.5-7B/14B), to demonstrate that our training pipeline incentivizes strong and generalizable search strategies, and fine-tuning prompt-based agent empowered by powerful LRM (QwQ-32B), to validate the scalability of our training pipeline in fine-tuning large-scale prompt-based LLM agents. We evaluate our agents with on multi-hop QA benchmarks and challenging benchmarks including GAIA [24] , xbench-DeepSearch [41], and Frames [14]. ASearcher-Local-7B/14B, trained only with local knowledge base, demonstrate surprisingly generalizability to realistic web search and achieve state-of-the art performances on multi-hop and single-hop QA tasks. Building up on QwQ-32B, ASearcher-Web-QwQ achieves an Avg@4 score of 42.1 on xBench-DeepSearch and 52.8 on GAIA, surpassing set of open-source agents. When evaluating Pass@4, ASearcher-Web-QwQ achieves 70.1 on GAIA and 68.0 on xBench-DeepSearch. Notably, through RL training, ASearcher-Web-QwQ obtains 46.7% and 20.8% improvements on xBench-DeepSearch and GAIA, respectively. ASearcher presents large-scale open-source online agentic RL pipeline for LRM-based and LLMbased search agents, unlocking Search Intelligence through scalable training and high-quality data. We hope our findings not only advance search agents but also inspire broader innovations in LLM agents for complex real-world tasks."
        },
        {
            "title": "2 Limitations of Existing Open-source Approaches",
            "content": "In this section, we provide detailed case study on an extremely challenging question from GAIA [24]. Specifically, we analyze Search-R1-32B [11] and Search-o1 (QwQ) [18] in Fig. 3. The detailed trajectories are provided in Appendix A. Solution Path of the Sample Question. In Fig. 3, our case study is carried out on question requiring finding some specific animal with 4 unknown variables. To identify the correct answer, the search agent should first find out the mentioned species according to condition genus named for Copenhagen, identify the correct 2021 article based on the citation in the wikipedia page of the species, and then find out the papers of the two mentioned persons. Finally, the correct answer should 3 Figure 3: case study on complex query from GAIA. Search-R1-32B is unable to break down the complex question and has severe hallucinations. Search-o1 (QwQ) can identify the corrects articles through extensive tool calls, but easily misses key information and fails to verify wrong conclusions. Our end-to-end RL agent, ASearcher-Web-QwQ, exhibits key behaviors featuring Search Intelligence: uncertainty-aware reasoning (list and examine candidate answers), precise extraction from noisy contents, cross-document inference, and grounded verification. 4 be determined by cross referencing the 2021 article and the papers. To summarize, this example is challenging for several reasons, High Uncertainty: The question involves multiple unknown variables that could point to many different entities. For example, the 2021 article could point to any article published in 2021 and could only be determined by checking the multicenter, randomized, double-blind study in the Wikipedia page of the alvei species. Requirement for Exact Information Extraction: To find the answer, the agent should list all animals mentioned on the webpages and making cross-document comparison. This would require the agent to precisely extract key information from the vast, noisy web contents, instead of simply summarizing the webpages. Misleading Answers: During the process of solving this task, there could be multiple misleading answers, such as \"pigs\". The agent should rigorously verify its conclusions by checking the intended answer in all related webpages and documents. Existing Online RL Approaches Fail to Learn Complex Search Strategies. In Fig. 3, SearchR1-32B is not able to decompose the complex query into individual components, consequently only making ambiguous queries that involve too many unknown information. The agent also has severe hallucinations, producing conclusions that are not supported by the search results. Finally, it fails to resolve all unknown information. This case study shows that existing online RL approaches only incentivize elementary search strategies. It is also worth noting that, since the turn limit is set as small value, e.g. 4, during training, the model only exhibits short tool-use horizon. Prompt-based LLM Agents Could Fail Due to Insufficient Capability of the LLM. In Fig. 3, Search-o1 (QwQ) can find the species name, as well as the 2021 article and the related papers through large amount of tool calls. However, when trying to find the answer, Search-o1 (QwQ) would easily miss key information, consequently making incorrect conclusions. Note that even when the agent finds information that directly links to the correct answer, it is still misguided by previous incorrect conclusions. Finally, the agent is unable to verify the correctness of previous conclusions. This case study reveals that, though an open-source model that is not explicitly trained on agentic tasks can perform extensive tool calls, it could not make expert-level reasoning based on the retrieved contents and history contexts. ASearcher-Web-QwQ. We also analyze the search strategy of our end-to-end RL agent, ASearcherWeb-QwQ.As shown in Fig. 3, ASearcher-Web-QwQ decomposes the complex query into precise queries. Unlike Search-o1 (QwQ) that visits large amount of websites after each search query, ASearcher-Web-QwQ focuses on visiting one website at time. ASearcher-Web-QwQ summarizes all related information from website. Specifically, all candidate answers are listed and carefully analyzed by the agent. When the search results do not directly point to the desired target, e.g. when searching with Olga Tapia Hafnia alvei animal studies to find the animals related to Olga Tapias paper, the agent does not get clear information but is able to infer the correct answer by making connection with the other paper. After the correct answer Mice is found, the agent spends further turns on verifying previous conclusions before reporting the final answer. In summary, ASearcher successfully train search agent that exhibits expert-level search behaviors, Uncertainty-aware reasoning: the agent exhaustively lists and examines all possibilities for uncertain entities Precise Key Information Extraction: the agent is able to identify the key information from vast, noisy web contents. Cross-document Inference: the agent is able to infer critical conclusions by making connections across multiple documents. Grounded Verification: the agent verifies the correctness of previous conclusions by accessing or searching the related materials."
        },
        {
            "title": "3 ASearcher",
            "content": "In this work, we present ASearcher, an open-source project for unlocking search intelligence in search agents through large-scale RL training. As shown in Fig. 3, ASearcher trains search agent 5 that is able to solve complex questions by exhaustively resolving all uncertainties and performing multi-turn tool calls. In the subsequent sections, we present the agent design, the training data as well as data synthesis agent, and fully asynchronous reinforcement learning training in ASearcher. 3.1 Agent Design We employ simple agent design in ASearcher, as illustrated in Fig. 2. Tools. Given user query, the agent can utilize two basic tools: search engine and web browser. The search engine takes text query as input and returns relevant snippets along with their corresponding URLs. In The web browser accepts URL and returns the content of the webpage. To effectively tackle complex problems, the model should strategically combine these tools and extract key information from the vast amount of data. Webpage Summarization. Webpages could contain excessively long contents, therefore we employ the agent to summarize the webpage into compact summary. At training time, this summarization process would also be optimized, allowing the agent to improve the summarization ability through RL training. Instantiating ASearcher with Base LLMs and Advanced LRMs. Within the framework of ASearcher, we investigate two specific instantiations of the search agent: either base LLMs such as Qwen2.5-7B/14B, or advanced Large Reasoning Models (LRMs) such as QwQ-32B. These two different types of instantiations require different design choices in history management and prompting. For base LLMs, we following prior works [11, 30], to adopt append-only style prompting for the agent. Specifically, starting from system prompt, all LLM-generated responses, search results and summaries of webpages are appended to the history. The agent takes as input the full history in chronological order and outputs some reasoning texts and actions. This approach ensures efficiency during inference time. For LRMs, LRMs are already equipped with instruction following capabilities. Therefore we instruct the LRM with different prompts for tool selection, summarization, and answering. We also note that LRMs typically generate long responses, and sometimes the history would be long. We need to ensure compact input to ensure the LRM generates tokens with sufficient budget. Therefore, in the history, we discard thinking processes but instead keep summarized thoughts and tool callings. When prompting the LRM, only the most recent 25k characters of the history are provided to the LRM as additional context. These simple designs ensure that the LRM receives an input of at most 10k tokens. End-to-End Reinforcement Learning. Finally, we highlight that the all LLM-generated responses of the agent, including the thinking process, tool calling, and summarization, are trained using Reinforcement Learning in an end-to-end manner. 3.2 Training Data Our training data are from two primary sources. First, we carefully filter samples from open-source datasets to ensure difficulty and quality. Second, we synthesize high-quality question-answer (QA) pairs specifically designed to guide the agent to learn generalizable search strategies. 3.2.1 Open-source Data. We begin with the training sets from HotpotQA[44] and 2WikiMultiHopQA[10], both of which are multi-hop QA datasets. We employ model-based filtering process. We first train model on the full set of open-source data with RL, and then generate 16 responses for each question using the trained model. Finally, we filter out questions that meat any of the following criteria, The model could not find correct answer out of 16 responses The model achieves 50% accuracy, meaning the question would not be challenging enough 6 Figure 4: Data Synthesis Agent. Starting from seed QA, the data synthesis agent iteratively modifies the question through two actions, Injection and Fuzz. Through injection, the agent enriches the question by adding some external facts. Through Fuzz, the agent blurs certain information to increase uncertainty and difficulty. The related fact to the question are tracked during the synthesis process. Each time the question is modified, quality verification step is applied to ensure quality and difficulty of the synthetic questions. Figure 5: Statistics from our data synthesis process. (Left) The distribution of the number of supporting facts. (Middle) The distribution of the number of fuzz actions and injection actions. (Right) The accuracy distribution of QwQ-32B in answering the generated questions without using any tools. The model finds correct answer with only few search turns (i.e., 1 turns). This filtering approach ensures we keep only the most challenging yet solvable questions that demand tool use. Finally, from total of 304k QA pairs, we retain 16k challenging samples for RL training. Additionally, we include set of question-answer (QA) pairs designed for accessing certain webpages. In particular, we incorporate small subset of WebWalkerQA[40] to help the model learn how to locate answers within noisy, real-world web search environments. 3.2.2 Data Synthesis Agent We further develop data synthesis agent to create high-quality question-answer pairs. As shown in Fig. 4, the data synthesis agent begins with seed question, and iteratively modifies the question to increase the complexity. To ensure the synthetic question is strictly aligned with reliable sources, list of supporting facts obtained during the question synthesis process is kept and continuously updated for quality verification. At each step, given the current question and list of supporting facts, the agent automatically selects between two key actions, 7 Table 1: Examples of the synthetic questions, where red indicates injected facts and cyan represents fuzzed content. Round Question Action Seed QA - When was Michael P. Hein born? Round 1 Round 2 Injection When was the Eckerd College alumnus who served as the first County Executive of Ulster County, New York, and graduated with Bachelor of Arts in Business Administration born? Injection When was the individual born who, as County Executive of Ulster County, New York, permitted the Catskill Mountain Railroad to continue operations between Kingston and Hurley during the 2016 United States House of Representatives elections and also held that position during the 2018 elections? Round 3 Fuzzing When was the individual born who, as County Executive of Ulster County, New York, permitted historic mountain railway to continue operations between Kingston and Hurley during the 2016 United States House of Representatives elections and also held that position during the 2018 elections? ... Seed QA ... - ... Where is the Riggs-Hamilton American Legion Post No. 20 located? Round 1 Injection Where is the American Legion Post in Russellville, Arkansas, built in 1934 and Round 2 Fuzzing Round 3 Fuzzing recognized as notable example of WPA Rustic architecture and listed on the National Register of Historic Places located? Where is the American Legion Post in Russellville, Arkansas, built in the early 1930s and recognized as notable example of New Deal-era public works architecture and listed on the National Register of Historic Places located? Where is the veterans organizations building in Russellville, Arkansas, built in the early 1930s and recognized as notable example of New Deal-era public works architecture and listed on the National Register of Historic Places located? ... ... ... Action 1: Injection aims to enrich the context of the question by inserting facts related to the question. The agent first selects an entity in the question and then obtains one piece of related fact about the selected entity from external sources such as Wikipedia. Then new question is proposed by injecting the fact into the question. This injection action increases complexity of the question. Action 2: Fuzzing blurs certain details in the question to increase the uncertainty level of the question. For example, \"Catskill Mountain Railroad\" could be replaced with \"a historic mountain railway\". Through fuzzing the question multiple times, both the uncertainty level and difficulty of the question would gradually increase. To ensure that synthetic question is of high quality and to precisely evaluate the difficulty, we incorporate rigorous quality verification phase for assessing synthetic questions, Step 1. Basic Quality. We employ an LLM to assess the basic quality of each question. This verification includes checking the clarity of the question and verifying whether the question-answer pair is accurate based on the supporting facts. This quality control step ensures that each question-answer pair is properly grounded in reliable sources. Step 2. Difficulty Measurement. We employ cutting-edge LRM (e.g., QwQ-32B) to generate multiple answers directly for the synthetic question, without using any external tool. This verification process also serves as measure of question difficulty. Step 3. Answer Uniqueness. The fuzzing action may loosen constraints excessively, compromising the uniqueness of the answer. To prevent ambiguity resulting from multiple correct answers, we evaluate whether any of the mismatched answers generated during the Difficulty Measurement step could serve as alternative valid answers. 8 We provide two illustrative examples in Tab. 1. Starting with simple question, the injection action replaces specific entities with related factual details. For instance, Michael P. Hein is expanded to who served as the first County Executive of Ulster County, New York.... The fuzzing action introduces ambiguity by generalizing precise information, replacing the exact year 1934 with the early 1930s or substituting Catskill Mountain Railroad with historic mountain railway. Through iterative injection and fuzzing, the data synthesis agent produces questions that involve complex information and high uncertainty, requiring extensive search and reasoning to find the correct answer. After completing the question synthesis process, we filter out questions that the LRM can directly generate the correct answer without relying on search tools. Since these questions can be answered solely based on the intrinsic knowledge of the model, they provide little value for enhancing search capabilities. Starting with 14,107 seed questions, we perform an average of 6.3 injections and 3.2 fuzzes per question. From the synthetic pool, we select up to three high-quality variations per seed question. This curation process produces final dataset of 25,624 entries, with the selected questions averaging 4.27 injections and 2.10 fuzzes each. 3.3 Asynchronous Agentic RL Training 3.3.1 Challenges of Scaling Up Trajectory Length in RL In this section, we first empirically show that complex tasks require extensive tool calls and therefore RL training with large turn limit is necessary for training advanced search agents. Then we show that variance of trajectory execution time is large during training, which could lead to significant idle time in batch generation RL systems. Figure 6: (Left) Test scaling of ASearcher-Web-QwQ. Data points are obtained by enforcing different minimum turns.The accuracy is averaged over GAIA, xBench-DeepSearch, and Frames. (Middle) Number of tool calls versus training steps. During training time, long trajectories require much more tool calls than short ones. (Right) Number of generated tokens versus training steps. The number of output tokens exhibits significant variance, with long trajectories exceeding short ones by up to two orders of magnitude. Complex Tasks Require Long Trajectories. Agentic tasks often require extensive LLM generations and multiple tool calls to solve complex problems, leading to prolonged trajectory execution time. As shown in Fig. 6(Left), we evaluate our RL-trained QwQ-32B agent on GAIA [24], xBenchDeepsearch [14] and Frames [14], forcing the agent to use tools for different minimal turn numbers. The results demonstrate that accuracy improves with more turns, confirming that complex tasks demand longer trajectories for effective problem-solving. High Variance in Trajectory Execution Time. Long trajectories also introduce significant variance in execution time. We analyze the number of tool calls and token generation during RL training of our QwQ agent  (Fig. 6)  and observe that the longest trajectories can span dozens more tool calls and two orders of magnitude more tokens than shorter ones. This disparity leads to highly unpredictable per-trajectory runtime, further complicating training efficiency. Efficiency Issues of Agentic RL Training. Both prolonged execution and high runtime variance degrade RL training efficiency. We take one-step-off RL training system [21] as representative 9 Figure 7: One-Step-off RL v.s. Fully Asynchronous RL. In batch generation systems, batch should wait for the longest trajectory, leading to significant GPU idle time. In contrast, fully asynchronous RL achieves faster training than batch generation RL by fully decoupling training and trajectory generation, achieving near-full resource utilization for trajectory generation. example for batch generation RL systems. In one-step-off RL training, training for step and trajectory generation for step N+1 are executed concurrently. As shown in Fig. 7, though this system overlaps trajectory rollouts with model training, batch generation remains bottlenecked by the slowest trajectory (e.g., trajectory 7), causing GPU idle time and under-utilization. 3.3.2 Fully Asynchronous RL Training. To ensure efficient agentic RL training, we adopt fully asynchronous training paradigm. Notably, our approach incorporates asynchornization at the two distinct aspects. Asynchronous Trajectory Rollouts. Trajectory rollouts are collected in parallel and do not directly interfere with each other. Each trajectory independently sends tool calling requests to corresponding servers and LLM generation requests to the LLM inference engine. Concurrent requests from different trajectories are automatically handled by the servers. Fully independent trajectory execution ensures trajectory does not need to wait for other trajectories when generating LLM responses and waiting for tool calling responses, thereby improving training efficiency. Decoupled Rollout and Training. Besides asynchronous rollout, trajectory rollouts and model updates are also fully decoupled. In Fig. 7, we compare our fully asynchronous RL training with one-step-off RL training, which utilizes asynchronous rollout within batches. In fully asynchronous RL training, long trajectories do not block generation and can span multiple versions, significantly reducing GPU idle time and achieving near-full GPU utilization during generation. On the training side, training step is launched as soon as sufficient trajectories are collected to form batch. As shown in Fig. 7, the training process does not wait for the extremely long trajectory 7 but instead proceeds with trajectory 9. 3.4 Training Details MDP Formulation. We follow the formulation of Markov Decision Process (MDP). Formally, an MDP is defined by the tuple (S, A, T, R). Here represents the state space, usually containing the history, search results, and retrieved webpages. denotes the action space and an action includes tokens generated by the agent. Some tool calling could be extracted from the action through specific tags, e.g. <search> search query </search>. (ss, a) is the transition probability, where is the updated state after applying the tool calling in action at state s. At each timestep, the agent receives state st and generates an action at with policy π : A. The goal of the agent is to maximize the return J(π) = at π(st) (cid:80) (cid:21) . (cid:20) t=0 R(st, at) (cid:12) (cid:12) (cid:12) (cid:12) 10 GRPO Training. We employ the GRPO [29] algorithm to train search agents. Specifically, for each input question x, trajectories τ1, τ2, , τG are generated where τi = (si ). To optimize the agent, we employ the following loss, 1, , si Ti 0, ai 0, si JGRP O(θ) = xD,{τi}G i=1πθold (x) (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "Ti1 (cid:88) t=0 (cid:32) min ai (cid:88) j= (cid:88) i=1 (cid:32) 1 (cid:80)Ti1 t=0 ai t,jst, ai t,jst, ai πθ(ai πθold (ai t,<j) t,<j) clip πθ(ai πθold (ai (cid:33) t,jst, ai t,jst, ai (cid:33)(cid:35) t,<j) t,<j) ˆAi, (1) , 1 ϵ, 1 + ϵ ˆAi where ϵ is hyperparameter, and ˆAi is the advantage for the i-th trajectory, computed based on the relative rewards of all trajectories within each group. Dynamic Filtering. To enhance training efficiency, we implement dynamic filtering to exclude queries that lack meaningful training signals. Specifically, we remove queries where all responses yield identical rewards (resulting in zero advantages), including both queries where the agent already achieves high accuracy and those with incorrectly labeled answers. Reward Function. For reward function, we adopt sparse-reward setting where rewards are computed at trajectory completion. When training from base LLMs, the reward function combines format reward and F1 score through multiplication. When fine-tuning LRM-based agents (e.g., QwQ), we utilize LLM-as-Judge[20][38] as the reward function and omit format rewards, as these models inherently maintain proper output formatting."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experiment Setup Benchmarks. We first evaluate the agents on single-hop and multi-hop QA tasks. For single-hop questions, we use Natural Questions [15], TriviaQA [12] and PopQA [23]. For multi-hop questions, we use HotpotQA [44], 2WikiMultiHopQA [10], MuSiQue [36], and Bamboogle [28]. We further perform evaluation on more challenging benchmarks including Frames [14], GAIA [24], and xBenchDeepSearch [41] as extra test sets. We evaluate our approach on 1000 randomly sampled instances from the validation sets of HotpotQA, 2WikiMultiHopQA, and MuSiQue. For Bamboogle, Frames, GAIA and xBench-DeepSearch, we use their full test sets. For GAIA, we use the 103 examples from the text-only validation subset [18]. Search Tools. We evaluate the search agents with two settings, each with different types of search tools. In the first setting, local knowledge base with RAG, agents interact with locally deployed RAG system to retrieve related information from Wikipedia 2018 corpus [13]. In the other webbased search and browsing setting, agents operate in an interactive web environment with access to both search engine and browser tool. For more challenging benchmarks, GAIA, xBenchDeepSearch and Frames, we only conduct evaluations under this web-based setting. Baselines We consider two groups of baselines aligned with the two benchmark categories. For the multi-hop and single-hop QA benchmarks, we include Search-R1(7B/14B/32B) [11], R1Searcher(7B) [30], Search-o1(QwQ-32B) [18], DeepResearcher [49] and SimpleDeepSearcher [32]. We also prompt Qwen-2.5-7B/32B to directly generate answers without using any tools. On the more challenging benchmarks, we compare against powerful 32B-scale models, including direct generation with QwQ-32B, Search-o1(QwQ-32B) [18], Search-R1-32B [11], WebThinkerQwQ [19],SimpleDeepSearcher-QwQ [32] and WebDancer-32B [39]. All baselines are evaluated using the same tools as our agent to ensure fair comparison. Evaluation Metrics We adopt two complementary evaluation metrics: F1 score and LLM-as-Judge (LasJ). The F1 score is computed at the word level, measuring the harmonic mean of precision and recall between the predicted and reference answers. For LLM-as-Judge, strong LLM (Qwen2.5-72BInstruct) is prompted to assess the correctness of model outputs according to task-specific instructions. Method Table 2: Results with Local Knowledge Base. Multi-Hop QA Single-Hop QA Avg. 2WikiMQA HotpotQA Bamboogle Musique F1 LasJ F1 LasJ LasJ LasJ F1 NQ F1 LasJ TriviaQA F1 LasJ PopQA F1 LasJ F1 LasJ 7B Models Qwen-2.5-7B Direct Gen. 30.4 54.7 Search-R1-7B 64.0 R1-Searcher-7B 29.4 58.1 67.1 29.2 30.9 37.2 42.4 11.8 11.0 27.9 29.4 50.4 59.8 21.5 20.5 29.8 31.9 57.6 60.8 55.8 58.4 28.2 27.1 58.7 49.9 68.0 78.0 57.3 55.7 54.3 55.4 57.1 61.0 51.8 56.0 28.7 27.3 51.2 49.1 62.0 72.8 50.9 49.5 52.2 54. ASearcher-Local-7B 72.3 77.6 62.6 67.6 55.0 60.0 34.4 32.6 55.6 54.5 68.1 79.3 57.9 55.9 58.0 61.0 14B/32B Models QwQ-32B Direct Gen. Search-R1-14B Search-R1-32B 34.6 48.2 63.1 35.4 49.8 67.5 37.1 40.2 56.9 61.6 16.8 16.1 36.9 38.2 65.4 75.8 27.9 26.3 39.4 41.9 56.2 58.9 52.8 51.2 27.0 25.7 60.0 51.2 71.0 79.9 56.1 54.3 53.0 53.0 60.5 64.0 60.0 61.6 34.4 32.9 60.8 52.2 72.0 82.1 60.3 58.2 58.7 59.8 ASearcher-Local-14B 72.2 79. 65.1 71.0 59.4 64.8 35.6 34.6 56.6 56.1 71.6 84.0 57.6 55.9 59.7 63.6 On GAIA, xBench-DeepSearch and Frames, we only use LLM-as-Judge and report the Avg@4 and Pass@4 scores for all models. Training Details of ASearcher. We set the turn limit as 32 for 7B and 14B models, and 128 for ASearcher-Web-QwQ. The batch size is set as 128 for 7B and 14B models, and 64 for ASearcher-WebQwQ. We curate two sets of training data, one for 7B/14B training and the other for QwQ-32B training. These two datasets are both of 35k sizes and open-sourced. Training of ASearcher-Web-QwQ takes approximated 7.6k H800 GPU hours. 4.2 Main Results We present the main experiment results across three evaluation settings: (1) local knowledge base with retrieval-augmented generation (RAG) on standard QA benchmarks, (2) web-based search and browsing on the same benchmarks, and (3) web-based search and browsing on more challenging benchmarks. ASearcher, instantiated with Qwen2.5-7B, Qwen2.5-14B, and QwQ-32B, consistently outperforms existing opensource agents of the same model scale on both F1 and LasJ metrics. ASearcher-14B achieves the best performance across 7B, 14B, and 32B models on suite of multihop and single-hop QA benchmarks, and ASearcher-QwQ significantly outperforms several strong baselines of comparable size on these challenging benchmarks. These results highlight the generality and scalability of ASearcher across diverse tasks and model sizes. Local Knowledge Base with RAG on Standard QA Benchmarks. As shown in Table 2, ASearcher-Local, trained via reinforcement learning with local knowledge base, achieves the best performance across 7B and 14B on suite of multi-hop and single-hop QA benchmarks. In the 7B setting, ASearcher attains an average F1 of 58.0, outperforming strong baselines such as Search-R1-7B (54.3) and R1-Searcher-7B (52.2). It also achieves LasJ score of 61.0, significantly outperforming Search-R1-7B (55.4) and R1-Searcher-7B (54.7). The gains are even more pronounced at the 14B scale, where ASearcher-Local-14B reaches an F1 of 60.0 and LasJ of 65.6, surpassing even the larger 32B retrieval-based baseline Search-R1-32B. Web-based Search and Browsing on Standard QA Benchmarks In Table 3, we evaluate agents in realistic web-based setting. Notably, we evaluate models trained entirely with local knowledge base in the web setting in zero-shot manner, to directly examine the generalizability of search strategies learned through RL. Across both model sizes, ASearcher consistently outperforms strong baselines. In particular, ASearcher-Web-14B achieves the best performance with an average F1 of 61.5, surpassing SimpleDeepSearcher, the strongest 32B baseline in this setting. Remarkably, ASearcher-Local-14B model exhibits strong generalization when tested in the web-based setting, achieving significant gains over all baseline models of similar or larger size in terms of LasJ. This confirms that ASearcher learns generalizable search strategies that transfer to different sources of information. 12 Table 3: Results with Web-based Search and Browsing. Method Training Setting Multi-Hop QA Single-Hop QA Avg. 2WikiMQA HotpotQA Bamboogle Musique F1 F1 LasJ F1 LasJ LasJ NQ TriviaQA PopQA F1 LasJ F1 LasJ F1 LasJ F1 LasJ F1 LasJ 7B Models Qwen-2.5-7B Direct Gen. Search-R1-7B R1-Searcher-7B DeepResearcher-7B Simple DS-7B ASearcher-Local-7B ASearcher-Web-7B QwQ-32B Direct Gen. Search-o1 (QwQ-32B) Search-R1-14B Search-R1-32B Simple DS-QwQ ASearcher-Local-14B ASearcher-Web-14B - local local web web local web - - local local web local web 30.8 30.9 28.6 29.5 37.2 39.6 10.6 1.9 29.6 29.9 51.2 59.3 19.8 17.4 29.7 29.8 58.9 64.8 59.0 62.8 66.3 73.6 29.4 25.4 58.4 51.1 73.1 84.1 53.0 51.3 56.9 59.0 66.6 69.4 56.8 61.6 62.8 72.0 28.7 25.3 49.6 48.7 67.6 79.5 46.5 45.2 54.1 57.4 61.0 64.1 57.1 61.0 68.8 76.8 26.8 24.5 52.0 52.9 70.0 82.8 48.9 45.7 54.9 58.3 67.4 73.9 57.6 62.5 61.5 72.0 26.4 26.2 43.9 53.1 73.9 85.4 43.7 48.8 53.5 60. 69.1 75.5 61.6 67.1 66.2 76.0 33.3 30.7 54.7 53.7 75.2 87.3 52.9 49.7 59.0 62.9 67.5 73.3 61.7 67.2 66.4 72.0 32.9 29.6 55.2 55.4 74 85.7 52.4 48.9 58.6 61.7 14B/32B Models 33.7 33.4 39.1 42.1 56.9 57.9 18.8 19.3 37.8 43.0 63.8 74.2 25.9 24.5 39.4 42.1 68.9 77.8 58.4 65.3 68.6 82.4 31.8 33.5 43.1 57.2 76.3 89.6 43.2 48.3 55.8 64.9 51.8 53.8 55.3 58.6 67.4 75.2 29.8 26.9 57.7 49.6 74.4 83.9 51.0 49.8 55.4 56.8 63.7 69.3 60.3 64.2 76.4 81.6 33.0 30.8 58.6 51.1 76.2 86.6 55.0 53.6 60.4 62.5 71.7 80.4 62.0 67.5 73.2 83.2 33.3 32.9 45.7 55.3 77.2 90.2 45.5 47.8 58.4 65.3 70.4 79.8 63.6 70.5 68.7 80.8 35.1 33.8 53.5 55.4 76.1 88.5 52.5 50.5 60.0 65.6 76.1 80.7 63.5 68.5 69.9 75.2 36.6 33.7 56.0 55.5 75.4 87.6 52.9 50.0 61.5 64.5 Table 4: Results on GAIA, xBench-DeepSearch, and Frames. The results are evaluated with LLMas-Judge. For baselines, we run the corresponding official codes for 4 seeds and report Avg@4 and Pass@4. Method QwQ-32B Direct Gen. Search-o1 (QwQ) Search-R1-32B WebThinker-QwQ Simple DS-QwQ WebDancer-QwQ ASearcher-Web-QwQ Avg@4 23.1 48.1 28.6 42.5 47.6 47.4 52.8 GAIA Pass@4 Avg@4 xBench-DeepSearch Pass@4 31.1 67.0 43.7 57.3 64.1 61.2 70.1 11.8 40.3 19.5 32.8 35.8 40.0 42. 23.0 65.0 37.0 52.0 61.0 68.0 68.0 Frames Avg@4 Pass@4 29.9 63.6 44.1 57.7 67.0 63. 70.9 39.9 81.1 61.0 79.5 82.2 81.4 84.0 Web-based Search and Browsing on Challenging Benchmarks. Table 4 shows experiment results on challenging QA tasks that require advanced problem-solving capabilities and search strategies. These benchmarks are specifically designed to assess the agents ability to interact with real web and retrieve up-to-date information that often go beyond the internal knowledge of LLMs. As result, direct generating answers from models (e.g., QwQ-32B) perform poorly across all datasets.Our agent, ASearcher-Web-QwQ, achieves the best Avg@4 scores on GAIA (52.8) and xBench-DeepSearch (42.1), outperforming previous state-of-the-art open-source agents. These results further highlight superiority in handling long-horizon planning, real-world tool use, and open-domain exploration. Besides Avg@4, we also report the Pass@4 score that computes the ratio of questions that an agent finds the correct answer out of 4 trials. ASearcher-Web-QwQ also outperforms state-of-the-art open-source agents in terms of pass rate. Effect of RL Training. As shown in Fig. 8, ASearcher-Web-QwQ obtains +9.1, +13.4, and +12.0 improvements on GAIA, xBench-DeepSearch and Frames respectively. When considering the pass rate, i.e. Pass@4, ASearcher-Web-QwQ also obtains significant gains, especially on xBenchDeepSearch with 17.0 improvements. Significant improvements in pass rate demonstrate that our training pipeline trains the agent to learn complex search strategies to perform precise searches, extract key information, and resolve conflict information. 4.3 Training Dynamics Training Dynamics of ASearcher-Local-7B/14B. In Fig. 9 and Fig. 10, we plot the number of generated tokens, search queries and webpage browsing for ASearcher-Local-7B and ASearcher13 Figure 8: Comparison of the performance of QwQ-32B agent before and after RL Training. (a) Generated Tokens (b) Search Queries (c) URL Accesses Figure 9: Training Dynamics of ASearcher-Local-7B. Local-14B training, respectively. With our training recipe, length increment and the increase in the number of tool callings are observed in both 7B and 14B scales. Notably, the number of search queries scale up to 6, which is higher than the numbers reported by prior works [11, 30]. Interestingly, we find that the 7B model fails to learn valid webpage browsing while the 14B model can learn to access webpage to solve challenging questions in the later stage of training. We hypothesize that the failure of 7B model in learning webpage browsing occurs because the model capacity is too small to stably learning summarize lengthy webpages in zero RL training setting. Training Dynamics of ASearcher-Web-QwQ. Similarly, the training dynamics of ASearcherWeb-QwQ are illustrated in Fig. 6. As the training progresses, the agent learns to perform more tool calls, reaching maximum of around 40 calls at the 200th step, with peak instances even achieving up to 70 calls. Also the QwQ-32B agent generates more tokens through training, with maximum of over 150k tokens. This scaling trend in both tool utilization and output length highlights the potential of fully asynchronous RL training for complex real-world agent applications. (a) Generated Tokens (b) Search Queries (c) URL Accesses Figure 10: Training Dynamics of ASearcher-Local-14B."
        },
        {
            "title": "5 Related Works",
            "content": "Search Agents. Some works have constructed agent workflows that enable large language models (LLMs) to leverage external tools for solving complex tasks, with notable examples include Search-o1[18] and ReAgent[48]. Prompt-based methods, while effective for rapid development, are fundamentally limited by the capacity of the underlying LLMs and could not be reliably improved with environment feedback. Some works attempt to construct SFT trajectories for LLMs. For instance, [4, 47] leverage large LLMs to synthesize retrieval and reasoning trajectories to fine-tune smaller models. Recently, some works investigate Reinforcement learning (RL) methods to enhance the LLM-based agents, mostly focusing on multi-hop QA benchmarks such as HotpotQA and 2WikiMultihop. [11, 30, 5, 49] perform RL training with multi-hop QA data and observe an increase in the number of tool uses. RAG-R1 [33] further combines SFT and RL to enhance the search strategies. More recently, researchers have begun to focus on more challenging tasks, by fine-tuning sophisticated prompt-based agents powered by Large Reasoning Models through offline RL [19], SFT on simulated trajectories with real-world web data [32, 17], and constructing challenging QAs for RL training. [34]. Synthetic Data for Search Agents. Rather than relying solely on large-scale human annotation, data synthesis has emerged as scalable approach to prepare training data for search agents. Recent approaches generate synthetic but realistic QA trajectories by interacting with real web pages and curating data using LRMs [32, 39, 17]. On the other hand, WebSailor [17] constructs structurally challenging tasks through sampling and fuzzing, and WebShaper [34] utilizes techniques from set theory to construct high-quality complex QAs. By contrast, ASearcher develops an autonomous LLM agent for synthesizing challenging QAs with high uncertainty, without relying on complex knowledge graphs. Both the data synthesis agent and the synthetic training data in ASearcher are fully open-sourced."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we present ASearcher, open-source project for large-scale RL training. Our contribution includes fully asynchronous agentic RL training system and data synthesis agent for large-scale high-quality QA construction. By instantiating ASearcher with base LLMs including Qwen2.5-7B/14B and prompt-based LLM agents based on QWQ-32B, ASearcher outperforms the state-of-the-art open-source agents across different model sizes and evaluation settings. With fully asynchronous agentic RL training and insight from our data synthesis pipeline, we hope our work could benefit future work on training advanced agents for broader range of applications."
        },
        {
            "title": "References",
            "content": "[1] Moonshot AI. Kimi-researcher: End-to-end rl training for emerging agentic capabilhttps://moonshotai.github.io/Kimi-Researcher/, 2025. URL https:// ities. moonshotai.github.io/Kimi-Researcher/. [2] Salaheddin Alzubi, Creston Brooks, Purva Chiniya, Edoardo Contente, Chiara von Gerlach, Lucas Irwin, Yihan Jiang, Arda Kaz, Windsor Nguyen, Sewoong Oh, et al. Open deep search: Democratizing search with open-source reasoning agents. arXiv preprint arXiv:2503.20201, 2025. [3] Chenxin An, Zhihui Xie, Xiaonan Li, Lei Li, Jun Zhang, Shansan Gong, Ming Zhong, Jingjing Xu, Xipeng Qiu, Mingxuan Wang, and Lingpeng Kong. Polaris: post-training recipe for scaling reinforcement learning on advanced reasoning models. URL https://hkunlp. github.io/blog/2025/Polaris. [4] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Selfreflective retrieval augmented generation. In NeurIPS 2023 workshop on instruction tuning and instruction following, 2023. [5] Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Pan, Wen Zhang, Huajun Chen, Fan Yang, et al. Learning to reason with search for llms via reinforcement learning. arXiv preprint arXiv:2503.19470, 2025. 15 [6] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning, 2021. URL https://arxiv.org/abs/2004. 07219. [7] Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, Tongkai Yang, Binhang Yuan, and Yi Wu. Areal: large-scale asynchronous reinforcement learning system for language reasoning, 2025. URL https: //arxiv.org/abs/2505.24298. [8] Google Team. Introducing Gemini deep research, 2025. URL https://gemini.google/ overview/deep-research/. Accessed: 2025-04-06. [9] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [10] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multi-hop qa dataset for comprehensive evaluation of reasoning steps, 2020. URL https: //arxiv.org/abs/2011.01060. [11] Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. [12] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. [13] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick SH Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In EMNLP (1), pages 67696781, 2020. [14] Satyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam Upadhyay, and Manaal Faruqui. Fact, fetch, and reason: unified evaluation of retrieval-augmented generation, 2024. URL https://arxiv.org/abs/2409.12941. [15] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: benchmark for question answering research. Trans. Assoc. Comput. Linguistics, 7:452466, 2019. doi: 10.1162/tacl_a_00276. URL https: //doi.org/10.1162/tacl_a_00276. [16] Jiazheng Li, Hong Lu, Kaiyue Wen, Zaiwen Yang, Jiaxuan Gao, Hongzhou Lin, Yi Wu, and Jingzhao Zhang. Questa: Expanding reasoning capacity in llms via question augmentation. arXiv preprint arXiv:2507.13266, 2025. [17] Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, et al. Websailor: Navigating super-human reasoning for web agent. arXiv preprint arXiv:2507.02592, 2025. [18] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint arXiv:2501.05366, 2025. [19] Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability. arXiv preprint arXiv:2504.21776, 2025. [20] Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, WeiarXiv preprint wei Deng, Feng Sun, and Qi Zhang. Calibrating llm-based evaluator. arXiv:2309.13308, 2023. 16 [21] Michael Luo, Sijun Tan, Roy Huang, Ameen Patel, Alpay Ariyak, Qingyang Wu, Xiaoxiang Shi, Rachel Xin, Colin Cai, Maurice Weber, Ce Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepcoder: fully open-source 14b https://pretty-radio-b75.notion.site/ DeepCoder-A-Fully-Open-Source-14B-Coder-at-O3-mini-Level-1cf81902c14680b3bee5eb349a512a51, 2025. Notion Blog. o3-mini coder level. at [22] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl. https://pretty-radio-b75.notion.site/ DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2, 2025. Notion Blog. [23] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, and Daniel Khashabi. When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories. arXiv preprint, 2022. [24] Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations, 2023. [25] OpenAI. Openai deep research. https://openai.com/index/ introducing-deep-research/. [26] OpenAI. Introducing deep research, 2025. URL https://openai.com/index/ introducing-deep-research/. Accessed: 2025-04-06. [27] Perplexity Team. URL https:// Introducing Perplexity deep research, 2025. www.perplexity.ai/hub/blog/introducing-perplexity-deep-research. Accessed: 2025-04-06. [28] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022. [29] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [30] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592, 2025. [31] Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and Bill Yuchen Lin. Trial and error: Exploration-based trajectory optimization for llm agents, 2024. URL https://arxiv.org/ abs/2403.02502. [32] Shuang Sun*, Huatong Song*, Yuhao Wang, Ruiyang Ren, Jinhao Jiang, Junjie Zhang, Lei Fang, Zhongyuan Wang, and Ji-Rong Wen Wayne Xin Zhao. Simpledeepsearcher: Deep information seeking via web-powered reasoning trajectory synthesis. 2025. URL https: //github.com/RUCAIBox/SimpleDeepSearcher. [33] Zhiwen Tan, Jiaming Huang, Qintong Wu, Hongxuan Zhang, Chenyi Zhuang, and Jinjie Gu. Rag-r1: Incentivize the search and reasoning capabilities of llms through multi-query parallelism. arXiv preprint arXiv:2507.02962, 2025. [34] Zhengwei Tao, Jialong Wu, Wenbiao Yin, Junkai Zhang, Baixuan Li, Haiyang Shen, Kuan Li, Liwen Zhang, Xinyu Wang, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. Webshaper: Agentically data synthesizing via information-seeking formalization, 2025. URL https://arxiv.org/abs/2507.15061. 17 [35] Prime Intellect Team, Sami Jaghouar, Justus Mattern, Jack Min Ong, Jannik Straube, Manveer Basra, Aaron Pazdera, Kushal Thaman, Matthew Di Ferrante, Felix Gabriel, et al. Intellect-2: reasoning model trained through globally decentralized reinforcement learning. arXiv preprint arXiv:2505.07291, 2025. [36] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554, 2022. [37] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345, 2024. [38] Minzheng Wang, Longze Chen, Cheng Fu, Shengyi Liao, Xinghua Zhang, Bingli Wu, Haiyang Yu, Nan Xu, Lei Zhang, Run Luo, et al. Leave no document behind: Benchmarking long-context llms with extended multi-doc qa. arXiv preprint arXiv:2406.17419, 2024. [39] Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Gang Fu, Yong Jiang, et al. Webdancer: Towards autonomous information seeking agency. arXiv preprint arXiv:2505.22648, 2025. [40] Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, et al. Webwalker: Benchmarking llms in web traversal. arXiv preprint arXiv:2501.07572, 2025. [41] Xbench-Team. Xbench-deepsearch, 2025. URL https://xbench.org/agi/aisearch. [42] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: survey. Science China Information Sciences, 68(2):121101, 2025. [43] Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao Yu, and Yi Wu. Is dpo superior to ppo for llm alignment? comprehensive study, 2024. URL https://arxiv.org/abs/2404.10719. [44] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. [45] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. [46] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [47] Tian Yu, Shaolei Zhang, and Yang Feng. Auto-rag: Autonomous retrieval-augmented generation for large language models. arXiv preprint arXiv:2411.19443, 2024. [48] Xinjie Zhao, Fan Gao, Xingyu Song, Yingjian Chen, Rui Yang, Yanran Fu, Yuyang Wang, Yusuke Iwasawa, Yutaka Matsuo, and Irene Li. Reagent: Reversible multi-agent reasoning for knowledge-enhanced multi-hop qa. arXiv preprint arXiv:2503.06951, 2025. [49] Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv preprint arXiv:2504.03160, 2025."
        },
        {
            "title": "A Full Case Study",
            "content": "In this section, we provide detailed case study on an extremely challenging question from GAIA [24]. Specifically, we analyze Search-R1-32B [11] and Search-o1 (QwQ) [18] in Fig. 11. 18 Figure 11: case study on complex query from GAIA. Search-R1-32B is unable to break down the complex question and has severe hallucinations. Search-o1 (QwQ) can identify the corrects articles through extensive tool calls, but easily misses key information and fails to verify wrong conclusions. Our end-to-end RL agent, ASearcher-Web-QwQ, exhibits key behaviors featuring Search Intelligence: uncertainty-aware reasoning (list and examine candidate answers), precise extraction from noisy contents, cross-document inference, and rigorous confirmation. 19 Solution Path of the Sample Question. In Fig. 11, our case study is carried out on question requiring finding some specific animal given 2 conditions and 4 unknown variables. To identify the correct answer, the search agent should first find out the mentioned species U1 according to condition C1, identify the correct article U2 that satisfies condition C2, and then find out the papers listed in U3.1 and U3.2. Finally, the correct answer should be determined by cross referencing the article U2 and the papers U3.1&U3.2. To summarize, this example is challenging for several main reasons, High Uncertainty: The question involves multiple unknown variables that could point to many different entities. For example, the 2021 article U2 could point to any article published in 2021 and could only be determined given the condition C2 and the alvei species U1. Requirement for Exact Information Extraction: To find the answer, the agent should list all animals mentioned on the webpages and making cross-document comparison. This would require the agent to precisely extract key information from the vast, noisy web contents, instead of simply summarizing the webpages. Misleading Answers: During the process of solving this task, there could be multiple misleading answers, such as \"pigs\". The agent should rigorously confirm its conclusions by checking the intended answer in all related webpages and documents. Existing Online RL Approaches Fail to Learn Complex Search Strategies. In Fig. 11, SearchR1-32B is not able to decompose the complex query into individual components, consequently only making redundant queries that involve too many unknown information. The agent also has severe hallucinations, producing conclusions that are not supported by the search results. Finally, it fails to resolve all unknown variables. This case study shows that existing online RL approaches only incentivize elementary search strategies. It is also worth noting that, since the turn limit is set as small value, e.g. 4, during training, the model only exhibits short tool-use horizon. Prompt-based LLM Agents Could Fail Due to Insufficient Capability of the LLM. In Fig. 11, Search-o1 (QwQ) can find the species name U1, as well as the 2021 article U2 and papers U3.1&U3.2 through large amount of tool calls. However, when trying to find the answer, Search-o1 (QwQ) would easily miss key information. Consequently, the agent makes incorrect conclusions. Notably, even when the agent finds information that directly links to the correct answer, it is still misguided by previous incorrect conclusions. Finally, the agent is unable to verify the correctness of previous conclusions. This case study reveals that, though an open-source model that is not explicitly trained on agentic tasks can perform extensive tool calls, it could not make expert-level reasoning based on the retrieved contents and history contexts. ASearcher-Web-QwQ. We also analyze the search strategy of our end-to-end RL agent, ASearcherWeb-QwQ.As shown in Fig. 11, ASearcher-Web-QwQ decomposes the complex query into precise and focused queries. Unlike Search-o1 (QwQ) that visits large amount of websites after each search query, ASearcher-Web-QwQ focuses on visiting the most relevant website. ASearcher-Web-QwQ summarizes all related information from website. Specifically, all candidate answers are listed and carefully analyzed by the agent. When trying to search for related facts in the papers U3.1&U3.2, the agent explicitly references the key information. When the search results do not directly point to the desired target, e.g. when searching with Olga Tapia (U3.2) Hafnia alvei (U1) animal studies to find the animals related to Olga Tapias paper, the agent does not get clear information but is able to infer the correct answer by make connection with the other paper U3.1. After the correct answer Mice is found, the agent spends further turns on confirming previous conclusions before reporting the final answer. In summary, ASearcher successfully train search agent that exhibits complex behaviors that feature Search Intelligence, Uncertainty-aware reasoning: the agent exhaustively lists and examines all possibilities for uncertain entities Price Key Information Extraction: the agent is able to identify the key information from vast, noisy web contents. Cross-document Inference: the agent is able to infer critical conclusions by making connections among multiple documents. Rigorous Confirmation: the agent verifies the correctness of previous conclusions with additional tool calls."
        }
    ],
    "affiliations": [
        "Ant Research, RL Lab",
        "IIIS, Tsinghua University",
        "University of Washington"
    ]
}