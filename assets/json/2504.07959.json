{
    "paper_title": "CCMNet: Leveraging Calibrated Color Correction Matrices for Cross-Camera Color Constancy",
    "authors": [
        "Dongyoung Kim",
        "Mahmoud Afifi",
        "Dongyun Kim",
        "Michael S. Brown",
        "Seon Joo Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Computational color constancy, or white balancing, is a key module in a camera's image signal processor (ISP) that corrects color casts from scene lighting. Because this operation occurs in the camera-specific raw color space, white balance algorithms must adapt to different cameras. This paper introduces a learning-based method for cross-camera color constancy that generalizes to new cameras without retraining. Our method leverages pre-calibrated color correction matrices (CCMs) available on ISPs that map the camera's raw color space to a standard space (e.g., CIE XYZ). Our method uses these CCMs to transform predefined illumination colors (i.e., along the Planckian locus) into the test camera's raw space. The mapped illuminants are encoded into a compact camera fingerprint embedding (CFE) that enables the network to adapt to unseen cameras. To prevent overfitting due to limited cameras and CCMs during training, we introduce a data augmentation technique that interpolates between cameras and their CCMs. Experimental results across multiple datasets and backbones show that our method achieves state-of-the-art cross-camera color constancy while remaining lightweight and relying only on data readily available in camera ISPs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 9 5 9 7 0 . 4 0 5 2 : r CCMNet: Leveraging Calibrated Color Correction Matrices for Cross-Camera Color Constancy Dongyoung Kim1 Mahmoud Afifi2 Dongyun Kim1 Michael S. Brown2 Seon Joo Kim1 1Yonsei University 2AI Center - Toronto, Samsung Electronics {dongyoung.kim,dongyunkim,seonjookim}@yonsei.ac.kr {m.afifi1,michael.b1}@samsung.com Project page: https://www.dykim.me/projects/ccmnet Figure 1. This paper introduces CCMNet, framework for cross-camera color constancy. CCMNet uses pre-calibrated color correction matrices (CCMs) from camera ISP hardware to train an encoder that generates camera fingerprint embedding (CFE), capturing the testing cameras color space. In (A), we show raw image from Canon 550D. In (B), we present C5 [6], which generalizes using randomly selected unlabeled images from the test cameraC5s performance varies depending on the image set. In (C), we show our results, relying only on fixed CCMs in the ISP. Neither method used Canon 550D data during training. Gamma correction was applied for visualization."
        },
        {
            "title": "Abstract",
            "content": "Computational color constancy, or white balancing, is key module in cameras image signal processor (ISP) that corrects color casts from scene lighting. Because this operation occurs in the camera-specific raw color space, white balance algorithms must adapt to different cameras. This paper introduces learning-based method for crosscamera color constancy that generalizes to new cameras without retraining. Our method leverages pre-calibrated color correction matrices (CCMs) available on ISPs that map the cameras raw color space to standard space (e.g., CIE XYZ). Our method uses these CCMs to transform predefined illumination colors (i.e., along the Planckian locus) into the test cameras raw space. The mapped illuminants are encoded into compact camera fingerprint embedding (CFE) that enables the network to adapt to unseen cameras. To prevent overfitting due to limited cameras and CCMs during training, we introduce data augmentation technique that interpolates between cameras and their CCMs. Experimental results across multiple datasets and backbones show that our method achieves state-of-the-art cross-camera color constancy while remaining lightweight and relying only on data readily available in camera ISPs. 1. Introduction Computational color constancy ensures that object colors remain consistent under varying lighting conditions [10]. In digital cameras, this is achieved through white balancing, which adjusts raw image colors to simulate neutral lighting [11, 20, 31]. This involves two main steps: illuminant estimation and linear white-balance correction [21]. Illuminant estimation predicts the color of the scenes light source under the assumption of single-source illumination [31]. The estimated illuminant color is then used in linear white-balance correction to counteract the effects of lighting and camera response biases [17, 26]. These steps are applied early in the image signal processor (ISP) pipeline to raw images [4] and are influenced by the cameras sensor-specific characteristics, such as response functions and lens properties [2, 52]. These factors complicate the generalization of illuminant estimation algorithms across cameras with varying characteristics [3]. Recent work on illuminant estimation achieves promising results using learning-based models [7, 30, 48, 65]. These models learn mapping between input image colors and scene illuminant colors using pairs of images and corresponding ground-truth illuminant colors, typically captured by the same camera used in testing [53]. Consequently, most learning-based methods struggle to generalize to new cameras with different characteristics than those used during training [3]. This limitation hinders their practical applicability in manufacturing, as fine-tuning or retraining is necessary for each new camera introduced. Some recent attempts have proposed solutions for improved adaptation through few-shot learning [63], or by using additional unlabeled images captured by the testing camera at inference time to facilitate generalization to the testing cameras color space [6]. While these methods show promising results, they require capturing new images with the testing camera for adaptation [46], making their performance inherently dependent on the characteristics of those images [6]. While camera ISPs rely on pre-calibrated, cameraspecific information to assist in color processing after white balance has been applied, to the best of our knowledge, no prior work has leveraged this calibrated information for the cross-camera color constancy task. Specifically, consumergrade ISPs rely on calibrated color correction matrices (CCMs) to transform the cameras raw color space to device-independent standard color space (e.g., CIE XYZ). These CCMs are carefully calibrated during ISP manufacturing [41], are easily accessible within the ISPs firmware [17, 21, 33], and are also available in DNG files for postcapture raw rendering [39]. The availability of this information motivated us to utilize this calibrated data to improve cross-camera generalization. Contribution In this paper, we present CCMNet, learning-based method for cross-camera color constancy built on the convolutional color constancy (CCC) framework [6, 12, 13, 37]. Our method leverages pre-calibrated color correction matrices (CCMs) available from camera ISPs to transform predefined illuminant colors along the Planckian locus from the device-independent CIE XYZ color space into the raw space of the test camera. These transformed illuminations encode the unique characteristics of the cameras response function and serve as reference points. The transformed illuminations are compressed into an 8-dimensional embedding, allowing learnable hypernetwork to adapt to the test cameras raw color space and generate camera-specific CCC model tailored to the input image. Additionally, we introduce an augmentation technique that maps training images from limited set of cameras to imaginary raw spaces, improving generalization. Consequently, CCMNet, which combines design that dynamically adapts to the raw space of various cameras with robust data augmentation strategy, accurately estimates illuminant colors for cameras unseen during training (see Fig. 1-C). Our approach is lightweight, accurate, and requires no additional test camera images, unlike prior work [6]. 2. Related Work cameras ISP includes several color processing modules applied in pipeline fashion. One of the early-stage modules corrects the colors of the captured raw image through (1) image white balancing two key steps [17, 26, 40]: (Sec. 2.1) and (2) transferring the camera raw colors to standard color space via CCMs (Sec. 2.2). 2.1. Auto White Balance As discussed in Sec. 1, auto white balance modules consist of two steps: illuminant estimation and correction. The correction is applied to the linear raw image colors, often using diagonal correction matrix [10]. Most research focuses on illuminant estimation, which determines the scenes illuminant color in the cameras raw space. This can be categorized into learning-free (statistical) methods (e.g., [18, 19, 23, 32, 45, 55, 56, 60, 61]) and learning-based methods (e.g., [7, 12, 36, 48, 59]). Statistical-based methods rely on specific hypotheses (e.g., gray-world [18], gray-edges [61], etc.) and use statistics derived from the input raw image colors to estimate the illuminant color. As result, they inherently generalize across different cameras. However, these methods often have limited accuracy and may fail in scenarios where the scenes illuminant color cannot be reliably inferred from the captured image. Learning-based methods (e.g., [12, 13, 16, 29, 30, 36, 47, 48, 53, 54, 58, 64, 65, 67]) improve accuracy by training models to map raw colors to illuminant colors. However, most fail to generalize to unseen cameras [3]. Some approaches attempt adaptation via meta-learning and fewshot learning [50], assuming access to range of illuminant colors from the testing camera [34, 68], or creating generic methods that require fine-tuning on the new camera [14]. Among these efforts, our work falls into category of methods designed to achieve adaptation without requiring paired set of images from the test camera, even if that set is small. To this end, the work in [3] (termed SIIE) maps input raw images from different cameras to learnable working space, reducing disparities in raw color spaces before illuminant estimation. However, this method assumes access to diverse range of training cameras to effectively learn this mapping, making its accuracy dependent on the variability of the training data. More recently, C5 [6] utilizes additional images captured by the test camera during inference to dynamically generate CCC model [12, 13]. While this method achieves promising results, its accuracy heavily depends on the characteristics of the additional images provided (see Fig. 1-B). In contrast, our method leverages static set of predefined guidance colors along with pre-calibrated data from the test camera, enabling consistently high accuracy without requiring additional images from the test camera. 2.2. Color Space Transfer via CCMs Camera sensors exhibit unique spectral sensitivity and bias, resulting in each camera having its own native RGB color space. Camera ISP manufacturers calibrate and apply color correction matrices (CCMs) to facilitate image processing, ensuring an appropriate transformation between the native RGB space and device-independent standard color space (e.g., CIE XYZ) within the imaging pipeline [15, 40]. Although the transformation between the cameras raw space and standard color space is often nonlinear [24, 25, 35, 38], cameras primarily rely on linear transformation matrices due to their simplicity and practical benefits [24]. CCMs are typically calibrated by fitting 3 3 matrix that maps the raw RGB values of calibration object (e.g., color chart) to their corresponding values in standard color space under an illuminant with specific correlated color temperature (CCT) [39, 41]. To accommodate diverse lighting conditions, CCMs are precomputed for at least two illuminants (typically for low and high CCTs [51]) and interpolated for intermediate conditions (see Fig. 2). CCMs serve as the critical link between cameras unique color characteristics and standard color space. While most CCMs are designed to transform whitebalanced camera raw-RGB to CIE XYZ, some types of CCMs within the ISP operate in the reverse direction, mapping observed CIE XYZ under specific illuminant back to the cameras native raw-RGB space, as shown in Fig. 2-A. This inverse transformation, in particular, provides insight into how various illuminants are represented in the native raw-RGB space. By leveraging this transformation, we can approximate the color trajectories of illuminants in the rawRGB domain across range of CCTs. We leverage this property of CCMs as bridge to introduce novel illuminant estimation method that adapts to the color space of unseen cameras. While previous studies have leveraged CCMs for data augmentation [6], none have explored their use during inference to improve illuminant estimation across different cameras. Additionally, we introduce data augmentation strategy that exploits the linearity of CCMs to enhance generalization. Specifically, we propose technique to generate imaginary camera images with corresponding CCMs, further improving the robustness and adaptability of our model. 3. Method 3.1. Preliminary Auto White Balance Formulation. Assuming single global illumination, given linear raw image, I, is formed as the element-wise product of its white-balanced counterpart, , and the global illuminant RGB color vector, ℓ, at every pixel location x. This can be mathematically deFigure 2. Example of CCM calibration (A) and application (B). CCMs are calibrated to transform between CIE XYZ chromaticity and camera-specific raw-RGB values under standard illuminants with predefined color temperatures (e.g., 2856K, 6504K). For other illuminants, the calibrated CCMs are interpolated. As result, CCMs reflect the cameras unique color characteristics, capturing how the camera perceives illuminants along the color temperature trajectory. scribed as follows: = ℓ x. (1) The conventional goal of the auto white-balance task is to optimize model that estimates the illumination RGB from given raw image I: [ˆℓR, ˆℓG, ˆℓB]T = (I). (2) Convolutional Color Constancy (CCC). As shown in the upper flow of Fig. 3-A, our method is fundamentally based on the CCC framework [12, 13], which transforms the image histogram into an illuminant heatmap , using filter and bias B. CCC reformulates the illumination estimation problem as coordinate localization task on logchroma histogram [22], commonly termed uv-histogram. Specifically, for an images RGB pixel [IR, IG, IB], the logchroma values, and v, are calculated as follows (pixel coordinate is omitted for simplicity): Iu = log(IG/IR), Iv = log(IG/IB). (3) After that, uv-histogram can be generated as follows: (u, v) = (cid:88) x2 [I u ϵ x v ϵ], (4) where ϵ is the width of the histogram bin and x2 is the weighting factor for each pixel, defined as the L2 norm of the raw RGB values of the pixel. In other words, the value of (u, v) represents the weighted count of pixels in image that fall within certain range (ϵ) around the point (u, v). The goal of CCC is to optimize global filter and bias B, to predict probability map of the illumination within the histogram space using the following equation: = σ(B + (cid:88) (Ni Fi)), (5) 3 Figure 3. Overview of the CCMNet architecture. (A) Based on CCC [12] and C5 [6], CCMNet includes network that generates filters and bias from the uv-histograms of the input image. To process query images from diverse camera domains with varying spectral sensitivities, CCMNet uses camera fingerprint embedding (CFE) as guidance. (B) The CFE for three example cameras (A, B, V)two real (A, B) and one imaginary (V)is constructed by mapping predefined illuminants (2500K7500K along the Planckian locus) from the CIE XYZ space to each cameras native raw RGB space using calibrated CCMs. These values are converted into 64 64 histogram and encoded into 1D vector via lightweight encoder. where and have the same shape as the uv-histogram N, represents the convolution operation (accelerated using fast Fourier transforms), σ represents the softmax operation over the uv-coordinate space, and the subscript denotes the index corresponding to the filter and histogram. Here, = 0 refers to the original raw image, while 1 corresponds to augmented images (e.g., texture, edge). We can interpret as heatmap of confidence for each u, coordinate, so the final prediction (ˆℓu, ˆℓv) is expressed as weighted sum of the coordinates using : ˆℓu = (cid:88) u,v uP (u, v), ˆℓv = (cid:88) u,v vP (u, v). (6) The final RGB illumination estimate, ˆℓ, is obtained by inverting the transformation in Eq. (3): ˆℓ = (cid:104) exp (cid:17) (cid:16) ˆℓu , 1, exp (cid:17)(cid:105) (cid:16) ˆℓv , (7) where the green channel is assumed to be G=1. Alternatively, ˆℓ can be normalized to ensure that the vector has unit length. The training objective of CCC is to optimize the filter and bias to minimize the angular error between the predicted illumination RGB and the ground truth illumination in the training dataset. For cross-camera color constancy, C5 [6] proposes hypernetwork version of CCC that dynamically generates and for the test image after analyzing histograms of additional images taken by the same camera. 3.2. CCMNet The proposed CCMNet framework is built upon C5 [6]. As mentioned above, C5 is hypernetwork version of CCC [12, 13], where the network dynamically generates filters and bias. However, unlike C5, CCMNet does not require additional images from the target camera (typically 68). Instead, our method leverages two pre-calibrated Color Correction Matrices (CCMs) for low and high correlated color temperatures (CCTs) embedded within the ISP (see Fig. 3A). These CCMs provide stable guidance via Camera Fingerprint Embedding (CFE) (see Fig. 3-B), ensuring consistent performance across diverse camera domains without extra test images. The core formulation of CCMNet is as follows: {F0, F1, B} = CCMNet(N0, N1, CCMlow, CCMhigh), (8) where N0 and N1 denote the original raw image and its edge-augmented counterpart, CCMlow and CCMhigh are pre-calibrated matrices corresponding to low and high CCTs (typically 2500K and 6500K). The outputs F0, F1, and B, generated by CCMNet, are used to estimate the final uv coordinate of the illumination through Eqs. (5)(7). In Sec. 3.3, we introduce Camera Fingerprint Embedding (CFE), method for extracting device-specific guidance features using CCMs. Additionally, in Sec. 3.4, we propose an imaginary camera augmentation technique to mitigate overfitting to the limited number of cameras and CCMs used during training. These strategies (CFE and augmentation) enhance CCMNets ability to generalize across diverse spectral sensitivities, achieving state-of-the-art performance in cross-camera color constancy tasks. 3.3. Camera Fingerprint Embedding Cross-camera color constancy aims to estimate the chromaticity of the light source while adapting to unseen sensor domains. To this end, we introduce device-aware guidance feature called Camera Fingerprint Embedding (CFE). CFE encodes the color trajectory of light sources observed by each camera within specific color temperature range into an 8-dimensional vector. As result, it inherently captures each cameras unique color characteristics, enabling the model to adapt to the color space of previously unseen cameras. As shown in Fig. 3-B, CFE is generated through two-step process. First, set of illuminants along the Planckian locus (covering color temperatures from 2500K to 7500K) is converted into the specific cameras native raw RGB space using its pre-calibrated CCMs. Second, the resulting RGB illuminant colors are transformed into uv-histogram, which is then processed by CNN-based encoder to extract device-specific feature. Camera-Native Guidance Illuminants. Our goal is to obtain L, the chromaticity set of light sources within specific color temperature range as observed by given camera. To achieve this, we use calibration matrices that transform the CIE XYZ coordinates of standard illuminants or D65 into the cameras raw space. These calibration data are typically provided during camera manufacturing and can be extracted from DNG files produced by most cameras. For simplicity, we refer to these matrices as CCMlow and CCMhigh throughout this paper, as used in Eq. (8). For details on CCM properties and extraction methods, please refer to the supplementary materials. First, illuminant colors along the Planckian locus in the device-independent CIE XYZ color space are sampled at 100K intervals within the 2500K to 7500K color temperature range. Each sampled XYZ point, Xt, corresponding to an illuminant with color temperature t, is then transformed into camera-native RGB color, Lt, using the following equation: Lt = CCMtXt, (9) where CCMt is transformation matrix for color temperature that maps the CIE XYZ values of an illuminant with color temperature to the target cameras raw space. Since CCMlow and CCMhigh are calibrated at specific color temperatures, interpolation is used to compute CCMt for an arbitrary color temperature t. The interpolation of CCMt is defined as: CCMt = gCCMlow + (1 g)CCMhigh, and CCMhigh are calibrated, typically around 2500K and 6500K, respectively. The resulting set of camera-native RGB colors, Lt {2500, 2600, . . . , 7500}, represents the illumination colors along the Planckian locus in the cameras raw RGB space, sampled within the 2500K 7500K range as observed by specific image sensor. Histogram Conversion & Encoding. The camera-specific guidance illuminant set, L, is transformed into uvhistogram using Eq. (3) and Eq. (4). As shown in Fig. 3B, the guidance illumination set follows distinct trajectories for each device in the uv-histogram space. To convert these trajectory differences into device-aware embedding, we employ lightweight CNN, the CFE encoder, consisting of four convolutional layers (with max pooling) followed by two-layer MLP. This network encodes each devices locus histogram into an 8-dimensional CFE feature. The encoded CFE feature is then repeated along the and axes to match the resolution of the input histograms. Finally, it is concatenated with the input histograms (N0, N1) along the channel dimension and provided as input to the CCC generator network , as shown in Fig. 3-A. 3.4. Imaginary Camera Augmentation In prior illuminant estimation research, most data augmentation techniques (e.g., [1, 5, 27, 49]) rely on transferring ground-truth illuminant colorsrandomly sampled from given datasetto other images within the same dataset (captured by the same camera) using chromatic adaptation. However, this approach is incompatible with our method, which is trained on raw images from different cameras, each with distinct raw color space. Another augmentation approach [6] leverages camera-specific information and CCMs to perform raw-to-raw augmentation by transferring images from source camera to target camera. While promising, this method remains constrained by the limited diversity of training camera raw spaces. To address these limitations, we propose novel augmentation strategy that increases the diversity of camera characteristics, even with limited set of training cameras. Specifically, we synthesize imaginary cameras by leveraging the CCMs of available training cameras. This expands the range of camera raw spaces encountered during training, significantly enhancing generalization. Imaginary Camera Image Synthesis. Under the assumption of single illuminant in the scene, the value of channel {R, G, B} at pixel in the cameras raw space can be expressed as: where = t1 CCT1 high CCT1 low CCT1 high (10) (cid:90) Ic(x) = S(λ, x)R(λ)Qc(λ) dλ, (11) , where S() and R() represent the spectral power distribution of the scene and illuminant at pixel x, respectively, and Qc is the cameras spectral sensitivity for color channel c. where CCTlow and CCThigh denote the color temperatures of the standard illuminants for which CCMlow Based on Eq. (9) and Eq. (12), the observed raw RGB values for camera can be obtained as follows: CCMV lowX = LV = αLA + (1 α)LB = α(CCMA = [αCCMA lowX) + (1 α)(CCMB low + (1 α)CCMB low]X, lowX) (13) where the superscript V, A, denotes the type of camera (omitted the subscript CCTlow for and for simplicity). As result, the CCMlow for the imaginary camera can be defined as:"
        },
        {
            "title": "CCMV",
            "content": "low = αCCMA low + (1 α)CCMB low. (14) This relationship also holds for CCThigh and any arbitrary color temperature within the range of low and high CCTs in the calibrated CCMs, as described in Eq. (10). We randomly select two cameras from the training dataset to generate an augmented set using the method outlined above. The augmented images and CCMs approximate the spectral sensitivity of the imaginary camera, enabling the CFE encoder to generalize to wider range of cameras despite the limited number of training cameras. 4. Experiments 4.1. Experimental Setup Training. The input image and camera-specific raw RGB illuminants (51 colors ranging from 2500K to 7500K, sampled at 100K intervals) are represented as 64 64 uvhistograms. The uv-ranges are empirically set to [-2.85, 2.85] for the input query image and [-0.5, 1.5] for CFE encoding. We use the Intel-TAU [44], Gehler-Shi [57], NUS-8 [19], and Cube+ [9] datasets for training and testing. Each dataset includes images captured by distinct cameras, with no overlap between datasets. The number of cameras varies between one (Cube+) and eight (NUS-8). Following the protocol in C5 [6], we adopt leave-oneout cross-dataset evaluation approach, where the network is trained on all datasets except the test dataset. For instance, when validating on Gehler-Shi, the network is trained using Intel-TAU, NUS-8, and Cube+, ensuring no camera overlap between training and test datasets. We exclude the SonyIMX subset of Intel-TAU due to the absence of CCM information, so Intel-TAU is used solely for training. The mean angular error serves as the loss function during training. Additional details on batch size, epochs, other training hyperparameters, and model architecture are provided in the supplementary materials. Data Augmentation. We augment the training data by selecting two cameras from the training datasets and applying Figure 4. Visualization of our imaginary camera augmentation process. An image from the Sony A57 is white-balanced using the ground-truth illuminant, converted to CIE XYZ space, and mapped to the target cameras raw space. We illustrate two cases: mapping to the raw space of real camera (Fujifilm XM1) and an imaginary camera. Brightness is adjusted for clarity. The integral is computed over λ, corresponding to wavelengths in the visible light spectrum. Since cameras characteristics are defined by its spectral sensitivity function Q, an image captured by an imaginary camera, denoted as , can be approximated by linearly combining the characteristics of cameras and with ratio α, defined as: (cid:90) = S(λ)R(λ)(αQA + (1 α)QB )(λ) dλ (12) = αI + (1 α)I , where superscripts A, B, and denote different cameras, including the imaginary camera, and α [0, 1] controls the contribution of each camera to the synthesized imaginary camera (omitting for simplicity). As illustrated in Fig. 4, this approach allows mapping raw images to specific target camera (e.g., Sony A57 with α = 1) or to an imaginary camera (e.g., blending the Sony A57 and Fujifilm XM1 with 0.5 to 0.5 ratio). Additionally, the ground truth illumination for the imaginary camera can be synthesized as linear combination of the ground truth illuminations of cameras and B, weighted by α. For additional details on augmentation methods, please refer to the supplementary materials. Derivation of the Imaginary Cameras CCM. Since CCMNet requires CCMs to encode CFE, it is also necessary to derive CCMs for the imaginary camera. Let us assume that cameras A, B, and the imaginary camera observe light source with correlated color temperature CCTlow. camera-to-camera mapping with random ratio interpolation to generate images and CCMs for imaginary cameras, as described in Sec. 3.4. The total number of augmented images matches the size of the original training set. Further details are provided in the supplementary materials. Testing. For evaluation, we report commonly used error statistics: the mean, median, and tri-mean angular errors, along with the arithmetic mean of the top and bottom 25% angular errors between the predicted and ground truth illuminations. 4.2. Results Results are presented in Table 1, where the first three tables show the main experimental results for three test datasets: Cube+, Gehler-Shi, and NUS-8. These results demonstrate that CCMNet achieves state-of-the-art performance across all datasets and metrics (see Fig. 5). Unlike other learning-based models that report zero-shot results, DMCC retrains target camera-specific network using calibrated matrices to transform training data (the Sony IMX-135 subset from the Intel-TAU dataset) into the test cameras color space. Similarly, C5 requires additional images from the test camera for guidance, making it difficult to determine the optimal number and content of these images. In contrast, CCMNet achieves superior and more consistent results by leveraging CFE features from two precalibrated CCMs. CCMNet is simpler, more robust and does not require retraining for each test camera or the use of additional images. Notably, no data or CCMs from test cameras are used during training, ensuring true zero-shot generalization. Additional visual results are provided in the supplementary materials. We also report results on the cross-sensor (CS) validation setup [3]. In this protocol, the network is trained on data from seven cameras in the NUS-8 dataset, excluding one as the test camera. This process is repeated for each of the eight cameras, and the results are averaged. Following this protocol, we train CCMNet using data from Intel-TAU, Cube+, Gehler-Shi, and seven cameras from NUS-8 (excluding the test camera), aggregating results over eight iterations. As shown at the bottom of Table 1, CCMNet outperforms other methods under this evaluation protocol. Another advantage of CCMNet is its lightweight design. Since the CFE feature is fixed once the camera device is determined, it only needs to be extracted once for new camera and can be reused thereafter. As result, CCMNets size and computational cost depend solely on the backbone , making it significantly more efficient than the C5 model, which requires 68 additional histogram encoders. As shown in the first table of Table 1, the C5 model (with an additional 8 histograms, = 9) requires approximately 2.09 MB of storage, whereas CCMNet, excluding the CFE Gehler-Shi [57] Mean Med. Tri. B.25% W.25% Size(MB) 2nd-order Gray-Edge [61] Shades-of-Gray [23] PCA-based B/W Colors [19] ASM [8] Woo et al. [62] Grayness Index [56] Cross-dataset CC [43] Quasi-Unsupervised CC [14] SIIE [3] FFCC [13] C5 (m = 7) [6] C5 (m = 9) [6] CCMNet (Ours) 5.13 4.93 3.52 3.80 4.30 3.07 2.87 3.46 2.77 2.95 2.36 2.50 2.23 4.44 4.01 2.14 2.40 2.86 1.87 2.21 2.23 1.93 2.19 1.61 1.99 1.53 4.62 4.23 2.47 2.70 3.31 2.16 - - - 2.35 1.74 2.03 1.62 2.11 1.14 0.50 - 0.71 0.43 - - 0.55 0.57 0.44 0.53 0.36 9.26 10.20 8.74 - 10.14 7.62 - - 6.53 6.75 5.60 5.46 5.46 - - - - - - - 622 10.3 0.22 1.74 2.09 1. Cube+ [9] Mean Med. Tri. B.25% W.25% Gray-world [18] 1st-order Gray-Edge [61] 2nd-order Gray-Edge [61] Shades-of-Gray [23] Cross-dataset CC [43] Quasi-Unsupervised CC [14] SIIE [3] FFCC [13] DMCC [66] C5 (m = 7) [6] C5 (m = 9) [6] CCMNet (Ours) 3.52 3.06 3.28 3.22 2.47 2.69 2.14 2.69 2.23 1.87 1.92 1.68 2.55 2.05 2.34 2.12 1.94 1.76 1.44 1.89 1.63 1.27 1.32 1.16 2.82 2.32 2.58 2.44 - 2.00 - 2.08 1.78 1.40 1.46 1. 0.60 0.55 0.66 0.43 - 0.49 0.44 0.46 0.49 0.41 0.44 0.38 7.98 7.22 7.44 7.77 - 6.45 5.06 6.31 4.95 4.36 4.44 3.89 NUS-8 [19] Mean Med. Tri. B.25% W.25% Gray-world [18] Shades-of-Gray [23] Local Surface Reflectance [28] PCA-based B/W Colors [19] Grayness Index [56] Cross-dataset CC [43] Quasi-Unsupervised CC [14] FFCC [13] C5 (m = 7) [6] C5 (m = 9) [6] CCMNet (Ours) 4.59 3.67 3.45 2.93 2.91 3.08 3.00 2.87 2.68 2.54 2. 3.46 2.94 2.51 2.33 1.97 2.24 2.25 2.14 2.00 1.90 1.71 3.81 3.03 2.70 2.42 2.13 - - 2.30 2.14 2.02 1.83 1.16 0.98 0.98 0.78 0.56 - - 0.71 0.66 0.61 0.53 9.85 7.75 7.32 6.13 6.67 - - 6.23 5.90 5.61 5.18 NUS-8 (CS) [19] Mean Med. Tri. B.25% W.25% DMCC (CS) [66] SIIE (CS) [3] C5 (m = 9, CS) [6] CCMNet (Ours, CS) 2.80 2.05 1.77 1.71 2.12 1.50 1.37 1.31 2.25 - 1.46 1.40 0.74 0.52 0.48 0.48 5.88 4.48 3.75 3. Table 1. Experimental results on three benchmark datasets. CCMNet achieves the best performance across all metrics on various datasets, including additional cross-sensor (CS) validation protocol. For C5 model, represents the total number of images used, including both the query image and additional images. encoder, occupies only 1.05 MBalmost half the size. This highlights CCMNets compactness, making it particularly well-suited for integration into ISP modules, where efficient resource utilization is crucial. 4.3. Generalization with SIIE Backbone We further explore using CFE with SIIE [3]. SIIE learns 33 matrix by processing the raw image uv-histogram to map raw colors to color working space. In this experiment, we replace C5 with SIIE as our backbone. Specifically, we input our CFE-concatenated uv-histograms, augmented with the imaginary camera transformation, into the SIIE backbone. As shown in Table 2, the best performance 7 Figure 5. Visual comparison of the results from C5 [6] with different additional image sets (second, third column) and CCMNet (fourth column). While C5 relies on additional images, CCMNet is optimized for fixed CFE guidance, ensuring consistent performance. Model Cube+ Gehler-Shi NUS-8 SIIE [3] w/ CFE w/ aug. w/ CFE & aug. 3.39 2.60 2.43 1.91 3.67 3.62 3.12 2.99 3.52 3.36 3.00 2. number of iterations is doubled to ensure the same number of model updates as in the other experiments. The results indicate that the CFE encoder in CCMNet and the imaginary camera augmentation play crucial roles in the cross-camera color constancy task. Table 2. Generalization with the SIIE [3] backbone. Reported results show the mean angular error. 5. Conclusion and Discussion Model Aug. method Cube+ Gehler-Shi NUS-8 Backbone CCMNet (f + CFE) w/o aug. α = 1 0 α 1 w/o aug. α = 1 0 α 1 2.22 1.94 1.78 2.23 1.86 1.68 2.79 2.87 2. 2.74 2.34 2.23 2.88 2.50 2.54 2.70 2.45 2.32 Table 3. Ablation studies on the impact of the CFE encoder and different augmentation strategies. The reported results are the mean angular error. (MAE) is achieved when both CFE and augmentation are applied, confirming that CCMNet generalizes to different backbones utilizing uv-histograms. 4.4. Ablation Studies Table 3 presents the performance of the backbone and CCMNet trained under three setups: without augmentation (w/o aug), with augmentation at α = 1, and with augmentation for 0 α 1. Architecturally, the backbone mirrors the = 1 structure from C5 [6], excluding additional images and encoders. Setting α = 1 in the augmentation process replicates the camera-mapping strategy used in C5. The training data is halved for the w/o aug. setup, and the In this paper, we propose CCMNet, lightweight and efficient method for cross-camera color constancy that leverages pre-calibrated CCMs available in camera ISPs. The model utilizes these CCMs, which map cameras raw color space to the device-independent CIE XYZ color space or vice versa, to encode the camera-specific illumination locus into guidance embedding. This feature, termed CFE, directs hypernetwork to quickly adapt to unseen cameras during testing, enabling the generation of appropriate filters and biases while achieving superior performance compared to previous methods. By taking advantage of the linearity of CCM operations, the proposed imaginary camera augmentation technique allows the model to learn broader range of virtual camera response functions during training, significantly improving CCMNets generalization capability. While most cameras include calibrated raw-to-XYZ CCMs in their ISPs and DNG files, some smartphones may not provide accurate CCMs in their DNGs. Instead, these devices often include single fixed matrix to convert raw images to linear sRGB. This limitation could hinder our methods ability to process DNG files from such devices or necessitate an additional conversion step to adapt to the raw-to-linear sRGB matrix. 8 CCMNet: Leveraging Calibrated Color Correction Matrices for Cross-Camera Color Constancy"
        },
        {
            "title": "Supplementary Material",
            "content": "A. CCMs & CCTs Extraction In this section, we describe the methodology used to extract the color correction matrices, low and high correlated color temperatures (CCTlow, CCThigh) information utilized in our approach. Since CCMs and their correlated CCTs are camera-dependent, they can be extracted once and remain consistent across all images captured by the same camera. To extract the CCMs and CCTs of specific camera, we followed these steps. First, to ensure consistency in data processing, we converted all raw images to the DNG format using Adobe DNG Converter, instead of relying on camera-specific raw file extensions. Second, we extracted metadata from the DNG files using ExifTool, specifically retrieving ColorMatrix1, ColorMatrix2, ForwardMatrix1, and ForwardMatrix2. These matrices were then used for our imaginary camera augmentation and for testing on previously unseen cameras during training. For convenience, we will refer to ColorMatrix and ForwardMatrix as CM and FM, respectively, throughout this supplementary material. Fig. 6 illustrates the relationships between color spaces and the transformation matrices involved. As shown, the FM transforms white-balanced camera raw colors to the CIE XYZ color space, while the CM converts from CIE XYZ to the cameras native raw color space under specific illuminant. The suffixes 1 and 2 in the matrix names indicate calibration for illuminants 1 and 2, corresponding to standard illuminant and D65, respectively. Accordingly, we define CCTlow and CCThigh as the color temperatures of illuminant (2856K) and D65 (6504K) and use these values for CCM interpolation, as described in Eq. (10) in the main paper. As defined in Eq. (9) in the main paper, the CCMlow and CCMhigh matrices used throughout this work correspond to CM1 and CM2, respectively. Additionally, CM1, CM2, FM1, FM2 are used in the imaginary camera augmentation process described in Sec. C. B. Details of the CFE Encoding Process In this section, we provide additional details on the CFE (Camera Fingerprint Embedding) encoding process described in Sec. 3.3. Further explanations. As shown in Fig. 7, the essence of what CFE fundamentally encodes is the color trajectory on the CIE xy-plane within the correlated color temperature (CCT) range of 2500K7500K. These colors correspond to the light emitted by black body at given CCT and are Figure 6. schematic diagram illustrating the use of ColorMatrix and ForwardMatrix. The ForwardMatrix (FM) transforms whitebalanced raw data into the CIE XYZ color space, while the ColorMatrix (CM) converts CIE XYZ values of standard light source into the cameras native raw color space. FM1 and CM1 are calibrated for standard illuminant (2856K), and FM2 and CM2 are calibrated for the D65 illuminant (6504K). intrinsic, invariant values. However, due to differences in the spectral sensitivity of imaging sensors, each device observes these reference colors as distinct loci. These trajectories inherently represent the unique color characteristics of each device. We leverage the fact that this observation process is precomputed for two illuminants during the ISP manufacturing stage and recorded as matrices (CCMs). By interpolating the two matrices, CCMlow and CCMhigh, and then applying to the Planckian XYZ locus, we replace the observation process for each device. The resulting device-specific locus is then converted into histogram, which is subsequently encoded into CFE feature that captures the fingerprint of each camera using CNN-based CFE encoder. Due to this design approach of the CFE feature, the CCMNet leverages CFE as guidance, enabling it to infer and adapt to the color space of previously unseen camera. This allows the model to learn generalized approach to illuminant color estimation without requiring explicit training on every individual camera. Technical details. For the XYZ locus corresponding to color temperatures from 2500K to 7500K, we used the colour.temperature.CCT to xy function from the colour Python library. total of 51 chromaticity coordinates were sampled at 100K intervals, ranging from 2500K to 7500K. As mentioned in the main paper, the sampled XYZ locus was transformed into the cameras native raw RGB space by interpolating between CM1 and CM2. This was further converted into histogram with 64 bins, within the uv range of [-0.5, 1.5]. The resulting 64641 histogram was processed by the CFE encoder, which outputs an 8dimensional embedding vector. The CFE encoder consists 9 Figure 7. Detailed visualization of CFE encoding process. As mentioned in the main paper, the cameras fingerprint is derived by converting the reference CIE XYZ colors (locus) along the correlated color temperature (CCT) range of 2500K7500K into the corresponding RGB locus as observed by each device, followed by an encoding process. Due to this characteristic, the CFE feature inherently reflects the color characteristics induced by each cameras spectral sensitivity. of four DoubleConvBlocks followed by projection head. Each DoubleConvBlock processes the input by applying two convolutional layers, each with kernel size of 3 3, stride of 1, and padding of 1, followed by LeakyReLU activation. This is then followed by 2 2 max-pooling layer and batch normalization. The projection head flattens the feature map and maps it to an 8dimensional embedding vector using an MLP with two hidden layers. C. Camera-to-Camera Mapping In Sec. 3.4 of the main paper, we introduced our imaginary camera augmentation, which assumes two versions of the same image in the cameras native raw RGB space. To satisfy this condition, we perform camera-to-camera mapping inspired by [6]. In this section, we provide detailed explanation of the camera-to-camera mapping process used in our work. Specifically, in Sec. C.1, we explain the process of computing the correlated color temperature (CCT) of light source in the target cameras native raw RGB space. Then, in Sec. C.2, we describe how to generate pool of white-balanced, camera-independent XYZ images using the RGB values of the light source and the corresponding CCT. In Sec. C.3, we describe the process of generating device-specific illumination pool for random sampling. Finally, Sec. C.4 explains our camera-to-camera mapping, which presents reference image in two different camera-native raw RGB spaces. The reference image is sampled from the XYZ image pool, while the illumination is sampled from the augmented ground-truth (GT) illumination pool of each camera. The overall process is visualized in Fig. 8. While our camera-to-camera mapping is inspired by the C5 augmentation approach [6], it differs in the following ways. First, we remove C5s restriction that limits sampling from the illumination pool to similar scenes with matching capturing settings (e.g., ISO, exposure time) and illumination CCT. Specifically, in C5, both the sampled scene image from the CIE XYZ space and the sampled illuminant from the target camera were required to have similar capturing settings and CCT. In contrast, our approach removes this constraint, eliminating the need to rely on capturing settings and allowing for greater diversity in augmentation. Additionally, instead of sampling from fitted cubic polynomial based on the target cameras illuminant samples, we use fitted cubic polynomial based on the illuminant values from the source cameras dataset (i.e., the camera from which the reference XYZ image was taken). The sampled illuminant is then transferred to the CIE XYZ space using the inverse of the source cameras CM, followed by transformation of these CIE XYZ illuminant values into the native raw RGB space of the target camera. C.1. Illumination RGB to CCT Conversion The illuminant estimation datasets used in the main paper provide GT illumination RGB labels for each scene in the cameras native raw RGB space. According to the Adobe DNG specification, given CM1 and CM2 (extracted for each camera as described in Sec. A), along with the GT ilFigure 8. Overall process of camera-to-camera mapping. In (A), subsets of images taken by different cameras from multiple datasets are white-balanced using the corresponding ground-truth illuminants, and the ForwardMatrix is used to convert them to the CIE XYZ space, creating the XYZ image pool. In (B), reference image is sampled from the pool, and an illumination color is sampled from the augmented illumination pool of the source camera (Camera A) that originally captured the image. The sampled illumination is then mapped to the native RGB space of randomly selected target camera (Camera B) using the ColorMatrix. Finally, in (C), the XYZ image is transformed into the white-balanced color space of Cameras and using the inverse of their respective ForwardMatrices, and illumination casting is applied by multiplying the images with the illumination RGB values of each camera space. lumination RGB, the CCT and CIE XYZ values of the light source can be computed using Algorithm 1. Algorithm 1 Conversion of Illuminant Raw RGB to CCT and XYZ Coordinates xy = [0.3127, 0.3290] while True do 1: function CAMNTRL TO XYZ(illum, cm1, cm2) 2: 3: 4: 5: 6: cct = colour.temperature.xy to CCT(xy) color matrix = interpolate ccm(cct, cm1, cm2) color matrix inv = np.linalg.inv(color matrix) xyz = np.dot(color matrix inv, illum) X, Y, = xyz xy new = [X / (X + + Z), / (X + + Z)] if np.allclose(xy, xy new, atol=1e-6) then 7: 8: 9: 10: 11: 12: return xyz, cct end if xy = xy new 13: end while 14: 15: end function The algorithm iteratively estimates the CCT and converts illuminant RGB values to the CIE XYZ space. Using metadata such as CM1 and CM2, it interpolates the appropriate color correction matrix for the estimated CCT and applies it to transform the input illumination into the CIE XYZ space. The resulting XYZ coordinates and CCT values are then used either to generate the camera-independent XYZ image pool in Sec. C.2 or to transform the illumination into the target camera RGB space in Sec. C.4. C.2. Unified XYZ Image Pool Generation In this section, we describe the process of creating an XYZ image pool for camera-to-camera mapping by converting images captured by various cameras into the deviceindependent XYZ color space. The process involves two main steps: (1) white balancing with GT labels, and (2) transforming to the CIE XYZ color space using the ForwardMatrix (FM). Refer to Fig. 6 and Fig. 8-(A). As explained in the main paper, we use multiple datasets captured by various cameras, each including GT illumination labels that enable accurate white balancing of images in the cameras native raw RGB space. As described in Sec. A, we extract FM1 and FM2 for each camera. Using the CCT of the GT illumination, we interpolate between FM1 and FM2 to transform the white-balanced images into the XYZ color space. The CCT is computed from the GT illumination RGB using the method detailed in Sec. C.1. This process mitigates the dependency on camera specifications, and in theory, the images are independent of camera models and illumination conditions. By aggregating these images, we construct unified XYZ image pool that serves as the foundation for camera-to-camera mapping. C.3. Camera-specific Illumination Pool Generation Next, we generate an illumination pool for each camera. While it is possible to use only the GT illuminations, we adopt the augmentation method proposed in [6] to enhance generality and diversity. This method involves fitting cubic polynomial to the GT illuminations for each camera vices. This data augmentation technique also interpolates the CCMs at the same ratios to generate the CCMs for these virtual cameras. E. Experimental Setup As mentioned in the main paper, the backbone uses the standard U-Net-like architecture from C5 [6]. However, unlike C5, we do not use additional images from the test Instead, we camera, so no extra encoders are employed. use single Encoder-Decoder U-Net architecture. The encoder and decoder are connected via skip connections, with each consisting of four DoubleConv layers. In the encoder, each DoubleConv layer is followed by max pooling, while in the decoder, feature upsampling and skip connections are applied before each DoubleConv layer. The batch size was set to 16, and training was conducted over 50 epochs with an initial learning rate of 5 104. learning rate decay of 0.5 was applied at epoch 25. The Adam optimizer [42] was used for training. For data augmentation, camera-to-camera mapping and imaginary camera augmentation are applied exclusively using the camera subsets from the training datasets, excluding the test dataset. For instance, when evaluating the Cube+ dataset, the augmented dataset used for model training is generated from images and CCMs from the Gehler-Shi [57], NUS-8 [19], and Intel-TAU [44] datasets. F. Additional Results We present additional visualization results in Fig. 10 and Fig. 11. As shown in Fig. 10, CCMNet achieves satisfactory accuracy across various scenes captured by camera it has never encountered during training. In Fig. 11, we demonstrate that CCMNet maintains robust accuracy across set of unseen cameras. and then introducing random shifts to augment the illuminations. For further details, please refer to the supplementary material of [6]. On the right side of Fig. 8-(B), we show plot of the illumination pool for specific camera (Camera A). In this plot, the red points represent the GT illumination labels extracted from the dataset, while the blue points correspond to the augmented illuminations. C.4. Camera-to-Camera Image Synthesis In this section, we describe camera-to-camera mapping method that simulates the same scene as if it were captured by two different cameras, using the image pool from Sec. C.2 and the illumination pool from Sec. C.3. See Fig. 8-(B) and (C). Scene and Illumination Sampling & Mapping. First, scene is randomly selected from the XYZ image pool. Next, an illumination is randomly sampled from the illumination pool of the source camera that captured the selected scene. This sampled illumination is then transformed into the native raw color space of randomly selected target camera from the set of cameras used (see Fig. 8-(B)). As illustrated in Fig. 6, the XYZ values of the sampled illumination are computed by applying the inverse of the source cameras ColorMatrix (CM). These XYZ values are then multiplied by the target cameras CM to obtain the native raw color of the illumination in the target cameras color space. The interpolation of each cameras CM is based on the CCT of the illumination, which is calculated using the steps described in Sec. C.1. Synthesizing Paired Scene from Two Cameras. Finally, as illustrated in Fig. 8-(C), we generate two raw images of the sampled scene, as if it were captured by the selected two cameras under the same sampled illumination. As shown in Fig. 6, the white-balanced XYZ image is transferred to the cameras native raw space in two steps. First, using the same CCT employed during CM interpolation in illumination mapping, the FMs of cameras and are interpolated, and their inverses are applied to the XYZ image. This step produces two white-balanced raw images, one for each camera. Next, the camera-native illumination RGB valuessampled from camera and mapped to camera as described in previous paragraphare multiplied with these raw images. The resulting image pair simulates the same scene and lighting conditions as captured by two different cameras, all derived from single XYZ image. D. Imaginary Camera Augmentation Visualizations Here, we provide additional visualizations of the imaginary camera augmentation. As shown in Fig. 9, Imaginary Camera Augmentation simulates images captured by virtual cameras that interpolate the properties of two real-world de12 Figure 9. Results of our imaginary camera augmentation. In each row, the leftmost and rightmost images represent the source and target camera images generated using the method described in Sec. C, while the three middle images represent those produced by the imaginary camera, generated by interpolating between the two devices at ratios of 0.25, 0.5, and 0.75, respectively. As explained in Sec. 3.4 of the main paper, the CCMs of the imaginary cameras are interpolated using the same alpha values applied during image interpolation, and the resulting CFE embeddings are generated for training. Brightness is adjusted for visibility. Figure 10. Additional results for Canon EOS 1Ds Mark III. CCMNet demonstrates superior performance on various scenes captured by unseen camera. Notably, CCMNet has never been exposed to any images or the CCM of the Canon 1Ds Mark III during training. 13 Figure 11. Additional results for various cameras show that CCMNet exhibits robust performance across range of unseen cameras. Importantly, it has not been exposed to any images or CCMs from the cameras shown in the figure during training."
        },
        {
            "title": "References",
            "content": "[1] Abdelrahman Abdelhamed, Abhijith Punnappurath, and Michael Brown. Leveraging the availability of two cameras for illuminant estimation. In CVPR, 2021. 5 [2] Mahmoud Afifi and Abdullah Abuolaim. Semi-supervised raw-to-raw mapping. In BMVC, 2021. 1 [3] Mahmoud Afifi and Michael Brown. Sensor-independent illumination estimation for DNN models. In BMVC, 2019. 1, 2, 7, 8 [4] Mahmoud Afifi, Brian Price, Scott Cohen, and Michael Brown. When color constancy goes wrong: Correcting imIn CVPR, pages 1535 properly white-balanced images. 1544, 2019. 1 [5] Mahmoud Afifi, A. Abdelhamed, Abdullah Abuolaim, Abhijith Punnappurath, and M. S. Brown. CIE XYZ Net: UnproIEEE cessing images for low-level computer vision tasks. Transactions on Pattern Analysis and Machine Intelligence, 44:46884700, 2020. [6] Mahmoud Afifi, Jonathan Barron, Chloe LeGendre, YunTa Tsai, and Francois Bleibel. Cross-camera convolutional color constancy. In ICCV, 2021. 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12 [7] Mahmoud Afifi, Zhenhua Hu, and Liang Liang. Optimizing illuminant estimation in dual-exposure HDR imaging. In ECCV, 2025. 1, 2 [8] Arash Akbarinia and C. Alejandro Parraga. Colour conIEEE Transstancy beyond the classical receptive field. actions on Pattern Analysis and Machine Intelligence, 40: 20812094, 2018. 7 [9] Nikola Banic, Karlo Koˇsˇcevic, and Sven Lonˇcaric. UnarXiv preprint supervised learning for color constancy. arXiv:1712.00436, 2017. 6, 7 [10] Kobus Barnard. Computational color constancy: Taking theory into practice. 1995. 1, 2 [11] Kobus Barnard, Lindsay Martin, Adam Coath, and Brian Funt. comparison of computational color constancy IEEE algorithmspart II: Experiments with image data. Transactions on Image Processing, 11(9):985996, 2002. 1 [12] Jonathan Barron. Convolutional color constancy. In ICCV, 2015. 2, 3, 4 [13] Jonathan Barron and Yun-Ta Tsai. Fast Fourier color constancy. In CVPR, 2017. 2, 3, 4, 7 [14] Simone Bianco and Claudio Cusano. Quasi-unsupervised color constancy. In CVPR, 2019. 2, 7 [15] Simone Bianco, Arcangelo Bruna, Filippo Naccari, and Raimondo Schettini. Color correction pipeline optimization for digital cameras. Journal of Electronic Imaging, 22(2): 023014023014, 2013. 3 [16] Simone Bianco, Claudio Cusano, and Raimondo Schettini. Color constancy using CNNs. In CVPRW, 2015. [17] MichaelS Brown. Color processing for digital cameras. Fundamentals and Applications of Colour Engineering, pages 8198, 2023. 1, 2 [18] Gershon Buchsbaum. spatial processor model for object 14 colour perception. Journal of the Franklin institute, 310(1): 126, 1980. 2, 7 [19] Dongliang Cheng, Dilip Prasad, and Michael Brown. Illuminant estimation for color constancy: Why spatialdomain methods work and the role of the color distribution. JOSA A, 31(5):10491058, 2014. 2, 6, 7, 12 [20] Dongliang Cheng, Brian Price, Scott Cohen, and Michael Brown. Beyond white: Ground truth colors for color constancy correction. In CVPR, 2015. [21] Mauricio Delbracio, Damien Kelly, Michael Brown, and Peyman Milanfar. Mobile computational photography: tour. Annual review of vision science, 7(1):571604, 2021. 1, 2 [22] Graham Finlayson and Steven Hordley. Color constancy at pixel. JOSA A, 18(2):253264, 2001. 3 [23] Graham Finlayson and Elisabetta Trezzi. Shades of gray In Color and Imaging Conference, and colour constancy. 2004. 2, 7 [24] Graham Finlayson and Yuteng Zhu. Designing color filters that make cameras more colorimetric. IEEE Transactions on Image Processing, 30:853867, 2020. [25] Graham Finlayson, Michal Mackiewicz, and Anya Hurlbert. Color correction using root-polynomial regression. IEEE Transactions on Image Processing, 24(5):14601470, 2015. 3 [26] Toadere Florin. Color processing in digital camera pipeline. In Advanced Topics in Optoelectronics, Microelectronics, and Nanotechnologies IV, pages 223227, 2009. 1, 2 [27] Damien Fourure, Remi Emonet, Elisa Fromont, Damien Muselet, Alain Tremeau, and Christian Wolf. Mixed pooling neural networks for color constancy. In ICIP, 2016. 5 [28] Shaobing Gao, Wangwang Han, Kaifu Yang, Chaoyi Li, and Y. Li. Efficient color constancy with local surface reIn European Conference on Computer flectance statistics. Vision, 2014. 7 [29] Peter Vincent Gehler, Carsten Rother, Andrew Blake, Tom Minka, and Toby Sharp. Bayesian color constancy revisited. In CVPR, 2008. [30] Arjan Gijsenij, Theo Gevers, and Joost Van De Weijer. Generalized gamut mapping using image derivative structures for color constancy. International Journal of Computer Vision, 86(2-3):127139, 2010. 1, 2 [31] Arjan Gijsenij, Theo Gevers, and Joost Van De Weijer. Computational color constancy: Survey and experiments. IEEE Transactions on Image Processing, 20(9):24752489, 2011. 1 [32] Arjan Gijsenij, Theo Gevers, and Joost Van De Weijer. Improving color constancy by photometric edge weighting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(5):918929, 2011. 2 [33] Samuel Hasinoff, Dillon Sharlet, Ryan Geiss, Andrew Adams, Jonathan Barron, Florian Kainz, Jiawen Chen, and Marc Levoy. Burst photography for high dynamic range and low-light imaging on mobile cameras. ACM Transactions on Graphics (ToG), 35(6):112, 2016. 2 [34] Daniel Hernandez-Juarez, Sarah Parisot, Benjamin Busam, Ales Leonardis, Gregory Slabaugh, and Steven McDonagh. multi-hypothesis approach to color constancy. In CVPR, 2020. 2 [35] Guowei Hong, Ronnier Luo, and Peter Rhodes. study of digital camera colorimetric characterization based on polynomial modeling. Color Research & Application, 26 (1):7684, 2001. [36] Yuanming Hu, Baoyuan Wang, and Stephen Lin. FC4: Fully convolutional color constancy with confidence-weighted pooling. In CVPR, 2017. 2 [37] Paul Hubel, Graham Finlayson, and Steven Hordley. White point estimation using color by convolution, 2007. US Patent 7,200,264. 2 [38] Po-Chieh Hung. Colorimetric calibration in electronic imaging devices using look-up-table model and interpolations. Journal of Electronic imaging, 2(1):5361, 1993. 3 [39] Adobe Systems Incorporated. Digital negative (DNG) specification. 2023. 2, 3 [40] Hakki Can Karaimer and Michael Brown. software platform for manipulating the camera imaging pipeline. In ECCV, 2016. 2, [41] Hakki Can Karaimer and Michael Brown. Improving color reproduction accuracy on cameras. In CVPR, 2018. 2, 3 [42] Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 12 [43] Samu Koskinen12, Dan Yang, Joni-Kristian Cross-dataset color constancy revisited and Kamarainen. using sensor-to-sensor transfer. BMVC, 2020. 7 [44] Firas Laakom, Jenni Raitoharju, Jarno Nikkanen, Alexandros Iosifidis, and Moncef Gabbouj. Intel-tau: color constancy dataset. IEEE access, 9:3956039567, 2021. 6, [45] Edwin Land. The retinex theory of color vision. Scientific american, 237(6):108129, 1977. 2 [46] Bing Li, Haina Qin, Weihua Xiong, Yangxi Li, Songhe Feng, Weiming Hu, and Stephen Maybank. Ranking-based color IEEE Transacconstancy with limited training samples. tions on Pattern Analysis and Machine Intelligence, 45(10): 1230412320, 2023. 2 [47] Shuwei Li and Robby Tan. NightCC: Nighttime color constancy via adaptive channel masking. In CVPRW, 2024. 2 [48] Yi-Chen Lo, Chia-Che Chang, Hsuan-Chao Chiu, Yu-Hao Huang, Chia-Ping Chen, Yu-Lin Chang, and Kevin Jou. CLCC: Contrastive learning for color constancy. In CVPR, 2021. 1, [49] Zhongyu Lou, Theo Gevers, Ninghang Hu, Marcel Lucassen, et al. Color constancy by deep learning. In BMVC, 2015. 5 [50] Steven McDonagh, Sarah Parisot, Fengwei Zhou, Xing Zhang, Ales Leonardis, Zhenguo Li, and Gregory Slabaugh. Formulating camera-adaptive color constancy as few-shot meta-learning problem. arXiv preprint arXiv:1811.11788, 2018. 2 [51] Jon McElvain and Walter Gish. Camera color correction In Color and Imaging using two-dimensional transforms. Conference, 2013. 3 15 [52] Rang Nguyen, Dilip Prasad, and Michael Brown. Rawto-raw: Mapping between image sensor color responses. In CVPR, 2014. [53] Seoung Wug Oh and Seon Joo Kim. Approaching the computational color constancy as classification problem through deep learning. Pattern Recognition, 61:405416, 2017. 2 [54] Yanlin Qian, Ke Chen, Jarno Nikkanen, Joni-Kristian Kamarainen, and Jiri Matas. Recurrent color constancy. In ICCV, 2017. 2 [55] Yanlin Qian, Said Pertuz, Jarno Nikkanen, Joni-Kristian Revisiting gray pixel arXiv preprint Kamarainen, and Jiri Matas. for statistical arXiv:1803.08326, 2018. 2 illumination estimation. [56] Yanlin Qian, Joni-Kristian Kamarainen, Jarno Nikkanen, and Jiri Matas. On finding gray pixels. In CVPR, 2019. 2, 7 [57] Lilong Shi. Re-processed version of the gehler color conhttp://www. cs. sfu. ca/ stancy dataset of 568 images. color/data/, 2000. 6, 7, 12 [58] Wu Shi, Chen Change Loy, and Xiaoou Tang. Deep specialized network for illuminant estimation. In ECCV, 2016. 2 [59] Yuxiang Tang, Xuejing Kang, Chunxiao Li, Zhaowen Lin, and Anlong Ming. Transfer learning for color constancy via statistic perspective. In AAAI, 2022. 2 [60] Oguzhan Ulucan, Diclehan Ulucan, and Marc Ebner. Multiscale color constancy based on salient varying local spatial statistics. The Visual Computer, 40(9):59795995, 2024. 2 [61] Joost Van De Weijer, Theo Gevers, and Arjan Gijsenij. Edgebased color constancy. IEEE Transactions on image processing, 16(9):22072214, 2007. 2, 7 [62] Sung-Min Woo, Sang-Ho Lee, Jun-Sang Yoo, and Jong-Ok Kim. Improving color constancy in an ambient light environment using the phong reflection model. IEEE Transactions on Image Processing, 27(4):18621877, 2017. [63] Jin Xiao, Shuhang Gu, and Lei Zhang. Multi-domain learnIn CVPR, ing for accurate and few-shot color constancy. 2020. 2 [64] Bolei Xu, Jingxin Liu, Xianxu Hou, Bozhi Liu, and Guoping Qiu. End-to-end illuminant estimation based on deep metric learning. In CVPR, 2020. 2 [65] Huanglin Yu, Ke Chen, Kaiqi Wang, Yanlin Qian, Zhaoxiang Zhang, and Kui Jia. Cascading convolutional color constancy. In AAAI, 2020. 1, 2 [66] Shuwei Yue and Minchen Wei. Effective cross-sensor color constancy using dual-mapping strategy. Journal of the Optical Society of America. A, Optics, image science, and vision, 41 2:329337, 2023. 7 [67] Shuwei Yue and Minchen Wei. Color constancy from pure color view. JOSA A, 40(3):602610, 2023. 2 [68] Shuwei Yue and Minchen Wei. Effective cross-sensor color constancy using dual-mapping strategy. JOSA A, 41(2): 329337, 2024."
        }
    ],
    "affiliations": [
        "AI Center - Toronto, Samsung Electronics",
        "Yonsei University"
    ]
}