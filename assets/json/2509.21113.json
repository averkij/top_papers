{
    "paper_title": "MOSS-ChatV: Reinforcement Learning with Process Reasoning Reward for Video Temporal Reasoning",
    "authors": [
        "Sicheng Tao",
        "Jungang Li",
        "Yibo Yan",
        "Junyan Zhang",
        "Yubo Gao",
        "Hanqian Li",
        "ShuHang Xun",
        "Yuxuan Fan",
        "Hong Chen",
        "Jianxiang He",
        "Xuming Hu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video reasoning has emerged as a critical capability for multimodal large language models (MLLMs), requiring models to move beyond static perception toward coherent understanding of temporal dynamics in complex scenes. Yet existing MLLMs often exhibit process inconsistency, where intermediate reasoning drifts from video dynamics even when the final answer is correct, undermining interpretability and robustness. To address this issue, we introduce MOSS-ChatV, a reinforcement learning framework with a Dynamic Time Warping (DTW)-based process reward. This rule-based reward aligns reasoning traces with temporally grounded references, enabling efficient process supervision without auxiliary reward models. We further identify dynamic state prediction as a key measure of video reasoning and construct MOSS-Video, a benchmark with annotated reasoning traces, where the training split is used to fine-tune MOSS-ChatV and the held-out split is reserved for evaluation. MOSS-ChatV achieves 87.2\\% on MOSS-Video (test) and improves performance on general video benchmarks such as MVBench and MMVU. The framework consistently yields gains across different architectures, including Qwen2.5-VL and Phi-2, confirming its broad applicability. Evaluations with GPT-4o-as-judge further show that MOSS-ChatV produces more consistent and stable reasoning traces."
        },
        {
            "title": "Start",
            "content": "MOSS-CHATV: REINFORCEMENT LEARNING WITH PROCESS REASONING REWARD FOR VIDEO TEMPORAL REASONING Sicheng Tao1, Jungang Li1,2, Yibo Yan1,2, Junyan Zhang1, Yubo Gao1, Hanqian Li1 ShuHang Xun3, Yuxuan Fan1, Hong Chen1,2, Jianxiang He1, Xuming Hu3 1 HKUST (GZ) 2 HKUST 3 HIT 5 2 0 2 5 ] . [ 1 3 1 1 1 2 . 9 0 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Video reasoning has emerged as critical capability for multimodal large language models (MLLMs), requiring models to move beyond static perception toward coherent understanding of temporal dynamics in complex scenes. Yet existing MLLMs often exhibit process inconsistency, where intermediate reasoning drifts from video dynamics even when the final answer is correct, undermining interpretability and robustness. To address this issue, we introduce MOSS-ChatV, reinforcement learning framework with Dynamic Time Warping (DTW)based process reward. This rule-based reward aligns reasoning traces with temporally grounded references, enabling efficient process supervision without auxiliary reward models. We further identify dynamic state prediction as key measure of video reasoning and construct MOSS-Video, benchmark with annotated reasoning traces, where the training split is used to fine-tune MOSS-ChatV and the held-out split is reserved for evaluation. MOSS-ChatV achieves 87.2% on the MOSS-Video (test) and improves performance on general video benchmarks such as MVBench and MMVU. The framework consistently yields gains across different architectures, including Qwen2.5-VL and Phi2, confirming its broad applicability. Evaluations with GPT-4o-as-judge further show that MOSS-ChatV produces more consistent and stable reasoning traces."
        },
        {
            "title": "INTRODUCTION",
            "content": "Multimodal Large Language Models (MLLMs) have shown remarkable progress in visionlanguage tasks such as image captioning, visual question answering, and video description (Cheng et al., 2024; Zhang et al., 2025a; Liang et al., 2024; Caffagni et al., 2024). Extending these advances from images to videos has attracted great attention, as videos contain richer temporal and causal information. However, video reasoningrequiring models to connect visual observations with temporal dynamics and causal dependenciesremains particularly challenging for current MLLMs. Existing Video-MLLMs are predominantly trained through supervised fine-tuning on large-scale videotext pairs (Li et al., 2024a). While effective for basic understanding, this paradigm leaves models weak in reasoning-intensive tasks. fundamental issue is the scarcity of datasets that provide fine-grained temporal reasoning supervision. Yet, videos inherently encode dense supervisory signals in their temporal evolution. The core challenge lies in exploiting these temporal signals to strengthen reasoning: models must not only recognize the present state but also infer future trajectories from context and world knowledge. Prior work such as VoT (Fei et al., 2024) has shown the close coupling between video prediction and reasoning, underscoring that temporal state prediction can serve as proxy for reasoning ability. To operationalize this insight, we construct MOSS-Video, dataset for video state prediction with annotated reasoning traces. The dataset is partitioned into training and test splits, enabling process-supervised learning while ensuring held-out evaluation. Reinforcement learning (RL) offers promising path for strengthening reasoning in MLLMs. However, recent studies (e.g., , Video-UTR (Yu et al., 2025)) reveal temporal hacking problem, where Core contribution. Correspondence: xuminghu97@gmail.com 1 Figure 1: Illustration of the responses across different models on the video state prediction task, where greentext indicates correctly reasoned key points and red text denotes reasoning errors. Comparative analysis reveals that MOSS-ChatV captures more fine-grained states (e.g., the surfers crouched position) compared to other models. Crucially, it accurately extrapolates this state (preparing for maneuver), thereby achieving more coherent and correct reasoning. models bypass temporal reasoning and directly guess outcomes. This highlights the necessity of explicit process-level supervision. RL with process feedback has proven effective in domains such as mathematics and code generation (Shao et al., 2024; Ye et al., 2025). Motivated by this, we design rule-based Process Reasoning Reward (PRR) for video reasoning. Specifically, we employ two-stage split-align strategy: (1) decomposing reasoning traces into sequential substeps, and (2) aligning generated and reference processes via subsequence Dynamic Time Warping (DTW). The resulting alignment distance provides reward signal that supervises temporal coherence without the need for learned reward model. Leveraging PRR together with the MOSS-Video training split, we fine-tune MOSS-ChatV using GRPO (DeepSeek-AI et al., 2025), as illustrated in Figure 2. Extensive experiments validate the effectiveness of our approach. See figure 1 for the case demonstrations. MOSS-ChatV achieves 87.2% accuracy on the MOSS-Video test set, surpassing strong closed-source baselines such as GPT-4o. It also improves general video understanding, reaching 67.6% on MVBench (Li et al., 2024b), and performs competitively on real-time benchmarks such as RTVBench (Xun et al., 2025). Moreover, the framework consistently boosts reasoning quality across architectures including Qwen2.5-VL and TinyLLaVA-Video. Automatic evaluation with GPT-4o as judge further shows that MOSS-ChatV produces more consistent and stable reasoning traces. Our main contributions are as follows: We construct MOSS-Video, video state prediction dataset with reasoning annotations, split into training and test partitions for process-supervised reinforcement learning and held-out evaluation. We propose rule-based Process Reasoning Reward (PRR) based on subsequence DTW and integrate it into reinforcement learning framework, MOSS-ChatV, trained with GRPO. This design enables efficient temporal alignment and process supervision without training additional reward models. Through extensive experiments, we demonstrate that MOSS-ChatV achieves state-of-the-art performance on the MOSS-Video (test), improves general video understanding benchmarks such as MVBench and MMVU, and yields consistent gains across different architectures including Qwen2.5-VL and TinyLLaVA-Video."
        },
        {
            "title": "2.1 VIDEO STATE PREDICTION AND REASONING",
            "content": "We consider video state prediction as follows: given video and query specifying target object, the model must (i) identify the object, (ii) infer its current or imminent state, and (iii) provide temporally grounded explanation. An illustrative example is shown in Figure 1. VoT (Fei et al., 2024) demonstrates that decomposing the task via Chain-of-Thought (CoT)including task definition, object recognition/tracking, behavior analysis, answer ranking, and verificationyields human-like reasoning path and highlights the tight coupling between prediction and reasoning. Different from the prompt-based paradigm in VoT, our approach learns this capability via reinforcement learning with process-level reward, integrating temporal reasoning into the models latent space to enable end-to-end reasoning and prediction."
        },
        {
            "title": "2.2 GROUP RELATIVE POLICY OPTIMIZATION (GRPO)",
            "content": "Recent work (DeepSeek-R1) (DeepSeek-AI et al., 2025) introduced Group Relative Policy Optimization (GRPO), which has spurred effective adaptations for multimodal LLMs (Feng et al., 2025; Li et al., 2025; Wang et al., 2025b; Zhang et al., 2025b). At high level, for each input, GRPO samples group of candidate responses from the current policy πθ, compares their relative performance via scalar reward, and updates the policy without learning value function. We adopt GRPO as our optimization backbone due to its simplicity and strong empirical stability. Notation. For one input, let the sampled response set be = {oi}G rewards {Ri}G i=1. GRPO computes standardized advantage for each response: i=1 with corresponding scalar Ai = Ri µ σ , µ = mean(cid:0){Ri}G i=1 (cid:1), σ = std(cid:0){Ri}G i= (cid:1). (1) The learning objective encourages higher-advantage responses under importance weighting, while regularizing the policy against fixed reference policy πref: (cid:34) (cid:19) (cid:19)(cid:35) (cid:18) πθ(oi) πold θ (oi) Ai, clip (cid:18) πθ(oi) πold θ (oi) , 1 ϵ, 1 + ϵ Ai (2) LGRPO(θ) = o(πold θ ) 1 (cid:0)πθ (cid:88) min i=1 (cid:13) (cid:13) πref (cid:1) β DKL θ Here πold denotes the behavior policy used for sampling the group, ϵ denotes the range of the clip operation, and DKL() is the KullbackLeibler divergence. The importance ratio reweights each response oi to correct for the sampling distribution, while the KL term (scaled by β > 0) controls policy drift. Accuracy Reward. For multiple-choice or short-answer settings, binary accuracy signal provides simple yet effective supervision: Racc(amodel, agt) = (cid:40) 1, 0, if amodel = agt, otherwise. (3) Format Reward. , <think>...</think><answer>...</answer>) to expose intermediate reasoning. Let omodel denote the full model output and the required format: In many applications, outputs must follow specified schema (e.g., Rfmt(omodel, F) = (cid:40) 1, 0, if omodel adheres to F, otherwise. (4) Accuracy and format rewards are effective foundations for RL fine-tuning, but they do not explicitly supervise temporal logic. In our method (Section 3), we therefore introduce process-level reward to align intermediate reasoning with reference temporal processes, complementing Racc and Rfmt within the GRPO framework. Algorithm 1 summarizes the overall optimization steps. 3 Figure 2: Overall training pipeline of MOSS-ChatV. (a) Construction of the MOSS-Video dataset from ShareGPT4Video with multi-level temporal annotations, where future states are masked as prediction targets. (b) Subsequence DTW alignment: green dashed lines denote strict sequential matching, while red solid lines allow jumps (jump step=2) to reduce cumulative distance. (c) GRPO workflow integrating accuracy, format, and process rewards."
        },
        {
            "title": "3 PROCESS REASONING REWARD",
            "content": "Addressing the limitations of conventional rewards in guiding complex temporal reasoning, we introduce Process Reasoning Reward (PRR), denoted as Rproc. This reward leverages reference annotations embodying an ideal gold standard reasoning process. Crucially, this mechanism achieves nuanced process supervision by effectively leveraging efficient, robust algorithms, avoiding the need for potentially complex or computationally expensive large model-based evaluators. Reasoning Step Serialization The first step is segmentation for reasoning texts. The models intermediate reasoning (e.g., content within <think>...</think> tags) and the reference counterpart are segmented into sequences of textual steps using NLP tools (e.g., nltk library). Though not affecting overall temporal information, this segmentation enables finer-grained analysis in the next step by splitting long texts into sequences. Let Tgen represent the intermediate reasoning content generated by the model, and Tref represent the reference reasoning content. These are segmented into sequences of textual steps using NLP tools (denoted as ): Seqgen = {g1, . . . , gm} = (Tgen) Seqref = {r1, . . . , rn} = (Tref ) (6) (7) Temporal Alignment via Subsequence DTW For the second step, We employ Subsequence Dynamic Time Warping (SDTW), detailed in Algorithm 2, highly efficient dynamic programming algorithm, to quantify the alignment between two sequences with different lengths. SDTW optimally identifies the best-matching subsequence within the models reasoning sequence (Seqgen) that corresponds to the entire reference sequence (Seqref ), by minimizing cumulative distance. This cumulative distance, minimized by SDTW, is built upon the pairwise distances d(gj, ri) between an individual generated step gj Seqgen and the annotated reference step ri Seqref . To define d(gj, ri), our goal is to comprehensively yet efficiently measure the textual similarity between these steps. This is achieved by leveraging several rule-based ROUGE scores. We use ROUGE-1 and ROUGE-2 to capture n-gram overlap between gj and ri. To evaluate sequence-level structural similarity, ROUGE-L is used for preserving the sentence-internal logical order within each step. The distance d(gj, ri) is then formally defined as one minus the average of average of these ROUGE scores: ROUGEavg(gj, ri) = ROUGE-1(gj, ri) + ROUGE-2(gj, ri) + ROUGE-L(gj, ri) 3 The minimum cumulative distance is then defined as: d(gj, ri) = 1 ROUGEavg(gj, ri) Dsdtw = SU BSEQU EN CE_DT (D) (8) (9) (10) 4 Algorithm 1 GRPO with Process Reasoning Reward (PRR) for one training sample Require: Training sample (V, Q, agt, Tref); Policy model Mpolicy(πθ); Reference model Mref(πref);"
        },
        {
            "title": "Format F",
            "content": "1: Sample candidate outputs from policy: = {oi = (Tgen, amodel)}G 2: Initialize reward list = [ ] 3: for each oi do 4: 5: 6: Segment reference: Seqref = {r1, . . . , rn} (Tref) Segment generation: Seqgen = {g1, . . . , gm} (Tgen) Build distance matrix Rmn with i=1 Dj,k = 1 ROUGEavg(gj, rk), [1, m], [1, n]. 7: 8: 9: Compute Dsdtw SUBSEQUENCE_DTW(D) Process reward: Rproc = exp(α Dsdtw) Total reward: Ri = Racc(amodel, agt) + Rfmt(oi, F) + Rproc"
        },
        {
            "title": "Append Ri to R",
            "content": "10: 11: end for 12: Standardize advantages: Ai = Ri µ σ , µ = mean(R), σ = std(R) 13: Compute GRPO objective with clipping: LGRPO(θ) = oπold θ (cid:34) 1 (cid:0)πθ (cid:88) (cid:32) min i=1 (cid:13) (cid:13) πref (cid:1). β DKL 14: Update policy: Mpolicy.update(LGRPO) πθ(oi) πold θ (oi) Ai, clip (cid:18) πθ(oi) πold θ (oi) (cid:19) (cid:33)(cid:35) , 1 ϵ, 1 + ϵ Ai (5) Algorithm 2 Subsequence DTW 1: function SUBSEQUENCE_DTW(D, kref, ktarget) D: Cost matrix (n m), kref: max reference jump, ktarget: max target jump 2: 3: 4: 5: 6: 7: Initialize R(n+1)(m+1) with P[0, j] 0 for [0, m], P[i, 0] for [1, n] for 1 to do for 1 to do diag_cost P[i 1, 1] up_cost min1kmin(kref,i) P[i k, j] (vertical move) Match current points (diagonal move) Skip points in reference sequence left_cost min1kmin(ktarget,j) P[i, k] (horizontal move) Skip points in target sequence P[i, j] D[i, j] + min(diag_cost, up_cost, left_cost) end for 8: 9: 10: 11: 12: end function end for return minj[1,m] P[n, j] Shortest distance to any endpoint in target sequence We adopt Subsequence Dynamic Time Warping (SDTW) for its ability to align reference reasoning path (Seqref ) within potentially longer model-generated sequences (Seqgen), enabling process supervision with explicit temporal signals. key advantage is SDTWs compatibility with reinforcement learning: it avoids penalizing exploratory segments outside the optimal alignment while still rewarding correct paths. The algorithm provides tunable alignment strictness through parameters like jump steps (Algorithm 2, figure 2), permitting controlled tolerance for minor deviations in the reasoning trajectory. This balance of flexibility and precision makes SDTW ideal for guiding reasoning processes without stifling exploration. 5 Distance-to-Reward Transformation The final minimum cumulative distance Dsdtw from SDTW is transformed into the reward value Rproc via transformation function : Rproc = (Dsdtw) (11) (Dsdtw) = exp(α Dsdtw) (12) where α > 0 is tunable hyperparameter that controls the sensitivity or decay rate of the reward with respect to the distance. Then we can get the total reward Rtotal,i for the i-th response in the sampled group of responses, by combining its specific process reward Rproc,i with its accuracy Racc,i and format Rfmt,i: Rtotal,i = Rproc,i + Racc,i + Rfmt,i (13) This Rtotal,i corresponds to the Ri used in the GRPO advantage calculation (Equation ??) for the i-th response within the group {o1, . . . , oG}. Consequently, the resulting Rproc provides computationally efficient yet powerful reward signal for reinforcement learning. It uniquely encourages temporal coherence in reasoning, validates the inclusion and ordering of essential logical steps, and maintains sensitivity to the relevance of generated content, thereby offering comprehensive guidance towards generating both accurate and logically sound reasoning processes. MOSS-Video Dataset To support process-supervised reinforcement learning, we construct MOSSVideo, large-scale video state prediction dataset derived from ShareGPT4Video (Chen et al., 2024). Each sample is annotated with object states and corresponding reasoning traces, enabling models to predict future states conditioned on visual context. The dataset is partitioned into training split (11,654 samples, 1,218 unique videos) and held-out test split (2,836 samples, 479 unique videos). Basic statistics are summarized in Table 1, including average video length and annotation span. Annotation pipelines and further details are provided in Appendix A. Table 1: Comparison of MOSS-Video with representative video temporal reasoning datasets. Our dataset uniquely supports state prediction with explicit reasoning annotations. Dataset #Samples Avg. Video Len (s) Understanding Reasoning Prediction ViTiB (Zhang et al., 2023) NeXT-QA (Xiao et al., 2021) Video-R1-CoT-165K (Feng et al., 2025) MOSS-Video (train) MOSS-Video (test) 1,382 3,870 116k 11,654 2,836 40 27.73 28."
        },
        {
            "title": "4 EXPERIMENT",
            "content": "We directly performed reinforcement fine-tuning on the Qwen2.5VL model, leveraging the training frameworks provided by Open-R1-Video (Wang & Peng, 2025) and Video-R1 (Feng et al., 2025), and utilizing the MOSS-Video train set. We selected comprehensive suite of benchmarks for the holistic evaluation of MOSS-ChatV. This suite includes MVBench (Li et al., 2024b), TempCompass (Liu et al., 2024a), Video MME (Fu et al., 2024), RTV-Bench (Xun et al., 2025) and the MOSS-Video test set for our state prediction scenarios. These benchmarks collectively assess wide range of video understanding capabilities, including temporal reasoning, action recognition, causal inference, and narrative comprehension. To demonstrate our methods generalizability, we further experiment on TinyLLaVA-Video (Zhang et al., 2025b), validating its effectiveness with different language model (Phi2) and visual encoder (SigLIP). The aggregated evaluation results are presented in Table 2. Specific configurations for our evaluations included sampling temperature of 0 to ensure deterministic outputs and an input video resolution of approximately 448x448 pixels. We tested our experiment on 4 NVIDIA A800 and trained MOSS-ChatV on 8 NVIDIA A800. 4.1 RESULTS In Table 2, our MOSS-ChatV model achieves state-of-the-art performance on MVBench, VideoMME, RTVBench, and MOSS-Video test compared to baseline models, Qwen2.5-VL, and the same architecture model Video-R1. It also demonstrates improvements over the Qwen2.5-VL on TempCompass. 6 Table 2: Results of MOSS-ChatV and baselines on (a) general video understanding benchmarks and (b) video reasoning benchmarks. All results use 32-frame input setting. Our method consistently improves performance across both categories. (a) General Benchmarks Model # LLM MVBench VideoMME TempCompass Qwen2.5-7B Qwen2.5-VL Bai et al. (2025) Qwen2-7B LLaVA-OneVision Li et al. (2024a) Phi2-3B TinyLLaVA-3B Zhang et al. (2025b) Phi2-3B TinyLLaVA-3B + PRR Qwen2-7B Video-UTR Yu et al. (2025) VideoChat-R1 Li et al. (2025) Qwen2.5-7B VideoChat-R1-thinking Li et al. (2025) Qwen2.5-7B Qwen2.5-7B Video-R1 Feng et al. (2025) MOSS-ChatV (ours) Qwen2.5-7B 67.1 56.7 28.8 29.0 58.8 66.2 63.9 67.6 59.7 58.2 34.5 35.1 52.6 58.8 58.3 59.3 60.0 72.2 32.4 45.1 59.7 73.9 75.0 73. 72.9 (b) Reasoning Benchmarks Model RTV-Bench MOSS-Videotest MMVUmc VideoMMMU VCR-Bmc VSI-Bmc VSI-Breg Qwen2.5-VL LLaVA-OneVision TinyLLaVA-3B TinyLLaVA-3B + PRR Video-UTR VideoChat-R1 VideoChat-R1-thinking Video-R1 MOSS-ChatV (ours) 32.8 34.5 46. 46.6 67.0 48.1 65.9 82.5 58.9 70.8 70.1 73.3 86.6 60.0 39.0 40.3 62.7 64.2 64.8 66.2 48.1 50.0 49.2 52. 50.2 33.7 34.5 35.3 38.4 35.3 35.3 35.9 30.8 35.2 24.3 30.0 39. 28.2 The results from MOSS-Video test particularly indicate that reasoning capabilities contribute positively to video prediction tasks. Notably, while neither Qwen2.5-VL nor Video-R1 were trained on MOSS-Video Train data, but Video-R1 shows significant metric improvements, suggesting the benefits of reasoning. We tested MOSS-Video using different input number of frames, result shown in Figure 3. The results demonstrate that increasing input frames enhances state prediction performance. MOSS-ChatV likely reaches peak accuracy with fewer frames due to its more efficient information extraction and reasoning capability. It is worth emphasizing that our solution utilizes only single-task dataset for video prediction, yet achieves performance gains across general video benchmarks. The improvements are especially pronounced on MVBench and VideoMME - both requiring complex reasoning - demonstrating that our approach effectively unlocks the models latent potential. These results collectively provide evidence that video prediction tasks indeed enhance models reasoning capabilities. Figure 3: Performance impact of varying input frame counts. 4.2 SDTW VS. DTW When comparing reasoning sequences, traditional Naive Dynamic Time Warping (DTW) and Subsequence DTW exhibit distinct behaviors. Naive DTW attempts to achieve complete match between two sequences through warping, which can lead to single element being mapped to multiple elements. This significantly inflates the distance metric for sequences of unequal length, characteristic we find undesirable as it unduly penalizes valid model explorations that extend beyond the shortest annotated reasoning chain. Our experiments, Figure 4, demonstrate that Naive DTW can induce reward hacking phenomenon: when the model outputs very short reasoning, the 7 Figure 4: Figure (a) shows that with Subsequence DTW (SDTW), response lengths initially fluctuate due to exploration but gradually converge to stable range. Figure (b) reports training without process supervision, where response lengths remain unstable. Figure (c) illustrates Naive DTW, which induces reward hacking: the model shortens its reasoning drastically to exploit the distance metric. DTW distance is minimized, leading to trivial outputs like <think>Based on the video content, the correct answer is A</think><answer>A</answer>. This observation also highlights that treating annotated reasoning processes solely as an absolute gold standard for training offers limited benefits for model improvement. Consequently, our strategy positions annotated reasoning as minimal gold standard. While ensuring the quality of reasoning, this approach avoids overly restricting the models legitimate explorations beyond this baseline, thereby aiming to more comprehensively unlock and leverage the models latent potential. 4.3 ABLATION STUDY Table 3: Ablation Results Model MVBench VideoMME MOSS-Video Qwen2.5-VL-7B Qwen2.5-VL-7B+SFT (MOSS-ChatV-SFT) Qwen2.5-VL-7B+T-GRPO (Video-R1) Qwen2.5-VL-7B+GRPO (MOSS-ChatV-no-PPR) Qwen2.5-VL-7B+GRPO+Process Reasoning Reward (MOSS-ChatV) 67.09 65.12 1.97 63.90 3.19 65.23 1.86 67.60 0.51 59.67 55.24 4.43 59.30 0.37 55.30 4.37 59.96 0.29 67.00 71.44 4.44 73.26 6.26 84.17 17.17 86.62 19. We conduct ablation experiments using the MOSS-Video Train dataset, comparing three variants: MOSS-ChatV , MOSS-ChatV-no-PRR (MOSS-ChatV without process supervision), and supervised fine-tuned MOSS-ChatV-SFT. The results (see Table 3) demonstrate that the complete MOSS-ChatV achieves superior performance across all benchmarks. The absence of process supervision in MOSSChatV-no-PRR leads to degraded temporal reasoning performance, confirming the importance of alignment signals for video understanding. Notably, even without temporal supervision, MOSSChatV-no-PRR outperforms MOSS-ChatV-SFT, highlighting the advantages of reinforcement learning over pure supervised training for video reasoning tasks. 4.4 MLLM AS JUDGE FOR REASONING QUALITY EVALUATION Table 4: MLLM as judge for evaluating the performance of reasoning across different models. Method Reasoning-Answer Consistency Reasoning Content Repetitiveness Logical Coherence & Knowledge Relevance to Video Content QWEN2.5-VL VIDEO-R1 MOSS-CHATV-NO-PRR MOSS-CHATV 0.69 0.78 0.72 0.79 8.87 4.14 7.80 7.23 6.97 6.87 7.80 7. 6.82 6.57 7.45 7.35 To investigate the quality of video reasoning texts, we employed GPT-4o as judge to conduct multi-dimensional quality assessment of the reasoning and answers generated by models. This assessment framework comprises four core metrics, for which GPT-4o assigns score for each dimension (detailed dimension and prompts can be found in Appendix B.1). Process supervision within reinforcement fine-tuning demonstrates significant contribution to enhancing the quality of reasoning across multiple dimensions. MOSS-ChatV exhibits well8 balanced and overall excellent performance profile, shown in table 4. Compared to Video-R1, while achieving comparable performance in Reasoning-Answer Consistency, MOSS-ChatV demonstrates higher information density (i.e., lower repetitiveness), more robust logical coherence, and greater relevance to video content. Furthermore, when contrasted with its variant MOSS-ChatV-no-PRR, MOSS-ChatV achieves higher degree of Reasoning-Answer Consistency. This suggests that process supervision effectively guides the model towards generating more credible and trustworthy outputs. Although Qwen2.5-VL records the highest information density, its comparatively lower scores on other metrics imply that this conciseness might stem from unconstrained cognitive divergence, which could be detrimental to the generation of high-quality reasoning content."
        },
        {
            "title": "5.1 ADVANCED VIDEO-LLM",
            "content": "With the burgeoning development of Multimodal Large Language Models (MLLMs), such as Qwen (Wang et al., 2024a) and InternVL (Wang et al., 2024b; 2025c), video understanding has emerged as critical dimension for evaluating model capabilities. To enhance models comprehension of video content, researchers have employed variety of strategies. For instance, VideoChatGPT (Maaz et al., 2023) focuses on improving model proficiency in video dialogue, description, and reasoning by introducing video-specific instruction-tuning datasets and quantitative evaluation framework. Other approaches, exemplified by models like NVILA (Liu et al., 2024b), LongVU (Shen et al., 2024), and VideoLLaMA3 (Zhang et al., 2025a), enhance their capacity to process long videos through various visual token compression techniques, such as removing redundant tokens or employing MLP-based compression. Furthermore, models such as LLaVA-OV (Li et al., 2024a) are typically pre-trained on large-scale video-text pair datasets (video training data) and subsequently fine-tuned using instruction data for tasks like video question answering and description generation to adapt to diverse video understanding scenarios. These works collectively provide an excellent foundation for advancing video reasoning capabilities in Video-LLMs. While these methods advance general video understanding, our work introduces reinforcement learning framework to directly supervise the temporal reasoning process. 5.2 REASONING AND REINFORCEMENT LEARNING IN VIDEO-LLMS To enhance the reasoning capabilities of video models, researchers have made numerous attempts, such as utilizing rationale construction, structural reasoning, objective granularity, and other methods (Wang et al., 2025a). Recent advances in Reinforcement Learning (RL) have significantly improved LLM alignment and specialized capabilities, as seen in reasoning LLMs (DeepSeekAI et al., 2025). This success has spurred RL-based enhancements for Multimodal LLMs (Yang et al., 2025; Meng et al., 2025). Specifically, for video modality, VideochatR1 (Li et al., 2025) and TimeZero (Wang et al., 2025b) leverage RL rewards for temporal grounding, while TinyLLaVA-VideoR1 (Zhang et al., 2025b) demonstrates RLs effectiveness even on small models. Video-R1 (Feng et al., 2025) employs contrastive RL to improve temporal understanding. Our approach is distinguished by novel, rule-based Process Reasoning Reward (PRR), which offers more granular supervision on the reasoning path itself."
        },
        {
            "title": "6 CONCLUSION",
            "content": "Through analyzing the relationship between video state prediction tasks and video reasoning capabilities, we demonstrate their mutual reinforcement. Based on this insight, we introduce MOSS-Video, dedicated dataset for training and evaluating video state prediction task. For reinforcement fine-tuning of video modalities, we propose Process Reasoning Reward (PRR), rule-based reward mechanism. Comparative and ablation experiments confirm the effectiveness of our approach. Using single-task training data alone, we achieve holistic improvements in video analysis performance while maintaining stable reasoning quality. In summary, we find that model reasoning capability in video contexts deserves greater attention. Through MOSS-ChatV, we verify that reinforcement fine-tuning with process supervision significantly enhances video reasoning performance, achieving performance gains and state-of-the-art results even under low-quality video inputs."
        },
        {
            "title": "7 ETHICS STATEMENT",
            "content": "This research complies with ethical standards. It utilizes datasets that are either synthetic or publicly available, and contains no sensitive or personally identifiable information. The study involves no direct human subjects, nor does it pose any privacy or security concerns. All methodologies and experiments were conducted in accordance with applicable laws and established research integrity practices. There are no conflicts of interest, no undue influence from external sponsorship, and no concerns related to discrimination, bias, or fairness. Moreover, this research does not lead to any harmful insights or applications."
        },
        {
            "title": "8 REPRODUCIBILITY STATEMENT",
            "content": "We have taken steps to ensure the reproducibility of the results presented in this paper. The experimental settings, including datasets and model designs, are thoroughly described in Section 4. Source code will be made publicly available upon acceptance."
        },
        {
            "title": "9 LLM USAGE STATEMENT",
            "content": "In this work, large language models (LLMs) were used exclusively to assist with writing, editing, and LaTeX formatting. Their role was confined to enhancing clarity, grammar, and overall presentation; they had no impact on the design of experiments, data processing, analysis, or the interpretation of results."
        },
        {
            "title": "REFERENCES",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. URL https://arxiv.org/abs/2502.13923. Davide Caffagni, Federico Cocchi, Luca Barsellotti, Nicholas Moratelli, Sara Sarto, Lorenzo Baraldi, Marcella Cornia, and Rita Cucchiara. The revolution of multimodal large language models: survey. arXiv preprint arXiv:2402.12451, 2024. Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, Li Yuan, Yu Qiao, Dahua Lin, Feng Zhao, and Jiaqi Wang. Sharegpt4video: Improving video understanding and generation with better captions. arXiv preprint arXiv:2406.04325, 2024. Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. CoRR, 2024. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Meishan Zhang, Mong-Li Lee, and Wynne Hsu. Video-of-thought: Step-by-step video reasoning from perception to cognition, 2024. URL https://arxiv.org/abs/2501.03230. Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms, 2025. URL https://arxiv.org/abs/2503.21776. Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024a. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2219522206, 2024b. Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal perception via reinforcement fine-tuning. arXiv preprint arXiv:2504.06958, 2025. Zijing Liang, Yanjie Xu, Yifan Hong, Penghui Shang, Qi Wang, Qiang Fu, and Ke Liu. survey of multimodel large language models. In Proceedings of the 3rd International Conference on Computer, Artificial Intelligence and Control Engineering, pp. 405409, 2024. Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos?, 2024a. URL https: //arxiv.org/abs/2403.00476. Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, et al. Nvila: Efficient frontier visual language models. arXiv preprint arXiv:2412.04468, 2024b. Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, and Wenqi Shao. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning, 2025. URL https://arxiv.org/abs/2503.07365. 11 Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/ 2402.03300. Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, et al. Longvu: Spatiotemporal adaptive compression for long video-language understanding. arXiv preprint arXiv:2410.17434, 2024. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a. Xiaodong Wang and Peixi Peng. Open-r1-video, 2025. Yaoting Wang, Shengqiong Wu, Yuecheng Zhang, Shuicheng Yan, Ziwei Liu, Jiebo Luo, and Hao Fei. Multimodal chain-of-thought reasoning: comprehensive survey, 2025a. URL https: //arxiv.org/abs/2503.12605. Ye Wang, Boshen Xu, Zihao Yue, Zihan Xiao, Ziheng Wang, Liang Zhang, Dingyi Yang, Wenxuan Wang, and Qin Jin. Timezero: Temporal video grounding with reasoning-guided lvlm. arXiv preprint arXiv:2503.13377, 2025b. Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Zun Wang, Yansong Shi, et al. Internvideo2: Scaling foundation models for multimodal video understanding. In European Conference on Computer Vision, pp. 396416. Springer, 2024b. Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, et al. Internvideo2. 5: Empowering video mllms with long and rich context modeling. arXiv preprint arXiv:2501.12386, 2025c. Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of questionIn Proceedings of the IEEE/CVF conference on answering to explaining temporal actions. computer vision and pattern recognition, pp. 97779786, 2021. Shuhang Xun, Sicheng Tao, Jungang Li, Yibo Shi, Zhixin Lin, Zhanhui Zhu, Yibo Yan, Hanqian Li, Linghao Zhang, Shikang Wang, et al. Rtv-bench: Benchmarking mllm continuous perception, understanding and reasoning through real-time video. arXiv preprint arXiv:2505.02064, 2025. Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, Bo Zhang, and Wei Chen. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization, 2025. URL https://arxiv.org/ abs/2503.10615. Yufan Ye, Ting Zhang, Wenbin Jiang, and Hua Huang. Process-supervised reinforcement learning for code generation, 2025. URL https://arxiv.org/abs/2502.01715. En Yu, Kangheng Lin, Liang Zhao, Yana Wei, Zining Zhu, Haoran Wei, Jianjian Sun, Zheng Ge, Xiangyu Zhang, Jingyu Wang, et al. Unhackable temporal rewarding for scalable video mllms. arXiv preprint arXiv:2502.12081, 2025. Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025a. Xingjian Zhang, Siwei Wen, Wenjun Wu, and Lei Huang. Tinyllava-video-r1: Towards smaller lmms for video reasoning. arXiv preprint arXiv:2504.09641, 2025b. Yongheng Zhang, Xu Liu, Ruihan Tao, Qiguang Chen, Hao Fei, Wanxiang Che, and Libo Qin. ViTCoT: Video-text interleaved chain-of-thought for boosting video understanding in large language models. In Proceedings of the 31st ACM International Conference on Multimedia (ACM MM), 2023. 12 DETAILS OF MOSS-VIDEO We leverage high-quality ShareGPT4Video as our primary data source, employing two parallel annotation pipelines to capture both coarseand fine-grained object states. In the coarse-grained pipeline, GPT4-o processes each videos annotation file to produce triplets of the form Object, State, Timestamp, thereby characterizing an objects state over defined temporal interval. Concurrently, in the fine-grained pipeline, GPT4-o extracts more detailed triplets Object, State: Description, Timestamp, which enrich each state with specific textual description at precise moment. Finally, we again invoke GPT4-o to integrate these two annotation streams into unified temporal model of object dynamics, from which we automatically generate questionanswer pairs that probe the predicted future states of objects. Figure A.1: The example of MOSS-Video. MLLM-AS-A-JUDGE FOR REASPONSE QUALITY We used GPT-4-o to verify the reasoning process and the final response results of the reasoning model. The specific evaluated dimensions are listed below: Reasoning-Answer Consistency (0 or 1): This is binary metric. score of 1 is awarded if the final conclusion of the reasoning aligns with the content of the model-selected option; otherwise, it receives score of 0. Reasoning Content Repetitiveness (0-10): This assesses the presence of redundant information in the reasoning process. Higher repetitiveness results in lower score, aiming to measure information density and avoid the amplification of potential biases or errors. Logical Coherence and Knowledge Accuracy (0-10): This directly evaluates the intrinsic quality of the reasoning process. The more rigorous the logic and the more accurate the application of world knowledge, the higher the score. Reasoning-Video Content Relevance (0-10): This measures how closely the reasoning is based on the video content. Higher relevance yields higher score, aiming to penalize unfounded speculations or associations unrelated to the video. In addition, the specificprompts used can refer to B.1. 13 Prompts for Reasoning Process Evaluation Given the following video captions, extract object-centric information for video prediction task. Reference Video Segments (what can be seen): for caption in reference_captions: prompt += f\"Time {caption[time_stamp]}s: {caption[content]}nn\" Prediction Segment (what needs to be predicted): Time {prediction_caption[time_stamp]}s: {prediction_caption[content]} Task 1: For each object in the video, provide coarse-grained information as <object, state, time> triplets for both reference and prediction segments. Task 2: For each object, provide fine-grained information as <object, change description, time> triplets for how objects change over time. Format your response as follows: COARSE-GRAINED INFORMATION: Object 1: At time [timestamp]: [state description] At time [timestamp]: [state description] . . . Object 2: At time [timestamp]: [state description] . . . FINE-GRAINED INFORMATION: Object 1: From time [start] to [end]: [detailed change description] . . . Object 2: From time [start] to [end]: [detailed change description] . . . Focus only on objects that appear in the prediction segment. Be specific and detailed in your descriptions. Figure A.2: Prompt template used for evaluating the reasoning process of video question answering models. Prompts for Reasoning Process Evaluation You are professional video question answering reasoning process evaluator. Your task is to evaluate the quality of the reasoning process ONLY, based on the provided video frames, question, options, and models reasoning text. You DO NOT need to judge the correctness of the models final answer. Please evaluate the reasoning process based on the following dimensions: 1. Reasoning Conclusion and Answer Tag Consistency (0 or 1 point): Criterion: Check whether the conclusion in the reasoning text semantically matches the option marked by the <answer> tag. You should carefully analyze and consider the logic within the <think> tag. Scoring: 1 point: Consistent. 0 points: Inconsistent. 2. Reasoning Content Repetitiveness (010 points, lower score for more repetition): Criterion: Assess whether the reasoning content contains unnecessary repetition of words, phrases, or semantics. Scoring Guidelines: 910 points: Very concise, no unnecessary repetition, high information density. 68 points: Slight repetition or reasonable restatement for emphasis, overall flow is smooth. 35 points: Obvious repetition, but core idea is still discernible. 02 points: Massive repetition, almost no new information. 3. Reasoning Logical Coherence and Knowledge Accuracy (010 points): Criterion: Evaluate if the reasoning steps are clear and coherent, the logical chain complete, and any assumptions reasonable and correct. Scoring Guidelines: 910 points: Rigorous logic, well-organized, sufficient argumentation, accurate assumptions. 68 points: Generally coherent with minor flaws. 35 points: Obvious breaks or minor errors not affecting main conclusion. 02 points: Chaotic or contradictory logic, erroneous assumptions. 4. Reasoning and Video Content Relevance (010 points, lower score for more deviation): Criterion: Assess whether observations and conclusions are strictly based on provided video frames. Scoring Guidelines: 910 points: Strictly based on video content with strong evidence. 68 points: Primarily based on content with minor reasonable inference. 35 points: Mostly imagination or misunderstanding of video. 02 points: Completely unrelated or speculative. [Input Information] Video Frames: {num_frames_provided} frames are provided. (Actual frames are sent as image data) Question: {question_text} Model Reasoning Text: {model_reasoning_text} Models Answer Tag Content: <answer>{model_answer_tag_content}</answer> [Your Evaluation Output] Please provide your evaluation scores strictly in the following format, one line per dimension, containing only the score: Dimension1_Score: Dimension2_Score: Dimension3_Score: Dimension4_Score: [0 or 1] [0-10] [0-10] [0-10] Figure B.1: Prompt template used for evaluating the reasoning process of video question answering models."
        }
    ],
    "affiliations": [
        "HIT",
        "HKUST",
        "HKUST (GZ)"
    ]
}