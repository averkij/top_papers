{
    "paper_title": "Predicting the Order of Upcoming Tokens Improves Language Modeling",
    "authors": [
        "Zayd M. K. Zuhri",
        "Erland Hilman Fuadi",
        "Alham Fikri Aji"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multi-Token Prediction (MTP) has been proposed as an auxiliary objective to improve next-token prediction (NTP) in language model training but shows inconsistent improvements, underperforming in standard NLP benchmarks. We argue that MTP's exact future token prediction is too difficult as an auxiliary loss. Instead, we propose Token Order Prediction (TOP), which trains models to order upcoming tokens by their proximity using a learning-to-rank loss. TOP requires only a single additional unembedding layer compared to MTP's multiple transformer layers. We pretrain models of 340M, 1.8B, and 7B parameters using NTP, MTP, and TOP objectives. Results on eight standard NLP benchmarks show that TOP overall outperforms both NTP and MTP even at scale. Our code is available at https://github.com/zaydzuhri/token-order-prediction"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 8 2 2 9 1 . 8 0 5 2 : r Early preprint"
        },
        {
            "title": "PREDICTING THE ORDER OF UPCOMING TOKENS\nIMPROVES LANGUAGE MODELING",
            "content": "Zayd M. K. Zuhri, Erland Hilman Fuadi & Alham Fikri Aji MBZUAI zayd.zuhri@mbzuai.ac.ae"
        },
        {
            "title": "ABSTRACT",
            "content": "Multi-Token Prediction (MTP) has been proposed as an auxiliary objective to improve next-token prediction (NTP) in language model training but shows inconsistent improvements, underperforming in standard NLP benchmarks. We argue that MTPs exact future token prediction is too difficult as an auxiliary loss. Instead, we propose Token Order Prediction (TOP), which trains models to order upcoming tokens by their proximity using learning-to-rank loss. TOP requires only single additional unembedding layer compared to MTPs multiple transformer layers. We pretrain models of 340M, 1.8B, and 7B parameters using NTP, MTP, and TOP objectives. Results on eight standard NLP benchmarks show that TOP overall outperforms both NTP and MTP even at scale. Our code is available at https://github.com/zaydzuhri/token-order-prediction"
        },
        {
            "title": "INTRODUCTION",
            "content": "Figure 1: An overview of Token Order Prediction (TOP). Given an input token sequence, vocabulary, sequence length of 4 and window size of 4, TOP target sequence is constructed via Algorithm 1. The output hidden representation of the final layer goes to two separate unembedding heads for NTP and TOP. The final loss to optimize is sum of the NTP and TOP loss. Current large language models (LLMs) are trained to predict the next token in sequence during training, an unsupervised learning task often referred to as next-token prediction (NTP). Although simple, NTP has been very successful in creating powerful language models that can solve complex tasks and even reason over their context. However, NTP has received various criticisms over the past few years. notable argument by LeCun (2024) claims that NTP at inference time accumulates errors over every time step and inevitably falls off greatly in accuracy. This was however refuted by Bachmann & Nagarajan (2024), where it is 1 Early preprint argued that the main issue of NTP lies not in inference time error accumulation; rather, that teacherforcing is unable to learn an accurate next-token predictor in the first place. Building off ideas such as ProphetNet (Qi et al., 2020), Multi-Token Prediction (MTP) (Gloeckle et al., 2024) has emerged as relatively successful auxiliary learning task to improve NTP in LLM training. MTP adds multiple heads to the end of transformer that each predict different offset of tokens ahead. All MTP heads share the same trunk of transformer layers, with the hope that having these auxiliary heads leads to the model learning better internal representations that are considerate of not only the next immediate token, but also future tokens that may come after it. It has been shown that MTP improves performance of language models on certain generative tasks that require some level of look-ahead, such as coding and summarization. This method was used in the training of DeepSeek-V3 (DeepSeek-AI et al., 2024), although with sequential instead of parallel MTP heads. However, the original MTP paper (Gloeckle et al., 2024) showed that MTP does not generally improve language modeling performance on every standard NLP task, as evident from the downstream task performance seen in their Appendix G. Also important to note is that MTP does not seem to work on smaller models. In generative tasks, MTP harms performance and only starts to gain advantage over NTP for models with over 1-3 billion parameters. The number of future tokens to predict is also hyperparameter that needs to be set before training. This is critical because increasing the future token count requires adding more heads, which adds more parameters to train and more compute required. Furthermore, the paper shows that increasing the number of future tokens does not guarantee better performance even on the benefiting tasks, where it is shown that 4 future tokens performs better than 8 in coding. We aim to improve upon MTP by introducing different auxiliary training objective with the same goal as MTP: enhancing next-token prediction performance through better internal representations. However, instead of exactly predicting multiple future tokens, we propose that better training objective is to predict the order of upcoming tokens in the sequence with learning-to-rank loss. In this paper, we contribute the following: 1. We introduce Token Order Prediction (TOP), novel auxiliary training loss in addition to NTP to improve language modeling in general. 2. For each of the three training strategiesNTP, MTP, and TOPwe pretrain language models with sizes of 340M, 1.8B, and 7B parameters to better understand the impact of these strategies on models of different scales. 3. We evaluate these models on standard NLP benchmarks and show that TOP improves upon NTP and MTP even at scale."
        },
        {
            "title": "2 BACKGROUND",
            "content": "Next-token prediction (NTP) is the standard training objective for present-day language models. This task is learned by optimizing the cross-entropy loss over the sequence length. Given sequence length , model dimension D, vocabulary size and = x0:T +1 = x0, ..., xT +1 ZT +1 as the input token sequence, this loss is written as LN = (cid:88) t=0 log(Pθ(xt+1x0:t)) (1) where Pθ is the output probability given by the language model with parameters θ. The probability of the next token xt+1 given this model is written as Pθ(xt+1x0:t) = sof tmax(uN (hL ))[xt+1] (2) RD is generated by transformer up to the final layer where the hidden representation hL conditioned on x0:t, and the NTP head uN : RD RV is linear unembedding layer to project hL onto the vocabulary. The probability is taken at the index of the target token [xt+1]. Multi-Token Prediction (MTP) (Gloeckle et al., 2024) was proposed as an architectural modification that adds additional MTP heads1 in the form of parallel, singular transformer layers that each output 1Disambiguation: Architecturally, MTP heads are transformer blocks, as we follow the terminology from Gloeckle et al. (2024). Meanwhile, NTP head and TOP head are linear unembedding layers. Early preprint future token prediction at offset positions. Given as the number of future tokens to predict (including the next token), the MTP loss can be written as LM = (cid:88) t=0 log(Pθ(xt+1:t+N x0:t)) = (cid:88) (cid:88) t=0 n=1 log(Pθ(xt+nx0:t)) (3) If we define hL1 as the hidden representation before the last transformer layer and have fi for = 1, .., as the MTP heads in the form of singular transformer layers for each future token, and all heads share the same unembedding layer or NTP head uN , then Pθ(xt+nx0:t) = sof tmax(uN (fn(hL1 )))[xt+n] (4) MTP promises better performance on generative tasks such as coding, summarization, mathematical and abstractive reasoning, and other tasks that benefit from the look-ahead nature of MTP. MTP also allows the model to do form of self-speculative decoding, which speeds up inference to some degree. However, as mentioned earlier, MTP does not seem to improve overall language modeling performance on downstream tasks other than those mentioned above, struggling on standard NLP benchmarks. There have also been other MTP variants such as the one used for training DeepSeek V3 (DeepSeekAI et al., 2024) and Joint Multi-Token Prediction (JTP) (Ahn et al., 2025). Notably, DeepSeek V3 only used = 2 for their MTP training, which means the model only learns to predict the next two tokens. This might be related to key point of our paper, which is that MTP is too difficult an objective to be an effective auxiliary loss to NTP, especially for larger look-ahead values ."
        },
        {
            "title": "3 MOTIVATION",
            "content": "Our hypothesis for why MTP only partially improves language modeling is that MTP is too difficult as learning objective. If we look at the original MTP paper (Gloeckle et al., 2024), there are two empirical results that support this argument. First, MTP does not improve performance of small language models on generative tasks, such as coding. This suggests that certain capability threshold is required for MTPs multi-token modeling to be effective, which we observe to be in the 1B-3B parameter range. Second, we see that increasing the number of future tokens in MTP does not guarantee better performance overall. The ideal number of future tokens varies across different tasks. Not only does this make it difficult to determine the optimal number beforehand, it also indicates that there are thresholds of look-ahead distance where the difficulty of prediction starts to hurt learning instead of helping it. Figure 2: Training loss of small MTP transformer model with 16 MTP heads. 3 Early preprint To illustrate our argument, we train small 16M parameter transformer with 16 MTP heads and visualize the training loss of each MTP head in Figure 2. clear pattern emerges where the losses of predicting the tokens at positions + 1, ..., + 16 arrange themselves from bottom to top. Each future token farther away significantly worsens in loss compared to the immediate next token loss and shows decreased rate of loss descent, indicating the difficulty of exactly predicting far ahead. We believe that relaxing this MTP objective will make it more useful as an auxiliary loss."
        },
        {
            "title": "4 METHOD",
            "content": "We propose Token Order Prediction (TOP), novel auxiliary training loss for language modeling. Given sequence of tokens = x0, ..., xT ZT , we construct TOP target sequence = y0, ..., yT ZT where each yt ZV is vector over the vocabulary, where each index contains score ranking each token in the vocabulary based on their order in the sequence x, going in descending order from closest to furthest first appearance after xt. We also introduce hyperparameter, window size, within which the token order is evaluated. To better understand this target sequence, please refer to the pseudocode in Algorithm 1 and the visualization in Figure 1 on how to construct it. In practice, we have an optimized triton kernel for this function that creates the target sequence on the fly during training and practically incurs no overhead. Alternatively, one could also pre-process an entire dataset beforehand. Algorithm 1 Convert token sequence to TOP target sequence Goal: For each position, compute proximity score to the next occurrence of every token within window of size . end if if < then if x[t] is valid then next[x[t]] Require: Token sequence of length + , vocab size , window size . Ensure: Tensor of shape (T, ) 1: Initialize 2: Initialize next[v] + for all [0, 1] 3: for + 1 down to 0 do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end for for each [0, 1] do next[v] if 0 < then y[t, v] end for end if end if Iterate backwards through sequence Check if token is in vocabulary Record most recent position of token Length of output is only Steps ahead until next Closer tokens get larger score To train the model to order upcoming tokens as in the target sequence, we borrow loss formulation from the learning-to-rank literature (Pobrotyn et al., 2020), more specifically from ListNet (Cao et al., 2007). This listwise ranking loss resembles soft version of cross-entropy loss where the target distribution is arbitrary and not one-hot encoded, and instead is vector of scores. The TOP auxiliary loss is defined as follows: LT OP = (cid:88) t=0 sof tmax(yt) log(sof tmax(uT OP (hL ))) (5) Note that sof tmax(uT OP (hL )) is not probability distribution by definition, hence why we do not write it as Pθ. The correct way to think about it is to see uT OP (hL ) as the model prediction of the ranking in the form of proximity scores, and sof tmax(y) log(sof tmax(ˆy)) as the ranking loss defined in ListNet. Also note that there are no additional transformer layers for TOP. There is however an additional linear unembedding layer uT OP : RD RV in parallel to the NTP head. Both unembedding heads uN and uT OP receive the same hidden state hL , which is the output Early preprint Table 1: Training configuration and hyperparameters for {340M, 1.8B, 7B} models. Parameter Value Parameter Value Hidden size Num. layers Num. heads Num. KV heads Sequence length RoPE θ Tied embeddings Vocab size TOP window size MTP num. future tokens {1024, 2048, 4096} Optimizer {24, 32, 30} {16, 32, 32} {16, 32, 8} 4096 10,000 False 32,000 4096 4 Learning rate LR schedule Minimum LR (%) Warmup steps Device batch size Grad. accum. steps Global batch size Training steps Gradient clip max. AdamW {3e-4, 2e-4, 1.2e-4} Cosine 10% {1000, 2000, 2000} {16, 16, 8} {1, 1, 2} 128 {100k, 200k, 200k} 1.0 of the final transformer layer. We refer to these unembedding layers as NTP head and TOP head respectively. The final loss being optimized is simply sum of the NTP loss and the TOP loss: = LN + LT OP (6) Through the specific target sequence formulation and ranking loss function, the model is expected to learn an internal representation that can approximately construct the future sequence by returning the most probable order of upcoming tokens. This is expected to be an easier task than trying to exactly predict future token at some offset. It is also much more scalable compared to MTP, requiring only one additional unembedding matrix. While this matrix can be large, no additional parameters are needed even when adjusting window size, unlike MTP where each additional future token requires full extra transformer layer. In practice, we use fused triton kernel that readily performs both the unembedding and loss calculation block-wise in one pass, thus making the overhead minimal. This kernel is modification to fused linear cross-entropy loss kernels from Yang & Zhang (2024), resulting in the same performance as the non-modified version. We find that we do not need additional transformer blocks like MTP for TOP because both the NTP and TOP heads are mainly aligned on the same objective: assigning the highest score to the next token. Although it is possible to train language model on only the TOP objective, the resulting model will only be able to do greedy generation. An NTP head is still needed for non-greedy, probability sampling-based inference. At inference time, we remove the TOP head and use only the NTP head, making the model equivalent to the original transformer architecture."
        },
        {
            "title": "5 EXPERIMENTS AND RESULTS",
            "content": "5.1 GENERAL LANGUAGE MODELING We train models for the training methods NTP, MTP, and TOP in 3 sizes each: 340M, 1.8B, and 7B. These sizes are an approximate naming scheme; each model of each training method will have slightly different parameter counts. We try to match the parameter count at training time, excluding embedding parameters. This means that by setting the MTP number of future tokens to 4, the shared trunk will be reduced by 3 layers to account for the added MTP heads, as is done in the original MTP paper. We train all models on the sample-100BT subset of FineWeb-Edu. The 340M models are trained on 52B tokens, while the 1.8B and 7B models are trained on 104B tokens. We use the Flame framework and flash-linear-attention (Yang & Zhang, 2024) repository to implement and train our models. The full training configuration and hyperparameters for all model sizes are detailed in Table 1. We evaluated our models on eight standard NLP benchmarks: ARC challenge (Clark et al., 2018), Lambada (Paperno et al., 2016), PIQA (Bisk et al., 2020), SciQ (Welbl et al., 2017), Social IQa (Sap et al., 2019), TriviaQA (Joshi et al., 2017), NaturalQuestions Open (Kwiatkowski et al., 2019), and HellaSwag (Zellers et al., 2019), with full results presented in Table 2. Across all model sizes, TOP demonstrates overall better performance over both MTP and the baseline NTP on most tasks. 5 Early preprint Table 2: General language modeling evaluation results of NTP vs MTP vs TOP on standard NLP benchmarks. We report the NTP head only final training loss, the accuracy and perplexity on Lambada, the normalized accuracy on HellaSwag, ARC Challenge, PIQA, and SciQ, the accuracy on Social IQa, and the exact match score on NaturalQuestions Open and TriviaQA. Green/red values indicate the performance difference from the NTP baseline. Task NTP Loss Lambada NTP 2.39 340M MTP 2.46 TOP 2.40 NTP 2. 1.8B MTP 2.14 TOP 2.07 NTP 1.87 7B MTP 1.95 TOP 1.88 36.35 35.32 -1.03 37.07 +0.72 49.58 47.93 -1.65 50.34 +0.76 55.89 53.13 -2.76 57.03 +1.14 7.64 -0.32 30.34 35.31 +4.96 28.76 -1.58 11.38 13.69 +2.31 11.19 -0. 8.99 +1.03 7.97 HellaSwag 42.53 42.73 +0.20 43.57 +1.04 60.05 58.29 -1.76 60.45 +0.40 67.44 65.85 -1.58 68.73 +1.29 ARC Chlg. 28.84 29.86 +1.02 29.35 +0.51 38.65 40.61 +1.96 42.32 +3.67 45.65 45.56 -0.09 46.42 +0.77 PIQA SciQ 66.65 66.49 -0.16 67.57 +0.92 73.50 73.07 -0.44 74.16 +0.65 76.99 75.73 -1.25 76.39 -0.60 74.90 77.40 +2.50 79.80 +4.90 86.40 87.20 +0.80 87.90 +1.50 88.60 89.30 +0.70 91.60 +3.00 Social IQa 39.82 39.00 -0.82 39.00 -0.82 41.56 42.12 +0.56 42.53 +0.97 44.37 44.11 -0.26 43.91 -0.46 NQ Open TriviaQA 1. 4.93 2.35 +0.42 2.22 +0.28 4.54 4.46 -0.08 5.37 +0. 7.31 7.40 +0.08 7.70 +0.39 2.55 -2.37 4.37 -0.55 11.85 15.98 +4.13 18.93 +7.07 24.28 23.36 -0.92 30.90 +6.63 Our reproduction of MTP shows smaller MTP models achieve competitive results. This finding complements the original MTP paper, which did not report on models smaller than 7B on the standard NLP benchmarks. Consistent with the original study however, the 7B MTP model underperforms in these tasks. While the MTP paper suggests that it scales effectively on coding tasks, our findings indicate that this scalability does not extend to non-coding tasks. In contrast, our TOP model improves in performance as it scales to 7B and surpasses the 7B NTP and MTP baseline. This suggests that in more general tasks, TOP performs and scales better than MTP. We also report the training loss recorded only on the NTP head of each model. Interestingly, TOP exhibits higher training loss than NTP, yet achieves lower Lambada perplexity and better benchmark scores. We argue that TOP may act as regularizer, mitigating overfitting on the limited FineWeb-Edu subset we used for training. However, this claim may need further investigation."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we propose Token Order Prediction (TOP) as novel auxiliary training loss for LLM pretraining. Our approach addresses some limitations of Multi-Token Prediction (MTP) by replacing the difficult task of exact future token prediction with the more tractable objective of ranking upcoming tokens by their proximity. TOP requires only single additional unembedding layer compared to MTPs multiple transformer layers, making it more parameter-efficient and scalable. Based on the results of our general language modeling experiments across three model sizes (340M, 1.8B, and 7B parameters), TOP overall improves performance over both NTP and MTP on standard NLP benchmarks. The method shows positive gains as parameter count grows, suggesting its potential value for larger-scale language models. These preliminary results indicate that TOP offers promising direction for improving language model training through effective auxiliary objectives."
        },
        {
            "title": "7 FUTURE ADDITIONS",
            "content": "This copy of our paper is an early preprint. Several more experiments and analysis will be added in the next versions of this paper: 1. Comparisons with the DeepSeek V3 (DeepSeek-AI et al., 2024) version of MTP. 2. Fine-tuning and evaluating on generative tasks such as summarization and coding. 6 Early preprint 3. Testing TOP on the star graph problem, synthetic task proposed by Bachmann & Nagarajan (2024). 4. Evaluating the self-speculative decoding potential of TOP. ACKNOWLEDGMENTS We would like to thank Manifold Labs (www.manifold.inc) for providing us with the compute needed to train the models in this paper."
        },
        {
            "title": "REFERENCES",
            "content": "Kwangjun Ahn, Alex Lamb, and John Langford. Efficient joint prediction of multiple future tokens, 2025. Gregor Bachmann and Vaishnavh Nagarajan. The pitfalls of next-token prediction, 2024. Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020. Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. Learning to rank: from pairIn Proceedings of the 24th International Conference on wise approach to listwise approach. Machine Learning, ICML 07, pp. 129136, New York, NY, USA, 2007. Association for ComISBN 9781595937933. doi: 10.1145/1273496.1273513. URL https: puting Machinery. //doi.org/10.1145/1273496.1273513. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457, 2018. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. Deepseek-v3 technical report, 2024. Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozi`ere, David Lopez-Paz, and Gabriel Synnaeve. Better & faster large language models via multi-token prediction, 2024. 7 Early preprint Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Vancouver, Canada, July 2017. Association for Computational Linguistics. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: benchmark for question answering research. Transactions of the Association of Computational Linguistics, 2019. Yann LeCun. Do large language models need sensory grounding for meaning and understanding? University Lecture, 2024. Denis Paperno, German Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The lambada dataset, Aug 2016. Przemyslaw Pobrotyn, Tomasz Bartczak, Mikolaj Synowiec, Radoslaw Bialobrzeski, and Jaroslaw Bojar. Context-aware learning to rank with self-attention. ArXiv, abs/2005.10084, 2020. Weizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang, and Ming Zhou. Prophetnet: Predicting future n-gram for sequence-to-sequence pre-training, 2020. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social iqa: Commonsense reasoning about social interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 44634473, 2019. Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. In NUT@EMNLP, 2017. Songlin Yang and Yu Zhang. Fla: triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/fla-org/ flash-linear-attention. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019."
        }
    ],
    "affiliations": [
        "MBZUAI"
    ]
}