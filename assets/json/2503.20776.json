{
    "paper_title": "Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile Gaussian Feature Fields",
    "authors": [
        "Shijie Zhou",
        "Hui Ren",
        "Yijia Weng",
        "Shuwang Zhang",
        "Zhen Wang",
        "Dejia Xu",
        "Zhiwen Fan",
        "Suya You",
        "Zhangyang Wang",
        "Leonidas Guibas",
        "Achuta Kadambi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in 2D and multimodal models have achieved remarkable success by leveraging large-scale training on extensive datasets. However, extending these achievements to enable free-form interactions and high-level semantic operations with complex 3D/4D scenes remains challenging. This difficulty stems from the limited availability of large-scale, annotated 3D/4D or multi-view datasets, which are crucial for generalizable vision and language tasks such as open-vocabulary and prompt-based segmentation, language-guided editing, and visual question answering (VQA). In this paper, we introduce Feature4X, a universal framework designed to extend any functionality from 2D vision foundation model into the 4D realm, using only monocular video input, which is widely available from user-generated content. The \"X\" in Feature4X represents its versatility, enabling any task through adaptable, model-conditioned 4D feature field distillation. At the core of our framework is a dynamic optimization strategy that unifies multiple model capabilities into a single representation. Additionally, to the best of our knowledge, Feature4X is the first method to distill and lift the features of video foundation models (e.g. SAM2, InternVideo2) into an explicit 4D feature field using Gaussian Splatting. Our experiments showcase novel view segment anything, geometric and appearance scene editing, and free-form VQA across all time steps, empowered by LLMs in feedback loops. These advancements broaden the scope of agentic AI applications by providing a foundation for scalable, contextually and spatiotemporally aware systems capable of immersive dynamic 4D scene interaction."
        },
        {
            "title": "Start",
            "content": "Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile Gaussian Feature Fields Shijie Zhou1* Hui Ren2* Yijia Weng3 Zhiwen Fan4 Zhangyang Wang4 Shuwang Zhang1 Zhen Wang1 Dejia Xu4 Leonidas Guibas3 Achuta Kadambi Suya You5 1UCLA 2MIT 3Stanford 4UT Austin 5DEVCOM ARL 5 2 0 2 M 6 2 ] . [ 1 6 7 7 0 2 . 3 0 5 2 : r https://feature4x.github.io/ Figure 1. Feature4X: Building 4D Interactive Scenes with Agentic AI from Monocular Videos. By dynamically distilling modelconditioned features and integrating 2D foundation models with LLMs in feedback loops, Feature4X enables multimodal tasks across 2D, 3D, and 4D with high-level language inputs or direct user interactions, including (but not limited to) segmentation, scene editing, and VQA across novel views and all time steps, unlocking new possibilities for 4D agentic AI."
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in 2D and multimodal models have achieved remarkable success by leveraging large-scale training on extensive datasets. However, extending these achievements to enable free-form interactions and highlevel semantic operations with complex 3D/4D scenes remains challenging. This difficulty stems from the limited availability of large-scale, annotated 3D/4D or multi-view datasets, which are crucial for generalizable vision and language tasks such as open-vocabulary and prompt-based segmentation, language-guided editing, and visual question answering (VQA). In this paper, we introduce Feature4X, universal framework designed to extend any functionality from 2D vision foundation model into the 4D realm, using only monocular video input, which is widely available from *Equal contribution. user-generated content. The in Feature4X represents its versatility, enabling any task through adaptable, modelconditioned 4D feature field distillation. At the core of our framework is dynamic optimization strategy that unifies multiple model capabilities into single representation. Additionally, to the best of our knowledge, Feature4X is the first method to distill and lift the features of video foundation models (e.g. SAM2, InternVideo2) into an explicit 4D feature field using Gaussian Splatting. Our experiments showcase novel view segment anything, geometric and appearance scene editing, and free-form VQA across all time steps, empowered by LLMs in feedback loops. These advancements broaden the scope of agentic AI applications by providing foundation for scalable, contextually and spatiotemporally aware systems capable of immersive dynamic 4D scene interaction. 1 1. Introduction The rapid evolution of 2D vision and multimodal models has been fueled by access to massive curated datasets with rich annotations, alongside breakthroughs spanning multiple domains. These developments have led to remarkable progress in various tasks, including image segmentation [66], image editing [6], promptable semantic segmentation [30, 68], and Visual Question Answering (VQA) [38, 84]. Despite these advances, the processing and interpretation of dynamic 3D datacrucial for applications such as autonomous driving, robotics, and 3D asset creationstill lag significantly behind the development of versatile and robust 2D foundation models. fundamental challenge in 3D vision lies in processing multi-view images and widely available monocular videos, which often suffer from limited camera information and scarcity of well-annotated, fine-grained per-frame datasets. To bridge this gap, we propose to leverage the latent features of well-trained representations from 2D and multimodal domains [30, 38, 66, 68, 84] and adaptively lift them into higher-dimensional feature fields (such as 3D and 4D) in scalable and efficient manner, enabling the direct transfer of 2D functionalities into 4D with minimal data annotation and training overhead. Constructing 4D feature fields from casually captured inthe-wild videos is significantly challenging. Existing approaches for static 3D feature fields [14, 29, 31, 34, 46, 65, 73, 81, 97, 102, 104] are not directly applicable to 4D scenarios for three primary reasons. First, these methods typically rely on well-calibrated, multi-view input images with precise camera poses, which are difficult to obtain from casual videos. Second, extending static feature fields to include an additional temporal dimension results in substantial memory demands, leading to unstable and costly optimization. Third, previous approaches are limited to task-specific feature fields, necessitating full-parameter retraining for adaptation to new tasks. In this paper, we set an ambitious goal: to parse in-thewild monocular videos into unified dynamic 4D representation that not only reconstructs accurate appearance and geometry but also seamlessly integrates advanced functionalities from range of 2D foundation models. To tackle the aforementioned challenges, we propose enhancing the dynamic 3D Gaussian Splatting-based 4D scene representation [28, 49] with unified latent feature capable of distilling diverse 2D foundation features, achieving both flexibility and efficiency. However, directly lifting 2D feature maps to dense, dynamic 3D Gaussians incurs significant computational costs and fails to scale with the spatial and temporal complexity of the scene. Inspired by recent advancements in Gaussian-based 4D reconstruction [35, 82], we propose to leverage the smooth, compact nature of underlying scene semantics and represent the dense 4D feature field using sparse set of base or Scaffold features, enabling efficient representation and scalable adaptation to various downstream tasks. Our pipeline is fully end-to-end differentiable, supervised by ground truth color and feature maps exported from 2D vision foundation models. Its general and modelagnostic design supports wide range of vision tasks, spanning 2D (semantic and promptable segmentation), 3D (scene editing), and 4D (spatial and temporal VQA). Furthermore, our compact and versatile 4D feature field serves as bridge, seamlessly integrating these tasks with LLMs in continuous feedback loop. This integration enables intuitive and efficient execution through free-form, high-level natural language inputs or direct user interactions. By unifying diverse vision tasks, our frameworkseamlessly integrated with LLMspaves the way for the development of advanced, contextually and spatiotemporally aware 4D agentic AI systems. We summarize our contributions as follows: We propose general 4D feature field distillation technique using Gaussian Splatting to build an interactive 4D scene and lift the functionalities of any 2D vision (image/video) foundation models into the 4D realm, relying solely on monocular video input. We introduce the first compact and versatile 4D Gaussian feature field representation, which leverages the smooth, low-rank nature of scene semantics by modeling the dense 4D feature field as interpolations of sparse set of base features. We develop an LLM-powered agentic AI capable of interpreting natural language prompts, dynamically adjusting configuration parameters, and iteratively refining results through trial and feedback, enabling intelligent 4D scene interaction and manipulation. 2. Related Works 2.1. 4D Representations for Reconstruction and"
        },
        {
            "title": "Generation",
            "content": "Various 4D representations are designed to support wide variety of 4D tasks. Among them, 4D reconstruction is the most widely studied topic that drives the field. Early methods [7, 11, 15, 17, 18, 33, 40, 55, 57, 59, 60, 64, 76, 85] implement variants of Neural Radiance Fields [52] to represent dynamic 4D scene through implicit or hybrid representations. More recently, 3D Gaussian [28] has prevailed as popular explicit 3D scene representation [9, 96, 103] and various variants [12, 23, 24, 35, 43, 49, 50, 75, 82, 86, 92, 93] are similarly proposed to represent dynamic 4D scenes. Additionally, the field of 4D Generation [45, 69, 70, 89, 94] has witnessed great advancements in representations that are more suitable for generative settings. In summary, most 4D representations can be categorized into deformation2 Figure 2. Method overview. Given an input monocular video, we infer 2D priors to segment static background (represented by static 3D Gaussians augmented with latent features) and dynamic foreground (represented by dynamic 3D Gaussians guided by Motion Scaffolds [35], set of nodes {vi} encoding 3D motion trajectories and latent features hi). Dynamic Gaussian features and motions are computed via interpolation from their K-nearest scaffold nodes. At each timestep, dynamic Gaussians are warped and fused with static Gaussians. parallel rasterization [102] generates RGB images and unified latent feature map, decoded into task-specific featuresillustrated here by SAM2 [68], CLIP-LSeg [36], and InternVideo2 [84] for representative 2D (novel view segmentation), 3D (scene editing), and 4D (spatiotemporal VQA) tasks. Our framework generalizes to any 2D vision foundation model and is trained end-to-end using input RGB frames and customized features from pretrained 2D models. At inference, rendered feature maps from arbitrary views and timesteps are directly fed into task-specific decoders, seamlessly supporting user prompts and LLM interactions to form unified 4D agentic AI system. based approaches [42, 49, 56, 57, 63, 77, 80, 87], temporal extended approaches [7, 12, 16, 92, 99], or an ensemble of multiple time-varying representations [19, 39, 41, 70, 88]. In our work, we extend the direction of dynamic 3D Gaussian Splatting to versatile Gaussian feature field that simultaneously embraces multiple features for various vision tasks beyond 4D reconstruction and generation. To the best of our knowledge, this direction has not been widely explored. 2.2. 4D Reconstruction from Monocular Video In the context of 4D reconstruction tasks, most methods [44, 54] focus on reconstructing dynamic 4D scenes from multiple well-calibrated videos. In contrast, another popular direction is to synthesize generic 4D scenes from monocular videos [19, 41, 56, 57, 63, 80, 87, 88]. For unposed casual videos, non-rigid structure-from-motion workflows [35, 10, 20, 25, 26, 32, 37, 53, 67, 79, 90, 91, 100] are adopted for estimating camera poses for each frame for further processing. Shape-of-motion [82] and MoSca [35] develop end-to-end workflows that reconstruct 4D scenes from casual captured videos with the help of 3D Gaussians [28, 48, 49]. Another series of work [39, 69, 70, 94] assumes the camera is fixed for the video and reconstructs 3D in the camera coordinate space. More recently, the success of DUSt3R [83], transformer-based novel paradigm for 3D reconstruction of arbitrary image collections, has In been adapted for dynamic scenes in MonST3R [98]. comparison, we aim to perform monocular 4D reconstruction and feature lifting simultaneously, which cannot be achieved in previous approaches. 2.3. Feature Field Distillation Research in novel view synthesis and feature field representation has been extensively developed within the NeRF framework [51] and 3D Gaussians [28]. Seminal works such as Semantic NeRF [101] and Panoptic Lifting [74] have integrated semantic information from segmentation networks into 3D spaces, revealing that combining noisy or inconsistent 2D labels within 3D context can produce clear and accurate 3D segmentations. Building on this concept, methods such as those in [71] have shown that minimal input, like basic foreground-background masks, can be effective for object segmentation in 3D. Moving beyond label estimation to optimize NeRF, approaches like Distilled Feature Fields [31], NeRF-SOS [13], LERF [29], and Neural Feature Fusion Fields [81] have embedded pixel-aligned feature vectors from tools such as LSeg or DINO [8] into NeRF structures. More recently, numerous works [34, 46, 65, 73, 97, 102, 104] adopt similar strategies to distill information from well-trained 2D models to 3D Gaussians. However, feature lifting into 4D fields is not properly solved yet. In this work, we make the first attempt to deliver versatile 4D Gaussian feature field framework that embraces multiple task features simultaneously. 3 and the node control radius. In other words, the dense, perGaussian motion trajectories are interpolations of much smaller (100) set of trajectory nodes. This design leverages the low-rank nature of the underlying scene motion and effectively regularizes Gaussian motions under limited supervision. We follow [35]s protocols to initialize and train 4D Motion Scaffolds and dynamic Gaussians that represent the dynamic foreground elements, as well as another set of static Gaussians that model the static background. Please refer to our supplementary material for full details. 3.2. Unified Latent Feature Distillation We aim to go beyond appearance reconstruction by building unified, versatile, and dynamic 4D feature field representation, capable of supporting diverse downstream visual tasks = {T 1, . . . , S} spanning 2D, 3D, and 4D, such as novel-view segmentation (2D), scene editing (3D), and scene-level spatiotemporal VQA (4D). straightforward way to construct this 4D feature field is to extend existing 3D feature field frameworks [102] by replacing their 3D reconstruction modules with dynamic 4D reconstruction. However, this approach addresses each downstream task independently: input frames = {I1, . . . , It} are encoded into task-specific 2D feature maps = {F } using dedicated vision foundation model encoders Es, leading to separate feature fields per task. Consequently, performing three tasks would require three separate reconstructions, making this strategy inefficient and redundant. 1 , . . . , To overcome this limitation, we propose distilling unified 4D feature field to coherently fuse diverse 2D vision foundation model features across views and timesteps, providing consistent feature access in 4D (3D space + time) for various downstream tasks. Specifically, as shown in Fig. 2, we utilize the parallel N-dimensional Gaussian rasterizer [102] to simultaneously render RGB images along with unified latent feature F, which is jointly learned with set of lightweight decoders {D1, . . . , DS}. Each decoder maps the shared latent feature RD into task-specific target features RDs, where and Ds denote the dimensions of the unified latent and task-specific features, respectively, with Ds. Compared to separately optimizing high-dimensional features for each task, our unified latent representation significantly reduces computational overhead during rasterization. During the optimization process, we attach the feature vector fj RD to each 3D Gaussian Gj G, warp Gj to the target timestep τ following the process introduced in [35], and rasterize fj the same way we rasterize Gaussian color cj as [102]. Conceptually, the RGB and feature reconstruction from viewpoint at timestep τ are computed as: ˆI τ = Rasterize(v, {warp(Gj, τ ), cj)}Gj G) ˆF τ = Rasterize(v, {warp(Gj, τ ), fj)}Gj G) (1) Figure 3. Segment Anything in Dynamic 4D Scenes with SAM2 Feature Field. For any rendered novel view video, we support: (a) Promptless segmentation (segment everything): when no user prompt is provided, segmentation masks are automatically assigned at the first frame (t = 0) and then propagated across all frames. (b) Promptable segmentation (segment anything): the user can segment any objectstatic or dynamicat any timestep using point or box prompt, and the corresponding mask is robustly tracked and propagated through subsequent frames. 3. Method Given monocular RGB video = I1, . . . , It, we reconstruct the underlying 4D scene as set of dynamic 3D Gaussians (Sec. 3.1), each augmented with unified latent feature embedding that distills informative features from various 2D foundation models for downstream tasks (Sec. 3.2). To address the challenge of handling high-dimensional features across large number of Gaussians and timesteps, we adopt compact feature representation (Sec. 3.3). 3.1. Preliminaries: Dynamic 3D Gaussian Splatting Following state-of-the-art monocular video-based dynamic 4D scene reconstruction approaches [35, 75, 82], we represent the scene with dynamic 3D Gaussians [49], namely set of persistent 3D Gaussians [28] that deform over time t. Specifically, we base our approach on MoSca [35], which can reconstruct the dynamic 4D scene from monocular casual video. Like other methods, MoSca addresses the challenge from single-view partial observation by leveraging priors from 2D foundation models [2, 21, 27, 62, 78] and by regularizing Gaussian motion trajectories. At the core of MoScas method is structured graph named 4D Motion Scaffold (V, E) that drives the deformation of individual 3D j=1 (see Fig. 2). Each node v(i) Gaussians = {Gj}n describes 3D motion trajectory [Q(i) ], = [R, t] SE(3) with control radius r(i). Edges describe k-nearest neighbor graph over the motion trajectories. Given any 3D Gaussian located at at time τ , to compute its deformation to time τ , we first find the trajectory node v(i) with the closest position p(i) τ at time τ , i.e., = arg mini p(i) τ x, then interpolate the deformation from nearest trajectory nodes {v(i)}iE(i), with weights {wi} computed from the Gaussian-to-trajectory distance 1 , . . . , Q(i) provides structural regularization, encouraging smoother learned features. 3.4. Interaction with AI Agent via Feature Fields Agentic AI typically requires cross-modal interactions, notably between language and vision. In this work, we aim to build an AI agent that supports direct language interactions with our dynamic 4D scene representation. Such interaction requires shared feature space between text and Gaussian features. However, text features are usually highdimensional. For instance, language-guided scene editing requires CLIP features of 512 dimensions. Directly assigning these high-dimensional features to each Gaussian is computationally expensive. Feature 3DGS [102] addresses this by assigning each Gaussian full 512-dimensional CLIP feature via parallel N-dimensional Gaussian rasterizer, enabling direct language interaction but resulting in slow rendering speeds and excessive memory usage. Although Feature 3DGS proposes CNN-based acceleration, it operates only on 2D rendered maps, failing to directly resolve the feature mismatch in 3D. In contrast, we optimize compact, lower-dimensional Gaussian feature (D = 32), train an MLP-based decoder on rendered 2D features, and apply it directly to 3D Gaussian features during inference. Given the intrinsic flexibility of MLPs, our approach efficiently bridges language features and our compact feature fields, enabling direct interaction between LLMs and our 4D scene through language. Furthermore, by leveraging InternVideo2 features, which can be rendered and decoded from any viewpoint in 3D space over time, we naturally lift the video chatbot LLM from 2D to 4D, enabling free-form language interaction with the AI agent within 4D space. Beyond direct language interaction, LLM agents can operate in the loop for manipulation tasks by interpreting user prompts, automatically optimizing hyperparameters, and iteratively refining results for downstream applications. For example, given scene-editing prompt like Delete the dog, the agent parses the operation command (delete) and the target object (dog), then generates an editing configuration with relevant hyperparameters, such as the softmax threshold for matching Gaussians to the target object. It tests various thresholds, evaluates the quality of rendered image samples, and selects the optimal configuration. This is then consistently applied across the entire 4D scene, enabling efficient, intelligent editing with minimal user input. This perception-reasoning-action loop empowers the LLM-driven 4D agent to interpret, execute, and refine complex scene manipulations, making it powerful tool for adaptive 4D scene editing and interaction. The system is particularly valuable for applications like interactive VR content creation and editing, where dynamic scene understanding and precise, context-aware modifications are essential. Figure 4. Baseline Comparison on SAM2 Inference. We compare segmentation quality and inference speed between (a) the naive RGB-based approach and (b) our feature-based method. Ours achieves comparable segmentation, accurately tracking the object over time, and avoids RGB artifacts (red box region at = 70), while reducing inference time to about 4 speed-up. The resulting feature map ˆFτ (with viewpoint omitted for simplicity) is decoded by Ds into task-specific feature map ˆF τ , supervised by the ground-truth feature map obtained from customized vision model encoder Es (e.g., SAM2). Concretely, we optimize the following feature loss Lfeat to jointly learn the feature field, alongside the photometric loss from [35] used for learning the radiance field. Lfeat = (cid:88) s=1 MSE( ˆF τ , τ ), τ = Ds( ˆFτ ), ˆF τ = Es(Iτ ). (2) (3) Note that the feature loss and photometric loss are independent, and introducing the feature field does not degrade the quality of the radiance field reconstruction (see Tab. 1). 3.3. Scaffold-Based Compact Feature While our unified feature field effectively reduces feature dimensionality, optimizing feature vector for every Gaussian remains computationally expensive. However, semantic features tend to be smooth in 3D and exhibit strong local correlations, similar to Gaussian motion trajectories. To leverage this correlation, we propose representing perGaussian features {fj} as linear combinations of smaller (100 fewer) and more compact set of base features {hi}, conveniently attached to nodes {v(i)} of the 4D Motion Scaffold (see Fig. 2). Recall that we compute per-Gaussian deformations by interpolating from their K-nearest trajectory nodes {v(i)}iE(i) with interpolation weights {wi}. We reuse these same weights to obtain the per-Gaussian unified features as: fj = (cid:88) wihi. iE(i) (4)"
        },
        {
            "title": "This compact feature representation significantly reduces\nthe number of parameters required for optimization and",
            "content": "5 ambiguous and inaccurate segmentation despite smoother masks. In contrast, our method achieves faster and more robust segmentation in 4D scenes by operating directly in feature space. 4.2. Scene Understanding with Semantics To achieve pixel-level semantic segmentation across any novel view and timestep, we extend the capabilities of CLIP-LSeg [36] by lifting its features into 4D CLIP feature field. CLIP-LSeg is language-driven semantic segmentation model that leverages the Contrastive LanguageImage Pre-training (CLIP) framework to align textual descriptions with visual content, enabling zero-shot segmentation by interpreting descriptive input labels. By lifting these 2D features into 4D representation, our approach captures both spatial and temporal information, facilitating consistent semantic understanding in dynamic scenes, even when objects undergo significant appearance changes over time. As depicted in Fig. 5, our model accurately identifies flower throughout its blooming processfrom bud to full bloomacross various viewpoints and timesteps. This demonstrates the models proficiency in maintaining consistent semantic understanding despite substantial visual transformations in dynamic scenes. Tab. 1 presents quantitative comparison between our method and baselines on the Nvidia dataset [47]. MoSca [35] is the baseline 4D reconstruction method, while MoSca + Feature 3DGS [102] is naive, noncompact baseline using RGB + feature rasterization. Our modelsusing 32-dimension latent feature and 512dimension CLIP-LSeg [36] as the targetmaintain comparable PSNR to MoSca, showing no loss in reconstruction quality. The single CLIP head variant achieves the highest mIoU, demonstrating effective feature distillation. Although the naive baseline yields slightly higher accuracy, our models are about 6.2 more space-efficient, with the full model delivering competitive performance while remaining compact and general-purpose. 4.3. Scene Editing with Language and AI Agent We employ an LLM agent (GPT-4o [1]) to interpret natural language prompts, optimize editing parameters, execute precise queries, and iteratively refine results for intelligent 3D scene editing. The agent begins by parsing the users high-level prompt to identify the editing actionsuch as extract, delete, or change colorand the target object (e.g., cow), as shown in Fig. 6. Based on the task, it generates candidate configurations with varying parameters relevant to the prompt. Specifically, we first compute the probability of each 3D Gaussian corresponding to language description of the target object as: p(l j) = es siL esi (cid:80) , = q(l) fj q(l)fj , (5) Figure 5. Semantic 4D Scene Understanding with CLIP Feature Field. By lifting CLIP-LSeg [36] features into 4D feature field, we enable pixel-level semantic segmentation from any view at any timestep. This allows robust 4D scene understanding, even as object appearances change over timefor example, accurately identifying blooming flower from bud to full bloom across views."
        },
        {
            "title": "Method",
            "content": "PSNR mIoU accuracy Size (MB) MoSca MoSca + Feature 3DGS Ours (single CLIP head) Ours (full model) 25.166 25.191 25.186 25.197 - 0.506 0.510 0. - 0.881 0.880 0.876 67.726 593.907 95.294 95.457 Table 1. Semantic segmentation on the Nvidia dataset [47]. Our method achieves comparable radiance reconstruction (PSNR) and segmentation performance, while significantly reducing memory usage compared to the baselines. 4. Experiments In this section, we present experimental results and analyses of the capabilities of our proposed Feature4X framework. Based on the type of desired output, we categorize tasks into 2D (segmentation; see Secs. 4.1 and 4.2), 3D (scene editing; see Sec. 4.3), and 4D (spatiotemporal VQA; see Sec. 4.4). 4.1. Scene Interaction with Segment Anything Segment Anything Model 2 (SAM2) [68] is an advanced segmentation model for promptable visual segmentation across images and videos, supporting various input types such as points and bounding boxes for interactive and precise results. In our approach, we extract per-frame features using SAM2s image encoder and lift them into 4D feature field, enabling direct decoding from novel-view rendered feature maps. This allows segmentation that is inherently aware of both viewpoint changes and temporal dynamics, as shown in Fig. 3. Masks are seamlessly propagated through space and time, regardless of object motion or camera trajectory, supporting both automatic and user-guided segmentation with temporal coherence. Notably, our approach bypasses the need to first render RGB videos and then apply the full SAM2 pipeline (see (a) in Fig. 4). We observe that artifacts in novel view RGB rendering can mislead SAM2 during encoding, resulting in 6 Figure 6. Scene Editing with AI Agent. Given user prompts, our GPT-powered agent interprets editing intent and autonomously performs scene edits via our 4D CLIP feature field. Examples include both geometric (e.g., extract and delete) and appearance (e.g., change color) editing in 3D space. While results may not be perfect due to imperfect fine-grained feature alignment and non-optimal editing parameter tuning, the agent adaptively refines parameters and applies edits consistently across views and timegreatly reducing the need for manual tuningand demonstrates robust, interactive 4D scene manipulation. where q(l) is the text feature, fj is the CLIP feature of Gaussian j, and is the label set of all possible scene objects. The agent filters Gaussians using various thresholds based on the softmax probability p(l j), applies the edit configuration, and evaluates the results via rendered image samples. It selects the parameter setting that best aligns with the intended edit and applies it consistently across all video frames, ensuring coherent editing throughout the 4D scene. This GPT-powered 4D Agentic AI system interprets, executes, and refines complex scene manipulations, serving as an intelligent assistant for adaptive editing and interaction. By autonomously exploring, selecting, and tuning parameterstasks typically handled by human editorsthe agent enhances responsiveness and consistency. GPT acts as self-refining decision-maker, illustrating the potential of agentic AI in creative and technical workflows where iterative optimization and goal-directed behavior significantly streamline and elevate editing tasks. 4.4. Scene Reasoning with 4D Chatbot Agent While large vision-language models (VLMs) have demonstrated impressive performance in visual question answering (VQA) tasks, their application has predominantly been confined to 2D visual modalities, such as images and videos. To our knowledge, we are the first to extend the capabilities of VLM (Video-LLM) into 4Dencompassing three spatial dimensions plus timeby integrating VLM knowledge into our 4D feature field. This advancement enables more comprehensive understanding and interaction with dynamic 4D scenes, capturing both spatial and temporal nuances beyond the static 2D perspective. We utilize InternVideo2 [38, 84]a state-of-the-art video foundation modelto extract comprehensive video features. Specifically, we sample the input video into several clips, each comprising 8 frames sampled at equal intervals to ensure enough GPU memory available during the whole process. These clips are processed through InternVideo2s vision transformer encoder to obtain segment-level features and then aggregated together. Additionally, for each segment, InternVideo2 generates class token representing high-level semantic information. We compute the average of these class tokens to form global video representation. This class feature is then concatenated with the aggregated segment features, effectively integrating temporal information which is necessary for dynamic scene reasoning. In our system, free-form visual question answering (VQA) is supported through language model [38], enabling users to interact with chatbot agent using natural language while exploring 4D dynamic scenes, as illustrated in Fig. 7. Unlike prior approaches that rely solely on 2D monocular videolimited to single view at single time stepour method leverages the reconstructed 4D radiance and feature fields to support richer inference sources, including both local novel views (via free camera movement) and global novel views (via zoom-out for broader context). In the example shown in Fig. 7, we observe that InternVideo2, when operating on the input monocular video, fails to correctly answer spatial and temporal questionseven when the answers are obvious to humanswhile the same model, using our global novel view feature, succeeds (see Tab. 2). This is because monocular video lacks the spatial coverage and contextual cues necessary for spatiotemporal reasoning. In contrast, our framework allows rendering of novel views and their corFigure 7. VQA with Chatbot Agent. (Left) Our model supports free-form VQA across diverse question typesgeneral, spatial, and temporalby distilling InternVideo2 [84] features. (Right) At each timestep, we reconstruct both 4D radiance field and 4D feature field, providing more inference sources beyond the input video frameincluding local (moving camera) and global (zoomed-out) novel views and their corresponding feature mapsthereby supporting VQA in 4D and enhancing the models spatiotemporal reasoning capabilities."
        },
        {
            "title": "Inference Source\nInput Video View\nLocal Novel View\nLocal Novel Feature\nGlobal Novel View\nGlobal Novel Feature",
            "content": "General Spatial Temporal Time (s) 33.65 33.86 11.99 33.37 12.24 Table 2. VQA performance across different inference sources with the scene shown in Fig. 7. Feature-based inference supports all question types with lower latency, while view-based methods are limited in spatial and temporal reasoning."
        },
        {
            "title": "Inference Source",
            "content": "Spatial Acc. Temporal Acc. Overall Acc. Time (s)"
        },
        {
            "title": "Input Video View\nLocal Novel View\nLocal Novel Feature\nGlobal Novel View\nGlobal Novel Feature",
            "content": "48.50 49.75 54.50 47.00 58.75 49.84 51.13 54.37 49.19 58.25 49.06 50.31 56.29 47.48 61.32 10.02 9.78 2.81 10.06 3.42 Table 3. Spatiotemporal VQA on DAVIS dataset [61]. Compared to 2D video inference, our 4D feature space inference (Global Novel Feature) enhances the Video-LLMs spatiotemporal reasoning while achieving approximately 3 faster inference. responding feature maps at arbitrary time steps, injecting explicit signals that enhance reasoning. To evaluate this more systematically, we construct 400 objective questions across 50 scenes in the DAVIS dataset [61], covering spatial only (e.g., Is the person facing left?), temporal only (e.g., How many times does the dancer spin?), and spatiotemporal (e.g., Which direction is the duck moving?) categories. As shown in Tab. 3, our method improves VQA accuracy by enriching scene understanding through 4D feature field reconstruction, while also accelerating inference by eliminating the need for video encoding. 8 5. Conclusion We have introduced Feature4X, general and scalable framework that bridges casually captured monocular videos to interactive 4D agentic AI systems. By distilling diverse 2D vision foundation model features into unified, dynamic 4D Gaussian feature field, our method supports wide range of tasks across 2D (segmentation), 3D (scene editing), and 4D (spatiotemporal VQA). Through compact and versatile representation, Feature4X achieves efficient training and inference while maintaining high-quality appearance reconstruction and robust semantic understanding. Furthermore, by integrating LLMs for language-based interaction and autonomous task refinement, our system elevates traditional perception tasks into perception-reasoning-action loop, enabling intelligent and adaptive manipulation of dynamic 4D scenes. We hope Feature4X opens new avenues in 4D vision and agentic AI research by facilitating immersive, multimodal interaction with dynamic visual content. Acknowledgement We especially thank Jiahui Lei for valuable guidance on MoSca implementation and insightful suggestions on experiments. This project was supported by LUCI program under the Basic Research Office and partially supported by ARL grants W911NF20-2-0158 and W911NF-21-2-0104 under the cooperative A2I2 program. Z.W. is supported by ARL and LUCI program, and an Army Young Investigator Award. L.G. is supported by Vannevar Bush Faculty Fellowship. A.K. is supported by DARPA Young Faculty Award, NSF CAREER Award IIS-2046737, and Army Young Investigator Award."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 6 [2] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Muller. Zoedepth: Zero-shot transfer by combining relative and metric depth. arXiv preprint arXiv:2302.12288, 2023. 4 [3] Volker Blanz and Thomas Vetter. morphable model for the synthesis of 3d faces. In CVPR, 1999. 3 [4] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael Black. Keep it smpl: Automatic estimation of 3d human pose and shape from single image. In ECCV, 2016. [5] Aljaz Bozic, Pablo Palafox, Michael Zollofer, Angela Dai, Justus Thies, and Matthias Nießner. Neural non-rigid tracking. In NeurIPS, 2020. [6] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. arXiv preprint arXiv:2211.09800, 2022. 2 [7] Ang Cao and Justin Johnson. Hexplane: fast representation for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 130141, 2023. 2, 3 [8] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 3 [9] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Luciddreamer: Domain-free generation of 3d gaussian splatting scenes. arXiv preprint arXiv:2311.13384, 2023. 2 [10] Brian Curless and Marc Levoy. volumetric method for building complex models from range images. In Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, pages 303312, 1996. 3 [11] Yilun Du, Yinan Zhang, Hong-Xing Yu, Joshua Tenenbaum, and Jiajun Wu. Neural radiance flow for 4d view synthesis and video processing. In ICCV, 2021. 2 [12] Yuanxing Duan, Fangyin Wei, Qiyu Dai, Yuhang He, Wenzheng Chen, and Baoquan Chen. 4d gaussian splatting: Towards efficient novel view synthesis for dynamic scenes. arXiv preprint arXiv:2402.03307, 2024. 2, [13] Zhiwen Fan, Peihao Wang, Yifan Jiang, Xinyu Gong, Dejia Xu, and Zhangyang Wang. Nerf-sos: Any-view selfsupervised object segmentation on complex scenes. arXiv preprint arXiv:2209.08776, 2022. 3 [14] Zhiwen Fan, Jian Zhang, Wenyan Cong, Peihao Wang, Renjie Li, Kairun Wen, Shijie Zhou, Achuta Kadambi, Zhangyang Wang, Danfei Xu, et al. Large spatial model: End-to-end unposed images to semantic 3d. Advances in Neural Information Processing Systems, 37:4021240229, 2024. 2 9 [15] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, and Angjoo Kanazawa. Kplanes: Explicit radiance fields in space, time, and appearance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1247912488, 2023. 2 [16] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, and Angjoo Kanazawa. Kplanes: Explicit radiance fields in space, time, and appearance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1247912488, 2023. 3 [17] Guy Gafni, Justus Thies, Michael Zollhofer, and Matthias Nießner. Dynamic neural radiance fields for monocuIn Proceedings of the lar 4d facial avatar reconstruction. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 86498658, 2021. [18] Guy Gafni, Justus Thies, Michael Zollhofer, and Matthias Nießner. Dynamic neural radiance fields for monocular 4d facial avatar reconstruction. In CVPR, 2021. 2 [19] Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang. Dynamic view synthesis from dynamic monocular video. In ICCV, 2021. 3 [20] Wei Gao and Russ Tedrake. Surfelwarp: Efficient nonarXiv volumetric single view dynamic reconstruction. preprint arXiv:1904.13073, 2019. 3 [21] Adam Harley, Zhaoyuan Fang, and Katerina Fragkiadaki. Particle video revisited: Tracking through occlusions using point trajectories. In European Conference on Computer Vision, pages 5975. Springer, 2022. 4 [22] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. arXiv preprint arXiv:2111.06377, 2022. https://arxiv.org/abs/2111.06377. 15 [23] Yi-Hua Huang, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, and Xiaojuan Qi. Sc-gs: Sparse-controlled gaussian splatting for editable dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 42204230, 2024. [24] Shengxiang Ji, Guanjun Wu, Jiemin Fang, Jiazhong Cen, Taoran Yi, Wenyu Liu, Qi Tian, and Xinggang Wang. Segment any 4d gaussians. arXiv preprint arXiv:2407.04504, 2024. 2 [25] Angjoo Kanazawa, Shubham Tulsiani, Alexei A. Efros, and Jitendra Malik. Learning category-specific mesh reconstruction from image collections. In ECCV, 2018. 3 [26] Abhishek Kar, Shubham Tulsiani, Joao Carreira, and Jitendra Malik. Category-specific object reconstruction from single image. In CVPR, 2015. 3 [27] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. CoarXiv preprint tracker: arXiv:2307.07635, 2023. 4 It is better to track together. [28] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics (ToG), 42(4):114, 2023. 2, 3, 4 [29] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik. Lerf: Language embedIn Proceedings of the IEEE/CVF Inded radiance fields. ternational Conference on Computer Vision, pages 19729 19739, 2023. 2, [30] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment In Proceedings of the IEEE/CVF International anything. Conference on Computer Vision, pages 40154026, 2023. 2 [31] Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitzmann. Decomposing nerf for editing via feature field distillation. In NeurIPS, 2022. 2, 3 [32] Nilesh Kulkarni, Abhinav Gupta, David Fouhey, and Shubham Tulsiani. Articulation-aware canonical surface mapping. In CVPR, 2020. 3 [33] Youngjoong Kwon, Dahun Kim, Duygu Ceylan, and Henry Fuchs. Neural human performer: Learning generalizable radiance fields for human performance rendering. NeurIPS, 2021. 2 [34] Hyunjee Lee, Youngsik Yun, Jeongmin Bae, Seoha Kim, and Youngjung Uh. Rethinking open-vocabulary segmentation of radiance fields in 3d space. arXiv preprint arXiv:2408.07416, 2024. 2, 3 [35] Jiahui Lei, Yijia Weng, Adam Harley, Leonidas Guibas, and Kostas Daniilidis. Mosca: Dynamic gaussian fusion from casual videos via 4d motion scaffolds. arXiv preprint arXiv:2405.17421, 2024. 2, 3, 4, 5, 6, 14, 15, 17, 18 [36] Boyi Li, Kilian Weinberger, Serge Belongie, Vladlen Koltun, and Rene Ranftl. Language-driven semantic segmentation. arXiv preprint arXiv:2201.03546, 2022. 3, 6, [37] Hao Li, Robert Sumner, and Mark Pauly. Global correspondence optimization for non-rigid registration of depth scans. In Computer graphics forum, pages 14211430. Wiley Online Library, 2008. 3 [38] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 2, 7, 16 [39] Renjie Li, Panwang Pan, Bangbang Yang, Dejia Xu, Shijie Zhou, Xuanyang Zhang, Zeming Li, Achuta Kadambi, Zhangyang Wang, and Zhiwen Fan. 4k4dgen: Panoramic 4d generation at 4k resolution. arXiv preprint arXiv:2406.13527, 2024. 3 [40] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, and Zhaoyang Lv. Neural 3d video synthesis from multi-view video. In CVPR, 2022. 2 [41] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-time view synthesis of dynamic scenes. In CVPR, 2021. 3 [42] Zhan Li, Zhang Chen, Zhong Li, and Yi Xu. Spacetime gaussian feature splatting for real-time dynamic view synIn Proceedings of the IEEE/CVF Conference on thesis. Computer Vision and Pattern Recognition, pages 8508 8520, 2024. 3 [43] Yiqing Liang, Numair Khan, Zhengqin Li, Thu NguyenPhuoc, Douglas Lanman, James Tompkin, and Lei Xiao. Gaufre: Gaussian deformation fields for real-time dynamic novel view synthesis. arXiv preprint arXiv:2312.11458, 2023. 2 [44] Youtian Lin, Zuozhuo Dai, Siyu Zhu, and Yao Yao. Gaussian-flow: 4d reconstruction with dynamic 3d gaussian particle. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21136 21145, 2024. 3 [45] Huan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fidler, and Karsten Kreis. Align your gaussians: Text-to-4d with dynamic 3d gaussians and composed diffusion models. arXiv preprint arXiv:2312.13763, 2023. 2 [46] Xinyi Liu, Tianyi Zhang, Matthew Johnson-Roberson, Splatraj: Camera trajectory generarXiv preprint and Weiming Zhi. ation with semantic gaussian splatting. arXiv:2410.06014, 2024. 2, [47] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Johannes Kopf, and Jia-Bin Huang. Robust dynamic radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. 6, 17 [48] Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, and Bo Dai. Scaffold-gs: Structured 3d gaussians for view-adaptive rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2065420664, 2024. 3 [49] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Tracking arXiv preprint Deva Ramanan. by persistent dynamic view synthesis. arXiv:2308.09713, 2023. 2, 3, 4 Dynamic 3d gaussians: [50] Marko Mihajlovic, Sergey Prokudin, Siyu Tang, Robert Maier, Federica Bogo, Tony Tung, and Edmond Boyer. Splatfields: Neural gaussian splats for sparse 3d and 4d reconstruction. In European Conference on Computer Vision, pages 313332. Springer, 2025. [51] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 3 [52] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 2 [53] Richard Newcombe, Dieter Fox, and Steve Seitz. Dynamicfusion: Reconstruction and tracking of non-rigid scenes in real-time. In CVPR, 2015. 3 [54] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Occupancy flow: 4d reconstruction by learning particle dynamics. In Proceedings of the IEEE/CVF international conference on computer vision, pages 53795389, 2019. 3 10 [55] Atsuhiro Noguchi, Xiao Sun, Stephen Lin, and Tatsuya Harada. Neural articulated radiance field. In ICCV, 2021. [56] Keunhong Park, Utkarsh Sinha, Jonathan Barron, Sofien Bouaziz, Dan Goldman, Steven Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 58655874, 2021. 3, 17, 18 [57] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan Barron, Sofien Bouaziz, Dan Goldman, Ricardo MartinBrualla, and Steven Seitz. higherdimensional representation for topologically varying neural radiance fields. ACM Transactions on Graphics (TOG), 40 (6):112, 2021. 2, 3 Hypernerf: [58] Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12:28252830, 2011. 16 [59] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic huIn Proceedings of the IEEE/CVF Interman bodies. national Conference on Computer Vision, pages 14314 14323, 2021. 2 [60] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes In CVPR, for novel view synthesis of dynamic humans. 2021. 2 [61] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. benchmark dataset and evaluation methodology for video In Proceedings of the IEEE conferobject segmentation. ence on computer vision and pattern recognition, pages 724732, 2016. 8, 15 [62] Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia Segu, Siyuan Li, Luc Van Gool, and Fisher Yu. Unidepth: Universal monocular metric depth estimation. arXiv preprint arXiv:2403.18913, 2024. [63] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and D-NeRF: Neural Radiance Francesc Moreno-Noguer. Fields for Dynamic Scenes. In CVPR, 2021. 3 [64] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1031810327, 2021. 2 [65] Minghan Qin, Wanhua Li, Jiawei Zhou, Haoqian Wang, and Hanspeter Pfister. Langsplat: 3d language gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2005120060, 2024. 2, 3 [66] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language superIn International conference on machine learning, vision. pages 87488763. PMLR, 2021. 2 [67] Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh. Reconstructing 3d human pose from 2d image landmarks. In ECCV, 2012. 3 [68] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 2, 3, 6, 15 [69] Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, and Ziwei Liu. Dreamgaussian4d: Generative 4d gaussian splatting. arXiv preprint arXiv:2312.17142, 2023. 2, 3 [70] Jiawei Ren, Kevin Xie, Ashkan Mirzaei, Hanxue Liang, Xiaohui Zeng, Karsten Kreis, Ziwei Liu, Antonio Torralba, Sanja Fidler, Seung Wook Kim, et al. L4gm: Large 4d gaussian reconstruction model. arXiv preprint arXiv:2406.10324, 2024. 2, 3 [71] Zhongzheng Ren, Aseem Agarwala, Bryan Russell, Alexander G. Schwing, and Oliver Wang. Neural vol- ( alphabetic umetric object selection. ordering). In CVPR, 2022. [72] Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen Wei, Haoqi Fan, Po-Yao Huang, Vaibhav Aggarwal, Arkabandhu Chowdhury, Omid Poursaeed, Judy Hoffman, Jitendra Malik, Yanghao Li, and Christoph Feichtenhofer. Hiera: hierarchical vision transformer without the bells-andwhistles. arXiv preprint arXiv:2306.00989, 2023. https: //arxiv.org/abs/2306.00989. 15 [73] Jin-Chuan Shi, Miao Wang, Hao-Bin Duan, and ShaoHua Guan. Language embedded 3d gaussians for openIn Proceedings of the vocabulary scene understanding. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 53335343, 2024. 2, 3 [74] Yawar Siddiqui, Lorenzo Porzi, Samuel Rota Bulo, Norman Muller, Matthias Nießner, Angela Dai, and Peter Kontschieder. Panoptic lifting for 3d scene understanding with neural fields. arXiv preprint arXiv:2212.09802, 2022. 3 [75] Colton Stearns, Adam Harley, Mikaela Uy, Florian Dubost, Federico Tombari, Gordon Wetzstein, and Leonidas Dynamic gaussian marbles for novel view Guibas. arXiv preprint synthesis of casual monocular videos. arXiv:2406.18717, 2024. 2, 4 [76] Shih-Yang Su, Frank Yu, Michael Zollhoefer, and Helge Rhodin. A-nerf: Articulated neural radiance fields for learning human shape, appearance, and pose. In NeurIPS, 2021. [77] Jiakai Sun, Han Jiao, Guangyuan Li, Zhanjie Zhang, Lei Zhao, and Wei Xing. 3dgstream: On-the-fly training of 3d gaussians for efficient streaming of photo-realistic freeviewpoint videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2067520685, 2024. 3 [78] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In ECCV, 2020. 4 11 [79] Zachary Teed and Jia Deng. Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras. Advances in neural information processing systems, 34:1655816569, 2021. 3 [80] Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollhofer, Christoph Lassner, and Christian Theobalt. Non-rigid neural radiance fields: Reconstruction and novel view synthesis of dynamic scene from monocular video. In ICCV, 2021. [81] Vadim Tschernezki, Iro Laina, Diane Larlus, and Andrea Vedaldi. Neural Feature Fusion Fields: 3D distillation of self-supervised 2D image representations. In 3DV, 2022. 2, 3 [82] Qianqian Wang, Vickie Ye, Hang Gao, Jake Austin, Zhengqi Li, and Angjoo Kanazawa. Shape of motion: arXiv preprint 4d reconstruction from single video. arXiv:2407.13764, 2024. 2, 3, 4 [83] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2069720709, 2024. 3 [84] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Zun Wang, Yansong Shi, et al. Internvideo2: Scaling foundation models for multimodal video understanding. In European Conference on Computer Vision, pages 396416. Springer, 2024. 2, 3, 7, 8, 15 [85] Chung-Yi Weng, Brian Curless, Pratul Srinivasan, Jonathan Barron, and Ira Kemelmacher-Shlizerman. Humannerf: Free-viewpoint rendering of moving people from monocular video. In CVPR, 2022. 2 [86] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. arXiv preprint arXiv:2310.08528, 2023. 2 [87] Tianhao Wu, Fangcheng Zhong, Andrea Tagliasacchi, Forrester Cole, and Cengiz Oztireli. D2nerf: Self-supervised decoupling of dynamic and static objects from monocular video. In NeurIPS, 2022. [88] Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil Kim. Space-time neural irradiance fields for free-viewpoint video. In CVPR, 2021. 3 [89] Dejia Xu, Hanwen Liang, Neel Bhatt, Hezhen Hu, Hanxue Liang, Konstantinos Plataniotis, and Zhangyang Wang. Comp4d: Llm-guided compositional 4d scene generation. arXiv preprint arXiv:2403.16993, 2024. 2 [90] Gengshan Yang, Deqing Sun, Varun Jampani, Daniel Vlasic, Forrester Cole, Huiwen Chang, Deva Ramanan, William Freeman, and Ce Liu. Lasr: Learning articulated In CVPR, shape reconstruction from monocular video. 2021. 3 [91] Gengshan Yang, Minh Vo, Neverova Natalia, Deva Ramanan, Vedaldi Andrea, and Joo Hanbyul. Banmo: Building animatable 3d neural models from many casual videos. In CVPR, 2022. 3 [92] Zeyu Yang, Hongye Yang, Zijie Pan, Xiatian Zhu, and Li Zhang. Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting. arXiv preprint arXiv:2310.10642, 2023. 2, 3 [93] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2033120341, 2024. [94] Yuyang Yin, Dejia Xu, Zhangyang Wang, Yao Zhao, and Yunchao Wei. 4dgen: Grounded 4d content generarXiv preprint ation with spatial-temporal consistency. arXiv:2312.17225, 2023. 2, 3 [95] Jae Shin Yoon, Kihwan Kim, Orazio Gallo, Hyun Soo Park, and Jan Kautz. Novel view synthesis of dynamic scenes with globally coherent depths from monocular camera. In CVPR, 2020. 15 [96] Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William Interactive 3d arXiv preprint Freeman, and Jiajun Wu. Wonderworld: scene generation from single image. arXiv:2406.09394, 2024. 2 [97] Justin Yu, Kush Hari, Kishore Srinivas, Karim El-Refai, Adam Rashid, Chung Min Kim, Justin Kerr, Richard Cheng, Muhammad Zubair Irshad, Ashwin Balakrishna, et al. Language-embedded gaussian splats (legs): Incrementally building room-scale representations with mobile robot. arXiv preprint arXiv:2409.18108, 2024. 2, 3 [98] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-Hsuan Yang. Monst3r: simple approach for estimating geometry in the presence of motion. arXiv preprint arXiv:2410.03825, 2024. 3 [99] Shuai Zhang, Huangxuan Zhao, Zhenghong Zhou, Guanjun Wu, Chuansheng Zheng, Xinggang Wang, and Wenyu Togs: Gaussian splatting with temporal opacity Liu. arXiv preprint offset for real-time 4d dsa rendering. arXiv:2403.19586, 2024. [100] Wang Zhao, Shaohui Liu, Hengkai Guo, Wenping Wang, and Yong-Jin Liu. Particlesfm: Exploiting dense point trajectories for localizing moving cameras in the wild. In European Conference on Computer Vision, pages 523542. Springer, 2022. 3 [101] Shuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, and Andrew Davison. In-place scene labelling and understanding with implicit scene representation. In ICCV, 2021. 3 [102] Shijie Zhou, Haoran Chang, Sicheng Jiang, Zhiwen Fan, Zehao Zhu, Dejia Xu, Pradyumna Chari, Suya You, Zhangyang Wang, and Achuta Kadambi. Feature 3dgs: Supercharging 3d gaussian splatting to enable distilled feature fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21676 21685, 2024. 2, 3, 4, 5, 6, 15, 18 [103] Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari, Tejas Bharadwaj, Suya You, Zhangyang Wang, and Achuta Kadambi. Dreamscene360: Unconstrained text-to-3d scene generation with panoramic gaus12 sian splatting. sion, pages 324342. Springer, 2024. In European Conference on Computer Vi- [104] Xingxing Zuo, Pouya Samangouei, Yunwen Zhou, Yan Di, and Mingyang Li. Fmgs: Foundation model embedded 3d gaussian splatting for holistic 3d scene understanding. International Journal of Computer Vision, pages 117, 2024. 2, 3 13 Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile Gaussian Feature Fields"
        },
        {
            "title": "Supplementary Material",
            "content": "This supplement is organized as follows: Section contains more details of 4D reconstruction; Section contains implementation details; Section contains the details of foundation models features extraction and downstream task inference; Section contains the algorithmic details of languageguided 4D editing with LLM; Section contains more baseline comparisons; Section contains ablation studies of our method. A. Details of 4D Reconstruction Our video-to-4D feature field reconstruction pipeline is based upon MoSca [35], state-of-the-art monocular videobased dynamic 3D scene reconstruction method. We augment their representation with our proposed unified feature field and follow their training procedure with added feature rendering loss to the original optimization objective. Here we give brief overview of the training pipeline. Please refer to MoSca [35] for more details. A.1. Dynamic Scene Representation Given monocular input video, the underlying dynamic 3D scene is modeled as the composition of static 3D background, represented with set of static 3D Gaussians {Gstatic}, and dynamic 3D foreground, represented by set of 3D Gaussians {G} that deform over time. The deformation of Gaussians {G} is modeled by structured representation named 4D Motion Scaffold. It is graph (V, E) where the nodes are 3D motion trajectories v(i) = [Q(i) ], = [R, t] SE(3). Intuitively, they describe the rigid transformations or 6DoF poses of points through time. They also each have control radius attribute r(i), describing the range of influence. 1 , . . . , Q(i) Given two nodes, v(i), v(j), we define their distance as the maximum distance of their translation component over all timesteps. Specifically, D(i, j) = maxτ t(i) τ . Intuitively, two nodes are close to each other only if they are close at all timesteps. Based on distance metric D, we construct K-Nearest Neighbor (KNN) Graph (V, E), describing the mutual influence of nodes on each other. τ t(j) The set of dynamic 3D Gaussians can be thought of as the union of 3D Gaussians from all timesteps. Each Gaussian is originally spawned at certain source timestep τ , with position µ and rotation R. However, to render the scene at target timestep τ , we fuse Gaussians from other source timesteps as well. This helps with the partiality of the single view observation at τ . To do so, we 14 need to compute the motion or the deformation of Gaussian from τ to τ , which we achieve by querying the Motion Scaffold (V, E). From high level, we find the node trajectories closest to and use the interpolation of their deformation (given by Qτ Q1 τ ) as the deformation of G. For computation efficiency, in practice, we first identify the node trajectory v(i) closest to based on their positions at τ . Specifically, = arg mini t(i) τ µ. We then use is K-Nearest neighbors {v(i)}iE(i) as an approximation of Gs closest nodes. We compute the interpolation weights {wi} as wi = NormalizeiE(i) exp( (cid:32) τ 2 2 µ t(i) 2r(i) (cid:33) ) + wi , Intuitively, nodes with closer spatial positions and larger control radius have more influence over G. wi is learnable offset jointly optimized with other model parameters, allowing more flexibility. The transformation of from time τ to τ is computed as Tτ τ = DQB({wi, Qτ τ }) SE(3), where DQB represents Dual Quaternion Blending that interpolates SE(3) elements. We apply Tτ τ to the position and rotation of when rendering it at timestep τ . A.2. Initialization from Lifted 2D Priors We rely on 2D priors from pretrained foundation models to initialize and constrain the system. The most important priors include 1) per-frame monocular metric depths for rough geometry initialization; 2) long-term pixel trajectories , where each trajectory describes points pixel position at each frame, providing rough motion initialization in 2D; and 3) per-frame epipolar error maps computed from dense optical flow, separating static background and dynamic foregrounds. We assume known camera intrinsic and poses for the following and refer the readers to [35] for camera initialization and optimization in case the cameras are unknown. We first compute foreground-background masks from epipolar error maps M. We initialize the static background 3D Gaussians by back-projecting points in the static background masks using depth estimations. For the dynamic foreground, we lift 2D pixel trajectories to 3D space with monocular depth predictions and filter out dynamic ones based on epipolar error maps, then sample subset of them based on trajectory distance metric as the initial Motion Scaffold nodes. As these trajectories only contain 3D positions, we initialize the rotational components with identity matrices. We initialize the dynamic Gaussians by back-projecting points in the dynamic foreground masks. A.3. Optimization We first optimize the static background 3D Gaussians following the standard 3D Gaussian Splatting optimization procedures, using supervision from static regions of the input video frames. The dynamic foreground is optimized in two stages. The first geometric optimization stage focuses on the Motion Scaffolds and produces reasonable deformation field, based on which the second photometric/feature stage spawns dynamic Gaussians and optimizes the final RGB and feature rendering objectives. During geometric optimization, the Scaffold trajectories are optimized subject to set of physics-inspired losses: an as-rigid-as-possible loss that encourages nearby nodes to deform in locally-rigid ways, and smoothness losses on the velocity and acceleration of points to encourage smooth motions. During photometric optimization, we deform the dynamic Gaussians with Motion Scaffolds, combine them with the static Gaussians, rasterize them into RGB images and feature maps, and jointly optimize all parameters including Gaussian parameters and the Motion Scaffolds. B. Implementation Details Feature Decoder Configuration While training our unified latent feature field, the rendered feature map from camera view is passed through decoder head to obtain the final feature map for each feature type. Our decoder heads are simple MLPs, which enlarge the input feature dimension by factor of 2 at each layer. For example, our latent feature dimension is 32 but the CLIP feature dimension is 512, so the decoder simply maps 32 to 64, 64 to 128, and finally 128 to 512. Between each linear layer in the decoder, we use ReLU activation function. N-Dimensional Feature Map Rendering We utilize the parallel N-dimensional Gaussian rasterizer from Feature 3DGS [102], which leverages point-based α-blending approach to rasterize feature maps. This method ensures that the rendered feature maps and RGB images are produced at the same resolution. Before calculating the loss, we align the size of the rendered feature map with the ground truth feature map. For SAM2 [68] and CLIP-LSeg [36] features, this is achieved using bilinear interpolation, while area resampling is applied to InternVideo [84] features due to the patchify mechanism used during feature extraction. Moreover, the parallel N-dimensional rasterizer is designed with adaptability, enabling flexible adjustments to the various dimensions of our unified latent feature field. Training Our model is trained end-to-end to jointly reconstruct the versatile Gaussian feature field and the radiance field. We adopt the same training schedule as MoSca [35]: 4000 iterations for static Gaussian optimization and 5000 for dynamic one on the DAVIS dataset [61], and 8000 iterations for static and 5000 for dynamic Gaussian optimization on the NVIDIA dataset [95]. C. Feature Extraction and Inference with"
        },
        {
            "title": "Foundation Models",
            "content": "sequentially using the image encoder SAM2 Feature We extract per-frame ground truth features from SAM2 [68]. Specifically, each frame is processed by Hiera [72] image encoder, which is pre-trained with MAE [22], to produce feature map with resolution of 64 64 and feature dimension of 256. Consequently, the ground truth features for the input video have shape of 256 64 64, where denotes the number of frames. Our promptable / promptless segmentation results are obtained by feeding the rendered SAM2 feature map into the SAM2 decoding architecture, which includes memory attention module, prompt encoder, and mask decoder. These components interact with memory encoder and memory bank to retain and propagate segmentation information. For promptable segmentation, points, boxes, or masks are input into the prompt encoder to define the objects extent in frame. For promptless segmentation, SAM2s automatic mask generator produces segmentation masks for frame, which are then input into the prompt encoder. Once segmentation is performed on any initial frame, SAM2s memory modules enable automatic tracking and propagation of masks across subsequent frames by conditioning each frames features on past predictions. CLIP-LSeg Feature For CLIP-LSeg, we utilize the CLIP ViT-L/16 image encoder to generate ground truth per-pixel feature maps and the ViT-L/16 text encoder for text feature extraction. This encoder can automatically resize any input image to generate feature map with the longer side set to 480. We use square input images in our experiments so the CLIP-LSeg image encoder produces ground truth features with resolution of 480 480 and feature dimension of 512. During inference for semantic segmentation, the rendered features with shape (512, 480, 480) are reshaped into (480 480, 512), referred to as the image feature. Meanwhile, the text feature extracted from the CLIP text encoder has shape of (C, 512), where is the number of categories. matrix multiplication is then performed between 15 Figure A. Feature Field Visualizations. We visualize our versatile Gaussian feature field along with its decoded SAM2, CLIP, and InternVideo feature fields using PCA. the image feature and the text feature to align pixel-level features with text queries. The resulting features are further processed using LSeg spatial regularization blocks to generate the semantic segmentation masks. InternVideo Feature Given an input video, internvideo first resizes it to 224 224 resolution, then takes 14 14 as patch, passing through convolutional neural network to get the initial feature map with 1408 channels. Then, pretrained class token input is concatenated to the initial feature map and passed through transformer encoder to get the final feature map, which is in the shape of (T 1616+ 1) 1408, where is the number of frames. As the class token represents the whole video class information and is not dependent on individual pixels, we save and detach it when distilling the 4D feature field. During inference, we first concatenate the class token back into the novel-view rendered InternVideo feature to obtain the gathered feature. This gathered feature is then directly input into Video-LLM [38] to perform free-form visual question answering (VQA). Since the feature map can be rendered from any viewpoint of the 4D scene at high speed, our approach enables seamless connection between the 4D scene and the AI agent (chatbot). Feature Fields Visualization As shown in Fig. A, we leverage Principal Component Analysis (PCA) from the scikit-learn library [58] to visualize feature fields from global novel views. We configure the PCA to use 3 components corresponding to RGB channels and compute the PCA mean by sampling every third element along the vectors. These vectors have feature dimensions of 32 (for unified latents), 256 (for SAM2), 512 (for CLIP), and 1408 (for InternVideo). The feature map is then transformed using the calculated PCA components and mean. This process involves centering the features using the PCA mean, followed by projection onto the PCA components. The transformed feature is subsequently normalized by removing outliers and scaling based on the minimum and maximum values, standardizing the feature values into uniform range suitable for visualization. D. Details of LLM-powered 4D Editing An overview of the editing pipeline is illustrated in Fig. B. It begins with user inputs, such as Make the dogs color look like Clifford, which are processed by GPT-4o model to extract editable configurations (e.g., objects, operations, targets, and thresholds). Each Gaussian xi is defined by (fi, αi, ci), where fi R512 is the semantic feature, αi is the opacity, and ci R3 is the color. Guided by input specified object categories (e.g., dog, cow), the CLIP ViT-B/32 text encoder encodes the text into features {t1, . . . , tC}, where ti R512 and is the number of categories. The inner product between text and semantic features, followed by softmax, produces semantic score matrix scores RN C. 16 Figure B. Overview of the editing framework. GPT-4o generates different editing configurations based on user prompts, selects target regions via hybrid filtering, evaluates their outputs, and selects the best configuration. To select Gaussians that corresponds to the editing, we use combination of two masking schemes derived from the semantic scores. The first one is binary thresholding. The query label {1, 2, . . . , C} (or {1, 2, . . . , C} if it is list of categories) determines the corresponding column of the score matrix scorel = [s1l, s2l, . . . , sN l]. Indices where sil th are selected (set to 1), while the rest are excluded (set to 0). The selected indices define the target region, and unselected Gaussians are masked out. The second masking scheme is determined by assigning each Gaussian to the category with the highest score using argmax as shown in Fig. B, producing category vector categories = [c1, c2, . . . , cN ], where ci = argmax{si1, . . . , siC}. Gaussians are selected if their category aligns with the query label l, and others are excluded. Their resulting masks are combined using bitwise OR operation. This method balances flexibility and precision, enabling more robust selection. To optimize editing parameters, GPT-4o generates multiple candidate configurations with varying thresholds or transformations. Each candidate is used to execute an editing operation, and the resulting edited images are rendered. GPT-4o then evaluates these outputs and selects the best configuration, which is subsequently applied across the entire video to ensure consistent and high-quality results. E. Baseline Comparisons Segment Anything (SAM2) In the main paper, we highlighted that our SAM2 inference implementation from the rendered unified latent feature map only needs to interact with the SAM2 decoding architecture. This is an improvement over the naive approach, which renders the RGB images (ordinary novel view synthesis like MoSca [35]) first and then processes it through the entire SAM2 encoderdecoder pipeline. We quantitatively evaluate our approach (denoted as Feature) against the naive baseline (rendered novel view RGB + SAM2, denoted as RGB) on two datasets (Nvidia [47] and Nerfies [56]) in Tab. A. point prompt is randomly selected on the dynamic object in the first frame of the ground-truth 17 NVIDIA Exp1 Exp2 Exp3 Mean Time (s) Nerfies Exp1 Exp2 Exp3 Mean Time (s) RGB 0.656 0.246 0.467 0.456 Feature 0.761 0.728 0.727 0.739 1.83 1.01 RGB 0.484 0.536 0.538 0.519 Feature 0.560 0.662 0.561 0.594 9.10 3.10 Table A. SAM2 Quantitative Results (mIoU) on NVIDIA and Nerfies Datasets. Method PSNR SSIM LPIPS accuracy mIoU Static Model Size (MB) Dynamic Model Size (MB) Size (MB) Scene Jumping Jumping Jumping Jumping Skating Skating Skating Skating Truck Truck Truck Truck Umbrella Umbrella Umbrella Umbrella Balloon1 Balloon1 Balloon1 Balloon1 Balloon2 Balloon2 Balloon2 Balloon MoSca [35] MoSca + Feature 3DGS [102] Ours (single CLIP head) Ours (full model) MoSca [35] MoSca + Feature 3DGS [102] Ours (single CLIP head) Ours (full model) MoSca [35] MoSca + Feature 3DGS [102] Ours (single CLIP head) Ours (full model) MoSca [35] MoSca + Feature 3DGS [102] Ours (single CLIP head) Ours (full model) MoSca [35] MoSca + Feature 3DGS [102] Ours (single CLIP head) Ours (full model) MoSca [35] MoSca + Feature 3DGS [102] Ours (single CLIP head) Ours (full model) MoSca [35] Playground Playground MoSca + Feature 3DGS [102] Playground Playground Ours (single CLIP head) Ours (full model) Mean Mean Mean Mean MoSca [35] MoSca + Feature 3DGS [102] Ours (single CLIP head) Ours (full model) 24.558 24.516 24.633 24. 31.478 31.568 31.572 31.666 26.688 26.619 26.630 26.610 23.355 23.362 23.433 23.392 22.666 22.687 22.668 22.691 26.827 27.018 26.904 26.871 20.591 20.569 20.463 20. 25.166 25.191 25.186 25.197 0.792 0.793 0.795 0.793 0.926 0.927 0.927 0.926 0.824 0.824 0.820 0.822 0.706 0.708 0.707 0.708 0.760 0.762 0.760 0. 0.850 0.854 0.851 0.853 0.777 0.776 0.775 0.775 0.805 0.806 0.805 0.805 0.092 0.092 0.090 0.090 0.059 0.059 0.059 0.059 0.115 0.115 0.122 0. 0.185 0.176 0.185 0.180 0.117 0.115 0.118 0.117 0.082 0.080 0.081 0.078 0.130 0.124 0.130 0.130 0.111 0.109 0.112 0.110 - 0.840 0.836 0. - 0.835 0.838 0.819 - 0.973 0.971 0.969 - 0.875 0.869 0.880 - 0.901 0.905 0.903 - 0.819 0.821 0.813 - 0.922 0.922 0. - 0.881 0.880 0.876 - 0.483 0.495 0.483 - 0.446 0.450 0.418 - 0.880 0.878 0.868 - 0.556 0.559 0.565 - 0.377 0.435 0. - 0.350 0.321 0.319 - 0.447 0.430 0.419 - 0.506 0.510 0.503 29.08 271.30 44.15 44.09 32.49 302.50 49.32 47.52 38.31 353.97 58.35 58. 71.29 657.96 107.87 107.58 56.88 525.09 85.95 85.72 51.82 475.71 78.22 77.77 92.74 857.32 141.02 140.44 53.230 491.979 80.697 80.183 29.70 212.58 30.51 30. 4.90 35.40 4.90 5.60 11.84 90.18 12.27 13.51 11.42 75.12 11.28 11.50 18.59 129.88 18.84 19.90 15.22 108.96 15.18 15.36 9.80 61.38 9.20 10. 14.496 101.929 14.597 15.274 58.78 483.88 74.66 74.93 37.39 337.90 54.22 53.12 50.15 444.15 70.62 71.67 82.71 733.08 119.15 119.08 75.47 654.97 104.79 105. 67.04 584.67 93.40 93.13 102.54 918.70 150.22 150.65 67.726 593.907 95.294 95.457 Table B. Detailed Performance of 7 scenes from the NVIDIA Dataset. novel view video, and SAM2 is used to generate per-frame ground-truth masks. We evaluate mIoU across the entire dataset and repeat the experiments three times (Exp1-3) to assess generalizability. Our approach outperforms the baseline, which, though yielding smoother masks, lacks robustness. We observe that artifacts in RGB rendering can mislead SAM2 during encoding, causing ambiguity and highly inaccurate segmentation. In contrast, our feature space inference mitigates these issues, enhancing robustness and increasing speed. Semantic Segmentation (CLIP-Lseg) In main paper Tab. 1, we provide the average performance metrics of our method and the comparison baselines across the 7 scenes of NVIDIA dataset. Here in Tab. B, we present the detailed performance metrics on each scene. We can conclude that our full model with versatile feature field is not only fast and compact but also on par with any other task-specific feature field distillation methods. Additionally, Fig. presents the qualitative results of semantic segmentation on Jumping scene using our full model, demonstrating the reliability of our approach. We provide additional quantitative results on the Nerfies dataset [56] in Tab. to evaluate our methods generalization capability. While we do not claim improving downstream task performance as contribution, our method surpasses the naive baseline (novel view rendered per-frame RGB + LSeg) on this dataset while achieving 7.7 faster 18 inference. F. Ablation Studies In all our experiments, we set the default dimension of the unified latent feature to 32, striking balance between speed and quality. In this section, we present an ablation study to explore the impact of varying the feature dimensions. In Fig. and Fig. we compare the training and rendering time for different dimensions of our unified latent feature. Notably, dimension of 32 marks the threshold where further increases in dimensions lead to significant increase in training and rendering times. This makes larger dimensions impractical for our use case. Segment Anything (SAM2) In Fig. we compare and contrast the SAM2 inference experiment results derived from different dimensions unified latent feature maps (8, 16, 32, 64, 128, 256, 512) to justify our choice of 32 dimensions. For dimensions higher than 32, we can see that the segmentation masks are of poor quality and cannot be accurately propagated through the video frames. For feature dimension 8, we see that while the segmentation mask tracks our intended object, it also erroneously includes much of the empty background. The segmentation masks for the 16 dimension experiment track accurately but do not fully cover the intended object at = 55 and = 85. Overall, the segmentation masks produced from 32 dimensional unified latent feature maps are the best in terms of segmentation quality and tracking accuracy. Given the relatively fast training and rendering, 32 should be the optimal dimension for our unified latent feature field in regards to SAM2 segmentation results. Semantic Segmentation (CLIP-LSeg) We study the effect of different latent scene feature dimensions on the Jumping scene of NVIDIA. In Tab. (with plots Fig. and Fig. H), we report the training time as well as the performance of the semantic segmentation. The results show that compared to 512, which is the original dimension of CLIP-LSeg feature, our method with dim = 32 achieves comparable performance on mIoU and accuracy, while being 5.23 faster on training. We also report the image quality metrics in Tab. (with plots Fig. and Fig. J) which shows that our feature field distillation method does not affect the image quality. 19 Figure C. CLIP semantic segmentation quality comparison. We compare the CLIP semantic segmentation quality between ground-truth (inference from RGB) and our implementation (inference from feature map) for both training and novel views."
        },
        {
            "title": "Nerfies Scene",
            "content": "mIoU Accuracy Time (s)"
        },
        {
            "title": "Feature",
            "content": "Broom Curls Tail Toby-sit"
        },
        {
            "title": "Mean",
            "content": "0.193 0.514 0.261 0.504 0.368 0.333 0.443 0.338 0.470 0.396 0.321 0.877 0.652 0.757 0. 0.610 0.872 0.860 0.737 0.770 207.59 155.25 389.89 355.82 277.14 30.22 20.82 46.43 45.96 35. Table C. Semantic Segmentation Quantitative Results on Nerfies Dataset."
        },
        {
            "title": "Dimension",
            "content": "Training Time (h) Rendering Time (s) mIoU Accuracy 8 2.30 4.818 0.468 0.827 16 2.48 4.898 0.483 0.831 2.83 4.872 0.482 0.830 64 3.45 4.967 0.485 0.834 128 4.22 5.815 0.471 0.832 512 8.75 10.167 0.494 0.837 14.80 16.313 0.497 0.841 Table D. Evaluation of Semantic Segmentation Performance On NVIDIA Jumping Scene Across Different Dimensions. This table presents the Time, mIoU, and Accuracy corresponding to each dimension level."
        },
        {
            "title": "Dimension",
            "content": "8 16 32 64 128 512 PSNR SSIM LPIPS 24.466 0.788 0.092 24.595 0.793 0.091 24.616 0.793 0.090 24.623 0.790 0. 24.585 0.792 0.091 24.629 0.796 0.090 24.491 0.789 0."
        },
        {
            "title": "MoSca",
            "content": "24.558 0.792 0.092 Table E. Evaluation of Image Quality Metrics On NVIDIA Jumping Scene Across Different Dimensions. This table presents the PSNR, SSIM, and LPIPS values corresponding to each dimension level. 21 Figure D. SAM2 segmentation quality comparison for different dimensions of unified latent feature maps Best performing SAM2 segmentation is derived from the 32-dimensional unified latent feature map. 22 Figure E. Training Time vs Unified Latent Feature Dimensions We show the training time required with different dimensions of unified latent feature map. Figure F. Rendering Time vs Unified Latent Feature Dimensions We show the rendering time required for different dimensions of unified latent feature map. Figure G. Training Time vs CLIP Feature Dimensions We show the training time required with different dimensions of rendered CLIP features. Figure H. Rendering Time vs CLIP Feature Dimensions We show the rendering time required for different dimensions of rendered CLIP features. Figure I. mIoU vs CLIP Feature Dimensions We show mIoU with respect to different rendered CLIP feature dimensions. Figure J. Accuracy vs CLIP Feature Dimensions We show accuracy with respect to different rendered CLIP feature dimensions."
        }
    ],
    "affiliations": [
        "DEVCOM ARL",
        "MIT",
        "Stanford",
        "UCLA",
        "UT Austin"
    ]
}