{
    "paper_title": "A Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement",
    "authors": [
        "Hui Yuan",
        "Yifan Zeng",
        "Yue Wu",
        "Huazheng Wang",
        "Mengdi Wang",
        "Liu Leqi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning from Human Feedback (RLHF) has become the predominant approach for language model (LM) alignment. At its core, RLHF uses a margin-based loss for preference optimization, specifying ideal LM behavior only by the difference between preferred and dispreferred responses. In this paper, we identify a common pitfall of margin-based methods -- the under-specification of ideal LM behavior on preferred and dispreferred responses individually, which leads to two unintended consequences as the margin increases: (1) The probability of dispreferred (e.g., unsafe) responses may increase, resulting in potential safety alignment failures. (2) The probability of preferred responses may decrease, even when those responses are ideal. We demystify the reasons behind these problematic behaviors: margin-based losses couple the change in the preferred probability to the gradient of the dispreferred one, and vice versa, often preventing the preferred probability from increasing while the dispreferred one decreases, and thus causing a synchronized increase or decrease in both probabilities. We term this effect, inherent in margin-based objectives, gradient entanglement. Formally, we derive conditions for general margin-based alignment objectives under which gradient entanglement becomes concerning: the inner product of the gradients of preferred and dispreferred log-probabilities is large relative to the individual gradient norms. We theoretically investigate why such inner products can be large when aligning language models and empirically validate our findings. Empirical implications of our framework extend to explaining important differences in the training dynamics of various preference optimization algorithms, and suggesting potential algorithm designs to mitigate the under-specification issue of margin-based methods and thereby improving language model alignment."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 1 ] . [ 1 8 2 8 3 1 . 0 1 4 2 : r Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement Hui Yuan1, Yifan Zeng2, Yue Wu3, Huazheng Wang4, Mengdi Wang5, Liu Leqi6 1,3,5Princeton University 2,4Oregon State University 6The University of Texas at Austin"
        },
        {
            "title": "ABSTRACT",
            "content": "Reinforcement Learning from Human Feedback (RLHF) has become the predominant approach for aligning language models (LMs) to be more helpful and less harmful. At its core, RLHF uses margin-based loss for preference optimization, which specifies the ideal LM behavior only in terms of the difference between preferred and dispreferred responses. In this paper, we identify common pitfall of margin-based methodsthe under-specification of ideal LM behavior on preferred and dispreferred responses individually, which results in two unintended consequences as the margin increases: (1) The probability of dispreferred (e.g., unsafe) responses may increase, resulting in potential safety alignment failures. (2) The probability of preferred responses may decrease, even when those responses are ideal. We demystify the reasons behind these problematic behaviors: margin-based losses couple the change in the preferred probability to the gradient of the dispreferred one, and vice versa, often preventing the preferred probability from increasing while the dispreferred one decreases, and thus causing synchronized increase or decrease in both probabilities. We term this effect, inherent in margin-based objectives, gradient entanglement. Formally, we derive conditions for general margin-based alignment objectives under which gradient entanglement becomes concerning: the inner product between the gradient of preferred log-probability and the gradient of dispreferred log-probability is large relative to the individual gradient norms. We theoretically investigate why such inner products can be large when aligning language models and empirically validate our findings. Empirical implications of our framework further extend to explaining important differences in the training dynamics of various preference optimization algorithms, and suggesting potential algorithm designs to mitigate the under-specification issue of margin-based methods and thereby improving language model alignment."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement Learning from Human Feedback (RLHF) has become primary approach for aligning Language Models (LMs) to improve their helpfulness and mitigate harmfulness (Stiennon et al., 2020; Bai et al., 2022; Ouyang et al., 2022). This pipeline typically consists of two stages: supervised fine-tuning (SFT), where demonstration data is used to directly teach the model desirable behaviors, and the reinforcement learning (RL) stage, which uses preference datacomparisons between different responses to the same promptto highlight the contrast between chosen and rejected responses, with the goal of helping the model learn distinctions between good and bad behaviors. In its vanilla form, the RL stage first employs contrastive lossbased on the margin between the scores of the chosen and rejected responsesto train reward model, followed by policy optimization methods to fine-tune the LM based : Leading Contributors. Corresponding to: huiyuan@princeton.edu, leqiliu@utexas.edu. Work done in part at Princeton Language & Intelligence. 3Code for the paper can be found at https://github.com/HumainLab/Understand_MarginPO. Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement on the reward model. Leveraging the structure of the problem, recent line of work has combined these two steps by directly optimizing the language model using margin-based preference optimization loss of the following general form (Rafailov et al., 2024; Azar et al., 2024; Xu et al., 2024; Ethayarajh et al., 2024; Hong et al., 2024; Pal et al., 2024; Park et al., 2024; Yuan et al., 2024; Meng et al., 2024; Zhao et al., 2023; Wu et al., 2024):4 ℓ(x, yw, yl; θ) = m(hw(log πθ(ywx)) hl(log πθ(ylx))), (1) 5 and log πθ(ylx) where for language model πθ, log πθ(ywx) specifies the log-probability of the chosen response yw specifies that of the rejected response yl, given the same prompt x. Most of the existing preference optimization losses can be interpreted as varying the scalar functions m, hw, hl (Section 3.2 and Table 2). At the core, they all rely on the margin between the chosen log-probability log πθ(ywx) and the rejected log-probability log πθ(ylx). The training dynamics of these margin-based preference optimization are quite intriguingthe log-probabilities of the chosen and rejected responses often show synchronized increase and decrease (Figure 1). It is worth noting that, by the end of the training, even though the margin increases (resulting in minimization of the margin-based loss), the log probability of both the chosen and rejected responses may increase (Figure 1a), or both may decrease (Figure 1b). (a) Mistral 7B (b) Llama-3 8B Figure 1: Training dynamics of the chosen and rejected log probabilities on the TL;DR dataset (Stiennon et al., 2020), with log probabilities reported on the evaluation set. As the margin between the two increases, the chosen and rejected log-probabilities exhibit synchronized increases and decreases per step. In Figure 1a, both chosen and rejected log-probabilities have an overall trend of increasing, especially towards the end of training, whereas in Figure 1b, both have trend of decreasing. This synchronized log-probability change exposes fundamental issue with using margin-based loss for preference optimization in language model alignment: it only specifies the ideal behavior of the margin between chosen and rejected log-probabilities, but not the ideal behavior of individual terms. This under-specification may have two problematic consequences: First, when the primary goal is to reduce the probability of generating rejected responses (e.g., in safety-related alignment tasks where certain undesirable responses should not be generated), merely increasing the margin (i.e., ensuring that the chosen response is preferred over the rejected one) does not guarantee that the log-probability of the rejected response is actually decreasing (Figure 1a). Second, even when the log-probability of the rejected response does decrease, the current margin-based losses often lead to simultaneous reduction in the log-probability of the chosen response (Figure 1b). This becomes particularly concerning when we want to retain or even increase the probability of generating the preferred responses. For example, for distilling strong language models into smaller ones (Dubey et al., 2024; Chiang et al., 2023; Tunstall et al., 2024; Taori et al., 2023), common practice is to synthesize chosen samples with those strong models; in some alignment applications (e.g., math problem-solving and coding), chosen samples can be the human demonstrations collected during the SFT phase (Chen et al., 2024). In both scenarios, the chosen responses are ideal and we want the probability of the chosen response to increaseor at least not decreaseto ensure the model retains high probability of generating these ideal responses. 4The reward modeling loss in vanilla RLHF is also an example of this general form. 5Subscript in chosen response yw stands for winner\", in yl stands for loser\". 2 Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement It is worth noting that there exist scenarios where the ideal behavior of LM on chosen and rejected samples is unclear, e.g., in the original RLHF procedure, the chosen and rejected pairs are drawn from models still in training (Stiennon et al., 2020). Our study is motivated by the previous two scenarios where, ideally, the LMs probabilities on chosen samples should increase and that on rejected samples should decrease during alignment. However, most margin-based methods fail to induce the ideal behavior (Figure 1, Figure 2), which highlights the need for understanding this common pitfall. Throughout the paper, we refer to log πθ(ywx) as the chosen log-probability and its gradient, θ log πθ(ywx), as the chosen gradient; similar definitions apply for the rejected case. In this work, we demystify the reasons why log πθ(ywx) and log πθ(ylx) exhibit synchronized increase or decrease during alignment. We uncover that the underlying cause is the gradient entanglement effect inherent in margin-based objectives: margin-based losses couple the change in the chosen log-probability to the gradient of the rejected one, and vice versa, preventing the chosen and rejected probabilities from changing independently. Formally, we characterize gradient entanglement happens because the change in the chosen and rejected probability depends on the inner product θ log πθ(ywx), θ log πθ(ylx) between the chosen and rejected gradients. This entanglement will result in synchronized changes in the chosen and rejected log-probability when the inner product is large relative to their individual norms, which we name by gradient condition\" (Section 3.1). Moreover, the precise definitions of large for different margin-based algorithms are captured by general version of the gradient condition (Section 3.2). The gradient conditions we derived enable us to characterize existing margin-based preference optimization methods, explain their differing training dynamics, and identify the most suitable scenarios for deploying these algorithms. Our theoretical findings are also validated through empirical observations (Section 3.3). We further investigate why the gradient inner product can be large when aligning model using language data. In synthetic settings, we theoretically show that (1) as the chosen and rejected responses share more similar tokens, their gradient inner product will increase, and (2) while the sentence-level gradient inner product may be large and positive, individual token-level inner products can be small and negative (Section 4.1, 4.2). We validate these theoretical insights empirically (Section 4.3), and our findings suggest two potential algorithm designs to mitigate the gradient entanglement effect: pairwise normalized gradient descent and sparsity regularized token masking (Section 5.1, 5.2). To summarize, our contributions are as follows: We identify fundamental issue with margin-based preference optimization: it under-specifies the ideal behavior of the LM on chosen and rejected responses individually, which often results in synchronized increase/decrease in the chosen and rejected log-probabilities (Section 1); We uncover that gradient entanglement is the inherent cause of the pitfalls in margin-based objectives, and provide general gradient inner product condition that captures when the synchronized movement of chosen and rejected log probabilities occurs (Section 3); We investigate the gradient inner product and explore when the condition may fail and the synchronized movement occurs theoretically and experimentally (Section 4). Using our framework, we outline two potential approaches to resolve gradient entanglement: one based on normalized gradients (Section 5.1) and the other leveraging token-level information (Section 5.2)."
        },
        {
            "title": "2 Background and Related Work",
            "content": "2.1 Preference optimization We consider auto-regressive language models π(ytx, y<t) that specify the distribution of the next token yt at index on finite vocabulary set V, given the prefix tokens including the prompt and the partially generated responses y<t. In the context of LM alignment, there is reference policy πref, usually obtained by large-scale pre-training and supervised fine-tuning, and serves as the sampling policy and start point of further alignment algorithms. 2.2 Existing methods There have been plenty of works on the design of preference optimization losses, motivated by various assumptions or considerations. Here we briefly review them and discuss their connection to the probability margin: 3 Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement Rafailov et al. (2024) derive the DPO loss from the KL-constrained reward maximization problem: max θ ExX ,yπθ(x)[r(y; x)] βExX [KL(πθ(x)πref(x))]. They further derive the DPO loss for any triplet (x, yw, yl) where the yw, yl are the chosen and rejected response, respectively: ℓDPO(x, yw, yl; θ; πref) := log σ β log (cid:32) (cid:20) (cid:19) (cid:18) πθ(ywx) πref(ywx) log (cid:18) πθ(ylx) πref(ylx) (cid:19)(cid:21)(cid:33) . (2) Motivated by non-transitive human preference and language model calibration respectively, Azar et al. (2024) and Zhao et al. (2023) propose IPO and SlicHF loss with similar forms that solely depend on the margin log πθ(ywx) log πθ(ylx). 1 Due to the length bias observed in practice, Park et al. (2024) propose to add length penalty term in the BT preference model, but the gradient still relies on the margin log πθ(ywx) log πθ(ylx). Meng et al. (2024) and Yuan et al. (2024) consider the setting of average rewards and derive loss dependent on the length-normalized margin yw log πθ(ywx) 1 Unlike prior work, Ethayarajh et al. (2024) and Wu et al. (2024) do not consider the difference between the likelihood, but deal with the chosen and rejected response separately. These works typically assign positive reward signal to the chosen response and negative reward signal to the rejected one, according to the logistic loss (Ethayarajh et al., 2024) or the square loss (Wu et al., 2024). yl log πθ(ylx). (Pal et al., 2024) observes decrease in the log-probability of chosen response during DPO when the edit distances between each pair of completions are small in preference datasets. To fix the decrease, natural way is to add explicit regularization to the loss objective, to force the increase of the chosen responses log-probability. In particular, (Pal et al., 2024) propose the DPOP loss that behaves the same as DPO when the chosen responses log-ratio log (cid:0) πθ(ywx) (cid:1) πref(ywx) is above 0, while adds an explicit regularization when the ratio is below 0. Similarly, Xu et al. (2024) and Zhao et al. (2023) also add explicit regularization to maximize the chosen responses log-probability. Among these works, the most relevant to ours is Pal et al. (2024), which touches upon similar failure mode of DPO. The main difference is that they focus on mitigating only the decrease mode of the chosen responses probability by new loss designs. In contrast, we dig deeper to obtain broader view on the synchronized change (increase or decrease) in chosen and rejected probabilities. We rigorously analyze the training dynamics and extract general success/failure conditions based on gradient correlation, which applies to range of margin-based losses for preference optimization."
        },
        {
            "title": "3 Gradient Entanglement",
            "content": "Margin-based preference optimization often results in synchronized increase/decrease in chosen and rejected logprobabilities (Section 1). Our key finding is that the synchronized change is caused by an effect we term as gradient entanglement. Starting with case study on DPO in Section 3.1, we formally define the gradient entanglement effect, from the definition we will see the entanglement is passed through the inner product between chosen and rejected gradients. We derive conditions on such inner product under which the gradient entanglement causes concerning synchronized change. In Section 3.2, we identify gradient entanglement for general margin-based preference optimization methods and apply our framework to explain the training dynamics of those methods. We validate our findings empirically in Section 3.3. 3.1 Case study: gradient entanglement in DPO Let us start with deriving the gradient of the DPO objective (2). To simplify the formula of DPO gradient, we define the implicit reward ˆrθ(x, y) := β log πθ(yx) πref (yx) (which is scalar) and introduce the notations: log πw(θ) := log πθ(ywx), log πl(θ) := log πθ(ylx), c(θ) := σ (ˆrθ (x, yl) ˆrθ (x, yw)) > 0. Then considering single sample (x, yw, yl), the DPO gradient can be rewritten as θℓDPO = βc(θ) (θ log πw(θ) θ log πl(θ)). (3) 6When the context is clear, we omit θ and just use log πw, log πl and . 4 Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement Suppose η > 0 is the step size for minimizing the DPO objective and let = ηβc(θ). After one step gradient descent with (3), simple analysis of the log-probability change in chosen and rejected responses uncovers the intriguing gradient entanglement effect as follows: Gradient Entanglement (DPO) The chosen log-probability change log πw depends on the rejected gradient log πl, and similarly, the rejected log-probability log πw change depends on the chosen gradient log πl: log πw (cid:0) log πw2 log πw, log πl(cid:1) , log πl (cid:0) log πw, log πl log πl2(cid:1) . (5) (4) (4) and (5) are derived by approximating log πw and log πl with first-order Taylor expansion (Appendix A.1). Beyond the DPO objective, the gradient entanglement effect is an inherent characteristic of margin-based objectives as the chosen and rejected log-probability are coupled in the definition of margin. In Section 3.2, we will formally derive gradient entanglement for general margin-based objectives for preference optimization. From the above definition, we can see that the entanglement effect is passed through the inner product log πw, log πl between chosen and rejected gradients. In the absence of log πw, log πl, the log-probability changes log πw and log πl will not depend on each other. In the sequel, we will derive conditions on this inner product under which the gradient entanglement will have concerning effects. 3.1.1 When will the gradient entanglement be concerning? If we measure the change in the margin between log πw and log πl, i.e., the quantitiy (log πw log πl), then the CauchySchwarz inequality ensures: (log πw log πl) ( log πw2 2 log πw, log πl + log πl2) 0, which fulfills the contrastive goal of the DPO loss: enlarging the difference between the chosen log-probability log πw and rejected log-probability log πl. However, due to the gradient entanglement effect, to individually ensure the increment of log πw and the decrement of log πl, the inner product between chosen and rejected gradient should satisfy conditions listed in Condition 1. We will refer to Condition 1 as gradient condition\" as it is imposed on the inner product of gradients. Condition 1 (Gradient condition for DPO). In DPO, to increase log πw and decrease log πl individually, (4) and (5) imply the following conditions: log πw, log πl log πw2 log πw 0, log πw increases; log πw, log πl log πl2 log πl 0, log πl decreases. Based on the two conditions above, in Table 1 we summarize three cases that depict all possible changes on the chosen and rejected log-probabilities and are categorized by the value of log πw, log πl. Case log πw, log πl log πw, log πl 1 2 3 log πw 0 log πl log πw log πl 0 log πw log πl log πw log πl log πw log πl 0 log πw log πl Condition log πw, log πl min( log πw2, log πl2) log πw2 log πw, log πl log πl2 log πl2 log πw, log πl log πw2 Table 1: Three possible cases of the changes on chosen and rejected log-probabilities in DPO. and indicate increase and decrease. Case 1 (Ideal): log πw increases and log πl decreases; Case 2: log πw and log πl both decreases but log πl decreases more; Case 3: log πw and log πl both increases but log πw increases more. As one notices, for DPO, the ideal case where the chosen log-probability log πw increases and rejected log-probability log πl decreases only happens when the gradient inner product log πw, log πl is less than the smaller one in the two squared gradient norms: log πw2 and log πl2. It suggests when the correlation between the two gradients is high, the gradient entanglement will cause the chosen and rejected log-probability to increase/decrease 5 Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement synchronously. In Section 3.2, we show for other margin-based preference optimization loss, similar condition on log πw, log πl can be derived, but the condition could be more lenient than that of DPO for some specific losses, explaining why the training dynamics of those methods may differ from DPO. 3.2 General gradient entanglement effect We now move on to the general margin-based loss (1). Here, we additionally consider regularizers used in these losses: (cid:16) ℓ(θ) = m(hw(log πw) hl(log πl)) + Λ(log πw) (cid:17) , (6) where Λ(log πθ(ywx)) is scalar regularizer depending on the chosen log-probability. We instantiate popular preference optimization methods from this general form in Table 2, where we denote cw ref := log πref(ylx), cref := cw ref. Terms that only depend on πref(yx) shall be viewed as constant, independent of θ. ref := log πref(ywx), cl ref cl m(a) hw(a) hl(a) Λ(a) DPO (Rafailov et al.) R-DPO (Park et al.) SimPO (Meng et al.) IPO (Azar et al.) RRHF (Yuan et al.) SlicHF (Zhao et al.) CPO (Xu et al.) DPOP (Pal et al.) KTO (Ethayarajh et al.) SPPO (Wu et al.) 2β ))2 log σ(a cref) log σ(a (cref + α(yw yl))) log σ(a γ) (a (cref + 1 min(0, a) min(0, δ) log σ(a) log σ(a cref) βa βa β yw yw βa βa λ max(0, log cw λwσ(βa (log cw (a β1)2 ref a) ref + zref)) 1 βa βa β yl yl βa βa λlσ((log cl (a + β1)2 λa λa λa ref + zref) a) Table 2: Instantiation of margin-based preference optimization losses. Constants satisfy β, γ, δ, λw, λl > 0. Based on this unified formulation of preference optimization objectives (6), we derive general gradient entanglement for all margin-based losses (derivations in Appendix A.1): Gradient Entanglement (General) The chosen log-probability change depends on the rejected gradient, and vice versa. The mutual dependency is characterized by: log πw η (cid:0)dwθ log πw2 dlθ log πw, θ log πl(cid:1) , log πl η (cid:0)dwθ log πw, θ log πl dlθ log πl2(cid:1) . In the general form of gradient entanglement, dw and dl are scalars defined as dw := m(hw(log πw) hl(log πl))h dl := m(hw(log πw) hl(log πl))h w(log πw) + Λ(log πw), l(log πl). (7) (8) We derive generalized version of DPOs gradient condition (Condition 1) for general margin-based losses. Condition 2 (Gradient condition for general margin-based objectives). For margin-based preference optimization objectives(6), the conditions for log πw to increase and for log πl to decrease are: log πw, log πl log πw, log πl dw dl dl dw log πw2 log πw 0, log πw increases; log πl2 log πl 0, log πl decreases. (9) (10) Accordingly, we can instantiate Condition 2 for different algorithms by using their specialized m, hw, hl, Λ in Table 2. Note that between conditions (9) and (10), for one condition to be more lenient (e.g., if dw/dl > 1 in the chosen condition), the other condition becomes more strict (then dl/dw < 1 in the rejected condition). When log πw and log πl have similar norms and are positively correlated, it is likely that one of (9) and (10) holds while the other fails, Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement explaining why it is easy to observe simultaneous increase or decrease in the probabilities of chosen and rejected responses. This general gradient inner product condition also suggests an interesting new algorithm to achieve our ideal case: we can reweigh the chosen and rejected log-probabilities in the margin-based loss such that dw/dl = log πl/ log πw, which ensures that both parts in Condition 2 are satisfied at the same time. We provide more discussion on potential algorithm design inspired by this observation in Section 5.1. 3.2.1 How do other margin-based methods work differently from DPO? Utilizing the gradient condition we derived, we provide in the following brief discussion on some existing preference optimization algorithms and explain why these algorithms may work differently from DPO under certain settings. DPO: dw dl = dl dw = 1, reproducing the Condition 1. SPPO: dw dl = β1log πw β1+log πl > 17, where β1 is large constant. Compared with DPO, SPPO loss ensures that it is easier for log πw to increase based on (9) and harder for log πl to decrease due to (10). KTO: dw dl λw λl , where λw, λl are two hyperparameters in KTO, fine-tuned according to different tasks and datasets. Thus no general conclusion on the chosen/rejected probability change can be made from our conditions. Explicit regularization on chosen log-probability (CPO, DPOP8, RRHF and Slic-HF): According to the formulas of dw and dl in (7) and (8), the negative log-likelihood (NLL) regularizer on chosen responses enlarges dw while having no influence on dl as Λ 0 and only appears in (7). As result, larger dw makes condition (9) more lenient dl and thus the chosen log-probability is more likely to increase. Length-normalization (SimPO, RRHF and IPO): In SimPO, dw dl as: = yl yw and condition (9) and (10) can be rewritten (cid:28) log πw yw , log πl yl (cid:29) (cid:13) (cid:13) (cid:13) (cid:13) log πw yw 2 (cid:13) (cid:13) (cid:13) (cid:13) ; (cid:28) log πw yw , log πl yl (cid:29) (cid:13) (cid:13) (cid:13) (cid:13) log πl yl 2 (cid:13) (cid:13) (cid:13) (cid:13) . (11) These conditions imply the following: to ensure increasing chosen log-probability while decreasing rejected log-probability, (11) should hold. This is more lenient than the corresponding condition posed for DPO that log πw, log πl min( log πw2, log πl2), when the length of chosen and rejected responses is biased, resulting in either the chosen or rejected gradient norm being significantly higher than the other. Therefore, compared to DPO, SimPO leans towards increasing the chosen probability and decreasing that of the rejected when the preference data is heavily length-biased. The same reasoning also applies to RRHF and IPO9 for their length normalization design. 3.3 Empirical observations We conduct experiments on the TL;DR dataset (Stiennon et al., 2020) to showcase the widely-existing phenomenon that the chosen and rejected log-probabilities have synchronized changes during preference optimization. In addition, Figure 1 depicts how different margin-based preference optimization algorithms influence the log-probability of chosen and rejected responses. For DPO and R-DPO, both the chosen and rejected log-probabilities tend to decrease simultaneously. This behavior proofs the existence of gradient entanglement, showing that methods purely dependent on the margin might result in both terms decreasing, with the rejected log-probability decreasing more significantly. This leads to an increase in the margin, which is the original learning objective, but not necessarily an increase in the chosen log-probability. SPPO demonstrates distinct trend where the log-probability of the chosen responses increases, while the logprobability of the rejected responses decreases. This matches the theoretical intuition obtained from the specialized gradient conditions for SPPO in Section 3.2. 7See Section A.2 for the derivation. 8For DPOP, the regularizer is included in its hw(a) term in Table 2, due to its design to turn on/off the regularizer based on the value of chosen log-probability. 9In the TRL library, the implementation of IPO averages the log-probabilities by the number of tokens. 7 Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement Figure 2: Training dynamics of the chosen and rejected log-probabilities on the TL;DR dataset for different algorithms trained on Mistral 7B. The corresponding plot for Llama3 8B is in Figure 5 (Appendix C.5). For SimPO and IPO, the log-probabilities are normalized by the response length, while in the other plots, the log-probabilities are of entire responses. All algorithms exhibit synchronized increases and decreases in the chosen and rejected log-probabilities. We also provide the cosine similarity plots between θ log πw and θ log πl in Appendix C.5 (Figure 6). For CPO, DPOP, RRHF, and Slic-HF, algorithms with explicit regularization on the chosen log-probability, we observe consistent increase in the log-probability of the chosen responses. This behavior reflects the effect of explicit regularizations in increasing the chosen log-probability, which also aligns with the conditions discussed in Section 3.2. SimPO and IPO10 in Figure 1 report the average log-probability of responses. The simultaneous decrease in both the (average) chosen and rejected log-probabilities is expected, because the loss only depends on the length-normalized yl log πθ(ylx). Again, an increase in the margin is guaranteed, but not necessarily an margin, increase in the average chosen log-probability due to the gradient entanglement effect. yw log πθ(ywx) 1 1 Overall, experimental results on various margin-based losses closely align with our analysis on the gradient entanglement and the gradient conditions outlined in Section 3.2, demonstrating how loss structures, explicit regularization, lengthnormalization and other design choices influence the dynamics of preference optimization."
        },
        {
            "title": "Investigation on Gradient Inner Product",
            "content": "The previous section reveals that the gradient entanglement effect is driven by the key quantity: the inner product θ log πw, θ log πl between chosen and rejected log-probabilities (Condition 1, 2: gradient condition). As demonstrated in Section 3.3 and widely observed in practice, margin-based objectives are often triggered to not behave in the ideal way, suggesting that the gradient condition is violated due to large gradient inner product. Therefore, in this section, we investigate into such inner product to understand why it can be large when aligning language models. Our investigation focuses on the representative margin-based objective DPO. To build our theoretical intuition, we use synthetic toy settings to analyze the gradient inner product and the changes in log-probabilities. Our theory offers explanations from two perspectives: (1) when the gradient condition holds and which factors do not contribute to enlarging the gradient inner product (Theorem 1, Corollary 2) and (2) when the gradient condition is violated and which factors do cause the gradient inner product to grow, leading to decrease in the 10In their original paper, Azar et al. (2024) proposed the IPO loss without average log-probability. The authors later claimed using average log-probability with IPO yields improved performance. 8 Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement chosen log-probability (Theorem 3). All proofs are provided in Appendix and we empirically verify our theoretical insights in Section 4.3. 4.1 Positive result: when the gradient condition holds We first provide positive result on when the gradient inner product is small, thus Condition 1 holds and DPO exhibits the ideal behavior that pushes up the log-probability of the chosen response and pushes down the log-probability of the rejected one. In the first synthetic setting, we analyze DPO for optimizing an LM with learnable last linear layer in single-token prediction task. Model Setup 1 (LM with learnable last linear layer). Let = be the vocabulary size. We assume for prompt and response y, at any index [L], the LM outputs: πθ(yi x, y<i) = s(h θ)[yi], where = y, θ RdV is the learnable parameter, hi Rd is the hidden state for the i-th token in response and : RV 11 denotes the softmax function. The hidden states are assumed as frozen during DPO. Data Setup 1. Both chosen and rejected responses contain only one token under the prompt x. That is, yw, yl 1, and yw[1] = yl[1]12. Theorem 1. Under Model Setup 1 and Data Setup 1, assume after the SFT stage, given prompt x, the model prediction on the first token in response is uniformly concentrated on tokens in the vocabulary V, then we have log πw, log πl = 1 h2, log πw2 = log πl2 = 1 h2, with being the hidden state of the last token in prompt x. Thus, both parts of Condition 1 hold, resulting in log πw increases and log πl decreases. Theorem 1 shows that for single-token prediction, log πw, log πl < 0. This suggests that the gradient descent steps of DPO ensures log πw increases and log πl decreases. This result can be easily extended to the data setup where the chosen and rejected responses have multiple tokens but only differ at the last one, i.e., yw[1 : 1] = yl[1 : 1], yw[L] = yl[L] with being the length of yw and yl. In this case, up to the L-th token where chosen and rejected differ, the hidden states are the same for the two responses. This is true because for yw[1 : 1] = yl[1 : 1], we have that hi = hi,w = hi,l for [L]. Corollary 2. Under Model Setup 1, the chosen and rejected responses only differ at their last token, assume after SFT the model prediction on the L-th token in response is uniformly concentrated on tokens in the vocabulary, we have log πw, log πl log πw2 = log πl2, and thus log πw increases and log πl decreases. 4.2 Negative result: when the gradient condition is violated From the previous results, we can see that the gradient inner product condition is not violated and DPO has the ideal behavior when the chosen and rejected responses differ only at the last token. To gain theoretical insights on what causes the violation of the condition, we level up our previous data setup to the following. Data Setup 2. Chosen and rejected responses have an edit distance 1 and the difference appears in the middle of response, i.e., the chosen and rejected responses yw and yl satisfy yw[1 : 1] = yl[1 : 1], yw[m] = yl[m], yw[m + 1 : L] = yl[m + 1 : L] for 1 < L. To analyze the optimization steps of DPO under this data setup, we adopt simpler setting for parameterizing the LM, where the LM has learnable logits. Model Setup 2 (LM with learnable logits). We first consider the setting where the LM output follows the structure: For index [L], πθ(x, y<i ) = sw,i, πθ(x, y<i ) = sl,i, 11Here, denote the probability simplex. 12For vector y, we use y[i] to denote its i-th entry and use y[i1 : i2] to denote its entry from i1 to i2. 9 Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement where sw,i, sl,i are the probability distributions of the chosen and rejected response at token i, respectively. The vectors sw,i and sl,i are configured as variables to optimize in the model and to which we take the derivative of chosen and rejected log probability. Because yw[1 : 1] = yl[1 : 1], we have that si = sw,i = sl,i for [m]. Since sw,i and sl,i are predicted by shared model, they are not independent and one may impose assumptions to characterize the relationship between them. We denote for [m + 1 : L], to be the vocabulary index of token appearing at yw[i] and yl[i]. As in Pal et al. (2024), we assume that sw,i[j . Under this assumption, Theorem 3 shows that in this case the log-probability of the chosen and rejected will likely both decrease after one DPO gradient descent step. Theorem 3. Under Model Setup 2 and Data Setup 2, after one DPO step, the per-token log-probability change in chosen response yw can be characterized with first-order Taylor expansion: for [1 : 1], the per-token chosen log-probability before the differing token stays unchanged: ] and sw,i[j] sl,i[j] for = ] sl,i[j (12) For = m, the chosen log-probability at the differing position will increase: suppose and are the indices of yw[m] and yl[m] in the vocabulary V, ) 0. log π(yi x, y<i For [m + 1 : L], the chosen log-probability at these positions will decrease: log π(ym x, y<m ) 1 + (sw,m[j] sw,m[k]) 0. (13) log π(yi x, y<i ) (1 sw,i[j ])(sl,i[j ] sw,i[j ]) sw,i[j](sl,i[j] sw,i[j]) 0, (14) (cid:88) j=j ] sw,i[j since sl,i[j ] 0 and sl,i[j] sw,i[j] 0. Given the change in sentence-wise log-probability of chosen is the summation of the per-token changes specified in (12), (13) and (14), as the same suffix following the differing tokens gets longer, log πw decreases more. Remark. While Theorem 3 adopts the same assumptions made in Pal et al. (2024), we precisely characterize the per-token log-probability changes based on the first-order approximation, and explicitly break down the sentence-wise probability change for chosen into 3 parts: before/at/after the differing position. Therefore, the analysis in Theorem 3 captures the varying probability change directions at different positions, uncovering the underlying dynamic behind the overall decreased chosen probability observed in experiments (Figure 3). It is worth mentioning that Theorem 3 explicitly presents the size of probability changes. The same conclusion on the change direction can also be derived with per-token gradient inner product condition similar to Condition 1, see Appendix B.2. The increase of chosen presented in (13) follows the same intuition in Theorem 1 that if two contrastive tokens are picked by chosen and rejected responses under similar context, then the chosen token probability will increase while the rejected decreases. An intuitive explanation of what causes the decrease of both the chosen and rejected in (14) could be that the chosen and rejected gradients are highly correlated as they pick the same token under similar context. Mathematically, the assumption we adopted implies that the gradient inner product between chosen and rejected can be lower bounded. Combining our insights gained in Section 4.1 and 4.2, we find that the gradient inner product increases as the chosen and rejected responses share more similar tokens. Additionally, the sentence-wise gradient inner product and their change in log probability may not necessarily reflect the individual token-wise gradient inner product and their probability changes.13 Below we verify our theoretical findings empirically. 4.3 Empirical observations We empirically verify our theoretical intuition regarding when the gradient condition may be held or violated, by aligning GPT-2 small to curated sentiment preference dataset. The preference dataset is curated from mteb/tweet_sentiment_extraction: for data point (x, yw, yl), prompt is statement, e.g., 1 week to my Birthday! The chosen response yw reflects the true sentiment label of x. We filter statements in the original dataset and only retain those with binary sentiments: positive\" or negative\", and set the rejected response yl to reflect the flipped wrong sentiment label of x. We curate four datasets with the following styles of responses: 13To be specific, by token-wise gradient, we mean θ log πθ(yix, y<i). 10 Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement single token (Data Setup 1): yw: Positive. yl: Negative. short suffix: yw: Positive sentiment. yl: Negative sentiment. long suffix: yw: Positive sentiment based on my judgement. yl: Negative sentiment based on my judgment. prefix+suffix (Data Setup 2): yw: It has positive sentiment based on my judgement. yl: It has negative sentiment based on my judgment. Our theoretical results suggest: (1) in the single token case, the chosen and rejected gradients will have negative inner product and thus DPO will allow the chosen log-probability to increase while the rejected to decrease (Theorem 1). (2) For the short suffix and long suffix cases, we expect DPO to reduce the chosen log probability more for the latter, as responses in long suffix contain more tokens following the differing spot, leading to more chosen tokens with decreasing log probability (Theorem 3). Additionally, (3) for the differing token (positive or negative), the token-wise gradient inner product would be negative, while for other identical tokens, the token-wise gradient inner product would be positive. Figure 3: Training dynamics of the chosen and rejected log probabilities for sentiment tasks. The three implications obtained from our theorems are validated by empirical observation. First, the chosen log probability increases only in the single token case, and the short suffix chosen log probability decreases less than that of the long suffix, aligning with our theoretical results (Figure 3). Second, the gradient cosine similarity in the single token case quickly declines and stays negative during training, while that in the short suffix and long suffix is positive and increases as the suffix length (i.e., the number of identical tokens after the difference) grows (Figure 4a). This aligns with our gradient condition (Condition 1), where the drop in chosen log probability depends on the magnitude of the gradient inner product. Finally, we inspect the token-wise gradient inner product for the prefix+suffix case. From the heat map of token-wise gradient similarities (Figure 4b), we observe that on the diagonal, the inner product between the gradients on the tokens positive and negative is below 0, whereas for other identical tokens in the two responses, the gradient cosine similarities are significantly higher and close to 1 for some tokens. Our theoretical and empirical investigation into the token-level gradient inner product suggests broader implications for general alignment tasks. Significant tokens (e.g., positive/negative) contrasting the chosen and rejected responses the most, exhibit negative gradient correlation and prevent gradient entanglement. Meanwhile, those non-contrastive insignificant tokens (e.g., identical tokens) cause gradient entanglement due to the high similarity in their gradients. This insight highlights the importance of token-level gradient dynamics and their contribution to the entanglement effect, motivating fine-grained alignment method that contrasts only the significant tokens in the chosen/rejected response pair. This approach retains the simplicity of margin-based methods while potentially reducing the gradient entanglement effect in margin-based losses. Further details on the potential algorithm design are discussed in Section 5.2."
        },
        {
            "title": "5 Empirical Implications: Algorithmic Design and More",
            "content": "Using our insights from the gradient inner product conditions (Section 3) and our investigation on when such conditions may be violated (Section 4), we present two potential ways to mitigate gradient entanglement, thus allowing the chosen and rejected probability to change in different directions simultaneously. 5.1 Design 1: pairwise normalized gradient descent As discussed in Section 3, to specify an increasing log-probability of the chosen response and decreasing logprobability of the rejected response, we can set dw/dl = log πl/ log πw so that (9) and (10) will hold 11 Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement (a) Cosine similarity between θ log πw and θ log πl. (b) Token-wise gradient cosine similarity. Figure 4: Gradient correlation behaviors on the sentence-level and token-level for sentiment tasks. Fig. 4a gives the cosine similarity between θ log πw and θ log πl for DPO on single token, short suffix and long suffix datasets, θ log πw,θ log πl θ log πwθ log πl . Fig. 4b shows the token-wise gradient similarity for an instance in the prefix+suffix defined as: task. simultaneously. This leads to the following gradient update rule: θℓ := (cid:18) θ log πw θ log πw θ log πl θ log πl (cid:19) , where is quantity relying on the specific preference optimization loss design. This update rule turns out to be the normalized gradient for the chosen and rejected responses respectively. For example, we can modify the gradient update for the DPO loss as: θℓDPO (θ) := βσ (ˆrθ (x, yl) ˆrθ (x, yw)) (cid:20) θ log πθ (yw x) θ log πθ (yw x) θ log πθ (yl x) θ log πθ (yl x) (cid:21) , and adjust the learning rate accordingly. 5.2 Design 2: sparsity regularized token masking An alternative approach to reduce gradient entanglement is by designing fine-grained margin-based loss that only contrasts significant tokens, as suggested in Section 4.3. For example, the following loss design could be potential good candidate for adapting the original DPO objective in this direction: ℓ (θ, uw, ul) = log σ (cid:32) (cid:88) i=1 I{ui r} log πθ(yi πref(yi wx, y<i ) wx, y<i ) I{ui r} log (cid:33) πθ(yi πref(yi x, y<i ) x, y<i ) + η (I{uw r}1, +I{ul r}1) , ), (x, y<i where η R+, are hyper-parameters and uw RL, ul RL are learnable weights depending on (x, y<i ) respectively, interpreted as the confidence in considering token significant. In practice, we can approximate the indicator I{ui r} with the sigmoid function σ(k (ui r)) for large > 0. The loss is inspired by sparsity-related ideas (e.g., LASSO (Tibshirani, 1996)), where the learnable masks I{ui r} ideally pick out the significant tokens in each response that enlarge the margin. The ℓ1 regularizer on the token-wise mask imposes sparsity on it. Other variants of preference optimization objectives may also adopt similar sparsity-related adaptations to leverage token-wise information in obtaining the margin. 5.3 Further Discussion In this paper, we touch upon common pitfall of margin-based preference optimization methods in language alignment: it under-specifies the ideal behavior of the LM on the chosen and rejected responses individually. Due to the gradient Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement entanglement effect, our gradient inner product condition suggests that when the chosen and rejected gradients are highly correlated, their log probabilities will exhibit synchronized increases/decreases. Beyond explaining differences in existing variants of margin-based methods and proposing new algorithmic designs to address gradient entanglement, our framework of gradient entanglement offers fresh perspective to understand existing avenues of RLHF methods: 1. The first group of methods implicitly adjust the criterion on the maximum size of the gradient inner product under which the synchronized changes do not occur, without invasively modifying the gradient inner product, as seen in the works listed in Table 2. Our proposal in Section 5.1 falls under this category. 2. The second group of methods modify the inner product of interest directly. As discussed in Section 4, while the sentence-level gradient inner product may be large, the token-level inner product can be small. line of research, such as advantage-based methods (Mudgal et al., 2023; Setlur et al., 2024; Yoon et al., 2024), exploiting token-level contrasts to improve RLHF falls under the second category, and so does our proposal in Section 5.2. 3. For the RLHF procedure that involves reward modeling and policy optimization as separate stages. While objectives for reward model learning also suffer from under-specification due to gradient entanglement, there is key difference: LM is not directly updated based on preference samples. Instead, we use on-policy samples from the LM to perform policy optimization, where responses with positive rewards are not necessarily the ones we want to increase or maintain the LMs probability on. precise characterization of how the under-specification manifests in this procedure is under future investigation. Finally, at high level, our work also highlights the need to reconsider the current margin-based preference optimization paradigm in language model alignment. While this approach enjoys high simplicity and enables language models to learn contrasts between good and bad responses, it may not be well-suited for scenarios where the focus the behavior of LM on either rejected or chosen samplessuch as in safety-critical alignment tasks or when distilling from strong model."
        },
        {
            "title": "References",
            "content": "Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics, pages 44474455. PMLR, 2024. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335, 2024. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. In International Conference on Machine Learning, 2024. Jiwoo Hong, Noah Lee, and James Thorne. Orpo: Monolithic preference optimization without reference model. arXiv preprint arXiv:2403.07691, 2(4):5, 2024. Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with reference-free reward. arXiv preprint arXiv:2405.14734, 2024. Sidharth Mudgal, Jong Lee, Harish Ganapathy, Yaguang Li, Tao Wang, Yanping Huang, Zhifeng Chen, Heng-Tze Cheng, Michael Collins, Trevor Strohman, Jilin Chen, Alex Beutel, and Ahmad Beirami. Controlled decoding from language models. arXiv [cs.LG], October 2023. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. 13 Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White. Smaug: Fixing failure modes of preference optimisation with dpo-positive. arXiv preprint arXiv:2402.13228, 2024. Ryan Park, Rafael Rafailov, Stefano Ermon, and Chelsea Finn. Disentangling length from quality in direct preference optimization. Findings of the Association for Computational Linguistics, 2024. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. Amrith Setlur, Saurabh Garg, Xinyang Geng, Naman Garg, Virginia Smith, and Aviral Kumar. RL on incorrect synthetic data scales the efficiency of LLM math reasoning by eight-fold. arXiv [cs.LG], June 2024. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:30083021, 2020. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Alpaca: strong, replicable instruction-following model. Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html, 3(6):7, 2023. Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society Series B: Statistical Methodology, 58(1):267288, 1996. Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, et al. Zephyr: Direct distillation of lm alignment. Conference on Language Modeling, 2024. Yue Wu, Zhiqing Sun, Huizhuo Yuan, Kaixuan Ji, Yiming Yang, and Quanquan Gu. Self-play preference optimization for language model alignment. arXiv preprint arXiv:2405.00675, 2024. Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, and Young Jin Kim. Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation. In International Conference on Machine Learning, 2024. Eunseop Yoon, Hee Suk Yoon, SooHwan Eom, Gunsoo Han, Daniel Wontae Nam, Daejin Jo, Kyoung-Woon On, Mark Hasegawa-Johnson, Sungwoong Kim, and Chang Yoo. Tlcr: Token-level continuous reward for fine-grained reinforcement learning from human feedback. arXiv preprint arXiv:2407.16574, 2024. Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf: Rank responses to align language models with human feedback. Advances in Neural Information Processing Systems, 36, 2024. Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter Liu. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023. 14 Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement Derivations for gradient entanglement and conditions in Section 3 A.1 Derivation for gradient entanglement DPO. After one step of gradient descent with step size η > 0 for decreasing the loss ℓDPO, the change in the log-probability of the chosen response denoted by log πw, as well as the change in the log-probability of the rejected response denoted by log πl, can be approximated by the first-order Taylor expansion: log πw θ log πw, ηθℓDPO = ηβc(θ) (cid:0) log πw2 log πw, log πl(cid:1) log πl θ log πl, ηθℓDPO = ηβc(θ) (cid:0) log πw, log πl log πl2(cid:1) . General Losses. First, the gradient of (6) can be written as θℓ = dwθ log πw dlθ log πl, where dw and dl are scalars such that dw := m(hw(log πw) hl(log πl))h dl := m(hw(log πw) hl(log πl))h w(log πw) + Λ(log πw), l(log πl). After one step of gradient descend with step size η > 0 for decreasing the loss ℓ, the changes in log-probabilities can be approximated by the first-order Taylor expansion: log πw θ log πw, ηθℓ = η (cid:0)dwθ log πw2 dlθ log πw, θ log πl(cid:1) , log πl θ log πl, ηθℓ = η (cid:0)dwθ log πw, θ log πl dlθ log πl2(cid:1) . A.2 Derivation for SPPO Denote = θ log π(w) and = θ log π(l). For DPO, we see that the direction of winner and loser is decided by a, and b, b. Similarly, for any pairwise loss ℓ(log π(w) log π(l)), the above statement still holds. Now we take look at non-pairwise loss ℓSPPO = (log π(w) β1)2 + (log π(l) + β1)2. We have dθ dt = θℓSPPO = (log π(w) β1)θ log π(w) (log π(l) + β1)θ log π(l). Then dt (cid:28) log π(i) = θ log π(i), (cid:29) dθ dt = (log π(w) β1)(cid:10)θ log π(i), θ log π(w)(cid:11) (log π(l) + β1)(cid:10)θ log π(i), θ log π(l)(cid:11). We have dt log π(w) (log π(w) β1)a, (log π(l) + β1)a, which means if we want log π(w) to increase, we need a, a, < β1 log π(w) β1 + log π(l) =: α. Note that the inequality above implicitly assume that β1 + log π(l) > 0. This is true in practice as we set β1 to be extremely large. Similarly, if we want log π(l) to decrease, we need a, b, < β1 + log π(l) β1 log π(w) =: α1. We have α > 1. It seems SPPO can make sure that log π(w) goes up more easily but also make log π(l) goes up more easily, compared to DPO. 15 Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement Proofs for the Gradient Inner product in Section 4 B.1 LM with learnable last linear layer: Single Token Case We prove Theorem 1 below. WLOG, assume Tw = Tl = L, log πw, log πl =(cid:10)θ log π(yL x, y<L ), θ log π(yL x, y<L )(cid:11) θ RdV , hL Rd is the hidden state for predicting the L-th token, s() is the softmax function. θ log π(yL θ log π(yL x, y<L x, y<L ) = θ ) = θ (cid:0)log s(h (cid:0)log s(h w](cid:1) θ)[yL ](cid:1) θ)[yL Compute the gradient with chain rule, θ log πL θ log πL = [s(1)hL, , (1 s(iw))hL, , s(il)hL, , s(V )hL] = [s(1)hL, , s(iw)hL, , (1 s(il))hL, , s(V )hL], (15) (16) (17) (18) and yL iw, il are the index of token yL for the i-th token in vocabulary. Suppose at the initialization of θ, s(1) = = s(iw) = = s(il) = s(v) = 1 for entries and the rest entries have s(j) = 0. We note that the exact indices of which s(j) = 1/M does not matter as it would be the same index for both the chosen and rejected gradients. in vocabulary, respectively. For any index i, s(iw) denote LLMs output logit log πL = [ log πL = [ 1 1 hL, . . . , hL , , , (cid:19) (cid:18) (cid:124) 1 1 (cid:123)(cid:122) iwth 1 hL (cid:124) (cid:123)(cid:122) (cid:125) iwth (cid:125) (cid:18) (cid:124) 1 hL] 1 hL] 1 hL (cid:124) (cid:123)(cid:122) (cid:125) ilth (cid:19) 1 (cid:123)(cid:122) ilth 1 (cid:125) 1 hL, , , 1 hL , (cid:10) log πL w, log πL (cid:11) = 2 2 hL2 2 hL2 = 1 and log πL hL2. is large: (19) (20) (21) (cid:10) log πL w, log πL (cid:11) is negative. While in comparison, the norm of log πL log πL w2 = log πL 2 = 1 2 hL2 + (cid:19)2 (cid:18) 1 1 hL2 = 1 hL2. Therefore, based on Condition 1: log πw, log πl = 1 hL2, log πw2 = log πl2 = 1 hL2, log πwincreases and log πl decreases. B.2 LM with learnable logits setting We prove Theorem 3 below. We will set up some new notations first. First, we work with the case where Tw = Tl = is sentence length, is the vocab size, yw[1 : m1] = yl[1 : m1], yw[m] = yl[m], and yw[m+1 : L] = yl[m+1 : L]. Note that for all [L], the token y[i] [V ] is an index, θw and θl are learnable logits in LM. Each row of the following matrix is πθ(x, y<i) [V ] where is the row index. (Here, there is slight abuse of notation: is the probability simplex.) : RV is the softmax function. 16 Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement [0, 1]LV πθ(x, yw) = s(θw) = s(θw[1, :]) ... s(θw[m, :]) s(θw[m + 1, :]) ... s(θw[L, :]) , πθ(x, yl) = s(θl) = s(θl[1, :]) ... s(θl[m, :]) s(θl[m + 1, :]) ... s(θl[L, :]) = s(θw[1, :]) ... s(θw[m, :]) s(θl[m + 1, :]) ... s(θl[L, :]) Each row s(θ[i, :]) . The first rows are the same for θw and θl because the tokens up to row are the same between yw and yl. The index at row corresponding to the selected token will be denoted as , generic vocab index is j. Note that, i,l for = m, and i,l for = m. i,w = i,w = = Next, the corresponding gradient matrices log s(θw), log s(θl) can be specified by: 0 ... θw[i,:] log s(θw[i, ]) ... , θ log s(θl) = 0 ... θl[i,:] log s(θl[i, ]) ... 0 . RLV θ log s(θw[i, i+1]) = where θ[i,:] log s(θ[i, i ]) RV , and for [V ], θ[i,:] log s(θ[i, ])[j] = (cid:40) s[i, j] 1 s[i, j] if = if = i where s[i, j] = s(θ[i, :])[j], log s(θ[i, gradient of log s(θ[i, ]). ]) is -th entry of log s(θ[i, :]), and log s(θ[i, ])[j] is the j-th entry of the The sentence-wise gradient is RLV θL = log s(θw[1, 1 ]) log s(θw[1, 1 ]) ... log s(θw[m, log s(θw[m + 1, m,w]) log s(θw[m, m,l]) m+1]) log s(θl[m + 1, m+1]) log s(θw[L, L]) log s(θl[L, L]) 0 ... log s(θw[m, log s(θw[m + 1, m,w]) log s(θw[m, m,l]) m+1]) log s(θ[m + 1, m+1]) ... ... log s(θw[L, L]) log s(θl[L, L]) Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement Now, lets first derive the token-wise condition for the selected token (learning rate η = 1): Chosen response: if = m, we have log s(θw[i, i,w]) (cid:88) log s(θw[m, m,w]), L[i, :] = log s(θw[m, m,w]), L[m, :] i=1 = log s(θw[m, m,w]), log s(θw[m, m,w]) log s(θw[m, m,l]) = (cid:88) sw[m, j] + (1 sw[m, m,w])2 j=j m,w (cid:88) sw[m, j]2 + sw[m, m,w](1 sw[m, m,w]) + sw[m, m,l](1 sw[m, m,l]) j=j =1 + (sw[m, m,w,j=j m,l] sw[m, m,l m,w]) 0, (22) where the last inequality is true because [0, 1]. Here, basically, this margin loss will just encourage increase the chosen logP (and reduce the rejected one) for the selected token. Chosen response: if = m, we have log s(θw[i, i,w]) (cid:88) log s(θw[i, ]), L[i, :] = log s(θw[i, i ]), L[i, :] i=1 ]), log s(θw[i, ]) log s(θl[i, ]) = log s(θw[i, =(1 sw[i, ])(sl[i, i ] sw[i, ]) sw[i, j](sl[i, j] sw[i, j]) (23) (cid:88) j=j Here, basically, the loss can only pick one direction to change both chosen and rejected entry. Connection to the derivation in Pal et al. (2024). The assumption in Pal et al. (2024) mainly ensures the sign of (23). Basically, smaugs assumption ensures that for [m + 1, L], sw[i, ] and sw[i, j] sl[i, j] for = . log s(θw[i, ]) log s(θl[i, ]) = ] sl[i, sl[i, 1] sw[i, 1] ... sl[i, ] sw[i, ] ... sl[i, ] sw[i, ] 0 ... 0 ... 0 = For (23), we have (1 sw[i, ])(sl[i, ] sw[i, i ]) sw[i, j](sl[i, j] sw[i, j]) 0. (cid:88) j=j This ensures the chosen token will have reduced logP. Condition on chosen tokens increasing and rejected token decreasing at m, and on chosen and rejected tokens decreasing after + 1: (22) 0 always holds, [m + 1, L], sw[i, ] sl[i, ], = , sw[i, j] sl[i, j] = (23)"
        },
        {
            "title": "C Experiment details",
            "content": "C.1 Hardware and Software Setup Our experiments were implemented using TRL version 0.11.0. The training was performed on hardware setup consisting of two NVIDIA H100 GPUs, providing substantial computational power for the training process. 18 Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement C.2 TL;DR Task Setup For the TL;DR summarization task, we utilized the CarperAI/openai_summarize_comparisons dataset. We employed two LLMs for this task: mistralai/Mistral-7B-Instruct-v0.3 (referred to as Mistral 7B) meta-llama/Meta-Llama-3-8B-Instruct (referred to as Llama-3 8B) We did not perform any supervised fine-tuning step prior to the RLHF training for these models. To optimize the training process, we applied Low-Rank Adaptation (LoRA) with rank of 64 to both models. The learning rate was set at 5 106 for all RLHF training. C.3 RLHF Algorithm Configurations We implemented several RLHF algorithms, each with its own specific configurations: Direct Preference Optimization (DPO): β = 0.1 Chosen NLL term (used in CPO, RRHF, and SLiC-HF): λ = 1 SLiC-HF: δ = 1 SimPO: γ = 0.5 R-DPO: α = 0.2 DPOP: λ = C.4 Sentiment Analysis Task Setup For the sentiment analysis task, we used specially curated sentiment dataset. Unlike the TL;DR task, we performed supervised fine-tuning on the GPT-2 model before proceeding with the RLHF training. The learning rate for this RLHF training was also set to 5 106. C.5 Additional empirical results Figure 5: Training dynamics of the chosen and rejected log probabilities on the TL;DR dataset for different preference optimization algorithms trained on Llama-3 8B. All algorithms exhibit synchronized increases and decreases in the chosen and rejected log probabilities. Note: For SimPO and IPO, the log probabilities are normalized, while in the other plots, they are the original log probabilities. 19 Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement Figure 6: Cosine similarity between θ log πw and θ log πl on the TL;DR dataset for different preference optimization algorithms trained on Llama-3 8B and Mistral 7B."
        }
    ],
    "affiliations": [
        "Oregon State University",
        "Princeton University",
        "The University of Texas at Austin"
    ]
}