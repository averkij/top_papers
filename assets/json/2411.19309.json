{
    "paper_title": "GRAPE: Generalizing Robot Policy via Preference Alignment",
    "authors": [
        "Zijian Zhang",
        "Kaiyuan Zheng",
        "Zhaorun Chen",
        "Joel Jang",
        "Yi Li",
        "Chaoqi Wang",
        "Mingyu Ding",
        "Dieter Fox",
        "Huaxiu Yao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite the recent advancements of vision-language-action (VLA) models on a variety of robotics tasks, they suffer from critical issues such as poor generalizability to unseen tasks, due to their reliance on behavior cloning exclusively from successful rollouts. Furthermore, they are typically fine-tuned to replicate demonstrations collected by experts under different settings, thus introducing distribution bias and limiting their adaptability to diverse manipulation objectives, such as efficiency, safety, and task completion. To bridge this gap, we introduce GRAPE: Generalizing Robot Policy via Preference Alignment. Specifically, GRAPE aligns VLAs on a trajectory level and implicitly models reward from both successful and failure trials to boost generalizability to diverse tasks. Moreover, GRAPE breaks down complex manipulation tasks to independent stages and automatically guides preference modeling through customized spatiotemporal constraints with keypoints proposed by a large vision-language model. Notably, these constraints are flexible and can be customized to align the model with varying objectives, such as safety, efficiency, or task success. We evaluate GRAPE across a diverse array of tasks in both real-world and simulated environments. Experimental results demonstrate that GRAPE enhances the performance of state-of-the-art VLA models, increasing success rates on in-domain and unseen manipulation tasks by 51.79% and 60.36%, respectively. Additionally, GRAPE can be aligned with various objectives, such as safety and efficiency, reducing collision rates by 44.31% and rollout step-length by 11.15%, respectively. All code, models, and data are available at https://grape-vla.github.io/"
        },
        {
            "title": "Start",
            "content": "GRAPE: Generalizing Robot Policy via Preference Alignment Zijian Zhang1*, Kaiyuan Zheng2, Zhaorun Chen3, Joel Jang2, Yi Li2, Chaoqi Wang3 Mingyu Ding1, Dieter Fox2, Huaxiu Yao1 1UNC Chapel-Hill 2University of Washington 3University of Chicago 4 2 0 2 8 2 ] . [ 1 9 0 3 9 1 . 1 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Despite the recent advancements of vision-language-action (VLA) models on variety of robotics tasks, they suffer from critical issues such as poor generalizability to unseen tasks, due to their reliance on behavior cloning exclusively from successful rollouts. Furthermore, they are typically fine-tuned to replicate demonstrations collected by experts under different settings, thus introducing distribution bias and limiting their adaptability to diverse manipulation objectives, such as efficiency, safety, and task completion. To bridge this gap, we introduce GRAPE: Generalizing Robot Policy via Preference Alignment. Specifically, GRAPE aligns VLAs on trajectory level and implicitly models reward from both successful and failure trials to boost generalizability to diverse tasks. Moreover, GRAPE breaks down complex manipulation tasks to independent stages and automatically guides preference modeling through customized spatiotemporal constraints with keypoints proposed by large vision-language model. Notably, these constraints are flexible and can be customized to align the model with varying objectives, such as safety, efficiency, or task success. We evaluate GRAPE across diverse array of tasks in both real-world and simulated environments. Experimental results demonstrate that GRAPE enhances the performance of state-of-the-art VLA models, increasing success rates on in-domain and unseen manipulation tasks by 51.79% and 60.36%, respectively. Additionally, GRAPE can be aligned with various objectives, such as safety and efficiency, reducing collision rates by 44.31% and rollout step-length by 11.15%, respectively. All code, models, and data are available at https://grape-vla.github.io/. 1. Introduction The recent rapid proliferation of vision-language-action (VLA) models has streamlined general robotic manipulation tasks, demonstrating impressive capability across *Equal Contribution. Correspondence to Huaxiu Yao at <huaxiu@cs.unc.edu> Figure 1. Comparison of GRAPE with SOTA VLA models finetuned on the same data across large variety of generalization and in-domain tasks in both real-world and simulated environments. range of tasks under controlled environmental variations [4, 7, 26, 44]. However, these models face several critical challenges such as poor generalizability across new environments, objects, tasks, and semantic contexts [26]. significant factor contributing to this limitation is their reliance on supervised fine-tuning (SFT), where VLAs simply imitate actions from successful rollouts via behavior cloning while not developing holistic understanding of the task goal or potential failure patterns [27]. While reinforcement learning (RL) algorithms such as PPO [42] have proved promising in enhancing their generalizability [52], the high cost of gathering sufficient online trajectories and explicitly defining reward make them impractical for training VLA [44]. Furthermore, training VLAs to solely replicate expert behaviors often results in behavior collapse [28] where the planned trajectories are often suboptimal [26]. This is because the SFT datasets are usually uncurated and consist of offline demonstrations collected from experts that embed implicitly different values (e.g. task completion, safety, and cost-efficiency) that are not clearly defined within the data [36, 45]. Simply imitating these behaviors via SFT can potentially confuse the model and result in suboptiFigure 2. Overview of GRAPE. Given complex manipulation task (top), GRAPE first adopts vision-language model to decompose the task into several temporal stages, then identifies spatial keypoints essential for each stages subtask completion. Then given user-specified alignment goals, GRAPE prompts powerful vision-language model to obtain series of cost functions for each stage, where lower cost implies higher alignment compliance. During iterative preference optimization (bottom), we sample multiple offline trajectories from the base VLA model and obtain trajectories with associated multi-stage costs. This score further incorporates the models self-evaluation of each trajectory and binary task success indicator. Then we rank the sampled trajectories with their corresponding scores to obtain list of preferences. Then we perform trajectory-wise preference optimization to obtain improved model, from which we further sample online trajectories and iterate until convergence. mal trajectories that deviate from the actual objective of the demonstrations. Some approaches attempt to address this challenge by explicitly defining set of objectives and solving them hierarchically [24]. However, this approach incurs additional inference overhead and lacks scalability [30]. To address these issues, we propose GRAPE: Generalizing Robot Policy via Preference Alignment to alleviate the high cost of training VLAs with RL objective, while offering flexibility for aligning towards customized manipulation objectives. As shown in Figure 2, GRAPE introduces trajectory-wise preference optimization (TPO) to align VLA policies on trajectory level by implicitly modeling reward from both successful and failure trials, boosting generalizability to diverse tasks. To further alleviate the difficulty in ranking trajectories and providing preferences towards arbitrary alignment objectives, GRAPE proposes to decompose the complex manipulation tasks into multiple independent stages and adopt large vision model to propose keypoints for each stage, each associated with spatial-temporal constraint. Notably, these constraints are flexible and can be customized to align the model with varying manipulation objectives, such as task completion, robotinteraction safety, and cost-efficiency. We evaluate GRAPE across wide range of real-world tasks and two simulated environments. Experimental results show that GRAPE outperforms state-of-the-art VLA models, improving success rates on both in-domain and unseen manipulation tasks by 51.79% and 60.36%, respectively. Moreover, GRAPE can be aligned to diverse objectives such as safety and efficiency, to further reduce collision rate by 44.31% and rollout step-length by 11.15%, respectively. 2. Generalizing Robot Policy via Preference"
        },
        {
            "title": "Alignment",
            "content": "2.1. Preliminaries During inference, VLA typically initializes with an task instruction q, and at each timestep t, it takes an environment observation ot (usually an image) and outputs an action at, where we can denote πθ(ai(oi, q)) as the action policy of VLA parameterized by θ. To complete the task, VLA iteratively interacts with the environment and obtains trajectory ζ = {o1, a1, , oT , aT q} of length . Typically, VLAs are fine-tuned to imitate expert behaviors via SFT: LSFT = (cid:88) (cid:88) (ζ,q)D t=1 log p(atot, q; πθ), (1) where = {(ζ1, q1), . . . , (ζN , qN )} denotes the training set containing expert trajectories. Specifically, LSFT enforces VLA to memorize the action associated with each observation sampled from distribution PD, resulting in poor generalizability to new task settings. It is worth to note that while we follow Brohan et al. [7], ONeill et al. [36] and consider the step-wise policy based on the Markov decision process (MDP) assumption [43], our approach can be easily adapted to both non-MDP case which takes past interaction histories (usually video or series of images) as state [9] and diffusion policy [16] which generates multiple future steps all at once [44]. 2.2. TPO: Trajectory-wise Preference Optimization To improve generalization, we follow Bai et al. [3], Schulman et al. [42] and further fine-tune VLA policies via RL objective. Let rϕ denote reward function parameterized by ϕ, we have max πθ Eζπθ [rϕ(ζ)] βDKL [πθ(ζ) πref(ζ)] , (2) where β controls the deviation from the base reference policy πref trained via SFT in Eq. (1) and π(ζ, q) is the likelihood of policy π generating the entire trajectory ζ under instruction q. Then we follow Rafailov et al. [40] and derive the analytical reparameterization of the trajectory reward r(ζ) as: r(ζ, q) = β log πθ(ζ q) πref(ζ q) + β log Z(ζ). (3) Similar to Rafailov et al. [40], we adopt the Bradley-Terry (BT) [5] model and model rϕ from set of trajectories ranked with preferences. Specifically, let ζw and ζl denotes the chosen and rejected trajectory starting from the same initial state, we can formulate the trajectory-wise reward modeling objective as: (ζw ζl) = exp (r(ζw), q) exp (r(ζw), q) + exp (r(ζl), q) . (4) Then, we follow Rafailov et al. [40] and substitute Eq. (3) into Eq. (4) and obtain the following trajectory-wise preference optimization (TPO) loss LTPO equivalent to Eq. (2): E(ζw ,ζl)D (cid:20) (cid:18) (cid:18) log σ β log πθ(ζw) πref(ζw) log (cid:19)(cid:19)(cid:21) , πθ(ζl) πref(ζl) (5) where we can further draw from MDP and decompose the likelihood of trajectory ζ into individual state-action pairs, i.e., π(ζ, q) = (cid:81)T i=1 π(ai (oi, q)) and further obtain log πθ(ζ, q) πref(ζ, q) = (cid:88) t=1 log πθ(ai (oi, q)) πref(ai (oi, q)) . (6) Then we can substitute Eq. (6) into Eq. (5) to obtain the TPO loss LTPO in terms of step-wise state-action pairs. Our TPO loss Eq. (6) is beneficial as it: (1) aligns policy πθ globally towards human preferences on trajectory level while simply using step-wise rollouts collected by VLAs; (2) it stabilizes the policy and steers it towards the final goal by backpropagating the gradients throughout all the stateaction pairs along the trajectory; (3) it significantly boosts generalizability by learning from both successful and failed trajectories via RL objective. Although Finn et al. [21] indicates that expanding the size of the sampled trajectory can reduce the bias in reward modeling, it also increases the training costs. Thus while our method can be easily scaled up, we keep our discussion to the binary case where only one chosen/rejected trajectory is present. 2.3. Guided-Cost Preference Generation While given the TPO objective Eq. (5) we can align the policy towards arbitrary objectives defined through trajectories ranked by the corresponding preference, it incurs high costs as it requires human expertise and lengthy manual annotation. Thus to better scale up the preference synthesis towards arbitrary alignment objectives (e.g. task completion, safety, efficiency), we propose Guided-Cost Preference Generation (GCPG) to automatically curate such preferences that integrate different alignment objectives. 2.3.1 Multi-Stage Temporal Keypoint Constraints Building on insights from Huang et al. [24], we address the complexity of specifying precise trajectory preferences for complex manipulation tasks by decomposing trajectories into temporal stages and assigning costs to quantify performance at each stage. Then, we aggregate these stagespecific costs to obtain holistic evaluation for each trajectory. Specifically, we adopt VLM-based stage decomposer MD (detailed in Appendix A), to partition trajectory ζ into sequence of consecutive stages, formulated as {ζ 1, . . . , ζ S} = MD(ζ, q), ζ = {(oi t, ai t)}Ti t=1, (7) where ζ represents the ith stage of trajectory ζ. After obtaining the stage decomposition, we further employ vision-language model (e.g. DINOv2 [37]) to identify keypoints that serve as reference metrics across each stage. Then we prompt powerful LLM [1] to propose multiple cost functions (see examples in Appendix E.2.) for each stage that correspond with the alignment objective, where lower cost indicates better objective compliance. Then the cost Si at stage Si is formulated as CSi = (cid:80)NSi j=1 βj max(0, Si ({κSi } τ Si ), {1, . . . , NSi }, (8) where Si ({κSi}) denotes the jth cost calculated using subset of keypoints {κSi} in stage Si. is tunable threshold parameter that adjusts the tolerance of each τ Si cost [14], where the cost will only be applied when it exceeds the threshold. βj is weight parameter that adjusts the importance of each cost. Then to aggregate the costs for the entire trajectory, instead of summing each stage linearly, we apply an exponential decay to capture the casual dependencies of each temporal stage (e.g. if trajectory incurs high costs in preceding stages it is not expected to perform well subsequently), defined as the external reward: Rext(ζ) = (cid:89) i=1 (cid:80)NSi j=1 βj max(0,CSi ({κSi }τ Si ), (9) where Eq. (9) aggregates the individual costs and subobjectives from each stage to tackle the curse of dimensionality and effectively adhere to the customized alignment. 2.3.2 Guided-Cost Preference Generation To further improve the stability and optimality of the preference synthesis, we draw inspirations from selfrewarding [54] and determine that more optimal trajectory should be confirmed by both the external judge (as in Eq. (9)) and the model itself. Thus we incorporate two additional rewards and obtain the GCPG reward: RGCPG(ζ) = λ1Rself(ζ) + λ2Rext(ζ) + λ3Isuccess(ζ) (10) where Rself(ζ) is the self-evaluated score provided by π, which equals the log-likelihood of generating trajectory ζ: Rself(ζ) = log(π(ζ, q)) = log( (cid:89) i=1 π(ai (oi, q))) (11) and Isuccess(ζ) is binary indicator function that indicates whether the trajectory ζ successfully completes the task: Isuccess(ζ) = (cid:40) if ζ is successful, 1, 0, otherwise. (12) where λ are the weight parameters that adjust the importance of each reward. Intuitively, Eq. (11) can be seen as dense approximation of the sparse signal provided by Eq. (12), which are further calibrated by Eq. (9) to obtain holistic evaluation of the trajectory that accounts for both its optimality and degree of alignment to customized objective specified through the external reward in Eq. (9). For example, during the kth iteration, we (1) first sample numerous trajectories for variety of tasks and obtain Dk; (2) then we calculate the costs for each trajectory using Eq. (10) and rank these trajectories accordingly per task; (3) we pair the top-m and bottom-m trajectories with each other for each task, and obtain m2 chosen-rejected trajectory pairs; (4) then we fine-tune the same sampling policy with TPO via Eq. (5) and obtain an updated policy. We iterate this process for times and obtain the final model aligned with the target objective. We detail the GRAPE iterative preference optimization procedure in Algorithm 1. Algorithm 1 GRAPE Iterative Preference Optimization Require: Base VLA policy πθ, collection of task instructions = {qi}, stage decomposer MD, max iterations K, reward weights {λ1, λ2, λ3}, stage-wise keypoints {κSi} cost functions {C Si } and thresholds {τ Si } Ensure: policy π aligned towards customized objective 1: for = 1, . . . , do Iterative fine-tuning with TPO Sample trajectories Dk = {ζi}M i=1 using πθ with 2: for trajectory ζ Dk do Preference Generation 3: Decompose ζ into multiple stages Eq. (7) 4: Compute the cost for each stage CSi Eq. (8) 5: Calculate external reward Rext(ζ) Eq. (9) 6: Compute policy self-reward Rself(ζ) Eq. (11) Eq. (12) Examine task success Isuccess(ζ) Aggregate GCPG reward RGCPG(ζ) Eq. (10) end for Rank Dk by their RGCPG(ζ) rewards Pair {ζw, ζl} from top-m and bottom-m trajectories Eq. (5) Update πθ using TPO loss 7: 8: 9: 10: 11: 12: 13: 14: end for 3. Experiment In this section, we evaluate GRAPEs performance in both real and simulated environments, addressing four key questions: (1) Does GRAPE improve the VLA models performance relative to SFT-based baseline models? (2) How effective are guided-cost preference selection and iterative preference optimization in enhancing the models performance? (3) What is the individual contribution of each reward component to overall model performance? (4) Can GRAPE support flexible alignment with different alignment objectives? 2.4. Iterative Preference Optimization 3.1. Experimental Setups After generating the preference, we then discuss our iterative preference optimization strategy. Inspired by the practices of on-policy RL [42] which often yield more optimal policy than off-policy training, we iteratively fine-tune the SFT VLA model via TPO with trajectories collected online. Implementation Details. We employ OpenVLA [26] as the backbone model, using LoRA fine-tuning with the AdamW optimizer for both supervised and preference fine-tuning. In the supervised fine-tuning stage, we use learning rate of 4105 with batch size of 16. For preference fine-tuning, Figure 3. Comparison of GRAPE with OpenVLA and Octo finetuned on the same data on the Simpler-Env environment. We report the in-domain performance, which includes four tasks and three generalization evaluations (subject, physical, and semantic), where each incorporates multiple tasks. we apply learning rate of 2 105 with the same batch size. Further details on the training process and datasets are available in Appendix and B. Baseline Models. We compare GRAPE with two leading robot learning models known for their strong performance in robot control tasks. The first model, Octo [44], is large transformer-based policy model. The second, OpenVLA [26], is 7B Vision-Language-Action model. Both models were supervised fine-tuned using the same dataset sampled from corresponding environments. We denote the supervised fine-tuned models as Octo-SFT and OpenVLASFT, respectively. 3.2. Evaluation in Simulation Environment Evaluation Setup. We evaluate GRAPEs performance in two robot simulation environments: Simpler-Env [29] and LIBERO [33]. In Simpler-Env, we evaluate the models in-domain performance as well as its generalization across three aspects: subject (generalize to unseen objects), physical (generalize to unseen object sizes/shapes), and semantic (generalize to unseen instructions) generalization. In LIBERO, we test our model on four tasks: LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, and LIBERO-Long. All tasks are in-domain tasks. Additional details about the experimental setup are provided in Appendix C.2. Results. We use the success rate across all tasks in SimpleEnv and LIBERO as our primary evaluation metric, while we also record the grasping rate in Simpler-Env. The results of Simple-Env and LIBERO are reported in Figure 3 and Figure 4, respectively. According to the results, we can observe that GRAPE outperforms Octo-SFT and OpenVLA-SFT in Simpler-Env by an average of 24.48% and 13.57%, respectively. GRAPE outperforms Octo-SFT and OpenVLA-SFT in LIBERO by an average of 4.1% and 2.7%, respectively. Additional results can be found in Appendix D. While GRAPE significantly boosts in-domain performance, it equivalently enhances the generalizability of VLA policies on OOD tasks, given the task completion Figure 4. Comparison of GRAPE with OpenVLA and Octo finetuned on the same data on the LIBERO environment. We report the performance on four LIBERO tasks, including LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, LIBERO-Long. alignment on trajectory level. 3.3. Evaluation in Real-World Robot Environment Evaluation Setup. We conducted 280 real-world experiments across 28 tasks to evaluate the generalization capabilities of GRAPE. The evaluation focus on in-distribution evaluation and five out-of-distribution generalization types: visual, subject, action, semantic, and language grounding generalizations. Here, visual generalization assesses the ability to adapt to new visual environments; subject generalization evaluates the recognition and handling of unfamiliar objects; action generalization measures performance across diverse actions; semantic generalization evaluates responses to prompts with similar meanings; and language grounding generalization gauges comprehension of spatial directions. Detailed experimental setup are provided in Appendix C.1 and illustrated in Figure 5. Results. In the real-world experiment, GRAPE significantly outperforms other models across variety of tasks. Notably, in in-domain tasks, GRAPE achieves success rate of 67.5%, which is 22.5% improvement over OpenVLA-SFTs 45% and substantially higher than OctoSFTs 20%. Additionally, in visual generalization tasks, GRAPE demonstrates higher adaptability with success rate of 56%. In the more challenging action generalization tasks, although OpenVLA-SFT shows modest performance, GRAPE still outperforms OpenVLA-SFT, indicating its potential in understanding various actions and executing commands based on language. Considering tasks across all categories, GRAPEs total average success rate is 52.3%, marking 19% improvement over OpenVLA-SFTs 33.3% and significantly ahead of Octo-SFTs 5.8%. This performance highlights GRAPEs effectiveness and high adaptability in handling complex and variable task environments. 3.4. Ablation Study of Reward Model In this section, we conduct an ablation study to analyze the contribution of each reward component in Eq. (10) to Figure 5. Comparison of GRAPE with OpenVLA and Octo fine-tuned on the same data on the real-world environment. We report the in-domain performance, which includes four tasks and five generalization evaluations (visual, subject, action, semantic, and language grounding), where each incorporates multiple tasks. We also report the average performance across all tasks. Figure 6. Performance of GRAPE during iterative preference optimization via TPO. We demonstrate the average success rate for each iteration across in-domain tasks and three types of generation tasks (subject, physical, semantics). the final performance: the external objective-aligned reward Rext(ζ), the self-evaluated reward Rself(ζ), and the success indicator Isuccess(ζ). Additionally, we perform separate ablation study to emphasize the importance of utilizing the entire reward score for preference selection. This approach is compared against method that randomly selects one successful trajectory as the preferred trajectory and one failed trajectory as the rejected trajectory. The results in the Simpler-Env environment are reported in Table 1. The results indicate that: (1) incorporating the full reward score Eq. (10) for preference ranking significantly enhances performance compared to random selection based on success alone; (2) all reward components contribute to improving model performance. These findings align with our expectations. Specifically, Rself(ζ) enhances the robustness of the GRAPE model by encouraging it to select trajectories with higher generation probabilities. In parallel, Rext(ζ) guides the model toward learning specific behaviors, such as safety and efficiency. Finally, Isuccess(ζ) serves as critical indicator, steering the model to prioritize successful trajectories. 3.5. Analysis of Iterative Preference Optimization In this section, we analyze the iterative preference optimization performance. We conduct the experiments on the Simpler-Env environment and report the results with respect to the training iterations in Figure 6. Here, SFT means the supervised fine-tuned OpenVLA model before preference optimization. In our experiments, GRAPE achieves 17.5%, 9.0%, 15.0%, 21.0% improvements in in-domain performance, subject generalization, physical generalization and semantic generation, respectively. The findings suggest that GRAPE progressively enhances model performance across iterations, showcasing its ability to enhance the quality of generated preference data and achieve better generalization. Notably, the magnitude of improvement diminishes over time, aligning with our expectations as the model approaches convergence. Table 1. Ablation study of reward score. Here, Random w/ Isuccess refers to randomly selecting one successful trajectory as the chosen trajectory and one failed trajectory as the rejected trajectory, Rself(ζ) is the self-evaluated score provided by the log-likelihood of generating trajectory ζ, Rext(ζ) represents objective-aligned multi-stage reward defined in Eq. (9), Isuccess(ζ) is binary indicator function that indicates whether the trajectory ζ successfully completes the task. In-domain Subject Gen. Physical Gen. Grasp Success Grasp Success Grasp Success Semantics Gen. Grasp Success Average Grasp Success Random w/ Isuccess w/o Rself(ζ) w/o Rext(ζ) w/o Isuccess 62.00% 35.50% 60.33% 33.00% 44.00% 33.50% 54.50% 36.50% 55.21% 34.63% 66.50% 38.00% 62.33% 37.00% 51.25% 36.75% 68.00% 42.50% 62.02% 38.56% 63.50% 37.50% 61.00% 34.33% 48.50% 35.50% 62.50% 40.00% 58.88% 36.83% 58.50% 32.00% 59.67% 34.67% 42.25% 31.75% 58.50% 39.00% 54.73% 34.36% GRAPE 71.00% 43.00% 62.67% 40.67% 63.50% 41.75% 72.00% 47.00% 67.29% 43.11% Table 2. Results with respect to different objectives. GRAPESafety, GRAPE-Efficiency are models trained with Safety and Efficiency objectives, respectively. Here, we use collision rate (CR), step length (SL), success rate (SR) to evaluate the safety, effeciency and generalization capabilities. We conduct experiments in both real-world and simulation scenarios. and step length respectively, meanwhile maintain comparable success rate, compared with OpenVLA-SFT. The results indicate that GRAPE can be easily adapted to account for flexible alignment objectives such as safety, efficiency by adjusting the multi-stage cost functions accordingly, while incurring minimal drop in task success rate. Method Real-World Simulation CR SL SR CR SL SR 53.33 142.32 34.61 66.50 72.68 27.50 OpenVLA-SFT 29.84 146.11 54.31 46.00 74.49 37.00 GRAPE-Safety GRAPE-Efficiency 58.45 125.79 51.67 57.50 64.92 38.50 38.60 131.66 58.46 59.50 70.24 42.50 GRAPE 3.6. Analysis of Different Alignment Objectives After demonstrating the effectiveness of GRAPE in improving the generalization of the VLA model (measured by success rate), we further investigate its potential to align the model with flexible objectives, such as efficiency and safety. Revisiting Eq. (9), we observe that adjusting the threshold parameters can guide the model to prioritize specific objectives by influencing trajectory preference selection. In this study, we focus on two new alignment objectives: safety and efficiency. Safety aims to minimize collisions between the robot and objects, while efficiency seeks to reduce the average number of steps required for the robot to complete task. To achieve these objectives, we set lower threshold for collision costs to emphasize safety and lower threshold for path costs to prioritize efficiency. These modified settings are then applied to the original real-world and simulation evaluations. We train models to align with the safety and efficiency objectives, referring to these models as GRAPE-Safety and GRAPE-Efficiency, respectively (see detailed experimental setup in Appendix C.2). The results are reported in Table 2, where we use collision rates, step lengths, and success rates to evaluate safety, efficiency and generalization capabilities, respectively. According to Table 2, the GRAPE-Safety and GRAPE-Efficient have better performance on collision rate 3.7. Case Study We further demonstrate case study in Figure 7 to analyze GRAPEs adaptability towards different alignment objectives. Specifically, we consider safety-critical pickup task where an obstacle is placed between the object and the target. Specifically, OpenVLA-SFT fails to complete the task without preference alignment. However, we can see that while GRAPE aligned towards task completion (on the second-row of Figure 7) can effectively pick up and place the object, it also collides with the obstacle, due to the policy is aligned to aggressively boost task success without explicitly addressing safety concerns. On the contrary, GRAPE-safety learns to avoid colliding with the obstacle while efficiently completing the task. Both Table 2 and Figure 7 indicates that by simply tweaking the cost function, GRAPE can effectively adapt to different objectives. More cases could be found in Appendix E.1. 4. Related Works 4.1. Vision-language-action Models Previous robot learning works [14, 23, 24, 30, 31, 34, 35] typically take hierarchical planning strategy. For example, Code as Policies [31] and EmbodiedGPT [35] adopt LLMs and VLMs to generate sequence of high-level action plans first and further leverage low-level controller to solve for local trajectories. However, such models suffer from limited low-level skills and are hard to generalize to everyday tasks. VLAs tend to scale up low-level tasks by incorporating large vision-language models as backbones and directly generating actions within the model. They generally achieve action planning via two mainstream approaches: Figure 7. Comparison of GRAPE aligned via safety objective with GRAPE aligned via task-completion objective and OpenVLA-SFT. Specifically, we assess their performance on safety-critical task with the instruction: pick up the white box and place into the black pot. (1) Discretization of action space [6, 7, 26], such as OpenVLA [26], which preserves the autoregressive language decoding objective and uniformly truncate the action space into small set of action tokens. As such discretization unavoidably introduces errors, some methods such as [4] adopt more recent structures [53] and directly integrate diffusion heads for action prediction which avoids discretization. (2) Diffusion models [2, 16, 18, 25, 32, 49] like Diffusion Policy [16] as the action head, which instead of generating one stepwise action each time, produces sequence of future actions through several denoising steps. While these models vary in structure, they are consistently supervised-trained on successful rollouts via behavior cloning, which can hardly be generalized to unseen manipulation tasks. However, our GRAPE first aligns VLA policies on trajectory level via trial and error, effectively boosting generalizability and customizability. 4.2. Reinforcement Learning and Preference Optimization Reinforcement learning (RL) [17, 42, 56] plays pivotal role in the post-training of SOTA foundation models (FMs), including text generation models [1, 12, 15, 19, 39], image generation models [13, 20], video generation [50], which has been extensively leveraged to align the pre-trained FMs to comply with human values embedded through preference data. In the meantime, RL has also shown tremendous success in training policies for robotics tasks [10, 11, 14, 42, 47, 48, 55]. While it is intuitively beneficial to post-align VLA via RL, no prior works have reported such success, mainly due to that (1) manipulation objectives are usually diverse and complex, which makes the reward hard to define analytically [21]; (2) and while such reward can be modeled from human preferences, annotating such preferences in robotics manipulation tasks are usually lengthy and requires human expertise [45]; (3) the imperfect numerical differentiation of rewards usually leads RL algorithms such as PPO [42] to collapse [8]. However, recent line of works [40, 46] has shown success in directly aligning the policy via RL without such explicit reward modeling. Inspired, GRAPE aligns the policy by contrasting trajectories with each other, avoiding the numerical issues in rewarding modeling. Besides, we introduce an automatic preference synthesis pipeline that easily scales with diverse manipulation tasks and can adapt to different alignment objectives. 5. Conclusion and Discussion In this work, we addressed the critical challenges faced by vision-language-action (VLA) models, including limited generalizability and adaptability to diverse manipulation objectives. We proposed GRAPE, which aligns VLA policies on trajectory level. GRAPE enhances generalizability by learning from both successful and failed trials, offering flexibility in aligning with objectives such as safety, efficiency, and task success through customized spatiotemporal constraints. Experimental results demonstrated significant improvements, with GRAPE enhancing success rates on both in-domain and unseen tasks while enabling flexible alignment. While GRAPE demonstrates significant improvements in generalization and alignment flexibility, it has several limitations. First, the reliance on preference ranking requires sufficient diversity of trajectories, potentially restricting the application in scenarios where only limited task settings are available. Second, while GRAPE supports customizable alignment, the manual tuning of threshold parameters for specific objectives may introduce subjective biases and require domain expertise. Future work may explore more efficient and automated methods for preference synthesis and adaptation."
        },
        {
            "title": "Author Contribution",
            "content": "We detail the authors contribution in this section. Conceptualization: Huaxiu Yao; Methodology: Zhaorun Chen, Chaoqi Wang, Mingyu Ding, Huaxiu Yao; Preference Optimization Model Implementation: Zijian Zhang; Simulation Experiment: Zijian Zhang; Real-world Experiment: Kaiyuan Zheng, Joel Jang, Yi Li; Paper Writing and Discussion: All Authors."
        },
        {
            "title": "Acknowledgement",
            "content": "This work is partially supported by Cisco Faculty Research Award."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 3, 8, 12, 14 [2] Anurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, and Pulkit Agrawal. Is conditional generative modeling all you need for decision making? In The Eleventh International Conference on Learning Representations, 2023. 8 [3] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. 3 [4] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: vision-languageaction flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. 1, 8 [5] Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. 3 [6] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. [7] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. 1, 3, 8 [8] Lucian Busoniu, Tim De Bruin, Domagoj Tolic, Jens Kober, and Ivana Palunko. Reinforcement learning for control: Performance, stability, and deep approximators. Annual Reviews in Control, 46:828, 2018. 8 [9] Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, et al. Gr-2: generative video-language-action model with web-scale knowledge for robot manipulation. arXiv preprint arXiv:2410.06158, 2024. 3 [10] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:1508415097, 2021. 8 [11] Yuanpei Chen, Tianhao Wu, Shengjie Wang, Xidong Feng, Jiechuan Jiang, Zongqing Lu, Stephen McAleer, Hao Dong, Song-Chun Zhu, and Yaodong Yang. Towards human-level bimanual dexterous manipulation with reinforcement learning. Advances in Neural Information Processing Systems, 35:51505163, 2022. [12] Zhaorun Chen, Zhen Xiang, Chaowei Xiao, Dawn Song, and Bo Li. Agentpoison: Red-teaming llm agents via poisoning memory or knowledge bases. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. 8 [13] Zhaorun Chen, Yichao Du, Zichen Wen, Yiyang Zhou, Chenhang Cui, Zhenzhen Weng, Haoqin Tu, Chaoqi Wang, Zhengwei Tong, Qinglan Huang, et al. Mj-bench: Is your multimodal reward model really good judge for text-toimage generation? arXiv preprint arXiv:2407.04842, 2024. 8 [14] Zhaorun Chen, Zhuokai Zhao, Tairan He, Binhao Chen, Xuhao Zhao, Liang Gong, and Chengliang Liu. Safe learning via hierarchical adaptive chancereinforcement In IEEE/RSJ International Conferconstraint safeguards. ence on Intelligent Robots and Systems (IROS), 2024. 4, 7, 8 [15] Zhaorun Chen, Zhuokai Zhao, Zhihong Zhu, Ruiqi Zhang, Xiang Li, Bhiksha Raj, and Huaxiu Yao. Autoprm: Automating procedural supervision for multi-step reasoning arXiv preprint via controllable question decomposition. arXiv:2402.11452, 2024. 8 [16] Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research, page 02783649241273668, 2023. 3, 8 [17] Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. [18] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 8 [19] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 8 [20] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Reinforcement learning for fine-tuning text-to-image diffusion models. Advances in Neural Information Processing Systems, 36, 2024. 8 [21] Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control via policy optimization. In International conference on machine learning, pages 4958. PMLR, 2016. 3, 8 [22] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. 12 [23] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. Voxposer: Composable 3d value maps for robotic manipulation with language models. arXiv preprint arXiv:2307.05973, 2023. [24] Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, and Li Fei-Fei. Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. arXiv preprint arXiv:2409.01652, 2024. 2, 3, 7 [25] Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synIn International Conference on Machine Learning, thesis. pages 99029915. PMLR, 2022. 8 [26] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. 1, 4, 5, 8 [27] Aviral Kumar, Joey Hong, Anikait Singh, and Sergey Levine. Should run offline reinforcement learning or behavioral In International Conference on Learning Reprecloning? sentations, 2021. 1 [28] Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. Training language arXiv models to self-correct via reinforcement learning. preprint arXiv:2409.12917, 2024. 1 [29] Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, Sergey Levine, Jiajun Wu, Chelsea Finn, Hao Su, Quan Vuong, and Ted Xiao. Evaluating real-world robot manipulation policies in simulation. arXiv preprint arXiv:2405.05941, 2024. 5, [30] Yi Li, Yuquan Deng, Jesse Zhang, Joel Jang, Marius Memmel, Caelan Reed Garrett, Fabio Ramos, Dieter Fox, Anqi Li, Abhishek Gupta, and Ankit Goyal. HAMSTER: Hierarchical action models for open-world robot manipulation. In 1st Workshop on X-Embodiment Robot Learning, 2024. 2, 7, 12 [31] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 94939500. IEEE, 2023. 7 [32] Zhixuan Liang, Yao Mu, Mingyu Ding, Fei Ni, Masayoshi Tomizuka, and Ping Luo. Adaptdiffuser: Diffusion models as adaptive self-evolving planners. In International Conference on Machine Learning, pages 2072520745. PMLR, 2023. 8 [33] Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking knowlarXiv preprint edge transfer for lifelong robot learning. arXiv:2306.03310, 2023. 5, 14 [34] Yao Mu, Junting Chen, Qing-Long Zhang, Shoufa Chen, Qiaojun Yu, GE Chongjian, Runjian Chen, Zhixuan Liang, Mengkang Hu, Chaofan Tao, et al. Robocodex: Multimodal code generation for robotic behavior synthesis. In Forty-first International Conference on Machine Learning, 2024. 7 [35] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. Embodiedgpt: Vision-language pre-training via embodied chain of thought. Advances in Neural Information Processing Systems, 36, 2024. 7 [36] Abby ONeill, Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, et al. Open x-embodiment: Robotic learning datasets and rt-x models. arXiv preprint arXiv:2310.08864, 2023. 1, 3 [37] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [38] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2024. 12 [39] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. 8 [40] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. 3, 8 [41] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 12 [42] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 1, 3, 4, 8 [43] Richard Sutton. Reinforcement learning: An introduction. Bradford Book, 2018. 3 [44] Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. 8 Hejna, Tobias Kreiman, Charles Xu, et al. An open-source generalist robot policy. arXiv:2405.12213, 2024. 1, 3, 5 Octo: arXiv preprint [45] Homer Rich Walke, Kevin Black, Tony Zhao, Quan Vuong, Chongyi Zheng, Philippe Hansen-Estruch, Andre Wang He, Vivek Myers, Moo Jin Kim, Max Du, et al. Bridgedata v2: dataset for robot learning at scale. In Conference on Robot Learning, pages 17231736. PMLR, 2023. 1, [46] Chaoqi Wang, Zhuokai Zhao, Chen Zhu, Karthik Abinav Sankararaman, Michal Valko, Xuefei Cao, Zhaorun Chen, Madian Khabsa, Yuxin Chen, Hao Ma, et al. Preference optimization with multi-sample comparisons. arXiv preprint arXiv:2410.12138, 2024. 8 [47] Siyue Wang, Zhaorun Chen, Zhuokai Zhao, Chaoli Mao, Yiyang Zhou, Jiayu He, and Albert Sibo Hu. EscIRL: Evolving self-contrastive IRL for trajectory prediction in In 8th Annual Conference on Robot autonomous driving. Learning, 2024. 8 [48] Tianhao Wu, Yunchong Gan, Mingdong Wu, Jingbo Cheng, Yaodong Yang, Yixin Zhu, and Hao Dong. Unidexfpm: Universal dexterous functional pre-grasp manipulation via diffusion policy. arXiv preprint arXiv:2403.12421, 2024. 8 [49] Zhou Xian, Nikolaos Gkanatsios, Theophile Gervet, TsungWei Ke, and Katerina Fragkiadaki. Chaineddiffuser: Unifying trajectory diffusion and keypose prediction for robotic manipulation. In 7th Annual Conference on Robot Learning, 2023. 8 [50] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 8 [51] Seonghyeon Ye, Joel Jang, Byeongguk Jeon, Sejune Joo, Jianwei Yang, Baolin Peng, Ajay Mandlekar, Reuben Tan, Yu-Wei Chao, Bill Yuchen Lin, et al. Latent action pretraining from videos. arXiv preprint arXiv:2410.11758, 2024. 12 [52] Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, et al. Fine-tuning large vision-language models as decision-making agents via reinforcement learning. arXiv preprint arXiv:2405.10292, 2024. 1 [53] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. [54] Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, and Huaxiu Yao. Calibrated self-rewarding vision language models. arXiv preprint arXiv:2405.14622, 2024. 4 [55] Henry Zhu, Abhishek Gupta, Aravind Rajeswaran, Sergey Levine, and Vikash Kumar. Dexterous manipulation with deep reinforcement learning: Efficient, general, and lowcost. In 2019 International Conference on Robotics and Automation (ICRA), pages 36513657. IEEE, 2019. 8 [56] Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and A. Additional Description of GRAPE and Hyperparameter Settings Customized Cost Generation. In our real-world experiments, we first input image-text pairs containing prompts and initial states into the Vision-Language Model (VLM) Hamster [30]. Using the stage information and stage points generated by Hamster, we segmented the collected trajectories. This helps analyze complex task sequences more precisely, giving detailed attention to each stage. And we utilized Grounded-SAM [41] or methods combining SAM [41] and DinoV2 [38] to extract key point information from the images. These key points, combined with our self-collected trajectory data, enable us to refine the execution steps and path planning of tasks based on the stage information generated by the Hamster model. For example, for simple pick-and-place task, we can decompose it into multiple explicit stages: Grasp the grape, Move the grape onto the plate, Place the grape on the plate. To generate detailed operational information and cost functions for each stage, we utilized GPT-4o [1] with customized prompts. This approach makes stage planning more precise and efficient, allowing us to meet specific task requirements and constraints. Furthermore, we enhanced our method by incorporating various task-specific constraints, including: Collision constraints: Ensuring the robot avoids collisions with obstacles. Path constraints: Optimizing the efficiency and safety of the robots movement path. By adopting this strategy, we achieve greater flexibility and specificity in task planning, and better adapting to different task scenarios. Iterative Preference Optimization. For Iterative Preference Optimization, we first utilize the fine-tuned VLA model for online data sampling. For each task, we sample Nt trajectories to facilitate further selection. To simplify the experimental setup, we set Nt = 5 for each task, which has been found to perform effectively in practice. After sampling, each trajectory is automatically labeled using the GCPG reward, as defined in Eq. (10). Based on the distribution of Rself, Rext, and Isuccess observed in preliminary experiments, we set λ1 = 0.01, λ2 = 0.01, and λ3 = 2. These values ensure that Rself, Rext, and Isuccess contribute comparably to the final reward value. Subsequent experiments validate the reasonableness of these parameter choices. Using the GCPG reward assigned to each trajectory, we identify the trajectory with the highest reward as yw and the trajectory with the lowest reward as yl for each task. This selection process enables the construction of the TPO Dataset, Dtraj, for TPO training. For the TPO training process, we employ LoRA [22] and the AdamW optimizer, setting the learning rate to 2 105 and the batch size to 16. The model is trained for single epoch before being utilized for iterative online sampling. During iterative online sampling, the experimental settings remain consistent with the aforementioned descriptions. B. Detail Experiment Datasets In this section, we describe the datasets collected for supervised fine-tuning (referred to as the SFT dataset) and preference alignment (referred to as the TPO dataset). B.1. Real-World Dataset SFT Dataset. In our real-world robot experiments, we use robotic platform composed of Franka robotic arm and Robotiq gripper for data collection. To ensure consistency in data collection and evaluation, all operations are performed in the same experimental environment. During data collection, we gathered dataset of 220 instances of pick and place tasks involving common objects such as bananas, corn, milk, and salt. Additionally, we collected data on 50 instances of tasks involving pressing buttons of different colors. Since the number of objects used for the button-pressing tasks is limited, we introduced background noise and interfering objects during the testing phase to create unseen scenarios. To further enhance the capabilities of OpenVLA in handling different actions, we also collected data on 50 instances of knock down tasks. These diverse task datasets help improve the models generalization ability in processing different types of actions. TPO Dataset. In the real-world experiments, we utilized model fine-tuned on the real-world SFT dataset via OpenVLA for trajectory sampling. Each task was conducted five times. In the TPO dataset, we experimented with 15 different tasks, including 10 pick and place tasks, 3 push button tasks, and 2 knock down tasks, accumulating total of 75 data entries. After selection process, we derived preference dataset consisting of 30 trajectories. B.2. Simulation Datasets SFT Dataset: For Simpler-Env, the SFT dataset comprises 100 trajectories, amounting to approximately 2,900 transitions. These rollouts are generated from Simpler-Env using Octo, following the methodology described in Ye et al. [51]. For LIBERO, it is worth noting that we neither collect new data nor fine-tune the OpenVLA model. Instead, we directly utilize the OpenVLA-SFT model provided by the OpenVLA team, which significantly streamlines the pipeline. TPO Dataset. In the case of Simpler-Env, trajectories are sampled for each task using the OpenVLA-SFT model, with five trials conducted per task. This process yields TPO dataset consisting of 80 trajectories. For LIBERO, OpenVLA-SFT models (one model per task) are employed to sample data across four tasks in LIBERO. For each task, five trajectories are sampled for each sub-task, resulting in TPO dataset comprising total of 20 trajectories. C. Detailed Experiment Settings and Additional Result C.1. Real-World C.1.1 Real-World Experiment Setup In real-world experiment, we used the Franka robot arm, which is known for its precision and flexibility. However, we encountered problem with the original Franka gripper, which was not long enough, limiting our ability to handle some of the tasks, resulting in inefficient completion and high failure rate. To solve this problem, we decided to replace the original Franka grippers with Robotiq grippers, which are not only longer, but also provide more grip and flexibility, which greatly improves the efficiency and success rate of the tasks. The purpose of this experiment was to assess the cross-task generalization capabilities of OpenVLA under the GRAPE framework and to compare its performance with several baseline models. Considering the generally poor zero-shot generalization performance of most VLA models, we performed supervised fine-tuning using the comprehensive rollout dataset Dr collected from real scenes to construct fine-tuned model. The selection of baseline models included those adjusted with domain-specific data, as well as the Octo model, RVT-2 model, and OpenVLA-SFT model. C.1.2 Real-World Tasks As shown in Figure 5, we performed comprehensive evaluation on real machine for several tasks. These tasks cover five different generalization scenarios: Visual Generalization, Subject Generalization, Action Generalization, Semantics Generalization, and Language Grounding. Specifically, for each generalization scenario, we set the following tasks: Visual Generalization includes 8 tasks, e.g., pick up the GRAPE and put it in the black bowl, with noise objects and noisy backgrounds. Some tasks have only noisy backgrounds. Subject Generalization includes 4 tasks, e.g., pick up the and put it in the black bowl. Action Generalization includes 5 tasks, e.g., knock down the green bottle. Semantics Generalization includes 4 tasks, e.g., stack carrot and put it on the blue plates. Language Grounding includes 3 tasks, e.g., pick up left object to left plate. We conducted experiments on 28 total different tasks, attempting each task ten times, totaling 280 executions. To ensure fairness in the evaluation, we maintained the same starting position in each model test. Additionally, we matched the image resolution when training all models and used exactly the same initial object positions in all evaluations. We set specific success criteria for each task. For example, in the pick-and-place task, successful grasp is defined as successfully grasping the target object. In the push-button and knock-down tasks, successful grasp is defined as correctly approaching and manipulating the target object. Overall task success is defined as the object being accurately placed at the target location, successfully knocked down, or the target button being successfully pressed. Due to the strictness of these criteria, some models found it difficult to achieve success in specific tasks. Table 3. Comparison of GRAPE models in diffierent iteration rounds. We assess their performance in in-domain tasks and three kinds of generalization evaluations. Each tasks performance is evaluated on the overall grasp rate and success rate. In-domain Subject Gen. Physical Gen. Grasp Success Grasp Success Grasp Success Grasp Semantics Gen. Success Average Grasp Success Iter-1 Iter-2 Iter71% 74% 74.5% 45.5% 64.67% 40.67% 66% 62.67% 40.67% 67.29% 43.11% 64.33% 40.33% 65.75% 44.25% 76% 49.5% 70.02% 44.77% 70.29% 44.92% 44.5% 76% 63.5% 41.75% 72% 43% 45% 47% 49% C.2. Simulation Experiments C.2.1 Simpler-Env We utilize Simpler-Env [29] as the experimental environment in our study. SIMPLER [29] (Simulated Manipulation Policy Evaluation for Real Robot Setups) is collection of simulated environments created to assess robot manipulation policies in way that closely reflects real-world scenarios. By leveraging simulated environments, SIMPLER effectively serves as practical alternative to real-world testing, which is often costly, time-consuming, and challenging to replicate. Simpler-Env Tasks. In our paper, we use four in-domain tasks from WidowX robot in Simpler-Env. We also design three kinds of generalization tasks in Simpler-Env. These tasks are described below: In-Domain Tasks Shown in Fig. 3: 1. Put Carrot on Plate: The robot is positioned in front of platform with plate and carrot. The robots goal is to grasp the carrot and put it onto the plate. 2. Put Eggplant in basket: The robot is positioned in front of sink with basket and Eggplant. The robots goal is to grasp the Eggplant and put it in the basket. 3. Stack Green Cube on Yellow Cube: The robot is positioned in front of platform with green cube and yellow cube. The robots goal is to grasp the green cube and stack it on the yellow cube. 4. Put Spoon on towel: The robot is positioned in front of platform with spoon and towel. The robots goal is to grasp the spoon and put it on the towel. Three Kinds of Generalization Tasks Shown in Fig. 3: 1. Subject Generalization: The robot is positioned in front of platform, similar to the environment in in-domain tasks. But the robots goal is to grasp some new objects(i.e. pepsi can, coke can, sprite can) and put it onto the plate. 2. Physical Generalization: The robot is positioned in front of platform, similar to the environment in in-domain tasks. But the robots goal is to grasp some original objects with different sizes and collision boxes, then put it onto the plate. 3. Semantics Generalization: The robot is positioned in front of platform, similar to the environment in in-domain tasks. And the instruction is similar to in-domain tasks, too. But the instruction has been modified by GPT-4o [1] while maintaining its original meaning. C.2.2 LIBERO We further utilize LIBERO [33] as the experimental environment in our study. LIBERO (LIfelong learning BEnchmark on RObot manipulation tasks) includes set of 130 language-conditioned robot manipulation tasks inspired by human activities, organized into four distinct suites. Each suite is crafted to examine distribution shifts in object types, spatial arrangements of objects, task goals, or combination of these factors. LIBERO is built to be scalable, extendable, and specifically tailored for advancing research in lifelong learning for robotic manipulation. LIBERO tasks In our paper, we use four in-domain tasks from LIBERO, which are shown in Fig. 4. These tasks is described below: 1. LIBERO-Spatial includes the same set of objects arranged in various layouts, testing the models ability to understand spatial relationships. 2. LIBERO-Object features consistent scene layouts with varying objects, evaluating the models ability to understand different object types. 3. LIBERO-Goal includes of the same objects and layouts but different task goals, testing the models knowledge of different task-oriented behaviors. 4. LIBERO-10 consists of long-horizon tasks with diverse objects, layouts, and tasks. Eash task mentioned above has 10 sub-tasks, with similar task instructions and scenes. D. Additional Real-World and Simulation Results We provide additional results in Table 4 , Table 5, and Figure 13 with detailed task description. Each table has in-domain tasks and several kinds of generalization evaluations. These experiments are conducted across Octo-SFT, OpenVLA-SFT and GRAPE. Table 4. We present the performance of various action policy on real-world robotic manipulation tasks categorized by different types of generalization. The tasks include in-domain, visual generalization with and without noise, subject generalization, action generalization, semantics generalization, and language grounding. Each tasks performance is evaluated based on the number of successful grasps and the overall success rate, comparing results from Octo-SFT, OpenVLA-SFT, and GRAPE. Average success rates are calculated for each generalization category to demonstrate the effectiveness of the tested models under different conditions. Generalization Task Octo-SFT OpenVLA-SFT GRAPE Grasp Success Grasp Success Grasp Success pick up the corn and put it in the black bowl pick up the banana and put it in the black bowl pick up the milk and put it in the white bowl pick up the salt bottle and put it in the white bowl Average 3 2 4 4 32.5% pick up the corn and put it in the black bowl pick up the banana and put it in the black bowl pick up the milk and put it in the white bowl pick up the salt bottle and put it in the white bowl pick up the GRAPE and put it in the black bowl Average pick up the GRAPE and put it in the black bowl pick up the milk and put it in the white bowl pick up the salt bottle and put it in the white bowl Average pick up the chips and put it in the red bowl pick up the and put it in the black bowl pick up the box juice and put it in the yellow plate pick up the Fanta can and put it in the white bowl Average push down the blue button push down the green button push yellow the button knock down the green bottle knock down the popcorn Average take green pepper and place it in the black bowl move icecream and put it in the red bowl stack carrot and put it on the blue plates Lift GRAPE and place it in the black bowl Average pick up left object to left plate push down right button pick up right object to right plate Average 2 0 4 2 0 16% 1 2 0 10% 4 2 2 2 25% 1 1 2 3 0 14% 0 0 0 0 0% 0 0 0 0% 3 0 2 3 20% 1 0 0 2 0 6% 0 1 0 3.3% 0 0 0 2 5% 0 0 2 1 0 6% 0 0 0 0 0% 0 0 0 0% 2 6 10 4 55% 6 3 4 6 6 50% 4 7 2 43.3% 2 4 8 4 45% 4 6 6 2 4 44% 10 6 8 2 65% 4 6 4 46.7% 2 6 8 2 45% 3 2 4 5 5 38% 2 5 2 30% 2 4 3 1 25% 4 4 3 2 2 30% 6 4 8 0 45% 0 2 4 20% 8 9 9 6 7 7 9 80% 67.5% 6 4 9 8 8 70% 6 5 8 6 1 7 8 6 56% 4 4 8 63.3% 53.3% 6 7 8 6 5 6 6 4 67.5% 52.5% 6 4 8 4 4 52% 10 4 6 2 55% 5 8 6 4 5 2 3 40% 8 4 6 2 50% 2 7 5 63.3% 46.7% 13.9% 5.8% 49.8% 33.3% 64.3% 52.3% In-domain Visual Generalization (w/o noise background) Visual Generalization (w/o noise background and object) Subject Generalization) Action Generalization Semantics Generalization Language Grounding Total Average E. Case Study E.1. Case Study of Real-World Generation Tasks We provide an illustration for each specific task included in the suite evaluation for in-domain tasks in Figure 8 and for each type of generation task, including subject generalization in Figure 9, language grounding in Figure 10, visual generalization in Figure 11, action generalization in Figure 12, and semantic generalization in Figure 13. Specifically, we demonstrate the initial and final states of GRAPE in handling each of these challenging tasks, as detailed in the corresponding captions. Table 5. We comparesthe performance of Octo-SFT, OpenVLA-SFT, and GRAPE across various robotic tasks within in-domain, subject, physical, and semantics generalization categories. It shows grasp percentages and success rates for each task, illustrating how each VLA performs under different generalizations. Generalization Task Octo-SFT Grasp Success OpenVLA-SFT Success Grasp GRAPE Grasp Success In-domain Subject Generalization (unseen objects) Physical Generalization (unseen object sizes/shapes) Semantics Generalization (unseen instructions) put the carrot on the plate put the eggplant in the basket stack the green cube on the yellow cube put the spoon on the towel Average put the coke can on the towel put the pepsi can on the towel put the sprite can on the towel Average put the carrot on the plate(size:0.5) put the carrot on the plate(size:1.1) put the carrot on the plate(wider collision box) put the carrot on the plate(longer collision box) put the spoon on the towel(size:0.5) put the spoon on the towel(size:1.1) put the spoon on the towel(wider collision box) put the spoon on the towel(longer collision box) Average put the vegetable on the plate move the eggplant into the basket put the green cube onto the yellow cube place the spoon onto the towel Average 32% 70% 52% 54% 52% 24% 28% 24% 25.33% 38% 26% 28% 32% 62% 52% 48% 56% 42.75% 16% 18% 32% 42% 27% 16% 44% 0% 36% 24% 14% 16% 12% 14% 22% 12% 16% 14% 38% 32% 30% 36% 25% 36% 58% 56% 52% 51% 60% 58% 62% 60% 30% 32% 20% 28% 28% 68% 84% 76% 56% 71% 48% 48% 40% 34% 43% 38% 38% 22% 78% 64% 46% 32.67% 62.67% 40.67% 32% 50% 40% 56% 32% 34% 38% 66% 50% 44% 54% 38% 24% 26% 30% 40% 28% 24% 26% 46.75% 29.5% 6% 8% 6% 26% 11.5% 32% 50% 62% 48% 48% 28% 30% 26% 28% 28% 78% 64% 62% 66% 72% 56% 50% 60% 63.5% 41.75% 64% 42% 42% 48% 38% 30% 32% 38% 66% 78% 88% 56% 72% 48% 44% 60% 36% 47% Total average 36.77% 18.63% 51.44% 29.54% 67.29% 43.11% Figure 8. Illustrations of real-world tasks that we evaluated for in-domain capabilities, where we report the detailed results in Table 4. Specifically, we demonstrate the initial and final state of GRAPE in handling each of the four challenging tasks detailed in the captions. Figure 9. Specifically, we demonstrate the initial and final state of GRAPE in handling each of the four challenging tasks detailed in the captions. Illustrations of real-world tasks that we evaluated for subject generation, where we report the detailed results in Table 4. Figure 10. Illustrations of real-world tasks that we evaluated for language generation, where we report the detailed results in Table 4. Specifically, we demonstrate the initial and final state of GRAPE in handling each of the five challenging tasks detailed in the captions. Illustrations of real-world tasks that we evaluated for visual generation, where we report the detailed results in Table 4. Figure 11. Specifically, we demonstrate the initial and final state of GRAPE in handling each of the eight challenging tasks detailed in the captions. Figure 12. Specifically, we demonstrate the initial and final state of GRAPE in handling each of the five challenging tasks detailed in the captions. Illustrations of real-world tasks that we evaluated for action generation, where we report the detailed results in Table 4. Figure 13. Illustrations of real-world tasks that we evaluated for semantic generation, where we report the detailed results in Table 4. Specifically, we demonstrate the initial and final state of GRAPE in handling each of the four challenging tasks detailed in the captions. E.2. Case Study of Multi-stage Cost Functions We demonstrate some case studies of the multi-stage cost functions generated using our proposed pipeline given different alignment objectives. E.2.1 Task Completion Cost Functions for Task Completion Alignment # The task involves picking up the grape and placing it in the black bowl. # The stages involved are: # 1. Grasp grape # 2. Move grape to black bowl # 3. Drop grape in black bowl num_stages = 3 ### stage 1: Grasp grape def stage1_target_constraint1(end_effector, keypoints): \"\"\"Align the end-effector with the grapes center.\"\"\" grape_center = keypoints[0] target_cost = np.linalg.norm(end_effector - grape_center) return target_cost ### stage 2: Move grape to black bowl def stage2_target_constraint1(end_effector, keypoints): \"\"\"Calculate the relative distance between grape and black bowl.\"\"\" black_bowl_center = keypoints[1]# Assuming keypoint 1 is the black bowl target_cost = np.linalg.norm(end_effector - black_bowl_center) return target_cost ### stage 3: Drop grape in black bowl def stage3_target_constraint1(end_effector, keypoints): \"\"\"Ensure the grape rests in the black bowl.\"\"\" black_bowl_center = keypoints[1] target_cost = np.linalg.norm(end_effector - black_bowl_center) return target_cost E.2.2 Safety Cost Functions for Cost-Efficiency Alignment # The task involves picking up the grape and placing it in the black bowl. # The stages involved are: # 1. Grasp grape # 2. Move grape to black bowl # 3. Drop grape in black bowl num_stages = 3 ### stage 1: Grasp grape def stage1_collision_constraint1(end_effector, keypoints): \"\"\"Approach the grape from above to avoid collision.\"\"\" grape_center = keypoints[0] collision_cost = 0 if end_effector[1] > grape_center[1] else 1 return collision_cost ### stage 2: Move grape to black bowl def stage2_collision_constraint1(end_effector, keypoints): \"\"\"Ensure the grape is aligned above the black bowl.\"\"\" obstacles = keypoints[2:]#Assuming keypoints[2:] are obstacles threshold = 0.1 # Minimum allowable clearance collision_cost = sum( max(0, threshold - np.linalg.norm(end_effector - obstacle)) for obstacle in obstacles ) return collision_cost ### stage 3: Drop grape in black bowl def stage3_collision_constraint1(end_effector, keypoints): \"\"\"Approach the grape from above to avoid collision.\"\"\" black_bowl_center = keypoints[1] collision_cost = 0 if end_effector[1] > black_bowl_center[1] else 1 return collision_cost E.2.3 Cost-Efficiency Cost Functions for Safety Alignment # The task involves picking up the grape and placing it in the black bowl. # The stages involved are: # 1. Grasp grape # 2. Move grape to black bowl # 3. Drop grape in black bowl num_stages = 3 ### stage 1: Grasp grape def stage1_path_constraint1(end_effector, keypoints): \"\"\"Align the end-effector with the grapes center.\"\"\" grape_center = keypoints[0] distance = np.linalg.norm(end_effector - grape_center) step_size = 0.01 # Assuming small step size path_cost = int(distance / step_size) return path_cost ### stage 2: Move grape to black bowl def stage2_path_constraint1(end_effector, keypoints): \"\"\"Calculate the relative distance between grape and black bowl.\"\"\" black_bowl_center = keypoints[1]# Assuming keypoint 1 is the black bowl distance = np.linalg.norm(end_effector - black_bowl_center) step_size = 0.01 # Assuming small step size path_cost = int(distance / step_size) return path_cost ### stage 3: Drop grape in black bowl def stage3_path_constraint1(end_effector, keypoints): \"\"\"Ensure the grape rests in the black bowl.\"\"\" black_bowl_center = keypoints[1] distance = np.linalg.norm(end_effector - black_bowl_center) step_size = 0.01 # Assuming small step size path_cost = int(distance / step_size) return path_cost Prompt Template for Multi-stage Cost Proposal USER: Instructions The image shows robot stage point in workspace, each point in the diagram represents the point of the stage split: Stage point 0 : Represents the initial position of the carrot. Stage point 1 : Represents the intermediate position above the carrot for grasping. Determine how many stages are involved in the task. Grasping must be an independent stage. Some examples: 1. Task: Put the carrot on the plate Stages: Grasp carrot Move carrot to plate Drop carrot on plate Stage 1: Grasp carrot Path constraints: Align the end-effector with the carrots center. Collision constraints: The end-effector must approach the carrot from above to avoid collision. Stage 2: Move carrot to plate Path constraints: Calculate the relative distance between carrot and plate. Collision constraints: The carrot is aligned above the plate. Stage 3: Drop carrot on plate Path constraints: The carrot must rest on the plate. The carrot should not bounce out of the basket. Collision constraints: The end-effector must approach the carrot from above to avoid collision. Note: Sum all Path constraints cost the path_cost variable. Sum all Grasp constraints cost the grasp_cost variable. Sum all Collision constraints cost the collision_cost variable. Each constraint function takes an end-effector point and set of keypoints as input, returning numerical cost. The constraint is satisfied if this cost is zero or less. Define any number of path constraints per stage, but avoid using if statements in the functions. Avoid using path constraints when manipulating deformable objects (e.g., towels). Input format: end_effector: np.array of shape (3,) representing the end-effector position. keypoints: np.array of shape (K, 3) representing the keypoints positions. Use Python and NumPy functions freely in constraint functions. Use pairs of keypoints to create vectors if needed. Keypoints are indexed starting from 0, matching their order in the keypoints array. Structure your output in single Python code block as follows: # ... num_stages = ? ### stage 1 path constraints (if any) def stage1_path_constraint1(end_effector, keypoints): \"\"\"Put your explanation here.\"\"\" ... return path_cost # Add more constraints if needed ... ### stage 1 collision constraints (if any) def stage1_collision_constraint1(end_effector, keypoints): \"\"\"Put your explanation here.\"\"\" ... return collision_cost # Add more constraints if needed ... # Repeat for more stages ... Query Query Task: {instruction} Query Image:"
        }
    ],
    "affiliations": [
        "UNC Chapel-Hill",
        "University of Chicago",
        "University of Washington"
    ]
}