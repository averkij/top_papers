{
    "paper_title": "Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation",
    "authors": [
        "Shaocong Xu",
        "Songlin Wei",
        "Qizhe Wei",
        "Zheng Geng",
        "Hong Li",
        "Licheng Shen",
        "Qianpu Sun",
        "Shu Han",
        "Bin Ma",
        "Bohan Li",
        "Chongjie Ye",
        "Yuhang Zheng",
        "Nan Wang",
        "Saining Zhang",
        "Hao Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT's depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: \"Diffusion knows transparency.\" Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation."
        },
        {
            "title": "Start",
            "content": "Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation Shaocong Xu1, Songlin Wei2, Qizhe Wei1, Zheng Geng1, Hong Li1,4, Licheng Shen3, Qianpu Sun3, Shu Han5 Bin Ma3, Bohan Li6,7, Chongjie Ye8, Yuhang Zheng9, Nan Wang1, Saining Zhang1, and Hao Zhao1,3 5 2 0 2 9 2 ] . [ 1 5 0 7 3 2 . 2 1 5 2 : r Abstract Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, synthetic video corpus of transparent/reflective scenes: 11k sequences (1.32M frames) rendered with Blender/Cycles. Scenes are assembled from curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from large video diffusion model, we learn video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and cotrain on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines (e.g., Depth-Anythingv2, DepthCrafter), and normal variant (DKT-Normal) sets the best video normal estimation results on ClearPose. compact 1.3B version runs at 0.17 s/frame (832480). Integrated into grasping stack, DKTs depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support broader claim: Diffusion knows transparency. Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation. Code and models are available at https://daniellli. github.io/projects/DKT/. I. INTRODUCTION Accurate depth estimation of transparent and reflective objects is fundamental to advancing 3D reconstruction [1] the intrinsic and robotic manipulation [2]. Nevertheless, physical ambiguities of these objects impose substantial limitations on depth-sensing cameras that rely on time-offlight measurements or stereo correspondence [3], [4]. In particular, transparent objects often produce missing regions 1Beijing Academy of Artificial Intelligence, scxu@baai.ac.cn 2University of Southern California. 3Tsinghua University, zhaohao@air.tsinghua.edu.cn. 4Beihang University. 5Wuhan University. 6Shanghai Jiao Tong University. 7European Institute of Innovation and Technology Ningbo. 8FNii, The Chinese University of Hong Kong, Shenzhen. 9National University of Singapore. In-the-Wild Qualitative Results. The first and third rows present Fig. 1. frames extracted from the input videos, while the second and fourth rows display the predictions. Our method achieves robust depth estimation for transparent objects in arbitrary-length, in-the-wild videos. For the full video, please refer to the Appendix video. in depth maps, which in turn lead to degraded performance in downstream tasks. Recent data-driven approaches have sought to address this challenge by constructing datasets [5] that encompass diverse lighting conditions and material properties, thereby approximating the visual characteristics of transparent and specular objects, and subsequently training models for depth prediction [4], [6]. However, such datasets remain constrained in diversity, and the resulting methods frequently exhibit suboptimal performance in real-world scenarios. We hypothesize that these methods tend to overfit to the limited datasets on which they are trained. To address the generalization challenge, recent works [4], [7] have increasingly leveraged pre-trained vision encoders, such as DINO [8], or harnessed text-to-image foundation models like Stable Diffusion [9] to train depth estimation networks. While these approaches have achieved notable improvements in single-frame depth accuracy, they continue to suffer from lack of temporal consistency across frame sequences [10]. This limitation is particularly detrimental to downstream tasks that rely on stable 3D perception to support consistent action policies, such as robotic manipulation [11], [12]. These tasks are often carried out in dynamic and unstructured environments, where robust perception and temporally coherent decision-making are indispensable. With recent advances in Video Diffusion Model (VDM) [13], [14], we observe their remarkable capacity to synthesize physically plausible videos of interactions with transparent objects, as illustrated in the first column of Fig. 2. Our central insight is that these models appear to have implicitly internalized the physical principles of light transportsuch as refraction and reflection through transparent or translucent materials. To leverage this knowledge for video depth estimation of transparent-object, we make contributions from two perspectives: data and learning. Data. We collect 3D asset collection consisting of diverse categories and shapes of transparent and highly reflective items. Subsequently, we introduce rendering pipeline that automatically generates physically plausible scenes using these assets and renders video data with varied light sources and camera trajectories, leading to the first synthetic video dataset, termed TransPhy3D, which focuses on transparent-objects, complementing existing image counterparts [4], [15], [16] that primarily study single-frame depth estimation problem. Learning. We propose paradigm shift to video depth estimation: reframing it from discriminative estimation task to video-to-video translation problem. We achieve this by repurposing VDM [14] using LoRA training strategy. To fully leverage existing frame-wise datasets, we introduce co-training strategy that enables joint training on mixture of frame-wise and video data. Finally, we introduce foundation model designed primarily for video depth estimation of transparent-object, termed DKT. We validate the effectiveness of DKT through comprehensive experiments, demonstrating that it achieves SOTA performance under zero-shot setting on both synthetic and real-world benchmarks. In summary, our main contributions are: introduce TransPhy3D, We synthetic transparent-object video dataset, comprising 11,000 videos and 1.32 million frames of data, to enable effective fine-tuning of VDM. first the We introduce the first foundation model for transparentobject video depth estimation by repurposing VDM through LoRA finetuning and we design co-training strategy for training on mixture data of available synthetic image datasets and TransPhy3D. We conduct comprehensive benchmarking of existing SOTA methods on several open datasets and demonstrate the superiority of DKT in both depth estimation accuracy and real-world robotic experiments. II. RELATED WORKS A. From Discriminative to Generative Depth Estimation Depth estimationparticularly for video and transparent objectshas long been dominated by discriminative approaches [17], [18], [19], [4], [10], [17], [20], [21], [22], [23], [24], [25], [24], [26]. Early methods such as FastDepth [17] and ClearGrasp [16] relied on synthetic data and hand-crafted geometric cues (e.g., surface normals, occlusion boundaries) to overcome the ambiguous visual appearance of transparency. Despite their innovation, these models suffered from significant domain gaps and limited generalization. Subsequent work incorporated stereo cues [26], [19], probabilistic volumetric representations [27], and metric learning techniques [25], [24], yet still operated within discriminative framework, attempting to directly map pixels to depth. turning point emerged with the adoption of generative modelsespecially diffusion modelswhich reframe depth estimation not as regression, but as conditional generative process. Methods such as D3RoMa [4] and VPD [27] leverage diffusion to iteratively refine depth predictions, incorporating physical constraints such as stereo consistency and temporal smoothness. These approaches implicitly learn optical priors, enabling more robust inference on challenging materials. More recent video-depth techniquesincluding RollingDepth [28], FlashDepth [29], and DepthCrafter [10]further demonstrate that generative architectures inherently capture scene dynamics and material properties, even under transparency. Our work builds upon this generative turn, but goes further: we show that largescale video diffusion models, pre-trained on internet-scale video data, already internalize rich prior of transparent phenomena. By fine-tuning such model entirely on synthetic data, we achieve SOTA zero-shot depth estimation without any real-world labels. B. The Rise of Generative Data and Physics-Aware Synthesis Parallel to advances in model architecture, the synthesis of training data has also undergone generative revolution [16], [4], [15], [30], [31], [11], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57]. Early synthetic datasets for transparency, such as those produced by ClearGrasp [16] and DREDS [15], relied on physically-based rendering (PBR) and careful domain randomization. While effective, these methods required significant expertise and computational resources to simulate realistic sensor noise and material variations. The advent of generative video models has introduced new paradigm: models such as TI2VZero [30] can produce photorealistic, temporally coherent videos of transparent objects without fine-tuning, implying that generative priors encapsulate complex optical laws. Subsequent tools like LayerDiffuse [58] and Clear-Splatting [33] further enable fine-grained control over transparency effects in generated content. These capabilities suggest that generative models have learned not only appearance, but underlying physical rules. In robotics, this shift has enabled policies and perception systems that leverage generative world models, as seen in Diffusion Policy [11] and VidMan [32]. Unlike traditional simulation-based data generation, generative models offer scalability and diversity, reducing reliance on handengineered graphics pipelines. Our approach fully embraces this idea: we use generative video diffusion model as both data synthesizer and perception backbone. By fine-tuning it on purely synthetic transparent sequences, we exploit its Fig. 2. We present DKT, foundational model for fine-grained, temporally consistent depth estimation of in-the-wild videos featuring transparent objects of arbitrary lengths. inherent physical understandingachieving superior generalization without real-world supervision. III. METHOD Despite the rapid progress in video depth and normal estimation [17], [59], [10], [60], existing methods remain limited when handling transparent and reflective objects. The core difficulty lies in the absence of reliable supervision: for realworld data, the SOTA pipeline [5]RGB-D capture followed by CAD model recoveryfails to account for background information, while for synthetic data, no video dataset with transparent objects is available. This scarcity of ground truth highlights the need for strong generative priors. However, adapting generative prior models such as VDMs to this specific domain introduces critical challengecatastrophic forgetting of their original priors. To tackle these challenges, we propose DKT, framework that couples large-scale video data curation of transparent and reflective objects with LoRA-based adaptation of VDMs. To mitigate the lack of supervision, we first construct 3D asset bank with diverse categories and shapes, and introduce rendering pipeline that generates physically plausible video scenes under varied lighting and camera trajectories. To further reduce rendering costs and enhance training efficiency, we incorporate existing synthetic image datasets and devise heuristic sampling strategy that enables joint training on both image and video data within unified pipeline. To address the scenario of catastrophic forgetting, we employ the LoRA strategy [61] to efficiently adapt VDM for transparency perception, achieving seamless fusion of its transparent priors with the essential knowledge required for new tasks. A. TransPhy3D To address the gap in video datasets featuring transparent and reflective objects, we construct the first synthetic video dataset of transparent and reflective objects. This dataset is characterized by diversity in object shapes and categories, varied camera trajectories, and high-quality annotations. Parametric & Static 3D Assets. As illustrated in Fig. 4, our asset repository integrates two complementary sources: Category-Rich Static 3D Assets and Shape-Rich Parametric 3D Assets, ensuring rich categories and shapes. For the former, we collected 5,574 assets from BlenderKit1. Each asset is assigned an aesthetic score by rendering an image and passing it through Qwen2.5-VL-7B [62] to identify objects with transparent or highly reflective properties. Consequently, this process results in final collection with 574 high-quality assets that is rich in categories, featuring transparent and reflective assets. For the latter, following [63], we develop procedural pipeline to generate parametric assets. As shown in the bottom of Fig.4, varying parameters of the same asset can produce different shapes. Consequently, this procedure yields collection that is rich in shape diversity. To give these models photorealistic appearance, we pair them with specially curated material library containing wide selection of transparent materials (like glass and plastic) and highly reflective ones (like metal and glazed ceramic). Scene Creation. This stage focuses on composing scenes dynamically through physics simulation. we randomly select assets and initialize their six-degree-of-freedom (6-DOF) poses and scales within predefined environment, such as container or tabletop, as shown in the topright of Fig. 4, We then employ Blenders integrated physics engine to simulate the objects as they fall and collide, allowing them to settle into physically plausible and natural final arrangement. Camera Sampling & Rendering. To capture diverse and dynamic viewpoints, our camera sampling method generates circular trajectories around the geometric center of objects, incorporating sinusoidal perturbations of varying amplitudes. We then utilize Blenders ray tracing engine, Cycles, to perform physically accurate lighting calculations and material rendering. This process precisely simulates complex light transport phenomena, including propagation, refraction, and reflection within transparent materials. As final step, we 1https://www.blenderkit.com/ Fig. 3. Overaview of DKT. DKT starts with pretrained video diffusion model [14] and is finetuned for video depth estimation by concatenating an extra RGB latent with the input latent using LoRA training strategy. use the NVIDIA OptiX-Denoiser to optimize image quality. The output of this pipeline is TransPhy3D, novel video dataset comprising 11,000 unique scenes. With each scene rendered as 120-frame video, the dataset contains total of 1,320,000 frames, sourced from both our parametric and static asset collections. B. Preliminaries of Video Diffusion Model This work builds upon WAN [14], which is comprised of three primary components: VAE, diffusion transformer consisting of multiple DiT blocks, and text encoder. The VAE compresses input videos into latent space and decodes predicted latents back to image space. The text encoder encodes text prompts into embeddings. The diffusion transformer predicts velocity given noisy latents and text embeddings. WAN leverages the flow matching framework [64] to model unified denoising diffusion process. During training, given an image or video latent x1, random noise x0 (0, I), and timestep U(0, 1), an intermediate latent xt serving as training input is obtained by: xt = tx1 + (1 t)x0. The ground truth velocity vt is obtained by: vt = dxt dt = x1 x0. (1) (2) The loss function is MSE between the output of velocity predictor and vt: = Ex0,x1,ctxt,t (cid:13) (cid:13) u(cid:0)xt, ctxt, t(cid:1) vt (cid:13) (cid:13) (cid:13) 2 (cid:13) (cid:13) (cid:13) , (3) where ctxt is the text embedding. C. Training Strategy As illustrated in Fig. 3, to enhance the training efficiency and alleviate the rendering burden of rendering process. We propose to co-train synthetic image and video data (TransPhy3D). We first sample constant number using: = 4N + 1 U(0, 5). Fig. 4. Rendering Pipeline. Scenarios are constructed using static and parametric 3D assets. RGB, depth, and normal videos are rendered by sampling circular trajectory within the scene. indicates the frame number for the video in this batch of data. Afterwards, if is equal to 1, we sample batch of paired data consisting of RGB and depth videos from both video and image datasets (with the video containing only one frame); otherwise, we sample only from video datasets. Afterwards, the overall pipeline of DKT is presented in Fig. 3, the depth video in one pair is converted to disparity. Both RGB and depth videos are normalized to [1, 1] to match the VAE training space, then encoded by the VAE into latents xc 1 and xd 1. The depth latent xd 1 is transformed by Eq. 1 into the intermediate latent xd . The input to the DiT blocks is then obtained by concatenating xd 1 along the channel dimension. The training loss is defined as the difference between the DiT output and the ground-truth velocity vd (constructed by Eq. 2), and is computed as: and xc = x0,xd 1 ,xc 1,ctxt,t (cid:13) (cid:13) u(cid:0)Concat(xd (cid:13) (cid:13) , xc 1), ctxt, t(cid:1) vd (cid:13) 2 (cid:13) (cid:13) (cid:13) , (5) where ctxt is the text embedding, and Concat denotes the concatenation operation along the channel dimension. All model components remain frozen except for small set of trainable LoRA [61] parameters in the DiT, which learn low-rank weight adaptations. D. Implementation We trained our model with learning rate of 1e 5 using AdamW [69] and batch size of 8. The model is trained using synthetic image datasets, including HISS [4], (4) Fig. 5. Qualitative comparison on the ClearPose [5]. For better visualizing the temporal quality, we show the temporal profiles of each result in green boxes, by slicing the depth values along the time axis at the red line positions. TABLE QUANTITATIVE COMPARISON FOR VIDEO DEPTH ESTIMATION ON CLEARPOSE AND TRANSPHY3D-TEST. BEST AND SECOND BEST ARE HIGHLIGHED. Methods ClearPose [5] TransPhy3D-Test REL RMSE δ1.05 δ1.10 δ1.25 Rank REL RMSE δ1.05 δ1.10 δ1.25 Rank Depth4ToM [65] (ICCV23) DAv2 [7] (NeurIPS24) Marigold-E2E-FT [66] (WACV25) MoGe [67] (CVPR25) VGGT [68] (CVPR25) DepthCrafter [10] (CVPR25) DKT (Ours) 12.38 10.85 16.44 13.13 15.38 11.32 9.72 14.02 12.21 16.65 13.40 15.68 12.34 14.58 28.74 32.21 16.99 24.09 19.93 31.92 38. 51.39 56.37 33.85 45.08 38.33 55.46 65.50 85.94 89.94 74.24 84.29 76.89 88.59 93.04 4.0 1.8 7.0 4.6 6.0 2.8 1. 18.01 14.02 23.58 31.91 32.30 11.32 2.96 720.13 74.86 42.63 136.74 49.82 12.34 19.50 31.54 31.36 11.42 19.29 13.64 31.92 87. 56.56 53.12 27.49 32.24 30.13 55.46 97.09 85.98 81.27 62.86 50.98 55.25 88.59 98.56 3.8 4.0 5.4 5.8 5.8 2.0 1. TABLE II QUANTITATIVE COMPARISON FOR VIDEO DEPTH ESTIMATION ON THE DREDS DATASETS. Methods DREDS-STD-CatKnown [15] DREDS-STD-CatNovel [15] REL RMSE δ1.05 δ1.10 δ1.25 Rank REL RMSE δ1.05 δ1.10 δ1.25 Rank Depth4ToM [65] (ICCV23) DAv2 [7] (NeurIPS24) Marigold-E2E-FT [66] (WACV25) MoGe [67] (CVPR25) VGGT [68] (CVPR25) DepthCrafter [10] (CVPR25) DKT (ours) 6.92 6.94 6.07 6.95 5.74 7.06 5.30 6.60 6.58 5.81 5.78 5.14 6.41 4.96 44.23 44.46 49.07 47.03 51.14 41.45 53. 74.15 73.66 79.15 74.44 80.94 72.32 84.93 98.19 98.32 99.36 97.46 99.79 98.68 99.89 5.6 5.4 3.2 4.8 2.0 6.2 1. 7.21 7.41 7.06 6.07 6.10 7.41 5.71 5.77 5.76 5.55 4.32 4.74 5.54 4.66 43.45 41.16 42.81 50.25 48.93 38.44 52. 71.13 69.77 71.51 78.95 78.56 70.23 79.51 98.04 98.05 98.71 99.31 99.53 98.51 99.84 5.6 6.2 4.4 2.0 2.8 5.6 1. DREDS [15], and ClearGrasp [16], along with our newly introduced video synthetic dataset, TransPhy3D. Following the training strategy of WAN, all datasets are resized to 832 480 for model training. The number of training iterations is 70K, which takes 8 Nvidia H100 GPUs for two days. Except for specific explanations, the denoising step is set to 5 for inference. By following the inference strategy in [10], we achieve arbitrary-length video inference by splitting the input into overlapping segments. Consecutive segments are then stitched together using complementary weight applied to the overlapping regions. For more details, please refer to [10]. IV. EXPERIMENT A. Evaluation Metrics Following the practice of evaluating the temporal consistency of depth maps[10], the predictions are aligned with the ground truth using global scale and shift. The following metrics are then calculated: δ1.05, δ1.10, δ1.25, and REL, which are expressed as percentages, along with RMSE, measured in centimeters. B. Evaluation Datasets DKT is evaluated using the following real-world and synthetic datasets under zero-shot setting to demonstrate its generalization, robustness, and potential effects on the robotic community. Fig. 6. Qualitative comparison for video normal estimation on ClearPose [5]. ClearPose: ClearPose [5] is real-world RGB-D benchmark for transparent and translucent objects. This testset consists of 27 real-world scenes, including different backgrounds, heavy occlusions, objects in translucent and opaque covers, non-planar surfaces, and even scenes filled with liquid. Each scene is captured in long video using the RealSense L515 depth camera. The foreground ground truth depth of transparent objects is recovered using object CAD models. The background region is masked by threshold of [0.3, 1.5] during evaluation. This dataset also provides normal annotation translated from depth map. STD: STD [15] is real-world dataset comprising specular, transparent, and diffuse objects. It is divided into two subsets, CatKnown and CatNovel, based on the commonality of the objects. The former comprises 12 scenes, while the latter contains 5 scenes. TransPhy3D-Test: We new transparent model [70] to render synthetic test set using the pipeline we introduce. This dataset consists of 28 scenes. employ C. State-of-the-art Comparision comparison is conducted with available foundation depth estimation methods, including the image depth estimation method Depth-Anything-V2 [7], MoGe [67], VGGT [68], Marigold-E2E-FT [66], Depth4ToM [65] and the video depth estimation method DepthCrafter [10]. As shown in Tab. and II, DKT sets new SOTA across three real-world datasets and one synthetic dataset. The performance gap peaks in ClearPose and TransPhy3D, both of which exclusively involve transparent and highly reflective objects. Specifically, we outperform the second-best method by scores of 5.69, 9.13, and 3.1 for δ1.05, δ1.10, and δ1.25 in ClearPose, and 55.25, 40.53, and 9.97 for δ1.05, δ1.10, and δ1.25 in TransPhy3D. TABLE III ABLATION FOR TRAINING STRATEGY IN CLEARPOSE. INDICATES NAIVE FINETUNING. Model Size LoRA REL RMSE δ1.05 δ1.10 δ1.25 1.3B 1.3B 14B 11.86 11.17 9.72 26.54 17.45 14.58 30.48 33.16 38.17 54.03 58.02 65. 88.30 90.65 93.04 Moreover, this advantage is clearly reflected in Fig. 5. [10], we present temporal profile to better Following illustrate the temporal consistency of the predicted video depth. DKT not only achieves superior identification of transparent objects in the first frame but also demonstrates optimal temporal consistency. What accounts for the significant performance gap in the TransPhy3D-Test? We attribute this phenomenon to characteristic of our rendering data: the camera trajectories during rendering describe circular path around the object. This feature increases the requirements for inter-frame consistency in the models predictions, as even minor error can lead to significant decline in prediction accuracy after global alignment. D. Ablation Study Training Strategies. As illustrated in Tab. III, naive finetuning results in high computational costs and suboptimal performance compared to LoRA fine-tuning. By adopting the LoRA training strategy and scaling up the model size, significant improvement in perforamnce is achieved. Inference Steps As illustrated in the upper part of Fig. 7, increasing the number of inference steps does not yield significant performance improvements. Moreover, as shown in the lower part of Fig. 7, fewer steps result in inaccurate predictions, while more inference steps lead to the loss TABLE VIDEO NORMAL ESTIMATION ON THE CLEARPOSE. mean med 11.25 22.5 30 NormalCrafter [60] (ICCV25) Marigold-E2E-FT [66] (WACV25) DKT-Normal-14B 27.08 27.08 26.03 20.29 19.40 18.59 26.10 29.78 30.06 55.37 57.22 59.63 68.81 69.30 70.98 Fig. 9. Demonstration of Surface Types. forms the previous SOTA video normal estimation method NormalCrafter [60] and the previous SOTA image estimation method Marigold-E2E-FT [66] by substantial margin on the zero-shot dataset ClearPose, using the same metrics as NormalCrafter. Moreover, DKT-Normal-14B achieves the sharpest normal prediction and the highest temporal consistency, as illustrated in Fig. 6. E. Real-world Grasping Experiments Experimental Setup. 1) Hardware System: As shown in Fig. 8, our real-world robotic manipulation environment consists of two PiPER ARM manipulators used to grasp objects and fixed-view Realsense D435 camera to provide RGB observations. 2) Environment Arrangement: As illustrated in Fig. 9, we set up three types of tabletop backgrounds: reflective, translucent, and diffusive surfaces. Various objects with specular, transparent, and diffusive properties are placed on the table. This setup allows for comprehensive evaluation of the effectiveness and robustness of different methods under complex real-world scenarios. Deployment Pipeline. Initially, an RGB image is captured by the D435 camera and processed with various relative depth estimation models, including DAv2-Large, DepthCrafter, and DKT-1.3B, to generate relative depth maps. These are then rescaled to metric depth using AprilTag [71]. Subsequently, the RGB image and metric depth are input into AnyGrasp [72] to generate 7-DoF grasp pose (end-effector pose + gripper width). Finally, CuRobo [73] is utilized to plan an executable trajectory, which is executed by the PiPER ARM. Results and Analysis. As demonstrated in Tab. VI, DKT consistently outperforms all baseline across all settings by significant margin. For the perception results and the grasping video, please refer to the appendix. V. CONCLUSIONS In this work, we dive into the challenging problem of video depth and normal estimation for transparent and highly reflective scenarios, which is crucial for robotic perception. Our contributions can be summarized in three parts. First, the first video dataset for transparent and highly reflective objects is introduced, accompanied by diverse transparent asset categories and an infinite variety of 3D shapes. Second, DKT Fig. 7. The effects of different Inference step. TABLE IV INFERENCE TIME PER FRAME (MS) AT RESOLUTION OF 832 480. Method Encoding Denoising Decoding All DAv2 [7] DepthCrafter [10] DKT-14B DKT-1.3B N/A 141.85 46.53 46.53 N/A 240.01 297.11 52.88 N/A 183.69 68.07 68. 277.75 565.55 411.71 167.48 of important details. To balance performance and inference efficiency, we set 5 as the default number of inference steps. Computational Efficiency To assess inference efficiency fairly, we reevaluated DKT, the baseline DAv2-Large [7], and DepthCrafter [10] on the Nvidia L20. The results, shown in Tab. IV, indicate that DKT-1.3B achieves the highest efficiency, with remarkable inference time of only 167.48ms per frame, surpassing DAv2 by 110.27ms. Moreover, the peak GPU memory occupancy of DKT-1.3B is only 11.19 GB, which is acceptable for most robot computational platforms. Fig. 8. integrates the PiPER Arm and RealSense D435 for our grasping tasks. Real-World Setup. We use the Cobot Magic system, which Video Normal Estimation To further validate the efficacy of our training strategy, DKT-Normal-14B is introduced following the same training strategy of DKT-14B. As demonstrated in Tab. V, DKT-Normal-14B significantly outperTABLE VI GRASPING SUCCESS RATE OF DIFFERENT DEPTH ESTIMATORS ON TRANSLUCENT, REFLECTIVE, DIFFUSIVE SURFACE, RESPECTIVELY. Method Translucent Reflective Diffusive Mean RAW DAv2 DepthCrafter DKT-1.3B 0.47 0.60 0.67 0. 0.18 0.27 0.23 0.59 0.56 0.56 0.625 0.81 0.384 0.46 0.48 0.73 is introduced, finetuned from video diffusion model using LoRA training strategy. Finally, we demonstrate DKTs performance in comprehensive benchmarks, including synthetic and real-world datasets, and DKT sets new SOTA across all of them in video depth and normal estimation. VI. ACKNOWLEDGEMENTS This study is supported by the Beijing Academy of Artificial Intelligence (BAAI), under its research funding programs."
        },
        {
            "title": "REFERENCES",
            "content": "[1] M. Grinvald, F. Tombari, R. Siegwart, and J. Nieto, Tsdf++: multiobject formulation for dynamic object tracking and reconstruction, in 2021 IEEE international conference on robotics and automation (ICRA), pp. 1419214198, IEEE, 2021. [2] W. Cui, C. Zhao, S. Wei, J. Zhang, H. Geng, Y. Chen, H. Li, and H. Wang, Gapartmanip: large-scale part-centric dataset for material-agnostic articulated object manipulation, in 2025 IEEE International Conference on Robotics and Automation (ICRA), pp. 14791 14798, IEEE, 2025. [3] J. Liu, H. Ma, Y. Guo, Y. Zhao, C. Zhang, W. Sui, and W. Zou, Monocular depth estimation and segmentation for transparent object with iterative semantic and geometric fusion, in 2025 IEEE International Conference on Robotics and Automation (ICRA), pp. 11162 11168, 2025. [4] S. Wei, H. Geng, J. Chen, C. Deng, C. Wenbo, C. Zhao, X. Fang, L. Guibas, and H. Wang, D3 roma: Disparity diffusion-based depth sensing for material-agnostic robotic manipulation, in ECCV 2024 Workshop on Wild 3D: 3D Modeling, Reconstruction, and Generation in the Wild, 2024. [5] X. Chen, H. Zhang, Z. Yu, A. Opipari, and O. Chadwicke Jenkins, Clearpose: Large-scale transparent object dataset and benchmark, in European conference on computer vision, pp. 381396, Springer, 2022. [6] J. Shi, A. Yong, Y. Jin, D. Li, H. Niu, Z. Jin, and H. Wang, Asgrasp: Generalizable transparent object reconstruction and 6-dof grasp detection from rgb-d active stereo camera, in 2024 IEEE international conference on robotics and automation (ICRA), pp. 54415447, IEEE, 2024. [7] L. Yang, B. Kang, Z. Huang, Z. Zhao, X. Xu, J. Feng, and H. Zhao, Depth anything v2, Advances in Neural Information Processing Systems, vol. 37, pp. 2187521911, 2024. [8] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin, Emerging properties in self-supervised vision transformers, in Proceedings of the IEEE/CVF international conference on computer vision, pp. 96509660, 2021. [9] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, High-resolution image synthesis with latent diffusion models, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. [10] W. Hu, X. Gao, X. Li, S. Zhao, X. Cun, Y. Zhang, L. Quan, and Y. Shan, Depthcrafter: Generating consistent long depth sequences for open-world videos, in Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 20052015, 2025. [11] C. Chi, S. Feng, Y. Du, Z. Xu, E. Cousineau, B. Burchfiel, and S. Song, Diffusion policy: Visuomotor policy learning via action diffusion, in Proceedings of Robotics: Science and Systems (RSS), 2023. [12] H. Geng, S. Wei, C. Deng, B. Shen, H. Wang, and L. Guibas, Sage: Bridging semantic and actionable parts for generalizable articulated-object manipulation under language instructions, arXiv preprint arXiv:2312.01307, vol. 2, 2023. [13] DeepMind, Veo: text-to-video generation system, Technical Report Veo-3 Tech Report, Google DeepMind, 2024. Describes the components of Veo 3, including the diffusion-based audio+video model, training data, and safety evaluations. [14] T. Wan, A. Wang, B. Ai, B. Wen, C. Mao, C.-W. Xie, D. Chen, F. Yu, H. Zhao, J. Yang, J. Zeng, J. Wang, J. Zhang, J. Zhou, J. Wang, J. Chen, K. Zhu, K. Zhao, K. Yan, L. Huang, M. Feng, N. Zhang, P. Li, P. Wu, R. Chu, R. Feng, S. Zhang, S. Sun, T. Fang, T. Wang, T. Gui, T. Weng, T. Shen, W. Lin, W. Wang, W. Wang, W. Zhou, W. Wang, W. Shen, W. Yu, X. Shi, X. Huang, X. Xu, Y. Kou, Y. Lv, Y. Li, Y. Liu, Y. Wang, Y. Zhang, Y. Huang, Y. Li, Y. Wu, Y. Liu, Y. Pan, Y. Zheng, Y. Hong, Y. Shi, Y. Feng, Z. Jiang, Z. Han, Z.-F. Wu, and Z. Liu, Wan: Open and advanced large-scale video generative models, arXiv preprint arXiv:2503.20314, 2025. [15] Q. Dai, J. Zhang, Q. Li, T. Wu, H. Dong, Z. Liu, P. Tan, and H. Wang, Domain randomization-enhanced depth simulation and restoration for perceiving and grasping specular and transparent objects, in European Conference on Computer Vision, pp. 374391, Springer, 2022. [16] S. Sajjan, M. Moore, M. Pan, G. Nagaraja, J. Lee, A. Zeng, and S. Song, Clear grasp: 3d shape estimation of transparent objects for manipulation, in 2020 IEEE international conference on robotics and automation (ICRA), pp. 36343642, IEEE, 2020. [17] D. Wofk, F. Ma, T.-J. Yang, S. Karaman, and V. Sze, Fastdepth: Fast monocular depth estimation on embedded systems, in 2019 International Conference on Robotics and Automation (ICRA), pp. 6101 6108, IEEE, 2019. [18] J. Okae, B. Li, J. Du, and Y. Hu, Robust scale-aware stereo matching network, IEEE Transactions on Artificial Intelligence, vol. 3, no. 2, pp. 244253, 2021. [19] B. Li, Y. Sun, Z. Liang, D. Du, Z. Zhang, X. Wang, Y. Wang, X. Jin, and W. Zeng, Bridging stereo geometry and bev representation with reliable mutual interaction for semantic scene completion, arXiv preprint arXiv:2303.13959, 2023. [20] H. Li, Y. Ma, Y. Gu, K. Hu, Y. Liu, and X. Zuo, Radarcam-depth: Radar-camera fusion for depth estimation with learned metric scale, in 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 1066510672, IEEE, 2024. [21] Y. Liao, L. Huang, Y. Wang, S. Kodagoda, Y. Yu, and Y. Liu, Parse geometry from line: Monocular depth estimation with partial laser observation, in 2017 IEEE international conference on robotics and automation (ICRA), pp. 50595066, IEEE, 2017. [22] S. S. Shivakumar, K. Mohta, B. Pfrommer, V. Kumar, and C. J. Taylor, Real time dense depth estimation by fusing stereo with sparse depth measurements, in 2019 International Conference on Robotics and Automation (ICRA), pp. 64826488, IEEE, 2019. [23] S. Pillai, R. Ambrus, and A. Gaidon, Superdepth: Self-supervised, super-resolved monocular depth estimation, in 2019 International Conference on Robotics and Automation (ICRA), pp. 92509256, IEEE, 2019. [24] C. Wang, Y. Qin, Z. Kang, N. Ma, and R. Zhang, Toward accurate camera-based 3d object detection via cascade depth estimation and calibration, in 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 20062012, IEEE, 2024. [25] L. Ebner, G. Billings, and S. Williams, Metrically scaled monocular depth estimation through sparse priors for underwater robots, in 2024 IEEE international conference on robotics and automation (ICRA), pp. 37513757, IEEE, 2024. [26] A. Li, A. Hu, W. Xi, W. Yu, and D. Zou, Stereo-lidar depth estimation with deformable propagation and learned disparity-depth conversion, in 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 27292736, IEEE, 2024. [27] B. Li, Y. Sun, J. Dong, Z. Zhu, J. Liu, X. Jin, and W. Zeng, One at time: Progressive multi-step volumetric probability learning for reliable 3d scene perception, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, pp. 30283036, 2024. [28] B. Ke, D. Narnhofer, S. Huang, L. Ke, T. Peters, K. Fragkiadaki, A. Obukhov, and K. Schindler, Video depth without video models, in Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 72337243, 2025. [29] G. Chou, W. Xian, G. Yang, M. Abdelfattah, B. Hariharan, N. Snavely, N. Yu, and P. Debevec, Flashdepth: Real-time streaming video depth estimation at 2k resolution, arXiv preprint arXiv:2504.07093, 2025. [30] H. Ni, B. Egger, S. Lohit, A. Cherian, Y. Wang, T. Koike-Akino, S. X. Huang, and T. K. Marks, Ti2v-zero: Zero-shot image conditioning for text-to-video diffusion models, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9015 9025, 2024. [31] B. Li, J. Guo, H. Liu, Y. Zou, Y. Ding, X. Chen, H. Zhu, F. Tan, C. Zhang, T. Wang, et al., Uniscene: Unified occupancy-centric driving scene generation, in Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1197111981, 2025. [32] Y. Wen, J. Lin, Y. Zhu, J. Han, H. Xu, S. Zhao, and X. Liang, Vidman: Exploiting implicit dynamics from video diffusion model for effective robot manipulation, Advances in Neural Information Processing Systems, vol. 37, pp. 4105141075, 2024. [33] A. Agrawal, R. Roy, B. P. Duisterhof, K. B. Hekkadka, H. Chen, and J. Ichnowski, Clear-splatting: Learning residual gaussian splats for transparent object manipulation, in RoboNerF: 1st Workshop On Neural Fields In Robotics at ICRA 2024, 2024. [34] A. Cheng, Z. Yang, H. Zhu, and K. Mao, Gam-depth: self-supervised indoor depth estimation leveraging gradient-aware mask and semantic constraints, in 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 53675374, IEEE, 2024. [35] S. Dai, X. Lou, P. Nilsson, S. Thakar, C. Meeker, A. Gordon, X. Kong, J. Zhang, B. Knoerlein, R. Liu, et al., Depth estimation through translucent surfaces, in 2025 IEEE International Conference on Robotics and Automation (ICRA), pp. 15501557, IEEE, 2025. [36] X. Yang, B. Li, S. Xu, N. Wang, C. Ye, Z. Chen, M. Qin, Y. Ding, Z. Zhu, X. Jin, et al., Orv: 4d occupancy-centric robot video generation, arXiv preprint arXiv:2506.03079, 2025. [37] N. Wang, X. Yan, X. Song, and Z. Wang, Semantic-guided gaussian splatting with deferred rendering, in ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 15, IEEE, 2025. [38] N. Wang, Y. Chen, L. Xiao, W. Xiao, B. Li, Z. Chen, C. Ye, S. Xu, S. Zhang, Z. Yan, et al., Unifying appearance codes and bilateral grids for driving scene gaussian splatting, arXiv preprint arXiv:2506.05280, 2025. [39] M. Ji, J. Gall, H. Zheng, Y. Liu, and L. Fang, Surfacenet: An endto-end 3d neural network for multiview stereopsis, in Proceedings of the IEEE international conference on computer vision, pp. 23072315, 2017. [40] Y. Liao, J. Xie, and A. Geiger, Kitti-360: novel dataset and benchmarks for urban scene understanding in 2d and 3d, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 3, pp. 32923310, 2022. [41] Q. Zheng, M. Lu, S. Wu, R. Hu, J. Lanir, and H. Huang, Imageguided color mapping for categorical data visualization, Computational Visual Media, vol. 8, no. 4, pp. 613629, 2022. [42] R. Liang, Z. Gojcic, H. Ling, J. Munkberg, J. Hasselgren, C.-H. Lin, J. Gao, A. Keller, N. Vijaykumar, S. Fidler, et al., Diffusion renderer: Neural inverse and forward rendering with video diffusion models, the Computer Vision and Pattern Recognition in Proceedings of Conference, pp. 2606926080, 2025. [43] Y.-T. Liu, L. Wang, J. Yang, W. Chen, X. Meng, B. Yang, and L. Gao, Neudf: Leaning neural unsigned distance fields with volume rendering, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 237247, 2023. [44] Y. Xie, A. Franz, M. Chu, and N. Thuerey, tempogan: temporally coherent, volumetric gan for super-resolution fluid flow, ACM Transactions on Graphics (TOG), vol. 37, no. 4, pp. 115, 2018. [45] H. Lin, S. Peng, J. Chen, S. Peng, J. Sun, M. Liu, H. Bao, J. Feng, X. Zhou, and B. Kang, Prompting depth anything for 4k resolution accurate metric depth estimation, in Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1707017080, 2025. [46] Y. Jiang, J. Tu, Y. Liu, X. Gao, X. Long, W. Wang, and Y. Ma, Gaussianshader: 3d gaussian splatting with shading functions for reflective surfaces, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 53225332, 2024. [47] T. Liu, A. W. Bargteil, J. F. OBrien, and L. Kavan, Fast simulation of mass-spring systems, ACM Transactions on Graphics (TOG), vol. 32, no. 6, pp. 17, 2013. [48] J. Chang, Y. Xu, Y. Li, Y. Chen, W. Feng, and X. Han, Gaussreg: Fast 3d registration with gaussian splatting, in European Conference on Computer Vision, pp. 407423, Springer, 2024. [49] C. Ye, Y. Wu, Z. Lu, J. Chang, X. Guo, J. Zhou, H. Zhao, and X. Han, Hi3dgen: High-fidelity 3d geometry generation from images via normal bridging, arXiv preprint arXiv:2503.22236, vol. 3, p. 2, 2025. [50] G. Chen, K. Han, and K.-Y. K. Wong, Ps-fcn: flexible learning framework for photometric stereo, in Proceedings of the European conference on computer vision (ECCV), pp. 318, 2018. [51] G. Chen, K. Han, B. Shi, Y. Matsushita, and K.-Y. K. Wong, Selfcalibrating deep photometric stereo networks, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 87398747, 2019. [52] B. Xiong, S.-T. Wei, X.-Y. Zheng, Y.-P. Cao, Z. Lian, and P.-S. Wang, Octfusion: Octree-based diffusion models for 3d shape generation, in Computer Graphics Forum, vol. 44, p. e70198, Wiley Online Library, 2025. [53] S. Tao, F. Xiang, A. Shukla, Y. Qin, X. Hinrichsen, X. Yuan, C. Bao, X. Lin, Y. Liu, T.-k. Chan, et al., Maniskill3: Gpu parallelized robotics simulation and rendering for generalizable embodied ai, arXiv preprint arXiv:2410.00425, 2024. [54] W. Chen, H. Ling, J. Gao, E. Smith, J. Lehtinen, A. Jacobson, and S. Fidler, Learning to predict 3d objects with an interpolation-based differentiable renderer, Advances in neural information processing systems, vol. 32, 2019. [55] J. He, H. Li, W. Yin, Y. Liang, L. Li, K. Zhou, H. Zhang, B. Liu, and Y.-C. Chen, Lotus: Diffusion-based visual foundation model for highquality dense prediction, arXiv preprint arXiv:2409.18124, 2024. [56] S. Huang, S. Qi, Y. Zhu, Y. Xiao, Y. Xu, and S.-C. Zhu, Holistic 3d scene parsing and reconstruction from single rgb image, in Proceedings of the European conference on computer vision (ECCV), pp. 187203, 2018. [57] M. Xu, C. Ye, H. Liu, Y. Wu, J. Chang, and X. Han, Stable-sim2real: Exploring simulation of real-captured 3d data with two-stage depth diffusion, in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 26092619, 2025. [58] L. Zhang and M. Agrawala, Transparent image layer diffusion using latent transparency, arXiv preprint arXiv:2402.17113, 2024. [59] C. Ye, L. Qiu, X. Gu, Q. Zuo, Y. Wu, Z. Dong, L. Bo, Y. Xiu, and X. Han, Stablenormal: Reducing diffusion variance for stable and sharp normal, ACM Transactions on Graphics (TOG), 2024. [60] Y. Bin, W. Hu, H. Wang, X. Chen, and B. Wang, Normalcrafter: Learning temporally consistent normals from video diffusion priors, arXiv preprint arXiv:2504.11427, 2025. [61] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen, et al., Lora: Low-rank adaptation of large language models., ICLR, vol. 1, no. 2, p. 3, 2022. [62] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, et al., Qwen2. 5-vl technical report, arXiv preprint arXiv:2502.13923, 2025. [63] Y. H. Kim, S. Kim, Y. Lee, and F. C. Park, T2sqnet: recognition model for manipulating partially observed transparent tableware objects, in CORL, 2024. [64] Y. Lipman, M. Havasi, P. Holderrieth, N. Shaul, M. Le, B. Karrer, R. T. Chen, D. Lopez-Paz, H. Ben-Hamu, and I. Gat, Flow matching guide and code, arXiv preprint arXiv:2412.06264, 2024. [65] A. Costanzino, P. Z. Ramirez, M. Poggi, F. Tosi, S. Mattoccia, and L. Di Stefano, Learning depth estimation for transparent and mirror surfaces, in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 92449255, 2023. [66] G. Martin Garcia, K. Abou Zeid, C. Schmidt, D. de Geus, A. Hermans, and B. Leibe, Fine-tuning image-conditional diffusion models is easier than you think, in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2025. [67] R. Wang, S. Xu, C. Dai, J. Xiang, Y. Deng, X. Tong, and J. Yang, Moge: Unlocking accurate monocular geometry estimation for opendomain images with optimal training supervision, in Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 5261 5271, 2025. [68] J. Wang, M. Chen, N. Karaev, A. Vedaldi, C. Rupprecht, and D. Novotny, Vggt: Visual geometry grounded transformer, in Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 52945306, 2025. [69] I. Loshchilov and F. Hutter, Decoupled weight decay regularization, arXiv preprint arXiv:1711.05101, 2017. [70] J. Kim, M.-H. Jeon, S. Jung, W. Yang, M. Jung, J. Shin, and A. Kim, Transpose: Large-scale multispectral dataset for transparent object, The International Journal of Robotics Research, vol. 43, no. 6, pp. 731738, 2024. [71] J. Wang and E. Olson, Apriltag 2: Efficient and robust fiducial detection, in 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 41934198, IEEE, 2016. [72] H.-S. Fang, C. Wang, H. Fang, M. Gou, J. Liu, H. Yan, W. Liu, Y. Xie, and C. Lu, Anygrasp: Robust and efficient grasp perception in spatial and temporal domains, IEEE Transactions on Robotics, vol. 39, no. 5, pp. 39293945, 2023. [73] B. Sundaralingam, S. K. S. Hari, A. Fishman, C. Garrett, K. Van Wyk, V. Blukis, A. Millane, H. Oleynikova, A. Handa, F. Ramos, et al., curobo: Parallelized collision-free minimum-jerk robot motion generation, arXiv preprint arXiv:2310.17274, 2023."
        }
    ],
    "affiliations": [
        "Beihang University",
        "Beijing Academy of Artificial Intelligence",
        "European Institute of Innovation and Technology Ningbo",
        "FNii, The Chinese University of Hong Kong, Shenzhen",
        "National University of Singapore",
        "Shanghai Jiao Tong University",
        "Tsinghua University",
        "University of Southern California",
        "Wuhan University"
    ]
}