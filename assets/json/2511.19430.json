{
    "paper_title": "Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution",
    "authors": [
        "Dingkang Liang",
        "Cheng Zhang",
        "Xiaopeng Xu",
        "Jianzhong Ju",
        "Zhenbo Luo",
        "Xiang Bai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Task scheduling is critical for embodied AI, enabling agents to follow natural language instructions and execute actions efficiently in 3D physical worlds. However, existing datasets often simplify task planning by ignoring operations research (OR) knowledge and 3D spatial grounding. In this work, we propose Operations Research knowledge-based 3D Grounded Task Scheduling (ORS3D), a new task that requires the synergy of language understanding, 3D grounding, and efficiency optimization. Unlike prior settings, ORS3D demands that agents minimize total completion time by leveraging parallelizable subtasks, e.g., cleaning the sink while the microwave operates. To facilitate research on ORS3D, we construct ORS3D-60K, a large-scale dataset comprising 60K composite tasks across 4K real-world scenes. Furthermore, we propose GRANT, an embodied multi-modal large language model equipped with a simple yet effective scheduling token mechanism to generate efficient task schedules and grounded actions. Extensive experiments on ORS3D-60K validate the effectiveness of GRANT across language understanding, 3D grounding, and scheduling efficiency. The code is available at https://github.com/H-EmbodVis/GRANT"
        },
        {
            "title": "Start",
            "content": "Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution Dingkang Liang1, Cheng Zhang1*, Xiaopeng Xu1, Jianzhong Ju2, Zhenbo Luo2, Xiang Bai1(cid:66) 1Huazhong University of Science and Technology 2MiLM Plus, Xiaomi Inc. {dkliang, czhang2024, xbai}@hust.edu.cn, {jujianzhong, luozhenbo}@xiaomi.com 5 2 0 2 4 2 ] . [ 1 0 3 4 9 1 . 1 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Task scheduling is critical for embodied AI, enabling agents to follow natural language instructions and execute actions efficiently in 3D physical worlds. However, existing datasets often simplify task planning by ignoring operations research (OR) knowledge and 3D spatial grounding. In this work, we propose Operations Research knowledge-based 3D Grounded Task Scheduling (ORS3D), new task that requires the synergy of language understanding, 3D grounding, and efficiency optimization. Unlike prior settings, ORS3D demands that agents minimize total completion time by leveraging parallelizable subtasks, e.g., cleaning the sink while the microwave operates. To facilitate research on ORS3D, we construct ORS3D-60K, large-scale dataset comprising 60K composite tasks across 4K real-world scenes. Furthermore, we propose GRANT, an embodied multi-modal large language model equipped with simple yet effective scheduling token mechanism to generate efficient task schedules and grounded actions. Extensive experiments on ORS3D-60K validate the effectiveness of GRANT across language understanding, 3D grounding, and scheduling efficiency. Code https://github.com/H-EmbodVis/GRANT"
        },
        {
            "title": "Introduction",
            "content": "Task scheduling is fundamental for embodied agents to efficiently execute human-assigned tasks (Duan et al. 2022; Wang et al. 2024; Huang et al. 2024b; Driess et al. 2023). Achieving this requires the seamless integration of natural language understanding, efficiency optimization, and spatial perception within real-world 3D environments. Recently, several works (Huang et al. 2024b; Chen et al. 2024; Zhang et al. 2024) have made preliminary attempts on plan generation in 3D environments, allowing models to generate step-by-step plans from human instructions (Fig. 1(a)). Nevertheless, these attempts are oversimplified and exhibit critical limitations that hinder their practical applications. First, they lack consideration of task properties and optimization of efficiency. Under their setting, model only needs to generate plausible actions in terms of natural language. In contrast, as shown in Fig. 1(b), an embodied agent is assumed to have the capacity to efficiently complete the task *Equal contribution. (cid:66) Corresponding author. Copyright 2026, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Figure 1: Comparison of different task completion schemes. An embodied agent is expected to use operations research knowledge to efficiently complete tasks through scheduling. by leveraging Operations Research (OR) knowledge. This includes identifying which subtasks can be executed concurrently with other subtasks and maximizing the use of waiting time to achieve optimal efficiency. Second, although their setting assumes an agent operating in 3D environments, it is often reduced to textual question answering, without explicitly grounding each step to the target objects location within the 3D scene. This lack of spatial grounding severely hinders the utility of such plans for downstream embodied executions that require spatial location information (e.g., navigation). To address these limitations and extend the capability of embodied agents for efficient task scheduling, we propose new and practical task named Operations Research knowledge-based 3D Grounded Task Scheduling (ORS3D). In this task, an embodied agent must generate efficient schedules by leveraging OR knowledge and locate the 3D positions of target objects in each action step to complete assigned tasks. As demonstrated in Fig. 2, when we assign composite task to an embodied agent, we hope it can efficiently complete it by utilizing the waiting periods of subtasks that can be performed concurrently. For example, \"Using the microwave\" allows the agent to perform other subtasks during its waiting period. To achieve maximum efficiency, the embodied agent must leverage these subtask properties and incorporate OR knowledge to generate an optimal task schedule. Meanwhile, to execute each step in the real world, the agent must accurately localize the target objects within the Figure 2: Illustration of the proposed Operations Research knowledge-based 3D Grounded Task Scheduling (ORS3D). When assigned composite task by human, the embodied agent needs to complete the subtasks efficiently by carefully scheduling using operations research knowledge and simultaneously locating the target objects in each step for navigation and manipulation. 3D scene. Therefore, ORS3D poses significant challenges to existing 3D agents (Huang et al. 2024b,a; Deng et al. 2025; Lin et al. 2023; Chen et al. 2024) in two essential aspects: 1) It requires OR knowledge to identify subtasks that can be performed concurrently and make efficient task schedules. 2) It entangles language and spatial understanding (i.e., an embodied agent is required to simultaneously generate actions and locate the target objects in the 3D scene). To facilitate research on this new task, we construct the ORS3D-60K dataset consisting of 60,825 composite tasks across 4,376 real-world indoor scenes. As shown in Tab. 1, compared to existing 3D understanding and task planningrelated datasets (Wu et al. 2023; Chen et al. 2024; Zhang et al. 2024; Huang et al. 2024b; Zhu et al. 2024a), ORS3D-60K is the first to incorporate OR knowledge. It also has the largest number of tasks and presents the most significant challenge by requiring models to generate lengthy textual solutions and provide 3D grounding for target objects. To assess the capability of existing methods in addressing this task, we evaluate several baselines (Huang et al. 2024b; Zhu et al. 2024b, 2023; Chen et al. 2024) from language understanding, efficiency optimization, and spatial perception, where the results show they struggle with this challenging task. To tackle the ORS3D problem, we further propose grounded task scheduling agent named GRANT, which is empowered by Multi-modal Large Language Model (MLLM) and equipped with simple yet effective Scheduling Token Mechanism (STM) to generate efficient task schedules. Specifically, we introduce learnable scheduling token that links to an external optimization solver to generate task schedules based on task property constraints provided by the MLLM. The solver employs dynamic programming algorithm to arrange subtasks within the available time intervals of those that can be performed concurrently, producing an optimal execution schedule. During inference, GRANT first predicts subtask properties as constraints, then uses the scheduling token to invoke the solver and generate the optimal schedule, which is subsequently injected back into the model to guide the generation of step-wise action descriptions and target object groundings. Compared to the baseline method (Chen et al. 2024), our approach yields significant 30.53% improvement in task completion time efficiency, along with notable gains of 1.38% in grounding accuracy and 10.46% in overall performance. As an initial attempt, our method paves the way for further exploration in ORS3D. In summary, our contributions are as follows: 1) We introduce operations research knowledge-based 3D grounded task scheduling, new and practical task that meets the common requirement of embodied agents to efficiently complete tasks in the physical world. 2) To support this new task, we construct large-scale dataset, ORS3D-60K. To the best of our knowledge, we are the first to incorporate operations research knowledge for task scheduling in 3D scenarios. 3) We propose GRANT, an embodied MLLM with simple yet effective scheduling token mechanism, integrating task scheduling with multimodal understanding to generate efficient, grounded task execution schedules."
        },
        {
            "title": "2.1 Task Planning",
            "content": "Task Planning (Ahn et al. 2022; Choi et al. 2024; Zhang et al. 2024; Chen et al. 2023) is crucial, as it enables embodied agents to execute human instructions efficiently. Wu et al. (Wu et al. 2023) propose TaPA, vision-language task planning agent that generates executable textual action steps"
        },
        {
            "title": "Dataset",
            "content": "arXiv 23 TaPA (Wu et al. 2023) arXiv 24 Embodied planning (Chen et al. 2024) arXiv 24 SG3D (Zhang et al. 2024) ScanReason (Zhu et al. 2024a) ECCV 24 LEO (Task planning) (Huang et al. 2024b) ICML 24 ICLR 25 Intent3D (Kang et al. 2025) Reference #Scenes #Task Avg. length Text output 3D Grounding Planning OR knowledge 80 15,418 4,357 1,319 4,895 22,346 1,456 12,929 478 13,848 1,042 44,990 69 37 71 29 98 9 ORS3D-60K (ours) - 4,376 60,825 311 Table 1: Comparison with related datasets. \"Avg. length\" denotes the average word length of each data item. Our dataset is the only one that introduces Operations Research (OR) knowledge for task scheduling. for robot navigation and manipulation using multi-view images of the 3D scene. Huang et al. (Huang et al. 2024b) construct task planning dataset that requires embodied agents to generate step-wise plans from instructions. SG3D (Zhang et al. 2024) proposes task-oriented sequential grounding in 3D scenes, where an agent is required to locate each target object in given sequence of actions. In contrast to previous works, we focus on more complex scheduling scenarios and the integration of multi-modal information processing. 2. 3D Scene Understanding 3D scene understanding is the foundation of embodied AI, enabling it to act in real-world scenes. 3D scene understanding includes depth estimation (Xu et al. 2023, 2025a), 3D object detection (Kolodiazhnyi et al. 2024b; Liang et al. 2025b; Zhou et al. 2025), segmentation (Takmaz et al. 2023; Xu et al. 2025b; Liang et al. 2024, 2025a), and grounding (Chen, Chang, and Nießner 2020; Huang et al. 2024c; Jiang et al. 2024). Mask3D (Schult et al. 2023) is often used as an offthe-shelf object proposal extractor for downstream tasks or as 3D scene encoder, as the flexible learned instance queries can be easily assembled to Transformer-based LLMs. OneFormer3D (Kolodiazhnyi et al. 2024a) is an end-to-end method that performs instance and semantic segmentation consistently, utilizing group of learnable instance queries. 2.3 3D Multi-modal Large Language Models 3D MLLMs (Chen et al. 2024; Wang et al. 2023; Huang et al. 2024a; Kang et al. 2024; Fu et al. 2025; Chen et al. 2025; Zhu et al. 2024b, 2025; Hong et al. 2023) narrow the gap between spatial understanding and natural language processing. Several methods (Huang et al. 2024b; Zhu et al. 2023; Kang et al. 2024; Zhu et al. 2024b) utilize point cloud object proposals from off-the-shelf 3D object detectors to extract 3D scene information. Another line of research (Hong et al. 2023; Zhu et al. 2025) leverages pretrained 2D encoders to reconstruct 3D information for the LLMs. In contrast, other approaches like Grounded 3D LLM (Chen et al. 2024) and 3D-LLaVA (Deng et al. 2025) directly process scene point clouds using 3D scene encoders that are jointly trained with LLMs. However, although existing 3D MLLMs excel at scene understanding, they still lack the ability to leverage OR knowledge for efficient task scheduling and completion."
        },
        {
            "title": "3 The ORS3D-60K Dataset\nIn this section, we introduce the definition of Operations\nResearch knowledge-based 3D Grounded Task Scheduling\n(ORS3D) and provide details of the proposed ORS3D-60K\ndataset.",
            "content": "Figure 3: Non-parallelizable subtask & parallelizable subtask."
        },
        {
            "title": "3.1 Design Principles\nUnderstanding human instructions, making efficient sched-\nules to complete human-assigned tasks, and interacting with\nobjects are common and frequent requirements in real-world\napplications for embodied agents.",
            "content": "As illustrated in Fig. 3, tasks assigned to embodied agents can be categorized into two types from an OR perspective: 1) Non-parallelizable subtask requires continuous attention of the agent to manipulate the target object, such as wiping the table or dusting the shelf. 2) Parallelizable subtask only requires the agent to initiate and recheck the target object upon completion, without continuous attention and manipulation, such as using the microwave to heat food or filling the water sink. The agent needs to exploit the time intervals of parallelizable subtasks to achieve an efficiency objective."
        },
        {
            "title": "3.2 Problem Formulation\nThe goal of OR knowledge-based 3D grounded task schedul-\ning is to generate an efficient schedule and accurately locate\nthe target object at each step to complete a composite task.\nSpecifically, suppose that an embodied agent in a 3D scene\nis assigned a composite task consisting of n subtasks, denoted\nas C = {τi}n\ni=1. Each subtask τi is an operation involving\na target object with an expected time, described by a natu-\nral language instruction. To achieve efficient task schedul-\ning, the agent needs to generate a time-efficient schedule\nA = {ai|(τi, li)}s\ni=1 consisting of s steps to accomplish",
            "content": "Figure 4: (a) The ORS3D-60K dataset generation pipeline, which first generates subtask meta-information from 3D scene graphs, then uses this information to generate the structured dataset. (b) composite task example from ORS3D-60K dataset. The green color mask indicates the ground-truth target object in the corresponding step. the composite task. Each step includes textual action description ai for subtask τi and the 3D location li (e.g., 3D bounding box or point mask) of the target object."
        },
        {
            "title": "3.3 Dataset Construction\nThe dataset construction pipeline is illustrated in Fig. 4(a). In\nStage I, we use 3D point clouds from five real-world datasets:\nScanNet (Rozenberszki, Litany, and Dai 2022), HM3D (Ra-\nmakrishnan et al. 2021), ARKitScenes (Baruch et al. 2021),\n3RScan (Wald et al. 2019), and MultiScan (Mao et al. 2022).\nThey are converted into textual 3D scene graphs (Jia et al.\n2024) for subtask meta-information generation via GPT-4o.\nWe refine the outputs for correctness and completeness, and\nperturb subtask expected times by ±10% to generate diverse\noptimal schedules. In Stage II, we compute the optimal task\nschedule using an optimization solver, then convert it into\nstep-wise natural language instructions with phrase-level ob-\nject grounding via GPT-4o. We also generate OR knowledge-\nbased scheduling explanations using templates.",
            "content": "Fig. 4(b) presents data example from the ORS3D-60K dataset. The composite task comprises list of subtasks that the embodied agent must complete. The solution consists of step-by-step actions with target object locations. At each step, the model is required to simultaneously produce an action description of the operation on subtask and locate the target object in the 3D scene. Figure 5: Distributions of (a) subtask number in each composite task, and (b) the expected time of each subtask."
        },
        {
            "title": "3.4 Dataset Characteristics\nThe ORS3D-60K dataset exhibits several distinctive charac-\nteristics that make it stand out from existing datasets.",
            "content": "First, our dataset is closely aligned with real-world taskcompletion scenarios, extending beyond existing 3D visual grounding and question-answering datasets (Chen, Chang, and Nießner 2020; Kang et al. 2025; Zhu et al. 2024a). It is characterized by the inclusion of OR knowledge, which is not considered in existing related 3D understanding datasets. Second, as shown in Tab. 1, our dataset has an exceptionally high average text length of 311 words, which poses Figure 6: Overview of GRANT. The scene point cloud is processed by 3D scene encoder into scene tokens. GRANT first infers task properties (stage 1), then uses scheduling token to generate an optimal schedule (stage 2). The grounding tokens are fed to the 3D grounding head to generate object masks. The input task description is simplified for brevity. significant challenge for the language processing capabilities of embodied agents. Our dataset contains 60,825 composite tasks across 4,376 scenes, representing the largest scale among existing related datasets. Besides, as shown in Fig. 5(a), our dataset covers different levels of difficulty, reflected by the varying number of subtasks (4 to 7) in each composite task. The expected time for each subtask (Fig. 5(b)) follows long-tail distribution, reflecting realworld variability. Furthermore, in our ORS3D setting, 3D grounding is entangled within the text, requiring the model not only to understand what actions to take, but also to accurately identify where each action should occur in the 3D scene. These characteristics make our dataset large-scale, diverse in task complexity, and highly representative of realworld scenarios."
        },
        {
            "title": "4.1 Multi-modal Input Processing\nGRANT takes both the scene point cloud and the textual de-\nscription as input, which are first tokenized and subsequently\nfed into the LLM for unified understanding.",
            "content": "Point cloud tokenization. For scene point cloud RN 6, where each point contains 6-dimensional information [x, y, z, r, g, b] and is the number of points, sparse convolutional network is employed to extract point-wise features RN d, where is the feature dimension. We then use pre-trained 3D scene encoder to further encode the point cloud features into scene tokens. The 3D scene encoder employs fixed set of learnable scene queries = {qi}K i=1, which interact with point cloud features via cross-attention to produce processed scene queries containing rich semantic information. This process can be formulated as: ˆQ = E(Q, F), (1) where ˆQ = {ˆqi}K i=1 is the processed scene queries. To align with the token embedding dimension of the LLM, the processed scene queries are projected into scene tokens Ts = i=1 via simple linear layer, where each si R1D {si}K represents scene token. Text tokenization. We employ text tokenizer to convert the input composite task description into sequence of text i=1, where each xi R1D and denotes tokens Tt = {xi}L the length of the input text. LLM processing. The LLM plays central role in our model by handling multi-modal inputs and understanding both point clouds and human instructions. It further identifies subtask types, solves complex task scheduling problems, generates descriptive action steps, and provides 3D positions of target objects in unified manner. As shown in Fig. 6, the scene tokens are prepended to the text tokens and fed to the LLM, which generates output tokens in an auto-regressive manner. The output tokens include specially designed tokens for task scheduling and 3D grounding, which will be elaborated in the following sections. Algorithm 1: Optimization-based Scheduling Solver (single parallelizable subtask) 1: Input: = {(τi, ci, ti)}n 2: Output: Schedule 3: Split subtasks into one parallelizable subtask τP (if any) and i=1 non-parallelizable set SP SP 4: if no parallelizable subtask then 5: 6: else 7: 8: TP duration of τP Select Sin SP s.t. (cid:80) τiSin mized Purely sequential ti TP and the sum is maxiSout SP Sin Sout + [τP] + Sin + [τP] 9: 10: 11: end if 12: return S"
        },
        {
            "title": "4.2 Scheduling Token Mechanism\nLLMs exhibit strong capabilities in natural language gen-\neration but are generally less effective at solving complex\nmathematical problems. To address this limitation, we in-\ntroduce a special <SCH> token that connects to an external\nsolver to obtain an optimal scheduling list. This list is then\nutilized to guide the LLM for step-wise action generation.\nSpecifically, for a composite task description, the LLM first\nidentifies the parallelizable and non-parallelizable subtasks\n(defined in Sec. 3.1). It then constructs the subtask type infor-\nmation as I = {(τi, ci, ti)}n\ni=1, where ci ∈ {P, P} denotes\nthe subtask type (P: parallelizable, P: non-parallelizable) and\nti is the expected time.",
            "content": "The information is passed to an external optimization solver via the <SCH> token. As defined in Alg. 1, the solver minimizes the total execution time given the subtask types and their expected times. This is formulated as 01 knapsack problem, where the waiting interval of parallelizable subtask plays the role of the capacity and the durations of non-parallelizable subtasks serve as item weights and values, so that the solver maximizes the utilization of the waiting time of parallelizable subtasks while minimizing the overall completion time. simple dynamic programming algorithm is employed to solve this problem. The solver finally returns the optimal schedule of subtask IDs. This scheduling process can be formulated as: = Solver(I), (2) where is the optimal subtask completion schedule, represented as list of subtask IDs. Then, is converted into natural language using predefined templates, then tokenized into Tl by the text tokenizer and concatenated with the preceding tokens to guide the LLM in generating step-wise action descriptions. 3D Grounding Head"
        },
        {
            "title": "4.3\nBesides generating action descriptions, the model also needs\nto simultaneously locate the corresponding target object in\norder to complete the task in the physical world. To achieve\nthis, we use a special <GRU> token to indicate the target\nobject for grounding in the output of LLM. To align with the",
            "content": "dimension of the processed scene queries, all output <GRU> tokens are passed through simple MLP head into = {gj}s j=1, where each gj R1d. The target scene query is selected through max cosine similarity. Specifically, we compute the cosine similarity between gj and each ˆqi. The scene query with the highest probability is selected as the best match one, denoted as Rd. This process can be formulated as: = arg max ˆqi ˆQ (cid:18) gj ˆqi gj ˆqi (cid:19) , (3) The grounding mask is generated by the matched scene query with point cloud features. The mask corresponding to the scene query is computed by taking the dot product between and the point cloud features, followed by sigmoid activation to obtain point mask, which is expressed as: = σ (F q) , (4) where RN is the predicted point mask of target object. Training objectives. For language modeling, we use nexttoken prediction with cross-entropy loss. For grounding, we align grounding tokens and scene queries via similarity matrix and supervise it using binary correspondence matrix with sigmoid focal loss."
        },
        {
            "title": "5.2 Evaluation Metrics\nThe model performance is evaluated across three aspects that\nalign with the challenges of the ORS3D task. For output lan-\nguage quality, we use NLP metrics (METEOR & ROUGE).\nFor 3D grounding accuracy on the target object at each step,\nwe adopt the AP@25% detection metric. Considering that the\ncore aspect of the ORS3D task is scheduling, we introduce\nthe Time Efficiency (TE) metric to measure how well the\nmodel utilizes the time intervals of parallelizable subtasks.\nRather than using raw completion time, TE normalizes the ef-\nficiency of each schedule between a naive sequential baseline\nand the optimal schedule. Formally, the TE of a predicted\ntask schedule is calculated as:",
            "content": "TE = Tworst Tpred Tworst Topt 100%, (5) where Tpred is the total time of the predicted task schedule, Topt is the total time of the ground-truth optimal schedule obtained by the OR solver, and Tworst is the total time when all subtasks are executed sequentially without any parallelism. Method Venue 3D Obj. Det. LLM Language Scheduling 3D Grounding Overall METEOR ROUGE TE Accuracy Commercial LLM/MLLMs (only text input) Gemini DeepSeek-R1 (Guo et al. 2025) GPT-4o - - - - - - Gemini-2.0-flash DeepSeek-V3 GPT-4o 41.67 32.40 49.16 58.48 41.50 62. 24.75 72.63 45.27 Object-level methods (with detected object proposals*) 3D-VisTA (Zhu et al. 2023) PQ3D (Zhu et al. 2024b) LEO (Huang et al. 2024b) Scene-level methods ICCV 23 ECCV 24 ICML 24 Mask3D Mask3D Mask3D - - Vicuna-1B Unsupported 46.61 60.32 45.63 Unsupported 54.90 56.12 Unsupported Grounded 3D LLM (Chen et al. 2024) GRANT (ours) arXiv 24 - - - Vicuna-1B Vicuna-1B 41.96 42. 53.71 62.78 42.46 72.99 34.00 35.38 31.22 36.63 39.15 13.73 14.03 38.14 43.03 53. Table 2: Experiment results on ORS3D-60K test set. We adapt LEO by replacing its LLM with Vicuna-1B for fair comparison. * indicates that these methods require object point clouds from an external 3D detector like Mask3D (Schult et al. 2023). Results are produced by directly providing step-wise schedules as input. Overall is the average of METEOR, ROUGE, TE, and Grounding Accuracy (treating unsupported metrics as 0). Method 3D Obj. Det. Detection Segmentation AP @0.25 AP @0. mIoU 3D-VisTA* PQ3D* Mask3D Mask3D Grounded 3D LLM GRANT (ours) - - 54.90 56. 34.00 35.38 41.88 44.01 23.93 24.79 43.29 46.37 25.56 26.71 Method Acc. Para. subtask Non-para. subtask Scheduling Prec. Recall F1 Prec. Recall F1 TE Grounded 3D LLM 77.14 73.80 50.15 59.72 95.17 82.66 88.48 42.46 LEO 79.73 78.19 41.57 54.28 95.90 87.37 91.43 45.63 GRANT (ours) 84.65 73.82 54.70 62.84 95.94 90.67 93.23 72.99 (a) 3D grounding performance comparison (b) Subtask type recognition and scheduling Table 3: (a) Comprehensive 3D grounding performance. * indicates that these methods require object point clouds from an external 3D detector like Mask3D (Schult et al. 2023). (b) Impact of subtask type recognition on scheduling efficiency. Intuitively, the numerator Tworst Tpred measures the time saved by the model compared to the naive baseline, while the denominator Tworst Topt is the maximum possible saving for that task instance. Thus, TE reflects the fraction of the theoretically achievable time savings that the model actually realizes, with TE = 0% indicating purely sequential execution and TE = 100% matching the optimal schedule."
        },
        {
            "title": "5.3 Main Results",
            "content": "We conduct comprehensive comparison between our proposed GRANT and existing methods on ORS3D-60K dataset, as reported in Tab. 2. We evaluate commercial LLM/MLLMs by providing only the text part of the instructions, as they only support images as visual input. Notably, DeepSeekR1 demonstrates strong performance on task scheduling (TE 72.63%) due to reinforcement learning on mathematical problems. However, these models cannot process point cloud data or directly locate objects in 3D environments, which limits their applicability in embodied scenarios. Object-level methods (Zhu et al. 2023, 2024b) process on object point clouds obtained from 3D detectors like Mask3D (Schult et al. 2023). While these methods achieve high grounding accuracy, they are limited by their inability to handle long textual inputs and generate multimodal outputs. LEO (Huang et al. 2024b) integrates an LLM for enhanced language performance but lacks target object grounding. These methods focus on object-centric 3D understanding, which is insufficient for task-driven embodied scenarios where an agent requires comprehensive understanding of the entire 3D environment. For scene-level methods, we use Grounded 3D LLM (Chen et al. 2024) as the baseline. By introducing the STM, our model achieves substantial gain (30.53%) in task scheduling and further boosts 3D grounding by 1.38%. Overall, our method consistently outperforms baseline methods, validating its effectiveness across language understanding, 3D grounding, and scheduling efficiency. We also compare the 3D target object grounding performance of different models in Tab. 3(a). Object-level methods achieve higher performance due to the use of additional 3D object detectors. However, their performance heavily depends on the detectors capability, introducing extra complexity in preprocessing 3D point clouds and leading to the loss of full scene information. In contrast, scene-level methods offer cleaner alternative for 3D grounding by directly processing the entire scene point cloud, making them more suitable for real-world applications. Accurate recognition of parallelizable subtasks is prerequisite for effective time scheduling. As shown in Tab. 3(b), Setting Language Scheduling 3D Grounding METEOR ROUGE TE Accuracy Method Subtask number Overall Four Five Six Seven No scheduling content + Scheduling content + STM (ours) GT scheduling content 35.60 41.29 42.82 53.34 48.89 55.28 62.78 75.06 21.03 47.04 72.99 90.29 15.95 34.74 35.38 38.52 14.82 14.15 13.40 13.73 PQ3D (Zhu et al. 2024b) LEO (Huang et al. 2024b) 42.14 40.12 36.42 33.91 Grounded 3D LLM (Chen et al. 2024) 54.35 45.13 36.59 36.04 60.23 52.98 52.03 48.70 GRANT (ours) 14.03 38.14 43.03 53.49 (a) Effect of scheduling token mechanism (b) Performance across task difficulty levels # LLM Params. Language Scheduling 3D Grounding METEOR ROUGE TE Accuracy 1B 7B 42.82 45. 62.78 63.55 72.99 73.21 35.38 36.25 Subtask number 4 6 7 10 20 50 Runtime (ms) 1.14 1.28 1.31 1.42 1.49 2. 3.94 (c) Effect of scaling LLM (d) Optimization solver runtime Table 4: Ablation studies. (a) Effect of scheduling token mechanism. (b) Performance across different task difficulty levels. (c) Effect of scaling LLM. (d) Runtime of the optimization solver. proposed ORS3D-60K dataset. Effect of scaling LLM. As summarized in Tab. 4(c), increasing the size of the LLM improves performance across language understanding, task scheduling, and 3D grounding. To balance performance and training cost, we adopt Vicuna1B as the default setting. Solver runtime. As the number of subtasks increases, the optimization solver remains extremely fast (Tab. 4(d)), where even with 50 subtasks the total runtime of the solver stays below 4 ms, introducing virtually no overhead. Qualitative analysis. Fig. 7 shows representative example where our model identifies subtask 2 (microwave operation) as parallelizable and schedules other subtasks during its 30-minute waiting period, saving 29 minutes (39% efficiency gain) compared to sequential execution. The model also accurately localizes target objects with high IoU, demonstrating effective integration of OR-based scheduling and spatial grounding."
        },
        {
            "title": "6 Conclusion\nIn this work, we introduce the ORS3D task that integrates task\nscheduling from operations research with spatial grounding\nfor embodied agents. We construct the ORS3D-60K dataset,\nand propose GRANT, an embodied 3D MLLM equipped with\nthe scheduling token mechanism to generate efficient task\nschedules with grounded actions. Experiments validate the\ncapability of GRANT across language, grounding, and task\nscheduling. We believe our initial work on the OR knowledge-\nintensive scenario can inspire future research to further im-\nprove the complex planning capabilities and multi-modal\nintegration of embodied agents.",
            "content": "Figure 7: qualitative example of GRANT. In the visualized point clouds, yellow shows correct predictions, red indicates false positives, and green marks missed ground truth regions. our model achieves the highest accuracy in subtask type recognition, with substantial improvements in recall and F1score for parallelizable subtasks, which in turn leads to significantly higher time efficiency. Therefore, robust subtask type recognition is critical for enhancing scheduling performance."
        },
        {
            "title": "5.4 Ablation Studies\nEffect of STM. As shown in Tab. 4(a), adding scheduling\ncontent substantially improves performance, demonstrating\nthe importance of explicit scheduling. The proposed STM\nfurther boosts time efficiency by 25.95%, highlighting the\ncritical role of proper scheduling. Using ground-truth con-\nstraints indicates that more accurate subtask recognition en-\nables additional performance gains.",
            "content": "Task difficulty levels. As shown in Tab. 4(b), performance consistently declines as the number of subtasks increases, indicating that all methods degrade with more complex task structures. This trend highlights the increased challenge in task scheduling and reflects the inherent complexity of the Acknowledgments This work was supported by the NSFC (Grant No. 62225603 and 623B2038) and in part by the Hubei Provincial Technology Innovation Program (Grant No. 2024BAA007). References Ahn, M.; Brohan, A.; Brown, N.; Chebotar, Y.; Cortes, O.; David, B.; Finn, C.; Fu, C.; Gopalakrishnan, K.; Hausman, K.; et al. 2022. Do as can, not as say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691. Baruch, G.; Chen, Z.; Dehghan, A.; Dimry, T.; Feigin, Y.; Fu, P.; Gebauer, T.; Joffe, B.; Kurz, D.; Schwartz, A.; et al. 2021. Arkitscenes: diverse real-world dataset for 3d indoor scene understanding using mobile rgb-d data. Proceedings of the Advances in Neural Information Processing Systems. Chen, D. Z.; Chang, A. X.; and Nießner, M. 2020. Scanrefer: 3d object localization in rgb-d scans using natural language. In Proceedings of European Conference on Computer Vision, 202221. Chen, Y.; Ge, Y.; Ge, Y.; Ding, M.; Li, B.; Wang, R.; Xu, R.; Shan, Y.; and Liu, X. 2023. Egoplan-bench: Benchmarking multimodal large language models for human-level planning. arXiv preprint arXiv:2312.06722. Chen, Y.; Sun, Y.; Chen, X.; Wang, J.; Shen, X.; Li, W.; and Zhang, W. 2025. Integrating Chain-of-Thought for Multimodal Alignment: Study on 3D Vision-Language Learning. arXiv preprint arXiv:2503.06232. Chen, Y.; Yang, S.; Huang, H.; Wang, T.; Xu, R.; Lyu, R.; Lin, D.; and Pang, J. 2024. Grounded 3d-llm with referent tokens. arXiv preprint arXiv:2405.10370. Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y.; Wu, Z.; Zhang, H.; Zheng, L.; Zhuang, S.; Zhuang, Y.; Gonzalez, J. E.; Stoica, I.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. Choi, J.-W.; Yoon, Y.; Ong, H.; Kim, J.; and Jang, M. 2024. LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents. Proceedings of the International Conference on Learning Representations. Deng, J.; He, T.; Jiang, L.; Wang, T.; Dayoub, F.; and Reid, I. 2025. 3D-LLaVA: Towards Generalist 3D LMMs with Omni Superpoint Transformer. Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition. Driess, D.; Xia, F.; Sajjadi, M. S. M.; Lynch, C.; Chowdhery, A.; Ichter, B.; Wahid, A.; Tompson, J.; Vuong, Q.; Yu, T.; Huang, W.; Chebotar, Y.; Sermanet, P.; Duckworth, D.; Levine, S.; Vanhoucke, V.; Hausman, K.; Toussaint, M.; Greff, K.; Zeng, A.; Mordatch, I.; and Florence, P. 2023. PaLM-E: An Embodied Multimodal Language Model. In Proceedings of the International Conference on Machine Learning. Duan, J.; Yu, S.; Tan, H. L.; Zhu, H.; and Tan, C. 2022. survey of embodied ai: From simulators to research tasks. IEEE Transactions on Emerging Topics in Computational Intelligence, 6(2): 230244. Fu, H.; Zhang, D.; Zhao, Z.; Cui, J.; Liang, D.; Zhang, C.; Zhang, D.; Xie, H.; Wang, B.; and Bai, X. 2025. Orion: holistic end-to-end autonomous driving framework by visionIn Proceedings of language instructed action generation. IEEE/CVF International Conference on Computer Vision. Guo, D.; Yang, D.; Zhang, H.; Song, J.; Zhang, R.; and Xu, R. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. Nature. Hong, Y.; Zhen, H.; Chen, P.; Zheng, S.; Du, Y.; Chen, Z.; and Gan, C. 2023. 3d-llm: Injecting the 3d world into large language models. Proceedings of the Advances in Neural Information Processing Systems, 2048220494. Hu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; Wang, L.; and Chen, W. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685. Huang, H.; Chen, Y.; Wang, Z.; Huang, R.; Xu, R.; Wang, T.; Liu, L.; Cheng, X.; Zhao, Y.; Pang, J.; et al. 2024a. Chatscene: Bridging 3d scene and large language models with object identifiers. In Proceedings of the Advances in Neural Information Processing Systems. Huang, J.; Yong, S.; Ma, X.; Linghu, X.; Li, P.; Wang, Y.; Li, Q.; Zhu, S.-C.; Jia, B.; and Huang, S. 2024b. An Embodied Generalist Agent in 3D World. In Proceedings of the International Conference on Machine Learning. Huang, K.-C.; Li, X.; Qi, L.; Yan, S.; and Yang, M.-H. 2024c. Reason3d: Searching and reasoning 3d segmentation via large language model. arXiv preprint arXiv:2405.17427. Jia, B.; Chen, Y.; Yu, H.; Wang, Y.; Niu, X.; Liu, T.; Li, Q.; and Huang, S. 2024. SceneVerse: Scaling 3D VisionLanguage Learning for Grounded Scene Understanding. In Proceedings of European Conference on Computer Vision. Jiang, X.; Lu, L.; Shao, L.; and Lu, S. 2024. Multimodal 3D Reasoning Segmentation with Complex Scenes. arXiv preprint arXiv:2411.13927. Kang, W.; Huang, H.; Shang, Y.; Shah, M.; and Yan, Y. 2024. Robin3D: Improving 3D Large Language Model via Robust Instruction Tuning. arXiv preprint arXiv:2410.00255. Kang, W.; Qu, M.; Kini, J.; Wei, Y.; Shah, M.; and Yan, Y. 2025. Intent3D: 3D Object Detection in RGB-D Scans Based on Human Intention. In Proceedings of the International Conference on Learning Representations. Kolodiazhnyi, M.; Vorontsova, A.; Konushin, A.; and Rukhovich, D. 2024a. Oneformer3d: One transformer for In Proceedings of the unified point cloud segmentation. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2094320953. Kolodiazhnyi, M.; Vorontsova, A.; Skripkin, M.; Rukhovich, D.; and Konushin, A. 2024b. UniDet3D: Multi-dataset Indoor 3D Object Detection. arXiv preprint arXiv:2409.04234. Liang, D.; Feng, T.; Zhou, X.; Zhang, Y.; Zou, Z.; and Bai, X. 2025a. Parameter-efficient fine-tuning in spectral domain for point cloud learning. IEEE Transactions on Pattern Analysis and Machine Intelligence. Liang, D.; Hua, W.; Shi, C.; Zou, Z.; Ye, X.; and Bai, X. 2025b. Sood++: Leveraging unlabeled data to boost oriented object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence. Xu, W.; Shi, C.; Tu, S.; Zhou, X.; Liang, D.; and Bai, X. 2025b. unified framework for 3d scene understanding. Advances in Neural Information Processing Systems, 37: 59468 59490. Zhang, Z.; Zhu, Z.; Li, P.; Liu, T.; Ma, X.; Chen, Y.; Jia, B.; Huang, S.; and Li, Q. 2024. Task-oriented sequential grounding in 3d scenes. arXiv preprint arXiv:2408.04034. Zhou, X.; Liang, D.; Tu, S.; Chen, X.; Ding, Y.; Zhang, D.; Tan, F.; Zhao, H.; and Bai, X. 2025. Hermes: unified selfdriving world model for simultaneous 3d scene understanding and generation. In Proceedings of IEEE/CVF International Conference on Computer Vision. Zhu, C.; Wang, T.; Zhang, W.; Chen, K.; and Liu, X. 2024a. Scanreason: Empowering 3d visual grounding with reasoning In Proceedings of European Conference on capabilities. Computer Vision, 151168. Zhu, C.; Wang, T.; Zhang, W.; Pang, J.; and Liu, X. 2025. LLaVA-3D: Simple yet Effective Pathway to Empowering LMMs with 3D-awareness. In Proceedings of IEEE/CVF International Conference on Computer Vision. Zhu, Z.; Ma, X.; Chen, Y.; Deng, Z.; Huang, S.; and Li, Q. 2023. 3D-VisTA: Pre-trained transformer for 3D vision and text alignment. In Proceedings of IEEE/CVF International Conference on Computer Vision, 29112921. Zhu, Z.; Zhang, Z.; Ma, X.; Niu, X.; Chen, Y.; Jia, B.; Deng, Z.; Huang, S.; and Li, Q. 2024b. Unifying 3D VisionLanguage Understanding via Promptable Queries. In Proceedings of European Conference on Computer Vision. Liang, D.; Zhou, X.; Xu, W.; Zhu, X.; Zou, Z.; Ye, X.; Tan, X.; and Bai, X. 2024. Pointmamba: simple state space model for point cloud analysis. In Proceedings of the Advances in Neural Information Processing Systems, volume 37, 32653 32677. Lin, B. Y.; Huang, C.; Liu, Q.; Gu, W.; Sommerer, S.; and Ren, X. 2023. On grounded planning for embodied tasks with language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, 1319213200. Mao, Y.; Zhang, Y.; Jiang, H.; Chang, A.; and Savva, M. 2022. MultiScan: Scalable RGBD scanning for 3D environments with articulated objects. Proceedings of the Advances in Neural Information Processing Systems, 35: 90589071. Ramakrishnan, S. K.; Gokaslan, A.; Wijmans, E.; Maksymets, O.; Clegg, A.; Turner, J.; Undersander, E.; Galuba, W.; Westbury, A.; Chang, A. X.; et al. 2021. Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai. Proceedings of the Advances in Neural Information Processing Systems. Rozenberszki, D.; Litany, O.; and Dai, A. 2022. Languagegrounded indoor 3d semantic segmentation in the wild. In Proceedings of European Conference on Computer Vision, 125141. Schult, J.; Engelmann, F.; Hermans, A.; Litany, O.; Tang, S.; and Leibe, B. 2023. Mask3d: Mask transformer for 3d semantic instance segmentation. In 2023 IEEE International Conference on Robotics and Automation, 82168223. IEEE. Takmaz, A.; Fedele, E.; Sumner, R. W.; Pollefeys, M.; Tombari, F.; and Engelmann, F. 2023. OpenMask3D: OpenVocabulary 3D Instance Segmentation. In Proceedings of the Advances in Neural Information Processing Systems. Wald, J.; Avetisyan, A.; Navab, N.; Tombari, F.; and Nießner, M. 2019. Rio: 3d object instance re-localization in changing indoor environments. In Proceedings of IEEE/CVF International Conference on Computer Vision, 76587667. Wang, T.; Mao, X.; Zhu, C.; Xu, R.; Lyu, R.; Li, P.; Chen, X.; Zhang, W.; Chen, K.; Xue, T.; et al. 2024. EmbodiedScan: Holistic Multi-Modal 3D Perception Suite Towards Embodied AI. Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition. Wang, Z.; Huang, H.; Zhao, Y.; Zhang, Z.; and Zhao, Z. 2023. Chat-3d: Data-efficiently tuning large language model for universal dialogue of 3d scenes. arXiv preprint arXiv:2308.08769. Wu, Z.; Wang, Z.; Xu, X.; Lu, J.; and Yan, H. 2023. Embodied task planning with large language models. arXiv preprint arXiv:2307.01848. Xu, G.; Wang, X.; Zhang, Z.; Cheng, J.; Liao, C.; and Yang, X. 2025a. Igev++: Iterative multi-range geometry encoding volumes for stereo matching. IEEE Transactions on Pattern Analysis and Machine Intelligence. Xu, G.; Wang, Y.; Cheng, J.; Tang, J.; and Yang, X. 2023. Accurate and efficient stereo matching via attention concatenation volume. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(4): 24612474."
        }
    ],
    "affiliations": [
        "Huazhong University of Science and Technology",
        "MiLM Plus, Xiaomi Inc."
    ]
}