{
    "paper_title": "SAGE: A Framework of Precise Retrieval for RAG",
    "authors": [
        "Jintao Zhang",
        "Guoliang Li",
        "Jinyang Su"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Retrieval-augmented generation (RAG) has demonstrated significant proficiency in conducting question-answering (QA) tasks within a specified corpus. Nonetheless, numerous failure instances of RAG in QA still exist. These failures are not solely attributable to the limitations of Large Language Models (LLMs); instead, they predominantly arise from the retrieval of inaccurate information for LLMs due to two limitations: (1) Current RAG methods segment the corpus without considering semantics, making it difficult to find relevant context due to impaired correlation between questions and the segments. (2) There is a trade-off between missing essential context with fewer context retrieved and getting irrelevant context with more context retrieved. In this paper, we introduce a RAG framework (SAGE), to overcome these limitations. First, to address the segmentation issue without considering semantics, we propose to train a semantic segmentation model. This model is trained to segment the corpus into semantically complete chunks. Second, to ensure that only the most relevant chunks are retrieved while the irrelevant ones are ignored, we design a chunk selection algorithm to dynamically select chunks based on the decreasing speed of the relevance score, leading to a more relevant selection. Third, to further ensure the precision of the retrieved chunks, we propose letting LLMs assess whether retrieved chunks are excessive or lacking and then adjust the amount of context accordingly. Experiments show that SAGE outperforms baselines by 61.25% in the quality of QA on average. Moreover, by avoiding retrieving noisy context, SAGE lowers the cost of the tokens consumed in LLM inference and achieves a 49.41% enhancement in cost efficiency on average. Additionally, our work offers valuable insights for boosting RAG."
        },
        {
            "title": "Start",
            "content": "SAGE: Framework of Precise Retrieval for RAG Jintao Zhang Department of Computer Science Tsinghua University zhang-jt24@mails.tsinghua.edu.cn Guoliang Li* Department of Computer Science Tsinghua University liguoliang@tsinghua.edu.cn Jinyang Su Department of Computer Science Tsinghua University sujinyanslip@gmail.com 5 2 0 2 M 3 ] . [ 1 3 1 7 1 0 . 3 0 5 2 : r AbstractRetrieval-augmented generation (RAG) has demonstrated significant proficiency in conducting question-answering (QA) tasks within specified corpus. Nonetheless, numerous failure instances of RAG in QA still exist. These failures are not solely attributable to the limitations of Large Language Models (LLMs); instead, they predominantly arise from the retrieval of inaccurate information for LLMs due to two limitations: (1) Current RAG methods segment the corpus without considering semantics, making it difficult to find relevant context due to impaired correlation between questions and the segments. (2) Theres trade-off between missing essential context with fewer context retrieved and getting irrelevant context with more context retrieved. It is hard to make an ideal balance. In this paper, we introduce RAG framework, named SAGE, designed to overcome these limitations. First, to address the issue of segmentation without considering semantics, we propose to train semantic segmentation model. This model is trained to segment the corpus into semantically complete chunks. Second, to ensure that only the most relevant chunks are retrieved while the irrelevant ones are ignored, we design chunk selection algorithm to dynamically select chunks based on the decreasing speed of the relevance score of chunks, leading to more relevant selection. Third, to further ensure the precision of the retrieved chunks, we propose letting LLMs assess whether retrieved chunks are excessive or lacking and then adjust the amount of context accordingly. Experimental results show that SAGE outperforms baselines by 61.25% in the quality of QA on average. Moreover, by avoiding retrieving noisy context, SAGE lowers the cost of the tokens consumed in LLM inference and achieves 49.41% enhancement in cost efficiency on average. Additionally, our work offers valuable insights for boosting RAG, contributing to the development of more effective RAG systems. I. INTRODUCTION Retrieval-augmented generation (RAG) is technique that enhances generation models ability to answer questions for given corpus by retrieving information related to the question. With the rise and advancement of large language models, RAG has demonstrated remarkable proficiency in QA tasks across both commercial applications and open-source communities [11], [28], [41]. Limitations. Typically, RAG system operates in three distinct phases. First, the given corpus will be segmented into many chunks. Second, in response to specific question, retriever identifies and selects the top most related chunks to use as context. Third, the question, alongside the context, will Guoliang Li is the corresponding author. This paper was supported by National Key R&D Program of China (2023YFB4503600), NSF of China (61925205, 62232009, 62102215), Huawei, TAL education, and Beijing National Research Center for Information Science and Technology (BNRist). Fig. 1. Three motivational examples illustrating the current limitations of precise retrieval for RAG. be inputted into LLM to generate an answer. Therefore, the effectiveness of RAG system heavily relies on three pivotal components: an effective and efficient method for segmenting the corpus into chunks in the first stage, an accurate mechanism for retrieving the most relevant chunks in the second stage, and, finally, LLM proficient in understanding and processing natural language for question answering in the third stage. Apart from the limitations of LLMs, current RAG systems have the following critical limitations. (L1) Ineffective Corpus Segmentation: Often, RAG systems segment the corpus into fixed-length chunks [41], [44] without effectively considering semantic coherence [14]. Consequently, retrieved chunks convey incomplete meanings, leading to incorrect answers. For example, Figure 1 (A) illustrates scenario where semantic-based segmentation is not applied. The Target Chunk (the segment crucial for deriving the correct answer) is segmented into two parts. Such segmentation can make these segments semantically unrelated to the question What is the color of my cats eyes? when assessed independently. Consequently, the probability of retrieving both necessary segments diminishes, leaving the LLM unable to provide the correct answer without the full Target Context. Challenge of addressing (L1): Actually, with the advanced natural language understanding capabilities of LLMs, such as GPT-4 [1], there is good approach to do segmentation. the entire corpus along with segmentaWe can input tion command (\"Please segment the corpus into chunks semantic completely.\") into LLM, and get the chunks. However, such method is impractical due to its high costs and prolonged processing time. For instance, segmenting corpus of 1e6 tokens with GPT-4 could spend more than 90 dollars and about 8 hours to complete. Such requirements are often unrealistic for most applications. We need to design much quicker and more cost-effective solution. (L2) Noisy and Missing Retrieval: Current RAG systems retrieve the top chunks deemed most relevant to the given question. However, this approach often leads to two significant issues: (1) Noisy Retrieval: This refers to instances where irrelevant information, called Noisy Chunks, are retrieved alongside relevant ones, misleading the LLM to produce incorrect responses [7]. Such chunks are unhelpful for answering questions. The problem arises because systems aim to avoid information by retrieving overlooking potentially useful fixed, and often excessive, number (K) of chunks as context for the LLM. For example, as demonstrated in Figure 1 (B), retrieving two chunks instead of one might result in 30% chance that the LLM will incorrectly answer Orange. Here, the first chunk is the Target Chunk, and the second one is Nosiy Chunk. In this case, Setting to 1 could allow the LLM to give the correct answer, but the is set to 2 because it is hard to ensure that the Target Chunk will always be ranked first by the retriever. Such strategy will easily retrieve misleading information that prevents the LLM from getting the correct answer. (2) Missing Retrieval: This issue occurs when the Target Chunk is not among the retrieved chunks, thereby losing crucial context. For example, Figure 1 (C) shows scenario where the retriever ranks the Target Chunk third while is set to 2. Then the Target Chunk is missing in the context, eliminating any chance of correct response. This issue is because ensuring the Target Chunk ranks within the top chunks is difficult for retrievers [7]. Challenge of addressing (L2): seemingly straightforward method is selecting the optimal fixed value for K. However, the trade-off between Noisy Retrieval and Missing Retrieval always exists for fixed K. Specifically, setting larger can increase the likelihood of incorporating Noisy Chunks, leading to more errors from Noisy Retrieval. Conversely, setting small risks losing Target Chunk, resulting in inaccuracies due to Missing Retrieval. It is necessary to devise method to determine the most appropriate value for dynamically, balancing the need to minimize both types of retrieval errors. Our approach. To overcome these limitations, we develop novel RAG framework, named SAGE, which incorporates semantic segmentation, gradient-based chunk selection, and self-feedback of LLMs to facilitate precise retrieval for RAG. SAGE is designed to tackle specific limitations as follows: To overcome (L1), we propose to train lightweight model to rapidly and accurately segment the corpus into semantically coherent chunks, ensuring that the retrieved information is semantically complete and relevant. Moreover, because our segmentation method divides the corpus into the smallest segments with complete semantics, it can minimize the number of context tokens required, thereby lowering the inference cost for the LLM in RAG system. To overcome (L2), we propose to select the most relevant chunks dynamically. Instead of retrieving fixed number of the top chunks, we employ sophisticated model to score each chunk, arranging them in descending order of relevance. We then select the most relevant chunks up to the point where significant drop in relevance scores occurs. This method prioritizes highly relevant chunks, preventing Noisy Chunks from being fed into the LLM. Additionally, to further ensure that the final context contains Target Chunk while excluding Noisy Chunks, we integrate self-feedback mechanism. Specifically, this technique leverages LLM to assess if the retrieved chunks are excessive or insufficient for accurate QA. Based on this assessment, the amount of chunks to be retrieved is adjusted automatically. By overcoming these limitations, SAGE enhances RAG systems capability to retrieve precise context, thereby facilitating the generation of accurate answers. Moreover, through the elimination of semantically incomplete and noisy chunks, SAGE lowers the cost of the tokens consumed during LLM inference. Contributions. Our key contributions are summarized below. (C1) We propose semantic segmentation method that segments corpus into short, semantically coherent chunks quickly, improving the QA capabilities of RAG. (C2) We develop gradient-based chunk selection method that dynamically selects the most relevant chunks while eliminating irrelevant chunks for RAG. (C3) We implement self-feedback mechanism to adjust the number of retrieved chunks, further ensuring the precision of retrieval. (C4) Through detailed experimentation, we demonstrate that our RAG framework outperforms existing baselines in both QA ability and cost-efficiency. (C5) We offer valuable insights into RAG tasks, providing researchers in the field with guidance for developing more effective RAG systems. TABLE NOTATIONS. Description Segmented Chunks Embedding model MLP model used in segmentation model Chunks queried from vector database Chunks after gradient based selection The number of chunks queried from vector database The number of retrieved chunks Price per input/output token of LLM Notation fe() Cs ci, co II. PRELIMINARIES A. Retrieval-augmented generation (RAG) RAG system operates through three principal phases: 2 Fig. 2. Workflow of SAGE, where the (cid:57)(cid:57)(cid:75) inidcates the pipelines of self-feedback. (1) Vector Database Construction: Initially, the selected corpus is divided into segments, or chunks, which are then converted into vector representations using an embedding model. These vector representations are subsequently stored in vector database for later retrieval. (2) Retrieval: Upon receiving question, the same embedding model used in the previous phase converts this question into vector. This question vector then serves to perform query process within the vector database, identifying the top chunks vectors most similar to it, typically determined through the shortest cosine distance. These chunks are then extracted as the context for LLM. (3) Answer Generation: The given question alongside the retrieved chunks is arranged into an appropriate prompt. This prompt is then fed into an LLM, which generates response as the final answer to the question. Through this structured approach, the RAG system could retrieve relevant context from the corpus to enhance the accuracy of answering the question. B. Cost of LLM inference The cost of LLM inference could be quantified by examining how much money is required to obtain answers from LLM. Typically, many RAG systems utilize services from LLM providers, which incur charges based on the volume of input and output tokens processed by designated LLM. For instance, OpenAIs GPT-4 [1] might charge 10 dollars for every one million (1e6) input tokens and 30 dollars for the same quantity of output tokens, respectively. Therefore, the inference cost of LLM is determined by calculating the expenses incurred for both input and output tokens processed by the LLM as follows. Cost = It ci + Ot co (1) Where It and Ot indicate the number of input tokens and output tokens of LLM, respectively. ci and co mean the cost per input token and per output token of the LLM, respectively. C. Cost efficiency Metric in RAG We introduce cost efficiency metric that considers both the quality of QA and the cost associated with LLM inference. The equation is given as follows:"
        },
        {
            "title": "Cost",
            "content": "ef iciency ="
        },
        {
            "title": "Acc\nCost",
            "content": "(2) In the above equation, Acc means accuracy or any other metrics used to evaluate the quality of an answer for given question, such as the F1-Score [37] or BELU-1 [35]. Meanwhile, Cost is derived from the cost Equation 1. Higher cost efficiency implies better performance-to-cost ratio. III. OVERVIEW"
        },
        {
            "title": "We will",
            "content": "introduce the workflow of SAGE as shown in Figure 2. A. Vector Database Creation As illustrated in Figure 2 (A), 1 - 2 , we first employ trained segmentation model to segment each paragraph split by in corpus into short but semantically complete chunks, arranging them is set denoted as T. Following segmentation, 3 - 4 we apply an embedding model, represented as fe( ), to convert the chunks into collection of vector embeddings fe(T). 5 These embeddings are then stored in vector database. Importantly, we maintain record of the mapping between the index of each chunk in and its corresponding vector in fe(T). This allows us to retrieve particular chunk based on its vector embedding easily. B. Retrieval As illustrated in Figure 2 (B), our retrieval process is designed to identify chunks that assist in answering specific question. Initially, 1 - 2 the question received from user is transformed into vector through the embedding model fe( ). Following this, 3 - 4 we proceed to query the vector database to extract the vectors that most closely align with the questions embedding, thereby retrieving the corresponding chunks. Subsequently, 5 - 6 these chunks undergo evaluation by reranking model, which assigns scores based on their relevance to the question. Finally, 7 - 8 , we select the top-ranking chunks that come before the point where significant dip in scores is observed, ensuring that only the chunks most related to the question are chosen for the next phase. We can regard the retrieval process as two stages. The first stage involves simple process of querying vector database, while the second stage scores the retrieved chunks using more complex model. Relying solely on the retriever may not efficiently rank results, while using only reranker can result in high latency. This two-stage recall approach is common technique in real-world information retrieval systems. C. Generation As demonstrated in Figure 2 (C), we integrate the retrieved chunks and the posed question into LLM to generate an answer for the user. Specifically, 1 we craft prompt incorporating both the question and the retrieved chunks, tailored to the questions typebe it multiple-choice or open-ended. 2 - 3 This prepared prompt is then inputted into an LLM to procure the response. Crucially, 4 - 5 we further organize the generated answer alongside the initial question and the retrieved chunks into feedback prompt. The purpose of this feedback prompt is twofold: 1) to evaluate the quality of the answer and 2) to assess whether the selected chunks are whether excessive or insufficient for accurate QA. By submitting the feedback prompt to an LLM, we acquire both the score of the answer and an assessment of the chunks. If the answers score surpasses predetermined threshold, 6 ((cid:57)(cid:57)(cid:75))- 7 it is subsequently returned to the user. Conversely, ) adjustments are made to the value of based on the 6 ( chunks assessment. Following this, 6 ((cid:57)(cid:57)(cid:75)) we go through the gradient-based chunk selection and generation processes again to improve the answer until the score of the answer surpasses the threshold or the feedback loop has been executed three times. IV. SEMANTIC SEGMENTATION A. High-level Idea Corpus segmentation plays crucial role in RAG system. Ineffective segmentation can often result in semantically incomplete chunks, leading to the retrieval of irrelevant and incomplete information [14], resulting in incorrect answers. We also show this observation with an experimental case presented in Section VIII. At present, two segmentation methods are predominantly employed in RAG systems. The first method divides the Fig. 3. Motivation of corpus segmentation. The number in 1 means the chunk ID. Fig. 4. Corpus segmentation model. corpus into segments based on predetermined number of tokens. The second method builds upon the first method by ensuring that each chunk contains complete sentences. Both to produce semantically complete methods frequently fail chunks. Specifically, the first, more straightforward strategy involves segmenting the corpus based on predetermined number of tokens. This method often leads to chunks containing incomplete sentences, thereby undermining the overall coherence and meaning of each chunk. Figure 3-A illustrates this issue, displaying chunks that are filled with incomplete sentences. Consequently, calculating the similarity between users question and these chunks becomes ineffective. Another widely used strategy involves segmenting contiguous sentences less than fixed length into chunk. According to this approach, if chunk exceeds the predetermined length limit, the last sentence is transferred intact to the following chunk instead of being truncated mid-sentence. This ensures that each chunk comprises complete sentences. However, this method can also distort the meaning of chunk. As depicted in Figure 3-B, the first and second chunks are concatenated in the original corpus. Once separated from the first chunk, the pronouns His and He in the second chunk become unclear references. Consequently, computing the similarity between users question and such chunks is also problematic because these chunks lack coherence, impairing the representativeness of their embeddings as well. Increasing the fixed length, as demonstrated in Figure 3-C, can mitigate the issue of chunks having incomplete meanings. However, this adjustment 4 Algorithm 1: Training of the segmentation model Input: Some passages of WikiPedia D, embedding model fe() and MLP model M. 1 Collect sentence pairs = {< s1, s2, label >i} from 2 for each epoch in the training process do 3 for in do 4 5 7 8 s1, s2, label = S; x1, x2 = fe(s1, s2); Score = M(x1, x2, (x1 x2), (x1 x2)); Loss = SE(Score, label); // Mean squared error loss. Update fe(), according to Loss.; may cause each chunk to contain an excessive amount of information. If users question targets specific segment of the corpus, the similarity calculation between the question and these overloaded chunks may be unsuccessful. Furthermore, even retrieving these overloaded chunks, this approach can result in feeding an excessive number of irrelevant tokens into the LLM, interfering with the quality of QA and significantly increasing costs. Note that while employing LLM for segmentation is conceivable approach, it proves to be costly (See Section VII-E). This is because the LLM requires the inputting and outputting of all tokens of the given corpus. Additionally, this method is markedly slow, as the LLMs capacity for parallel processing is limited by its substantial GPU memory and computation requirements. To address these issues, we propose to develop lightweight and effective segmentation model. As illustrated in Figure 3D, chunks segmented by our model ensure that each chunk conveys focused and complete meaning without containing an excessive number of tokens. B. Model Construction As demonstrated in Figure 4, our segmentation model employs structure that integrates an embedding model with Multi-layer Perceptron (MLP) model. The model is structured into three main components. Initially, the embedding model, which utilizes state-of-theart and lightweight design [26], embeds two sentences to generate two vectors, denoted as x1 and x2. Subsequently, feature augmentation module receives x1 and x2, performing operations to subtract and multiply these embeddings, yielding x2. the results of their difference x1 x2 and product x1 The final component, an MLP model, takes both the x1 and x2), to produce score. x2) and (x1 x2, alongside (x1 This score determines whether the two input sentences should be segmented or kept as contiguous part. The decision to x2) in our model was based include both (x1 on the observation that even with context-sensitive embeddings like BERT, embeddings can still meaningfully reflect semantic differences or similarities between sentences [38]. x2) and (x1 C. Model Training"
        },
        {
            "title": "To develop a segmentation model capable of",
            "content": "judging whether two sentences are closely related, it is essential to first gather substantial amount of semantically segmented training 5 data. good source for this is the Wikipedia dataset [12], where almost all passages have been segmented semantically into paragraphs. Typically, sentences that are closely related appear within the same paragraph consecutively, whereas unrelated sentences are found in separate paragraphs. This structure allows for the collection of numerous sentence pairs, each pair comprising two sentences. These pairs are accompanied by label that indicates whether the sentences should be grouped into single chunk. Here, if two sentences are consecutive and within the same paragraph, then lable = 1, representing they should be grouped into the same chunk. Otherwise, lable = 0, suggesting they should be segmented into different chunks. The training procedures are detailed in Algorithm 1. We feed pairs of sentences from the collected dataset into the model sequentially, and learn the parameters of the embedding model and MLP model by adjusting them to fit the output score to the label of each sentence pair, utilizing the gradient descent optimization method. D. Model Inference The inference process of the segmentation model is straightforward. Each two adjacent sentences in the corpus is input into the segmentation model to obtain score. If this score falls below predetermined segmentation score threshold, ss, which ranges between 0 and 1, e.g., 0.5, the sentences are segmented into separate chunks. Conversely, if the score is above or equal to ss, the sentences are retained within the same chunk. Note that the segmentation process for any given corpus is executed swiftly (See Section VII-E), because our lightweight segmentation model can be run in parallel in GPU. For instance, we can gather all pairs of sentences within corpus and organize them into multiple batches, each with size of 512. Subsequently, the segmentation model is called to perform inference in parallel. E. Corpus Segmentation Given corpus, we initially segment it into coarse-grained chunks, each comprising complete sentences, as depicted in Figure 3 (C). Subsequently, we employ our trained segmentation model further to segment these chunks into semantically coherent, fine-grained segments. Specifically, we begin by partitioning the corpus into chunks of approximately tokens in length. Then, our segmentation model evaluates each pair of adjacent sentences in these coarse-grained chunks, as described in Section IV-D, getting the final fine-grained segments. V. GRADIENT-BASED CHUNK SELECTION A. High-level Idea Advanced RAG systems typically leverage reranking model to assess the relevance of chunks to given question. The process involves using the reranking model to assign relevance scores to chunks queried from the vector database, which possesses the shortest embedding distance to the question. Subsequently, the highest-scoring chunks are selected second input is min k, which specifies the minimum number of chunks the algorithm should return. The third input is gradient threshold utilized to identify the significant drop in scores. We aim to select top chunks before decrease rate of among the scored chunks in descending order. The output is Cs, indicating the selected chunks. The algorithm proceeds as follows. Initially, each chunk in is evaluated using the state-of-the-art reranking model, resulting in set of scores S. Then, is sorted in descending order based on S. As second step, we select the top min chunks as the initial Cs, ensuring at least min chunks are chosen. Next, we examine the remaining chunks; if the score of chunk exceeds 1/g times the score of its predecessor, we include it in Cs. The selection process terminates when chunks score does not meet this condition, at which point Cs is returned. In short, our reranking method leverages sophisticated, trainable scoring model and dynamic selection process, enhancing contextual relevance for given questions. VI. LLM SELF-FEEDBACK A. Feedback Loop As depicted in Figure 2 (C), after each QA session 3 conducted by the LLM, we organize self-feedback prompt. This prompt integrates the question with the retrieved chunks, the generated answer, and any additional requests, as illustrated in Figure 6. Referred to as the prompt of self-feedback, its purpose is to evaluate the LLMs current answer based on two criteria: (1) the answers quality score to the question and (2) the suitability of the retrieved chunks - whether it contains redundant chunks or lacks necessary chunks for answering the question. The feedback process yields two outcomes. The first is an evaluation score ranging from 1 to 10. The 1 or 1. second denotes context adjustment, signified as 1 indicates the presence of redundant context adjustment of information within the retrieved chunks, while 1 suggests that additional information is required to answer the question sufficiently. Upon receiving this feedback output, our initial step involves examining the score of answer quality. Should this score meet or exceed threshold of feedback score s, for example, 9, the generated answer is considered acceptable and subsequently presented to the user. If not, the context 1 implies that the minimum numadjustment is considered. ber of retrieved chunks, min k, should be reduced by one, as outlined in Figure 2 (C) 6 ((cid:57)(cid:57)(cid:75)). Conversely, 1 requires an increase in min by one. Following any adjustments to min k, the process delineated in Figure 2 (B) 6 - 8 and Figure 2 (C) is repeated. This feedback loop continues until the feedback score surpasses the threshold or the feedback loop has been executed three times. Summary. We propose to adjust the number of retrieved chunks using LLMs, further addressing the problems of noisy retrieval and missing retrieval. Fig. 5. Two general cases in relevance scores of retrieved segmentations. as context for QA. To minimize the risk of overlooking useful chunks, is often set to large number. However, such an approach can unintentionally include irrelevant chunks. Unnecessary information in the collected chunks can confuse LLMs, making it harder for them to give correct answers. This issue will be further illustrated through some experimental cases in Section VIII. Fortunately, we observe that current state-of-the-art reranking models have the capability to precisely score chunks related to given question, effectively assigning lower scores to the irrelevant ones. Furthermore, Figure 5 shows two general cases of the relevance score of chunks for two articles in dataset. The scoring pattern across chunks often reveals sharp decline before gradual slope. If we only select the top three chunks for Article-1 and one chunk for Article-2, the correct answer is easy to get. We will demonstrate specific cases in Section VIII. This indicates that chunks preceding the sharp drop are more significantly related to the question than those following it. Building on this observation, we propose dynamic selection of chunks based on the gradient of the scores of sorted chunks, rather than sticking to fixed number. We aim to identify the most relevant chunks to given question more accurately. Algorithm 2: Gradient-based Chunk Selection Input: chunks C, Reranking Model R, Retrieval minimum number min k, threshold of gradient g. Output: Retrieved chunks. 1 = R(C); 2 Sort(C, S); // Sort chunks based on scores 3 Cs = C[:, k]; 4 score = C[k 1]; 5 for each in [min k, ) do 6 if S[i] > score/g then Cs.append(C[i]); 7 9 else Break; 10 return Cs; B. Gradient based Selection Our chunk selection algorithm is outlined in Algorithm 2 and corresponds to steps 5 - 8 in Figure 2 (B). This algorithm requires three inputs. The first is C, collection of chunks that are closest in embedding distance to the question. The 6 Fig. 6. Prompt of Self-Feedback. TABLE II EFFECTIVENESS EVALUATION ON NARRATIVEQA DATASET (USING GPT-4O-M I). Metric Model SBERT with SAGE SBERT without SAGE BM25 with SAGE BM25 without SAGE DPR with SAGE DPR without SAGE OpenAI Embedding with SAGE OpenAI Embedding without SAGE ROUGE 27.56% 27.34% 25.93% 22.12% 24.67% 22.94% 26.56% 24.82% BLEU-1 BLEU-4 METEOR 14.56% 12.99% 14.44% 10.95% 12.15% 10.87% 12.74% 11.24% 0.89% 0.94% 0.99% 0.89% 0.99% 0.25% 1.44% 1.16% 13.97% 12.61% 13.61% 11.07% 11.75% 11.12% 11.68% 10.80% Metric TABLE III EFFECTIVENESS EVALUATION ON QUALITY AND QASPER DATASET (USING GPT-4O-M I). Accuracy (QuALITY) 73.14% 72.48% 74.98% 72.18% 74.37% 72.38% F1-Match (QASPER) 40.23% 37.57% 39.95% 37.30% 40.06% 37.41% Model SBERT with SAGE SBERT without SAGE BM25 with SAGE BM25 without SAGE DPR with SAGE DPR without SAGE OpenAI Embedding with SAGE OpenAI Embedding without SAGE 78.30% 75.32% 41.23% 38.94% VII. EXPERIMENTS A. Experimental Setup Large language models. In our experiments, we utilize four LLMs in RAG frameworks, as detailed below. (1) GPT3.5 turbo [5]. Developed by OpenAI, the GPT3.5 Turbo model can comprehend and generate both natural language and code, marking significant advancement in language model capabilities. (2) GPT4 [1]. successor to GPT3.5 turbo, GPT-4 is large, multimodal model provided by OpenAI. It accepts both text and image inputs, generating text outputs with noteworthy accuracy and problem-solving capabilities surpassing those of its predecessor. (2) GPT4-o-mini [32]. It is OpenAIs most advanced and cheapest LLM in the small models category. GPT-4o Mini is multimodal model. It boasts superior intelligence to GPT-"
        },
        {
            "title": "3.5 Turbo while matching its speed, emphasizing efficiency\nalongside enhanced cognitive performance.\n(4) UnifiedQA-3B [19]. Unified QA-3B focuses on ques-\ntion answering (QA) and uses a wide range of QA datasets.\nThis helps it perform really well with different kinds of\nquestions and subjects. Itâ€™s a helpful tool for understanding\nlanguage and creating responses.\nDatasets. We use three widely-used QA datasets: (1) QuAL-\nITY [34]. This dataset comprises multiple-choice questions\nbased on articles around 5,000 tokens each. It assesses rea-\nsoning across entire documents for QA tasks. It features a\nchallenging portion, QuALITYHARD, with questions most\nannotators got wrong under time constraints. We present\naccuracies for both the full dataset and the QuALITYHARD\nsubset. Performance is evaluated via the Accuracy metric.\n(2) QASPER [8]. Spanning 5,049 questions from 1,585 NLP\npapers, QASPER delves into information within full texts.\nAnswers vary,\nincluding answerable/unanswerable, yes/no,\nabstractive, and extractive. Performance is evaluated via the\nF1-Match metric. (3) NarrativeQA [21], [48]. Consisting of\nquestion-answer pairs from books and movie scripts (1,572\ndocuments in total), the NarrativeQA demands a thorough\ngrasp of the narrative for accurate responses, testing com-\nprehension of extensive literary texts. Performance metrics\ninclude BLEU-1, BLEU-4, ROUGE, and METEOR. (4) Trival\nQA [17]. TRIVIA_QA is a reading comprehension dataset\ncontaining over 650K question-answer-evidence triples, where\npassages with a maximum length of 470,719 tokens.\nMetrics. Our evaluation of RAG tasks encompasses two\naspects: QA ability and Cost-efficiency (see Equa-",
            "content": "TABLE IV ABLATION STUDY ON NARRATIVEQA DATASET (USING GPT-4O-M I). Metric Model Naive RAG Naive RAG with Segmentation Naive RAG with Selection Naive RAG with Feedback SAGE ROUGE 28.45% 29.74% 29.15% 30.59% 31.65% BLEU-1 12.73% 13.98% 13.18% 14.89% 15.27% BLEU-4 0.29% 0.33% 0.42% 0.81% 1.70% METEOR 12.73% 12.84% 12.92% 14.34% 14.42% TABLE COMPARISON ON QASPER DATASET (USING GPT-3.5-T O). Metric Model Title+ Abstract BM25 DPR SAGE GPT-3.5 F1-Match GPT-4-o mini F1-Match 16.82% 35.26% 35.73% 41.06% 16.41% 37.30% 37.41% 41.23% TABLE VI COMPARISON ON NARRATIVEQA DATASET (USING UN E DQA-3B). Metric Model BiDAF [21] BM25+BERT [30] Recursively Summarizing Books [48] SAGE +UnifiedQA ROUGE 6.20% 15.5% 21.06% 22.22% METEOR 3.70% 5.0% 10.06% 12.05% TABLE VII COMPARISON ON THE QUALITY DATASET (USING GPT-4). Metric Model Longformer-base [4] DPR+DeBERTaV3-large [34] CoLISA(DeBERTaV3-large) [9] RAPTOR+GPT-4 SAGE +GPT-4 Accuracy in Test Set 39.5% 55.4% 62.3% 82.6% 90.10% Accuracy in Hard Set 35.3% 36.1% 54.7% 76.2% 76.3% tion 2). For assessing QA ability, we utilize different metrics tailored to each dataset: For the QuALITY dataset, which features multiple-choice questions, we measure implementation success using Accuracy, meaning the ratio of correct multiple-choice questions to the total number. In the QASPER and Trival QA datasets, we employ the F1-Match metric [37], considering its diverse question types. For the NarrativeQA dataset, we apply combination of metrics, including ROUGE [27], BLEU-1 and BLEU-4 [35], and METEOR [3], to evaluate comprehensive narrative understanding. Retrievers. We employ four distinct retrievers, most comprising an embedding model and vector database, outlined as follows: (1) OpenAI Embedding [31]. We use an OpenAI embedding model alongside vector database to store and query embeddings. We specifically adopt the text-embedding3-small [33] model paired with Faiss vector database [10]. (2) BM25 [39]. We use the BM25 algorithm, probabilistic information retrieval model that ranks documents based on the term frequency-inverse document frequency (TF-IDF) of query terms appearing in each document, adjusted by the length of the document. This method is highly effective for text retrieval tasks, especially for shorter documents or passages. (3) DPR [18]. We utilize the Dense Passage Retriever (DPR), which leverages dense embedding model to encode passages and queries into the same vector space. The embeddings are then stored in vector database, allowing for efficient and accurate retrieval based on vector similarity. This method has been shown to outperform traditional sparse retrieval techniques in many scenarios. (4) SBERT [38]. We employ the Sentence-BERT (SBERT) embedding model, which fine-tunes BERT using Siamese network structure to generate semantically meaningful sentence embeddings. These embeddings are stored in vector database, enabling quick and precise retrieval based on semantic similarity. SBERT is particularly effective for tasks requiring nuanced understanding of sentence-level semantics. Comparison methods. In our evaluation, we compare variety of methods as detailed below. (1) Naive RAG. This approach divides continuous texts into segments of 200 tokens each, ensuring sentences are not split across different chunks. It then employs LLM alongside the previously described retriever to execute the RAG task. (2) Title+Abstract. This method utilizes the title and abstract of documents as the sole context for retrieval and generation. (3) BM25+BERT [30]. This approach combines the BM25 algorithm for initial retrieval and BERT for re-ranking the retrieved documents based on their relevance to the question, leveraging the strengths of both sparse and dense retrieval methods. (4) Recursively Summarizing Books [48]. This method involves recursively summarizing long documents into shorter, coherent summaries, which are then used as context for retrieval and generation tasks. (6) CoLISA [9]. CoLISA first selects relevant sentences from long passage according to the given question and its multiple options to construct short context; then, it has multiple options that interact within specific question in order to predict the final answer. We use the DeBERTaV3-large [15] language model in this method. (7) BiDAF [21]. The BiDAF model employs bidirectional attention flow mechanism to capture the interactions between the query and the context, enabling more precise QA by understanding the context at multiple levels. (9) Longformer-base [4]. model designed to process long sequenced data, addressing limitation of the Transformer model. Longformer replaces the standard self-attention mechanism with local windowed attention coupled with taskspecific global attention, which scales linearly with sequence length. (9) Raptor [41]. This method innovates retrievalaugmented language models by creating summarization tree for comprehensive document understanding. (10) SAGE. Our 8 TABLE VIII MEMORY USAGE, OFFLINE AND ONLINE LATENCY, AND END-TO-END PERFORMANCE OF SAGE ON LARGE SCALE TRIVIA_QA DATASET IN CONCURRENT ENVIRONMENT. (5X)/(10X) INDICATES FIVE (TIME) TIMES CONCURRENCY. USING GPT4-O-M I. Methods Host memory usage GPU memory usage Naive RAG BM25 + Naive RAG BM25 + SAGE SAGE SAGE (5x) SAGE (10x) 0.580 GB 0.605 GB 4.870 GB 5.170 GB 6.320 GB 7.270 GB 0.000 GB 0.000 GB 2.200 GB 2.200 GB 2.765 GB 3.300 GB Latency of building vector database 0.000s 0.005s 0.005s 0.012s 0.012s 0.012s Latency of segmentation Latency of retrieval Latency of feedback Latency of answering F1-Match 9.696s (1066 tokens/s) 9.696s (1066 tokens/s) 5.070s (644 tokens/s) 16.050s (725 tokens/s) 0.8 + 0.50s 16.050s (725 tokens/s) 0.8 + 1.40s 16.050s (725 tokens/s) 0.8 + 2.25s 0.914s 0.003s 0.502s - - 1.791s 1.831s 1.834s 1.831s 1.817s 1.810s 1.791s 1.794s 1.799s 1.791s 0.704 0.704 0.718 0.724 0.724 0.724 TABLE IX MEMORY USAGE, OFFLINE AND ONLINE LATENCY, AND END-TO-END PERFORMANCE OF SAGE ON LARGE SCALE TRIVIA_QA DATASET IN CONCURRENT ENVIRONMENT. (5X)/(10X) INDICATES FIVE (TIME) TIMES CONCURRENCY. USING UN E DQA-3B. Methods Host memory usage GPU memory usage Naive RAG BM25 + Naive RAG BM25 + SAGE SAGE SAGE (5x) SAGE (10x) 3.256 GB 3.243 GB 13.150 GB 13.150 GB 15.500 GB 17.250 GB 12.20 GB 12.20 GB 15.24 GB 15.61 GB 16.175 GB 16.720 GB Latency of building vector database 0.004s 0.005s 0.005s 0.012s 0.012s 0.012s Latency of segmentation Latency of retrieval Latency of feedback Latency of answering F1-Match 9.696s (1066 tokens/s) 9.696s (1066 tokens/s) 5.070s (644 tokens/s) 16.050s (725 tokens/s) 0.8 + 0.50s 16.050s (725 tokens/s) 0.8 + 1.40s 16.050s (725 tokens/s) 0.8 + 2.25s 0.914s 0.003s 0.502s - - 2.810s 2.793s 2.793s 2.801s 1.089s 1.097s 1.084s 1.091s 1.106s 1.097s 0.652 0.594 0.612 0.671 0.671 0.671 TABLE VERIFICATION OF THE EFFECTIVENESS OF FEATURE AUGMENTATION FOR SEMANTIC SEGMENTATION ON QASPER DATASET. Metric Features (x1), (x2) (x1), (x2), (x1 x2) (x1), (x2), (x1 x2) (x1), (x2), (x1 x2), (x1 x2) Segmentation Accuracy 84.5% 85.6% 88.4% 91.8% TABLE XI COMPARISON OF COST EFFICIENCY (USING GPT-4O-M I). Metric Model BM25 DPR SBERT SAGE Number of tokens 140699 142008 140888 104939 Accuray 65.0% 70.0% 67.5% 75% Relative Cost Efficiency 0.646 0.689 0.670 1.0 RAG framework, which extends the Naive RAG method by incorporating the corpus segmentation technique as in Section IV, the gradient-based chunks selection mechanism as in Section V, and the self-feedback method as in Section VI. The default retriever used in SAGE is OpenAI Embedding. Hyper-parameters. The segmentation score threshold ss, referenced in Section IV-D, is defined at 0.55. The length of coarse-grained discussed in Section IV-E is set to 400. The initial minimum number of retrieved chunks min k, as outlined in Section V-B, is determined to be 7. The gradient threshold discussed in Section V-B is set to 0.3. The feedback score threshold s, discussed in Section VI-A, is set to 9. Environment. All experiments were performed on ubuntu22.04 server with 20-core Intel(R) Xeon(R) 6242R 3.10GHz CPU, Nvidia RTX3090 GPU, and 256GB DDR4 RAM. Fig. 7. Segmentation Overhead Evaluation. B. End-to-End Question Answering Ability In this subsection, we evaluate the QA ability of various methods on different datasets. Exp-1: Effectiveness on NarrativeQA. We conduct an experiment on NarrativeQA dataset, to compare the End-to-End QA ability of different retrievers with and without the help of SAGE. The LLM used in this experiment is GPT4-o-mini. Table II shows the comparision results. We can observe that with the help of SAGE, the performance of retrievers SBERT, BM25, DPR, OpenAI Embedding increase 8.15% in ROUGE, 17.27% in BLEU-1, 81.51% in BLEU-4, 11.89% in METEOR on average. We can find that SAGE is effective for RAG systems with different retrievers. This is because SAGE is effective and orthometric with the embedding model and vector database modules in RAG systems. Exp-2: Effectiveness on QuALITY and QASPER. We conduct an experiment on QuALITY and QASPER datasets, to compare the End-to-End QA ability of different retrievers with and without the help of SAGE. The LLM used in this experiment is GPT4-o-mini. Table III shows the comparison results. For QuALITY dataset, we can observe that 9 the performance of retrievers SBERT, BM25, DPR, OpenAI Embedding increase 2.88% in Accuracy on average. For QASPER dataset, we can observe that the performance of retrievers SBERT, BM25, DPR, OpenAI Embedding increase 6.79% in F1-Match on average. The superior performance of RAG systems with SAGE is also because SAGE is effective and orthometric with the retrievers in RAG. We can find that the performance increment in QuALITY dataset is smaller than QASPER dataset. This is because QuALITY dataset is multiple-choice QA dataset, and Accuracy in multiplechoice QA is easier than F1-Match in open-domain QA for both SAGE and the baselines. Exp-3: Comparison with baselines on QuALITY. (Longformer-base, We conduct an experiment on QuALITY dataset, to compare the End-to-End QA ability of SAGE with other methods. Table VII shows the comparision results between SAGE DPR+DeBERTaV3-large, and CoLISA, and RAPTOR+GPT-4). Note that RAPTOR+GPT-4 is the state-of-the-art method in QuALITY dataset in the up-to-date leaderboard currently [36]. In the normal test set of QuALITY dataset, SAGE outperform these baselines by 128.4%, 62.82%, 44.78%, and 9.2%, respectively. In the hard test set of QuALITY dataset, SAGE outperforms these baselines by 116.1%, 114.1%, 39.49%, and 0.13%, respectively. These results verify the effectiveness of SAGE. We find that SAGE outperforms RAPTOR+GPT-4 in the normal test set more than the hard test set. This is because many questions in the hard set are challenging to answer using RAG methods. For instance, there are some questions that need to be solved by reading the whole corpus and using the elimination method, which can not be solved by only retrieving few information for LLMs. Exp-4: Comparison with baselines on NarrativeQA. We conduct an experiment on NarrativeQA dataset, to compare the End-to-End QA ability of SAGE with other methods. This experiment is conducted using UnifiedQA-3B language model. Table VI shows the comparison results between SAGE and (BiDAF, BM25+BERT, and Recursively Summarizing Books). We can find that SAGE outperforms these baselines by 258.4%, 43.35%, and 5.51% in ROUGE and 225.7%, 141%, and 19.78% in METEOR, respectively. Exp-5: Comparison with baselines on QASPER. We conduct an experiment on QASPER datasets to compare the End-to-End QA ability of SAGE with other methods. This experiment is conducted using GPT-3.5 turbo and GPT4-o-min language models. Table shows the comparison results between SAGE and (Title+Abstract and Raptor). For GPT-3.5 turbo, we find that SAGE outperforms these baselines by 144.1%, 16.45%, and 14.92%, respectively. For GPT4-o-mini, we find that SAGE outperforms these baselines by 151.2%, 10.54%, and 10.21%, respectively. C. Scalability Evaluation Exp-6: Scalability evaluation. We utilize the TRIVIA_QA dataset to test the scalability of SAGE, under varying degrees of concurrency (5x and 10x). We conduct experiments using different language models: the GPT4-o-mini via web interface for Table VIII and the UnifiedQA-3B on our local server for Table IX. Our findings indicate that SAGE maintains superior performance over naive RAG system even at high concurrency levels, with minimal increases in memory usageonly 27% even at 10x concurrency. This efficiency is attributed to the high parallelism capabilities of GPUs, which allow single to handle multiple forward LLM and segmentation model computations simultaneously. Furthermore, the experiments demonstrate that while the retrieval latency increases slightly under higher concurrency (less than two seconds at 10x), the latency for feedback and answering processes remains consistent, regardless of the concurrency level. This stability is due to the effective parallel processing capacities of LLMs on GPUs, which effectively manage real-time latency demands. Additionally, the construction of the vector database and segmentation processes perform only once, showing consistent latency across different concurrency levels, ensuring that the initial setup does not impact the systems overall responsiveness in subsequent operations. D. Ablation Study In this subsection, we will verify the effectiveness of each main module of SAGE. Exp-7: Ablation of each module of SAGE. We conduct an experiment on NarrativeQA dataset, to compare the End-to-End QA ability of Naive RAG, Naive RAG with each main module of SAGE, and SAGE. This experiment is conducted using GPT4-o-min language model. Table IV shows the comparison results. We can find that Naive RAG with Segmentation, Naive RAG with Selection, Naive RAG with Feedback, and SAGE outperform Naive RAG by 4.53%, 2.46%, 7.52% and 11.25% in ROUGE, 9.82%, 3.53%, 16.97% and 19.95% in BLEU-1, 13.79%, 44.83%, 179.31% and 486.21% in BLEU-4, and 0.86%, 1.49%, 12.65% and 13.28% in METEOR. We find that the increments in BLEU-4 and METEOR are smaller than that in ROUGE nad BLEU-4. These results verify the effectiveness of each module of SAGE. We find that SAGE outperforms Naive RAG with each main module. This is because the three modules do not negatively affect each other. Exp-8: Ablation of feature selection. To validate the effectiveness of the feature augmentation in training the segmentation model, we conduct an ablation study comparing the models accuracy with and without the x2. Specifically, we augmented features x1 divided the articles from the QASPER dataset into training set and validation set using an 8:2 ratio. We then trained the segmentation model using various combinations of features on the training set and evaluated the segmentation accuracy on the validation set. As shown in Table X, we find that x2 and x1 10 the segmentation performance is higher when using the feax2) compared to using only x2), (x1 tures (x1), (x2), (x1 (x1), (x2) or any other incomplete feature combinations. E. Cost Efficiency In this subsection, we conduct an experiment to verify the superiority of Cost-efficiency of SAGE. Exp-9: Comparison of Cost-efficiency. We conduct an experiment on QuALITY dataset, to compare the token consuming of LLM, Accuracy of QA, and the Cost-efficiency of SAGE and naive rag systems with different embedding models. The LLM used in this experiment is GPT4-o-mini. Table XI shows the results. We can observe that SAGE outperforms the three baselines by 53.85%, 45.14%, and 49.25% of Cost-efficiency on average. Such improvements mainly because SAGE could achieve better QA performance while saving the input token of LLMs by eliminating noisy chunks. F. Segmentation Overhead and Cost In this subsection, we conduct an experiment to compare the efficiency and money cost of corpus segmentation by our segmentation model and by LLM. Exp-10: Comparison of Segmentation Overhead. We conduct an experiment on three articles sampled from QuALITY, NarrativeQA, and QASPER datasets separately, to compare the time and money consuming of corpus segmentation by SAGE and GPT-4 (See Section I). The timeconsuming and money-consuming of our segmentation model is calculated by rented server with RTX3090 GPU, whose rental price is 5.3 dollars per day [45]. Table 7 shows that our segmentation model saves time by 90.71% and money by 99.69% than using GPT-4 in QuALITY dataset, saves time by 89.43% and money by 99.65% than using GPT-4 in NarrativeQA dataset, and saves time by 91.49% and money by 99.72% than using GPT-4 in QASPER dataset. We can find that our segmentation method can save huge amount of both time and money than using GPT-4. This is because our segmentation model is lightweight, achieving fast inference speed with high parallelism. Specifically, it can do inference fast and only occupies little GPU memory (0.2 GB), allowing it to process batch of 512 sentence pairs in one fast inference. TABLE XII ACCURACY ON QUALITY DATASET WITH DIFFERENT LLMS. Metric Model BM25 DPR SAGE GPT-3.5 Accuracy GPT-4o-mini Accuracy 62.70% 60.4% 64.5% 73.50% 73.0% 77.1% VIII. INSIGHTS OF RAG TASKS We summarize the following insights in RAG tasks: 1) Noisy chunks retrieved considerably undermine the effectiveness of RAG systems (See Exp-13). Fig. 8. case of noisy retrieval. Fig. 9. case of missing retrieval. Fig. 10. case of ineffective corpus segmentation. 2) Non-semantics-based corpus segmentation will impair the effectiveness of RAG systems (See Exp-14). 3) The proficiency level of LLMs plays crucial role in the effectiveness of RAG systems (See Exp-15). 4) Interestingly, embedding models, though useful, are not as important as LLMs (See Exp-15). Exp-11: Case Study of noisy retrieval. Figure 8 demonstrates real case from QuALITY dataset, illustrating the issue of noisy retrieval. For the given question, the correct answer is Option1, with the Target Chunk being ranked second in terms of relevance. However, there are some noisy chunks containing some information supporting the Option2 in the other chunks. When with fewer noisy 10), LLM could choose the correct option. chunks (2 11), the LLM However, when with more noisy chunks (K might be misled into choosing the wrong answer Option2. Exp-12: Case Study of missing retrieval. Figure 9 demonstrates real case from QuALITY dataset, showing the issue of missing retrieval. For the given question Which technology not be developed?, it needs lots of context to use the elimination method to choose correct answer. Therefore, we can find that < 6 will lead to wrong answer, and = 15 will lead to correct answer. From the relevance scores of chunks, we can see that the sorted scores are smooth. With our gradient-based chunk selection algorithm, SAGE will choose more chunks and get the correct answer. Exp-13: Case Study of incomplete chunks. Figure 10 illustrates case showing the issue of ineffective corpus segmentation. In the original corpus, Chunk1 and Chunk2 are consecutive. LLM could produce correct answer if consecutive (Chunk1, Chunk2) are retrieved. However, if using fixed-length segmentation, they may be segmented. Furthermore, the relevance score of Chunk2 is higher than Chunk1, so they are impossible to retrieve together as in the original corpus. Finally, for the given question, it is infeasible to conclude that The moderator asked Gavir to sing tribal song., resulting in wrong answer. Exp-14: Different LLMs and embedding models. To evaluate the effectiveness of different LLMs, Table XII shows the Accuracy of RAG methods using two proficiency levels of LLMs (GPT-3.5-turbo and GPT-4o-mini) on QuALITY dataset. The results show that RAG methods using GPT-4o-mini outperform RAG methods using GPT-3.5-turbo 17.38%, 20.86%, and 19.53% on average. These results indicate the importance of the proficiency level of LLMs in RAG. To evaluate the effectiveness of different embedding models, Tabel II shows the performance of RAG methods with different embedding models. We can find that the performance order is SBERT > OpenAI Embedding > DPR > BM25, but the variance is smaller than that with different LLMs. This result suggests that while embedding models contribute to RAGs performance, they are not as influential as the choice of LLMs. IX. RELATED WORK A. Data Management Our work is general framework to enhance the retrieval accuracy of information retrieval applications. It can be applied to the recent data management works that need to retrieve data according to the embedding distances of retrieved items. For instance, in multimodal retrieval, such as the systems developed by [60] and [25], our framework could improve semantic thereby enhancing alignment and cross-modal consistency, overall retrieval precision. For some efficient machine learning systems-related work, such as [?], [16], [23], [47], [49], [50], [55][57], our framework can be used in conjunction with these works within large models of RAG systems, thus ensuring both inference efficiency and retrieval accuracy. For some systems in databases, such as [43], [52][54], [58], [59], our framework can be utilized to detach unstructured data from databases and use RAG technology for retrieval. For distributed systems like those discussed in [51], integrating our framework could optimize data retrieval processes, enhancing efficiency and reducing operational costs. B. Retrieval-Augmented Generation. The Retrieval-Augmented Generation (RAG) framework is key advancement in natural language processing. It improves how generation systems understand and create text by using extra information from outside sources. RAG works by finding relevant information first and then using it to make better and more relevant text outputs. For example, one study [22] introduced model that efficiently locates pertinent documents to aid in answering questions or verifying facts. This method was much better at these tasks because it used external knowledge. Another important development, Dense Passage Retrieval (DPR) [18], makes it easier to find the right pieces of information in large sets of data. This helps in creating accurate and to-the-point texts, which is the main goal of RAG. Recent works have further refined RAGs effectiveness through various strategies, including prompt engineering [24], [40], [62], rewriting questions [13], [29], adding knowledge basese [2], [61], and iterative answering [6], [42], [46]. However, these methods often overlook the need to segment the corpus semantically and reduce irrelevant context retrieved, that SAGE, aims to solve. In summary, RAG continues to evolve through continuous improvement in merging retrieval and generation. Our system SAGE focuses on the retrieval side. X. CONCLUSION AND FUTURE WORK In this paper, we propose framework of precise retrieval for RAG systems, named SAGE. We propose to train segmentation model to segment given corpus semantically and with low latency. Moreover, we propose chunk selection algorithm to select the most relevant chunks rather than larger fixed number of chunks. Lastly, we propose selffeedback method to enable LLMs to adjust the number of retrieved chunks automatically. Experiments show that SAGE can achieve better QA performance with lower cost of LLM inference than other baselines. Looking ahead, we find three promising directions: (1) Multi-hop retrieval. We find the importance of scenarios where answers come from multiple documents like AiR Baleen [20] and leave comprehensive design for such scenarios as future work. (2) Integration of SAGE with fine-tuning of LLMs. We have found that the proficiency level of LLMs is very important for the performance of RAG system (See Section VIII). However, using the most powerful LLMs, e.g., GPT-4, is expensive. Fine-tuning is simple way to enhance the QA ability of LLM for given corpus. For example, we can generate several batches of question-answer pairs to finetune GPT-3.5-turbo. Then, we might achieve the same QA performance based on the inexpensive LLM. (3) more flexible chunk selection strategy. Currently, SAGE selects top chunks with higher relevance scores. Although SAGE selects dynamic number of chunks, it is still possible there are useless chunks, e.g., the chunk with the highest relevance score is useless. Therefore, more flexible chunk selection strategy might help. For example, we can train LLM smart enough to select relevant chunks directly."
        },
        {
            "title": "REFERENCES",
            "content": "[26] X. Li and J. Li. Angle-optimized text embeddings. arXiv preprint [1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. J. C. Park, and S. Jeong, M. Kang, [2] J. Baek, S. J. Hwang. arXiv preprint Knowledge-augmented language model verification. arXiv:2310.12836, 2023. [3] S. Banerjee and A. Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 6572, 2005. [4] I. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. [5] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [6] X. Cheng, D. Luo, X. Chen, L. Liu, D. Zhao, and R. Yan. Lift yourself up: Retrieval-augmented text generation with self-memory. Advances in Neural Information Processing Systems, 36, 2024. [7] F. Cuconasu, G. Trappolini, F. Siciliano, S. Filice, C. Campagnano, The power of noise: Y. Maarek, N. Tonellotto, and F. Silvestri. Redefining retrieval for rag systems. In SIGIR, pages 719729, 2024. [8] P. Dasigi, K. Lo, I. Beltagy, A. Cohan, N. A. Smith, and M. Gardner. dataset of information-seeking questions and answers anchored in research papers. arXiv preprint arXiv:2105.03011, 2021. [9] M. Dong, B. Zou, Y. Li, and Y. Hong. Colisa: inner interaction via contrastive learning for multi-choice reading comprehension. In European Conference on Information Retrieval, pages 264278. Springer, 2023. [10] M. Douze, A. Guzhva, C. Deng, J. Johnson, G. Szilvasy, P.-E. Mazare, M. Lomeli, L. Hosseini, and H. Jegou. The faiss library. arXiv preprint arXiv:2401.08281, 2024. [11] D. Edge, H. Trinh, N. Cheng, J. Bradley, A. Chao, A. Mody, S. Truitt, and J. Larson. From local to global: graph rag approach to queryfocused summarization. arXiv preprint arXiv:2404.16130, 2024. [12] W. Foundation. Wikimedia downloads. [13] L. Gao, X. Ma, J. Lin, and J. Callan. Precise zero-shot dense retrieval without relevance labels. arXiv preprint arXiv:2212.10496, 2022. [14] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, and H. Wang. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2023. [15] P. He, J. Gao, and W. Chen. Debertav3: Improving deberta using electrastyle pre-training with gradient-disentangled embedding sharing. arXiv preprint arXiv:2111.09543, 2021. [16] Y. Hu, K. Zhao, W. Huang, J. Chen, and J. Zhu. Accelerating transformer pre-training with 2: 4 sparsity. arXiv preprint arXiv:2404.01847, 2024. [17] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. triviaqa: Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. arXiv e-prints, page arXiv:1705.03551, 2017. [18] V. Karpukhin, B. Oguz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, and W.-t. Yih. Dense passage retrieval for open-domain qa. arXiv preprint arXiv:2004.04906, 2020. [19] D. Khashabi, S. Min, T. Khot, A. Sabharwal, O. Tafjord, P. Clark, and H. Hajishirzi. Unifiedqa: Crossing format boundaries with single qa system. arXiv preprint arXiv:2005.00700, 2020. [20] O. Khattab, C. Potts, and M. Zaharia. Baleen: Robust multi-hop reasoning at scale via condensed retrieval. Advances in Neural Information Processing Systems, 34:2767027682, 2021. [21] T. KoË‡cisk`y, J. Schwarz, P. Blunsom, C. Dyer, K. M. Hermann, G. Melis, and E. Grefenstette. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317 328, 2018. [22] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Kuttler, M. Lewis, W.-t. Yih, T. Rocktaschel, et al. Retrievalaugmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:94599474, 2020. [23] B. Li, J. Chen, and J. Zhu. Memory efficient optimizers with 4-bit states. Advances in Neural Information Processing Systems, 36, 2024. [24] G. Li, X. Zhou, and X. Zhao. LLM for data management. Proc. VLDB Endow., 17(12):42134216, 2024. [25] T. Li, X. Yang, Y. Ke, B. Wang, Y. Liu, and J. Xu. Alleviating the In 2024 inconsistency of multimodal data in cross-modal retrieval. IEEE 40th International Conference on Data Engineering (ICDE), pages 46434656. IEEE, 2024. arXiv:2309.12871, 2023. [27] C.-Y. Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pages 7481, 2004. [28] LlamaIndex. Llamaindex, 2024. Available online: https://www. llamaindex.ai/open-source. [29] X. Ma, Y. Gong, P. He, H. Zhao, and N. Duan. Query rewritarXiv preprint ing for retrieval-augmented large language models. arXiv:2305.14283, 2023. [30] X. Mou, M. Yu, B. Yao, C. Yang, X. Guo, S. Potdar, and H. Su. Frustratingly hard evidence retrieval for qa over books. arXiv preprint arXiv:2007.09878, 2020. [31] OpenAI. Guide to embeddings, 2023. Available online: https://platform. openai.com/docs/guides/embeddings. [32] OpenAI. gpt-4o-mini, 2024. Available online: https://openai.com/index/ gpt-4o-mini-advancing-cost-efficient-intelligence/. [33] OpenAI. text-embedding-3-small, 2024. Available online: https://openai. com/index/new-embedding-models-and-api-updates/. [34] R. Y. Pang, A. Parrish, N. Joshi, N. Nangia, J. Phang, A. Chen, V. Padmakumar, J. Ma, J. Thompson, H. He, et al. Quality: Question answering with long input texts, yes! arXiv preprint arXiv:2112.08608, 2021. [35] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318, 2002. [36] QuALITY Team. Quality leadboard, 2024. Available online: https: //nyu-mll.github.io/quality/. [37] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. Squad: 100,000+ arXiv preprint for machine comprehension of text. questions arXiv:1606.05250, 2016. [38] N. Reimers. Sentence-bert: Sentence embeddings using siamese bertnetworks. arXiv preprint arXiv:1908.10084, 2019. [39] S. E. Robertson, S. Walker, S. Jones, M. M. Hancock-Beaulieu, M. Gatford, et al. Okapi at trec-3. Nist Special Publication Sp, 109:109, 1995. [40] P. Sahoo, A. K. Singh, S. Saha, V. Jain, S. Mondal, and A. Chadha. systematic survey of prompt engineering in large language models: Techniques and applications. arXiv preprint arXiv:2402.07927, 2024. [41] P. Sarthi, S. Abdullah, A. Tuli, S. Khanna, A. Goldie, and C. D. Manning. Raptor: Recursive abstractive processing for tree-organized retrieval. arXiv preprint arXiv:2401.18059, 2024. [42] Z. Shao, Y. Gong, Y. Shen, M. Huang, N. Duan, and W. Chen. Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. arXiv preprint arXiv:2305.15294, 2023. [43] J. Sun, J. Zhang, Z. Sun, G. Li, and N. Tang. Learned cardinality estimation: design space exploration and comparative evaluation. Proceedings of the VLDB Endowment, 15(1):8597, 2021. [44] R. Teja. Evaluating the ideal chunk size for rag system llausing llamaindex. maindex. ai/blog/evaluating-the-ideal-chunk-size-for-a-ragsystem-usingllamaindex-6207e5d3fec5, 30:31, 2023. LLAMAi,[Online]. Available: https://www. [45] vast.ai. vast.ai. Available online: https://cloud.vast.ai/. [46] J. Wang and G. Li. Aop: Automated and interactive llm pipeline orchestration for answering complex queries. CIDR, 2025. [47] Z. Wang, J. Chen, and J. Zhu. Efficient backpropagation with variancecontrolled adaptive sampling. arXiv preprint arXiv:2402.17227, 2024. [48] J. Wu, L. Ouyang, D. M. Ziegler, N. Stiennon, R. Lowe, J. Leike, and P. Christiano. Recursively summarizing books with human feedback. arXiv preprint arXiv:2109.10862, 2021. [49] H. Xi, Y. Chen, K. Zhao, K. J. Teh, J. Chen, and J. Zhu. Jetfire: Efficient and accurate transformer pretraining with int8 data flow and per-block quantization. arXiv preprint arXiv:2403.12422, 2024. [50] H. Xi, S. Yang, Y. Zhao, C. Xu, M. Li, X. Li, Y. Lin, H. Cai, J. Zhang, D. Li, J. Chen, I. Stoica, K. Keutzer, and S. Han. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity, 2025. [51] F. Yao, Q. Tao, W. Yu, Y. Zhang, S. Gong, Q. Wang, G. Yu, and J. Zhou. Ragraph: region-aware framework for geo-distributed graph processing. Proceedings of the VLDB Endowment, 17(3):264277, 2023. [52] C. Zhang, G. Li, and T. Lv. HyBench: New Benchmark for HTAP Databases. Proceedings of the VLDB Endowment, 17(5):939951, 2024. [53] C. Zhang, G. Li, J. Zhang, X. Zhang, and J. Feng. Htap databases: survey. IEEE Transactions on Knowledge and Data Engineering, 2024. 13 [54] C. Zhang, J. Lu, P. Xu, and Y. Chen. UniBench: Benchmark for Multi-model Database Management Systems. In TPCTC, pages 723. Springer, 2018. [55] J. Zhang, H. Huang, P. Zhang, J. Wei, J. Zhu, and J. Chen. Sageattention2: Efficient attention with thorough outlier smoothing and per-thread int4 quantization, 2024. [56] J. Zhang, J. Wei, P. Zhang, J. Zhu, and J. Chen. Sageattention: Accurate 8-bit attention for plug-and-play inference acceleration. In International Conference on Learning Representations (ICLR), 2025. [57] J. Zhang, C. Xiang, H. Huang, J. Wei, H. Xi, J. Zhu, and J. Chen. Spargeattn: Accurate sparse attention accelerating any model inference, 2025. [58] J. Zhang, C. Zhang, G. Li, and C. Chai. Autoce: An accurate and In 2023 efficient model advisor for learned cardinality estimation. IEEE 39th International Conference on Data Engineering (ICDE), pages 26212633. IEEE, 2023. [59] J. Zhang, C. Zhang, G. Li, and C. Chai. Pace: Poisoning attacks on learned cardinality estimation. Proceedings of the ACM on Management of Data, 2(1):127, 2024. [60] J. Zheng, M. Liang, Y. Yu, Y. Li, and Z. Xue. Knowledge graph In ICDE, enhanced multimodal transformer for image-text retrieval. pages 7082. IEEE, 2024. [61] X. Zhou, G. Li, Z. Sun, Z. Liu, W. Chen, J. Wu, J. Liu, R. Feng, and G. Zeng. D-bot: Database diagnosis system using large language models. Proc. VLDB Endow., 17(10):25142527, 2024. [62] X. Zhou, Z. Sun, and G. Li. DB-GPT: large language model meets database. Data Sci. Eng., 9(1):102111, 2024."
        }
    ],
    "affiliations": [
        "Department of Computer Science Tsinghua University"
    ]
}