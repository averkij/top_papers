{
    "paper_title": "Voxify3D: Pixel Art Meets Volumetric Rendering",
    "authors": [
        "Yi-Chuan Huang",
        "Jiewen Chan",
        "Hao-Jen Chien",
        "Yu-Lun Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Voxel art is a distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, a differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervision. Our core innovation lies in the synergistic integration of three components: (1) orthographic pixel art supervision that eliminates perspective distortion for precise voxel-pixel alignment; (2) patch-based CLIP alignment that preserves semantics across discretization levels; (3) palette-constrained Gumbel-Softmax quantization enabling differentiable optimization over discrete color spaces with controllable palette strategies. This integration addresses fundamental challenges: semantic preservation under extreme discretization, pixel-art aesthetics through volumetric rendering, and end-to-end discrete optimization. Experiments show superior performance (37.12 CLIP-IQA, 77.90\\% user preference) across diverse characters and controllable abstraction (2-8 colors, 20x-50x resolutions). Project page: https://yichuanh.github.io/Voxify-3D/"
        },
        {
            "title": "Start",
            "content": "Voxify3D: Pixel Art Meets Volumetric Rendering Yi-Chuan Huang Jiewen Chan Hao-Jen Chien Yu-Lun Liu"
        },
        {
            "title": "National Yang Ming Chiao Tung University",
            "content": "5 2 0 2 8 ] . [ 1 4 3 8 7 0 . 2 1 5 2 : r Figure 1. Stylized voxel art with controllable abstraction. Voxify3D converts 3D meshes into stylized voxel art using discrete color palettes, pixel art supervision, and voxel-based radiance fields. This teaser showcases the flexibility and quality of our method. (a) Diverse voxel art outputs across object types and use cases. (b) Comparison of different palette selection methods. (c) Control over the resolution of the voxel grid (20, 30, 50) allows balance of detail and abstraction. (d) The variation in color count (2, 4, 8) shows the impact of palette size on expressiveness. (e) Input-output comparisons on multiple objects demonstrate faithful voxel structure, semantic clarity, and voxel art aesthetics."
        },
        {
            "title": "Abstract",
            "content": "Voxel art is distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervision. Our core innovation lies in the synergistic integration of three components: (1) orthographic pixel art supervision that eliminates perspective distortion for precise voxel-pixel alignment; (2) patch-based CLIP alignment that preserves semantics across discretization levels; (3) palette-constrained Gumbel-Softmax quantization enabling differentiable optimization over discrete color spaces with controllable palette strategies. This integration addresses fundamental challenges: semantic preservation under extreme discretization, pixel-art aesthetics through volumetric rendering, and end-to-end discrete optimization. Experiments show superior performance (37.12 CLIP-IQA, 77.90% user preference) across diverse characters and controllable abstraction (2-8 colors, 20-50 resolutions). Project page: https://yichuanh.github.io/Voxify-3D/ 1. Introduction Voxel art is distinctive form of 3D digital artwork, characterized by its minimalist aesthetic and discrete volumetric 1 niques: (1) Alignment: Perspective projection causes pixelvoxel misalignment, producing blurry gradients during optimization. Prior neural stylization [55, 93] uses perspective rendering, unsuited for discrete art styles. (2) Semantic Preservation: As resolution decreases, critical features (facial details, limb articulation) collapse. Standard perceptual losses on full images fail to capture local semantic importance. (3) Discrete Optimization: Voxel art requires small palettes (2-8 colors), but gradient-based methods produce continuous values. Existing quantization [19] lacks differentiability or user-controllable palette extraction. We present Voxify3D, principled framework addressing these challenges through synergistic technical design. We bridge 3D optimization with 2D pixel art supervision via: (1) six-view orthographic rendering that eliminates perspective distortion for precise alignment; (2) patch-based CLIP loss adapted to preserve semantics across discretization levels; (3) palette-constrained Gumbel-Softmax enabling differentiable discrete optimization with flexible extraction strategies. This integrationnot simple combinationrequires careful synchronization of rendering strategy, loss formulation, and quantization timing. Our two-stage pipeline first initializes coarse voxel geometry and color via neural volume rendering, then refines using orthographic pixel art supervision with semantic and discrete color constraints. Our technical contributions include: Orthographic pixel art supervision. First framework to bridge 2D pixel art with 3D voxel optimization by eliminating perspective misalignment, enabling precise gradient flow for discrete stylization across six canonical views. Resolution-adaptive semantic preservation. Patch-based CLIP formulation maintaining object identity under extreme discretization (20-50), addressing semantic collapse that standard perceptual losses fail to prevent. Palette-constrained differentiable quantization. Endto-end optimization pipeline integrating Gumbel-Softmax with flexible palette extraction (4 strategies), temperature scheduling, and logit-based representation for controllable discrete color spaces (2-8 colors). 2. Related Work 3D Representations: From Pixels to Voxels. Pixel art generation evolved from interpolation [25] and contentaware downscaling [14, 40] to deep learning: paired [37] and unsupervised translation [29, 102], GANs [17, 84], diffusion [3], and vector methods [36, 38, 105]. For 3D, voxel-based methods accelerate neural fields [60, 66, 68] through explicit grids [6, 21, 23, 71, 79, 82, 87], differentiable voxelization [63], unified frameworks [101], hierarchical structures [80], sparse architectures [12], and compression [46, 111]. Multi-scale voxel representations [53], geometry-aware voxel features [90], tensorial decomposition [7], and MVS-based methods [86] further enhance Input Instruct-N2N [30] Ours Input Vox-E [83] Ours Ours Input Blender Figure 2. Existing methods often miss key features in voxelization. While IN2N [30], Vox-E [83], and Blender (Geometry Nodes) generate outputs that are coarse, blurry, or semantically inconsistent, they frequently lose critical elements such as facial features. In contrast, our method preserves structural details and produces visually appealing voxel art with sharp abstraction. structure. Despite its growing popularity in games and digital media, creating high-quality voxel art remains challenging, requiring significant artistic expertise and manual effort. While recent works have achieved promising results in 2D pixel art stylization [3, 17, 28, 84, 102], these techniques do not trivially extend to 3D voxel art. Directly using 2D pixel art for 3D reconstruction faces fundamental obstacles: projection-induced misalignment, multi-view inconsistencies, and ambiguous color representations. Current voxel art generation from 3D meshes is limited. Simple downsampling loses semantic features, yielding overly coarse outputs. Voxel-based neural radiance fields [6, 21, 87] target photorealistic rendering, not stylistic abstraction. Neural editing methods [30, 56, 94] struggle with clean, discrete representations. Procedural tools like Blenders Geometry Nodes require extensive manual tuning and lack unified optimization for discrete color control and semantic preservationboth critical for voxel art aesthetics. As Fig. 2 shows, existing methods miss key features. Voxel art generation poses three interrelated challenges that cannot be addressed by naively combining existing techreconstruction quality. Voxels support geometry processing [16], storage [72], and simulation [62]. Recent feedforward generation achieves scale through structured latents [103], hierarchical diffusion reaching 1024³ [80], cascaded point clouds [110], transformers on voxelized shapes [69], and voxelized SDFs [47]. Unlike 2D stylization or 3D photorealism, we address discrete, palette-constrained voxel art by bridging pixel art supervision with volumetric optimization via orthographic alignment, extending voxel radiance fields [87] with palette quantization. Stylization and Discrete Color Control. Neural 3D stylization progressed from score distillation [75] and CLIP guidance [67] to zero-shot transfer [55], painterly rendering [15, 22, 93, 113], high-resolution generation [10, 52], and local control [13, 18, 26, 58]. Gumbel-Softmax [39, 64, 85] enables discrete optimization in NAS [4, 54], VQ-VAE [88, 91], and neural fields [9, 59]. Score-based generative models [5] provide conditional generation through likelihood matching. Palette methods include 2D quantization [3], 3D color decomposition [43], material extraction [61], vector quantization [33], and interactive editing [44], with alternatives like VQGAN [19] and latent upsampling [65]. In contrast to continuous stylization and fixed codebooks, we integrate Gumbel-Softmax with user-controllable palette extraction (K-means, Max-Min, Median Cut, Simulated Annealing), synchronized scheduling, and logit-based representation for pixel-precise voxel art. Multi-view Supervision and Semantic Preservation. Multi-view consistency uses RL refinement [104], view aggregation [86, 107], and latent diffusion [92]. Orthographic projection serves specialized domains: aerial orthophotos [11, 109], CAD reconstruction [114], and furniture assembly [32]. CLIP [78] enables semantic guidance [8, 20, 42, 49, 70, 73, 89, 95], with text supervision extending to semantic segmentation [99]. Semantic preservation under discretization uses masked autoencoders [50], context-aware transformers [108], semantic structures [51], geometry-aware downsampling [74], and hierarchical upsampling [81]. Unlike perspective stylization or orthographic reconstruction, we combine orthographic rendering with pixel art supervision, designing resolution-adaptive patchbased CLIP loss preventing semantic collapse at 20-50 discretization where image-level losses fail. Applications and Datasets. Mesh generation exploits diffusion and sparse views [2, 31, 35, 48, 52, 57, 96, 98, 103, 106], with character datasets [97, 100]. Game assets require structural decomposition [34], PBR materials [112], and procedural libraries [41, 77]. Fabrication includes LEGO generation [24, 76], Earth voxelization [45], and 3D printing [1]. These inform our evaluation but dont address mesh-to-voxelart conversion with semantic fidelity, palette constraints, and controllable abstraction. 3. Method We propose two-stage framework for converting 3D meshes into stylized voxel art with high fidelity and semantic consistency  (Fig. 3)  . Stage 1 (Sec. 3.1) builds coarse voxel radiance field using DVGO [87] to establish geometric and color foundations. Stage 2 (Sec. 3.2) refines the grid under orthographic pixel-art supervision, with CLIP-based loss (Sec. 3.3) for semantic alignment and depth loss for geometric preservation. To achieve clean abstraction and coherent palette, we replace the RGB grid with learned color-logit grid and apply Gumbel-Softmax for differentiable palette quantization (Sec. 3.4). This pipeline retains abstract details, enforces dominant palette, and conveys the distinctive style of voxel art across resolutions. 3.1. Coarse Voxel Grid Training The first stage adapts DVGO [87] to build coarse voxel representation. Unlike NeRFs using MLPs, DVGO directly optimizes two explicit voxel grids: density grid for spatial occupancy and color grid = (r, g, b) for appearance. This explicit structure enables faster training and efficient rendering. We partition the objects bounding box into grid of resolution (W/cell size)3, where is the canonical orthographic image width (pixels) and cell size is the number of pixels per voxel edge. Each voxel stores density and RGB color c. The rendered color C(r) along camera ray is computed as: C(r) = (cid:88) k= Tkαkck, Tk = exp djδj , k1 (cid:88) j=1 αk = 1 exp(dkδk), (1) where is the number of samples along the ray, dk the density, δk the distance between consecutive samples, Tk the accumulated transmittance, and αk the opacity at sample k. The coarse voxel grid is optimized with: Ltotal = Lrender + λdLdensity + λbLbg, (2) where Lrender minimizes the MSE between rendered and target colors to ensure visual fidelity, Ldensity regularizes the density to suppress noise, prevent near-clip artifacts, and employs total variation (TV) regularization to enforce spatial smoothness, and Lbg uses entropy loss to maintain clear geometry and reduce background artifacts. This stage provides good initialization for color and density. 3.2. Orthographic Pixel Art Fine-tuning To utilize the abstract features and clean edges of pixel art for 3D grid supervision, we fine-tune the voxel space by rendering orthographic projections from six axis-aligned views and Figure 3. Our two-stage voxel art generation pipeline. (a) Coarse voxel grid training: Given 3D mesh, we render multi-view images and optimize voxel-based radiance field (DVGO [87]) using MSE loss to learn coarse RGB and density. (b) Orthographic pixel art fine-tuning: We refine the voxel grid using six orthographic pixel art views, which also serve to extract discrete color palette (e.g., via k-means). Optimization includes appearance, depth, and alpha losses. (c) CLIP-guided optimization: CLIP loss computed over rendered patches and mesh images encourages semantic alignment while being memory-efficient. (d) Differentiable discrete color selection via Gumbel-Softmax: Each voxel stores palette logits. Gumbel-Softmax enables differentiable sampling for end-to-end color optimization, yielding coherent, stylized voxel art. Ldepth = D(r) Dgt1 , where C(r) and D(r) are the rendered color and depth along ray r, Cpixel is the RGB color from the pixel-art supervision, and Dgt is the mesh-projected depth. (4) We also use an alpha loss to suppress density in background regions, enforcing background transparency to avoid floating density artifacts: Lα = Mα α2 , (5) where Mα {0, 1}HW is binary mask from the pixel art alpha channel (1 for background), and α denotes the accumulated ray opacity from volume rendering, which is encouraged to be 0 for background rays to allow full transparency. This encourages transparent regions in the pixel art to remain fully transmissive, preventing the formation of undesired voxels in areas without valid supervision. By leveraging pixel art as the supervision signal, each voxel grid more effectively captures and expresses the most important structural and appearance information. 3.3. CLIP-based Semantic Loss To incorporate semantic supervision, we sample half of the total rays to form patches for computing CLIP-based perceptual loss. During training, we randomly sample patch Figure 4. Perspective vs. Orthographic. (Left) Six-view pixel art pipeline. (Right) Perspective views (red) misalign pixels, while six orthographic views (green) enable precise pixelvoxel alignment. comparing them against pixel art supervision generated by the pixel art generator [102]. This six-view setup compactly covers the major surfaces of the object, while orthographic rendering formulates parallel ray casting ri(t) = oi + td, where oi is the ray origin of pixel pi and is the fixed ray direction. All rays are parallel, ensuring pixel-to-voxel alignment without perspective distortions  (Fig. 4)  ."
        },
        {
            "title": "We apply two foundational losses to supervise geometry",
            "content": "and structure: Lpixel = C(r) Cpixel2 2 (3) 4 rays (opatch, dpatch) from rendered ground-truth mesh images Imesh. Given the rendered patch ˆIpatch and the corresponding mesh-based patch mesh patch, we extract CLIP features [20, 78] and compute perceptual loss via cosine similarity: Lclip = 1 cos (cid:16) (cid:17) CLIP( ˆIpatch), CLIP(I mesh patch) , (6) where cosine similarity is defined as cos(a, b) = a,b b , and CLIP () denotes the CLIP image encoder output. This loss encourages voxel-rendered outputs to remain semantically aligned with the input mesh while supporting stylized abstraction, as illustrated in stage (c) of Fig. 3. 3.4. Discrete Color Selection via Gumbel-Softmax To generate clean and stylized voxel appearances while allowing flexible color selection strategies, we adopt palettebased quantization scheme where each voxel selects color from predefined palette. This palette is extracted from the six-view pixel art images using chosen clustering method before Gumbel-Softmax quantization. Instead of regressing RGB values, each voxel (i, j, k) stores color logit vector λi,j,k RC, where is the number of discrete colors in the predefined palette. During training, Gumbel noise Gi,j,k Gumbel(0, 1) RC is added to produce noisy logits: Yi,j,k = λi,j,k + Gi,j,k, (7) where Yi,j,k,n denotes the noisy logit for the n-th palette color at voxel (i, j, k), with {1, . . . , C}. temperaturecontrolled softmax [39, 64] is then applied: si,j,k,n(τ ) = exp(Yi,j,k,n/τ ) n=1 exp(Yi,j,k,n/τ ) (cid:80)C , (8) where si,j,k,n(τ ) is the probability of selecting the n-th color in the palette for voxel (i, j, k), and τ is the temperature parameter controlling distribution sharpness. In early training, we use the soft distribution si,j,k directly. Later, we switch to the straight-through variant, where the forward pass uses one-hot selection at arg maxn si,j,k, while gradients are backpropagated through the soft weights. We anneal the temperature τ during training to encourage smooth exploration in the early stages and sharper, more discrete selections later. The sampled RGB value is computed as: RGBi,j,k = si,j,k,n cn, (9) (cid:88) n=1 where cn R3 is the n-th color in the palette. After training, we directly select the color with the highest logit:"
        },
        {
            "title": "RGBvoxel",
            "content": "i,j,k = carg max λi,j,k,n, (10) 5 producing fully discrete voxel outputs. This process is illustrated in stage (d) of Fig. 3. To enhance flexibility in stylization, this strategy allows users to choose the color selection method and number of colors, enabling explicit control over both color richness and overall style of the voxel art, making the design process more aligned with practical usage scenarios. 3.5. Loss Summary and Training Procedure The overall loss optimized during fine-tuning is weighted sum of multiple components that jointly supervise pixel-art faithfulness, geometry consistency, semantic alignment, and spatial regularity: Ltotal = λpixelLpixel+λdepthLdepth+λalphaLalpha+λclipLclip, (11) where Lpixel, Ldepth, and Lclip encourage pixel-level accuracy, depth consistency, and semantic alignment, respectively, while Lalpha suppresses background opacity to yield clean silhouettes. In Stage 2, rays are split into two groups: (1) Lpixel, Ldepth, and Lalpha, and (2) Lclip on rendered patches, all computed via volumetric rendering (Eq. (1)). Thus, geometric supervision of the density grid is provided by Lpixel, Ldepth, and Lalpha, while semantic supervision comes from Lclip, which guides voxel appearance toward the intended pixel-art style. 4. Experiments 4.1. Experimental Setup Dataset. We evaluate our method on three mesh datasets: Rodin [97], Unique3D [100], and TRELLIS [103]. Rodin and Unique3D primarily feature character 3D assets with rich semantic details, making them ideal for evaluating voxel abstraction and stylized representation. We also evaluate on diverse categories including architecture and vehicles; see supplementary material for details. Implementation details. Training follows two-stage schedule: (a) Coarse Voxelization: optimize the voxel grid for 8000 iterations to capture global structure; (b) Pixel Art Supervision: fine-tune for 6500 iterations with MSE, Depth, and CLIP losses on six orthographic views, using fixed 80 80 patches randomly sampled each iteration for CLIP loss. In the final 2000 iterations, supervision is applied only to the front view to enhance key abstract features. Gumbel-Softmax sampling is performed over fixed palette, with temperature τ annealed from 1.0 to 0.1. Baseline methods. We compare against: 1. Pixel art to 3D extension: Render the input mesh into images, stylize them into pixel art, then train the original DVGO with these pixel-art images, using the coarse voxel grid as the final output. Figure 5. Qualitative comparisons on character models from the Rodin [97] dataset. We compare our voxel art results with Pixel art to 3D extension, IN2N [30], Vox-E [83], and Blenders voxelization. Our method produces stylized yet consistent voxel representations with pixel art aesthetics. 2. IN2N [30]: Language-guided mesh editing with viewconsistent 3D stylization. 3. Vox-E [83]: Language-to-voxel generation prioritizing semantics over fine geometry. 4. Blender Geometry Nodes: Procedural mesh-to-voxel conversion, fast but without semantic or stylization control. 4.2. Qualitative Comparisons We qualitatively compare our method with Pixel art to 3D extension, IN2N, Vox-E, and Blender on eight character meshes from the evaluation datasets  (Fig. 5)  , with an additional eight groups of comparisons provided in the supplementary material. IN2N preserves coarse structure but suffers from large variations across different guidance images, often failing to produce consistent voxelized results; Vox-E yields smoother volumes yet misses the discrete, blocky style of voxel art; Blender produces clean abstraction through procedural voxelization, which is akin to simple downsampling, but requires manual tuning and lacks semantic alignment. Our method preserves key cues (e.g., ears, eyes) with sharp edges across 2550 resolutions, achieving both expressive stylization and semantic fidelity. Additional results are provided in the supplementary material. Table 1. Average CLIP-IQA scores over all 35 examples. Best scores are highlighted. Method Pixel IN2N Vox-E Blender Ours CLIP-IQA 35.53 23.93 35.02 36.31 37.12 4.3. Quantitative Comparisons To assess stylization fidelity and semantic preservation, we adopt the CLIP-IQA framework. For each character, we use GPT-4 to generate detailed textual description based on the original mesh images, prepended with voxel art of... (e.g., voxel art of pink teddy bear with red bow and heart-shaped feet). We use OpenAIs ViT-B/32 CLIP model and compute the average cosine similarity between each prompt and the rendered images from different methods. As shown in Table 1, the reported CLIP-IQA scores are averaged over all 35 cases. Our method consistently achieves the highest score, demonstrating superior semantic alignment and stylized abstraction across diverse set of character meshes. 4.4. Color Palette Controllability We evaluate the controllability of our discrete palette by varying color counts (2, 3, 4, 8) and extraction methods (K6 Table 3. User studies. (a) 35 examples (72 participants). (b) Color quantization (10 art-trained). (a) Image quality (user votes, %) Metric Abstract Appeal Geometry Ours Others 77.90 22.10 80.36 19.64 96.55 3. (b) Color quantization preference (%) w/o Gumbel w/ Gumbel Preferred 11.11 88.89 4.6. User Study We conducted user study with 72 participants to evaluate our method against four baselines: Pixel Art to 3D extension, IN2N [30], Vox-E [83], and Blender Geometry Nodes. The study included 12 questions in two parts: (1) Stylization Evaluation (35 examples): Participants viewed colored input meshes alongside five voxel outputs, and selected the version with the best abstract detail and voxel art appeal. (2) Geometry Evaluation (4 examples): Participants compared grayscale voxel renderings and judged which better preserved the original shape. As shown in Table 3 (a), our method received the majority of votes across all metrics: 77.90% for abstract detail, 80.36% for visual appeal, and 96.55% for geometry faithfulness, substantially outperforming all baselines. Expert Study on Color Preference. We further conducted focused evaluation on color quantization with 10 arttrained participants, all of whom had formal undergraduate education in art or design. Participants were asked to compare voxel art results with and without Gumbel-Softmax across 10 example pairs, and answered the following two questions: Abstract detail: Which voxel art version most clearly and prominently represents abstract details, such as facial features, clothing, and textures? Voxel art appeal: Which version looks most visually appealing as voxel art character, like something you might see in Minecraft or stylized game? As illustrated in Fig. 9, we present four representative examples comparing results with and without Gumbel-Softmax. Across responses from 10 participants on 10 question pairs, 88.89% favored the with Gumbel-Softmax results for voxelart appeal (Table 3 (b)), confirming its importance in producing dominant tones and clear edges. 5. Conclusion We introduce Voxify3D, novel framework for transforming 3D meshes into stylized voxel art with strong semantic abstraction and structural consistency. By combining coarse voxel optimization, orthographic pixel art supervision, and palette-based color quantization, our method achieves expressive and visually appealing results across variety of Figure 6. Effect of Palette Selection and Color Count. Each row corresponds to different palette extraction method: K-means, Max-Min, Median Cut, and Simulated Annealing. Each column shows increasing color counts (2, 3, 4, 8). Each method produces unique color clustering effects. Table 2. CLIP-IQA ablation across voxel sizes. CLIP loss improves semantic alignment consistently. Voxel Size 25 30 40 50 w/o CLIP Loss w/ CLIP (ours) 40.89 41.35 40.55 41. 38.92 40.07 38.64 40.14 means, Median Cut, Max-Min, Simulated Annealing), as shown in Fig. 6. More examples with additional meshes and settings are in the supplementary material. 4.5. Ablation Study We analyze the impact of each design component by removing key modules one at time, including pixel art supervision, orthographic projection, coarse grid initialization, depth loss, CLIP loss, and Gumbel Softmax  (Fig. 7)  . Each removal consistently leads to degraded quality: blurred abstraction, geometric distortions, and ambiguous colors, highlighting the necessity of each element. We compare the effect of CLIP loss across different voxel sizes. Applying CLIP loss consistently improves semantic alignment across all tested resolutions. This confirms the role of CLIP loss in maintaining character identity under voxel abstraction. 7 Figure 7. Ablation study on model components. We show outputs after removing key modules: pixel art supervision, orthographic projection, grid initialization, depth loss, CLIP loss, and Gumbel Softmax. Each row shows different input; columns compare ablations. The full model yields coherent stylization, while removals cause distortions, color artifacts, or semantic loss. Figure 8. Fabrication: LEGO render. Rendered using KeyShot 2023. Our method extends to LEGO applications, where achieving rich visual results within the limited color palette is crucial for practical fabrication. Figure 9. Ablation user study of Gumbel. Four representative examples comparing results with and without Gumbel-Softmax. Without Gumbel-Softmax, voxel colors become blurred and features less distinct. character assets. Extensive experiments and user studies confirm its advantages over existing baselines in both geometric faithfulness and artistic stylization. In addition to digital results, we further illustrate the fabrication potential of our voxel outputs by rendering them as LEGO-style assemblies  (Fig. 8)  , demonstrating the diverse applications of our work. Limitations and Future Work. Voxify3D struggles with highly intricate shapes, where thin structures or fine facial details may be lost at low voxel resolutions. Future work may explore integrating geometric priors or training strategies to enhance detail preservation and scalability, as well as adopting assembly-aware fabrication strategies inspired by LEGO brick design and connection principles to improve the physical realizability of voxel-based models. 8 Acknowledgements. This research was funded by the National Science and Technology Council, Taiwan, under Grants NSTC 112-2222-E-A49-004-MY2 and 113-2628E-A49-023-. The authors are grateful to Google, NVIDIA, and MediaTek Inc. for their generous donations. Yu-Lun Liu acknowledges the Yushan Young Fellow Program by the MOE in Taiwan."
        },
        {
            "title": "References",
            "content": "[1] Christoph Bader, Dominik Kolb, James Weaver, Sunanda Sharma, Ahmed Hosny, Joao Costa, and Neri Oxman. Making data matter: Voxel printing for the digital fabrication of data across scales and domains. Science advances, 4(5): eaas8652, 2018. 3 [2] Maciej Bala, Yin Cui, Yifan Ding, Yunhao Ge, Zekun Hao, Jon Hasselgren, Jacob Huffman, Jingyi Jin, JP Lewis, Zhaoshuo Li, et al. Edify 3d: Scalable high-quality 3d asset generation. arXiv preprint arXiv:2411.07135, 2024. 3 [3] Alexandre Binninger and Olga Sorkine-Hornung. Sd-πxl: Generating low-resolution quantized imagery via score disIn SIGGRAPH Asia Conference Papers, pages tillation. 112, 2024. 2, 3 [4] Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct neural architecture search on target task and hardware. In International Conference on Learning Representations (ICLR), 2019. 3 [5] Chen-Hao Chao, Wei-Fang Sun, Bo-Wun Cheng, Yi-Chen Lo, Chia-Che Chang, Yu-Lun Liu, Yu-Lin Chang, ChiaPing Chen, and Chun-Yi Lee. Denoising likelihood score matching for conditional score-based data generation. arXiv preprint arXiv:2203.14206, 2022. 3 [6] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In European conference on computer vision, pages 333350. Springer, 2022. 2 [7] Bo-Yu Chen, Wei-Chen Chiu, and Yu-Lun Liu. Improving robustness for joint optimization of camera pose and decomposed low-rank tensorial radiance fields. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 9901000, 2024. 2 [8] Lianggangxu Chen, Xuejiao Wang, Jiale Lu, Shaohui Lin, Changbo Wang, and Gaoqi He. Clip-driven open-vocabulary 3d scene graph generation via cross-modality contrastive learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 27863 27873, 2024. 3 [9] Lei Chen, Yuan Meng, Chen Tang, Xinzhu Ma, Jingyan Jiang, Xin Wang, Zhi Wang, and Wenwu Zhu. Q-dit: Accurate post-training quantization for diffusion transformers. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2830628315, 2025. 3 [10] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for highquality text-to-3d content creation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2224622256, 2023. 3 9 [11] Shihan Chen, Qingsong Yan, Yingjie Qu, Wang Gao, Junxing Yang, and Fei Deng. Ortho-nerf: generating true digital orthophoto map using the neural radiance field from unmanned aerial vehicle images. Geo-spatial Information Science, 28(2):741760, 2025. [12] Yukang Chen, Jianhui Liu, Xiangyu Zhang, Xiaojuan Qi, and Jiaya Jia. Voxelnext: Fully sparse voxelnet for 3d object detection and tracking. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2167421683, 2023. 2 [13] Yingshu Chen, Huajian Huang, Tuan-Anh Vu, Ka Chun Shum, and Sai-Kit Yeung. Stylecity: Large-scale 3d urban scenes stylization. In European conference on computer vision, pages 395413. Springer, 2024. 3 [14] Sungjoon Choi and Munchurl Kim. Content-adaptive image downscaling. In ICCV, pages 261269, 2015. 2 [15] SeungJeh Chung, JooHyun Park, and HyeongYeop Kang. 3dstyleglip: Part-tailored text-guided 3d neural stylization. arXiv preprint arXiv:2404.02634, 2024. 3 [16] David Coeurjolly, Pierre Gueth, and Jacques-Olivier Lachaud. Regularization of voxel art. In ACM SIGGRAPH 2018 Talks, pages 12. 2018. [17] Flavio Coutinho and Luiz Chaimowicz. Generating arXiv preprint pixel art character sprites using gans. arXiv:2208.06413, 2022. Submitted to SBGames 2022. 2 [18] Dale Decatur, Itai Lang, Kfir Aberman, and Rana Hanocka. 3d paintbrush: Local stylization of 3d shapes with cascaded In Proceedings of the IEEE/CVF conscore distillation. ference on computer vision and pattern recognition, pages 44734483, 2024. 3 [19] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1287312883, 2021. 2, 3 [20] Kevin Frans, Lisa Soros, and Olaf Witkowski. Clipdraw: Exploring text-to-drawing synthesis through language-image encoders. Advances in Neural Information Processing Systems, 35:52075218, 2022. 3, 5 [21] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 55015510, 2022. [22] Haruo Fujiwara, Yusuke Mukuta, and Tatsuya Harada. Stylenerf2nerf: 3d style transfer from style-aligned multi-view images. In SIGGRAPH Asia 2024 Conference Papers, pages 110, 2024. 3 [23] Stephan Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien Valentin. Fastnerf: High-fidelity neural rendering at 200fps. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1434614355, 2021. 2 [24] Jiahao Ge, Mingjun Zhou, and Chi-wing Fu. Learn to create simple lego micro buildings. ACM Transactions on Graphics (TOG), 43(6):113, 2024. 3 [25] Timothy Gerstner, Doug DeCarlo, Marc Alexa, Adam Finkelstein, Yotam Gingold, and Andrew Nealen. Pixelated image abstraction with integrated user constraints. Computers & Graphics, 37(5):333347, 2013. 2 [26] Guilherme Gomes Haetinger, Jingwei Tang, Raphael Ortiz, Paul Kanyuk, and Vinicius Azevedo. Controllable neural style transfer for dynamic meshes. In Acm siggraph 2024 conference papers, pages 111, 2024. [27] Google DeepMind. Gemini models: Product overview. https : / / deepmind . google / technologies / gemini/, 2025. Accessed: November 21, 2025. 14, 16, 18 [28] Chu Han, Qiang Wen, Shengfeng He, Qianshu Zhu, Yinjie Tan, Guoqiang Han, and Tien-Tsin Wong. Deep unsupervised pixelization. ACM Transactions on Graphics (SIGGRAPH Asia 2018 issue), 37(6):243:1243:11, 2018. 2 [29] Chu Han, Qiang Wen, Shengfeng He, Qianshu Zhu, Yinjie Tan, Guoqiang Han, and Tien-Tsin Wong. Deep unsupervised pixelization. ACM Transactions on Graphics (TOG), 37(6):111, 2018. 2 [30] Ayaan Haque, Matthew Tancik, Alexei A. Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf: Editing 3d scenes with instructions. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 88748884, 2023. 2, 6, 7, 16 [31] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023. 3 [32] Wentao Hu, Jia Zheng, Zixin Zhang, Xiaojun Yuan, Jian Yin, and Zihan Zhou. Plankassembly: Robust 3d reconstruction from three orthographic views with learnt shape programs. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1849518505, 2023. 3 [33] Siyu Huang, Jie An, Donglai Wei, Jiebo Luo, and Hanspeter Pfister. Quantart: Quantizing image style transfer towards high visual fidelity. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 59475956, 2023. 3 [34] Yuhang Huang, Shilong Zou, Xinwang Liu, and Kai Xu. Part-aware shape generation with latent 3d diffusion of neural voxel fields. IEEE Transactions on Visualization and Computer Graphics, 2025. [35] Zixuan Huang, Mark Boss, Aaryaman Vasishta, James Rehg, and Varun Jampani. Spar3d: Stable point-aware reconstruction of 3d objects from single images. arXiv preprint arXiv:2501.04689, 2025. 3 [36] Yuki Igarashi and Takeo Igarashi. Pixel art adaptation for handicraft fabrication. Computer Graphics Forum (Pacific Graphics 2022), 41(7):489494, 2022. 2 [37] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei Efros. Image-to-image translation with conditional adversarial networks. In CVPR, pages 11251134, 2017. 2 [38] Ajay Jain, Amber Xie, and Pieter Abbeel. Vectorfusion: Textto-svg by abstracting pixel-based diffusion models. arXiv preprint arXiv:2211.13845, 2022. 2 [39] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In Proceedings of the 5th International Conference on Learning Representations (ICLR), 2017. 3, 5 [40] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. arXiv preprint arXiv:1603.08155, 2016. 2 [41] Bumsoo Kim, Sanghyun Byun, Yonghoon Jung, Wonseop Shin, Sareer UI Amin, and Sanghyun Seo. Minecraftify: Minecraft style image generation with text-guided arXiv preprint image editing for in-game application. arXiv:2402.05448, 2024. [42] Jeong Joon Kim, Youngjung Hwang, Jaesung Park, Jaejun Choi, and Taesup Kim. Diffusionclip: Text-guided image manipulation using diffusion models. arXiv preprint arXiv:2110.02711, 2022. 3 [43] Zhengfei Kuang, Fujun Luan, Sai Bi, Zhixin Shu, Gordon Wetzstein, and Kalyan Sunkavalli. Palettenerf: Palette-based appearance editing of neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2069120700, 2023. 3 [44] Jae-Hyeok Lee and Dae-Shik Kim. Ice-nerf: Interactive color editing of nerfs via decomposition-aware weight optimization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 34913501, 2023. 3 [45] Ryan Hardesty Lewis. Voxelizing google earth: pipeline for new virtual worlds. In ACM SIGGRAPH 2024 Labs, pages 12. 2024. 3 [46] Lingzhi Li, Zhen Shen, Zhongshu Wang, Li Shen, and Liefeng Bo. Compressing volumetric radiance fields to 1 mb. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 42224231, 2023. 2 [47] Muheng Li, Yueqi Duan, Jie Zhou, and Jiwen Lu. Diffusionsdf: Text-to-shape via voxelized diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1264212651, 2023. 3 [48] Ming-Feng Li, Yueh-Feng Ku, Hong-Xuan Yen, Chi Liu, Yu-Lun Liu, Albert YC Chen, Cheng-Hao Kuo, and Min Sun. Genrc: Generative 3d room completion from sparse image collections. In European Conference on Computer Vision, pages 146163. Springer, 2024. [49] Yuxuan Li, Robin Rombach, Yiqin Zhang, Xiaohang Zhan, Wenqiang Xu, Patrick Esser, and Bjorn Ommer. Blended latent diffusion: Text-driven editing of natural images. arXiv preprint arXiv:2301.11093, 2023. 3 [50] Yiming Li, Zhiding Yu, Christopher Choy, Chaowei Xiao, Jose Alvarez, Sanja Fidler, Chen Feng, and Anima Anandkumar. Voxformer: Sparse voxel transformer for cameraIn Proceedings of based 3d semantic scene completion. the IEEE/CVF conference on computer vision and pattern recognition, pages 90879098, 2023. 3 [51] Yuan Li, Zhihao Liu, Bedrich Benes, Xiaopeng Zhang, and Jianwei Guo. Svdtree: Semantic voxel diffusion for single image tree reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 46924702, 2024. 3 [52] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 300309, 2023. 3 10 [53] Chin-Yang Lin, Chung-Ho Wu, Chang-Han Yeh, Shih-Han Yen, Cheng Sun, and Yu-Lun Liu. Frugalnerf: Fast convergence for extreme few-shot novel view synthesis without learned priors. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1122711238, 2025. [54] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. In International Conference on Learning Representations (ICLR), 2019. 3 [55] Kunhao Liu, Fangneng Zhan, Yiwen Chen, Jiahui Zhang, Yingchen Yu, Abdulmotaleb El Saddik, Shijian Lu, and Eric Xing. Stylerf: Zero-shot 3d style transfer of neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8338 8348, 2023. 2, 3 [56] Lingjie Liu et al. Editing neural radiance fields by scene operations. In CVPR, 2022. 2 [57] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. Advances in Neural Information Processing Systems, 36, 2024. 3 [58] Richard Liu, Daniel Fu, Noah Tan, Itai Lang, and Rana Hanocka. Wir3d: Visually-informed and geometry-aware 3d shape abstraction. arXiv preprint arXiv:2505.04813, 2025. [59] Weihang Liu, Xue Xian Zheng, Jingyi Yu, and Xin Lou. Content-aware radiance fields: Aligning model complexity with scene intricacy through learned bitwidth quantization. In European Conference on Computer Vision, pages 239 256. Springer, 2024. 3 [60] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Johannes Kopf, and Jia-Bin Huang. Robust dynamic radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1323, 2023. 2 [61] Ivan Lopes, Fabio Pizzati, and Raoul de Charette. Material palette: Extraction of materials from single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 43794388, 2024. 3 [62] Frank Losasso, Frederic Gibou, and Ronald Fedkiw. Simulating water and smoke with an octree data structure. ACM Transactions on Graphics (TOG), 23(3):457462, 2004. 3 [63] Yihao Luo, Yikai Wang, Zhengrui Xiang, Yuliang Xiu, Guang Yang, and ChoonHwai Yap. Differentiable voxelization and mesh morphing. arXiv preprint arXiv:2407.11272, 2024. 2 [64] Chris Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: continuous relaxation of discrete random variables. In Proceedings of the 5th International Conference on Learning Representations (ICLR), 2017. 3, 5 [65] Sachin Menon, Alex Damian, Shijia Hu, Namkug Ravi, and Cynthia Rudin. Pulse: Self-supervised photo upsampling via latent space exploration of generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 24372445, 2020. 3 [66] Andreas Meuleman, Yu-Lun Liu, Chen Gao, Jia-Bin Huang, Changil Kim, Min Kim, and Johannes Kopf. Progressively optimized local radiance fields for robust view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1653916548, 2023. 2 [67] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka. Text2mesh: Text-driven neural stylization for meshes. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 13492 13502, 2022. [68] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 2 [69] Shentong Mo, Enze Xie, Ruihang Chu, Lanqing Hong, Matthias Niessner, and Zhenguo Li. Dit-3d: Exploring plain diffusion transformers for 3d shape generation. Advances in neural information processing systems, 36:6796067971, 2023. 3 [70] Ron Mokady, Amir Hertz, and Amit Bermano. Clipcap: Clip prefix for image captioning. In European Conference on Computer Vision (ECCV), pages 531547. Springer, 2022. 3 [71] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. In ACM Transactions on Graphics (TOG), pages 102:1102:15. ACM, 2022. 2 [72] Ken Museth. Vdb: High-resolution sparse volumes with dynamic topology. ACM Transactions on Graphics (TOG), 32(3):122, 2013. 3 [73] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip: Text-driven manipulation of stylegan imagery. arXiv preprint arXiv:2103.17249, 2021. 3 [74] Sai Karthikey Pentapati, Anshul Rai, Arkady Ten, Chaitanya Atluru, and Alan Bovik. Geoscaler: Geometry and renderingaware downsampling of 3d mesh textures. In 2025 IEEE International Conference on Image Processing (ICIP), pages 10071012. IEEE, 2025. [75] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 3 [76] Ava Pun, Kangle Deng, Ruixuan Liu, Deva Ramanan, Changliu Liu, and Jun-Yan Zhu. Generating physically stable and buildable lego designs from text. arXiv preprint arXiv:2505.05469, 2025. 3 [77] Adarsh Pyarelal, Aditya Banerjee, and Kobus Barnard. Modular procedural generation for voxel maps. In AAAI Fall Symposium, pages 85101. Springer, 2021. 3 [78] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 3, 5 [79] Claudius Reiser, Gernot Riegler, Anton Kaplanyan, and Marc Pollefeys. Kilonerf: Scalable neural radiance fields In Proceedings of the with thousands of tiny mlps. IEEE/CVF International Conference on Computer Vision (ICCV), pages 1432514334, 2021. 2 [80] Xuanchi Ren, Jiahui Huang, Xiaohui Zeng, Ken Museth, Sanja Fidler, and Francis Williams. Xcube: Large-scale 3d generative modeling using sparse voxel hierarchies. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 42094219, 2024. 2, 3 [81] Xuanchi Ren, Yifan Lu, Hanxue Liang, Zhangjie Wu, Huan Ling, Mike Chen, Sanja Fidler, Francis Williams, and Jiahui Huang. Scube: Instant large-scale scene reconstruction using voxsplats. Advances in Neural Information Processing Systems, 37:9767097698, 2024. 3 [82] Katja Schwarz, Axel Sauer, Michael Niemeyer, Yiyi Liao, and Andreas Geiger. Voxgraf: Fast 3d-aware image synthesis with sparse voxel grids. In Advances in Neural Information Processing Systems (NeurIPS), 2022. 2 [83] Etai Sella, Gal Fiebelman, Peter Hedman, and Hadar Averbuch-Elor. Vox-e: Editing 3d scenes by talking to generative models. arXiv preprint arXiv:2306.09322, 2023. 2, 6, 7, 16 [84] Ygor Reboucas Serpa and Maria Andreia Formico Rodrigues. Towards machine-learning assisted asset generation for games: study on pixel art sprite sheets. In 2019 18th Brazilian Symposium on Computer Games and Digital Entertainment (SBGames), pages 219228. IEEE, 2019. 2 [85] Rushi Shah, Mingyuan Yan, Michael Curtis Mozer, and Improving discrete optimisation via decouarXiv preprint Dianbo Liu. pled straight-through gumbel-softmax. arXiv:2410.13331, 2024. [86] Chih-Hai Su, Chih-Yao Hu, Shr-Ruei Tsai, Jie-Ying Lee, Chin-Yang Lin, and Yu-Lun Liu. Boostmvsnerfs: Boosting mvs-based nerfs to generalizable view synthesis in largescale scenes. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. 2, 3 [87] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5459 5469, 2022. 2, 3, 4, 14 [88] Yuhta Takida, Yukara Ikemiya, Takashi Shibuya, Kazuki Shimada, Woosung Choi, Chieh-Hsin Lai, Naoki Murata, Toshimitsu Uesaka, Kengo Uchida, Wei-Hsiang Liao, et al. Hq-vae: Hierarchical discrete representation learning with variational bayes. arXiv preprint arXiv:2401.00365, 2023. 3 [89] Jiaxiang Tang, Yuxuan Zhang, Fanbo Xu, Liyuan Jiang, Bo Dai, Peihao Li, Jianmin Bao, Chen Dong, Ping Luo, et al. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2311.17048, 2023. 3 [90] Tao Tu, Shun-Po Chuang, Yu-Lun Liu, Cheng Sun, Ke Zhang, Donna Roy, Cheng-Hao Kuo, and Min Sun. Imgeonet: Image-induced geometry-aware voxel representation for multi-view 3d object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 69967007, 2023. 2 [91] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. In Advances in Neural discrete representation learning. Neural Information Processing Systems (NeurIPS), 2017. 3 [92] Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. Sv3d: Novel multi-view synthesis and 3d generation from single image using latent video diffusion. In European Conference on Computer Vision, pages 439457. Springer, 2024. [93] Can Wang, Ruixiang Jiang, Menglei Chai, Mingming He, Dongdong Chen, and Jing Liao. Nerf-art: Text-driven neural radiance fields stylization. IEEE Transactions on Visualization and Computer Graphics, 30(8):49834996, 2023. 2, 3 [94] Junfeng Wang et al. Clip-nerf: Text-and-image driven manipulation of neural radiance fields. In CVPR, 2023. 2 [95] Kai-En Wang, Yuxuan Zhang, Yifan Yang, Xiaoyong Shen, and Jiaya Jia. Clip-nerf: Text-and-image driven manipulation of neural radiance fields. arXiv preprint arXiv:2112.05139, 2022. 3 [96] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, and Yu-Gang Jiang. Pixel2mesh: Generating 3d mesh models from single rgb images. In Proceedings of the European conference on computer vision (ECCV), pages 5267, 2018. 3 [97] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang Wen, and Baining Guo. Rodin: generative model for sculpting 3d digital avatars using diffusion. arXiv preprint arXiv:2212.06135, 2022. 3, 5, 6, 14, 16, 18 [98] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems, 36, 2024. 3 [99] Ji-Jia Wu, Andy Chia-Hao Chang, Chieh-Yu Chuang, Chun-Pei Chen, Yu-Lun Liu, Min-Hung Chen, Hou-Ning Image-text coHu, Yung-Yu Chuang, and Yen-Yu Lin. decomposition for text-supervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2679426803, 2024. [100] Kailu Wu, Fangfu Liu, Zhihan Cai, Runjie Yan, Hanyang Wang, Yating Hu, Yueqi Duan, and Kaisheng Ma. Unique3d: High-quality and efficient 3d mesh generation from single image. arXiv preprint arXiv:2405.20343, 2024. 3, 5 [101] Shuang Wu, Songlin Tang, Guangming Lu, Jianzhuang Liu, and Wenjie Pei. Univoxel: Fast inverse rendering by unified voxelization of scene representation. In European Conference on Computer Vision, pages 360376. Springer, 2024. 2 [102] Zongwei Wu, Liangyu Chai, Nanxuan Zhao, Bailin Deng, Yongtuo Liu, Qiang Wen, Junle Wang, and Shengfeng He. Make your own sprites: Aliasing-aware and cell-controllable pixelization. ACM Transactions on Graphics (TOG), 41(6): 116, 2022. 2, 4, 14 [103] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong 12 et al. Gaussiancad: Robust self-supervised cad reconstruction from three orthographic views using 3d gaussian splatting. arXiv preprint arXiv:2503.05161, 2025. 3 Yang. Structured 3d latents for scalable and versatile 3d generation. arXiv preprint arXiv:2412.01506, 2024. 3, [104] Desai Xie, Jiahao Li, Hao Tan, Xin Sun, Zhixin Shu, Yi Zhou, Sai Bi, Soren Pirk, and Arie Kaufman. Carve3d: Improving multi-view reconstruction consistency for diffusion models with rl finetuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 63696379, 2024. 3 [105] Ximing Xing, Haitao Zhou, Chuang Wang, Jing Zhang, Dong Xu, and Qian Yu. Svgdreamer: Text guided svg generation with diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Accepted by CVPR 2024; arXiv preprint arXiv:2312.16476. 2 [106] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024. 3 [107] Jiayu Yang, Ziang Cheng, Yunfei Duan, Pan Ji, and Hongdong Li. Consistnet: Enforcing 3d consistency for multiview images diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 70797088, 2024. 3 [108] Zhu Yu, Runmin Zhang, Jiacheng Ying, Junchen Yu, Xiaohai Hu, Lun Luo, Si-Yuan Cao, and Hui-Liang Shen. Context and geometry aware voxel transformer for semantic scene completion. Advances in Neural Information Processing Systems, 37:15311555, 2024. 3 [109] Dongdong Yue, Xinyi Liu, Yi Wan, Yongjun Zhang, Maoteng Zheng, Weiwei Fan, and Jiachen Zhong. Nerfortho: Orthographic projection images generation based on neural radiance fields. International Journal of Applied Earth Observation and Geoinformation, 136:104378, 2025. [110] LAN Yushi, Shangchen Zhou, Zhaoyang Lyu, Fangzhou Hong, Shuai Yang, Bo Dai, Xingang Pan, and Chen Change Loy. Gaussiananything: Interactive point cloud flow matching for 3d generation. In The Thirteenth International Conference on Learning Representations, 2025. 3 [111] Yu-Ting Zhan, Cheng-Yuan Ho, Hebi Yang, Yi-Hsin Chen, Jui Chiu Chiang, Yu-Lun Liu, and Wen-Hsiao Peng. Cat-3dgs: context-adaptive triplane approach to ratedistortion-optimized 3dgs compression. arXiv preprint arXiv:2503.00357, 2025. 2 [112] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):120, 2024. 3 [113] Yuechen Zhang, Zexin He, Jinbo Xing, Xufeng Yao, and Jiaya Jia. Ref-npr: Reference-based non-photorealistic radiance fields for controllable scene stylization. In Proceedings of the ieee/cvf conference on computer vision and pattern recognition, pages 42424251, 2023. 3 [114] Zheng Zhou, Zhe Li, Bo Yu, Lina Hu, Liang Dong, Zijian Yang, Xiaoli Liu, Ning Xu, Ziwei Wang, Yonghao Dang, A. Overview This supplementary material provides additional details that complement our main paper. We include implementation details (Sec. B) covering our codebase and training architecture, pixel art generator, logit grid initialization, parameter settings, loss design, temperature annealing schedule, crossview inconsistency handling, and palette selection strategies. We also provide experimental information (Sec. C) including our CLIP-IQA evaluation protocol, user study details, expert study on color preference, and run time analysis. Additionally, we present additional qualitative results (Sec. D) with more comparisons against baselines, results with varying palette settings, and results under different voxel sizes. We also provide comparisons with recent voxel art generation methods, including Gemini 3 [27] and Rodin [97]. Finally, we show failure cases and analyze potential future directions (Sec. E). B. Implementation Details Codebase and training architecture. Our implementation builds on DVGO [87]. We adopt two-stage training pipeline. In Stage 1, we follow DVGO to train coarse voxel grid, which initializes both color and density representations. In Stage 2, the input consists of six orthographic views stylized into pixel art. Using orthographic projection, each pixel from the pixel art is directly aligned with the voxel grid, ensuring per-pixel to voxel correspondence. After 4500 iterations, training is restricted to the front view, which typically contains the most salient semantic features (e.g., facial structures), allowing the model to refine key abstract details while maintaining consistency from the earlier multi-view supervision. Pixel art generator."
        },
        {
            "title": "Our pipeline requires stylized pixel art inputs rather than",
            "content": "simple low-resolution downsampling. We adopt the MYOS [102] generator to transform mesh renderings into high-quality pixel art, which preserves sharp boundaries and stylized abstractions. As illustrated in Fig. 10, naıve downsampling produces blurry textures, while MYOS yields pixelated structures with clear edges, better aligned with voxel abstraction. Logit grid initialization. In Stage 2, we initialize each voxels logit vector by the negative distance between its Stage 1 RGB color and the palette entries. This provides stable bias toward closer colors and converges better than random initialization. Parameter settings. We summarize the key training parameters for Stage 1 (voxel grid initialization) and Stage 2 (logit grid optimization). Loss design. We adopt different objectives across the two training stages. Figure 10. Downsample vs. Pixel Art Stage 1 (Coarse voxelization). The voxel grid is optimized with MSE reconstruction loss, regularized by density and background terms: Ltotal = Lrender + λdLdensity + λbLbg, where Lrender is MSE between rendered and target colors, Ldensity applies density regularization and total variation smoothing, and Lbg uses entropy to suppress background noise. This stage provides stable initialization for both color and density. Stage 2 (Pixel-art supervision). The fine-tuning objective combines pixel accuracy, geometry regularization, semantic alignment, and silhouette clarity: Ltotal = λpixelLpixel + λdepthLdepth + λalphaLalpha + λclipLclip. Implementation details. Lpixel (MSE) is up-weighted to ensure faithful color abstraction. Ldepth is scaled by voxel resolution: 20 normally, and increased to 30 after step 4500. Lalpha enforces clean silhouettes via transparency regularization. Lclip is applied until step 6000, using 8080 patches per iteration for semantic alignment. After step 6000, optimization focuses mainly on background transparency (Lalpha), while CLIP loss is disabled. This scheduling ensures early semantic guidance, followed by refinement of geometry and silhouettes. The detailed training parameters are included in Tab. 4, and the specific loss weights are detailed in Tab. 5. Temperature annealing schedule. We apply step-wise annealing schedule for the Gumbel-Softmax temperature τ , gradually lowering it to encourage sharper palette selection as training progresses. The temperature starts high to allow exploration of multiple colors, and progressively decreases to enforce deterministic palette assignments toward convergence. The complete annealing schedule is shown in Tab. 6. Cross-view inconsistency. Supervision from six orthographic views keeps inconsistencies minimal, mostly near 14 Table 4. Training parameters for Stage 1 (left) and Stage 2 (right)."
        },
        {
            "title": "Parameter",
            "content": "Iterations (Niters) Batch size (Nrand) Learning rate (density grid) Learning rate (color grid k0) LR decay step"
        },
        {
            "title": "Value",
            "content": "8000 8192 1 101 1"
        },
        {
            "title": "Parameter",
            "content": "Iterations (Niters) Batch size (Nrand) Learning rate (density grid) Learning rate (logit grid) LR decay step"
        },
        {
            "title": "Value",
            "content": "6500 8192 5 103 1 101 20 Table 5. Loss weights used in our implementation. λpixel 10 λdepth 10 / 20 (30 after 4500 iter) λalpha 20 λclip λb λd 1 (until 6000 iter) 0.5 (Stage 1) 0 default (Stage 1) boundaries. To further refine salient cues, the last 2000 iterations are trained only on the front view (rich in facial details), reinforcing key features while preserving global consistency from earlier multi-view supervision. Palette selection strategies. We explored multiple strategies for extracting compact color palettes from input images: K-means clustering: baseline method that partitions pixels into clusters and uses centroids as representative colors. K-means with rare color boosting: explicitly incorporates infrequent colors to prevent palette collapse into dominant tones. Median cut: recursively splits the RGB space by channel ranges to ensure balanced coverage of color distributions. Maxmin picking: iteratively selects farthest colors in feature space to maximize palette diversity. Simulated annealing: formulates palette extraction as discrete optimization problem, refining palettes via stochastic search. C. Experimental Information CLIP-IQA evaluation protocol. We evaluate stylization fidelity and semantic preservation using CLIP-IQA: GPT-4 generates text prompts (A voxel art of...) from mesh images, and ViT-B/32 CLIP computes cosine similarity with rendered results, averaged over 35 cases. While training employs CLIP loss in an imageimage setting, evaluation is based on GPT-4-generated text prompts. This design ensures that CLIP-IQA reflects semantic fidelity rather than overfitting to the training objective. In addition, we provide visual comparisons and user study to further validate the reliability of the evaluation. User study details. We conducted user study with 72 participants, who were presented with 35 colored voxel art examples and Figure 11. Greyscale examples. grayscale voxel renderings Fig. 11. The interface is illustrated in Fig. 12."
        },
        {
            "title": "Each colored example was accompanied by the following",
            "content": "two questions: Abstract detail: Which voxel art version most clearly and prominently represents abstract details, such as facial features, clothing, and textures? 15 Table 6. Step-wise annealing schedule of the Gumbel-Softmax temperature τ during Stage 2. < 1000 1000 30003999 40004999 50006000 > 6001 1.0 0.8 0. 0.6 0.3 0.1 Figure 12. User study UI. Voxel art appeal: Which version looks most visually appealing as voxel art character, like something you might see in Minecraft or stylized game? For the grayscale examples, participants answered: Geometry preservation: Which grayscale voxel rendering more closely resembles the original 3D mesh in terms of overall geometry? Expert study on color preference. We further conducted focused evaluation on color quantization with 10 art-trained participants, all of whom had formal undergraduate education in art or design. Participants were asked to compare voxel art results with and without Gumbel-Softmax across 10 example pairs, and answered the following two questions: Abstract detail: Which voxel art version most clearly and prominently represents abstract details, such as facial features, clothing, and textures? Voxel art appeal: Which version looks most visually appealing as voxel art character, like something you might see in Minecraft or stylized game? Across responses from 10 participants on 10 question pairs, 88.89% favored the with Gumbel-Softmax results for voxel-art appeal, confirming its importance in producing dominant tones and clear edges. Runtime analysis. On single RTX 4090, Stage 1 (coarse voxelization) finishes in 8.5 minutes and Stage 2 (logit grid optimization with CLIP) in 108 minutes, totaling under 2 hourssubstantially faster than SD-piXL (4h). D. Additional Qualitative Results More comparisons with baselines. In total, we evaluated 35 character models for CLIP-IQA. Here, we additionally present 8 representative examples for qualitative comparison against the baselines: Pixel art to 3D extension, IN2N [30], Vox-E [83], and Blender Geometry Nodes, as illustrated in Fig. 13. While IN2N [30] is effective in certain cases, we found it often fails in our setting. This is mainly because each guidance image used during training can differ significantly, leading to large inconsistencies across views. Results with varying palette settings. As shown in Fig. 14, we present comparisons under different color selection strategies and palette sizes, with K-means adopted as our default palette extraction method. Results under different voxel sizes. Fig. 15 illustrates voxel art renderings generated with varying voxel resolutions, demonstrating how grid granularity influences the level of abstraction, sharpness of edges, and overall visual fidelity of the outputs. Comparison with LLM-based voxel generation. We also compare with Gemini 3 [27], the latest state-of-the-art large language model, which can generate 3D voxel art through code generation in AI Studio. As shown in Fig. 16, while Gemini 3 excels at creating interactive voxel-based applications and can produce detailed voxel art through its advanced coding capabilities, it lacks precise control over abstraction details, resolution, and color palette selection. In contrast, our method enables fine-grained control over voxel resolution and palette constraints while faithfully preserving the visual characteristics through multi-view optimization. This demonstrates the advantage of Voxify3D for controllable and appearance-faithful voxel art generation. Comparison with single-image 3D reconstruction. We also compare with Rodin [97], which performs well for image-to-mesh generation but is not designed for voxel art. Figure 13. Additional qualitative comparisons with baselines. Eight representative examples compared with Pixel, IN2N, Vox-E, and Blender Geometry Nodes. Figure 14. Results with varying palette settings. Examples using different palette extraction strategies and palette sizes. As shown in Fig. 17, Rodin sometimes produces non-voxel outputs (right), and due to the single-image input, it often fails to capture reliable depth, resulting in flat structures (left). This further underscores the benefit of our multi-view voxel optimization pipeline. E. Failure cases and analysis Finally, representative failure cases are shown in Fig. 18, mainly arising from complex shapes that exceed the capacity of the limited voxel resolution. These examples suggest that voxel art is better suited for capturing abstract details conveyed through color patterns and tonal contrasts, whereas fine-grained geometric intricacies are more likely to be lost under coarse discretization. promising future direction is to adopt adaptive voxel resolutions, where regions requiring fine details use smaller voxels while simpler areas maintain coarser ones, enabling better preservation of geometric complexity without sacrificing the aesthetic appeal of voxel art. 17 Figure 15. Results under different voxel sizes. Figure 16. Comparison with Gemini 3 [27]. While Gemini 3 can generate voxel art through code, it lacks precise control over resolution, palette, and visual fidelity to input references. Figure 17. Comparison with Rodin [97]. Rodin excels at image-to-mesh but is not tailored for voxel art, often yielding non-voxel outputs (right) or flat geometry (left). 18 Figure 18. Representative failure cases. Complex shapes with finegrained geometric details are difficult to represent under limited voxel resolution, resulting in loss of intricate structures."
        }
    ],
    "affiliations": []
}