{
    "paper_title": "Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging",
    "authors": [
        "Shiqi Chen",
        "Jinghan Zhang",
        "Tongyao Zhu",
        "Wei Liu",
        "Siyang Gao",
        "Miao Xiong",
        "Manling Li",
        "Junxian He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language Models (VLMs) combine visual perception with the general capabilities, such as reasoning, of Large Language Models (LLMs). However, the mechanisms by which these two abilities can be combined and contribute remain poorly understood. In this work, we explore to compose perception and reasoning through model merging that connects parameters of different models. Unlike previous works that often focus on merging models of the same kind, we propose merging models across modalities, enabling the incorporation of the reasoning capabilities of LLMs into VLMs. Through extensive experiments, we demonstrate that model merging offers a successful pathway to transfer reasoning abilities from LLMs to VLMs in a training-free manner. Moreover, we utilize the merged models to understand the internal mechanism of perception and reasoning and how merging affects it. We find that perception capabilities are predominantly encoded in the early layers of the model, whereas reasoning is largely facilitated by the middle-to-late layers. After merging, we observe that all layers begin to contribute to reasoning, whereas the distribution of perception abilities across layers remains largely unchanged. These observations shed light on the potential of model merging as a tool for multimodal integration and interpretation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 4 6 4 5 0 . 5 0 5 2 : r Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging Shiqi Chen * 1 Jinghan Zhang * 2 Tongyao Zhu 3 Wei Liu 2 Siyang Gao 1 Miao Xiong 3 Manling Li 4 Junxian He"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Vision-Language Models (VLMs) combine visual perception with the general capabilities, such as reasoning, of Large Language Models (LLMs). However, the mechanisms by which these two abilities can be combined and contribute remain poorly understood. In this work, we explore to compose perception and reasoning through model merging that connects parameters of different models. Unlike previous works that often focus on merging models of the same kind, we propose merging models across modalities, enabling the incorporation of the reasoning capabilities of LLMs into VLMs. Through extensive experiments, we demonstrate that model merging offers successful pathway to transfer reasoning abilities from LLMs to VLMs in training-free manner. Moreover, we utilize the merged models to understand the internal mechanism of perception and reasoning and how merging affects it. We find that perception capabilities are predominantly encoded in the early layers of the model, whereas reasoning is largely facilitated by the middle-to-late layers. After merging, we observe that all layers begin to contribute to reasoning, whereas the distribution of perception abilities across layers remains largely unchanged. These observations shed light on the potential of model merging as tool for multimodal integration and interpretation. Our code is publicly available at: https://github.com/shiqichen17/VLM Merging. *Equal contribution 1City University of Hong Kong 2Hong Kong University of Science and Technology 3National University of Singapore 4Northwestern University. Correspondence to: Shiqi Chen <schen438-c@my.cityu.edu.hk>, Jinghan Zhang <jzhangjv@cse.ust.hk>, Junxian He <junxianh@cse.ust.hk>. Proceedings of the 41 st International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). 1 Multimodal reasoning is crucial for variety of important applications, such as interpreting charts and figures in scientific publications and government reports. Despite the great successes of Vision-Language Models (VLMs) in tasks requiring perceptual and linguistic integration (Li et al., 2024; Liu et al., 2024; Bai et al., 2023; Wang et al., 2024b), these models struggle with complex multimodal reasoning tasks (Lu et al., 2024; Zhang et al., 2024b). This limitation partly due to the scarcity of multimodal reasoning data leaves them lagging far behind their language model counterpart which has made remarkable advancement in reasoning tasks (Yang et al., 2024; DeepSeek-AI et al., 2025). Perception and reasoning are two fundamental components in this context. While language models primarily represent the reasoning ability, VLMs demand both to succeed. Therefore, it is natural to ask: can we incorporate the reasoning ability of LMs into VLMs? Achieving such combination is challenging, as the interaction between perception and reasoning within VLMs remains poorly understood. In this work, we investigate these questions through the lens of model merging (Ilharco et al., 2023), straightforward approach to explore whether perception and reasoning can be combined across modalities and how these two abilities are embedded within VLMs. Concretely, model merging generates new model by performing arithmetic operations on the parameters of existing models, without requiring additional training. This strategy works based on the assumption that models fine-tuned from shared initialization reside in connected subspace of the parameter space. While previous works on model merging focus on models of the same kind (Yadav et al., 2023; Yu et al., 2024b), it remains unknown that whether models across different modalities are connectable to yield benefits. In this work, we specifically focus on the textual components of the VLMs and select LLMs with task-specific reasoning abilities that match the VLMs configuration, performing weighted average operation on their parameters as demonstrated in Figure 1. We conduct extensive experiments by merging commonly Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging Figure 1: Illustration of our work investigating how model merging works when transferring reasoning ability from Math-specific LLM to the VLM, showcasing the effects and results of the merged model, as well as the interpretation of layer-wise abilities. The abilities are represented in corresponding colors: red indicates perception ability hidden in early layers, while blue denotes reasoning ability hidden in relatively later layers. used VLMs with LLMs trained on diverse math reasoning datasets (4). Our findings demonstrate that model merging consistently improves the reasoning capabilities of VLMs across all math benchmarks, with minimal impact on the perception-dominant tasks. For instance, merging with Dart (Tong et al., 2024) enhances LLaVAs performance on MathVistas (Lu et al., 2024) math-related subset, yielding 3.6-point absolute improvement. Even in Vision-Only mode of MathVerse (Zhang et al., 2025), where questions are presented in images, it also achieves 1.4-point absolute improvement. Experiments on multiple reasoning-related benchmarks and various VLMs consistently show improvement. These results underscore the potential of parameter merging as simple yet powerful mechanism for capability transfer across model architectures. Furthermore, we delve deeper into understanding how model merging works and how perception and reasoning interplay in this context. Specifically, we analyze parameter changes during the merging process to investigate whether different capabilities, such as high-level reasoning and lowlevel perception, are disentangled within distinct subspaces of the models parameter space (5). Through knockout analysis, we identify two key observations: (1) Image perceptual abilities and world knowledge are predominantly embedded in the early layers of the model, whereas mathematical reasoning skills are concentrated in the middle-tolate layers; and (2) Merging with reasoning models brings reasoning abilities to all the layers, while having minimal impact on the layer distribution of perception ability. These findings contribute to better understanding of how reasoning can be transferred between models and provide insights into model compositionality, offering promising approach for enhancing multi-modal reasoning systems. 2. Model Merging Across Modalities In this section, we introduce model merging for transferring the reasoning abilities of textual LMs to VLMs. typical VLM consists of three key components: vision tower, language Model, and projector that bridges these two parts. The vision tower processes images, enabling the model to see visual content, while the language model serves as the reasoning engine, processing knowledge and generating responses. Therefore, we target the language model (θvlm) for merging while keeping the vision tower and projector unchanged. Model merging has emerged as promising free lunch technique, enabling performance improvements by reusing existing models through simple arithmetic operations on their parameters, without requiring additional training. For simplicity, we adopt linear merging (Ilharco et al., 2023), widely used and robust merging strategy, in our main experiments. We also experiment with TIES merging (Yadav et al., 2023) in some cases to compare both methods in Appendix C. The core idea of model merging relies on task vectors, the modifications made during fine-tuning, which is usually the information necessary to do well on given task. Given base model θbase and fine-tuned model θft, the corresponding task vector τtask is defined as: τtask = θft θbase. Task vectors provide an interpretable way to understand how fine-tuning adapts model to particular task. In the 2 Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging Category Name VLMs LLaVA-NeXT (Liu et al., 2024) Idefics2-8B (Laurencon et al., 2024) InternVL2 (Chen et al., 2024b) Size 8B 8B 76B Domain Task Vectors Dart-Math (Tong et al., 2024) Math Domain MetaMath (Yu et al., 2024a) Math Domain MAmmoTH-1 (Yue et al., 2024a) Math Domain MAmmoTH-2 (Yue et al., 2024b) General Domain Math Domain Magpie-v0.3 (Xu et al., 2024) Deepseek-R1-Distill (DeepSeek-AI et al., 2025) Math Domain Base Model LLaMA-series Mistral-series LLaMA-series Base Model LLaMA/Mistral-series LLaMA-series LLaMA-series LLaMA/Mistral-series LLaMA-series LLaMA-series Table 1: Overview of Vision-Language Models (VLMs) and Task Vectors with the attributes -size, base models and domains. context of VLMs, we define the adaptation of the language model component as: τvlm = θvlm θbase. where τvlm captures the changes introduced when adapting the base LLM into the VLM. Similarly, for reasoningspecialized LLM θreason, we define its corresponding task vector: τreason = θreason θbase. To enhance the reasoning ability of the VLM, we merge its language model with strong reasoning-specialized LLM by linear merging: θ vlm = θbase + λτvlm + (1 λ)τreason. Here, λ determines the weight assigned to the VLM task vector, allowing us to control the balance between the original multi-modal capabilities of the VLM and the newly introduced reasoning strength from the LLM. 3. Experiment settings In this section, we describe our experimental setup, detailing the selected models, datasets, and evaluation protocols used to evaluate the effectiveness of model merging and understand its internal workings. VLMs We span models in different sizes and base models to verify the generalization ability of merging. For VLMs, we use LLaVA-Next-LLaMA3-8B (Liu et al., 2024), Idefics28B (Laurencon et al., 2024), and InternVL2-LLaMA376B (Chen et al., 2024b) (Abbreviated as LLaVA, Idefics, and InternVL in our paper) ranging from 8B to 76B and including both Mistral-based and LLaMA-based models. Reasoning Task Vectors We span task vectors across different reasoning domains, beginning with mathematical reasoning tasks featured in Dart-Math (Tong et al., 2024), which includes two variants: Dart-Uniform and DartProp2diff. In this paper, we refer to the latter as Dart-Prop. Additionally, we examine MAmmoTH-1(Yue et al., 2024a), Magpie-v0.3(Xu et al., 2024), MetaMath(Yu et al., 2024a), and Deepseek-R1-Distill(DeepSeek-AI et al., 2025). Our scope further extends to broader reasoning task vectors obtained in MAmmoTH-2(Yue et al., 2024b). The base models and task vectors are detailed in Table 1. Hyperparameters In our main analysis and experimental sections, we employ linear merging strategy for all task vectors under the same hyperparameter settings to ensure fair comparison. This approach assigns weight of 0.9 to the textual component of LLaVA-Next-LLaMA3-8B and 0.1 to the reasoning task vector, where λ = 0.9. This parameter is tuned on MathVista based on Dart-Prop (Tong et al., 2024). We choose the best value from the range (0.8, 0.85, 0.9). For our additional experiments analyzing the effects of merging across different base VLMs, we adjust the parameter within range of 0.05 across the intervals (0.8, 0.85, 0.9) based on Dart-Prop on MathVista and apply the same hyperparameter across all benchmarks and other task vectors if they exist. Evaluation We evaluate the performance on series of VLM benchmarks. We apply five benchmarks: MathVista (Lu et al., 2024), MathVerse (Zhang et al., 2025), MathVision (Wang et al., 2024a), Dynamath (Zou et al., 2024) and MMStar (Chen et al., 2024a). Among these benchmarks, MathVista is diverse benchmark that includes both math-related reasoning tasks and general visual question answering tasks. Each data sample in MathVista is meticulously annotated with metainformation such as source, task etc., allowing us to evaluate various aspects of improvement, such as identifying the specific scenarios where our method is most effective. Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging 4. Can Model Merging Enhance VLM Capabilities? In this section, we present model merging as viable approach for enhancing inherent VLM capabilities, specifically exploring how merging with specialized models can augment core functionalities such as underdeveloped reasoning abilities in standard VLMs. Merging VLMs with math-specialized models consistently improves performance across all math benchmarks. We posit that VLMs primarily rely on two fundamental capabilities: perception and chain-of-thought reasoning ability to solve the visual reasoning problems. Additionally, they also leverage the knowledge-recall ability to enhance their decision-making. Among these, chainof-thought reasoning remains as bottleneck in current VLMs (Zhang et al., 2024a), despite significant advances in this area by LLMs. This observation motivates our investigation into whether merging VLMs with math-specialized LLMs can enhance their inherent reasoning capabilities. To explore this hypothesis, we use LLaVA-Next-LLaMA38B, commonly employed VLM, as the base model, and integrate it with 5 state-of-the-art reasoning models (see task vectors in Table 1) that were specifically fine-tuned on reasoning tasks: Dart-Math (Tong et al., 2024) (Dart has two variants: Dart-Uniform and Dart-Prop2diff, abbreviated as Dart-Prop in our paper), MAmmoTH-1 (Yue et al., 2024a), MAmmoTH-2 (Yue et al., 2024b), and Magpie-v0.3 (Xu et al., 2024). For fair comparison of the task vectors, we employ the linear merging strategy to all task vectors in the same hyper-parameter setting, which assigns weight of 0.9 to the textual component of LLaVA-Next-LLaMA3-8B and 0.1 to the math task vector. The merged model is evaluated across five datasets (see 3), and the results are summarized in Table 2. As indicated by the green arrows, integrating VLMs with math-specialized models such as Dart (Tong et al., 2024) consistently improves performance over the baseline across all five mathematical datasets. Notably, on the MathVerse Benchmark in Text-Dominant evaluation mode, merging with Dart-Prop yields 30% relative improvement over the baseline (a 6-point absolute increase). Additionally, it boosts performance on math-related VQA in MathVista by 3.5 absolute points. Whereas merging with general-purpose reasoning LLM like MagPie(Xu et al., 2024) offers only modest gains. Moreover, we extend our methods to other VLMs and task vectors. Table 3 presents results for Idefics2-8B (Yadav et al., 2024) and InternVL2-76B (Chen et al., 2024b) using task vectors available in the open-source community. For Idefics, MAmmoTH-1 achieves the best performance, improving accuracy by approximately 1.0 absolute points on average, while most reasoning task vectors fail to provide Figure 2: Accuracy changes after merging compared to the baseline. Generally, datasets directly requiring mathrelated and text-dominant capabilities, such as textbook QA and math word problem, exhibit clear improvements while domains requiring visual processing such as figure QA show performance degradation. benefit. We attribute this to Idefics already being extensively fine-tuned on large-scale text-only data, including math SFT data, leading to high degree of overlap with existing task vectors, which limits further improvements through merging. For InternVL2-76B, integrating Dart improves all benchmarks by approximately 1 absolute point, demonstrating that merging can also be beneficial for large-scale VLMs. Model merging exhibits minimal improvement or even decreased performance on vision-dominant tasks and general knowledge-centric tasks. closer analysis of performance across different subtasks reveals that on the MathVerse benchmarks, model merging yields higher performance gains for math-related and text-dominant samples, while vision-only questions show limited improvement (see Table 2). This phenomenon is also observed in the MathVista dataset (Figure 2), where we visualize performance changes relative to the baseline for each sub-domain dataset using heatmap. Notably, datasets that directly require math-related and text-dominant capabilities, such as geometric reasoning, algebraic reasoning and textbook question answering, exhibit consistent improvements. However, domains requiring extensive visual processing (e.g., visual question answering and figure question answering) show slight performance degradation. Vision-only and vision-dominant tasks consist of questions embedded in figures, which require robust image perception to accurately recognize the question before employing knowledge-recall and reasoning to derive the answer. The inability of model merging to improve these tasks raises critical question: if the bottleneck lies in image perception, then enhancing the 4 Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging Model Method MathVista MathVerse Benchmarks MMStar All General Math Overall T-D T-L V-I V-D V-O All Math DM MV LLaVA 37.4 Baseline 38.2 +Dart-Uniform 36.7 +MAmmoTH-1 37.4 +MAmmoTH-2 +Magpie-v0.3 36.8 +DeepSeek-R1-Distill 38.1 38.0 +Dart-prop 51.7 49.8 50.0 51.1 49.7 51.1 48.7 25.4 20. 25.9 20.8 21.1 16.5 16.0 43.8 30.0 22.7 13.8 28.3 2.9 23.6 3.5 32.0 25.6 25.4 19.3 17.4 42.5 31.2 1.2 24.5 1.8 15.8 2.0 25.4 0.0 21.1 1.0 26.9 23.1 22.6 16.6 16.4 44.1 30.8 0.8 22.6 0.1 15.8 2.0 25.7 0.3 20.6 0.5 26.0 22.0 22.8 16.4 16.1 43.8 30.0 0.0 22.5 0.2 14.1 0.3 25.9 0.5 20.7 0.6 26.8 22.2 22.2 16.6 15.5 44.1 30.8 0.8 22.7 0.0 16.4 2.6 27.0 1.6 21.2 1.1 28.4 22.7 22.5 17.3 15.1 43.7 33.2 3.2 24.3 1.6 15.1 1.3 25.5 19.8 17.4 43.6 33.6 3.6 24.5 1.8 14.8 1.0 28.9 3.5 23.7 3.6 30.7 24.8 Table 2: The performance of LLaVA-Next-LLaMA3-8B model with merged task vectors across math-related Benchmarks: MathVista (All, General, and Math-related categories), MathVerse (Overall, Text-Dominant, Text-Lite, Vision-Integrated, Vision-Dominant, and Vision-Only categories), MMStar (All and Math split), DynaMath (annotated as DM), MathVision (annotated as MV). We include both variants of Dart (Tong et al., 2024) for comparison. We bold the highest value in each benchmark, and the gray row indicates the best task vectors on average. Model Method MathVista MathVerse Benchmarks MMStar All General Math Overall T-D T-L V-I V-D V-O All Math DM MV 51.8 Baseline 53.2 +MetaMath +Dart-Prop 51.6 +Dart-Uniform 51.6 +MAmmoTH-1 53.0 +MAmmoTH-2 52.8 Baseline 65.6 +Dart-Uniform 66.1 57.0 57.8 58.0 57.0 58.5 58.3 67.0 67. 47.4 19.4 24.4 21.3 20.7 19.7 11.0 49.5 39.6 21.8 17. 49.3 1.9 20.0 0.6 25.3 22.3 21.1 18.7 12.4 48.1 39.2 0.4 22.7 0.9 11.8 5.3 46.1 1.3 20.0 0.6 26.3 21.8 21.6 18.9 11.2 48.4 39.6 0.0 22.7 0.9 14.8 2.3 47.0 0.4 20.5 1.1 27.3 22.6 21.1 19.5 12.2 47.9 38.4 1.2 22.7 0.9 14.8 2.3 48.3 0.9 20.4 1.0 26.0 22.5 21.3 19.8 12.1 48.3 40.8 1.2 23.2 1.4 16.8 0.3 48.1 0.7 18.4 1.0 25.6 22.7 20.7 19.4 12.4 48.5 40.0 0.4 24.0 2.2 16.8 0.3 64.4 43.1 54.1 47.5 44.8 43.8 25.3 67.3 75.2 38. 23.7 65.2 0.8 44.3 1.2 53.9 48.1 46.3 44.5 28.6 67.5 74.8 0.4 39.6 0.9 25.3 1. Idefics InternVL Table 3: The performance of Idefics2-8B model and InternVL2-LLaMA3-76B model with merged task vector across mathrelated Benchmarks: MathVista (All, General, and Math-related categories), MathVerse (Overall, Text-Dominant, Text-Lite, Vision-Integrated, Vision-Dominant, and Vision-Only categories), MMStar (All and Math split), DynaMath (annotated as DM) and MathVision (annotated as MV). For benchmarks with Math subsets, only the Math score is included in the average score calculation. We bold the highest value in each benchmark, and the gray row indicates the best task vectors on average. textual component through model merging may fail to yield performance gains. Merging with math models brings inference time scaling ability. We hypothesize that the reasoning ability transferred to VLMs is primarily reflected in the improvement of chain-of-thought capabilities. To support this hypothesis, we analyze answer lengths before and after merging with Dart, highlighting significant shifts in task-specific behaviors. As shown in Figure 3, the performance improvement exhibits nearly linear relationship with the increase in answer length, indicating that merging enables VLMs to scale inference time effectively. When looking closer at the specific tasks, chain-of-thought-intensive tasks such as geometry problem solving, geometry reasoning, and algebraic reasoning experienced substantial increase in average prediction length, exceeding 250% of the original answer length. In contrast, changes in visual-intensive tasks like figure question answering and visual question answering were relatively modest, with nearly the same original answer length, even showing decrease in performance. These results suggest that merging with math-focused model like Dart not only enhances the detail and depth of responses in reasoning-intensive tasks but also maintains efficiency and stability in perception-driven tasks, highlighting the adaptability of the merged framework across diverse domains. We show the details in Figure 9. 5. Merging as Interpretability Tool Dive into the inner parameter space of LLaVA In this section, we leverage model merging as an analytical tool to decompose and understand the internal mechanics of VLMs. By analyzing parameter modifications during the merging process, we aim to identify and isolate distinct parameter subspaces responsible for specific capabilities, such as visual perception and reasoning. We conduct fine-grained analysis of LLaVA on the MathVista (Lu et al., 2024) dataset, which categorizes the examples into General VQA and Math VQA. We hypothesize that General 5 Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging formance on math reasoning tasks significantly improves through merging. Furthermore, we demonstrate in 4 that this improvement stems from the infused inference-time scaling ability. To pinpoint where these infused chain-ofthought abilities reside in the parameter space, we masked the parameters of the Dart-Merged LLaVA using those from the original LLaVA model. 2 and 4 at Figure 4 indicates that masking the later layers has more pronounced impact on math-related tasks, suggesting that the later five or more layers are crucial for math reasoning. Moreover, merging with math-focused model boosts math reasoning abilities while only minimally affecting general VQA performance. This implies that integrating math model with small weights incurs only slight trade-off in VQA capabilities while yielding significant gains for math-specific questions. Overall, these findings suggest that perception and chain-of-thought reasoning abilities occupy distinct regions within the LLaVA parameter space and can be largely disentangled. Expore the threshold of the absolute ability of LLaVA for each module In previous experiments, we masked individual modules using those from another model to comparatively evaluate specific capabilities. To precisely quantify the role of each modulesuch as attention or MLP layers in VLMs for both general VQA and math VQA tasks, we replace each modules weights with uniform distribution (i.e., assigning each parameter value of 1/N , where is the size of the weight matrixs first dimension). This substitution introduces significant noise, effectively disabling the modules functionality. By measuring the resulting performance drop, we can assess each layers absolute contribution to the models performance on both tasks. Figure 5 shows the performance drops of disabling LLaVA ( 5 and 7 ) and Dart-Merged LLaVA ( 6 and 8 ). The first observation is that 1) early-to-middle layers are more crucial for both general and math-related tasks in the LLaVA model, as evidenced by the significant drops, i.e., 25% absolute accuracy drop in general tasks and 10% in math-targeted VQA (highlighted in red gray square). This suggests that the early-to-middle layers play unique and indispensable roles in VLMs, this is intuitive since early layers handle perception, and accurately perceiving the image is prerequisite for answering correctly while the later layers are less important and more robust to noise. Secondly, mathtargeted VQA shows smaller performance drop than general VQA after masking out parameters, due to its inherently weak math reasoning ability. When certain parameters are masked out, general visual question answering (VQA) tasks (left side of 5 and 7 ) exhibit larger performance decrease than math-related questions (right side of 5 and 7 ). This can be explained by the LLaVA models limited knowledge of math tasks and its inherent weak reaFigure 3: The relationship between answer length change (characters) and accuracy improvement after merging (The x-axis represents the relative change from the original answer, while the y-axis shows the absolute change in accuracy), classified by task and skills. VQA primarily evaluates perception ability and knowledge recall, while Math VQA assesses perception ability and reasoning ability. Mask Out We begin by using the masking out technique to assess the influence of each module. Specifically, for each layer, we replace the parameters of the MLP and attention modules with those from alternative modules (e.g., uniform distribution or parameters from another model) to evaluate the absolute and relative impact of each module. Our hypothesis is that greater performance drop when masking particular module indicates its higher importance for the task, while smaller drop suggests more trivial effect on the task. Locate the perception ability in the parameter space We first analyze the impact of supervised fine-tuning (SFT) of LLaVA compared to the pretrain LLaMA, by progressively masking out LLaVAs parameters and replacing them with LLaMAs parameters, layer by layer for each module. Our hypothesis is that LLaVAs SFT training improves the models perception ability, and we use the Masking Out technique to identify key regions where perception ability is located. 1 and 3 at Figure 4 shows that, from layer-wise perspective, masking out the early layers has greater impact on general VQA tasks than masking the later layers, suggesting that perception abilities are primarily located in the early layers. We also observe that while general VQA performance declines after masking out, Math VQA performance improves consistently. This implies that LLaVAs SFT enhances perceptual abilities at the cost of reasoning capabilities, which motivates our efforts to improve reasoning ability in VLMs. Locate the chain-of-thought reasoning ability in the parameter space As shown in Table 2, LLaVAs per6 Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging Figure 4: LLaVA LLaMA ( 1 and 3 ): the accuracy changes after replacing the parameters of each MLP ( 1 ) and Attention ( 3 ) layer of the LLaVA model with parameters from LLaMA. We find that masking out the early layers has greater impact on general VQA tasks than masking the later layers, suggesting that perception abilities gained from LLaVA-sft training are primarily located in the early layers. Dart-Merged LLaVA LLaVA ( 2 and 4 ): the accuracy changes after replacing each MLP ( 2 ) and Attention layer ( 4 ) of the Dart-Merged LLaVA model to that of LLaVA. significant drop in accuracy in math-targeted VQA tasks is observed from 5 layer onwards. (highlighted in blue), suggesting that the reasoning ability is mainly located on these layers. soning abilities compared to the abilities demonstrated in VQA tasks (e.g., perception and world knowledge). In other words, the models limited mathematical reasoning ability makes it less sensitive to parameter alterations, which also enhances our motivation to incorporate external reasoning ability into it. Merging with Reasoning Models Enhances Almost All Layers Math Reasoning Ability As shown in Figure 5, after merging with Dart (right side of 6 and 8 ), we observe that nearly all layershighlighted by the blue maskdrop more in math reasoning tasks compared to the base LLaVA (right side of 5 and 7 ), where fewer layers are influenced. This suggests that reasoning ability has been successfully integrated into all layers without substantially affecting the layer distribution of perception ability. Several qualitative examples supporting this are presented in Figure 6. The first and third examples illustrate how the model better perceives key entities and makes decisions through chain-of-thought reasoning. However, for general VQA tasks, we also see reduced activation in the early layers, indicating slight loss in world knowledge due to the model merging. 6. Related Work VLMs Large Vision-Language Models (VLMs) consist of three main components: visual encoder for processing images, such as CLIP (Radford et al., 2021) or SigLip (Zhai et al., 2023); language model (e.g., LLaMA model (Dubey et al., 2024) or Mistral model (Jiang et al., 2023)) for processing textual inputs and image features to generate responses; and projector, typically implemented as multilayer perceptron (MLP), to bridge the gap between the visual and language components. This module maps features from the visual space to the language space, facilitating interaction between the two modalities. Model Merging Model merging offers free lunch by repurposing fine-tuned models for downstream tasks (Wortsman et al., 2022; Ilharco et al., 2023; Zhang et al., 2023). It creates new model through simple arithmetic on existing parameters, requiring no extra training or inference cost. Several model merging techniques have been proposed to improve performance, such as calculating different weights for model parameters using data and internal model acti7 Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging Figure 5: LLaVA 1/N ( 5 and 7 ): the accuracy changes after replacing the parameters of each MLP ( 5 ) and Attention ( 7 ) layer of the LLaVA model with 1 , where is the first dimension of the weight matrix. The highlighted red area shows that early-to-middle layers are more crucial for both general and math-related tasks in the LLaVA model, as evidenced by the significant drops, i.e., 0.25 absolute accuracy drop in general tasks and 0.10 in math-targeted VQA. Dart-Merged LLaVA 1/N ( 6 and 8 ): the accuracy changes after replacing the parameters of each MLP ( 6 ) and Attention ( 8 ) layer of the Dart-Merged LLaVA model with 1 . Comparing before and after merging when applied masking out, we observe larger drop in accuracy in math-targeted VQA tasks across all layers (highlighted in blue), suggesting that the contribution of all most all layers to math reasoning has increased. Figure 6: Qualitative study: three examples that can be fixed by merging with Dart (Tong et al., 2024). vations (Matena & Raffel, 2022; Jin et al., 2023). Some approaches initially sparsify the models to reduce conflicts across different functions (Yadav et al., 2023; Yu et al., 2024b). Recently, Layer Swapping (Bandarkar et al., 2024) was introduced, retains specific layers while merging others to enhance transfer learning. Despite their differences, simple averaging is often preferred for its simplicity and robustness. As model scales grow, performance gaps among 8 Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging merging techniques shrink (Yadav et al., 2024), making linear merging natural choice. Merging for VLMs Although much prior research has focused on merging vision models for tasks such as image recognition and scene understanding (Ilharco et al., 2023; Yang et al., 2023), the composition for VLMs remain insufficiently explored, particularly in terms of transferring specialized skills across modalities. Recently, REMEDY (Anonymous, 2025) proposes practical merging recipe, which merges projector and front-layer modules of VLMs, focusing on multitasking and transfer in low-shot settings across various VQA types. Additionally, Sakana AI demonstrates the potential for multilingual capabilities in VLMs by transferring Japanese comprehension and generation abilities from an expert LLM to VLMs (Akiba et al., 2024). However, key challenge lies in transferring reasoning skills across modalities. Our study addresses this by integrating the reasoning capabilities of LLMs into VLMs with comprehensive analysis. 7. Conclusion In this paper, we investigate the use of model merging methods to bridge the intelligence of cross-modalities, specifically transitioning from pure textual modality to visiontextual modalities. By incorporating various math-specific LLMs into different VLMs through model merging, we demonstrate that this approach effectively enhances the reasoning capabilities of VLMs. Our experimental results indicate an improvement of up to 12% in performance on math reasoning tasks compared to the baseline. And by employing model merging as an interpretability tool, we further unlock the parameter space of VLM. We find that for VLMs trained solely on image-text pairs, the abilities of perception and reasoning can be decomposed within the parameter space. Specifically, perception ability resides in the early layers, while reasoning ability is concentrated in the later layers. This decomposition further validates our experimental results."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning Interpretability. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Akiba, T., Shing, M., Tang, Y., Sun, Q., and Ha, D. Evolutionary optimization of model merging recipes. ArXiv preprint, abs/2403.13187, 2024. URL https: //arxiv.org/abs/2403.13187. Anonymous. REMEDY: Recipe merging dynamics in In The Thirteenth Inlarge vision-language models. ternational Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=iX7eHHE5Tx. Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., and Zhou, J. Qwen-vl: versatile visionlanguage model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. Bandarkar, L., Muller, B., Yuvraj, P., Hou, R., Singhal, N., Lv, H., and Liu, B. Layer swapping for zero-shot crosslingual transfer in large language models. ArXiv preprint, abs/2410.01335, 2024. URL https://arxiv.org/ abs/2410.01335. Chen, L., Li, J., Dong, X., Zhang, P., Zang, Y., Chen, Z., Duan, H., Wang, J., Qiao, Y., Lin, D., et al. Are we on the right way for evaluating large vision-language models? ArXiv preprint, abs/2403.20330, 2024a. URL https://arxiv.org/abs/2403.20330. Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S., Zhong, M., Zhang, Q., Zhu, X., Lu, L., et al. Internvl: Scaling up vision foundation models and aligning for In Proceedings of the generic visual-linguistic tasks. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2418524198, 2024b. DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., and Liu..., A. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/ 2501.12948. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. ArXiv preprint, abs/2407.21783, 2024. URL https://arxiv.org/ abs/2407.21783. Ilharco, G., Ribeiro, M. T., Wortsman, M., Schmidt, L., Hajishirzi, H., and Farhadi, A. Editing models with task arithmetic. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https: //openreview.net/pdf?id=6t0Kwf8-jrj. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. ArXiv preprint, abs/2310.06825, 2023. URL https://arxiv.org/ abs/2310.06825. 9 Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging Jin, X., Ren, X., Preotiuc-Pietro, D., and Cheng, P. Dataless knowledge fusion by merging weights of language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https: //openreview.net/pdf?id=FCnohuR6AnM. Laurencon, H., Tronchon, L., Cord, M., and Sanh, V. What matters when building vision-language models?, 2024. Li, B., Zhang, Y., Guo, D., Zhang, R., Li, F., Zhang, H., Zhang, K., Zhang, P., Li, Y., Liu, Z., et al. Llavaonevision: Easy visual task transfer. ArXiv preprint, abs/2408.03326, 2024. URL https://arxiv.org/ abs/2408.03326. Liu, H., Teng, Z., Cui, L., Zhang, C., Zhou, Q., and Zhang, Y. LogiCoT: Logical chain-of-thought instrucIn Bouamor, H., Pino, J., and Bali, tion tuning. K. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 29082921, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp. 191. URL https://aclanthology.org/2023. findings-emnlp.191/. Liu, H., Li, C., Li, Y., Li, B., Zhang, Y., Shen, Improved reaURL S., and Lee, Y. J. soning, ocr, and world knowledge, 2024. https://llava-vl.github.io/blog/ 2024-01-30-llava-next/. Llava-next: Lu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi, H., Cheng, H., Chang, K.-W., Galley, M., and Gao, J. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations (ICLR), 2024. in Neural (eds.), Advances Matena, M. and Raffel, C. Merging models with fisher-weighted averaging. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, Information ProA. cessing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - DeURL http://papers. cember 9, 2022, 2022. nips.cc/paper_files/paper/2022/hash/ 70c26937fbf3d4600b69a129031b66ecAbstract-Conference.html. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable In visual models from natural language supervision. Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of 10 Proceedings of Machine Learning Research, pp. 8748 8763. PMLR, 2021. URL http://proceedings. mlr.press/v139/radford21a.html. Sun, K., Bai, Y., Qi, J., Hou, L., and Li, J. Mm-math: Advancing multimodal math evaluation with process evaluation and fine-grained classification, 2024. URL https://arxiv.org/abs/2404.05091. Tong, Y., Zhang, X., Wang, R., Wu, R., and He, J. Dartmath: Difficulty-aware rejection tuning for mathematical problem-solving. ArXiv preprint, abs/2407.13690, 2024. URL https://arxiv.org/abs/2407.13690. Wang, K., Pan, J., Shi, W., Lu, Z., Zhan, M., and Li, H. Measuring multimodal mathematical reasoning with mathvision dataset, 2024a. Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., Fan, Y., Dang, K., Du, M., Ren, X., Men, R., Liu, D., Zhou, C., Zhou, J., and Lin, J. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024b. Wortsman, M., Ilharco, G., Gadre, S. Y., Roelofs, R., Gontijo-Lopes, R., Morcos, A. S., Namkoong, H., Farhadi, A., Carmon, Y., Kornblith, S., et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International conference on machine learning, pp. 23965 23998. PMLR, 2022. Xu, Z., Jiang, F., Niu, L., Deng, Y., Poovendran, R., Choi, Y., and Lin, B. Y. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. ArXiv preprint, abs/2406.08464, 2024. URL https: //arxiv.org/abs/2406.08464. Yadav, P., Tam, D., Choshen, L., Raffel, C. A., and Bansal, M. Ties-merging: Resolving interference when merging models. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers. nips.cc/paper_files/paper/2023/hash/ 1644c9af28ab7916874f6fd6228a9bcfAbstract-Conference.html. Yadav, P., Vu, T., Lai, J., Chronopoulou, A., Faruqui, M., Bansal, M., and Munkhdalai, T. What matters for model merging at scale? ArXiv preprint, abs/2410.03617, 2024. URL https://arxiv.org/abs/2410.03617. Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging in visual math problems? In European Conference on Computer Vision, pp. 169186. Springer, 2025. Zhang, Y., Bai, R. H., ZHANG, R., Gu, J., Zhai, S., Susskind, J. M., and Jaitly, N. How far are we from In First Conintelligent visual deductive reasoning? ference on Language Modeling, 2024b. URL https: //openreview.net/forum?id=pYEnhZ6NAv. Zou, C., Guo, X., Yang, R., Zhang, J., Hu, B., and Zhang, H. Dynamath: dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models, 2024. URL https://arxiv.org/ abs/2411.00836. Yang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., Lu, K., Xue, M., Lin, R., Liu, T., Ren, X., and Zhang, Z. Qwen2.5-math technical report: Toward mathematical expert model via selfimprovement, 2024. URL https://arxiv.org/ abs/2409.12122. Yang, E., Wang, Z., Shen, L., Liu, S., Guo, G., Wang, X., and Tao, D. Adamerging: Adaptive model merging for multi-task learning. ArXiv preprint, abs/2310.02575, URL https://arxiv.org/abs/2310. 2023. 02575. Yu, L., Jiang, W., Shi, H., YU, J., Liu, Z., Zhang, Y., Kwok, J., Li, Z., Weller, A., and Liu, W. Metamath: Bootstrap your own mathematical questions for large language models. In The Twelfth International Conference on Learning Representations, 2024a. URL https: //openreview.net/forum?id=N8N0hgNDRt. Yu, L., Yu, B., Yu, H., Huang, F., and Li, Y. Language models are super mario: Absorbing abilities from homologous models as free lunch. In Forty-first International Conference on Machine Learning, 2024b. Yue, X., Qu, X., Zhang, G., Fu, Y., Huang, W., Sun, H., Su, Y., and Chen, W. MAmmoTH: Building math generalist models through hybrid instruction tuning. In The Twelfth International Conference on Learning Representations, 2024a. URL https://openreview.net/forum? id=yLClGs770I. Yue, X., Zheng, T., Zhang, G., and Chen, W. Mammoth2: Scaling instructions from the web. Advances in Neural Information Processing Systems, 2024b. Zhai, X., Mustafa, B., Kolesnikov, A., and Beyer, L. Sigmoid loss for language image pre-training. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pp. 1194111952. IEEE, 2023. doi: 10.1109/ICCV51070. 2023.01100. URL https://doi.org/10.1109/ ICCV51070.2023.01100. Zhang, J., Liu, J., He, J., et al. Composing parameterefficient modules with arithmetic operation. Advances in Neural Information Processing Systems, 36:12589 12610, 2023. Zhang, R., Zhang, B., Li, Y., Zhang, H., Sun, Z., Gan, Z., Yang, Y., Pang, R., and Yang, Y. Improve vision language model chain-of-thought reasoning. ArXiv preprint, abs/2410.16198, 2024a. URL https://arxiv.org/ abs/2410.16198. Zhang, R., Jiang, D., Zhang, Y., Lin, H., Guo, Z., Qiu, P., Zhou, A., Lu, P., Chang, K.-W., Qiao, Y., et al. Mathverse: Does your multi-modal llm truly see the diagrams 11 Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging A. The checkpoints used in experiments We show all the checkpoints we use for experiments in Table 4. Category Huggingface ckpt LLaVA-Next lmms-lab/llama3-llava-next-8b Idefics2-8B HuggingFaceM4/idefics2-8b Task Vectors MAmmoTH1 MAmmoTH2 Magpie-v0.3 Dart-Uniform Dart-Prop DeepSeek-R1-Distill MAmmoTH MAmmoTH2 MetaMath Dart-Uniform Dart-Prop Huggingface ckpt of task vectors EtashGuha/llama3-mammoth-dcft TIGER-Lab/MAmmoTH2-8B Magpie-Align/Llama-3-8B-Magpie-Align-SFTv0.3 hkust-nlp/dart-math-llama3-8b-uniform hkust-nlp/dart-math-llama3-8b-prop2diff deepseek-ai/DeepSeek-R1-Distill-Llama-8B TIGER-Lab/MAmmoTH-7B-Mistral TIGER-Lab/MAmmoTH2-7B meta-math/MetaMath-Mistral-7B hkust-nlp/dart-math-mistral-7b-uniform hkust-nlp/dart-math-llama3-8b-prop2diff InternVL2 OpenGVLab/InternVL2-Llama3-76B Dart-Prop hkust-nlp/dart-math-llama3-70b-prop2diff Table 4: All the huggingface checkpoints we use in our experiments B. More analysis for MathVista MathVista includes various metadata, such as Task and Task&Skills, allowing the dataset to be classified into subsets using different methods. We present MathVistas performance improvement after merging with Dart (Tong et al., 2024) across different Tasks (there are in total 5 tasks in MathVista) at Figure 7, along with the correlation between answer length and accuracy improvement for each task. We can see that two figures both show consistent pattern with findings at Section 4. Figure 7 suggests that the math-specialized task vectors primarily benefit math reasoning tasks, but may hinder performance on knowledge-intensive general VQA tasks. Figure 8 shows that merging with Dart enhances the models inference-time scaling ability. Figure 7: The accuracy difference after merging across several subtasks in MathVista for LLaVA. Figure 8: The relationship between answer length change and accuracy improvement after merging, classified by task. C. Different merging methods perform generally comparable We are also interested in whether different merging methods affect the performance of reasoning ability transfer. To investigate this, we employ TIES (Yadav et al., 2023), state-of-the-art merging method, to compare its performance with that of linear merging. Figure 10 shows that while TIES merging may demonstrate superior performance on certain benchmarks, such as MathVision and MathVerse (Text-Dominant), the overall average performance remains comparable to that of linear merging. Additionally, without the constraint that the sum of hyperparameters must be balanced across both task vectors, the hyperparameter search space expands significantly. As result, TIES merging requires more hyperparameter tuning to achieve comparable performance. 12 Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging Figure 9: The relationship between answer length change and accuracy improvement after merging, classified by task. Figure 10: Performance comparison of TIES and linear merging methods across MathVerse (Text-Dominant) and MathVision benchmarks. Baseline presents the result of LLaVA-Next-LLaMA3-8B. Linear shows the result of merging Dart-prop vector with hyperparameters of (0.9, 0.1). Performance reached by TIES merging with the same two task vectors are labeled as the hyperparameter pair used. MathVision MathVerse-Text-Dominant Baseline Linear TIES(1, 0.4) TIES(1.6, 0.2) Dare-TIES(0.8, 0.2) Dare-Linear(0.8, 0.2) 13.8 14.8 15.1 14.5 17.8 17.4 25.9 30.7 29.7 31.4 22.7 21.7 Table 5: Performance comparison on MathVision and MathVerse-Text-Dominant datasets. We also expand our experiments on Dare merging (Dare-TIES and Dare-Linear) and results shown in 5, another SOTA merging method to explore additional merging techniques. We adopt the same hyperparameter search strategy as TIES, parameterized by (α1, α2), where α1 is tuned for the VLM and α2 for the Math LLM, to obtain the most comparable checkpoints on benchmarks emphasizing visual and textual reasoning. The results show that DARE performs comparably to linear merging and TIES, consistent with our finding that different merging methods yield similar performance, with none significantly outperforming simple averaging. This supports our choice to adopt the linear merging method. This finding is also consistent with the previous finding in Yadav et al. (2024) that different merging methods tend to exhibit similar behaviors at larger scales. Given this, we chose not to focus extensively on exploring alternative merging methods but instead to explore more about the interpretability and composition of the models internal abilities. D. More results on MM-Math In Table 6, we provide detailed results of our method applied to the MM-Math (Sun et al., 2024) benchmark, presenting general improvement brought by model merging in visual reasoning. E. Additional results demonstrating generalization from logical to mathematical reasoning To investigate whether reasoning skills from other domains, such as logical reasoning, can generalize to multi-modal math reasoning, we fine-tune LLaMA3-8B on LogiCoT (Liu et al., 2023)a logical chain-of-thought datasetand merge it with LLaVA. As shown in Table 7, this logic-focused model improves performance on math reasoning tasks, suggesting 13 Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging Model Method MM-Math LLaVA Idefics InternVL Baseline +Dart-prop +Dart-uniform +MAmmoTH-1 +MAmmoTH-2 +Magpie-v0.3 +DeepSeek-R1 +Dart-keep-5layers Baseline +MetaMath +Dart-prop +Dart-uniform +MAmmoTH-1 +MAmmoTH-2 Baseline +Dart-uniform 0.61 0.71 0.10 0.86 0.25 0.68 0.07 1.46 0.85 1.30 0.69 0.27 0.34 1.05 0.44 4.00 4.68 0.68 2.63 1.37 2.73 1.27 4.03 0.03 3.80 0.20 22.70 22.80 0. Table 6: Addition experiment result on MM-Math. Model Method MathVista General All Math Overall TD TL VI VD VO MathVerse Benchmarks MV LLaVA Baseline +logic 37.4 37.0 0.4 51.7 49.1 2.6 25.4 26.7 1. 20.1 23.5 3.4 25.9 30.5 4.6 20.8 25.0 4.2 21.1 26.4 5.3 16.5 20.3 3.8 16.0 15.1 0. 13.8 11.2 2.6 Table 7: Performance (%) of LLaVA before and after merging with the LLM fine-tuned on logical training data (Liu et al., 2023) on MathVista, MathVerse and MathVision benchmarks. Arrows denote absolute change from the baseline. generalization capabilities across the reasoning domains and highlighting the potential for transferring reasoning skills from textual to multi-modal settings. F. Significance Test To evaluate whether the improvements from merging are significant, we conduct significance test. Merging general reasoning models like Mammoth2 and Magpie does not yield statistically significant enhancements, exhibiting pattern of marginal accuracy gains, as shown in Table 2 of our paper. In contrast, merging math-focused LLMs, such as Dart-Prop, results in statistically significant improvements. This supports our conclusion that merging with math-related models offers the greatest advantage. Model Method MathVista MathVerse MMStar DM LLaVA +Dart-Uniform +MAmmoTH-2 +Magpie-v0.3 +Dart-prop Math 0.15 0.77 0.67 0.06 T-D 0.00 0.58 1.00 0.00 T-L 0.00 0.30 0.06 0. V-I V-D V-O Math Math 0.00 0.04 0.07 0.00 0.02 0.70 0.84 0. 0.46 0.35 0.62 0.02 0.56 1.00 0.56 0.05 0.21 1.00 0.96 0.00 Table 8: P-values across the models and datasets. Bolded values indicate < 0.05, which pass the significance test."
        }
    ],
    "affiliations": [
        "City University of Hong Kong",
        "Hong Kong University of Science and Technology",
        "National University of Singapore",
        "Northwestern University"
    ]
}