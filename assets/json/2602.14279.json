{
    "paper_title": "Whom to Query for What: Adaptive Group Elicitation via Multi-Turn LLM Interactions",
    "authors": [
        "Ruomeng Ding",
        "Tianwei Gao",
        "Thomas P. Zollo",
        "Eitan Bachmat",
        "Richard Zemel",
        "Zhun Deng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Eliciting information to reduce uncertainty about latent group-level properties from surveys and other collective assessments requires allocating limited questioning effort under real costs and missing data. Although large language models enable adaptive, multi-turn interactions in natural language, most existing elicitation methods optimize what to ask with a fixed respondent pool, and do not adapt respondent selection or leverage population structure when responses are partial or incomplete. To address this gap, we study adaptive group elicitation, a multi-round setting where an agent adaptively selects both questions and respondents under explicit query and participation budgets. We propose a theoretically grounded framework that combines (i) an LLM-based expected information gain objective for scoring candidate questions with (ii) heterogeneous graph neural network propagation that aggregates observed responses and participant attributes to impute missing responses and guide per-round respondent selection. This closed-loop procedure queries a small, informative subset of individuals while inferring population-level responses via structured similarity. Across three real-world opinion datasets, our method consistently improves population-level response prediction under constrained budgets, including a >12% relative gain on CES at a 10% respondent budget."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 5 1 ] . [ 1 9 7 2 4 1 . 2 0 6 2 : r Whom to Query for What: Adaptive Group Elicitation via Multi-Turn LLM Interactions Ruomeng Ding1, Tianwei Gao*1, Thomas P. Zollo2, Eitan Bachmat3, Richard Zemel2, and Zhun Deng1 1University of North Carolina at Chapel Hill 2Columbia University 3Ben-Gurion University of the Negev Abstract Eliciting information to reduce uncertainty about latent group-level properties from surveys and other collective assessments requires allocating limited questioning effort under real costs and missing data. Although large language models enable adaptive, multiturn interactions in natural language, most existing elicitation methods optimize what to ask with fixed respondent pool, and do not adapt respondent selection or leverage population structure when responses are partial or incomplete. To address this gap, we study adaptive group elicitation, multi-round setting where an agent adaptively selects both questions and respondents under explicit query and participation budgets. We propose theoretically grounded framework that combines (i) an LLM-based expected information gain objective for scoring candidate questions with (ii) heterogeneous graph neural network propagation that aggregates observed responses and participant attributes to impute missing responses and guide per-round respondent selection. This closed-loop procedure queries small, informative subset of individuals while inferring population-level responses via structured similarity. Across three real-world opinion datasets, our method consistently improves population-level response prediction under constrained budgets, including > 12% relative gain on CES at 10% respondent budget. Code is available at: https: //github.com/ZDCSlab/Group-Adaptive-Elicitation."
        },
        {
            "title": "1 Introduction",
            "content": "Surveys and other collective assessments work by asking limited set of questions to subset of the population of interest in order to infer latent population properties, for example, countylevel political inclination, student skill profiles, or employee preferences over policy changes. Because each additional question and each completed response incurs real cost, including respondent burden, interviewer time, and participation incentives, instruments are typically kept short, and modern survey practice increasingly emphasizes strategic effort allocation to manage the cost error tradeoff [Dillman et al., 1993; Groves and Heeringa, 2006; Chun et al., 2018]. These constraints often result in missing data and breakoffs, so downstream estimates Equal contribution. Contact: zhundeng@cs.unc.edu 1 Figure 1: Overview of group adaptive elicitation. The method jointly selects informative queries and leverages structured group relations, asking only few respondents while inferring remaining responses to learn shared latent group preference. in policy and the social sciences are frequently based on sparse, partially observed responses [Meyer et al., 2015]. Large language models (LLMs) offer natural interface for making these processes adaptive: they can pose questions in natural language, condition on interaction history, and produce predictive distributions over responses. Recent work has begun using autoregressive models as tools for adaptive elicitation, selecting questions to maximize expected information gain from observed histories [Wang et al., 2025]. However, most existing approaches primarily optimize what to ask under an implicit fixed respondent pool, while the dominant bottleneck in many deployments is the number of completed responses that can be collected; improving efficiency therefore also requires deciding who to ask, and leveraging population structure (e.g., demographics and similarity across individuals) so that information gathered from few respondents generalizes to the broader group. To address these challenges, we study adaptive group elicitation: multi-round interaction in which central agent jointly selects the next question to ask and subset of respondents to query in order to reduce uncertainty about latent group quantity under fixed budget. In an election survey, this means deciding not only which policy question to ask next, but also which voters to contact when only small fraction can be interviewed each round. Our approach couples LLM-based predictive inference with population-level propagation: meta-trained LLM scores candidate questions by expected information gain given the current interaction history, while heterogeneous graph neural network (GNN) aggregates observed responses and demographics to impute missing responses and refine representations of similarity across respondents [Suh et al., 2025]. The resulting loop uses these graph-informed representations to select diverse, representative respondent subset each round, and updates the graph with new observations before repeating. Our contribution. We (i) formalize adaptive group elicitation as joint decision problem over questions and respondents under query and participation budgets, (ii) propose practical instantiation that combines an LLM-based expected-information-gain objective with heterogeneous GNN propagation for imputation and respondent selection, and (iii) provide theoretical justification via predictive (de Finettistyle) perspective for graph-structured data and near-optimal guarantees for greedy joint selection under standard assumptions. We then evaluate on three real-world opinion datasets, showing consistent gains under constrained budgets (including 2 > 12% relative improvement on CES at 10% respondent budget). The remainder of the paper presents the setup and framework, develops the theory, and reports experimental results and ablations."
        },
        {
            "title": "2 Preliminaries",
            "content": "We begin by introducing several key components that provide the theoretical and practical foundations of our framework."
        },
        {
            "title": "2.1 De Finetti’s Theorems and Predictive Inference",
            "content": "The theoretical foundation of our approach builds on de Finettis predictive perspective, which connects uncertainty quantification over latent entity with forecasting the future behavior of the induced observables. In survey-style applications such as estimating groups political inclination, the latent entity would represent an underlying preference state, and predictive uncertainty corresponds to uncertainty about how the population will respond to future questions. Under this perspective, the latent entity is accessed only through the predictive distribution it induces; accordingly, throughout our framework and experiments we treat reductions in predictive entropy over future observations as reductions in uncertainty about the latent entity. Specifically, generalized form of de Finettis theorem (see Fact 2.1) implies that for an infinite } sequence of random variables {Yt t=1, the one-step-ahead predictive distribution pt(yY1:t) := p(Yt+1 = Y1:t) can be understood as arising from an underlying latent entity that governs future behavioral patterns. Through this lens, all uncertainty about future observations is induced by uncertainty about : there exists latent space and measure µ such that p(Y1:T ) = (cid:90) T(cid:89) t=1 p(Yt )µ(dU ). Building on this perspective, one can apply recursive, autoregressive update of beliefs about as new observations arrive, yielding principled notion of predictive uncertainty and supporting adaptive elicitation at the individual level. Fact 2.1 (Informal, Generalized De Finettis Theorem [Berti et al., 2004; Fong et al., 2024]). For } sequence of random variables {Yt t=1 taking values in Polish space E, under certain martingale } condition on {pt for some limiting measure and we can recover the latent entity t=1, pt from the limiting measure p. We further discuss this assumption and justify Fact 2.1 in Section 4 and Section A.2."
        },
        {
            "title": "2.2 Heterogeneous GNN for Group Simulation",
            "content": "Modeling relational structure in group settings is challenging due to partial observations and complex dependencies among individuals. Heterogeneous graph neural networks [Schlichtkrull et al., 2018; Battaglia et al., 2018; Kipf and Welling, 2017; Scarselli et al., 2009] provide flexible framework by capturing multiple node types and diverse relational patterns. Through message passing, heterogeneous GNN propagates information across node and relation types, enabling observed attributes and responses to jointly inform respondent representations and predictions, 3 which is particularly valuable under sparse observations. Relevant to our setting, Suh et al. [2025] cast discrete-choice human simulation as link prediction on heterogeneous graph with three node types: individuals, subgroups (defined by individual features such as demographic attribute bins), and choices (each choice corresponds to (question, option) pair). Individuals connect to subgroups via membership edges and to choices via response edges indicating which option they selected; the model predicts responses by scoring an individual against the candidate choice nodes for given question. Notably, their GNN model can match or even surpass strong LLM baselines despite being three orders of magnitude smaller in parameter count, highlighting that heterogeneous GNNs can be highly effective for prediction and imputation under sparse group observations."
        },
        {
            "title": "2.3 Notation",
            "content": "For clarity, we introduce generic notation that applies across all application settings. Let = (V , E) denote graph with node set and edge set E. For node , (v) represents the set of its neighbors, and S(v) = (N (v) {v}) is the set of nodes not adjacent to v. We let denote generic space of latent entities that are the target of inference, set of candidate queries, and the space of possible responses. When represents group of members, for any subset of members and query , we denote by the vector of responses from members in . For random variable Z, we define its entropy to be H(Z) = (cid:82) p(z) log p(z)dz, where is the probability density function of Z. Accordingly, we can follow the standard definition of Z2) = conditional entropy and obtain the conditional entropy of Z1 with respect to Z2 as H(Z1 (cid:82) z2 p(z1, z2) log p(z1 z2)dz1dz2. (cid:82) z"
        },
        {
            "title": "3 Group Adaptive Elicitation Framework",
            "content": "As discussed in the introduction, in many real world applications the goal is to uncover group latent properties through limited interactions. The resulting data is typically sparse, both in the number of queries posed and in the responses obtained. For instance, political surveys can administer only small set of key questions under constraints of time, cost, and privacy, and responses are often collected from only subset of voters due to budget limits and nonparticipation. Overview of our mechanism. To address limited query budgets and missing responses in the interactive scenarios described above, we propose framework that iteratively reduces group level uncertainty by combining individual predictive modeling with relational imputation. At each iteration, the framework adaptively selects both questions and subset of group members based on the interaction history, inferred latent entities, and learned relational structure. This joint selection enables informative queries to be directed towards representative respondents to maximize expected information gain about the underlying group property. Our approach is guided by two principles: ❶ quantifying uncertainty for each group member by aggregating interaction history, including imputed responses, and ❷ leveraging the groups relational structure to share information and recover missing data. Together, these principles give rise to two-stage process at each interaction round: 4 1. Individual uncertainty quantification: We maintain predictive model for each group member with latent entity given the interaction history t1. Our overall uncertainty about the group is computed as the sum of individual uncertainties. This is achieved by meta-training an LLM on interaction history to accurately predict individual responses. 2. Adaptive group interaction strategy: At each round t, we select the next question Xt and to query. The selection aims to (approximately) maximize subgroup of respondents Rt the reduction in overall group uncertainty. Crucially, after observing new responses, we leverage heterogeneous GNN that propagates information across the groups relational graph. This allows us to impute responses from group members not being queried, making the aggregation of individual uncertainties increasingly informed and efficient. We provide theoretical justification for our elicitation strategy in Section 4. The proposed formulation enables computationally tractable adaptive group elicitation by combining individual level inference with relational reasoning to reduce group level uncertainty under sparse query responses. Below, we formally present our framework for efficiently characterizing group latent entities."
        },
        {
            "title": "3.1 Meta-training",
            "content": "In the meta-training phase, our framework includes two components: predictive language model for quantifying individual uncertainty and heterogeneous GNN for leveraging relational structure. The language model predicts individual responses from interaction history and predefined features determined by the dataset, which may include demographic information in political surveys or other relevant attributes depending on the application context, while the GNN propagates information across members to impute missing answers and reduce uncertainty about the group latent entity. Training data. To maintain consistency with the notation used in our later GNN formulation, we denote group of members as = {vi }n i=1, where indexes group members. By observing query-response sequences over interaction rounds, we obtain training dataset train = 1:T , vi {(Xvi 1:T ) denotes the query-response sequence for the i-th group member vi. i=1, where (Xvi 1:T , vi 1:T )}n LLM for prediction inference. We first meta-train language model to perform predictive inference at the individual level, avoiding the need for parametric prior over the complex latent space , as in [Ye and Namkoong, 2024; Wang et al., 2025]. We finetune the parameters θ of an LLM to maximize the likelihood of autoregressive prediction: (cid:98)θ = arg max θ n(cid:88) 1(cid:88) i=1 t= log pθ(Y vi t+1 Xvi t+1, Hvi ) (1) where Hv denotes the interaction history collected from group member v. As in [Ye and Namkoong, 2024; Wang et al., 2025], the above objective trains the LLM to serve as robust predictor of individual group member behavior, enabling reliable uncertainty quantification and determining the informativeness of queries with respect to the latent entity given each group members interaction history. This predictive capability allows the LLM to estimate the expected information gain of candidate queries, which is essential for guiding adaptive query selection in our framework. Heterogeneous GNN for group simulation. The LLMs predictive inference benefits from complete interaction histories for each group member; however, in our setting the observed query-response data can be sparse in both the number of queries and the subset of respondents. To impute missing responses and thereby bolster the LLMs ability to characterize uncertainty over the latent entity, we employ heterogeneous GNN. The GNN is well suited for this setting, as it propagates information across the group to perform robust imputation that compensates for partial observations. heter, heter = heter = (V Inspired by the approach in [Suh et al., 2025], we model the problem of response prediction for group members as link prediction in heterogeneous graph with relational GNN. Specifically, we construct an extended version of graph heter) that contains three types of nodes corresponding to group members , attributes (e.g. demographic features) , and choices contains two subsets of edges of different types, the edge set c. The edge set connects members to their features and response edge set connects members to their selected choices. Specifically, based on the training dataset train and query set , we construct the graph as follows. The node subsets and the edges between different types of nodes are given by: ❶ Group member nodes : We associate each group member in with node in heter and identify heter. ❷ Feature nodes: the group in the training dataset with the associated node subset in Each node represents an individual feature. For instance, in political survey, we discretize demographic features such as age, gender and education level into categorical bins, such as \"Age: 18-29\", each bin corresponds to feature node. member node connects to all matching feature nodes. ❸ Query-choice nodes c: Each node represents unique (query x, choice c) pair. member is connected to all the query-choice nodes (query x, choice c) such that it selects choice for query x. The GNN is trained to learn the feature and query-choice nodes embeddings and relation-specific message passing mechanism using link prediction objective. Letting be the embedding dimension, both the feature and query-choice nodes embeddings are initiated with d-dimensional learnable embeddings and the embeddings of member nodes are initiated with uniform vector := (1, . . . , 1) Rd. We construct the training objective as follows. First, we randomly mask subset of member-choice edges mask. Then for each member and any query with its set of choices q, the probability that selects choice is: p(c v, q) = (cid:80) exp(hv, hc cC exp(hv, hc /τ) /τ) , (2) where hv and hc are the final-layer embedding vectors obtained by the GNN using message passing on the partially masked graph, , is the dot product, and τ is learnable temperature parameter. The training optimization objective is to minimize the cross entropy loss for the masked edges: (cid:88) min log p(c v, q(c)). where q(c) is the query corresponding to choice c. (v,c)E mask (a) Embedding-based clustering and subgroup selection (b) Imputation using updated embedding Figure 2: Overview of heterogeneous GNN-based subgroup selection and imputation."
        },
        {
            "title": "3.2 Test-Time Adaptive Inference",
            "content": "c , and At test time, we are given new group of members test, candidate query set held-out evaluation set c. Responses to questions in serve as proxies for the latent entity of interest. We construct the heterogeneous graph for the testing group test following the same procedure as in the training phase with the same set of query-choice nodes and feature nodes as defined during training. Initially, when no query-response data is observed, we connect the test group members to feature nodes based on their demographic attributes. We initialize feature and query choice nodes with learned embeddings and member nodes uniformly. By integrating the meta-trained LLM and heterogeneous GNN, we iteratively select questions and respondents to reduce uncertainty about the group latent entity. Performance is evaluated by prediction accuracy on held-out queries in across all members, enabling robust inference under sparse interactions. Adaptive query selection. At each round t, given the interaction history t1, we select both test that maximize the expected information question Xt and subset of respondents Rt gain (EIG) about the groups latent entity. First, for member v, to quantify uncertainty about the latent entity Uv, we adopt de Finettis predictive perspective in [Wang et al., 2025], which uses the conditional entropy over future observations as proxy. Specifically, we define the uncertainty as: H(Uv Hv t1) := H(Y X = x, Hv t1), (3) (cid:88) xX where the entropy is computed over the held-out evaluation set h. This provides practical measure of the confidence with which we can predict the group member vs responses to diagnostic questions. We then compute EIG using the meta-trained LLM p(cid:98)θ as the sum of individual-level information gains: EIG(x; t1) = (cid:88) (cid:16) H(Uv vV test Hv t1) t [H(Uv (cid:98)Hv )] (cid:17) , (4) Xt = x, Hv p(cid:98)θ(Y where (cid:98)Y = Hv )} is the updated history. EIG is widely used criterion that assesses how much {(x, (cid:98)Y query-response pair reduces uncertainty about the latent entity, and thus we select the query that maximizes the group EIG: t1) is the simulated response of member to query x, and (cid:98)Hv t1 Xt = arg max {Xi xX }t1 i= EIG(x; t1). (5) Group-relational respondents selection. To select representative subgroup of respondents, we leverage relational structure using embeddings from the heterogeneous GNN. Let hv denote 7 the final layer embedding for member test. Since similar embeddings imply similar response patterns, we select diverse subgroup by maximizing coverage of the embedding space. Given budget k, we cluster members based on these embeddings and choose the cluster centers as the representative subgroup Rt. Message propagation for imputation and belief updating. At interaction round t, suppose . After collecting we have already selected query Xt and subgroup of respondents Rt actual responses Rt for the selected query Xt from the chosen subgroup of respondents Rt, we add new edges between group member nodes and query-choice nodes corresponding to the observation in the heterogeneous graph. We then perform message passing using the pre-trained GNN over the updated graph structure to compute refined node embeddings. These updated embeddings enable more accurate imputation of unobserved user responses to query Xt. For group member node (cid:60) Vt, the prediction of the unobserved response is obtained using Eq. (2). The imputed predictions for unobserved responses are added into the interaction history to inform the next round of adaptive selection. Specifically, we set for the subgroup for unqueried members. Then we add the of queried members and the GNN imputation edges corresponding to (cid:98)Y test into the heterogeneous graph and perform message passing to update node embeddings. This enrichment of the graph structure allows the GNN to propagate the newly acquired information throughout the network. Consequently, the integration of both collected and imputed responses effectively reduces uncertainty about the groups latent entity. This is achieved through two mechanisms: the updated node embeddings enhance the accuracy of future GNN-based imputations, while the more complete individual-level interaction history improves the LLMs uncertainty quantification for subsequent adaptive query selections. is the concatenation of observation Rt test )}, where (cid:98)Y test {(Xt, (cid:98)Y Rt t = t"
        },
        {
            "title": "4 Theoretical Results",
            "content": "Next, we provide theoretical justification for our approach. First, we prove that our two-stage selection algorithm is near-optimal under standard submodularity assumptions. Second, we connect the training objective in Eq. (1) to Fact 2.1, providing justification for our predictive framework. t)}T Performance guarantee for greedy selection. The adaptive elicitation strategy in Section 3.2 requires solving sequential decision problem: at each round, we select pair (Rt, xt) (a subgroup of respondents and query) to maximize the cumulative information gain over horizon . However, finding the optimal sequence {(R t=1 is computationally intractable t, due to its combinatorial complexity over both the user set and the query space as shown in Krause et al. [2008]. We therefore adopt the two-stage greedy algorithm in Section 3.2 that selects the pair (Rt, xt) at each step to maximize immediate information gain given the current interaction history. To justify this approach, we show that under standard assumptions, both the joint and two-stage greedy strategies achieve near optimal solutions, provably within constant factor of the optimum. Let : 2V R0 be utility function that measures the information gained about the groups latent properties, such as the expected information gain (EIG) defined in Section 3.2. The marginal gain of adding new set of pairs to history is defined as (A S) = (S A) (S). We now prove the following two theorems on near-optimality of 8 greedy algorithms. Theorem 4.1 (Near optimality of joint greedy selection). Under Assumption A.1, let {(Rt, xt)}T be the sequence selected by the greedy algorithm, where at each step: t= with S0 := and St := St1 tion A.1: (Rt, xt) = arg max AV , Rk xX {b1,...,bt1 } ({(R, x)} St1), (6) {(Rt, xt)}. If {(R t, t)}T t=1 is the optimal sequence, then under Assump- ({(R t, t)}T t=1) ({(Rt, xt)}T t=1). where C1 is constant independent of and . Theorem 4.2 (Near optimality of two-stage greedy algorithm). Let {(Rt, xt)}T selected by the two-stage greedy algorithm, where at each step: t=1 be the sequence xt = arg max xX ((V , x) St1), Vt = arg max , k ((R, xt) St1) (7) with S0 := and St := St1 t)}T t=1 is the optimal sequence, then under Assumptions A.1, there exists constant C2 independent of and , such that the following inequality holds: {(Rt, xt)}. If {(R t, ({(R t, t)}T t=1) C2 ({(Rt, xt)}T t=1), The above results extend the classic guarantee for submodular utility maximization proved in [Krause et al., 2008; Wang et al., 2025] to our setting of joint user-query selection, ensuring near optimality of greedy algorithms. Remark 4.3 (Greedy vs. Multi-step Planning). Theorem 4.1 and 4.2 guarantee that the greedy algorithm is already near optimal. This is further supported by our empirical results in Section 5.5, where more complex multi step planning methods provide only marginal improvements while incurring substantial computational cost. Although finite horizon planning may achieve higher information gain in principle, the greedy algorithms combination of tractability and provable near optimality makes it especially well-suited for our principled adaptive group elicitation framework, where efficiency is essential for real-time interaction. De Finettis theorem and meta-training. For individual-level queryresponse processes where latent entity governs group members behavior, as is the cases of political survey and student assessment, Remark A.9 proves the martingale condition holds and Theorem A.13 shows, conditional on the history , predictive inference can recover the latent entity . Thus, the training objective of the LLM in Eq. (1) is designed to directly minimize the KL-divergence between the predictive distribution induced by LLM and the true data-generating process, ensuring unbiased belief updating and uncertainty quantification about latent entity similar as in [Wang et al., 2025]."
        },
        {
            "title": "5 Experiments",
            "content": "Our experimental evaluation is structured around the following questions: 9 Figure 3: Accuracy on target questions over interaction rounds (one question per round) under different respondent budgets. Upper: CES. Middle: OpinionQA. Lower: Twin-2k. RQ1: Does adaptive group elicitation improve inference of group properties under different query budgets? Sec. 5.2 RQ2: What drives gains from group-relational respondent selection, and who benefits most? Sec. 5.3 RQ3: How do query selection and GNN imputation contribute to performance? Sec. 5.4 RQ4: How does multi-step planning compare to greedy selection in practice? Sec. 5."
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "Datasets. We evaluate adaptive elicitation on three opinion datasets: CES Schaffner and Ansolabehere [2025], nationally representative U.S. election survey; OpinionQA Santurkar et al. [2023], large-scale collection of public opinion questions across social and political topics; and Twin-2k Toubia et al. [2025], which measures economic preferences, cognitive biases, and personality traits for 2,000 individuals. All datasets include multiple-choice questions and respondent demographic attributes, enabling controlled evaluation of population-level inference under constrained observation budgets. Additional dataset details are provided in Appendix B.1. For all three datasets, we exploit geographic variation to construct realistic generalization setting. Respondents from the South region are used for LLM pretraining and meta-learning, including learning priors over response distributions and query utilities, while respondents from the West region are used for adaptive elicitation inference, where the model actively selects queries and infers unobserved responses. Additional cross-region results are reported in Appendix C.4 to assess robustness and geographic transferability. This regional split mirrors practical deployment scenarios in which elicitation policies learned from one population must generalize to related but distributionally distinct groups. Evaluation protocol and metrics. Methods iteratively select both questions and respondents over 10 multiple rounds under fixed observation budget. Performance is evaluated on disjoint set of target questions never queried during elicitation, measuring the ability to infer unobserved responses. We report average predictive accuracy on target questions as the primary metric, with Perplexity Jelinek et al. [1977] and Brier Score Glenn et al. [1950] used to assess calibration (Appendix B.2). Unless stated otherwise, results are averaged over 10 independent trials per dataset, each with 20 candidate questions and 5 target questions. Models and training. We meta-train our query-selection policy by fine-tuning pretrained Llama-3.1-8B model with LoRA and respondent-level data splits, using 80/20 train/validation in the in-distribution region (South) and holding out all respondents from the out-of-distribution region (West) for testing. We select the final checkpoint by validation loss, and additionally report results with smaller Llama-3.2-1B model trained under the same protocol (full finetuning) in Appendix C.3. For population modeling and imputation, we train heterogeneous R-GCN on graph with respondent, demographic-feature, and querychoice nodes, with all splits performed at the user level to prevent leakage. The GNN is trained under simulated partial observability (including partial-edge masking and cold-start users), and evaluation is conducted in strict cold-start setting where all userquery edges are removed from the message-passing graph. See Appendix B.4 for more details. Baseline methods. We compare our method against three baselines that vary in their query selection strategy and imputation mechanism, including variants that incorporate ideas from prior work Wang et al. [2025]: Meta-Random. This baseline selects questions uniformly at random at each interaction round. pretrained LLM is used to predict responses to held-out queries from the interaction history. No imputation is performed; responses to unqueried questions are neither imputed nor included in the interaction history. Meta-Greedy. Following the meta-policy framework of Wang et al. [2025], this baseline uses pretrained LLM to greedily select queries that maximize expected information gain given the current interaction history. At each round, the model scores all candidate questions based on their predicted informativeness and selects the highest-scoring one. No imputation is performed; responses to unqueried questions are neither imputed nor included in the interaction history. Meta-Greedy-Imp. This baseline extends Meta-Greedy by incorporating LLM-based imputation of missing responses. After each interaction round, pretrained LLM predicts responses for respondents that have not been queried, conditioned on the observed interaction history. These predicted responses are treated as observed when selecting subsequent queries; however, imputation is performed independently for each respondent and does not exploit population-level structure. Here, Meta denotes methods that adapt pretrained LLM as conditional query policy following the formulation in Wang et al. [2025]. Implementation details are deferred to Appendix B.4."
        },
        {
            "title": "5.2 Overall Gains from Adaptive Elicitation",
            "content": "Figure 3 shows accuracy on target questions over interaction rounds (one question per round), where columns correspond to different respondent budgets per round (10%, 30%, and 50%) 11 (a) CES, Budget = 30% (b) CES, Budget = 50% (c) OpinionQA, Budget = 30% (d) OpinionQA, Budget= 50% Figure 4: Relative recovery. Top % denotes respondents in the highest % by sensitivity. across three datasets. Across all datasets and budget levels, Ours consistently outperforms existing baselines. For example, at 10% respondent budget on CES, our method achieves consistent gains over the strongest baseline, with relative improvements ranging from 17.1% at round 1 to 12.6% by round 4. These results indicate that effective adaptive elicitation requires jointly reasoning about who to ask and what to ask, rather than optimizing query selection alone. By explicitly modeling group structure and propagating partial observations, our framework allows information collected from small subset of respondents to inform predictions for many others, leading to substantially more efficient use of limited budgets. Similar trends are observed for Perplexity and Brier Score (Appendix C.1). In addition, Meta-Greedy-Imp does not consistently outperform Meta-Greedy, suggesting that LLM-based imputation alone is unreliable without explicit population-level structure, and highlighting the importance of graph-based propagation."
        },
        {
            "title": "5.3 When Does Respondent Selection Help Most?",
            "content": "We analyze our group-relational respondent selection to understand where adaptive elicitation gains arise and which respondents benefit most. While population-level propagation can amplify observed information, its effectiveness depends on which respondents are observed. We stratify respondents by sensitivity, where high sensitivity users are those for whom observing groundtruth responses yields the largest performance improvements (see Appendix B.3 for details on how this is measured). We report gains using Relative Recovery, RelRec(t) = Acc(t)Acc(0) Acc(0) , which quantifies the fraction of the round-0 to full-observation gap recovered by round t. AccFull As shown in Figure 4, gains are highly non-uniform and concentrate on highly sensitive (hard) respondents, reaching up to 20% relative recovery under 50% budget across both datasets. Absolute accuracy exhibits consistent trends across sensitivity tiers and budgets (see Table 2). This pattern suggests that respondent selection focuses limited budget on the hardest-to-impute individuals, which in turn yields more accurate inference of latent group properties from sparse observations."
        },
        {
            "title": "5.4 Ablations on Query Selection and Imputation",
            "content": "Figure 5a compares three variants that fix respondent selection while varying query selection (random vs. greedy) and imputation (with vs. without GNN-based propagation), isolating the contribution of each component across budgets and interaction rounds. Effect of Query Selection. To isolate the effect of query selection, we compare RandomQ-Imp and GreedyQ-Imp while holding imputation fixed. Greedy query selection consistently achieves 12 (a) Ablation of query selection and imputation. Upper: CES. Lower: OpinionQA. (b) Imputation-only (Ours-Imp) vs. active observation (Ours). Upper: CES. Lower: OpinionQA. Figure 5: Ablations on query selection and imputation across datasets. Table 1: Round 4 accuracy (averaged over 5 trials) for greedy vs. multi-step planning under different budgets. Budget Method (Top %) Global Broad (50%) (100%) Inter. Hard (10%) (30%) Extreme (5%) 10% 30% 50% Greedy Multi-step Greedy Multi-step Greedy Multi-step 0.488 0.485 0.500 0. 0.507 0.507 0.426 0.439 0.464 0.478 0.510 0.504 0.384 0.398 0.438 0. 0.507 0.500 0.356 0.372 0.441 0.464 0.545 0.547 0.322 0.336 0.432 0. 0.561 0.542 higher accuracy than random querying, with performance gaps widening over interaction rounds. These gains are stable across respondent budgets, indicating that selecting more informative queries yields reliable benefits once imputation is enabled. Effect of Imputation. We compare GreedyQ-NonImp and GreedyQ-Imp, holding the query selection strategy fixed. GreedyQ selects questions greedily based on expected information gain, while NonImp and Imp respectively denote disabling or enabling GNN-based imputation over the respondent graph. Enabling imputation yields substantially larger gains across all respondent budgets and interaction rounds. Without imputation, accuracy improves only modestly as more interactions are observed; in contrast, GNN-based imputation propagates partial observations across the group, enabling faster improvement and more sustained performance gains. Benefits of Actively Selecting Observations. Figure 5b illustrates the effect of actively selecting which respondents to observe. Ours-Imp performs group-level imputation at each round without directly observing any respondent answers, instead leveraging population-level structure and group relations to infer responses. Comparing it with Ours shows that observing even small number of strategically selected respondents per round yields consistent gains beyond imputation alone, highlighting the complementary roles of observation and graph-based propagation."
        },
        {
            "title": "5.5 Multi-Step Planning: Are the Gains Worth It?",
            "content": "Table 1 compares greedy selection with multi-step planning across budgets and sensitivity tiers. Under multi-step planning, we first identifies the top-k=10 candidate queries according to the greedy criterion, then simulates their future impact by performing =3 simulated rollouts per candidate, with each rollout unrolling up to the remaining interaction steps. Rollouts approximate the downstream effect of query by alternately selecting subsequent queries and imputing responses under the current belief state, and the candidate with the highest average rollout utility is selected. Consistent with Remark 4.3, this procedure yields at most marginal gains over greedy selection: performance differences on the global population are negligible, and improvements on highly sensitive subsets are small, inconsistent, and largely disappear or reverse at higher budgets (50%). Overall, these limited and unstable gains do not justify the substantially higher computational cost of multi-step planning."
        },
        {
            "title": "6 Related Work",
            "content": "Inference for graphical models number of previous works have focused on modeling relational structures within groups and analyzing data with inherent graphical properties, such as spatial outcomes. Early efforts, including Gelfand and Vounatsou [2003]; Dobra et al. [2011]; De Oliveira [2012], employed conditional autoregressive models (CAR) to model graphical signals, where nodes outcome is predicted using the outcomes of its neighboring nodes. While these models are interpretable and effective for spatial economics and biomedical data, they are parametric and struggle to handle complex natural language information which is key component of tasks like political surveys, where queries and responses are often phrased in natural language. More recently, [Suh et al., 2025] proposed using heterogeneous GNNs to process group information without explicitly building relational structure inside group. We adopt this approach and use the heterogeneous GNN as tool for message propagation. Predictive inference De Finettis predictive perspective on inference establishes that uncertainty originates from unobserved data. Berti et al. [2004, 2013]; Fong et al. [2024] extend this viewpoint and propose computational schemes for predictive Bayesian inference. Building on the interpretation of In-Context Learning (ICL) as Bayesian inference [Xie et al., 2021; Ye and Namkoong, 2024], in Wang et al. [2025], the authors demonstrate that fine-tuned large language models (LLMs) constitute valid tools for predictive inference. Our work adopts this approach by utilizing the ICL ability of LLMs as an effective predictive inference mechanism. Multi-turn elicitation Latent entity elicitation involves gathering information about unobservable characteristics, such as student ability, political intentions, or patient health status. Sequential Bayesian experimental design has been predominant approach for such problems. Rainforth et al. [2024] provides comprehensive review of modern Bayesian experimental design, ranging from Bayesian adaptive methods in Cheng and Shen [2005] to deep learning techniques adopted in Foster et al. [2021]; Ivanova et al. [2021]. Recently, Wang et al. [2025] proposed using autoregressive models like LLMs without explicit prior specification. However, these methods are confined to individual-level inference. Our principle-based framework extends elicitation to group settings by incorporating the communitys relational information, enabling uncertainty quantification for collective latent entities and group-aware selection of queries and respondents, capability absent in prior work. 14 Group interaction with LLM Many studies [Hong et al., 2024; Khan et al., 2024; Tang et al., 2024; Luo et al., 2025] have focused on interaction between LLM agents or multi-agent systems and social groups. However, previous frameworks designed for collaborative decision-making cannot be directly applied to our group evaluation settings, which require decentralized predictions for each agent or group member based on partial observations. We address this by utilizing the heterogeneous GNN approach proposed in [Suh et al., 2025] to impute unobserved responses and enhance LLM-based evaluation, making better use of the strong natural language conversation and reasoning ability of LLM."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we study adaptive group elicitation under limited query and respondent budgets, and show that effective elicitation depends not only on which questions are asked, but also on which respondents are observed and how partial information is propagated across the population. We propose principled framework that jointly performs adaptive query selection and group-relational respondent selection, together with graph-based imputation of missing responses. Across multiple real-world opinion datasets, our approach consistently improves inference accuracy and calibration under tight budgets, achieving substantial gains in early elicitation rounds. Further analyses show that these gains concentrate on highly sensitive respondents and persist across model scales, training regimes, and geographic regions."
        },
        {
            "title": "References",
            "content": "Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar Gulcehre, Francis Song, Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu. Relational inductive biases, deep learning, and graph networks, 2018. URL https://arxiv.org/abs/1806.01261. Patrizia Berti, Luca Pratelli, and Pietro Rigo. Limit theorems for class of identically distributed random variables. The Annals of Probability, 32(3):20292052, 2004. ISSN 00911798. URL http://www.jstor.org/stable/3481602. Patrizia Berti, Luca Pratelli, and Pietro Rigo. Exchangeable sequences driven by an absolutely continuous random measure. The Annals of Probability, pages 20902102, 2013. Yi Cheng and Yu Shen. Bayesian adaptive designs for clinical trials. Biometrika, 92(3):633646, 2005. Asaph Chun, Steven Heeringa, and Barry Schouten. Responsive and adaptive design for survey optimization. Journal of Official Statistics, 34:581597, 09 2018. doi: 10.2478/jos-2018-0028. Victor De Oliveira. Bayesian analysis of conditional autoregressive models. Annals of the Institute of Statistical Mathematics, 64(1):107133, 2012. Don A. Dillman, Michael D. Sinclair, and Jon R. Clark. Effects of questionnaire length, respondent-friendly design, and difficult question on response rates for occupant-addressed census mail surveys. The Public Opinion Quarterly, 57(3):289304, 1993. ISSN 0033362X, 15375331. URL http://www.jstor.org/stable/2749091. Adrian Dobra, Alex Lenkoski, and Abel Rodriguez. Bayesian inference for general gaussian graphical models with application to multivariate lattice data. Journal of the American Statistical Association, 106(496):14181433, 2011. doi: 10.1198/jasa.2011.tm10465. URL https://doi.org/10.1198/jasa.2011.tm10465. PMID: 26924867. Edwin Fong, Chris Holmes, and Stephen Walker. Martingale posterior distributions. Journal of the Royal Statistical Society Series B: Statistical Methodology, 85(5):13571391, 02 2024. ISSN 1369-7412. doi: 10.1093/jrsssb/qkad005. URL https://doi.org/10.1093/jrsssb/ qkad005. Adam Foster, Desi Ivanova, Ilyas Malik, and Tom Rainforth. Deep adaptive design: Amortizing sequential bayesian experimental design. In International conference on machine learning, pages 33843395. PMLR, 2021. Alan Gelfand and Penelope Vounatsou. Proper multivariate conditional autoregressive models for spatial data analysis. Biostatistics, 4(1):1115, 2003. Brier Glenn et al. Verification of forecasts expressed in terms of probability. Monthly weather review, 78(1):13, 1950. Robert M. Groves and Steven G. Heeringa. Responsive design for household surveys: tools for actively controlling survey errors and costs. Journal of the Royal Statistical Society Series A, 16 169(3):439457, July 2006. doi: 10.1111/j.1467-985X.2006.00423.x. URL https://ideas. repec.org/a/bla/jorssa/v169y2006i3p439-457.html. Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber. MetaGPT: Meta programming for multi-agent collaborative framework. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=VtmBAGCN7o. Desi Ivanova, Adam Foster, Steven Kleinegesse, Michael Gutmann, and Thomas Rainforth. Implicit deep adaptive design: Policy-based experimental design without likelihoods. Advances in neural information processing systems, 34:2578525798, 2021. Fred Jelinek, Robert Mercer, Lalit Bahl, and James Baker. Perplexitya measure of the difficulty of speech recognition tasks. The Journal of the Acoustical Society of America, 62(S1): S63S63, 1977. Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, Samuel R. Bowman, Tim Rocktäschel, and Ethan Perez. Debating with more persuasive llms leads to more truthful answers. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks, 2017. URL https://arxiv.org/abs/1609.02907. Andreas Krause, Ajit Singh, and Carlos Guestrin. Near-optimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies. Journal of Machine Learning Research, 9(8):235284, 2008. URL http://jmlr.org/papers/v9/krause08a.html. Junyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Junwei Yang, Yiyang Gu, Bohan Wu, Binqi Chen, Ziyue Qiao, Qingqing Long, Rongcheng Tu, Xiao Luo, Wei Ju, Zhiping Xiao, Yifan Wang, Meng Xiao, Chenwu Liu, Jingyang Yuan, Shichang Zhang, Yiqiao Jin, Fan Zhang, Xian Wu, Hanqing Zhao, Dacheng Tao, Philip S. Yu, and Ming Zhang. Large language model agent: survey on methodology, applications and challenges, 2025. URL https: //arxiv.org/abs/2503.21460. Bruce D. Meyer, Wallace K. C. Mok, and James X. Sullivan. Household surveys in crisis. Journal of Economic Perspectives, 29(4):199226, November 2015. doi: 10.1257/jep.29.4.199. URL https://www.aeaweb.org/articles?id=10.1257/jep.29.4.199. G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An analysis of approximations for maximizing submodular set functionsi. Math. Program., 14(1):265294, December 1978. ISSN 0025-5610. doi: 10.1007/BF01588971. URL https://doi.org/10.1007/BF01588971. Tom Rainforth, Adam Foster, Desi Ivanova, and Freddie Bickford Smith. Modern bayesian experimental design. Statistical Science, 39(1):100114, 2024. Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and Tatsunori In International Conference on Hashimoto. Whose opinions do language models reflect? Machine Learning, pages 2997130004. PMLR, 2023. Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):6180, 2009. doi: 10.1109/TNN.2008.2005605. Brian F. Schaffner and Stephen Ansolabehere. Cooperative election study common content, 2024, 2025. Michael Schlichtkrull, Thomas Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max Welling. Modeling relational data with graph convolutional networks. In European semantic web conference, pages 593607. Springer, 2018. Joseph Suh, Suhong Moon, and Serina Chang. Rethinking llm human simulation: When graph is what you need, 2025. URL https://arxiv.org/abs/2511.02135. Xiangru Tang, Anni Zou, Zhuosheng Zhang, Ziming Li, Yilun Zhao, Xingyao Zhang, Arman Cohan, and Mark Gerstein. MedAgents: Large language models as collaborators for zero-shot medical reasoning. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 599621, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl. 33. URL https://aclanthology.org/2024.findings-acl.33/. Olivier Toubia, George Z. Gui, Tianyi Peng, Daniel J. Merlau, Ang Li, and Haozhe Chen. Twin2k-500: dataset for building digital twins of over 2,000 people based on their answers to over 500 questions. In First Workshop on Social Simulation with LLMs, 2025. Jimmy Wang, Thomas Zollo, Richard Zemel, and Hongseok Namkoong. Adaptive elicitation of latent information using natural language. In ICLR Workshop: Quantify Uncertainty and Hallucination in Foundation Models: The Next Frontier in Reliable AI, 2025. URL https: //openreview.net/forum?id=63c2erbMoc. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021. Naimeng Ye and Hongseok Namkoong. Exchangeable sequence models quantify uncertainty over latent concepts, 2024. URL https://arxiv.org/abs/2408.03307."
        },
        {
            "title": "A Proof of Theoretical Results",
            "content": "In this section, we provide proofs and justification for results in Section 4 with details. A.1 Near Optimality of Greedy Algorithm Recall the settings in Section 4, we let St = {(R1, x1), . . . , (Rt, xt)} denote the set of userquery pairs selected up to round t, : 2V R0 be utility function that measures the information gained about the groups latent properties, such as the expected information gain defined in Section 3.2. The marginal gain of adding new set of pairs to history is defined as (A S) = (S A) (S). Assumption A.1 (Submodularity and Monotonicity). The utility function satisfies the following properties for any subsets W1 and any : W2 (a) (Monotonicity). (W2) (W1). (b) (Submodularity). (W W1) (W W2). Monotonicity means that adding more observations does not decrease utility. Submodularity captures the principle of diminishing returns: the marginal benefit of adding new observation diminishes as the history grows, these property are commonly assumed in adaptive information acquisition problems [Krause et al., 2008; Wang et al., 2025]. Theorem A.2 (Restatement of Theorem 4.1). Let {(Rt, xt)}T algorithm, where at each step: t=1 be the sequence selected by the greedy (Rt, xt) = arg max RV , Rk xX {x1,...,xt1 } ({(R, x)} St1), with S0 = and St = St1 tion A.1: {(Rt, xt)}. If {(R t, t)}T t=1 is the optimal sequence, then under Assump- ({(R t, t)}T t=1) 2 1 e2 ({(Rt, xt)}T t=1). Proof of Theorem 4.1. Assume that is monotone and submodular on the set of all pairs (R, x) where and pool, as per Assumption A.1. We aim to show that for the greedy sequence {(Rt, xt)}T t=1, the following approximation holds: t=1 and the optimal sequence {(R t)}T t, (S ) 1 e2 (ST ) where Si = {(R1, x1), . . . , (Ri, xi)} for 1 and S0 = , and similarly for By monotonicity of , for any with 0 , (S write ) (S . We assume () = 0. Si). Since Si Si, we can (S Si) = (Si) + (cid:104) (S Si) (S j1 (cid:105) Si) T(cid:88) j=1 19 where (S j1 is the set of the first pairs in the optimal sequence. Fix i, denote = (S Si), we have Si) (S ) (Si) + T(cid:88) j=1 j. Now we bound the terms for 1 . Let Xi = {x1, . . . , xi Xi we define Γi = {j {1, . . . , } : for each Γi, let σi(j) be the unique index in {1, . . . , i} such that was selected). We bound in the following two cases. where Case 1: Γi. Here, is already in Si. By submodularity, since Sσi (j) }, the set of indices where the optimal query } be the set of queries in Si. First, is in Si, then = xσi (j) (i.e., the greedy step Si j1 Si: (Sσi (j)1 j)}) (Sσi (j)1) {(R j, j) was chosen to maximize gain from Sσi (j)1, By the greedy choice at step σi(j), the pair (Rσi (j), thus (Sσi (j)) (Sσi (j)1). Case 2: (cid:60) Γi Here, is not in Si. By submodularity, since Si j)}) (Si) {(R j, (Si j1 Si: At step + 1, the greedy algorithm selects pair (Ri+1, xi+1) that maximizes the gain from Si. Since j) is considered at step + 1, so: (cid:60) Xi, the pair (R j, (Si {(R j, j)}) (Si) (Si+1) (Si) Thus we have (Si+1) (Si) in this case. Splitting the sum over by the two different cases T(cid:88) j=1 = (cid:88) jΓi + (cid:88) j(cid:60)Γi j, from the above discussion, we have (cid:88) jΓi (cid:88) jΓi (cid:105) (cid:104) (Sσi (j)) (Sσi (j)1) , (cid:88) j(cid:60)Γi (cid:88) j(cid:60)Γi [f (Si+1) (Si)] Since σi is an injection from Γi to [i], the sum over Γi covers distinct increments, by monotonicity, it holds that (cid:105) (cid:104) (Sσi (j)) (Sσi (j)1) (cid:88) jΓi i(cid:88) t=1 [f (St) (St1)] = (Si), Note the number of (cid:60) Γi is Γi (cid:88) , by monotonicity, [f (Si+1) (Si)] (T Γi )[f (Si+1) (Si)] [f (Si+1) (Si)]. j(cid:60)Γi 20 Hence (cid:80)T j=1 (Si) + [f (Si+1) (Si)]. Substituting into the inequality for (S ), we have (S ) 2f (Si) + [f (Si+1) (Si)]. Define δi = (S ) 2f (Si). Then by the above inequality, δi+1 δi 2 δi = δi (cid:19) (cid:18) 1 2 Note that δ0 = (S ) since (S0) = 0, we have: δT δ0 (cid:19)T (cid:18) 1 2 (S 2 )e Rearranging gives (S ) 2 1e2 (ST ). To prove Theorem 4.2, we establish the auxiliary Assumption A.3 and Theorem A.4. Assumption A.3. There exists constant C(k) > 0 such that for any query and any subset of previously selected pairs, there exists subset R(x, S) with R(x, S) satisfying ((R(x, S), x) S) C(k) ((V , x) S). Theorem A.4 (Near optimality of two-stage greedy algorithm). Let {(Rt, xt)}T selected by the two-stage greedy algorithm, where at each step: t=1 be the sequence xt+1 = arg max xX ((V , x) St), Rt+1 = arg max , k ((V , xt+1) St), with S0 = and St = St1 tions A.1 and A.3: {(Rt, xt)}. If {(R t, t)}T t=1 is the optimal sequence, then under Assump- ({(R t, t)}T t=1) 2 1 e2C(k) ({(Rt, xt)}T t=1). Lemma A.5 (Bound on C(k)). Under Assumption A.1, Assumption A.3 holds with constant C(k) = 1 1 . By Lemma A.5, we can see that Theorem A.4 is generalization of Theorem 4.2. Theorem A.6 (Restatement of Theorem 4.2). Let {(Rt, xt)}T stage greedy algorithm, where at each step: t=1 be the sequence selected by the twoxt = arg max xX ((V , x) St1), Vt = arg max , k ((R, xt) St1) (8) with S0 := and St := St1 t)}T t=1 is the optimal sequence, then under Assumptions A.1, there exists constant C2 independent of and , such that the following inequality holds: {(Rt, xt)}. If {(R t, ({(R t, t)}T t=1) C2 ({(Rt, xt)}T t=1), Proof of Theorem 4.2. By Lemma A.5 and Theorem A.4, take C2 = (S ) Cf (ST ). 2 1 exp(2(1 1/e)) , we have Now we prove the Theorem A.4 and Lemma A.5. Proof of Theorem A.4. Assume that is monotone and submodular on the set of all pairs (R, x) where and pool, as per Assumption A.3. We aim to show that for the two-stage greedy sequence {(Rt, xt)}T t=1, the following approximation holds: t=1 and the optimal sequence {(R t, t)}T (S ) 2 1 e2C(k) (ST ) where Si = {(R1, x1), . . . , (Ri, xi)} for 1 and S0 = , and similarly for By monotonicity of , for any with 0 , (S write: ) (S . We assume () = 0. Si). Since Si Si, we can (S Si) = (Si) + T(cid:88) (cid:104) (S Si) (S (cid:105) Si) where (S j1 is the set of the first pairs in the optimal sequence. Fix i, denote = (S Si), we have Si) j=1 (S ) (Si) + T(cid:88) j=1 j. Now we bound the terms for 1 . Let Xi = {x1, . . . , xi we define Γi = {j {1, . . . , } : Xi for each Γi, let σi(j) be the unique index in {1, . . . , i} such that was selected). We bound in the following two cases. where Case 1: Γi. Here, is already in Si. By submodularity, since Sσi (j) }, the set of indices where the optimal query } be the set of queries in Si. First, is in Si, and = xσi (j) (i.e., the greedy step Si j1 Si: (Sσi (j)1 {(R j, j)}) (Sσi (j)1) j) was chosen to maximize gain from Sσi (j)1, By the greedy choice at step σi(j), the pair (Rσi (j), thus (Sσi (j)) (Sσi (j)1). Case 2: (cid:60) Γi Here, is not in Si. By submodularity, since Si j1 Si: By Assumption A.3 and the two-stage greedy selection: (Si {(R j, j)}) (Si) 1 C(k) [f (Si+1) (Si)] 22 Splitting the sum over by the two different cases T(cid:88) j=1 = (cid:88) jΓi + (cid:88) j(cid:60)Γi j, from the above discussion, we have (cid:88) (cid:105) (cid:104) (Sσi (j)) (Sσi (j)1) (cid:88) jΓi (cid:88) j(cid:60)Γi jΓi Γi C(k) [f (Si+1) (Si)] C(k) [f (Si+1) (Si)] Since σi is an injection from Γi to [i], the sum over Γi covers distinct increments, by monotonicity: (cid:104) (Sσi (j)) (Sσi (j)1) (cid:105) (cid:88) jΓi i(cid:88) t=1 [f (St) (St1)] = (Si)"
        },
        {
            "title": "Hence",
            "content": "(cid:80)T j=1 (Si) + C(k) [f (Si+1) (Si)]. Substituting into the inequality for (S ), we have (S ) 2f (Si) + C(k) [f (Si+1) (Si)] Define δi = (S ) 2f (Si). Then by the above inequality, δi+1 = δi 2[f (Si+1) (Si)] δi 2 C(k) δi = δi (cid:33) (cid:32) 1 2C(k) Note that δ0 = (S ) since (S0) = 0, we have: δT δ0 (cid:33)T (cid:32) 1 2C(k) (S )e 2C(k) Rearranging gives (S ) 1e2C(k) (ST ). Proof of Lemma A.5. Note that with fixed and history S, the problem of selecting an optimal subset with to maximize ((R, x) S) is monotonic, submodular univariate function, maximization problem with cardinality constraint and the near optimality bound has been proved in [Nemhauser et al., 1978]. A.2 Generalization of de Finetti Theorem and Related Justification This section presents the theoretical foundation for Fact 2.1, which generalizes de Finettis theorem, and establishes the martingale condition Remark A.9 and frequentist consistency Theorem A.13 of the copula-based predictive inference framework. This justification ensures that our method provides valid uncertainty quantification under the martingale condition, extending Bayesian principles to sequential prediction settings without explicit likelihood specification [Ye and Namkoong, 2024; Wang et al., 2025]. We begin by introducing key definitions. The conditionally identically distributed (c.i.d.) sequence serves as the foundational concept for predictive inference, generalizing exchangeable sequences through martingale formulation that accommodates broader data-generating processes. In the following context, we always use capital letter represent cumulative distribution function, and small letter represent distribution density function. Definition A.7 (Conditionally identically distributed (c.i.d.) sequence). sequence of random variables Y1, Y2, . . . is conditionally identically distributed (c.i.d.) if, for all 1 and all > 0, the following holds almost surely: (Yt+k Y1, . . . , Yt) = Pt(y) where Pt(y) := (Yt+1 yY1, . . . , Yt) denotes the predictive cumulative distribution function after observing Y1, . . . , Yt. This implies that, conditional on the past, all future observations are identically distributed according to the current predictive distribution. The following martingale formulation of c.i.d. sequences provides the theoretical basis for the approaches in [Fong et al., 2024; Ye and Namkoong, 2024] and our work. This formulation is particularly valuable as it enables the application of martingale convergence tools to predictive inference. Lemma A.8 (Martingale formulation, [Berti et al., 2004]). For sequence of random variables Y1:, it is conditionally identically distributed (c.i.d.) if and only if the sequence of predictive distribution functions {Pi(y)} constitutes martingale for each y. That is, for > 1: E[Pt(y) Y1, . . . , Yt1] = Pt1(y) almost surely. (9) This martingale property ensures unbiased updating of the predictive distribution sequence, condition termed predictive coherence in [Fong et al., 2024]. Now we first provide basic results that justifies the martingale assumption in the individuallevel elicitation setting. Remark A.9. Let Y1, Y2, . . . be an exchangeable sequence of random variables. Under the Bayesian mixture model with latent variable and prior µ(U ), the sequence of predictive distributions = σ (Y1:t). That is, for all 1: Mt = p(Yt+1 Y1:t) forms martingale with respect to the filtration E[Mt+1 t] = Mt almost surely. This martingale property justifies our assumption in Section 4 that the predictive distributions of underlying data generation process satisfy the martingale condition, as it demonstrates that whenever there exists latent entity governing the observed outcomes, as is the case in political surveys, student assessments, and other group elicitation scenarios, the sequence of predictive beliefs naturally exhibits martingale behavior under coherent Bayesian updating. Proof of Remark A.9. By the law of total expectation and Bayesian updating: E[Mt+1 Y1:t] = E[p(Yt+2 Y1:t+1) Y1:t] (cid:90) = p(Yt+2 Y1:t+1)p(Yt+1 Y1:t)dYt+1. Substituting the Bayesian predictive distributions gives: E[Mt+1 Y1:t] = (cid:90) (cid:32)(cid:90) (cid:33) )dµ(U Y1:t+1) p(Yt+2 p(Yt+ Y1:t)dYt+1. Using Bayes rule for the posterior update, we have: E[Mt+1 Y1:t] = (cid:90) (cid:90) (cid:90) (cid:90) = p(Yt+2 ) p(Yt+1 )dµ(U Y1:t) Y1:t) p(Yt+ p(Yt+1 Y1:t)dYt+1 p(Yt+2 )p(Yt+1 )dµ(U Y1:t)dYt+1. By Fubinis theorem and the fact that Y1:t) = Mt, completing the proof. (cid:82) p(Yt+1 )dYt+1 = 1, we obtain p(Yt+2 Y1:t) = p(Yt+1 The martingale property is crucial as it facilitates the application of convergence theorems. By Doobs martingale convergence theorem, Pt(y) converges almost surely to limiting random distribution function P(y) as . The distribution of this limit constitutes the martingale posterior. As demonstrated in Theorem A.13, in settings where latent entity governs data generation, this limiting distribution effectively recovers the latent entity. To operationalize this theoretical framework, [Fong et al., 2024] established that any predictive distribution sequence satisfying the martingale condition in Eq. (9) necessarily arises from copula-based updating rule. This provides constructive method for building valid predictive inference machines. We now introduce the bivariate copula concept before presenting the key lemma. Definition A.10 (Bivariate Copula). bivariate copula : [0, 1]2 [0, 1] is cumulative distribution function on the unit square with uniform marginal distributions. Its density is denoted c(u, v). Sklars Theorem establishes that any bivariate distribution can be expressed through its marginals and copula capturing the dependence structure. Lemma A.11 (Corollary 1 in [Fong et al., 2024]). The sequence of conditional densities p0, p1, . . . satisfies the martingale condition Eq. (9) if and only if there exists unique sequence of bivariate copula densities c1, c2, . . . such that pt+1(y) = ct+1 {Pi(y), Pt(yt+1)}pt(y) for {0, 1, . . .}, where Pt represents the cumulative distribution function corresponding to pt. Lemma A.11 guarantees that the following copula-based update rule satisfies the martingale condition in Eq. (9). The Gaussian copula provides practical choice due to its parametric } simplicity and flexibility in modeling dependencies. Specifically, let {αn n=1 be deterministic learning rate sequence with αn = O(1/n), and let cρ denote the density of the Gaussian copula with correlation parameter ρ. Denoting by Pi() the cumulative distribution function corresponding to pt(), the following update rule after observing yt+1 induces predictive distribution sequence satisfying the martingale condition: pt+1(y) = (cid:104) 1 αt+1 + αt+1 cρ {Pt(y), Pi(yt+1)}(cid:105) pt(y) (10) We now present the central convergence and consistency results for c.i.d. sequences induced by the predictive distribution sequence in Eq.(10), which provide the theoretical justification for our adaptive elicitation framework. These results primarily draw from [Berti et al., 2004; Fong et al., 2024]. The following theorem justifies the existence of limiting distribution under the martingale condition, generalizing the traditional de Finetti theorem. While de Finettis theorem guarantees latent entity governing data generation under exchangeability, this extension accommodates the broader class of c.i.d. sequences, making it applicable to non-exchangeable settings commonly encountered in practical machine learning scenarios. Theorem A.12 ([Berti et al., 2004]). For c.i.d. sequence, the sequence of predictive distributions Pt converges weakly to limiting random probability measure almost surely as . Theorem A.13 establishes the frequentist consistency of the predictive density pt estimated via the copula method. It demonstrates that pt converges to the true data-generating density f0 iid f0. This consistency result validates predictive inference under the martingale when data Y1:t condition, ensuring that we can recover the latent entity from the limiting distribution. The combination of Theorems A.12 and A.13 guarantees that our predictive inference framework provides valid approach for latent entity recovery and uncertainty quantification through predictive sampling. Specifically, Theorem A.12 generalizes de Finettis representation by ensuring the existence of limiting martingale posterior for c.i.d. sequences, while Theorem A.13 ensures that this posterior concentrates on the true data-generating distribution, thereby bridging the generalized representation with the traditional notion of latent entity. Theorem A.13. Under the following conditions: (a) ρ (0, 1) and the learning rate sequence is αt = a(t + 1)1 with < 2/5, (b) There exists < such that f0(y)/p0(y) for all Rd (i.e., the initial guess p0 has tails at least as heavy as f0), the sequence pt given by the updating rule Eq. (10) is Hellinger consistent at f0. That is, 2(pt, f0) = 0 almost surely, lim where the squared Hellinger distance is defined as 2(p, ) = 1 (cid:82) (cid:112) p(y)f (y)dy."
        },
        {
            "title": "B Experiment Details",
            "content": "B.1 Datasets We evaluate group adaptive elicitation on three real-world opinion datasets spanning political attitudes, social values, and economic preferences. All datasets consist of real human responses 26 collected via probability-based or nationally representative survey panels. Below, we summarize each dataset with respect to its data source, preprocessing procedures, demographic attributes, question selection, and experimental usage. CES. The Cooperative Election Study (CES)1 is one of the largest academic surveys of the American public and core resource for studying U.S. political attitudes and electoral behavior. We use the 2024 wave of the CES and filter out respondents with missing values in either demographic attributes or selected opinion questions, resulting in final sample of 3,326 respondents. We retain eight demographic variables capturing age cohort, race, gender, education, family income, religion, party identification, and political ideology. From the survey, we extract 30 policy questions, each formulated as binary stance indicating whether the respondent supports or opposes given policy. The selected questions span major policy domains, including health care, gun control, immigration, abortion, climate policy, policing and public safety, and executive authority (e.g., executive orders). These questions are chosen to ensure topical diversity, substantial response variation, and clear stance-based interpretation suitable for adaptive elicitation. OpinionQA. OpinionQA is derived from the American Trends Panel (ATP)2, Pew Research Centers primary probability-based panel for U.S. public opinion research, which consists of approximately 10,000 adults recruited to be nationally representative of the U.S. population, with surveys administered in English and Spanish. We use three ATP wavesW50, W54, and W92covering topics related to American families, economic inequality, and political typology. After filtering respondents with missing demographic information or incomplete question responses, the resulting dataset contains 2,368 respondents. We include eight standardized demographic variables capturing age category, gender, education, party affiliation, political ideology, income, religion, and race/ethnicity. We extract 30 opinion questions, each with 35 discrete response options (e.g., degrees of agreement or categorical preferences), enabling evaluation of adaptive elicitation beyond binary supportoppose judgments. Twin-2K. Twin-2K3 is four-wave, nationally representative U.S. panel fielded in January February 2025 on Prolific, designed for studying LLM-based human simulation. Participants complete questions spanning demographics, personality scales, cognitive ability tests, economic preferences, and heuristics-and-biases experiments. We exclude respondents with missing demographic attributes or incomplete responses on selected questions, yielding final sample of 2,368 respondents. Demographic variables include age, sex, education, political party, political ideology, income, religion, and race. From the full survey, we select 40 questions primarily related to economic preferences and behavioral decision-making, which are well-suited for evaluating whether adaptive elicitation can infer latent economic and behavioral traits from limited observations. 1https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/X11EP6 2https://www.pewresearch.org/american-trends-panel-datasets/ 3https://huggingface.co/datasets/LLM-Digital-Twin/Twin-2K-500 27 B.2 Evaluation Metrics Accuracy. Let pi instance i, and let yi RC denote the predicted probability distribution over answer options for {1, . . . , C} be the ground-truth class index. Accuracy is defined as: Acc = 1 N(cid:88) (cid:20) i= (cid:21) arg max pi,c = yi , (11) where is the number of evaluated instances and I[] is the indicator function. Perplexity (PPL). Perplexity is defined as the exponential of the average negative log-likelihood: PPL = exp 1 N(cid:88) i=1 log pi(yi) , (12) where pi(yi) denotes the predicted probability assigned to the ground-truth option. Note that this differs from standard token-level language model perplexity; here PPL measures uncertainty over discrete choices. Brier Score (BS). The Brier score measures the squared error between the predicted probability distribution and the one-hot encoded ground truth: BS ="
        },
        {
            "title": "1\nN",
            "content": "N(cid:88) C(cid:88) i=1 c=1 (cid:0)pi,c yi,c (cid:1)2 , (13) where yi {0, 1}C is the one-hot representation of yi. B.3 Respondent Sensitivity Definition Group-level propagation does not benefit all respondents equally: observing the ground-truth responses of some individuals yields substantially larger gains than observing others. To characterize this heterogeneity, we introduce notion of sensitivity, which quantifies how informative respondent is for downstream inference. Definition. We define respondents sensitivity as the improvement in predictive performance attributable to observing that respondents ground-truth responses, relative to an imputationonly baseline. Concretely, for each respondent u, we compute Sensitivity(u) = Accfull(u) Accimpute(u), (14) where Accfull(u) denotes the per-user prediction accuracy when all responses from are observed (full-observation setting), and Accimpute(u) denotes the accuracy when none of us responses are observed and predictions rely solely on group-level imputation. This difference captures the marginal utility of observing respondent beyond what can be inferred from others. 28 (a) CES (b) OpinionQA Figure 6: Calibration Performance (Brier Score) across Elicitation Rounds on CES and OpinionQA. Estimation protocol. Sensitivity is estimated using an independently trained LLM-based predictor. For each dataset and random seed, we evaluate per-user accuracies under two controlled settings: (i) full-observation setting, in which all responses from given user are revealed, and (ii) an imputation-only setting, in which the same user is entirely unobserved. We compute the accuracy gap for each user and average results across multiple random seeds to reduce variance. Users are then ranked by their sensitivity scores in descending order. Sensitivity tiers and Interpretation. To facilitate analysis, we stratify respondents into sensitivity tiers based on percentile ranks of the accuracy gap distribution. Specifically, we consider the top 50%, 30%, 10%, and 5% most sensitive respondents, corresponding to users for whom observing ground-truth responses yields the largest performance improvements. These tiers allow us to study where adaptive elicitation and group-relational respondent selection provide the greatest benefits, and to contrast performance on highly sensitive respondents against the broader group. Intuitively, high-sensitivity respondents are those whose preferences are poorly captured by group-level propagation aloneeither because they are atypical, weakly connected, or exhibit high uncertaintysuch that direct observation substantially improves inference. In contrast, low-sensitivity respondents are well-explained by relational structure and contribute limited additional information when observed. This stratification enables more fine-grained evaluation of adaptive elicitation strategies beyond aggregate accuracy. B.4 Implementation Details All experiments were conducted on single machine equipped with 8 NVIDIA RTX 6000 Ada Generation GPUs (48 GB memory each) using CUDA 12.9. LLM Meta-Training. We split the training data by entity at the respondent level, using an 80%/20% split for training and validation in the in-distribution region (South), while reserving all users from the out-of-distribution region (West) exclusively for testing. To meta-train our query selection model, we initialize pretrained Llama-3.1-8B model and fine-tune it using parameter-efficient adaptation with LoRA, with rank = 8, scaling factor α = 24, and dropout rate 0.1, applied to the attention projection layers. Optimization is performed using AdamW with learning rate 1 104, β = (0.9, 0.95), and weight decay 0.1. Training is conducted in mixed 29 (a) CES (b) OpinionQA Figure 7: Calibration Performance (Perplexity) across Elicitation Rounds on CES and OpinionQA. precision (BF16) with gradient accumulation to achieve an effective batch size of 16, maximum sequence length of 1024 tokens, gradient clipping at 1.0, and cosine learning-rate schedule with linear warmup. The model is trained for 10,000 iterations with the final checkpoint selected based on the lowest validation loss. For robustness, we additionally meta-train smaller Llama3.2-1B model under the same framework, using full-parameter fine-tuning, batch size 8, and reduced weight decay of 0.01, while keeping the learning rate, optimizer settings, sequence length, precision, scheduling strategy, and evaluation protocol unchanged. Heterogeneous GNN. We train the GNN imputation module on heterogeneous opinion graph comprising three node types: group member nodes (respondents), feature nodes (demographic), and querychoice nodes (answer options). To prevent information leakage, all splits are performed at the user level, assigning all responses from respondent to the same split. For geographic generalization, region-based filtering is applied prior to splitting, with the South region treated as in-distribution and the West region as out-of-distribution (OOD). Within the in-distribution region, users are randomly partitioned into training and validation sets with an 80/20 split using fixed random seed, while all users from the OOD region are held out exclusively for testing. During training, we simulate partial observability via an epoch-wise user-level masking protocol: in each epoch, training users are divided into three disjoint groups, including fully observed users whose edges are retained for message passing, partially observed users for whom fixed fraction (50%) of edges are masked and used as supervision targets, and cold-start users whose edges are entirely withheld for supervision. Validation and test splits are evaluated under strict cold-start setting, where all userquery edges are removed from the message-passing graph. Each supervision instance corresponds to multi-class classification problem over the candidate option set of the associated query. Unless otherwise specified, we use hidden dimension of 64, two RGCN layers, dropout rate 0.1, batch size 2048, learning rate 2 103 with weight decay 104, and train for 500 epochs with fixed random seed. Table 2: Accuracy across sensitivity tiers. Left: CES; Right: OpinionQA. Budget Method (Top %) Global Broad (50%) (100%) Inter. Hard (10%) (30%) Extreme (5%) Budget Method (Top %) Global Broad (50%) (100%) Inter. Hard (10%) (30%) Extreme (5%) 0% 30% 50% Full Imp. 0.760 0.705 0.635 0.477 Random Group-relational Random Group-relational 0.784 0.790 0.793 0.801 0.773 0.782 0.795 0.812 0.737 0. 0.770 0.798 0.641 0.684 0.714 0.780 100% Full Obs. 0. 0.842 0.844 0.861 0.420 0.617 0.690 0.720 0. 0.909 0% 30% 50% Full Imp. 0. 0.383 0.308 0.220 Random Group-relational Random Group-relational 0.480 0. 0.490 0.496 0.451 0.460 0.487 0.496 0.416 0.432 0.472 0.485 0.398 0. 0.473 0.522 100% Full Obs. 0.507 0.570 0. 0.687 0.168 0.377 0.415 0.465 0.529 0."
        },
        {
            "title": "C Additional Experiment Results",
            "content": "C.1 Calibration Results Figure 6 and 7 reports calibration results on CES and OpinionQA, measured by Brier Score and Perplexity as function of elicitation rounds under different observation budgets (10%, 30%, and 50%). Lower values indicate better calibration. Across both datasets and all budgets, our method consistently achieves the lowest Brier scores and perplexities after the first elicitation round, and the gap widens as more rounds are performed. This indicates that adaptive elicitation not only improves point accuracy but also produces better-calibrated predictive distributions. In contrast, Meta-Greedy-Imp, which relies on LLM-independent imputation without group-relational respondent selection or propagation, often exhibits degraded calibration as the number of elicitation rounds increases, particularly under low-budget settings. This degradation is most pronounced in perplexity, indicating overconfident predictions arising from the propagation of uncertain or weakly informative signals. Meta-Random and Meta-Greedy show modest but consistent improvements with additional rounds, but remain substantially worse than our approach. Overall, these results demonstrate that group-relational respondent selection is crucial for calibration: observing highly informative respondents early leads to uncertainty reduction that propagates reliably across the population, whereas indiscriminate propagation can amplify miscalibrated beliefs. The trends are consistent across datasets and budgets, confirming that the calibration gains of our method are robust and not an artifact of specific domain. C.2 Ablation on Respondent Selection This subsection provides supplementary ablation results for Section 5.3, focusing on how respondent selection interacts with sensitivity tiers under different observation budgets. While the main text emphasizes relative recovery to highlight where adaptive elicitation gains concentrate, here we report absolute accuracy at the final elicitation round to give more complete picture of performance across the population. Table 2 reports round-4 accuracy on CES and OpinionQA, respectively, stratified by respondent sensitivity. Sensitivity tiers are defined based on the accuracy gap between full observation and imputation-only settings (Appendix B.3), ranging from Global (all respondents) to increasingly restrictive subsets of high-sensitivity respondents (Broad, Intermediate, Hard, and Extreme). We compare Group group-relational respondent selection against random baseline at fixed budgets. Across both datasets, Group-relational respondent selection consistently outperforms random selection at the same budget, with gains that become more pronounced as sensitivity increases. In particular, under 50% budget, the largest absolute improvements are observed (a) CES (b) OpinionQA Figure 8: Accuracy across interaction rounds under different query budgets using the Llama-3.2-1B backbone. (a) CES-West (b) CES-Midwest Figure 9: Accuracy across interaction rounds under different query budgets on CES across regions. in the Hard and Extreme tiers, where group-relational selection recovers substantial fraction of the full-observation performance. In contrast, gains in the Global or Broad tiers are smaller, reflecting the fact that many low-sensitivity respondents are already well explained by grouplevel propagation. These ablations support the main conclusion of Section 5.3: respondent selection is most effective when targeted toward highly sensitive (hard) individuals, and its benefits cannot be attributed solely to increased observation volume. Instead, selecting the right respondents is crucial for translating limited budgets into meaningful accuracy gains. C.3 Results with Different Base Model We evaluate the robustness of our approach to the choice of base language model by repeating the main elicitation experiments using smaller backbone, Llama-3.2-1B, trained with full fine-tuning. Figure 8 reports accuracy across interaction rounds under different query budgets on CES and OpinionQA. Despite the substantially reduced model capacity, our method consistently outperforms all baselines across datasets, budgets, and rounds, and exhibits qualitative 32 trends similar to those observed with the Llama-3.1-8B backbone fine-tuned with LoRA. In particular, our approach achieves rapid gains in early rounds, especially under low-budget settings, indicating that group-relational respondent selection and group-relational propagation remain effective even when the underlying LLM is smaller. Notably, we observe that full fine-tuning of the Llama-3.2-1B model yields performance generally comparable to LoRA-based fine-tuning of the Llama-3.1-8B model. These results indicate that the benefits of adaptive elicitation are largely attributable to the elicitation and propagation mechanism itself, rather than reliance on highly capable backbone, and that the framework remains effective across different model scales and training regimes. C.4 Results with Different Region We further evaluate the robustness of adaptive elicitation under geographic distribution shift by testing the model on regions not used during meta-training. Following the same experimental setup as in Section 5.2, elicitation policies are learned using respondents from the South region and evaluated on held-out regions, including the West and Midwest. Figure 9 reports accuracy across interaction rounds under different query budgets on CES for these regions. Across both regions, our method consistently outperforms baseline approaches and exhibits qualitative trends similar to those observed in the in-distribution setting. In particular, adaptive elicitation achieves substantial gains in the early rounds, even under low-budget constraints, indicating that group-relational respondent selection and group-relational propagation transfer effectively across regions. While absolute accuracies vary due to regional differences in demographic composition and opinion distributions, the relative improvements over Meta-Random, MetaGreedy, and Meta-Greedy-Imp remain stable. These results suggest that the proposed elicitation framework generalizes beyond the region used for meta-training and remains effective under realistic geographic distribution shifts."
        }
    ],
    "affiliations": [
        "Ben-Gurion University of the Negev",
        "Columbia University",
        "University of North Carolina at Chapel Hill"
    ]
}