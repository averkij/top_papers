{
    "paper_title": "Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory",
    "authors": [
        "Lin Long",
        "Yichen He",
        "Wentao Ye",
        "Yiyuan Pan",
        "Yuan Lin",
        "Hang Li",
        "Junbo Zhao",
        "Wei Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce M3-Agent, a novel multimodal agent framework equipped with long-term memory. Like humans, M3-Agent can process real-time visual and auditory inputs to build and update its long-term memory. Beyond episodic memory, it also develops semantic memory, enabling it to accumulate world knowledge over time. Its memory is organized in an entity-centric, multimodal format, allowing deeper and more consistent understanding of the environment. Given an instruction, M3-Agent autonomously performs multi-turn, iterative reasoning and retrieves relevant information from memory to accomplish the task. To evaluate memory effectiveness and memory-based reasoning in multimodal agents, we develop M3-Bench, a new long-video question answering benchmark. M3-Bench comprises 100 newly recorded real-world videos captured from a robot's perspective (M3-Bench-robot) and 929 web-sourced videos across diverse scenarios (M3-Bench-web). We annotate question-answer pairs designed to test key capabilities essential for agent applications, such as human understanding, general knowledge extraction, and cross-modal reasoning. Experimental results show that M3-Agent, trained via reinforcement learning, outperforms the strongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o, achieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-web and VideoMME-long, respectively. Our work advances the multimodal agents toward more human-like long-term memory and provides insights into their practical design. Model, code and data are available at https://github.com/bytedance-seed/m3-agent"
        },
        {
            "title": "Start",
            "content": "Seeing, Listening, Remembering, and Reasoning: Multimodal Agent with Long-Term Memory Lin Long1,2,,, Yichen He1,, Wentao Ye1,2, Yiyuan Pan1,3, Yuan Lin1,, Hang Li1, Junbo Zhao2, Wei Li1 1ByteDance Seed, 2Zhejiang University, 3Shanghai Jiao Tong University Equal contribution, Corresponding author"
        },
        {
            "title": "Abstract",
            "content": "We introduce M3-Agent, novel multimodal agent framework equipped with long-term memory. Like humans, M3-Agent can process real-time visual and auditory inputs to build and update its long-term memory. Beyond episodic memory, it also develops semantic memory, enabling it to accumulate world knowledge over time. Its memory is organized in an entity-centric, multimodal format, allowing deeper and more consistent understanding of the environment. Given an instruction, M3-Agent autonomously performs multi-turn, iterative reasoning and retrieves relevant information from memory to accomplish the task. To evaluate memory effectiveness and memory-based reasoning in multimodal agents, we develop M3-Bench, new long-video question answering benchmark. M3-Bench comprises 100 newly recorded real-world videos captured from robots perspective (M3-Bench-robot) and 929 web-sourced videos across diverse scenarios (M3-Bench-web). We annotate question-answer pairs designed to test key capabilities essential for agent applications, such as human understanding, general knowledge extraction, and crossmodal reasoning. Experimental results show that M3-Agent, trained via reinforcement learning, outperforms the strongest baseline, prompting agent using Gemini-1.5-pro and GPT-4o, achieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-web and VideoMME-long, respectively. Our work advances the multimodal agents toward more human-like long-term memory and provides insights into their practical design. Model, code and data are available at https://github.com/bytedance-seed/m3-agent. Date: August 14, 2025 Correspondence: linyuan.0@bytedance.com Project Page: https://m3-agent.github.io 5 2 0 2 3 1 ] . [ 1 6 3 7 9 0 . 8 0 5 2 : r"
        },
        {
            "title": "Introduction",
            "content": "Imagine that in the future household robot can autonomously carry out household tasks without your explicit instructions; it must have learned the operational rules of your home through daily experiences. In the morning, it hands you cup of coffee without asking \"coffee or tea?\", because it has gradually formed memory of you, tracking your preferences and routines through long-term interaction. For multimodal agent, achieving such level of intelligence fundamentally relies on three capabilities: (1) continuously perceiving the world through multimodal sensors; (2) storing its experiences in long-term memory and gradually building knowledge about the environment; (3) reasoning over this accumulated memory to guide its actions. To achieve the goals, we propose M3-Agent, novel multimodal agent framework equipped with long-term memory. As shown in Figure 1, it operates through two parallel processes: memorization, which continuously perceives real-time multimodal inputs to construct and update long-term memory; and control, which interprets external instructions, reasons over the stored memory, and executes the corresponding tasks. During memorization, M3-Agent processes the incoming video stream, capturing both fine-grained details and high-level abstractions by generating two types of memory, analogous to human cognitive systems [42, 43]: Episodic memory: Records concrete events observed within the video. For example, \"Alice takes the coffee and says, cant go without this in the morning,\" and \"Alice throws an empty bottle into the green garbage bin.\" Semantic memory: Derives general knowledge from the clip. For example, \"Alice prefers to drink coffee in the morning\" and \"The green garbage bin is used for recycling.\" The generated memories are then stored in long-term memory, which supports multimodal information such as faces, voices and textual knowledge. Moreover, the memory is organized in an entity-centric structure. For example, information related to the same person (e.g., their face, voice and associated knowledge) is connected in graph format, as shown in Figure 1. These connections are incrementally established as the agent extracts and integrates semantic memory. Figure 1 Architecture of M3-Agent, comprising multimodal large language model (MLLM) and multimodal long-term memory. The system consists of two parallel processes: memorization and control. During memorization, M3-Agent processes video and audio streams online to generate episodic and semantic memory. During control, it executes instructions by iteratively reasoning and retrieving from long-term memory. The long-term memory is structured as multimodal graph. During control, M3-Agent leverages its long-term memory to reason and complete tasks. It autonomously retrieves relevant information from its long-term memory across different dimensions, such as events or 2 characters. Instead of using single-turn retrieval-augmented generation (RAG) to load memory into context [20], M3-Agent employs reinforcement learning to enable multi-turn reasoning and iterative memory retrieval, resulting in higher task success rates. The memorization task relates to long video description [11, 16, 55] but goes beyond it, introducing two key challenges: (1) Infinite information processing. Memorization requires handling infinitely long input streams. Existing methods optimize architectural efficiency to process longer, but still finite, offline videos [12, 38 40, 56]. In contrast, M3-Agent continuously processes arbitrarily long multimodal streams online, more closely mimicking how human long-term memory forms, through ongoing perception and incremental experience integration. (2) World knowledge construction. Traditional video description [1, 22, 24, 25, 53] often focuses on low-level visual details while overlooking high-level world knowledge [10, 17, 34] such as character identity and entity attributes, which may lead to ambiguity and inconsistency in long-term contexts. M3-Agent addresses this by incrementally building world knowledge through an entity-centric memory structure. It forms rich, multimodal representations of key entities, enabling coherent and consistent long-term memory. We evaluate M3-Agent on long video question answering (LVQA), where the videos simulate the multimodal input streams (visual and auditory) received by an agent. Most existing LVQA benchmarks [2, 9, 48, 60] mainly focus on visual understanding, such as action recognition and spatial/temporal perception, leaving gap in evaluating higher-level cognitive abilities that rely on long-term memory and are crucial for real-world agents, such as understanding humans, extracting general knowledge, and performing cross-modal reasoning. To bridge this gap, we introduce M3-Bench, new LVQA benchmark designed to evaluate multimodal agents ability to reason with long-term memory. M3-Bench consists videos from two sources: (1) M3-Bench-robot, consisting of 100 real-world videos recorded from robots perspective, and (2) M3-Bench-web, comprising 929 YouTube videos spanning broader range of content and scenarios. We define five question types, as shown in Table 1, targeting different aspects of memory-based reasoning. In total, we annotate 1,344 QA pairs for M3-Bench-robot and 5,037 QA pairs for M3-Bench-web. We conduct experiments on the M3-Bench-robot, M3-Bench-web, and VideoMME-long [9]. Results show that M3-Agent trained via reinforcement learning outperforms all baselines on all three benchmarks. Compared to the strongest baseline, Gemini-GPT4o-Hybrid, which implements M3-Agent framework by prompting Gemini-1.5-Pro [41] for memorization and GPT-4o [15] for control, M3-Agent improves accuracy by 6.7%, 7.7%, and 5.3% on M3-Bench-robot, M3-Bench-web, and VideoMME-long, respectively. Our ablation study demonstrates the importance of semantic memory: removing it reduces accuracy by 17.1%, 19.2% and 13.1% on M3-Bench-robot, M3-Bench-web, and VideoMME-long, respectively. Furthermore, we examine the impact of RL training, inter-turn instructions, and reasoning mode on control performance. Specifically, RL training improves accuracy by 10.0%, 8.0%, and 9.3% on the respective benchmarks. Removing inter-turn instruction results in 10.5%, 5.8% and 5.9% decrease in accuracy, while disabling reasoning mode leads to accuracy declines of 11.7%, 8.8% and 9.5% on the three benchmarks. The main contributions of this paper are summarized as follows: We introduce M3-Agent, novel framework for multimodal agents with long-term memory. M3-Agent continuously processes real-time multimodal inputs (seeing and listening), incrementally builds world knowledge by generating both episodic and semantic memories (remembering), and performs reasoning over these memories to complete complex instructions (reasoning). We develop M3-Bench, new LVQA benchmark designed to evaluate the effectiveness of memory and memory-based reasoning for multimodal agents. Our experiments demonstrate that M3-Agent, trained by reinforcement learning, consistently outperforms agents based on prompted commercial models across multiple benchmarks."
        },
        {
            "title": "2.1 Long-Term Memory of AI Agents",
            "content": "Long-term memory is essential for AI agents [8], enabling them to retain distant contextual information and support more advanced reasoning. common approach is to append entire agent trajectories, such as dialogues [27, 31, 44, 59] or execution trajectories [14, 27, 29, 35, 36, 46], directly to memory. Beyond raw data, some methods incorporate summaries [14, 21, 44, 59], latent embeddings [6, 28, 40, 56], or structured knowledge representations [33, 50]. Recent systems further construct sophisticated memory architectures, giving agents finer control on memory management [5, 18, 44]. However, most existing approaches focus on LLM agents. In contrast, multimodal agents process broader range of inputs and store richer, multimodal content and concepts in memory. This also introduces new challenges, particularly in maintaining consistency of long-term memory. Moreover, just as humans acquire world knowledge through experience, multimodal agents should form internal world knowledge in memory, rather than merely storing description of experience."
        },
        {
            "title": "2.2 Online Video Understanding",
            "content": "For multimodal agent, memory formation is closely related to online video understanding, challenging task requires real-time processing of video streams and decision-making based on past observations. Traditional approaches to long video understanding, such as extending the context window in multimodal models [4, 58] or compressing visual tokens to increase temporal coverage [19, 47, 47], do not scale effectively for infinitely long video streams. In practical settings, such as interactive agent scenarios, reprocessing the entire video history for each new instruction is computationally prohibitive. To improve scalability, memory-based methods [12, 40, 56, 57] introduce memory modules that store encoded visual features for future retrieval. These architectures are suited for online video processing. However, they face fundamental limitation: maintaining long-term consistency. Because they store only visual features, these methods struggle to maintain coherent tracking of entities such as human identities or evolving events over time. With the rapid advancement of large multimodal and language models [1, 15, 41, 51, 53], the Socratic Models framework [26, 54, 55] has emerged as promising approach for online video understanding. By leveraging multimodal models to generate video descriptions as language-based memory, this method improves scalability. Nevertheless, it still encounters challenges in maintaining long-term consistency across complex, evolving video content."
        },
        {
            "title": "3 Datasets",
            "content": "In this section, we introduce M3-Bench, an LVQA dataset designed to evaluate the capability of multimodal agents to perform reasoning over long-term memory. Each instance in M3-Bench comprises long video simulating the perceptual input of an agent, along with series of open-ended question-answer pairs. The dataset is organized into two subsets: (1) M3-Bench-robot, which contains 100 real-world videos recorded from robots first-person perspective, and (2) M3-Bench-web, which includes 929 web-sourced videos covering wider variety of content and scenarios. To comprehensively assess an agents ability to recall past observations and perform memory-based reasoning, we curate five distinct types of questions, as summarized in Table 1. Overall, M3-Bench is featured by (1) long-duration, real-world videos that encompass diverse real-life scenarios relevant to the deployment of multimodal agents, and (2) challenging questions that extend beyond shallow perceptual understanding and require complex reasoning over long-term contexts. Figure 2 presents examples from M3-Bench. The overall statistics of M3-Bench is shown in Figure 3. Table 2 provides comparative analysis with existing LVQA benchmarks. The remainder of this section elaborates on the data collection and annotation procedures for M3-Bench-robot and M3-Bench-web, respectively. 4 Figure 2 Examples from M3-Bench. M3-Bench-robot features long videos from realistic robotic work scenarios, while M3-Bench-web expands the video diversity to support broader evaluation. The question-answering tasks are designed to assess multimodal agents ability to construct consistent and reliable long-term memory, as well as to reason effectively over that memory."
        },
        {
            "title": "3.1 M3-Bench-robot",
            "content": "Robots are representative examples of multimodal agents. general-purpose robot should be able to maintain long-term memory and reason with it to guide its actions. For example, as it processes observations, the robot may remember persons name, where they left their coat, or their coffee preference. Reasoning over long-term memory enables higher-level cognitive functions, such as inferring persons personality, understanding relationships among individuals, or identifying the functions of surrounding objects. To systematically evaluate these capabilities, we record new collection of videos from robots perspective and manually annotate corresponding question-answer pairs. Scripts Design We begin by designing video scripts for M3-Bench-robot across seven everyday scenarios where robots are expected to operate: living room, kitchen, bedroom, study, office, meeting room, and gym. Each script involves one robot interacting with two to four humans. Annotators are instructed to design humanrobot interactions that reflect the desirable capabilities of general-purpose service robots. To ensure diversity in the script content, we introduce multiple thematic variations for each scenario. For example, the living room scenario may include themes such as meeting friends, engaging in family conversations, or hosting Thanksgiving party. Annotators write one script for each theme, thereby ensuring broad coverage and high variability across scripts. Specifically, each script is structured as sequence of discrete events and questions. Some events are designed as reference events, containing information relevant to future question. Questions may appear after any event or at the end of the script. When appearing within the event sequence, questions are typically closely tied to the current plot; moving them can alter their answers or affect difficulty. An example script is provided in Table 8 ( A.5). 5 Question Type Multi-detail Reasoning Explanation and Example This requires aggregating multiple pieces of information distributed across the video. Example: Which collection has the highest starting price among the five items shown in the video? The agent must identify and recall the starting price from five distinct segments, then compare these recalled prices to determine the highest. Multi-hop Reasoning Cross-modal Reasoning Human Understanding This involves step-by-step reasoning across different segments to reach conclusion. Example: Which bubble tea shop did they visit after going to Ding Cha? The agent must first locate the visit to Ding Cha, then follow subsequent segments to identify the next bubble tea shop. This requires reasoning across multiple modalities, such as visual and audio content. Example: (Bob shows Robot red folder and says, \"The confidential documents should go in this folder,\" then shows white folder and says, \"The normal documents should go in this one.\") Which folder should confidential documents be placed in? The agent must combine visual cues (folder color) with dialogues to infer the correct answer. This involves reasoning about human-related attributes such as identity, emotions, personality, or relationships. Example: Is Lucas skilled at cooking? The video does not directly reveal the answer, but the agent must aggregate Lucass behavior across multiple cooking scenes to infer his skill level. General Knowledge Extraction This evaluates whether the agent can extract general knowledge from specific events. Example: (A person is shown classifying different groceries into various shelves of refrigerator) Which shelf is suitable for storing vegetables? The agent must recognize typical storage rules from its observation to answer correctly. Table 1 Explanations of different question types and their corresponding examples in M3-Bench. To ensure the complexity of video content and the quality of downstream video filming and annotation, annotators must meet the following criteria: Annotate at least 15 questions, each labeled with the reference events required to answer them. Ensure each question is assigned to at least one type listed in Table 1. Each script must contain at least 70 events to ensure minimum video duration of 30 minutes. Video Filming Recording videos with actual robots poses significant challenges due to high operational costs, hardware limitations, and deployment complexities. To address these constraints, we adopted practical alternative: employing human actors to simulate robot behavior. This approach simplifies data collection while preserving both the first-person robot perspective and the multimodal quality required for our benchmark. Each script involves multiple actors, with one designated to simulate the robot. This actor wore head-mounted camera equipment to capture the robots egocentric visual and auditory perspective. The resulting footage constituted the final videos in M3-Bench-robot. To ensure diversity and minimize location bias, we recruited 67 actors and filmed across 51 distinct locations, with no more than three videos recorded at each location. We collected two types of audio tracks for each video. The first was directly recorded by the head-mounted device, reflecting the raw auditory input robot would naturally receive, including ambient sounds and spatial acoustic variations. The second was captured using individual lapel microphones worn by each actor, providing high-fidelity voice recordings to complement the primary audio stream. Annotations After recording the videos, annotators curated QA pairs for each video. Although some questions was pre-scripted, the final video content may deviate from the original script due to realistic filming conditions. Consequently, not all scripted questions remain applicable. Annotators carefully reviewed each scripted question to determine whether it should be retained, revised, or discarded, and provided corresponding 6 Figure 3 Statistical overview of M3-Bench benchmark. Each question may correspond to multiple question types. Benchmark #Videos Len.(s) #QAs Anno. Form. Agent Present Modal QA CrossEgoSchema [30] LongVideoBench [48] HourVideo [2] MVBench [23] Video-MME [9] MLVU [60] M3-Bench-robot M3-Bench-web 5,063 3,763 500 3,641 900 1,730 100 929 180.0 473.0 2,742.0 16.0 1,017.9 930.0 2,039.9 1,630. 5,063 M/A 6,678 12,976 M/A 4,000 2,700 3,102 M/A 1,276 5,037 M C O/C Human Knowledge QA QA Table 2 Comparison of M3-Bench with existing long-video question answering benchmarks across key dimensions: number of videos (#Videos), average video length in seconds (Len.), number of QA pairs (#QAs), annotation method (Anno., M/A denote manually/automatic), question format (Form., O/C indicate open-ended/close-ended), presence of an agent in the video (Agent Present), inclusion of cross-modal reasoning questions (Cross-Modal QA), human understanding questions (Human QA), and questions about general knowledge (Knowledge QA). answers when necessary. For all retained or revised questions, annotators were required to specify the precise timestamp at which the question should be asked. Importantly, the timestamp must precede the robots corresponding response or action to avoid inadvertently revealing the answer. In addition to the script-based questions, annotators were also required to create new questions to ensure that each video contained at least 12 QA pairs. All newly added questions should also align with one or more of the question types listed in Table 1. Besides QA pair creation, annotators also generated subtitles to enhance the usability of the dataset. Specifically, they manually annotated the start and end timestamps for each dialogue segment, together with the speakers identity and the transcribed dialogue content. Full annotation guidelines, annotators information and quality control details for M3-Bench-robot annotation are presented in Appendix A."
        },
        {
            "title": "3.2 M3-Bench-web",
            "content": "To further increase video diversity, we collect extra videos from YouTube following existing practice [7, 9, 32]. Video Collection The video collection adopts question-driven approach: annotators selected videos that could support the design of at least five questions belong to the types listed in Table 1. This strategy naturally led to the selection of videos with rich narratives and complex inter-entity relationships, making them well-suited for assessing agents capability of reasoning with long-term memory. To promote video diversity and avoid overrepresentation of easily annotated content, we provided annotators 7 with reference list of video categories emphasizing high information density and relevance to real-world multimodal agent applications. Annotators are required to submit up to 20 videos from each category and are allowed to suggest new categories, which are included if deemed sufficiently distinct from the existing category list by the authors. The final dataset comprises 46 distinct video types, as summarized in Figure 3. QA Annotations The same annotator who collected the video also generates at least five corresponding questionanswer pairs. Each question must correspond to at least one type defined in Table 1. In M3-Bench-web, all question timestamps are set to the end of the video. All questions are required to be specific, objective, and have single unambiguous answer that can be reasonably derived from clues in the video, ensuring both the effectiveness and fairness of subsequent evaluation. For example, questions answerable from multiple perspectives or with ambiguous references, such as \"the man\" or \"in the middle part of the video,\" are not considered valid. Appendix provides the full annotation guidelines, annotators information, and quality control details for M3-Bench-web."
        },
        {
            "title": "3.3 Automatic Evaluation",
            "content": "We use GPT-4o as an automatic evaluator for M3-Bench by prompting it to assess the correctness of generated answer by comparing it to the corresponding reference answer for the same question. The prompt template is shown in Table 18 ( H.1). To validate GPT-4o as reliable judge, we construct test set of 100 randomly sampled triples, each consisting of question, its reference answer, and generated answer from our method or various baselines ( 5.1). Three authors independently evaluate the correctness of each generated answer, and GPT-4os judgments are compared with the majority vote of human annotations. GPT-4o achieves 96% agreement with human judges, confirming its effectiveness as an automatic evaluator."
        },
        {
            "title": "4 Approach",
            "content": "As shown in Figure 1, M3-Agent consists of multimodal LLM and long-term memory module. It operates through two parallel processes: memorization, which enables continuous processing of arbitrarily long video streams and builds lifelong memory; and control, which reasons over long-term memory to execute instructions. In the following subsections, we detail long-term memory storage, memorization, and control, respectively. Attribute Description id type content embedding weight extra_data unique identifier for the node. The modality type of the node (e.g., text, image, audio). For example, natural language memory is stored as text node, face as an image node, and spoken dialogue as an audio node. The raw content of the node, such as plain text, base64 image, or base64 audio. The vector representation of the node content, used for similarity-based retrieval. numeric value indicating the confidence of the node. JSON object containing additional metadata, such as timestamps. Table 3 Attributes and their descriptions for memory node."
        },
        {
            "title": "4.1 Long-Term Memory",
            "content": "Long-term memory is implemented as an external database that stores information in structured, multimodal format (text, images, audio). Specifically, memory entries are organized as memory graph, where each node represents distinct memory item. Each node includes unique ID, modality type, raw content, weight, embeddings, and other metadata such as timestamps. See Table 3 for details. Nodes are connected by 8 Function search_node search_clip Description Accepts query and returns the top-k most relevant nodes. Supports multimodal queries (text, image, or audio) and modality-specific retrieval. Retrieves memory from the top-k relevant video clips for text query, including episodic and semantic memory. Table 4 Search functions supported by long-term memory. undirected edges that capture logical relationships between memory items. These connections serve as threads that facilitate the retrieval of relevant memories. The agent constructs its memory by incrementally adding new text, image, or audio nodes, along with edges connecting them, or by updating the content and weights of existing nodes. Conflicting information may be introduced during construction. To resolve this, M3-Agent applies weight-based voting mechanism during inference: entries that are reinforced more frequently accumulate higher weights, allowing them to override conflicting entries with weaker reinforcement. This mechanism ensures the robustness and consistency of the memory graph over time. Search Tool To facilitate memory access, we provide suite of search tools that enable the agent to retrieve relevant memories based on specific requirements. In particular, we implement two types of search mechanisms operating at different levels of granularity, as summarized in Table 4. Detailed implementation of these retrieval mechanisms is provided in Appendix C."
        },
        {
            "title": "4.2 Memorization",
            "content": "As shown in Figure 1, during memorization, M3-Agent processes the incoming video stream in clip-by-clip manner, generating two types of memory: episodic memory, which captures visual and auditory content from the raw video; and semantic memory, which extracts general knowledge such as character identities, attributes, relationships, and other world knowledge. Semantic memory not only enriches the memory content, but also provides additional retrieval cues, enhancing retrieval effectiveness for control process. Consistent Entity Representation key challenge in constructing high-quality long-term memory is maintaining consistent representations of core conceptssuch as main characters and objectsacross arbitrarily long time spans. Existing works typically generates language-based descriptions, such as \"a man with beard\" or \"a woman in red dress\". However, such textual descriptions are inherently ambiguous and prone to inconsistencies when accumulated over time. To address this issue, M3-Agent preserves the original multimodal features and constructs persistent identity representations within its long-term memory. This approach provides more stable and robust foundation ensuring consistency over time. Specifically, we equip M3-Agent with suite of external tools, including facial recognition and speaker identification. These tools extract the faces and voices of characters appearing in the clip and return their corresponding identities from the long-term memory. Each extracted face or voice is associated with an existing node by using search_node function or assigned to newly created node. The resulting identifiers (face_id or voice_id) serve as persistent references to the corresponding characters. By leveraging the globally maintained memory graph as unifying structure, M3-Agent ensures consistent character identity mapping across local memories from different clips, thereby forming coherent long-term memory. This approach can be generalized to encode more concepts, such as key locations or objects, into long-term memory, thereby further improving the consistency of memory generation. Detailed implementations of both tools are provided in Appendix C. Memory Generation Having the face and voice identities, M3-Agent continues to generate both episodic and semantic memory. Each character must be referenced by their face_id or voice_id. For example: \"<face_1> wears red hat and blue top,\" or \"<voice_2> speaks to <face_3>, How are you doing today?\" This mechanism ensures that each character is unambiguously grounded with physical features stored in long-term memory. Specially, in semantic memory, M3-Agent can perform cross-modal reasoning to infer relationships between different entity IDs (e.g., linking face and voice belonging to the same person). These inferred equivalences can then be used to update the connections between face and voice nodes in the memory graph. Once linked, the pair is treated as single character. During retrieval, connected nodes are unified under shared <character_id>, enabling the model to reason about characters more consistently across modalities. With respect to the output format, M3-Agent generates both episodic and semantic memory as list of text entries. Each entry is stored in the memory graph as text node, except for entity ID relationships represented as edges. As described in the memory storage, conflicting information is resolved through voting mechanism. For example, <voice_3> corresponds to <face_0>, but in some challenging clips, the system might temporarily link it to different face. Over time, as correct associations accumulate, the weight of the correct mapping (<voice_3>, <face_0>) increases and dominates. This allows the system to robustly learn and maintain accurate knowledge, even in the presence of occasional local errors."
        },
        {
            "title": "4.3 Control",
            "content": "When an instruction is received, the control process is triggered. As illustrated in Figure 1, during control, M3-Agent autonomously performs multi-turn reasoning and invokes search functions to retrieve relevant information from long-term memory, up to maximum of rounds. M3-Agent can independently decide which search function to invoke, such as search_clip to retrieve specific memory clips, or search_node to obtain the character ID of particular character. Specifically, the MLLM in M3-Agent can be viewed as the policy model πθ. Given question and the current long-term memory M, the control process is executed according to Algorithm 1. To facilitate this process, we design three types of prompts: (1) system prompt at the beginning of each session, specifying the overall task objectives. (2) An instruction prompt appended at the start of each round (except the last), providing the question and detailed guidance. (3) last-round prompt, used only in the final round, signaling the agent that it is the final opportunity to respond. The specific prompts are provided in Table 22 ( H.3). Algorithm 1 Control Process Require: Input question q, policy model πθ, long-term memory M, maximum number of rounds H. Ensure: complete trajectory τ generated by the agent. 1: τ [{role: \"system\", content: Format(system_prompt, q)}, {role: \"user\", content: instruction_prompt}] 2: 3: 0 4: while < do 5: 6: 7: τi πθ( τ ) Append {role:\"assistant\", content: τi} to τ action, information Parse(τi) if action = \"[Search]\" then Initialize the trajectory Execute up to rounds Extract action and content from τi memory Search(M, information) else Break Search memory using the content as query The trajectory ends when action is \"[Answer]\" end if + 1 Append {role: \"user\", content: memory + instruction_prompt} to τ Append search results and prompt for next round 8: 9: 10: 11: 12: 13: 14: Append {role: \"user\", content: memory + last_round_prompt} to τ if = 1 then 15: 16: end if 17: 18: end while 19: return τ"
        },
        {
            "title": "4.4 Training",
            "content": "We apply reinforcement learning to optimize the M3-Agent. Although the memorization and control are conceptually handled by single model, we trained two separate models to achieve optimal performance. Memorization relies strong multimodal understanding, while control requires strong reasoning capabilities. Accordingly, we initialized each model with different foundation models: Qwen2.5-Omni [49], an advanced open-source multimodal model supporting both visual and audio inputs, for memorization; and Qwen3 [51], an open-source large language model with powerful reasoning abilities, for control. The training data are sourced from our in-house video dataset, which we have permissions for model training. We collect videos along with corresponding question-answer pairs, adhering to the same annotation standards used in the M3-Bench-web dataset. In total, the training dataset comprises 500 long videos, corresponding to 26,943 30-second clips, and 2,736 question-answer pairs. Memorization To improve the models ability to generate desired memory, we perform imitation learning on Qwen2.5-Omni-7b to create memory-7b-sft. The process begins with constructing high-quality synthetic demonstration dataset. We segment each video in the dataset into 30-second clips, and corresponding memory annotations are generated through three-stage process: (1) Episodic memory synthesis: We perform hybrid annotation strategy by jointly prompting Gemini-1.5-Pro and GPT-4o. Accordingly, GPT-4o supplies frame-level cues, which serve as priors for Gemini-1.5-Pro; the two outputs are merged to form richer narrative summaries than either alone. (2) Identity equivalence detection: We propose an algorithm that automatically mines high-confidence meta-clips, short monologue clips containing exactly one face and one voice, from long video to construct global face-voice correspondence. These meta-clips offer clear identity cues, enabling accurate face-voice pairing. Once the global mapping is established, it can be used to automatically annotate face-voice associations in any 30-second subclip. (3) Other semantic memory synthesis: We design prompt templates to extract semantic memories from various perspectives, guiding semantic memories to include information listed in Table 10 ( D). Details of the data synthesis process are provided in Appendix D. In total, we synthesize 10,952 samples: 10,752 for training and 200 for validation. Fine-tuning is conducted for 3 epochs with learning rate of 1e 5 and batch size of 16, using 16 GPUs with 80GB memory. Control We first set up the environment for RL training. For each video in the dataset, we generate the corresponding long-term memory using memory-7b-sft. For any given question, the agent is restricted to searching within the memory generated from the video associated with that question. We then train the policy model πθ using DAPO [52], which initialized from control-32b-prompt. For each question-answer pair (q, a) sampled from training dataset D, the policy πθ rollouts group of trajectories , using the algorithm shown in Algorithm 1. For each trajectory τi, the final submitted answer yi is τ i=1 extracted and evaluated using the GPT-4o evaluator introduced in Section 3.3. The reward of the i-th trajectory is given by: (cid:40) Ri = 1, gpt4o_evaluator(q, a, yi) = True 0, otherwise Then, the advantage of the i-th response is calculated by normalizing the group-level rewards {Ri}G i=1 : ˆAi,t = Ri mean({Ri}G std({Ri}G i=1) i=1) . Note that during training, we compute loss only on LLM-generated tokens. The optimization objective is: JDAPO(θ) = (q,a)D,{τi}G i=1πold θ (q) (cid:34) 1 (cid:80)τi t=1 (cid:80)G i=1 clip (cid:18) πθ(τi,tτi,<t) πold θ (τi,tτi,<t) , 1 ϵlow, 1 + ϵhigh I(τi,t) (cid:19) ˆAi,t (cid:88) τi (cid:88) t=1 i=1 (cid:19)(cid:35) , I(τi,t) min (cid:18) πθ(τi,tτi,<t) πold θ (τi,tττ,<t) ˆAi,t, s.t. 0 < (cid:88) i=1 Ri < G, (3) where the indicator I(τi,t) = 1 if τi,t is an LLM-generated token; and 0 otherwise. Table 14 ( F) lists the hyperparameters used during the DAPO training process. 11 (1) (2)"
        },
        {
            "title": "5.1 Baselines",
            "content": "We evaluate MM-Agent against three types of baselines: Socratic Models This baseline adapts the Socratic Models framework [54], which uses multimodal model to describe 30-second video clips. These descriptions are stored as long-term memory. To answer question, an LLM performs retrieval augmented generation (RAG) [20]: It first invokes search_clip function to retrieve memory relevant to the question, and then generates response based on the retrieved content. We implement both closed-source and open-source multimodal models for memory generation: Gemini-1.5-Pro [41]: Takes the full 30-second video clip as input. GPT-4o [15]: Since it does not process audio, we provide video frames sampled at 0.5 fps and ASR transcripts. Qwen2.5-Omni-7b [49]: An advanced open-source multimodal model that supports both visual and audio inputs. It receives the full video as input. Qwen2.5-VL-7b [1]: An open-source vision-language models with SOTA results in visual-language tasks. Like GPT-4o, it receives both video frames (sampled at 0.5 fps) and ASR transcripts. For all variants, GPT-4o serves as the LLM for RAG-based question answering. We apply extensive prompt engineering to optimize performance for each setup. All prompts are provided in Appendix H.2. Online Video Understanding Methods We further compare our approach with three online video understanding frameworks: MovieChat [40], MA-LMM [12], and Flash-VStream [56]. Unless otherwise specified, we adopt their official pretrained weights and default configurations. MovieChat [40]: It uses sliding-window to extract frame-level features and stores them in hybrid memory; the LLM performs QA conditioned on this memory. MA-LMM [12]: It processes frames in an online manner, consisting of feature extraction (1 fps), temporal modeling (100-frame input), and LLM decoding. Flash-VStream [56]: It adopts two-stage asynchronous pipeline: stream video frame compression (1 fps), and LLM-based QA over the compressed features. Agent Methods We also compare M3-Agent with agents implemented via prompting closed-source commercial models. Specifically, we consider the following two baselines: Gemini-Agent: Gemini-1.5-Pro is prompted separately for memory access and control process. During memory access, it receives the full video with audio, facial recognition results and speaker identification results to generate episodic and semantic memories, denoted as memory-gemini-prompt. In the control process, it performs memory searches and generates responses, referred to as control-gemini-prompt. Gemini-GPT4o-Hybrid: We also evaluate setup where GPT-4o is prompted to perform memory search and generate responses (control-gpt4o-prompt). The memory access remains handled by memory-gemini-prompt. The prompts are provided in Appendix H.3. We set the maximum number of execution rounds to 5 for M3-Agent and all agent-based baselines. In the implementation of search_clip, the top 2 most relevant memory clips (i.e., = 2) are returned if any relevant clips are found. If none of such clips can be found, the method returns an empty result."
        },
        {
            "title": "5.2 Dataset and Evaluation",
            "content": "We evaluate M3-Agent and all baselines on both M3-Bench-robot and M3-Bench-web. To demonstrate the generality of our approach, we also test M3-Agent on long-video understanding benchmark, VideoMME12 long [9], following its official evaluation protocol1."
        },
        {
            "title": "5.3 Main Results",
            "content": "Method M3-Bench-robot M3-Bench-web MD MH CM HU GK All MD MH CM HU GK All VideoMME-Long Socratic Model Qwen2.5-Omni-7b Qwen2.5-VL-7b Gemini-1.5-Pro GPT-4o 2.1 2.9 6.5 9.3 1.4 3.8 7.5 9.0 1.5 3.6 8.0 8. 1.5 4.6 9.7 10.2 2.1 3.4 7.6 7.3 2.0 3.4 8.0 8.5 8.9 11.9 18.0 21.3 8.8 10.5 17.9 21.9 13.7 13.4 23.8 30. 10.8 14.0 23.1 27.1 14.1 20.9 28.7 39.6 MovieChat MA-LMM Flash-VStream 13.3 25.6 21.6 9.8 23.4 19.4 12.2 22.7 19. 15.7 39.1 24.3 7.0 14.4 14.1 11.2 24.4 19.4 12.2 26.8 24.5 6.6 10.5 10.3 12.5 22.4 24. 17.4 39.3 32.5 11.1 15.8 20.2 Online Video Understanding Methods 11.3 14.9 23.2 28.7 12.6 24.3 23.6 Agent Method Gemini-Agent Gemini-GPT4o-Hybrid 15.8 21.3 17.1 25.5 15.3 22.7 20.0 28.8 M3-Agent 32.8 29.4 31.2 43.3 15.5 23.1 19. 16.9 24.0 29.3 35.9 20.9 26.2 33.8 37.6 34.6 43.8 45.0 52. 34.1 41.2 30.7 45.9 28.4 44.3 59. 53.9 48.9 42.2 46.9 38.0 38.8 19.4 7.2 25.0 55.1 56.5 61. Table 5 Results on M3-Bench-robot, M3-Bench-web, and VideoMME-long. We also present comparison of all methods across different question types in M3-Bench: multi-detail reasoning (MD), multi-hop reasoning (MH), cross-modal reasoning (CM), human understanding (HU), and general knowledge extraction (GK). As shown in Table 5, M3-Agent outperforms all baselines on M3-Bench-robot, M3-Bench-web, and VideoMMElong. Specifically, on M3-Bench-robot, M3-Agent achieves 6.3% accuracy improvement over the strongest baseline, MA-LLM. On M3-Bench-web and VideoMME-long, it surpasses the strongest baseline, GeminiGPT4o-Hybrid, by 7.7% and 5.3%, respectively. We further evaluate M3-Agent against all baselines across different question types in M3-Bench. M3-Agent shows strong performance in human understanding and cross-modal reasoning. Specifically, compared to the best-performing baseline on M3-Bench-robot, MA-LMM, M3-Agent achieves improvements of 4.2% in human understanding and 8.5% in cross-modal reasoning. On M3-Bench-web, M3-Agent outperforms the top baseline, Gemini-GPT4o-Hybrid, with gains of 15.5% and 6.7% in the respective categories. These results demonstrate M3-Agent superior ability to maintain character consistency, deepen human understanding, and effectively integrate multimodal information."
        },
        {
            "title": "5.4 Ablation Study",
            "content": "Memorization Model memory-gemini-prompt memory-7b-prompt memory-7b-sft (M3-Agent) memory-7b-sft w/o equivalence memory-7b-sft w/o semantic memory M3-Bench-robot M3-Bench-web Video-MME-Long 46.3 52.7 28. 25.3 30.7 19.5 13.6 39.9 48.9 39.7 29.7 50.8 61.8 52.1 48. Table 6 Impact of different memorization models on final performance. The control model is fixed as control-32b-rl. To evaluate the impact of memorization on overall performance, we fixed the control model to control-7b-rl and compared different memorization methods, as shown in Table 6. First, we replaced the memory with that generated by memory-gemini-prompt, resulting in accuracy drops of 2.0%, 2.6%, and 9.1% on M3-Bench-robot, 1https://github.com/thanku-all/parse_answer/blob/main/eval_your_results.py 13 M3-Bench-web, and VideoMME-long, respectively. This suggests that memory-7b-sft produces higher-quality memory than memory-gemini-7b. Next, we evaluated memory-7b-prompt, which led to accuracy reductions of 5.4%, 9.0%, and 11.0% on the same benchmarks, highlighting the importance of imitation learning in generating effective memory. Finally, we ablated key components in the memory generation process. The results show that removing character identity equivalence or semantic memory significantly degrades QA performance. Control Model control-32b-grpo control-8b-prompt control-8b-rl control-14b-prompt control-14b-rl control-32b-prompt control-32b-rl (M3-Agent) control-32b-prompt w/o inter-turn instruction control-32b-rl w/o inter-turn instruction control-32b-rl w/o reasoning M3-Bench-robot M3-Bench-web Video-MME-Long 47.7 30.0 58.7 16.4 24.6 18.3 28.2 20. 30.7 12.8 20.2 19.0 35.7 40.5 36.9 46.9 40.9 48.9 32.3 43.1 40.1 45.3 50.8 49.1 56.0 52.5 61.8 48.3 55.9 52.3 Table 7 Impact of control methods on final performance, including: (1) comparison between GRPO and DAPO training algorithms; (2) performance gains from DAPO scale with model size; (3) the effect of removing inter-turn instruction and reasoning. The memorization model is fixed as memory-7b-sft. Next, we investigate the impact of the control on final performance. We fix memorization model as memory-7b-sft and evaluate various control process models, as shown in Table 7. First, we compare two RL algorithms: GRPO and DAPO. Training details for GRPO are provided in Appendix F. Our results show that control-32b-rl trained with DAPO consistently outperform control-32b-grpo across all test sets. Second, we analyze how DAPOs performance scales with model size. The results indicate substantial improvements across all sizes. Specifically, after DAPO training, control-32b-rl achieves improvements of 10.0%, 8.0%, and 9.3% in accuracy over control-32b-prompt on M3-Bench-robot, M3-Bench-web, and VideoMME-long, respectively. Finally, we ablate two designs: inter-instruction and reasoning. Both are shown to be critical. Removing inter-instruction results in accuracy drops of 10.5%, 5.8%, and 5.9% on M3-Bench-robot, M3-Bench-web, and VideoMME-long, respectively. Removing reasoning leads to decreases of 11.7%, 8.8%, and 9.5% on the same benchmarks."
        },
        {
            "title": "5.5 Case Study\nMemorization Table 15, 16 (§ G) present two examples illustrating the episodic and semantic memories\ngenerated during memory access. Compared to memory-gemini-prompt, memory-7b-sft demonstrates\n(1) more detailed episodic memory generation, including richer scene descriptions, character actions and\nexpressions, and dialogue; (2) improved recognition of identity equivalence, enabling consistent long-term\ntacking of human identities; and (3) richer semantic memory extraction, proactively generating knowledge\nabout characters and environments.",
            "content": "Control To illustrate the control process in detail, Table 17 ( G) presents complete generation trajectory of control-32b-rl. The input question is: \"Is Tomasz person with rich imagination or someone who lacks imagination?\" In the first round, the agent searches its memory for Tomaszs character ID. In the second round, having identified Tomasz as <character_4>, it attempts direct query: \"What is <character_4>s personality regarding imagination?\" Finding no relevant memory in the third round, the agent reasons based on <character_4>s role as CTO of company and generates more targeted query: \"What are <character_4>s creative problemsolving methods?\" This yields relevant memory: \"<character_4> is innovative and forward-thinking, as evidenced by his interest in scaling drone technology for personal flight.\"a piece of semantic memory. By the fourth round, the agent has collected enough information in its context to generate the final answer. 14 Hard Case in M3-Bench The accuracy of various methods demonstrates that M3-Bench, particularly M3Bench-robot, presents significant challenge. We perform detailed error analysis of M3-Agent on M3-Bench, identifying two representative hard cases and their associated challenges that demand further investigation. The first category involves reasoning about fine-grained details. For instance, questions like \"Who wants to eat the ham sausage?\" or \"Which coat rack should Emmas hat be laced, taller one or shorter one?\" require the agent to extract precise information from its observations. However, retaining all such details in memory is impractical and may cause cognitive overload. To address this, the agent must use attention mechanisms that enables selective memorization. During execution, it can develop task-specific world knowledge, allowing it to focus on relevant details while ignoring the irrelevant, thereby improving task performance. Another category of hard cases is related to spatial reasoning. In the M3-Bench-robot, number of questions challenge the agents capability on spatial cognition, such as understanding spatial layout and tracking spatial changes. Examples include: \"Where can the robot get the snacks?\" and \"Is Leos water cup currently on the second or third shelf from the top of the rack?\" Since verbal memory is generally less effective than visual memory for retaining spatial information, the long-term memory should be designed to incorporate richer visual content, e.g., snapshots, to better support spatial reasoning."
        },
        {
            "title": "6 Conclusion and Future Work",
            "content": "In this paper, we introduce M3-Agent, multimodal agent framework equipped with long-term memory. M3-Agent perceives real-time video and audio streams to build both episodic and semantic memories, enabling it to accumulate world knowledge and maintain consistent, context-rich memory over time. When responding to instruction, M3-Agent can autonomously reason and retrieve relevant information from memory to complete tasks more effectively. To evaluate memory effectiveness and reasoning, we develop M3-Bench, LVQA benchmark featuring real-world, robot-perspective videos in practical environments, and challenging questions revolving human understanding, knowledge extraction, and cross-modal reasoning, also closely reflecting real-world demands. We evaluate our method against various baselines, including Socratic models, online video understanding methods, and M3-Agent implemented by prompting closed-source models. Experimental results on M3-Bench-robot, M3-Bench-web and VideoMME-long show that M3-Agent consistently outperforms all baselines, demonstrating its superior memorization and reasoning capabilities. Furthermore, by conducting detailed case studies, we identify key limitations that point to promising future directions. These including enhancing attention mechanisms for semantic memory formation and developing richer yet more efficient visual memory."
        },
        {
            "title": "7 Acknowledgment",
            "content": "We would like to thank Xiran Suo, Wanjun Wang, and Liu Ding of ByteDance for their help with data annotation, and Peng Lin for creating the illustration."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [2] Keshigeyan Chandrasegaran, Agrim Gupta, Lea Hadzic, Taran Kota, Jimming He, Cristóbal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, and Fei-Fei Li. Hourvideo: 1-hour video-language understanding. Advances in Neural Information Processing Systems, 37:5316853197, 2024. [3] Yafeng Chen, Siqi Zheng, Hui Wang, Luyao Cheng, Qian Chen, Shiliang Zhang, and Junjie Li. Eres2netv2: Boosting short-duration speaker verification performance with computational efficiency. arXiv preprint arXiv:2406.02167, 2024. [4] Yukang Chen, Fuzhao Xue, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. Longvila: Scaling long-context visual language models for long videos. arXiv preprint arXiv:2408.10188, 2024. [5] Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. Mem0: Building productionready ai agents with scalable long-term memory. arXiv preprint arXiv:2504.19413, 2025. [6] Anxhelo Diko, Tinghuai Wang, Wassim Swaileh, Shiyan Sun, and Ioannis Patras. Rewind: Understanding long videos with instructed learnable memory. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1373413743, 2025. [7] Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbench-video: long-form multi-shot benchmark for holistic video understanding. Advances in Neural Information Processing Systems, 37:8909889124, 2024. [8] Peiyuan Feng, Yichen He, Guanhua Huang, Yuan Lin, Hanchong Zhang, Yuchen Zhang, and Hang Li. Agile: novel reinforcement learning framework of llm agents. Advances in Neural Information Processing Systems, 37: 52445284, 2024. [9] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2410824118, 2025. [10] Pascale Fung, Yoram Bachrach, Asli Celikyilmaz, Kamalika Chaudhuri, Delong Chen, Willy Chung, Emmanuel Dupoux, Hervé Jégou, Alessandro Lazaric, Arjun Majumdar, et al. Embodied ai agents: Modeling the world. arXiv preprint arXiv:2506.22355, 2025. [11] Tengda Han, Max Bain, Arsha Nagrani, Gül Varol, Weidi Xie, and Andrew Zisserman. Autoad: Movie description in context. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1893018940, 2023. [12] Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam Lim. Ma-lmm: Memory-augmented large multimodal model for long-term video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [13] Yichen He, Yuan Lin, Jianchao Wu, Hanchong Zhang, Yuchen Zhang, and Ruicheng Le. Storyteller: Improving long video description through global audio-visual character identification. arXiv preprint arXiv:2411.07076, 2024. [14] Mengkang Hu, Tianxing Chen, Qiguang Chen, Yao Mu, Wenqi Shao, and Ping Luo. Hiagent: Hierarchical working memory management for solving long-horizon agent tasks with large language model. arXiv preprint arXiv:2408.09559, 2024. [15] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [16] Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, and Gedas Bertasius. Video recap: Recursive captioning of hour-long videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1819818208, 2024. [17] Anna Ivanova, Aalok Sathe, Benjamin Lipkin, Unnathi Kumar, Setayesh Radkani, Thomas Clark, Carina Kauf, Jennifer Hu, RT Pramod, Gabriel Grand, et al. Elements of world knowledge (ewok): cognition-inspired framework for evaluating basic world knowledge in language models. arXiv preprint arXiv:2405.09605, 2024. [18] Jiazheng Kang, Mingming Ji, Zhe Zhao, and Ting Bai. Memory os of ai agent. arXiv preprint arXiv:2506.06326, 2025. [19] Xiaohan Lan, Yitian Yuan, Zequn Jie, and Lin Ma. Vidcompress: Memory-enhanced temporal compression for video understanding in large language models. arXiv preprint arXiv:2410.11417, 2024. [20] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:94599474, 2020. [21] Hao Li, Chenghao Yang, An Zhang, Yang Deng, Xiang Wang, and Tat-Seng Chua. Hello again! llm-powered personalized agent for long-term dialogue. arXiv preprint arXiv:2406.05925, 2024. [22] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. [23] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2219522206, 2024. [24] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. [25] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2668926699, 2024. [26] Kevin Lin, Faisal Ahmed, Linjie Li, Chung-Ching Lin, Ehsan Azarnasab, Zhengyuan Yang, Jianfeng Wang, Lin Liang, Zicheng Liu, Yumao Lu, et al. Mm-vid: Advancing video understanding with gpt-4v (ision). arXiv preprint arXiv:2310.19773, 2023. [27] Na Liu, Liangyu Chen, Xiaoyu Tian, Wei Zou, Kaijiang Chen, and Ming Cui. From llm to conversational agent: memory enhanced architecture with fine-tuning of large language models, 2024. URL https://arxiv. org/abs/2401.02777. [28] Weijie Liu, Zecheng Tang, Juntao Li, Kehai Chen, and Min Zhang. Memlong: Memory-augmented retrieval for long text modeling. arXiv preprint arXiv:2408.16967, 2024. [29] Zhiwei Liu, Weiran Yao, Jianguo Zhang, Liangwei Yang, Zuxin Liu, Juntao Tan, Prafulla Choubey, Tian Lan, Jason Wu, Huan Wang, et al. Agentlite: lightweight library for building and advancing task-oriented llm agent system. arXiv preprint arXiv:2402.15538, 2024. [30] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very long-form video language understanding. Advances in Neural Information Processing Systems, 36:4621246244, 2023. [31] Kai Mei, Xi Zhu, Wujiang Xu, Wenyue Hua, Mingyu Jin, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge, and Yongfeng Zhang. Aios: Llm agent operating system. arXiv preprint arXiv:2403.16971, 2024. [32] Junbo Niu, Yifei Li, Ziyang Miao, Chunjiang Ge, Yuanhang Zhou, Qihao He, Xiaoyi Dong, Haodong Duan, Shuangrui Ding, Rui Qian, et al. Ovo-bench: How far is your video-llms from real-world online video understanding? In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1890218913, 2025. [33] Felix Ocker, Jörg Deigmöller, Pavel Smirnov, and Julian Eggert. grounded memory system for smart personal assistants. arXiv preprint arXiv:2505.06328, 2025. [34] Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun Xie, Fei Huang, and Huajun Chen. Agent planning with world knowledge model. Advances in Neural Information Processing Systems, 37:114843114871, 2024. [35] Gabriel Sarch, Yue Wu, Michael Tarr, and Katerina Fragkiadaki. Open-ended instructable embodied agents with memory-augmented large language models. arXiv preprint arXiv:2310.15127, 2023. 17 [36] Yu Shang, Yu Li, Keyu Zhao, Likai Ma, Jiahe Liu, Fengli Xu, and Yong Li. Agentsquare: Automatic llm agent search in modular design space. arXiv preprint arXiv:2410.06153, 2024. [37] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [38] Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, et al. Longvu: Spatiotemporal adaptive compression for long video-language understanding. arXiv preprint arXiv:2410.17434, 2024. [39] Yan Shu, Zheng Liu, Peitian Zhang, Minghao Qin, Junjie Zhou, Zhengyang Liang, Tiejun Huang, and Bo Zhao. Video-xl: Extra-long vision language model for hour-scale video understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2616026169, 2025. [40] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1822118232, 2024. [41] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [42] Endel Tulving. episodic and semantic memory, in organization of memory. (No Title), page 381, 1972. [43] Endel Tulving. How many memory systems are there? American psychologist, 40(4):385, 1985. [44] Bing Wang, Xinnian Liang, Jian Yang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and Zhoujun Li. Enhancing large language model with self-controlled memory framework. arXiv preprint arXiv:2304.13343, 2023. [45] Jiawei Wang, Liping Yuan, Yuchen Zhang, and Haomiao Sun. Tarsier: Recipes for training and evaluating large video description models. arXiv preprint arXiv:2407.00634, 2024. [46] Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, et al. Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [47] Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi Fan, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer. Memvit: Memory-augmented multiscale vision transformer for efficient long-term video recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1358713597, 2022. [48] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. Advances in Neural Information Processing Systems, 37:2882828857, 2024. [49] Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025. [50] Wujiang Xu, Kai Mei, Hang Gao, Juntao Tan, Zujie Liang, and Yongfeng Zhang. A-mem: Agentic memory for llm agents. arXiv preprint arXiv:2502.12110, 2025. [51] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [52] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. 18 [53] Liping Yuan, Jiawei Wang, Haomiao Sun, Yuchen Zhang, and Yuan Lin. Tarsier2: Advancing large vision-language models from detailed video description to comprehensive video understanding. arXiv preprint arXiv:2501.07888, 2025. [54] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, et al. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022. [55] Chaoyi Zhang, Kevin Lin, Zhengyuan Yang, Jianfeng Wang, Linjie Li, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. Mm-narrator: Narrating long-form videos with multimodal in-context learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1364713657, 2024. [56] Haoji Zhang, Yiqin Wang, Yansong Tang, Yong Liu, Jiashi Feng, Jifeng Dai, and Xiaojie Jin. Flash-vstream: Memory-based real-time understanding for long video streams. arXiv preprint arXiv:2406.08085, 2024. [57] Pan Zhang, Xiaoyi Dong, Yuhang Cao, Yuhang Zang, Rui Qian, Xilin Wei, Lin Chen, Yifei Li, Junbo Niu, Shuangrui Ding, et al. Internlm-xcomposer2. 5-omnilive: comprehensive multimodal system for long-term streaming video and audio interactions. arXiv preprint arXiv:2412.09596, 2024. [58] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. [59] Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large language models with long-term memory. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1972419731, 2024. [60] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Zhengyang Liang, Shitao Xiao, Minghao Qin, Xi Yang, Yongping Xiong, Bo Zhang, et al. Mlvu: Benchmarking multi-task long video understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1369113701, 2025."
        },
        {
            "title": "Appendix",
            "content": "A M3-Bench-robot A.1 Script Annotation Guidelines Actor Setup Four to five actors participate, including one playing the role of robot. The robot actor wears head-mounted camera, either an iPhone 16 Pro, Xiaomi 14 Ultra, or GoPro HERO13, to capture single point-of-view video from the robots perspective. Definitions 1. Script: Consists of events and questions and provides actors with dialogue and stage instructions. 2. Robot: Played by human actor. It is an ideal highly intelligent robot with reasoning and memory abilities similar to humans. 3. Scenario: living room, kitchen, bedroom, study, office, meeting room, and gym. 4. Event: complete, short plot within the script. reference event includes information relevant to future questions, such as robots interacting with humans while observing and learning human preferences or the placement of objects in real-world scenes. 5. Question: Designed to evaluate the robots memory. Each question must align with at least one type listed in Table 1. Requirements Annotate at least 15 questions, each labeled with the corresponding reference events. Each script must contain at least 70 events to ensure minimum video duration of 30 minutes. Avoid asking questions that rely solely on common sense or that can be answered without watching the video. Do not ask questions that remain unanswerable even after watching the video. Avoid questions that can be answered based solely on the dialogue. Do no include questions that are weakly related to the reference events. The question should have clear and unambiguous answer that can be objectively verified by comparing it to the reference answer. A.2 QA Annotation Guidelines Background In the future, robots will help humans complete many tasks in indoor environments such as homes. Based on this imagination, we filmed video from the perspective of robot. In order to evaluate the models ability, we set questions at different timestamps, typically related to the robots upcoming tasks. Correct answers are essential for the successful completion of these tasks. Some questions require manual review or additional annotations to ensure each video includes at least 10 questions. Task Provide 3045 minute video along with corresponding script that includes series of questions. Note: Minor script modifications may occur during filming to accommodate practical constraints. As result, the script may not perfectly align with the final video. 1. Review existing questions. For each question in the script: Annotate the corresponding timestamp in the video based on the related script event. Determine whether the question can be answered using the video content up to that point. If so, annotate the answer. If the question is unanswerable, consider whether modifying it could make it answerable. If applicable, revise the question and provide the answer. For each question-answer pair, annotate the reasoning process used to derive the answer and specify the question types according to Table 1. 2. Annotate additional questions: If fewer than 10 questions remain after reviewing the script, generate new questions that must belong to at least one type listed in Table 1. A.3 Quality Control The annotation process consists of two rounds. In the first round, the goal is to ensure that annotators fully understand the annotation guidelines. Each annotator is required to perform QA annotations on three videos. The authors then review the annotations, provide feedback, and the annotators may revise their annotation accordingly. Based on the quality of these initial annotations, the authors determine whether the annotator is qualified to proceed to the formal annotation phase. In the second round, each annotator annotates five videos at time. The authors randomly select one video from each batch for quality inspection. If more than one invalid question-answer is found in the selected video, the entire batch must be re-annotated. Otherwise, the batch is considered accepted. Two authors are involved in the quality control process throughout the annotation workflow. In addition, to ensure the quality of the questions in M3-Bench-robot, we recruited five annotators to answer each question. Annotators were allowed to first read the question and then watch the video as many times as needed. The final human accuracy on M3-Bench-robot is 90.7%. Our error analysis shows that the most common mistakes are counting-related problems. A.4 Annotator Information All annotators are employed by commercial data annotation company. We sign contract with the company and pay the company for the annotation work at market price. The annotators are all college graduates with strong English proficiency. For script annotation, eleven annotators are involved. Video filming engage 67 actors. For QA annotation, five annotators participate. A.5 Data Examples Table 8 provides an example of script annotation. Event ID 1 2 3 Event Rose is in the room talking to Amy on the phone. She thanks Amy for the tulips and (reference) takes photo of the blooming flowers to share with her. Rose tells the robot that the delicate teddy bear is gift for Rachel. (reference) After hanging up with Amy, Rose calls Rachel and Leo to remind them not to forget to come over today. (Continued on next page) 21 Rose looks at pile of packages in the corner of the bedroom. They are recently purchased clothes. She asks the robot to unpack them and place the clothes on the first shelf of the wardrobe. She points to the bottom of the wardrobe, where pile of delicate little toys is stored, (reference) and tells the robot, \"Put the teddy bear there.\" At that moment, the doorbell rings and Rose excitedly runs to open the door. ... Rachel sees the dolls on the bed and exclaims, \"Wow, these dolls are so cute, let me pamper them!\" Rose says, \"Dont rush, theres another surprise,\" and then calls the robot. Question: Is Rachels gift on the top shelf or the bottom shelf of the wardrobe? Reference: event-2 and event-5 The robot takes teddy bear from the wardrobe, hands it to Rachel, and says, \"This is gift prepared for you.\" ... Rachel teases that Rose just doesnt want to admit it, but the robot surely knows. She then turns to the robot and asks who gave Rose the flowers. Question: Who gave Rose the flowers? Reference: event-1 ... 4 5 6 ... 10 12 ... 58 ... Table 8 An example of the M3-Bench-robot script. M3-Bench-web B.1 Annotation Guidelines To better help the annotators understand the requirements and better ensure the overall quality, safety, and validity of the datasets, we provide the following detailed guidelines, which clearly specify the acceptable and unacceptable annotation practices. Questions must allow for verifiable and objective evaluation of correctness. This entails avoiding overly open-ended questions, compound questions that mix multiple sub-questions, or questions with multiple equally valid answers. Each video must include at least two questions targeting character attribute modeling and two questions involving commonsense reasoning. All visual information required to answer question must remain clearly recognizable at lower resolutions (720p), ensuring that all questions are answerable. For videos between 20 and 40 minutes in length, 5 questions should be generated; for videos exceeding 40 minutes, 10 questions should be provided. Compensation considers both the number and duration of the videos. For commonsense reasoning questions, annotators must also specify the commonsense knowledge being tested, in addition to the question and its answer. It is not permissible for all questions to be answerable using only audio. reasonable proportion of questions must be vision-centric, requiring understanding of visual content in the video. Redundant questions within the same video are not allowed. For instance, asking \"Describe Davids appearance\" and \"Describe Alices appearance\" would be considered repetitive. 22 Questions that can be answered solely based on brief moment or short clip should be avoided. Specifically, the context required to answer valid question should span more than 10 seconds of video content. Videos must not contain sensitive, offensive, or NSFW content. Avoid asking questions that rely solely on commonsense knowledge and do not require viewing the video. Such questions do not meaningfully test video understanding. Avoid questions that are too easy to guess based on social priors or language bias alone. For example, question like \"Did the teacher appear impatient when students repeatedly interrupted the class?\" may be too easily answered with \"No\" due to cultural expectations of teacher behavior, regardless of the actual video content. This undermines the goal of evaluating visual understanding. Do not directly convert characters spoken lines into questions. These are typically answerable via simple string matching or keyword retrieval, which again does not effectively test video comprehension. Balance the number of questions with answer Yes and No. B.2 Quality Control The annotation process includes the following quality control stages: Stage 1: Candidate annotators complete trial task, collecting one video and labeling corresponding QA pairs. The authors review the submission and provide feedback. Once the annotator demonstrates clear understanding of the annotation guidelines, they proceed to formal annotation. Stage 2: The annotator submits batch of 10 videos with corresponding QA pairs. The authors randomly review 2 of them and provide feedback. The annotator revise the entire batch accordingly. If the qualified rate of the submitted questions is below 90%, the authors re-sample the revised batch for further inspection. Otherwise, the batch is accepted. Annotators who pass this stage on the first attempt can proceed to Stage 3. Stage 3: The annotator submits batch of 30 videos with QA pairs. The authors randomly inspect 5 of them and provide feedback. The annotator revises the full batch as needed. If the QA qualified rate is below 90%, follow-up review of the revised batch is conducted. Otherwise, the batch is accepted. Two authors are involved in the quality control process. B.3 Annotator Information All annotators are from commercial data annotation company. We have contract with this company and compensate them at market rates for the annotation work. All annotators are college graduates with strong English proficiency. total of ten annotators participated in the annotation of M3-Bench-web."
        },
        {
            "title": "C Implementation Details of Tools",
            "content": "Here, we provide the implementation details of the tools for representation extraction introduced in Section 4.2. Facial Recognition To perform facial recognition, we uniformly sample video frames at rate of 5 frames per second. For each sampled frame, we employ the buffalo_l predefined model suite from the InsightFace2 library to extract facial attributes, including bounding box coordinates, identity embeddings, and detection/quality scores. Low-quality detectionssuch as those with abnormal aspect ratios or extremely low confidence scoresare discarded. We then apply HDBSCAN clustering on the embeddings of the remaining high-quality faces to group them by character identity. This yields set of reliable facial representations, clustered by character. 2https://github.com/deepinsight/insightface 23 Voice Identification For speaker identification, we use Gemini-1.5-Pro to extract audio segments corresponding to distinct speaker voices, while simultaneously performing automatic speech recognition (ASR) on each segment. Segments shorter than 2 seconds are filtered out to ensure reliability. We then apply voice embedding model ERes2NetV2[3] to encode each segment into speaker-specific representation. Based on the resulting voice embeddings, we cluster and merge segments that correspond to the same speakeri.e., those with similar vocal characteristics. This process produces set of high-quality speaker representations, also grouped by character. The prompt used for voice processing is shown in Table 9. The Prompt for Voice Processing You are given video. Your task is to perform Automatic Speech Recognition (ASR) and audio diarization on the provided video. Extract all speech segments with accurate timestamps and segment them by speaker turns (i.e., different speakers should have separate segments), but without assigning speaker identifiers. Return JSON list where each entry represents speech segment with the following fields: start_time: Start timestamp in MM:SS format. end_time: End timestamp in MM:SS format. asr: The transcribed text for that segment. Example Output: [ \"start_time\": \"00:05\", \"end_time\": \"00:08\", \"asr\": \"Hello, everyone.\", \"start_time\": \"00:09\", \"end_time\": \"00:12\", \"asr\": \"Welcome to the meeting.\" ] Strict Requirements: Ensure precise speech segmentation with accurate timestamps. Segment based on speaker turns (i.e., different speakers utterances should be separated). Preserve punctuation and capitalization in the ASR output. Skip the speeches that can hardly be clearly recognized. Return only the valid JSON list (which starts with \"[\" and ends with \"]\") without additional explanations. If the video contains no speech, return an empty list (\"[]\"). Now generate the JSON list based on the given video: Table 9 Prompt used for voice processing. Search All memory-based retrieval is implemented via Maximum Inner Product Search (MIPS), with modalityspecific adaptations. Each face and voice node maintains set of representative feature snapshots. When new face or voice features are extracted from video clip, we compute the average cosine similarity between each extracted feature and all stored snapshots per node. The node with the highest similarity exceeding pre-defined threshold (0.3 for image, 0.6 for voice) is considered match; otherwise, new node is created. Matched nodes are updated with the new features to refine their representations over time. For textual memory, we apply MIPS between the input query and all existing text nodes, using OpenAIs text-embedding-3-large3 as the embedding model. To support multi-entry retrieval, we apply top-k retrieval with similarity threshold t. Specifically, we return the most relevant nodes whose similarities exceed t. To ensure retrieval coherence, we also perform clip-level retrieval: each clip is scored by the highest similarity among its memory entries, and we return the top-ranked clips accordingly. For all experiments, we adopt relatively strict hyperparameter setting (k = 2, = 0.5) to reduce retrieval randomness and enable consistent evaluation across models. 3https://openai.com/index/new-embedding-models-and-api-updates/"
        },
        {
            "title": "D Demonstration Data Synthesis for Memorization",
            "content": "During memorization, the multimodal model takes inputs including: video, audio, facial identifications (via facial recognition), and voice identities (via voice identification). It generates two outputs, episodic memory and semantic memory. To construct training data, we segment training videos into 30-second clips. For each clip, we then synthesize the corresponding episodic memory, entity identity relationships in semantic memory, and other semantic memory, as detailed below. In total, we synthesize 10,752 training samples for 200 validation samples. Memory Type Episodic Memory Explanation Specific events or experience, capturing not just what happened, but also when, where, and in what context. The episodic memory should captures details such as the people involved, their appearance, actions and spoken words, and the broader environment. Semantic Memory Character-Identity Equivalence: Captures equivalence relationships across different character modality identity Character-Level Attributes: Extracts attributes for each character, such as name, personality traits (e.g., confident, nervous), role or profession (e.g., host, newcomer), interests, and background information. Interpersonal Relationships: Describes the relationships and interactions among characters, such as social roles (e.g., hostguest, leadersubordinate), emotional tone (e.g., respect, tension), power dynamics (e.g., who leads), and evidence of cooperation, exclusion, or conflict. Contextual and General Knowledge: Encompasses general knowledge inferred from the video, such as likely setting or genre (e.g., corporate meeting, game show), cultural or procedural norms, real-world facts (e.g., \"Alice Market is pet-friendly\"), common sense, and the functional roles or attributes of objects within the scene. Table 10 Explanations of different memory types. D.1 Episodic Memory Synthesis We employ hybrid synthetic strategy that integrates the complementary strengths of Gemini-1.5-Pro and GPT-4o. Gemini-1.5-Pro supports audio inputs and excels at generating high-level, event-based descriptions, whereas GPT-4o provides more fine-grained visual details. To leverage both models effectively, we first prompt GPT-4o to generate detailed visual description of the video using frames sampled at 0.5 fps. This output serves as contextual input for Gemini-1.5-Pro, which is then prompted to generate the final episodic memory. The prompt explicitly instructs Gemini-1.5-Pro to incorporate information from GPT-4os description when it deems it accurate. We find that using GPT-4os detailed visual output as context significantly enhances the richness of the final memory produced by Gemini-1.5-Pro. The full prompt template is shown in Table 11. Prompt of Episodic Memory Synthesis (GPT-4o) [Video] includes 16 frames of video. Using this information, generate detailed description of the video. Following the requirements below: 1. Carefully describe the visual elements in each frame, noting colors, objects, movements, environment, people (including actions, clothing, expressions), and any noticeable details or changes between frames. 2. If audio elements or sounds are visible through textual or visual cues within the frames (such as subtitles, audio indicators, or written sound effects), accurately describe these details. 3. Do not speculate or infer information beyond what is explicitly visible in these 16 frames. Avoid using external knowledge or assumptions. (Continued on next page) 25 4. Generate only the detailed description based solely on the given frames. Do not produce any additional commentary or explanations. Prompt of Episodic Memory Synthesis (Gemini-1.5-Pro) You are provided with the following data: [Video]: video clip in mp4 format. [Faces]: list of facial features detected in the video, each linked to unique face ID (e.g., <face_1>). [Dialogues]: list of speech segments in the video, including start_time, end_time, speaker ID (e.g., <voice_2>), and the corresponding transcribed text. [Reference Description]: description of the video that may contain both accurate and inaccurate details. Your Tasks: Based on the video content and reference descriptions, generate detailed and cohesive description of the video clip. The description should focus on the entire event, incorporating all relevant aspects of the characters, their actions, spoken dialogue, and interactions in narrative format. The description should include (but is not limited to) the following categories: Characters Appearance: Describe clothing, physical features, notable accessories, etc. Characters Actions & Movements: Describe gestures, movement across the scene, or interactions. Characters Spoken Dialogue: Quoteor, if necessary, summarizespoken content from the dialogue track. Characters Contextual Behavior: Describe emotional states, relationships, roles, and reactions. Environmental or Temporal Cues: Describe the physical setting and time-of-day if visible. Strict Requirements: Incorporate correct elements from the [Reference Description], and correct any mistakes you identify. Add any additional details visible or inferable from the [Video], [Faces], and [Dialogues] that are missing from the reference. Since the given dialogues may be incomplete, reconstruct the entire conversation from the raw audio as precisely as possible. If character has an associated feature ID in the input context (either face or voice), refer to them only using that feature ID (e.g., <face_1>, <voice_2>) Use face ID (e.g., <face_1>) when the detail is grounded in visual data. Use speaker ID (e.g., <voice_1>) when the detail is grounded in speech. Do not use non-existent <face_ID> or <voice_ID>. We reiterate the above-mentioned list of available IDs here: {ID_list} For characters without associated feature IDs, refer to them using concise visual or contextual descriptor (e.g., \"a man in blue shirt\", \"a young woman by the window\"). Do not use pronouns (e.g., \"he\", \"she\", \"they\") or inferred character names. Your output should be Python list of well-formed, concise English sentences (one detail per sentence). Example Output: [ \"In the bright conference room, <face_1> enters confidently, adjusting his black suit with white shirt and tie. He has short black hair and wears glasses, giving professional appearance as he approaches <face_2> to shake hands.\", \"<face_2>, dressed in striking red dress with long brown hair, smiles warmly and greets <face_1>. She then sits down at the table beside him, glancing at her phone briefly while occasionally looking up.\", \"<voice_1> speaks to the group, Good afternoon, everyone. Lets begin the meeting. His voice commands attention as the room quiets, and all eyes turn to him.\", (Continued on next page) 26 \"<face_2> listens attentively to <voice_1>s words, nodding in agreement while still occasionally checking her phone. The atmosphere is professional, with the participants settling into their roles for the meeting.\", \"<face_1> adjusts his tie and begins discussing the agenda, engaging the participants in productive conversation.\" ] Please only return the valid string list (which starts with \"[\" and ends with \"]\"), without any additional explanation or formatting. Table 11 Prompt templates used for generating synthetic episodic memory. D.2 Entity ID Relationship Detection There is special type of semantic memory, extracting cross-modal identity equivalences from video. This remains challenging task, even for advanced models like Gemini-1.5-Pro, particularly in scenes with multiple faces and voices [13]. To address this, we propose progressive annotation algorithm. The key idea is to identify meta-clips, segments containing exactly one face identity and one voice identity, from the raw long video. These meta-clips are used to build meta-dictionary that maps voice IDs to face IDs across the entire video. This dictionary enables automatic annotation of any 30-second clip extracted from the original video. Meta-Clip Extraction First, for long video, we can use facial recognition tools and voice identity tools introduced in Appendix to construct corresponding global ID for each face and voice that appears in the video. Next, we segment the video into series of short clips, each no longer than 5 seconds in duration, using keyframe-based division. This method ensures that each clip is visually stable, with minimal changes in characters or scenes. Then, we apply facial recognition and voice identity tools to each short clip individually to extract the faces and voices present, along with their global IDs. If clip contains only one face ID and one voice ID, we refer to it as meta-clip. In this case, it is highly likely that the face and voice in the clip belong to the same person. Therefore, we can use the meta-clip as high-confidence sample for establishing the association between faces and voices. Meta-Dictionary Construction Based on all meta-clips extracted from the long video, we construct set of mappings between face IDs and voice IDs. However inconsistencies may arise due to small number of clips where the speaker is not visible. To address this issue, we employ voting mechanism to generate the final meta-dictionary. The detailed algorithm is described in Algorithm 2. New-Clip Annotation After obtaining the meta-dictionary, we can use it to annotate arbitrary clips from the full-length video. Specifically, for each 30-second clip, if both face ID and voice ID appearing in the clip and also found in the meta-dictionary, we generate semantic memory in the form: \"Equivalence: <face_id>, <voice_id>\". Since not all IDs can be found using the meta-dictionary, we reject any clip containing voice ID that is not present in the meta-dictionary from the final training dataset for memorization. In total, we collected 10,952 30-second clips with valid identity equivalence annotations. We manually review 48 randomly sampled mappings, and found the accuracy to be 95.83%. D.3 Semantic Memory Synthesis To construct semantic memory, we adopt hybrid strategy similar to that used for episodic memory. We define several key dimensions that semantic memory should address, as outlined in Table 10. Specifically, we first prompt GPT-4o to generate preliminary semantic memory based on video frames and episodic memory. Next, we provide the video, episodic memory, and GPT-4o-generated semantic memory to Gemini-1.5-Pro, prompting it to produce the final semantic memory. Detailed prompts are provided in Table 12. Algorithm 2 Meta-Dictionary Construction Require: long video , threshold Ensure: mapping dictionary : from voice IDs to face IDs 1: Extract global face ID set = {f1, . . . , fN } and voice ID set = {v1, . . . , vN } from video 2: Divide into sequence of short clips = {c1, c2, . . . , cT } using keyframes-based segmentation 3: Initialize meta-clip set Cmeta 4: for ct do 5: Detect face set Ft and voice set Vt in ct if Ft = 1 and Vt = 1 then end if Add pair (ct, f, v) where Ft, Vt to Cmeta 6: 7: 8: 9: end for 10: Construct bipartite graph = (F, V, E) where edge (f, v) has weight: w(f, v) = {(ct, f, v) Cmeta} 11: Remove all edges from with weight equal to 1. 12: for do 13: 14: Let Nf = {vi (f, vi) E} Let = arg maxviNf w(f, vi) if w(f,vi) then w(f,v) (cid:80) viNf Keep only edge (f, v) and remove others 15: else end if Remove all edges incident to 16: 17: 18: 19: 20: end for 21: for do 22: 23: 24: 25: end for 26: Initialize mapping dictionary 27: for (f, v) do 28: 29: end for 30: return Add mapping M[v] Let Nv = {fj (fj, v) E} Let = arg maxfj Nv w(fj, v) Keep only edge (f , v) and remove others Prompt of Semantic Memory Synthesis (GPT-4o, Gemini-1.5-Pro) You are provided with the following data: [Video]: 16 frames of video. (Gemini-1.5-Pro Variant: video clip in mp4 format.) [Faces]: list of facial features detected in the video, each linked to unique face ID (e.g., <face_1>). [Dialogues]: list of speech segments in the video, including start_time, end_time, speaker ID (e.g., <voice_2>), and the corresponding transcribed text. [Video Descriptions]: description of the video. (Gemini-1.5-Pro Variant: [Refence conclusions]: list of high-level conclusions that may contain inadequate or incorrect information.) Your Task: Based on the given character features, video content, and reference conclusions, generate list of high-level, reasoning-based conclusions within the scope of the following category: 1. Character-Level Attributes (Continued on next page) 28 Infer abstract attributes for each character, such as: Name (if explicitly stated), Personality (e.g., confident, nervous), Role/profession (e.g., host, newcomer), Interests or background (when inferable), Distinctive behaviors or traits (e.g., speaks formally, fidgets). Avoid restating visual factsfocus on identity construction. 2. Interpersonal Relationships & Dynamics Describe the relationships and interactions between multiple characters: Roles (e.g., host-guest, leader-subordinate), Emotions or tone (e.g., respect, tension), Power dynamics (e.g., who leads), Evidence of cooperation, exclusion, conflict, etc. For individual character or cases where character relationships cannot be determined, do not generate conclusion relevant to the corresponding character. 3. Video-Level Plot Understanding Summarize the scene-level narrative, such as: Main event or theme, Narrative arc or sequence (e.g., intro discussion reaction), Overall tone (e.g., formal, tense), Cause-effect or group dynamics. Do not involve specific characters. 4. Contextual & General Knowledge Include general knowledge that can be learned from the video, such as: Likely setting or genre (e.g., corporate meeting, game show), Cultural/procedural norms, Real-world knowledge (e.g., \"Alice market is pet-friendly\"), Common-sense or format conventions. Attributes and functional roles of objects in the video (e.g., the trash bin is used for disposing of kitchen waste). Output Format: Python list of concise English sentences, each expressing one high-level conclusion. Do not include reasoning steps or restate input observations. Only output the final conclusions. Strict Requirements: Only include conclusions under the given category. Do not go beyond it. Your conclusions must be informed by the video and reference content. Each conclusion should reflect deeper reasoning and insight, not surface-level observations already evident from the plot description. If character has an associated feature ID in the input context (either face or voice), refer to them only using that feature ID (e.g., <face_1>, <voice_2>). Use face ID (e.g., <face_1>) when the detail is grounded in visual data. Use speaker ID (e.g., <voice_1>) when the detail is grounded in speech. Do not use non-existent <face_ID> or <voice_ID>. We reiterate the above-mentioned list of available IDs here: {ID_list} For characters without associated feature IDs, refer to them using concise visual or contextual descriptor (e.g., \"a man in blue shirt\", \"a young woman by the window\"). Do not use pronouns (e.g., \"he\", \"she\", \"they\") or inferred character names. (Continued on next page) 29 Maintain strict accuracy in referring to characters and their correct IDs or descriptions. Do not restate the input observations or reasoning stepsonly output the final, distilled conclusions. Your output should be Python list of well-formed, concise English sentences (one per item). Example Output (Note: example only represent the format, not fully corresponding to the provided category): [ \"<face_1>s name is David.\", \"<face_1> holds position of authority, likely as the meetings organizer or senior executive.\", \"<voice_2> shows social awareness and diplomacy, possibly indicating experience in public or client-facing roles.\", \"<face_1> demonstrates control and composure, suggesting high level of professionalism and confidence under pressure.\", \"The interaction between <face_1> and <voice_2> suggests working relationship built on mutual respect.\", \"The overall tone of the meeting is structured and goal-oriented, indicating it is part of larger organizational workflow.\" ] Please only return the valid string list (which starts with \"[\" and ends with \"]\") without any additional explanation or formatting. Table 12 The prompt used in generating synthetic semantic memory. D.4 Quality of the Synthetic Data Although the demonstration data is synthetic, it is of high quality. Our synthetic memory averages 245.7 words for episodic memory and 276.2 words for semantic memory, compared to 151.3 and 81.4 words respectively for Gemini-1.5-pro, indicating our memory captures more detail. For content accuracy, we randomly sampled 10 clips from different videos, totaling 353 memory items. Manual review showed an accuracy of 95.5%. Most errors stemmed from the speaker recognition tool: background noise and overlapping speech occasionally caused minor omissions or misidentifications in extracting speaker dialogue for episodic memory."
        },
        {
            "title": "E Evaluation of Memorization",
            "content": "we evaluate the memorization model during training using held-out validation set of 200 samples and select the best checkpoint. Two evaluation metrics are used. First, AutoDQ [45] assesses memory description quality by comparing generated outputs to reference descriptions, measuring episodic and semantic memory excluding identity equivalence. Second, for identity equivalence, we compute precision, recall and F1 score against ground-truth in the validation set. Based on the results in Table 13, we select the checkpoint obtained after training for 3 epochs. For additional comparison, we also report results from two baseline models, memory-gemini-prompt and memory-7b-prompt, on the same validation set. Our model, memory-7b-sft, significantly outperforms both baselines."
        },
        {
            "title": "F RL Training Details",
            "content": "F.1 Details of DAPO Training Table 14 lists the hyperparameters used during the training process. Figure 4 depicts the RL training curves, which show steady increase in score with the training steps. 30 Model memory-gemini-prompt memory-7b-prompt memory-7b-sft (1 epoch) memory-7b-sft (2 epochs) memory-7b-sft (3 epochs) memory-7b-sft (4 epochs) memory-7b-sft (5 epochs) AutoDQ-P AutoDQ-R AutoDQ-F1 0.539 0. 0.606 0.414 0.692 0.495 0.634 0.628 0.635 0.616 0.609 0.596 0.610 0.620 0.618 0.621 0.616 0. 0.627 0.617 0.615 Eq.-P 0.472 0.117 0.742 0.845 0.836 0.825 0.813 Eq.-R 0.805 0.192 0.817 0. 0.856 0.839 0.840 Eq.-F1 0.595 0.145 0.778 0.827 0.846 0.832 0.827 Table 13 Evaluation of memorization models using AutoDQ and Equivalence (Eq.) metrics. Here, P, R, and F1 denote precision, recall, and the F1 score, respectively. Parameter Name Batch Size GPU with 80GB memory Rollout Model Parallel Size Learning Rate Maximum Number of Rounds Number of Samples in Group Total Steps ϵlow ϵhigh Model Size 14B 32 16 1 1e-6 5 4 180 0.2 0.28 8B 32 16 1 1e-6 5 4 180 0.2 0.28 32B 32 32 2 1e-6 5 4 180 0.2 0.28 Table 14 The hyperparameters used in DAPO training. F.2 GRPO Training We also use Group Relative Policy Optimization (GRPO)[37] to optimize the policy model in the ablation study. GRPO optimizes the policy model πθ by maximizing the following objective: JGRPO(θ) = (q,a)D,{τi}G i=1πold θ (q) (cid:34) 1 (cid:88) i=1 1 I(τi,t) (cid:80)τi t=1 τi (cid:88) t=1 I(τi,t) min (cid:18) πθ(τi,tτi,<t) πold θ (τi,tτi,<t) ˆAi,t, clip(cid:0) πθ(τi,tτi,<t) πold θ (τi,tτi,<t) , 1 ϵ, 1 + ϵ(cid:1) ˆAi,t (cid:19) DKL [πθπref] = 1 (cid:80)τ t=1 I(τt) τ (cid:88) t=1 I(τt) (cid:16) πref(τtτ<t) πθ(τtτ<t) log πref(τtτ<t) πθ(τtτ<t) (cid:17) (cid:35) βDKL [πθπref] (4) (5) where ϵ and β are set to 0.2 and 0.01 respectively, and the other hyperparameters are the same as those in DAPO training."
        },
        {
            "title": "G Case Study",
            "content": "Table 15 and Table 16 present two examples illustrating the episodic and semantic memories generated during memorization. Table 17 presents complete generation trajectory in the control. Video (Illustrated as 12 frames) (Continued on next page) 31 Faces Dialogues <voice_1>: (00:03-00:12) Every, but feel like every sibling dynamic is different. was the youngest and just took it. just took it. didnt have the aggression to fight back like <voice_2>: (00:00-00:02) these experiences through my children, (00:11-00:16) close to, uh, its neck and neck. Yeah, they they go after each other. (00:24-00:29) were gonna have the fish coming to the fish feed and then were gonna have ducks coming too. Its gonna be <voice_812>: (00:17-00:23) Wow. So were gonna were gonna go fishing at the pond. Were gonna Im gonna get fish feeder and were gonna have memory-gemini-prompt Episodic memory sunglasses. <face_1412> is wearing light blue t-shirt with graphic design, black baseball cap, and mirrored He is standing in backyard with brick house, wooden deck, fence, and some trees. <voice_2> says hes reliving experiences through his children. woman, presumably his wife, walks by in the background carrying some items. <voice_1> says every sibling dynamic is different and that she was the youngest and just took it. <voice_2> says the children are neck and neck and go after each other. <face_1412> and the woman are holding small yellow ducklings. <voice_812> says theyre going fishing at the pond and will get fish feeder. Two children are playing in the backyard with squirt guns. <voice_812> says theyll have fish coming to the fish feed and ducks coming too. Semantic memory activities. <face_1412> appears to be father who enjoys spending time with his family and doing outdoor <face_1412> and <face_1413> have casual and comfortable relationship. The video depicts family preparing for fishing outing at their pond and discussing their childrens sibling dynamics. The setting is rural or suburban home with large backyard and pond. memory-7b-sft (Ours) Episodic memory green mirrored sunglasses. <face_1412> wears white t-shirt with green and yellow graphic design, black baseball cap, and (Continued on next page) 32 <face_1412> is outdoors on sunny day, in grassy area with brick building and wooden fence in the background. <face_1412> speaks to the camera, saying, \"these experiences through my children, which is\" <face_1413> joins <face_1412>, wearing white tank top and denim shorts. <voice_1> says, \"Every, but feel like every sibling dynamic is different. was the youngest and just took it. just took it. didnt have the aggression to fight back like\" <face_1412> points finger upwards. <voice_2> says, \"close to, uh, its neck and neck. Yeah, they they go after each other.\" <face_1413> and <face_1412> hold two small, fluffy yellow ducklings. The ducklings are placed on the grass near black pet carrier. <face_1413> kneels by the pet carrier, holding yellow duckling. <voice_812> says, \"Wow. So were gonna were gonna go fishing at the pond. Were gonna Im gonna get fish feeder and were gonna have\" <face_1412> gestures with his hand. <voice_2> says, \"were gonna have the fish coming to the fish feed and then were gonna have ducks coming too. Its gonna be a\" child in blue outfit is visible in the background. Semantic memory Equivalence: <face_1412>, <voice_812> Equivalence: <face_1412>, <voice_2> Equivalence: <face_1413>, <voice_1> <face_1412> exhibits relaxed and informal demeanor. <face_1412> displays an interest in outdoor activities and family-oriented experiences. <face_1413> demonstrates nurturing and caring personality. <voice_1> appears to be reflective and analytical when discussing personal experiences. <voice_2> expresses enthusiasm and anticipation for future activities. <voice_812> seems knowledgeable about pet care and enjoys engaging in outdoor recreational activities. <face_1412> and <face_1413> appear to have comfortable and supportive relationship, likely romantic partners or close family members. <voice_1> and <face_1412> share close bond, possibly as parent and child, marked by open communication and shared experiences. The video depicts slice-of-life scene centered around family interaction and leisure activities. The narrative progresses from reflective discussion about sibling relationships to forward-looking anticipation of shared activities. The overall tone is lighthearted and positive, emphasizing the joys of family connection and shared experiences. The video showcases blend of indoor and outdoor settings, creating dynamic visual narrative. The video likely belongs to the family vlog genre, documenting everyday life and experiences. The video showcases typical suburban or rural setting with house, fence, and outdoor space. Keeping ducklings requires specific pet care knowledge, including appropriate housing and feeding. The video implies the cultural norm of family members engaging in shared recreational activities. Fishing is popular outdoor leisure activity often enjoyed in natural settings like ponds. The video suggests the practice of using fish feeder to attract and manage fish populations in pond. Table 15 Comparison of memory generations between memory-7b-sft and memory-gemini-prompt for case KHslnSzK2SU, clip 23:00-23:30, M3-Bench-web. Yellow highlights indicate additional important details provided by our model. 33 Figure 4 Average scores (on training set) and accuracy (on dev set) curves during the DAPO training process. The smoothing method of the curve in the left figure is the exponential moving average(EMA) formula that aligns with the one used in WandB, and the smoothing weight is set to 0.9 Video (Illustrated as 12 frames) Faces Dialogues <voice_53>: (00:00-00:03) Robot, can you at least bring some fun games or puzzles we can play? (00:13-00:17) What am supposed to do? Sometimes they come with no pieces. What am supposed to do? (00:20-00:22) Well, what am supposed to? Im bored. want to play game. <voice_204>: (00:04-00:08) Really? Puzzle is wasting time for you. You dont like puzzles. (00:08-00:13) But every time you play puzzle, you miss many pieces of it, which love. (00:23-00:27) dont know. Go out and play some games, some games outside. <voice_316>: (00:18-00:20) So just leave it. Just dont take it. memory-gemini-prompt Episodic memory <face_4> sits on light-colored couch, facing <face_5>, who sits at small table. <face_5> wears light pink sweatshirt, while <face_4> wears dark shirt and dark pants. <voice_53> asks <face_5> to bring some fun games or puzzles. <face_5> tells <voice_53> that puzzles are waste of time. <face_5> explains that <voice_53> always loses puzzle pieces. <voice_53> expresses frustration about missing puzzle pieces and asks what to do. <face_5> suggests <voice_53> just leave the puzzle incomplete. <voice_53> complains of boredom and wanting to play game. <face_5> suggests <voice_53> go outside and play. <voice_53> asks if <face_5> dislikes Legos too, because Lego pieces can also get lost. (Continued on next page) 34 Semantic memory relationship. <face_4>/<voice_53> appears bored and frustrated. <face_5>/<voice_204> appears slightly annoyed or dismissive. <face_4>/<voice_53> and <face_5>/<voice_204> seem to have casual, possibly familial The interaction revolves around <face_4>/<voice_53>s boredom and <face_5>/<voice_204>s suggestions for alternative activities. The setting appears to be living room or family room. memory-7b-sft (Ours) Episodic memory <face_4> sits on beige couch, wearing black t-shirt and black pants. <face_5> sits next to <face_4> on the couch, wearing pink sweatshirt. small round table with red and white checkered tablecloth is placed between <face_4> and On the table are bottle of red liquid, white cup, notebook, and pen. vase with dried flowers and decorative metal piece are on small table behind the couch. projector and small green figurine are visible in the background. <voice_53> asks the robot to bring games or puzzles. <voice_204> tells <voice_53> that puzzles are waste of time and that <voice_53> doesnt like <face_5>. puzzles. <voice_204> says that <voice_53> always misses pieces when doing puzzles, which <voice_204> enjoys finding. <voice_53> expresses frustration, saying that sometimes puzzles come with no pieces. <voice_204> suggests that <voice_53> just leave the puzzle if they dont want to do it. <voice_53> says they are bored and want to play game. <voice_204> suggests that <voice_53> go outside and play games. <voice_53> asks if <voice_204> doesnt like Legos either, because sometimes Lego pieces are missing. <voice_204> responds with \"I dont know\". <face_5> gestures with their hands while speaking. <face_4> gestures with their hands while speaking. <face_5> writes in the notebook while speaking. Semantic memory Equivalence: <face_4>, <voice_53> Equivalence: <face_4>, <voice_316> Equivalence: <face_5>, <voice_204> <voice_53> appears to be easily bored and seeks immediate entertainment. <voice_204> displays pragmatic and solution-oriented approach to problem-solving. <voice_204> seems to enjoy challenges and finding solutions, even in seemingly mundane tasks like puzzles. requests. <voice_53> expresses preference for active recreation over quiet, puzzle-based activities. <voice_204> and <voice_53> appear to have comfortable, familiar dynamic, likely indicative of close relationship. The interaction between <voice_204> and <voice_53> suggests playful banter, with <voice_204> teasing <voice_53> about their attitude. <voice_204> takes on mentoring or guiding role, offering suggestions and solutions to <voice_53>s The scene depicts common household scenario of differing preferences and mild conflict regarding leisure activities. The narrative unfolds as request for entertainment, followed by disagreement on suitable options and suggestion for more intellectually engaging activity. (Continued on next page) 35 The overall tone is casual and familiar, reflecting the dynamics of comfortable, possibly familial or romantic, relationship. The scene showcases power dynamic where one individual attempts to influence the others choices, leading to mild power negotiation. The video likely takes place in private residence, possibly an apartment or recently built home, given the modern interior design. The presence of projector suggests focus on entertainment and media consumption within the household. The availability of puzzles and Lego sets implies focus on recreational activities and potentially family-oriented entertainment. The red liquid in the bottle is likely type of soda or juice, commonly consumed as refreshing beverage during leisure time. The checkered tablecloth on the table signifies casual dining or activity setup, possibly for snack or casual game session. The dried flowers and decorative metal piece function as home decor, contributing to the overall aesthetic and ambiance of the space. The small green figurine may be collectible item or decorative ornament, adding personal touch to the rooms decor. Table 16 Comparison of memory generations between memory-7b-sft and memory-gemini-prompt for living_room_22, clip 4:00-4:30, M3-Bench-robot. Yellow highlights indicate additional important details provided by our model. Question: Is Tomasz person with rich imagination or someone who lacks imagination? Answer: person with rich imagination. system (system_prompt) You are given question and some relevant knowledge. Your task is to reason about whether the provided knowledge is sufficient to answer the question. If it is sufficient, output [Answer] followed by the answer. If it is not sufficient, output [Search] and generate query that will be encoded into embeddings for vector similarity search. The query will help retrieve additional information from memory bank. Question: What is Theresa Yeaps attitude like in her explanation of emergency services? user (instruction_prompt) Searched knowledge: {} Output the answer in the format: Action: [Answer] or [Search] Content: {content} If the answer cannot be derived yet, the {content} should be single search query that would help retrieve the missing information. The search {content} needs to be different from the previous. You can get the mapping relationship between character ID and name by using search query such as: \"What is the name of <character_{i}>\" or \"What is the character id of {name}\". After obtaining the mapping, it is best to use character ID instead of name for searching. If the answer can be derived from the provided knowledge, the {content} is the specific answer to the question. Only name can appear in the answer, not character ID like <character_{i}>. assistant <think>Okay, lets see. The user is asking whether Tomasz is person with rich imagination or someone who lacks imagination. The problem is that theres no provided knowledge about Tomasz in the given context. The Searched knowledge is empty, so dont have any information to base an answer on. (Continued on next page) 36 First, need to check if the existing knowledge can answer the question. Since the knowledge is empty, theres nothing to reference. The user mentioned that if the answer cant be derived, should output search query. The instructions also say to use character IDs if possible, but since we dont know Tomaszs ID yet, need to find that first. So the first step is to find the character ID for Tomasz. The user suggested using query like \"What is the character id of Tomasz\" to get that mapping. Once have the ID, can then search for information related to his imagination. But without the ID, any search might not be effective. Therefore, the initial search should be to find the character ID. Then, using that ID, look into his traits. Since the current knowledge is empty, the correct action is to search for the character ID first.</think> Action: [Search] Content: What is the character id of Tomasz [\"<character_2>s name is Angus Whitley.\"], \"CLIP_15\": user Searched knowledge: {\"CLIP_4\": [\"<character_4>s name is Tomasz Patan.\", \"The man in the white shirt, now identified as Tomasz Patan, Founder & Chief Technology Officer of Jetson, speaks to the camera in workshop setting.\"], \"CLIP_5\": [\"<character_4>s name is Patán.\", \"<character_2> identifies <character_4> as Patán, the founder and CTO of Jetson, company developing an eVTOL vehicle.\"], \"CLIP_8\": [\"<character_0>s name is Akshat Rathi.\", \"<character_0> wears glasses and has beard.\"], \"CLIP_17\": [\"<character_5>s name is Matthew Clark.\"], \"CLIP_20\": [\"<character_3>s name is Tom Muniz.\", \"<character_3> identifies himself as Tom Muniz, Chief Technology Officer of Archer Aviation.\"], \"CLIP_21\": [\"<character_7> is likely pilot or flight controller, given their interaction with air traffic control.\"], \"CLIP_26\": [\"<character_2>s name is Robin Riedel.\", \"<character_2>, wearing glasses and light blue shirt, is identified as Robin Riedel, partner at McKinsey & Company and co-leader of the McKinsey Center for Future Mobility.\", \"<character_2> is certified commercial pilot.\"], \"CLIP_31\": [\"The graphic then shows user profile icon appearing, representing pilot requirements.\"], \"CLIP_36\": [\"<character_2> is shown again in the library.\"], \"CLIP_37\": [\"<character_1> displays cautious and rule-abiding personality.\"], \"CLIP_40\": [\"<character_12>s name is Isabel Metz.\", \"<character_12> identifies the Hamburg airport on the map.\"], \"CLIP_42\": [\"A name tag identifies <character_8> as Bianca Schuchardt, Researcher, German Aerospace Center.\"]} instruction_prompt assistant <think>Okay, lets see. The user wants to know if Tomasz has rich imagination or lacks it. From the previous search, we found that Tomasz Patan is <character_4>. Now, need to check the provided knowledge to see if theres any info about his imagination. Looking through the CLIP entries for <character_4> (Tomasz Patan), the info given is about his role as Founder & CTO of Jetson, speaking in workshop. Theres no mention of his imagination. Other clips mention other characters but not their traits related to imagination. Since theres no data here about Tomaszs imagination, cant derive an answer. The next step is to search for information on his imagination using his character ID. The query should be about his imagination, so maybe \"Does <character_4> have rich imagination?\" or similar. Need to make sure the query is different from previous ones. Lets go with \"What is <character_4>s personality regarding imagination?\" to retrieve relevant info.</think> Action: [Search] Content: What is <character_4>s personality regarding imagination? user Searched knowledge: {} (The search result is empty. Please try searching from another perspective.) instruction_prompt (Continued on next page) assistant <think>Okay, lets see. The user is asking if Tomasz is person with rich imagination or someone who lacks imagination. So far, weve tried searching and couldnt find any info about his imagination. The first step was to get his character ID, which is <character_4>. Then, we tried searching for his personality regarding imagination, but that came back empty. Since theres no direct information about Tomaszs imagination in the provided knowledge, cant derive an answer from the existing data. The users instructions say that if the answer cant be derived yet, should generate new search query. The previous query was about his personality regarding imagination, which didnt yield results. So need to approach it from another angle. Maybe instead of asking about his personality, can look for any mentions of his traits in general. Perhaps theres clip where his characteristics are discussed, but its not related to imagination. Alternatively, maybe his role as CTO or founder of company could imply something about his creativity or innovation, which are related to imagination. However, the knowledge provided doesnt mention anything about his traits beyond his professional role. Given that the knowledge is still empty, should create new query thats different from the previous one. Perhaps asking about his creative abilities or how others perceive his problem-solving skills. Alternatively, checking for any quotes or statements he made that might indicate his imaginative nature. The key is to generate query thats specific enough to retrieve information if it exists. So maybe \"What are <character_4>s creative problem-solving methods?\" or \"How does <character_4> approach innovation in his work?\" These queries might uncover information about his imagination through his work style or achievements. Since the user wants the search query to be different from the previous ones, Ill go with one of these alternatives. Lets pick \"What are <character_4>s creative problem-solving methods?\" to see if that yields any relevant information about his imagination.</think> Action: [Search] Content: What are <character_4>s creative problem-solving methods? user Searched knowledge: {\"CLIP_6\": [\"<character_4>, wearing white polo shirt with the Jetson logo, stands in room with partially assembled aircraft frame behind him.\", \"<character_4> holds clear plastic cup and gestures with his hands as he speaks.\", \"<character_4> says, test and we have lots of data from it that we had redundant sensors on the vehicle that day. \", \"A person wearing yellow vest labeled TEST TEAM adjusts red propeller on the aircraft outdoors.\", \"Another person wearing black jacket and white and orange sneakers steps onto scale.\", \"The person in the black jacket, now wearing Sparco racing suit and helmet, walks towards an open field.\", \"<character_4> says, So for 11 years, Ive been doing drones with camera gimbals. \", \"<character_4> continues, These drones were getting bigger and bigger. \", \"<character_4> says, And at some point, asked myself the question, what if we scaled it? \", \"<character_4> puts on Sparco helmet and fastens the chin strap.\", \"<character_4> adjusts the visor on his helmet.\", \"<character_4> buckles the seatbelt inside the aircraft.\", \"<character_4> says, So could sit in there and pilot myself. \", \"<character_4> sits in the aircraft as it takes off, raising one hand in gesture.\", \"The aircraft is white and black with four propellers.\", \"<character_4> is an engineer or designer involved in the development of the Jetson aircraft.\", \"<character_4> has background in working with drones and camera gimbals, demonstrating expertise in aerial technology.\", \"<character_4> is innovative and forward-thinking, as evidenced by his interest in scaling drone technology for personal flight.\",\"<character_4> possesses hands-on, experimental approach to his work, as shown by his involvement in testing and data analysis.\", \"<character_4> displays sense of pride and enthusiasm for his work on the Jetson project.\", \"The video follows narrative arc of innovation and realization, starting (Continued on next page) 38 with the concept of scaling drone technology and culminating in successful test flight.\", \"The video showcases blend of technical detail and personal narrative, offering insights into both the engineering process and the driving force behind the project.\", \"The overall tone of the video is optimistic and forward-looking, emphasizing the potential of personal aerial vehicles.\", \"The video implicitly highlights the importance of data analysis and redundant sensor systems in ensuring flight safety and reliability.\", \"The video likely belongs to the genre of technology demonstration or promotional content, showcasing the Jetson aircraft.\", \"The setting includes both indoor and outdoor locations, suggesting combination of design, testing, and operational phases of the project.\", \"The use of safety gear like helmets and vests indicates adherence to safety protocols in aviation and engineering environments.\", \"The presence of test team suggests structured approach to quality control and validation of the aircrafts performance.\", \"The scale model aircraft implies phased approach to development, starting with smaller prototypes before moving to full-sized vehicles.\", \"The video showcases procedural norms in aviation, such as preflight checks, helmet usage, and controlled testing environments.\", \"The Jetson aircraft represents novel application of drone technology, aiming to bridge the gap between unmanned aerial vehicles and personal flight.\", \"The video implies growing trend of personal aerial mobility, driven by technological advancements and innovative design.\", \"The videos narrative structure follows common format in technology demonstrations, starting with the problem statement (drones size limitations) and presenting solution (the Jetson aircraft).\", \"The videos focus on data analysis suggests data-driven approach to design and optimization in the development of the Jetson aircraft.\"]} instruction_prompt assistant <think>Okay, lets tackle this question. The user wants to know if Tomasz is person with rich imagination or someone who lacks imagination. From the provided knowledge, we have bunch of clips about Tomasz Patan (character_4). Let me go through the info again. In CLIP_6, theres lot of detail about his work with drones and the Jetson aircraft. He mentions scaling drone technology for personal flight, which shows hes thinking beyond the current applications. The notes say hes innovative and forward-thinking, with hands-on approach. Hes involved in testing and data analysis, and hes enthusiastic about his work. The videos tone is optimistic, highlighting his problem-solving methods and the development process. The key points here are his background in drones, his idea to scale them into personal aircraft, and his active role in testing. The description explicitly states hes innovative and forward-thinking. These traits are closely related to having rich imagination. Since hes taking existing technology (drones) and envisioning new application (personal flight), that requires creativity and imagination. Also, the narrative of the video presents him as someone who asks \"what if\" questions, which is hallmark of imaginative thinking. dont see any information suggesting he lacks imagination. All the provided details support the idea that hes imaginative. Therefore, the answer should be that Tomasz is person with rich imagination.</think> Action: [Answer] Content: Tomasz is person with rich imagination. Table 17 complete generation trajectory in the control process."
        },
        {
            "title": "H Prompt Templates",
            "content": "H.1 Prompt for Automatic Evaluator of M3-Bench Table 18 presents the prompt used by GPT-4o to assess M3-Bench. 39 The prompt for GPT-4o evaluation You are provided with question, ground truth answer, and an answer from an agent model. Your task is to determine whether the ground truth answer can be logically inferred from the agents answer, in the context of the question. Do not directly compare the surface forms of the agent answer and the ground truth answer. Instead, assess whether the meaning expressed by the agent answer supports or implies the ground truth answer. If the ground truth can be reasonably derived from the agent answer, return \"Yes\". If it cannot, return \"No\". Important notes: Do not require exact wording or matching structure. Semantic inference is sufficient, as long as the agent answer entails or implies the meaning of the ground truth answer, given the question. Only return \"Yes\" or \"No\", with no additional explanation or formatting. Input fields: question: the question asked ground_truth_answer: the correct answer agent_answer: the models answer to be evaluated Now evaluate the following input: Input: question: {question} ground_truth_answer: {ground_truth_answer} agent_answer: {agent_answer} Output (Yes or No): Table 18 Prompt used by GPT-4o to evaluate M3-Bench. H.2 Prompts for Socratic Models Table 19 presents the prompt used in Socratic Models baselines. Through prompt engineering, we find that placing the question after the long context (e.g., video detailed descriptions) enhances the models ability to retain the question and focus on relevant information, leading to improved answer accuracy. Accordingly, in our Socratic Models experiments, we adopt this approach by appending the question to the end of the retrieved clip descriptions during the RAG-based QA stage. Caption Generation Prompt (Gemini-1.5-Pro, Qwen-2.5-Omni) You are an advanced video description generator tasked with providing detailed, cohesive description of video clip. Follow these high-level principles to ensure your output is accurate and meaningful: 1. Focus on Observable Content. 2. Provide Context for the Environment and Timing. 3. Incorporate Audio Dialogue Information. You are provided with current video clip. (GPT-4o, Qwen2.5-VL-7b Variant: You are provided with 15 key frames from current video clip and audio text information <a list where each item represents speech segment dict with the following fields: start time, end time, asr. The time information is the time in the current clip and not the global time>.) (Continued on next page) 40 Your Task: Based on the video clip, generate detailed and cohesive description of the video clip. The description should focus on the entire event, incorporating all relevant aspects of the characters, their actions, spoken dialogue, and interactions in narrative format. The description should include (but is not limited to) the following categories: 1. Characters Appearance: Describe the characters appearance, including their clothing, facial features, body language, or any distinguishing characteristics that are noticeable in the frames. 2. Characters Actions & Movements: Describe specific gestures, movements, or interactions performed by the characters. Include both major and minor actions that contribute to the overall scene, emphasizing any transitions between different actions. 3. Characters Spoken Dialogue: Use the provided audio dialogue information to accurately transcribe or summarize the dialogue spoken by the characters. Include emotional tone, volume, or context if relevant (e.g., shouting, whispering, laughing). 4. Characters Contextual Behavior and Attributes: Describe the characters roles in the scene, their emotional states, motivations, or relationships with other characters. Highlight any conflict, bonding, or change in dynamics. 5. Environmental Context: Include relevant details about the environment where the scene takes place. Describe the physical location, setting, lighting, or any other environmental factors that affect the atmosphere or context of the video clip. 6. Temporal Context: Provide information about the timing of events within the scene. Describe the natural progression of time (e.g., morning, afternoon, evening) or any time-sensitive elements that contribute to the unfolding of the events. Strict Requirements: Do not use generic descriptions, inferred names, or pronouns to refer to characters (e.g., \"he,\" \"they,\" \"the man\"). The generated descriptions of the video clip should include every detail observable in the frames and mentioned in the audio dialogues. (GPT-4o, Qwen2.5-VL-7b Variant: The generated descriptions of the video clip should include every detail observable in the frames and mentioned in the audio dialogues.) Pay close attention to any introduction of characters names, titles, or other identifiers provided in the frames or audio. Whenever possible, include natural time expressions and physical location cues in the descriptions to improve contextual understanding. These should be based on inferred situational context (e.g., \"in the evening at the dinner table,\" \"early morning outside the building\"). Include relevant background, common knowledge and environmental factors when needed (e.g., location, weather, setting) to provide fuller understanding of the context. Maintain natural, narrative flow in the description, ensuring that it reads like coherent summary of the events in the video. Remember you are looking at key frames and audio dialogue information, not the full video, so focus on what can be observed from these specific materials. (GPT-4o, Qwen2.5-VL-7b Variant: Remember you are looking at key frames and audio dialogue information, not the full video, so focus on what can be observed from these specific materials.) Example Output: (Continued on next page) \"As Margaret returns with the teapot, Tom stands up to help her pour the tea, gesturing politely as she hands him cup. Margaret sits back down. Margaret leans forward slightly, her hands resting on the table, and after moment of silence, she speaks again, her voice steady but filled with hint of urgency. Tom listens closely, his brow furrowing slightly as he takes in her words. He responds quietly, nodding slowly as he processes the information.\" RAG Answer Prompt (GPT-4o) Based on the following video description, answer the question as concisely as possible. Provide only the direct answer without explanations or reasoning. Question: {question} Relevant Video Clip Captions: {retrived_clips} Answer: Table 19 The prompts for the experiments of the Socratic Models. For models that take either raw video (gemini1.5-pro, Qwen2.5-Omni-7b) input or video frames with ASR transcripts (GPT4o, Qwen2.5-VL-7b), the description generation prompt has minor differences, which are indicated in italicized parentheses. H.3 Prompts for M3-Agent Table 20 shows the prompt used by Gemini-Agent and Gemini-GPT4o-Hybrid during memorization. Table 21 shows the prompt used by Gemini-Agent and Gemini-GPT4o-Hybrid during control. Table 22 shows the prompt used by M3-Agent during the control process. Memorization Prompt ( memory-gemini-prompt, memory-7b-prompt ) You are given video along with set of character features. Each feature is either: Face: single video frame with bounding box, or Voice: one or more speech segments, each containing start_time (MM:SS), end_time (MM:SS) and asr (transcript). Every feature has unique ID enclosed in angle brackets (e.g. <face_1>, <voice_2>). Your Tasks (produce both in the same response) : 1. Episodic Memory (the ordered list of atomic captions) Using the provided feature IDs, generate detailed and cohesive description of the current video clip. The description should capture the complete set of observable and inferable events in the clip. Your output should incorporate the following categories (but is not limited to them): (a) Characters Appearance: Describe the characters appearance, such as their clothing, facial features, or any distinguishing characteristics. (b) Characters Actions & Movements: Describe specific gesture, movement, or interaction performed by the characters. (c) Characters Spoken Dialogue: Quoteor, if necessary, summarizewhat are spoken by the characters. (d) Characters Contextual Behavior: Describe the characters roles in the scene or their interaction with other characters, focusing on their behavior, emotional state, or relationships. 2. Semantic Memory (the ordered list of high-level thinking conclusions) (Continued on next page) 42 Produce concise, high-level reasoning-based conclusions across five categories: Identification Identify which face and voice features refer to the same character. Use the exact format: Equivalence: <face_x>, <voice_y>. Include as many confident matches as possible. (a) Equivalence (b) Character-level Attributes Infer abstract attributes for each character, such as: Name (if explicitly stated), Personality (e.g., confident, nervous), Role/profession (e.g., host, newcomer), Interests or background (when inferable), istinctive behaviors or traits (e.g., speaks formally, fidgets). Avoid restating visual factsfocus on identity construction. (c) Interpersonal Relationships & Dynamics Describe the relationships and interactions between characters: Roles (e.g., host-guest, leader-subordinate), Emotions or tone (e.g., respect, tension), Power dynamics (e.g., who leads), Evidence of cooperation, exclusion, conflict, etc. (d) Video-level Plot Understanding Summarize the scene-level narrative, such as: Main event or theme, Narrative arc or sequence (e.g., intro discussion reaction), Overall tone (e.g., formal, tense), Cause-effect or group dynamics. (e) Contextual & General Knowledge Include general knowledge that can be learned from the video, such as: Likely setting or genre (e.g., corporate meeting, game show), Cultural/procedural norms, Real-world knowledge (e.g., \"Alice market is pet-friendly\"), Common-sense or format conventions. Strict Requirements (apply to both sections unless noted) 1. If character has provided feature ID, refer to that character only with the ID (e.g. <face_1>, <voice_2>). 2. If no ID exists, use short descriptive phrase (e.g. \"a man in blue shirt\"). 3. Do not use \"he,\" \"she,\" \"they,\" pronouns, or invented Names. 4. Keep face/voice IDs consistent throughout. 5. Describe only what is grounded in the video or obviously inferable. 6. Include natural Time & Location cues and setting hints when inferable. 7. Each Episodic Memory line must express one event/detail; split sentences if needed. 8. Output English only. 9. Output Python list of sentences for each memory type. Additional Rules for Episodic Memory 1. Do not mix unrelated aspects in one memory sentence. 2. Focus on appearance, actions/movements, spoken dialogue (quote or summary), contextual behavior. Additional Rules for Semantic Memory 1. For Equivalence lines, use the exact format: Equivalence: <face_x>, <voice_y>. 2. Do not repeat simple surface observations already in the captions. 3. Provide only final conclusions, not reasoning steps. Expected Output Format Return the result as single Python dict containing exactly two keys: { \"episodic_memory\": [ \"In the bright conference room, <face_1> enters confidently, giving professional appearance as he approaches <face_2> to shake hands.\", \"<face_1> wears black suit with white shirt and tie. He has short black hair and wears glasses.\", \"<face_2>, dressed in striking red dress with long brown hair.\", (Continued on next page) 43 \"<face_2> smiles warmly and greets <face_1>. She then sits down at the table beside him, glancing at her phone briefly while occasionally looking up.\", \"<voice_1> speaks to the group, Good afternoon, everyone. Lets begin the meeting. His voice commands attention as the room quiets, and all eyes turn to him.\", \"<face_2> listens attentively to <voice_1>s words, nodding in agreement while still occasionally checking her phone. The atmosphere is professional, with the participants settling into their roles for the meeting.\", \"<face_1> adjusts his tie and begins discussing the agenda, engaging the participants in productive conversation.\" ], \"semantic_memory\": [ \"Equivalence: <face_1>, <voice_1>\", \"<face_1>s name is David.\", \"<face_1> holds position of authority, likely as the meetings organizer or senior executive.\", \"<face_2> shows social awareness and diplomacy, possibly indicating experience in public or client-facing roles.\", \"<face_1> demonstrates control and composure, suggesting high level of professionalism and confidence under pressure.\", \"The interaction between <face_1> and <face_2> suggests working relationship built on mutual respect.\", \"The overall tone of the meeting is structured and goal-oriented, indicating it is part of larger organizational workflow.\" ] } Please only return the valid python dict (which starts with \"{\" and ends with \"}\") containing two string lists in \"episodic_memory\" and \"semantic_memory\", without any additional explanation or formatting. Table 20 Memorization prompt for memory-gemini-prompt and memory-7b-prompt. Control Prompt You are given question and some relevant knowledge about specific video. You are also provided with retrieval plan, which outlines the types of information that should be retrieved from memory bank in order to answer the question. Your task is to reason about whether the provided knowledge is sufficient to answer the question. If it is sufficient, output [ANSWER] followed by the answer. If it is not sufficient, output [SEARCH] and generate query that will be encoded into embeddings for vector similarity search. The query will help retrieve additional information from memory bank that contains detailed descriptions and high-level abstractions of the video, considering the question, the provided knowledge, and the retrieval plan. Your response should contain two parts: 1. Reasoning Analyze the question, the knowledge, and the retrieval plan. If the current information is sufficient, explain why and what conclusions you can draw. If not, clearly identify what is missing and why it is important. 2. Answer or Search [ANSWER]: If the answer can be derived from the provided knowledge, output [ANSWER] followed by short, clear, and direct answer. When referring to character, always use their specific name if available. Do not use ID tags like <character_{1}> or <face_{1}>. (Continued on next page) 44 [SEARCH]: If the answer cannot be derived yet, output [SEARCH] followed by single search query that would help retrieve the missing information. Instructions for [SEARCH] queries: Use the retrieval plan to inform what type of content should be searched for next. These contents should cover aspects that provide useful context or background to the question, such as character names, behaviors, relationships, personality traits, actions, and key events. Use keyword-based queries, not command sentences. Queries should be written as compact keyword phrases, not as full sentences or instructions. Avoid using directive language like \"Retrieve\", \"Describe\", or question forms such as \"What\", \"When\", \"How\". Keep each query short and focused on one point. Each query should target one specific type of information, without combining multiple ideas or aspects. Avoid over-complexity and unnecessary detail. Do not include too many qualifiers or conditions. Strip down to the most essential keywords needed to retrieve valuable content. The query should target information outside of the existing knowledge that might help answer the question. For time-sensitive or chronological information (e.g., events occurring in sequence, changes over time, or specific moments in timeline), you can generate clip-based queries that reference specific clips or moments in time. These queries should include reference to the clip number, indicating the index of the clip in the video (a number from 1 to N, where smaller number indicates an earlier clip). Format these queries as \"CLIP_x\", where should be an integer that indicates the clip index. Note only generate clip-based queries if the question is about specific moment in time or sequence of events. You can also generate queries that focus on specific characters or characters attributes using the id shown in the knowledge. Make sure your generated query focus on some aspects that are not retrieved or asked yet. Do not repeatedly generate queries that have high semantic similarity with those generated before. Example 1: Input: Question: How did the argument between Alice and Bob influence their relationship in the story? Knowledge: [ {{ \"query\": \"What happened during the argument between Alice and Bob?\", \"related memories\": {{ \"CLIP_2\": [ \"<face_1> and <face_2> are seen arguing in the living room.\" \"<face_1> raises her voice, and <face_2> looks upset.\" \"<face_1> accuses <face_2> of not listening to her.\" ], }} }} ] Output: It seems that <face_1> and <face_2> are arguing about their relationship. need to figure out the names of <face_1> and <face_2>. [SEARCH] What are the names of <face_1> and <face_2>? (Continued on next page) 45 Example 2: Input: Question: How did the argument between Alice and Bob influence their relationship in the story? Knowledge: [ {{ \"query\": \"What happened during the argument between Alice and Bob?\", \"related memories\": {{ \"CLIP_2\": [ \"<face_1> and <face_2> are seen arguing in the living room.\" \"<face_1> raises her voice, and <face_2> looks upset.\" \"<face_1> accuses <face_2> of not listening to her.\" ], }} }}, {{ \"query\": \"What are the names of <face_1> and <face_2>?\", \"related memories\": {{ \"CLIP_1\": [ \"<face_1> says to <face_2>: am done with you Bob!\"\", \"<face_2> says to <face_1>: What about now, Alice?\"\" ], }} }} ] Output: It seems that content in CLIP_2 shows exactly the argument between Alice and Bob. To figure out how did the argument between Alice and Bob influence their relationship, need to see what happened next in CLIP_3. [SEARCH] What happened in CLIP_3? Now, generate your response for the following input: Question: {question} Knowledge: {search_results} Output: Control Prompt (last round) You are given question about specific video and dictionary of some related information about the video. Each key in the dictionary is clip ID (an integer), representing the index of video clip. The corresponding value is list of video descriptions from that clip. Your task is to analyze the provided information, reason over it, and produce the most reasonable and well-supported answer to the question. (Continued on next page) 46 Output Requirements: Your response must begin with brief reasoning process that explains how you arrive at the answer. Then, output [ANSWER] followed by your final answer. The format must be: Here is the reasoning... [ANSWER] Your final answer here. Your final answer must be definite and specific even if the information is partial or ambiguous, you must infer and provide the most reasonable answer based on the given evidence. Do not refuse to answer or say that the answer is unknowable. Use reasoning to reach the best possible conclusion. Additional Guidelines: When referring to character, always use their specific name if it appears in the video information. Do not use placeholder tags like <character_1> or <face_1>. Avoid summarizing or repeating the video information. Focus on reasoning and answering. The final answer should be short, clear, and directly address the question. Input: Question: {question} Video Information: {search_results} Output: Table 21 Control prompt for Gemini-Agent and Gemini-GPT4o-Hybrid. system_prompt You are given question and some relevant knowledge. Your task is to reason about whether the provided knowledge is sufficient to answer the question. If it is sufficient, output [Answer] followed by the answer. If it is not sufficient, output [Search] and generate query that will be encoded into embeddings for vector similarity search. The query will help retrieve additional information from memory bank. Question: instruction_prompt Output the answer in the format: Action: [Answer] or [Search] Content: {content} If the answer cannot be derived yet, the {content} should be single search query that would help retrieve the missing information. The search {content} needs to be different from the previous. You can get the mapping relationship between character ID and name by using search query such as: \"What is the name of <character_{i}>\" or \"What is the character id of {name}\". After obtaining the mapping, it is best to use character ID instead of name for searching. If the answer can be derived from the provided knowledge, the {content} is the specific answer to the question. Only name can appear in the answer, not character ID like <character_{i}>. last_round_prompt The Action of this round must be [Answer]. If there is insufficient information, you can make reasonable guesses. Table 22 The prompts used by M3-Agent during the control process."
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "Shanghai Jiao Tong University",
        "Zhejiang University"
    ]
}