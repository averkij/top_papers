{
    "paper_title": "On Robustness and Reliability of Benchmark-Based Evaluation of LLMs",
    "authors": [
        "Riccardo Lunardi",
        "Vincenzo Della Mea",
        "Stefano Mizzaro",
        "Kevin Roitero"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) effectiveness is usually evaluated by means of benchmarks such as MMLU, ARC-C, or HellaSwag, where questions are presented in their original wording, thus in a fixed, standardized format. However, real-world applications involve linguistic variability, requiring models to maintain their effectiveness across diverse rewordings of the same question or query. In this study, we systematically assess the robustness of LLMs to paraphrased benchmark questions and investigate whether benchmark-based evaluations provide a reliable measure of model capabilities. We systematically generate various paraphrases of all the questions across six different common benchmarks, and measure the resulting variations in effectiveness of 34 state-of-the-art LLMs, of different size and effectiveness. Our findings reveal that while LLM rankings remain relatively stable across paraphrased inputs, absolute effectiveness scores change, and decline significantly. This suggests that LLMs struggle with linguistic variability, raising concerns about their generalization abilities and evaluation methodologies. Furthermore, the observed performance drop challenges the reliability of benchmark-based evaluations, indicating that high benchmark scores may not fully capture a model's robustness to real-world input variations. We discuss the implications of these findings for LLM evaluation methodologies, emphasizing the need for robustness-aware benchmarks that better reflect practical deployment scenarios."
        },
        {
            "title": "Start",
            "content": "On Robustness and Reliability of Benchmark-Based Evaluation of LLMs Riccardo Lunardi, Vincenzo Della Mea, Stefano Mizzaro and Kevin Roitero* University of Udine, Italy 5 2 0 2 4 ] . [ 1 3 1 0 4 0 . 9 0 5 2 : r Abstract. Large Language Models (LLMs) effectiveness is usually evaluated by means of benchmarks such as MMLU, ARC-C, or HellaSwag, where questions are presented in their original wording, thus in fixed, standardized format. However, real-world applications involve linguistic variability, requiring models to maintain their effectiveness across diverse rewordings of the same question or query. In this study, we systematically assess the robustness of LLMs to paraphrased benchmark questions and investigate whether benchmarkbased evaluations provide reliable measure of model capabilities. We systematically generate various paraphrases of all the questions across six different common benchmarks, and measure the resulting variations in effectiveness of 34 state-of-the-art LLMs, of different size and effectiveness. Our findings reveal that while LLM rankings remain relatively stable across paraphrased inputs, absolute effectiveness scores change, and decline significantly. This suggests that LLMs struggle with linguistic variability, raising concerns about their generalization abilities and evaluation methodologies. Furthermore, the observed performance drop challenges the reliability of benchmark-based evaluations, indicating that high benchmark scores may not fully capture models robustness to real-world input variations. We discuss the implications of these findings for LLM evaluation methodologies, emphasizing the need for robustness-aware benchmarks that better reflect practical deployment scenarios."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) capabilities have rapidly advanced in recent years, demonstrating state-of-the-art performance across variety of natural language processing tasks. Their effectiveness is primarily assessed through standardized benchmark evaluations, such as the Massive Multitask Language Understanding (MMLU) [16], Winogrande [32], and HellaSwag [46]. These benchmarks provide structured and widely accepted framework for comparing models, ensuring consistency in evaluation, in multiple-choice question answering setting. However, questions are presented in fixed, standardized wording and this controlled setting does not fully account for the variability inherent in real-world interactions, where users may express the same intent in multiple ways. crucial open question is whether LLMs evaluated in these rigid benchmark settings exhibit robustness to linguistic variation. Recent studies have shown that LLMs are highly sensitive to changes in prompt formatting and minor alterations in wording, often leading to significant performance fluctuations [35, 47]. This phenomenon raises concerns about model robustness, i.e., the ability of LLMs to All authors are corresponding authors. Email: {name.surname}@uniud.it generalize across semantically equivalent yet reworded inputs. If models effectiveness drops drastically when faced with paraphrased questions, its real-world applicability and generalization capabilities may be overestimated. At the same time, these observations challenge the reliability of benchmark-based evaluations. If benchmark scores are highly dependent on the specific phrasing of questions, do they truly reflect models reasoning ability, or do they merely capture performance on particular wording of the task? While prior research has explored the impact of prompt variations on LLM outputs [23], less attention has been given to the extent to which benchmark evaluations remain stable and reliable when test questions have systematic rewording. In this study, we investigate both of these fundamental issues. First, we assess the robustness of LLMs by systematically paraphrasing benchmark questions and measuring the resulting changes in performance. Second, we evaluate the reliability of benchmark-based evaluations by analyzing whether the relative ranking of models remains stable across paraphrased inputs. By introducing controlled linguistic and syntactic variations, our study provides direct test of models ability to generalize beyond the specific phrasings seen during training. This allows us to assess whether current benchmark methodologies truly capture models underlying reasoning capabilities or whether they merely reflect performance on narrowly framed tasks, ultimately risking an overstatement of model effectiveness. We address the following research questions: RQ1 Are benchmark-based evaluations reliable? Do LLM evaluation results change significantly when benchmark questions are replaced by simple paraphrases? RQ2 Are LLMs robust to question paraphrases? Does question paraphrasing decrease LLM effectiveness, revealing limitations in their generalization capabilities? Our findings reveal that while the rankings of LLMs remain relatively stable, their absolute performance drops significantly when questions are paraphrased. This suggests that while benchmarks may provide reasonable comparative measure of models, they overestimate absolute performance and generalization abilities. All data and code, including the complete set of questions, paraphrases, model predictions, and high-resolution plots, are available at https://osf.io/ jq56f/?view_only=aa6d0c67db7e47c08cff3b3e1a610764."
        },
        {
            "title": "2 Background and Related Work",
            "content": "Effectiveness evaluation is fundamental problem across multiple disciplines, including Information Retrieval (IR), Natural Language Processing (NLP), and Artificial Intelligence (AI). Reliable evaluation frameworks are essential for ensuring that progress in the field reflects genuine improvements rather than artifacts of specific test conditions. Effectiveness evaluation has long tradition especially in the IR field, where rich body of research has explored how to design evaluations that are both efficient and reliable, particularly when human annotations are expensive or limited [15, 34, 36]. Studies have proposed methods to reduce the number of relevance judgments without compromising evaluation validity [6, 7], analyzed optimal topic set sizes for stable system comparisons [14], and investigated model ranking stability under incomplete assessments [40]. These works collectively emphasize that relative rankings of retrieval systems tend to remain stable even under noisy or incomplete assessments [3, 38, 39]. Recent work has also raised concerns about the long-term reliability of test collections, showing that even if system rankings remain stable, benchmarks may lose discriminative power over time as models overfit to narrow interpretations of relevance, effectively expiring in their utility [29]. Insights from decades of IR evaluation research can be leveraged in the context of LLM benchmarking, where structured evaluations have become the de-facto standard for measuring model capabilities. While benchmarks are widely used to track progress and compare model performance, their quality varies significantly, and many commonly used benchmarks suffer from issues such as poor design, limited replicability, or lack of statistical rigor [31]. Benchmarks such as MMLU [16], ARC-C [11], and HellaSwag [46] provide standardized datasets that enable reproducible model comparisons. However, these benchmarks assume rigidly controlled testing environment in which questions are presented in fixed wording and format. This does not reflect real-world applications, where linguistic variability is the norm [25, 26, 37]. In fact, humans may naturally phrase the same question in multiple ways depending on context, intent, or background knowledge, making it essential to assess whether models can maintain their effectiveness across diverse rewordings. Recent studies have shown that LLMs are highly sensitive to prompt variations, with minor changes in wording, structure, or formatting leading to substantial performance shifts [35, 47]. While prompt engineering techniques have investigated how to optimize LLM outputs by refining input phrasing [41], this does not necessarily indicate robustness of models. Instead, it often exposes an overreliance on surface-level patterns, where high benchmark scores may be driven by implicit dataset artifacts rather than genuine reasoning abilities [23]. This raises the question of whether LLM evaluations conducted under fixed benchmark conditions truly reflect models generalization capacity, or if they merely measure performance on narrow set of specific phrasings. Another relevant line of research focuses on Benchmark Agreement Testing (BAT), which investigates whether different benchmarks yield consistent rankings of models. While there is evidence that rankings are preserved despite fluctuations in absolute scores, less attention has been given to whether the rankings remain stable when test questions themselves are paraphrased. If LLM rankings are highly sensitive to minor linguistic changes, this would suggest that benchmark-based evaluations may not fully capture the robustness of LLMs to real-world input variability. Beyond static benchmarks, alternative evaluation methodologies have been proposed, such as using LLMs themselves as judges to assess model outputs [18, 48, 50]. While LLM-based evaluation provides scalability and cost-effectiveness, it introduces potential biases, particularly in favor of models that share similar architectures or training data distributions. Human evaluation remains the gold standard for assessing open-ended tasks, as seen in large-scale efforts such as ChatBot Arena [48], which aggregates human preferences to compute model rankings. However, human evaluation is costly, subject to annotator variability, and difficult to scale across extensive benchmark datasets. These limitations reinforce the continued reliance on structured benchmarks, despite their constraints. The question of whether benchmark-based evaluations reliably reflect model robustness remains largely unexplored. While existing studies have investigated the stability of model rankings across different datasets or across answers [33], the impact of systematic question paraphrasing on LLM performance and benchmark reliability remains an open issue. Understanding whether LLMs maintain effectiveness across linguistic variations is crucial for ensuring that benchmark scores translate to real-world applicability. We systematically examine the effects of paraphrased inputs on LLM evaluation, contributing to broader discussion on the limitations of current benchmarking methodologies and on the need for robustness-aware evaluation frameworks."
        },
        {
            "title": "3 Methodology",
            "content": "We detail the benchmarks, the LLMs, the paraphrases generation method, and the prompts that we use in our experiments."
        },
        {
            "title": "3.1 Benchmarks",
            "content": "We leverage six well-established benchmarks covering diverse reasoning and knowledge domains. ARC-C. The AI2 Reasoning Challenge (ARC-C) [11] includes science questions that require deeper logical reasoning beyond simple fact retrieval. The dataset is explicitly constructed to be challenging for models relying on pattern recognition, demanding causal reasoning, scientific knowledge application, and inference abilities. HellaSwag. HellaSwag [46] assesses commonsense reasoning by presenting context followed by four possible continuations, one of which is the correct next sentence. Incorrect choices are adversarially generated to appear plausible, forcing models to rely on deeper narrative understanding rather than surface-level cues. MMLU. The Massive Multitask Language Understanding (MMLU) benchmark [16] evaluates LLMs across 57 diverse subjects, including history, mathematics, law, and computer science. It consists of questions designed to assess both factual recall and reasoning skills. MMLU is widely used due to its broad coverage and structured format. OpenBookQA. OpenBookQA [24] tests an LLMs ability to apply elementary science knowledge beyond direct memorization. It consists of questions grounded in curated set of science facts, requiring models to reason over both the provided knowledge and external commonsense information. RACE. RACE [20] is reading comprehension dataset derived from English language exams for middle-high school students. Questions require to extract implicit meanings, summarize main ideas, and perform higher-level inference. Its diverse passage types and complex reasoning tasks make it challenging benchmark. SciQ. SciQ [44] covers physics, chemistry, biology, and earth sciences. Each question is paired with four answers and supporting paragraph, allowing for retrieval-augmented evaluation. It assesses factual knowledge and ability to leverage explanations. Table 1: Benchmarks statistics. The release date is presumed based on the first public appearance of the dataset. Considered Questions Paraphrasable Questions Number of Paraphrases Benchmark Name Release Date ARC-C HellaSwag MMLU OpenBookQA RACE SciQ Total 2,566 15,011 18,957 1,487 11,949 2,996 52,966 2,566 15,011 18,955 1,487 11,938 2,996 52,935 2018-03-14 2019-05-29 2020-09-10 2018-09-08 2017-04-15 2017-0712,830 75,050 94,775 7,435 59,690 14,980 264,761 Each question has four possible answers, and only one of them is the correct choice. Table 1 reports key statistics of the datasets. For the considered benchmarks, we paraphrased the full test split along with an additional sample of training instances, with sample size varying according to benchmark size: 5,000 examples for HellaSwag, MMLU, and RACE (larger benchmarks), 1,119 for ARC-C, 1000 for SciQ, and 500 for OpenBookQA (smaller ones). Additionally, for ARC-C, 13 questions were excluded as they contained 3 or 5 answer options instead of the expected 4. We observed qualitatively similar patterns of variability across both benchmark test and training instances, in terms of how model responses changed across paraphrases. This held consistently across different benchmarks and model families. Given this, we present results over both test and sampled training data, treating them uniformly in our evaluation. Several well-known benchmarks have been excluded from our study due to their format, task design, or evaluation constraints. Some datasets, such as CRASS [13], WinoGrande [32], PIQA [4], and BoolQ [10], feature ternaryor binary-choice questions, limiting their applicability to our multiple-choice evaluation framework. Others, like GPQA [30], contain highly technical questions with extreme difficulty, making paraphrase evaluation impractical. Benchmarks such as GSM-8K [12] and MATH [17] require natural language answers assessed via automatic scoring, diverging from our controlled multiple-choice setup. Datasets focused on conversational or multi-turn interactions, including MT-Bench [2], QuAC [9], and ACI-Bench [45], were omitted due to their dependency on dialogue history. Similarly, retrieval-based datasets like MS-MARCO [27] and QMSum [49] do not align with our evaluation approach, as they emphasize passage retrieval rather than direct question answering. Code-related benchmarks such as BigCodeBench [51], CodeXGLUE [22], HumanEval [8], and MBPP [1] were also excluded, as our focus remains on natural language understanding. Lastly, we omitted datasets designed for model-based judgment and ranking, including LLM Judge [48], Prometheus [18], and JudgeLM [50], as they evaluate LLMs in self-referential manner rather than assessing their robustness to question paraphrasing."
        },
        {
            "title": "3.2 Large Language Models",
            "content": "We evaluate diverse selection of 34 LLMs, using wide range of model sizes, architectures, and training paradigms. They are listed in Table 2, which also presents the average effectiveness (accuracy over 4 classes) of LLMs across the 6 benchmarks. These models include both compact, efficiency-focused variants and large-scale architectures with billions of parameters, allowing us to analyze how model scale influences robustness to paraphrased inputs. The models are predominantly instruction-tuned, meaning they have undergone fine-tuning on human-generated prompts using Reinforcement Learning from Human Feedback (RLHF) [28] to improve their ability to follow instructions and generalize across different tasks. All open-source models in this study have been accessed through HugTable 2: Four-class accuracy values across benchmarks."
        },
        {
            "title": "Model",
            "content": "1. Mistral-Large-Instruct-2411 2. Qwen2.5-72B-Instruct 3. Llama-3.1-Nemotron-70B-Instruct-HF 4. Meta-Llama-3.1-70B-Instruct 5. gpt4omini 6. gemma-2-27b-it 7. Qwen2.5-7B-Instruct 8. gemma-2-9b-it 9. Phi-3-mini-4k-instruct 10. Pixtral-12B-2409 11. Mixtral-8x7B-Instruct-v0.1 12. Phi-3-mini-128k-instruct 13. aya-expanse-32b 14. Mistral-Small-Instruct-2409 15. Nemotron-Mini-4B-Instruct 16. Mistral-Nemo-Instruct-2407 17. Meta-Llama-3.1-8B-Instruct 18. aya-expanse-8b 19. Qwen2-VL-2B-Instruct 20. gemma-1.1-7b-it 21. zephyr-7b-beta 22. Meta-Llama-3-8B-Instruct 23. Ministral-8B-Instruct-2410 24. gemma-2-2b-it 25. EuroLLM-9B-Instruct 26. Mistral-7B-Instruct-v0.1 27. Llama-3.2-1B-Instruct 28. gemma-1.1-2b-it 29. gemma-7b-it 30. falcon-7b-instruct 31. Mistral-Nemo-Base-2407 32. bloom-7b1 33. bloom-560m 34. EuroLLM-1.7B-Instruct"
        },
        {
            "title": "Average",
            "content": "g a H - A M o e C I .94 .89 .80 .88 .74 .94 .94 .83 .80 .89 .74 .95 .93 .79 .81 .88 .73 .95 .92 .78 .80 .87 .73 .95 .92 .82 .76 .85 .71 .94 .89 .77 .74 .83 .70 .93 .87 .75 .70 .80 .69 .92 .89 .75 .71 .79 .68 .91 .85 .76 .67 .79 .65 .90 .85 .74 .64 .78 .67 .85 .82 .69 .68 .78 .66 .89 .83 .74 .65 .76 .63 .90 .79 .75 .65 .75 .67 .89 .81 .61 .63 .77 .63 .88 .78 .61 .58 .76 .64 .83 .71 .53 .57 .62 .61 .75 .66 .43 .54 .62 .57 .85 .60 .67 .53 .56 .59 .72 .60 .50 .49 .59 .55 .79 .64 .47 .53 .54 .56 .78 .63 .41 .52 .55 .55 .70 .63 .31 .51 .58 .51 .80 .58 .49 .50 .50 .57 .65 .46 .26 .38 .38 .38 .70 .42 .26 .38 .37 .41 .65 .38 .27 .38 .36 .45 .55 .33 .25 .33 .43 .29 .60 .29 .26 .29 .28 .29 .40 .22 .25 .23 .27 .23 .25 .22 .25 .23 .27 .22 .25 .22 .25 .23 .27 .22 .26 .22 .25 .23 .27 .22 .25 .22 .25 .23 .27 .22 .25 .22 .25 .23 .27 .22 .25 .63 .53 .53 .59 .53 .71 r .87 .86 .85 .84 .83 .81 .79 .79 .77 .75 .75 .75 .75 .72 .70 .63 .61 .61 .59 .59 .56 .56 .55 .43 .41 .40 .37 .30 .24 .24 .24 .24 .24 . ging Face, ensuring consistency in implementation and evaluation. Additionally, we include closed-source reference model, ChatGPT (gpt4o-mini), accessed via OpenAIs API. This allows us to compare publicly available models against proprietary system that benefits from continuous updates and private optimization strategies."
        },
        {
            "title": "3.3 Paraphrases Generation",
            "content": "To assess the robustness of LLMs against linguistic variation, we systematically paraphrase benchmark questions while ensuring their semantic integrity remains intact. This allows us to isolate the impact of rewording on model effectiveness, providing clearer understanding of how linguistic variability affects performance. By maintaining the original intent and reasoning requirements of each question, we ensure that performance changes are due to model robustness rather than altered question difficulty. Additionally, to avoid introducing confounding factors, we preserve the original order of answer choices, ensuring that only the question phrasing is modified. Given the large scale of our benchmarks, fully manual paraphrasing would be infeasible. Therefore, we rely on automatic paraphrasing, which we then validate to control that the generated variations maintain the original meaning and align with the intended answer, while introducing linguistic diversity. Further details on the validation methodology and results are discussed below in Section 4.1.3. Paraphrased versions of the benchmark questions are generated usParaphrase the following text 5 times, preserving the original meaning and avoiding negations. If it is question, ensure the paraphrases keep it in question form; if it is sentence fragment, complete it in natural way. Each paraphrase must be unique, numbered, and placed on new line. Original Text: {original text}. Output each paraphrase as follows: 1. 2. 3. 4. 5. Figure 1: Paraphrasing prompt. ing OpenAIs GPT-4o mini model,1 accessed via API. The model is prompted to produce five alternative phrasings of each question while adhering to strict constraints to preserve meaning. The paraphrasing prompt is shown in Figure 1. This paraphrasing process allows us to test model robustness to rewording without introducing other changes that could affect performance. While the vast majority of benchmark questions were successfully paraphrased, we found that for small subset of 31 instances (i.e., less than 0.0004% of the 53,966 questions, see details in Table 1), the model refused to generate paraphrases, typically due to the presence of explicit language or content that triggered its safety mechanisms. These few cases were excluded from our experiments to preserve the integrity and consistency of the evaluation."
        },
        {
            "title": "3.4 Model Prompting",
            "content": "To systematically evaluate model responses across benchmarks, we employ standardized prompting strategy that ensures consistency across all LLMs. Each model is presented with question in multiple-choice format and tasked with selecting the most appropriate answer among the four possible options. To maintain uniformity, we enforce constraints on response generation, ensuring that models produce outputs strictly adhering to the predefined answer format of each benchmark. We use the prompt as in Figure 2 to present the questions to the models, where {question} is the text of the question, and {choices list} refers to the multiple-choice options presented in numbered list. The structured format prompt ensures that models generate responses strictly in numerical format, avoids output variability, and allows for straightforward evaluation of correctness. Although we did not systematically experiment with prompt variations, we observed no changes in model responses when modifying the phrasing of the prompt, suggesting that answer selection remained stable across minor prompt rewordings. For answer selection, we adopt standard approach based on top1 token probability, where the models final prediction corresponds to the highest-probability token in its output distribution. This decoding strategy is widely used in benchmark-based LLM evaluation settings [16], as it avoids stochasticity and facilitates reproducibility. Since we preserve the original order of answer choices, the observed performance differences should stem from linguistic variation only, rather than choice position bias. All evaluations are conducted in zero-shot setting [19, 41], where models receive no examples and are expected to select an answer directly. While this setup avoids prompt engineering, it also limits the models capacity for structured reasoning. To explore the impact of more complex reasoning strategies, we also run some additional experiments. Although we do not have the space to discuss results in detail, we report some data as an indication that they do not seem 1 https://platform.openai.com/docs/models#gpt-4o-mini You will be asked to answer question by choosing the correct option number only (14). Please respond with just the number, without any additional text, symbols, or punctuation. Question: {question} Options: {choices list} Provide only the number of the correct answer (14). For example, if the correct answer is the first option, respond with 1 only. Answer: Figure 2: Evaluation prompt. promising in terms of improving (or changing) the effectiveness and consistency of the models substantially. For example, we run experiments using Chain-of-Thought (CoT) prompting, in which each model generates 5 reasoning paths per input, followed by majority voting over the predicted answers. We found that only subset of models were able to handle CoT prompting reliably, while others frequently produced hallucinated reasoning steps [42, 43]. Even among the models that managed CoT effectively, the average improvement in accuracy over the zero-shot setting was typically below 3%, with the exception of models 16, 25 and 34, which nonetheless did not achieve accuracies higher than 0.70 (and therefore are not competitive with the best zero-shot models, as can be seen by comparing with the accuracy values in Table 2). Moreover, CoT prompting led to comparable drop in response consistency of approximately 7% across models. For these reasons, and to remain aligned with standard benchmark protocols, we focus our evaluation on zero-shot prompting with deterministic top-1 decoding, leaving deeper investigation into CoT and other complex reasoning strategies for future work."
        },
        {
            "title": "4 Results",
            "content": "We analyze the two research questions in the following subsections."
        },
        {
            "title": "4.1 RQ1: Are Benchmark Evaluations Reliable?",
            "content": "We discuss how paraphrases affect both answer consistency and effectiveness of the models, as well as paraphrases generation validity."
        },
        {
            "title": "4.1.1 Consistency of Models Across Paraphrases",
            "content": "We start by analyzing the consistency of model answers across paraphrased versions of the same question. Figure 3 reports the cumulative distribution of answer diversity across all evaluated models and benchmarks. In each plot (one for each dataset), the x-axis shows the number of different answers (from 1 to 4, as each question has four possible choices) model gives to paraphrases of the same original question (i.e., 6 possible cases: the original question and the five generated paraphrases). The y-axis shows the cumulative proportion of such cases. Each colored line corresponds to one model, and the dashed black line denotes the average trend across all models. The plots show that only minority of models consistently select the same answer across all paraphrases of question (the few lines on the top part of the plots, for which the proportion of only one answer given is around 100%). Most models vary their answers depending on the specific formulation of the question. Even if there is quite variation across models, we can summarize these data by looking at the average (black dashed line): around 70%85% of questions receive single consistent answer from model, with the remaining 15%30% of cases showing two, three, or even four distinct answers across paraphrases. The fractions of questions receiving more than 2 distinct answers are low but not zero. For the six datasets from left to right, the fractions of questions receiving exactly 3 distinct answers Figure 3: Cumulative proportion of different answers. Model numbers as in Table 2. are 2.58%, 3.26%, 3.31%, 3.99%, 4.77%, 1.6%, and 4 distinct answers are 0.26%, 0.67%, 0.41%, 0.41%, 0.77%, 0.13%. This phenomenon is robust across all benchmarks, with similar patterns observed for all of them. These results, and specifically the 15%30% of cases for which at least two answers are selected, point to substantial degree of response variability in current LLMs when evaluated with paraphrased questions provided as inputs. In particular, even state-of-the-art models occasionally exhibit significant sensitivity to surface-level changes in question phrasing. While this metric does not directly indicate whether the selected answers are correct, it raises concerns about the stability of benchmark-based evaluations. If models output varies across semantically equivalent rewordings, benchmark accuracy scores may reflect the particular wording of test items more than the models reasoning abilities. It is important to note that some models (i.e., 1, 2, 3, 5 and 6) demonstrate high consistency, suggesting that more stable behavior is achievable (although often at the expense of low accuracy, as we discuss shortly). However, even the most robust models fail to maintain perfect consistency across all questions. The plots also show some differences across benchmarks; in particular, models consistently achieve lower consistency on OpenBookQA and, especially, RACE. These differences might arise from the greater complexity and ambiguity of questions in these datasets. For instance, OpenBookQA and RACE often require multi-step inference."
        },
        {
            "title": "4.1.2 Model Accuracy, and Relationship with Consistency",
            "content": "We now turn to investigate whether models that are more consistent are also more accurate, and how effectiveness is affected when question rewordings are introduced. We analyze the correlation between models accuracy and its proportion of consistent answers across paraphrased questions over all datasets in Figure 4: each point represents model on dataset, with accuracy on the x-axis and the proportion of consistent answers on the y-axis. Models are divided by size, with smaller models (015B parameters) shown in blue and larger models (16150B) in orange. For each of the two groups, we fit linear regression line and report the Pearsons ρ correlation coefficient and its associated p-value. As we can see by inspecting the plot and the regression lines, the two model groups exhibit opposite trends. For smaller models, there is statistically significant negative correlation between accuracy and consistency (ρ = 0.51). That is, less capable models tend to be more consistent in their predictions, although they are often wrong. This behavior may be attributed to over-simplicity: such models may repeat the same, often wrong, answer regardless of subtle rewordFigure 4: Correlation between models accuracy and proportion of consistent answers. ings due to their limited real understanding of the questions semantics (i.e., true meaning), although this does not explain why the most stubborn models are also those that are more frequently wrong. In contrast, larger models show strong and significant positive correlation between accuracy and consistency (ρ = 0.79), suggesting that as models become larger, they not only improve in accuracy over benchmarks questions but also become more robust to linguistic variability: these models are more able to generalize and to preserve correct answers across different paraphrased inputs. These findings confirm what can be perceived from the shape of the plot, i.e., that consistency is not always proxy for correctness: high consistency on paraphrased questions can coincide with low accuracy in under-performing models. However, for more advanced models, consistency becomes meaningful indicator of robustness and generalization, as they are more likely to preserve correct answers across different questions phrasings. This duality highlights the importance of interpreting consistency in context, particularly when using it as proxy for benchmark reliability. More generally, our results suggest that benchmark evaluations as often conducted today, i.e., reporting single average accuracy score over the original questions, may not fully capture models true semantic understanding [5]. Although the models with the highest accuracy (right part of the plot) are also among the most consistent ones, model with high average accuracy might in fact be not robust to surface-level changes in question formulation. This means that benchmarks relying only on original question formulations may overestimate models real-world robustness, as they fail to account for performance drops under minor linguistic variations. LLMs. This further motivates the need for robustness-aware evaluation methodologies that account for variation in question wording."
        },
        {
            "title": "4.2 RQ2: Are LLMs Robust to Question Paraphrases?",
            "content": "We now turn to deeper analysis of whether benchmark-based evaluations remain stable after question paraphrasing. Specifically, we aim to understand if paraphrasing affects how models are ranked on the basis of their accuracy or significantly impacts evaluation outcomes beyond raw accuracy degradation. To this end, Figure 6 presents multi-faceted view of the relationship between models effectiveness on original questions and sampled generated paraphrases. Each plot corresponds to one of the six benchmarks, and shows the performance of models sorted as in Table 2, by their average accuracy on the original (i.e., non-paraphrased) benchmarks (i.e., the last column of the table). Only models 129 are shown, given the systematically low accuracy of models 3034. The red cross indicates the models accuracy on the original questions for that dataset (i.e., the values in the column of the table corresponding to that dataset), while the gray boxplots represent the distribution of accuracy obtained from 1,000 samples of generated paraphrases. The sampling was performed only on the test set and on the validation set, if present, to focus the evaluation on generalization capabilities. Each sample selects different random paraphrase for each question, sometimes the original, sometimes one of the five generated variants, thus simulating realistic linguistic variability during inference. This approach allows us to examine how paraphrased question variants affect model accuracy and rankings in aggregate. For example, one sampled instance might use paraphrase 1 for Question 1, paraphrase 3 for Question 2, and so on. We classify each case into three categories: (i) Over, where the original score is higher than the paraphrase distribution, indicating performance degradation with rewordings; (ii) In, where the red cross lies inside the interquartile range of the boxplot, suggesting that paraphrasing has minimal impact; (iii) Under, where the original score is lower than the paraphrase sample accuracy, suggesting that the model benefits from rephrasing. Across all benchmarks, we observe clear trend: the majority of models fall into the Over category, meaning that their performance degrades when faced with paraphrased inputs. This observation underscores the limited robustness of LLMs to surface-level linguistic changes. Only small fraction of models remain stable across paraphrases (In), and very few show improvements (Under). For example, in MMLU, only 5 models fall within the paraphrase boxplot, 28 perform worse, and just 1 improves. This pattern is consistent across all benchmarks and it is further supported by the comparison of model accuracy on the first and last generated paraphrase, represented in Figure 6 as orange and blue small circles, respectively. Across benchmarks, these points often alternate in position (i.e., sometimes the first paraphrase yields higher accuracy, sometimes the last) with no consistent pattern. On average, the accuracy across all models is 0.54 for both the first and the last paraphrase, underscoring the semantic equivalence of the variants discussed in Section 4.1.3. This result supports the idea that no single paraphrase systematically favors or disadvantages model effectiveness, and thus somehow validates their use for probing model robustness. We also compute Kendalls τ between the rankings derived from original accuracy and those derived from median paraphrased performance (i.e., the central horizontal line of each boxplot in figure, for each dataset). The resulting τ values are shown in figure and are all above 0.9 and statistically significant (p < 0.01): they indicate Figure 5: Cumulative proportion of distinct answers as the number of generated paraphrases per question increases, starting from the original question (0) plus one generated paraphrase, up to five. In the rightmost plot, the generated paraphrases are in the inverse order. Consequently, their reliability as tools for LLM evaluation is limited, since true language understanding requires stability in answering across diverse phrasings. Incorporating paraphrased inputs into evaluation protocols can help address this limitation, offering more realistic and comprehensive measure of model capabilities."
        },
        {
            "title": "4.1.3 Evaluation of Generated Paraphrases",
            "content": "To analyze the reliability of benchmark evaluations, we examine how the number of generated paraphrases influences the consistency of model responses. Figure 5 reports the cumulative proportion of distinct answers produced by models as additional paraphrases are introduced, ranging from only the original question (0) to up to six total variants (original plus five paraphrases). Each line represents the average consistency across all models for given number of available paraphrased versions. Only three benchmarks are shown, as the remaining ones have very similar trends. In the rightmost plot, the paraphrases are presented in reverse order (from the fifth to the first). By focusing on the first 3 plots of the figure, we observe clear and consistent trend across all benchmarks: as more paraphrases are added, the number of distinct answers increases. This is reflected by the systematic downward shift of each line as the number of generated paraphrases grows. In particular, the proportion of questions for which models provide single, consistent answer decreases steadily with each added variant. This means that paraphrasing indeed introduces increasing confusion for LLMs, thereby reducing consistency and revealing their sensitivity to linguistic variation. The lines are well-separated and ordered as expected, i.e., from 0 to 0,1,2,3,4,5: this confirms that the generated paraphrases introduce meaningful variability while preserving enough semantic fidelity to challenge the models. If the lines were overlapping, or if the addition of paraphrases did not affect the number of unique responses, it would suggest either low-quality paraphrases (e.g., overly similar or trivially equivalent) or overly rigid models that default to the same answer regardless of wording. Conversely, an inverse or erratic ordering might indicate that paraphrases are not semantically aligned with the original question, thus changing the task rather than preserving it. This interpretation is further supported by the fact that repeating the experiment with paraphrases in reverse order (i.e., 5,4,3,2,1,0) produces nearly identical plot (last plot of Figure 5), with lines that closely overlap or slightly deviate from the original ones, reinforcing the semantic consistency of the paraphrases. These results suggest that the paraphrases are effective in injecting linguistic diversity, and that current benchmark practices, typically relying on single question formulations, fail to capture the semantic fragility, or brittleness as discussed by Lewis and Mitchell [21], of Figure 6: Accuracy on original questions (red crosses), sampled paraphrase sets (boxplots), 1st (orange circle) and 5th (blue circle) generated paraphrases. Models are sorted by original accuracy (Table 2; only models 129 are shown). bilities."
        },
        {
            "title": "5 Conclusions and Future Work",
            "content": "Our study reveals insights into the robustness of LLMs to linguistic variability and the reliability of benchmark-based evaluations. We show that while model rankings tend to remain relatively stable when benchmark questions are paraphrased, absolute accuracy scores drop significantly. This discrepancy suggests that current evaluations, based on static question wordings, may overestimate models true generalization capabilities by failing to account for natural linguistic variation. Our findings highlight the need for evaluation methodologies that better reflect real-world usage. We also show that consistency across paraphrased inputs (i.e., providing the same answer to different formulations of question) is not always reliable indicator of correctness. Less capable models can exhibit high consistency while consistently providing wrong answers, whereas stronger models tend to be both consistent and accurate. This suggests the necessity of interpreting consistency metrics carefully and discourages relying solely on average benchmark scores as proxy for reasoning ability. We also uncover some evidence of potential data contamination, particularly in older benchmarks, where models show lower agreement between original and paraphrased performances. This raises concerns about memorization rather than genuine understanding, emphasizing the importance of evaluating models on fresh, unseen, and linguistically diverse inputs. Our work has limitations, that call for future developments. We addressed exclusively multiple-choice benchmarks and left out openended tasks, which may exhibit different patterns. While paraphrases were automatically generated and validated to ensure semantic fidelity, they may not perfectly capture the full spectrum of real-world linguistic variability. Further validation of the generated paraphrases is needed. We plan to do so with manual checks and by using alternative generation processes. Indeed, we plan to explore paraphrasing strategies that better mimic natural human linguistic diversity, including user-generated ones. We also plan to extend our analysis to more complex reasoning settings and to release paraphraseaugmented benchmark suites. Ultimately, we advocate for shift from static, rigid benchmarks towards dynamic, linguistically diverse evaluation frameworks that better capture the complexities of realworld language. Figure 7: Correlation between the number of models in the Over, In, and Under categories across benchmark release dates  (Table 1)  . that despite accuracy fluctuations, the relative ordering of models remains largely preserved. This is quantitative confirmation of the trends that can be intuitively seen in figure. Finally, the plot in Figure 7 explores the relationship between benchmark release date and the number of models falling into each of the three accuracy-related categories (Over, In, Under). We observe not-significant negative correlation for the Over category (ρ = 0.33): for older benchmarks there is the tendency to have more models whose accuracy on the original questions is higher than on paraphrased ones. Conversely, the In category shows weak positive correlation (ρ = 0.28), indicating that newer benchmarks tend to yield more stable results across paraphrases. Taken together, these trends suggest the conjecture that models may overfit to older benchmarks, potentially due to data contamination from pretraining, thus achieving inflated accuracy on original formulations. It seems that paraphrasing disrupts this memorization/leak effect, exposing weaker generalization. On the other hand, more recent benchmarks appear less affected, either because they are less likely to be part of training data or because they are designed to better resist shallow memorization. The Under category shows positive trend (ρ = 0.48), opposite to that for Over, consistently with the above conjecture (although the absolute numbers are very low). In summary, these findings reveal that while model rankings are largely preserved, paraphrasing exposes serious limitations in model generalization, and benchmarks may overestimate true model capaAcknowledgments This research is partially supported by the PRIN 2022 Project MoTThe Measure of Truth: An EvaluationCentered Machine-Human Hybrid Framework for Assessing Information Truthfulness Code No. 20227F2ZN3, CUP No. G53D23002800006 Funded by the European Union Next Generation EU PNRR M4 C2 I1.1. and by the ESF+ 2021/2027 Regional Program of the Autonomous Region Friuli Venezia Giulia (PPO 2023, Program No. 22/23 LINE A: PhD programmes)."
        },
        {
            "title": "References",
            "content": "[1] J. Austin et al. Program synthesis with large language models. arXiv:2108.07732, 2021. [2] G. Bai et al. Mt-bench-101: fine-grained benchmark for evaluating large language models in multi-turn dialogues. arXiv:2402.14762, 2024. [3] P. Bailey, N. Craswell, I. Soboroff, P. Thomas, A. P. de Vries, and E. Yilmaz. Relevance assessment: are judges exchangeable and does it matter. In Proc. of SIGIR, page 667674, 2008. [4] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi. Piqa: Reasoning about physical commonsense in natural language. In AAAI, 2020. [5] R. Burnell et al. Rethink reporting of evaluation results in AI. Science, 380(6641):136138, 2023. [6] B. Carterette, J. Allan, and R. Sitaraman. Minimal test collections for retrieval evaluation. In Proc. of SIGIR, page 268275, 2006. [7] B. Carterette, A. Bah, and M. Zengin. Dynamic test collections for retrieval evaluation. In Proc. of the 2015 International Conference on The Theory of Information Retrieval, page 91100, 2015. [8] M. Chen et al. Evaluating large language models trained on code. arXiv:2107.03374, 2021. [9] E. Choi, H. He, M. Iyyer, M. Yatskar, W.-t. Yih, Y. Choi, P. Liang, and L. Zettlemoyer. QuAC: Question answering in context. In Proc. of the 2018 EMNLP, pages 21742184, 2018. [10] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In NAACL, 2019. [11] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. arXiv:1803.05457, 2018. [12] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems. arXiv:2110.14168, 2021. [13] J. Frohberg and F. Binder. CRASS: Novel Data Set and Benchmark to Test Counterfactual Reasoning of Large Language Models. In Proc. of 13th LREC, pages 21262140, 2022. [14] J. Guiver, S. Mizzaro, and S. Robertson. few good topics: Experiments in topic set reduction for retrieval evaluation. TOIS, 27(4), 2009. Information retrieval evaluation. Morgan & Claypool, [15] D. Harman. 2011. [16] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring Massive Multitask Language Understanding. ICLR, 2021. [17] D. Hendrycks et al. Measuring mathematical problem solving with the math dataset. arXiv:2103.03874, 2021. [18] S. Kim et al. Prometheus: Inducing fine-grained evaluation capability in language models. In ICLR, 2023. [19] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large Language Models are Zero-Shot Reasoners. In NeurIPS, volume 35, pages 2219922213, 2022. [20] G. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy. RACE: Large-scale ReAding Comprehension Dataset From Examinations. In Proc. of the 2017 EMNLP, pages 785794, 2017. [21] M. Lewis and M. Mitchell. Evaluating the robustness of analogical reasoning in large language models. arXiv:2411.14215, 2024. [22] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. B. Clement, D. Drain, D. Jiang, D. Tang, G. Li, L. Zhou, L. Shou, L. Zhou, M. Tufano, M. Gong, M. Zhou, N. Duan, N. Sundaresan, S. K. Deng, S. Fu, and S. Liu. Codexglue: machine learning benchmark dataset for code understanding and generation. CoRR, abs/2102.04664, 2021. [23] R. Lunardi, D. La Barbera, and K. Roitero. The Elusiveness of Detecting Political Bias in Language Models. In Proc. 33rd ACM Int. Conf. on Information and Knowledge Management, page 39223926, 2024. [24] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal. Can Suit of Armor Conduct Electricity? New Dataset for Open Book Question Answering. In Proc. of the 2018 EMNLP, pages 23812391, 2018. [25] M. Mitchell. How do we know how smart AI systems are? Science, 381 (6654):eadj5957, 2023. [26] M. Mitchell. Abstraction and analogy in AI. Annals of the New York Academy of Sciences, 1524(1):1721, 2023. [27] T. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary, R. Majumder, and L. Deng. MS MARCO: human generated machine reading comprehension dataset. CoRR, abs/1611.09268, 2016. [28] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with human feedback. In Proc. of NIPS, 2024. [29] A. Parry, M. Fröbe, H. Scells, F. Schlatt, G. Faggioli, S. Zerhoudi, S. MacAvaney, and E. Yang. Variations in relevance judgments and the shelf life of test collections. arXiv:2502.20937, 2025. [30] D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. GPQA: Graduate-Level Google-Proof Q&A Benchmark. In First Conf. on Language Modeling, 2024. [31] A. Reuel-Lamparth, A. Hardy, C. Smith, M. Lamparth, M. Hardy, and M. J. Kochenderfer. BetterBench: Assessing AI Benchmarks, Uncovering Issues, and Establishing Best Practices. Advances in Neural Information Processing Systems, 37:2176321813, 2024. [32] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. WinoGrande: An Adversarial Winograd Schema Challenge at Scale. Commun. ACM, 64(9):99106, 2021. [33] E. S. Salido, J. Gonzalo, and G. Marco. None of the Others: General Technique to Distinguish Reasoning from Memorization in MultipleChoice LLM Evaluation Benchmarks. arXiv:2502.12896, 2025. [34] M. Sanderson. Test collection based evaluation of information retrieval systems. Foundations and Trends in IR, 4(4):247375, 2010. [35] M. Sclar, Y. Choi, Y. Tsvetkov, and A. Suhr. Quantifying Language Models Sensitivity to Spurious Features in Prompt Design or: How In The Twelfth learned to start worrying about prompt formatting. International Conference on Learning Representations, 2024. [36] K. Sparck Jones and C. J. Van Rijsbergen. Information retrieval test collections. Journal of documentation, 32(1):5975, 1976. [37] C. E. Stevenson, A. Pafford, H. L. van der Maas, and M. Mitchell. Can large language models generalize analogy solving like people can? arXiv:2411.02348, 2024. [38] E. M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. In Proc. of SIGIR, pages 315323, 1998. In Proc. [39] E. M. Voorhees. Evaluation by highly relevant documents. 24th SIGIR, page 7482, 2001. [40] E. M. Voorhees. The philosophy of information retrieval evaluation. In Workshop of the cross-language evaluation forum for european languages, pages 355370, 2001. [41] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le. Finetuned language models are zero-shot learners. arXiv:2109.01652, 2021. [42] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Proc. of the 36th NeurIPS, 2022. [43] J. Wei et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022. [44] J. Welbl, N. F. Liu, and M. Gardner. Crowdsourcing Multiple Choice In Proc. of the 3rd Workshop on Noisy UserScience Questions. generated Text, pages 94106, 2017. [45] W.-w. Yim, Y. Fu, A. Ben Abacha, N. Snider, T. Lin, and M. Yetisgen. Aci-bench: novel ambient clinical intelligence dataset for benchmarking automatic visit note generation. Scientific data, 10(1):586, 2023. [46] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can machine really finish your sentence? In Proc. of the 57th ACL, pages 47914800, 2019. [47] Y. Zhao, L. Yan, W. Sun, G. Xing, S. Wang, C. Meng, Z. Cheng, Z. Ren, and D. Yin. Improving the Robustness of Large Language Models via Consistency Alignment. In LREC-COLING, pages 89318941, 2024. [48] L. Zheng et al. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. Advances in NeurIPS, 36:4659546623, 2023. [49] M. Zhong et al. QMSum: new benchmark for query-based multidomain meeting summarization. arXiv:2104.05938, 2021. [50] L. Zhu, X. Wang, and X. Wang. JudgeLM: Fine-tuned Large Language Models are Scalable Judges. arXiv:2310.17631, 2023. [51] T. Y. Zhuo et al. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv:2406.15877, 2024."
        }
    ],
    "affiliations": [
        "University of Udine, Italy"
    ]
}