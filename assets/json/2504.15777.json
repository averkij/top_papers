{
    "paper_title": "Tina: Tiny Reasoning Models via LoRA",
    "authors": [
        "Shangshang Wang",
        "Julian Asilis",
        "Ömer Faruk Akgül",
        "Enes Burak Bilgin",
        "Ollie Liu",
        "Willie Neiswanger"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "How cost-effectively can strong reasoning abilities be achieved in language models? Driven by this fundamental question, we present Tina, a family of tiny reasoning models achieved with high cost-efficiency. Notably, Tina demonstrates that substantial reasoning performance can be developed using only minimal resources, by applying parameter-efficient updates during reinforcement learning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B parameter base model. This minimalist approach produces models that achieve reasoning performance which is competitive with, and sometimes surpasses, SOTA RL reasoning models built upon the same base model. Crucially, this is achieved at a tiny fraction of the computational post-training cost employed by existing SOTA models. In fact, the best Tina model achieves a >20\\% reasoning performance increase and 43.33\\% Pass@1 accuracy on AIME24, at only \\$9 USD post-training and evaluation cost (i.e., an estimated 260x cost reduction). Our work reveals the surprising effectiveness of efficient RL reasoning via LoRA. We validate this across multiple open-source reasoning datasets and various ablation settings starting with a single, fixed set of hyperparameters. Furthermore, we hypothesize that this effectiveness and efficiency stem from LoRA rapidly adapting the model to the structural format of reasoning rewarded by RL, while largely preserving the base model's underlying knowledge. In service of accessibility and open research, we fully open-source all code, training logs, and model weights \\& checkpoints."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 7 7 7 5 1 . 4 0 5 2 : r Tina: Tiny Reasoning Models via LoRA Shangshang Wang1, Julian Asilis1, Ömer Faruk Akgül1, Enes Burak Bilgin1, Ollie Liu1, and Willie Neiswanger 1University of Southern California How cost-effectively can strong reasoning abilities be achieved in language models? Driven by this fundamental question, we present Tina, family of tiny reasoning models achieved with high cost-efficiency. Notably, Tina demonstrates that substantial reasoning performance can be developed using only minimal resources, by applying parameter-efficient updates during reinforcement learning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B parameter base model. This minimalist approach produces models that achieve reasoning performance which is competitive with, and sometimes surpasses, SOTA RL reasoning models built upon the same base model. Crucially, this is achieved at tiny fraction of the computational post-training cost employed by existing SOTA models. In fact, the best Tina model achieves >20% reasoning performance increase and 43.33% Pass@1 accuracy on AIME24, at only $9 USD post-training and evaluation cost (i.e., an estimated 260x cost reduction). Our work reveals the surprising effectiveness of efficient RL reasoning via LoRA. We validate this across multiple open-source reasoning datasets and various ablation settings starting with single, fixed set of hyperparameters. Furthermore, we hypothesize that this effectiveness and efficiency stem from LoRA rapidly adapting the model to the structural format of reasoning rewarded by RL, while largely preserving the base models underlying knowledge. In service of accessibility and open research, we fully open-source all code, training logs, and model weights & checkpoints. Notion Blog: https://shangshangwang.notion.site/tina Code Repository: https://github.com/shangshang-wang/Tina Training Logs: https://wandb.ai/upup-ashton-wang-usc/Tina Model Weights & Checkpoints: https://huggingface.co/Tina-Yi 1. Introduction Language models (LMs) demonstrate increasing proficiency across variety of tasks, but achieving robust, multi-step reasoning remains frontier challenge (Wang and Neiswanger, 2025, Xu et al., 2025). Notably, such reasoning abilities are crucial for applications demanding complex problem-solving, from scientific discovery to intricate planning. Enhancing complex reasoning via supervised fine-tuning (SFT) is welladopted technique, often utilizing distillation process (Min et al., 2024, Huang et al., 2024) by which the model learns to mimic reasoning traces (e.g., step-by-step thinking) generated by more advanced models such as o1 (OpenAI, 2024). This approach, while effective, relies upon the quality and availability of such expert demonstrations, which can be costly to obtain. Furthermore, it can run the risk of instilling shallow form of imitation in the learning model, rather than fostering dynamic exploration of reasoning paths. In contrast, reinforcement learning (RL) enables models to learn directly and flexibly from verifiable reward signals derived from curated data (DeepSeek-AI, 2025, Lambert et al., 2025). In doing so, RL can lead the model to explore greater variety of logical paths and possibly discover more robust solutions. However, RL pipelines are often complex and notoriously resource-intensive, typically involving substantial compute. This raises fundamental question anchoring our research: How cost-effectively can one perform RL to efficiently instill reasoning abilities in LMs? Corresponding author(s): Shangshang Wang shangshangwang.github.io; Willie Neiswanger neiswang@usc.edu Tina: Tiny Reasoning Models via LoRA Figure 1: Overall comparison between Tina and baseline models. The Tina model in the figure corresponds to the best checkpoint in Table 10. Reasoning performance denotes the average score across AIME24/25, AMC23, MATH500, GPQA, and Minerva, as described in Section 3. The calculation of each comparative metric is detailed in Appendix A. Our pursuit of this question necessitates deliberate move towards minimalism. Rather than utilizing models with tens of billions of parameters (such as Qwen-7B/32B, QwQ-32B-preview, and their variants (Min et al., 2024, NovaSky Team, 2025, Zeng et al., 2025, Muennighoff et al., 2025, Cui et al., 2025, Lyu et al., 2025, OpenThoughts Team, 2025, Hu et al., 2025)), we instead direct our attention to tiny models. In particular, we use the 1.5B parameter model, DeepSeek-R1-Distill-Qwen-1.5B (DeepSeek-AI, 2025). Our choice of this base model aligns with common practices in recent research (RUCAIBox STILL Team, 2025, Luo et al., 2025, Dang and Ngo, 2025): we begin with foundation that, owing to its specific lineage (DeepSeek/Qwen) and distillation process, likely possesses stronger initial reasoning aptitude compared to generic pre-trained model of equivalent size. This strategic starting point allows us to more-rigorously evaluate the incremental reasoning enhancements imparted by RL, thereby isolating and measuring the effectiveness of the technique itself over competent baseline. More importantly, selecting such an architecture dramatically lowers the computational and financial threshold for experimentation. Complementing the choice of compact base model, we further amplify efficiency during the RL phase and integrate parameter-efficient post-training by employing low-rank adaptation (LoRA) (Hu et al., 2021). Notably, LoRA enables the modification of models behavior by training only an exceptionally small number of new parameters. This dovetails with our central motivation: achieving reasoning capabilities through the most economical means possible. Integrating the previous two componentsa tiny model architecture and tiny post-training via LoRAbased RLwe release the Tina (Tiny Reasoning Models via LoRA) family of models, which attain substantial reasoning performance at strikingly low cost. In total, we summarize our contributions as follows: Surprising Effectiveness of Efficient RL Reasoning. We show that our Tina models achieve performance competitive with, and in some cases even superior to, SOTA baseline models built on the same base model with full-parameter training, as shown in Figure 1 and in more detail in Table 3. In particular, the best Tina model achieves >20% performance increase and 43.33% Pass@1 accuracy on AIME24. Rapid Reasoning Format Adaptation Hypothesis. Based on our observations in post-training Tina, we hypothesize that LoRAs effectiveness and efficiency stem from rapidly adapting the reasoning format under RL while preserving base model knowledgea likely more compute-efficient process than the deep knowledge integration of full-parameter training. Partial support comes from studies showing tiny LMs can reason effectively (Hugging Face, 2025, DeepSeek-AI, 2025), while large LMs can store broader world knowledge (Allen-Zhu and Li, 2025). This distinction suggests reasoning capabilities can be significantly enhanced by focusing on adapting the output format itself, consistent with our hypothesis about LoRA. To test this, we exclusively train LoRA parameters in RL settings, focusing on leveraging this format adaptation mechanism. 2 Tina: Tiny Reasoning Models via LoRA Figure 2: Release timeline of open-source models that aim to replicate the performance of advanced reasoning models like o1(-preview) (OpenAI, 2024) and R1 (DeepSeek-AI, 2025), which we refer to as open-source reasoning replicas. Democratizing RL Reasoning. We provide reproducible and highly cost-effective approach, enabling wider participation in the exploration of RL techniques without requiring extensive computational resources. Notably, the cost of reproducing the best Tina checkpoint stands at only $9, and of reproducing all our experiments and everything presented in this paper from scratch at $526. Furthermore, in line with our goal of promoting accessible research, we release all code, training logs, evaluation scripts, and all Tina checkpoints. 2. Related Work 2.1. Open-Source Reasoning Replicas As shown in Figure 2, following the release of o1-preview (OpenAI, 2024), number of open-source models have emerged to replicate or exceed its reasoning capabilities. STILL (Min et al., 2024) introduced minimal yet high-quality training recipe designed to elicit reasoning with modest compute, demonstrating that imitation learning from curated traces remains competitive. Sky-T1 (NovaSky Team, 2025) further explored scaling using open instruction-tuned checkpoints, while SimpleRL (Zeng et al., 2025) highlighted the potential of lightweight RL without requiring large-scale reward models. PRIME (Cui et al., 2025) and DeepScaleR (Luo et al., 2025) introduced process supervision and scaling experiments to isolate how reasoning quality evolves with model size and context length. s1 (Muennighoff et al., 2025) showed that even strong base models such as Qwen2.5-32B-Instruct benefit from fine-tuning on only 1k high-quality and long chain-of-thought data, which is curated to elicit reasoning capabilities. L1 (Aggarwal and Welleck, 2025) combined prompt engineering with data curation for RL, resulting in models that can efficiently and adaptively control their response length. Meanwhile, OREAL (Lyu et al., 2025) and OpenThinker (OpenThoughts Team, 2025) investigated self-correction and latent structure emergence through unsupervised and hybrid paradigms. The release of Open Reasoner Zero (Hu et al., 2025) and Open-RS (Dang and Ngo, 2025) further emphasized efficient RL-based strategies for reasoning with small models, completing landscape of public alternatives for interpretability and reproducibility. 3 Tina: Tiny Reasoning Models via LoRA 2.2. RL with Verifiable Rewards Reasoning tasks are well-suited to RL paradigms, as the correctness or quality of the final output often provides verifiable reward signals (e.g., the validity of logical deduction). Such signal can effectively guide the model towards learning more robust reasoning strategies. Consequently, various RL approaches have been explored within this domain. Certain methods introduce auxiliary reward models or critics to assess reasoning quality, such as ReFT (Luong et al., 2024) and REFINER (Paul et al., 2024). Other techniques employ explicit rule-based verification for self-correction (Wu et al., 2024). Some leverage self-play dynamics and exploration, such as mutual reasoning (Qi et al., 2024), or utilize inference-aware fine-tuning that optimizes performance under different sampling strategies (Chow et al., 2024). Notably, Group Relative Policy Optimization (GRPO) has been proposed as variant of Proximal Policy Optimization (PPO) which removes the need for separate value network by using group-based baseline for advantage estimation, improving training efficiency and leading to better reward alignment (Shao et al., 2024), as demonstrated by DeepSeek-R1 (DeepSeek-AI, 2025). Subsequently, Dr.GRPO (Liu et al., 2025) introduced subtle modification of GRPO addressing its bias to produce long responses. For completeness, we provide the standard formulation of GRPO in Appendix B. 2.3. Low-Rank Adaptation While most existing open models that enable reasoning rely on the more expensive full-parameter training (Min et al., 2024, NovaSky Team, 2025, Zeng et al., 2025, Muennighoff et al., 2025, Aggarwal and Welleck, 2025, Cui et al., 2025, Luo et al., 2025, Lyu et al., 2025, OpenThoughts Team, 2025, Hu et al., 2025, Dang and Ngo, 2025), we investigate the use of LoRA for parameter-efficient post-training of reasoning models (Hu et al., 2021). Our goal is to assess whether updating only small fraction of parameters can still yield strong reasoning capabilities (Han et al., 2024). In addition to its computational efficiency, LoRA provides modularity: by training only low-rank decomposition of the parameter updates, it becomes possible to toggle reasoning behavior without maintaining multiple full model copies. For completeness, we provide the standard formulation of LoRA in Appendix B. 3. Tina: Tiny Reasoning Models via LoRA Tina is our family of models created by post-training the DeepSeek-R1-Distill-Qwen-1.5B base model using LoRA during RL (employing GRPO-style algorithm). The Tiny designation encapsulates deliberate focus on minimalism and efficiency across the entire framework. This encompasses not only the tiny base model architecture and the tiny parameter updates enabled by LoRA, but also extends to tiny overall resource footprint. This minimized footprint is achieved through an efficient training pipeline leveraging accessible open-source datasets and codebase (detailed in Section 3.1), and requires only minimal hardware and budget resources (described in Section 3.2). 3.1. Training Pipeline: Baselines & Datasets To facilitate meaningful comparisons and enable precise ablations, we post-train our Tina models via RL using the datasets and setups from publicly available reasoning models. All Tina and baseline models adopt DeepSeek-R1-Distill-Qwen-1.5B as their base model checkpoint with default open-source weights. STILL-3-1.5B-preview (RUCAIBox STILL Team, 2025) is slow-thinking reasoning model developed through iterative RL on curated dataset of 33k reasoning traces. The data originates from mathematics competitions and includes problems from MATH (Hendrycks et al., 2021, Lightman et al., 4 Tina: Tiny Reasoning Models via LoRA 2023), NuminaMathCoT (LI et al., 2024), and AIME (19832023) (Art of Problem Solving, 2024). Tina-STILL-3-1.5B-preview uses the same dataset and reward pipeline. DeepScaleR-1.5B-Preview (Luo et al., 2025) focuses on long-context mathematical reasoning via RL, and is trained over approximately 40k problem-answer pairs drawn from the AIME (Art of Problem Solving, 2024), AMC (Art of Problem Solving, 2023), OMNI-MATH (Gao et al., 2024a), and STILL (RUCAIBox STILL Team, 2025) datasets. Tina-DeepScaleR-1.5B-Preview uses this dataset and mirrors the reward design. Open-RS1/2/3 (Dang and Ngo, 2025) are three models from the Open-RS project exploring reasoning performance in 1.5B models trained via RL. All Open-RS models are trained on small, high-quality datasets further curated from the s1 (Muennighoff et al., 2025) (i.e., Open-S1) and DeepScaleR (Luo et al., 2025) (i.e., Open-DeepScaleR) datasets. The Tina models (Tina-Open-RS1/2/3) replicate these setups, using identical data splits and reward scaffolding. 3.2. Training Setup: Infrastructure & Budget Training Codebase. Our implementation builds upon OpenR1,1 fully open reproduction of DeepSeekR1 (DeepSeek-AI, 2025) which combines the Accelerate (Gugger et al., 2022) and Trl (von Werra et al., 2020) libraries and the DeepSpeed ZeRO optimization (Rajbhandari et al., 2019). It aims to transparently replicate and extend RL methods used for improving reasoning in language models, particularly focusing on aligning model behavior with reasoning-oriented objectives via verifiable reward signals. Our methodology inherits its scaffolding, training utilities, and reward interfaces. Training Hyperparameters. We initiated parameter selection by replicating key parameters from OpenR1 (Hugging Face, 2025) and OpenRS (Dang and Ngo, 2025). For all experiments presented in this paper, we deliberately adopted the default or recommended hyperparameter configurations provided in their works. These settings were kept largely fixed across different runs  (Table 5)  . For the main Tina results (Section 4.2), only reward function parameters were adjusted per task, and for ablation studies (Section 4.3), only the specific factor under investigation (e.g., learning rate, LoRA rank/alpha, RL algorithm) was varied  (Table 6)  . This approach intentionally circumvents costly hyperparameter search procedures for our specific setup, ensuring negligible tuning overhead and focusing on the efficacy of the core LoRA-based RL methodology. Training Hardware. key element of our low-cost approach was minimizing the hardware footprint. While distributed RL training algorithms like GRPO often benefit from using three or more GPUs (e.g., dedicating one GPU to an inference engine such as vLLM for faster sample generation), we deliberately targeted minimal setup using only two NVIDIA L40S GPUs.2 To enable this, we co-located the RL training process and the vLLM on the same two GPUs by constraining vLLMs GPU memory usage. The training itself utilized data parallelism across both GPUs. While running inference and training concurrently on two GPUs might result in longer wall-clock training time compared to setup with dedicated inference GPUs, it significantly reduces the hardware requirement. Training Budget. The NVIDIA L40S GPUs we use are accessible via commercial cloud platforms at an approximate rate of $1 USD per GPU hour, including 300 GB storage, based on pricing observed at the time of writing (Cudo Compute). The RL training process for our LoRA models proved highly efficient, with 1 https://github.com/huggingface/open-r1 2Occasionally, NVIDIA RTX 6000 Ada GPUs were used instead, which is reflected in the system configuration metadata on Weights & Biases. From our practical experience, these two GPU types are similar in terms of cost and computational performance. For consistency, we report costs and compute metrics based on the L40S. 5 Tina: Tiny Reasoning Models via LoRA"
        },
        {
            "title": "Experimental Task",
            "content": "Tr aining Cost Est. Evaluation Cost Est. Total Cost Est. Baseline: Model Re-Evaluation Main: Tina-STILL-3-1.5B-preview Main: Tina-DeepScaleR-1.5B-Preview Main: Tina-Open-RS Main: Tina-Open-RS2 Main: Tina-Open-RS3 Ablation: OpenThoughts Dataset Ablation: OpenR1 Dataset Ablation: LIMR Dataset Ablation: DrGRPO Algorithm Ablation: Learning Rate Ablation: LoRA Rank/Alpha Total: All Tasks Total: Main Tasks Total: Best Ckpt. in Each Main Task Total: All Ckpt. in Best-Performance Task Total: Best Ckpt. in Best-Performance Task - $59 $84 $40 $ $15 $84 $59 $4 $15 $ $14 $396 $213 $80 $14 $ $6 $7 $10 $11 $17 $ $10 $7 $4 $17 $8 $ $130 $62 $5 $17 $1 $ $66 $94 $51 $32 $32 $ $66 $8 $32 $15 $30 $ $275 $85 $31 $9 Table 1: Computational cost breakdown. Costs for all experimental tasks in this paper, measured in USD. The row Best Ckpt. in Each Main Task denotes the cost of reproducing the best checkpoint in each of Table 7, 8, 9, 10, 11. The row All Ckpt. in Best-Performance Task denotes the cost of reproducing all checkpoints in Table 10. Best Ckpt. in Best-Performance Task denotes the cost of reproducing the best checkpoint in Table 10, i.e., the checkpoint at step 450. single RL step typically completing within one minute on this hardware. Evaluating model checkpoint across our entire suite of six reasoning benchmarks required approximately 1 L40S GPU hours on average. To ensure cost control, we initially established conservative maximum budget of $100 USD for each complete experimental run, encompassing all stages from training to evaluation and miscellaneous tasks. As detailed in Table 1, our actual expenditures were significantly below this ceiling. Our calculation is based on the full Tina model evaluation performance in Appendix D. We believe this low cost makes our setup an accessible testbed for the research community. 4. Surprising Effectiveness of Efficient RL Reasoning via LoRA 4.1. Experiments Stage I: Baseline Model Re-Evaluation Before presenting Tinas performance, it is crucial to establish fair and reliable comparisons against existing SOTA reasoning models. We note that performance scores reported in the literature for relevant models often stem from evaluations using disparate frameworks (e.g., verl (Sheng et al., 2025), lighteval (Fourrier et al., 2023), lm-eval-harness (Gao et al., 2024b)) and inconsistent inference settings (such as different generation hyperparameters or varying numbers of GPUs). These variations can significantly influence reported metrics, creating potential inconsistencies and hindering reliable comparisons between models. To mitigate these confounding factors, we performed comprehensive re-evaluation of key baseline models using single, consistent methodology throughout this paper. All baseline evaluations reported herein utilize the lighteval framework integrated with the vLLM (Kwon et al., 2023) inference engine for efficient 6 Tina: Tiny Reasoning Models via LoRA"
        },
        {
            "title": "Baseline Model",
            "content": "AIME24 AIM 25 AM C23 AT H50 0 GP QA in rva Avg . DeepSeek-R1-Distilled-Qwen-1.5B STILL-3-1.5B-preview DeepScaleR-1.5B-Preview Open-RS1 Open-RS Open-RS3 23.33 26.67 36.67 26.67 26. 43.33 16.67 26.67 26.67 20.00 13. 20.00 62.50 67.50 77.50 72.50 62. 67.50 82.60 86.40 87.80 83.60 85. 83.00 31.82 34.34 31.82 35.35 34. 33.84 30.15 27.57 41.18 44.86 31. 48.74 28.68 26.84 28.68 44.47 41. 46.06 Table 2: Baseline model re-evaluation. Performance evaluation of baseline models on six reasoning tasks. generation. For comparability with prior work such as OpenR1, we maintained fixed hardware configuration (two L40S GPUs) and applied standardized set of vLLM inference parameters across all evaluated baseline models. All scores are zero-shot pass@1 performance. The exact command structure employed for these evaluations is provided in Appendix C.2 for transparency and reproducibility. The results stemming from this consistent re-evaluation protocol are presented in Table 2. Particularly, we evaluate the reasoning capabilities of our Tina models and the baselines across diverse suite of six challenging benchmarks, primarily focused on mathematical and scientific reasoning: AIME24/25 (Art of Problem Solving, 2024) contains 30 high-school-level math problems in algebra, geometry, number theory, and combinatorics from the 2024/2025 American Invitational Mathematics Examination. Each problem demands precise multi-step reasoning. AMC23 (Art of Problem Solving, 2023) includes 40 problems from the 2023 American Mathematics Competition, offering mix of logic and symbolic manipulation tasks. MATH500 (Hendrycks et al., 2021, Lightman et al., 2023) is benchmark comprising 500 competition mathematics problems derived from various sources, covering different difficulty levels and often necessitating multi-step derivation and calculation. GPQA Diamond (Rein et al., 2024), hereafter referred to as GPQA, consists of 198 PhD-level science questions across biology, chemistry, and physics. Each question is multiple-choice with subtle distractors. Minerva (Lewkowycz et al., 2022) includes 272 quantitative reasoning problems generally at the undergraduate level. The questions span multiple STEM fields, including physics, biology, chemistry, and economics, often requiring mathematical modeling or calculation steps. Includes tasks such as calculating enzyme kinetics from reaction data. 4.2. Experiments Stage II: Tina Model Evaluation We now present the core evaluation results for our Tina models. These experiments assess the reasoning capabilities attained by post-training the DeepSeek-R1-Distill-Qwen-1.5B with minimal parameter updates via LoRA-based RL. The results presented in Table 3 demonstrate that significant reasoning performance can be achieved efficiently, yielding models that are competitive with, or outperform, relevant baselines despite the inherent resource constraints of using parameter-efficient tuning. Table 3 summarizes the performance of five distinct Tina models across suite of six reasoning tasks: 3Tables 3 and 4 adopt consistent naming pattern where Tina-X denotes our model is the LoRA counterpart of baseline model or is trained on dataset (possibly followed with an extra ablation setup). This can reflect the model origin and serve as direct reference to the public checkpoints for reproducibility. 7 Tina: Tiny Reasoning Models via LoRA ina Mode Steps ( % of 1 Epoc h) AIME24 AIME25 AMC23 MATH500 GPQA Minerva Avg. Base line Tina-STILL-3-1.5B-preview Tina-DeepScaleR-1.5B-Preview Tina-Open-RS1 Tina-Open-RS Tina-Open-RS3 53% 19% 34% 51% 57% 36.67 43.33 43.33 43.33 36.67 30. 26.67 20.00 26.67 23.33 77.50 67. 80.00 77.50 82.50 84.60 86.20 84. 87.00 85.20 33.33 37.88 35.35 36. 37.37 26.84 28.68 28.68 32.72 31. 48.16 48.38 48.56 50.60 49.45 44. 48.74 44.47 41.60 46.06 Table 3: Tina model evaluation. Performance comparison between Tina models and corresponding full-parametertrained SOTA models on six reasoning tasks. The value in the Steps column indicates the training steps of the best model checkpoint within one epoch, the full model checkpoint evaluation is shown in Appendix D. The Baseline column represents the average score achieved by baseline model with full-parameter RL in Table 2. AIME24/25, AMC23, MATH500, GPQA, and Minerva. For each Tina model, we report the extent of training completed (as percentage of predefined training stpes within 1 epoch) and the percentage scores achieved on each task. The results compellingly demonstrate the efficacy of our economical LoRA-based RL strategy. All Tina models exhibit substantial reasoning aptitude, achieving average scores in the range of 48.16% to 50.60%. Significantly, nearly all Tina models notably outperform their corresponding baseline average scores , indicating marked improvements instilled by the parameter-efficient RL. The Tina-Open-RS2 model yielded the highest average performance observed at 50.60%. Furthermore, these strong results were achieved with remarkably limited training durations, ranging from just 19% to 57% of full training epoch, highlighting the efficiency and rapid adaptation enabled by the Tina approach. These findings strongly support our central hypothesis: robust reasoning capabilities can be effectively and economically cultivated in small language models through the targeted application of LoRA and RL. 4.3. Experiments Stage III: Tina Ablation Variants To better understand the factors influencing the performance and efficiency of our Tina models within the proposed low-cost framework, we conducted series of ablation studies. These studies systematically investigate the impact of key design choices and hyperparameter: the underlying training dataset, the learning rate for LoRA updates, the rank of the LoRA adapters, and the specific RL algorithm employed. In each study, we typically varied one factor while holding others constant, often based on high-performing configuration identified in our main experiments or preliminary runs. The results, summarized in Table 4, provide valuable insights into the robustness and sensitivity of our economical approach. Impact of Training Dataset. The first section of Table 4 highlights the influence of the dataset used for RL. We compared seven distinct datasets, varying significantly in size (from 1.4k to 94k samples). Strikingly, the Tina-Open-RS model, trained on concise dataset of merely 7k examples, achieved the highest average score (50.60%). This outcome surpasses models trained on considerably larger datasets, such as Tina-OpenR1 (93.7k samples, 49.26% avg). This observation strongly supports our core Tiny premise and reflects the intuition that the quality and diversity of the dataset matter more than the data size. Sensitivity to Learning Rate. Using the Tina-LIMR configuration as testbed (second section of Table 4), 7), we assessed sensitivity to the learning rate. Among the tested values (5 10 6 yielded the optimal average performance (48.47%) for this setup. While performance learning rate of 1 10 differences were not drastic, this indicates that learning rate selection remains factor, although effective results were obtained without extensive tuning. 6, and 5 10 6, 1 Effect of LoRA Rank. The third ablation study investigated the impact of LoRA rank, which directly controls the number of trainable parameters. Testing ranks 4, 8, 16, 32, and 64 on the Tina-LIMR setup, we observed 8 Tina: Tiny Reasoning Models via LoRA"
        },
        {
            "title": "A blation on Datasets",
            "content": "Steps (% of 1 Epoch) AIME24 AIME25 AMC23 MATH500 GPQA Minerva Avg. Tina-OpenR1 (93.7k) Tina-OpenThoughts (66.1k) Tina-DeepScaleR (40.3k) Tina-STILL-3 (33k) Tina-Open-S1 (18.6k) Tina-Open-RS (7k) Tina-LIMR (1.39k) 13% 30% 19% 53% 34% 51% 58% 36.67 36. 43.33 36.67 43.33 43.33 46.67 26. 26.67 26.67 30.00 20.00 26.67 20. 75.00 72.50 67.50 77.50 80.00 77. 75.00 86.80 84.80 86.20 84.60 84. 87.00 83.80 39.90 41.41 37.88 33. 35.35 36.36 34.85 30.51 33.09 28. 26.84 28.68 32.72 30.51 49.26 49. 48.38 48.16 48.56 50.60 48."
        },
        {
            "title": "A blation on Learning Rate",
            "content": "Steps (% of 1 Epoch) AIME24 AIME25 AMC23 MATH500 GPQA Minerva Avg. Tina-LIMR-5e-6-lr Tina-LIMR-1e-6-lr Tina-LIMR-5e-7-lr 29% 58% 58% 36.67 46.67 43.33 26. 20.00 16.67 75.00 75.00 77.50 83. 83.80 84.60 35.86 34.85 34.85 29. 30.51 30.51 47.87 48.47 47."
        },
        {
            "title": "A blation on L oRA Rank",
            "content": "Steps (% of 1 Epoch) AIME24 AIME25 AMC23 MATH500 GPQA Minerva Avg. Tina-LIMR-64-LoRA-rank Tina-LIMR-32-LoRA-rank Tina-LIMR-16-LoRA-rank Tina-LIMR-8-LoRA-rank Tina-LIMR-4-LoRA-rank 29% 58% 58% 29% 86% 20.00 46.67 43.33 30.00 36.67 30. 20.00 33.33 26.67 20.00 77.50 75. 70.00 82.50 85.00 84.20 38.38 31. 83.80 83.20 83.80 83.80 34.85 35. 33.84 31.82 30.51 28.31 30.51 29. 46.95 48.47 48.92 47.89 47.72 blation on RL lgori hm Step (% of 1 Epoch) AIME24 AIME25 AMC23 MATH500 GPQA Minerva Avg. Tina-Open-RS3-GRPO Tina-Open-RS3-DrGRPO 57% 17% 36. 43.33 23.33 23.33 82.50 80.00 85. 85.00 37.37 35.35 31.62 30.15 49. 49.53 Table 4: Tina ablation variants evaluation. Performance evaluation of Tinas ablation variants on six reasoning tasks. The value in the Steps column indicates the training steps of the best model checkpoint within one epoch, the full model checkpoint evaluation is shown in Appendix D. For the number in parentheses (the ablation on datasets), it means the data size of dataset. During training, this number should be multiplied by the number of generation in GRPO-like algorithm (in our case, that multiplier is 4). For the model names, Tina-LIMR, Tina-LIMR-1e-6-lr and Tina-LIMR-32-LoRA-rank are the same model, we duplicate them for better visualization. The same idea applies to Tina-DeepScaleR and Tina-DeepScaleR-1.5B-Preview, Tina-STILL-3 and Tina-STILL-3-1.5B-preview, Tina-Open-S1 and Tina-Open-RS1, Tina-Open-RS and Tina-Open-RS2, Tina-Open-RS3-GRPO and Tina-Open-RS3. considerable robustness. Ranks 8, 16, and 32 all produced strong results, with average scores clustering between 47.89% and 48.92%. Notably, rank 16 achieved the peak performance (48.92%) in this comparison, slightly outperforming rank 32 (48.47%). Performance decreased slightly at the extremes (rank 4 and 64). This study validates that highly parameter-efficient configurations (low ranks like 16 or 32) are effective, further enhancing the cost-effectiveness and minimal overhead of the Tina approach. Comparison of RL Algorithms. Finally, we compared two RL algorithms, GRPO and Dr.GRPO (Liu et al., 2025), using the Tina-Open-RS3 setup (final section of Table 4). Both algorithms led to similar peak average performance levels (49.45% for GRPO vs. 49.53% for Dr.GRPO). However, Dr.GRPO reached its best checkpoint significantly earlier in the training process (17% of an epoch vs. 57% for GRPO). This suggests potential advantages in sample efficiency for Dr.GRPO in this context with an alternative normalization in loss calculation, offering potentially faster convergence and further reductions in training time and cost. 9 Tina: Tiny Reasoning Models via LoRA 5. Hypothesis for Effective and Efficient LoRA: Rapid Format Adaptation Less is More LoRA-based RL. To understand why LoRA facilitates both effective and efficient reasoning improvements via RL, we analyze the relationship between training compute and performance, alongside training dynamics. As illustrated in Figure 3, plotting reasoning performance against approximate training FLOPs reveals stark contrast between full-parameter and LoRA-based training regimes. First, our LoRA-based Tina models achieve reasoning scores comparable or superior to fully fine-tuned baselines while requiring (in some cases) orders of magnitude fewer training FLOPs. We observe that in LoRA models, increased training compute inversely affects performance, in contrast to full-parameter models. This observation highlights less compute can yield more performance phenomenon. Figure 3: Less is more LoRA-based RL. Approximate training FLOPs vs reasoning performance comparison between Tina and baseline models. The calculation is detailed in Appendix A. This finding supports our hypothesis regarding how LoRA achieves such remarkable efficiency, which relates to the principle of learn structure/format, maintain knowledge. We posit that LoRA excels in this scenario because RL for reasoning heavily rewards the models ability to generate outputs in specific, verifiable format or structure (e.g., step-by-step reasoning chains). LoRA appears to be highly adept at learning these structural and stylistic patterns with minimal parameter changes, thus requiring very few FLOPs. At the same time, because LoRA modifies only tiny fraction of the weights, it largely preserves the base models vast pre-trained knowledge. Therefore, LoRA efficiently teaches the model how to format its existing knowledge into effective reasoning traces, rather than potentially imposing costly relearning of concepts or procedures that extensive full-parameter updates might entail. We hypothesize that this focus on structural adaptation allows Tina to achieve high reasoning performance with minimal computational investment. Phase Transition in LoRA-based RL. Further insights into the LoRA-based RL mechanism arise from analyzing the training logs. That is, distinct pattern emerges in Figure 4, which displays accuracy rewards, format rewards, and completion lengths over training steps for various Tina model runs. We consistently observe training phase transition or turning point evident in the format-related metrics (format reward, row 2; completion length, row 3) across most Tina models. Around this transition point (indicated by the green vertical dashed line), the format reward often peaks or destabilizes, while the completion length frequently reaches minimum before potentially reversing its trend. Notably, this relatively sharp transition observed in format and length metrics does not typically have corresponding distinct turning point in the accuracy reward plots (row 1). The accuracy reward often exhibits more gradual fluctuations or slower drift over the Tina: Tiny Reasoning Models via LoRA training duration, without clear inflection aligned with the format transition. Figure 4: Phase transition in LoRA-based RL. The raw data is from the Weights & Biases training logs and smoothed via exponential moving average (EMA) with factor 0.1. The training turning point in the legend means the step where the format-like metrics (e.g., format reward, completion length) start to destabilize. Refer to Appendix for the full set of plots. Another crucial observation is the timing of optimal performance: the best-performing checkpoint, yielding the highest reasoning accuracy on held-out evaluations, consistently occurs just prior to or around this observed phase transition point in the format metrics (indicated by the red vertical dashed line). This decoupling between the dynamics of accuracy-based and format-based metrics suggests that the LoRA-based RL process rapidly optimizes the models ability to adhere to the structural and stylistic elements rewarded by the format score and length constraints. The subsequent transition point may signify where this structural optimization saturates, becomes unstable, or perhaps begins to compromise generative quality in other ways (e.g., by overly constraining or expanding length). The fact that peak reasoning accuracy is achieved just before this format-driven transition implies that while learning the correct output format is essential and efficiently achieved via LoRA, pushing further on format-centric optimization alone does not necessarily 11 Tina: Tiny Reasoning Models via LoRA yield better reasoning, and may even be detrimental. This reinforces our hypothesis that LoRA efficiently adapts the model by primarily learning the form required for effective reasoning. 6. Conclusion We presented Tina to demonstrate that effective reasoning capabilities can be instilled in language models with efficiency and effectiveness. The principal contribution of Tina lies in democratizing access to RL-driven reasoning model development. By combining LoRA with RL on 1.5B parameter base model, we achieved reasoning performance competitive with significantly larger models, accomplishing this within an estimated computational budget of only $9. This outcome prompts reflection on the factors enabling such minimalist approaches, and on their possible future trajectories. Despite encouraging results, this work is subject to certain limitations: Base Model Scale: Our experiments centered on 1.5B parameter model. While showcasing costperformance efficiency, the absolute reasoning ceiling achievable with this tiny model may naturally be lower for complex, multi-step reasoning problems than what larger models can offer. Reasoning Task Scope: Our evaluation focused primarily on mathematical and formal logic reasoning benchmarks (AIME, AMC, MATH, GPQA, Minerva). The effectiveness and transferability of the learned reasoning skills to other domains, such as coding, warrants further investigation. Hyperparameter Optimization: We intentionally minimized hyperparameter tuning costs by adopting established configurations. While this demonstrates certain form of robustness to our methodology, there may be potential for further performance gains derived from additional tuning, perhaps tailored to the interplay between LoRA, the RL algorithm, and the target reasoning tasks. 7. Acknowledgment We want to express our gratitude to the broader open-source community. This research was made possible by leveraging numerous publicly available resources, including training and evaluation framework, open datasets, accessible pre-trained language models, and the insights shared through technical reports. The computational resources required for the experiments described herein were provided by the Center for Advanced Research Computing (CARC) at the University of Southern California (USC). We are grateful for the support which enabled the training and evaluation of our models. J.A. was supported by the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE-1842487. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. 12 Tina: Tiny Reasoning Models via LoRA"
        },
        {
            "title": "References",
            "content": "Pranjal Aggarwal and Sean Welleck. L1: Controlling how long reasoning model thinks with reinforcement learning, 2025. URL https://arxiv.org/abs/2503.04697. Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.3, knowledge capacity scaling laws. In Proceedings of International Conference on Learning Representations (ICLR), 2025. Art of Problem Solving. Amc problems and solutions, 2023. URL https://artofproblemsolving.com/ wiki/index.php/AMC_12_Problems_and_Solutions. Art of Problem Solving. Aime problems and solutions, February 2024. URL https:// artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions. Yinlam Chow, Guy Tennenholtz, Izzeddin Gur, Vincent Zhuang, Bo Dai, Sridhar Thiagarajan, Craig Boutilier, Rishabh Agarwal, Aviral Kumar, and Aleksandra Faust. Inference-aware fine-tuning for Best-of-N sampling in large language models, 2024. URL https://arxiv.org/abs/2412.15287. Cudo Compute. Nvidia L40S pricing. URL https://www.cudocompute.com/products/gpu-cloud/ nvidia-l40s. Accessed: 2025-04-21. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, Jiarui Yuan, Huayu Chen, Kaiyan Zhang, Xingtai Lv, Shuo Wang, Yuan Yao, Xu Han, Hao Peng, Yu Cheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou, and Ning Ding. Process reinforcement through implicit rewards, 2025. URL https://arxiv.org/abs/2502.01456. Quy-Anh Dang and Chris Ngo. Reinforcement learning for reasoning in small llms: What works and what doesnt, 2025. URL https://arxiv.org/abs/2503.16219. DeepSeek-AI. DeepSeek-R1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Clémentine Fourrier, Nathan Habib, Hynek Kydlíček, Thomas Wolf, and Lewis Tunstall. Lighteval: lightweight framework for llm evaluation, 2023. URL https://github.com/huggingface/ lighteval. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. Omni-MATH: universal olympiad level mathematic benchmark for large language models, 2024a. URL https://arxiv.org/abs/2410. 07985. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 07 2024b. URL https://zenodo.org/records/12608602. Sylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp Schmid, Zachary Mueller, Sourab Mangrulkar, Marc Sun, and Benjamin Bossan. Accelerate: Training and inference at scale made simple, efficient and adaptable., 2022. URL https://github.com/huggingface/accelerate. Tina: Tiny Reasoning Models via LoRA Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, and Sai Qian Zhang. Parameter-efficient fine-tuning for large models: comprehensive survey, 2024. URL https://arxiv.org/abs/2403.14608. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021. URL https: //arxiv.org/abs/2103.03874. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models, 2021. URL https://arxiv.org/abs/ 2106.09685. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, and Heung-Yeung Shum Xiangyu Zhang. Open-ReasonerZero: An open source approach to scaling reinforcement learning on the base model, 2025. URL https: //github.com/Open-Reasoner-Zero/Open-Reasoner-Zero. Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, and Pengfei Liu. O1 replication journey part 2: Surpassing o1-preview through simple distillation, big progress or bitter lesson?, 2024. URL https://arxiv.org/abs/2411.16489. Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. URL https://github. com/huggingface/open-r1. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of Symposium on Operating Systems Principles (SOSP), 2023. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training, 2025. URL https://arxiv.org/abs/2411.15124. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages 38433857, 2022. Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. NuminaMath, 2024. URL https://huggingface.co/AI-MO/NuminaMath-CoT. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In Proceedings of International Conference on Learning Representations (ICLR), 2023. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective, 2025. URL https://arxiv.org/abs/2503. 20783. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. DeepScaleR: Surpassing o1-preview with 1.5b model by scaling rl, 2025. URL https://agentica-project.com/. 14 Tina: Tiny Reasoning Models via LoRA Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. ReFT: Reasoning with reinforced fine-tuning, 2024. URL https://arxiv.org/abs/2401.08967. Chengqi Lyu, Songyang Gao, Yuzhe Gu, Wenwei Zhang, Jianfei Gao, Kuikun Liu, Ziyi Wang, Shuaibin Li, Qian Zhao, Haian Huang, Weihan Cao, Jiangning Liu, Hongwei Liu, Junnan Liu, Songyang Zhang, Dahua Lin, and Kai Chen. Exploring the limit of outcome reward for learning mathematical reasoning, 2025. URL https://arxiv.org/abs/2502.06781. Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan. PEFT: State-of-the-art parameter-efficient fine-tuning methods, 2022. URL https://github.com/ huggingface/peft. Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, and Ji-Rong Wen. Imitate, explore, and self-improve: reproduction report on slow-thinking reasoning systems, 2024. URL https: //arxiv.org/abs/2412.09413. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. URL https://arxiv.org/abs/2501.19393. NovaSky Team. Sky-T1: Train your own o1 preview model within $450, 2025. URL https://novasky-ai. github.io/posts/sky-t1. OpenAI. OpenAI o1 system card, 2024. URL https://arxiv.org/abs/2412.16720. OpenThoughts Team. Open Thoughts, January 2025. URL https://open-thoughts.ai. Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. REFINER: Reasoning feedback on intermediate representations. In Proceedings of European Chapter of the ACL (EACL), pages 11001126, 2024. Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li Lyna Zhang, Fan Yang, and Mao Yang. Mutual reasoning makes smaller LLMs stronger problem-solvers, 2024. URL https://arxiv.org/abs/2408.06195. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimization towards training trillion parameter models. CoRR, abs/1910.02054, 2019. URL http://arxiv.org/abs/ 1910.02054. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. GPQA: graduate-level google-proof Q&A benchmark. In Proceedings of Conference on Language Modeling (COLM), 2024. RUCAIBox STILL Team. STILL-3-1.5B-preview: Enhancing slow thinking abilities of small models through reinforcement learning. 2025. URL https://github.com/RUCAIBox/Slow_Thinking_with_LLMs. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. DeepSeekMath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. Tina: Tiny Reasoning Models via LoRA Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of European Conference on Computer Systems (EuroSys), EuroSys 25, page 12791297. ACM, March 2025. doi: 10.1145/3689031.3696075. URL http://dx.doi.org/10.1145/3689031.3696075. Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec. Trl: Transformer reinforcement learning, 2020. URL https://github.com/huggingface/trl. Shangshang Wang and Willie Neiswanger. LLM reasoning: Curated insights, 2025. URL https: //shangshangwang.notion.site/llm-reasoning. Zhenyu Wu, Qingkai Zeng, Zhihan Zhang, Zhaoxuan Tan, Chao Shen, and Meng Jiang. Large language models can self-correct with key condition verification. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1284612867, 2024. Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, Chenyang Shao, Yuwei Yan, Qinglong Yang, Yiwen Song, Sijian Ren, Xinyuan Hu, Yu Li, Jie Feng, Chen Gao, and Yong Li. Towards large reasoning models: survey of reinforced reasoning with large language models, 2025. URL https://arxiv.org/abs/2501.09686. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. SimpleRL-Zoo: Investigating and taming zero reinforcement learning for open base models in the wild, 2025. URL https://arxiv.org/abs/2503.18892. 16 Tina: Tiny Reasoning Models via LoRA"
        },
        {
            "title": "Appendix",
            "content": "A. Cost Breakdown This section provides further details on how training data amounts, computational cost, time cost, and performance metrics reported in this paper particularly those presented in figures like Figures 1 and 3 were determined and should be interpreted. Overall Comparison (Figure 1). For the baseline models included in Figure 1, the approximate training data amounts, computational costs (typically reported as GPU hours or total FLOPs), and training times are sourced from their respective technical reports or publications, leveraging the helpful summary provided in the Open-RS paper (Dang and Ngo, 2025). Reasoning performance scores for all models, encompassing both baselines and our Tina models, stem from results presented in Tables 2 and 3. Also, it is crucial to understand the scope of reported costs: Epoch vs. Best Checkpoint: Costs cited for Tina and baseline models reflect the resources needed to complete full training epoch or predefined training run, not necessarily the minimal cost to reach the single best-performing checkpoint within that run. Training vs. Evaluation: Reported costs cover training only, omitting the computational expense required for model evaluation across benchmarks since such information is missing from several baseline models. Particularly, the $9 USD in the abstract represents the estimated cost to train the Tina model up to its bestperforming checkpoint and subsequently evaluate that specific checkpoint. For context comparing potential full training runs, the cost to train Tina model for complete epoch is $14 USD (training only). Including evaluation costs for such full run would increase the total to approximately $31 USD. We emphasize the $9 as representing the efficient path to the best Tina model. FLOPs Estimation (Figure 3). The approximate training FLOPs shown in Figure 3 serve as hardwareagnostic measure of computational work. For both Tina and baseline models, these values were estimated based on reported training durations and hardware configurations sourced from technical reports or the Open-RS summary, using standard FLOPs calculation methodologies. 17 Tina: Tiny Reasoning Models via LoRA B. Background behind Tina Training B.1. GRPO Formulation Recall the following formulation of GRPO: For each question q, GRPO samples group = {o1, o2, . . . , oG} of outputs from the old policy πθold by maximizing the following objective: and optimizes the policy πθ qP(Q), πθold (Oq) i=1 {oi} ["
        },
        {
            "title": "1\nG",
            "content": "G i=1 (min ( πθ(oiq) πθold(oiq) Ai, clipped ( πθ(oiq) πθold(oiq) , 1 ϵ, 1 + ϵ) Ai) β DKL(πθπref))] ."
        },
        {
            "title": "Here Ai",
            "content": "denotes the advantage computed from group of rewards {r1, r2, . . . , rG} and Ai = ri mean({r1, r2, . . . , rG}) std({r1, r2, . . . , rG}) , DKL(πθπref) = πref(oiq) πθ(oiq) log πref(oiq) πθ(oiq) 1. Note that ϵ and β are parameters controlling the clipping range and KL penalty, respectively. B.2. LoRA Formulation We follow the standard LoRA setup (Hu et al., 2021). Given frozen pretrained weight matrix W0 Rdk and trainable low-rank matrices Rdr and Rrk with min(d, k), the original forward pass h(x) = W0x is modified as We use the default LoRA implementation provided in the PEFT (Mangrulkar et al., 2022) library. ˆh(x) = W0x + ABx. 18 Tina: Tiny Reasoning Models via LoRA C. Additional Experimental Details C.1. Hyperparameters We show our default choice of hyperparameter in Table 5 for all the LoRA-based RL experiments. Tina-STILL-3-1.5B-preview Tina-DeepScaleR-1.5B-Preview Tina-Open-RS{X}-{Y} Tina-LIMR-{Z} Tina-OpenR1 Tina-OpenThoughts"
        },
        {
            "title": "LoRA\nLoRA\nLoRA\nLoRA\nLoRA\nLoRA",
            "content": "LoRA Modules LoRA Rank LoRA α LoRA Dropout"
        },
        {
            "title": "Algorithm\nOptimizer\nOptimizer Momentum\nLearning Rate\nLR Scheduler\nWarmup Ratio\nPrecision",
            "content": "query, key, value, dense 32 128 0.05 β1 , β2 GRPO AdamW = 0.9, 0.999 1e-6 Cosine with Min LR 0.1 BF16-mixed"
        },
        {
            "title": "Max Prompt Length\nMax Completion Length\nNumber of Generation\nVllm GPU Memory Utilization\nVllm Max Model Length",
            "content": "4 32 1 2 NVIDIA L40S 512 3584 4 0.4 4608 Table 5: Common hyperparameter settings. We also show the varied hyperparameter in Table 6 for all the LoRA-based RL experiments. Particularly, all the reward types including Accuracy, Format, Length, Cosine, Tag Count, Reasoning Steps, Repetition Penalty, are defined and implemented by the OpenR1 code repository.4 4https://github.com/huggingface/open-r1 Tina: Tiny Reasoning Models via LoRA 1 1 1 1 1 1 1 1 1 1 1 , 2 , 2 , 2 , , 2 , 2 , 2 , 2 , 2 , , 2 , 2 , 2 , 1 , 1 , , 1 1 , 1 , 1 , 1 , , 1 , 1 1 , 1 , 1 n , r A r , r A r , s a F , s a F , r A r , r A r , r A r , r A r , r A r , r A r , r A r , r A r , r A , s , r A , n , r , o T , t n a t e i e , s , r A , n , r , o T , t i o R a n i e - - - - - - - 6 - 5 7 - - - - - - - - - - G - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 6 5 2 4 6 2 6 1 - - - - - - - - - - - 4 6 6 1 8 4 - - i P - 5 . 1 - a e - i v - 5 . 1 - 3 - T - i G - 3 - O - i 3 - O - i r - L - 4 6 - L - i r - L - 6 1 - L - i a - L - 8 - L - i r - L - 4 - L - i 2 - O - i 1 - O - i L - i - 6 - 5 - L - i - 7 - 5 - L - i 1 p - i g T O - i h W w p r R R r i i l o D L p R n R e . 5 a i n e m e o e h s m - h n e t r e d a : b 20 Tina: Tiny Reasoning Models via LoRA C.2. Evaluation Command The following is the evaluation command we use to combine lighteval and vLLM for performance evaluation on reasoning tasks. The MODEL_PATH should be replaced with either the local path or huggingface identifier to the model to be evaluated. TASK should be one of the six reasoning tasks including aime24, aime25, amc23, math_500, gpqa:diamond, and minerva. PATH_TO_OPEN_R1_EVALUATE_SCRIPT should be the path to the custom evaluate script provided by OpenR1.5 MODEL_ARGS=\"pretrained=$MODEL_PATH, dtype=bfloat16, data_parallel_size=2, max_model_length=32768, gpu_memory_utilization=0.5, generation_parameters={max_new_tokens:32768,temperature:0.6,top_p:0.95}\" lighteval vllm $MODEL_ARGS \"custom$TASK00\" --custom-tasks $PATH_TO_OPEN_R1_EVALUATE_SCRIPT --use-chat-template 5https://github.com/huggingface/open-r1/blob/4f5b21e21dec473af9729bce8e084deb16223ae4/src/open_r1/evaluate.py 21 Tina: Tiny Reasoning Models via LoRA D. Full Tina Model Performance Evaluation In this section, we present all Tina models detailed evaluation performance during post-training across six reasoning tasks including AIME24/25, AMC23, MATH500, GPQA and Minerva. Ch ec po in St ep (3740 te ps er Epoch ) AIME24 AIME25 AMC23 MATH500 GPQA Minerva Avg. 500 1500 2000 2500 3000 3500 30. 36.67 26.67 36.67 33.33 30.00 30. 13.33 20.00 20.00 30.00 30.00 20. 26.67 75.00 65.00 70.00 77.50 70. 67.50 67.50 83.60 84.80 83.80 84. 83.00 82.60 82.20 35.86 32.32 37. 33.33 35.35 30.81 32.32 32.35 27. 26.84 26.84 27.57 25.74 26.10 45. 44.46 44.11 48.16 46.54 42.78 44. Table 7: Performance evaluation of Tina-STILL-3-1.5B-preview. Checkpoin St ep (5039 te ps er Epoch ) AIME24 AIME25 AMC23 MATH500 GPQA Minerva Avg. 500 1000 2000 2500 3000 3500 4000 5000 30.00 43.33 30.00 20.00 13. 26.67 23.33 20.00 23.33 20.00 23. 26.67 20.00 26.67 16.67 16.67 23. 20.00 20.00 26.67 67.50 67.50 80. 57.50 52.50 57.50 62.50 70.00 72. 75.00 82.40 86.20 84.80 80.60 75. 78.60 80.40 82.00 80.80 80.80 39. 37.88 32.83 29.29 31.31 28.79 31. 41.41 34.85 33.33 31.25 28.68 29. 24.26 18.01 23.16 24.26 27.94 26. 29.41 45.65 48.38 46.17 39.72 34. 38.57 40.94 43.56 42.99 44.20 Table 8: Performance evaluation of Tina-DeepScaleR-1.5B-Preview. 22 Tina: Tiny Reasoning Models via LoRA Checkpoint Steps (875 Steps per Epoch) ME 24 ME 25 MC 23 ATH5 00 GP QA ine rva Avg. 50 150 200 250 300 350 450 500 550 600 650 750 800 850 26.67 30.00 36. 20.00 36.67 33.33 40.00 30.00 36. 36.67 26.67 30.00 20.00 33.33 33. 30.00 26.67 23.33 30.00 16.67 26. 20.00 26.67 16.67 16.67 26.67 23. 16.67 26.67 23.33 13.33 23.33 23. 26.67 75.00 65.00 65.00 70.00 65. 70.00 77.50 70.00 70.00 82.50 80. 70.00 80.00 72.50 75.00 65.00 75. 84.20 83.00 84.80 83.80 84.60 85. 84.40 82.80 85.60 85.20 86.00 84. 85.00 85.00 83.60 84.20 83.80 37. 37.37 27.78 33.33 38.38 30.81 39. 35.86 33.84 37.37 35.35 37.88 33. 40.40 31.31 38.38 31.82 29.04 29. 27.94 27.94 28.31 30.15 27.94 31. 32.72 31.62 29.78 29.78 27.94 31. 27.57 29.04 27.94 45.94 45.86 43. 43.62 45.49 46.03 47.74 44.43 47. 49.45 45.75 46.49 44.93 46.09 45. 44.99 45.32 Table 9: Performance evaluation of Tina-Open-RS3. Checkpoint Steps (875 Steps per Epoch) ME 24 ME 25 MC 23 ATH5 00 GP QA ine rva Avg. 100 150 200 250 300 400 450 500 550 600 700 750 800 850 33.33 36. 40.00 26.67 46.67 30.00 33.33 26. 43.33 20.00 40.00 33.33 33.33 23. 30.00 30.00 26.67 23.33 23.33 23. 23.33 13.33 26.67 20.00 16.67 26. 23.33 23.33 20.00 23.33 26.67 23. 26.67 23.33 77.50 72.50 72.50 70. 72.50 75.00 75.00 70.00 84.20 84. 85.80 83.80 82.60 84.00 84.80 83. 77.50 87.00 67.50 72.50 72.50 57. 70.00 72.50 75.00 70.00 84.20 83. 84.20 83.80 82.40 84.20 84.40 83. 38.89 31.31 30.30 39.39 31.82 33. 37.37 37.37 36.36 33.84 40.91 32. 34.85 33.33 38.89 32.32 35.86 29. 28.68 30.51 29.41 30.51 29.04 28. 27.57 47.72 46.12 47.07 45.43 46. 46.34 46.53 43.58 32.72 50.60 29. 30.88 30.88 30.51 28.68 29.04 29. 28.68 43.05 48.54 45.62 43.89 44. 46.33 46.30 44.72 Table 10: Performance evaluation of Tina-Open-RS2. 23 Tina: Tiny Reasoning Models via LoRA Ch ec po in St ep (2327 te ps er Epoch ) AIME24 AIME25 AMC23 MATH500 GPQA Minerva Avg. 400 600 800 1200 1400 1600 1800 2000 2400 33.33 30.00 43.33 33.33 36. 30.00 23.33 26.67 30.00 30.00 30. 20.00 30.00 20.00 20.00 20.00 20. 13.33 20.00 26.67 23.33 23.33 75. 77.50 80.00 82.50 67.50 67.50 65. 75.00 72.50 70.00 67.50 83.80 84. 84.00 84.40 84.40 83.40 83.40 84. 83.00 81.40 81.80 31.82 34.34 35. 35.86 37.88 31.82 35.86 34.34 36. 30.81 30.30 29.78 31.62 28.68 29. 30.15 29.78 26.84 27.57 27.94 26. 27.57 45.62 47.94 48.56 47.64 46. 43.75 41.29 44.63 46.08 43.67 43. Table 11: Performance evaluation of Tina-Open-RS1. Checkpoint Steps (174 Steps per Epoch) ME 24 ME 25 MC 23 ATH5 00 GP QA ine rva Avg. 50 100 200 20.00 46.67 26.67 33.33 26. 20.00 20.00 30.00 67.50 75.00 72. 62.50 85.40 83.80 84.00 83.40 37. 34.85 37.37 29.80 30.51 30.51 30. 30.88 44.66 48.47 45.12 44.99 Table 12: Performance evaluation of Tina-LIMR. Checkpoint Steps ( 11716 Step per Epoch) AIM 24 AIME25 AM C2 3 MAT 500 GP QA Mi erva Avg. 500 1000 1500 2500 3000 3500 30.00 30.00 36. 26.67 30.00 20.00 36.67 20.00 23. 26.67 23.33 23.33 30.00 23.33 77. 72.50 75.00 67.50 72.50 67.50 67. 85.20 85.60 86.80 83.20 83.80 84. 83.60 33.84 33.84 39.90 29.80 33. 34.34 31.31 30.15 26.67 30.51 31. 26.84 28.31 25.74 46.12 45.32 49. 43.69 45.05 44.13 44.69 Table 13: Performance evaluation of Tina-OpenR1. Checkpoin St ep (8259 te ps er Epoch ) AIME24 AIME25 AMC23 MATH500 GPQA Minerva Avg. 500 1000 1500 2000 3000 3500 4000 4500 5000 33. 33.33 30.00 30.00 36.67 26.67 20. 33.33 30.00 20.00 16.67 23.33 23. 23.33 26.67 23.33 16.67 23.33 20. 33.33 77.50 80.00 70.00 70.00 72. 75.00 60.00 72.50 65.00 65.00 84. 85.20 86.00 84.20 84.80 83.60 84. 83.60 85.00 84.80 35.86 24.75 37. 33.33 30.15 32.72 29.04 28.31 46. 46.56 46.04 44.86 41.41 33.09 49. 34.34 32.32 38.38 33.84 40.91 32. 26.10 27.94 26.84 30.88 45.94 39. 46.51 43.45 45.82 Table 14: Performance evaluation of Tina-OpenThoughts. 24 Tina: Tiny Reasoning Models via LoRA Checkpoint Steps (875 Steps per Epoch) ME 24 ME 25 MC 23 ATH5 00 GP QA ine rva Avg. 50 100 150 250 300 350 400 450 550 600 650 700 750 850 33.33 16.67 43.33 30.00 33. 36.67 26.67 36.67 36.67 30.00 30. 33.33 26.67 36.67 30.00 20.00 23. 16.67 20.00 23.33 23.33 30.00 16. 30.00 23.33 16.67 20.00 23.33 26. 20.00 20.00 26.67 30.00 20.00 75. 70.00 80.00 70.00 65.00 67.50 75. 72.50 72.50 72.50 77.50 72.50 77. 80.00 75.00 75.00 72.50 83.80 83. 85.00 84.00 83.80 84.40 84.00 84. 85.60 85.60 84.80 83.80 82.40 83. 84.20 82.40 85.40 37.37 33.33 35. 39.90 34.34 37.88 37.88 32.83 29. 37.37 36.87 30.30 37.88 35.35 38. 35.86 36.36 26.84 26.47 30.15 28. 28.31 29.78 29.78 27.57 27.57 29. 31.62 28.31 27.94 31.25 27.57 28. 30.15 45.50 41.61 49.53 45.99 45. 45.48 47.22 46.22 44.72 45.81 47. 45.82 45.40 47.85 47.06 45.26 44. Table 15: Performance evaluation of Tina-Open-RS3-DrGRPO. Checkpoint Steps (174 Steps per Epoch) ME 24 ME 25 MC 23 ATH5 00 GP QA ine rva Avg. 50 100 200 20.00 46.67 26.67 33.33 26. 20.00 20.00 30.00 67.50 75.00 72. 62.50 85.40 83.80 84.00 83.40 37. 34.85 37.37 29.80 30.51 30.51 30. 30.88 44.66 48.47 45.12 44.99 Table 16: Performance evaluation of Tina-LIMR-5e-6-lr with learning rate 5e-6. Checkpoint Steps (174 Steps per Epoch) ME 24 ME 25 MC 23 ATH5 00 GP QA ine rva Avg. 50 100 150 40.00 43.33 30.00 33.33 13.33 16. 23.33 13.33 72.50 77.50 72.50 70. 83.00 84.60 86.20 83.20 34.34 34. 37.37 29.29 29.04 30.51 30.51 31. 45.37 47.91 46.65 43.40 Table 17: Performance evaluation of Tina-LIMR-5e-7-lr with learning rate 5e-7. Checkpoint Steps (174 Steps per Epoch) ME 24 ME 25 MC 23 ATH5 00 GP QA ine rva Avg. 50 100 150 200 20. 30.00 36.67 33.33 30.00 77.50 23. 20.00 20.00 72.50 70.00 72.50 84. 84.60 83.40 85.00 38.38 31.62 46. 32.32 31.82 29.80 29.78 30.88 29. 45.42 45.46 45.01 Table 18: Performance evaluation of Tina-LIMR-64-LoRA-rank with LoRA rank 64 and alpha 512. 25 Tina: Tiny Reasoning Models via LoRA Checkpoint Steps (174 Steps per Epoch) ME 24 ME 25 MC 23 ATH5 00 GP QA ine rva Avg. 50 100 150 33.33 43.33 26.67 36.67 23.33 33. 16.67 20.00 62.50 70.00 72.50 75. 84.20 83.20 83.40 83.00 38.89 31. 35.35 35.35 39.39 28.31 29.04 30. 45.58 48.92 43.94 47.43 Table 19: Performance evaluation of Tina-LIMR-16-LoRA-rank with LoRA rank 16 and alpha 64. Checkpoint Steps (174 Steps per Epoch) ME 24 ME 25 MC 23 ATH5 00 GP QA ine rva Avg. 50 100 150 200 30. 26.67 53.33 23.33 26.67 82.50 16. 20.00 20.00 72.50 60.00 72.50 83. 84.00 83.20 85.40 33.84 36.87 37. 32.83 30.51 29.78 30.88 28.68 47. 44.42 47.46 43.86 Table 20: Performance evaluation of Tina-LIMR-8-LoRA-rank with LoRA rank 8 and alpha 32. Checkpoint Steps (174 Steps per Epoch) ME 24 ME 25 MC 23 ATH5 00 GP QA ine rva Avg. 50 100 150 200 30.00 26. 36.67 33.33 23.33 26.67 20.00 23. 65.00 72.50 85.00 77.50 85.00 82. 83.80 85.40 35.35 34.85 31.82 29. 29.04 29.0 35.86 28.31 44.74 45. 47.72 47.29 Table 21: Performance evaluation of Tina-LIMR-4-LoRA-rank with LoRA rank 4 and alpha 16. 26 Tina: Tiny Reasoning Models via LoRA E. Full Tina Model Training Phase Transition transitions clear phase In this section, we present all Tina models training phase transitions along the training dynamics. Specifically, training of Tina-DeepScaleR-1.5B-Preview, we observe Tina-STILL-3-1.5B-preview, and Tina-Open-RS3-GRPO, as shown in Figures 5, 6, and 7. For Tina-OpenR1 and Tina-Thoughts (Figures 8 and 9), the observation is similar, except the best-performing checkpoint is achieved after the training turning point, rather than before. However, we do not observe such transition in all Tina variants on the LIMR dataset, as shown in Figures 10, 11, and 12, possibly because its small data size leads to training periods which are too brief to extract meaningful information. in the Tina-Open-RS1, Tina-Open-RS2, Tina-Open-RS3, Figure 5: Phase transition in Tina-DeepScaleR-1.5B-Preview and Tina-STILL-3-1.5B-preview. The raw data is from the Weights & Biases training logs and smoothed via exponential moving average (EMA) with factor 0.1. 27 Tina: Tiny Reasoning Models via LoRA Figure 6: Phase transition in Tina-Open-RS1 and Tina-Open-RS2. The raw data is from the Weights & Biases training logs and smoothed via exponential moving average (EMA) with factor 0.1. 28 Tina: Tiny Reasoning Models via LoRA Figure 7: Phase transition in Tina-Open-RS3 and Tina-Open-RS3-GRPO. The raw data is from the Weights & Biases training logs and smoothed via exponential moving average (EMA) with factor 0.1. 29 Tina: Tiny Reasoning Models via LoRA Figure 8: Phase transition in Tina-OpenR1. The raw data is from the Weights & Biases training logs and smoothed via exponential moving average (EMA) with factor 0.1. 30 Tina: Tiny Reasoning Models via LoRA Figure 9: Phase transition in Tina-OpenThoughts. The raw data is from the Weights & Biases training logs and smoothed via exponential moving average (EMA) with factor 0.1. 31 Tina: Tiny Reasoning Models via LoRA Figure 10: Phase transition in Tina-LIMR, Tina-LIMR-64-LoRA-rank and Tina-LIMR-16-LoRA-rank. The raw data is from the Weights & Biases training logs and smoothed via exponential moving average (EMA) with factor 0.1. 32 Tina: Tiny Reasoning Models via LoRA Figure 11: Phase transition in Tina-LIMR-8-LoRA-rank and Tina-LIMR-4-LoRA-rank. The raw data is from the Weights & Biases training logs and smoothed via exponential moving average (EMA) with factor 0.1. 33 Tina: Tiny Reasoning Models via LoRA Figure 12: Phase transition in Tina-LIMR-5e-6-lr and Tina-LIMR-5e-7-lr. The raw data is from the Weights & Biases training logs and smoothed via exponential moving average (EMA) with factor 0.1."
        }
    ],
    "affiliations": [
        "University of Southern California"
    ]
}