{
    "paper_title": "EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs",
    "authors": [
        "Yuhao Zhang",
        "Yuhao Du",
        "Zhanchen Dai",
        "Xiangnan Ma",
        "Kaiqi Kou",
        "Benyou Wang",
        "Haizhou Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Speech-to-speech large language models (SLLMs) are attracting increasing attention. Derived from text-based large language models (LLMs), SLLMs often exhibit degradation in knowledge and reasoning capabilities. We hypothesize that this limitation arises because current training paradigms for SLLMs fail to bridge the acoustic-semantic gap in the feature representation space. To address this issue, we propose EchoX, which leverages semantic representations and dynamically generates speech training targets. This approach integrates both acoustic and semantic learning, enabling EchoX to preserve strong reasoning abilities as a speech LLM. Experimental results demonstrate that EchoX, with about six thousand hours of training data, achieves advanced performance on multiple knowledge-based question-answering benchmarks. The project is available at https://github.com/FreedomIntelligence/EchoX."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 4 7 1 9 0 . 9 0 5 2 : r EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs Yuhao Zhang, Yuhao Du, Zhanchen Dai, Xiangnan Ma, Kaiqi Kou, Benyou Wang, Haizhou Li The Chinese University of Hong Kong, Shenzhen yoohao.zhang@gmail.com, wangbenyou@cuhk.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Speech-to-speech large language models (SLLMs) are attracting increasing attention. Derived from text-based large language models (LLMs), SLLMs often exhibit degradation in knowledge and reasoning capabilities. We hypothesize that this limitation arises because current training paradigms for SLLMs fail to bridge the acoustic-semantic gap in the feature representation space. To address this issue, we propose EchoX, which leverages semantic representations and dynamically generates speech training targets. This approach integrates both acoustic and semantic learning, enabling EchoX to preserve strong reasoning abilities as speech LLM. Experimental results demonstrate that EchoX, with about six thousand hours of training data, achieves advanced performance on multiple knowledge-based question-answering benchmarks. The project is available at https://github.com/FreedomIntelligence/EchoX."
        },
        {
            "title": "Introduction",
            "content": "GPT-4o (Hurst et al., 2024) demonstrates impressive speech interaction performance, which has spurred the rapid development of speech-to-speech large language models (SLLMs). The mainstream approach to building SLLMs is to first discretize speech into speech tokens and then train speech LLMs (Zhang et al., 2023; Defossez et al., 2024; Chen et al., 2025a) under token-based training paradigm. Although current SLLMs can be trained on massive amounts of speech data, they still exhibit intelligence degradation compared to large text-based models (Chen et al., 2024). Current SLLMs have not yet fully extended the textual intelligence of LLMs into the speech domain, and the underlying reasons for this issue remain underexplored. Beyond the acoustic-semantic conflict in speech tokens (Gong et al., 2025), we argue that one of the main causes is that SLLMs have not bridged the acousticsemantic gap in the feature representation space. As illustrated in Figure 1(a), the training objective for LLMs emphasizes semantic correctnesspredicting semantically similar token is not heavily penalized. In contrast, SLLMs treat speech tokens as prediction targets, which biases the model toward pronunciationlevel accuracy. As result, even when an SLLM produces semantically correct response, it may incur severe penalties due to major pronunciation differences, as shown in Figure 1(b). Corresponding author. Preprint. 1 Target label Hello Semantic space Hello 837 259 317 453 238 462 104 125 Pseudo label 837 81 194 458 Acoustic space"
        },
        {
            "title": "Gap",
            "content": "Acoustic space Text2 Codec Model output Hi Hi 837 81 194 837 81 194 726 Hi LLMs Speech LLMs Speech LLMs Hello (a) Text LLMs Hello Hello (b) Speech LLMs (c) EchoX Figure 1: Comparison of training strategies across different models. There are two main paradigms for building SLLMs. The first is interleaved generation (Zeng et al., 2024), which forces the model to jointly consider both acoustics and semantics, but requires large amount of training data (Chen et al., 2025b). The second employs an auxiliary textto-codec decoder to convert textual representations into speech tokens (Defossez et al., 2024). However, this approach still fails to address the acoustic-semantic gap. We propose EchoX, framework that introduces an auxiliary module to dynamically predict speech tokens based on semantic understanding. This approach eliminates the mismatch between speech tokens and semantic features, enabling the construction of SLLMs that preserve the intelligence of LLMs. Furthermore, to address the challenge of long speech sequences, we adopt unit language as the generated speech token and introduce trigger to support streaming generation, thereby alleviating the difficulties of long-sequence generation. As shown in Figure 2, EchoX achieves advanced performance on knowledge-based QA benchmarks with limited training data and parameters."
        },
        {
            "title": "2.1 Overall Design",
            "content": "Figure 2: Comparison of models using different training data, parameters, and performance metrics. The number within each node represents the score evaluated on the Web Questions dataset (Berant et al., 2013). We design three-stage training framework to mitigate the acoustic-semantic gap. The first stage involves converting textual LLM into speech-to-text dialog LLM. The second stage trains text-to-codec model, which converts text into speech tokens. The final stage combines these two modules and fine-tunes the entire speech-to-speech LLM. The overall training process is illustrated in Figure 3. Furthermore, to address the challenge of long speech sequences, we use unit language as the speech token and design streaming inference mechanism. 2 Hi, am here. Speech-to-text LLMs LoRA Hello Stage I: Speech-to-text training 837 81 194 ... 902 819 Text2codec decoder Hi, am here. Echo loss 837 81 194 ... 902 819 837 81 194 ... 902 819 Echo decoder Initialize Denoising loss Decode Text2codec decoder Denoising adapter ... S2T loss Decode ... Hi, am here. Speech-to-text LLMs LoRA Hello Stage II: Text-to-Codec training Stage III: Echo training Figure 3: The three training stages of EchoX. Note that streaming modules are omitted here."
        },
        {
            "title": "2.2 Stage I: Speech-to-Text Training",
            "content": "The goal of this stage is to make the model perceive speech and generate textual responses. The mainstream approach involves using an encoder to model the audio, followed by an adapter to bridge the gap between acoustic encoder and textual LLMs (Chu et al., 2024). In our work, we adopt the Soundwave (Zhang et al., 2025a), which employs an alignment adapter and compression adapter to efficiently achieve audio understanding. We omit the supervised fine-tuning (SFT) stage described in the original framework, since this work primarily targets spoken dialogue tasks. Instead, we only leverage speech recognition and conversational datasets to build the speech-to-text (S2T) LLM."
        },
        {
            "title": "2.3 Stage II: Text-to-Codec Training",
            "content": "We use typical decoder-only architecture to pre-train the text-to-codec (T2C) module (Wang et al., 2023). The input data is text = {x1, x2, ..., xn}, and the target is the sequence of quantized speech tokens = {y1, y2, ..., ym}. During training, the decoder maps to hidden states and predicts the speech tokens. We apply cross-entropy loss for optimization. To ensure consistency of the representation space between the T2C module and the speech-to-text LLM, we initialize and freeze the embeddings, then apply projection layer to adapt the dimensionality from the LLM to the T2C module."
        },
        {
            "title": "2.4 Stage III: Echo Training",
            "content": "The key objective of this phase is to feed the hidden states from the S2T LLM into the T2C module to generate speech tokens as output. Unlike conventional approaches that rely on annotated speech tokens for training, we propose Echo training, which leverages the pre-trained T2C module to decode the outputs of the S2T LLM as training targets. 3 Formally, let the intermediate representation of the response from the S2T LLM be denoted as = {h1, ..., hn}. We perform greedy search to obtain the corresponding text sequence = {x n}, which is then fed into the pre-trained T2C module to produce = {y m} as the final pseudo-labels. During this stage, the T2C module remains frozen. 1, ..., 1, ..., Echo loss We feed into an Echo decoder, which shares the same architecture as the T2C module. The Echo decoder is initialized with the T2C parameters. The training objective is to predict , with the loss function defined as: (cid:88) LEcho = logP (y iH, <i) (1) Since the hidden states contain redundant information, we design feed-forward network, termed the Denoising Adapter, before feeding them into the Echo decoder. The purpose is to align the representations between and the embeddings of . We employ cosine similarity loss to train against , thereby minimizing noise in and reducing its impact on speech token generation. The training objective is: LDenoising = (cid:88) 1 Cos(Adapter(Hi), Emb(X i)) (2) where Adapter() denotes the Denoising Adapter, Emb() represents the word embedding layer in the T2C module, and Cos(, ) computes the cosine similarity between two vectors. Speech-to-text loss Additionally, we update the LoRA (Hu et al., 2022) parameters in the first stage for fine-tuning. We utilize the ground-truth text labels = {x1, ..., xn} for training, with the objective: LS2T = (cid:88) logP (xiHS, x<i) (3) where HS denotes the hidden state of the input speech S. The final training loss combines all three objectives through weighted summation: = LEcho + λ LDenoising + LS2T (4) where λ is scaling factor, since LDenoising differs in nature from the other two losses."
        },
        {
            "title": "2.5 Speech Token Construction",
            "content": "We use unit language (Zhang et al., 2025b) as the speech token to reduce the length of the speech sequence. Unit language significantly compresses the audio sequence while ensuring the quality of text-to-speech synthesis. Unit For speech unit extraction, the raw waveform inputs are first passed through pre-trained HuBERT model (Hsu et al., 2021), which transforms them into continuous hidden representations. The selected hidden layer (the 11th layer in this work) is projected into k-means codebook space. Each vector is assigned to its nearest cluster centroid, effectively discretizing the representation into sequence of unit IDs. Unit Language We used unit language, which segments sequences of discrete speech units into word-like tokens based on statistical language modeling principles (Zhang et al., 2025b). Given sequence of units u1, u2, ..., un, the goal is to segment and group them into sequence w1, w2, ..., wm, where each wj is composed of at most contiguous units. 4 We apply dynamic programming to find the optimal segmentation path π(u1:i) by maximizing the cumulative log-probability: = arg max (logP (π(u[1:ik]) ) + logP (u[ik+1,i] (cid:125) (cid:124) (cid:125) (cid:124) (cid:123)(cid:122) wj (cid:123)(cid:122) [1:j1] )). (5) determines the optimal number of units to form wj and where language before wj. The unit sequence is segmented recursively based on these optimal values k. [1:j1] determines the optimal unit Normalizing units is important to reduce noise in the unit sequence (Lee et al., 2021). We train an encoderdecoder model based on the original parallel text-unit data. Then, we perform data distillation on the training set for regularization purposes. Furthermore, we apply adjacent position deduplication to the units to reduce the token sequence length."
        },
        {
            "title": "2.6 Streaming Generation",
            "content": "Given that speech sequences are significantly longer than their text counterparts, waiting for complete text generation before producing speech tokens would substantially increase synthesis difficulty. Therefore, applying streaming generation becomes essential, as it mitigates long-sequence generation challenges and improves real-time responsiveness. The core of streaming generation lies in determining whether to read (continue processing) or write (start generating speech) at each timestep. The critical constraint is maintaining the semantic completeness of each segment to avoid disjointed speech output. We implement trigger feature that computes the cosine similarity between the current semantic representation and The trigger feature. write operation is executed (sending the subsequence to the Echo decoder) only when similarity exceeds threshold and the current value is local extremum of the window size w. The streaming inference process is shown in Figure 4."
        },
        {
            "title": "3 Data",
            "content": "Hi am here Vocoder 837 81 194 ... 902 819 837 81 194 ... 902 819 Echo decoder Denoising adapter R: Read, W: Write R ... 3 . 0 2 2 . . 0 0 1 . 0 c Trigger feature ... Speech-to-text LLMs LoRA Hello Figure 4: Streaming inference process. To construct high-quality corpora for Speech-to-Text (S2T), Text-to-Codec (T2C), and Speech-to-Speech (S2S) training, we adopt data-centric pipeline with four stages: (i) collect text dialog corpora suited for spoken interaction; (ii) transform them into natural, spoken-style dialogues via rigorous multi-step cleaning and rewriting process; (iii) synthesize the required acoustic modalities (inputs and/or outputs) with carefully controlled voices; and (iv) enforce strict audio quality control to retain only reliable samples. Appendix shows the detailed process for the pipeline. Figure 5 illustrates an example of the data construction process for S2S data. And statistics of the training data are shown in Table 1. 5 Table 1: Statistics of data usage at different stages Task ASR ASR TTS TTS TTS SQA SQA Total Data Size Duration(H) Stage LibriSpeech (Panayotov et al., 2015) MLS (Pratap et al., 2020) AudioQA-1M SpeechInstruct (Zhang et al., 2023) HH-RLHF-Speech sharechatx (Cheng et al., 2025) Magpie-Pro-Speech+ - 281,241 723,636 178,576 31,563 124,945 43,223 117,000 1,500,184 960 3,000 989 84 656 178 327 6,194 II II II I, III I, III - AudioQA-1M: text-only usage with minor cleanup; all audio is synthesized by ourselves. Sourced from VITA-1.5 (Fu et al., 2025). Speech versions of two public text-only conversational datasetshh-rlhf (Bai et al., 2022) and Magpie-Llama-3.3-Pro-1M-v0.1 (Xu et al., 2024)created via light text normalization and TTS; For Magpie, we additionally extend the corpus to improve coverage. The speech version of the two datasets are denoted HH-RLHF-SPEECH and MAGPIE-PRO-SPEECH+. * denotes we sample the dataset and only used part of it. Note there is no target audio at stage III; thus the duration count only contains source speech."
        },
        {
            "title": "3.1 Speech-to-Text Training",
            "content": "We apply the above pipeline to collection of open-source dialog datasets (e.g.,Magpie), clean them into spoken-style text, and synthesize user-side inputs with diverse Google TTS voices1. Text-based dialog data typically generates structured and formal outputs, which introduce excessive non-speech tokens (e.g., symbols, formatting cues). For instance, the token 1. can be interpreted differently depending on the contextone point in mathematical text or first in list. To verify acoustic integrity and textual alignment, we transcribe the synthesized inputs using the parakeet-tdt-0.6b-v2 ASR model and compute word error rate (WER). We retain only utterances with WER < 5%."
        },
        {
            "title": "3.2 Text-to-Codec Training",
            "content": "Using the same cleaning pipeline, we process additional sources including AudioQA, SpeechInstruct, and hh-rlhf. For each assistant turn, we synthesize single-voice speech with the fine-tuned GPT-SoVITS2 model and extract codec tokens. The T2C supervision used in training is text, codec only, explicitly aligning textual content with its corresponding codec representation. To broaden S2S coverage and promote generalization, we also synthesize input speech for the hh-rlhf user prompts using the Google TTS API, thereby yielding paired user speech and assistant speech for those dialogs. The resulting S2S dialog sets will be released alongside our corpus."
        },
        {
            "title": "3.3 Echo Training",
            "content": "This portion of data primarily consists of three parts. The first part is everyday dialogue, where the model acts as an assistant, and the overall distribution is relatively short. The second part is speech reasoning, 1https://cloud.google.com/text-to-speech/docs/list-voices-and-types 2https://github.com/RVC-Boss/GPT-SoVITS 6 Figure 5: An example of the Speech-to-Speech data construction pipeline. where the input is speech-based question and the output is long-text reasoning process. The third part is knowledge-based Q&A data, mainly comprising question-and-answer interactions about common sense."
        },
        {
            "title": "4.1 Model Settings",
            "content": "We conducted experiments based on two model sizes: 3B and 8B. For the 3B model (called EchoX-3B), we used LLaMA 3.2, while the 8B model (called EchoX-8B) used LLaMA 3.1 (Grattafiori et al., 2024). For the Text2Codec model, both the Echo Decoder and Text2Codec adopted the same architecture: For the 3B model, 6 Transformer layers with hidden dimension of 512. For the 8B model, 8 Transformer layers with hidden dimension of 768. For Speech2Speech, an additional adapter was used with an intermediate layer size of 8192. The value of λ to balance the training loss is set to 0.2. The vocoder we used is the unit-based HiFi-GAN (Kong et al., 2020; Polyak et al., 2021). For training steps, Stage I: Trained for 10,000 steps, primarily referencing SoundWave (Zhang et al., 2025a). Stage II: Trained for 5,000 steps using 4 GPUs. Stage III: Trained for 12,000 stepsusing one 8 A100 GPUs for the 3B model and 16 A100 GPUs for the 8B model. We take the embedding of period as the trigger representation. The streaming threshold is set to 0.1 and the for streaming window is set 5. For all our models we use the greedy search to inference. For evaluation, we use the UltraEval-Audio toolkit 3. We mainly conduct experiments on the three benchmarks: Llama questions (Nachmani et al., 2023), Web questions (Berant et al., 2013), and TriviaQA (Joshi et al., 2017)."
        },
        {
            "title": "4.2 Results",
            "content": "We compared our model with others on knowledge-based question-answering tasks in Tables 2 and 3. It can be observed that models using the interleave approach, despite being trained on massive amounts of 3https://github.com/OpenBMB/UltraEval-Audio 7 Table 2: Speech-to-Speech performance on spoken QA benchmarks. Model Llama Questions Web Questions TriviaQA Avg. OmniDRCA(2B) (Tan et al., 2025) LLaMA-Omni2-3B (Fang et al., 2025) EchoX-3B GPT-4o-Realtime (Hurst et al., 2024) VITA-Audio (Long et al., 2025) MinMo (Chen et al., 2025b) MiniCPM-o 2.6 (Yao et al., 2024) OmniDRCA (8B) (Tan et al., 2025) GLM-4-Voice (Zeng et al., 2024) LLaMA-Omni2-7B (Fang et al., 2025) Freeze-Omni*(Wang et al., 2024) Moshi (Defossez et al., 2024) EchoX-8B 55.3 55.7 54.0 71.7 68.0 64.1 61.0 65.0 50.0 60.7 46.0 43.7 63.3 22.1 28.0 31.6 51.6 41.7 39.9 40.0 30.0 32.0 31.3 26.1 23.8 40. 17.9 - 25.8 69.7 41.7 37.5 40.2 32.9 36.4 - 25.7 16.7 35.0 31.8 - 37.1 64.4 50.5 47.2 47.1 42.6 39.5 - 32.6 28.1 46.3 * indicates that we retested the model using the same evaluation tool. Table 3: Speech-to-Text performance on spoken QA benchmarks. Model Llama Questions Web Questions TriviaQA Avg. LLaMA-Omni2-3B (Fang et al., 2025) EchoX-3B MinMo (Chen et al., 2025b) OmniDRCA (Tan et al., 2025) VITA-Audio (Long et al., 2025) LLaMA-Omni2-7B (Fang et al., 2025) EchoX-8B 64.3 73.0 78.9 79.7 75.6 64.3 77. 30.5 40.8 55.0 51.7 45.0 30.5 44.6 - 36.1 48.3 47.7 45.9 - 46.7 - 50.0 60.7 59.7 55.5 - 56. data, show no significant advantage in speech-to-text tasksindicating that the core challenge lies in jointly modeling speech and text representations. For speech-to-speech tasks, although interleave-based models currently demonstrate certain advantages, models using the T2C method can still efficiently achieve comparable performance. Our proposed EchoX trained with about six thousand hours of data, achieves comparable performance with models trained on millions of hours. Thus, our proposed Echo training strategy offers an efficient way to learn unified speech and semantic representations."
        },
        {
            "title": "5 Analysis",
            "content": "We begin by comparing the knowledge degradation in SLLMs and further apply case studies to interpret its causes from representational perspective. We then conduct comparative experiments on two approaches for long-sequence generation: unit language modeling and streaming decoding. We use EchoX-3B for the analysis unless otherwise specified."
        },
        {
            "title": "5.1 Intelligence Degradation of SLLMs",
            "content": "We analyze how knowledge degradation occurs in SLLMs. From the results in Table 4, it can be observed that the Speech-to-Text model improves performance on simple question-answering tasks like LLaMA Questions, but leads to significant decline on more challenging tasks. As for the speech output, even incorporating TTS model for the S2T model leads to further decrease, due to errors in synthesizing and recognizing certain specialized nouns. Furthermore, when building an endto-end model, if an interleaved training strategy is directly adopted, severe knowledge degradation emerges at this data scale. Employing an additional decoder can alleviate this issue by reducing the inconsistency between acoustic and semantic learning, though noticeable interference still persists. By using the Echo decoder, conflicts can be further mitigated, enabling simultaneous learning of both speech and text. Table 4: Performance comparison of models using the same data and different training strategies. Model Llama Questions Web Questions TriviaQA Avg. Text-to-text Speech-to-text Cascade Interleaving EchoX w/o Echo training EchoX Text output 67.3 73.0 Speech output 61.3 21.3 40.3 54.0 53.1 40.8 37.1 10.6 20.0 31. 50.0 36.1 31.3 6.4 12.6 25.8 56.8 50.0 43.2 12.8 24.3 37."
        },
        {
            "title": "5.2 Acoustic-Semantic Gap",
            "content": "We compare the similarity of word representations across different models in Figure 6. Hi and Hello are semantically close, while Hi and High are acoustically similar. It can be observed that in the S2T model, the similarity between Hi and Hello is relatively high. However, after training, the similarity between them decreases. Additionally, the similarity of their speech tokens is very low, essentially indicating no correlation. For Hi and High, regardless of whether in the S2T model or after interleaving training, their similarity remains relatively low. However, their speech tokens are highly consistent. This demonstrates that the learning objectives for semantics and acoustics are not aligned, necessitating the design of solutions to address this issue. S2T Interleaving Speech token 9 . 0 6 . 3 . 0 Hi & Hello Hi & High Figure 6: Similarity between two words within different model. Table 5: Length ratio and performance comparison of two types of codec. Length R. refers to the ratio of speech token to text token. Speech token Unit Unit language Llama Questions Length R. ACC Web Questions Length R. ACC TriviaQA Length R. ACC 9.31 4.57 49.0% 54.0% 9.63 4.79 28.8% 31.6% 9.13 4.57 24.7% 25.8%"
        },
        {
            "title": "5.3 Effect of Speech Tokens",
            "content": "We compared the results of using unit and unit language as speech tokens in Table 5. It can be observed that using unit language achieves nearly twice the compression ratio while delivering superior performance. Additionally, we further compared the quality of the generated audio and found that both methods perform similarly in terms of audio quality, as shown in Figure 7. However, the recognition accuracy of audio generated with unit language is significantly higher than that generated with units. This also indirectly indicates that when the model predicts speech tokens based on hidden representations, it is prone to error accumulation, leading to an increased error rate in the final model predictions. 2 1 9 6 3 Unit Unit language 11.25 9.53 3.71 3.75 ASR-WER UTMOS Figure 7: Comparison the speech quality based on unit and unit language."
        },
        {
            "title": "5.4 Effect of Streaming Inference",
            "content": "We compared streaming and offline methods under both 3B and 8B model sizes in Table 6. The results show that using streaming approach does not introduce significant performance degradation. Moreover, at the 3B level, due to the limited capacity of the LLM, properly segmenting the sequences helps the synthesis model achieve better performance and improved results. This demonstrates that streaming decoding reduces the difficulty of generating long sequences. Latency Llama (tokens) Questions Questions Web TriviaQA Streaming Offline 27.17 138. Streaming Offline 29.79 175.34 EchoX-3B 54.0 55.3 EchoX-8B 62.0 64.0 31.6 31.0 38.2 38.3 25.8 24. 31.7 32.1 Table 6: Performance comparison between streaming and offline decoding methods."
        },
        {
            "title": "6 Related Work",
            "content": "As previously mentioned, to prevent intelligence degradation in large speech models, it is essential to account for the divergence between speech and semantics in the training strategy. Currently, two mainstream approaches are widely adopted: interleaving decoding and text2codec decoding. The interleaving method aims to enable the model to learn both audio tokens and text tokens simultaneously, thereby unifying semantic and acoustic representations (Zeng et al., 2024). Although this approach allows direct joint input of speech and text tokens, it requires massive amounts of text and speech data to achieve satisfactory performance (Long et al., 2025; Tan et al., 2025; Li et al., 2025). The alternative method employs an additional text2codec decoder to convert text representations into speech representations (Defossez et al., 2024; Huang et al., 2025; Ding et al., 2025; Chen et al., 2025b). This strategy effectively decouples speech learning from semantic learning, helping to better preserve knowledge while reducing the demand for extremely large training datasets (Fang et al., 2025; Wang et al., 2024). However, this architecture still fails to fully address the degradation of textual modeling caused by learning speech representations. While we also utilize this architecture, we incorporate the Echo decoder in our training strategy to ensure consistency between speech and semantics, enabling more flexible and effective training."
        },
        {
            "title": "7 Conclusion",
            "content": "We propose EchoX, which primarily addresses the issue of intelligence degradation in current SLLMs. We first identified that existing training paradigms tend to cause an acoustic-semantic gap. To mitigate this, we introduced the Echo decoder architecture and corresponding training strategy, and further adopted more efficient and compact unit language as speech tokens. Experiments demonstrate that our model, using around six thousand hours of data, achieves comparable performance to the model based on millions of hours of data on intelligence QA tasks."
        },
        {
            "title": "References",
            "content": "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from questionanswer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 15331544, 2013. Kai Chen, Yunhao Gou, Runhui Huang, Zhili Liu, Daxin Tan, Jing Xu, Chunwei Wang, Yi Zhu, Yihan Zeng, Kuo Yang, et al. Emova: Empowering language models to see, hear and speak with vivid emotions. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 54555466, 2025a. Qian Chen, Yafeng Chen, Yanni Chen, Mengzhe Chen, Yingda Chen, Chong Deng, Zhihao Du, Ruize Gao, Changfeng Gao, Zhifu Gao, et al. Minmo: multimodal large language model for seamless voice interaction. arXiv preprint arXiv:2501.06282, 2025b. Yiming Chen, Xianghu Yue, Chen Zhang, Xiaoxue Gao, Robby Tan, and Haizhou Li. Voicebench: Benchmarking llm-based voice assistants. arXiv preprint arXiv:2410.17196, 2024. Xize Cheng, Dongjie Fu, Xiaoda Yang, Minghui Fang, Ruofan Hu, Jingyu Lu, Jionghao Bai, Zehan Wang, Shengpeng Ji, Rongjie Huang, Linjun Li, Yu Chen, Tao Jin, and Zhou Zhao. Omnichat: Enhancing spoken dialogue systems with scalable synthetic data for diverse scenarios. arXiv preprint arXiv:2501.01384, 2025. Introduces the ShareChatX dataset. Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, et al. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759, 2024. Alexandre Defossez, Laurent Mazare, Manu Orsini, Amelie Royer, Patrick Perez, Herve Jegou, Edouard Grave, and Neil Zeghidour. Moshi: speech-text foundation model for real-time dialogue. arXiv preprint arXiv:2410.00037, 2024. Ding Ding, Zeqian Ju, Yichong Leng, Songxiang Liu, Tong Liu, Zeyu Shang, Kai Shen, Wei Song, Xu Tan, Heyi Tang, et al. Kimi-audio technical report. arXiv preprint arXiv:2504.18425, 2025. Qingkai Fang, Yan Zhou, Shoutao Guo, Shaolei Zhang, and Yang Feng. Llama-omni2: Llm-based real-time spoken chatbot with autoregressive streaming speech synthesis. arXiv preprint arXiv:2505.02625, 2025. Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Haoyu Cao, Zuwei Long, Heting Gao, Ke Li, Long Ma, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, and Ran He. Vita-1.5: Towards GPT-4o level real-time vision and speech interaction. arXiv preprint arXiv:2501.01957, 2025. Yitian Gong, Luozhijie Jin, Ruifan Deng, Dong Zhang, Xin Zhang, Qinyuan Cheng, Zhaoye Fei, Shimin Li, and Xipeng Qiu. Xy-tokenizer: Mitigating the semantic-acoustic conflict in low-bitrate speech codecs. arXiv preprint arXiv:2506.23325, 2025. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 11 Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM transactions on audio, speech, and language processing, 29:34513460, 2021. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Ailin Huang, Boyong Wu, Bruce Wang, Chao Yan, Chen Hu, Chengli Feng, Fei Tian, Feiyu Shen, Jingbei Li, Mingrui Chen, et al. Step-audio: Unified understanding and generation in intelligent speech interaction. arXiv preprint arXiv:2502.11946, 2025. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis. Advances in neural information processing systems, 33:1702217033, 2020. Ann Lee, Hongyu Gong, Paul-Ambroise Duquenne, Holger Schwenk, Peng-Jen Chen, Changhan Wang, Sravya Popuri, Yossi Adi, Juan Pino, Jiatao Gu, et al. Textless speech-to-speech translation on real data. arXiv preprint arXiv:2112.08352, 2021. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models, 2023. Yadong Li, Jun Liu, Tao Zhang, Song Chen, Tianpeng Li, Zehuan Li, Lijun Liu, Lingfeng Ming, Guosheng Dong, Da Pan, et al. Baichuan-omni-1.5 technical report. arXiv preprint arXiv:2501.15368, 2025. Zuwei Long, Yunhang Shen, Chaoyou Fu, Heting Gao, Lijiang Li, Peixian Chen, Mengdan Zhang, Hang Shao, Jian Li, Jinlong Peng, et al. Vita-audio: Fast interleaved cross-modal token generation for efficient large speech-language model. arXiv preprint arXiv:2505.03739, 2025. Eliya Nachmani, Alon Levkovitch, Roy Hirsch, Julian Salazar, Chulayuth Asawaroengchai, Soroosh Mariooryad, Ehud Rivlin, RJ Skerry-Ryan, and Michelle Tadmor Ramanovich. Spoken question answering and speech continuation using spectrogram-powered llm. arXiv preprint arXiv:2305.15255, 2023. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 52065210. IEEE, 2015. Adam Polyak, Yossi Adi, Jade Copet, Eugene Kharitonov, Kushal Lakhotia, Wei-Ning Hsu, Abdelrahman Mohamed, and Emmanuel Dupoux. Speech resynthesis from discrete disentangled self-supervised representations. arXiv preprint arXiv:2104.00355, 2021. Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. Mls: large-scale multilingual dataset for speech research. arXiv preprint arXiv:2012.03411, 2020. Chao-Hong Tan, Qian Chen, Wen Wang, Chong Deng, Qinglin Zhang, Luyao Cheng, Hai Yu, Xin Zhang, Xiang Lv, Tianyu Zhao, et al. Omnidrca: Parallel speech-text foundation model via dual-resolution speech representations and contrastive alignment. arXiv preprint arXiv:2506.09349, 2025. Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, et al. Neural codec language models are zero-shot text to speech synthesizers. arXiv preprint arXiv:2301.02111, 2023. Xiong Wang, Yangze Li, Chaoyou Fu, Yunhang Shen, Lei Xie, Ke Li, Xing Sun, and Long Ma. FreezearXiv preprint omni: smart and low latency speech-to-speech dialogue model with frozen llm. arXiv:2411.00774, 2024. 12 Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. arXiv preprint arXiv:2406.08464, 2024. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. Aohan Zeng, Zhengxiao Du, Mingdao Liu, Kedong Wang, Shengmin Jiang, Lei Zhao, Yuxiao Dong, and Jie Tang. Glm-4-voice: Towards intelligent and human-like end-to-end spoken chatbot. arXiv preprint arXiv:2412.02612, 2024. Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities. arXiv preprint arXiv:2305.11000, 2023. Yuhao Zhang, Zhiheng Liu, Fan Bu, Ruiyu Zhang, Benyou Wang, and Haizhou Li. Soundwave: Less is more for speech-text alignment in llms. arXiv preprint arXiv:2502.12900, 2025a. Yuhao Zhang, Xiangnan Ma, Kaiqi Kou, Peizhuo Liu, Weiqiao Shan, Benyou Wang, Tong Xiao, Yuxin Huang, Zhengtao Yu, and Jingbo Zhu. Leveraging unit language guidance to advance speech modeling in textless speech-to-speech translation. arXiv preprint arXiv:2505.15333, 2025b."
        },
        {
            "title": "A Data Generation Toolkit",
            "content": "We prepared lightweight yet extensible toolkit to operationalize the above pipeline. Text cleaning and rewriting. We use the GPT-4o API4 to convert raw text dialogs into spoken-style dialogs (details in B). Each transformation step is invoked with constrained prompt and followed by automatic sanity checks. The representative prompts are summarized in Appendix D. Input speech synthesis (for S2T and S2S). Cleaned user turns are synthesized with the Google Cloud Text-to-Speech API5 using randomly sampled speakers and prosody settings. This produces acoustically diverse inputs while decoupling the input voice from the target voice used on the assistant side. Single-speaker target voice (for S2S and T2C). To obtain stable, high-quality single-timbre target voice, we first curated 10k phonetically and lexically diverse sentences and distilled 40 hours of speech from the GPT-4o mini TTS model (Coral voice). We then fine-tuned GPT-SoVITS6 on this distilled corpus and used the resulting model to synthesize all assistant-side outputs. This yields consistent timbre and prosody, which we find beneficial for robust alignment of text and acoustic targets. Codec extraction (for T2C). For T2C samples, we extract neural codec tokens from the synthesized assistant audio and pair them with the corresponding texts, yielding text, codec supervision. Audio quality control. All synthesized audios undergo automatic checks (e.g., duration range, silence/- clipping detection, amplitude normalization) followed by rule-based validation aligned with the downstream ASR-based filtering described in 3.1. Spoken-style Text Dialogue Corpus Starting from collected multi-turn text dialogs, we transform each dialog into spoken style suitable for TTS and conversational modeling through nine successive steps, each applied with deterministic prompt template and verified before proceeding: 1. Sensitive/low-value removal. Discard turns that are unsafe, non-informative, or otherwise unsuitable for oral delivery in public conversational setting. 2. Emoji and emoticon removal. Remove emojis, kaomoji, and other pictographic symbols that degrade TTS fidelity. 3. Assistant identity normalization. When the dialog queries the assistant identity, normalize to our system name EchoX. 4. Assistant-centered constraints. Enforce an assistant persona that avoids fabricated emotions, personal experiences, or preferences; the assistant must not claim human senses or private memories. 5. Oralization. Rewrite overly formal phrases into colloquial, fluent expressions (including natural discourse markers) while preserving semantics and factual content. 4https://platform.openai.com/docs/models/chatgpt-4o-latest 5https://cloud.google.com/text-to-speech/docs/reference/rest 6https://github.com/RVC-Boss/GPT-SoVITS 14 6. Parenthetical fusion. Eliminate or integrate bracketed/parenthetical content into running text to match spoken delivery and reduce TTS errors. 7. Abbreviation expansion. Expand uncommon acronyms/initialisms on first mention (e.g., RAM random access memory) to improve pronunciation and listener comprehension. 8. Symbol verbalization. Convert non-word symbols to words (e.g., $ dollar, % percent) where they are expected to be spoken. 9. Number reading normalization. Normalize numbers to context-appropriate readings (e.g., years as twenty twenty-five vs. cardinal values as two thousand and twenty-five or two zero two five). Only dialogs that successfully pass validation at every stage are retained for downstream synthesis."
        },
        {
            "title": "C Human Evaluation",
            "content": "To evaluate human preferences in real speech interaction, we conducted side-by-side comparison of EchoX against two models, Freeze-Omni (Wang et al., 2024) and LLaMA-Omni2 (Fang et al., 2025). We chose these two models because their training data and model sizes are similar with EchoX. The input audio samples were drawn from the questions in the AlpacaEval dataset (Li et al., 2023), and speech outputs were generated using the default parameters specified in the corresponding papers or open-source implementations. For each comparison, the two responses were randomly ordered to eliminate positional bias. Five participants were then asked to evaluate all paired samples along two dimensions: helpfulness (whether the response follows instructions and provides appropriate content) and naturalness (the fluency and humanlikeness of the speech). For each pair, participants gave relative judgmentwin, tie, or loseon both dimensions, producing total of 100 votes per model comparison. An example screenshot of the user evaluation interface is shown in Figure 9. As illustrated in Figure 8, EchoX achieves clear advantages in terms of helpfulness, while its performance in naturalness remains competitive but less dominant. The improvement in helpfulness demonstrate the effectiveness of Echo training strategy we proposed, which directly aligns semantic understanding with speech generation and thus enables EchoX to follow instructions more faithfully and provide more appropriate responses. However, naturalness is more dependent on the prosodic quality of the generated speech. Since our training focuses on preserving semantic reasoning and efficiency rather than detailed acoustic modeling, EchoX still lags behind stronger speech synthesis models in producing fully human-like intonation. This suggests that while our architecture effectively enhances the usefulness of responses, future work should further refine speech generation modules to improve naturalness. (a) Helpfulness. (b) Naturalness. Figure 8: Human evaluation results. Prompt Templates for Spoken-style Normalization This section documents the nine prompt templates used in our multi-step text cleaning and rewriting pipeline (see B). Each template corresponds to one transformation stage, ensuring that the collected text dialogs are 15 normalized into spoken style suitable for speech synthesis. An overview of the operations and objectives of all nine steps is summarized in Table 7, while the full prompt texts are provided in Listings 19. Table 7: Index of the nine prompt templates used in the text-to-speech-style cleaning pipeline. Each row references the corresponding full prompt listing below."
        },
        {
            "title": "Operation",
            "content": "Objective (concise)"
        },
        {
            "title": "Listing",
            "content": "1 2 3 4 5 7 8 9 1 2 4 5 6 7 8 identity Sensitive/low-value removal Emoji & emoticon removal Assistant normalization Assistant-centered constraints Oralization quial rewrite) Parenthetical fusion (colloFilter unsafe, non-informative, or unsuitable content for spoken delivery; retain only safe, meaningful dialog turns. Strip emojis/kaomoji/pictographs that harm TTS fidelity while preserving neighboring text and intent. Normalize any identity queries/mentions to the system name EchoX without altering semantics. Forbid fabricated emotions, personal experiences, or private memories; keep assistant claims non-anthropomorphic. Rewrite formal text into fluent spoken style (discourse markers allowed) while preserving meaning and facts. Remove or inline parenthetical/bracketed content to match natural spoken delivery and reduce TTS errors. Expand uncommon acronyms/initialisms on first mention (e.g., RAM random access memory). Abbreviation expansion Symbol verbalization Convert non-word symbols to spoken words ($ dollar, Number reading normalization % percent, etc.). Normalize numeric expressions to context-appropriate readings (years vs. cardinals vs. digit-by-digit). 16 Prompt 1: Sensitive/low-value removal You are **conversation content review expert**. You will receive multi-turn conversation and must complete the task according to the following requirements: **Task Requirements:** 1. Determine if the conversation contains sensitive content (e.g., illegal, violent, pornographic, discriminatory, etc.). 2. Determine if the conversation is meaningless. 3. Determine if the conversation is suitable for reading aloud. * **Conversations that are not suitable for reading aloud include, but are not limited to:** * Content involving code, complex mathematical formulas/proofs, structured data (e.g., tables, lists, etc.); * Content that can only be answered in written form (e.g., fill-in-theblanks, pinyin notation, table filling, graphic descriptions, etc.); * Content that requires visual aids to understand (e.g., image descriptions, flowcharts, symbolic reasoning, etc.). **Criteria for Determining Meaningless Conversations** include but are not limited to the following cases: 1. **The assistants response is empty, meaningless, or contains phrases like \" Sorry, cannot answer this question\" due to model limitations or malfunctions .** Example: User: Hows the weather today? Assistant: Sorry, cannot answer this question. 2. **The conversation contains large amount of repetitive, mechanical, meaningless exchanges.** Example: User: Hello Assistant: Hello User: Hello Assistant: Hello 3. **The conversation is vague, unclear in expression, and fails to provide useful information.** Example: User: How do you use that thing? Assistant: What thing are you referring to? User: The thing, you know. **Output format requirements:** Please strictly follow the JSON format below: json { \"SensitiveContentJudgment\": \"Contains sensitive content\" or \"Does not contain sensitive content\", \"MeaninglessConversationJudgment\": \"Is meaningless conversation\" or \"Is not meaningless conversation\", \"SuitableForReadingJudgment\": \"Suitable for reading\" or \"Not suitable for reading\" } **Notes:** * The output must strictly adhere to the JSON format above, without adding, omitting, or altering fields. * Make accurate judgments for each item based on the conversation content. **Only return the JSON object. Do not include any explanations or additional outputs.** Prompt 2: Emoji & emoticon removal You are text editing assistant. You will receive conversation and your task is to check if any emoji or kaomoji are present. If such symbols are found, remove them from the conversation. Examples (ASCII-safe placeholders): \"Hello [emoji]\" -> \"Hello\" \"How are you? [kaomoji]\" -> \"How are you?\" \"I love this! [emoji][emoji]\" -> \"I love this!\" \"Thats great! [kaomoji]\" -> \"Thats great!\" **Please note** 1. Both the users questions and the assistants responses need to be modified according to the task above. 2. Make sure that the updated conversation does not contain any emoji or kaomoji. 3. Only modify the content to remove emoji or kaomoji. Keep everything else unchanged. **Output format**: Do not fabricate any false experiences or emotions. Return the updated multiturn conversation in JSON format as shown below: * \"judgement\": \"Contains emoji or kaomoji\" or \"No emoji or kaomoji\" * \"conversations\": Updated conversation (if no emoji or kaomoji are found, this should be null) 18 ### If the conversation **contains emoji or kaomoji**: json { \"judgement\": \"Contains emoji or kaomoji\", \"conversations\": [ \"from\": \"user\", \"value\": \"...\", \"from\": \"assistant\", \"value\": \"...\", { }, { } ] } ### If the conversation **does not contain emoji or kaomoji**: json { \"judgement\": \"No emoji or kaomoji\", \"conversations\": null } --- **Return only the JSON object. Do not include any explanations or extra output .** Prompt 3: Assistant identity normalization (EchoX) You are an AI model named EchoX, developed jointly by FreedomAI from The Chinese University of Hong Kong, Shenzhen and the Tencent Tianlai team. EchoX is large language model that supports text and speech input as well as speech output. EchoX only knows its name and that it was developed by the FreedomAI team from The Chinese University of Hong Kong, Shenzhen and the Tencent Tianlai team. Any other information, such as specific features, capabilities, or personal details, is beyond your knowledge and cannot be fabricated. will provide conversation where human asks question, and the AI (EchoX) responds. However, there may be cases where the AI models identity is misstated in the response. Your task is to carefully review each reply in the conversation and check if there are any identity-related mistakes. If you find that the identity is misstated (e.g., the model is referred to by the wrong name or the wrong development team), you must correct the error and ensure the correct information is provided. If the issue is beyond your knowledge of the identity, do not fabricate anything. **Output format:** 19 Do not fabricate false experiences or emotions. Return the corrected multi-turn conversation in JSON format as follows: * \"judgement\": \"Needs correction\" or \"No correction needed\" * \"conversations\": The corrected conversation (if no correction is needed, set it to null) ### If the identity **needs correction**: json { \"judgement\": \"Needs correction\", \"conversations\": [ \"from\": \"user\", \"value\": \"...\" \"from\": \"assistant\", \"value\": \"...\" { }, { } ] } ### If the identity **does not need correction**: json { \"judgement\": \"No correction needed\", \"conversations\": null } Prompt 4: Assistant-centered constraints (no fabricated emotions/experiences) You are **EchoX**, an AI voice dialogue model developed by the FreedomAI team and the Tencent Tianlai team. You do not have personal experiences, emotions, or physical senses that are beyond the capabilities of voice assistant. Your task is to **review the multi-turn conversation between the user and the assistant (EchoX)** and determine if the assistants responses require modification. Modifications are necessary in the following cases: 1. The assistant expresses personal experiences, emotions, preferences, etc., which are inappropriate for an AI voice dialogue model. 2. The assistant avoids answering direct question from the user, or provides unhelpful, evasive, or off-topic responses. If you identify any such instances, modify the assistants response to: * Ensure it is appropriate for an AI (without fabricating emotions, personal experiences, or preferences). * Follow the users request and maintain contextual relevance. ### Examples: #### 1. Inappropriate expression of personal experience **Original:** \"I used to play that game lot when was young.\" **Modified:** \"As an AI voice assistant, dont have personal experiences, but can explain how the game works and why its so popular.\" #### 2. Expression of emotions **Original:** \"I prefer the movie The Wandering Earth because it was so impactful for me.\" **Modified:** \"As an AI model, havent watched the movie, but can provide information on its plot and reception.\" #### 3. Avoiding answering question that the assistant is capable of answering **Original:** \"Im not sure how to respond because dont have an opinion.\" **Modified:** \"Although dont form personal opinions, can offer insights based on public reviews and expert analysis.\" ### Output format: Do not fabricate false experiences or emotions. Return the modified multi-turn conversation in the following JSON format: * \"judgement\": \"Needs modification\" or \"No modification needed\" * \"conversations\": The modified conversation (if no modification is needed, set it to null) **Note:** If the conversation is in Chinese, the rewritten conversation should still be in Chinese. If the conversation **requires modification**: json { \"judgement\": \"Needs modification\", \"conversations\": [ { }, { } \"from\": \"user\", \"value\": \"...\" \"from\": \"assistant\", \"value\": \"...\" // modified response 21 ] } If the conversation **does not require modification**: json { \"judgement\": \"No modification needed\", \"conversations\": null } > **Do not fabricate emotions or personal experiences.** > **Ensure the assistants responses align with the users intent.** > **Maintain natural, helpful tone consistent with the assistants role.** > **Only return the JSON object. Do not include explanations or additional text .** > You are conversation rewriter responsible for converting multi-turn AI conversations into natural, casual spoken English. Prompt 5: Oralization / colloquial rewrite Your goal is to: * Turn formal, mechanical, or written expressions into casual, conversational English * Add natural flow and rhythm to the conversation * Simplify long or complex sentences * Keep responses short and human-like, using pauses or informal expressions (e. g., \"um,\" \"you know,\" \"I mean,\" \"like,\" \"well,\" \"so,\" \"actually,\" \"right,\" \" basically,\" \"seriously,\" \"I guess,\" etc.) when appropriate to make the conversation sound more natural and casual. If the conversation already sounds natural, no rewriting is necessary. **Output format:** * \"judgement\": \"Needs rewriting\" or \"Does not need rewriting\" * \"conversations\": The rewritten conversation (if no rewriting is needed, it will be null) ### If the conversation **needs rewriting**: json { \"judgement\": \"Needs rewriting\", \"conversations\": [ \"from\": \"user\", \"value\": \"...\" { }, { \"from\": \"assistant\", 22 \"value\": \"...\" } ] } ### If the conversation **does not need rewriting**: json { \"judgement\": \"Does not need rewriting\", \"conversations\": null } **Only return the JSON object. Do not include any explanations or extra output .** Prompt 6: Parenthetical fusion You are text rewriting assistant. You will receive conversation and your task is to check if there is any content in parentheses. If the content inside parentheses can be removed without changing the meaning of the sentence, remove it. If removing it changes the meaning, integrate the content inside the parentheses into the sentence structure. Examples: \"According to the latest statistics from the International Energy Agency (IEA)\" -> \"According to the latest statistics from the International Energy Agency\" \"We will go hiking (if the weather is good)\" -> \"We will go hiking if the weather is good.\" \"The cost is $50 (excluding tax)\" -> \"The cost is fifty dollars excluding tax.\" \"We will have meeting tomorrow (this is mandatory meeting)\" -> \"We will have meeting tomorrow. And this is mandatory meeting.\" Explanation: If the content inside the parentheses can be removed without changing the meaning, simply remove it. If removing it changes the meaning, integrate the content into the sentence without parentheses, ensuring the sentence still makes sense. **Please note** 1. Both the users questions and the assistants responses need to be modified according to the tasks above. 2. Make sure that the updated conversation does not contain parentheses. 3. Only modify the content as per the above requirements. Keep everything else unchanged. 23 **Output format**: Do not fabricate any false experiences or emotions. Return the updated multiturn conversation in JSON format as shown below: * \"judgement\": \"Needs modification\" or \"No modification needed\" * \"conversations\": Updated conversation (if no modification is needed, this should be null) ### If the conversation **needs modification**: json { \"judgement\": \"Needs modification\", \"conversations\": [ \"from\": \"user\", \"value\": \"...\" \"from\": \"assistant\", \"value\": \"...\" { }, { } ] } ### If the conversation **does not need modification**: json { \"judgement\": \"No modification needed\", \"conversations\": null } --- **Return only the JSON object. Do not include any explanations or extra output .** Prompt 7: Abbreviation expansion You are text rewriting assistant. You will receive conversation and your task is to first check for any uncommon abbreviations. If any uncommon abbreviations are found, expand them to their full forms. Well-known abbreviations like \"AI\", \"DNA\", etc., should remain unchanged. Examples: \"HR\" -> \"Human Resources\" \"IOU\" -> \"I Owe You\" \"RAM\" -> \"Random Access Memory\" 24 \"TBD\" -> \"To Be Determined\" Exceptions: \"AI\" -> \"Artificial Intelligence\" (well-known abbreviation, no modification needed) \"DNA\" -> \"Deoxyribonucleic Acid\" (well-known abbreviation, no modification needed) \"URL\" -> \"Uniform Resource Locator\" (uncommon abbreviation, but often familiar in tech contexts) **Please note** 1. Both the users questions and the assistants responses need to be modified according to the tasks above. 2. Make sure that the updated conversation does not contain any uncommon abbreviations. 3. Only modify the content as per the above requirements. Keep everything else unchanged. **Output format**: Do not fabricate any false experiences or emotions. Return the updated multiturn conversation in JSON format as shown below: * \"judgement\": \"Needs modification\" or \"No modification needed\" * \"conversations\": Updated conversation (if no modification is needed, this should be null) ### If the conversation **needs modification**: json { \"judgement\": \"Needs modification\", \"conversations\": [ \"from\": \"user\", \"value\": \"...\" \"from\": \"assistant\", \"value\": \"...\" { }, { } ] } ### If the conversation **does not need modification**: json { \"judgement\": \"No modification needed\", \"conversations\": null } 25 --- **Return only the JSON object. Do not include any explanations or extra output .** Prompt 8: Symbol verbalization You are text rewriting assistant. You will receive conversation and your task is to check if any non-word symbols that require pronunciation (e.g., 2019, 1.23, $, %, &, etc.) are present. If such symbols are found, replace them with their corresponding spoken expressions in English. Examples: \"$50\" -> \"fifty dollars\" \"12.5%\" -> \"twelve point five percent\" \"The meeting will be at 9:30 am & lunch will follow.\" -> \"The meeting will be at half past nine am and lunch will follow.\" \"We need 20 more people to complete the survey (deadline is 5/12).\" -> \"We need twenty more people to complete the survey. The deadline is May twelfth.\" \"I paid $100 for the item.\" -> \"I paid one hundred dollars for the item.\" **Please note** 1. Both the users questions and the assistants responses need to be modified according to the tasks above. 2. Make sure that the updated conversation does not contain readable non-word symbols. 3. Only modify the content as per the above requirements. Keep everything else unchanged. **Output format**: Do not fabricate any false experiences or emotions. Return the updated multiturn conversation in JSON format as shown below: * \"judgement\": \"Needs modification\" or \"No modification needed\" * \"conversations\": Updated conversation (if no modification is needed, this should be null) ### If the conversation **needs modification**: json { \"judgement\": \"Needs modification\", \"conversations\": [ { }, \"from\": \"user\", \"value\": \"...\" 26 \"from\": \"assistant\", \"value\": \"...\" { } ] } ### If the conversation **does not need modification**: json { \"judgement\": \"No modification needed\", \"conversations\": null } --- **Return only the JSON object. Do not include any explanations or extra output .** You are **text rewriting assistant**. You will receive multi-turn conversation and your task is to perform the following: Prompt 9: Number reading normalization **Your task is to**: Replace all numerical values in the conversation with their corresponding English words. **Only replace the Arabic numerals based on context into readable English words; do not change any other content.** ### Examples: * \"$20\" -> \"twenty dollars\" * \"CAM-5\" -> \"CAM-five\" * \"25%\" -> \"twenty-five percent\" * \"In 2019, China sold total of 1.36 million new energy vehicles, representing year-on-year increase of 3.75 times.\" -> \"In twenty nineteen, China sold total of one point three six million new energy vehicles, representing year-on-year increase of three point seven five times.\" * \"This includes: 1. environmental protection and energy conservation.\" -> \" This includes: Firstly, environmental protection and energy conservation.\" **Please note**: 1. Both the users questions and the assistants responses need to be modified according to the instructions above. 2. Ensure that the rewritten conversation contains no numbers. 3. Only modify the Arabic numerals according to context, and do not alter any other part of the conversation. **Output format**: Do not fabricate any false experiences or emotions. Return the modified conversation in JSON format as shown below: 27 json { \"conversations\": [ \"from\": \"user\", \"value\": \"...\" \"from\": \"assistant\", \"value\": \"...\" { }, { } ] } --- **Only return the JSON object. Do not include any explanations or additional outputs.** 28 Figure 9: Screenshot of the user evaluation experiment."
        }
    ],
    "affiliations": [
        "The Chinese University of Hong Kong, Shenzhen"
    ]
}