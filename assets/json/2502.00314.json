{
    "paper_title": "A Study on the Performance of U-Net Modifications in Retroperitoneal Tumor Segmentation",
    "authors": [
        "Moein Heidari",
        "Ehsan Khodapanah Aghdam",
        "Alexander Manzella",
        "Daniel Hsu",
        "Rebecca Scalabrino",
        "Wenjin Chen",
        "David J. Foran",
        "Ilker Hacihaliloglu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The retroperitoneum hosts a variety of tumors, including rare benign and malignant types, which pose diagnostic and treatment challenges due to their infrequency and proximity to vital structures. Estimating tumor volume is difficult due to their irregular shapes, and manual segmentation is time-consuming. Automatic segmentation using U-Net and its variants, incorporating Vision Transformer (ViT) elements, has shown promising results but struggles with high computational demands. To address this, architectures like the Mamba State Space Model (SSM) and Extended Long-Short Term Memory (xLSTM) offer efficient solutions by handling long-range dependencies with lower resource consumption. This study evaluates U-Net enhancements, including CNN, ViT, Mamba, and xLSTM, on a new in-house CT dataset and a public organ segmentation dataset. The proposed ViLU-Net model integrates Vi-blocks for improved segmentation. Results highlight xLSTM's efficiency in the U-Net framework. The code is publicly accessible on GitHub."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . e [ 1 4 1 3 0 0 . 2 0 5 2 : r A Study on the Performance of U-Net Modifications in Retroperitoneal Tumor Segmentation Moein Heidaria, Ehsan Khodapanah Aghdamb, Alexander Manzellac, Daniel Hsud,e, Rebecca Scalabrinof,g, Wenjin Chenh, David J. Foranh, and Ilker Hacihaliloglui,j aSchool of Biomedical Engineering, University of British Columbia, British Columbia, Canada bIndependent Researcher, Tabriz, Iran cRutgers Robert Wood Johnson Medical School, New Brunswick, NJ, United States dBeth Israel Deaconess Medical Center, Boston, MA, United States eHarvard Medical School, Boston, MA, United States fWeill Cornell Medical School, New York, NY, United States gMemorial Sloan Kettering Cancer Center, New York, NY, United States hCenter for Biomedical Imaging and Informatics, Rutgers Cancer Institute, New Brunswick, NJ, United States iDepartment of Medicine, University of British Columbia, British Columbia, Canada jDepartment of Radiology, University of British Columbia, British Columbia, Canada"
        },
        {
            "title": "ABSTRACT",
            "content": "The retroperitoneum presents diverse array of pathologies, encompassing both rare benign tumors and malignant neoplasms, which can be either primary or metastatic. Diagnosing and treating these tumors can be challenging due to their infrequency, late presentation, and their close association with critical structures in the retroperitoneal space. Estimating the volume of retroperitoneal tumors is often challenging due to their large dimensions and irregular shape. Automatic semantic segmentation of tumors is crucial for comprehensive medical image analysis, impacting accurate cancer diagnosis and treatment planning due to manual segmentations time-consuming and tedious nature. U-Net and its variants incorporate the various convolutions of Vision Transformer (ViT) designs and have delivered top-notch results in 2D and 3D medical image segmentation tasks across diverse imaging modalities. ViTs excel at extracting global information, yet face scalability issues due to high computational and memory costs, making them challenging to use in medical applications with hardware constraints. Recently, architectures like the Mamba State Space Model (SSM) have been developed to address the quadratic computational demands of Transformers. Additionally, Extended Long-Short Term Memory (xLSTM) has emerged as noteworthy successor to traditional LSTMs, offering competitive edge in sequence modeling. Like SSMs, xLSTM excels at managing long-range dependencies while maintaining linear computational and memory efficiency. This study aimed to evaluate the U-Net-based modifications from the convolutional neural networks (CNNs), ViT, mamba, and the new xLSTM companions over the newly introduced in-house CT dataset along publicly available organ segmentation dataset. Specifically, we introduce ViLU-Net designed by incorporating Vi-blocks into the encoder-decoder framework to advance biomedical image segmentation. The results point out the efficacy of xLSTM within the U-Net structure. The codes are publicly available at GitHub. Keywords: Retroperitoneal tumor, U-Net, Mamba, Transformer, xLSTM 1. INTRODUCTION AND RELATED WORKS Medical image segmentation plays pivotal role in medical imaging by partitioning images into meaningful structures, thereby enabling accurate diagnosis, treatment planning, and disease monitoring.1 By isolating regions of interest, such as organs, tissues, tumors, or pathological areas, segmentation enhances the precision of image analysis and improves the efficiency of radiological and clinical workflows. The significance of automated Corresponding author: I. Hacihaliloglu (ilker.hacihaliloglu@ubc.ca) medical image segmentation is particularly noteworthy, as it addresses the limitations of manual segmentation, which is often labor-intensive, subjective, and susceptible to human error. The significance of automated medical image segmentation lies in its ability to overcome the limitations of manual segmentation, which is often timeconsuming, subjective, and reliant on extensive domain knowledge and technical expertise.2 By utilizing advanced algorithms and machine learning techniques, automated segmentation ensures consistent, reproducible, and efficient results, thereby enhancing the reliability of medical imaging and contributing to improved clinical outcomes.3 Tumor segmentation is critical task in medical imaging, particularly within the field of oncology. It involves delineating tumor boundaries in imaging modalities such as MRI, CT, and PET scans.1 Accurate tumor segmentation is essential for variety of clinical applications, including diagnosis, treatment planning, and evaluation of treatment response. By precisely identifying the size, shape, and location of tumors, healthcare professionals can make more informed decisions regarding surgical interventions, radiotherapy, and chemotherapy. Moreover, tumor segmentation enables the monitoring of tumor progression or regression over time, facilitating personalized treatment adjustments and improving patient outcomes.4 The automation of the segmentation process significantly enhances efficiency while reducing inter-observer variability, ensuring more reliable and standardized clinical evaluations. The segmentation of retroperitoneal tumors is particularly critical due to the regions complex anatomy, which includes major blood vessels, kidneys, and adrenal glands. These tumors, often sarcomas or lymphomas, pose significant imaging challenges due to their size, shape, location, and the involvement of diverse tissue types.5 Accurate segmentation is essential for delineating tumor boundaries and planning surgical interventions, which are typically the primary treatment approach. Precise tumor segmentation not only minimizes the risk of damage to adjacent vital organs but also reduces surgical complications and postoperative risks. Furthermore, it plays key role in assessing treatment response and detecting tumor recurrence, ultimately contributing to improved prognosis and quality of life for patients. Automating this process further enhances these outcomes by delivering rapid and consistent results, thereby supporting timely and effective clinical decision-making. Thanks to advancements in deep learning over the past decade, convolutional neural networks (CNNs) have become the primary tool for (medical) segmentation tasks. They can extract feature representations from inputs, eliminating the need for hand-crafted features in image segmentation, and their superior performance and accuracy make them the top choice in this field. U-Net6 by Ronneberger et al. is one of the standard de facto architectures. It is fully convolutional network utilized in various tasks and grand challenges in biomedical image segmentation.1, 7, 8 Due to the modular, multi-scale, and symmetrical design (encoder-decoder structure) of U-Net and its partial ability to preserve the localization representation within the skip connections, numerous modifications have been proposed since its introduction, such as Attention U-Net,9 UNet++,10 UNet3+,11 and H-DenseUNet.12 However, the locality of convolution operation causes the limited and fixed receptive field and the inherent inductive bias, which imposes the struggle to capture long-range dependencies and contextual information that span larger image areas. Notably, the current success of sequence modeling in natural language processing (NLP) and large Language Models further extended the vision recognition tasks by capturing global information using the self-attention mechanism. The Vision Transformer (ViT),13 as vision-oriented adaptation of the Transformer architecture, has demonstrated strong ability to capture global context. This capability has inspired the development of U-Net-like structures incorporating either pure Transformer-based designs or hybrid CNN-Transformer modifications within encoder-decoder frameworks, such as TransUNet14 and Swin-Unet,15 among others. Furthermore, Transformer-driven enhancements to the traditional U-Net structure, including improved skip connection designs like HiFormer16 and Laplacian-Former,17 have achieved superior performance compared to the vanilla U-Net and other CNN-based U-shaped structures, particularly in multi-scale representation, crucial for accurate medical image segmentation.18 However, both Transformer-based and hybrid CNN-Transformer models are generally computationally intensive or still heavily depend on CNN backbone, as the self-attention mechanism at their core scales quadratically with input size, resulting in higher resource demands. Additionally, due to the absence of spatial inductive bias, these models require larger datasets for effective training, making them more challenging to optimize.19 Recently, State Space Models (SSM) in the field of NLP have emerged as highly promising method for long sequence modeling with linear complexity, positioning it as strong competitor to Transformer architectures.20, 21 Moreover, SSMs support efficient computation through recurrence or convolution operations, enabling linear or In addition, SSMs near-linear scaling with sequence length, which significantly reduces computational costs. 2 provide modeling capabilities comparable to Transformers while maintaining this linear scalability.22 Inspired by recent advancements in SSMs, Mamba (Selective State Space Model)23 introduces simple yet effective selection mechanism that filters irrelevant information while preserving the necessary and relevant data. Building on this advancement, Visual Mamba (VMamba)24 proposes new approach to long-range visual representation learning within the framework of SSMs. This architecture includes an innovative scanning mechanism that effectively addresses direction-sensitive issues in the visual data. Other Mamba-based models have also emerged in the field of computer vision,2426 leveraging these principles. For instance, U-Mamba,27 designed for biomedical image segmentation, synergizes U-Net with Mambas capabilities to capture intricate and broad semantics, enhancing model performance. U-Mamba combines CNNs and SSMs in hybrid framework, capitalizing on CNNs proficiency in local feature extraction and SSMs ability to understand extensive relationships within images. This architecture effectively manages long-range data and adapts well to variety of segmentation tasks. On the other hand, the Extended Long Short-Term Memory (xLSTM)28 model represents significant advancement in sequence modeling, addressing longstanding limitations of the traditional Long Short-Term Memory (LSTM) through the introduction of exponential gating and parallelizable matrix memory structure. This innovation has revitalized the relevance of LSTMs in the era of LLMs, showing competitive performance compared In the domain of computer vision, Vision LSTM (ViL)29 has been to models like Transformers and SSMs. introduced as versatile backbone built upon xLSTM blocks. Like vision-specific adaptations of SSMs, ViL achieves linear computational and memory complexity with respect to sequence length and demonstrates superior performance, making it particularly well-suited for tasks requiring high-resolution image processing, such as medical image segmentation. An example of this application is xLSTM-UNet,30 UNet-based deep learning architecture that incorporates Vision-LSTM (xLSTM) as its core. By combining the local feature extraction capability of convolutional layers with the long-range dependency modeling of xLSTM, xLSTM-UNet provides robust framework for comprehensive medical image analysis. In this study, we look closer at the performance of these state-of-the-art (SOTA) methods and highlight the potential of xLSTM-based architecture to offer improved accuracy and efficiency across an in-house CT dataset. Specifically, our contributions are ❶ We present novel CT dataset specifically focusing on retroperitoneal tumors. This dataset includes 82 cases with 3D scans, accompanied by meticulously annotated segmentation maps provided by expert radiologists. ❷ We conduct comprehensive comparison of SOTA deep learning methods for segmenting these images. This includes evaluating convolutional U-Net-based methods and their variants that incorporate ViTs and Mamba modifications. ❸ We introduce U-Net-xLSTM-based network and demonstrate that our approach performs on par with or superior to most contemporary methods. Additionally, our model offers significant reduction in complexity. ❹ We fully open-source our codes, making them accessible to the research community to foster further advancements in the field. 2. METHOD The evolution of medical image segmentation has transitioned from solely relying on CNNs to exploring hybrid models that combine CNNs with ViTs. Current research emphasizes creating architectures that are not only high-performing but also computationally efficient and suitable for deployment on systems with limited resources. Despite the advantages of transformers in capturing global dependencies, they face challenges like high computational and memory demands. Therefore, it is crucial to develop algorithms that balance performance with computational efficiency, ensuring scalability in real-world scenarios. In clinical settings with limited computing resources, models must accurately delineate target structures from volumetric medical data while minimizing computational power and memory usage, which is crucial for cost-effective healthcare solutions. Akin to xLSTM-UNet,30 our design adopts standard architecture similar to that of traditional U-Net. The input data is initially processed through convolutional layer for down-sampling. Following this, the core of the encoder is composed of multiple layers built with the xLSTM modules, designed to effectively extract local features while modeling long-range dependencies. Specifically,the integration of CNNs with Vision-xLSTMs within U-shaped framework for medical image segmentation tasks are presented. CNNs capture fine-grained textural details and local patterns, while Vision-xLSTM blocks encode the global context efficiently. 3 Figure 1. (a) Schematic representation of the proposed method, ViLU-Net, (b) the ViL block, (c) convolutional stem, and (d) Up Sampler and Down Sampler blocks, where IN stands for Instance Normalization operation."
        },
        {
            "title": "2.1 ViL Block",
            "content": "The Vision x-LSTM (ViL) block is the core component of the proposed model, designed to capture both spatial and temporal dependencies within feature maps. Each ViL block incorporates novel layer of mLSTM (modified long-short-term memory), which extends the capabilities of traditional LSTMs by introducing mechanisms that enhance spatial feature integration and long-range contextual learning.28 The ViL block (see Figure 1(b)) operates on sequence of patch embeddings, which are generated by splitting the input image into non-overlapping patches and projecting them linearly into feature space. Each ViL block processes its input features Fl using an mLSTM-based mechanism for spatial-temporal feature integration as Ct = ftCt1 + itvtk , nt = ftnt1 + itkt, (1) where Ct and nt are the cell and normalizer states, respectively, and ft, it, ot represent the forget, input, and output gates. The hidden state ht is computed as: qt, 1(cid:1) , Here, inspired by attention models,31 qt, kt, vt are the query, key, and value inputs derived from the linear transformations of the input features. (2) ht = ot ht. ht = Ctqt max (cid:0)n"
        },
        {
            "title": "2.2 ViLU-Net",
            "content": "The ViLU-Net integrates the ViL framework into U-shaped architecture designed for semantic segmentation tasks. This hybrid design combines the strengths of ViL blocks, which capture long-range spatial and temporal dependencies, with the hierarchical structure of U-Net, enabling precise feature localization and semantic segmentation. The model comprises three main components: convolutional stem, an encoder-decoder structure, and segmentation head. 4 The model begins with convolutional stem that processes the input image RW HC into base feature map as F0 = LeakyReLU(IN(Conv(X))), where Conv() is convolutional layer, IN() represents instance normalization, and LeakyReLU() introduces non-linearity. This step generates robust feature representation for subsequent processing. The encoder progressively downsamples the feature map while increasing its channel dimensions to capture multi-scale contextual information. We utilize xLSTM layers in our frameworks encoder and decoder, ensuring the effective modeling of long-range dependencies throughout the architecture. According to Figure 1 (a), our ViLU-Net consists of two successive ViL blocks, which is works as in Section 2.1. As illustrated in Figure 1, the core of ViL consists of alternating mLSTM blocks. These blocks are fully parallelizable and feature matrix memory combined with covariance update rule. The odd-numbered mLSTM blocks process patch tokens from the top left to the bottom right, while the even-numbered blocks process them from the bottom right to the top left. The processed features undergo normalization and projection = UpProjection(LayerNorm(ht)), followed by skip connections to preserve fine-grained details. After two ViL blocks, spatial dimensions are halved, and channel dimensions are doubled using down-sampling module Fl+1 = DownSampler(F ). The decoder path employs up-sampling modules to restore the spatial resolution of feature maps progressively. Skip connections from the encoder ensure that fine-grained details are preserved and merged with coarse features. For each decoder stage, the output features from the previous stage dec l+1 are combined with the corresponding skip connection Sl by dec l+1) + Sl. The architecture concludes with segmentation head, where the extracted feature map is processed through convolutional layer and Softmax activation to produce the final segmentation map. To ensure fair evaluation of the latest state-of-the-art models in deep learning for medical image segmentation, we examine four U-Net variants, each incorporating distinct architectural components: CNN, transformer, Mamba, and X-LSTM blocks. Specifically, we used SwinUNETR32 transformer-based method, along with the the Mamba-based method U-Mamba27 and nnU-Net33 as pure CNN-based method. = UpSampler(F dec 3. RESULTS Datasets. For this study, we utilize the dataset from the FLARE Challenge held at MICCAI 2022,34 along with our newly introduced dataset, to further validate and showcase the results achieved. The challenge dataset aimed to segment 13 different abdominal organs. These included the liver, spleen, pancreas, kidneys (both right and left), stomach, gallbladder, esophagus, aorta, inferior vena cava, adrenal glands (right and left), and the duodenum. We employed the widely utilized nn-UNet framework to train our proposed methods.33 We fully elaborate on the details of our new dataset in sections 3.2 and 3.2.1. Training and implementation procedures. Our network implementations were based on U-Mamba27 and xLSTM-UNet30 which both use the popular nnU-Net33 framework. Our implementation, developed using the PyTorch framework, runs on an NVIDIA V100 GPU with 32GB of memory. For both datasets, we set batch size of 2 and utilized the Adam optimizer with base learning rate of 0.005. The training process spans 300 epochs, employing loss function that combines Dice loss and cross-entropy loss with equal weighting. We used Dice Similarity Coefficient (DSC), Normalized Surface Distance (NSD), Hausdorff distance, and Intersection over Union (IoU) for our semantic segmentation tasks."
        },
        {
            "title": "3.1 Abdomen CT Dataset",
            "content": "Table 1 provides comprehensive comparison of different methods. U-Xlstm surpasses CNN, mamba and Transformer-based segmentation networks, achieving average DSC scores of 0.8594 on the abdomen CT. This advantage is consistently reflected across additional metrics, including HD distance, NSD, and IoU. Notably, the convolution-based nnU-Net framework demonstrates competitive performance, surpassing other approaches in several aspects. Moreover, in Figure 2, we showcase visual segmentation results from the abdomen CT dataset. The figure illustrates how the predicted segmentations closely correspond to the ground truth, accurately capturing the anatomical structures of various organs. Our xLSTM-based method demonstrates superior boundary preservation, effectively delineating complex organ shapes while maintaining intricate details. Compared to other models, our approach exhibits enhanced consistency and precision, highlighting its capability in segmenting abdominal soft tissues with high fidelity 5 Table 1. Results summary of 3D organ segmentation on the abdomen CT dataset."
        },
        {
            "title": "Methods",
            "content": "nnU-Net33 SwinUNETR32 U-Mamba27 ViLU-Net (Ours)"
        },
        {
            "title": "Organs in Abdomen CT",
            "content": "DSC 0.8469 0.8259 0.8480 0.8594 NSD 0.8881 0.8680 0.8883 0.8944 HD 12.54 27.99 17.12 11.98 IoU 0.9662 0.9551 0.9657 0.9716 Figure 2. Visualized segmentation examples of abdominal organ segmentation in CT. The ViLU-Net excels at differentiating intricate soft tissues within the abdominal region."
        },
        {
            "title": "3.2 Retroperitoneal Tumour Dataset",
            "content": "The new dataset we present comprises 82 cases annotated by expert radiologists at Rutgers University, each with over five years of experience. The metadata for each dataset includes an Anonymized Medical Record Number (MRN) and an Anonymized Accession Number. Additionally, each entry contains the presumed diagnosis based on imaging studies and the biopsy results. The images are provided in three views: sagittal, coronal, and axial. The cases are categorized into five disease categories: Metastatic Disease: 36 samples, Retroperitoneal Carcinoma: 22 samples, Hematologic Cancers: 20 samples, Benign: 5 samples, and Adrenal Carcinoma: 2 samples. The dataset is provided in the .nrrd format, suitable for handling multidimensional image data. Raw data includes various glitches essential for training deep learning models, such as different orientations and spacings, which are handled in the pre-processing stage. Figure 4 showcases some examples from the dataset, highlighting the variety of annotated cases. 3.2.1 Data Preprocessing In this study, an initial dataset comprising 85 cases of retroperitoneal tumor CT scans was utilized. However, 8 cases were excluded due to issues such as incorrect orientation, flawed scanning, or the absence of associated segmentation masks, resulting in final dataset of 77 cases. The corresponding segmentation masks were meticulously annotated by team of experienced physicians at Rutgers Hospital to ensure accuracy and reliability. The preprocessing pipeline involved converting the images to NumPy format, clipping the intensity values to the range [125, 275] to standardize the data, and normalizing each 3D image to the range [0, 1]. Subsequently, re-spacing paradigm was applied to harmonize the voxel spacing across all scans, ensuring consistency in spatial resolution for downstream analysis. 3.2.2 Results Table 2 presents comprehensive performance comparison of different methods, evaluated on our in-house dataset. In this study, the ViLU-Net model demonstrates superior performance by attaining the highest average DSC, NSD, IoU and the lowest average HD, highlighting its effectiveness in precisely delineating tumor boundaries and regions. Although the nnU-Net model performs competitively, particularly in DSC score, our method stands (a) Input Image (b) Ground Truth (e) U-Mamba27 Figure 3. Visual comparisons of different methods on our in house dataset. (d) SwinUNETR32 (c) nnU-Net33 (f) ViLU-Net (Ours) Figure 4. Sample visualization of our in-house dataset from the 3 different views along with the corresponding segmentation map out for its consistent superiority across all tumor segmentation metrics, establishing it as dependable choice for comprehensive and precise segmentations. Figure 3 presents qualitative analysis of segmentation results for our dataset, illustrating how the ViLU-Net model compares to other SOTA models. Each row in the figure represents distinct patients slice, showcasing the segmentation of the retroperitoneal tumor region. Notably, the ViLU-Net model shows more robust and smoother segmentations, closely aligning with the ground truth, and effectively capturing tumor boundaries with minimal false positives compared to other approaches. Specifically, in the case of both patients, the irregular shape of the small tumor regions poses considerable challenge. Several models, such as U-Mamba and SwinUNETR, generate additional segmented areas that are distant from the actual tumor location, suggesting the presence of false positives. Overall, our method exhibits enhanced robustness and consistency in its segmentation results, accurately capturing the tumor shape with improved precision. Table 2. Results summary of 3D tumour segmentation on the retroperitoneal tumour dataset."
        },
        {
            "title": "Methods",
            "content": "nnU-Net33 SwinUNETR32 U-Mamba27 ViLU-Net (Ours)"
        },
        {
            "title": "Retroperitoneal Tumour",
            "content": "DSC 0.9013 0.8310 0.7694 0.9309 NSD 0.9002 0.8478 0.7475 0.9292 HD 24.64 51.28 64.35 11.19 IoU 0.8322 0.7481 0.6620 0.8720 4. DISCUSSION AND FUTURE WORKS While our primary focus in this paper was on tumor segmentation and the introduction of novel dataset, there are several avenues we aim to explore in the future to further enhance the robustness and clinical applicability of our approach. In clinical applications, the integration of multiple imaging modalities, such as MRI, CT, and 7 PET, provides more comprehensive representation of patients condition. With the increasing availability of multi-modal datasets, the fusion of diverse imaging modalities has become crucial area of research, enabling richer and more informative feature space. Our future work will focus on developing robust multi-modal fusion techniques that effectively combine information from different medical imaging sources to enhance the accuracy and reliability of tumor segmentation. We aim to explore novel deep learning architectures that efficiently process and integrate multi-modal data while addressing challenges such as data heterogeneity, varying resolutions, and modality-specific noise. Specifically, we plan to explore advanced fusion strategies that integrate different architectures to achieve enhanced segmentation performance. As demonstrated in the paper, various architectures offer unique advantages, such as improved long-range dependency modeling, local feature preservation, and optimized computational complexity. Effectively combining these strengths through an optimal fusion approach can lead to the development of novel architectures with superior performance, making this an exciting avenue for future research. 5. CONCLUSION In this work, we introduced novel CT dataset for retroperitoneal tumors, consisting of 82 cases with expertannotated segmentation maps, and evaluated state-of-the-art segmentation methods, including U-Net and its enhanced variants with Transformers and Mamba. Our proposed ViLU-Net model achieved superior performance with reduced complexity, demonstrating its potential for clinical applications. To support further research, we have made our code publicly available. Additionally, we highlighted future directions, including multi-modal fusion of imaging modalities, optimizing model efficiency for clinical deployment, and enhancing interpretability. 6. ACKNOWLEDGEMENT This work was supported by the Mitacs Accelerate program (grant number BC-ISED-IT32063), the Canadian Foundation for Innovation-John R. Evans Leaders Fund (CFI-JELF) (grant number 42816), and the Natural Sciences and Engineering Research Council of Canada (NSERC) (funding reference number RGPIN-2023-03575). Cette recherche été financée par le Conseil de recherches en sciences naturelles et en génie du Canada (CRSNG), [numéro de référence RGPIN-2023-03575]."
        },
        {
            "title": "REFERENCES",
            "content": "[1] Azad, R., Aghdam, E. K., Rauland, A., Jia, Y., Avval, A. H., Bozorgpour, A., Karimijafarbigloo, S., Cohen, J. P., Adeli, E., and Merhof, D., Medical image segmentation review: The success of u-net, arXiv preprint arXiv:2211.14830 (2022). 1, 2 [2] Xu, Y., Quan, R., Xu, W., Huang, Y., Chen, X., and Liu, F., Advances in medical image segmentation: comprehensive review of traditional, deep learning and hybrid approaches, Bioengineering 11(10), 1034 (2024). 2 [3] Rayed, M. E., Islam, S. S., Niha, S. I., Jim, J. R., Kabir, M. M., and Mridha, M., Deep learning for medical image segmentation: State-of-the-art advancements and challenges, Informatics in Medicine Unlocked , 101504 (2024). 2 [4] Ranjbarzadeh, R., Caputo, A., Tirkolaee, E. B., Ghoushchi, S. J., and Bendechache, M., Brain tumor segmentation of mri images: comprehensive review on the application of artificial intelligence tools, Computers in biology and medicine 152, 106405 (2023). 2 [5] Liu, Z., Tong, L., Chen, L., Jiang, Z., Zhou, F., Zhang, Q., Zhang, X., Jin, Y., and Zhou, H., Deep learning based brain tumor segmentation: survey, Complex & intelligent systems 9(1), 10011026 (2023). 2 [6] Ronneberger, O., Fischer, P., and Brox, T., U-net: Convolutional networks for biomedical image segmentation, in [Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18], 234241, Springer (2015). 2 [7] Bilic, P., Christ, P., Li, H. B., Vorontsov, E., Ben-Cohen, A., Kaissis, G., Szeskin, A., Jacobs, C., Mamani, G. E. H., Chartrand, G., et al., The liver tumor segmentation benchmark (lits), Medical Image Analysis 84, 102680 (2023). 2 [8] Liu, L., Cheng, J., Quan, Q., Wu, F.-X., Wang, Y.-P., and Wang, J., survey on u-shaped networks in medical image segmentations, Neurocomputing 409, 244258 (2020). 2 [9] Oktay, O., Schlemper, J., Folgoc, L. L., Lee, M., Heinrich, M., Misawa, K., Mori, K., McDonagh, S., Hammerla, N. Y., Kainz, B., Glocker, B., and Rueckert, D., Attention u-net: Learning where to look for the pancreas, in [Medical Imaging with Deep Learning], (2018). 2 [10] Zhou, Z., Rahman Siddiquee, M. M., Tajbakhsh, N., and Liang, J., Unet++: nested u-net architecture for medical image segmentation, in [Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support: 4th International Workshop, DLMIA 2018, and 8th International Workshop, ML-CDS 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 20, 2018, Proceedings 4], 311, Springer (2018). 2 [11] Huang, H., Lin, L., Tong, R., Hu, H., Zhang, Q., Iwamoto, Y., Han, X., Chen, Y.-W., and Wu, J., Unet 3+: full-scale connected unet for medical image segmentation, in [ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP) ], 10551059, IEEE (2020). 2 [12] Li, X., Chen, H., Qi, X., Dou, Q., Fu, C.-W., and Heng, P.-A., H-denseunet: hybrid densely connected unet for liver and tumor segmentation from ct volumes, IEEE transactions on medical imaging 37(12), 26632674 (2018). [13] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N., An image is worth 16x16 words: Transformers for image recognition at scale, in [International Conference on Learning Representations ], (2021). 2 [14] Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., Lu, L., Yuille, A. L., and Zhou, Y., Transunet: Transformers make strong encoders for medical image segmentation, arXiv preprint arXiv:2102.04306 (2021). 2 [15] Cao, H., Wang, Y., Chen, J., Jiang, D., Zhang, X., Tian, Q., and Wang, M., Swin-unet: Unet-like pure transformer for medical image segmentation, in [European conference on computer vision], 205218, Springer (2022). 2 [16] Heidari, M., Kazerouni, A., Soltany, M., Azad, R., Aghdam, E. K., Cohen-Adad, J., and Merhof, D., Hiformer: Hierarchical multi-scale representations using transformers for medical image segmentation, in [Proceedings of the IEEE/CVF winter conference on applications of computer vision ], 62026212 (2023). 2 [17] Azad, R., Kazerouni, A., Azad, B., Khodapanah Aghdam, E., Velichko, Y., Bagci, U., and Merhof, D., Laplacian-former: Overcoming the limitations of vision transformers in local texture detection, in [International Conference on Medical Image Computing and Computer-Assisted Intervention ], 736746, Springer (2023). 2 [18] Kolahi, S. G., Chaharsooghi, S. K., Khatibi, T., Bozorgpour, A., Azad, R., Heidari, M., Hacihaliloglu, I., and Merhof, D., Msa2net: Multi-scale adaptive attention-guided network for medical image segmentation, in [35th British Machine Vision Conference 2024, BMVC 2024, Glasgow, UK, November 25-28, 2024], BMVA (2024). 2 [19] Heidari, M., Azad, R., Kolahi, S. G., Arimond, R., Niggemeier, L., Sulaiman, A., Bozorgpour, A., Aghdam, E. K., Kazerouni, A., Hacihaliloglu, I., et al., Enhancing efficiency in vision transformer networks: Design techniques and insights, arXiv preprint arXiv:2403.19882 (2024). [20] Qu, H., Ning, L., An, R., Fan, W., Derr, T., Liu, H., Xu, X., and Li, Q., survey of mamba, arXiv preprint arXiv:2408.01129 (2024). 2 [21] Heidari, M., Kolahi, S. G., Karimijafarbigloo, S., Azad, B., Bozorgpour, A., Hatami, S., Azad, R., Diba, A., Bagci, U., Merhof, D., et al., Computation-efficient era: comprehensive survey of state space models in medical image analysis, arXiv preprint arXiv:2406.03430 (2024). 2 [22] Wang, X., Wang, S., Ding, Y., Li, Y., Wu, W., Rong, Y., Kong, W., Huang, J., Li, S., Yang, H., et al., State space model for new-generation network alternative to transformers: survey, arXiv preprint arXiv:2404.09516 (2024). 3 [23] Gu, A. and Dao, T., Mamba: Linear-time sequence modeling with selective state spaces, arXiv preprint arXiv:2312.00752 (2023). 9 [24] Zhu, L., Liao, B., Zhang, Q., Wang, X., Liu, W., and Wang, X., Vision mamba: Efficient visual representation learning with bidirectional state space model, arXiv preprint arXiv:2401.09417 (2024). 3 [25] Huang, T., Pei, X., You, S., Wang, F., Qian, C., and Xu, C., Localmamba: Visual state space model with windowed selective scan, arXiv preprint arXiv:2403.09338 (2024). 3 [26] Yang, C., Chen, Z., Espinosa, M., Ericsson, L., Wang, Z., Liu, J., and Crowley, E. J., Plainmamba: Improving non-hierarchical mamba in visual recognition, arXiv preprint arXiv:2403.17695 (2024). 3 [27] Ma, J., Li, F., and Wang, B., U-mamba: Enhancing long-range dependency for biomedical image segmentation, arXiv preprint arXiv:2401.04722 (2024). 3, 5, 6, 7 [28] Beck, M., Pöppel, K., Spanring, M., Auer, A., Prudnikova, O., Kopp, M., Klambauer, G., Brandstetter, J., and Hochreiter, S., xlstm: Extended long short-term memory, arXiv preprint arXiv:2405.04517 (2024). 3, 4 [29] Alkin, B., Beck, M., Pöppel, K., Hochreiter, S., and Brandstetter, J., Vision-lstm: xlstm as generic vision backbone, arXiv preprint arXiv:2406.04303 (2024). 3 [30] Chen, T., Ding, C., Zhu, L., Xu, T., Ji, D., Wang, Y., Zang, Y., and Li, Z., xlstm-unet can be an effective 2d & 3d medical image segmentation backbone with vision-lstm (vil) better than its mamba counterpart, arXiv preprint arXiv:2407.01530 (2024). 3, 5 [31] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I., Attention is all you need, Advances in neural information processing systems 30 (2017). 4 [32] Hatamizadeh, A., Nath, V., Tang, Y., Yang, D., Roth, H. R., and Xu, D., Swin unetr: Swin transformers for semantic segmentation of brain tumors in mri images, in [International MICCAI brainlesion workshop], 272284, Springer (2021). 5, 6, 7 [33] Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., and Maier-Hein, K. H., nnu-net: self-configuring method for deep learning-based biomedical image segmentation, Nature methods 18(2), 203211 (2021). 5, 6, 7 [34] Ma, J., Zhang, Y., Gu, S., Ge, C., Ma, S., Young, A., Zhu, C., Meng, K., Yang, X., Huang, Z., et al., Unleashing the strengths of unlabeled data in pan-cancer abdominal organ quantification: the flare22 challenge, arXiv preprint arXiv:2308.05862 (2023)."
        }
    ],
    "affiliations": [
        "Beth Israel Deaconess Medical Center, Boston, MA, United States",
        "Center for Biomedical Imaging and Informatics, Rutgers Cancer Institute, New Brunswick, NJ, United States",
        "Department of Medicine, University of British Columbia, British Columbia, Canada",
        "Department of Radiology, University of British Columbia, British Columbia, Canada",
        "Harvard Medical School, Boston, MA, United States",
        "Independent Researcher, Tabriz, Iran",
        "Memorial Sloan Kettering Cancer Center, New York, NY, United States",
        "Rutgers Robert Wood Johnson Medical School, New Brunswick, NJ, United States",
        "School of Biomedical Engineering, University of British Columbia, British Columbia, Canada",
        "Weill Cornell Medical School, New York, NY, United States"
    ]
}