{
    "paper_title": "FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning",
    "authors": [
        "Zhaopeng Qiu",
        "Shuang Yu",
        "Jingqi Zhang",
        "Shuai Zhang",
        "Xue Huang",
        "Jingyi Yang",
        "Junjie Lai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) for large language models (LLMs) is increasingly bottlenecked by rollout (generation), where long output sequence lengths make attention and KV-cache memory dominate end-to-end step time. FP8 offers an attractive lever for accelerating RL by reducing compute cost and memory traffic during rollout, but applying FP8 in RL introduces unique engineering and algorithmic challenges: policy weights change every step (requiring repeated quantization and weight synchronization into the inference engine) and low-precision rollouts can deviate from the higher-precision policy assumed by the trainer, causing train-inference mismatch and potential instability. This report presents a practical FP8 rollout stack for LLM RL, implemented in the veRL ecosystem with support for common training backends (e.g., FSDP/Megatron-LM) and inference engines (e.g., vLLM/SGLang). We (i) enable FP8 W8A8 linear-layer rollout using blockwise FP8 quantization, (ii) extend FP8 to KV-cache to remove long-context memory bottlenecks via per-step QKV scale recalibration, and (iii) mitigate mismatch using importance-sampling-based rollout correction (token-level TIS/MIS variants). Across dense and MoE models, these techniques deliver up to 44% rollout throughput gains while preserving learning behavior comparable to BF16 baselines."
        },
        {
            "title": "Start",
            "content": "FP8-RL: Practical and Stable Low-Precision Stack for LLM Reinforcement Learning Zhaopeng Qiu1 Shuang Yu1 Jingqi Zhang1 Shuai Zhang1 Xue Huang1 Jingyi Yang1 Junjie Lai1 1NVIDIA, Beijing, China alexq@nvidia.com shuangy@nvidia.com larkz@nvidia.com shuazhang@nvidia.com xueh@nvidia.com sopyang@nvidia.com julienl@nvidia.com"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) for large language models (LLMs) is increasingly bottlenecked by rollout (generation), where long output sequence lengths make attention and KV-cache memory dominate end-to-end step time. FP8 offers an attractive lever for accelerating RL by reducing compute cost and memory traffic during rollout, but applying FP8 in RL introduces unique engineering and algorithmic challenges: policy weights change every step (requiring repeated quantization and weight synchronization into the inference engine) and low-precision rollouts can deviate from the higher-precision policy assumed by the trainer, causing train inference mismatch and potential instability. This report presents practical FP8 rollout stack for LLM RL, implemented in the veRL ecosystem with support for common training backends (e.g., FSDP/Megatron-LM) and inference engines (e.g., vLLM/SGLang). We (i) enable FP8 W8A8 linear-layer rollout using blockwise FP8 quantization, (ii) extend FP8 to KV-cache to remove long-context memory bottlenecks via per-step QKV scale recalibration, and (iii) mitigate mismatch using importance-sampling-based rollout correction (token-level TIS/MIS variants). Across dense and MoE models, these techniques deliver up to 44% rollout throughput gains while preserving learning behavior comparable to BF16 baselines."
        },
        {
            "title": "1 Introduction",
            "content": "Reinforcement learning (RL) has become key post-training stage in the development of state-of-theart large language models (LLMs). However, compared with pretraining and supervised finetuning, RL pipelines impose distinctive system bottleneck: every optimization step requires fresh rollouts (generation) from the current policy. Recent system analyses show that rollout can dominate end-toend iteration timefor example, Seer reports the rollout phase consumes approximately 80% of total iteration time in synchronous LLM RL settings [1]. Consequently, accelerating rollout is often the most direct and high-leverage way to improve overall RL throughput and shorten experimentation cycles. FP8 is an increasingly mature low-precision technology that can accelerate both training and inference on modern GPU architectures, and it is already supported in widely used inference engines (e.g., TensorRT-LLM [2], vllm [3], and SGLang [4]). Yet applying FP8 in RL is fundamentally more challenging than static inference. First, RL updates policy weights every step, so the rollout engine must repeatedly ingest the newest weights and re-apply quantization at step granularity. This becomes especially challenging in modern RL stacks that decouple training and inference components and Preprint. Under review. allow independent backend choices, which increases the engineering burden of robust, step-level weight synchronization. Second, low-precision rollouts can introduce traininginference mismatch: updating high-precision training policy using rollouts generated by low-precision rollout policy. This effectively introduces an off-policy component, which can substantially destabilize RL and even cause training collapse [5], [6], [7]. This report focuses on building systematic FP8 solution for LLM RL rollouts, with an emphasis on practicality and reproducibility. We present an FP8 rollout workflow that supports dynamic weight synchronization and quantized loading, and comprehensively evaluate it across model architectures and quantization components in long-context RL training. Our main contributions are: 1. Design and implementation of practical FP8 rollout solution for RL. We present production-ready FP8 rollout workflow that handles dynamic weight synchronization and supports multiple training backends (FSDP/Megatron-LM) and inference engines (vLLM/SGLang), addressing the unique engineering challenges of applying FP8 in RL where policy weights change every step. 2. FP8 quantization for rollout acceleration. We enable FP8 quantization in rollout through (i) blockwise W8A8 quantization for linear layers to reduce compute cost, and (ii) KV-cache quantization via per-step QKV scale recalibration to substantially increase effective cache capacity and concurrency under memory pressure in long-context generation. 3. Comprehensive validation across model types. We validate our approach on both dense (Qwen3-8B-Base) and MoE (Qwen3-30B-A3B-Base) models in long-context RL training, demonstrating substantial rollout throughput gains (up to 44% speedup) while maintaining learning behavior comparable to BF16 baselines when paired with standard rollout correction techniques."
        },
        {
            "title": "2 Our Solution: FP8 Rollout for LLM RL",
            "content": "We present an FP8 rollout system for LLM reinforcement learning, implemented within the veRL framework with support for common training backends (FSDP/Megatron-LM) and inference engines (vllm/SGLang). Our approach addresses two core challenges: (i) engineering robust dynamic weight synchronization where policy weights change every step, and (ii) mitigating train-inference mismatch introduced by low-precision rollout. The system enables FP8 quantization at two levels: W8A8 quantization for linear layers to reduce compute cost, and KV-cache quantization to alleviate memory bottlenecks in long-context generation. Both are paired with importance-sampling-based rollout correction to maintain training stability. In the following sections, we detail the W8A8 quantization for linear layers (Section 2.1), followed by KV-cache quantization (Section 2.3), and conclude with end-to-end FP8 RL exploration (Section 2.4). 2.1 W8A8 Quantization for Linear Layers Our quantization approach operates at three levels: Quantization scheme: We apply blockwise FP8 quantization to linear layers, with weights quantized statically and activations quantized dynamically to balance accuracy and efficiency. Weight synchronization: At each RL step, updated weights from the training backend are quantized and loaded into the inference engine, ensuring the rollout policy reflects the latest training updates. Mismatch mitigation: We employ importance-sampling-based corrections to address the distribution shift between the quantized rollout policy and the full-precision training policy. 2.1.1 Blockwise FP8 Quantization 2 Quantization format and granularity. We adopt the E4M3 FP8 format, which allocates 4 bits to the exponent and 3 bits to the mantissa, providing dynamic range of approximately [448, 448] with sufficient precision for neural network computations. To balance quantization error and computational efficiency, we employ blockwise quantization with block size ùêµ = 128 128 [8]. For weight matrix ùëä ‚Ñùùëöùëõ, we partition it into blocks and compute per-block scaling factors, yielding quantized weights: ùëäùëñùëó = round( ùëäùëñùëó scaleùëñùëó ) FP8 (1) where scaleùëñùëó is derived from the maximum absolute value within each block. This fine-grained approach significantly outperforms per-tensor quantization in preserving model accuracy while remaining hardware-friendly for modern tensor cores. Quantization scope. We apply W8A8 quantization to the following linear layers: Quantized: attention projections (q_proj, k_proj, v_proj, o_proj), MLP layers (gate_proj, up_proj, down_proj), and MoE expert layers (fc1, fc2). Excluded: embedding layers, normalization layers, and output projection (lm_head). Quantizing lm_head may introduce noticeable degradation in generation quality due to its direct impact on vocabulary logits. Static vs dynamic quantization. Weights are quantized statically during the synchronization phase (see below), as they remain constant throughout rollout batch. Activations, however, vary across inputs and are quantized dynamically during each forward pass to maintain accuracy. 2.1.2 Dynamic Weight Synchronization Pipeline The rollout pipeline follows three phases as illustrated in Figure 1: Initialization phase: Configure FP8 quantization and apply necessary patches to the inference engine to enable dynamic loading of quantized weights. Weight synchronization phase: At each RL step, BF16/FP16 weights are retrieved from the training backend (FSDP/Megatron-LM), quantized to FP8 using the blockwise scheme described above, and loaded into the inference engine. Inference phase: The inference engine executes generation using the pre-loaded FP8 weights. During computation, activation quantization is performed dynamically to maintain accuracy while leveraging FP8 acceleration. This design supports vllm and SGLang rollout engines and integrates with common RL training backends (including FSDP and Megatron-LM), with support status tracked in the implementation notes. Figure 1: The RL workflow in veRL with FP8 W8A8 Linear quantization. 2.1.3 Importance Sampling for Rollout Correction Low-precision rollout introduces policy mismatch: the training algorithm optimizes ùúãùúÉ assuming rollouts come from ùúãùúÉ itself, but samples are actually generated by the quantized policy ùúãFP8 . This off-policy component can destabilize training if left unaddressed. ùúÉ Importance weighting. We correct this mismatch using importance sampling, which reweights each action ùëé by the likelihood ratio: ùë§(ùëéùë†) = ùúãùúÉ(ùëéùë†) ùúãFP8 (ùëéùë†) ùúÉ (2) In practice, we compute these ratios at token level during training, using the BF16 policy ùúãùúÉ to evaluate the log-probabilities of tokens sampled by the FP8 rollout policy ùúãFP8 . ùúÉ Truncated importance sampling (TIS). Naive importance weights can exhibit high variance, particularly when ùúãùúÉ and ùúãFP8 diverge due to quantization error accumulated over long sequences. We adopt token-level truncated importance sampling (TIS), which clips the importance weights to bounded range: ùúÉ ùë§TIS(ùëéùë†) = clip(ùë§(ùëéùë†), ùê∂) (3) where ùê∂ is the clipping threshold (we use ùê∂ = 2 in all experiments). This bounded correction stabilizes training while still correcting for the primary distribution shift. 2.1.4 Implementation Our FP8 W8A8 linear rollout is implemented within the verl framework [9] and can be enabled by setting actor_rollout_ref.rollout.quantization=fp8 in the training configuration. The system automatically handles weight conversion and loading into the inference backend. For optimal performance, we recommend using CUDA 12.9+ and enabling the DeepGEMM1 library for accelerated FP8 GEMM operations (enabled by default in vllm 0.11+ and SGLang 0.55+). It is strongly recommended to pair FP8 rollout with importance-sampling-based rollout correction (e.g., token-level TIS) to maintain training stability. Detailed configuration examples and usage instructions are provided in Section A. 2.2 Experiments In this section, we detail our experimental setup and present results for both dense and MoE models, demonstrating the efficacy of FP8 W8A8 linear rollout in end-to-end RL training scenarios. 2.2.1 Settings Training Setup We evaluate our approach using the DAPO [10] algorithm with online validation on the AIME24 benchmark. All experiments are conducted on H100-class GPUs, using 8H100 for the 8B dense model and 28H100 for the 30B MoE model. We use vllm as the rollout engine and FSDP as the training backend. Hyperparameters we use prompt batch size of 32 with n=16 responses per prompt, resulting in rollout batch size of 32316. Both training batch size and PPO mini-batch size are set to 32 to ensure the policy is updated using rollout outputs only once per iteration, which helps isolate the impact of quantization by removing additional off-policy noise. The maximum response length is set to 20K tokens to stress-test the system under long-context generation scenarios. To mitigate train-inference mismatch introduced by FP8 quantization, we apply token-level truncated importance sampling (TIS) with clipping threshold C=2 for all FP8 configurations. While RL training hyperparameters are highly case-dependent, these settings represent verified working recipe for the models tested. Complete configuration details, including software versions and training scripts, are provided in Section A. 1https://github.com/deepseek-ai/DeepGEMM Metrics We evaluate training effectiveness and system performance using the following metrics: (i) validation accuracy on the AIME24 test set, measuring the percentage of correctly solved problems as the primary indicator of policy learning quality; (ii) reward, the average reward per response computed by the reward model during training, tracking optimization progress; (iii) response length, the average token count per generated response, which typically increases as the policy learns more complex reasoning chains in long-context RL; (iv) mismatch KL, the KL divergence ùê∑KL(ùúãFP8 ùúãùúÉ) between the rollout policy ùúãFP8 (inference) and training policy ùúãùúÉ, quantifying the distribution shift introduced by quantizationlower values indicate better alignment, while elevated values can destabilize training if left uncorrected; and (v) rollout performance, measured via time-per-token (ms/token) during generation, where lower values indicate faster rollout and reduced end-to-end training iteration time. ùúÉ ùúÉ 2.2.2 Dense model: Qwen3-8B-Base We conduct Qwen3-8B-Base model based experiments under three configurations: (i) BF16 baseline without rollout correction (orange), (ii) FP8 W8A8 with token-level TIS (blue), and (iii) FP8 W8A8 without TIS (green) for ablation analysis. Figure 2 presents the training dynamics across all configurations. Figure 2: Training curves for Qwen3-8B-Base. Orange: BF16 baseline without TIS. Blue: FP8 W8A8 with token-level TIS. Green: FP8 W8A8 without TIS. Training effectiveness. Our solution, FP8 W8A8 with token-level TIS (blue), closely tracks the BF16 baseline (orange) across all key training metrics. Validation accuracy curves align well throughout training, with both configurations achieving comparable final accuracy on the AIME24 benchmark. Reward curves overlap significantly, confirming that policy optimization proceeds effectively under FP8 rollout. Response length grows consistently in both configurations, indicating that the policy successfully learns to generate increasingly complex reasoning chains regardless of rollout precision. The mismatch KL, which quantifies the distribution shift between the rollout policy ùúãFP8 and the training policy ùúãùúÉ, is naturally higher for the FP8 run due to quantization-induced precision loss. However, with TIS enabled, this divergence remains stable and within an acceptable range, ensuring training stability while maintaining accuracy comparable to the BF16 baseline. ùúÉ Importance of rollout correction. The ablation configurationFP8 W8A8 without TIS (green) exhibits noticeable accuracy degradation compared to both the BF16 baseline and the FP8+TIS 5 configuration. This performance drop demonstrates the critical role of importance sampling in mitigating the distribution mismatch introduced by quantization. Rollout performance. We evaluate the system-level performance impact of FP8 W8A8 linear rollout using CUDA 12.9 to maximize DeepGEMM kernel efficiency. Figure 3 shows time-per-token measurements across varying response lengths. FP8 rollout consistently outperforms BF16 across all sequence lengths, yielding an overall speedup of approximately 1020% for the 8B dense model. Performance gains are more pronounced at longer sequence lengths, where memory bandwidth constraints become the dominant bottleneck and FP8s reduced memory traffic provides greater benefit. Figure 3: Rollout performance on Qwen3-8B-Base. Time-per-token (ms/token) during generation and throughput for varying response lengths. Blue: FP8 W8A8 linear rollout. Orange: BF16 baseline. 2.2.3 MoE model: Qwen3-30B-A3B-Base We evaluate FP8 W8A8 linear rollout on the Qwen3-30B-A3B-Base MoE model, comparing two configurations: (i) BF16 with token-level TIS (orange) and (ii) FP8 W8A8 with token-level TIS (blue). Unlike the dense model experiments, we apply rollout correction to both configurations because MoE models exhibit more pronounced train-inference mismatch even at full precision, making correction beneficial for optimal convergence. Figure 4 presents the training dynamics for both configurations. Figure 4: Training curves for Qwen3-30B-A3B-Base MoE. Orange: BF16 with token-level TIS. Blue: FP8 W8A8 with token-level TIS. 6 Training effectiveness. With token-level TIS enabled, the FP8 rollout (blue) aligns remarkably well with the BF16 baseline (orange) across all key training metrics. Validation accuracy curves track closely throughout training, demonstrating that FP8 quantization preserves learning quality for MoE models. Reward and response length curves also overlap significantly, confirming that policy optimization proceeds effectively under FP8 rollout despite the additional complexity introduced by mixture-of-experts routing. MoE-specific observations. Unlike the dense model where mismatch KL remains stable, MoE models inherently exhibit trend of increasing mismatch KL during training in both the BF16 and FP8 runs. This phenomenon stems from the dynamic nature of MoE architectures, where slight differences in precision or implementation between the inference engine and training backend can lead to inconsistent expert selection across the two systems. As training progresses and the policy evolves, these routing inconsistencies accumulate, causing the mismatch KL to gradually increase. While token-level TIS successfully kept this divergence manageable in our experimentsas evidenced by the aligned validation accuracyscenarios with more severe instability may require advanced correction techniques such as Masked Importance Sampling (MIS) or Rollout Router Replay (RRR), which enforces consistent expert selection between rollout and training. Rollout performance. FP8 W8A8 linear rollout delivers substantially larger performance gains for the MoE model compared to the dense model. As shown in Figure 5, FP8 rollout (blue) achieves significantly lower time-per-token compared to BF16 (orange), yielding an overall speedup of approximately 3050% across varying response lengths. This represents 23 larger improvement compared to the 1020% speedup observed for the 8B dense model. Figure 5: Rollout performance on Qwen3-30B-A3B-Base MoE. Time-per-token (ms/token) during generation for varying response lengths. Blue: FP8 W8A8 linear rollout. Orange: BF16 baseline. Performance analysis. The substantially larger speedup for the 30B MoE model can be attributed to three complementary factors. First, larger models exhibit higher arithmetic intensity, making FP8s compute acceleration more impactfulthe 30B models matrix multiplications are significantly larger than those in the 8B model, allowing FP8 tensor cores to deliver greater absolute time savings. Second, reduced memory traffic becomes increasingly beneficial at scale: loading and transferring the massive 30B parameter set consumes substantial bandwidth in BF16, and FP8s 2 memory footprint reduction directly translates to faster weight loading and reduced memory pressure. Third, quantizing the 30B weights frees significant GPU memory, which expands the available KV-cache capacity. This expanded headroom allows the system to host more concurrent requests and tokens, reducing preemption frequency and increasing overall throughput. These factors compound in longcontext generation scenarios where both compute intensity and memory pressure are high, explaining why the 30B MoE model benefits disproportionately from FP8 W8A8 linear rollout compared to smaller dense models. 2.3 Beyond Linear Layers: FP8 KV-Cache Quantization Because long-context RL rollouts are often memory-bound, KV-cache quantization is high-impact extension beyond linear layer only quantization, targeting the dominant attention/memory bottleneck during generation. 2.3.1 Implementation strategy: dynamic QKV scale recalibration 7 To enable FP8 KV-cache in an RL setting, the key challenge is handling dynamic weights. Unlike static inference, the policy changes every step, meaning QKV scales must be recalibrated frequently. We explored two design paradigms to address this: Figure 6: Two implementation strategies for FP8 KV Cache. Inference-Side calibration: Modern inference engines (like vllm) natively support dynamic QKV scale calculation, typically performing this calibration during the first forward pass after initialization. As shown in Figure 6, this approach leverages that builtin capability but adapts it for RL: we trigger forced recalibration (e.g., by resetting the internal calculate_kv_scales flags of all the attention layers) before the rollout phase of each RL step. This ensures that the KV-cache quantization accurately reflects the newly updated policy weights from the previous training step without external intervention. Trainer-Side calibration: As depicted by Figure 6, at the end of each training step, the training backend recalibrates QKV scales using the updated policy weights and subset of training data (prompts and generated responses). These new scales are then synchronized to the inference engine for the subsequent rollout. This method introduces tighter inference backend coupling and minor calibration overhead but offers fine-grained control over the calibration data and process. We implemented Inference-Side calibration in verl and Trainer-Side calibration in NeMo-RL [11]. In the following experiments, we present results from the verl implementation. Usage instructions for both frameworks and additional experimental validation of the NeMo-RL implementation are provided in Section B. 2.3.2 Experiments We evaluate KV-cache quantization on Qwen3-8B-Base using the verl implementation with Inference-Side calibration. The experimental setup follows the same DAPO recipe and H100 hardware configuration as the W8A8 linear rollout experiments. We compare four configurations: BF16 baseline: Full BF16 precision throughout. Linear W8A8: W8A8 quantization for linear layers only, as described in Section 2.1. KV-cache FP8 only: FP8 quantization applied exclusively to KV-cache storage, while linear layers and attention computations remain in BF16. This isolates the impact of KVcache quantization. Full FP8: FP8 quantization across all three componentslinear layers (W8A8), KVcache storage, and attention computationsrepresenting the most aggressive quantization setting. 8 All FP8 configurations use token-level TIS (C=2) for rollout correction and target maximum response length of 20K tokens. Figure 7 presents the training dynamics. Figure 7: Training curves for Qwen3-8B-Base with KV-Cache FP8. Blue: BF16 baseline. Yellow: Linear W8A8 + TIS. Red: KV-Cache FP8 only + TIS. Green: Full FP8 (Linear + KV + Attention) + TIS. Training effectiveness. KV-cache FP8 only aligns closely with the BF16 baseline on validation accuracy, indicating that quantizing the KV-cache alone has minimal impact on model convergence. Reward curves and response length growth also track the baseline, confirming that policy optimization proceeds effectively despite KV-cache quantization. The KV-cache FP8 only run exhibits slightly higher mismatch KL compared to the Linear W8A8 run. We hypothesize this stems from error accumulation in long-context generation: unlike weight quantization which is static per layer, KV-cache quantization affects every tokens attention computation dynamically over the entire long sequence. However, despite this slight increase in KL divergence, validation accuracy and reward remain stable and aligned with the baseline, demonstrating that token-level TIS successfully mitigates this mismatch. Full FP8, which combines linear, KV-cache, and attention quantization, shows the largest mismatch KL as expected from compounded quantization errors. Nevertheless, with token-level TIS enabled, it still follows the general accuracy trend, confirming that even aggressive full-stack quantization can be viable for RL training when paired with robust mismatch mitigation. Rollout performance. Figure 8 shows rollout performance across different quantization configurations. Linear W8A8 achieves approximately 20% speedup over the BF16 baseline, while KV-cache FP8 only delivers substantially larger 38% speedup. Combining both linear and KV-cache quantization yields cumulative 44% speedup, demonstrating the complementary benefits of quantizing both components. 9 Figure 8: Rollout speedup on Qwen3-8B-Base. Blue: BF16 baseline. Yellow: Linear W8A8 + TIS. Red: KV-Cache FP8 only + TIS. Green: Full FP8 (Linear + KV + Attention) + TIS. Performance analysis. KV-cache quantization provides disproportionately large gains ( 38%) compared to linear quantization alone ( 20%) in this experiment. This outcome is highly dependent on model size and use case: for small dense models (like 8B) generating very long responses (maximum 20K tokens), the workload becomes heavily memory-bound and bottlenecked on KVcache capacity rather than compute. In our experiments with the BF16 baseline and linear W8A8 configurations, vLLM monitoring revealed frequent request preemptions caused by insufficient KVcache space, which wasted computation and throttled throughput. By enabling KV-cache FP8, we effectively double the KV-cache capacity, increasing maximum concurrency and reducing preemption frequency. This expanded headroom significantly mitigates memory pressure, resulting in stable, high-throughput generation stream that maximizes GPU utilization. 2.4 End-to-End FP8 RL The preceding sections evaluated FP8 rollout in isolation, maintaining BF16 precision during training. While this configuration already delivers substantial system-level gains, applying FP8 endto-endacross both rollout and trainingoffers three additional benefits. First, FP8 training has been validated at scale in pretraining and supervised finetuning, where it matches BF16 convergence when using appropriate formats and scaling rules. Second, end-to-end FP8 can reduce precisioninduced mismatch between training and inference: rollout-only FP8 keeps training in BF16/FP16 while generating trajectories in FP8, amplifying distribution drift due to quantization/dequantization error and kernel-level differences. By aligning the precision of both paths, end-to-end FP8 makes the distribution optimized during learning closer to the distribution produced during rollout. Third, FP8 training can accelerate the RL loop by speeding up the learner-side forward/backward passes for policy and value networks, improving time-to-train and reducing end-to-end cost beyond rolloutonly acceleration. 2.4.1 Settings Experimental setup. We evaluate end-to-end FP8 on the Qwen3-8B-Base model using the same DAPO + AIME24 validation recipe and hardware configuration as Section 2.1. All experiments are conducted on 8H100 GPUs. The training hyperparameters follow the same settings: prompt batch size of 32 with n=16 responses per prompt (rollout batch size 32316), training batch size and PPO mini-batch size both set to 32, and maximum response length of 20K tokens. Token-level TIS with C=2 is applied to all FP8 configurations for rollout correction. These experiments are implemented in the NeMo-RL framework. Configurations. We compare three precision settings to isolate the impact of end-to-end FP8: BF16 training + BF16 rollout: Full BF16 baseline BF16 training + FP8 rollout: FP8 rollout-only (the configuration same as Section 2.1) FP8 training + FP8 rollout: End-to-end FP8 quantization 2.4.2 Results Figure 9 presents the training dynamics across all three configurations, including validation accuracy, reward, response length, sampling importance ratio, and mismatch KL. 10 Figure 9: Training curves for end-to-end FP8 RL. Blue: BF16 training + BF16 rollout. Orange: BF16 training + FP8 rollout. Green: FP8 training + FP8 rollout. Accuracy alignment with BF16. FP8 training + rollout (green) closely tracks the BF16 training + rollout baseline (blue) across response length, reward, and validation accuracy. This demonstrates that end-to-end FP8 can preserve the learning dynamics and final quality of the BF16 run under this setup. Reduced mismatch vs FP8 rollout-only. Compared to FP8 rollout-only (orange), FP8 training + rollout (green) exhibits smaller traininginference mismatch, reflected by sampling importance ratio closer to 1 and lower mismatch KL. This confirms that aligning precision between training and rollout reduces distribution drift. However, FP8 training + rollout still shows slightly higher mismatch than BF16 training + rollout, suggesting that precision alignment helps but does not fully remove all sources of mismatch. Training-side speedup. Enabling FP8 training reduces the learner-side training time. In our runs, FP8 training + rollout achieves approximately 20% lower training time compared to BF16 training + rollout, indicating meaningful end-to-end efficiency gains beyond rollout-only acceleration."
        },
        {
            "title": "3 Related Work",
            "content": "3.1 FP8 Quantization for LLMs As models scale to trillions of parameters, low-precision formats have become essential for practical training and inference. The foundational specification for FP8 formats was established by Micikevicius et al. [12], who proposed the E4M3 and E5M2 encodings that balance dynamic range and precision. Building on this foundation, FP8-LM [13] demonstrated that FP8 can be systematically applied to weights, gradients, and optimizer states during LLM pre-training and fine-tuning, achieving 2 throughput speedup. More recently, DeepSeek-V3 [8] reported the first successful industrialscale application of FP8 training. It employs fine-grained quantization (1128 tiles for activations and 128128 blocks for weights) and uses the E4M3 format for both forward and backward GEMM operations, setting new benchmark for high-performance low-precision training. On the inferen ce side, practical FP8 deployment is primarily driven by high-performance serving frameworks (e.g., vLLM [3], TensorRT-LLM [2], and SGLang [4]) leveraging hardware-native support on NVIDIA Hopper and Blackwell architectures. Rather than relying on single technique, they combine (i) reduced-precision representations to lower weight/activation and memory-traffic costs, with (ii) highly optimized attention/decoding kernels, paged KV-cache management, and efficient runtime scheduling to improve throughput under long-context and high-concurrency workloads. In memory-bandwidth-bound regimes, lowering the KV-cache precision can substantially reduce cache footprint, effectively increasing the number of cached tokens and the achievable concurrency, 11 while favorable hardwarekernel support often translates these savings into meaningful end-to-end throughput gains. 3.2 RL Systems for LLMs Modern RL systems for LLMs must orchestrate complex interactions between Actor, Critic, Reward, and Reference models. Early frameworks like DeepSpeed-Chat [14] utilized synchronous execution, which often led to poor resource utilization in large clusters. Several high-performance frameworks have since emerged to address these challenges. OpenRLHF [15] utilizes Ray-based distributed architecture to decouple model roles across GPUs and integrates vLLM for accelerated generation. veRL [9] introduces flexible programming model that allows seamless integration of various training backends (FSDP, Megatron-LM) with inference backends. NeMo-RL [11] builds on NVIDIAs NeMo ecosystem, providing tight integration with MegatronLM and native support for FP8 training and inference. ROLL [16] focuses on hardware-aware workload mapping for massive clusters. slime [17] provides lightweight framework natively integrated with SGLang, specifically optimizing for routing consistency in MoE models. To overcome the inherent latency of sequential decoding, asynchronous architectures have also been proposed. AReaL [18] decouples generation and training workflows entirely, allowing rollout nodes to generate samples continuously without waiting for training updates. 3.3 Training-Infernece Mismatch in LLM RL Modern LLM RL stacks typically decouple rollout from training: trajectories are produced by high-throughput inference engines, while updates are computed in training backends. Even with synchronized weights, differences in numerics, kernels and parallelism can make the rollout policy deviate from the training policy, effectively turning nominally on-policy updates into off-policy ones and causing instability or collapse. Addressing this traininginference mismatch has become central concern. common algorithmic approach is importance sampling (IS): it corrects off-policy updates by reweighting gradient contributions using the likelihood ratio between the training policy and the rollout policy. In practice, truncated or masked IS variants (TIS/MIS) control variance by clipping extreme weights or masking unreliable tokens/sequences [5], [7], [19]. For MoE models, Rollout Routing Replay (R3) records inference-time expert choices and replays them during training to align routing [20]. Complementary system efforts pursue bitwise consistency by aligning kernels and enforcing determinism across parallel configurations [21] . Finally, Qi et al. [22] argue root cause can lie in floating-point precision itself: BF16 rounding error accumulates during autoregressive decoding, and using FP16 consistently across training and inference substantially reduces divergence."
        },
        {
            "title": "4 Summary and Future Work",
            "content": "We presented FP8-RL, practical and stable FP8 rollout stack for LLM reinforcement learning. Our system addresses the unique challenges of applying FP8 in RLdynamic weight synchronization and traininference mismatchthrough combination of engineering and algorithmic techniques. W8A8 linear quantization delivers 1020% rollout speedup for dense models and 3050% for MoE models, with KV-cache quantization providing further gains by expanding effective cache capacity under memory pressure, achieving up to 44% combined improvement. End-to-end FP8, which aligns precision across both training and rollout, reduces distribution drift compared to FP8 rolloutonly configurations. Across all settings, importance-sampling-based correction remains essential for maintaining accuracy comparable to BF16 baselines. Future work includes exploring more aggressive formats such as NVFP4 (noting reported instability from accumulated quantization error), scaling validation to larger models, and extending to multiturn and agentic RL scenarios where long contexts further stress rollout efficiency."
        },
        {
            "title": "References",
            "content": "12 [1] R. Qin et al., Seer: Online Context Learning for Fast Synchronous LLM Reinforcement Learning. [Online]. Available: https://arxiv.org/abs/2511.14617 [2] NVIDIA, TensorRT LLM. [Online]. Available: https://github.com/NVIDIA/TensorRT-LLM [3] W. Kwon et al., Efficient Memory Management for Large Language Model Serving with PagedAttention, in Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [4] [5] SGLang. [Online]. Available: https://github.com/sgl-project/sglang J. Liu, Y. Li, Y. Fu, J. Wang, Q. Liu, and Z. Jiang, When Speed Kills Stability: Demystifying RL Collapse from the Training-Inference Mismatch. [Online]. Available: https://richardli.xyz/rl-collapse [6] L. Liu, F. Yao, D. Zhang, C. Dong, J. Shang, and J. Gao, FlashRL: 8Bit Rollouts, Full Power RL. [Online]. Available: https://fengyao.notion.site/flash-rl [7] F. Yao, L. Liu, D. Zhang, C. Dong, J. Shang, and J. Gao, Your Efficient RL Framework Secretly Brings You Off-Policy RL Training. [Online]. Available: https://fengyao.notion.site/off-policy-rl [8] DeepSeek-AI, DeepSeek-V3 Technical Report, CoRR, 2024. [9] G. Sheng et al., HybridFlow: Flexible and Efficient RLHF Framework, arXiv preprint arXiv: 2409.19256, 2024. [10] Q. Yu et al., DAPO: An Open-Source LLM Reinforcement Learning System at Scale, CoRR, 2025. [11] NeMo RL: Scalable and Efficient Post-Training Library. 2025. [12] P. Micikevicius et al., FP8 Formats for Deep Learning, CoRR, 2022. [13] H. Peng et al., FP8-LM: Training FP8 Large Language Models, CoRR, 2023. [14] Z. Yao et al., DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales. [Online]. Available: https://arxiv.org/abs/2308.01320 [15] J. Hu et al., OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework, arXiv preprint arXiv:2405.11143, 2024. [16] W. Wang et al., Reinforcement Learning Optimization for Large-Scale Learning: An Efficient and UserFriendly Scaling Library. [Online]. Available: https://arxiv.org/abs/2506.06122 [17] Z. Zhu, C. Xie, X. Lv, and slime Contributors, slime: An LLM post-training framework for RL Scaling. 2025. [18] W. Fu et al., AReaL: Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning. [Online]. Available: https://arxiv.org/abs/2505. [19] X. Zhao et al., Small Leak Can Sink Great ShipBoost RL Training on MoE with IcePop!. [Online]. Available: https://ringtech.notion.site/icepop [20] W. Ma et al., Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers. [Online]. Available: https://arxiv.org/abs/2510.11370 [21] vLLM and T. Teams, No More Train-Inference Mismatch: Bitwise Consistent On-Policy Reinforcement Learning with vLLM and TorchTitan. [Online]. Available: https://blog.vllm.ai/2025/11/10/bitwiseconsistent-train-inference.html [22] P. Qi et al., Defeating the Training-Inference Mismatch via FP16. [Online]. Available: https://arxiv.org/ abs/2510.26788 Appendix: FP8 W8A8 Linear Rollout Configuration This appendix provides detailed configuration instructions and usage examples for enabling FP8 W8A8 linear rollout in the veRL framework. A.1 Basic Configuration To enable FP8 quantization, simply add the following argument to the command line: actor_rollout_ref.rollout.quantization=fp 13 With this specification, the system automatically handles the weight conversion and loading into the vLLM backend during the rollout phase. A.2 Complete Example For complete example, please refer to the DAPO Qwen3-30B FP8 Rollout Recipe. Below is snippet showing how this is typically configured in startup script: # ... existing configuration ... actor_rollout_ref.rollout.temperature=${temperature} actor_rollout_ref.rollout.top_p=${top_p} actor_rollout_ref.rollout.top_k=${top_k} +actor_rollout_ref.rollout.quantization=fp8 actor_rollout_ref.rollout.name=vllm # ... remaining configuration ... A.3 Performance Optimization Requirements To achieve the best performance, ensure your environment meets the following requirements: DeepGEMM library: The DeepGEMM library significantly accelerates FP8 GEMM operations. For vLLM 0.11+ and SGLang 0.55+, DeepGEMM is enabled by default. For vLLM 0.10.x, you must enable it manually by setting VLLM_USE_DEEP_GEMM=True when launching the RL training. CUDA version: Must be 12.9 or higher, as newer CUDA versions provide significant speedups for FP8 DeepGEMM kernels. A.4 Rollout Correction Configuration As highlighted in the main text, FP8 quantization can introduce errors that exacerbate the discrepancy between training and inference distributions. To prevent this training-inference mismatch from degrading training accuracy or destabilizing the RL process, it is strongly recommended to pair FP8 rollout with mismatch correction techniques, such as Token-level Importance Sampling (TIS). The veRL framework provides comprehensive suite of functionalities to address this. For detailed guidance on configuring rollout correction, please refer to the Rollout Correction Documentation. KV-Cache Quantization Usage B.1 veRL Implementation (Inference-Side Calibration) The veRL implementation leverages vLLMs native dynamic QKV scale calculation. To enable KVcache FP8: actor_rollout_ref: rollout: quantization: kv_cache_dtype: fp8_e4m3 calculate_kv_scales: True Implementation details: Triggers forced recalibration before each rollout phase by resetting internal calculate_kv_scales flags Requires vLLM version with FP8 KV-cache support (vLLM 0.11+) Automatically handles per-step QKV scale updates without external intervention Can be combined with FP8 linear W8A8 for maximum throughput 14 B.2 NeMo-RL Implementation (Trainer-Side Calibration) The NeMo-RL implementation performs QKV scale recalibration on the training side. Configuration example: policy: generation: vllm_cfg: precision: fp8 kv_cache_dtype: fp8 Implementation details: At the end of each training step, recalibrates QKV scales using updated policy weights Uses subset of training data (prompts and generated responses) for calibration Synchronizes new scales to inference engine for subsequent rollout Calibration overhead: approximately 2-3% of total step time Provides fine-grained control over calibration data distribution B.3 NeMo-RL Experimental Validation We validated the Trainer-Side calibration approach on Qwen3-8B-Base using the NeMo-RL framework. B.3.1 Experimental Setup The experimental setup maintains identical dataset selection and hyperparameters as specified in the main text. Configurations evaluated: BF16 baseline (standard BF16 training) Linear W8A8 (W8A8 quantization for linear layers only) Full FP8 (FP8 quantization across all three componentslinear layers (W8A8), KVcache storage, and attention computationsrepresenting the most aggressive quantization setting.) All FP8 configurations use token-level TIS (C=2) for rollout correction. B.3.2 Training Effectiveness Figure 10 presents the training curves across different quantization configurations. Key observations: Accuracy alignment: After enabling token-level TIS, the validation accuracy of Full FP8 successfully aligns with both the BF16 baseline and Linear W8A8 configurations. Mismatch KL: The mismatch KL divergence is higher when both KV cache and attention are quantized to FP8, compared to the Linear W8A8 setting. This is expected due to compounded quantization errors across multiple components. Consistency with Inference-Side calibration: These observations from the Trainer-Side calibration approach are largely consistent with the Inference-Side calibration results reported in the main text, indicating that both calibration paradigms achieve comparable effectiveness in maintaining training stability under FP8 KV cache quantization. 15 Figure 10: Training curves for Qwen3-8B-Base with Trainer-Side KV-cache calibration (NeMo-RL). Blue: BF16 baseline, Orange: Linear W8A8, Green: Full FP8 (Linear + KV Cache + Attention). B.3.3 Performance Results Figure 11 illustrates the rollout performance across different quantization schemes. Key findings: Speedup magnitude: Enabling FP8 for both KV cache and attention operations yields an additional 30% speedup on top of the Linear W8A8 configuration, resulting in an overall 48% speedup compared to the BF16 baseline. Length dependency: Performance gains are particularly pronounced at longer response lengths, where attention computations constitute larger fraction of the overall workload, making FP8 quantization of attention components increasingly beneficial. Calibration overhead: The QKV scale recalibration process consumes approximately 2-3% of the total step time, representing minor cost relative to the substantial rollout acceleration achieved. Figure 11: Rollout performance on Qwen3-8B-Base with Trainer-Side calibration (NeMo-RL). Blue: BF16 baseline, Orange: Linear W8A8, Green: Full FP8 (Linear + KV Cache + Attention). These results demonstrate that Trainer-Side calibration achieves comparable training effectiveness and performance gains to Inference-Side calibration, validating both approaches as viable strategies for FP8 KV-cache quantization in RL settings."
        }
    ],
    "affiliations": [
        "NVIDIA, Beijing, China"
    ]
}