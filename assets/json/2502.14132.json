{
    "paper_title": "Can Community Notes Replace Professional Fact-Checkers?",
    "authors": [
        "Nadav Borenstein",
        "Greta Warren",
        "Desmond Elliott",
        "Isabelle Augenstein"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Two commonly-employed strategies to combat the rise of misinformation on social media are (i) fact-checking by professional organisations and (ii) community moderation by platform users. Policy changes by Twitter/X and, more recently, Meta, signal a shift away from partnerships with fact-checking organisations and towards an increased reliance on crowdsourced community notes. However, the extent and nature of dependencies between fact-checking and helpful community notes remain unclear. To address these questions, we use language models to annotate a large corpus of Twitter/X community notes with attributes such as topic, cited sources, and whether they refute claims tied to broader misinformation narratives. Our analysis reveals that community notes cite fact-checking sources up to five times more than previously reported. Fact-checking is especially crucial for notes on posts linked to broader narratives, which are twice as likely to reference fact-checking sources compared to other sources. In conclusion, our results show that successful community moderation heavily relies on professional fact-checking."
        },
        {
            "title": "Start",
            "content": "Can Community Notes Replace Professional Fact-Checkers?"
        },
        {
            "title": "University of Copenhagen",
            "content": "nb@di.ku.dk grwa@di.ku.dk augenstein@di.ku.dk 5 2 0 2 9 1 ] . [ 1 2 3 1 4 1 . 2 0 5 2 : r de@di.ku.dk"
        },
        {
            "title": "Abstract",
            "content": "Two commonly-employed strategies to combat the rise of misinformation on social media are (i) fact-checking by professional organisations and (ii) community moderation by platform users. Policy changes by Twitter/X and, more recently, Meta, signal shift away from partnerships with fact-checking organisations and towards an increased reliance on crowdsourced community notes. However, the extent and nature of dependencies between fact-checking and helpful community notes remain unclear. To address these questions, we use language models to annotate large corpus of Twitter/X community notes with attributes such as topic, cited sources, and whether they refute claims tied to broader misinformation narratives. Our analysis reveals that community notes cite factchecking sources up to five times more than previously reported. Fact-checking is especially crucial for notes on posts linked to broader narratives, which are twice as likely to reference fact-checking sources compared to other sources. In conclusion, our results show that successful community moderation heavily relies on professional fact-checking."
        },
        {
            "title": "Introduction",
            "content": "The proliferation of misinformation on social media (Arnold, 2020; Diakopoulos, 2020), along with the rise of generative AI (Augenstein et al., 2024) have led to increasing concerns about its current and future potential harms, (e.g., to health (Clemente et al., 2022)) and threats to democracy and political stability (Reglitz, 2022). Fact-checkers play crucial role in combatting misinformation (Graves, 2017), and in recent years, have partnered with social media platforms, e.g., Meta, YouTube, and TikTok, to tackle its spread on these platforms. However, due to the scale of misleading content shared online, community moderation (e.g., options to flag potential misinformation, group/server moderators) is often employed Figure 1: An example of community note. Notice the fact-checking link and rating. in parallel (Morrow et al., 2022), as complementary approach (e.g., (Google, 2025); see also the practice of snoping (Pilarski et al., 2024)). The expansion of fact-checking projects in the last decade (Lauer and Graves, 2024), alongside their broader initiatives to curb misinformation (e.g., citizen media literacy programmes (Juneja and Mitra, 2022)) have been aided by partnerships with social media platforms such as Meta and Google (Graves and Anderson, 2020), which fund independent fact-checking agencies to fact-check potentially false claims on their platform.1 However, political pressure and accusations of bias and censorship, and most recently, Metas announcement of its plans to end its partnerships with fact-checkers in the U.S. and implement community moderation model (Meta, 2025), threatens the financial stability of fact-checking organisations, and hence, their ability to keep up with the increasing volume and sophistication of misinformation spread (Stencel et al., 2024; IFCN, 2024). Metas recent policy shift also implies that these two strategies (fact-checking and community notes) are independent and in opposition, rather than two 1Fact-checkers provide judgment of claim veracity and exert no influence on the platforms content moderation policies (Catalanello and Sanders, 2025). complementary strategies of tackling online misinformation. In this paper, we examine Twitter/X community notes as case study to understand how fact-checking is used in community notes. Specifically, we investigate the following two questions: (RQ1) To what extent do community notes rely on the work of professional fact-checkers? and (RQ2) What are the traits of posts and notes that rely on fact-checking sources? Studying the relationship between fact-checking and community notes is vital for understanding the shared role of expert and community-driven fact-checking in the global information ecosystem. We find that at least 1 in 20 community notes rely explicitly on the work of professional fact-checkers, while this reliance is higher still for high-stakes topics such as health and politics. Our experiments also show that fact-checking is vital for debunking misleading content linked to broader narratives or conspiracy theories. These findings imply that high-quality community notes cannot be produced independently of professional fact-checking. They further suggest that the pressure on fact-checkers exerted by platforms and politicians by defunding and discrediting fact-checking organisations will have corrosive effects on the quality of notes and destructive implications for information integrity more widely."
        },
        {
            "title": "2.1 Community notes",
            "content": "Community moderation has been proposed as means of addressing the scalability (Martel et al., 2024) and cross-partisanship trust (Flamini, 2019) challenges associated with fact-checking. Twitter/Xs Community Notes programme (piloted in 2021 and publicly launched in October 2022 (Twitter/X, 2021)) is notable example of such system. Any platform user may volunteer as Community Notes contributor, although they must achieve particular rating impact score before they can write notes (Twitter/X, 2024b). Notes that achieve helpful rating appear underneath the post, explaining why the post is misleading (see Fig. 1). To be rated helpful, note must receive similar levels of helpfulness rating from users with diverse viewpoints (Twitter/X, 2024a)."
        },
        {
            "title": "2.1.1 Characteristics of Community Notes",
            "content": "A small but growing body of work has analysed Twitter/Xs Community Notes dataset, focusing on the targets, sources, and limitations of notes. Targets of notes. Community notes tend to focus on misleading posts from large accounts (Pilarski et al., 2024), focusing on posts that lack important content or present unverified claims as facts (Pröllochs, 2022; Drolsbach and Pröllochs, 2023). Sources in notes. Analyses have indicated that notes were rated more helpful if they link to trustworthy sources (Pröllochs, 2022) and that the majority of sources cited by notes were trustworthy left-leaning news outlets. recent study finds that 55% of URLs used in notes were related to news websites, 18% to research, 9% to social media, 9% to encyclopedic sources, but just 1.2% to factchecking sources (Kangur et al., 2024). Limitations of notes. Only 11% of submitted notes reach helpful status (i.e., shown to users) by achieving cross-perspective (Renault et al., 2024; Wirtschafter and Majumder, 2023), and the time frame for notes to reach the algorithms required agreement level (15.5 hours on average) limits its capacity to halt misinformation spread (Renault et al., 2024). Additional concerns about the notes efficacy highlight their indifference to the expertise needed for certain claims and reliance on subjective helpfulness rather than objective facts, free labour and inadequate support and guardrails regarding explicit content (Gilbert, 2025). Our work provides novel insights into the targets, sources and limitations of community notes by shedding light on the relationship between notes and professional fact-checking. Namely, we study the extent to which fact-checking sources form the basis of note-writers efforts to counter misinformation and identify the strategies they employ. 2.1."
        },
        {
            "title": "Impact of Community Notes on\nmisinformation spread",
            "content": "Posts identified by community notes as misleading have been found to attain less virality (reposts, quote tweets and replies) than non-misleading posts (Drolsbach and Pröllochs, 2023; Renault et al., 2024). Community notes have also been shown to increase the probability of tweet retractions and deletions and speed up the retraction process (Gao et al., 2024; Renault et al., 2024). However, other studies have found less positive evidence; for example, that users followers, likes and engagement increase after their post receives community note (Wirtschafter and Majumder, 2023). Curiously, one study claims that showing community notes on posts reduced the spread of misleading posts by an average of 61% (Chuai et al., 2024a), while more recent analysis by the same authors found no effect of community notes on engagement with misinformation (Chuai et al., 2024c). People shown community notes alongside misleading social media posts were more accurate in identifying misleading posts, and the notes were judged to be more trustworthy than contextfree misinformation flags (e.g., \"Checked by factcheckers\" or \"Checked by other social media users\"), regardless of (US-centric) political beliefs (Drolsbach et al., 2024). People shown either community notes or related news article suggestions were both less likely to to believe and report misleading information compared to control group: community notes were more effective in reducing belief and sharing intention for positive rumours, while articles were more effective for negative rumours (Kankham and Hou, 2024). On the other hand, displaying community notes leads users to post more negative and angry replies to misleading posts (Chuai et al., 2024b), while crowd workers are also prone to cognitive biases, such as overestimating statements truthfulness the more they liked its claimant, and general overconfidence in their ability to ascertain truthful statements (Draws et al., 2022)."
        },
        {
            "title": "2.2 Professional fact-checking and community",
            "content": "garding the biases of the note-writers. Numerous studies have documented the structured workflow that fact-checkers follow: (i) claim selection; (ii) collecting evidence; (iii) deciding on verdict; and (iv) writing the fact-checking article (Graves, 2017; Micallef et al., 2022; Warren et al., 2025). Fact-checking articles, which are subject to multiple rounds of editorial scrutiny, are more formal and standardised in tone and style than community notes, which vary considerably. Fact-checkers must rely on credible sources and evidence to convince the reader, while community note writers may employ range of persuasion techniques, such as appeals to emotion or other logical fallacies. Moreover, community notes typically serve as direct rebuttals to misleading posts, while fact-checking articles may address more general claim than is expressed in specific post. Finally, fact-checking articles are one-way exchange, while community notes represent more horizontal and interactive dialogue between writer and recipient of the factcheck (Kankham and Hou, 2024). Our work builds on current understanding of the relationship between professional fact-checking and amateur community moderation by examining the extent to which community note writers deploy the work of professional fact-checkers in their notes. note practices"
        },
        {
            "title": "3 Dataset",
            "content": "Although fact-checks and community notes share similarities in how they address misleading claims, they also differ in key elements of practice and techniques of persuasion and communication (Kankham and Hou, 2024). Fact-checking typically involves the analysis and verification of public claims (e.g., statements in news reports and social media). In addition to verifying claims, in recent years many fact-checking organisations have also assumed wider role in combatting misinformation spread, conducting long-term investigative journalism projects and citizen media literacy programs (Juneja and Mitra, 2022). Professional fact-checkers in organisations signatory to the International Fact-Checking Network (IFCN) follow rigorous set of principles and transparency commitments.2 In contrast, any platform user can contribute to community notes under anonymity, and the rating approach relies on the wisdom of crowds, with little oversight or transparency reWe download files containing all community notes and their metadata from the official website,3 which amounts to 1.5M notes authored between January 28th 2021 and January 6th 2025. Of these, total of 135K are rated by the community as Helpful, 51K are rated Not helpful, and 1.3M are unpublished, i.e., did not receive enough community ratings to reach verdict. See Fig. 6 in App. for statistics. We filter the notes as follows. First, we remove 526K non-English notes, which we identify by applying the language detection library fastlangdetect.4 Then, we further filter 268K unnecessary notesnotes attached to tweets that are classified by the community as not misleading. Finally, to focus only on notes that are used to address misinformation, we filter out 44K notes that contain one of the words ad, spam, or phishing. Following these filtration steps, we are left 3https://communitynotes.x.com/guide/e n/under-the-hood/download-data 2https://www.ifcncodeofprinciples.poy 4https://github.com/LlmKira/fast-langd nter.org/the-commitments etect Figure 2: The categories of links used by Community notes authors as source. with dataset containing 664K notes. The next step involves categorising the sources that the note authors use to support their claims. First, we use regex to extract all the URLs found in the notes. See Tab. 4 in App. for list of the top-100 most common domains. We classify each URL in our dataset of 664K notes into one of 13 categories (detailed in Fig. 2) using the pipeline described below. 1. Check whether the domain name of the URL is found in manually curated list of domains of professional fact-checking organisations (See Tab. 3 in App. for the full list). If so, classify the URL as fact-checking. 2. Otherwise, search for paraphrases of the word fact-check in the URL,5 and classify it as fact-checking if match was found. 3. Otherwise, check whether the domain name is found in Tab. 4, which the authors of this paper manually annotated. 4. Otherwise, use GPT-46 to classify the domain name into one of the 13 categories. Listing 1 in App. details the prompt we used. 5. Finally, if GPT-4 fails or outputs an unknown category, label the URL as unknown. Using this pipeline, we successfully classify 95% of the URLs to one of the 13 categories. Moreover, we further subsample the notes for performing the in-depth analysis required for answering RQ2 (4.2). From the notes rated as Helpful, we sample 3.5K notes with Fact-checking source and random sample of 22K additional notes. We then used web crawling to scrape the text of the posts to which these notes were attached. We name this subset Stext for simplicity."
        },
        {
            "title": "4 Analysis",
            "content": "We analyse the dataset prepared in 3 to answer the two research questions defined in 1. 5These URLs mostly link to the fact-checking divisions of news outlets, e.g., https://apnews.com/article/f act-checking-909101991741 6Version gpt-4o-2024-08-06. Figure 3: Mean scores of community annotations of misleading posts."
        },
        {
            "title": "4.1 RQ1: To what degree do community notes",
            "content": "rely on fact-checkers? According to Fig. 2, at least 5% of all English community notes contain an external link to professional fact-checkers. This number grows to 7% when only considering notes rated as helpful (Fig. 7 in App. A). Conversely, only 1% of notes rated as not helpful contain fact-checking source (Fig. 8 in App. A). These figures are significantly larger than what was reported in previous studies (1.2% (Kangur et al., 2024)), possibly because Kangur et al. (2024) utilise smaller dataset of fact-checking agencies and classify fact-checking divisions of popular journals as news. The results imply that notes incorporating fact-checking sources are generally considered more helpful. We further assess whether notes with factchecking sources are perceived to be of higher quality by analysing individual user ratings of notes both with and without such sources. Specifically, we collect user ratings for balanced (i.e., including of fact-checking source or not) sample of 20K notes rated by at least 50 users, and calculated the average ratings for the notes. As can be seen in Fig. 9 in App. A, community notes with fact-checking sources are generally rated higher than their counterparts. Interestingly, while notes Figure 4: (a) strategies in debunking claims related to broader narratives. (b) the different ways in which factchecking sources are used to debunk claims. with fact-checking links are more likely to be regarded as having good source (higher HelpfulGoodSources), they are also more likely to be rated as notHelpfulSourcesMissingOrUnreliable. Tab. 5 in App. contains sample of such notes."
        },
        {
            "title": "4.2 RQ2: What are the traits of posts and",
            "content": "notes that rely on fact-checking sources? We begin by performing topic analysis, comparing topics of posts whose notes reference factchecking sources to those citing other sources. To this end, we apply strong zero-shot text classification model7 to our Stext subset by classifying spans of the form Tweet:<POST TEXT>; Note <NOTE TEXT> into one of 13 classes. The authors manually evaluated the quality of the classification results and considered it satisfactory. Notably  (Fig. 5)  , fact-checking sources are more likely to be included in posts related to high-stakes issues such as health, science, and scams and less likely to be included in posts on tech or sports. We then analyse annotations (binary attributes explaining the warrant for the note) by community note authors. Fig. 3 contains the full breakdown of annotations for notes with and without fact-checking sources. Notes containing link to fact-checking sources are overrepresented in posts where unverified information is presented as fact or when the post contains factual error. Conversely, they are under-represented in posts with outdated information or satirical content. Tab. 1 contains sample of such notes. These results indicate that community noteFigure 5: Distribution of notes topics, with and without fact-checking source. writers adapt their strategies based on the stakes and scope of the claim, and the depth of research needed to counter misinformation. We hypothesise that they are more likely to rely on external fact-checking when refuting complex or unverifiable claims (Wuehrl et al., 2024), as well as claims related to broader narratives or conspiracy theories which cannot be fully addressed in the scope of note.8 Conversely, claims involving misleading media can often be debunked with examples alone, making fact-checking sources unnecessary. To investigate this hypothesis, the authors of this paper manually annotated 400 < post, note > pairs from Stext with attributes related to the complexity of the claims and how community notes address them. (See App. B.1 for annotation guidelines). The results (Fig. 4.a) support our hypothesis. Claims related to broader narratives or conspiracy theories are much more likely to include link to factchecking source. In contrast, other types of claims are more likely to be addressed by providing missing context or by invalidating the credibility of the claims source. Additionally, Fig. 4.b depicts the different ways in which fact-checking sources are used to debunk claims. It demonstrates how such sources are rarely used to provide missing context but rather focus on discrediting sources of claims and providing scientific evidence. We extend the manual annotation to an LLMbased analysis of 8K balanced (post, note) pairs from Stext. We task OpenAIs GPT-49 with determining whether pair relates to broader narrative or conspiracy theory. Listing 2 in App. details the prompt used. To evaluate model accuracy, two authors independently labelled 100 balanced pairs, achieving an agreement rate of 0.88 and resolving disagreements through discussion. The model at7https://huggingface.co/r-f/ModernBER T-large-zeroshot-v1 with default settings. 8For example, the claim Michelle Obama is male. 9Version gpt-4o-2024-08-06. l i n i fi l A c s d O a I r i i a g t E r t c g u i a g i Tweet Note The NASA War Document is absolutely terrifying https://t.co/... BREAKING NEWS: International Criminal Investigation calls on every public citizen to recommend indictments for Bill Gates, Anthony Fauci, Pfizer, BlackRock, Tedros and Christian Drosten for pushing everyone to receive the ineffective highly dangerous lethal experimental vaccines... misrepresenting presentation by NASA scientist Dennis Bushnell, The lecture was not detailing plans by NASA to attack the world it was lecture for defense industry professionals, and how defense tactics might rise to meet evolving threats in the future. https://leadstories.com/hoax-alert/2 021/06/fact-check-the-future-is-now-is-n ot-a-nasa-war-document-plan-for-world-d omination-and-phasing-out-of-humans.html Video has been fact-checked by USA Today, was found to be misleading, and promotes conspiracy theory about COVID ... https://ca.movies.yahoo.com/movies/fact-c heck-viral-video-promotes-204414488.html 1) California is RED. It is just because of the MASSIVE Election Fraud that stupid, brainwashed people believe Calif. is blue. Joe Biden won only in the SFO Bay area ... The map shows the results of Reagans reelection in 1984, not Bidens election in 2020. https://en.wikipedia.org /wiki/1984_United_States_presidential_el ection_in_California Davis blows up $100,000 fireworks in Kai Cenat setup During the Mr Beast Stream ... @cnviolations swear community notes are the only good thing Elon added since he bought Twitter. Thailand will become the first country to make the contract null and void, meaning that Pfizer will become responsible for all vaccine injuries ... Hilarious tweets by footballers, thread: 1. Virgil Van Dijk [Current Liverpool Captain] https://t.co/... The second photo is from house fire in Atlanta in 2019. http s://www.11alive.com/article/news/local/w oodland-brook-drive-cause-of-house-fire/ 85-ecb7df9b-5f65-44e9-bf9d-8c162d36c334 Community notes was first launched under former Twitter CEO Jack Dorsey in 2021 under the name of Birdwatch. The only thing Elon Musk did was that he renamed the feature to community notes. https://blog.twitter.com/en_ us/topics/product/2021/introducing-birdw atch-a-community-based-approach-to-misin formation https://www.reuters.com/article/ factcheck-elon-birdwatch-idUSL1N31Z2VG/ Thailand has no plans to void its Pfizer COVID vaccine contract, an official with the countrys National Vaccine Institute said. Thailands Department of Disease Control also rejected the claims as fake news. ... https://apnews.com/artic le/fact-check-covid-vaccine-pfizer-thail and-203948163859 Virgil Van Dijk did not tweet this, the tweet was made by fan account in his name. https://www.pinkvilla.com/ sports/fact-check-did-virgil-van-dijk-r eally-root-for-man-u-because-no-one-lik es-liverpool-in-resurfaced-viral-tweet-1 287250 Rob Reiner announces hes on the Epstein Client List and Epstein Flight logs. What fool! When lawyer tells me to STFU, STFU! https://t.co/... This is digitally altered photo that might be misinterpreted even if used as joke. The name Rob Reiner is misspelled, and the text is not on Reiners timeline. https://twitter. com/robreiner?t=iqu43-NszIW5oOM_KqRSpw Table 1: sample of tweets, notes, and their community annotations, as well as whether the note contains fact-checking link. FC source - n c 22% 11% 28% 39% Table 2: Percentage of samples related to broader narrative or conspiracy vs. have fact-checking source. tained an F1 score of 0.85strong performance for this challenging task. The results (Tab. 2) support our hypothesis: pairs related to broader narrative or conspiracy theory are twice as likely to cite fact-checking sources compared to other sources. In contrast, other pairs are nearly 30% less likely to do so. These findings also highlight the prevalence of such claims and further underscore the importance of fact-checking in combating complex misinformation narratives."
        },
        {
            "title": "Our",
            "content": "results In this work, we annotate large corpus of Twitter community notes with attributes such as topic, cited sources, and whether they refute claims tied to broader misinformation narratives. We find that effective community moderation depends on professional fact-checking to an extent far greater than previously reported. We find that community notes linked to broader narratives or conspiracy theories are particularly reliant on fact-checking. reveal that community notes and professional fact-checking are deeply interconnectedfact-checkers conduct in-depth research beyond the reach of amateur platform users, while community notes publicise their work. The move by platforms to end their partnerships and funding for fact-checking organisations will hinder their ability to fact-check and pursue investigative journalism, which community note writers rely on. This, in turn, will limit the efficacy of community notes, especially for high-stakes claims tied to broader narratives or conspiracies."
        },
        {
            "title": "Limitations",
            "content": "The main limitations of our work concern the characteristics of the dataset we analyse. First, we restrict our analysis to notes written in English, excluding over half million notes in other languages. This decision was made to avoid potential noise and biases arising from the authors unfamiliarity with public discourse in different regions and reliance on machine translation. In future work, we aim to extend our analysis to other languages. Moreover, except for small subset of notes, we did not have access to the original tweets they were written for. Even when the tweet text was available, many contained non-text media, were written in internet vernacular that was challenging to interpret, or lacked important context. These factors limit the accuracy and effectiveness of our models and analysis. Finally, due to resource constraints, our manual annotation study was limited to relatively small sample of tweets and notes. In future work, we wish to utilise crowd workers to not only annotate larger dataset but also increase the diversity and perspective of the annotators."
        },
        {
            "title": "Broader Impact and Ethical\nConsiderations",
            "content": "Given that this work analyses real-world posts, ethical concerns may arise from using this data for research purposes. Posts from non-protected accounts and Community Notes on Twitter/X are publicly available, however, we acknowledge that they may contain sensitive personal information. To minimise any breach of anonymity and privacy, we anonymised links to individual accounts, and we do not publicly release this information. We do not analyse the posts or notes by individual users, and instead examine aggregated data in the form of topics and sources cited. Although the Community Notes dataset represents attempts to curb harmful misinformation and conspiracies, given the intense partisanship involved (Allen et al., 2022; Draws et al., 2022), as well as the explicit content of some claims, some instances may be considered offensive. We also acknowledge that our own perspectives and biases as authors shape the impact of our findings in certain ways. For example, as mentioned in the previous section, we were unable to analyse non-English posts in-depth, so our conclusions are likely somewhat focused on discourse in the Anglosphere (e.g., the US, UK, Ireland, Canada, Australia, New Zealand etc.). Furthermore, although we based our criteria for conspiracy theories on well-established sources, e.g., AP News, FactCheck.org, the European Commission, and identified conspiratorial narratives from both leftand right-wing sources, our own perspectives (i.e., as scientists from Western countries) may also have impacted what we considered to be conspiracy theories."
        },
        {
            "title": "Acknowledgements",
            "content": "This research was co-funded by the European Union (ERC, ExplainYourself, 101077481), by the European Unions Horizon 2020 research and innovation program under grant agreement No. 101135671 (TrustLLM), and by the Pioneer Centre for AI, DNRF grant number P1."
        },
        {
            "title": "References",
            "content": "Jennifer Allen, Cameron Martel, and David G. Rand. 2022. Birds of feather dont fact-check each other: Partisanship and the evaluation of news in twitters birdwatch crowdsourced fact-checking program. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, CHI 22, New York, NY, USA. Association for Computing Machinery. Phoebe Arnold. 2020. The challenges of online fact checking: how technology can (and cant) help. Technical report, Full Fact. Isabelle Augenstein, Timothy Baldwin, Meeyoung Cha, Tanmoy Chakraborty, Giovanni Luca Ciampaglia, David Corney, Renee DiResta, Emilio Ferrara, Scott Hale, Alon Halevy, Eduard Hovy, Heng Ji, Filippo Menczer, Ruben Miguez, Preslav Nakov, Dietram Scheufele, Shivam Sharma, and Giovanni Zagni. 2024. Factuality challenges in the era of large language models and opportunities for fact-checking. Nature Machine Intelligence, pages 112. Rebecca Catalanello and Katie Sanders. 2025. Meta is ending its third-party fact-checking partnership with us partners. heres how that program works. Yuwei Chuai, Moritz Pilarski, Gabriele Lenzini, and Nicolas Pröllochs. 2024a. Community notes reduce the spread of misleading posts on X. Yuwei Chuai, Anastasia Sergeeva, Gabriele Lenzini, and Nicolas Pröllochs. 2024b. Community Fact-Checks Trigger Moral Outrage in Replies to Misleading Posts on Social Media. ArXiv:2409.08829 [cs]. Yuwei Chuai, Haoye Tian, Nicolas Pröllochs, and Gabriele Lenzini. 2024c. Did the Roll-Out of Community Notes Reduce Engagement With Misinformation on X/Twitter? Proceedings of the ACM on Human-Computer Interaction, 8(CSCW2):152. Suárez Vicente Javier Clemente, Eduardo NavarroJiménez, Juan Antonio Simón-Sanjurjo, Ana Isabel Beltran-Velasco, Carmen Cecilia Laborde-Cárdenas, Juan Camilo Benitez-Agudelo, Álvaro BustamanteSánchez, and José Francisco Tornero-Aguilera. 2022. Misdis information in covid-19 health crisis: narrative review. International Journal of Environmental Research and Public Health, 19(9). Nicholas Diakopoulos. 2020. Computational news discovery: Towards design considerations for editorial orientation algorithms in journalism. Digital journalism, 8(7):945967. Tim Draws, David La Barbera, Michael Soprano, Kevin Roitero, Davide Ceolin, Alessandro Checco, and Stefano Mizzaro. 2022. The Effects of Crowd Worker Biases in Fact-Checking Tasks. In 2022 ACM Conference on Fairness, Accountability, and Transparency, pages 21142124, Seoul Republic of Korea. ACM. Chiara Patricia Drolsbach and Nicolas Pröllochs. 2023. Diffusion of Community Fact-Checked Misinformation on Twitter. Proceedings of the ACM on HumanComputer Interaction, 7(CSCW2):122. Chiara Patricia Drolsbach, Kirill Solovev, and Nicolas Pröllochs. 2024. Community notes increase trust in fact-checking on social media. PNAS Nexus. Daniela Flamini. 2019. Most republicans dont trust fact-checkers, and most americans dont trust the media. Yang Gao, Maggie Zhang, and Huaxia Rui. 2024. Can Crowdchecking Curb Misinformation? Evidence from Community Notes. Sarah Gilbert. 2025. Three reasons Meta will struggle with community fact-checking. MIT Technology Review. Google. 2025. Misinformation policies - youtube help. Lucas Graves. 2017. Anatomy of fact check: Objective practice and the contested epistemology of fact checking. Communication, culture & critique, 10(3):518537. Lucas Graves and C.W. Anderson. 2020. Discipline and promote: Building infrastructure and managing algorithms in structured journalism project by professional fact-checking groups. New Media & Society, 22(2):342360. IFCN. 2024. State of the fact-checkers report. Technical report, International Fact-Checking Network. Prerna Juneja and Tanushree Mitra. 2022. Human and technological infrastructures of fact-checking. Proc. ACM Hum.-Comput. Interact., 6(CSCW2). Uku Kangur, Roshni Chakraborty, and Rajesh Sharma. 2024. Who Checks the Checkers? Exploring Source Credibility in Twitters Community Notes. ArXiv:2406.12444 [cs]. Sarawut Kankham and Jian-Ren Hou. 2024. Community Notes vs. Related Articles: Assessing RealWorld Integrated Counter-Rumor Features in Response to Different Rumor Types on Social Media. International Journal of HumanComputer Interaction, pages 115. Amelie Wuehrl, Yarik Menchaca Resendiz, Lara Grimminger, and Roman Klinger. 2024. What makes medical claims (un)verifiable? analyzing entity and relation properties for fact verification. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 20462058, St. Julians, Malta. Association for Computational Linguistics. Laurens Lauer and Lucas Graves. 2024. How to grow transnational field: network analysis of the global fact-checking movement. New Media & Society, 0(0):19. Cameron Martel, Jennifer Allen, Gordon Pennycook, and David G. Rand. 2024. Crowds can effectively identify misinformation at scale. Perspectives on Psychological Science, 19(2):477488. PMID: 37594056. Meta. 2025. More speech and fewer mistakes. Nicholas Micallef, Vivienne Armacost, Nasir Memon, and Sameer Patil. 2022. True or false: Studying the work practices of professional fact-checkers. Proc. ACM Hum.-Comput. Interact., 6(CSCW1). Garrett Morrow, Briony Swire-Thompson, Jessica Montgomery Polny, Matthew Kopec, and John Wihbey. 2022. The emerging science of content labeling: Contextualizing social media content moderation. Journal of the Association for Information Science and Technology, 73(10):13651386. Moritz Pilarski, Kirill Olegovich Solovev, and Nicolas Pröllochs. 2024. Community Notes vs. Snoping: How the Crowd Selects Fact-Checking Targets on Social Media. Proceedings of the International AAAI Conference on Web and Social Media, 18:12621275. Nicolas Pröllochs. 2022. Community-based factchecking on twitters birdwatch platform. In Proceedings of the International AAAI Conference on Web and Social Media, volume 16, pages 794805. Merten Reglitz. 2022. Fake news and democracy. J. Ethics & Soc. Phil., 22:162. Thomas Renault, David Restrepo Amariles, and Aurore Troussel. 2024. Collaboratively adding context to social media posts reduces the sharing of false news. ArXiv:2404.02803 [econ]. Mark Stencel, Erica Ryan, and Joel Luther. 2024. With half the planet going to the polls in 2024, factchecking sputters. Technical report, Duke Reporters Lab. Twitter/X. 2021. Introducing birdwatch, communitybased approach to misinformation. Twitter/X. 2024a. Note ranking algorithm. Twitter/X. 2024b. Rating and writing impact. Greta Warren, Irina Shklovski, and Isabelle Augenstein. 2025. Show me the work: Fact-checkers requirements for explainable automated fact-checking. In Proceedings of the CHI Conference on Human Factors in Computing Systems, CHI 25, New York, NY, USA. Association for Computing Machinery. Valerie Wirtschafter and Sharanya Majumder. 2023. Future challenges for online, crowdsourced content moderation: Evidence from twitters community notes. Journal of Online Trust and Safety, 2(1). was edited (edited with Photoshop, the clip was cut, etc.). Link to direct source If the community note shares link to source where an entity says that claim made about them is false. Link official source If the community note shares link to an official source such as government website. Link scientific source If the community note shares link to some scientific article or website. Link world knowledge If the community note shares link to some reference resources such as Wikipedia. Link fact-checking If the community note shares link to professional fact-checking organisation. In-note fact-checking If the community note performs an in-note fact-check by crossreferencing several sources and constructing compelling argument."
        },
        {
            "title": "A Additional Material",
            "content": "This section details additional results or material referenced from the papers main body. Fig. 6 histogram of the number of community notes written every month and their rating (helpful, not helpful, or needs more data). Fig. 7 The categories of links used by Community notes authors as source, filtering for notes rated as helpful. Fig. 8 The categories of links used by Community notes authors as source, filtering for notes rated as not helpful. Fig. 9 Community ratings of notes with and without fact-checking source. Tab. 3 List of professional fact-checking organisations and their URLs. Tab. 3 List of top 100 most common domains found in the community notes dataset, and their categorization. Tab. 5 Examples of community notes containing fact-checking sources that are rated as having notHelpfulSourcesMissingOrUnreliable."
        },
        {
            "title": "B Reproducibility",
            "content": "Listing 1 The prompt used to classify URLs into categories. Listing 2 The prompt used to classify tweets and notes into broader narratives and conspiracy theories. B.1 Manual Annotation Setup We annotate 400 (tweet, note) pairs from Stext with 12 binary attributes. Each (tweet, note) pair was annotated in multi-label fashion, i.e., more than one attribute can be selected at the same time. Fig. 10 depict our simple annotation setup, with the 12 attributes being as follows. Broader narrative Whether the (tweet, note) pair is related to broader narrative or conspiracy theory. Discredit source of claim If the community note describes the source shared by the original post as non-credible. Add missing context If the community note provides some missing context to refute claim. Highlight AI generated If the community note claims that the post shared AI-generated content."
        },
        {
            "title": "Language",
            "content": "Region/domain Lead stories AFP Factuel AAP FactCheck Full Fact Science Feedback Politifact HoaxEye Logically Facts FactCheckNI DFRLab FactReview Lupa Check your fact Climate feedback Factcheck Health feedback Snopes aosfatos Demagog FakeReporter litmus factcheck Climate Feedback AFP USA Today Statesman Dallas News Google Fact Check MediaBias/FactCheck MedDMO Poynter Newsmeter Africa Check Fact Crescendo India Factseeker Fact Crescendo Thailand Fact Crescendo Afghanistan Only Fact Factly Fact Crescendo Sri Lanka Fact Crescendo Cambodia Becid Fact Hunt Tec Arp 10 news RMIT Fact Check Gigafact Ayupp The Journal leadstories.com factuel.afp.com aap.com.au/factcheck fullfact.org science.feedback.org politifact.com hoaxeye.wordpress.com logicallyfacts.com factcheckni.org dfrlab.org factreview.gr lupa.uol.com.br/jornalismo checkyourfact.com climatefeedback.org factcheck.org healthfeedback.org snopes.com aosfatos.org demagog.org.pl/fake_news fakereporter.net litmus-factcheck.jp climatefeedback.org factcheck.afp.com usatoday.com/story/news/factcheck statesman.com dallasnews.com/news/politifact toolbox.google.com/factcheck mediabiasfactcheck.com meddmo.eu poynter.org/fact-checking newsmeter.in/fact-check africacheck.org english.factcrescendo.com factseeker.lk thailand.factcrescendo.com afghanistan.factcrescendo.com onlyfact.in factly.in srilanka.factcrescendo.com cambodia.factcrescendo.com becid.eu facthunt.in techarp.com 10news.com/news/fact-or-fiction rmit.edu.au gigafact.org ayupp.com/fact-check thejournal.ie English French English English English English, Spanish English Multiple English English Greek Portuguese English English English English English Portuguese Polish Hebrew Japanese English English English English English English English English, Greek English English, Tamil English English English Thai Persian English English Sinhala Cambodian Baltic langs English English English English English English English Global Global Australia Global Science USA Images Europe/India North Ireland Global Global Global Global Climate USA Health US Global Poland Israel Japan Global Global USA USA USA Global Global Greece, Cyprus, Malta USA India Africa India Sri Lanka Thailand Afghanistan India India Sri Lanka Cambodia Baltic India Global (based in Malaysia) USA Australia USA India Ireland Table 3: List of professional fact-checking organisations and their URLs."
        },
        {
            "title": "Domain",
            "content": "social media x.com social media twitter.com social media youtube.com social media youtu.be organisation un.org news u.today social media t.co fact checking snopes.com reference en.m.wikipedia.org reference en.wikipedia.org search engine google.com social media instagram.com reference britannica.com news reuters.com news bbc.co.uk news apnews.com news bbc.com news nytimes.com news theguardian.com news vice.com news usatoday.com fact checking factcheck.org cnn.com news washingtonpost.com news ncbi.nlm.nih.gov nbcnews.com help.twitter.com cdc.gov npr.org forbes.com newsweek.com fullfact.org dailymail.co.uk cbsnews.com web3antivirus.io timesofisrael.com help.x.com nypost.com aljazeera.com reddit.com independent.co.uk usgs.gov abcnews.go.com nature.com gov.uk web.archive.org foxnews.com tiktok.com edition.cnn.com academic news reference government news news news fact checking news news database news reference news news social media news academic news academic government database news social media news thehill.com amp.theguardian.com whitehouse.gov news.sky.com merriam-webster.com techarp.com cbc.ca politifact.com pbs.org telegraph.co.uk businessinsider.com time.com justice.gov cnbc.com wsj.com sciencedirect.com msn.com statista.com business.x.com amp.cnn.com congress.gov factcheck.afp.com yahoo.com timesofindia.indiatimes.com thelancet.com hrw.org healthfeedback.org fda.gov m.youtube.com law.cornell.edu medium.com healthfeedback.org who.int haaretz.com axios.com mayoclinic.org nejm.org scienceexchange.caltech.edu indiatoday.in bloomberg.com pewresearch.org jamanetwork.com leadstories.com dictionary.cambridge.org jpost.com archive.ph healthline.com abc.net.au france24.com"
        },
        {
            "title": "Category",
            "content": "news news government news reference news news fact checking commercial news news news government news news academic news reference commercial news government fact checking search engine news academic organisation fact checking government social media academic blog post fact checking organisation news news commercial academic academic news news academic academic news reference news database commercial news news Table 4: List of top 100 most common domains found in the community notes dataset, and their categorization."
        },
        {
            "title": "ID summary",
            "content": "0 1 2 3 4 This claim ruled mostly false. https://www.politifact.com/factchecks/ 2020/may/07/facebook-posts/facebook-post-cites-doctors-widel y-disputed-calcul/ The RedState article claims the shots do not stop transmission of the virus. This is false. Vaccines provide significant protection from getting it infection and spreading it transmission even against the delta variant. Source: https://www.usatoday.com /story/news/factcheck/2021/11/17/fact-check-covid-19-vaccine s-protect-against-infection-transmission/6403678001/ There is no proof of this, the photo is real, its not the last photo of the child. But snoops say there is tenuous link the parents used the same law firm to represent them as Maxwell https://www.snopes.com/fact-check/ghislaine-maxwell-jonbene t-ramsey/ unfounded https://www.snopes.com/fact-check/ashley-biden-diary -afraid/ The mRNA vaccine does not cause cancer: https://www.factcheck.org/2024/0 5/still-no-evidence-covid-19-vaccination-increases-cancer-r isk-despite-posts/"
        },
        {
            "title": "5 Many of the details in this popular essay are inaccurate and too numerous to list here. The\nessay was fact checked by Snopes in 2005: https://www.snopes.com/fact-che\nck/the-price-they-paid/\nPOLITIFACT - rates False. The report analysed a small sample of 128 temp stations out of\nseveral thousand volunteer-run stations, then extrapolated results. NOAA uses 2 programs to\nrecord daily temps. The report did not look at the 900 more sophisticated automated stations.\nhttps://www.politifact.com/factchecks/2022/aug/19/facebook-p\nosts/fact-checking-talking-point-about-corrupted-climat/\nThere is no verifiable evidence of campaign espionage in either the 2020 or the 2016\npresidential elections. https://www.snopes.com/fact-check/obama-spyin\ng-trump-campaign/ https://www.washingtonpost.com/politics/20\n19/05/06/whats-evidence-spying-trumps-campaign-heres-your-g\nuide/\nLadapo did get caught altering COVID vaccine study findings. Ladapo replaced the language\nfrom an earlier study draft that found no significant risk from COVID vaccines, to then state\nthere was a high risk https://healthfeedback.org/claimreview/analysi\ns-florida-department-health-surgeon-general-joseph-ladopo-c\nontains-multiple-methodological-problems-covid-19-mrna-vacci\nnes/ https://healthexec.com/topics/clinical/COVID-19/florid\na-surgeon-general-altered-covid-19-study-findings",
            "content": "8 Table 5: Examples of community notes containing fact-checking sources that are rated as having notHelpfulSourcesMissingOrUnreliable. Figure 6: histogram of the number of community notes written every month and their rating (helpful, not helpful, or needs more data. The grey vertical line (December 2022) indicates the date when the community notes became visible worldwide. Figure 7: The categories of links used by Community notes authors as source, filtering for notes rated as helpful. Figure 8: The categories of links used by Community notes authors as source, filtering for notes rated as not helpful. Figure 9: Community ratings of notes with and without fact-checking source."
        },
        {
            "title": "SYSTEM PROMPT\nYou are a professional IT system who has a vast knowledge of the internet and its",
            "content": "content. Your goal is simple, but very important: Classify URLs into categories. Choose only from the provided categories! USER PROMPT Read the following URLs. Your goal is to categorize each url into one of the pre-defined categories. Chose from the following list of categories: Categories = [ such as the CDC, department of education, # Social media sites like Facebook, Twitter, Youtube etc. # Government agencies and organisations, as well as websites # Websites of news outlets or other organisations that report current \"social media\", \"news\", events, such as the nytimes, the guardian, etc. \"government\", related to policies and guidelines, FDA, etc. \"academic\", # Academic sources, journals, and magazines, such as pubmed, nature, sciencedirect, etc. \"blog post\", # Independent blog posts about various topics, including cooking, travel, home improvement, fandom, reviews, etc. \"fact checking\", \"database\", # Public databases such as google drive, archive.com, dropbox, etc. \"commercial\", etc. \"reference\", # Public resources such as encyclopedias, dictionaries, advocacy sources, guides, DIYs, statistics, religious sources, travel information, usage guidelines, Q&As, terms of services, etc. \"organisation\", the UN, Greenpeace, LA-Lakers, etc. \"other\", categories. \"unknown\", # if it is impossible to determine the category of the webpage. # Webpages of commercial organisations such as BMW, Delta, Nike, # non-commercial and non-government organisations such as WHO, # Any other website that does not fit into one of the previous # professional fact checking organisations ] Output format example: [ id: <ID>, url: <URL>, category: <CATEGORY>, { } ] URLs: <URLS> Listing 1: The prompt used to classify URLs into categories. SYSTEM PROMPT You are professional fact-checker who specializes in analyzing misinformation spread on social media. Your goal is to analyse tweet and community note written about the tweet and decide whether the tweet spread misinformation related to known conspiracy theory or misleading wider narrative, and if so, which one is it. USER PROMPT Read the following tweets and community notes written about them.nYour goal is to analyse them and decide whether each tweet spread misinformation related to known conspiracy theory or similar misleading wider narrative, and if so (and only if so!), which one. Include your reasoning. Output the results as json file. If tweet does not relate to conspiracy theory or misleading wider narrative, output \"none\" in the json. - Tweets *do not* discuss wider narrative if the misleading information is tied to specific singular event that is not connected to major topics on the public discourse."
        },
        {
            "title": "They do discuss a wider narrative if the misleading information is tied to a known",
            "content": "conspiracy theory or to major topics on the public discorse. Chose from the following list of theories and wider narrative: CONSPIRACY_THEORIES = [ September 11, October 7, the great replacement, COVID was intentionally spread, the COVID outbreak is fake, 2020 election fraud, vaccines cause autism, 5G towers, Russian invasion of Ukraine, flat earth, chemtrails, Q-Anon and deep state, Epstein files, Barack Obama was not born in the USA, Michelle Obama is man, LGBT grooming, fluorite in the water, climate change, Holocaust denial, Hunter Biden and Ukraine, other, ] Output format example: [ { } ] id: <ID>, is_related_to_conspiracy: <True/False>, conspiracy: <CONSOIRACY (or None)>, reasoning: <REASONING> Tweets and notes: <TWEETS_AND_NOTED> Listing 2: The prompt used to classify tweets and notes into broader narratives and conspiracy theories. Figure 10: Our annotation setup."
        }
    ],
    "affiliations": [
        "University of Copenhagen"
    ]
}