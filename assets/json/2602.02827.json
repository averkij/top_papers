{
    "paper_title": "Col-Bandit: Zero-Shot Query-Time Pruning for Late-Interaction Retrieval",
    "authors": [
        "Roi Pony",
        "Adi Raz",
        "Oshri Naparstek",
        "Idan Friedman",
        "Udi Barzelay"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multi-vector late-interaction retrievers such as ColBERT achieve state-of-the-art retrieval quality, but their query-time cost is dominated by exhaustively computing token-level MaxSim interactions for every candidate document. While approximating late interaction with single-vector representations reduces cost, it often incurs substantial accuracy loss. We introduce Col-Bandit, a query-time pruning algorithm that reduces this computational burden by casting reranking as a finite-population Top-$K$ identification problem. Col-Bandit maintains uncertainty-aware bounds over partially observed document scores and adaptively reveals only the (document, query token) MaxSim entries needed to determine the top results under statistical decision bounds with a tunable relaxation. Unlike coarse-grained approaches that prune entire documents or tokens offline, Col-Bandit sparsifies the interaction matrix on the fly. It operates as a zero-shot, drop-in layer over standard multi-vector systems, requiring no index modifications, offline preprocessing, or model retraining. Experiments on textual (BEIR) and multimodal (REAL-MM-RAG) benchmarks show that Col-Bandit preserves ranking fidelity while reducing MaxSim FLOPs by up to 5$\\times$, indicating that dense late-interaction scoring contains substantial redundancy that can be identified and pruned efficiently at query time."
        },
        {
            "title": "Start",
            "content": "Col-Bandit: Zero-Shot Query-Time Pruning for Late-Interaction Retrieval Roi Pony 1 Adi Raz 1 Oshri Naparstek 1 Idan Friedman 1 Udi Barzelay"
        },
        {
            "title": "Abstract",
            "content": "Multi-vector late-interaction retrievers such as ColBERT achieve state-of-the-art retrieval quality, but their query-time cost is dominated by exhaustively computing token-level MaxSim interactions for every candidate document. While approximating late interaction with single-vector representations reduces cost, it often incurs substantial accuracy loss. We introduce Col-Bandit, query-time pruning algorithm that reduces this computational burden by casting reranking as finite-population Top-K identification problem. Col-Bandit maintains uncertainty-aware bounds over partially observed document scores and adaptively reveals only the (document, query token) MaxSim entries needed to determine the top results under statistical decision bounds with tunable relaxation. Unlike coarse-grained approaches that prune entire documents or tokens offline, Col-Bandit sparsifies the interaction matrix on the fly. It operates as zero-shot, drop-in layer over standard multi-vector systems, requiring no index modifications, offline preprocessing, or model retraining. Experiments on textual (BEIR) and multimodal (REAL-MM-RAG) benchmarks show that Col-Bandit preserves ranking fidelity while reducing MaxSim FLOPs by up to 5, indicating that dense late-interaction scoring contains substantial redundancy that can be identified and pruned efficiently at query time. 6 2 0 2 2 ] . [ 1 7 2 8 2 0 . 2 0 6 2 : r 1. Introduction Multi-vector such as Collate-interaction retrievers, BERT (Khattab & Zaharia, 2020), have emerged as powerful alternative to single-vector dense retrieval. By representing each query and document as set of token embeddings, these models capture fine-grained semantic matches that single-vector representations miss (Wang et al., 2023; Formal et al., 2021). This paradigm has been widely adopted 1IBM Research Israel. Correspondence to: Roi Pony <roi.pony@ibm.com>. Preprint. February 4, 2026. 1 in recent text and multimodal systems (Faysse et al., 2024; Team, 2025a; Warner et al., 2025; Team, 2025b; Xu et al., 2025; Gunther et al., 2025), becoming standard foundation for high-accuracy neural retrieval. However, this granularity comes with cost. Unlike single-vector retrieval, where scoring is cheap dot product, exact late interaction requires evaluating grid of token-level operations (MaxSim) for every document. Consequently, this computation often becomes the bottleneck in modern pipelines, motivating methods that reduce these operations without sacrificing ranking fidelity (Santhanam et al., 2022a; Engels et al., 2023). The Hiring Analogy. To build intuition for the inefficiency of standard late interaction, consider manager hiring the top-K candidates from applicants. Each applicant takes short tests, and their final score is the sum of the results. Administering all tests to every applicant guarantees the correct shortlist, but it is wasteful. resource-efficient manager would allocate tests adaptively, giving few to everyone and focusing the remaining budget on the candidates whose ranking is unclear, stopping once the top-K is statistically certain. Standard late-interaction retrieval mirrors this wasteful strategy. It sums tokenwise interactions for every document, even though partial evaluation often suffices to rule documents out (or in). The Opportunity: Removing Redundancy. In the vectorset setting, the total score is sum of independent components. Naıvely, systems evaluate the full sum for every candidate in the pool D. However, for any specific query, we do not need to know the exact score of document that is clearly irrelevant, nor do we need perfect precision for clear winner. We only need enough information to distinguish the true Top-K documents from the rest. This implies that the computational budget should be spent asymmetrically, heavily on the borderline cases and sparsely on everything else. Our Approach: Col-Bandit. We propose viewing this resource allocation problem as progressive matrix completion. We treat the token-level scores as values in table that can be revealed on-demand. Our objective is to reveal just enough cells to confidently identify the Top-K set, minimizing computation while maintaining user-defined level of statistical reliability (Figure 1). To this end, we introduce Col-Bandit, purely query-time Col-Bandit: Zero-Shot Query-Time Pruning for Late-Interaction Retrieval Figure 1. Schematic of Col-Bandit. Given query (e.g., human mobility...) and set of candidate documents (e.g., Nature, Auto), the goal is to identify the Top-2 relevant documents. (A) Full ColBERT determines the exact score for every document by summing all interaction cells (MaxSims), requiring 100% of the compute budget. (B) Col-Bandit approximates these sums using partial cell observations. By adaptively revealing informative cells (green) and skipping others (hatched), it maintains confidence intervals for the total score. The algorithm terminates as soon as positive separation gap emerges: the Lower Bound of the weakest winner (Sports) is strictly higher than the Upper Bound of the strongest loser (Auto). This enables the identification of the correct Top-K ranking while saving 60% of the query-time computations. algorithm that operates directly on vanilla ColBERT. Unlike prior acceleration methods that require quantization or distilling document representations, Col-Bandit works on top of standard indices and model weights, requiring no index-time changes and no retraining. We formulate the task as finite-population Top-K identification problem. By exploiting the fact that document token sequences are finite, we utilize Serfling-type concentration inequalities (Bardenet & Maillard, 2015) to construct tighter confidence intervals than standard bandit approaches. We further introduce calibration parameter to optimize the trade-off between theoretical certification and practical FLOP reduction. Contributions. Formulation: We cast late-interaction reranking as finite-population Top-K identification problem using progressive scoring framework. Algorithm: We introduce Col-Bandit, Lower-Upper Confidence Bound (LUCB) (Kalyanakrishnan et al., 2012) style algorithm that leverages variance-adaptive Serfling bounds for tighter estimation and tunable relaxation parameter for efficiency. Drop-in Acceleration: We demonstrate substantial FLOP reductions on standard benchmarks without requiring any offline index modifications or model retraining. 2. Background and Related Work 2.1. Preliminaries: Late Interaction Retrieval ColBERT Late Interaction Scoring. Consider query and document from collection of size . ColBERT represents the query as set of token embeddings = {q1, q2, . . . , qT } RM , and each document as set of Ld token embeddings E(d) = {ed,1, ed,2, . . . , ed,Ld } RM , where is the embedding dimension, is the query length, and Ld is the length of document d. The ColBERT scoring function computes relevance through late interaction mechanism. For each query token [T ] (where [T ] {1, 2, . . . , }), ColBERT identifies the most similar document token using the MaxSim operation: h(d, t) max j[Ld] sim(ed,j, qt), (1) where sim(, ) is similarity function (typically cosine similarity).1 The final query-document score aggregates 1More generally, we assume sim is bounded in known interval [a, b] (e.g., [1, 1] for cosine similarity on normalized vectors), hence each h(d, t) is also bounded. 2 Col-Bandit: Zero-Shot Query-Time Pruning for Late-Interaction Retrieval Figure 2. Cost-Accuracy trade-off for Col-Bandit compared to Random Reveal (Doc-Uniform) and Greedy Top-Margin (Doc-TopMargin) across three retrieval settings (text and multimodal). Each star marker denotes Col-Bandit operating point obtained by sweeping the relaxation parameter αef . The top-right corner (Overlap@5 = 1.0, Cost = 100%) corresponds to full exhaustive scoring. these per-query-token maxima: S(d; Q) (cid:88) t=1 h(d, t). (2) Top-K Retrieval. The retrieval objective is to identify the set of documents from search set (e.g., candidate pool produced by an ANN stage) with the highest scores: K(Q) arg topK dD S(d; Q). (3) Index-Time vs. Query-Time. Retrieval systems separate index-time (offline) processing, which extracts representations and builds index structures, from query-time (online) computation, which encodes the query and ranks candidates. In late-interaction systems, the full similarity computation performed at query time is typically treated as reranking stage. Atomic Cost. We define the atomic cost unit of query-time work as computing single MaxSim value h(d, t) in Eq. 1. Standard exact reranking evaluates all such values, which can dominate query-time cost even after candidate retrieval. 2.2. Related Work We categorize related work by when and what they prune (Figure 3). Col-Bandit is the first to prune at the MaxSim operation level during query-time scoring. Index-Time Compression & Token Pruning. Approaches like PLAID (Santhanam et al., 2022a), ColBERTv2 (Santhanam et al., 2022b), and MUVERA (Dhulipala et al., 2024) accelerate retrieval via centroid-based compression, quantization, or fixed-dimensional encodings, improving the practicality of late-interaction methods that were initially constrained by considerable storage requirements. Additional system and indexing advances such as WARP (Scheerer et al., 2025) further improve scalability and usability. Similarly, token pruning methods (Lassance et al., 2021; Tonellotto & Macdonald, 2021) permanently discard non-informative tokens to reduce the index size (N ) or query length (T ), including near-lossless vector count reduction (Clavie et al., 2024) and approaches that use fixed number of representative tokens (MacAvaney et al., 2025). While effective, these methods are fixed at index-time and typically require offline modifications. Col-Bandit is orthogonal to these approaches, it operates purely at query-time on standard indices, dynamically pruning the atomic interaction matrix during scoring. Efficient Systems & Bound-Based Pruning. Systemlevel optimizations like DESSERT (Engels et al., 2023) use approximate retrieval to speed up candidate generation. In sparse retrieval, algorithms like WAND (Broder et al., 2003) and BMW (Ding & Suel, 2011) use score upper bounds to skip documents. Col-Bandit bridges these concepts, applying bound-based early stopping to dense late-interaction. Unlike WAND, which prunes inverted list pointers, we prune atomic MaxSim operations h(d, t) to certify the Top-K set with statistical guarantees. MaxSim-Level Pruning (Our Approach). To our knowledge, no prior work adaptively prunes interactions within the exact scoring loop. Existing methods reduce the number of candidates (N ) or tokens (T ) before scoring. Col-Bandit frames the scoring process itself as finite-population TopK identification problem, progressively revealing only the subset of MaxSim entries needed to certify the ranking. Finite-Population Bandits and Top-k Arm Identification. Our method is inspired by fixed-confidence Top-K Arm Col-Bandit: Zero-Shot Query-Time Pruning for Late-Interaction Retrieval Index-Time (Static / Offline) Efficient Late-Interaction Retrieval Query-Time (Dynamic / Online) Compression & Approx. (Quantization, Sketches) e.g., PLAID, DESSERT, ColBERTv2, MUVERA Doc Token Pruning (Remove tokens) e.g., Static Pruning Candidate Reduction (Reduce docs) e.g., HNSW, Query Token Pruning Interaction Pruning (Reduce MaxSims) e.g., Col-Bandit (Ours) Figure 3. Taxonomy of efficient late-interaction retrieval. Methods are classified by when they prune (index-time vs. query-time) and what they prune. Col-Bandit is the first to dynamically prune the atomic interaction matrix during query-time scoring. Identification in bandits (Kalyanakrishnan et al., 2012; Chen et al., 2014). The multi-armed bandit (MAB) framework has been extensively studied for resource-constrained selection problems, with two main paradigms: fixed-budget and fixed-confidence best arm identification (BAI). We focus on the fixed-confidence setting, where the goal is to identify the best arms with high probability while minimizing sample complexity, in our setting, we treat each document as an arm and connect reranking to Top-K identification. Standard algorithms include UCB (Auer et al., 2002) and UCB-E (Audibert & Bubeck, 2010), which typically assume infinite sampling with replacement. The LUCB (Lower-Upper Confidence Bound) framework (Kalyanakrishnan et al., 2012) provides an efficient strategy for Top-K identification by adaptively sampling arms based on confidence intervals, motivating our interval-driven reveal policy and stopping criterion. Late-interaction retrieval has fundamentally different structure: each document has finite set of token scores that can be sampled without replacement. Recent work has explored MAB techniques in related contexts, including prompt learning (Shi et al., 2024), LLM evaluation (Zhou et al., 2024), and approximate k-NN search (Indyk & Wagner, 2019). We adapt the fixed-confidence Top-K framework to exploit this finite-population structure. Specifically, we employ the BernsteinSerfling inequality (Bardenet & Maillard, 2015) to derive variance-adaptive confidence bounds that shrink deterministically as document is fully scored, providing tighter guarantees than standard infinitepopulation bandit bounds. 3. Problem Formulation We formalize the efficient retrieval problem as sequential decision process over sparsely observed matrix, mapping the task to fixed-confidence Multi-Armed Bandit (MAB) setting with finite populations. 3.1. The MaxSim Matrix and Observation Model Consider query with tokens and search set of documents, = {d1, . . . , dN }. We define the implicit MaxSim Matrix, RN , where each entry corresponds to the maximum similarity (1) of query token with documents tokens: Hi,t h(di, t) = max j[Ldi ] sim(edi,j, qt). (4) The total late-interaction score for document is the rowsum: Si (cid:88) t=1 Hi,t. (5) correOur objective is to identify the set of indices sponding to the documents with the highest scores Si. At any time step, the algorithm has access to an observed set of entries Ω [N ] [T ]. For each document i, we denote the set of observed token indices as Oi {t : (i, t) Ω} and the unobserved counterpart as Ui [T ] Oi. Revealing new entry (i, t) / Ω incurs unit cost, returns the exact value Hi,t, and updates Ω Ω {(i, t)}. We measure computational cost via coverage, defined as the fraction of the matrix revealed. At any time step of our algorithm the cost is: γ(Ω) Ω = 1 N (cid:88) i=1 Oi. (6) 3.2. Mapping to Finite-Population Bandits This formulation mirrors the Best-K Identification problem in stochastic bandits, where each document is an arm and Si is its mean reward (up to scaling factor ). However, two key structural properties distinguish our setting from standard literature: (i) Finite Population (Sampling without Replacement): Standard bandits assume that pulling arm reveals sample from an infinite distribution Pi. In contrast, our arm consists of fixed, finite population of values {Hi,1, . . . , Hi,T }. Repeatedly querying the same document samples these values without replacement. This implies that as Oi , the uncertainty about Si collapses to zero deterministically. (ii) Bounded Support: The similarity function is bounded (e.g., cosine similarity in [1, 1]), providing strict support [a, b] for all unobserved entries Hi,t. Col-Bandit: Zero-Shot Query-Time Pruning for Late-Interaction Retrieval 3.3. Objective: δ-PAC Identification We seek an adaptive policy π that decides which entry (i, t) to reveal next, and stopping rule τ . The algorithm must satisfy the Probably Approximately Correct (PAC) condition: (cid:16) ˆTK = (cid:17) 1 δ, (7) where ˆTK is the returned set and δ (0, 1) is user-defined error tolerance. Among all δ-PAC policies, we aim to minimize the expected coverage E[γ(Ωτ )]. 4. Method: Col-Bandit We view this problem as competitive matrix completion task: entries of the matrix are revealed adaptively until the Top-K documents can be separated. Col-Bandit maintains per-document decision bounds [LCBi, UCBi] that guide (i) where to allocate additional computation and (ii) when to stop. At each iteration, the algorithm compares the weakest current Top-K candidate against the strongest current non-Top-K candidate and continues revealing entries until they are separated under the maintained decision bounds. The decision radius we use follows finite-population, variance-adaptive template (empirical BernsteinSerfling style) and is calibrated empirically The complete procedure is summarized in Algorithm 1. Algorithm 1 Col-Bandit (Adaptive Late-Interaction Pruning) Require: Docs D, Query Q, K, δ, Relaxation αef , Bounds [a, b], Exploration ϵ 1: Init: Ω , RN (sparse) 2: Compute initial LCBi, UCBi for all [N ] using bounds 3: while True do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: end while else ˆSi (cid:98)TK arg topKi[N ] i+ arg mini (cid:98)TK LCBi arg maxi / (cid:98)TK UCBi if LCBi+ UCBi then return (cid:98)TK Weakest Winner Strongest Loser Top-K separated end if arg maxi{i+,i}(UCBi LCBi) Sample Uniform(0, 1) if < ϵ then Select uniform random Ui Exploration arg maxtUi (bi,t ai,t) Max-Width end if Hi,t h(di , t) Ω Ω {(i, t)} Update ˆµi , ˆσi using Hi,t Update LCBi , UCBi via Eq. 13,14 Reveal MaxSim 4.1. Inputs and Exploration Strategies effectively to instance-specific sparsity. Col-Bandit takes as input the document set D, target K, and user-specified tolerance knob δ that controls the conservativeness of the decision radius. Optionally, it utilizes token-wise bounds Hi,t [ai,t, bi,t] for unrevealed entries; when unavailable, we default to global similarity range (e.g., [0, 1] or [1, 1]). To ensure robust variance estimation and avoid premature stopping early in the process, we evaluate two exploration strategies. Static warm-up. We initialize with uniform random sample Ω0 [N ] [T ] of size Ω0 = γinitN , drawn without replacement. All entries (i, t) Ω0 are revealed to populate the initial interaction matrix, and the adaptive procedure starts from Ω Ω0. Dynamic ϵ-greedy. We integrate an ϵ-greedy policy (Sutton et al., 1998) directly into the refinement step (Algorithm 1, lines 1016). At each iteration, with probability ϵ we reveal random unobserved token from the selected document to encourage exploration; otherwise, we select the token with the highest heuristic utility (exploitation). We ablate this policy against static warm-up and study sensitivity to γinit and ϵ in Section 5.3. Empirically, dynamic ϵ-greedy consistently outperforms static warm-up by adapting more 4.2. Ranking Proxy and Decision Bounds Let ni = Oi denote the number of observed query-token and (cid:98)µi be the empirical mean of observed token scores: 1 ni (cid:98)µi = Hi,t. (cid:88) (8) tOi We define the estimated total score (cid:98)Si (cid:98)µi. This estimate is used to order candidates and form the tentative set (cid:98)TK inside LUCB. (9) Deterministic (Hard) Bounds. Using the known range of unrevealed entries, we compute bounds that are always valid: LBhard = Bhard = (cid:88) tOi (cid:88) tOi Hi,t + Hi,t + (cid:88) tUi (cid:88) tUi ai,t, bi,t. (10) (11) Variance-Adaptive Decision Radius. To adapt sampling to the variability of token interactions, we use an empiri5 Col-Bandit: Zero-Shot Query-Time Pruning for Late-Interaction Retrieval cal BernsteinSerfling style radius (Bardenet & Maillard, 2015): reff αef (cid:124)(cid:123)(cid:122)(cid:125) Calibration (cid:98)σi (cid:124) (cid:115) 2 log(cN/δ) ni (cid:123)(cid:122) Variance-Aware Shrinkage (cid:125) , ρni (cid:124) (cid:123)(cid:122) (cid:125) FP Correction (12) where (cid:98)σi is the empirical standard deviation over {Hi,t}tOi and ρni is finite-population correction satisfying ρni 0 as ni (definitions in Appendix A). This functional form has three useful properties in our setting: it scales with observed variability ((cid:98)σi), shrinks roughly ni with additional reveals, and collapses to zero as 1/ as row becomes fully observed through ρni . We treat αef (0, 1] as calibration parameter controlling conservativeness: αef = 1 uses the unshrunk form, while αef < 1 tightens the radius and improves the qualitycoverage tradeoff in practice. Hybrid Decision Interval. We combine deterministic hard bounds with the variance-adaptive decision radius: LCBi = max (cid:16) UCBi = min (cid:16) LBhard , (cid:98)Si reff Bhard , (cid:98)Si + reff (cid:17) (cid:17) , . (13) (14) Clipping to [LBhard lation from partial observations. , Bhard ] prevents excessive extrapoUniform-within-row mode. In non-adaptive variant where, once document is selected, the next revealed token is sampled uniformly from the remaining unrevealed tokens in that row, setting αef = 1 matches the conditions of the empirical BernsteinSerfling bound (Appendix C). Empirically, the corresponding high-coverage endpoint attains exact agreement with full scoring  (Fig. 8)  . 4.4. Practical Calibration In practice, αef governs the aggressiveness of pruning: smaller values tighten the decision radius and reduce coverage, while larger values are more conservative. We therefore select αef based on desired qualitycoverage trade-off (Section 5.3). Unless stated otherwise, our default configuration uses dynamic ϵ-greedy refinement with an empirically calibrated αef < 1 and reports both retrieval quality and achieved coverage. 5. Experiments We evaluate Col-Bandit on five text retrieval datasets from BEIR (Thakur et al., 2021) using ColBERTv2 (Santhanam et al., 2022b) and Jina-ColBERT-v2 (Jha et al., 2024), and four multimodal datasets from REAL-MMRAG (Wasserman et al., 2025) using Granite Vision Embedding 3.2 (Team, 2025a) multimodal embedding model. Dataset statistics are in Appendix Table 3. 4.3. LUCB-Based Refinement Policy We adopt the LUCB framework for Top-K identification, summarized in Algorithm 1. Let (cid:98)TK be the documents with largest (cid:98)Si, and define the weakest winner and strongest loser as i+ arg min (cid:98)TK LCBi, arg max / (cid:98)TK UCBi. If LCBi+ UCBi, we terminate with separated Top-K set under the maintained decision bounds (as illustrated in Figure 1). Otherwise, we first pick the more ambiguous document = arg max i{i+,i} (UCBi LCBi), and then reveal one additional token for using the dynamic ϵ-greedy strategy (Section 4.1): with probability ϵ we sample uniformly from Ui (exploration), otherwise we select = arg max tUi (bi,t ai,t), which targets the unrevealed token with the largest remaining deterministic uncertainty. Baselines We compare Col-Bandit against two baselines. The first is naive random strategy, denoted as DocUniform, which reveals MaxSim cells uniformly at random within each document (row) under given coverage budget. The second is greedy heuristic method, denoted as Doc-TopMargin, which reveals the MaxSim cells with the largest support (Section 3.2) within each row, subject to the same coverage budget. We describe the full configurations for two baselines in the appendix A.3 Algorithm 2, and 3. 5.1. Experimental Setup Our evaluation follows the standard two-stage lateinteraction retrieval pipeline (Khattab & Zaharia, 2020). We evaluate all approaches at {1, 5, 10}. In the first stage, an approximate nearest-neighbor (ANN) index is leveraged to retrieve candidate set for each query (in our experiments, we instantiate this stage using precomputed exact kNN per query token for reproducibility). In the second stage, the candidates are re-ranked using late interaction (e.g., MaxSim aggregation). In our evaluation, Col-Bandit operates in this second stage, adaptively revealing only subset of MaxSim interactions within the querydocument table. The first-stage retrieval provides informative bounds that can initialize Col-Bandit (Section 4.1). For each query 6 Col-Bandit: Zero-Shot Query-Time Pruning for Late-Interaction Retrieval Table 1. Universal Efficiency Analysis (Text and Multimodal). We report the mean coverage budget (std)across BEIR and REAL-MM-RAG datasets required to achieve 90% (White) and 95% (Gray) Overlap@1 and Overlap@5. Savings (vs. Full) is the compute reduction factor relative to full ColBERT reranking (i.e., 100%/Mean). Method Overlap@ Overlap@5 90% 95% 90% 95% 90% 95% 90% 95% Mean Coverage (std) Savings Mean Coverage (std) Savings ColBERTv2 (BEIR) Doc-Uniform Doc-TopMargin Col-Bandit (Ours) 65% (35.4) 56% (33.0) 13% (10.2) 71% (36.8) 63% (36.2) 14% (10.7) Jina-ColBERT-V2 (BEIR) Doc-Uniform Doc-TopMargin Col-Bandit (Ours) 80% (37.6) 44% (26.2) 11% (5.3) 83% (35.8) 57% (36) 14% (7.6) Granite Vision Embedding (REAL-MM-RAG) Doc-Uniform Doc-TopMargin Col-Bandit (Ours) 91% (9.5) 77% (12.4) 16% (6.6) 98% (3.9) 89% (8.6) 18% (6.7) 1.5 1.8 7.7 1.2 2.3 9.1 1.1 1.3 6.3 1.4 1.6 7.1 1.2 1.7 7.1 1.0 1.1 5.9 98% (1.2) 79% (5.5) 28% (6.3) 100% (0.0) 91% (4.4) 33% (8.3) 99% (1.2) 61% (8.6) 26% (4.6) 100% (0.0) 76% (7.0) 34% (8.4) 96% (0.0) 86% (3.5) 31% (3.1) 100% (0.0) 93% (2.5) 41% (3.8) 1.0 1.3 3.6 1.0 1.6 3.8 1.0 1.2 3.2 1.0 1.1 3.1 1.0 1.3 3.0 1.0 1.1 2.5 token qt: ai,t = 0, bi,t = (cid:40) h(di, t) s(t) if di retrieved for token otherwise (15) where h(di, t) is the actual MaxSim value computed during ANN retrieval (if di was retrieved for token t), and s(t) is the similarity of the k-th neighbor for token t. These tokenlevel bounds translate into row-wise bounds for Col-Bandits confidence intervals (Eq. 10,11) enable faster convergence. Appendix A.1 details our two-stage retrieval pipeline. Metrics. All results are measured relative to full lateinteraction scoring over the entire candidate set, which serves as the non-pruned reference. The ranking fidelity is measured by Overlap@K: Intersection between the approximate Top-K set and the exact Top-K set returned by full candidate set scoring. Overlap@K = K(Q) ˆTK(Q) (16) Overlap@K measures how faithfully pruning methods recover the ranking produced by full late-interaction scoring. In addition, we evaluate retrieval effectiveness, which reflects end-task performance. We report standard IR metrics - Recall@K, MRR@K, and nDCG@K, computed against relevance labels. These metrics allow us to assess whether computational savings come at the cost of end-task quality. These perspectives answer different questions: the first evaluates approximation quality (can we reproduce Full ColBERT cheaply?), while the second evaluates task quality (do we hurt retrieval performance?) We evaluate all the methods along two complementary dimensions quality and coverage. For visualization, we plot quality metrics (x-axis) against the resulting coverage Table 2. Retrieval effectiveness at different coverage levels on both REAL-MM-RAG and BEIR. Results are averaged across datasets and models. Full reranking at 100% coverage serves as the reference. Method Coverage Recall@5 nDCG@5 MRR@5 Full ColBERT Col-Bandit (Ours) Col-Bandit (Ours) Doc-TopMargin Doc-Uniform 100% 20% 40% 40% 40% 0. 0.60 0.65 0.61 0.54 0.58 0.54 0.57 0.54 0.46 0. 0.57 0.60 0.56 0.48 Relative Retention at 20% Coverage (vs. Full ColBERT) Col-Bandit (Ours) 90.9% 93.1% 93.4% Relative Retention at 40% Coverage (vs. Full ColBERT) Col-Bandit (Ours) Doc-TopMargin Doc-Uniform 98.8% 93.1% 82.6% 98.9% 92.3% 79.1% 99.1% 92.7% 78.9% γ (y-axis). For Col-Bandit, operating points are generated by sweeping the relaxation parameter αef [103, 1] with fixed confidence δ = 0.01. For exploration, Col-Bandit employs ϵ-greedy2 with ϵ = 0.1. Baselines are evaluated at fixed coverage budgets γ {0.05, 0.1, . . . , 1.0}. 5.2. Main Results Ranking Fidelity: Cost-Accuracy Trade-off. We measure the costaccuracy trade-off via Top-K ranking recovery as function of coverage γ. Varying the relaxation parameter αef yields tunable efficiency frontier (Fig. 2; summarized in Table 1). At matched coverage, Col-Bandit consistently attains higher ranking fidelity than all non-adaptive baselines. Table 1 reports the mean coverage required to reach 90% and 95% overlap at K=1 2In our implementation, we first reveal one uniformly random cell per document to initialize empirical statistics. 7 Col-Bandit: Zero-Shot Query-Time Pruning for Late-Interaction Retrieval Figure 4. Exploration Strategy Ablation. Trade-off on Jina ColBERTv2 / HotPotQA. The dynamic ϵ-greedy policy (purple) consistently dominates static warm-up schedules (green), avoiding wasteful reveals on easy queries. Figure 5. Effect of ANN-derived bounds. Col-Bandit (purple) outperforms the corresponding baseline (gray) in both settings: with retrieval bounds (solid) and without (dashed). Granite Vision Embedding / TechSlides. and K=5 (averaged over BEIR and REAL-MM-RAG; per-dataset results and additional plots for K=1, 5, 10 are in Appendix B.1, 6, 7). Overall, Col-Bandit reaches target fidelity with substantially lower coverage, with the largest gains at small (Top-1) and still sizable savings at K=5. These trends hold for both text-only retrievers (ColBERTv2, Jina-ColBERTv2) and multimodal embeddings (Granite Vision Embedding on REAL-MM-RAG), indicating that the adaptive reveal framework is modeland modality-agnostic. Retrieval Effectiveness: Impact on End-Task Performance. We test whether adaptive pruning harms retrieval by reporting Recall@5, nDCG@5, and MRR@5 under different coverage budgets (Table 2; K=1 in Appendix 8). Col-Bandit preserves relevance quality under substantial compute reduction (e.g., at 40% coverage it nearly matches full scoring), while heuristic baselines degrade more sharply. Even at 20% coverage, Col-Bandit remains competitive, showing graceful quality degradation as compute decreases. 5.3. Ablation Studies Impact of Exploration Strategy. We compare our dynamic ϵ-greedy policy with static Warm-up baseline that reveals fixed fraction γ upfront. As shown in Fig. 4, ϵ-greedy yields better efficiency frontier by avoiding irreducible fixed costs on easy queries and allocating exploration only when rankings are ambiguous. We therefore use ϵ-greedy in all main experiments. Benefit of ANN-Based Bounds. In realistic deployments, Col-Bandit can leverage bounds derived from the ANN retrieval stage (Section 5.1). However, Col-Bandit can also operate without external bounds, using only generic similaritymetric bounds (e.g., [0, 1] for normalized embeddings). Figure 5 compares these settings (see Appendix for additional datasets). Using ANN bounds consistently improves the accuracy-coverage trade-off, enabling Col-Bandit to achieve higher ranking fidelity at the same compute budget. For example, on the Granite Vision Embedding / TechSlides setting, achieving 0.9 Overlap@5 requires only 30% coverage when using ANN-derived bounds, compared to 50% for the generic-bounds variant. Importantly, even without ANNbased initialization, Col-Bandit still substantially outperforms Doc-Uniform (0.9 vs. 0.65 at 50% coverage), which similarly operates without ANN-derived bounds, demonstrating that the adaptive reveal strategy provides value beyond the availability of strong initial bounds. 6. Conclusion We presented Col-Bandit, an adaptive framework for accelerating late-interaction reranking at query time by selectively revealing MaxSim computations until the Top-K set stabilizes. Across BEIR and REAL-MM-RAG, Col-Bandit consistently exposes substantial redundancy in dense lateinteraction scoring, reducing MaxSim FLOPs by up to 5 while preserving high overlap with exhaustive reranking. single calibration knob, αef (Eq. 12), provides practical control over the qualitycompute trade-off and yields strong Pareto frontiers against uniform and greedy reveal baselines. Col-Bandit is drop-in reranking layer that requires no retraining or offline index changes, making it easy to deploy on top of standard search pipelines. 8 Col-Bandit: Zero-Shot Query-Time Pruning for Late-Interaction Retrieval Limitations. Col-Bandit is designed for precision-oriented tasks with small K; as grows, more candidates cluster near the decision boundary, reducing efficiency gains. Our strongest empirical configuration uses adaptive token selection, for which the variance-based radius should be viewed as calibrated decision heuristic rather than formal certificate. Finally, our evaluation measures FLOP reductions; realizing wall-clock speedups requires batched implementations to amortize GPU kernel overheads. Future Work. We plan to develop batched implementation that reveals blocks of high-uncertainty cells simultaneously, enabling efficient parallel execution on modern GPU hardware."
        },
        {
            "title": "References",
            "content": "Audibert, J.-Y. and Bubeck, S. Best arm identification in multi-armed bandits. In COLT-23th Conference on learning theory-2010, pp. 13p, 2010. Auer, P., Cesa-Bianchi, N., and Fischer, P. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2):235256, 2002. Bardenet, R. and Maillard, O.-A. Concentration inequalities for sampling without replacement. Bernoulli, 21(3):1361 1385, 2015. doi: 10.3150/14-BEJ605. URL https: //arxiv.org/abs/1309.4029. Broder, A. Z., Carmel, D., Herscovici, M., Soffer, A., and Zien, J. Efficient query evaluation using two-level retrieval process. In Proceedings of the twelfth international conference on Information and knowledge management, pp. 426434, 2003. Chen, S., Lin, T., King, I., Lyu, M. R., and Chen, W. Combinatorial pure exploration of multi-armed bandits. Advances in neural information processing systems, 27, 2014. Clavie, B., Chaffin, A., and Adams, G. Reducing the footprint of multi-vector retrieval with minimal perarXiv preprint formance impact via token pooling. arXiv:2409.14683, 2024. Cohan, A., Feldman, S., Beltagy, I., Downey, D., and Weld, D. S. Specter: Document-level representation learning using citation-informed transformers. arXiv preprint arXiv:2004.07180, 2020. Dhulipala, L., Hadian, M., Jayaram, R., Lee, J., and Mirrokni, V. MUVERA: Multi-vector retrieval via fixed dimensional encodings. In Advances in Neural Information Processing Systems 37 (NeurIPS 2024), 2024. URL https://arxiv.org/abs/2405.19504. Ding, S. and Suel, T. Faster top-k document retrieval using block-max indexes. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pp. 9931002, 2011. Engels, J., Coleman, B., Lakshman, V., and Shrivastava, A. DESSERT: An efficient algorithm for vector set In Advances in Neusearch with vector set queries. ral Information Processing Systems 36 (NeurIPS 2023), 2023. URL https://openreview.net/forum? id=kXfrlWXLwH. Faysse, M., Sibille, H., Wu, T., Omrani, B., Viaud, G., Hudelot, C., and Colombo, P. Colpali: Efficient document retrieval with vision language models. arXiv preprint arXiv:2407.01449, 2024. Formal, T., Piwowarski, B., and Clinchant, S. white In European Conference on box analysis of colbert. Information Retrieval, pp. 257263. Springer, 2021. Gunther, M., Sturua, S., Akram, M. K., Mohr, I., Ungureanu, A., Wang, B., Eslami, S., Martens, S., Werk, M., Wang, N., et al. jina-embeddings-v4: Universal embeddings for multimodal multilingual retrieval. In Proceedings of the 5th Workshop on Multilingual Representation Learning (MRL 2025), pp. 531550, 2025. Indyk, P. and Wagner, T. Adaptive estimation for approximate k-nearest-neighbor computations. arXiv preprint arXiv:1902.09465, 2019. URL https://arxiv. org/abs/1902.09465. Jha, R., Wang, B., Gunther, M., Mastrapas, G., Sturua, S., Mohr, I., Koukounas, A., Akram, M. K., Wang, N., and Xiao, H. Jina-colbert-v2: general-purpose multilingual late interaction retriever. arXiv preprint arXiv:2408.16672, 2024. Kalyanakrishnan, S., Tewari, A., Auer, P., and Stone, P. Pac subset selection in stochastic multi-armed bandits. In Proceedings of the 29th International Conference on Machine Learning, pp. 655662, 2012. Khattab, O. and Zaharia, M. ColBERT: Efficient and effective passage search via contextualized late interaction over BERT. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pp. 3948, 2020. Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019. Lassance, C., Maachou, M., Park, J., and Clinchant, S. study on token pruning for colbert. arXiv preprint arXiv:2112.06540, 2021. 9 Col-Bandit: Zero-Shot Query-Time Pruning for Late-Interaction Retrieval MacAvaney, S., Mallia, A., and Tonellotto, N. Efficient constant-space multi-vector retrieval. In European Conference on Information Retrieval, pp. 237245. Springer, 2025. Santhanam, K., Khattab, O., Potts, C., and Zaharia, M. PLAID: An efficient engine for late interaction retrieval. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management, CIKM 22, pp. 17471756, New York, NY, USA, 2022a. Association for Computing Machinery. doi: 10.1145/3511808. 3557325. Santhanam, K., Khattab, O., Saad-Falcon, J., Potts, C., and Zaharia, M. ColBERTv2: Effective and efficient retrieval via lightweight late interaction. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 37153734, Seattle, United States, 2022b. Association for Computational Linguistics. Wang, X., Macdonald, C., Tonellotto, N., and Ounis, I. Reproducibility, replicability, and insights into dense multirepresentation retrieval models: from colbert to col. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 25522561, 2023. Warner, B., Chaffin, A., Clavie, B., Weller, O., Hallstrom, O., Taghadouini, S., Gallagher, A., Biswas, R., Ladhak, F., Aarsen, T., et al. Smarter, better, faster, longer: modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 25262547, 2025. Wasserman, N., Pony, R., Naparstek, O., Goldfarb, A. R., Schwartz, E., Barzelay, U., and Karlinsky, L. Real-mmrag: real-world multi-modal retrieval benchmark. arXiv preprint arXiv:2502.12342, 2025. Scheerer, J. L., Zaharia, M., Potts, C., Alonso, G., and Khattab, O. Warp: An efficient engine for multi-vector retrieval. In Proceedings of the 48th international ACM SIGIR conference on research and development in information retrieval, pp. 25042512, 2025. Xu, M., Moreira, G., Ak, R., Osmulski, R., Babakhin, Y., Yu, Z., Schifferer, B., and Oldridge, E. Llama nemoretriever colembed: Top-performing text-image retrieval model. arXiv:2507.05513, 2025. URL https: //arxiv.org/abs/2507.05513. Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W., Salakhutdinov, R., and Manning, C. D. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 conference on empirical methods in natural language processing, pp. 23692380, 2018. Zhou, J. P., Walder, C., et al. On speeding up language model evaluation. In International Conference on Learning Representations (ICLR), 2024. URL https:// openreview.net/forum?id=3cvwO5DBZn. Shi, C., Yang, K., Yang, J., and Shen, C. Best arm identification for prompt learning under limited budget. arXiv preprint arXiv:2402.09723, 2024. Sutton, R. S., Barto, A. G., et al. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. Team, I. R. Granite-vision-3.3-2b-embedding, 2025a. URL https://huggingface.co/ibm-granite/ granite-vision-3.3-2b-embedding. Team, N. Nomic embed multimodal: Interleaved text, image, and screenshots for visual document retrieval, 2025b. URL https://nomic.ai/blog/posts/ nomic-embed-multimodal. Thakur, N., Reimers, N., Ruckle, A., Srivastava, A., and Gurevych, I. Beir: heterogenous benchmark for zeroshot evaluation of information retrieval models. arXiv preprint arXiv:2104.08663, 2021. Tonellotto, N. and Macdonald, C. Query embedding pruning In Proceedings of the 30th ACM for dense retrieval. International Conference on Information & Knowledge Management, pp. 34533457, 2021. Wachsmuth, H., Syed, S., and Stein, B. Retrieval of the best counterargument without prior topic knowledge. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 241251, 2018. 10 Col-Bandit: Zero-Shot Query-Time Pruning for Late-Interaction Retrieval A. Details of Variance-Adaptive Radius Empirical Standard Deviation. The empirical standard deviation (cid:98)σi used in the standard variance bound is calculated over the set of observed tokens Oi: (cid:98)σ2 = 1 ni 1 (cid:88) tOi (Hi,t (cid:98)µi)2 . (17) In the edge case where ni 1, the variance is undefined; we strictly set reff hard bounds. = + and rely solely on the deterministic Finite Population Correction (ρn). The term ρni in Eq. (12) accounts for sampling without replacement from finite set of size . It is defined piecewise as: 1 , /2, ρn (cid:16) 1 (cid:17) (cid:18) 1 1 + (cid:19) 1 , > /2. (18) This formulation ensures that the confidence interval shrinks faster than standard Bernstein bounds as . Specifically, when = , the term (1 n/T ) becomes zero, collapsing the radius entirely as required for fully observed document. Bias and Stopping Conditions. The standard term in Eq. (12) omits the O(1/n) bias term typically found in empirical BernsteinSerfling inequality (Bardenet & Maillard, 2015) Theorem 4.3. In our framework, the relaxation factor αef practically compensates for this approximation. Furthermore, while the stopping time is adaptive, the procedure requires full separation of the top-K set, making it substantially less sensitive to optional stopping risks compared to classical sequential hypothesis tests. A.1. Two-Stage Retrieval Pipeline Our evaluation follows the standard two-stage late-interaction retrieval pipeline (Khattab & Zaharia, 2020), which separates candidate generation from exact reranking: Stage 1: Candidate Generation (ANN). Given query = {q1, . . . , qT }, we first use an Approximate Nearest Neighbor (ANN) index to retrieve candidate set from the full corpus C. For each query token qt, we perform top-k nearest neighbor search in the document token embedding space, retrieving the most similar document tokens. We then aggregate all documents whose tokens appear in any of these top-k results. Let = D, this produces candidate set with C, where is the full corpus, defining our MaxSim matrix RN from Eq. 4 and we set Ω = . In our experiments, we use = 10 per query token, resulting in candidate sets of average size 250 documents for text retrieval and 500 for multimodal retrieval. Stage 2: Exact Reranking. For each candidate document D, we compute the exact ColBERT score (Eq. 2) by evaluating all MaxSim operations, revealing all matrix cells. This stage is the computational bottleneck. A.2. Datasets and Models We evaluate Col-Bandit on five widely used text retrieval datasets from the BEIR benchmark (Thakur et al., 2021): ArguAna (Wachsmuth et al., 2018), Quora (Thakur et al., 2021), SciDocs (Cohan et al., 2020), NQ (Kwiatkowski et al., 2019), and HotPotQA (Yang et al., 2018). We use two state-of-the-art late-interaction text embedding models: ColBERTv23 (Santhanam et al., 2022b) and Jina-ColBERT-v24 (Jha et al., 2024). Both models produce token embeddings of dimension = 128 and use fixed query token length of = 32. In addition, we evaluate Col-Bandit on visual document retrieval task using the REAL-MM-RAG (Wasserman et al., 2025) benchmark which include 4 subsets: FinReports, FinSlides, TechReports and TechSlides. In this setting, we employ the Granite Vision Embedding 3.25 (Team, 2025a) model, vision-language embedding model that produces = 128-dimensional token embeddings, with variable-length query representations and 729 document tokens per image. Table 3 summarizes the key statistics of all evaluation datasets. 3https://huggingface.co/lightonai/colbertv2.0 4https://huggingface.co/jinaai/jina-colbert-v2 5https://huggingface.co/ibm-granite/granite-vision-3.3-2b-embedding 11 Col-Bandit: Zero-Shot Query-Time Pruning for Late-Interaction Retrieval Table 3. Evaluation datasets statistics. Dataset Corpus Queries"
        },
        {
            "title": "BEIR\nArguAna\nQuora\nSciDocs\nNQ\nHotPotQA",
            "content": "REAL-MM-RAG Fin. Reports Fin. Slides Tech. Reports Tech. Slides 8.7K 523K 25K 2.68M 5.23M 2.6K 2.3K 1.7K 2K Tq 32 32 32 32 32 modality"
        },
        {
            "title": "Text\nText\nText\nText\nText",
            "content": "1.4K 5K 1K 3.5K 5.5K 853 10100 1K 10100 1.3K 10100 1.4K 10100 Image+Text Image+Text Image+Text Image+Text Tq: query token count; Ld: document token count. A.3. Compared Methods All our compared methods operate (same as Col-Bandit) on Stage 2 A.1, where the candidate set is defined and we have MaxSim matrix with Ω = . In the static baselines, the budget is an explicit integer (or equivalently coverage fraction γ with = γT ) that fixes the number of revealed cells per document row. Each baseline reveals exactly token positions for every document (either uniformly at random or by arg Top Bt[T ](bi,t ai,t)) and ranks documents by the sum of the revealed MaxSim values. Algorithm 2 Doc-Uniform (Static Random Reveal) Algorithm 3 Doc-TopMargin (Static Top-Margin Reveal) Require: Docs D, Query Q, K, γ [0, 1] 1: D, γT 2: Ω , RN 3: for = 1 to do 4: 5: 6: 7: 8: 9: 10: 11: end for 12: return arg topKi[N ] (cid:101)Si Sample Ri [T ] uniformly s.t. Ri = for each Ri do Hi,t h(di, t) Ω Ω {(i, t)} end for (cid:101)Si (cid:80) Hi,t tRi Cells per row w/o replacement Reveal MaxSim Static score Require: Docs D, Q, K, γ, Bounds [a, b] 1: D, γT 2: Ω , RN 3: for = 1 to do 4: 5: 6: 7: 8: 9: 10: end for 11: return arg topKi[N ] (cid:101)Si Gi arg Top Bt[T ](bi,t ai,t) for each Gi do Hi,t h(di, t) Ω Ω {(i, t)} end for (cid:101)Si (cid:80) Hi,t tGi Cells per row Largest widths Reveal MaxSim Static score B. Extended Experimental Results B.1. Detailed Efficiency Results per Dataset In the main text  (Table 1)  , we presented efficiency metrics averaged across the BEIR and REAL-MM-RAG benchmark suites to provide concise summary of performance. Tables 4 and 5 (Text) and Tables 6 and 7 (Multimodal) below provide the granular, per-dataset breakdown of these results, reporting the mean coverage required to achieve 90% and 95% Overlap@K for = {1, 5} for the BEIR and REAL-MM-RAG datasets, respectively. This detailed view confirms that the efficiency gains of Col-Bandit are robust across diverse data distributions. While the exact magnitude of the savings varies depending on the document length and query difficulty of each specific corpus, Col-Bandit consistently outperforms the baselines on every individual dataset. Additionally, we extend the Cost-Accuracy trade-off analysis from Figure 2 to broader range of settings. 12 Col-Bandit: Zero-Shot Query-Time Pruning for Late-Interaction Retrieval Generalization across domains and models. Figures 6 through 7 (below) visualize the efficiency frontiers for additional embedding models and datasets at = 5 and = 10. As in the main text, each star marker represents Col-Bandit operating point obtained by sweeping the relaxation parameter αef . Regardless of the underlying embedding model (Granite Vision Embedding, ColBERTv2, or Jina-ColBERT-V2) or the data modality (Text vs. Multimodal), Col-Bandit consistently maintains superior Pareto frontier compared to the baselines. Table 4. Universal Efficiency Analysis Top-1 (BEIR). We report the coverage budget required to achieve 90% (White) and 95% (Gray) Overlap@1 across the textual BEIR datasets. Under Average, we report mean coverage (std) across datasets, and Savings (vs. Full) is the compute reduction factor relative to full reranking (i.e., 100%/Mean). Task Domain Method ColBERTv2 Text Retrieval Benchmarks (BEIR) Average SciDocs Quora NQ HotpotQA ArguAna Mean (std) Savings (vs. Full) Doc-Uniform Doc-TopMargin Col-Bandit (Ours) 65% (35.4) 100% 100% 75% 97% 88% 56% (33.0) 97% 63% 75% 29% 29% 7% 9% 15% 20% 9% 9% 3% 3% 13% (10.2) 97% 100% 50% 50% 3% 6% 91% 47% 47% 3% 3% 81% 71% (36.8) 63% (36.2) 14% (10.7) 1.54 1.79 7.69 Jina-ColBERT-V2 Doc-Uniform Doc-TopMargin Col-Bandit (Ours) 100% 100% 97% 100% 100% 100% 97% 100% 6% 13% 80% (37.6) 72% 44% (26.2) 15% 21% 10% 10% 15% 15% 11% 19% 3% 3% 11% (5.3) 81% 56% 72% 3% 3% 88% 31% 41% 56% 83% (35.8) 57% (36.0) 14% (7.6) 1.25 2.27 9.09 1.41 1.59 7.14 1.20 1.75 7.14 Table 5. Universal Efficiency Analysis Top-5 (BEIR). We report the coverage budget required to achieve 90% (White) and 95% (Gray) Overlap@5 across the textual BEIR datasets. Under Average, we report mean coverage (std) across datasets, and Savings (vs. Full) is the compute reduction factor relative to full reranking (i.e., 100%/Mean). Task Domain Method ColBERTv2 Doc-Uniform Doc-TopMargin Col-Bandit (Ours) Jina-ColBERT-V2 Doc-Uniform Doc-TopMargin Col-Bandit (Ours) Text Retrieval Benchmarks (BEIR) Average SciDocs Quora NQ HotpotQA ArguAna Mean (std) Savings (vs. Full) 97% 100% 97% 100% 97% 100% 100% 100% 97% 100% 98% (1.2) 79% (5.5) 81% 30% 40% 30% 42% 25% 30% 36% 36% 17% 19% 28% (6.3) 97% 72% 88% 88% 97% 81% 88% 88% 75% 100% 100% 100% 100% 100% 100% 100% 100% 97% 100% 99% (1.2) 66% 61% (8.6) 26% 42% 28% 34% 27% 30% 32% 44% 18% 21% 26% (4.6) 81% 63% 81% 63% 81% 47% 72% 75% 56% 100% (0.0) 91% (4.4) 33% (8.3) 1.02 1.27 3.57 100% (0.0) 76% (7.0) 34% (8.4) 1.01 1.64 3.85 1.00 1.10 3.03 1.00 1.32 2.94 Table 6. Universal Efficiency Analysis Top-1 (REAL-MM-RAG). We report the coverage budget required to achieve 90% (White) and 95% (Gray) Overlap@1 across the REAL-MM-RAG multimodal datasets. Under Average, we report mean coverage (std) across datasets, and Savings (vs. Full) is the compute reduction factor relative to full reranking (i.e., 100%/Mean). Task Domain Method Granite-Vision Doc-Uniform Doc-TopMargin Col-Bandit (Ours) Multimodal Benchmarks (REAL-MM-RAG) Average Fin. Reports Fin. Slides Tech. Reports Tech. Slides Mean (std) Savings (vs. Full) 91% (9.5) 96% 100% 100% 100% 91% 100% 75% 91% 86% 96% 56% 75% 77% (12.4) 96% 81% 23% 23% 16% 24% 18% 18% 5% 7% 16% (6.6) 86% 91% 98% (3.9) 89% (8.6) 18% (6.7) 1.1 1.3 6.2 1.0 1.1 5.6 Table 7. Universal Efficiency Analysis Top-5 (REAL-MM-RAG). We report the coverage budget required to achieve 90% (White) and 95% (Gray) Overlap@5 across the REAL-MM-RAG multimodal datasets. Under Average, we report mean coverage (std) across datasets, and Savings (vs. Full) is the compute reduction factor relative to full reranking (i.e., 100%/Mean). Task Domain Method Granite-Vision Doc-Uniform Doc-TopMargin Col-Bandit (Ours) Multimodal Benchmarks (REAL-MM-RAG) Average Fin. Reports Fin. Slides Tech. Reports Tech. Slides Mean (std) Savings (vs. Full) 96% 100% 96% 100% 96% 100% 96% 100% 96% (0.0) 91% 96% 81% 91% 86% 86% (3.5) 33% 43% 26% 35% 34% 45% 31% 39% 31% (3.1) 86% 91% 96% 100% (0.0) 93% (2.5) 41% (3.8) 1.0 1.2 3.2 1.0 1.1 2.4 13 Col-Bandit: Zero-Shot Query-Time Pruning for Late-Interaction Retrieval Figure 6. Cost-Accuracy trade-off for Col-Bandit compared to Random Reveal (Doc-Uniform) and Greedy Top-Margin (Doc-TopMargin) across three retrieval settings (text and multimodal). Each star marker denotes Col-Bandit operating point obtained by sweeping the relaxation parameter αef . The top-right corner (Overlap@K=1.0, Cost=100%) corresponds to full exhaustive scoring . Col-Bandit: Zero-Shot Query-Time Pruning for Late-Interaction Retrieval Figure 7. Cost-Accuracy trade-off for Col-Bandit compared to Random Reveal (Doc-Uniform) and Greedy Top-Margin (Doc-TopMargin) across three retrieval settings (text and multimodal). Each star marker denotes Col-Bandit operating point obtained by sweeping the relaxation parameter αef . The top-right corner (Overlap@K=1.0, Cost=100%) corresponds to full exhaustive scoring . B.2. Extended Retrieval Effectiveness (Top-1 Analysis) In the main text  (Table 2)  , we focused on Top-5 ranking metrics to demonstrate the robustness of Col-Bandit for identifying small set of relevant documents. Table 8 below complements this by reporting the Top-1 retrieval effectiveness (Recall@1, nDCG@1, and MRR@1) across varying coverage levels. The results in the Top-1 regime reinforce the trends observed at = 5. Col-Bandit maintains near-lossless performance compared to the Full ColBERT baseline, even when pruning significantly more aggressively than non-adaptive methods. For instance, at lower coverage budgets, the gap between Col-Bandit and the heuristic baselines (Doc-Uniform and DocTopMargin) becomes even more pronounced, highlighting the necessity of variance-aware sampling for correctly identifying the single best document with high confidence. B.3. Extended Ablation: Impact of ANN-Based Bounds In the main text (Section 5.3), we demonstrated that initializing Col-Bandit with bounds derived from the preceding ANN search significantly improves efficiency. Figure 8 extends this analysis to the REAL-MM-RAG datasets. Across all evaluated settings, the trend remains consistent: ANN-derived bounds provide tighter starting interval for the confidence sets, allowing the algorithm to prune non-competitive documents earlier in the process. While the magnitude of the gain varies depending on the quality of the initial ANN approximation, the ANN-guided variant (purple curves) consistently dominates the generic-bound variant (gray curves). However, even in the absence of informative priors (the generic case), Col-Bandit successfully adapts its sampling to identify the Top-K documents, confirming that the core efficiency gains stem from the variance-adaptive sampling strategy itself rather than solely from initialization quality. C. Theoretical Validity in Uniform-Sampling Mode (Special Case) We state special case in which the empirical BernsteinSerfling radius used in Eq. 12 is δ-valid when αef = 1. Assume that the algorithm may choose documents adaptively, but whenever document is selected, it reveals the next token index uniformly from the remaining unrevealed indices in that row (sampling uniformly without replacement from [T ]). 15 Col-Bandit: Zero-Shot Query-Time Pruning for Late-Interaction Retrieval Table 8. Retrieval effectiveness at different coverage levels on both REAL-MM-RAG (Fin. Reports, Fin. Slides, Tech. Reports, Tech. Slides) and BEIR (ArguAna, Quora, SciDocs, NQ, HotPotQA). Results are averaged across datasets. Full reranking at 100% coverage serves as the reference."
        },
        {
            "title": "Method",
            "content": "Coverage Recall@1 nDCG@1 MRR@"
        },
        {
            "title": "Full ColBERT",
            "content": "100% Col-Bandit Col-Bandit Doc-TopMargin Doc-Uniform Doc-TopMargin Doc-Uniform 20% 40% 20% 20% 40% 40% 0. 0.40 0.41 0.33 0.23 0.37 0.31 0.51 0.50 0.50 0.42 0.28 0.47 0.38 Relative Retention at 20% Coverage (vs. Full ColBERT) Col-Bandit Doc-TopMargin Doc-Uniform 98.9% 81.0% 55.9% 98.7% 82.1% 55.6% Relative Retention at 40% Coverage (vs. Full ColBERT) Col-Bandit Doc-TopMargin Doc-Uniform 99.1% 90.8% 74.9% 98.9% 91.9% 74.6% 0.51 0.50 0.50 0.42 0.28 0.47 0. 98.7% 82.1% 55.6% 98.9% 91.9% 74.6% Figure 8. Effect of ANN bounds and calibration. Qualitycoverage trade-off for Col-Bandit with and without ANN-derived token bounds across four document retrieval settings. The without ANN bounds variant corresponds to uniform-within-row token reveals; in this setting, the unshrunk radius (αef = 1) matches the conditions of the empirical BernsteinSerfling bound (Appendix C). Fix document and let Oi be the set of revealed token indices with ni = Oi. Define the row mean and sum µi 1 (cid:88) t=1 Hi,t, Si (cid:88) t=1 Hi,t = µi, and the empirical mean/standard deviation over the revealed entries (cid:98)µi = 1 ni (cid:88) tOi Hi,t, (cid:98)σ2 = 1 ni 1 (cid:88) tOi (Hi,t (cid:98)µi)2. Under uniform-without-replacement sampling within the row and bounded support Hi,t [a, b], an empirical Bernstein Serfling inequality (Bardenet & Maillard, 2015) implies that, for any fixed (i, n), (cid:32) Pr Si (cid:98)µi (cid:98)σi (cid:114) 2 log(c/δi,n) ρn (cid:33) 1 δi,n. 16 Col-Bandit: Zero-Shot Query-Time Pruning for Late-Interaction Retrieval To obtain time-uniform statement over all documents and all sample sizes, set δi,n = δ/(N ) and union bound over [N ] and [T ]. Therefore, with probability at least 1 δ, simultaneously for all and all n, Si (cid:104) (cid:98)µi rth (cid:105) (n) , (n) (cid:98)σi rth (cid:114) 2 log(cN /δ) ρn. In this uniform-within-row mode, choosing αef = 1 in Eq. 12 recovers the above theoretical form (up to the constant c), justifying its use as δ-valid decision radius. Empirical sanity check. Figure 8 shows that in the without ANN bounds (uniform-within-row) mode, increasing coverage drives Overlap@5 toward 1.0, consistent with the fact that the uncertainty collapses as ni for all rows. D. Table of Notations The notations used in the paper are described below. Table 9. Notations used in the paper. Symbol Description Input = {q1, . . . , qT } query represented as set of token embeddings = {d1, . . . , dN } Ld document from the collection The candidate document set with documents The number of query tokens The number of tokens in document The embedding dimension The number of top documents to identify Scoring sim(, ) h(d, t) S(d; Q) (Q) (cid:98)TK Matrix & Observation Similarity function (e.g., cosine similarity) MaxSim score: maxj[Ld] sim(ed,j, qt) Total late-interaction score: (cid:80)T t=1 h(d, t) The true Top-K document set The returned (estimated) Top-K document set RN Si Ω [N ] [T ] Oi Ui ni γ(Ω) The MaxSim matrix with entries Hi,t = h(di, t) Total score for document i: (cid:80)T The set of observed (revealed) matrix entries Observed token indices for document Unobserved token indices: [T ] Oi Number of revealed tokens for document i: Oi Coverage: fraction of matrix revealed, Ω/(N ) t=1 Hi,t Bounds & Estimation [ai,t, bi,t] ˆµi ˆSi ˆσi , UBhard reff LCBi, UCBi LBhard (cid:80) Bounds for unrevealed entry Hi,t Empirical mean: 1 Hi,t tOi ni Estimated total score: ˆµi Empirical standard deviation over {Hi,t}tOi Deterministic hard bounds (Eq. 10, 11) Variance-adaptive decision radius (Eq. 12) Hybrid lower/upper confidence bounds (Eq. 14, 13) Algorithm Parameters δ (0, 1) αef (0, 1] ρn ϵ i+ Error tolerance for δ-PAC identification Calibration parameter controlling bound tightness Finite-population correction factor Exploration probability in ϵ-greedy policy Weakest winner: arg mini (cid:98)TK Strongest loser: arg maxi / (cid:98)TK LCBi UCBi"
        }
    ],
    "affiliations": [
        "IBM Research Israel"
    ]
}