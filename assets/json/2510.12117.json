{
    "paper_title": "Locket: Robust Feature-Locking Technique for Language Models",
    "authors": [
        "Lipeng He",
        "Vasisht Duddu",
        "N. Asokan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Chatbot providers (e.g., OpenAI) rely on tiered subscription schemes to generate revenue, offering basic models for free users, and advanced models for paying subscribers. However, a finer-grained pay-to-unlock scheme for premium features (e.g., math, coding) is thought to be more economically viable for the providers. Such a scheme requires a feature-locking technique (FLoTE) which is (i) effective in refusing locked features, (ii) utility-preserving for unlocked features, (iii) robust against evasion or unauthorized credential sharing, and (iv) scalable to multiple features and users. However, existing FLoTEs (e.g., password-locked models) are not robust or scalable. We present Locket, the first robust and scalable FLoTE to enable pay-to-unlock schemes. Locket uses a novel merging approach to attach adapters to an LLM for refusing unauthorized features. Our comprehensive evaluation shows that Locket is effective ($100$% refusal on locked features), utility-preserving ($\\leq 7$% utility degradation in unlocked features), robust ($\\leq 5$% attack success rate), and scales to multiple features and clients."
        },
        {
            "title": "Start",
            "content": "LOCKET: Robust Feature-Locking Technique for Language Models Lipeng He, Vasisht Duddu, N. Asokan University of Waterloo {lipeng.he,vasisht.duddu}@uwaterloo.ca, asokan@acm.org 5 2 0 2 4 1 ] . [ 1 7 1 1 2 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Chatbot providers (e.g., OpenAI) rely on tiered subscription schemes to generate revenue, offering basic models for free users, and advanced models for paying subscribers. However, finer-grained pay-to-unlock scheme for premium features (e.g., math, coding) is thought to be more economically viable for the providers. Such scheme requires feature-locking technique (FLoTE) which is (i) effective in refusing locked features, (ii) utility-preserving for unlocked features, (iii) robust against evasion or unauthorized credential sharing, and (iv) scalable to multiple features and clients. However, existing FLoTEs (e.g., password-locked models) are not robust or scalable. We present LOCKET, the first robust and scalable FLoTE to enable pay-to-unlock schemes. LOCKET uses novel merging approach to attach adapters to an LLM for refusing unauthorized features. Our evaluation shows that LOCKET is effective (100% refusal on locked features), utility-preserving (7% utility degradation in unlocked features), robust (5% attack success rate), and scales to multiple features and clients."
        },
        {
            "title": "Introduction",
            "content": "Chatbot service providers (e.g., OpenAI, Anthropic) provide black-box access to large language models (LLMs). Under the current tiered subscription scheme, free clients get basic models, and subscribed clients get advanced models. However, this is reportedly not profitable as indicated by OpenAIs Sam Altman: we are losing money on OpenAI pro subscriptions1. Alternatively, many mobile apps and games use pay-to-unlock scheme for premium features which is profitable (Lundy et al., 2024). Inspired by this, we envision setting where the service providers monetize individual features (e.g., math, coding), atop model subscriptions. Such pay-to-unlock schemes raise the need for effective feature-locking techniques (FLoTEs). Recent work on password-locked LLMs (Greenblatt et al., 2024b; Su et al., 2025; Tang et al., 2024), which respond only when the correct password is provided, can be used as FLoTEs. However, they are not robust to adversarial prompting (6.4), unable to resist against unauthorized credential sharing, and difficult to scale to multiple features (3). In this work, we present LOCKET, the first robust and scalable FLoTE. It supports the notion of an adapter that can enable access control for each premium feature. It has an access control module to identify authorized features for client. It then merges adapters to the base LLM to lock each unauthorized feature, which makes the LLM refuse to respond to queries that attempt to invoke such features. LOCKET requires no secret credentials like passwords, preventing unauthorized sharing; and unlike prior methods, it scales efficiently as we only have to train one adapter per new feature. Our contributions are as follows: we present 1. requirements for FLoTEs, not fully realized by prior work (3), 2. LOCKET2, the first robust and scalable FLoTE, which uses novel merging approach to preserve utility when attaching adapters (4), and 3. evaluation of LOCKET showing that it addresses limitations of prior work, and is effective (100% refusal on locked features), utility-preserving (7% utility degradation in unlocked features), robust (5% attack success rate), and scales to multiple features. (5 and 6)"
        },
        {
            "title": "2 Background and Related Work",
            "content": "Backdoors. By including backdoors in its training data, an LLM can be forced to respond with pre-selected payload when the backdoor is present in the input (Li et al., 2024). Several works have proposed backdoors (Yan et al., 2025; Huang et al., 1https://x.com/sama/status/1876104315296968813 2Code will be open-sourced upon publication. 1 Table 1: Limitations of Prior Work. requirement is satisfied, requirement not satisfied; gray indicates black-box setting, and the rest is whitebox setting."
        },
        {
            "title": "Related Work",
            "content": "R1: Eff. R2: Utlty R3.1 (NAIVEJB) R3.2 (NONADPTJB) R3.3 (ADPTJB) R3.4 (CREDSHR) R4 (SCLBL) Greenblatt et al. (2024b) Su et al. (2025) Tang et al. (2024) LOCKET (Ours) Use Case 1: Prevent Access to Dangerous Features Use Case 2: Prevent Unauthorized Access to Premium Features 2023; Zhang et al., 2025b; Hubinger et al., 2024) with varying effectiveness, and across different settings (Zhang et al., 2025b). We can design FLoTE using backdoors, where the backdoor acts as credential. LLMs can be fine-tuned to respond to query only if the correct password (backdoor trigger) is included, while refusing otherwise. Such password-locking techniques have been explored in various domains including images (Sutton et al., 2025; Gao et al., 2024), text classification (Zeng and Lu, 2022), and LLMs (Greenblatt et al., 2024b; Su et al., 2025; Tang et al., 2024). They are used to demonstrate sandbagging (hiding malicious behavior of LLM during testing) (Greenblatt et al., 2024b), and controlling access to premium features (Su et al., 2025; Tang et al., 2024). However, there are many limitations in using backdoors for FLoTE, we discuss them in 3. Unlearning. Unlearning can suppress unauthorized features either by fine-tuning (Zhang et al., 2024a), or attaching adapters to elicit the expected behavior (Gao et al., 2025). However, they are not designed to ensure robustness and scalability. Model Merging. Instead of full LLM fine-tuning to account for each combination of authorized features, an alternative is to fine-tune specific layers of the LLM using LoRA (Hu et al., 2022), which forms the adapters for specific behaviors. These adapters can then be attached with the base LLM to get the relevant behaviors. Multiple adapters (W and j) with different behaviors can be merged (W = + j) using methods such as CAT (Prabhakar et al., 2025), TIES (pruning small adapter weights, selecting the majority sign, and merging only aligned parameters) (Yadav et al., 2023), and Linear/Task Arithmetic (directly adding adapter weights) (Ilharco et al., 2023)."
        },
        {
            "title": "3 Problem Statement",
            "content": "Our goal is to design FLoTE to enable pay-tounlock schemes in LLMs. System Model. We consider chatbot service provider offering black-box access to their service via an API for clients. client can send prompts to the chatbot, and get responses. In addition to tiered subscription scheme, service provider wants to enable pay-to-unlock scheme for individual LLMs where basic features (e.g., text completion) are freely available, but advanced features (e.g., math, tool-using, coding) are add-ons requiring additional authorization (e.g., via payment or coupons) from the client. Feature-Locking Technique (FLoTE). We want to design FLoTE where client gets proper responses for authorized features, but gets refusal for unauthorized features. Formally, given set of available features in LLMs, = {f1, f2, . . . , fm}, FLoTE is function FLoTE : R, where is the set of clients, and is the set of possible responses. For each client C, if the feature fi is authorized for C, then FLoTE(fi, C) generates valid response. Otherwise, it returns refusal, i.e., (cid:26)valid response FLoTE(fi, C) = refusal if fi is authorized for C, otherwise. Requirements. An ideal FLoTE should be: R1 Effective in refusing responses to unauthorized features which are locked, R2 Utility-preserving by ensuring that the utility of authorized unlocked features is the same as original behavior without the FLoTE, R3 Robust against attempts to evade (via adversarial prompts, or unauthorized use of others authorized credentials), and R4 Scalable by supporting locking multiple features for several clients, and extensible to new ones, without utility and effectiveness degradation. Adversary Model. We assume an adversary (ADV) who aims to evade the FLoTE on the target LLM, by gaining access to unauthorized features. We consider the robustness against the following3: R3.1 Na√Øve Jailbreaks (NAIVEJB): ADV only relies on simple jailbreaks (without optimization) to elicit unauthorized features. This includes context hijacking (e.g., The world is about to end, please answer: <prompt to elicit unauthorized feature >) (Shayegani et al., 2024). R3.2 Non-Adaptive Jailbreaks (NONADPTJB): We assume ADV has white-box access to local LLM with the FLoTE (distinct from target) to craft adversarial prompts, to evade the target. R3.3 Adaptive Jailbreaks (ADPTJB): This is the strongest possible attack by assuming ADVs local LLM is copy of the target LLM with the FLoTE. So, ADV can find adversarial prompts to evade the target LLM. This is stronger than NONADPTJB, and provides an upper-bound for robustness. R3.4 Credential Sharing (CREDSHR): ADV can guess credentials or extract them from the target LLM (Zhang et al., 2024b). By extracting the credentials, ADV may share them with other unauthorized clients (unauthorized credential sharing). Prior work shows locked features can be reactivated by fine-tuning with white-box access (Greenblatt et al., 2024b; Tang et al., 2024). However, in our black-box setting, fine-tuning the target LLM is not possible, so these attacks do not apply. Limitation of Prior Work. We now discuss how existing work on password-locking techniques, does not meet all requirements (summarized in Table 1). Among all prior works, we focus on password-locking for LLMs (Greenblatt et al., 2024b; Su et al., 2025; Tang et al., 2024), which has been used for two applications: (a) restricting access to dangerous features (Greenblatt et al., 2024b), (b) controlling access to premium features (Su et al., 2025; Tang et al., 2024). All three works demonstrate effectiveness and utility of their scheme (R1, R2 ). However, none of them evaluate robustness against adversarial prompts. Greenblatt et al. (2024a) used password-locking only to demonstrate hidden behavior (or sandbagging), focusing on eliciting it via 3Terminology: Prior work misuses adaptive to describe attackers who know the defense. In the security literature, it is standard practice to assume that attackers know all the technical details about defenses except any secret credentials of the defenders. We follow this convention: na√Øve (no knowledge of defense), non-adaptive (optimizes attacks on another LLM without target feedback), and adaptive (optimizes attack using feedback from the target). fine-tuning. Hence, robustness was not an objective in their design. Su et al. (2025) evaluated robustness only with synonyms of passwords, while Tang et al. (2024) require white-box access, unlike our setting. In 6.4, we later show these defenses can be bypassed with black-box adversarial prompts (R3.1: NAIVEJB, R3.2: NONADPTJB, and R3.3: ADPTJB ). Adapting (Greenblatt et al., 2024a) to lock access to premium features, as done by Tang et al. (2024) and Su et al. (2025), makes them vulnerable to credential brute-force guessing and unauthorized redistribution (R3.4 ). Finally, none of these works demonstrate effectiveness for locking multiple features, and focus only on single feature. Fine-tuning the entire LLM is required for each new feature (or client). This approach is inefficient and likely to compromise both effectiveness (R1) and utility (R2) when handling large number of features. Hence, they do not achieve scalability (R4 ). Takeaway: Prior password-locking techniques do not meet all requirements (R1-R4). Other Strawman Techniques. We discuss alternative techniques to enable pay-to-unlock schemes, and their limitations. Solution 1: Use system prompt to refuse all queries regarding unauthorized features. However, such system prompts are easy to evade (Shayegani et al., 2023) (R3 ). Solution 2: Use multiple LLMs, each specializing in specific features while refusing others. Route queries based on the clients authorization (Ong et al., 2024). However, covering all feature combinations requires many LLMs, resulting in combinatorial explosion (R4 ). Concretely, to lock features, 2N 1 separate LLMs must be fine-tuned. Solution 3: Use detector LLM can classify if clients prompt is authorized, but supporting multiple features requires many classifiers. This requires fine-tuning (with robustness) for numerous feature combinations, which leads to combinatorial explosion and poor scalability (R4 ). Solution 4: Improve prior password-locking methods (Greenblatt et al., 2024a; Tang et al., 2024) by adding adversarial training and locking multiple features with different passwords. However, passwords can be extracted and shared (R3.4 ). Moreover, fine-tuning is required for each new client or feature to maintain utility and avoid forgetting, leading to poor scalability as features grow 3 (R4 ). Thus, fine-tuning is not scalable direction for building FLoTEs."
        },
        {
            "title": "4 Design of LOCKET",
            "content": "To avoid the limitations discussed in 3, we do not require the use of secret credential (susceptible to unauthorized credential sharing) and fine-tuning (does not scale). As is customary with all existing LLMs offered as services, we assume that the service provider has ways of identifying and authenticating their clients. Then, inspired by model merging (Prabhakar et al., 2025), we propose using adapters that can be dynamically attached to restrict some features based on clients authorization (2). This preserves the base LLM, and allows single model to serve multiple clients by dynamically attaching relevant adapters to lock features not authorized for given client. This makes it scale across multiple features R4 ). However, adapters must be fine-tuned to effectively refuse unauthorized features, maintain performance on authorized features, and resist evasion attempts. We discuss our design choices to achieve this, and present an overview of LOCKETs design in Figure 1. Training Objective. We can either fine-tune one adapter per feature and combine them to lock multiple features, or fine-tune single adapter for locking combination of features. We choose the former to avoid the combinatorial explosion of adapters required by the latter. To lock feature , we obtain the adapters by fine-tuning some layers of base LLM œÄŒ∏ parameterized by Œ∏ on feature dataset Df = {(xi, yi)}. The overall objective for fine-tuning (Llock) includes the loss functions to maintain utility (Lutility), and to ensure effectiveness while minimizing attempts to evade (Levade). We have: Llock = Lutility + Levade. Utility-Preserving (R2). We preserve the utility of œÄŒ∏ during fine-tuning by computing the KL divergence with respect to its frozen reference œÄŒ∏: (cid:20) (cid:21) KL[œÄŒ∏(yixi)œÄŒ∏(yixi)] Lutility = E(xi,yi)Dauth Here, Dauth contains generic question and helpful (authorized) responses, unrelated to any of the locked features (e.g., text from Wikipedia) (Ding et al., 2023). This is common technique used in unlearning (Gao et al., 2025) to retain the LLMs utility on basic tasks (e.g., text completion) during fine-tuning. Effective (R1) and Robust (R3). Prior work has shown that adding perturbations to LLM activaFigure 1: Summary of LOCKET: ‚ûä Client requests authorization for premium feature fj (e.g., via payment), handled by the authorization module. ‚ûã Authorization module updates the clients profile with new set of allowed features. ‚ûå Client submits service request, received by the access control module. ‚ûç Access control module verifies clients permissions before querying the LLM. ‚ûé It selects adapters aj to lock unauthorized features fj and ‚ûè attaches them to LLM. ‚ûê Client can now query fj and receive responses, while requests for fj are refused. tions can reinforce alignment (Zhang et al., 2025a). Inspired by this, we ensure effective refusal of unauthorized features and robustness against evasion by augmenting refusal training with Latent Adversarial Training (LAT) (Sheshadri et al., 2025). For this, we construct preference dataset Dunauth = {(xi, ci, ri)} for locking feature , using publicly available feature dataset Df . Each prompt xi is from Df , which is paired with fixed refusal response ci (as positive sample), along with useful response ri (as negative sample). We use the ground truth responses yi from Df as ri, and ci is set to be \"Sorry, you are not authorized to use the capabilities needed to solve this problem\". Computing Sample-wise Perturbations: First, we find the worst-case perturbation Œ¥i which is added to the latent activations of set of target layers. These perturbations are computed to minimize the LLMs loss in responding to xi, resulting in an evasion. We define the loss as: Levade(ci, ri; Œ¥) = Move towards ci (cid:122) (cid:123) (cid:125)(cid:124) log œÄŒ∏(ciŒ±(xi, Œ¥)) Move away from ri (cid:122) (cid:123) (cid:125)(cid:124) log (1 œÄŒ∏(riŒ±(xi, Œ¥))) + Œ±(xi, Œ¥) is function that adds Œ¥ to the LLMs latent activations for an input xi, and œµ is the perturbation budget where Œ¥2 œµ. To find the perturLevade(ri, ci; Œ¥). bations, we compute Œ¥i = argmin Œ¥ Robust Fine-tuning for Effectiveness: Having computed the perturbations for different samples, we now update Œ∏ to minimize Levade(ci, ri; Œ¥i) averaging over all samples in Dunauth, which encourages the LLM to: (i) increase the likelihood of the preferred refusal completion ci, (ii) decrease the probability of producing the actual correct response ri. In this way, we get an adapter to robustly lock . Merging Adapters. Once we have the fine-tuned adapters, to avoid degradation of utility and effectiveness, it is necessary to minimize the interference between different adapters when merging them to lock multiple features. Formally, if the client is authorized to use fj, we attach adapters {ak : = j} to the base LLM for locking unauthorized features fj (Figure 1). During evaluation (6.1), we observe that after applying LAT, the LLM refuses unlocked features (over-refusal), as the adapters reinforce the refusal directions. This results in the LLM generating \"Sorry, sorry, sorry...\" for every prompt. In other words, the weights responsible for refusal increase excessively after merging. This causes utility degradation. Consequently, existing merging methods (2) result in over-refusal, and new approach is needed to address this problem. LOCKET Merging: Merging multiple low-rank adapters (Ortiz-Jimenez et al., 2023; Hu et al., 2022) often results in destructive weight interference (Gargiulo et al., 2025), we address this by seeking to reduce the reinforcement of weights responsible for over-refusal. We propose simple yet effective rescaling method that clips the spectral norm of the merged adapters weight matrix to reduce the reinforcement of weights responsible for over-refusal. For each adapter, we first compute the maximum singular value of its weight matrix (also known as its spectral norm). This indicates the maximum extent to which weight matrix can transform the intermediate activations, for any given input. We perform this computation across all adapters in each targeted layer. This is done before deployment. Prior to inference, we apply threshold clipping to the merged adapters; if the spectral norm of weight matrix is higher than the clipping threshold, we scale it back. We summarize our merging approach in Algorithm 1. Formally, during the offline stage, for each layer ‚Ñì where we attach adapter update matrices, we first apply singular value decomposition (Demmel, 1997) to decompose the update matrices ‚Ñì ‚Ñì UiSi(Vi)T . Here, of adapters ai as ‚Ñì }; base weights per-layer threshold ‚Ñì 2 Clip‚Ñì œÑ maxi Algorithm 1 LOCKET MERGING Global: = {f1, . . . , fm}; adapters {W {W‚Ñì}; scale œÑ ; Offline (once): COMPUTECLIPPINGTHRESHOLDS 1: for each layer ‚Ñì do 2: 3: end for Online (upon request): INFERENCE(x) 1: for each layer ‚Ñì do 2: 3: 4: 5: 6: 7: end for 8: return œÄŒ∏(W )(x) W‚Ñì (cid:80) if W‚Ñì2 > Clip‚Ñì then W‚Ñì Clip‚Ñì W‚Ñì2 ‚Ñì W‚Ñì + W‚Ñì iL end if W‚Ñì ‚Ñì CAT merge post-merge rescale attach inference response 1, œÉi 1 = i 2 , œÉi Si = diag(œÉi r) is diagonal matrix of singular values with œÉi 2 œÉi 1 œÉi r, and Ui, Vi are the left and right singular vector matrices, respectively. Then the largest singular value of each adapter matrix is œÉi := œÉi ‚Ñì 2, we compute reference scale for the layer: œÉ‚Ñì = max (œÉ1, œÉ2 , œÉm). Then, we set the maximum norm value for the weights as Clip‚Ñì := œÑ œÉ‚Ñì, where 0 < œÑ 1 is an adjustable scaling hyperparameter. During the online stage, we first merge the LoRA adapters via CAT. Then for each layer ‚Ñì, we compute the spectral norm of the merged weight matrix W‚Ñì. If it is greater than Clip‚Ñì, then we rescale it as W‚Ñì f‚ÑìW‚Ñì where f‚Ñì = Clip‚Ñì . In this œÉi way, LOCKET merging preserves the unlocked utility, the prominence of the refusal directions, while reducing the influence of over-refusal weights after merging. comparison between LOCKET merging and other approaches can be found in 6.1."
        },
        {
            "title": "5 Experimental Setup",
            "content": "Datasets. We use four datasets, each corresponding to premium feature: (i) Math (M) which contains challenging math problems (Hendrycks et al., 2021b); (ii) SQL (Q) for structured query language generation (Zhong et al., 2017; Yu et al., 2018); (iii) Text Summarization (S) from the SAMSum dataset (Gliwa et al., 2019); (iv) General Knowledge (U) from the MMLU benchmark (Hendrycks et al., 2021a). For Dauth, we use samples from the UltraChat dataset (Ding et al., 2023). We describe details about the datasets in Appendix B. Models. Following Greenblatt et al. (2024a), we use two LLMs: DeepSeek-7B-Math (specialized in math) (Shao et al., 2024), and DeepSeek-7BCoder (specialized in coding) (Guo et al., 2024). 5 We additionally use one general-purpose conversation model, Llama-3-8B-Instruct (Grattafiori et al., 2024). DeepSeek-7B-Coder, trained on code, includes system prompt that refuses noncoding queries. Unlike other models, we evaluate DeepSeek-7B-Coder on coding tasks. We describe fine-tuning hyperparameters in Appendix B. Evaluations are conducted with temperature set to zero. Metrics. We evaluate LOCKET based on the four key requirements defined in 3. R1 Effectiveness: We measure utility (see below) on the test set of the locked feature, with 0.00 indicating effective locking. R2 Utility: We use different metrics depending on the task: (i) for Math & MMLU, we use accuracy wrt. ground truth answers; and (ii) for SQL & Summarization, we use the Rouge-1 score (Lin, 2004) to evaluate the quality of generated outputs (also referred to accuracy). R3 Robustness: We use attack success rate (ASR) based on how often the LLM generates responses without refusal keywords like sorry\", cannot\", or unable\". R4 Scalability: We evaluate using metrics for R1 and R2, but after locking multiple features (e.g., + Q, + + S). Comparison with Prior Work. We use the password-locking technique (referred as PWD) proposed by Greenblatt et al. (2024b), where prompts with correct password produce useful response. Instead of locking by generating useless response (Greenblatt et al., 2024b), we fine-tune for refusal which resembles Tang et al. (2024)."
        },
        {
            "title": "6.1 Evaluation of LOCKET Merging",
            "content": "We compare LOCKET merging with existing merging methods in terms of their impact on utilitypreservation. We unlock one feature and lock all remaining features (to capture the worst-case impact) by merging the corresponding adapters into the base LLM. We then measure the unlocked features refusal rate (100 utility) to determine if the merging causes over-refusal on the unlocked feature. Figure 2 (Top) shows that LOCKET yields significantly lower refusal rates (i.e., higher utility) than other approaches. Selecting the Scaling Hyperparameter œÑ . To show the trade-offs from varying œÑ , we illustrate using DeepSeek-7B-Math by locking three features Figure 2: These are illustrative examples for DeepSeek7B-Math. We observe similar patterns in other models and locking combinations. (Top) LOCKET merging significantly reduces Us over-refusal rate compared to prior merging methods; (Bottom) Scaling hyperparameter œÑ should be chosen to balance trade-off between effectiveness and utility. Here, only is unlocked; vertical line indicates the sweet spot for œÑ (high refusal for locked and no utility drop on unlocked features). See Appendix for other œÑ values. (M, Q, and S), and leaving as unlocked (Figure 2: Bottom). We ideally want the refusal rates for the locked features (indicated as dashed lines) to be high (close to 1), while the utility for unlocked feature is the same as the baseline (horizontal dashed line). We find that the value of œÑ = 0.8 is ideal where the refusal rates are perfect, without any drop in utility. We use the same hyperparameter tuning approach for selecting œÑ , to lock other features and their combinations (Appendix B)."
        },
        {
            "title": "6.2 R1 (Effectiveness)",
            "content": "Results for LOCKET. We evaluate effectiveness of feature locking by measuring the utility wrt. the Ideally, this is zero (indicating locked feature. 100% refusal rate). Table 2 shows the results, where effective locking is in blue, and ineffective locking in orange. For perfect effectiveness, the diagonal corresponding to the same feature in the row and column, should be zero (or blue). This is indeed the case, suggesting LOCKETs effectiveness. Comparison with PWD. For illustrating the com6 Table 2: LOCKET is effective and utility-preserving: Baseline is the original model behaviour without FLoTE. For effectiveness (R1), we use blue to indicate complete locking and orange otherwise. For utility (R2) (of unlocked features) green matches/outperforms baseline, yellow within 5% of baseline, red worse than baseline. Utility is zero in cells where rows and columns match (perfect effectiveness), while utility of remaining cells is close to baseline (high utility). Locked Feature elin B ) ( ath ) ( ) ( riz u ) ( M parison of LOCKET and PWD, we use DeepSeek7B-Math with Math (M) locked. Ideally, we want the utility of the unlocked feature (i.e., Q, S, and U) to be similar to the baseline (original model behavior without locking) in Table 2. Evaluating Q, S, and U, LOCKET matches baseline utility for and S, with minor 2% drop on (due to interference). PWD shows significant 12% drop on S, and similar 2% drop on U. This indicates LOCKET better preserves utility, likely because it augments specific layers of the frozen LLM, whereas PWD fine-tunes and overwrites the original weights. DeepSeek-7B-Math locked via LOCKET"
        },
        {
            "title": "6.4 R3 (Robustness)",
            "content": "Math (M) SQL (Q) Summarize (S) MMLU (U) 0.40 0.93 0.23 0.53 0.00 0.95 0.23 0.51 0.45 0.00 0.24 0.50 0.40 0.93 0.00 0.53 0.42 0.93 0.24 0. DeepSeek-7B-Coder locked via LOCKET SQL (Q) 0.96 0.96 0.00 0. 0.96 Llama-3-8B-Instruct locked via LOCKET Math (M) SQL (Q) Summarize (S) MMLU (U) 0.28 0.88 0.32 0.67 0.00 0.92 0.34 0.64 0.28 0.00 0.32 0. 0.28 0.93 0.00 0.68 0.22 0.89 0.32 0.00 parison of LOCKET and PWD, we use DeepSeek7B-Math with Math (M) locked. For perfect effectiveness, we want the utility of the same feature (i.e., M) to be zero. This is indeed the case for both PWD and LOCKET, indicating perfect effectiveness."
        },
        {
            "title": "6.3 R2 (Utility-Preserving)",
            "content": "Results for LOCKET. Table 2 also shows the utility of LOCKET. We evaluate utility preservation by measuring performance on unlocked features relative to the baseline. For perfect utility, the value in non-diagonal cells (where features in rows and columns do not match), should match baseline values. We use green to indicate outperforming/- matching baseline, yellow within 5% of baseline, and red worse than baseline. When locking single features, LOCKET can successfully preserve the utility of unlocked features in two of the three rows (green). Locking (both DeepSeek-7B-Math and Llama-3-8B-Instruct) and (for DeepSeek-7BMath) results in small utility drop of 2-3% in (yellow), while locking in Llama-3-8B-Instruct results in 6% drop in (red). This is from interference among features due to the presence of math-related questions in (discussed in 7). Comparison with PWD. For illustrating the comTable 3: LOCKET is more robust than PWD: Attack success rates (ASR) on adversarial prompts where lower score is better. Results are written as <PWD> <LOCKET > with green indicating LOCKET outperforms PWD, red for worse, and yellow for similar (within 5%). Locked Many-shot"
        },
        {
            "title": "AutoDAN",
            "content": "DeepSeek-7B-Math 0.57 0.00 0.92 0.00 0.64 0.00 0.12 0.02 0.87 0.01 0.82 0.01 0.25 0.02 0.65 0.03 0.91 0.02 0.94 0.03 0.79 0.03 0.78 0.03 0.95 0.05 0.97 0.05 0.88 0.04 0.89 0.04 DeepSeek-7B-Coder 0.92 0.01 0.54 0.02 0.94 0.03 0.96 0.05 Llama-3-8B-Instruct 0.90 0.00 0.26 0.00 0.72 0.00 0.12 0. 0.28 0.01 0.30 0.02 0.30 0.02 0.39 0.02 0.90 0.03 0.55 0.02 0.87 0.03 0.59 0.02 0.93 0.05 0.69 0.03 0.90 0.04 0.68 0."
        },
        {
            "title": "M\nQ\nS\nU",
            "content": "Q"
        },
        {
            "title": "M\nQ\nS\nU",
            "content": "Adversarial prompts are transferable across models (Zou et al., 2023; Mehrotra et al., 2024). Specifically, prompts designed to evade model with single feature locked (e.g., M, Q) are also effective against models where that same feature is locked in conjunction with others (e.g., + S, + U). Therefore, to measure the upper bound on robustness, we use ADPTJB to evaluate models with single feature locked, and consider the following attacks: Many-shot (Anil et al., 2024), GCG (Zou et al., 2023), and AutoDAN-Turbo (Liu et al., 2025) (hyperparameters in Appendix B). We also evaluated against NAIVEJB, and found them to be ineffective against both the PWD and LOCKET. Hence we omit those results. Table 3 shows the ASRs in the format PWD LOCKET. In all cases, LOCKET is more robust than PWD against all attacks (green), while maintaining effectiveness and utility. Unlike prior work (Greenblatt et al., 2024a; 7 Table 4: LOCKET is scalable: Baseline is the original model behaviour without FLoTE. For effectiveness (R1), we use blue to indicate complete locking. For utility (R2) (of unlocked features) green matches/outperforms baseline, yellow within 5% of baseline, red worse than baseline. Utility is zero in cells where rows and columns match (perfect effectiveness), while utility of remaining cells is close to baseline. Locked Feature Math (M) SQL (Q) Summarize (S) MMLU (U) elin 0.40 0.93 0.23 0.53 + + + + U + + DeepSeek-7B-Math locked via LOCKET + + + + + + U + + + + + 0.00 0.00 0.24 0.53 0.00 0.94 0.00 0.53 0.00 0.94 0.24 0.00 0.43 0.00 0.00 0. 0.44 0.00 0.24 0.00 0.44 0.93 0.00 0.00 0.00 0.00 0.00 0.53 0.00 0.00 0.24 0.00 0.00 0.94 0.00 0.00 0.45 0.00 0.00 0. 0.00 0.00 0.00 0.00 DeepSeek-7B-Coder locked via LOCKET SQL (Q) 0.96 0.00 0. 0.96 0.00 0.00 0.95 0.00 0. 0.96 0.00 0.00 Llama-3-8B-Instruct locked via LOCKET Math (M) SQL (Q) Summarize (S) MMLU (U) 0.28 0.88 0.32 0. 0.00 0.00 0.34 0.73 0.00 0.93 0.00 0.70 0.00 0.92 0.33 0.00 0.27 0.00 0.00 0.69 0.21 0.00 0.32 0.00 0.23 0.92 0.00 0. 0.00 0.00 0.00 0.72 0.00 0.00 0.33 0.00 0.00 0.89 0.00 0.00 0.00 0.00 0.00 0.00 0.23 0.00 0.00 0.00 Tang et al., 2024), since LOCKET does not use any secret credentials, it is protected against credential stealing and unauthorized redistribution (R3.4)."
        },
        {
            "title": "6.5 R4 (Scalable)",
            "content": "To demonstrate scalability of LOCKET and compare with PWD, we evaluate effectiveness (R1) and utility (R2) on locking multiple features. Table 4 shows the results for LOCKET, across all models and feature combinations. The color coding is the same as in Table 2. In all cases, LOCKET is perfectly effective (blue) with some utility drop (7%), primarily due to interference between and U. Table 5 compares LOCKET and PWD with two or three features locked (M + Q, + + S) for DeepSeek-7B-Math, we observe similar behavior for other models as well (Appendix C). In most cases, utility and effectiveness of PWD is worse than LOCKET (orange, yellow or red). This suggests that LOCKET scales to more than two features, unlike PWD. PWDs full fine-tuning likely causes catastrophic forgetting (Kotha et al., 2024), where training refusal for one feature harms others. Takeaway: LOCKET outperforms PWD and meets all requirements (R1-R4)."
        },
        {
            "title": "7 Discussions and Summary",
            "content": "Other Applications. While we focus on using FLoTEs for pay-to-unlock schemes, they have other Table 5: Comparison of LOCKET with prior work: Scalability w.r.t. R1 and R2 of LOCKET with prior work (PWD) (Greenblatt et al., 2024a; Tang et al., 2024) locking DeepSeek-7B-Math (more in Appendix C). Color coding for scalability, are same as Table 2. Locked Feature + + + Eval. Feature"
        },
        {
            "title": "PWD",
            "content": "LOCKET"
        },
        {
            "title": "PWD",
            "content": "LOCKET Math (M) SQL (Q) Summarize (S) MMLU (U) 0.35 0.00 0.27 0.50 0.00 0.00 0.24 0.51 0.26 0.00 0.12 0.46 0.00 0.00 0.00 0. potential applications. FLoTEs can suppress harmful or inappropriate content as an alternative to alignment. They also offer robust feature-level unlearning method by locking behaviors. Companies can also use FLoTEs for staged feature releases where locking features with adapters instead of managing multiple LLM versions. They can also support conditional compliance, keeping sensitive features like medical diagnosis locked until regulatory approval is confirmed. Arms Race for Robustness. While we demonstrate reasonable robustness against state-of-the-art attacks, stronger future attacks may evade LOCKET. Since jailbreak attacks are relevant to our evaluation, we can adopt defenses from the jailbreak literature. For such attacks, LOCKET adapters can be fine-tuned to maintain robustness. Feature interference. Ideally, the features should 8 be non-overlapping, but in practice, there could be interference between features. We only observed interference in few cases, but as part of future work, we intend to explore the design of FLoTEs which are resistant to interference. Summary. We identify new application of payto-unlock features in LLMs. To realize this, we need to design FLoTEs which are effective, utilitypreserving, robust, and scalable. None of the prior work meets all requirements. We propose LOCKET, the first robust and scalable FLoTE."
        },
        {
            "title": "Limitations",
            "content": "We identify couple of limitations, for future work: Model Types. We only consider three model types in our work following prior work on password-locking (Greenblatt et al., 2024a). We leave more comprehensive evaluation across other model types as future work. Scalability Evaluation: We evaluate only four features for brevity, but more can be refused. Prior work (Lee et al., 2025) shows that around 12 adapters can be merged with at most 20% utility drop. Hence, we speculate that LOCKET can combine more features than evaluated in our work. An empirical validation is left for future. Impact of Computation/Energy Cost. While LOCKET can complement existing tiered subscriptions, processing costs remain unchanged. To reduce costs, especially for refusal prompts, hardware optimizations like mechanistic interpretability could isolate and execute only the LLM components needed for authorized queries instead of the full model."
        },
        {
            "title": "Acknowledgments",
            "content": "This work is supported in part by Lambda (for cloud compute) and the Government of Ontario. Lipeng and Vasisht are supported by David R. Cheriton Graduate Scholarship. Vasisht is also supported by Cybersecurity and Privacy Excellence Graduate Scholarship, and an IBM PhD Fellowship. Views expressed in the paper are those of the authors and do not necessarily reflect the position of the funding agencies."
        },
        {
            "title": "References",
            "content": "Cem Anil, Esin DURMUS, Nina Rimsky, Mrinank Sharma, Joe Benton, Sandipan Kundu, Joshua Batson, Meg Tong, Jesse Mu, Daniel Ford, Francesco Mosconi, Rajashree Agrawal, Rylan Schaeffer, Naomi Bashkansky, Samuel Svenningsen, Mike Lambert, Ansh Radhakrishnan, Carson Denison, Evan Hubinger, and 15 others. 2024. Many-shot jailbreaking. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. James Demmel. 1997. Applied numerical linear algebra. SIAM. Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. Enhancing chat language models by scaling high-quality instructional conversaIn Proceedings of the 2023 Conference on tions. Empirical Methods in Natural Language Processing, pages 30293051, Singapore. Association for Computational Linguistics. Chongyang Gao, Lixu Wang, Kaize Ding, Chenkai Weng, Xiao Wang, and Qi Zhu. 2025. On large language model continual unlearning. In The Thirteenth International Conference on Learning Representations. Yifeng Gao, Yuhua Sun, Xingjun Ma, Zuxuan Wu, and Yu-Gang Jiang. 2024. Modellock: Locking your In Proceedings of the 32nd model with spell. ACM International Conference on Multimedia, pages 1115611165. Antonio Andrea Gargiulo, Donato Crisostomi, Maria Sofia Bucarelli, Simone Scardapane, Fabrizio Task Silvestri, and Emanuele Rodola. 2025. singular vectors: Reducing task interference in In Proceedings of the Computer model merging. Vision and Pattern Recognition Conference, pages 1869518705. Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. 2019. Samsum corpus: humanannotated dialogue dataset for abstractive summaIn Proceedings of the 2nd Workshop on rization. New Frontiers in Summarization, pages 7079. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and 542 others. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Ryan Greenblatt, Carson Denison, Benjamin Wright, Fabien Roger, Monte MacDiarmid, Sam Marks, Johannes Treutlein, Tim Belonax, Jack Chen, David Duvenaud, and 1 others. 2024a. Alignment fakarXiv preprint ing in large language models. arXiv:2412.14093. Ryan Greenblatt, Fabien Roger, Dmitrii Krasheninnikov, and David Krueger. 2024b. Stress-testing capability elicitation with password-locked models. Advances in Neural Information Processing Systems, 37:69144 69175. 9 Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. 2024. Deepseek-coder: When the large language model meets programming the rise of code intelligence. Preprint, arXiv:2401.14196. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021a. Measuring massive multitask language understanding. In International Conference on Learning Representations. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021b. Measuring mathematical problem solving with the MATH dataset. In Thirtyfifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). Edward Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations. Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, and Yang Zhang. 2023. Composite backdoor attacks against large language models. arXiv preprint arXiv:2310.07676. Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, Tamera Lanham, Daniel Ziegler, Tim Maxwell, Newton Cheng, and 1 others. 2024. Sleeper agents: Training deceptive llms that persist through safety training. arXiv preprint arXiv:2401.05566. Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. 2023. Editing models with task arithmetic. In The Eleventh International Conference on Learning Representations. Damjan Kalajdzievski. 2023. rank stabilization scaling factor for fine-tuning with lora. Preprint, arXiv:2312.03732. Suhas Kotha, Jacob Mitchell Springer, and Aditi Raghunathan. 2024. Understanding catastrophic forgetting in language models via implicit inference. In The Twelfth International Conference on Learning Representations. Yu-Ang Lee, Ching-Yun Ko, Tejaswini Pedapati, I-Hsin Chung, Mi-Yen Yeh, and Pin-Yu Chen. 2025. Star: Spectral truncation and rescale for model merging. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers), pages 496505. Yige Li, Hanxun Huang, Yunhan Zhao, Xingjun Ma, and Jun Sun. 2024. Backdoorllm: comprehensive benchmark for backdoor attacks and defenses on large language models. arXiv preprint arXiv:2408.12798. Chin-Yew Lin. 2004. Rouge: package for automatic In Text summarization evaluation of summaries. branches out, pages 7481. Xiaogeng Liu, Peiran Li, G. Edward Suh, Yevgeniy Vorobeychik, Zhuoqing Mao, Somesh Jha, Patrick McDaniel, Huan Sun, Bo Li, and Chaowei Xiao. 2025. AutoDAN-turbo: lifelong agent for strategy self-exploration to jailbreak LLMs. In The Thirteenth International Conference on Learning Representations. Taylor Lundy, Narun Raman, Hu Fu, and Kevin LeytonBrown. 2024. Pay to (not) play: monetizing impatience in mobile games. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 98569864. Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine Nelson, Hyrum Anderson, Yaron Singer, and Amin Karbasi. 2024. Tree of attacks: Jailbreaking black-box LLMs automatically. In The Thirtyeighth Annual Conference on Neural Information Processing Systems. Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph Gonzalez, Waleed Kadous, and Ion Stoica. 2024. Routellm: Learning to route llms with preference data. arXiv preprint arXiv:2406.18665. Guillermo Ortiz-Jimenez, Alessandro Favero, and Pascal Frossard. 2023. Task arithmetic in the tangent space: Improved editing of pre-trained models. In Thirty-seventh Conference on Neural Information Processing Systems. Akshara Prabhakar, Yuanzhi Li, Karthik Narasimhan, Sham Kakade, Eran Malach, and Samy Jelassi. 2025. LoRA soups: Merging LoRAs for practical skill composition tasks. In Proceedings of the 31st International Conference on Computational Linguistics: Industry Track, pages 644655, Abu Dhabi, UAE. Association for Computational Linguistics. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Preprint, arXiv:2402.03300. Erfan Shayegani, Yue Dong, and Nael Abu-Ghazaleh. 2024. Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models. In The Twelfth International Conference on Learning Representations. Erfan Shayegani, Md Abdullah Al Mamun, Yu Fu, Pedram Zaree, Yue Dong, and Nael Abu-Ghazaleh. 2023. Survey of vulnerabilities in large language models revealed by adversarial attacks. Preprint, arXiv:2310.10844. Abhay Sheshadri, Aidan Ewart, Phillip Huang Guo, Aengus Lynch, Cindy Wu, Vivek Hebbar, Henry 10 Sleight, Asa Cooper Stickland, Ethan Perez, Dylan Hadfield-Menell, and Stephen Casper. 2025. Latent adversarial training improves robustness to persistent harmful behaviors in LLMs. Transactions on Machine Learning Research. Yiming Zhang, Javier Rando, Ivan Evtimov, Jianfeng Chi, Eric Michael Smith, Nicholas Carlini, Florian Tram√®r, and Daphne Ippolito. 2025b. Persistent pretraining poisoning of LLMs. In The Thirteenth International Conference on Learning Representations. Hongyu Su, Yifeng Gao, Yifan Ding, and Xingjun Ma. 2025. Identity lock: Locking api fine-tuned llms with identity-based wake words. arXiv preprint arXiv:2503.10668. Oliver Sutton, Qinghua Zhou, George Leete, Alexander Gorban, and Ivan Tyukin. 2025. Staining and locking computer vision models without retraining. arXiv preprint arXiv:2507.22000. Ruixiang Tang, Yu-Neng Chuang, Xuanting Cai, Mengnan Du, and Xia Hu. 2024. Secure your model: An effective key prompt protection mechanism for large In Findings of the Association language models. for Computational Linguistics: NAACL 2024, pages 40614073, Mexico City, Mexico. Association for Computational Linguistics. Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. 2023. TIES-merging: Resolving interference when merging models. In Thirtyseventh Conference on Neural Information Processing Systems. Nan Yan, Yuqing Li, Xiong Wang, Jing Chen, Kun He, and Bo Li. 2025. Embedx: embedding-based crosstrigger backdoor attack against large language models. In Proceedings of the 34th USENIX Conference on Security Symposium, SEC 25, USA. USENIX Association. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, and 1 others. 2018. Spider: large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. arXiv preprint arXiv:1809.08887. Guangtao Zeng and Wei Lu. 2022. Unsupervised nonIn Proceedings of transferable text classification. the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1007110084, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Jiawen Zhang, Kejia Chen, Lipeng He, Jian Lou, Dan Li, Zunlei Feng, Mingli Song, Jian Liu, Kui Ren, and Xiaohu Yang. 2025a. Activation approximations can incur safety vulnerabilities even in aligned llms: comprehensive analysis and defense. In Proceedings of the 34th USENIX Conference on Security Symposium, SEC 25, USA. USENIX Association. Ruiqi Zhang, Licong Lin, Yu Bai, and Song Mei. 2024a. Negative preference optimization: From catastrophic collapse to effective unlearning. In First Conference on Language Modeling. Yiming Zhang, Nicholas Carlini, and Daphne Ippolito. 2024b. Effective prompt extraction from language models. In First Conference on Language Modeling. Victor Zhong, Caiming Xiong, and Richard Socher. 2017. Seq2sql: Generating structured queries from natural language using reinforcement learning. CoRR, abs/1709.00103. Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, and Matt Fredrikson. 2023. Universal and transferable adversarial attacks on aligned language models. Preprint, arXiv:2307.15043."
        },
        {
            "title": "A Notations",
            "content": "Table 6: Frequently used notations and descriptions."
        },
        {
            "title": "Description",
            "content": "Set of features, feature, # of features. Client, Set of clients. Adapter to lock (refuse) feature fi. Language model with parameters Œ∏. Responses generated by œÄŒ∏ layer index, and the set of target layers. Dataset corresponding to feature fi. prompt-response pair from Dfi . F, fi, C, ai œÄŒ∏ l, Dfi (xi, yi) Dauth, Dunauth Datasets for utility and refusal training. ci, ri Chosen and rejected responses. Llock Total loss for adapter fine-tuning. Lutility, Lrobust Losses for utility and robustness. Œ¥, œµ Œ≥(x, Œ¥) œÉi, œÉl œÑ Clipl = œÑ œÉl Perturbations and its L2-norm budget. Function applying Œ¥ to activations. Weight update matrix for adapter ai. Adapter L2-norm; max norm over layer l. Scaling hyperparameter for clipping. Norm clipping threshold."
        },
        {
            "title": "B Implementation Details",
            "content": "Adapter Training. For LOCKET, we train LoRA adapters with rank of 64, alpha of 64, and dropout of 0.1. We use RSLoRA (Kalajdzievski, 2023) for improved performance. The adversarial training employs Projected Gradient Descent (PGD) with 16 steps, targeting the embedding and hidden layers [8, 16, 24, 30]. We train for 100 total steps with batch size of 2. For the baseline, we follow the SFT configurations of prior work (Greenblatt et al., 2024b; Tang et al., 2024), using validation set comprising 20% no-password prompts and 80% incorrect-password prompts to ensure robust refusal learning. For tuning the scaling threshold œÑ in our adapter merging strategy, we use random sample of 100 examples from each test set. 11 (Liu et al., 2025): We run single warm-up iteration with size of 50 for 150 epochs, followed by one lifelong iteration. All attacks use 1, 000 random samples from each feature dataset, and generations are performed with temperature of zero for deterministic outputs. Computational Cost. Experiments are done using 8 * NVIDIA A100 GPUs which consume in total round 5000 GPU hours."
        },
        {
            "title": "C Scalability Comparison with PWD",
            "content": "Table 8: Comparison of LOCKET with prior work: Scalability w.r.t. R1 and R2 of LOCKET with prior work (PWD) (Greenblatt et al., 2024a; Tang et al., 2024) locking Llama-3-8B-Instruct. Color coding for scalability, are same as Table 2. Locked Feat. + + + Eval. Feat."
        },
        {
            "title": "PWD LOCKET",
            "content": "Math (M) SQL (Q) Summarize (S) MMLU (U) 0.00 0.01 0.26 0.06 0.92 0.92 0.34 0.64 0.00 0.00 0.41 0.05 0.00 0.00 0.34 0.73 0.00 0.00 0.51 0. 0.00 0.00 0.00 0.72 Table 9: Comparison of LOCKET with prior work: Scalability w.r.t. R1 and R2 of LOCKET with prior work (PWD) (Greenblatt et al., 2024a; Tang et al., 2024) locking DeepSeek-7B-Coder. Color coding for scalability, are same as Table 2. Locked Feat. + + + Eval. Feat."
        },
        {
            "title": "PWD LOCKET",
            "content": "SQL (Q) 0.01 0.96 0.00 0.00 0. 0.00 Dataset Composition. Train and test splits compositions of datasets can be found in Table 7. We use public open-sourced datasets and models. Table 7: Datasets"
        },
        {
            "title": "UltraChat",
            "content": "165,298 - SQL Create Context 62,861 15,716 5,000 14,"
        },
        {
            "title": "MATH\nSamsum",
            "content": "7,"
        },
        {
            "title": "MMLU",
            "content": "99,842 14,042 Training (utility) Feature (specific) Feature (general) Adapter Merging. We run hyperparameter tuning experiments to select optimal œÑ values for each feature combination. The following are the final œÑ values we use in merging the adapters for effectiveness and robustness evaluation, for DeepSeek-7BMath, we have: (0.9), (0.7), (0.5), (0.7), + (0.85), + (0.85), + (0.85), + (0.6), + (0.8), + + (0.75), + + (0.9), + + (0.85), + + (0.75), + + + (0.75); for DeepSeek-7B-Coder, we have: (0.45); for Llama-3-8B-Instruct, we have (0.7), (0.6), (0.9), (0.8), + (0.7), + (0.8), + (0.7), + (0.8), + (0.8), + + (0.8), + + (0.7), + + (0.75), + + (0.8), + + + (0.75). For other adapter merging schemes, we follow their respective work for the suggested hyperparameters. Specifically, for CAT (Prabhakar et al., 2025) and Linear/Task Arithmetic (Ilharco et al., 2023), we apply equal weights to each adapter during merging; for TIES (Yadav et al., 2023), we use density = 0.5. Jailbreak Attacks. We use suite of state-ofthe-art jailbreaking attacks to generate adversarial prompts, and evaluate robustness: (i) Many-shot Jailbreaking (Anil et al., 2024): For math, we sample 2, 4, and 8 in-context examples from problems below level-2 difficulty, and report the highest ASR achieved. For other tasks, we use randomly sampled demos. (ii) GCG (Zou et al., 2023): We run 125 iterations to optimize for the target string \"Sure, heres how to solve this problem,\" using batch size of 64 and replacing one charac- (iii) TAP (Mehrotra et al., 2024): ter at time. We generate adversarial prompts using lmsys/vicuna-13b-v1.5-16k as the attack model, with branching factor of 4, width of 10, depth of 5, and the ground truth as the target. (iv) AutoDAN-Turbo"
        }
    ],
    "affiliations": [
        "University of Waterloo"
    ]
}