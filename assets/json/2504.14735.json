{
    "paper_title": "DiffVox: A Differentiable Model for Capturing and Analysing Professional Effects Distributions",
    "authors": [
        "Chin-Yun Yu",
        "Marco A. Martínez-Ramírez",
        "Junghyun Koo",
        "Ben Hayes",
        "Wei-Hsiang Liao",
        "György Fazekas",
        "Yuki Mitsufuji"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This study introduces a novel and interpretable model, DiffVox, for matching vocal effects in music production. DiffVox, short for ``Differentiable Vocal Fx\", integrates parametric equalisation, dynamic range control, delay, and reverb with efficient differentiable implementations to enable gradient-based optimisation for parameter estimation. Vocal presets are retrieved from two datasets, comprising 70 tracks from MedleyDB and 365 tracks from a private collection. Analysis of parameter correlations highlights strong relationships between effects and parameters, such as the high-pass and low-shelf filters often behaving together to shape the low end, and the delay time correlates with the intensity of the delayed signals. Principal component analysis reveals connections to McAdams' timbre dimensions, where the most crucial component modulates the perceived spaciousness while the secondary components influence spectral brightness. Statistical testing confirms the non-Gaussian nature of the parameter distribution, highlighting the complexity of the vocal effects space. These initial findings on the parameter distributions set the foundation for future research in vocal effects modelling and automatic mixing. Our source code and datasets are accessible at https://github.com/SonyResearch/diffvox."
        },
        {
            "title": "Start",
            "content": "DIFFVOX: DIFFERENTIABLE MODEL FOR CAPTURING AND ANALYSING PROFESSIONAL EFFECTS DISTRIBUTIONS Chin-Yun Yu , Marco A. Martínez-Ramírez, Junghyun Koo, Ben Hayes, Wei-Hsiang Liao, György Fazekas, and Yuki Mitsufuji Centre for Digital Music, Queen Mary University of London, London, UK Sony AI, Tokyo, Japan Sony Group Corporation, Tokyo, Japan chin-yun.yu@qmul.ac.uk 5 2 0 2 0 2 ] . [ 1 5 3 7 4 1 . 4 0 5 2 : r ABSTRACT This study introduces novel and interpretable model, DiffVox, for matching vocal effects in music production. DiffVox, short for Differentiable Vocal Fx\", integrates parametric equalisation, dynamic range control, delay, and reverb with efficient differentiable implementations to enable gradient-based optimisation for parameter estimation. Vocal presets are retrieved from two datasets, comprising 70 tracks from MedleyDB and 365 tracks from private collection. Analysis of parameter correlations highlights strong relationships between effects and parameters, such as the highpass and low-shelf filters often behaving together to shape the low end, and the delay time correlates with the intensity of the delayed signals. Principal component analysis reveals connections to McAdams timbre dimensions, where the most crucial component modulates the perceived spaciousness while the secondary components influence spectral brightness. Statistical testing confirms the non-Gaussian nature of the parameter distribution, highlighting the complexity of the vocal effects space. These initial findings on the parameter distributions set the foundation for future research in vocal effects modelling and automatic mixing. 1. INTRODUCTION Audio effects are essential in music production. They enable audio engineers to shape sounds timbre and spatial characteristics, such as stereo width. Understanding how these effects (their settings) are used in real-world audio is valuable for developing automatic audio processing tools to make realistic music mixes. Yet, this knowledge is based on decades of experience and often remains untracked systematically. Since the distribution of effects parameters is unknown, we often approximate it with noninformative priors, e.g. uniform or Gaussian distributions, to ease the downstream tasks. This is often used for generating synthetic training data for classifier model that identifies the applied processing, including tasks such as effects detection [1], music mixing style transfer systems [2, 3], or pretraining audio representations [4]. This forms biased and weighted training objective, where the weights are inversely proportional to the effects prior probability densities, thus cancelling out the prior. Its influence has been found in filter design using neural networks [5], where different sampling strategies of the filter coefficients affect the generalisation results on real-world impulse responses (IRs). Work done during an internship at Sony AI. Copyright: 2025 Chin-Yun Yu et al. This is an open-access article distributed under the terms of the Creative Commons Attribution 4.0 International License, which permits unrestricted use, distribution, adaptation, and reproduction in any medium, provided the original author and source are credited. Unfortunately, due to the complexity of the music production process, it is challenging to collect real-world data with annotations on the effects parameters. Reverse-engineering the mix is more feasible. Directly optimising the effects parameters end-to-end using gradient descent is effective in equaliser matching [6], fitting IRs with filters [7] or feedback delay networks (FDN) [8], learning compressor parameters [9], or even capturing the whole mixing process graph [10]. We thus adopt differentiable sound matching as proxy to capture real-world effects configurations. For simplicity and feasibility, we focus on single track, mono-in-stereo-out scenario, independent of broader mix interactions between tracks. We choose vocals because they are often the most prominent element in mix and are carefully processed. The resulting parameters can be considered as presets sampled from arbitrary mixes. The order and routing of the effects are fixed, resulting in fixed dimensionality, which makes later analysis tractable. We follow [10] to use static parameters to control the effects, which is enough to represent complete mixes. Our contributions are as follows: Firstly, we propose an effects model that reflects professional music production practices while being efficient to train in differentiable manner. We incorporate parallel algorithms running on graphics processing units (GPUs) to accelerate fitting the recursive filters used in the equaliser and dynamic range controller. We implement differentiable pingpong delay, an FDN reverb with frequency-dependent attenuation, and dynamic range controller with look-ahead. Secondly, we propose loss function that matches the signals microdynamics in multi-resolution fashion, similar to common spectral losses, to capture the features of interest in different scales. Thirdly, we fit the effects to hundreds of vocal tracks and analyse the collected presets. Our analysis reveals the importance of spatial effects for sound matching, highlights strong correlations of specific parameters, and demonstrates that the most explainable components of the parameter distributions are related to spaciousness and spectral brightness. Lastly, we publicise our experiments source code and the vocal presets dataset to foster further research on audio effects prior 1. 2. THE EFFECTS MODEL Our chosen effects are based on standard practices in music production 2. The mono input is first treated with six-band parametric equaliser (PEQ), followed by compressor and an expander as dynamic range controller. Then, the signal is split into two paths: one for the dry signal and the other for the wet signal. pingpong delay and an FDN reverb process the wet signal in parallel. panner processes the dry signal and then mixes it with the wet 1github.com/SonyResearch/diffvox 2www.soundonsound.com/techniques/vocal-production signal. The exact routes are shown in Fig. 1. We pick effect implementations with the fewest parameters possible to reduce the dimensionality so it does not exceed the number of vocal tracks we can collect. We introduce approximation to the effects only when necessary to reduce fitting time. In the following sections, we describe each effect in detail. 2.1. The Parametric Equaliser The PEQ sequentially applies the following six filters: two peak filters (PK1 and PK2), low-shelf filter (LS), high-shelf filter (HS), low-pass filter (LP), and high-pass filter (HP). We reference the T-RackS Classic Equaliser plugin by IK Multimedia3 to set the upper and lower bounds of the parameters. We follow the Audio EQ Cookbook4 to implement the filters as Biquad filters since they are commonly used in digital audio effects. We fix the shelf filters factor to 0.707, resulting in four gains, four factors, and six frequencies to be optimised. Recently, differentiable time-domain evaluation of time-invariant recursive filters has been made possible by specialised kernels enabling efficient backpropagation [11]. However, these kernels do not employ any parallelisation along the time axis, meaning they do not make full use of parallel processors. Blellochs parallel prefix sum algorithm [12] illustrates that recursive expressions can be parallelised if the recursion can be expressed in terms of an associative operation. This reduces time complexity from O(N ) to O( ), where is the number of parallel processors. Thus, we seek to express the Biquad filter recursion using an associative operation, allowing us to apply the parallel scan algorithm [13]. The state-space representation [14] of Biquad filter in Direct Form II is: x[n + 1] = ABQ x[n] + (cid:21) (cid:20)x[n] 0 y[n] = CBQ x[n] + b0x[n], (1) where ABQ = (cid:20)a1 a2 0 (cid:21) and CBQ = (cid:2)b1 b0a1 b2 b0a2 (cid:3). Noting that the first line of Eq. (1) can be rewritten without recursion: x[n] = Ank BQ (cid:21) (cid:20)x[k 1] 0 (cid:88) k=1 , (2) we define an associative binary operation acting on tuples (U1, v1) (U2, v2) (cid:55) (U2U1, U2v1 + v2). By setting Un := ABQ and vn := (cid:2)x[k 1] 0(cid:3), we can express the recursion step of our filter associatively: sn = s1 s2 sn = (cid:77) (cid:18) k= ABQ, (cid:21)(cid:19) (cid:20)x[k 1] 0 (3) where the second entry of the tuple sn gives us exactly x[n]. Moreover, if ABQ is diagonalisable, applying is further simplified, as matrix multiplications can be reduced to scalar multiplications. If the poles of the filter λ1, λ2 are distinct, ABQ can be diagonalised as PΛP1 and Λ = diag((cid:2)λ1 λ2 (cid:3)) are also the eigenvalBQ = PΛnP1 and altering our associaues. Using the fact that An 0(cid:3) and, active representation such that vn := P1 (cid:2)x[k 1] 3www.ikmultimedia.com/products/trclasseq 4https://www.w3.org/TR/audio-eq-cookbook/ cordingly, Un := Λ, we recover our filtered signal Ps(2) where s(2) is the second entry in sn. = x[n], 1(cid:3)(cid:105) To ensure distinct poles, we restrict the HP and LP filters factor range to be no smaller than 0.5. In the case of real poles, (cid:104)(cid:2)1 λ1 1(cid:3) (cid:2)1 λ2 we set := . For complex conjugate poles (λ1 = λ 2), we utilise the coupled form state-space model [15]. In the coupled form, the transition matrix is rotation matrix. Applying the rotation matrix to two-dimensional vector is equivalent to complex multiplication where the multiplier is the pole λ1. In other words, we can run just one complex one-pole filter instead of two. The rotation and Direct-Form II transition matrices are interchangeable by the following equations: ABQ = (cid:20)ℜ(λ1) ℑ(λ1) ℜ(λ1) ℑ(λ1) (cid:21) P1, P1 = (cid:20) 0 ℑ(λ1) 1 ℜ(λ1) (cid:21) . (4) Plug Eq. (4) into the first line of Eq. (1), multiply P1 on both sides, and convert every vector into complex number, we get the following recursion: x[n + 1] = λ1 x[n] ix[n]. (5) Let vn := ix[n] and Un := λ1 in the associative operation, ℜ(s(2) = x[n]. The then the filtered signal is computational cost is the same as in Eq. (1), but the implementation is more straightforward since we can treat it as one-pole filter. ) ℑ(s(2) ) (cid:105) (cid:104) 2.2. The Feed-Forward Compressor and Expander (COMP) We adopt the compressor and expander model from the DAFx textbook [16]. The effect is controlled by the following parameters: the compressor/expander thresholds CT /ET , the compressor/expander ratios CR/ER, the attack/release/RMS smoothing factors αat/αrt/αrms, and the make-up gain. We use the differentiable implementation torchcomp by Yu et al. [17]. The RMS level detector and the backpropagation of the attack/release ballistic filter are also one-pole filters; thus, we accelerate them using the parallel scan. Due to the causal smoothing effect of the RMS level detector, the gain reduction signal g[n] is slightly delayed. Modern digital compressors often have look-ahead feature to compensate for this delay. To learn the continuous delay time R+, we approximate the look-ahead by truncated sinc interpolation: g(n + l) L2+1 (cid:88) k=L1 g[n + k]sinc (k l) , (6) where [0, L2] and L1, L2 are the truncation lengths. The sinc function is defined as (cid:55) sin(πx) . The output of the compressor is then yCOMP[n] = g(n + l)x[n]. πx 2.3. The Ping-Pong Delay (DLY: R2) Ping-Pong delay is stereo delay effect where the delays pop from the left and right channels alternately. It is implemented by two delay lines whose outputs are each others inputs. In modern music production, the panning of the delay is more flexible and not limited to hard left and right. We thus add two separate panners to control the panning of two delay lines. We add an LP filter in the feedback path to simulate the decaying echoes. The LP filter is the same Biquad filter as in Section 2.1. Figure 1: The proposed model (middle) and individual effects for vocal effects processing. We adopt the damped sinusoidal approach to learn the delay time using frequency-sampling (FS) [18, 19], representing the delay effects as convolutions with truncated finite impulse response (FIR) hDLY[n]0n<NDLY . The following equations approximate the transfer functions of the delays: Hodd(z) = γ1 DLY Heven(z) = zd 1 γDLYHLP(z)z2d (cid:106) NDLY 2d (cid:88) (cid:16) (cid:107) γDLYHLP(z)η k=1 γDLYHLP(z)z2d 1 γDLYHLP(z)z2d (cid:107) 2π NDLY zd(cid:17)2k1 , (7) (8) (cid:106) NDLY 2d (cid:88) 1 (cid:16) γDLYHLP(z)η 2π NDLY zd(cid:17)2k , k=1 where η [0, 1] is the surrogate variable, γDLY [0, 1] is the decay factor, is the delay time, and HLP(z) is the transfer function 2π NDLY zd forms damped sinusoidal in the of the LP filter. η frequency domain, surrogate way to learn the true delay operator zd. We set η := 1 during inference. We explicitly compute each delayed impulse within the range 0 < NDLY to reduce the time aliasing effect when FS the original transfer function [20]. The two impulses are then joined together as hDLY[n] = PANodd(hodd[n]) + PANeven(heven[n]) (9) where PAN : R2 is the panning function. Following [19], we use the straight-through estimator to backpropagate the gradients to the damped sinusoidal. The final output of the pingpong delay is the convolution of the input signal with the IR yDLY[n] = gDLY(yCOMP[n] hDLY[n]) and gDLY [0, 1] controls the volume of the delayed signal. 2.4. The Feedback Delay Network Reverb (FDN: R2 R2) FDN is an artificial reverberation algorithm that uses network of delay lines with feedback to create dense reverberations [21]. In this work, we use stereo FDN with six delay lines. It is best described in state-space form as: where R66, R62, R26, and mi are the delay times. controls how the energies spread across delay lines. The delay times are co-primes to increase echo density. We set the delay times to = [997, 1153, 1327, 1559, 1801, 2099] proposed in [22]. Computing the recursion of Eq. (10) directly in automatic differentiation frameworks is time-consuming due to the overhead of an enormous amount of function calls to register computational nodes, and each contributes little computation [11]. Furthermore, there are no specialised kernels for Eq. (10) (in contrast to the PEQ in Section 2.1); thus, we use the FS method to approximate the IR of the FDN. The transfer function HFDN(z) = YFDN(z) is given by the following equation: X(z) HFDN(z) = (cid:0)Dm(z)1 A(z)(cid:1)1 Dm(z) = diag (cid:0)(cid:2)zm1 zm . . . B, zm6 (cid:3)(cid:1) . (11) (12) We parametrise A(z) = UΓ(z) where is an orthogonal matrix and Γ(z) = diag([γ(z)m1 . . . γ(z)m6 ]), γ(z) [0, 1]. We follow Mezza et al. [8] to parametrise to be unitary, thus satisfying the unilosslessness condition [23]. γ(z) is the attenuation filter that controls the decay rate of the reverb. In most of the previous works on differentiable FDN [8, 22, 24], frequency-independent γ is used to parametrise A, which limits the flexibility of the model. The difficulty is that we usually want the decay time to be delay-independent, and designing such filters is non-trivial. Mezza et al. [25] tackle this problem indirectly by learning separate FIRs for each delay line and training them with frequency-dependent objective. Here, we choose more straightforward approach that fits the FS method. We sample 49 points 5 of γ(z) with equal spacing from 0 to π. The attenuation coefficients are then upsampled to the desired length of FFT during FS. After fitting, we can approximate the FDN by calculating the linear-phase filter from the magnitude response of γ(z)mi and then applying Mezzas model for real-time purposes. In addition, the decaying time of the reverb in Eq. (11) is frequency-dependent, while the initial gain of the reverb is not. To correct this, we add PEQ after the reverb, which contains two peak filters and two shelf filters from Section 2.1. In practice, we apply PEQ on the impulse response of the reverb hFDN[n] before convolving it with the input signal for efficiency. 2.5. The Effect Sends (SEND) and Parametrisation x1[n + m1] x2[n + m2] ... x6[n + m6] yFDN[n] = Cx[n], = Ax[n] + Bx[n], Both the reverb (Section 2.4) and the delay (Section 2.3) only model the wet signal. To further increase the models flexibility, we also (10) 5The number was decided empirically based on early experiments by reducing the number of points until it loses spectrum details. send the delayed signal to the reverb to colourise the delays to have similar acoustic characteristics to the direct signal, controlled by the send level gSEND [0, 1]. The total number of parameters in our effect chain is 1526. Compared to GRAFx [10, 19], package that provides differentiable effects modelling, similar effects signal chain requires at least 264 more parameters 7, since in GRAFx the goal is to represent the mix faithfully, thus their delay and reverbs are over-parametrised to be expressive enough for various mixing materials. In contrast, our model specialises in vocals, and we prioritise having compact representation of parameters for analysis over expressiveness. Since many effects parameter ranges are bounded, we apply different parametrisation to the parameters, which are summarised in Table 1. 3. EXPERIMENTS 3.1. Datasets We apply our effects model to two datasets: 1) the MedleyDB [26, 27] and 2) our private multi-track dataset Internal with paired dry and wet stems [10]. The latter consists mainly of modern mainstream Western music. Both datasets are sampled at 44.1 kHz. We use the official metadata of MedleyDB to pick the vocal tracks. For Internal, the pairing information between raw tracks and processed stems is missing. We calculate the cross-correlations between each songs dry tracks and wet stems and use these correlations to recover their mapping. We then drop non-vocal stems based on their filenames. We select stems processed from only one raw track to fit our problem setting (mono-in-stereo-out). Some input tracks are stereo, possibly due to the exporting process from DAW. For these tracks, we first peak-normalise both channels and then calculate their difference (side channel). We drop the track if the maximum side energy exceeds 10 dB. We then take the average of the two channels to form mono source. Since the raw tracks and processed stems are not always aligned in time, we time-align them with the time-shift that maximises cross-correlation with the processed stems. 3.2. Optimisation The loss functions we use are 1) the multi-resolution STFT (MRS) loss, 2) the multi-resolution Loudness Dynamic Range (MLDR) loss, and 3) the regularisation loss on the surrogate variable η. The MRS loss is defined as LMRS(ˆy[n], y[n]) = 1 3 (cid:88) {128,512,2048} ˆYN YN 2 YN + log( ˆYN ) log(YN )1 MN , (13) where ˆYN and YN are the magnitude spectrograms of the predicted and ground-truth signals, respectively, computed with FFT size and hop size 4 . MN is the number of frames in the spectrogram. Similar to [10], we use auraloss8 to compute the MRS loss, with A-weighting applied before the STFT [28]. This loss minimises the distance in the spectral domain. 614 for PEQ, nine for the compressor and expander, eight for the delay, 119 for the FDN reverb, one for the panning, and one for the send level. 7This number is based on replacing our delay and reverbs with theirs. 8github.com/csteinmetz1/auraloss Inspired by previous work on differentiable microdynamics metrics [29], we propose the MLDR loss to match the dynamics of the predicted and ground-truth signals, thus better guiding the fitting of the compressor. Given signal x[n], its LDR is defined as LDR(x[n], tshort, tlong) = log RMS RMS (cid:0)x[n]2, tshort (cid:16) x[n + tlongtshort 2Ts (cid:1) , (cid:17) (14) ]2, tlong where RMS calculates the energy envelopes of the signal, tshort/tlong are the integration times in second, and Ts is the sampling period. The longer the integration time, the smoother the RMS envelope. For more calculation details, please refer to [29]. tlong tshort, so the LDR describes how the loudness varies locally in scale proportional to t1 long. Similar to MRS loss, we want to match the LDR from coarse to fine scales. Thus, we propose the following L1 loss: LMLDR(ˆy[n], y[n]) = 1 (cid:88) 1 (cid:88) t{1,2} n=0 (cid:18) LDR ˆy[n], (cid:12) (cid:12) (cid:12) (cid:12) 20 (cid:19) (cid:18) , LDR y[n], 20 , (cid:19)(cid:12) (cid:12) (cid:12) (cid:12) (15) where is the length of the signal. The final loss function is the weighted sum of the MRS and MLDR losses on the left, right, mid, and side channels, plus the regularisation loss on η: L(ˆy[n], y[n]) = 1 2 2 (cid:88) (cid:104) i=1 LMRS(ˆyi[n], yi[n]) + 1 2 LMRS( 2H(ˆy[n])i, 2H(y[n])i) + 1 2 LMLDR(ˆyi[n], yi[n]) (cid:105) + + (1 η)2, LMLDR(H(ˆy[n])i, H(y[n])i) 1 4 (cid:2)x + y(cid:3) is the Hadamard where : (cid:2)x y(cid:3) (cid:55) 1 transform that converts left/right to mid/side channels. The last term encourages the damped sinusoidal to be on the unit circle. The weights are set empirically to balance the initial magnitude of the individual losses. (16) 2 3.3. Fitting Details We normalise the input and target vocals to have 18 dB LUFS [30] using pyloudnorm9. Each track is then split into twelve-second segments with five seconds of overlap. The overlapped region is used as warm-up, and the loss is calculated only for the last seven seconds, similar to [10]. We drop silent segments and select up to 35 segments in each training step to form batch. We train the effects on each track for 2k steps using the Adam optimiser with 0.01 learning rate and pick the checkpoint with the lowest loss. We use the CUDA implementation of parallel scan by Martin et al. [31]. The fitting time of each track ranges from 20 to 40 minutes on single RTX 3090 GPU. The PK and LS/HS filters in the PEQ are initialised with zero gains. The PK filters cut-off frequencies are bounded differently, so their parameters are ordered and not permutation-invariant. The cutoff frequencies for LP and HP filters are initialised to 17.5 kHz and 200 Hz, respectively. The dynamic range controls are initialised 9github.com/csteinmetz1/pyloudnorm Table 1: The parametrisation of the effects. tri(X) is the upper triangular part of the matrix X. For details on the bounds, please refer to our code repository. Condition (P) 0 1 0 0 XX = Parametrisation (R P) Parameters (θ R) θ (cid:55) θ σ : θ (cid:55) 1 1+eθ θ (cid:55) + σ(θ)(b a) θ (cid:55) θ mod θ (cid:55) log(1 + eθ) Θ (cid:55) etri(Θ)tri(Θ) Equalisers/make-up gain, CT, ET, B, Panning, αrms, αat, αrt, ER, γDLY, gDLY, gSEND Equalisers and frequency, CR, d, γ(z) log(η) with CR = 2, ER = 1 2 , CT = 18, ET = 48, and make-up gain 0 dB. We initialise the delay with γDLY = gDLY = 0.1 and delay time to 400 ms. We initialise the FDN with = 1, = 0, γ(z) (0.4, 0.6). The send level is initialised to 0.01. The model is initialised very close to an identity function, which is the upper bound for the loss, so we can easily detect problematic runs. We set the impulse response length to four seconds for the delay and 12 seconds for the FDN reverb. We bound the damping factor γ(z) to have maximum of nine seconds T60 time to reduce the aliasing effect but still be long enough to capture most of the reverb tail. few tracks have non-linear effects such as distortion and modulations that are not modelled in our effects. To exclude them, we drop fitting runs that 1) have minimum loss above certain threshold, 2) have loss that fluctuates heavily during fitting, or 3) have loss that is not decreasing. The thresholds are set empirically after checking runs with apparent outlier fitting losses. Out of 76 tracks from MedleyDB, 6 tracks are excluded ( 8%); out of 370 tracks from Internal, 5 tracks are excluded ( 1.3%). 4. RESULTS AND DISCUSSIONS 4.1. Sound Matching Performance We fit different configurations to test the benefits of having spatial effects. We denote the complete model as DiffVox. The evaluation metrics are the fitting losses on the left/right and mid/side channels. The averaged scores across tracks are shown in Table 2. Table 2: Matching performance with different configurations. Dataset Configuration MRS l/r m/s MLDR m/s l/r Internal MedleyDB No processing DiffVox w/o Approx. No processing 1.44 0.76 0. 1.27 0.75 DiffVox w/o Approx. 0.77 0.79 w/o FDN w/o DLY 0.76 w/o DLY&FDN 0.61 2.39 0.94 0.95 2.16 0.98 1.00 1.14 0.99 0. 1.82 0.34 0.38 1.00 0.39 0.42 0.48 0.40 0.82 2.08 0.41 0. 1.35 0.45 0.48 0.62 0.47 1.17 From Table 2, we can see that with just PEQ and compressor, the rendered audio matches well regarding spectral content indicated by the MRS on MedleyDB. However, it fails to match the microdynamics indicated by the MLDR loss. Adding the delay or the FDN improves the matching of the microdynamics. DiffVox achieves the best matching performance in MLDR and lower MRS than with just Delay or FDN, proving its ability to match the sound in spectral and dynamic aspects. After removing the approximation, the performance drops slightly, which is expected since we swap truncated FIRs of reverb and delay with infinite IRs during inference, so the longer tails in the IRs introduce mismatch. Nevertheless, it is still better than without FDN. 4.2. Parameter Correlation Analysis We analyse the correlation between the parameters on both datasets. Specifically, on the minimum set of parameters to reproduce the effects θ R130, which excludes the surrogate variable η and the lower triangular part of the logits of U. Since the parameters are unlikely to be normally distributed, we compute their Spearman correlation coefficient (SCC) instead of regular correlations10. We exclude the correlations of multi-dimensional parameters γ(z), B, C, since interpreting them is not straightforward. We notice high correlations between the sampled γ(z), implying that the attenuation coefficients can be decomposed further into fewer parameters. The following discussions are based on the top ten most correlated pairs of parameters. MedleyDBs most correlated pairs are not aligned with Internal, showing that the two datasets have different characteristics. Since MedleyDB has less data compared to Internal (70 vs. 365 tracks), the correlation we see is also less reliable. Thus, we primarily focus on the Internal dataset. High correlations between the delay effect parameters are observed. The negative correlations between the delay time and the feedback gain (-.58) and the delay gain (-.51) imply that when longer delay time is used, the delay effects diminish. The positive correlation of the feedback gain and the cut-off frequency of the delays LP filter (.49) tells us that when darker delay is used, it also fades out faster, and vice versa. In addition, high correlations are observed between the gain and the factor of the PEQs PK2 filter (-.60) and the gain and cut-off frequency of the FDNs PK2 filter (-.46). The counter-interaction between the compressor threshold and the make-up gain is also observed (-.55), which is expected as the two parameters are usually adjusted together to ensure consistency in the loudness before and after the compressor. We see that the attenuation coefficients above 19.7 kHz are highly correlated (ranging from .53 to .60) with the LP filter cut-off frequency. The reverb tends to compensate for the high-frequency 10For convenience, the analysis is performed on the parameter logits before the parametrisation. This does not affect SCC for most of the scalar parameters (excludes and l) since their parametrisation in Table 1 is monotonic. loss by reducing the decay rate because we set the maximum cut-off frequency to 18 kHz according to the T-RackS EQ. possible solution is to set higher bound for the cut-off frequency or incorporate wet/dry mix ratio on LP similar to GRAFx [19]. To analyse the correlation from broader perspective, we measure the correlations between individual effects in the effects model on Internal. We define the effect-wise correlation as the average of the absolute SCCs between the parameters of the two effects. For correlations within the same effect, the diagonal elements are excluded. We see that most of the effects have high correlations to themselves, except for the PEQs LS filter. The LS filter has high correlations with the HP filter (.198) but very low self-correlation (.011). The LS filter only has two parameters, the gain and frequency, and low correlation means that the two parameters operate nearly independently. This implies the two filters are used collaboratively to shape the low end. Based on the correlations, the hierarchical clustering using Wards method [32] shows three main clusters: all the spatial effects, the HP and LS filters, and other filters with the compressor and expander. These clusters could be used to inform simpler design of vocal effects. 4.3. Principal Component Analysis Inspired by similar dataset work [33], we perform principal component analysis (PCA) [34] on the parameters logits to analyse the distribution of the parameters. Although PCA is limited to linear relationships, its simplicity and interpretability make it good starting point for understanding the effects prior. Given the sample covariance matrix Σθ R130130 of the logits θ, we compute the eigenvalues λθ Rr and the eigenvectors Vθ R130r such that Σθ = Vθdiag((cid:2)λθ λθ2 (cid:3))V θ , λθ1 λθ2 . . . λθr > 0. . . . λθr (17) Vθ is an orthogonal matrix, and the eigenvectors are the principal components (PCs) of the parameters. The eigenvalues represent the variance of the parameters in the direction of the PCs. is the rank of Σθ and equals the number of non-zero eigenvalues. The cumulative percentage of total variance (CPV, [34]) is simple way to examine the capacity of the PCA model. The CPV given number of components to retain is defined as 100 (cid:80)p i=1 λθi / (cid:80)r i=1 λθi . The CPV of both models is shown in Fig. 2. Internal has more variance explained by the first components than MedleyDB, indicated by the larger area under the curve (AUC In other words, the parameters in In- = 91.0 % vs 88.5 %). ternal are more densely distributed than MedleyDB. To see how MedleyDB is explained by the Internals PCA model, we also compute another CPV by replacing the eigenvalues with the sum of squared projections of MedleyDBs parameters onto each Internals PC11 and plot it in Fig. 2. It shows that 65 % of the variances in MedleyDB can be captured by Internals first ten PCs, and the CPV trends are similar, but for higher PCs, the discrepancy increases. One application of having PCA model is using it as generative model, drawing samples from the distribution (0, diag(λθ)). To test this assumption, we perform two multivariate normality tests, Roystons test [35] and Henze-Zirklers test [36], on the PC weights of Internal. The p-values of the tests are nearly zero given the first 75 PCs ( 99 % of the total variance), indicating that the logits 11The PCA projection is defined as (cid:55) θ (x µθ) where µθ is the sample mean of the parameters. Figure 2: Cumulative total variance as function of the percentage of retained PCs (100p/r) from both PCA models. are unlikely to be drawn from multivariate normal distribution. We plot the frequency responses of the PEQ, the delay, the tone correction PEQ, and the reverb decay time in Fig. 3. The mean parameters (first column) show reasonable configuration that we could expect for professionally processed vocal. Both the high and low ends are boosted, and the mid-range close to 1 kHz is slightly attenuated with narrow Q. An adequate amount of delay and reverb are added, with the high frequencies in the reverb attenuated. The reverbs decay time is around 2 seconds, starting at low frequencies and gradually decreasing to zero. The compression ratio is set to 3.5:1, with threshold of 22 dB and make-up gain of 2.4 dB. The vocal sounds polished and professionally processed. To see how the PCs affect the parameters, we add the ith PC to µθ with scalings of {3, 1, 1, 3} λθi . The first PC (second column) mainly affects the spaciousness of the acoustic property where the vocals are placed. The feedback gain and overall volume of the delay are increased. The decay time of the reverb is also significantly increased, especially in frequencies above 4 kHz, which creates very shimmery and spacious sound. The second PC (third column) creates band-pass effect similar to telephone, which can be seen from how drastically the HS and LP filters are changed. This aligns with McAdams timbre space [37], where their second dimension is related to the spectral centroid. Long reverberations could also be viewed as way to smooth vocals attack time, which is related to McAdams first dimension. The dynamic range compression gets slightly heavier in the fourth PC but does not change much in the first three. 5. CONCLUSIONS This paper presents an expressive differentiable model for vocal effects processing and methodology for capturing the effects parameters from professional mixes. Spatial effects are essential to achieve good matching performance, as indicated by lower microdynamics losses. The parameter correlation analysis reveals meaningful relationships between effect parameters, such as the interaction between delay time and feedback gain and between compressor threshold and make-up gain. The first two principal components of the PCA model on our private dataset reveal primary directions that alter the vocals, including control of spaciousness and frequency bandwidth manipulation (telephone-like effects), which have some connections to McAdams timbre space. While our approach successfully captures effect parameters for Figure 3: The mean (column one) and the first four principal components (column two to five with their percentage of explained variance) of the Internal dataset. The first and third rows show the frequency response of the PEQ and the tone correction filter. The second row shows the frequency response of the delayed signals, with colour intensity proportional to the delay time. The fourth row shows the frequency-dependent decay of the FDN reverb. most tracks, limitations remain in handling vocals with other effects we did not model or heavy automation. Addressing these limits and extending the model to multi-track scenarios are left for future work. The non-normality of the parameter distribution suggests that more sophisticated generative model is needed to capture the actual distribution. We release the dataset of 435 vocal presets parameter logits produced in this study, plus test set of 20 additional presets from the Internal dataset. The dataset is accompanied by the DiffVox model implemented in PyTorch, scripts to reproduce the results on the MedleyDB dataset, and the PCA models. We hope these resources can advance the development of future automatic mixing tools or neural audio effect models. 6. ACKNOWLEDGMENTS We thank Sungho Lee for providing the cross-correlation data of the Internal dataset. This research is supported jointly by UKRI (grant number EP/S022694/1) and QMUL and utilised Queen Marys Apocrita HPC facility, supported by QMUL ResearchIT. doi: 10.5281/zenodo.438045 7. REFERENCES [1] Matthew Rice, Christian Steinmetz, George Fazekas, and Joshua Reiss, General purpose audio effect removal, in IEEE WASPAA, 2023, pp. 15. [2] Christian J. Steinmetz, Nicholas J. Bryan, and Joshua D. Reiss, Style transfer of audio effects with differentiable signal processing, JAES, vol. 70, no. 9, pp. 708721, 2022. [3] Junghyun Koo, Marco A. Martínez-Ramírez, Wei-Hsiang Liao, Stefan Uhlich, Kyogu Lee, and Yuki Mitsufuji, Music mixing style transfer: contrastive learning approach to disentangle audio effects, in IEEE ICASSP, 2023, pp. 15. [4] Christian J. Steinmetz, Shubhr Singh, Marco Comunità, Ilias Ibnyahya, Shanxin Yuan, Emmanouil Benetos, and Joshua D. Reiss, ST-ITO: Controlling audio effects for style transfer with inference-time optimization, in ISMIR, Nov. 2024, pp. 661668. [22] Gloria Dal Santo, Karolina Prawda, Sebastian Schlecht, and Vesa Välimäki, Differentiable feedback delay network for colorless reverberation, in DAFx, 2023, pp. 244251. [5] Joseph Colonel, Christian Steinmetz, Marcus Michelen, and Joshua Reiss, Direct design of biquad filter cascades with deep learning by sampling random polynomials, in IEEE ICASSP, 2022, pp. 31043108. [6] Shahan Nercessian, Neural parametric equalizer matching using differentiable biquads, in DAFx, 2020, pp. 265272. [7] Purbaditya Bhattacharya, Patrick Nowak, and Udo Zölzer, Optimization of cascaded parametric peak and shelving filters with backpropagation algorithm, in DAFx, 2020, pp. 101 108. [8] Alessandro Ilic Mezza, Riccardo Giampiccolo, Enzo De Sena, and Alberto Bernardini, Data-driven room acoustic modeling via differentiable feedback delay networks with learnable delay lines, EURASIP JASM, vol. 2024, no. 1, pp. 51, 2024. [9] Alec Wright and Vesa Valimaki, Grey-box modelling of dynamic range compression, in DAFx, 2022, pp. 304311. [10] Sungho Lee, Marco Martínez-Ramírez, Wei-Hsiang Liao, Stefan Uhlich, Giorgio Fabbro, Kyogu Lee, and Yuki Mitsufuji, Searching for music mixing graphs: pruning approach, in DAFx, 2024, pp. 147154. [11] Chin-Yun Yu and György Fazekas, GOLF: Singing Voice Synthesiser with Glottal Flow Wavetables and LPC Filters, TISMIR, Dec 2024. [12] Guy E. Blelloch, Prefix sums and their applications, Tech. Rep. CMU-CS-90-190, School of Computer Science, Carnegie Mellon University, Nov. 1990. [13] Mark Harris, Shubhabrata Sengupta, and John Owens, Parallel prefix sum (scan) with CUDA, GPU gems, vol. 3, no. 39, pp. 851876, 2007. [14] Julius O. Smith, Introduction to Digital Filters with Audio Applications, W3K Publishing, 2007. [15] Jean Laroche, On the stability of time-varying recursive filters, JAES, vol. 55, pp. 460471, 2007. [16] Udo Zölzer, DAFX: Digital Audio Effects, chapter Nonlinear Processing, pp. 110112, John Wiley & Sons, 2011. [17] Chin-Yun Yu, Christopher Mitcheltree, Alistair Carson, Stefan Bilbao, Joshua D. Reiss, and György Fazekas, Differentiable all-pole filters for time-varying audio systems, in DAFx, 2024, pp. 345352. [18] Ben Hayes, Charalampos Saitis, and György Fazekas, Sinusoidal frequency estimation by gradient descent, in IEEE ICASSP, 2023, pp. 15. [19] Sungho Lee, Marco Martínez-Ramírez, Wei-Hsiang Liao, Stefan Uhlich, Giorgio Fabbro, Kyogu Lee, and Yuki Mitsufuji, GRAFX: An Open-Source Library for Audio Processing Graphs in PyTorch, in DAFx Demo Papers, 2024, pp. 475478. [20] Sungho Lee, Hyeong-Seok Choi, and Kyogu Lee, Differentiable artificial reverberation, IEEE/ACM TASLP, vol. 30, pp. 25412556, 2022. [21] John Stautner and Miller Puckette, Designing multi-channel reverberators, Computer Music Journal, vol. 6, no. 1, pp. 5265, 1982. [23] Sebastian Schlecht and Emanuël AP Habets, On lossless feedback delay networks, IEEE Transactions on Signal Processing, vol. 65, no. 6, pp. 15541564, 2016. [24] Riccardo Giampiccolo, Alessandro Ilic Mezza, Alberto Bernardini, et al., Differentiable MIMO feedback delay networks for multichannel room impulse response modeling, in DAFx, 2024, pp. 278285. [25] Alessandro Ilic Mezza, Riccardo Giampiccolo, Alberto Bernardini, et al., Modeling the frequency-dependent sound energy decay of acoustic environments with differentiable feedback delay networks, in DAFx, 2024, pp. 238245. [26] Rachel Bittner, Justin Salamon, Mike Tierney, Matthias Mauch, Chris Cannam, and Juan Pablo Bello, MedleyDB: multitrack dataset for annotation-intensive mir research., in ISMIR, 2014, pp. 155160. [27] Rachel Bittner, Julia Wilkins, Hanna Yip, and Juan Bello, MedleyDB 2.0: New data and system for sustainable data collection, ISMIR Late Breaking and Demo Papers, 2016. [28] Alec Wright and Vesa Välimäki, Perceptual loss function for neural modeling of audio systems, in IEEE ICASSP, 2020, pp. 251255. [29] Shahan Nercessian, Russell McClellan, and Alexey Lukin, direct microdynamics adjusting processor with matching paradigm and differentiable implementation, in DAFx, 2022, pp. 248255. [30] R. ITU-R, ITU-R BS. 1770-4: Algorithms to measure audio programme loudness and true-peak audio level, International Telecommunication Union, 2015. [31] Eric Martin and Chris Cundy, Parallelizing linear recurrent neural nets over sequence length, in ICLR, Vancouver, Canada, 2018. [32] Joe Ward Jr, Hierarchical grouping to optimize an objective function, Journal of the American statistical association, vol. 58, no. 301, pp. 236244, 1963. [33] Corentin Guezenoc and Renaud Seguier, Wide Dataset of Ear Shapes and Pinna-Related Transfer Functions Generated by Random Ear Drawings, The Journal of the Acoustical Society of America, vol. 147, no. 6, pp. 40874096, 2020. [34] I. T. Jolliffe, Principal Component Analysis, Springer, 2002. [35] J. P. Royston, Some techniques for assessing multivarate normality based on the Shapiro-Wilk W, J. R. Stat. Soc. Ser. Appl. Stat., vol. 32, no. 2, pp. 121133, 6 1983. [36] N. Henze and B. Zirkler and, class of invariant consistent tests for multivariate normality, Commun. Stat. Theory Methods, vol. 19, no. 10, pp. 35953617, 1990. [37] Stephen McAdams, Suzanne Winsberg, Sophie Donnadieu, Geert De Soete, and Jochen Krimphoff, Perceptual scaling of synthesized musical timbres: Common dimensions, specificities, and latent subject classes, Psychological research, vol. 58, pp. 177192, 1995. A. SUPPLEMENTARY RESULTS Fig. 4 shows the histograms of the panning on the direct signal, gSEND, γDLY, gDLY, and (delay time). For panning, 0 is the centre, -100 is hard left, and 100 is hard right. Fig. 5 shows the correlation matrix of the individual effects in Internals effects model and the hierarchical clustering of the effects (Section 4.2). The hierarchical clustering is based on the correlation matrix with diagonal elements set to one. The colour of the cells indicates the strength of the correlation. Table 3 shows the top ten most correlated parameters of Internal (Section 4.2). fLP is the cut-off frequency of the low-pass filter, gPK2 is the gain of the second peak filter, DLY.LP means the low-pass filter in the delay effect, etc. Fig. 6 shows 2D projection on the first two PCs (divided by λθi ) of Internals PCA model. Fig. 7 shows the dynamic range compression curves of the compressor and expander. Fig. 8, 9, and 10 show the spectrograms of the configurations stated in Section 4.1 on three songs from MedleyDB and their differences to the target sound. Figure 5: Left: The correlation matrix of Internals effects. Right: The hierarchical clustering. Table 3: The top ten most correlated parameters of Internal and their correponding SCC from MedleyDB. Parameter Parameter 2 SCC Internal MedleyDB fLP gPK2 fLP CT fLP ET γDLY gFDN.PK2 48 π) γ(ei 44 QPK2 γDLY γ(ei 43 48 π) make-up γ(ei 45 48 π) ER gDLY fDLY.LP fFDN.PK2 0.60 -0.60 -0.58 0.56 -0.55 0.53 -0.52 -0.51 0.49 -0.46 0.32 -0.10 -0.20 0.35 0.06 0.19 -0.30 -0.02 0.41 -0.47 Figure 4: The histograms of the panning, delay send, and delay parameters. Figure 6: The scatter plot of the first two PCs weights using the PCA model of Internal. Figure 7: The compression curves (black) of the mean and the first four PCs of Internals PCA model. Figure 8: Matching the song Torres_NewSkin from MedleyDB using different effect configurations. Figure 9: Matching the song MusicDelta_Country1 from MedleyDB using different effect configurations. Figure 10: Matching the song StrandOfOaks_Spacestation from MedleyDB using different effect configurations."
        }
    ],
    "affiliations": [
        "Centre for Digital Music, Queen Mary University of London, London, UK",
        "Sony AI, Tokyo, Japan",
        "Sony Group Corporation, Tokyo, Japan"
    ]
}