{
    "paper_title": "Vision Transformers Don't Need Trained Registers",
    "authors": [
        "Nick Jiang",
        "Amil Dravid",
        "Alexei Efros",
        "Yossi Gandelsman"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We investigate the mechanism underlying a previously identified phenomenon in Vision Transformers -- the emergence of high-norm tokens that lead to noisy attention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a sparse set of neurons is responsible for concentrating high-norm activations on outlier tokens, leading to irregular attention patterns and degrading downstream visual processing. While the existing solution for removing these outliers involves retraining models from scratch with additional learned register tokens, we use our findings to create a training-free approach to mitigate these artifacts. By shifting the high-norm activations from our discovered register neurons into an additional untrained token, we can mimic the effect of register tokens on a model already trained without registers. We demonstrate that our method produces cleaner attention and feature maps, enhances performance over base models across multiple downstream visual tasks, and achieves results comparable to models explicitly trained with register tokens. We then extend test-time registers to off-the-shelf vision-language models to improve their interpretability. Our results suggest that test-time registers effectively take on the role of register tokens at test-time, offering a training-free solution for any pre-trained model released without them."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 0 1 0 8 0 . 6 0 5 2 : r Vision Transformers Dont Need Trained Registers Nick Jiang Amil Dravid Alexei A. Efros Yossi Gandelsman UC Berkeley Equal contribution {nickj,amil_dravid,aaefros,yossi_gandelsman}@berkeley.edu"
        },
        {
            "title": "Abstract",
            "content": "We investigate the mechanism underlying previously identified phenomenon in Vision Transformers the emergence of high-norm tokens that lead to noisy attention maps (Darcet et al., 2024). We observe that in multiple models (e.g., CLIP, DINOv2), sparse set of neurons is responsible for concentrating high-norm activations on outlier tokens, leading to irregular attention patterns and degrading downstream visual processing. While the existing solution for removing these outliers involves retraining models from scratch with additional learned register tokens, we use our findings to create training-free approach to mitigate these artifacts. By shifting the high-norm activations from our discovered register neurons into an additional untrained token, we can mimic the effect of register tokens on model already trained without registers. We demonstrate that our method produces cleaner attention and feature maps, enhances performance over base models across multiple downstream visual tasks, and achieves results comparable to models explicitly trained with register tokens. We then extend test-time registers to off-the-shelf vision-language models to improve their interpretability. Our results suggest that test-time registers effectively take on the role of register tokens at test-time, offering training-free solution for any pre-trained model released without them."
        },
        {
            "title": "Introduction",
            "content": "Vision Transformers (ViTs) (Dosovitskiy et al., 2021) have become dominant architecture in computer vision, offering strong performance across wide range of tasks (Oquab et al., 2024; Radford et al., 2021). Recently, Darcet et al. (2024) observed surprising property in these models: the emergence of high-norm intermediate tokens at seemingly random locations in the image during the internal computation of the ViT (see \"Original\" column in Figure 1). These outlier tokens were shown to appear in low-information image areas (e.g., uniform background patches) and were demonstrated to capture global image information. Darcet et al. (2024) interpreted these high-norm tokens as form of emergent global memory mechanism through which the model stores and retrieves global information, analogous to registers in CPUs. Based on this interpretation, they proposed to eliminate these outlier image tokens by augmenting the input with dedicated non-image tokens during training, calling them register tokens. This allows the patch tokens to focus solely on encoding local content, leading to improved performance on dense visual tasks. However, this technique requires re-training existing models from scratch with these extra register tokens, limiting its applicability in practice. In this work, we argue that while registers are indeed useful, the models dont need to be retrained with them. Instead, we show that registers can be added post hoc, without any additional training. 1Project Page: https://avdravid.github.io/test-time-registers Code: https://github.com/nickjiang2378/test-time-registers Preprint. Under review. Figure 1: Controlling high-norm tokens in Vision Transformers. As shown in Darcet et al. (2024), high-norm outlier tokens emerge in ViTs and lead to noisy attention maps (Original). By identifying the mechanism responsible for their emergence, we demonstrate that we can shift these outlier tokens to arbitrary positions at test time (Shifted). Shifting the outlier tokens outside of the image area mimics register behavior at test-time (w/ Test-time Register), resulting in more interpretable attention patterns and downstream performance comparable to models retrained with registers. To do this, we first investigate the mechanism underlying the emergence of high-norm tokens. We identify sparse set of neurons register neurons that create the outlier tokens by contributing high-norm values to them. By directly editing the activation maps of the register neurons during the forward pass of the network, we can induce the formation of the high-norm activations at arbitrary token positions (see Figure 1). We use it to create new register tokens at test-time, even in models that were never trained with them, by appending new tokens and activating the register neurons in their positions. We show that models with test-time registers provide comparable performance to models with trained registers on various downstream tasks (e.g., classification, segmentation, and depth prediction) and largely improve over models without registers for unsupervised object discovery (20-point correct localization improvement) and attention-based zero-shot segmentation (+5 mIOU). Next, we demonstrate that test-time registers reduce high-norm artifacts in vision-language models, improving the alignment between textual outputs and relevant visual regions. Finally, we leverage the emergent property of outlier tokens to mask local information, shifting their positions to appear on adversarial regions and improving robustness to typographic attacks by 43%. In summary, our contributions are as follows: We determine the mechanism behind the creation of high-norm tokens in ViTs by finding register neurons that, when activated, cause the appearance of these outliers (Section 3.1). We demonstrate that activating the register neurons at test-time in other image locations shifts the high-norms to the corresponding tokens (Section 3.2). We present training-free method for adding registers to models that were trained without them, by appending additional tokens and activating register neurons in their positions (Section 4). We evaluate the performance of models with test-time registers and show that it is comparable to models with trained registers, thus eliminating the need for retraining models with registers from scratch (Section 5)."
        },
        {
            "title": "2 Related work",
            "content": "Feature visualization in vision models. Visualizing features of computer vision models has been used for diagnostics long before the transition of the field to deep-learning (e.g., Vondrick et al. (2013)). Features in early CNN-based models were visualized to interpret their emergent computation (Zeiler 2 & Fergus, 2014; Bau et al., 2017) and to approximate saliency maps (Itti et al., 2002). In ViTs, the attention map from the CLS token has been used for various attribution methods (Caron et al., 2021; Chefer et al., 2021), and has also been steered to improve model performance (Shi et al., 2023). Darcet et al. (2024) showed that the newer ViT-based models (Oquab et al., 2024; Radford et al., 2021) exhibit artifacts in their attentions, affecting their applicability for visualization and downstream tasks. These artifacts were connected to high-norm tokens that emerge in the transformers. High-norm tokens in transformers. Transformers tend to create high-norm tokens during their internal computation, when trained on language tasks (Xiao et al., 2024) or on vision tasks (Darcet et al., 2024). For language models, Xiao et al. (2024) showed that transformers allocate excessive attention to these high-norm tokens, and named them attention sinks. Sun et al. (2024) demonstrated that attention sinks emerge due to previous massive neuron activations. Yona et al. (2025) linked the emergence of attention sinks to the inability of language models to repeatedly generate single token, and suggested test-time fix by zeroing out the relevant activated neurons. For vision models, Darcet et al. (2024) demonstrated the emergence of high-norm tokens in low-informative areas, and suggested retraining the model with extra tokens (registers) for removing these high-norm artifacts from the image tokens. Wang et al. (2024) used simpler fine-tuning approach in DINOv2 to avoid complete re-training. We aim to shift outliers at test-time without any additional training by editing the activations of the neurons that are responsible for their emergence. Explaining neural functionality in vision models. The role of individual neurons (post non-linearity single channel activations) has been broadly studied in vision models, demonstrating that some neurons recognize low-level image features as well as high-level perceptual and semantic properties of the inputs (Schwettmann et al., 2023; Bau et al., 2017; Hernandez et al., 2021; Gandelsman et al., 2025; Dravid et al., 2023). Nevertheless, most of the research has focused on linking neural behavior to features of the input or output, neglecting other possible neural functionality that may not be related to any specific image feature. Robinson et al. (2024) discovered sparse neural activations that indicate the absence of features rather than serving as feature detectors. Sun et al. (2024) found massive activations in ViTs that operate as constant biases for attention layers. Differently, we discover and edit neuron activations responsible for creating high-norm tokens in ViTs to mimic registers."
        },
        {
            "title": "Interpreting the outlier tokens mechanism in ViTs",
            "content": "As shown by Darcet et al. (2024), high-norm outlier patches emerge during inference in various pre-trained ViTs. These patches strongly draw attention from the CLS token, resulting in artifacts in the attention maps (Figure 1). Outlier patches appear primarily in areas that exhibit high similarity to their neighboring areas (e.g., uniform background patches). Moreover, they were shown to capture global image information and lose their local patch contents (pixel + position). In this section, we study how the outlier tokens emerge in ViTs during the forward pass and discover sparse set of neurons whose activations dictate the location of outliers. We then show that we can ablate these neurons to move the outlier positions. Our main analysis is applied to OpenCLIP ViT-B/16 (Cherti et al., 2023), with similar findings on DINOv2-L/14 (Oquab et al., 2024) shown in Appendix A.6. 3.1 How do outliers emerge in ViTs? Outlier patches appear after MLPs. To identify the transformer component most responsible for outlier patches, we track the maximum norm of image patches after attention blocks and MLPs across 1000 ImageNet images (Deng et al., 2009). Figure 2 shows that outliers appear after the MLP of layer 6 in OpenCLIP. We also measure the maximum weight the CLS token attends to any patch across all heads in each layer and observe that high attention occurs after the layer 6 MLP. This observation suggests that the MLP increases the norms of certain patches, consequently creating attention sinks. small subset of neurons shows consistently high activations before outlier patches. Given that outliers appear at layer 6, we evaluate the norm contribution of individual neurons. In Figure 3, we compute the average activation of all MLP neurons in layer 6 across 1000 images, comparing the top outlier patch with randomly selected non-outlier patch. The distribution is heavily skewed for outlier patches and more symmetric for randomly selected non-outlier patches. We find that <10 neurons consistently have large activations and appear to drive outlier formation. 3 Figure 2: Outlier patches appear after MLPs; attention sinks appear after outlier patches. Left: Max norms across image patches (OpenCLIP ViT-B/16). Right: max attention scores of the CLS token in the last layer. In both plots, we average across 1000 images. The outlier norms and attention sinks occur in consecutive layers. Figure 3: Neuron activation distributions differ between outlier and non-outlier patches. We average the neuron activations at layer 6 for the top outlier patch and randomly selected non-outlier patch. Non-outlier patches have more symmetric distribution (left), whereas outlier patches show skewed distribution with most activations near zero. small subset of neurons (<10) consistently exhibit high activations for outlier patches across images (right). Highly activated neurons activate on all outlier locations. We present the full activation maps of three neurons with high activations in Figure 4. These neurons sparsely activate on all outlier positions, not just the maximum outlier position. This suggests that they are not position-specific but, rather, are responsible for outliers generally. Given these observations, we develop method to automatically detect these neurons in the next section. 3.2 Register neurons Based on our previous analysis, we hypothesize that small, consistent set of sparsely activating neurons control where outliers emerge. These neurons appear in the preceding layers before outliers form. Given their importance for the formation of outliers, we call them register neurons. Detecting register neurons. Based on our hypothesis, we formulate simple algorithm to find register neurons in Algorithm 1. Our method finds neurons whose average activation at outlier positions is consistently high across images. To detect outlier positions (FINDOUTLIERS), we follow Darcet et al. (2024) and output the positions for which the norms of the corresponding image tokens are above predefined threshold. Our algorithm searches for neurons in layers before outliers start, by setting the top_layer parameter. We output top_k neurons with the highest average activations. For OpenCLIP, we set top_layer = 5, the outlier threshold at 75, and top_k = 10. Since register neurons manage where outliers emerge, we can also intervene upon them to move outliers to arbitrary positions. Moving outliers with register neurons. To demonstrate the importance of these register neurons, we can use them to move outliers to different image locations. We apply FINDREGISTERNEURONS to detect the register neurons and modify their activation pattern during the forward pass. More specifically, for each register neuron, we copy the highest neuron activation across the tokens into the locations of the tokens to which we want to move the outliers. We zero out the activations of the neuron elsewhere. 4 Figure 4: Highly activated neurons on the top outlier activate on all outlier positions. We present activation maps of three neurons from layer 6 that activate highly on the top outlier patch (Figure 3). These maps near-perfectly align with the high-norm outliers (\"patch norms\"). Algorithm 1 FINDREGISTERNEURONS Input: image set = {I1, . . . , IM }, maximum layer index top_layer, number of neurons to return top_k, number of neurons per layer Output: set of the top-k register neurons 1: avg_act zeros(top_layer, ) 2: for all Ii do 3: 4: 5: initialize zeroed top_layer matrix loop over images indices of top-norm patches FINDOUTLIERS(Ii) for all [1, ..., top_layer] do for all [1, ..., ] do 6: avg_act[ℓ, n] avg_act[ℓ, n] + end for end for 7: 8: 9: end for 10: return top_k neurons with largest avg_act 1 OM (cid:88) pO activationℓ,n(Ii, p) Register neurons causally set the position of outliers. To test our ability to move outliers to arbitrary spots, we assign the highest activation to random patch and measure its last-layer output norm, the maximum norm of other patches, and the highest last-layer CLS attention, both to the selected outlier and any other patch. Successful interventions yield high norms and attention for the targeted patch, with low values elsewhere. As shown in Figure 5, modifying the activations of register neurons effectively controls where outliers emerge. In contrast, intervening on random neurons has little effect (Appendix A.5). Figure 1 demonstrates that intervening on register neurons can make outliers appear in various counts and spatial patterns (e.g. heart). We use this technique to prevent typographic attacks (Section 5.5) and for mimicking registers, as follows in the next section."
        },
        {
            "title": "4 Adding registers at test-time",
            "content": "Given that register neurons can be used to move outliers arbitrarily, we investigate moving outliers outside the image entirely into extra tokens we call test-time registers. Moving outliers to added tokens. As outlier patches lose their local patch information (Darcet et al., 2024), it is undesirable to have them within the image since it may affect downstream performance. Previous work has suggested retraining ViTs with extra tokens to remove high-norm artifacts and attention sinks. However, retraining existing ViTs is expensive, so we propose to add an extra input token and move the outliers there with register neurons. Test-time register initialization. We initialize our extra token to be vector of zeros. We assess several initialization strategies, but we find that they do not significantly affect the ability of test-time registers to store the high norms (Appendix A.2). Additionally, we focus our investigation on using one test-time register and report the impact of using more registers in Appendix A.1. Outliers move outside the image after adding test-time registers. To evaluate whether test-time registers can absorb the outliers, we apply our intervention and measure the max norms of image patch outputs and the test-time register. Our intervention results are nearly identical to the distribution 5 Figure 5: Intervening on activations of register neurons effectively shifts outliers to random patches and test-time registers. For all register neurons, we copy their highest activation into selected patch and zero out the activations elsewhere. Left: norm of chosen random patch (yellow) and max norm of any other patch (blue). Right: CLS attention to chosen random patch (yellow) and max CLS attention (blue) to any other patch. Our intervention can shift outliers to randomly selected patches as well as test-time registers (see Appendix A.4). IN1k CF10 CF100 CLS token central token outlier token trained register test-time register 85.6 99.4 73.3 98.0 84.5 99.2 83.1 99.2 84.5 99.1 93.4 88.1 92.8 93.0 93.0 Table 1: Linear probing classification results (DINOv2 ViT-L/14). Test-time registers achieve higher performance on linear probing than non-outlier tokens, suggesting that they hold global information similarly to trained registers. They match the performance of outlier tokens, indicating that they have absorbed the role of outliers. Figure 6: Qualitative results on attention maps w/ test-time registers. We produce the mean CLS attention maps of the last layer in DINOv2 and compare them to the model with trained registers. Test-time registers produce similarly high-quality maps as trained registers. of patch norms and CLS attention after shifting outliers to random patches, previously shown in Figure 5 (see results for test-time registers in Appendix A.4). The image patches no longer have outliers, whereas the test-time register absorbs the outlier norms. This change is also present in the attention maps (Figure 6), which no longer have noisy artifacts and match the quality of attention maps from models with trained registers.2 See Appendix A.3 for attention maps in all model layers. Test-time registers hold global information. We verify that test-time registers encapsulate global image information (e.g., image class) similarly to learned registers (Darcet et al., 2024). To assess this, we perform linear probing on both learned and patched registers for classification on ImageNet (Deng et al., 2009), CIFAR-10, and CIFAR-100 (Krizhevsky et al., 2009). We also compare their performance to the CLS token and token that corresponds to central patch in the image. As shown in Table 1, the classification accuracy of test-time registers closely matches that of trained registers and is slightly lower than the CLS token performance. These results suggest that test-time registers, like their learned counterparts, effectively capture global image-level information."
        },
        {
            "title": "5 Experiments",
            "content": "We evaluate how adding test-time registers affects the downstream performance of models trained without registers. We begin by detailing the evaluated models. Next, we compare performance across classification, dense prediction, unsupervised object discovery, and zero-shot segmentation tasks, finding that test-time registers perform comparably to their retrained counterparts. We then apply test-time registers to an off-the-shelf VLM to improve its interpretability while maintaining 2As there is no open-source OpenCLIP trained with registers, we only present comparisons on DINOv2. 6 IN Top-1 mIoU ADE20k NYUd rmse DINOv2 DINOv2 + trained registers DINOv2 + test-time registers OpenCLIP OpenCLIP + test-time registers 86.3 86.7 86.3 77.4 77.5 48.2 49.1 49.1 40.2 40. 0.387 0.382 0.378 0.603 0.595 Table 2: Linear probing results. The performance of models with test-time registers maintains or improves performance over the unedited models, and largely matches models with trained registers."
        },
        {
            "title": "OpenCLIP",
            "content": "ViT-L/14 + test-time registers ViT-B/16 + test-time registers Acc. 76.4 76.4 71.3 71.3 Table 3: OpenCLIP zeroshot ImageNet classification. Adding test-time registers maintains performance. performance. Finally, we demonstrate register neurons ability to mask local information, moving their activations to adversarial image patches to prevent typographic attacks. Models. We evaluate using DINOv2 (Oquab et al., 2024) and OpenCLIP (Cherti et al., 2023). For DINOv2, we use the publicly released ViT-L/14 checkpoints trained on LVD-142M, including both the standard model and variant trained with four registers. These two models serve as our baselines, while our approach applies edits to the standard model. For OpenCLIP, we evaluate the ViT-L/14 and ViT-B/16 models trained on LAION-2B (Schuhmann et al., 2022). As no checkpoints with trained registers are available for OpenCLIP, we only compare our approach to the standard models. 5.1 Classification and dense prediction Linear probing. We conduct linear probing on ImageNet classification (Deng et al., 2009), ADE20k segmentation (Zhou et al., 2017), and NYUv2 monocular depth estimation (Nathan Silberman & Fergus, 2012), following the procedure outlined in (Oquab et al., 2024; Darcet et al., 2024). The results, presented in Table 2, show that models edited with test-time registers maintain their performance on ImageNet classification with slight performance gains over the base model on segmentation and depth estimation tasks. These improvements are consistent with the performance gains observed in DINOv2 explicitly trained with registers. Thus, we demonstrate that intervening on model with test-time registers does not degrade the models representations, and in some cases, even enhances performance on prediction tasks. While we observe small improvement in classification performance with the DINOv2 model trained with registers, we note that this was an independently trained model. Hence, the difference could be due to variations in initialization and training dynamics, rather than the effect of trained registers fixing artifacts. We present additional experiment details in Appendix A.7. Zero-shot classification. We evaluate zero-shot ImageNet classification with OpenCLIP to assess whether test-time registers preserve the semantic structure of the original representation space. Unlike linear probing, where trained linear head can compensate for small shifts in representation, zero-shot classification is more sensitive to such changes. We compare zero-shot performance before and after applying test-time registers across both ViT-L/14 and ViT-B/16 in Table 3, observing that the intervention does not sacrifice performance. 5.2 Zero-shot segmentation To validate that test-time registers result in more interpretable attention maps, we follow standard protocol for evaluating heatmap-based explainability methods (Hooker et al., 2019) binarizing the heatmap into foreground/background segmentation map, and evaluating its segmentation quality. We compute the mean attention map for the CLS token in the last layer (Chefer et al., 2021) for the original model without registers, the model with test-time registers, and model trained with registers if available. We evaluate the zero-shot segmentation quality on ImageNet-segmentation (Guillaumin et al., 2014), which contains subset of 4,276 images from the ImageNet validation set with annotated segmentations. Results. We present our zero-shot segmentation scores in Table 4. We also qualitatively compare the attention maps for DINOv2 between test-time and trained registers in Figure 6, demonstrating similarly high-quality attention maps. Using test-time registers on both DINOv2 and OpenCLIP"
        },
        {
            "title": "Mean IOU Pixel Accuracy mAP",
            "content": "DINOv2 DINOv2 + trained registers DINOv2 + test-time registers OpenCLIP OpenCLIP + test-time registers 38.3 33.9 38.9 34.7 40.0 87.6 91.4 87.6 76.0 74. 80.6 79.0 81.1 79.2 82.1 Table 4: Zero-shot segmentation results on ImageNet. We use the mean CLS attention maps from the last layer and find that test-time registers outperform the original models and DINOv2 models with trained registers on mean IOU and mAP with small drops in pixel accuracy. VOC 2007 VOC 2012 COCO 20k DINOv2 DINOv2 + trained registers DINOv2 + test-time registers OpenCLIP OpenCLIP + test-time registers 32.2 56.2 53.8 30.8 30.9 36.8 60.2 57.9 35.9 35.9 25.4 42.3 41.9 23.4 23. Table 5: Unsupervised Object Discovery with LOST (Siméoni et al., 2021). We report the best correct localization score (corloc) over the key, query, and value features from the last four layers. Adding test-time registers significantly boosts performance for DINOv2, effectively closing the gap with DINOv2 trained with registers. outperforms the original model on mean IOU and mAP with minimal drops in pixel accuracy. We suspect that the performance gains are higher for OpenCLIP because its attention maps naturally have more artifacts. Test-time registers also show slight boosts over the retrained DINOv2 with registers on mean IOU and mAP, suggesting that test-time registers lead to attention maps as clean as those from trained registers. 5.3 Unsupervised object discovery We evaluate models edited with test-time registers for unsupervised object discovery, extracting their features for downstream processing. Darcet et al. (2024) found that the performance on this task correlates with the smoothness of models attention maps, particularly for DINOv2, and showed that attention features from models with trained registers lead to better object localization. Evaluation setting. We apply the LOST (Siméoni et al., 2021) object discovery method on the PASCAL VOC 2007, PASCAL VOC 2012 (Everingham et al., 2010), and COCO 20k (Lin et al., 2014) datasets. Our evaluation involves sweeping over the key, query, or value features over the last four layers, and manually adding bias value to the Gram matrix of features as suggested by Darcet et al. (2024). We compare DINOv2 and OpenCLIP edited with test-time registers against the unedited models and, if available, those with trained registers. Test-time registers improve unsupervised object discovery. We report correct localization (corloc) scores in Table 5. We find that LOST performance improves significantly over the base model, on features computed with our method for DINOv2 (increase of 21 in corloc). Adding the test-time register closes the gap between the baseline model and model with trained registers, reaching within 02 corloc. This suggests that the test-time register mimics the role of trained registers on this task. However, we note that test-time registers only marginally affect the results for OpenCLIP, phenomenon also observed in Darcet et al. (2024) with trained registers. Further analysis can be found in Appendix A.8. 5.4 Applying test-time registers to vision-language models Beyond discriminative vision models, we explore the effect of test-time registers on vision-language modeling (VLM) tasks. Adding test-time register to the image encoder of VLM preserves performance on several multimodal benchmarks while improving the interpretability of feature maps. 8 Evaluation setting. Using Algorithm 1, we find set of 100 register neurons (out of 100K neurons) from CLIP ViT-L/14 vision encoder of LLaVA-Llama-3-8B.3. We then create test-time register to collect their activations and evaluate the model on the eight main benchmarks from the VLMEvalKit toolkit (Duan et al., 2024). The benchmarks span across OCR, chart interpretation, visual Q/A, etc. Test-time registers maintain performance. We report our benchmark results in Table 6. Overall, we observe that adding test-time register preserves performance for multi-modal processing. We do not pass the register to the LLM; we leave exploring its use as global memory for the LLM or leveraging multiple registers for adaptive computation to future work. Benchmark Baseline w/ Test-time Register Avg. 46.2 46.2 HallusionBench MMVet MMMU_VAL OCRBench MMStar MathVista AI2D MMBenchv1. 28.6 33.4 40.4 41.6 46.3 40.9 69.9 68.5 29.4 33.9 40.1 41.3 46.4 41.3 69.4 68.0 Test-time registers improve VLM interpretability. While adding test-time register has only minor impact on performance, it improves the interpretability of attention maps, as illustrated in Figure 7. We visualize the norms of the patch outputs from the vision encoder, highlighting the presence of outlier tokens. Next, we visualize the average attention aggregated across all layers and heads of the language model from the token responsible for answering the question to the visual tokens. Without registers, the outlier visual tokens create artifacts in the language models attention. However, adding test-time register removes outliers and results in more interpretable cross-modal attention. This provides clearer insights into the models behavior (e.g., the attention map shows incorrect localization of the man closest to us\"). We provide more visualizations in Appendix A.9. Our results suggest that test-time registers can support diagnostics and the development of more transparent multimodal systems. Table 6: Adding test-time register maintains overall performance across multi-modal tasks. Figure 7: Test-time registers improve interpretability of LLaVA-Llama-3-8B. We visualize the patch norms of the vision encoder before projection and the average attention from the answer token to the visual tokens. We observe that outliers leak into the language models attention to visual tokens, while adding test-time register mitigates this and leads to more interpretable maps. 5.5 Typographic attacks We demonstrate here additional evidence of our ability to shift high-norm artifacts arising from register neurons. We strategically place high-norm artifacts within an image to mask out adversarial patches while preserving semantic content. This intervention is highly localized and relies on activating only small fraction of neurons to produce targeted changes in the image representation. Azuma & Matsui (2023) showed that CLIP is vulnerable to typographic attacks, where images can be misclassified based on written text in the image rather than the depicted object (Figure 8). 3https://huggingface.co/xtuner/llava-llama-3-8b 9 Experimental setup. Following Azuma & Matsui (2023), we use OpenAI CLIP (Radford et al., 2021) and identify register neurons with Algorithm 1 on this model. Then, we localize the largest region of text in the image with OCR and, using the register neurons, move high-norm outliers onto the patches corresponding to the text location. We evaluate on the RTA-100 dataset (Azuma & Matsui, 2023), which contains approximately 1000 images with text written on top of an object from one of 100 classes. We calculate zero-shot accuracy by comparing CLIPs image embedding to the real and attack labels text embedding."
        },
        {
            "title": "Model",
            "content": "Attack success % CLIP w/ pixel ablation w/ test-time registers Results. Qualitative results in Figure 8 show that shifting the activation maps of register neurons early in the models computation causes outlier patches to later appear corresponding to the text area. In Table 7, we present the attack success rate and find that it significantly drops after our intervention. This indicates that our method masks portion of the models internal representation of the image without harming the semantic content. We also present results of an alternative masking procedure that operates in the input space. We set all of the pixels of patches containing text content to the mean pixel value of that region. Our intervention matches this alternative methods performance with only sparse intervention. Whereas 10% of the input space must be masked in order to have causal effect on CLIPs output, our intervention repurposes CLIPs existing internal mechanisms, modifying roughly 0.02% of all neurons. This sparse and targeted modification in activation space highlights the influence of register neurons in guiding CLIPs visual representation and output behavior. Table 7: Typographic attack success rate. We leverage register neurons to shift outliers to tokens at areas with text, effectively masking the adversarial text out in activation space. 50.5 7.6 7.5 Figure 8: Qualitative results on typographic attacks. We show the patch norms of the last layer before (Original) and after (Shifted) intervening on register neurons. Shifting the outliers to the text location masks the text in activation space and results in more accurate classification."
        },
        {
            "title": "6 Discussion, limitations, and future work",
            "content": "We uncovered simple emergent mechanism in ViTs, sparse set of neurons that is responsible for creating high-norm tokens in low-information image locations. Editing this mechanism at test-time allowed us to shift the high norms into additional registers, removing artifacts from patches and yielding more interpretable feature maps while preserving or modestly improving downstream performance. Next, we discuss limitations of our analysis and conclude with future work. While our analysis shows that we can steer the location of high-norm tokens, we only addressed one component type that is responsible for their creation neurons, while neglecting other possible elements that can contribute to their formation (e.g., attention layers or input tokens). The edited tokens result in slight performance differences with their learned counterpart  (Table 1)  , suggesting that the test-time registers are not fully equivalent to the high-norm tokens. Finally, similarly to Darcet et al. (2024), we mostly focus on individual models (CLIP-ViT-B/16 and DINOv2-ViT-L/14). We do not present results on other, less commonly used, pretrained ViTs, and leave it for future work. The mechanism that we found points to an intriguing property about model neurons not all neurons have feature-related role. Register neurons, for example, are responsible for igniting high-norm tokens an image-independent role, and can not be discovered by correlating their activations to the image features. Uncovering other similar image-independent roles of neurons can shed additional light on the computational process of deep neural networks. We plan to develop methodology for such automatic discovery in future work."
        },
        {
            "title": "7 Acknowledgments",
            "content": "We thank Katie Luo, Lisa Dunlap, Yaniv Nikankin, and Arjun Patrawala for their feedback on our paper. AD is supported by the US Department of Energy Computational Science Graduate Fellowship. YG is supported by the Google Fellowship. Additional support came from the ONR MURI grant."
        },
        {
            "title": "References",
            "content": "Hiroki Azuma and Yusuke Matsui. Defense-prefix for preventing typographic attacks on clip. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 36443653, 2023. David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifying interpretability of deep visual representations. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 33193327, 2017. doi: 10.1109/CVPR.2017.354. Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the International Conference on Computer Vision (ICCV), 2021. Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 782791, June 2021. Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 28182829, 2023. Timothée Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=2dnO3LLiJ1. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248255, 2009. doi: 10.1109/CVPR.2009.5206848. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021. Amil Dravid, Yossi Gandelsman, Alexei Efros, and Assaf Shocher. Rosetta neurons: Mining the common units in model zoo. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 19341943, 2023. Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM international conference on multimedia, pp. 1119811201, 2024. Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88:303338, 2010. Yossi Gandelsman, Alexei Efros, and Jacob Steinhardt. Interpreting the second-order effects of neurons in CLIP. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=GPDcvoFGOL. Matthieu Guillaumin, Daniel Küttel, and Vittorio Ferrari. Imagenet auto-annotation with segmentation propagation. Int. J. Comput. Vision, 110(3):328348, dec 2014. ISSN 0920-5691. doi: 10.1007/ s11263-014-0713-9. URL https://doi.org/10.1007/s11263-014-0713-9. 11 Evan Hernandez, Sarah Schwettmann, David Bau, Teona Bagashvili, Antonio Torralba, and Jacob Andreas. Natural language descriptions of deep visual features. In International Conference on Learning Representations, 2021. Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. benchmark for interpretability methods in deep neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/ 2019/file/fe4b8556000d0f0cae99daa5c5c5a410-Paper.pdf. Laurent Itti, Christof Koch, and Ernst Niebur. model of saliency-based visual attention for rapid scene analysis. IEEE Transactions on pattern analysis and machine intelligence, 20(11): 12541259, 2002. Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer vision ECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pp. 740755. Springer, 2014. Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, 2012. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. Transactions on Machine Learning Research Journal, pp. 131, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Brian Robinson, Nathan Drenkow, Colin Conwell, and Michael Bonner. sparse null code emerges in deep neural networks. In Proceedings of UniReps: the First Workshop on Unifying Representations in Neural Models, pp. 302314. PMLR, 2024. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. Sarah Schwettmann, Neil Chowdhury, Samuel Klein, David Bau, and Antonio Torralba. Multimodal neurons in pretrained text-only transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 28622867, 2023. Baifeng Shi, Siyu Gai, Trevor Darrell, and Xin Wang. Toast: Transfer learning via attention steering, 2023. URL https://arxiv.org/abs/2305.15542. Oriane Siméoni, Gilles Puy, Huy Vo, Simon Roburin, Spyros Gidaris, Andrei Bursuc, Patrick Pérez, Renaud Marlet, and Jean Ponce. Localizing objects with self-supervised transformers and no labels. arXiv preprint arXiv:2109.14279, 2021. Mingjie Sun, Xinlei Chen, Zico Kolter, and Zhuang Liu. Massive activations in large language models. In First Conference on Language Modeling, 2024. URL https://openreview.net/ forum?id=F7aAhfitX6. Carl Vondrick, Aditya Khosla, Tomasz Malisiewicz, and Antonio Torralba. Hoggles: Visualizing object detection features. In Proceedings of the IEEE International Conference on Computer Vision, pp. 18, 2013. Haoqi Wang, Tong Zhang, and Mathieu Salzmann. Sinder: Repairing the singular defects of dinov2, 2024. 12 Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks, 2024. URL https://arxiv.org/abs/2309.17453. Itay Yona, Ilia Shumailov, Jamie Hayes, and Yossi Gandelsman. Interpreting the repeated token phenomenon in large language models. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/forum?id=WVth3Webet. Matthew Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pp. 818833. Springer, 2014. Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 633641, 2017."
        },
        {
            "title": "A Appendix",
            "content": "We investigate the impact of the number of test-time registers and the initialization strategy in Appendices A.1 and A.2. Additional qualitative attention visualizations are provided in Appendix A.3, followed by further analysis of OpenCLIP register neuron activations in Appendices A.4 and A.5. We apply the analysis from Section 3 to DINOv2 in Appendix A.6. Additional results from the experiments in Section 5 are included in Appendices A.7 and A.8. Finally, we provide additional VLM results with test-time registers in Appendix A.9. A.1 Ablating number of register tokens In the main text, we focused on evaluation using single register. Here, we examine the impact of employing multiple registers. Following the analysis from Sun et al. (2024), we separate the influence of high-norm tokens on the attention output (i.e. after multiplication with value features). The attention output at each token can be decomposed into two components: value contributions from the register tokens R, and value contributions accumulated over the CLS and patch tokens. Attention(Q, K, )t = (cid:88) pt ivi = (cid:88) pt ivi + (cid:88) pt ivi (1) iR (cid:124) (cid:123)(cid:122) (cid:125) registers contribution /R (cid:124) (cid:123)(cid:122) (cid:125) non-registers contribution where pt with token i. is the attention weight of query token to token i, and vi is the value embedding associated Experimental setup for test-time registers. Given Equation (1), we analyze the contribution of test-time registers to the value updates of all other tokens in the final attention layer of DINOv2 ViT-L/14. We first compute the value update for each token across the ImageNet validation set using single test-time register. Then, we vary the number of registers and, for each setting, compute the cosine similarity between the resulting value update and the one obtained with single register, averaged over all tokens. Since all test-time registers are initialized to zero, we break symmetry by randomly assigning which test-time register receives the outlier register neuron activation. Thus, each test-time register holds the activations from different set of register neurons. Cosine Sim. w.r.t. 1 Register # Test-time Registers Increasing the number of test-time registers beyond one has negligible impact. Our results in Table 8 show that increasing the number of registers has marginal impact on the internal computation of the model. We see the qualitative effect of this on the last layers CLS token attention map averaged across all heads in Figure 9. One test-time register removes all the high-norm artifacts from the original models attention map. These results align with the finding from Sun et al. (2024) that register tokens act as attention biases. Thus, single register token can be sufficient to induce the same value update across tokens. We further corroborate this behavior on trained registers next. 1.000 0.998 0.995 0.991 0.985 1 2 3 4 5 Table 8: Cosine similarity between value updates from one test-time register and increasingly larger sets of register tokens. Increasing register count beyond one has marginal effect. Experimental setup for trained registers. We use DINOv2 ViT-L/14 trained with four registers and calculate the value update contributions from each of the four registers to both image patch tokens and the CLS token. To approximate their collective effect with single token, we construct synthetic register by assigning, for each embedding dimension, the value (including sign) with the largest absolute magnitude across the four trained registers. We then compute the value update induced by this synthetic register and measure its cosine similarity with the original update from the four-register setup, averaged over all tokens. Multiple trained registers can be approximated by one token. We obtain cosine similarity of 0.834 between the value update from four registers and from the one constructed token. This suggests that single register can retain the majority of the representational effect of multiple trained registers. This suggests that the main role of trained registers may be holding large activations. However, since the cosine similarity is not 1.0, this implies that the registers may also involve additional mechanisms, beyond simply holding large activations, in contributing to the models behavior. 14 Figure 9: One test-time register is sufficient to remove artifacts. We increment the number of test-time registers where each register holds the activations from different set of register neurons. The last layers average CLS token attention map from DINOv2 ViT-L/14 does not significantly change with more test-time registers. Figure 10: Different test-time register initialization strategies yield similar attention maps. We experiment with three different initialization strategies and find that they do not impact the test-time registers ability to hold high norms and clean up attention maps. A.2 Evaluating different initial values for test-time registers Test-time register initialization strategies. We experiment with various initialization strategies for the test-time registers and evaluate their performance through linear probing on ImageNet, CIFAR10, and CIFAR100 classification tasks. Specifically, we test three initialization methods: initializing the token to zero, initializing it with Gaussian distribution matching the mean and standard deviation of the patch tokens, and initializing it to the mean of the patch tokens. Different initializations produce similar results. All three approaches yield similar probing performance as reported in Table 9. We attribute this outcome to the fact that the primary contribution of the register during attention comes from the large activations it holds, while the other values, which are much smaller in magnitude, have little impact on the final result. In Figure 10, we visualize the last layers average CLS token attention map from DINOv2 ViT-L/14, and observe that different initialization strategies do not significantly impact the test-time registers ability to remove artifacts. IN1k CF10 CF100 [reg] (zero init.) [reg] (rand. init.) [reg] (mean init.) 84.5 99.1 84.6 99.2 84.5 99.1 93.0 92.8 92. Table 9: Image classification via linear probing the test-time register token (DINOv2 ViT-L/14). We sweep over different initialization strategies for the token and find that they yield similar results. A.3 Attention maps after intervening upon register neurons We present attention maps from all layers for several example images, applying our intervention on OpenCLIP (Figure 11) and DINOv2 (Figure 12 and Figure 13). Our results show that intervening upon register neurons produces clean attention maps free of the high-norm artifacts. A.4 Full experiment results on test-time registers in OpenCLIP In Section 4, we moved outlier activations to an added, test-time register in OpenCLIP and observed that it removes outliers from the image patches. After moving the outliers to single test-time register, we measured the test-time registers output norm, the maximum norm of image patch outputs, and the highest last-layer CLS attention, both to the test-time register and any image patch. We now present Figure 11: Attention maps for OpenCLIP with test-time register. We show the mean CLS attention maps from all layers for several input images. Outliers appear in layer 6 for the original model but do not appear with test-time registers, producing clean, interpretable attention maps. 16 Figure 12: Attention maps for DINOv2 with test-time register (Layers 0-11). We show the mean CLS attention maps from layers 0-11 for several input images. As we do not intervene in these layers, there is no difference in the attention maps. 17 Figure 13: Attention maps for DINOv2 with test-time register (Layers 12-23). We show the mean CLS attention maps from layers 12-23 for several input images. Artifacts appear in uniform regions around layer 18 for the original model. With test-time registers, attention maps become clean and reveal the images main objects. Figure 14: Intervening on activations of register neurons effectively shifts outliers to test-time registers. For all register neurons, we copy their highest activation into the test-time register and zero out the activations elsewhere. The test-time register absorbs the high norms and attention from the CLS token, indicating that the image patches no longer have outliers. Figure 15: Intervening on random neurons does not shift outliers. We attempt to apply our intervention method with random neurons to move outliers to arbitrary patches. However, we find that the selected patch fails to absorb the high norms, suggesting that our selection of register neurons is meaningful. the full results in Figure 14 and find nearly identical distribution to Figure 5, indicating that the image patch outliers have been moved to the test-time register. A.5 Additional experiments on register neurons in OpenCLIP Intervening on random neurons is ineffective for shifting outliers. While register neurons are highly effective for shifting outliers (Section 3), we evaluate random baseline to further justify our selection of neurons. We perform similar intervention method used for register neurons on random neurons in OpenCLIP ViT-B/16, but we find they are ineffective for shifting outliers. For each layer, we randomly select the same number of neurons as those in our corresponding set of register neurons. During the forward pass, we intervene on the activation maps of these random neurons. We copy the highest activation across the original activation map to randomly selected patch. In Figure 15, we observe that the selected patch does not absorb the high norms, demonstrating that using random neurons fails to shift outliers. Intervening on random neurons with high activations is similarly ineffective. We also test the hypothesis that it is the high activations specifically of register neurons that cause the outliers to appear. Instead of using the highest activation for the random neuron, we instead copy the highest activation from the corresponding register neuron. However, we find similarly ineffective results in Figure 16. The inability of random neurons to shift outliers reinforces our hypothesis that register neurons (i.e., their decoder direction) specifically are crucial for outlier emergence. Decoders of register neurons have large weights in certain dimensions. Sun et al. (2024) observe that outlier patches have massive activations in certain dimensions. Given this observation, we extract the decoder weights (post-non-linearity) for four register neurons from layer 5 and plot the distribution of weight magnitudes across dimensions in Figure 17. We find that certain dimensions (e.g. 579, 408) consistently have large weights across register neurons, which we do not observe with 19 Figure 16: Copying in high activations into random neurons does not shift outliers. To verify that it is not the high activations of register neurons that create outliers, we intervene upon random neurons but copy in the highest activation from corresponding register neuron. However, we find that the norm of the selected patch remains low, supporting our selection of register neurons to intervene upon. Figure 17: Decoder weights of register neurons have large values in specific dimensions. We take the absolute value of decoder weights from layer 5 neurons in OpenCLIP and plot the distribution of magnitudes across dimensions. Top row: layer 5 register neurons. Bottom row: layer 5 random neurons. other random neurons. This property further supports the hypothesis that register neurons are the key mechanism that leads to outlier formation. A. Investigating outliers in DINOv2 We outline similar investigation as presented in Section 3 on DINOv2-L/14 and find sparse set of neurons that can be used to move outliers to arbitrary patches. Outlier patches appear after MLPs. To evaluate whether the attention block or MLP causes outliers to form, we plot the maximum norm of the residual stream across all patches for every attention and MLP component. In Figure 18, we observe that the outliers appear at the MLP of layer 17, with attention sinks emerging soon after. These observations mirror Figure 2, where we also find single layer in OpenCLIP that outliers tend to start appearing. 20 Figure 18: Outlier patches appear after MLPs in DINOv2; attention sinks appear after outlier patches. Left: Max norms across image patches (DINOv2-L/14). Right: max attention scores of the CLS token in the last layer. In both plots, we average across 1000 images. The increase in max norms and emergence of attention sinks occur in consecutive layers. Figure 19: small set of neurons in DINOv2 have high activations on the top outlier patch (right). We average the neuron activations at layer 17 in DINOv2 for the top outlier patch and randomly selected, non-outlier patch. Both types of patches have skewed distributions. small subset of neurons (<25) consistently exhibit high activations for outlier patches across images (right). small subset of neurons shows consistently high activations before outlier patches. To investigate layer 17, we evaluate the distribution of activations for its MLP neurons on the top outlier patch, averaged across 1000 images. To identify the top outlier patch, we find the patch with the maximum norm in the 2nd-to-last layers output (layer 22). We choose the 2nd-to-last layer to identify outliers because the maximum patch norm drops in the last layer output (Figure 18), making it difficult to differentiate outliers from normal patches. In Figure 19, we observe that the activation distribution for outliers is heavily skewed, with <25 neurons having disproportionately high activations. We find similar skew in OpenCLIP (Figure 3), suggesting that outliers form due to sparse set of neurons across different ViTs. Highly activated neurons activate on all outlier locations. Having seen that small set of neurons highly activate on the top outlier, we now investigate whether these neurons highly activate across all outliers. We show three qualitative examples of activation maps in Figure 20 that closely align with the outliers. This alignment suggests that these highly activating neurons are responsible for outliers generally, which corroborates the results from OpenCLIP (Figure 4). Figure 20: Highly activated neurons on the top outlier in DINOv2 activate on all outlier positions. We present activation maps of three neurons from layer 17 that activate highly on the top outlier patch (Figure 3). These maps near-perfectly align with the high-norm outliers (\"patch norms\"). 21 Figure 21: Moving outliers in DINOv2 to random patches. For all register neurons, we copy their highest activation into selected patch and zero out the activations elsewhere. Left: norm of chosen random patch (yellow) and max norm of other patches (blue). Right: CLS attention to chosen random patch (yellow) and max CLS attention (blue) to any other patch. We find that the outliers shift to randomly selected patches, similar to OpenCLIP (Figure 5). Detecting register neurons. Given that these highly activating neuronswhich we refer to as register neuronsappear responsible for outlier formation, we develop an automatic discovery algorithm to identify and intervene upon them to shift outliers. To identify register neurons (Algorithm 1), we compute the average activations at outlier patches for all MLP neurons and return the top neurons. To move outliers to arbitrary patches, we follow the same intervention method from Section 3.2. For each register neuron, we copy the highest activation across patches into the selected patch and zero out the activations elsewhere. For applying Algorithm 1 to DINOv2, we set top_k = 45, highest_layer = 17, and the outlier threshold to 150. Register neurons causally set the position of outliers. Following Section 3.2, we evaluate whether our intervention method can move outliers to arbitrary patches in DINOv2. After intervening, we measure the max norm of the selected patch, max norm of all other patches, and the highest last-layer CLS attentions to the selected patch and any other patch. We show in Figure 21 that our intervention successfully causes any selected patch to absorb the high norms and attention sinks. In Figure 1, we use register neurons to shift attention artifacts to form arbitrary spatial patterns, further demonstrating our ability to control outliers. These findings mirror that of OpenCLIP (Figure 5), exhibiting the generalizability of our intervention across ViTs. A.7 Extended Dense Prediction Results We present additional dense prediction results from the experiments in Section 5.1. In Table 10, we show mean intersection over union (IOU), mean accuracy of each class (mAcc), and all pixel accuracy (aAcc) for ADE20k segmentation. In Table 11, we provide additional metrics for monocular depth estimation on the NYUv2 dataset. The rmse score measures root mean-squared error, while AbsRel is the absolute difference between the predicted depth and the true depth, relative to the true depth itself. δ1, δ2, and δ3 accuracy are defined as the percentage of pixels where the predicted depth is within 1.25, 1.252, and 1.253 times the true depth, respectively. Models with test-time registers maintain or slightly improve performance across all metrics, with comparable performance to models trained explicitly with registers. DINOv2 ViT-L/14 w/ trained registers w/ test-time register OpenCLIP ViT-B/16 w/ test-time register mIoU mAcc aAcc 48.2 49.1 49.1 40.2 40.3 59.3 81.9 60.8 81.7 61.5 81.9 52.3 76.1 52.7 76.4 Table 10: Linear probe results for semantic segmentation on the ADE 20k dataset. DINOv2 ViT-L/14 w/ trained registers w/ test-time register rmse AbsRel δ1 δ2 δ3 89.0 98.9 99.8 38.7 89.1 98.9 99.8 38.2 89.5 98.9 99.8 37.8 11.3 11.0 11.0 OpenCLIP ViT-B/16 w/ test-time register 60.3 59.5 18.9 18.5 70.1 93.4 98.6 70.7 93.7 98. Table 11: Linear probe results for monocular depth estimation on the NYUv2 dataset. 22 A.8 Additional LOST Object Discovery Results LOST Algorithm Overview. The LOST unsupervised object discovery algorithm (Siméoni et al., 2021) begins by using patch representations to construct Gram matrix A, which encodes pairwise patch similarities. Next, we form an undirected patch similarity graph where two nodes are connected if their similarity is positive. The initial seed is selected as the patch with the lowest degree in the graph. During the subsequent expansion phase, the algorithm iteratively selects the next lowest-degree patch that is positively correlated with the current seed, forming set of patches S. binary mask is then computed by comparing all patch features to those in and retaining the patches that, on average, have positive correlation with features in S. Finally, the bounding box of the connected component in that contains the initial seed is used as the detected object. Visualizing Intermediate LOST Steps. We visualize the impact of test-time registers on the intermediate computation steps of the LOST object discovery algorithm in Figure 22. The first row (LOST score) displays the inverse degree of each patch in the similarity graph G. The second row shows the similarity between all patch features and the initial seed. The third row highlights the initial seed in yellow and illustrates the resulting seed expansion. For DINOv2, adding test-time register cleans up the intermediate maps used during LOST, translating to the performance gains in Table 5. Notably, the resulting intermediate steps resemble those obtained when using trained registers. Adding test-time register to OpenCLIP also refines the intermediate maps. However, since OpenCLIP already produces higher-quality intermediate LOST maps compared to DINOv2, the resulting improvement in unsupervised object discovery is marginal, which was also observed by Darcet et al. (2024). OpenCLIP LOST Feature Analysis. We visualize the LOST score for OpenCLIP using the key, query, and value projection features in Figure 23. Adding test-time register results in maps that are more focused on the object. However, the value projection from the original model already filters out much of the noise in the background regions, making the baseline maps relatively clean. As result, the improvement from using test-time register is less pronounced compared to DINOv2. Darcet et al. (2024) suggest that for OpenCLIP, this may be the case since outliers seem to reside in the null space of the value projection layer. Figure 22: Intermediate computation steps for LOST unsupervised object discovery. Adding test-time register produces sharper intermediate maps. Figure 23: OpenCLIP LOST maps using different features. Adding test-time register sharpens intermediate maps for different features. However, gains are limited for the values since the value projection already suppresses background noise. 23 A.9 Additional VLM Results We present additional visualizations of LLaVA-Llama-3-8B patch norms and attention maps in Figure 24. As in Section 5.4, we visualize the patch norm map from the final layer of the vision encoder prior to projection into the language model input space. The patch norm maps highlight the presence of sparse set of high-norm tokens, if any. We then visualize the average attention across all layers and heads of the language model from the response token to the visual tokens. We find that adding test-time register to the vision encoder of the VLM leads to more interpretable attribution of the text output to the visual tokens. Figure 24: Additional visualizations of LLaVA-Llama-3-8B patch norms and attention maps. As in Figure 7, we show patch norms of the vision encoder and the average attention from the answer token to the visual tokens. Adding test-time register mitigates high-norm artifacts in the vision encoder which would otherwise lead to anomalous attention patterns in the language model."
        }
    ],
    "affiliations": [
        "UC Berkeley"
    ]
}