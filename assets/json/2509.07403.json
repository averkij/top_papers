{
    "paper_title": "LongEmotion: Measuring Emotional Intelligence of Large Language Models in Long-Context Interaction",
    "authors": [
        "Weichu Liu",
        "Jing Xiong",
        "Yuxuan Hu",
        "Zixuan Li",
        "Minghuan Tan",
        "Ningning Mao",
        "Chenyang Zhao",
        "Zhongwei Wan",
        "Chaofan Tao",
        "Wendong Xu",
        "Hui Shen",
        "Chengming Li",
        "Lingpeng Kong",
        "Ngai Wong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) make significant progress in Emotional Intelligence (EI) and long-context understanding. However, existing benchmarks tend to overlook certain aspects of EI in long-context scenarios, especially under realistic, practical settings where interactions are lengthy, diverse, and often noisy. To move towards such realistic settings, we present LongEmotion, a benchmark specifically designed for long-context EI tasks. It covers a diverse set of tasks, including Emotion Classification, Emotion Detection, Emotion QA, Emotion Conversation, Emotion Summary, and Emotion Expression. On average, the input length for these tasks reaches 8,777 tokens, with long-form generation required for Emotion Expression. To enhance performance under realistic constraints, we incorporate Retrieval-Augmented Generation (RAG) and Collaborative Emotional Modeling (CoEM), and compare them with standard prompt-based methods. Unlike conventional approaches, our RAG method leverages both the conversation context and the large language model itself as retrieval sources, avoiding reliance on external knowledge bases. The CoEM method further improves performance by decomposing the task into five stages, integrating both retrieval augmentation and limited knowledge injection. Experimental results show that both RAG and CoEM consistently enhance EI-related performance across most long-context tasks, advancing LLMs toward more practical and real-world EI applications. Furthermore, we conducted a comparative case study experiment on the GPT series to demonstrate the differences among various models in terms of EI. Code is available on GitHub at https://github.com/LongEmotion/LongEmotion, and the project page can be found at https://longemotion.github.io/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 3 0 4 7 0 . 9 0 5 2 : r LongEmotion: Measuring Emotional Intelligence of Large Language Models in Long-Context Interaction Weichu Liu1*, Jing Xiong2*, Yuxuan Hu3, Zixuan Li4, Minghuan Tan5, Ningning Mao6, Chenyang Zhao7, Zhongwei Wan8, Chaofan Tao2, Wendong Xu2, Hui Shen2,9 Chengming Li1, Lingpeng Kong2, Ngai Wong2 1Shenzhen MSU-BIT University, 2The University of Hong Kong, 3City University of Hong Kong, 4Institute of Automation, Chinese Academy of Sciences, 5Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, 6Beijing Normal University, 7The University of California, Los Angeles, 8The Ohio State University, 9University of Michigan Abstract Large language models (LLMs) make significant progress in Emotional Intelligence (EI) and long-context understanding. However, existing benchmarks tend to overlook certain aspects of EI in long-context scenarios, especially under realistic, practical settings where interactions are lengthy, diverse, and often noisy. To move towards such realistic settings, we present LONGEMOTION, benchmark specifically designed for long-context EI tasks. It covers diverse set of tasks, including Emotion Classification, Emotion Detection, Emotion QA, Emotion Conversation, Emotion Summary, and Emotion Expression. On average, the input length for these tasks reaches 8,777 tokens, with long-form generation required for Emotion Expression. To enhance performance under realistic constraints, we incorporate Retrieval-Augmented Generation (RAG) and Collaborative Emotional Modeling (COEM), and compare them with standard prompt-based methods. Unlike conventional approaches, our RAG method leverages both the conversation context and the large language model itself as retrieval sources, avoiding reliance on external knowledge bases. The COEM method further improves performance by decomposing the task into five stages, integrating both retrieval augmentation and limited knowledge injection. Experimental results show that both RAG and COEM consistently enhance EI-related performance across most long-context tasks, advancing LLMs toward more practical and real-world EI applications. Furthermore, we conducted comparative case study experiment on the GPT series to demonstrate the differences among various models in terms of EI. (cid:135)Code https://github.com/LongEmotion/LongEmotion (cid:140)Project https://longemotion.github.io/ QEmail weichuliu1023@gmail.com QEmail junexiong@connect.hku.hk"
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) are increasingly adopted (Wang in the domain of Emotional Intelligence (EI) *These authors contributed equally. Corresponding author. instance, et al. 2023; Sabour et al. 2024). For the EmoBench (Sabour et al. 2024) highlights the necessity for robust, psychological-theory-grounded evaluation across both emotional understanding and generation, demonstrating that current LLMs lag behind human-level performance. By leveraging their advanced language understanding and generation capabilities, LLMs become valuable tools for facilitating emotional expression (Ishikawa and Yoshino 2025; Lu et al. 2025), with recent work showing their capacity to simulate specified emotional states in accordance with established models such as Russells Circumplex (Russell 1980, 2003). LLMs are increasingly serving in roles ranging from mental health assistants (Guo et al. 2024; Malgaroli et al. 2025; Fu et al. 2024) to everyday conversational companions (Fu et al. 2024; Duan et al. 2024; Zhang et al. 2025). This growing integration into emotionally sensitive domains places greater demand on LLMs to maintain emotional coherence over time not only to understand but also to remember, adapt, and respond empathetically in prolonged interactions (Zhong et al. 2024). In particular, during long-context interactions (Maharana et al. 2024), LLMs are expected to recognize emotional cues embedded across temporally dispersed user inputs and to deliver nuanced, empathetic responses that reflect continuity in emotional expression. As such, users increasingly turn to LLM-based chatbots for both knowledge and emotional support in dynamic, evolving conversations. However, existing benchmarks (Sabour et al. 2024; Maharana et al. 2024; Paech 2023; Hu et al. 2025) for assessing the EI of LLMs lack the contextual and temporal depth required to evaluate EI in long-context settings. While these efforts lay strong foundation, they fall short in capturing the nuanced, dynamic nature of emotional communication as it occurs in real-world long-contexts (Xiong et al. 2025, 2024). More specifically: i) In most current benchmarks, emotion recognition tasks rely on input texts that are short, explicit, and often directly labeled with clear emotional cues. This simplifies the task and does not reflect natural conversations, where emotional content is frequently subtle, embedded across multiple turns, and obscured by irrelevant (a) Token distributions across tasks. (b) Distribution of sample counts. Figure 1: (a) Token distributions across tasks. For Emotion Expression, the sequence length refers to the average length of model-generated outputs, whereas for the other tasks, it corresponds to the average length of input contexts. (b) Distribution of sample counts across the six tasks, illustrating the overall composition of the dataset. or noisy information. ii) Existing generation-based benchmarks largely focus on short, multi-turn dialogues with limited number of conversational turns. As result, they fail to challenge LLMs to maintain emotional coherence over extended interactions. iii) Moreover, an equally important but underexplored aspect is the LLMs ability to generate its own emotional expressions in long-form outputs, not just recognize or respond to othersan area still lacking robust evaluation. iv) The capacity of LLMs to leverage internalized emotional knowledgesuch as theoretical emotion models, social-emotional reasoning, or culturally grounded affective normsis crucial to demonstrating higher-order EI. Yet, current evaluations rarely include tasks that assess how models apply such knowledge across longer contexts or evolving emotional trajectories. Given these limitations, can existing benchmarks truly capture the core dimensions of Emotional Intelligencesuch as perception, expression, and regulationwithin realistic long-context settings? To bridge the gap between realistic scenarios and longcontext evaluation, we introduce LONGEMOTION, benchmark designed to mirror real-world conversational dynamics when assessing LLMs EI over long-context interactions. LONGEMOTION comprises six complementary tasks. Four tasksEmotion Classification, Emotion Detection, Emotion Conversation, and Emotion Expressionmeasure models ability to recognize and generate emotional content when context spans many dialogue turns or involves complex scenarios. Two knowledge-intensive tasksEmotion QA and Emotion Summaryprobe how effectively model leverages and applies its internalized emotional knowledge in authentic scenarios. Figure 1 depicts the datasets distribution, and high-level overview appears in Figure 2. To handle these realistic settings, we develop RetrievalAugmented Generation (RAG) approach as well as novel multi-agent emotional modeling framework called Collaborative Emotional Modeling (COEM). Unlike standard RAG systems that pull from static, external corpora, our method treats the conversation history itself as dynamic vector store to capture aspect-level sentiment terms. To further enhance long-context emotional understanding, we introduce COEM, where the context is divided into coherent chunks, roughly ranked by relevance, and then processed by multiple collaborating agents (e.g., an auxiliary GPT-4o instance (OpenAI 2024b)). After second-stage re-ranking, these agents collectively generate an emotional ensemble response. This pipeline not only reflects the unpredictability and noise of real-world dialogue but also emphasizes how emotionally salient information can be continuously extracted, re-contextualized, and articulated over long-context interaction. Our contributions are summarized as: We present LONGEMOTION, long-context EI benchmark with six diverse tasks targeting recognition, generation, and knowledge application. We propose RAG and CoEM frameworks to enhance performance by retrieving and enriching contextually relevant information. We perform extensive experiments across all settings, offering detailed analyses of LLMs EI in long-context scenarios."
        },
        {
            "title": "2 Related Work\nEmotional Intelligence Benchmarks. Many benchmarks\nare developed to assess LLMs’ Emotional Intelligence (EI).\nEmoBench (Sabour et al. 2024; Hu et al. 2025) draws\non psychological theories to evaluate both emotional un-\nderstanding and application across 400 English–Chinese\nhandcrafted questions, exposing significant gaps between\nmodel and human EI levels. EQ-Bench (Paech 2023) mea-\nsures LLMs’ ability to rate emotional intensity in dialogues\nthrough 60 English queries, showing strong correlation with\nmulti-domain reasoning benchmarks. More recently, Emo-\ntionQueen (Chen et al. 2024b) offers a specialized bench-\nmark for empathy, requiring LLMs to recognize key events,\nimplicit emotions, and generate empathetic responses. De-\nspite their strengths, all of these focus on short or synthetic\ninteractions and lack the long contextual depth critical for\nassessing EI in extended conversational or narrative settings.",
            "content": "Long-Context Understanding. LLMs make strides in processing long documents, yet robust evaluation remains an open challenge. LongBench (Bai et al. 2023) introduces Figure 2: An illustrative overview of the LongEmotion dataset. To comprehensively evaluate the Emotional Intelligence of LLMs in long-context interaction, we design six tasks: Emotion Classification, Emotion Detection, Emotion QA, Emotion Conversation, Emotion Summary, and Emotion Expression. bilingual, multi-task benchmark covering QA, summarization, and code tasks with average context lengths over 6,000 words, revealing that even state-of-the-art models struggle with extended inputs. Complementing this, LooGLE (Li et al. 2023) evaluates long-context reasoning using realistic documents exceeding 24k tokens, uncovering dependencies that span across distant spans. For extreme-length evaluation, XL2Bench (Ni et al. 2024) includes tasks on fiction, law, and scientific papers with inputs up to 100k+ wordsyet LLMs still fall short in handling long-range dependencies. Beyond these, RULER (Chen et al. 2023) focuses on complex reasoning chains in long-form texts via fine-grained question types and inter-paragraph dependencies, providing valuable diagnostic lens into model reasoning depth. InfiniteBench (Sun, Gao et al. 2024), meanwhile, evaluates LLMs abilities on open-ended, unbounded contexts with theoretically unlimited input lengths, highlighting model degradation as input exceeds trained context windows. Survey work such as Liu et al. (2025) offers broad overview of long-context modeling and evaluation paradigms but emphasizes that most benchmarks primarily target information retrieval or general comprehensionnot emotional intelligence or affective computing."
        },
        {
            "title": "3 LongEmotion: Construction and Task",
            "content": "Building on LongEmotion, we evaluate models EI capabilities using three prompt-based methods: Base, RAG, and CoEM. The statistical overview of LongEmotion dataset can be found in Table 1. Appendix provides detailed explanation of metrics used in tasks where LLMs act as evaluators."
        },
        {
            "title": "3.1 Emotion Classification\nTask Design. This task requires the model to identify the\nemotional category of a target entity within long-context\ntexts that contain lengthy spans of context-independent\nnoise (Kamradt 2023). Model performance is evaluated\nbased on the consistency between the predicted label and\nthe ground truth.",
            "content": "Data Construction. We embed short emotional excerpts from Emobench into passages drawn from BookCorpus (Zhu et al. 2015). The construction involves: i) random insertion of emotional snippets, and ii) manual adjustments to ensure syntactic and contextual coherence. Proper nouns are modified to avoid identity overlap, while preserving the intended emotional tone."
        },
        {
            "title": "3.2 Emotion Detection\nTask Design. The model is given N+1 emotional seg-\nments. Among them, N segments express the same emotion,\nwhile one segment expresses a unique emotion. The model is\nrequired to identify the single distinctive emotional segment.\nDuring evaluation, the model’s score depends on whether the\npredicted index matches the ground-truth index.",
            "content": "Dataset Construction. Construction follows two steps: i) grouping texts from the Covid-worry dataset by emotion label, and ii) inserting mismatched segment into each group to form challenging contrast sets."
        },
        {
            "title": "3.3 Emotion Conversation\nTask Design.\nIn our four-stage long-context dialogue\ndataset, we select the quartile, half, and three-quarter points\nof each stage as evaluation checkpoints to assess the model’s",
            "content": "Task ID Construction Method Metric Avg len Count EC Segment Insertion & Manual Adjustment ED Reorganization of Existing Dataset QA Human Annotation Emotion Classification Emotion Detection Emotion QA Emotion Conversation MC Model Expansion & Human Annotation ES Model Expansion & Human Annotation Emotion Summary EE Reorganization of Existing Dataset Emotion Expression Accuracy Accuracy F1 LLM as Judge LLM as Judge LLM as Judge 16691 4106 11207 4856 3129 8546* 200 136 120 100 150 428 Table 1: statistical overview of the LongEmotion dataset. ID represents the abbreviations of each task. Among them, EC, ED, QA, MC, and ES are long text input tasks, where Avg len refers to the average context length of the entries. EE is long text generation task, with Avg len indicating models average generation length, and an asterisk (*) placed at the top right corner for special notation. LLM as Judge indicates scoring performed by GPT-4o. EI capabilities. At these checkpoints, the model is required to act as psychological counselor and provide empathetic and contextually appropriate emotional support to the patient. Following the generation of model responses, we conduct comprehensive evaluation using stage-specific metrics meticulously designed from the perspective of professional psychological counseling. The scoring is performed by GPT-4o, which serves as the evaluator to ensure consistency and scalability in assessment. To better highlight the advantages of our CoEM in long-context scenarios, we only apply the RAG and CoEM methods in the fourth stage of the dialogue. We evaluate the performance of all models across all stages under the Base setting. Dataset Construction. Based on CPsyCoun (Zhang et al. 2024), we construct 100 emotionally rich dialogues by expanding seed prompts into four functional stages: i) Reception and Inquiry, ii) Diagnostic, iii) Consultation, and iv) Consolidation and Ending. Each stage reflects key elements in therapeutic progression. To assess model performance across stages, we introduce 12 specialized metrics informed by five major therapeutic frameworks: Cognitive Behavioral Therapy (CBT) (Beck 2021), Acceptance and Commitment Therapy (ACT) (Waltz and Hayes 2010), Humanistic Therapy (Elliott 2002), Existential Therapy (May 1994), and Satir Family Therapy (Rebner 1972). The complete definitions of these 12 metrics are in Appendix F. To ensure high dataset quality, we conduct quality assessment using two parallel evaluation protocols: i) manual scoring by psychology experts, and ii) automated assessment using GPT-4o. Table 2 reports the average quality performance by stage, where the pearson correlation coefficient between LLMs and human scores reaches 0.934 (p = 0.066), indicating strong alignment between automated and manual evaluations. We use inter-annotator agreement (Fleiss 1971) to measure the consistency among annotators, as shown in Appendix B. For the qualifications of psychology annotators, please refer to Appendix A."
        },
        {
            "title": "3.4 Emotion QA",
            "content": "Task Design. In this task, the model is required to answer questions grounded in long-context psychological literature. Stage Evaluator GPT-4o Annotator Reception and Inquiry Stage Diagnostic Stage Consultation Stage Consolidation and Ending Stage 4.36 4.06 3.79 4. 3.89 3.86 3.65 3.96 Table 2: Quality Evaluation on Emotion Conversation. Figure 3: Annotation process of Emotion QA. Model performance is evaluated using the F1 score between its responses and the ground truth answers. Dataset Construction. The annotation pipeline can be referred to in Figure 3. The construction process involves: i) expert-written questions targeting emotional understanding, ii) refinement of reference answers for clarity and consisFigure 4: The pipeline of Collaborative Emotional Modeling (CoEM). CoEM consists of five stages: Chunking, Initial Ranking, Multi-Agent Enrichment, Re-Ranking, and Emotional Ensemble Generation. tency with F1-based evaluation, and iii) filtering based on model performance to exclude overly ambiguous or trivial examples. Through this series of manual annotation and selection process, we finally obtain 120 high-quality pairs of psychological knowledge questions and answers."
        },
        {
            "title": "3.5 Emotion Summary\nTask Design.\nIn this task, the model is required to summa-\nrize the following aspects from long-context psychological\npathology reports: (i) causes, (ii) symptoms, (iii) treatment\nprocess, (iv) illness characteristics, and (v) treatment effects.\nAfter generating the model’s response, we employ GPT-4o\nto evaluate its factual consistency, completeness, and clarity\nwith respect to the reference answer. These three evaluation\ncriteria are validated in CPsyExam (Zhao et al. 2024).",
            "content": "Dataset Construction. Drawing on CPsyCounR dataset, we first expand the experience and reflection section of the dataset to meet our requirements for long-context inputs. Next, psychology annotators label each sample across five standardized dimensions: i) Causes, ii) Symptoms, iii) Treatment Process, iv) Illness Characteristics, and v) Treatment Effect. Finally, by filtering samples based on format, content richness, and precision, we select final set of 150 samples."
        },
        {
            "title": "3.6 Emotion Expression\nTask Design.\nIn this task, the model is situated within a\nspecific emotional context and prompted to produce a long-\nform emotional self-narrative. Models first complete a psy-\nchometric self-assessment (e.g., PANAS), followed by the\ngeneration of a structured narrative spanning five phases:\n(i) Immediate Reaction, (ii) Cognitive Appraisal, (iii) Emo-\ntional and Physiological Expression, (iv) Regulation Strate-\ngies, and (v) Reflective Integration. The evaluation encom-\npasses six dimensions: emotional consistency, content re-\ndundancy, expressive richness, cognition–emotion interplay,\nself-reflectiveness, and narrative coherence. All dimensions\nare assessed by GPT-4o, which serves as the evaluator to\nscore the model’s capacity for emotional expression.",
            "content": "Dataset Construction. We utilize the situations in the EmotionBench (Huang et al. 2024) to provide the model with specific emotional context. Each instance specifies: i) an emotional category (e.g., anger), and ii) psychologically meaningful trigger (e.g., being ignored in group setting)."
        },
        {
            "title": "4 Collaborative Emotional Modeling\nFigure 4 illustrates the pipeline of CoEM. To address EI\ntasks involving long contexts, we propose a hybrid retrieval-\ngeneration architecture that combines Retrieval-Augmented\nGeneration (RAG) with modular multi-agent collaboration.\nThe framework consists of five key stages:",
            "content": "Chunking. The input context is segmented into semantically coherent or token-length-constrained chunks. This enables efficient retrieval and minimizes irrelevant content during similarity estimation. Initial Ranking. retrieval agent, implemented as CoEM-Rank, evaluates the relevance of each chunk to the query using dense semantic similarity. Top-ranked chunks are passed forward for enhancement. Multi-Agent Enrichment. reasoning agent called CoEM-Sage, functioning as knowledge assistant, enriches the selected chunks by incorporating external knowledge or latent emotional signals. These signals, derived from psychological theories or curated priors, enhance emotional reasoning without introducing task-specific leakage. Re-Ranking. The enriched chunks are then re-evaluated by CoEM-Rank for both semantic relevance and emotional alignment. This ensures that the final input is both factually grounded and affectively coherent. Emotional Ensemble Generation. The selected and enriched content, along with the prompt, is fed into generation model denoted as CoEM-Core. This model (e.g., longcontext LLM or an instruction-tuned model) produces the final task-specific output, whether it be classification, summarization, or dialogue generation. This modular approach encourages interpretability, emotional awareness, and task robustness. The CoEM setting encompasses all five stages, while the RAG setting only comprises Chunking, Re-Ranking, and Emotional Ensemble Generation. For the parameter settings and application details of RAG and CoEM, please refer to Appendix E. Method Model EC 28.50 51.17 44.00 38.50 26.17 GPT-4o-mini GPT-4o DeepSeek-V3 Qwen3-8B Llama3.1-8B-Instruct (Extended Comparison Models) GPT-5 GPT-4o-mini GPT-4o DeepSeek-V3 Qwen3-8B Llama3.1-8B-Instruct GPT-4o-mini GPT-4o DeepSeek-V3 Qwen3-8B Llama3.1-8B-Instruct 64.50 38.33 54.67 52.17 39.67 28.00 48.00 52.83 54.33 52.83 38.17 ED 16.42 19.12 24.51 18.14 9.80 22.79 21.57 22.55 23.53 19.12 11.27 20.59 25.00 23.04 18.14 11.27 QA MC-4 48.61 50.12 45.53 44.75 45.74 43.22 50.72 51.81 50.44 44.34 47.04 47.51 47.24 46.52 46.31 44.79 3.75 3.77 3.99 3.97 4.00 4.67 3.78 3.80 4.34 4.14 3.94 3.77 3.81 4.34 4.14 4.00 ES 4.14 4.19 4.28 4.21 3.98 4.37 4.19 4.13 4.28 4.20 3.71 3.91 4.02 4.12 4.09 3.60 EE 86.77 81.03 81.75 73.40 75.61 86.77 80.41 79.49 81.83 73.28 75.16 80.38 80.41 82.83 73.59 75.71 Base RAG"
        },
        {
            "title": "CoEM",
            "content": "Table 3: Experiment result across different prompting settings (Base, RAG, CoEM). EC represents Emotion Classification, ED represents Emotion Detection, QA represents Emotion QA, MC-4 represents the fourth stage of Emotion Conversation, ES represents Emotion Summary, and EE represents Emotion Expression. (a) Impact of different CoEM-Sage models on MC-4. (b) Impact of different CoEM-Sage models on ES. Figure 5: Ablation experiments on CoEM-Sage models."
        },
        {
            "title": "5.1 Experiment Setup",
            "content": "In our experiments, for closed-source models, we choose GPT-4o-mini (OpenAI 2024a) and GPT-4o, while for opensource models, we select DeepSeek-V3 (DeepSeek-AI 2024), Llama3.1-8B-Instruct (Grattafiori et al. 2024), and Qwen3-8B (Team 2025). For tasks employing automatic evaluation, we adopt GPT-4o as the evaluator. Under the base setting, we compare broader range of advanced opensource and closed-source models. We have currently evaluated the performance of GPT-5 (OpenAI 2025) and plan to include additional models in future experiments. To accelerate inference, we use vllm library (Kwon et al. 2023) as the inference engine and set temperature=0.8 and top p=0.9 for all models. For Qwen3-8B, we enable its thinking capabilities and manually remove the reasoning process between <think> and </think> to keep the answers concise. All our experiments are conducted on single A800 80G GPUs. In the Emotion Classification, Emotion Detection, Emotion QA, and Emotion Expression, we employ GPT-4o as the CoEM-Sage, while Deepeek-V3 is used for the Emotion Conversation-4 and Emotion Summary in the same role. For the retrieval and ranking components across both the RAG and CoEM settings, we adopt bge-m3 (Chen et al. 2024a) as the CoEM-Rank. The generation models listed in Table 3 are used as the CoEM-Core. Full configuration details for both Model Stage Stage 2 Stage 3 Stage 4 1-1 1-2 12-1 2-2 2-3 3-1 3-2 34-1 4-2 4-3 GPT-4o-mini GPT-4o Deepseek-V3 Qwen3-8B Llama-3.1-8B-Instruct 4.23 4.06 4.38 4.46 4.22 4.17 4.17 4.47 4.51 4. 4.19 4.19 4.45 4.50 4.28 3.53 3.47 3.86 3.99 3.69 3.48 3.44 3.88 3.90 3.65 3.44 3.41 3.78 3.78 3.65 3.41 3.44 3.55 3.75 3.48 3.54 3.54 3.69 3.87 3. 3.59 3.60 3.81 3.95 3.74 3.61 3.63 3.75 3.80 3.84 3.76 3.75 3.98 3.97 3.95 3.87 3.92 4.22 4.15 4.22 Avg 3.73 3.72 3.98 4.05 3. Table 4: Performance of models at each stage of the Emotion Conversation task under the Base setting. The entire conversation is divided into four stages: i) Reception and Inquiry, ii) Diagnostic, iii) Consultation, and iv) Consolidation and Conclusion. Each stage includes 3 checkpoints, denoted as X-Y, where indicates the stage number and indicates the checkpoint index. the RAG and CoEM frameworks are in Appendix E."
        },
        {
            "title": "5.2 Results on LongEmotion\nThe overall experimental results can be seen in Table 3. We\nevaluate the performance of each model on all tasks under\nthe Base, RAG, and CoEM settings. As the first three stages\nof the dialogue are relatively brief, RAG and CoEM are only\napplied in the fourth stage of the Emotion Conversation task.\nThe performance of models in all stages under the Base set-\nting can be seen in Table 4.",
            "content": "By analyzing the experimental results, we can observe the following: i) While GPT-4o and DeepSeek-V3 generally exhibit stronger Emotional Intelligence, Llama-3.1-8BInstruct and Qwen3-8B significantly outperform GPT-4o and GPT-4o-mini in the Emotion Conversation-4 task. This is further supported by our experimental results across all stages in the Base setting, as shown in Table 4. ii) In the Emotion Classification and Emotion Detection tasks, which heavily test the models reasoning and classification abilities, we maximize the potential of the models through the use of CoEM. iii) In contrast, in the Emotion QA and Emotion Summary tasks, which are strongly context-based, the models score largely depends on the alignment between the models response and the original text. Therefore, injecting external knowledge may introduce harmful noise into the context, leading to drop in the score. iv) In the Emotion Expression task, we use GPT-4o as the CoEM-Sage to enrich the models expression. Compared to the results of RAG and CoEM, the score of GPT-4o-mini drops, while the scores of the other four models improve. This indicates that the ability of the CoEM-Sage greatly influences the performance of the tested models. Our ablation study on the CoEM-Sage models for Emotion Detection and Emotion Summary further supports this conclusion, as shown in Figure 5. To explore models ability in emotion recognition across different context lengths, we evaluate their performance on the Emotion Classification under the Base setting, as shown in Figure 6. It can be observed that GPT-4o demonstrates the overall best performance, while DeepSeek-V3 shows the highest stability. In the longest range of 24k-27k, Llama-3.18B-Instruct experiences significant drop in performance, reflecting its limitations in handling long contexts. We also conduct ablation experiments on RAG with different chunk sizes and retrieval quantities, as shown in FigFigure 6: Model accuracy by context length range on Emotion Classification. ure 7. From the image, it can be seen that GPT-4o-mini achieved the best performance in the 128 tokens/chunk setting with 8 retrieved chunks. Furthermore, although increasing the chunk size or retrieved count allows the model to acquire more information, it also introduces more noise, which can harm the models performance. Therefore, selectively incorporating useful information and discarding irrelevant information is crucial to improving RAG performance. Figure 7: Impact of chunk size and retrieved count on GPT4o-minis RAG performance on Emotion QA. In constructing the Emotion Conversation dataset, we employ the CPsyCoun two-stage data-generation framework and enhance our prompt design. To illustrate the richness of emotional features in our synthetic data, we conduct comparative quality experiments against plain prompts. The experimental results are detailed in the Appendix D."
        },
        {
            "title": "5.3 Comparison of GPT series models\nFrom Table3, it can be seen that GPT-5’s overall capabili-\nties surpass those of GPT-4o and GPT-4o-mini. In the tasks\nof Emotion Classification and Emotion Detection, we only\nprompt the models to output the final label. The results show\nthat GPT-5’s reasoning ability is significantly better than that\nof GPT-4o and GPT-4o-mini.",
            "content": "In the Emotion QA task, GPT-4o and GPT-4o-mini tend to respond more literally based on the original text, which can be seen in Figure 8. In contrast, GPT-5 modifies content according to its own understanding, which leads to lower F1 score due to reduced alignment with the ground truth. Figure 8: Comparison of the performance of different versions of GPT models on Emotion QA. In the Emotion Conversation task, GPT-5 achieved higher scores based on our psychology theory-driven metrics. However, by examining the model outputs in Figure 9, we can see that GPT-5 merely makes better use of psychological knowledge to offer advice to the patient, rather than genuinely demonstrating empathy toward the client. Figure 10: Comparison of the performance of different versions of GPT models on Emotion Expression. In the Emotion Summary task, GPT-4o-mini and GPT-4o directly analyzed various features of the case, whereas GPT5 structured its analysis based on psychological theories, resulting in higher score, as shown in Figure 11. Figure 9: Comparison of the performance of different versions of GPT models on Emotion Conversation. In the Emotion Expression task, GPT-4o-mini performed more like real person, with the generated content closely resembling what an actual individual might say in given situation. In contrast, GPT-4os expressions were more like rigidly told story, lacking natural fluidity. Meanwhile, GPT5s generation was more comprehensive and balanced, providing well-rounded and objective description of emotions across various features, as clearly shown in Figure 10. Figure 11: Comparison of the performance of different versions of GPT models on Emotion Summary. From the tasks above, we can conclude that GPT-4o-mini behaves more like human, with richer emotional features, but its application of psychological theory is somewhat lacking. On the other hand, GPT-5 has better understanding of psychological theories, but the output is too rigid and mechanical, which might lead to less empathetic user experience in practice. GPT-4o strikes more balanced approach between theoretical understanding and emotional features."
        },
        {
            "title": "7 Future Work\nIn future work, we will gradually supplement the perfor-\nmance of new open-source and closed-source models under\nthe base setting, and open-source all data, code, and evalua-\ntion results.",
            "content": "References Bai, Y.; Lv, X.; Zhang, J.; Lyu, H.; Tang, J.; Huang, Z.; Du, Z.; Liu, X.; Zeng, A.; Hou, L.; et al. 2023. Longbench: bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508. Beck, J. S. 2021. Cognitive Behavior Therapy: Basics and Beyond. The Guilford Press, 3rd edition. Chen, J.; Xiao, S.; Zhang, P.; Luo, K.; Lian, D.; and Liu, Z. 2024a. BGE M3-Embedding: Multi-Lingual, MultiFunctionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation. arXiv:2402.03216. Chen, Y.; Lin, Y.; Zhou, J.; and Huang, M. 2023. RULER: Diagnostic Benchmark for Long-Context Reasoning. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP). Chen, Y.; Wang, H.; Yan, S.; Liu, S.; Li, Y.; Zhao, Y.; and Xiao, Y. 2024b. Emotionqueen: benchmark for evaluating empathy of large language models. arXiv preprint arXiv:2409.13359. DeepSeek-AI. 2024. arXiv:2412.19437. Duan, J.; Zhao, X.; Zhang, Z.; Ko, E. G.; Boddy, L.; Wang, C.; Li, T.; Rasgon, A.; Hong, J.; Lee, M. K.; et al. 2024. An Exploration of LLM-Guided Conversation in Reminiscence Therapy. In GenAI for Health: Potential, Trust and Policy Compliance. Elliott, R. 2002. The effectiveness of humanistic therapies: meta-analysis. Fleiss, J. L. 1971. Measuring nominal scale agreement among many raters. Psychological bulletin, 76(5): 378. DeepSeek-V3 Technical Report. Fu, Y.; Wu, J.; Wang, Z.; Zhang, M.; Shan, L.; Wu, Y.; and Li, B. 2024. LaERC-S: Improving LLM-based emotion recognition in conversation with speaker characteristics. arXiv preprint arXiv:2403.07260. Grattafiori, A.; Dubey, A.; Jauhri, A.; et al. 2024. The Llama 3 Herd of Models. arXiv:2407.21783. Guo, Z.; Lai, A.; Thygesen, J.; Farrington, J.; Keen, T.; and Li, K. 2024. Large language model for mental health: systematic review. arXiv 2024. arXiv preprint arXiv:2403.15401. Hu, H.; Zhou, Y.; You, L.; Xu, H.; Wang, Q.; Lian, Z.; Yu, F. R.; Ma, F.; and Cui, L. 2025. Emobench-m: Benchmarking emotional intelligence for multimodal large language models. arXiv preprint arXiv:2502.04424. Huang, J.; Lam, M. H.; Li, E. J.; Ren, S.; Wang, W.; Jiao, W.; Tu, Z.; and Lyu, M. R. 2024. Apathetic or Empathetic? Evaluating LLMs Emotional Alignments with Humans. In Advances in Neural Information Processing Systems 37. Ishikawa, S.-n.; and Yoshino, A. 2025. AI with Emotions: Exploring Emotional Expressions in Large Language Models. arXiv preprint arXiv:2504.14706. Kamradt, G. 2023. Needle in Haystack - Pressure Testing LLMs. https://github.com/gkamradt/LLMTest NeedleInAHaystack. Accessed: 2025-07-23. Kwon, W.; Li, Z.; Zhuang, S.; Sheng, Y.; Zheng, L.; Yu, C. H.; Gonzalez, J. E.; Zhang, H.; and Stoica, I. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Li, J.; Wang, M.; Zheng, Z.; and Zhang, M. 2023. Loogle: Can long-context language models understand long contexts? arXiv preprint arXiv:2311.04939. Liu, J.; Zhu, D.; Bai, Z.; He, Y.; Liao, H.; Que, H.; Wang, Z.; Zhang, C.; Zhang, G.; Zhang, J.; et al. 2025. comprehensive survey on long context language modeling. arXiv preprint arXiv:2503.17407. Lu, H.; Chen, J.; Liang, F.; Tan, M.; Zeng, R.; and Hu, X. 2025. Understanding emotional body expressions via large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, 14471455. Maharana, A.; Lee, D.-H.; Tulyakov, S.; Bansal, M.; Barbieri, F.; and Fang, Y. 2024. Evaluating very long-term arXiv preprint conversational memory of llm agents. arXiv:2402.17753. Malgaroli, M.; Schultebraucks, K.; Myrick, K. J.; Loch, A. A.; Ospina-Pinillos, L.; Choudhury, T.; Kotov, R.; De Choudhury, M.; and Torous, J. 2025. Large language models for the mental health community: framework for translating code to care. The Lancet Digital Health, 7(4): e282e285. May, R. 1994. Discovery of being: Writings in existential psychology. WW Norton & Company. Ni, X.; Cai, H.; Wei, X.; Wang, S.; Yin, D.; and Li, P. 2024. XL 2 Bench: Benchmark for Extremely Long Context Understanding with Long-range Dependencies. arXiv preprint arXiv:2404.05446. Zhong, W.; Guo, L.; Gao, Q.; Ye, H.; and Wang, Y. 2024. Memorybank: Enhancing large language models with longterm memory. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, 1972419731. Zhu, Y.; Kiros, R.; Zemel, R.; Salakhutdinov, R.; Urtasun, R.; Torralba, A.; and Fidler, S. 2015. Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books. In The IEEE International Conference on Computer Vision (ICCV). 2025. OpenAI. 2024a. GPT-4o Mini: Advancing Cost-Efficient Intelligence. https://openai.com/zh-Hans-CN/index/gpt4o-mini-advancing-cost-efficient-intelligence/. Accessed: 2025-07-24. OpenAI. 2024b. OpenAI: Hello GPT-4o. https://openai. com/zh-Hans-CN/index/hello-gpt-4o/. Accessed: 2025-0724. OpenAI. 2025. OpenAI: GPT-5. https://openai.com/zhHans-CN/gpt-5/. Accessed: 2025-08-24. Paech, S. J. 2023. Eq-bench: An emotional intelligence arXiv preprint benchmark for large language models. arXiv:2312.06281. Rebner, I. 1972. Conjoint family therapy. Psychotherapy: Theory, Research & Practice, 9(1): 6266. Russell, J. A. 1980. circumplex model of affect. Journal of personality and social psychology, 39(6): 1161. Russell, J. A. 2003. Core affect and the psychological construction of emotion. Psychological review, 110(1): 145. Sabour, S.; Liu, S.; Zhang, Z.; Liu, J. M.; Zhou, J.; Sunaryo, A. S.; Li, J.; Lee, T.; Mihalcea, R.; and Huang, M. 2024. Emobench: Evaluating the emotional intelligence of large language models. arXiv preprint arXiv:2402.12071. Sun, M.; Gao, L.; et al. 2024. InfiniteBench: Towards Evaluating LLMs on Unbounded Long-context Tasks. arXiv preprint arXiv:2403.07486. Team, Q. arXiv:2505.09388. Waltz, T. J.; and Hayes, S. C. 2010. Acceptance and Commitment Therapy. In Kazantzis, N.; Reinecke, M. A.; and Freeman, A., eds., Cognitive and Behavioral Theories in Clinical Practice, 148192. The Guilford Press. Wang, X.; Li, X.; Yin, Z.; Wu, Y.; and Liu, J. 2023. Emotional intelligence of large language models. Journal of Pacific Rim Psychology, 17: 18344909231213958. Xiong, J.; Shen, J.; Ye, F.; Tao, C.; Wan, Z.; Lu, J.; Wu, X.; Zheng, C.; Guo, Z.; Kong, L.; et al. 2024. UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large Language Model Inference. arXiv preprint arXiv:2410.03090. Xiong, J.; Shen, J.; Zheng, C.; Wan, Z.; Zhao, C.; Yang, C.; Ye, F.; Yang, H.; Kong, L.; and Wong, N. 2025. ParallelComp: Parallel Long-Context Compressor for Length Extrapolation. arXiv preprint arXiv:2502.14317. Zhang, C.; Li, R.; Tan, M.; Yang, M.; Zhu, J.; Yang, D.; Zhao, J.; Ye, G.; Li, C.; and Hu, X. 2024. Cpsycoun: report-based multi-turn dialogue reconstruction and evaluation framework for chinese psychological counseling. arXiv preprint arXiv:2405.16433. Zhang, X.; Wang, M.; Zhuang, X.; Zeng, X.; and Li, Q. 2025. CDEA: Causality-Driven Dialogue Emotion Analysis via LLM. Symmetry, 17(4): 489. Zhao, J.; Zhu, J.; Tan, M.; Yang, M.; Li, R.; Yang, D.; Zhang, C.; Ye, G.; Li, C.; Hu, X.; et al. 2024. CPsyExam: Chinese Benchmark for Evaluating Psychology using Examinations. arXiv preprint arXiv:2405.10212. Technical Report. Qwen3 Qualifications of Annotators Our annotation team consists of psychology researchers and computer science researchers. In the psychology research team, there is postdoctoral fellow expert specializing in psychology and seven Masters students majoring in the same field. The theoretical foundation of our dataset and metrics involves deep participation from the psychology team. Under the guidance of the expert, the seven psychology Masters students carry out the annotation work. In the computer science research team, there are three Masters students and one PhD student majoring in computer science. Their main responsibility is to modify, adjust, and organize the data annotated by the psychology team according to the characteristics of the tasks. Inter-annotator Agreement We use inter-annotator agreement to measure the consistency among human annotators. Specifically, our annotators independently re-annotate the same set of 20 Emotion Conversation examplesyielding total of 240 metric-level judgments. We calculate inter-annotator agreement using Fleiss Kappa coefficient, with results presented in Table 5. Metric Establishing the Therapeutic Alliance Emotional Acceptance and Exploration Guidance Systematic Assessment Recognizing Surface-Level Reaction Patterns Deep Needs Exploration Pattern Interconnection Analysis Adaptive Cognitive Restructuring Emotional Acceptance and Transformation Value-Oriented Integration Consolidating Change Outcomes and Growth Narrative Meaning Integration and Future Guidance Autonomy and Resource Internalization Result -0.064 0.037 -0.156 0.045 -0.005 -0.011 0.004 -0.057 -0.055 -0.065 -0.111 -0.022 Table 5: Fleiss kappa coefficient for inter-annotator agreement in Emotion Conversation. Case Study In this section, we conduct concrete analysis of how the information retrieved by the RAG and CoEM methods affects model performance. We collect all of the information retrieved by RAG and CoEM for every task in Figure 12. In models final generation prompts, the Base setting includes none of the information; the RAG setting includes only the Content information; and the CoEM setting includes both the Content and Summary information. Emotion Classification. In this task, the model is given long context in which an emotional segment is embedded within unrelated noise. The RAG method enables the model to retrieve more accurate segment, leading to improved performance; CoEM further conducts emotional analysis on the retrieved segment, resulting in the greatest performance improvement. Emotion Detection. In this task, the model receives multiple emotional segments. The RAG method ranks the original segments based on their relevance, while CoEM further enhances the emotional features of the segments and ranks the enriched packs. This relevance-based ranking approach significantly boosts the models ability to distinguish emotions. Emotion QA. In this task, we evaluate the models responses based on the F1 similarity with the ground truth. RAG helps the model retrieve more relevant source content, thereby improving its performance. However, the CoEM method, when introducing external knowledge, may alter certain internal details, which can lead to drop in model performance. In Figure 12, we highlight the differences between the summary and the original text in red for comparison. Emotion Conversation. In this task, the model is placed within multi-turn dialogue context. The RAG method ranks the context chunks based on their relevance to the previous three dialogue turns. CoEM, after the initial ranking, generates summary by combining the previous three turns with the initially selected chunks, and then performs second round of relevance ranking between the initially filtered chunks and this summary, further ensuring the accuracy of the relevance assessment. Emotion Summary. In this task, the model is required to summarize specific characteristics of psychological counseling report. RAG ranks the chunks based on their similarity to the target characteristics. CoEM further injects the analysis of these chunks provided by CoEM-Sage. However, since psychological counseling is holistic process, analyzing only isolated chunks may lead to incorrect conclusions, resulting in decline in model performance. Emotion Expression. In this task, the model is placed in an emotional situation, where it is required to answer the PANAS scale and express its emotions. RAG ranks the context chunks based on the query at each stage, while CoEM performs finer-grained emotional analysis of these chunks. The CoEM-Sage model, with its stronger emotional intelligence (EI) capabilities, captures emotional cues more precisely, which in turn helps the tested CoEM-Core model better understand and express its own emotions. Synthetic Data Ablation We employ the two-stage generation framework of CPsyCoun to generate Emotion Conversation dataset, and compare it with the direct use of single-stage straightforward generation without the counseling note and the detailed skills in the prompt. The prompt we use can be found in Figure 13, and the comparison of experimental results can be seen in Table 7. Details of RAG and CoEM We present the application details of the CoEM framework in Table 8. To ensure the accuracy of the ranking, in the Emotion Detection task, we skip the initial ranking and directly carry out multi-agent enrichment. The Chunking and"
        },
        {
            "title": "H Comprehensive Prompt Collections",
            "content": "section presents This the complete set of prompts used throughout the framework, encompassing Evaluation, Multi-agent Enrichment, and Emotional Ensemble Generation stages across all tasks. For tasks adopting automatic evaluation as the metric, we utilize GPT-4o as the evaluation model, with detailed evaluation prompts illustrated in Figures 20 to 25. During the Multi-Agent Enrichment stage, task-specific prompts are designed to guide agent collaboration and reasoning, as shown in Figures 26 to 31. Finally, in the Emotional Ensemble Generation stage, we employ carefully constructed prompts to support emotional diversity and coherence in response generation, with the full set depicted in Figures 32 to 37. Re-Ranking in the table are also applicable to the RAG framework. We also report the chunk size and retrieved count for each task in Table 9. In QA, models use different chunk sizes. For EE, the retrieved counts correspond to stages 25. LLM as Judge Metrics Design In this section, we provide detailed presentation of the metric designs that employ large models as evaluators. Emotion Summary. In the Emotion Summary, we design three metricsconsistency, completeness, and claritywith respect to the reference answer. Table 6 shows the explanations of these metrics: Metric Description Factual Consistency Completeness Clarity Is the model output factually aligned with the ground truth? Does the model include all key details found in the ground truth? Is the expression clear and coherent? Table 6: Design of Emotion Summary evaluation metrics. Emotion Conversation. In the Emotion Conversation task, we design metrics for each dialogue stage based on Cognitive Behavioral Therapy (CBT), Acceptance and Commitment Therapy (ACT), Humanistic Therapy, Existential Therapy, and Satir Family Therapy. The description and theoretical foundations for the design of each metric can be found in Table 10. Further explanations and practical applications of each metric can be referenced in the evaluation prompts, which are presented in Appendix H. Emotion Expression. In the Emotion Expression task, we design six metricsemotional consistency, content redundancy, expressive richness, cognitionemotion interplay, self-reflectiveness, and narrative coherence. Table 11 shows the detailed explanations of these six metrics. Unified Format of Data We present data samples for each task in Figures 14 to 19. In the Emotion Classification task, the model analyzes the subjects emotional state based on the given context. Emotion Detection requires the model to identify segments that carry distinct emotional expressions. In Emotion QA, the model answers questions grounded in contextual information. The Emotion Conversation task places the model in the role of psychological counselor, responding to the clients previous turn. Emotion Summary challenges the model to generate structured summary of counseling session, including the cause, symptoms, treatment process, illness characteristics, and treatment effect. Finally, in the Emotion Expression task, the model is immersed in an emotional situation, responds to the PANAS scale, and articulates its emotional state. Figure 12: An overview of retrieved chunks across all tasks. In models final generation prompts, the Base setting includes none of the information; the RAG setting includes only the Content information; and the CoEM setting includes both the Content and Summary information. Metric One-Stage Generation Two-Stage Generation Establishing the Therapeutic Alliance Emotional Acceptance and Exploration Guidance Systematic Assessment Recognizing Surface-Level Reaction Patterns Deep Needs Exploration Pattern Interconnection Analysis Adaptive Cognitive Restructuring Emotional Acceptance and Transformation Value-Oriented Integration Consolidating Change Outcomes and Growth Narrative Meaning Integration and Future Guidance Autonomy and Resource Internalization"
        },
        {
            "title": "Avg",
            "content": "4.88 4.36 3.86 4.13 4.13 3.66 3.60 4.12 3.94 4.52 4.16 4.84 4.18 Table 7: The comparison experiment results of synthetic data. 4.92 4.38 3.79 4.1 4.32 3.77 3.73 3.96 3.69 4.63 4.19 4.86 4.20 Figure 13: Dataset generation prompt for Emotion Conversation. Task Chunking Initial Ranking Multi-Agent Enrichment Re-Ranking EC"
        },
        {
            "title": "Compute similarity with\nquery",
            "content": "GPT-4o injects external EI analysis into each chunk Compute chunks similarity with query ED"
        },
        {
            "title": "Skip this stage",
            "content": "GPT-4o injects external EI analysis into each chunk QA"
        },
        {
            "title": "Compute similarity with\nquery",
            "content": "GPT-4o injects external EI analysis into each chunk For each chunk, compute pairwise similarity with other chunks and select those with the lowest similarity"
        },
        {
            "title": "Compute similarity with the query",
            "content": "MC-4 Chunk by length"
        },
        {
            "title": "Compute similarity with\nquery",
            "content": "GPT-4o generates an overall summary Compute chunks similarity with summary ES"
        },
        {
            "title": "Compute similarity with\nquery",
            "content": "GPT-4o injects external EI analysis into each chunk Compute chunks similarity with query EE Chunk by length Compute similarity with query GPT-4o injects external EI analysis into each chunk Compute chunks similarity with query Table 8: Application details in the CoEM framework. Chunk Size Retrieved Count Task Chunk Size Count 1 Count 2 Num of segs Task EC ED QA GPT-4o-mini GPT-4o Deepseek-V3 Qwen3-8B Llama-3.1-8B-Instruct MC-4 ES EE 128 128 512 128 128 128 128 1 8 4 4 4 4 4 2,4,4,4 EC ED QA GPT-4o-mini GPT-4o Deepseek-V3 Qwen3-8B Llama-3.1-8B-Instruct MC-4 ES EE 128 Num of segs 128 128 512 512 128 128 128 1 16 16 8 16 8 10 1 8 8 4 4 4 4 4 4,8,8,8 2,4,4, (a) Parameter settings applied to RAG. For EE, the retrieved counts correspond to stages 25. (b) Parameter settings applied to CoEM. Count 1 represents the number of chunks retrieved in the Initial Ranking stage, and Count 2 represents the number of chunks retrieved in the Re-Ranking stage. Table 9: Parameter settings for RAG and CoEM approaches. Stage Metric Name Description Theoretical Foundation Reception & Inquiry"
        },
        {
            "title": "Emotional Acceptance and\nExploration Guidance",
            "content": "Diagnostic"
        },
        {
            "title": "Systematic Assessment",
            "content": "Recognizing Surface-Level Reaction Patterns Deep Needs Exploration Pattern Interconnection Analysis Establish initial trust through empathy and nonjudgmental attitude, providing safe foundation for further interventions. Guide the client to express emotions (e.g., anxiety, helplessness) in safe atmosphere, demonstrating acceptance. Integrate cognitive, behavioral, emotional, relational, and existential factors into multidimensional assessment."
        },
        {
            "title": "All theoretical orientations",
            "content": "Humanistic, ACT, Satir Family CBT, Existential, Satir Family Identify the clients automatic cognitive, emotional, and behavioral responses. All theoretical orientations Reveal unmet psychological needs such as security, autonomy, connection, or meaning. Understanding the interaction of problems within the individuals internal systems and external systems; integrating findings from various dimensions to present panoramic view of how the problem is maintained. All theoretical orientations All theoretical orientations Adaptive Cognitive Restructuring Consultation By examining the truthfulness and constructiveness of thoughts, build more adaptive cognitive framework. CBT, Existential, Satir Family Emotional Acceptance and Transformation Developing Emotional Awareness, Acceptance, and Transformation Skills. All theoretical orientations Value-Oriented Integration Anchor Change to the Life Dimension Beyond Symptoms. All theoretical orientations Consolidation & Ending Consolidating Change and Growth Narrative Review therapeutic progress and reinforce positive change through coherent personal narrative. All theoretical orientations Meaning Integration and Future Guidance Internalize therapy gains into life philosophy and create value-driven future plan. Humanistic, Existential, ACT Autonomy and Resource Internalization Strengthen the clients internal coping resources and ability to continue growth independently. CBT, Existential, Satir Family Table 10: Design of Emotion Conversation evaluation metrics. Theoretical Foundation denotes the source of inspiration for the design of this metric, and All theoretical orientations indicates that the metric draws upon all five theoretical orientations. Metric Description"
        },
        {
            "title": "Consistency Between Emotional Ratings\nand Generated Text",
            "content": "Repetition of Content Richness and Depth of Content Evaluate whether the emotional ratings from the scale align with the content in the models self-description. Are the emotions rated in the scale accurately reflected in the models self-description? Also, assess whether the intensity of the ratings matches the emotional expression in the generated text. Check if there is noticeable repetition in the generated text, especially in the emotional descriptions. Are there repeated emotional, thought, or behavioral descriptions that make the text feel redundant or unnatural? Also, evaluate whether the generated text avoids repeating the same emotional descriptions and provides multi-dimensional analysis. Assess whether the generated text thoroughly explores the different dimensions of emotions (e.g., psychological, physical, and behavioral responses). Examine whether it delves into the origins, progression, and impact of the emotions, and whether it uses sufficient detail and examples to enrich emotional expression. Interaction Between Emotion and Cognition Determine whether the generated text effectively showcases the interaction between emotions and cognition. For example, does it demonstrate how the protagonist adjusts emotional reactions based on thoughts and situation evaluations? Also, check whether the emotions and behaviors in the text are consistent. Emotional Reflection and Self-awareness Overall Quality and Flow of the Text Evaluate whether the protagonist reflects on their emotional reactions. Does the text explore personal growth, self-awareness, or suggest strategies for emotional improvement? Assess whether the generated text flows smoothly and has clear structure. Is there natural progression from emotional reaction to evolution and reflection? Also, does the text use varied sentence structures and expressions to avoid monotony? Table 11: Design of Emotion Expression evaluation metrics. Figure 14: Emotion Classification dataset example. Figure 15: Emotion Detection dataset example. Figure 16: Emotion QA dataset example. Figure 17: Emotion Conversation dataset example. Figure 18: Emotion Summary dataset example. Figure 19: Emotion Expression dataset example. Figure 20: Evaluation prompt for the first stage of Emotion Conversation. Figure 21: Evaluation prompt for the second stage of Emotion Conversation. Figure 22: Evaluation prompt for the third stage of Emotion Conversation. Figure 23: Evaluation prompt for the fourth stage of Emotion Conversation. Figure 24: Evaluation prompt for Emotion Summary. Figure 25: Evaluation prompt for Emotion Expression. Figure 26: Multi-agent enrichment prompt for Emotion Classification. Figure 27: Multi-agent enrichment prompt for Emotion Detection. Figure 28: Multi-agent enrichment prompt for Emotion Conversation. Figure 29: Multi-agent enrichment prompt for Emotion QA. Figure 30: Multi-agent enrichment prompt for Emotion Summary. Figure 31: Multi-agent enrichment prompt for Emotion Expression. Figure 32: Emotional ensemble generation prompt for Emotion Classification. Figure 33: Emotional ensemble generation prompt for Emotion Detection. Figure 34: Emotional ensemble generation prompt for Emotion Conversation. Figure 35: Emotional ensemble generation prompt for Emotion QA. Figure 36: Emotional ensemble generation prompt for Emotion Summary. Figure 37: Emotional ensemble generation prompt for Emotion Expression. The prompt for the Emotion Expression task was originally structured in multiple stages; for better clarity and intuitive understanding, it has been consolidated into single prompt."
        }
    ],
    "affiliations": [
        "Beijing Normal University",
        "City University of Hong Kong",
        "Institute of Automation, Chinese Academy of Sciences",
        "Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences",
        "Shenzhen MSU-BIT University",
        "The Ohio State University",
        "The University of California, Los Angeles",
        "The University of Hong Kong",
        "University of Michigan"
    ]
}