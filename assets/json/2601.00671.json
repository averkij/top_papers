{
    "paper_title": "Fast-weight Product Key Memory",
    "authors": [
        "Tianyu Zhao",
        "Llion Jones"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Sequence modeling layers in modern language models typically face a trade-off between storage capacity and computational efficiency. While Softmax attention offers unbounded storage at prohibitive quadratic costs, linear variants provide efficiency but suffer from limited, fixed-size storage. We propose Fast-weight Product Key Memory (FwPKM), a novel architecture that resolves this tension by transforming the sparse Product Key Memory (PKM) from a static module into a dynamic, \"fast-weight\" episodic memory. Unlike PKM, FwPKM updates its parameters dynamically at both training and inference time via local chunk-level gradient descent, allowing the model to rapidly memorize and retrieve new key-value pairs from input sequences. Experiments reveal that FwPKM functions as an effective episodic memory that complements the semantic memory of standard modules, yielding significant perplexity reductions on long-context datasets. Notably, in Needle in a Haystack evaluations, FwPKM generalizes to 128K-token contexts despite being trained on only 4K-token sequences."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 ] . [ 1 1 7 6 0 0 . 1 0 6 2 : r 2026-1Fast-weight Product Key Memory Tianyu Zhao1 and Llion Jones1 1Sakana AI Sequence modeling layers in modern language models typically face trade-off between storage capacity and computational efficiency. While Softmax attention offers unbounded storage at prohibitive quadratic costs, linear variants provide efficiency but suffer from limited, fixed-size storage. We propose Fast-weight Product Key Memory (FwPKM), novel architecture that resolves this tension by transforming the sparse Product Key Memory (PKM) from static module into dynamic, fast-weight episodic memory. Unlike PKM, FwPKM updates its parameters dynamically at both training and inference time via local chunk-level gradient descent, allowing the model to rapidly memorize and retrieve new key-value pairs from input sequences. Experiments reveal that FwPKM functions as an effective episodic memory that complements the semantic memory of standard modules, yielding significant perplexity reductions on long-context datasets. Notably, in Needle in Haystack evaluations, FwPKM generalizes to 128K-token contexts despite being trained on only 4K-token sequences."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Product Key Memory 3 Fast-weight Product Key Memory 4 Experiments 5 Interpretability Analyses 6 Cost Analyses 7 Related Work 8 Conclusion Detailed Training Settings Ablation Study More Visualization Examples Corresponding author(s): Tianyu Zhao (tianyu@sakana.ai) 2 2 4 9 11 15 16 22 23 24 Fast-weight Product Key Memory 1. Introduction Sequence modeling layers, or token mixers, are the foundational components in modern language models. The most successful architectures today can be fundamentally understood as forms of associative memory (Dao and Gu, 2024; Peng et al., 2025; Vaswani et al., 2017; Yang et al., 2024b, 2025), characterized by their ability to maintain key-value associations, execute retrieval, and perform memorization (Gershman et al., 2025). Within this framework, existing layers lie on spectrum defined by the trade-off between storage capacity and computational efficiency. Standard softmax attention (Vaswani et al., 2017) acts as an associative memory with unbounded storage, yet its computational cost becomes increasingly prohibitive as the sequence length grows (Zhong et al., 2025). Conversely, linear attention variants (Behrouz et al., 2025c; Dao and Gu, 2024; Gu and Dao, 2024; Katharopoulos et al., 2020; Schlag et al., 2021b; Sun et al., 2025; Yang et al., 2025) provide efficient, sub-quadratic mechanisms but rely on fixed storage capacities that often struggle to capture the same depth of information. We focus our investigation on resolving this specific tension: balancing large-scale storage with low computational overhead. We posit that an ideal associative memory should satisfy four key properties: 1. Key-value Association: The ability to link keys to values. 2. Large Storage: Capacity that is extensive, if not unbounded. 3. Low Cost: Sub-quadratic computational complexity w.r.t. input length. 4. Retrieval and Memorization: The capability to retrieve information and, crucially, memorize new key-value pairs from inputs at any time. Product Key Memory (PKM, Lample et al. 2019) is an architecture that elegantly satisfies the first three properties. Its sparse key-value design handles an enormous number of memory slots (e.g., ùëÅ = 106) with fixed and low computation. However, PKM was originally designed as slow-weight channel mixer similar to Feed-Forward Networks (FFN) meaning it is updated only during training and remains frozen during inference. Consequently, it lacks the ability to rapidly adapt to new inputs during deployment, failing property 4. In this paper, we propose to convert PKM from static, slow-weight module into Fast-weight Product Key Memory (FwPKM). By redesigning PKM to update its parameters dynamically at both training and inference time, we enable it to function as high-fidelity episodic memory. FwPKMcan store episodes directly from input sequences and carry that memory across different contexts, offering promising new path for continual learning and personalized AI agents. 2. Product Key Memory Top-ùëò Key-value Memory standard key-value memory consists of key matrix ùêæ ùëÅ ùê∑ùêæ and value matrix ùëâ ùëÅ ùê∑ùëâ , where ùëÅ represents the number of memory slots and ùê∑{ ùêæ,ùëâ } are the hidden dimensions. common approach to learning large memory without sacrificing computation efficiency is to exploit sparsity via Top-ùëò operation (Rae et al., 2016; Weston et al., 2015). Given an input query vector q, the model computes score ùë†ùëñ for each memory slot as the inner product between the query and the keys. Top-ùëò operation Tùëò then selects the indices of the ùëò slots with ùëñ }, and the the highest scores. The selected scores are normalized via softmax to produce weights {ùë† 2 Fast-weight Product Key Memory Table 1 Comparison of PKM and FwPKM Weight Type PKM Slow Weights Similar Modules FFN, MoE-FFN, PKM FwPKM Fast Weights Softmax Attn., Linear Attn., DeltaNet Role Channel Mixer. Mixes features within single token representation. Token Mixer. Mixes information across time steps (sequence positions). Parameter Update Updated at training; Frozen at inference. Updated at both training and inference. Memory Horizon Long-term (Semantic). Stores dataset-wide facts and general rules (e.g., world knowledge). Short-term (Episodic). Stores context-specific variable bindings (the Context Window). Learning Objective Global Objective of next token prediction Local Objective of memorization retrieval output ÀÜv is computed as the weighted sum of the corresponding value slots. ùë†ùëñ = ùêæùëñ = Tùëò (s) ùëñ } = softmax({ùë† ùëó} ùëó I) ÀÜv = (cid:205)ùëñ ùëâùëñùë† ùëñ {ùë† (1) (2) (3) (4) Product Key Memory While the Top-ùëò operation restricts the number of accessed memory slots, it still requires computing scores for all ùëÅ key rows to find the top candidates. This ùëÇ(ùëÅ) complexity prohibits scaling to enormous memory sizes (e.g. ùëÅ = 106). Lample et al. (2019) proposed Product Key Memory (PKM) to address this problem. In PKM, the query vector is decomposed into two sub-queries q1 and q2 as shown below in Equation 5, where [; ] is concatenation. Instead of single large key matrix, PKM maintains two smaller sub-key matrices ùêæ {1,2} each of size ùëÅ ùê∑ùêæ. The memory slots are arranged in Cartesian grid of size ùëÅ, and the slot at index (ùëñ, ùëó) corresponds to the interaction between the ùëñ-th sub-key from ùêæ1 and the ùëó-th sub-key from ùêæ2. The score for this slot is defined as the sum of the scores of its corresponding sub-queries: ùë†(ùëñ, ùëó) = ùë†1 . Importantly, under this formulation, the Top-ùëò elements of this Cartesian product can be obtained without computing all ùëÅ scores. Instead, it first identifies the Top-ùëò indices for each sub-query independently and then searches within the smaller length-ùëò2 Cartesian product of these selected sets. ùëñ + ùë†2 ùëÅ ùëó = q{1,2} ùêæ {1,2} [q1; q2] = ùë†{1,2} ùëñ {1,2} = Tùëò (s{1,2}) ùëñ + ùë†2 = Tùëò ({ùë†1 ùëñ ùëó ùëñ I1, ùëó I2}) (5) (6) (7) (8) Once the final indices are selected, the remaining retrieval process (i.e. score normalization and weighted sum) keeps the same. ÀÜv = PKM(q; ùêæ, ùëâ) = (cid:205)ùëñ ùëâùëñùë† ùëñ (9) 3 Fast-weight Product Key Memory The PKM architecture provides an elegant way to build large-scale memory module at low cost. 106-slot memory can be indexed using two small key matrices of size 103, requiring only 2 103 score calculations. Furthermore, PKM naturally maintains mapping between keys and values, thus making it an ideal foundation for the high-capacity associative memory we want to build. 3. Fast-weight Product Key Memory The standard PKM is designed as slow-weight channel mixer. In other words, its parameters are updated via global objective (next-token prediction) over the entire training corpus but remain frozen during inference. Consequently, while it can store general semantic knowledge, it cannot adapt to new inputs or remember immediate contexts. We propose to transform PKM into Fast-weight PKM (FwPKM). We redesign the module to update its parameters dynamically at both training and inference time, by optimizing local objective. This enables FwPKM to function as high-fidelity episodic memory that captures key-value associations from the input stream itself. 3.1. Fast Weights In standard neural networks, knowledge is stored in slow weights parameters ùúô that are optimized over massive training dataset but frozen after training. While effective for storing general knowledge in the dataset, slow weights lack the ability to rapidly adapt to new contexts. The concept of fast weights (Hinton and Plaut, 1987; Schmidhuber, 1992) addresses this problem by introducing set of parameters that change dynamically according to every new input. The fast-weight parameters ùúÉ can be thought of as the storage of episodic memory. Recent works like Test-Time Training (TTT, Sun et al. 2025) demonstrate that fast-weight module can be implemented as neural model ùëì (; ùúÉ) and its parameters are updated by minimizing an MSE objective of an input sequence h1, , hùëá . Formally, for an input hùë° in the sequence, the model performs gradient descent step to minimize the MSE of reconstructing hùë° from its corrupted version hùë° (e.g. via low-rank projection): ùúÉ = ùúÉ ùúÇùúÉLMSE( ùëì (hùë°; ùúÉ), hùë°) = ùúÉ ùúÇùúÉhùë° ùëì (hùë°; ùúÉ)2 2, (10) (11) where ùúÇ is the learning rate. Through this optimization process, fast weights ùúÉ learn to encode information of the input sequence, at both training and inference time. Following TTT, we implement fast weights by performing gradient descent updates on local MSE objective during the forward pass. Specifically, we treat the key and value matrices as fast-weight parameters ùúÉ = {ùêæ, ùëâ}. These parameters are updated via chunk-level gradient descent to minimize the reconstruction error of chunk of query-value pairs. Formally, for chunk of size ùê∂, we generate query and target value inputs {(qùë°, vùë°)}ùê∂ weight projections of the hidden states1. ùë°=1 using slowqùë° = Linear vùë° = Linearùë£ ùëû ùúô(RMSNorm ùúô(RMSNormùë£ ùëû ùúô(hùë°)) ùúô(hùë°)). (12) (13) The model then computes prediction ÀÜvùë° using the current fast weights ùúÉ and updates it to minimize the Mean Square Error (MSE) between the prediction and target. We set the learning rate ùúÇ to 1. 1We abuse the subscript of tensor to index both the sequence dimension and the feature dimension(s). We use ùëñ, ùëó to index the feature dimensions and ùë° to index the sequence dimension. 4 Fast-weight Product Key Memory and multiply the loss with constant factor of 0.5. ÀÜvùë° = PKM(qùë°; ùúÉ) ùúÉ = ùúÉ (cid:205)ùê∂ ùë°=1 ùúÉ 1 2 LMSE(ÀÜvùë°, vùë°). (14) (15) 3.2. Memorization Optimization 3.2.1. MSE loss The optimization goal is to rewrite the memory such that retrieving with query qùë° yields the target value vùë°. We employ the Mean Squared Error (MSE) as our objective due to its favorable gradient properties for explicit memory rewriting. The loss for single sample is LMSE(ÀÜv, v) = 1 2 ÀÜv2 2. Notably, we use constant factor 0.5 and learning rate of ùúÇ = 1.0 as in Eq. 15. This is not arbitrary because it achieves the effect of one-step 1 rewriting. The gradient w.r.t. the prediction ÀÜv is ÀÜv 2 LMSE(ÀÜv, v) = (v ÀÜv). single gradient step therefore updates the prediction directly to the target. ÀÜv = ÀÜv 1.0 ((v ÀÜv)) = v. (16) This allows FwPKM to instantly memorize the new key-value association. In practice, we optimize the value matrix that produced the prediction, and the gradient w.r.t. the ùëñ-th value matrix row is instead given by ùëâùëñ 1 2 LMSE(ÀÜv, v) = (v ÀÜv)ùë† ùëñ . (17) 3.2.2. Loss aggregation and gradient shaping The last section showed the gradients for single sample, but we process in chunks. As result, a) an MSE loss will be calculated on every feature for every predicted value ÀÜv and b) multiple tokens may attempt to write the same value row simultaneously. In the following, we show the proper way to reduce the MSE losses in chunk into scalar loss for backpropagation and how to aggregate the gradients for row in the value matrix. Reducing MSE losses to scalar by summation Instead of using averaging reduction, we reduce the MSE losses of chunk of samples by summing over the sample and feature dimensions. Otherwise the gradient w.r.t. value matrix element ùëâùëñ, ùëó will be proportional to the actual update signal with constant of 1/(num_samples ùê∑ùëâ). Weighting gradient by row contribution Let ùëÅ read be the row contribution, i.e. the number of times value row ùëâùëñ is accessed within the current chunk. We scale the gradient for row ùëñ by 1/ùëÅ read ùëñ . ùëñ agg ùëâùëñ = 1 ùëÅ read ùëñ (cid:205)ùê∂ ùë°=1 ùëâùëñ 1 2 LMSE(ÀÜvùë°, vùë°). (18) This averaging strategy acts as consensus mechanism, ensuring that competitive memory writes from different tokens in the same chunk are balanced rather than accumulative. Weighting MSE loss by token importance While the averaging strategy mitigates excessive memory writing, it treats all writings to the same slot equally. We weigh each MSE loss by gating value ùëîùë° (defined in Section 3.4.4) to prioritize important updates within competitive writings. This scalar value represents the strength of FwPKMs impact on the language model and the usefulness of its corresponding token. 5 Fast-weight Product Key Memory No gradient clipping Unlike standard training, we explicitly avoid gradient clipping to fast-weight updates. Since the target values are unbounded, unclipped gradients are helpful for the memory to fully adapt to the scale of the target values. 3.3. Addressing Optimization 3.3.1. Marginal entropy loss Sparse memory suffers from memory collapsing where the model learns to utilize only small number of memory slots. Lample et al. (2019) showed that vanilla PKM of 512 512 = 262ùëò slots can only reach 64.4% slot usage when selecting effectively Top-128 slots per token, with multi-head mechanism where 4 heads Top-32 per head = 128 slots. The authors proposed applying batch normalization to queries and it raises the usage to 97.9%. However, we found the query normalization technique to become inefficient when the effective Top-ùëò is small (e.g. 1 head with Top-8 slots per head), and small effective Top-ùëò is shown to be important for building performant FwPKM (See ablation study in Appendix B). We counteract FwPKMs memory collapsing by optimizing an auxiliary addressing objective based on marginal entropy maximization. The goal is to encourage the model to access all memory slots uniformly on average across chunk, without forcing uniform distribution for any single individual ùëÅ denote the normalized query-key scores query. Formally, for each of FwPKMs sub key set, let after Top-ùëò selection for token ùë° in chunk, where unselected indidces have score 0. We compute the marginal distribution representing the average slot usage over the chunk and define the addressing loss as the marginal entropy of ‚Ñù ùë° ‚Ñù ùëÅ: = 1 ùê∂ (cid:205)ùê∂ ùë°=1 ùë° Laddr = ùêª (p) = (cid:205) ùëÅ ùëñ= ùëùùëñ log ùëùùëñ. (19) (20) While minimizing the MSE loss optimizes FwPKMs value matrix ùëâ to store input values, minimizing the marginal entropy loss trains the key matrix ùêæ to adapt to input queries distribution, making the key vectors cover the query representation space more effectively and uniformly. 3.3.2. IDW score query-key score ùë†ùëñ in PKM is the dot product between query and key row ùêæùëñ. However, key row can learn to change its magnitude to produce larger score for target query, without having to be close to the query in the representation space. Inverse distance weight (IDW, McCarter 2023) score is an alternative to dot-product score that produces different key layout. ùë†IDW ùëñ = log(ùúñ + ùêæùëñ 2 2), (21) where ùúñ = 103 following the paper. Due to the use of Euclidean distance, gradients produced by IDW scores will push keys to be prototypes centroids of query clusters. We found that IDW score yields better performance than dot-product score. 3.4. Target Value Construction 3.4.1. Value residual Target values are produced by slow-weight network vùë° = Linearùë£ ùúô(hùë°)). These slowweight parameters, however, only directly participate in the computation of the fast-weight MSE loss whose gradients cannot reach slow weights. We add residual connection from the output of the value projection layer to the output of FwPKM, so the value projection parameters are on the forward/backward path of the LM loss, which provides learning signals to make them produce target values that are useful for next token prediction. ùúô(RMSNormùë£ 6 Fast-weight Product Key Memory 3.4.2. Lookahead value In similar spirit to using short convolution in linear attention variants (Gu and Dao, 2024; Peng et al., 2023; Yang et al., 2024a), we pair queries with lookahead values when applying chunk-level updates. Essentially, we slightly modify the update rule in Eq. 15 by changing the timestep subscript of target value from ùë° to ùë° + 1. In this way, FwPKM associates each tokens key to its next tokens value, which provides more useful information for next token prediction. ÀÜvùë°+1 = PKM(qùë°; ùúÉ) (22) 3.4.3. Target value normalization We also found it useful to z-score normalize target values on the feature dimension. Although the absence of grad clipping already ensures that value matrix ùëâ can adapt inputs of arbitrary scale, constraining the target values to have mean of 0 and standard deviation of 1 improves training stability. 3.4.4. Gating Next token prediction does not always rely on episodic memory, so we devise gating mechanism to give the model the freedom of determining how much information is extracted from FwPKM outputs. Similar to the computation of query and value vectors, we feed hidden state hùë° to an RMS normalization and linear layer to compute scalar value: and the final output oùë° is an interpolation between the FwPKM output and the value residual: ùëîùë° = Linear ùëî ùúô(RMSNorm ùëî ùúô(hùë°)), oùë° = ùëîùë° PKM(qùë°; ùúÉ) + (1 ùëîùë°) vùë° = ùëîùë° ÀÜvùë°+1 + (1 ùëîùë°) vùë°. (23) (24) (25) 3.5. FwPKM Summary We put everything together and show the complete formulation of FwPKM in the following and in Figure 1. For each token ùë° in chunk, slow-weight network computes inputs to FwPKM. qùë°, vùë°, ùëîùë° = Linear ùëû,ùë£,ùëî ùúô (RMSNorm ùëû,ùë£,ùëî ùúô (hùë°)) (26) FwPKMs forward pass predicts the value at ùë° + 1 from query at ùë°, which is ÀÜvùë°+1 = FWPKM(qùë°; ùêæ {1,2}, ùëâ). step-by-step process is as follows. We omit timestep subscript ùë° for variables other than the input qùë° and output ÀÜvùë°+1. [q1; q2] = qt ùë†{1,2} ùëñ {1,2} = Tùëò (s{1,2}) = log(ùúñ + q{1,2} ùêæ {1,2} ùëñ 2 2) {ùë†{1,2} ùëñ } = softmax({ùë†{1,2} = Tùëò ({ùë†1 ÀÜvùë°+1 = (cid:205)ùëñ ùëâùëñùë† ùëñ + ùë†2 ùëó ùëñ } ùëó I) ùëó ùëñ I1, ùëó I2}) (27) (28) (29) (30) (31) (32) Fast-weight Product Key Memory Figure 1 Architecture of FwPKM. The predicted value is combined with the value residual using the above gating value as weight. Lastly, an RMS normalization layer and linear layer of slow weights are used to transform the output. oùë° = ùëîùë° ÀÜvùë°+1 + (1 ùëîùë°) vùë° ùë° = Linearùëú ùúô(RMSNormùëú ùúô(oùë°)) (33) (34) Once we have collected the predicted values and target values for chunk, we apply an update to the FwPKMs parameters ùúÉ = {ùëâ, ùêæ1, ùêæ2}. The value matrix is updated using shaped gradients from the MSE losses weighted by gating values. (cid:205)ùê∂ = agg ùëâùëñ 1 ùëÅ read ùëñ agg ùëâ ùëñ = ùëâùëñ ùëâùëñ . ùë°=1 ùëâùëñ 1 2 ùëîùë°LMSE(ÀÜvùë°, vùë°) The key matrices are updated using gradients from the marginal entropy-based addressing loss. = 1 ùê∂ (cid:205)ùê∂ ùë°=1 ùë° Laddr = (cid:205) ùëÅ ùëùùëñ log ùëùùëñ ùëñ=1 ùêæ = ùêæ ùêæ Laddr, ùêæ {ùêæ1, ùêæ2} (35) (36) (37) (38) (39) 8 Fast-weight Product Key Memory 4. Experiments 4.1. Training Setting Model We implement language models based on the QwenNext architecture2, which interleaves Gated DeltaNet (GDN, Yang et al. 2025) and gated softmax attention layers (Qiu et al., 2025). Our investigation uses these 12-layer baseline configurations. GDN: Gated DeltaNets at all 12 layers GDN+SWA: GDN interleaved with Sliding Window Attention (SWA, window size 512) at 3: ratio GDN+FA: GDN interleaved with Full Attention (FA) at 3:1 ratio FA: Full Attention at all layers We introduce PKM and FwPKM modules into these baselines at specific depths (layers 2, 6, and 10). standard PKM layer replaces the original FFN, while an FwPKM layer is inserted between the token mixer (Attention/GDN) and the FFN. Both PKM and FwPKM have 5122 memory slots. PKM retrieves effectively Top-128 slots (4 heads and 32 slots per head) while FwPKM retrieves Top-8 slots (1 head and 8 slots per head). We further include two types of baselines. For the first kind, we substitute the PKM architecture in FwPKM@2,6,10 with SwiGLU MLP (Shazeer, 2020) that maintains three fast-weight matrices and their biases for up-/gating-/down-projection. This variant is denoted as FwMLP@2,6,10. In addition, we use LaCT (Zhang et al., 2025b) as the second baseline. LaCT is an improved TTT (Sun et al., 2025) model that uses sliding window attention, fast-weight SwiGLU MLP, and slow-weight SwiGLU MLP in every layer. Its fast weights are updated using SGD+momentum with L2 weight normalization (Salimans and Kingma, 2016). Data To incentivize the learning of cross-chunk dependencies via FwPKM, we use 5B tokens from LongContext64 (Buckman), dataset consisting of long-context documents (> 64ùêæ tokens) sourced from RedPajama V2 (Weber et al., 2024). We supplement this with 5B tokens from FinewebEdu (Penedo et al., 2024) to maintain high-quality language modeling capabilities. All models are trained with sequence length of 4K tokens. We refer readers to Appendix for more details of the architectures and optimization. 4.2. PPL Evaluation We evaluate perplexity (PPL) on three distinct datasets to assess different memory capabilities: Fineweb-Edu (knowledge-intensive, short context), LC64 (in-domain, long context), and LAMBADA (Paperno et al. 2016, out-of-domain, long context). Each evaluation dataset contains 8M tokens. We evaluate 4K-token sequences in their original order with batch size of 1 so that sequences from the same document are presented sequentially. This enables fast weights to capture the dependency between adjacent sequences. Figure 2 shows the perplexities on the datasets for all models. Finding 1: FwPKM and PKM serve distinct, complementary roles Baseline models with layouts GDN and GDN+SWA lack components for modeling long-range dependency, and their FwPKM versions address the weakness. As shown in Figure 2, FwPKM significantly reduces PPL on long-context datasets (LC64, LAMBADA), confirming its role as an episodic memory. In contrast, standard PKM provides the largest gains on Fineweb-Edu, acting as semantic memory for global knowledge. The combination of both modules yields the best performance across all metrics, suggesting they address orthogonal limitations in the baseline architectures. 2https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking 9 Fast-weight Product Key Memory Figure 2 Perplexity on Fineweb-Edu, LC64, and LAMBADA Figure 3 FwPKM gating value distribution on Fineweb-Edu, LC64, and LAMBADA test sets. Each row represents one FwPKM layer. 10 Fast-weight Product Key Memory Finding 2: FwPKM competes with Full Attention When FwPKM is added to baselines with unrestricted Full Attention (FA and GDN+FA), PPL improvements are marginal. Analysis of gating values in Figure 3 reveals that these models learn to ignore FwPKM, with gating weights clustering near zero. Finding 3: Restricting full attention facilitates FwPKM use. To mitigate the above issue, we restrict full attentions long-range perception at training time, by imposing sliding attention window of length 512 to all full attentions with probability of 0.9. Denoted by suffix pSWA, these new models learn to use FwPKM more as suggested by more gating value distribution mass on the highvalue end in Figure 3. While pSWA has minimal impact in the PPL evaluation, we will see more significant difference in the following section. 4.3. NIAH Evaluation We conduct Needle in Haystack (NIAH, Kamradt 2023; Mohtashami and Jaggi 2023) evaluation to further verify FwPKMs functionality as episodic memory. In the basic setting, we construct 500 NIAH samples from the LAMBADA dataset. Each sample contains haystack 4K-length sequence from the LAMBADA dataset with 5 needles inserted at random positions, where each needle contains unique 4-character key and 6-digit value and question that requests the value of specific key. To test FwPKMs large memory storage, we additionally construct test sets of 8K, 32K, and 128K context lengths. Iterative Memorization (ùëõ-iter NIAH) unique feature of FwPKM is its ability to improve memory fidelity by re-processing the same input. We define ùëõ-iter NIAH setting, where the model forwards the same haystack context ùëõ times before answering. FwPKMs process chunk size ùê∂ is accordingly changed to the length of each haystack context such that it updates its memory once after reading an entire haystack. The basic setting is 1-iter NIAH. Accuracy results of ùëõ-iter NIAH (ùëõ {1, 2, 3, 4}) on the four test sets of 4K-128K context lengths are shown in Figure 4. The accuracies of all iterations are drawn as stacked bar. Finding 4: Iterative reading boosts retrieval accuracy. For GDN and GDN+SWA layouts, single pass (1-iter) is often insufficient for perfect retrieval. However, second pass (2-iter) yields massive boost in accuracy (jumping from < 10% to > 70% in many cases). This confirms that FwPKM effectively exploits test-time training to consolidate episodic memories, surpassing softmax attention. In addition, effective iterative memorization (in Figure 4) correlates with high gating values (in Figure 3). Finding 5: FwPKM generalizes to 128K contexts. Despite being trained on only 4K-token sequences, FwPKM generalizes effectively to 128K tokens. While FA baselines degrade rapidly on context lengths unseen during training, FwPKM maintains robust retrieval performance. Finding 6: Longer context requires more memorization iterations. As context length grows from 4K to 128K, the 2-iter accuracy significantly reduces from > 70% to 40% for GDN PKM@6 + FwPKM@2,10 and more severely for other models. However, more iterations largely fill in the gap. 5. Interpretability Analyses key advantage of FwPKM over black-box architectures is its inherent interpretability. Because memory slots are explicitly written and read, we can trace retrieved information back to specific input tokens. 11 Fast-weight Product Key Memory Figure 4 Stacked bar plots for NIAH accuracy results on 4K-/8K-/32K-/128K-length test sets. Each stacked bar shows the accuracies of {1, 2, 3, 4}-iter NIAH evaluations. 5.1. Case Study: Probing Memory in NIAH We analyze the memory access patterns of the GDN+PKM@6+FwPKM@2,10 model during the 4-iter NIAH task (4K context). At each generation step, every memory layer returns Top-8 retrieved slots. We show the content stored in each slot from the latest memorization3, namely the query token, the target value token, and its surrounding context. In addition, [HIT] label indicates whether slot stores the ground-truth next token. High-precision retrieval As shown in Figure 5, the majority of retrieved slots contain the correct target tokens associated with the query needle. Incorrect retrievals typically share the same query token as the input but lack the Error analysis correct prefix context. This suggests that retrieval errors stem from imperfect prefix matching in the latent query space rather than storage failure. Robust aggregation Despite the slot errors, the model successfully aggregates information across the 16 slots from 2 FwPKM layers to generate the correct 6-digit value, demonstrating robust distributed storage. 3A slot may contain information from earlier updates, but we omit them due to space limit. 12 Fast-weight Product Key Memory Figure 5 An example of FwPKM slot access of GDN+PKM@6+FwPKM@2,10 during generating an NIAH-4K answer. The model memorizes the haystack for 3 extra iterations, i.e. 4-iter NIAH. 13 Fast-weight Product Key Memory Figure 6 GDN+PKM@6+FwPKM@2,10s FwPKM gating values on tokens from the Wikipedia article for Sakana AI. 5.2. Case Study: Selective Gating We further examine the gating values ùëîùë° to understand when the model chooses to rely on episodic memory versus its static slow weights. Using the Wikipedia article for Sakana AI4 as test case, we change the models update chunk size from 512 to 32 to adapt for the articles length. As shown in Figure 6, we can observe distinct behaviors across layers. Layer specialization The lower-layer FwPKM tends to maintain high gating values across all tokens, serving as general-purpose buffer. In contrast, the higher-layer FwPKM exhibits highly selective activation. Novelty detection In the higher layer, gating values spike specifically for tokens related to rare named entities (e.g. Sakana AI, David Ha, Llion Jones, and Ren Ito). This indicates that the model effectively distinguishes between general linguistic patterns and novel, context-specific entities, which are processed by slow weights and fast weights, respectively. 4https://en.wikipedia.org/wiki/Sakana_AI 14 Fast-weight Product Key Memory Table 2 Comparison of model size and computation cost. Model"
        },
        {
            "title": "GDN",
            "content": "PKM@2,6,10 FwPKM@2,6,10 PKM@6 + FwPKM@2,10 GDN+SWA PKM@2,6,10 FwPKM@2,6,10 PKM@6 + FwPKM@2,10 GDN+FA PKM@2,6,10 FwPKM@2,6,10 PKM@6 + FwPKM@2,10 FA PKM@2,6,10 FwPKM@2,6,10 PKM@6 + FwPKM@2,10 Parameters (M) FLOPs (T) FLOPS (T/sec.) Samples/sec. 112.11 509.08 519.12 515. 112.65 509.61 519.66 516.31 112.65 509.61 519.66 516.31 114.25 511.22 521.26 517.91 22.05 20.91 22.74 22.14 22.14 21.03 22.86 22.23 22.14 21.03 22.86 22. 22.47 21.33 23.16 22.56 121.82 84.47 46.44 54.63 124.42 88.16 47.16 55.30 120.67 84.69 46.51 54.99 120.45 81.22 46.63 54.43 42.81 30.09 16.10 19. 43.45 31.25 16.27 19.39 42.08 30.07 16.05 19.29 41.55 28.49 15.89 18.84 6. Cost Analyses We compare the main models parameter numbers and computation costs in Table 2. In particular, we report FLOPs (Floating Point Operations) to measure the required computation as well as FLOPS (Floating Point Operations Per Second), which is FLOPs divided by running time (in seconds), to additionally take into account implementation efficiency. The sparsity of PKM/FwPKM makes it even less FLOPs-intensive than baseline MLP layer despite much increased model size. However FLOPS numbers suggest that PKM/FwPKM components take longer to run. One main reason is the large gap between their efficient implementation. Softmax attention and linear attention (e.g. GDN) are fast using kernels from FlashAttention (Dao, 2024; Dao et al., 2022) and FlashLinearAttention (Yang and Zhang, 2024). An important future direction for this work is to design more efficient kernels to facilitate easier scaling up and broader adoption of the FwPKM model. 7. Related Work Softmax attention and linear variants Standard softmax attention has been foundational to the success of Transformers (Vaswani et al., 2017), fundamentally acting as powerful form of associative memory (Zhong et al., 2025). However, its quadratic complexity limits its application to extremely long sequences. To address this, various efficient architectures have been proposed. Linear attention (Katharopoulos et al., 2020) reduces complexity to linear time by changing the order of association. This direction includes Recurrent Neural Network (RNN) and State Space Model (SSM) evolutions such as Mamba (Gu and Dao, 2024) and Mamba2 (Dao and Gu, 2024), as well as specific variants like DeltaNet (Schlag et al., 2021b; Yang et al., 2024b), Gated DeltaNet (Yang et al., 2025), and RWKV7 (Peng et al., 2025). Other approaches like Memory Mosaics (Zhang and Bottou, 2025; Zhang et al., 2025a) improve softmax attention by techniques such as time-dimension smoothing and hierarchical memory design. Fast weights and test-time training The concept of fast weights offers powerful lens for unifying sequence modeling. Rooted in early work by Schmidhuber (1992) and Ba et al. (2016), this 15 Fast-weight Product Key Memory framework views linear transformers as fast weight programmers (Schlag et al., 2021a). Recently, this paradigm has been revitalized by Test-Time Training (TTT) (Sun et al., 2025; Zhang et al., 2025b) and Titans (Behrouz et al., 2025c), which explicitly update parameters during inference using gradient descent, enabling the model to memorize the current context. Theoretical frameworks like MIRAS (Behrouz et al., 2025a) and Test-Time Regression (Wang et al., 2025) unified various sequence models under the umbrella of test-time optimization and associative memory. These frameworks pointed out several directions for new model designs, namely memory architecture, memorization rule, memory retention rule, and optimizer. Our proposal of FwPKM contributes novel memory architecture and specific memorization rule accommodated for its structural sparsity. Hybrid architectures Recognizing the complementary strengths of different sequence models, recent work has increasingly focused on hybrid architectures that combine the high-fidelity retrieval of quadratic attention with the efficiency of linear or recurrent layers. This includes hybrid models trained from scratch (Irie et al., 2025), as well as large-scale Hybrid LLMs such as Samba (Ren et al., 2025) and KimiLinear (Team et al., 2025). QwenNext also employs this interleaved design. Furthermore, approaches like Artificial Hippocampus Networks (AHN, Fang et al. 2025) explore fine-tuning techniques to integrate these distinct memory systems effectively. Experiments in Section 4 demonstrated interactions between memories of different characteristics. The combination of linear attention (i.e. GDN), softmax attention, slow-weight sparse memory (i.e. PKM), and fast-weight sparse memory (i.e. FwPKM) implements versatile memory system that excels at various tasks. Memory models Beyond implicit knowledge in weights, explicit memory modules have been explored to enhance storage capacity. Early works include Memory Networks (Weston et al., 2015), while recent studies suggest even simple MLPs can function as memory (Csord√°s et al., 2023). To scale up capacity without prohibitive costs, sparse access mechanisms are essential. Product Key Memory (PKM, Berges et al. 2025; Lample et al. 2019) and PEER (He, 2024) utilize sparsity to access massive memory banks efficiently. Ultra Sparse Memory (Huang et al., 2025a,b) is line of work that extends the PKM architecture with more expressive keys and other improvements. Our work builds upon the PKM structure but transitions it from static slow memory to dynamic fast memory. Continual and episodic learning Finally, the ability to update memory parameters allows for continual learning and adaptation. Lin et al. (2025) showed that parameter-efficient fine-tuning of PKM effectively mitigates catastrophic forgetting. It demonstrated one dimension for continual learning optimizing sparse slow weights via self-supervised learning to update semantic memory. FwPKM opens up the possibility for different dimension of updating episodic memory by learning fast weights in an online learning manner. Nested Learning (Behrouz et al., 2025b) and TNT (Li et al., 2025) explored the direction of stacking multiple fast weight layers (e.g. Titans) and apply memory updates at varying frequencies. This nested memory structure enables gradually digesting changes in faster weights into slower weights and exhibits strong performance. FwPKM maintains huge memory bank and is updated at low frequency to amortize optimization cost. It is promising direction to design hybrid memory system of varying-size FwPKMs and other memory components such as Titans with different optimization strategies as in Nested Learning (Behrouz et al., 2025b). 8. Conclusion In this work, we introduced Fast-weight Product Key Memory (FwPKM), memory-augmented layer that unifies PKMs large-scale sparse storage with the rapid adaptability of fast weights. Concretely, FwPKM extends Product Key Memory from static retrieval module into context-responsive component whose parameters can be updated online, allowing the model to write information into memory and later retrieve it. This addresses key limitation of prior sparse memory modules, namely their inability to efficiently incorporate new evidence at inference time, and makes PKM better 16 Fast-weight Product Key Memory suited for long-context settings where relevant information may be separated by many thousands of tokens. Empirically, we find that FwPKM remains effective far beyond its training regime. Models trained on 4K-token sequences generalize to 128K-token contexts, while exhibiting behavior consistent with robust episodic memory that complements the semantic knowledge stored in standard layers. At the same time, several challenges remain. Online updates introduce additional computation and choices such as chunk size, update frequency, and optimization hyperparameters. Scaling these updates efficiently motivates further systems work, including faster sparse update kernels and better implementation strategies. On the modeling side, future works include more powerful and robust memory design regarding its architecture, update rule, and retention. Overall, by integrating sparse storage with fast, context-driven updates, FwPKM offers promising step toward language models with versatile and mutually complementary memory components. Acknowledgments We thank Kai Arulkumaran, Luke Darlow, Stefania Druga and the rest of the Sakana AI team for fruitful discussions throughout the project. 17 Fast-weight Product Key Memory References Jimmy Ba, Geoffrey E. Hinton, Volodymyr Mnih, Joel Z. Leibo, and Catalin Ionescu. Using fast weights to attend to the recent past. In NeurIPS, 2016. Ali Behrouz, Meisam Razaviyayn, Peilin Zhong, and Vahab Mirrokni. Its all connected: journey through test-time memorization, attentional bias, retention, and online optimization, 2025a. URL https://arxiv.org/abs/2504.13173. Ali Behrouz, Meisam Razaviyayn, Peilin Zhong, and Vahab Mirrokni. Nested learning: The illusion of deep learning architectures. In NeurIPS, 2025b. URL https://openreview.net/forum?id= nbMeRvNb7A. Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. Titans: Learning to memorize at test time. In NeurIPS, 2025c. URL https://openreview.net/forum?id=8GjSf9Rh7Z. Vincent-Pierre Berges, Barlas Oguz, Daniel Haziza, Wen-tau Yih, Luke Zettlemoyer, and Gargi Ghosh. Memory layers at scale. In ICML, 2025. Jacob Buckman. Longcrawl64: Long-Context Natural-Language Dataset. R√≥bert Csord√°s, Kazuki Irie, and J√ºrgen Schmidhuber. Approximating two-layer feedforward networks for efficient transformers. In Findings of EMNLP. Association for Computational Linguistics, 2023. Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In ICLR, 2024. Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. In ICML, 2024. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R√©. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In NeurIPS, 2022. Yunhao Fang, Weihao Yu, Shu Zhong, Qinghao Ye, Xuehan Xiong, and Lai Wei. Artificial hippocampus networks for efficient long-context modeling, 2025. URL https://arxiv.org/abs/2510.073 18. Samuel J. Gershman, Ila Fiete, and Kazuki Irie. Key-value memory in the brain. Neuron, 113(11): 16941707.e1, 2025. ISSN 0896-6273. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In COLM, 2024. URL https://openreview.net/forum?id=tEYskw1VY2. Xu Owen He. Mixture of million experts, 2024. URL https://arxiv.org/abs/2407.04153. Geoffrey Hinton and David Plaut. Using fast weights to deblur old memories. In Proceedings of the ninth annual conference of the Cognitive Science Society, pages 177186, 1987. Zihao Huang, Yu Bao, Qiyang Min, Siyan Chen, Ran Guo, Hongzhi Huang, Defa Zhu, Yutao Zeng, Banggu Wu, Xun Zhou, and Siyuan Qiao. Ultramemv2: Memory networks scaling to 120b parameters with superior long-context learning, 2025a. URL https://arxiv.org/abs/2508.18756. Zihao Huang, Qiyang Min, Hongzhi Huang, Yutao Zeng, Defa Zhu, Ran Guo, and zhou Xun. Ultrasparse memory network. In ICLR, 2025b. Kazuki Irie, Morris Yau, and Samuel J. Gershman. Blending complementary memory systems in hybrid quadratic-linear transformers. In NeurIPS, 2025. 18 Fast-weight Product Key Memory Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein. Muon: An optimizer for hidden layers in neural networks, 2024. URL https://kell erjordan.github.io/posts/muon/. Greg Kamradt. Needle In Haystack - Pressure Testing LLMs. https://github.com/gkamradt/ LLMTest_NeedleInAHaystack, nov 2023. Accessed: 2025-12-20. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran√ßois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In ICML, volume 119 of Proceedings of Machine Learning Research, pages 51565165. PMLR, 2020. Guillaume Lample, Alexandre Sablayrolles, MarcAurelio Ranzato, Ludovic Denoyer, and Herv√© J√©gou. Large memory layers with product keys. In NeurIPS, 2019. Zeman Li, Ali Behrouz, Yuan Deng, Peilin Zhong, Praneeth Kacham, Mahdi Karami, Meisam Razaviyayn, and Vahab Mirrokni. Tnt: Improving chunkwise training for test-time memorization, 2025. URL https://arxiv.org/abs/2511.07343. Jessy Lin, Luke Zettlemoyer, Gargi Ghosh, Wen-Tau Yih, Aram Markosyan, Vincent-Pierre Berges, and Barlas Oƒüuz. Continual learning via sparse memory finetuning, 2025. URL https://arxiv.or g/abs/2510.15103. Calvin McCarter. Inverse distance weighting attention. In Associative Memory & Hopfield Networks in 2023, 2023. URL https://openreview.net/forum?id=dHmAhYu89E. Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context length for transformers. In NeurIPS, 2023. Denis Paperno, Germ√°n Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern√°ndez. The LAMBADA dataset: Word prediction requiring broad discourse context. In ACL. The Association for Computer Linguistics, 2016. Guilherme Penedo, Hynek Kydl√≠cek, Loubna Ben Allal, Anton Lozhkov, Margaret Mitchell, Colin A. Raffel, Leandro von Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the finest text data at scale. In NeurIPS, 2024. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan S. Wind, Stanislaw Wozniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. RWKV: reinventing rnns for the transformer era. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of EMNLP. Association for Computational Linguistics, 2023. Bo Peng, Ruichong Zhang, Daniel Goldstein, Eric Alcaide, Xingjian Du, Haowen Hou, Jiaju Lin, Jiaxing Liu, Janna Lu, William Merrill, Guangyu Song, Kaifeng Tan, Saiteja Utpala, Nathan Wilce, Johan S. Wind, Tianyi Wu, Daniel Wuttke, and Christian Zhou-Zheng. RWKV-7 goose with expressive dynamic state evolution. In COLM, 2025. URL https://openreview.net/forum?id=ayB1PA CN5j. Zihan Qiu, Zekun Wang, Bo Zheng, Zeyu Huang, Kaiyue Wen, Songlin Yang, Rui Men, Le Yu, Fei Huang, Suozhi Huang, Dayiheng Liu, Jingren Zhou, and Junyang Lin. Gated attention for large language models: Non-linearity, sparsity, and attention-sink-free. In NeurIPS, 2025. URL https://openreview.net/forum?id=1b7whO4SfY. 19 Fast-weight Product Key Memory Jack W. Rae, Jonathan J. Hunt, Ivo Danihelka, Timothy Harley, Andrew W. Senior, Gregory Wayne, Alex Graves, and Tim Lillicrap. Scaling memory-augmented neural networks with sparse reads and writes. In NeurIPS, 2016. Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba: Simple hybrid state space models for efficient unlimited context language modeling. In ICLR, 2025. Tim Salimans and Diederik P. Kingma. Weight normalization: simple reparameterization to accelerate training of deep neural networks. In NeurIPS, 2016. Imanol Schlag, Kazuki Irie, and J√ºrgen Schmidhuber. Linear transformers are secretly fast weight programmers. In Marina Meila and Tong Zhang, editors, ICML, volume 139 of Proceedings of Machine Learning Research, pages 93559366. PMLR, 2021a. Imanol Schlag, Tsendsuren Munkhdalai, and J√ºrgen Schmidhuber. Learning associative inference using fast weight memory. In ICLR, 2021b. J√ºrgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. Neural Comput., 4(1):131139, 1992. Noam Shazeer. Glu variants improve transformer, 2020. URL https://arxiv.org/abs/2002.0 5202. Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, Tatsunori Hashimoto, and Carlos Guestrin. Learning to (learn at test time): Rnns with expressive hidden states. In ICML, 2025. Kimi Team, Yu Zhang, Zongyu Lin, Xingcheng Yao, Jiaxi Hu, Fanqing Meng, Chengyin Liu, Xin Men, Songlin Yang, Zhiyuan Li, Wentao Li, Enzhe Lu, Weizhou Liu, Yanru Chen, Weixin Xu, Longhui Yu, Yejie Wang, Yu Fan, Longguang Zhong, Enming Yuan, Dehao Zhang, Yizhi Zhang, T. Y. Liu, Haiming Wang, Shengjun Fang, Weiran He, Shaowei Liu, Yiwei Li, Jianlin Su, Jiezhong Qiu, Bo Pang, Junjie Yan, Zhejun Jiang, Weixiao Huang, Bohong Yin, Jiacheng You, Chu Wei, Zhengtao Wang, Chao Hong, Yutian Chen, Guanduo Chen, Yucheng Wang, Huabin Zheng, Feng Wang, Yibo Liu, Mengnan Dong, Zheng Zhang, Siyuan Pan, Wenhao Wu, Yuhao Wu, Longyu Guan, Jiawen Tao, Guohong Fu, Xinran Xu, Yuzhi Wang, Guokun Lai, Yuxin Wu, Xinyu Zhou, Zhilin Yang, and Yulun Du. Kimi linear: An expressive, efficient attention architecture, 2025. URL https://arxiv.org/abs/2510.26692. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. Ke Alexander Wang, Jiaxin Shi, and Emily B. Fox. Test-time regression: unifying framework for designing sequence models with associative memory, 2025. URL https://arxiv.org/abs/25 01.12352. Maurice Weber, Daniel Y. Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, Rahul Chalamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher R√©, Irina Rish, and Ce Zhang. Redpajama: an open dataset for training large language models. In NeurIPS, 2024. Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In ICLR, 2015. Songlin Yang and Yu Zhang. FLA: triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/fla-org/flash-lin ear-attention. 20 Fast-weight Product Key Memory Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. In ICML, 2024a. Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length. In NeurIPS, 2024b. Songlin Yang, Jan Kautz, and Ali Hatamizadeh. Gated delta networks: Improving mamba2 with delta rule. In ICLR, 2025. Jianyu Zhang and Leon Bottou. Memory mosaics at scale. In NeurIPS, 2025. URL https://openre view.net/forum?id=IfD2MKTmWv. Jianyu Zhang, Niklas Nolte, Ranajoy Sadhukhan, Beidi Chen, and L√©on Bottou. Memory mosaics. In ICLR, 2025a. Tianyuan Zhang, Sai Bi, Yicong Hong, Kai Zhang, Fujun Luan, Songlin Yang, Kalyan Sunkavalli, William T. Freeman, and Hao Tan. Test-time training done right, 2025b. URL https://arxiv. org/abs/2505.23884. Shu Zhong, Mingyu Xu, Tenglong Ao, and Guang Shi. Understanding transformer from the perspective of associative memory, 2025. Fast-weight Product Key Memory A. Detailed Training Settings FwMLP To rule out the impact of implementation details unrelated to the PKM architecture, we propose baseline model that replaces the PKM in FwPKM with SwiGLU-MLP that maintains three fast-weight matrices (and their biases) for up, gating, and down projection. The baseline, denoted as FwMLP, updates its fast weights by minimizing the MSE loss between its predicted values and target lookahead values at chunk level. Due to its dense nature, we reduce the MSE losses in chunk by averaging over both sample and feature dimensions, and we do not apply the loss aggregation and gradient shaping techniques mentioned in Section 3.2.2. For the same reason, addressing optimization in Section 3.3 is irrelevant too. LaCT We adopt the official implementation of LaCT (Zhang et al., 2025b)5 as strong TTT (Sun et al., 2025) baseline. The LaCT architecture consists of sliding window attention, fast-weight SwiGLU MLP, and slow-weight SwiGLU MLP in every layer. The fast weights are optimized to minimize dot product loss via SGD with momentum or Muon (Jordan et al., 2024), and we found SGD with momentum achieves better performance in our experiments. LaCT uses data-dependent learning rate and L2 weight normalization to improve memorization and retention. One notable difference between LaCT (or more generally TTT) and FwMLP/FwPKM is that LaCT/TTT maintains an individual set of fast weights for each sequence in mini batch, while FwMLP/FwPKM uses shared set of fast weights for all sequences. We compared several configurations of sliding window size (W) and update chunk size (C). Among 512ùê∂ + 512ùëä, 512ùê∂ + 2048ùëä, 512ùê∂ + 4096ùëä, and 2048ùê∂ + 2048ùëä, the best model is 512ùê∂ + 2048ùëä. Table 3 Modeling and training hyper-parameters used in the experiments. Hyperparameter Value General vocab. size # layers hidden dim RMS norm ùúñ Attention head dim # query heads # k/v heads GDN conv. size head dim # heads PKM key dim value dim # heads Top-ùêæ # slots 32000 12 768 0.00001 64 12 4 4 64 8 512 512 4 32 Hyperparameter Value FwPKM key dim value dim # heads Top-ùêæ # slots chunk size addr. loss weight FwMLP input dim intermediate dim output dim LR ùúÇ 512 512 1 8 5122 512 10 512 2304 512 0.1 LaCT sliding window size TTT chunk size 512 2048 Hyperparameter Value Training max. LR min. LR global batch size micro batch size # warmup steps # total steps weight decay 0.001 0.0001 128 8 100 20000 0.1 5https://github.com/a1600012888/LaCT/tree/main/lact_llm Fast-weight Product Key Memory B. Ablation Study To understand the influence of techniques proposed in Section 3, we conduct ablation experiments based on the GDN FwPKM@2,6,10 model. The following variants are trained and evaluated using the same pipelines. w/ 1 head Top-32 uses different Top-ùêæ setting as the name suggests. w/ 4 heads Top-8 uses different multi-head setting as the name suggests. w/o value norm does NOT z-score normalize target values. w/o addr loss does NOT use the marginal entropy loss to update key matrices, instead it uses the MSE loss to update both the keys and value matrices. w/o gating does NOT use ùëîùë°. w/o loss weight uses ùëîùë° but does NOT weight MSE loss with it. w/o lookahead does NOT use lookahead values as MSE targets. In addition, we found that replacing IDW score with dot-product score often results in memory collapsing and loss divergence so do not include it in the ablation study. As shown in Figures 8, 7, and 9, removing lookahead values yields the most significant harm to model performance. Many techniques bring slight PPL improvement, but lead to less healthy memory utility and subsequently worse NIAH accuracies to different extents. Figure 7 Ablation study: perplexity on Fineweb-Edu, LC64, and LAMBADA Figure 8 Ablation study: FwPKM gating value distribution on Fineweb-Edu, LC64, and LAMBADA test sets. Each row represents one FwPKM layer. 23 Fast-weight Product Key Memory Figure 9 Ablation study: stacked bar plots for NIAH accuracy results on 4K-/8K-/32K-/128K-length test sets. Each stacked bar shows the accuracies of {1, 2, 3, 4}-iter NIAH evaluations. C. More Visualization Examples Figure 10 shows the token-level FwPKM gating values for the Introduction section of this paper. 24 Fast-weight Product Key Memory Figure 10 GDN+PKM@6+FwPKM@2,10s FwPKM gating values on tokens from the Introduction section."
        }
    ],
    "affiliations": [
        "Sakana AI"
    ]
}