{
    "paper_title": "HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video",
    "authors": [
        "Hongchi Xia",
        "Chih-Hao Lin",
        "Hao-Yu Hsu",
        "Quentin Leboutet",
        "Katelyn Gao",
        "Michael Paulitsch",
        "Benjamin Ummenhofer",
        "Shenlong Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Digitizing the physical world into accurate simulation-ready virtual environments offers significant opportunities in a variety of fields such as augmented and virtual reality, gaming, and robotics. However, current 3D reconstruction and scene-understanding methods commonly fall short in one or more critical aspects, such as geometry completeness, object interactivity, physical plausibility, photorealistic rendering, or realistic physical properties for reliable dynamic simulation. To address these limitations, we introduce HoloScene, a novel interactive 3D reconstruction framework that simultaneously achieves these requirements. HoloScene leverages a comprehensive interactive scene-graph representation, encoding object geometry, appearance, and physical properties alongside hierarchical and inter-object relationships. Reconstruction is formulated as an energy-based optimization problem, integrating observational data, physical constraints, and generative priors into a unified, coherent objective. Optimization is efficiently performed via a hybrid approach combining sampling-based exploration with gradient-based refinement. The resulting digital twins exhibit complete and precise geometry, physical stability, and realistic rendering from novel viewpoints. Evaluations conducted on multiple benchmark datasets demonstrate superior performance, while practical use-cases in interactive gaming and real-time digital-twin manipulation illustrate HoloScene's broad applicability and effectiveness. Project page: https://xiahongchi.github.io/HoloScene."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 0 6 5 5 0 . 0 1 5 2 : r HoloScene: Simulation-Ready Interactive 3D Worlds from Single Video Hongchi Xia1 Chih-Hao Lin1 Hao-Yu Hsu1 Quentin Leboutet2 Katelyn Gao2 Michael Paulitsch2 Benjamin Ummenhofer2 Shenlong Wang1 1University of Illinois Urbana-Champaign 2Intel Figure 1: Overview of HoloScene: From single input videoalong with visual cues such as segmentation and monocular depthHoloScene reconstructs simulation-ready, interactive 3D digital twin represented as scene graph with complete geometry, physically plausible dynamics, and realistic rendering. The resulting model enables variety of downstream applications, including real-time interactive gaming, 3D editing, immersive experience capture, and dynamic visual effects."
        },
        {
            "title": "Abstract",
            "content": "Digitizing the physical world into accurate simulation-ready virtual environments offers significant opportunities in variety of fields such as augmented and virtual reality, gaming, and robotics. However, current 3D reconstruction and sceneunderstanding methods commonly fall short in one or more critical aspects, such as geometry completeness, object interactivity, physical plausibility, photorealistic rendering, or realistic physical properties for reliable dynamic simulation. To address these limitations, we introduce HoloScene, novel interactive 3D reconstruction framework that simultaneously achieves these requirements. HoloScene leverages comprehensive interactive scene-graph representation, encoding object geometry, appearance, and physical properties alongside hierarchical and interobject relationships. Reconstruction is formulated as an energy-based optimization problem, integrating observational data, physical constraints, and generative priors into unified, coherent objective. Optimization is efficiently performed via hybrid approach combining sampling-based exploration with gradient-based refinement. The resulting digital twins exhibit complete and precise geometry, physical stability, and realistic rendering from novel viewpoints. Evaluations conducted on multiple benchmark datasets demonstrate superior performance, while practical use-cases in interactive gaming and real-time digital-twin manipulation illustrate HoloScenes broad applicability and effectiveness. Project page: here. 39th Conference on Neural Information Processing Systems (NeurIPS 2025)."
        },
        {
            "title": "Introduction",
            "content": "Imagine wanting, decades later, to revisit the home you live in and love todayhow would you capture its memory? Photographs and videos record authentic details but lack immersion; 3D Gaussian splats or photogrammetry can be immersive, yet static chairs and tables feel lifeless. Ideally, we would digitize our environment into fully interactive digital twin: complete, composable, photorealistic, and manipulable just like the real world. Our work takes step toward this goal by enabling users to create in silico twins of their surroundings from single video. Digitizing the physical world into simulation-ready virtual environment offers immense opportunities in augmented and virtual reality, gaming, and robotics. However, despite advances in 3D modeling and scene understanding, key challenges remain: capturing complete geometry and appearance in occluded regions, inferring inter-object relationships, and ensuring physical plausibility and interactivity. Existing Real2Sim methods produce incomplete geometry [88, 45, 71] or unstable physics [77, 75]; existing amodal reconstruction focuses on single-image setting [86, 11], individual objects [76, 15, 39], neglects physical plausibility [49] or relies on asset retrieval [10] sacrificing fidelity and practicality; and prior physically plausible reconstruction [48, 15] is limited to simple objectscene interactions or requires full observations. To address these gaps, we introduce HoloScene, an interactive 3D reconstruction framework that unifies geometry completeness, object completeness, physical plausibility, realistic rendering, and physical interaction. HoloScene represents scene as an interactive scene graph encoding object geometry, appearance, and physical properties in hierarchical structure. We cast scene-graph recovery from video as structured energy-based optimization, integrating observational data, physical constraints, and generative priors into single objective. To solve this challenging problem, we propose novel divide-and-conquer strategy combining sampling-based tree-structured search with gradient-based refinement. The resulting scene models exhibit complete, accurate geometry; stable physical interactions; and realistic rendering from novel viewpoints. Experiments on three challenging benchmarks demonstrate superior geometry accuracy and physical plausibility, with rendering performance comparable to state-of-the-art amodal and physics-aware reconstruction methods. We further showcase HoloScenes versatility through practical applications in interactive gaming, realistic video effects, and real-time digital-twin manipulation."
        },
        {
            "title": "2 Related Works",
            "content": "Method Visual Input Twin Fidelity Physics Capacity Amodal 3D Recon Real-time Rendering Physics Optimization Single Object Scene image image image image video video video video video video video video Differentiable Differentiable ACDC [10] Gen3DSR [11] PhysComp [15] CAST [86] NeRF [45] BakedSDF [88] ObjectSDF++ [75] Video2Game [77] DRAWER [78] PhyRecon [48] DP-Recon [49] HoloScene (Ours) Interactive 3D Scene Model Recent advances in 3D scene modeling [52, 93, 17, 5, 68] reconstruct 3D scene from input images or videos, representing the scene as neural fields [46, 65, 19, 62, 61, 25, 3, 4], signed distance functions (SDF) [71, 87, 51, 53, 88, 91, 29], and 3D Gaussians [23, 20, 92, 90]. While producing realistic renderings from novel views, these works cannot provide 3D assets that allow user interactions (e.g, move the chairs to different poses). Reconstructing realistic and interactive environments from real images and videos remains challenging due to limited observation, occlusion, and physical reasoning. Some previous works reconstruct 3D objects from sparse viewpoints [39, 35, 79, 7, 41], and some estimate physical properties from visual observation [94, 15]. Nevertheless, these works focus on object-level tasks and cannot handle large and complex indoor scenes. PhyRecon [48] optimizes stable 3D scenes with differentiable physical engines, but does not model inter-object interactions and realistic appearance. DP-Recon [49], Video2Game [77], Drawer [78] leverage generative prior [1, 55] and foundation models [59, 60] to reconstruct decomposed 3D scenes. However, these works can only produce limited components or lack physical stability. Text-to-3D generation [83, 33, 72] creates diverse layouts from text prompts, but fails to reconstruct digital twins that are faithful to visual inputs. In this work, we propose to reason Table 1: Comparison of Interactive 3D Scene Models. Single Object Single Object Objects-Ground Differentiable Diff & Sampling Scene 2 object interaction with the scene graph, and utilize generative priors and novel sampling strategy to reconstruct the geometry and appearance of every component, constructing realistic, physically plausible, and interactable 3D environments. Data-driven Simulation Simulation plays pivotal role across robotics, self-driving, and content creation, but building high-fidelity virtual scenes remains costly, and the sim-to-real gap poses great challenges. To address this, data-driven simulation [2, 8, 40, 44, 63, 84] has emerged, enabling the modeling of physical dynamics [38, 28, 80, 21, 13, 22, 95], lighting conditions [32, 58, 31], and actionconditioned outcomes [21, 36, 6, 77, 22, 37, 78, 42], directly from real-world data. These methods have also been applied in robot learning [8, 44, 57, 82, 85, 84], LiDAR simulation [34, 44, 81, 84, 99, 98], and interactive media [18, 77]. In robotics, related real-to-sim approaches [9, 10, 67, 70, 27, 43] reconstruct interactable environments from the real world for reproducible embodiment. However, they still lack physical realism. Recent works [48, 6, 97] leverage differentiable physics or priors in reconstruction, but they neglect complex inter-object relationships. The closest work to ours is CAST [86], which also considers physical plausibility in scene reconstruction. However, our proposed HoloScene differs fundamentally in three key aspects: (1) Problem formulation: CAST takes single-image generative approach where fidelity to input appearance and geometry is not enforced, while HoloScene performs multi-view joint optimization from videos, explicitly ensuring faithful reconstruction of observed scenes through inverse neural rendering. (2) Physical stability: CASTs differentiable optimization only avoids penetrations but cannot guarantee long-term stability. HoloScene employs simulator-in-the-loop optimization with Isaac Sim [47] to ensure objects remain physically stable over time. (3) Inference strategy: CAST follows multi-step pipeline with greedy sequential optimization, whereas HoloScene adopts unified energy-based formulation (Eq. 2) that jointly optimizes observation fidelity, physical plausibility, and object completion through generative sampling and scoring. We compare HoloScene with prior works in Tab. 1."
        },
        {
            "title": "3 Method",
            "content": "Given the observations = {Ot}T t=0, which include the input video sequence {It}, camera poses {ξt} (inferred or ground truth), and instance masks {Mt} (inferred or ground truth), our goal is to reconstruct realistic, complete, and physically plausible digital twin of the input scene, yielding interactive, sim-ready assets compatible with simulators and game engines, and which can be used to generate novel visual content. To this end, we represent the scene as an interactive 3D scene graph representation that encodes object geometry, appearance, physical properties, and hierarchical inter-object relationships (Sec. 3.1). We combine observational evidence, generative priors for shape completion, and physical simulation for stability to formulate scene-graph recovery as an energy minimization problem (Sec. 3.2). Finally, we propose an inference method that integrates sampling-based tree search with differentiable optimization (Sec. 3.3). Fig.2 summarizes our approach."
        },
        {
            "title": "3.1 Scene Representation",
            "content": "We represent the scene as an interactive 3D scene graph = (V, E). Each node vi = {vi}N i=0 represents either the background scene or one of the objects present. node vi = (gi, fi, pi, Ti) is comprised of geometry gi, appearance fi, physical properties pi, and dynamic states Ti. Each edge ei,j = (vi, vj) encodes an objectobject relationship in G. Geometry: We represent the geometry of each node vi in the scene with an instance-level neural SDF gi(x; θ) : R3 R, where R3 is any point in space and θ are learnable parameters. Additionally, to facilitate physical simulation and efficient rendering, we maintain mesh representation Mi = MarchingCube(gi) for each object, extracted from its SDF using the marching cubes algorithm. Appearance: For each object vi, we encode appearance fi = (ci, αi, µi, Σi) as Gaussian splats, enabling real-time, high-quality rendering. ci, αi, µi, Σi are color, opacity, mean, and covariance of Gaussians, respectively. Gaussians capture finer detail than colored meshes but hinder consistency between appearance, geometry, and simulation; following recent work [78], we adopt Gaussianson-Mesh (GoM) approach and attach each splat to its mesh to ensure alignment and enable physical interactions. Given camera intrinsics and extrinsics ξ, we denote the splat-rendered RGB images, masks, depth and normal maps as I, M, D, = SplatRender(G; K, ξ). 3 Figure 2: Overview of HoloScene Optimization Stages: Given multiple posed images as well as some visual cues (instance masks, monocular geometry priors), we first employ gradient-based optimization as the initialization. Then we adopt generative sampling and tree search strategy along the topology of the scene graph to obtain the complete geometry with physical plausibility. Finally, the final fine-tuning over the scene further enhances the realism of the reconstructed scene. Physics: Each object in our scene graph is modeled as rigid body. Its physical parameters pi = (mi, κi, ζi, ri) comprise mass mi, friction κi (resistance to sliding against other surfaces), damping ζi (energy dissipation during motion), and restitution ri (elasticity upon impact). These parameters are used in downstream physical simulations to model the objects response to external forces and its interactions with other objects and the background scene. Object States: All object intrinsic attributes above are defined in object-centric coordinates and remain invariant under motion. To handle dynamic changes, we encode each objects rigid body state by rigid transform Ti from its object-centric frame to the world frame. During static reconstruction, Ti is fixed; in dynamic simulation, it may vary over time. Let = {Ti}N i=0 denote the set of all object states, GT the scene graph under those states, and the scene graph under the static state during reconstruction. Object Relationships: Each edge ei,j links two nodes with one of three relationships: 1) support, where vi rests in stable equilibrium on its unique parent vpa(i) under gravity (each object has exactly one such parent, so support edges form tree in the static scene graph); 2) beside, where siblings (pa(j) = pa(i)) have touching surfaces, causing occlusions without hierarchy or instability; and 3) collide, where contacts with nonzero momentum yield dynamic effectsignored during static reconstruction but employed in simulation. Note that the object relationship might change depending on its dynamic status during simulation. Interaction & Simulation: Our 3D scene graphs distinguishing feature is its support for physical interactions. Formally, at time t, given the dynamic scene graph GT with current object states as well as an input action at, the next states are computed as t+1 = Sim(T t, at; GT t), (1) where Sim is rigid-body physical simulator using the mesh {Mi} as collision geometry. Here, at can represent external inputs forces, torques, or control actionsapplied to the objects at time t."
        },
        {
            "title": "3.2 Problem Formulation",
            "content": "Our framework takes input observations of static scene and recovers the scene graph = (V, E). The resulting scene graph must (i) explain the observations well; (ii) be geometrically complete and plausible; and (iii) reflect the scenes static, physically stable nature. To this end, we cast the problem as structured energy-minimization problem: min Ergb(I, G) + Emask(M, G) + Emono(D, G) (cid:123)(cid:122) (cid:125) (cid:124) observation terms + Ecomp(G) + Egeo(G) + Ephysics(G) (cid:123)(cid:122) (cid:125) regularization terms (cid:124) . (2) For simplicity, we omit the hyperparameter linear weights for each term. Next, we discuss each energy term. Observation Terms: The observation terms quantify the discrepancy between the reconstructed 3D scene and the input observations. Let ˆIt, ˆMt, ˆDt, ˆNt = SplatRender(G; K, ξt) denote the rendered RGB image, instance mask, depth map, and normal map at camera pose ξt. We then define three energy terms: the RGB energy Ergb(I, G) = (cid:80) LMSE(ˆIt, It) + LLPIPS(ˆIt, It), where It is the ground-truth color image and the loss combines MSE and LPIPS losses [96]; the mask energy Emask(M, G) = (cid:80) CE( ˆMt, Mt), where CE is cross-entropy and Mt is either given labeled mask [64, 89] or one inferred via segmentation tracking [24]; and the monocular geometry energy Emono(D, G) = (cid:80) 2 + Lnorm( ˆDt, Dt), where Nt and Dt are monocular normal and depth priors and Lnorm is the scaleand shift-invariant L2 loss [91]. ˆNt Nt2 Regularization Terms: Because videos only partially observe 3D scene, optimizing observations alone cannot yield complete, plausible, and physically valid reconstruction; we therefore impose generative, geometric, and physical priors as regularizers to enable fully interactive 3D scenes. The completeness energy Ecomp encourages complete reconstruction of each objects shape despite the partial observations. Inspired by generative image-to-3D methods [39], for each object we synthesize virtual observations Oi = {Ii, Di, Mi, Ni} by shooting it from multiple virtual viewpoints with pretrained multi-view diffusion model Wonder3D [39]. Unlike the single object setting for most image-to-3D works, because our complex scenes often feature inter-object occlusions (e.g., sofa covered by blanket), we first inpaint occluded regions using LaMa [66] before generating these views. Given the synthesized observations, we define the completeness energy as Ecomp = (cid:88) (cid:0)Emask(Ii, {vi}) + Ergb( Mi, {vi}) + Emono( Di, {vi})(cid:1), (3) where Ergb, Emono, Emask are the observation losses defined similarly in our observation terms, although they are measured at virtual viewpoints here. The geometry energy Egeo ensures geometry compatibility between each object, such that their geometry does not intersect with each other: Egeo(G) = (cid:88) (Epene_sdf (gi; G) + Epene_mesh(gi; G)) . (4) k=i max(cid:0)0, gk(x) gi(x)(cid:1) ensures no two The SDF-penetration term Epene_sdf = (cid:80) (cid:80) object SDFs overlap, where R(i) = {x R3 arg mink gk(x) = i} is the set of points belong to instance i. Intuitively, if lies in instance i, then for any other instance k, gk(x) gi(x) must hold to prevent intersections. Similarly, each objects mesh should not intersect with any other object mesh. This can be measured by measuring whether intersecting two meshes resulting empty set or not: Epene_mesh = 1(inter(Mi, Mj) = ). xRi Finally, it is important to ensure that our recovered digital twin of the scene is simulatable; hence, physical plausibility is crucial. To this end, we introduce physics energy, which measures physical plausibility via two terms: Ephysics = Estable + Etouch = Diff(cid:0)T , Sim(T , agravity; G)(cid:1) + (cid:88) (i,j)E dist(Mi, Mj) . (5) The stable term Estable(G) = Diff(cid:0)T , Sim(T , agravity; G)(cid:1) quantifies translational and rotational deviations of each object, with Diff(T , ) = (cid:80) i)) and Sim is i) + rad(T1 i(trans(T1 i 5 N/A Reference Image + + j o y n R - r o - r r o c a A Whole Object 1 Object 2 Object 3 Figure 3: Qualitative Comparisons on Object Geometry and Appearance Reconstruction: Our method delivers superior reconstructions by smoothly inpainting occluded regions with LaMa and completing invisible back-facing geometry with Wonder3D. Unlike baselines, our approach eliminates object interpenetration, ensuring physical stability during simulation. the forward physical simulator step as defined in Eq. 1; low Estable indicates static equilibrium under gravity, i.e. scene remains static in the simulator. The touch term Etouch = (cid:80) (i,j)E dist(Mi, Mj) encourages each supporting pair (i, j) to make contact, dist is the Chamfer distance between meshes. 3."
        },
        {
            "title": "Inference",
            "content": "Optimizing the scene graph from Sec. 3.2 is challenging because it mixes discrete variables (graph topology, objectobject relations) with continuous ones (neural SDFs, Gaussians, and physical parameters) and includes non-differentiable terms like physical stability. We therefore use three-stage divide-and-conquer approach: first, recover initial geometry and appearance by minimizing the observation terms; next, refine shapes and physical parameters by minimizing the geometry and physics terms through generative sampling combined with structured tree search; and finally, fine-tune appearance by re-minimizing observation terms. This yields fully plausible, interactive 3D scene  (Fig. 2)  . Stage 1: Gradient-based Optimization: We first optimize each object nodes appearance ai and geometry gi to match the observations via gradient-based optimization. Specifically, we minimize the observation terms plus SDF-penetration regularization through differentiable volume renderingsimilar to neural SDF methods [75, 74, 30]to obtain per-instance SDFs gi. Additionally, we recover small objects by balancing training samples across all instances. We then extract initial meshes Mi via marching cubes and refine each objects Gaussians fi via splat rendering and RGB rendering, mask, and monocular geometry losses, yielding our dual scene representation per each instance [16]. Stage 2: Sampling-based Optimization: The Stage 1 scene model supports freeview rendering and accurate visible-region geometry but remains incomplete, nonphysical, and noninteractive. Directly minimizing Ecomplete, Ephysics, and Egeo, however, is challenging due to complex highorder interactions (e.g., multi-object physical interaction), intrinsic multi-modality (invisible regions admit multiple solutions), and non-differentiable components (e.g., mesh intersections, physics simulations). To address this, we adopt an approach that combines the diverse proposal capability of 6 generative sampling with the combinatorial optimization strength of tree search to minimize our structured objective. Scene graph edges creation: After the gradient-based optimization, our framework infers the topology of the scene graph from the instance meshes extracted from neural SDF, where edges encode support relations in tree rooted at v0 (the background, e.g., the room). We build this tree by analyzing physical contact and support relationships between objects: each object is assigned parent based on which surface provides its primary support, as determined by the geometry and orientation of contact surfaces. We begin by identifying objects resting directly on the background, then recursively process the remaining objects in an order that respects the physical dependency structureobjects that support others are registered before those they support. Starting from v0, we repeat until all observed instances have been added to the tree. Generative sampling: We begin by sampling diverse, complete shapes for each instance: we prompt Wonder3Ds multi-diffusion model with real-world observations and generate virtual views Ii from various viewpoints. Thanks to its generative nature, resampling multiple times with different seeds yields diverse virtual observations even from the same viewpoints. Then, for each virtual observation, we minimize Ecomp independently, producing diverse set of 3D shape candidates per object. Structured tree search: We have generated multiple complete shape samples per instance that all fit the observations, but it remains unclear which combination is most physically plausible. Exhaustively evaluating every combination is impractical, since the physical-plausibility energy Ephysics entails high-order combinatorial optimization. To address this, we perform tree search over our generative samples. Starting at the root node, we traverse each node in breadth-first order; at each active node (object), we evaluate Ephysics for all samples and retain the sample with the lowest energy among those evaluated. We then adjust its state and physical parameters to enforce stability and prevent interpenetration (see details in the supplementary material). Remark: The key novelty and advantage of the proposed inference algorithm is the combination of generative sampling with structured tree search for amodal, physically plausible reconstruction. Unlike scene-level amodal methods [10] that rely on asset retrieval, our sampling is asset-free and generates input consistent, diverse shape hypotheses. Unlike prior simulation-verification methods [49, 48], which only enforces objectground consistency, our tree search ensures global stability along every support chain. By driving non-differentiable simulator (e.g., IsaacSim) end-to-end, we eliminate any reconstruction-to-deployment gap. Stage 3: Gradient-based Refinement Since Stage 2 adjusts object states, physical parameters, and shape, it is necessary to further refine the Gaussians attached to the surface to ensure complete and realistic appearance. To this end, we fine-tune Gaussians for all objects using splat rendering by minimizing the observation terms via gradient descent. This yields our final scene graph."
        },
        {
            "title": "4 Experiments",
            "content": "Dataset: We conduct the experiments across multiple datasets: 3 scenes from Replica [64], 3 scenes from Scannet++ [89], 2 scenes from iGibson [26], and one self-captured scene. The Replica and Scannet++ datasets cover diverse indoor structures and lighting conditions, and the iGibson dataset offers complete geometry of every object, allowing per-object reconstruction evaluation. Instance masks are provided by dataset annotations or estimated with SAM [24]. Metrics: We evaluate geometry quality with Chamfer Distance (CD), F-Score (F1), and Normal Consistency (NC) [91], and assess rendering quality using PSNR, SSIM, and LPIPS. For physical evaluation, we adopt consistent physical parameters, put all scene components in the Isaac Sim [47], and measure stability with translation/rotation changes when gravity is applied. The stability ratio is calculated as: Stable % = #Stable Instances , where instances are identified as stable if changes are under #All Instances certain threshold. We also report the object reconstruction ratio: OR% = #Reconstructed Instances , which quantifies how many objects are present in the final reconstruction, regardless of their completeness. #All Instances Baselines: We evaluate our framework against SOTA approaches in instance-aware amodal 3D scene reconstruction. ObjectSDF++ [75] uses per-instance SDF for scene representation. PhyRecon [48] extends instance-aware scene reconstruction by incorporating differentiable physical loss to 7 Reference Image t e i p c h i e r p Objectsdf++ PhyRecon DP-Recon Ours DP-Recon (no physics) DP-Recon (with physics) Ours (no physics) Ours (with physics) Figure 4: Qualitative Comparisons on Physical Simulation: We compare geometry layouts and appearance before and after physical simulation, with the table geometry reconstructions highlighted in inset figures. HoloScenes complete, non-interpenetrating geometry remains stable in physics simulators, unlike baseline methods. Our Gaussian on mesh delivers high-quality, real-time rendering throughout the simulation process. Method ObjSDF++ PhyRecon DP-Recon Ours l + ObjSDF++ + PhyRecon DP-Recon Ours ObjSDF++ PhyRecon DP-Recon Ours i ObjSDF++ PhyRecon DP-Recon Ours b e S j Geometry Rendering Physics CD F1 NC PSNR SSIM LPIPS 6.72 4.52 3.45 4.05 25.20 31.16 22.96 21.93 12.33 11.27 30.31 12.00 3.52 5.47 5.81 3.17 64.36 71.07 87.66 83. 70.71 39.57 65.48 63.11 38.64 45.49 21.89 34.15 79.03 70.71 61.31 81.31 88.53 92.06 94.23 92.21 87.15 82.28 87.13 88.09 83.74 83.85 70.81 82. 75.30 71.89 70.61 78.13 29.12 23.19 22.10 27.82 27.46 22.32 21.44 25.88 29.60 27.40 21.94 25.88 11.03 8.92 13.90 16.55 0.851 0.764 0.728 0. 0.887 0.791 0.715 0.873 0.891 0.860 0.728 0.854 0.571 0.609 0.770 0.863 0.355 0.434 0.420 0.304 0.292 0.432 0.466 0.268 0.299 0.333 0.432 0. 0.134 0.250 0.301 0.185 Real Time OR% Stable (Ground) % Stable (All) % 98.6 77.5 56.3 100.0 96.5 92.9 90.6 100.0 65.0 62.9 74.2 100.0 65.0 62.9 74.2 100. 78.3 56.5 21.7 95.7 81.6 67.3 20.0 93.9 44.2 45.3 16.3 74.4 44.2 45.3 16.3 74.4 39.4 5.6 8.5 81.7 28.2 9.4 9.4 70. 36.1 5.2 4.1 71.1 36.1 5.2 4.1 71.1 Table 2: Quantitative Results on Scene Reconstruction: HoloScenes generative sampling and scene graph-based tree search produce the most physically plausible reconstructions while preserving high-quality geometry and supporting real-time and realistic rendering. We highlight the best and second-best methods with distinct colors. For rendering quality, we only compare with DP-Recons texture mesh-based rendering since ObjSDF++ and PhyRecon lack real-time rendering capabilities. optimize unstable objects. However, it only handles objectground interactions and does not support hierarchical or interobject relationships. DP-Recon [49] incorporates diffusion Score Distillation Sampling (SDS) [55] for amodal sparse-view reconstruction. However, it does not handle inter-object occlusions. All three methods use instance-level SDFs, so the SDF values of different instances might interfere with one another. We adopt their open-source codes and adapt them for the testing benchmarks. Please refer to the supplementary material for more implementation details. Implementation details: Our inference pipeline consists of three stages. Stage 1 employs gradientbased optimization for 100k steps with loss weights λrgb = 1.0, λmask = 0.5, λdepth = 0.5, and λnormal = 0.1. Stage 2 uses sampling-based optimization with λrgb = 2.0, λmask = 0.5, λdepth = 10.0, λnormal = 10.0, λpene = 5.0, generating three samples per instance. Stage 3 refines Gaussians via gradient-based optimization with λt1 = 0.95 and λssim = 0.05. The optimization takes approximately 4 hours, 4 hours, and 20 minutes for stages 1, 2, and 3, respectively, on single A6000 GPU. We utilize Marigold [14] for monocular depth and normal estimation. Level Dataset Scene Replica Object iGibson TexMesh / GoM TexMesh GoM GoM GoM GoM GoM Physical Energy Scene Graph CD F1 NC PSNR SSIM LPIPS OR% 3.50 3.50 4.32 4.05 4.20 3.17 88.00 88.00 80.19 83.21 79.22 81.31 92.30 92.30 92.10 92. 76.96 78.13 26.45 28.30 27.14 27.82 13.74 16.55 0.810 0.845 0.839 0.849 0.735 0.863 0.329 0.349 0.334 0. 0.204 0.185 100.0 100.0 100.0 100.0 100.0 100.0 Stable (All) % 43.7 43.7 69.0 81.7 41.2 71. Table 3: Ablation Study on Model Design: We observe improved physical stability and object-level reconstruction with the help of generative priors, physical energy, and scene graph representation, and there is trade-off between scene-level reconstruction and instance-level physical stability."
        },
        {
            "title": "4.1 Experimental Results",
            "content": "Scene-level evaluations: We first evaluate the reconstruction at the scene level. In Tab. 2, we compare our framework with the three baselines across Replica, Scannet++. and iGibson dataset, with qualitative results shown in Fig. 3 and Fig. 4. Our method achieves the best physical reconstruction results in terms of both object reconstruction ratio and stable object ratio, while maintaining comparable scene-level reconstruction quality in geometry and appearance. Unlike baselines [75, 48, 50] where small objects often disappear due to SDF interference from larger adjacent objects, our framework benefits from balancing training samples across all instances during the optimization stage 1, recovering all instances in the scenes. PhyRecon assumes that all objects rest directly on the ground, which damages geometry when objects are supported by other objects. DP-Recon prioritizes completeness via SDS over physical stability, failing to adequately address object interpenetration. In contrast, our sampling-based optimization and completion approach yields the most physically stable results. Object-level evaluations: We evaluate object-level reconstruction using the iGibson dataset, which provides complete ground truth geometry and appearance for each object. This evaluation is especially challenging as it tests the reconstruction of occluded regions that models never observe. Our evaluation compares reconstructed geometry directly with ground truth and renders 6 viewpoints around each object to assess appearance quality. Results in Tab. 2 demonstrate that our method outperforms all baselines in reconstructing invisible and occluded regions, validating our frameworks effectiveness in completing objects beyond directly observed surfaces. Ablation study: Our ablation study (Tab. 3) reveals the contribution of each component. Switching from textured mesh to Gaussian rendering improves visual quality, while adding physics energy with generative priors enhances physical stability. The scene graph inter-object relationships further improve physics performance by better reconstructing occluded regions, especially those from support relationships. However, we identify trade-off between scene-level reconstruction accuracy and physical stability, where optimizing for physical plausibility may occasionally compromise pixel-alignment with original observations. Failure rate analysis: We conduct statistical analysis of failure modes in geometry and physics reconstruction. In Tab. 4, we compare HoloScene against reconstruction-based baselines [75, 48, 50] across geometry failures on iGibson and physics failures on all three datasets, with failure criteria defined by F-score thresholds (70.0) and simulation stability metrics. Geometry failures include invalid surfaces, partial and oversized shapes, while physics failures arise from invalid shapes incompatible with simulators, overlooked object-object contacts, inter-object penetration causing repelling forces, and inaccurate contact modeling leading to positional drift. Our method achieves the lowest failure rates in both categories. Unlike ObjSDF++ [75] and PhyRecon [48] which lack shape priors and produce invalid geometry, our generative priors eliminate all invalid geometry failures through reliable structural guidance and balanced sampling. DP-Recon [50] emphasizes visual completeness over physical constraints, resulting in high penetration failures. Additionally, PhyRecons ground-contact assumption overlooks contact pairs that can destabilize the simulation. In contrast, we integrate generative priors with physics-aware constraints directly into the reconstruction process, which substantially reduces both invalid shapes and inter-object penetrations, demonstrating the effectiveness of physical consistency in the optimization. 9 Method ObjSDF++ PhyRecon DP-Recon Ours Geometry Failure (iGibson) Physics Failure Rate (%) Physics Failure Breakdown (All Datasets) Total # Fail # Invalid # Partial # Oversized # Replica ScanNet++ iGibson Total # Invalid # Overlap # Penetration # Drifting # 97 97 97 97 52 64 70 24 34 36 25 0 13 21 33 13 5 7 12 60.6 94.4 91.5 18.3 71.8 90.6 90.6 29.4 53.6 66.0 72.2 24.7 235 235 235 235 38 58 64 0 0 121 0 76 33 102 6 52 24 69 60 Table 4: Quantitative Failure Rate Comparison: HoloScene achieves significantly lower geometry and physics failure rates compared to reconstruction-based baselines, demonstrating more robust 3D reconstruction and physically plausible scene modeling. 4."
        },
        {
            "title": "Interactive Environment Applications",
            "content": "Real-Time Interactive Game With our reconstructed environment, we can create real-time interactive game with Unreal Engine [12]. As Fig. 1 shows, we build third-person game with the reconstructed texture meshes. The objects could be physically rearranged in the game world, and the game agent could also interact with the scene through realistic physics. Interactive 3D Editing In our simulation environment, we could also achieve high-quality interactive 3D editing by moving the object Gaussians with its underlying physical mesh geometry. In Fig. 1, we demonstrate this by changing the location and orientation of the interactable chair. Immersive Experience Recording We show our interactable reconstructed 3D objects with immersive experience recording. Given static RGB video of person manipulating an object, we aim to recover the objects 6D pose and resimulate its motion in virtual 3D scene. We recover the camera pose with VGGT [69], adjust the predicted depth [54] to align with the virtual scene, and adopt FoundationPose [73] for object tracking with our reconstructed 3D object for model-based 6D pose estimation. As shown in Fig. 1, we enable consistent replay of real-world interactions in virtual scenes while accurately recovering the objects pose from visual input. Figure 5: Dynamic VFX Results. We augment the inferred interactive 3D scene with various visual effects such as dropping objects, adding animations, and fires. To enhance immersion, we augment the scene with dynamic visual effects, including rigid body simulations, character animations, and particle effects. We adopt visual effects from AutoVFX [18] to overlay virtual content and shadows onto the image. As Fig. 4 shows, we produce effects that blend naturally with the scene."
        },
        {
            "title": "5 Conclusion & Limitation",
            "content": "We presented HoloScene, novel interactive 3D modeling framework that uses scene graphs and energy-based optimization to reconstruct environments with realistic appearance, complete geometry, and interactive physical plausibility, achieving superior real-time rendering, geometric accuracy, and stable simulation. Limitations: HoloScene currently only handles videos of static indoor scenes; dynamic scenes and large outdoor environments remain challenging. Future work will focus on relightable reconstruction and extending support to articulated and deformable objects. Acknowledgment: This project is supported by the Intel AI SRS gift and NSF Awards #2331878, #2340254, #2312102, #2414227, and #2404385. We greatly appreciate the NCSA for providing computing resources through Delta and Delta AI program."
        },
        {
            "title": "References",
            "content": "[1] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023) [2] Amini, A., Wang, T.H., Gilitschenski, I., Schwarting, W., Liu, Z., Han, S., Karaman, S., Rus, D.: Vista 2.0: An open, data-driven simulator for multimodal sensing and policy learning for autonomous vehicles. In: ICRA (2022) [3] Barron, J.T., Mildenhall, B., Tancik, M., Hedman, P., Martin-Brualla, R., Srinivasan, P.P.: Mip-nerf: multiscale representation for anti-aliasing neural radiance fields. In: ICCV (2021) [4] Barron, J.T., Mildenhall, B., Verbin, D., Srinivasan, P.P., Hedman, P.: Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In: CVPR (2022) [5] Berger, M., Tagliasacchi, A., Seversky, L.M., Alliez, P., Levine, J.A., Sharf, A., Silva, C.T.: State of the art in surface reconstruction from point clouds. In: 35th Annual Conference of the European Association for Computer Graphics, Eurographics 2014-State of the Art Reports. The Eurographics Association (2014) [6] Chen, B., Jiang, H., Liu, S., Gupta, S., Li, Y., Zhao, H., Wang, S.: Physgen3d: Crafting miniature interactive world from single image. arXiv preprint arXiv:2503.20746 (2025) [7] Chen, Y., Ni, J., Jiang, N., Zhang, Y., Zhu, Y., Huang, S.: Single-view 3D scene reconstruction with high-fidelity shape and texture. In: 2024 International Conference on 3D Vision (3DV). IEEE (2024) [8] Chen, Y., Rong, F., Duggal, S., Wang, S., Yan, X., Manivasagam, S., Xue, S., Yumer, E., Urtasun, R.: Geosim: Realistic video simulation via geometry-aware composition for self-driving. In: CVPR (2021) [9] Chen, Z., Walsman, A., Memmel, M., Mo, K., Fang, A., Vemuri, K., Wu, A., Fox, D., Gupta, A.: Urdformer: pipeline for constructing articulated simulation environments from real-world images. arXiv (2024) [10] Dai, T., Wong, J., Jiang, Y., Wang, C., Gokmen, C., Zhang, R., Wu, J., Fei-Fei, L.: Acdc: Automated creation of digital cousins for robust policy learning. arXiv (2024) [11] Dogaru, A., Özer, M., Egger, B.: Generalizable 3d scene reconstruction via divide and conquer from single view. arXiv preprint arXiv:2404.03421 (2024) [12] Epic Games: Unreal engine https://www.unrealengine.com [13] Feng, Y., Feng, X., Shang, Y., Jiang, Y., Yu, C., Zong, Z., Shao, T., Wu, H., Zhou, K., Jiang, C., et al.: Gaussian splashing: Dynamic fluid synthesis with gaussian splatting. arXiv e-prints pp. arXiv2401 (2024) [14] Garcia, G.M., Abou Zeid, K., Schmidt, C., De Geus, D., Hermans, A., Leibe, B.: Fine-tuning imageconditional diffusion models is easier than you think. In: 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). pp. 753762. IEEE (2025) [15] Guo, M., Wang, B., Ma, P., Zhang, T., Owens, C., Gan, C., Tenenbaum, J., He, K., Matusik, W.: Physically compatible 3d object modeling from single image. Advances in Neural Information Processing Systems 37, 119260119282 (2024) [16] Guédon, A., Lepetit, V.: Sugar: Surface-aligned gaussian splatting for efficient 3d mesh reconstruction and high-quality mesh rendering (2023) [17] Han, X.F., Laga, H., Bennamoun, M.: Image-based 3d object reconstruction: State-of-the-art and trends in the deep learning era. IEEE transactions on pattern analysis and machine intelligence 43(5), 15781604 (2019) [18] Hsu, H.Y., Lin, Z.H., Zhai, A., Xia, H., Wang, S.: Autovfx: Physically realistic video editing from natural language instructions. arXiv preprint arXiv:2411.02394 (2024) [19] Hu, S., Zhou, K., Li, K., Yu, L., Hong, L., Hu, T., Li, Z., Lee, G.H., Liu, Z.: Consistentnerf: Enhancing neural radiance fields with 3d consistency for sparse view synthesis. arXiv preprint arXiv:2305.11031 (2023) [20] Huang, B., Yu, Z., Chen, A., Geiger, A., Gao, S.: 2d gaussian splatting for geometrically accurate radiance fields. In: SIGGRAPH. Association for Computing Machinery (2024) [21] Jiang, H., Hsu, H.Y., Zhang, K., Yu, H.N., Wang, S., Li, Y.: Phystwin: Physics-informed reconstruction and simulation of deformable objects from videos. arXiv preprint arXiv:2503.17973 (2025) 11 [22] Jiang, Y., Yu, C., Xie, T., Li, X., Feng, Y., Wang, H., Li, M., Lau, H., Gao, F., Yang, Y., et al.: Vr-gs: physical dynamics-aware interactive gaussian splatting system in virtual reality. In: ACM SIGGRAPH 2024 Conference Papers. pp. 11 (2024) [23] Kerbl, B., Kopanas, G., Leimkühler, T., Drettakis, G.: 3d gaussian splatting for real-time radiance field rendering. TOG (2023) [24] Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. In: ICCV (2023) [25] Kwak, M.S., Song, J., Kim, S.: Geconerf: Few-shot neural radiance fields via geometric consistency. arXiv preprint arXiv:2301.10941 (2023) [26] Li, C., Xia, F., Martín-Martín, R., Lingelbach, M., Srivastava, S., Shen, B., Vainio, K., Gokmen, C., Dharan, G., Jain, T., et al.: igibson 2.0: Object-centric simulation for robot learning of everyday household tasks. arXiv preprint arXiv:2108.03272 (2021) [27] Li, X., Li, J., Zhang, Z., Zhang, R., Jia, F., Wang, T., Fan, H., Tseng, K.K., Wang, R.: Robogsim: real2sim2real robotic gaussian splatting simulator. arXiv preprint arXiv:2411.11839 (2024) [28] Li, Y., Lin, Z.H., Forsyth, D., Huang, J.B., Wang, S.: Climatenerf: Physically-based neural rendering for extreme climate synthesis. arXiv (2022) [29] Li, Z., Müller, T., Evans, A., Taylor, R.H., Unberath, M., Liu, M.Y., Lin, C.H.: Neuralangelo: High-fidelity neural surface reconstruction. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 84568465 (2023) [30] Li, Z., Lyu, X., Ding, Y., Wang, M., Liao, Y., Liu, Y.: Rico: Regularizing the unobservable for indoor compositional reconstruction. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 1776117771 (2023) [31] Lin, Z.H., Huang, J.B., Li, Z., Dong, Z., Richardt, C., Li, T., Zollhöfer, M., Kopf, J., Wang, S., Kim, C.: Iris: Inverse rendering of indoor scenes from low dynamic range images. arXiv preprint arXiv:2401.12977 (2024) [32] Lin, Z.H., Liu, B., Chen, Y.T., Forsyth, D., Huang, J.B., Bhattad, A., Wang, S.: Urbanir: Large-scale urban scene inverse rendering from single video (2023) [33] Ling, L., Lin, C.H., Lin, T.Y., Ding, Y., Zeng, Y., Sheng, Y., Ge, Y., Liu, M.Y., Bera, A., Li, Z.: Scenethesis: language and vision agentic framework for 3d scene generation. arXiv preprint arXiv:2505.02836 (2025) [34] Liu, J.Y., Chen, Y., Yang, Z., Wang, J., Manivasagam, S., Urtasun, R.: Real-time neural rasterization for large scenes. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 84168427 (2023) [35] Liu, R., Wu, R., Van Hoorick, B., Tokmakov, P., Zakharov, S., Vondrick, C.: Zero-1-to-3: Zero-shot one image to 3d object (2023) [36] Liu, S., Ren, Z., Gupta, S., Wang, S.: Physgen: Rigid-body physics-grounded image-to-video generation. In: European Conference on Computer Vision. pp. 360378. Springer (2024) [37] Liu, Y., Jia, B., Lu, R., Ni, J., Zhu, S.C., Huang, S.: Building interactable replicas of complex articulated objects via gaussian splatting. arXiv preprint arXiv:2502.19459 (2025) [38] Liu, Z., Zhou, G., He, J., Marcucci, T., Li, F.F., Wu, J., Li, Y.: Model-based control with sparse neural dynamics. Advances in Neural Information Processing Systems 36, 62806296 (2023) [39] Long, X., Guo, Y.C., Lin, C., Liu, Y., Dou, Z., Liu, L., Ma, Y., Zhang, S.H., Habermann, M., Theobalt, C., et al.: Wonder3d: Single image to 3d using cross-domain diffusion. arXiv preprint arXiv:2310.15008 (2023) [40] Lu, F., Xu, Y., Chen, G., Li, H., Lin, K.Y., Jiang, C.: Urban radiance field representation with deformable neural mesh primitives (2023) [41] Lu, R., Chen, Y., Ni, J., Jia, B., Liu, Y., Wan, D., Zeng, G., Huang, S.: Movis: Enhancing multi-object novel view synthesis for indoor scenes. arXiv preprint arXiv:2412.11457 (2024) [42] Luo, R., Geng, H., Deng, C., Li, P., Wang, Z., Jia, B., Guibas, L., Huang, S.: Physpart: Physically plausible part completion for interactable objects. arXiv preprint arXiv:2408.13724 (2024) 12 [43] Ma, S., Du, W., Yu, C., Jiang, Y., Zong, Z., Xie, T., Chen, Y., Yang, Y., Han, X., Jiang, C.: Grip: general robotic incremental potential contact simulation dataset for unified deformable-rigid coupled grasping. arXiv preprint arXiv:2503.05020 (2025) [44] Manivasagam, S., Wang, S., Wong, K., Zeng, W., Sazanovich, M., Tan, S., Yang, B., Ma, W.C., Urtasun, R.: Lidarsim: Realistic lidar simulation by leveraging the real world. In: CVPR (2020) [45] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. In: ECCV (2020) [46] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. ACM Communications (2021) [47] Mittal, M., Yu, C., Yu, Q., Liu, J., Rudin, N., Hoeller, D., Yuan, J.L., Singh, R., Guo, Y., Mazhar, H., Mandlekar, A., Babich, B., State, G., Hutter, M., Garg, A.: Orbit: unified simulation framework for interactive robot learning environments. IEEE Robotics and Automation Letters 8(6), 37403747 (2023). https://doi.org/10.1109/LRA.2023.3270034 [48] Ni, J., Chen, Y., Jing, B., Jiang, N., Wang, B., Dai, B., Li, P., Zhu, Y., Zhu, S.C., Huang, S.: Phyrecon: Physically plausible neural scene reconstruction. arXiv preprint arXiv:2404.16666 (2024) [49] Ni, J., Liu, Y., Lu, R., Zhou, Z., Zhu, S.C., Chen, Y., Huang, S.: Decompositional neural scene reconstruction with generative diffusion prior (2025) [50] Ni, J., Liu, Y., Lu, R., Zhou, Z., Zhu, S.C., Chen, Y., Huang, S.: Decompositional neural scene reconstruction with generative diffusion prior. arXiv preprint arXiv:2503.14830 (2025) [51] Oechsle, M., Peng, S., Geiger, A.: Unisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 55895599 (2021) [52] Özsoy, E., Czempiel, T., Örnek, E.P., Eck, U., Tombari, F., Navab, N.: Holistic or domain modeling: semantic scene graph approach. International Journal of Computer Assisted Radiology and Surgery 19(5), 791799 (2024) [53] Park, J.J., Florence, P., Straub, J., Newcombe, R., Lovegrove, S.: Deepsdf: Learning continuous signed distance functions for shape representation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 165174 (2019) [54] Piccinelli, L., Sakaridis, C., Yang, Y.H., Segu, M., Li, S., Abbeloos, W., Van Gool, L.: Unidepthv2: Universal monocular metric depth estimation made simpler. arXiv preprint arXiv:2502.20110 (2025) [55] Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: Dreamfusion: Text-to-3D using 2D diffusion. arXiv preprint arXiv:2209.14988 (2022) [56] Powell, M.J.: An efficient method for finding the minimum of function of several variables without calculating derivatives. The computer journal 7(2), 155162 (1964) [57] Pun, A., Sun, G., Wang, J., Chen, Y., Yang, Z., Manivasagam, S., Ma, W.C., Urtasun, R.: Lightsim: Neural lighting simulation for urban scenes (2023), https://arxiv.org/abs/2312.06654 [58] Pun, A., Sun, G., Wang, J., Chen, Y., Yang, Z., Manivasagam, S., Ma, W.C., Urtasun, R.: Neural lighting simulation for urban scenes. NeurIPS (2024) [59] Qian, S., Fouhey, D.F.: Understanding 3D object interaction from single image. In: ICCV (2023) [60] Ren, T., Liu, S., Zeng, A., Lin, J., Li, K., Cao, H., Chen, J., Huang, X., Chen, Y., Yan, F., Zeng, Z., Zhang, H., Li, F., Yang, J., Li, H., Jiang, Q., Zhang, L.: Grounded sam: Assembling open-world models for diverse visual tasks (2024), https://arxiv.org/abs/2401.14159 [61] Seo, S., Han, D., Chang, Y., Kwak, N.: Mixnerf: Modeling ray with mixture density for novel view synthesis from sparse inputs. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2065920668 (2023) [62] Somraj, N., Soundararajan, R.: Vip-nerf: Visibility prior for sparse input neural radiance fields. In: ACM SIGGRAPH 2023 Conference Proceedings. pp. 111 (2023) [63] Son, S., Qiao, Y.L., Sewall, J., Lin, M.C.: Differentiable hybrid traffic simulation. TOG (2022) 13 [64] Straub, J., Whelan, T., Ma, L., Chen, Y., Wijmans, E., Green, S., Engel, J.J., Mur-Artal, R., Ren, C., Verma, S., Clarkson, A., Yan, M., Budge, B., Yan, Y., Pan, X., Yon, J., Zou, Y., Leon, K., Carter, N., Briales, J., Gillingham, T., Mueggler, E., Pesqueira, L., Savva, M., Batra, D., Strasdat, H.M., Nardi, R.D., Goesele, M., Lovegrove, S., Newcombe, R.: The Replica dataset: digital replica of indoor spaces. arXiv preprint arXiv:1906.05797 (2019) [65] Sucar, E., Liu, S., Ortiz, J., Davison, A.J.: imap: Implicit mapping and positioning in real-time. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 62296238 (2021) [66] Suvorov, R., Logacheva, E., Mashikhin, A., Remizova, A., Ashukha, A., Silvestrov, A., Kong, N., Goka, H., Park, K., Lempitsky, V.: Resolution-robust large mask inpainting with fourier convolutions. arXiv preprint arXiv:2109.07161 (2021) [67] Torne, M., Simeonov, A., Li, Z., Chan, A., Chen, T., Gupta, A., Agrawal, P.: Reconciling reality through simulation: real-to-sim-to-real approach for robust manipulation. arXiv (2024) [68] Wang, H., Li, M.: new era of indoor scene reconstruction: survey. IEEE Access (2024) [69] Wang, J., Chen, M., Karaev, N., Vedaldi, A., Rupprecht, C., Novotny, D.: Vggt: Visual geometry grounded transformer. arXiv preprint arXiv:2503.11651 (2025) [70] Wang, L., Guo, R., Vuong, Q., Qin, Y., Su, H., Christensen, H.: real2sim2real method for robust object grasping with neural surface reconstruction. In: 2023 IEEE 19th International Conference on Automation Science and Engineering (CASE). pp. 18. IEEE (2023) [71] Wang, P., Liu, L., Liu, Y., Theobalt, C., Komura, T., Wang, W.: Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. arXiv (2021) [72] Wang, Y., Qiu, X., Liu, J., Chen, Z., Cai, J., Wang, Y., Wang, T.H., Xian, Z., Gan, C.: Architect: Generating vivid and interactive 3d scenes with hierarchical 2d inpainting. Advances in Neural Information Processing Systems 37, 6757567603 (2024) [73] Wen, B., Yang, W., Kautz, J., Birchfield, S.: Foundationpose: Unified 6d pose estimation and tracking of novel objects. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1786817879 (2024) [74] Wu, Q., Liu, X., Chen, Y., Li, K., Zheng, C., Cai, J., Zheng, J.: Object-compositional neural implicit surfaces. In: European Conference on Computer Vision. pp. 197213. Springer (2022) [75] Wu, Q., Wang, K., Li, K., Zheng, J., Cai, J.: Objectsdf++: Improved object-compositional neural implicit surfaces. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 2176421774 (2023) [76] Wu, T., Zheng, C., Guan, F., Vedaldi, A., Cham, T.J.: Amodal3r: Amodal 3d reconstruction from occluded 2d images. arXiv preprint arXiv:2503.13439 (2025) [77] Xia, H., Lin, Z.H., Ma, W.C., Wang, S.: Video2game: Real-time interactive realistic and browser-compatible environment from single video. In: CVPR (2024) [78] Xia, H., Su, E., Memmel, M., Jain, A., Yu, R., Mbiziwo-Tiapo, N., Farhadi, A., Gupta, A., Wang, S., Ma, W.C.: Drawer: Digital reconstruction and articulation with environment realism (2025), https: //arxiv.org/abs/2504.15278 [79] Xiang, J., Lv, Z., Xu, S., Deng, Y., Wang, R., Zhang, B., Chen, D., Tong, X., Yang, J.: Structured 3d latents for scalable and versatile 3d generation. arXiv preprint arXiv:2412.01506 (2024) [80] Xie, T., Zong, Z., Qiu, Y., Li, X., Feng, Y., Yang, Y., Jiang, C.: Physgaussian: Physics-integrated 3d gaussians for generative dynamics. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 43894398 (2024) [81] Xiong, Y., Ma, Wei-Chiu Wang, J., Urtasun, R.: Ultralidar: Learning compact representations for lidar completion and generation. CVPR (2023) [82] Yang, J., Ivanovic, B., Litany, O., Weng, X., Kim, S.W., Li, B., Che, T., Xu, D., Fidler, S., Pavone, M., Wang, Y.: Emernerf: Emergent spatial-temporal scene decomposition via self-supervision. In: ICLR (2024) [83] Yang, Y., Sun, F.Y., Weihs, L., VanderBilt, E., Herrasti, A., Han, W., Wu, J., Haber, N., Krishna, R., Liu, L., et al.: Holodeck: Language guided generation of 3d embodied ai environments. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1622716237 (2024) 14 [84] Yang, Z., Chen, Y., Wang, J., Manivasagam, S., Ma, W.C., Yang, A.J., Urtasun, R.: Unisim: neural closed-loop sensor simulator. CVPR (2023) [85] Yang, Z., Chai, Y., Anguelov, D., Zhou, Y., Sun, P., Erhan, D., Rafferty, S., Kretzschmar, H.: Surfelgan: Synthesizing realistic sensor data for autonomous driving. In: CVPR (2020) [86] Yao, K., Zhang, L., Yan, X., Zeng, Y., Zhang, Q., Xu, L., Yang, W., Gu, J., Yu, J.: Cast: Component-aligned 3D scene reconstruction from an rgb image. arXiv preprint arXiv:2502.12894 (2025) [87] Yariv, L., Gu, J., Kasten, Y., Lipman, Y.: Volume rendering of neural implicit surfaces. Advances in Neural Information Processing Systems 34, 48054815 (2021) [88] Yariv, L., Hedman, P., Reiser, C., Verbin, D., Srinivasan, P.P., Szeliski, R., Barron, J.T., Mildenhall, B.: Bakedsdf: Meshing neural sdfs for real-time view synthesis. In: SIGGRAPH Conference (2023) [89] Yeshwanth, C., Liu, Y.C., Nießner, M., Dai, A.: Scannet++: high-fidelity dataset of 3D indoor scenes. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 1222 (2023) [90] Yu, Z., Chen, A., Huang, B., Sattler, T., Geiger, A.: Mip-splatting: Alias-free 3d gaussian splatting (2024) [91] Yu, Z., Peng, S., Niemeyer, M., Sattler, T., Geiger, A.: Monosdf: Exploring monocular geometric cues for neural implicit surface reconstruction. arXiv (2022) [92] Yu, Z., Sattler, T., Geiger, A.: Gaussian opacity fields: Efficient and compact surface reconstruction in unbounded scenes. arXiv (2024) [93] Yuan, Y., Bleier, M., Nuchter, A.: Scenefactory: workflow-centric and unified framework for incremental scene modeling. IEEE Transactions on Robotics (2025) [94] Zhai, A.J., Shen, Y., Chen, E.Y., Wang, G.X., Wang, X., Wang, S., Guan, K., Wang, S.: Physical property understanding from language-embedded feature fields (2024) [95] Zhang, K., Li, B., Hauser, K., Li, Y.: Particle-grid neural dynamics for learning deformable object models from rgb-d videos. In: Proceedings of Robotics: Science and Systems (RSS) (2025) [96] Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable effectiveness of deep features as perceptual metric. In: CVPR (2018) [97] Zhao, H., Wang, H., Zhao, X., Wang, H., Wu, Z., Long, C., Zou, H.: Automated 3d physical simulation of open-world scene with gaussian splatting. arXiv preprint arXiv:2411.12789 (2024) [98] Zyrianov, V., Che, H., Liu, Z., Wang, S.: Lidardm: Generative lidar simulation in generated world (2024), https://arxiv.org/abs/2404.02903 [99] Zyrianov, V., Zhu, X., Wang, S.: Learning to generate realistic lidar point clouds. In: ECCV (2022)"
        },
        {
            "title": "A More Interactive Environment Application Results",
            "content": "Real-Time Interactive Game With the reconstruction results of HoloScene, we could create an interactive game in Unreal Engine [12], which supports real-time interactions with our created assets. We import all the objects and the background texture mesh into the game engine and create thirdperson game, where the agent can freely explore, navigate, jump, and interact with the objects in the scene. Thanks to the complete geometry and appearance, as well as the great physical plausibility of HoloScene, the objects could remain stable in the game. Also, the movements of objects could follow physical rules when the objects interact. The Fig. 1 shows the game screenshots, and more videos could be found in the supp. material as well. Interactive 3D Editing With each objects appearance and geometry reconstructed, we support interactive 3D scene editing by allowing users to manipulate individual Gaussians through control panel. Objects can be translated along the 2D xy-plane and rotated around the vertical (z) axis. As illustrated in Fig. 2, the reconstructed swivel chair is repositioned freely and rendered realistically, resulting in coherent and intuitive scene modifications. Immersive Experience Recording We demonstrate the usage of the reconstructed 3D objects from HoloScene for immersive experience recording. Given static RGB video of person manipulating an object, our goal is to recover the objects 6D pose and resimulate its motion within virtual 3D scene. We use the first frame of the video as the target view for camera calibration. To determine the camera pose, we first estimate relative transformations between the target and nearby training views using VGGT [69]. We then solve least-squares problem with Powells method [56] to derive global transformation that aligns the VGGT-predicted poses with the ground-truth poses of the training views. This transformation is applied to register the target view within the training coordinate system. We use UniDepthV2 [54] to predict the depth map for each frame and compute scale ratio between the predicted monocular depth and depth rendered from the 3D Gaussians. This ratio adjusts the predicted depth to align accurately within the virtual scene. For object tracking, we adopt FoundationPose [73], using our high-quality reconstructed 3D asset as CAD model for model-based 6D pose estimation. The recovered pose sequence is then applied to the 3D object and its 3D Gaussians for final rendering. As illustrated in Fig. 3, we accurately recover the objects pose from visual input, enabling consistent replay of real-world interactions in virtual scenes. Dynamic Visual Effects We showcase an immersive application by augmenting the reconstructed scenes with dynamic visual effects. These include rigid body simulations, 3D character animations, and particle-based effects, all spatially grounded within the virtual scene. We adopt multiple visual effects created by AutoVFX [18] to simulate virtual content and overlay it, along with shadows, onto the input image. As shown in Fig. 4, we produce effects that blend naturally with the scene, enhancing both visual richness and physical realism. Figure 1: More Interactive Game Results: We show more screenshots of the interactive game built by Unreal Engine here. The agent can run, jump, and interact with all the objects in the game."
        },
        {
            "title": "B More Physical Simulation Results",
            "content": "In this section, we will show more physical simulation results of Holoscene and its comparison with the baseline methods [75, 48, 49]. In Fig. 5 and Fig. 6, we show the comparisons of our method and 1 Figure 2: More Interactive 3D Editing Results: The user could interact with the object in the scene, change the location and orientation of the object the control panel, and attain the edited scene. Pose 0 (Init) Pose 2 Pose 0 (Init) Pose 2 Pose 1 Pose 3 Pose 1 Pose 3 Figure 3: More Immersive Experience Recording Results: Here we show more results of the immersive experience recording of two objects in the scene. For each image pair, the original recording is shown on the left, and the simulated object rendering with Gaussians is on the right. The estimated object 6D poses are marked with oriented bounding boxes. the baselines in the Scannet++ [89] dataset. In Fig. 7, we show the comparisons in the Replica [64] dataset. In Fig. 8, we show the comparisons in the iGibson [26] dataset. From the results, we can observe that HoloScene can achieve the best physical stability and plausibility, as well as realistic rendering quality."
        },
        {
            "title": "C Method Details",
            "content": "Structured Tree Search When we perform the breadth-first traversal on the scene tree structure, for each node, we will end up choosing the reconstructed object candidate with the lowest physical energy. Then we will further adjust the state of the chosen candidate object to ensure its stability. Specifically, we will regularize the SDF value and the mesh vertex locations to prevent the interpenetration of objects. Before applying marching cube to the instance SDF, we will first prune the SDF value of the points if those points fall into the boundary of its parent object or the nearby objects. After we attain the mesh with marching cube, we will perform collision test of the reconstructed object mesh with its parent object and the nearby objects. And if there exist collisions, we will dynamically optimize the locations of the vertices to avoid the collisions. After the optimization, the contact surfaces of 2 Figure 4: More Dynamic Visual Effect Results: We show more results of dynamic visual effects, including animations, dropping objects, and setting fires. objects will be regularized and avoid inter-object collisions, so that the stability of the scene can be further improved."
        },
        {
            "title": "D Experiment Details",
            "content": "D.1 Datasets We adopt three datasets as we mentioned in our main paper: Replica [64], Scannet++ [89], and iGibson [26]. Replica: The original Replica dataset only provides the texture mesh with instance annotations, and no existing videos with camera trajectories are provided. We manually design dense camera trajectory of several hundred frames for each scene, and render images with size of (512, 512). Scannet++: The scannet++ dataset provides multi-view posed images and the aligned 3D scan with instance annotation. However, limited by the quality of the scan, the instance mask derived from the scan annotation is not ideal. We adopt [24] to get cleaner instance masks instead. For the image resolution, we downsample the original image by 2. iGibson: The iGibson dataset provides the complete geometry and appearance of every object and the background scene, but without provided camera trajectories. We manually design dense camera trajectory of several hundred frames for each scene, and render images with size of (512, 512). D.2 Baselines We adopt the open-source codes of the baselines and adapt them for our benchmark. Our baselines are ObjectSDF++ [75], PhyRecon [48], and DP-Recon [49]. Now we will elaborate the details of them. ObjectSDF++: The original codebase only supports their rendering images from Replica [64], where each image has size of (384, 384). In order to make it adaptable to other image resolutions, we implement new data loader in the code. PhyRecon: Similar to ObjectSDF, the original code base only supports their rendering images from Replica [64] and different resolution of resized images from Scannet++ [89]. For the adaption to other image resolutions, we implement new data loader in the code. The PhyRecon has final stage 3 Ref Image j o y o - r y m n R - c a A O No Physics With Physics (Intermediate) With Physics (Final) Figure 5: More Physical Simulation Results in Scannet++ Dataset Scene 1: We show the geometry and appearance of each object in the scene with physical simulation, from no physics state, intermediate state with physics, and final state with physics. of applying physical loss to every object. But it may happen that some objects fail to reconstruct themselves, so the physical loss can not be applied to those objects. In this case, we skip those objects. DP-Recon: Similar to the baselines above, we implement new data loader to adapt the code to our dataset. DP-Recon has the geometry stage and the following appearance stage. We follow the officially released code to run those experiments. DP-Recon needs to generate the object bounding boxes in the running process. But DP-Recon overlooks the possibility that some objects may fail to be reconstructed and disappear. So in this case, we set the bounding box of the object as the largest possible bounding box in the scene to fix the issue. We evaluate the appearance, geometry, and physics based on the final texture mesh results from the second stage of DP-Recon. D.3 The inference details of HoloScene The inference stage of HoloScene is composed of three stages after generating the scene graph in stage 0, as we mentioned in the main paper. In stage 1 of the gradient-based optimization, we follow the previous instance-level SDF [75] to set the hyperparameters and train for 200k iterations similarly. In stage 2 of the sampling-based optimization, we adopt the Isaac Sim [47] as the physical simulator, Ref Image j o y o - r r o n R - c a A O No Physics With Physics (Intermediate) With Physics (Final) Figure 6: More Physical Simulation Results in Scannet++ Dataset Scene 2: We show the geometry and appearance of each object in the scene with physical simulation, from no physics state, intermediate state with physics, and final state with physics. and set the physical parameters in the simulation as default. In the sampling with generative models, we randomly choose the seeds and generate at most five candidates for each object. In stage 3 of the gradient-based refinement, we refine the trained Gaussians globally. We optimize the Gaussians for 30k iterations following [23], and in each iteration, we also use generated images to optimize the instance Gaussians as well. For other learning parameters, they are set to default in our experiments. 5 Ref Image j o y o - s n R - r r o c a A No Physics With Physics (Intermediate) With Physics (Final) Figure 7: More Physical Simulation Results in Replica Dataset: We show the geometry and appearance of each object in the scene with physical simulation, from no physics state, intermediate state with physics, and final state with physics. 6 Ref Image j n R n R - r o - r r o e r p No Physics With Physics (Intermediate) With Physics (Final) Figure 8: More Physical Simulation Results in iGibson Dataset: We show the geometry and appearance of each object in the scene with physical simulation, from no physics state, intermediate state with physics, and final state with physics. DP-Recon fails to reconstruct the complete background mesh, leading to object falling in the figure."
        }
    ],
    "affiliations": [
        "Intel",
        "University of Illinois Urbana-Champaign"
    ]
}