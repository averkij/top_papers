{
    "paper_title": "World in a Frame: Understanding Culture Mixing as a New Challenge for Vision-Language Models",
    "authors": [
        "Eunsu Kim",
        "Junyeong Park",
        "Na Min An",
        "Junseong Kim",
        "Hitesh Laxmichand Patel",
        "Jiho Jin",
        "Julia Kruk",
        "Amit Agarwal",
        "Srikant Panda",
        "Fenal Ashokbhai Ilasariya",
        "Hyunjung Shim",
        "Alice Oh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In a globalized world, cultural elements from diverse origins frequently appear together within a single visual scene. We refer to these as culture mixing scenarios, yet how Large Vision-Language Models (LVLMs) perceive them remains underexplored. We investigate culture mixing as a critical challenge for LVLMs and examine how current models behave when cultural items from multiple regions appear together. To systematically analyze these behaviors, we construct CultureMix, a food Visual Question Answering (VQA) benchmark with 23k diffusion-generated, human-verified culture mixing images across four subtasks: (1) food-only, (2) food+food, (3) food+background, and (4) food+food+background. Evaluating 10 LVLMs, we find consistent failures to preserve individual cultural identities in mixed settings. Models show strong background reliance, with accuracy dropping 14% when cultural backgrounds are added to food-only baselines, and they produce inconsistent predictions for identical foods across different contexts. To address these limitations, we explore three robustness strategies. We find supervised fine-tuning using a diverse culture mixing dataset substantially improve model consistency and reduce background sensitivity. We call for increased attention to culture mixing scenarios as a critical step toward developing LVLMs capable of operating reliably in culturally diverse real-world environments."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 2 7 8 7 2 2 . 1 1 5 2 : r World in Frame: Understanding Culture Mixing as New Challenge for Vision-Language Models Eunsu Kim1, , Junyeong Park1, , Na Min An1,, Junseong Kim1,, Hitesh Laxmichand Patel2,, Jiho Jin1,, Julia Kruk3, Amit Agarwal2, Srikant Panda2, Fenal Ashokbhai Ilasariya4, Hyunjung Shim1,, Alice Oh1, 1KAIST, 2Oracle, 3Meta, 4Stevens Institute of Technology {kes0317, jjjunyeong9986, naminan}@kaist.ac.kr"
        },
        {
            "title": "Abstract",
            "content": "In globalized world, cultural elements from diverse origins frequently appear together within single visual scene. We refer to these as culture mixing scenarios, yet how Large Vision-Language Models (LVLMs) perceive them remains underexplored. We investigate culture mixing as critical challenge for LVLMs and examine how current models behave when cultural items from multiple regions appear together. To systematically analyze these behaviors, we construct CultureMix, food Visual Question Answering (VQA) benchmark with 23k diffusion-generated, human-verified culture mixing images across four subtasks: (1) food-only, (2) food+food, (3) food+background, and (4) food+food+background. Evaluating 10 LVLMs, we find consistent failures to preserve individual cultural identities in mixed settings. Models show strong background reliance, with accuracy dropping 14% when cultural backgrounds are added to food-only baselines, and they produce inconsistent predictions for identical foods across different contexts. To address these limitations, we explore three robustness strategies. We find supervised fine-tuning using diverse culture mixing dataset substantially improve model consistency and reduce background sensitivity. We call for increased attention to culture mixing scenarios as critical step toward developing LVLMs capable of operating reliably in culturally diverse real-world environments. huggingface.co/datasets/EunsuKim/CultureMix 1. Introduction In todays globalized society, cultures are blending and being shared more than ever. For instance, we often see food serving as medium where cultures meet, from diverse cuisines in buffets to ramen shops near the Eiffel Tower. , denote equal contributions, Senior Authors. 1The figure was generated with assistance from ChatGPT. Figure 1. Conceptual illustration of LVLMs in Culture Mixing scenarios. Real-world contexts often contain multiple cultural elements that humans can easily identify, yet LVLMs struggle to identify them. This phenomenon, referred to as culture mixing [1], captures the coexistence and fusion of multiple cultural cues within single scene, spanning objects, foods, and environments (see Figure 1). Nevertheless, each element retains its distinct cultural identity regardless of its location or the cultural elements that coexist with [24]. The ability to recognize and preserve these individual cultural identities within Figure 2. Dataset construction and evaluation pipeline. CultureMixaims to benchmark state-of-the-art LVLMs on their cultural knowledge in diverse mixing scenarios by asking models to identify food names and their countries of origin, featuring various combinations of foods and backgrounds from over 30 countries. All images are synthetically generated using diffusion models with human-in-the-loop validation. Model responses in this figure are from InternVL2-14B. mixed contexts is essential for maintaining authenticity and preventing the dilution or misrepresentation of any culture. Meanwhile, as LVLMs increasingly interact with humans across diverse cultural contexts, recent studies have explored cultural understanding through Visual Question Answering (VQA) benchmarks [57]. However, these benchmarks depict scenes rooted in single cultural context, ignoring culture mixing scenarios. Since culture mixing poses unique challenge requiring models to distinguish and integrate multiple conflicting cultural cues, models trained on singleculture data may not generalize effectively to such contexts. To this end, we define culture mixing as new challenge for LVLMs and evaluate how current models behave under such scenarios. We introduce CultureMix, systematic benchmark dataset designed to reveal how LVLMs recognize cultural elements at different levels of culturally mixed contexts (Figure 2). Our large-scale dataset consists of 23k synthetic images (featuring 247 unique food items) spanning 30 countries and 50 unique background seed images, along with 100 real-world images (featuring 219 unique food items). It adopts VQA format in which models identify target food item and determine its cultural origin in the presence of other cultural elementsreferred to as cultural distractors. Depending on the types of these distractorsnone, food, background, or bothour dataset enables structured analysis of how different cultural cues and their combinations influence model behavior. Additionally, it consists of multiple levels of cultural distance between the target item and its distractors, allowing quantitative examination of the effect of cultural distance on model performance. With CultureMix, we evaluate 10 LVLMs and uncover critical limitations in culturally mixed scenarios, along with key insights to improve model robustness. Across all subtasks, performance decreases noticeably compared to singleculture images, with declines of 1-8%p in food identification and 1-14%p in cultural origin prediction. These declines further intensify as the cultural distance between the target and distractor elements increases. Our analysis reveals that background cues exert stronger influence than food distractors, frequently shifting predictions toward the distractors cultural source and undermining output consistency. This pattern indicates that LVLMs rely heavily on contextual signals rather than the target object itself, and that their predictions are heavily influenced by the cultural relationship between the target and its surrounding context. This issue becomes particularly problematic when visual cues conflict (i.e., co-existing objects from culturally distant regions), often leading to misidentification and cultural bias toward the culture with which the model is more familiar. Motivated by these findings, we explore both trainingfree and training-based (SFT) methods to improve model robustness in culturally mixed settings. Both approaches yield consistent gains in accuracy and consistency, highlighting promising directions to enhance cross-cultural understanding while leaving open the question of optimal training objectives for culture mixing scenarios. In summary, our contributions are as follows: Task. We introduce CultureMix, benchmark for systematically evaluating LVLMs cross-cultural understanding in culturally mixed contexts. Dataset and Resources. We build and release 23k synthetic images using diffusion models that blends cultural elements from diverse regions. Analysis. Evaluating 10 LVLMs, we find that existing models struggle to interpret culturally mixed scenes, with accuracy decreasing as cultural distance increases. Mitigation. We explore training-free and training-based methods to improve model robustness, showing consistent gains while highlighting remaining challenges in designing optimal training objectives for culture mixing. 2. CultureMix We propose CultureMix, novel and challenging task to evaluate cross-cultural awareness of LVLMs through the concept of culture mixing. In this section, we outline the task ( 2.1), the dataset construction pipeline ( 2.2), and dataset statistics ( 2.3). 2.1. Task Overview We formulate our task as VQA problem in which models infer both the food name and the country of origin of target food item. Each target food item is evaluated under four distinct subtasks that systematically vary the type of cultural distractorsvisual cultural elements that co-occur with the target food item in the image. The conditions include: no distractors, only food-type distractors, only background-type distractors, or both types combined. The four subtasks are defined as follows: Single Food (SF ) An image of single dish without any cultural distractors. This subtask isolates the target food item. Multiple Foods (MF ) An image containing multiple dishes, introducing foodtype distractors. Single Food with Background (SFB ) An image of single dish accompanied by backgroundtype cultural distractors, that reflect specific cultural context. Multiple Foods with Background (MFB ) An image containing multiple dishes together with culturally informative backgrounds, containing both food and background distractors. Using SF as baseline, our task design reveals how the models behavior shifts when distractors from different types and diverse countries are introduced. Specifically, we ensure diverse pairings between target foods and distractors in terms of (1) country combinations and (2) cultural distance (See 2.3 for details). 2.2. Dataset Construction Pipeline We aim to construct dataset where distractors are systematically introduced, aligned with our four-subtask design. We construct synthetic dataset using editing-based textto-image diffusion models (FLUX.1-Kontext [8] and QwenImage-Edit [9]). We use existing datasets annotated with the names of foods and their countries of origin as seed dataset. Table 1. Dataset composition and statistics. Background (BG) includes street and landmark images. In MFB, single background image from each continent is randomly selected and combined due to the large number of possible combinations. Type Description Composition Food Food BG Background 30 countries, 4 continents countries, 5 continents, 2 types (landmark, street) SF MF SFB MFB Food + Food 3 Data Augmentation Food Food pairs from SF Food + Food SF (5 continents {5 landmark, 5 street}) Food + BG Food + Food + BG MF (5 continents {1 landmark, 1 street}) Size 247 50 988 948 12,350 9,480 rigorous human-in-the-loop pipeline supports the entire process. Seed Dataset Collection. We sample food and background images from existing datasets: For foods, we collected 247 seed food images from existing multicultural VQA datasets: WorldCuisines [10] and WorldWideDishes [11]. Based on the availability in existing datasets, we selected 30 countries that each have at least 10 food images in our seed sources. From this, we select countries to ensure balanced representation across four continents and across their cultural resource levels. Since resource levels for images are not available, we use language resource levels as proxy, including both highresource (e.g., United States, United Kingdom, and China) and low-resource (e.g., Philippines, Algeria, and Croatia). We also sample 50 seed background images from existing datasets: landmark images from VIPPGeo [12] and street images from the Google Landmarks Dataset v2 [13]. We include five images from each of the five continentsthe same four continents as in the food list, with the addition of South America. The complete list of countries and their statistics is provided in the Appendix A.1.2 , MF SF . Using the images from the seed dataset, we remove the original background and replace it with plain white background to eliminate any additional elements (e.g., text, humans, tables, or scenery) that could unintentionally influence the models inference. For this process, we generate candidate images using both FLUX.1-Kontext and Qwen-Image-Edit. Among the two outputs, one of the authors manually selects the image that best preserves fidelity to the original. The resulting images constitute our SF set. We then concatenate two SF images to compose each MF image. , MFB SFB . SFB and MFB are close-up food images with visible backgrounds. We first vertically concatenate background image with SF and MF to form SFB and MFB, respectively. We then apply diffusion model for image harmonization. We initially start with FLUX.1-Kontext, and 2When suitable images were unavailable, we manually collected additional ones via Google Image Search with clear textual labels in the accompanying metadata. (a) Country and food name identification accuracy. (b) Country identification predictions. Figure 3. Overall model performance on country and food name identification. (a) Accuracy comparison across models for each subtask. (b) Country identification target-prediction heatmaps for each subtask. For every golden country, the plots show the distribution of predicted countries, illustrating both correct predictions and systematic confusions across models. when it does not yield satisfactory results, we apply both FLUX.1-Kontext and Qwen-Image-Edit multiple times until the generated images meet our criteria.3 Verification. All generation steps were conducted under multiple rounds of human verification until the entire dataset was validated. For each subtask, we established set of criteria, and any images that did not satisfy the criteria were either regenerated or removed to ensure the quality. We provide description of the human verification procedure, including image selection criteria, and statistics of the filtered images in Appendix A. 2.3. Dataset Statistics We illustrate the procedure for selecting the seed images and constructing their combinations for each subset. Table 1 summarizes the main statistics for these subsets. Food Combinations. For every food item, we aim to construct combinations that reflect varying levels of cultural distance. We operationalize cultural distance through geographic proximity, creating three levels as follows [14]: (i) target and distractor from the same country, (ii) from different countries within the same continent, and (iii) from different continents. In addition, we account for singleimage classification performance across three baseline models (Qwen2.5-VL-72B-Instruct, GPT-image, and Gemini2.5-flash). We include both easy cases (all three models correct) and hard cases (all three models incorrect), from cases where all three models are correct to cases where all fail, enabling analysis of culture mixing effects at varying difficulty levels. Following these, we construct 948 image pairs spanning 30 countries. 3We provide the input examples used for the diffusion models and examples of errored images in Appendix A.3, and background images, images of SF, MFB, and CultureMix-real samples in Appendix A.4. Background Food Combinations. For SFB, each of the 247 food items is paired with 10 background combinations (2 types 5 continents), with 5 images per continent, yielding total of 12,350 images. For MFB, each of the 948 food image pairs is combined with randomly sampled backgrounds from each continent, resulting in 9,480 images. 3. Experiments 3.1. Experimental Setup As illustrated in Figure 2, we evaluate models in VQA format by querying, What is the name of the food, and which country is it most closely associated with?. In MF and MFB, the target food is placed on the left and distractor on the right. Thus, we specify left food in place of food in the prompt for MF and MFB. We measure the food and country identification accuracy of the target food. Food identification accuracy is computed using similarity matching with predefined threshold. Country identification accuracy is computed using exact string matching, while accounting for known variations in country names.4 We further examine potential biases related to the position of the target item (left vs. right) and its relative size. Both factors show negligible influence on model performance. (See Appendix B.2.) Models. We evaluate 10 LVLMs, including 2 proprietary models (GPT-5 (gpt-5-2025-08-07) [15], Gemini-2.5Pro [16]) and 8 open-source models. We test the opensource models spanning parameter sizes from 8B to 72B: InternVL3 (8, 14, 38, and 78B) [17], Ovis2.5-9B [18, 19], QwenVL3 series (8 and 32B) [20], and Molmo-72B [21]. 4For food accuracy, qe use weighted Jaccard character n-gram similarity (0.7 bigrams, 0.3 unigrams) with threshold of 0.4. Two authors validated the evaluation method by manually reviewing 100 randomly sampled predictionevaluation pairs from Gemini and InternVL-8B, confirming 95% correctness for food-name scoring and 100% for country scoring. 3.2. Overall Prediction Accuracy on CultureMix Figure 3a presents the accuracy of all evaluated models on our CultureMix. Challenges in Understanding Culturally Mixed Images. Across subtasks, the decrease in performance from SF to MF, SFB, and MFB across all models indicates that models experience greater difficulty in recognizing target food items under culture-mixing conditions. We observe trend across models: SF MF > MFB SFB. Compared with food-item distractors (MF), the background distractors (SFB) result in 13% larger drop in country accuracy and 7% larger drop in food name accuracy on average. Street and landmark backgrounds show similar effects (see Figure 18 in Appendix B.1). Performance Gap between Proprietary and Open Models. Both Gemini and GPT substantially outperform all open models across our subtasks, demonstrating stronger multimodal reasoning and cultural understanding. Among the open models, OVIS2.5-9B performs most competitively despite its relatively small parameter size, followed by InternVL3-78B, QwenVL3-32B, and QwenVL3-8B. 3.3. Country Prediction Patterns Figure 3b compares the country prediction distributions across subtasks. Skewed Predictions Toward High-Resource Countries in SF. In SF, models relatively predict correct country labels, as indicated by the prominent blue diagonal trend. Accuracy is particularly high for countries such as India, Korea, Japan, and China among Asian regions, Italy and Poland among European regions, and the United States. When misidentifying the country in the SF subtask, models tend to confuse countries within the same continent (as seen in the Asian and European regions of the SF heatmap). Additionally, predictions are often biased toward high-resource (WEIRD [22]) countries. Specifically, African and Asian countries are frequently misclassified as India or China, while European and North American countries are often mislabeled as the United States. Prediction Shifts under Culturally Mixed Settings. Under the culturally mixed settings (MF, SFB, MFB), accuracy declines, and model predictions are distributed more broadly across regions, deviating from the pattern observed in SF. Notably, in SFB, predictions exhibit strong clustering around certain countries such as Mexico or Japan, in contrast to the high-resource region bias observed in SF. We further analyze these patterns in depth in 4. (a) Prediction shifts across models (b) Prediction shifts across subtasks Figure 4. Effect of cultural distractors on country prediction label shifts. 4. Discussion We examine how cultural distractors shift model predictions (4.1), how cultural distance affect these shifts (4.2), how they compare with culturally agnostic distractors (4.3), and whether single-food cultural awareness correlates with robustness in culture mixing contexts (4.4). We utilize three complementary metrics: 1) accuracy, 2) entropy for prediction confidence and consistency, and 3) label shift (%), the proportion of predictions that differ from the SF baseline in mixed settings, indicating how strongly distractors alter model outputs. 4.1. Effects of Cultural Distractors on LVLMs Distractors pull prediction shift toward their culture. Figure 4a shows how model predictions change when distractor is added. Across all models, an average 15% of the predictions shift directly to the distractors country, and an additional 12% shift to country in the same continent, indicating clear directional influence from culturally related distractors. While Gemini achieves higher accuracy than GPT-5, its country predictions are more susceptible to distractor influence, exhibiting larger label shifts toward the distractors country. Among open models, QwenVL3-8B and QwenVL3-32B show the strongest susceptibility, with the highest deviations toward the distractors country (26.6%, 23.3%) under culture mixing settings. Background Distractors Exert Stronger Influence Than Food Items. Figure 4b compares distractor influence across subtasks by plotting the proportion of predictions that maintain SF labels against those that shift toward the distractor. Models appearing in the upper-left region of the plot are more resistant to distractors, whereas those in the lowerright region are highly affected. MF shows high retention (40-80%) with low shift (20%), showing models robustness to food-item distractors. In contrast, SFB yields lower retention (20-40%) and higher shifts (40%), confirming that background cues provide stronger cultural signals than food items alone. MFB shows performance between that of SFB and MF. Specifically, when food and background cues Figure 5. Effect of cultural distance between target food and background distractor in SFB on country identification accuracy and prediction entropy. Figure 6. Comparison of distractor types on country identification accuracy and prediction label entropy (Gemini). (ns: not significant, **:p<0.01, ***: p<0.001) align, they reinforce predictions, but when they conflict, they increase shifts. This balance places MFB between the foodonly and background-only conditions. 4.2. Effect of Elements Cultural Distance on Prediction We examine how cultural distance (whether the target and distractor originate from the same country, same continent, or different continents) affects model accuracy. As shown in Figure 5, models achieve the highest accuracy and lowest entropy when the target and distractor come from the same country, which suggests that culturally consistent cues enhance recognition by providing coherent context. In contrast, accuracy is lowest and entropy is highest when the target and distractor originate from different continents. Similar patterns appear in SF, MFB settings and food name identification task, as detailed in Figure 19 in Appendix B.1, which shows consistent monotonic improvement with decreasing cultural distance. 4.3. Comparison Between Cultural and Culturally-"
        },
        {
            "title": "Agnostic Distractors",
            "content": "To verify that these effects stem from cultural information rather than general distractor complexity, we repeat our experiment using four culturally-agnostic objects (apple, car, scissors, and teddy bear) as distractors. As shown in Figure 6, cultural distractors generally yield similar or lower Figure 7. Relationship between model confidence in the SF setting and the label shift ratio under cultural mixing (MF MFB) settings. Confidence is measured by entropy (lower entropy = higher confidence) Each point represents Country Identification Accuracy of each food item. accuracy and higher entropy than agnostic ones, confirming that cultural signals drive the observed shifts, rather than visual content alone. Although distractors sometimes yield slightly higher accuracy than object distractors, this is primarily due to the distractors from the same country enhancing prediction ( 4.2). Consistent with 4.1, background distractors have stronger effects than food distractors, shown by the largest accuracy drops and highest entropy increases. 4.4. Relationship Between Cultural Awareness of Single Foods and Multi-Foods Finally, we examine whether models understanding of individual foods predicts robustness in mixed-cultural settings. Figure 7 relates SF entropy (a proxy for prediction confidence) to label shift under culture mixing. Lower confidence generally corresponds to larger shifts, indicating that weaker item-level understanding leaves models more vulnerable to distractors. However, low entropy does not guarantee robustness in culturally mixed contexts, as several lowentropy foods still exhibit high shifts. These cases imply that distractor resistance depends not only on target knowledge but also on the relative strength of competing visual cues in the image. 5. CultureMix-Real: Real-World Culturally"
        },
        {
            "title": "Mixed Food Dataset",
            "content": "5.1. Dataset Collection To assess whether the behaviors observed in our synthetic datasets also arise in natural images in the real world, we collect real-world culture mixing dataset (for MF setting). Images were gathered with web-based image searches using systematically generated combinations of culturally specific keywords (e.g., Korean and Italian food on table), and survey within our institute, where participants submitted photos with explicit consent. We collect 100 MF images, consisting of 50 same-culture and 50 cross-culture food combinations spanning 10 countries. By cropping individual food regions from these MF images, we extract 219 single-food Table 2. Performance of Ovis-2.5-9B and InternVL3-8B across mitigation methods. Lower is better for Entropy, higher is better for Accuracy. Underline indicates statistically significant improvement; gray boxes indicate results better than the Base setting. Entropy () Accuracy (, %) Model (Setting) MF SFB MFB SF MF SFB MFB Ovis2.5 Base romptDirect romptCoT SF InternVL3 Base romptDirect romptCoT SF 1.28 0.97 1.24 1.02 1.16 1.12 1.54 1.13 3.34 3.31 3.60 2.52 3.77 3.77 3.97 2. 3.07 2.99 3.21 2.36 3.43 3.36 3.65 2.45 13.89 16.67 15.28 15.28 11.11 9.72 9.72 9.72 10.74 14.07 14.81 11.85 5.19 8.52 7.78 8. 5.65 6.00 6.62 8.59 2.14 2.62 2.65 4.16 6.14 6.07 6.73 8.95 3.33 3.47 2.74 5.14 item across all four subsets (SF, MF, SFB, and MFB), resulting in 5K training images, to reduce the overall training cost. For evaluation, we use the complete test set of 7K images to assess all three mitigation methods. 6.1. Approaches for Improvement Through evaluation on CultureMix, we reveal that: 1) even when LVLMs demonstrate strong knowledge or confidence about certain items in isolation, they fail when these items appear in culturally mixed contexts, and 2) this failure stems heavily from their reliance on co-occurring cultural items and backgrounds. These findings indicate that understanding culture mixing does not necessarily align with cultural awareness of individual items, requiring approaches that go beyond single-item recognition. 1. Direct Prompting Engineering (P romptDirect) We explicitly instruct the model to focus on the target item rather than the background or other elements by adding direct guidance to the prompt. This approach aims to mitigate the models reliance on surrounding cultural cues identified in our analysis. 2. Chain of Thought (CoT) Prompting (P romptCoT ) We explore Chain-of-Thought (CoT) prompting because culture mixing often requires models to integrate multiple, partially informative visual cues [23]. Such multi-cue integration amounts to form of visualcontextual reasoning, and CoT has been shown to improve LLM performance on various tasks. 3. Supervised Finetuning (SF ) We fine-tune models on culturally mixed images to help them learn to ignore cultural distractors. During training, each food item is presented in the following order: SFMFSFBMFB. This progression exposes the model to increasingly complex cultural mixtures while encouraging consistent predictions across these varying contexts. Figure 8. Overall prediction accuracy in CultureMix-real across models. (SF) images, enabling direct comparison between isolated and culturally mixed contexts. 5.2. Experiment and Results We evaluate the same set of models described in 3. Because food items in real-world images are not always symmetrically positioned (e.g., side by side), we mark the target food with red bounding box and prompt the model to identify the food within the bounding box. As shown in Figure 8, the results reveal trend of MF (same) > SF > MF (diff), indicating that the cultural relationship between co-occurring elements affects the model performance in the real world as well. This confirms that the degradation of cultural understanding observed in our synthetic CultureMixsettings also manifests in real-world scenarios, underscoring the persistent challenge of LVLMs. 6. Improving Understanding of Culture Mixing: An Exploratory Study As observed in 3-4, existing LVLMs struggle with culturally mixed scenarios, yet understanding such contexts is critical for building culturally aware models for real-world use. This leaves us with an important open question: How can we build models that are capable of handling culture mixing scenarios? Building on the insights from our analysis, we conduct several exploratory approaches as starting point to address this challenge, examining both training-free and training-based methods. We use two open source models, Ovis2.5-9B and InternVL3-8B, in our experiment. Training and Test Datasets. From the entire set of datasets included in CultureMix, we create train-test splits as follows: First, SF images are divided with 7:3 train-test ratio, ensuring balanced country distribution across both sets. MF, SFB, and MFB datasets are then split correspondingly based on the inclusion of their associated SF images. For the training dataset used in our training-based approach, we randomly sample one-third of instances from each food Table 3. Comparison of cultural benchmark datasets. This table summarizes existing datasets in terms of cultural content, task type, image modality, geographic and linguistic coverage, and the presence of culture mixing. Our benchmark dataset is the first to explicitly include cultural mixing, covering diverse foods and backgrounds across 30 countries in English, and supporting both real and synthetic images. Cultural Element Evaluation Type Task Type Image Type Countries Languages Culture Mixing Bhatia et al. (2024) [24] Yin et al. (2021) [25] Vayani et al. (2025) [26] Romero et al. (2024) [27] Nayak et al. (2024) [28] Nikandrou et al. (2025) [29] Object Zhou et al. (2025) [30] Kim et al. (2025) [31] CU R EMI Object Scene Food, Scene, Object Scene, Object Object, Scene Food (text only) Ethnicity, Background VQA VQA Food, Background Retrieval VQA VQA, Captioning Cultural knowledge Cultural knowledge VQA Cultural knowledge VQA Contextual adaptation VQA Cultural knowledge Probing Cultural bias Cultural knowledge Visual grounding Real Commonsense reasoning Real Real Real Real Real / Hybrid Mostly Text Hybrid Both No 50 countries English English 4 regions No 100 languages No 73 countries No 30 countries 31 languages No 11 countries English 5 languages No 5 Countries No 6 Languages 14 countries Yes 5 countries English Yes 30 countries English 6.2. Results Table 2 compares the entropy and accuracy of three mitigation methods against the base model. Direct prompting and SFT consistently enhance LVLMs performance. Overall, Direct prompting and SFT consistently improve performance across most tasks, though only SFT achieves statistically significant gains (paired T-test, < .01). For MFB and SFB tasks, SFT remarkably reduces entropy by substantial margin while also increasing accuracy, notably leading to more robust performance. In the MF setting, while SFT decreases entropy, Direct prompting demonstrates marginally better effectiveness for robustness. These results suggest that training-free strategies may be sufficient for simple mixing scenarios, but as contexts become more complex, particularly when background elements are involved, training-based approaches like SFT become necessary to adequately equip models with the capability to recognize culturally mixed situations. CoT prompting does not always help. Our results show that CoT prompting sometimes improves accuracy on CultureMix, but fails to enhance and occasionally even degrades label consistency. To understand this discrepancy, we analyze the cases where CoT introduces new errors, and find that it often magnifies the models reliance on background cues, ultimately leading to incorrect predictions. This finding aligns with recent research on LLM reasoning in cultural and social tasks [32], which demonstrates that CoT can overamplify the influence of misleading cues when models need to aggregate information from multiple, potentially conflicting sources/contexts. As our analysis reveals, while prompting shows promise, its limitations highlight the need for training approaches. Our preliminary results indicate that supervised fine-tuning on culturally mixed scenarios yields meaningful improvements. We call for research into training objectives designed for culture mixing, essential for LVLMs operating in culturally diverse contexts. 7. Related Work 7.1. Cultural Awareness of LVLMs The growing recognition that cultural differences impact visual understanding [33, 34] has spurred significant interest in the cultural awareness of visionlanguage models (VLMs) [35]. Consequently, researchers have investigated cultural biases across wide range of tasks, including image captioning [36, 37], text-to-image generation [38, 39], retrieval and visual grounding [24], and safety [40, 41]. Common evaluation approaches include the Visual Question Answering (VQA) setting [2529] and the use of culturally specific datasets, for instance those featuring food from various countries [10, 30]. However, these datasets generally contain images rooted in single cultural context, making it difficult to evaluate how models interpret cultural ambiguity or blended settings. In particular, Kim et al. [31] introduce framework that replaces individuals in real images with people of different ethnic backgrounds to reveal ethnicity-driven biases in recognizing cultural elements. While their work explores the intersection of ethnic background and cultural artifacts, it focuses on isolated elements rather than scenarios where multiple cultural artifacts (e.g., food and clothing) coexist within the same visual scene. As shwon in Table 3, while existing datasets cover various combinations of objects, scenes, and food items across different countries and languages, none explicitly target culture mixing in visual content. In contrast, CultureMix systematically constructs images where multiple cultural elements co-occur, encompassing diverse foods and backgrounds across 30 countries and supporting both real and synthetic images. To the best of our knowledge, this is the first work to leverage such culture-mixing scenarios to reveal how LVLMs behave under cross-cultural ambiguity and to provide benchmark for evaluating and improving their cross-cultural reasoning and generalization. 7.2. Image Fusion and Composition Image fusion, composition, and blending have been extensively studied for applications including object detection, image generation, and data augmentation. Early research focused on integrating information from different sensor modalities to enrich visual data [42, 43]. Image composition methods have facilitated the automated creation of synthetic data and augmentation strategies [44, 45]. These approaches not only enhance model generalization and pretraining efficiency but also provide scalable alternatives to manual annotation. The advent of generative models has enabled the synthesis of multiple concepts and the transfer of visual styles or semantics with high realism [4649]. While existing work has primarily focused on enhancing model capabilities, our research pivots to use these techniques for evaluation. We repurpose image composition to create challenging visual scenarios by combining images with diverse cultural elements to assess the cultural awareness of visionlanguage models. 8. Conclusion This study presents systematic evaluation of LVLMs to investigate their capabilities in culturally entangled visual contexts. By introducing large-scale benchmark of global food and scene images, we reveal that while current LVLMs generally perform well when presented with single food image (SF), the performance decreases with the existence of cultural distractors (MF, MFB, and SFB). Our results suggest that although these models regard new cultural components as distractors rather than integrating them as complementary cues, there is need for interventions to improve crosscultural understanding. To address this, we present trainingfree and training-based approaches on medium-sized opensource models to guide future works in developing mixedcultural-aware LVLMs. We believe our findings and proposed approaches could provide foundation for building LVLMs that reason more effectively across culturally diverse contexts."
        },
        {
            "title": "Limitations and Future Work",
            "content": "While our benchmark covers wide range of global cuisines and scenes, it should be noted that there remains an underexplored cultural context, especially for underrepresented or low-resource regions. Also, our evaluation primarily focuses on food and scene recognition due to the challenges of collecting diverse, high-quality data (and the relatively lower controversy of these cultural elements). We leave for future work to explore other culturally rich domains, such as festivals, clothing, and daily-life interactions present in visual contexts. Finally, incorporating user-centered evaluations (other than ground-truth annotations) from diverse cultural backgrounds would provide richer insights into the practical effectiveness of LVLMs in real-world multicultural scenarios."
        },
        {
            "title": "Acknowledgement",
            "content": "We express our gratitude to Professor Diyi Yang for her insightful feedback, which shaped the direction of this research."
        },
        {
            "title": "References",
            "content": "[1] Jia Hao, Dongmei Li, Luluo Peng, Siqing Peng, and Carlos Torelli. Advancing our understanding of culture mixing. Journal of Cross-Cultural Psychology, 47(10):12571267, 2016. 1 [2] Lee Martin, Bo Shao, and David Thomas. The role of early immersive culture mixing in cultural identifications of multiculturals. J. Cross. Cult. Psychol., 50(4):508523, May 2019. 1 [3] Junjie Ye, Michele J. Gelfand, and Young-Hoon Kim. Multiculturalism, culture mixing, and prejudice: Effects of cultural blending on intergroup attitudes. Frontiers in Psychology, 12:713257, 2021. doi: 10.3389/fpsyg.2021.713257. URL https://pmc.ncbi.nlm.nih.gov/articles/ PMC8343399/. [4] Roy Y. J. Chua. Making sense of cultural diversitys complexity: Addressing culture mixing in leadCross Cultural & Strategic Management, 30 ership. (4):812830, 2023. doi: 10.1177/ 14705958231214623. URL https://journals.sagepub.com/doi/10. 1177/14705958231214623. 1 [5] Olena Burda-Lassen, Aman Chadha, Shashank Goswami, and Vinija Jain. How culturally aware are vision-language models? In 2025 IEEE 6th International Conference on Image Processing, Applications and Systems (IPAS), pages 16. IEEE, 2025. 2 [6] Shudong Liu, Yiqiao Jin, Cheng Li, Derek Wong, Qingsong Wen, Lichao Sun, Haipeng Chen, Xing Xie, and Jindong Wang. Culturevlm: Characterizing and improving cultural understanding of vision-language models for over 100 countries. arXiv preprint arXiv:2501.01282, 2025. [7] Srishti Yadav, Zhi Zhang, Daniel Hershcovich, and Ekaterina Shutova. Beyond words: Exploring cultural value sensitivity in multimodal models. arXiv preprint arXiv:2502.14906, 2025. 2 [8] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas MÃ¼ller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. URL https://arxiv.org/abs/2506.15742. 3 [9] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. 3 [10] Genta Indra Winata, Frederikus Hudi, Patrick Amadeus Irawan, David Anugraha, Rifki Afina Putri, Wang Yutong, Adam Nohejl, Ubaidillah Ariq Prathama, Nedjma Ousidhoum, Afifa Amriani, Anar Rzayev, Anirban Das, Ashmari Pramodya, Aulia Adila, Bryan Wilie, Candy Olivia Mawalim, Cheng Ching Lam, Daud Abolade, Emmanuele Chersoni, Enrico Santus, Fariz Ikhwantri, Garry Kuwanto, Hanyang Zhao, Haryo Akbarianto Wibowo, Holy Lovenia, Jan Christian Blaise Cruz, Jan Wira Gotama Putra, Junho Myung, Lucky Susanto, Maria Angelica Riera Machin, Marina Zhukova, Michael Anugraha, Muhammad Farid Adilazuarda, Natasha Christabelle Santosa, Peerat Limkonchotiwat, Raj Dabre, Rio Alexander Audino, Samuel Cahyawijaya, Shi-Xiong Zhang, Stephanie Yulia Salim, Yi Zhou, Yinxuan Gui, David Ifeoluwa Adelani, En-Shiun Annie Lee, Shogo Okada, Ayu Purwarianti, Alham Fikri Aji, Taro Watanabe, Derry Tanti Wijaya, Alice Oh, and Chong-Wah Ngo. WorldCuisines: massive-scale benchmark for multilingual and multicultural visual question answering on global cuisines. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 32423264, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-189-6. doi: 10.18653/v1/2025.naacl-long.167. URL https://aclanthology.org/2025.naacllong.167/. 3, 8 [11] Jabez Magomere, Shu Ishida, Tejumade Afonja, Aya Salama, Daniel Kochin, Yuehgoh Foutse, Imane Hamzaoui, Raesetje Sefala, Aisha Alaagib, Samantha Dalal, Beatrice Marchegiani, Elizaveta Semenova, Lauren Crais, and Siobhan Mackenzie Hall. The world wide recipe: communitycentred framework for fine-grained data collection and the regional bias operationalisation. 2025 ACM Conference on Fairness, Accountability, and Transparency, FAccT 25, page 246282, New York, NY, USA, 2025. Association for Computing Machinery. ISBN 9798400714825. doi: 10.1145/3715275.3732019. URL https://doi.org/10.1145/3715275.3732019. 3 In Proceedings of [12] Omran Alamayreh, Giovanna Maria Dimitri, Jun Wang, Benedetta Tondi, and Mauro Barni. Which country is this picture from? new data and methods for dnn-based counIn ICASSP 2023 - 2023 IEEE Internatry recognition. tional Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15, 2023. doi: 10.1109/ICASSP49357. 2023.10094908. 3 [13] Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Google Landmarks Dataset v2 Large-Scale Sim. Benchmark for Instance-Level Recognition and Retrieval In 2020 IEEE/CVF Conference on Computer Vision . and Pattern Recognition (CVPR), pages 25722581, Los Alamitos, CA, USA, June 2020. IEEE Computer Society. doi: 10.1109/CVPR42600.2020.00265. URL https: / / doi . ieeecomputersociety . org / 10 . 1109 / CVPR42600.2020.00265. 3 [14] Jing Li, Xue Yang, Xiaoli Lu, and Dengsheng Wu. Making journals more international: Language subject differences and impact performance. Learned Publishing, 36(4):596618, 2023. doi: 10.1002/leap.1567. 4 [15] OpenAI. Gpt-5 system card. https://cdn.openai. com/gpt-5-system-card.pdf, August 2025. 4 [16] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 4 [17] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Xuehui Wang, Yue Cao, Yangzhou Liu, Xingguang Wei, Hongjie Zhang, Haomin Wang, Weiye Xu, Hao Li, Jiahao Wang, Nianchen Deng, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Conghui He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Yingtong Xiong, Wenwen Qu, Peng Sun, Penglong Jiao, Han Lv, Lijun Wu, Kaipeng Zhang, Huipeng Deng, Jiaye Ge, Kai Chen, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, 2025. URL https://arxiv.org/abs/2504.10479. 4 [18] Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye. Ovis: Structural embedding alignment for multimodal large language model. arXiv:2405.20797, 2024. 4 [19] Shiyin Lu, Yang Li, Yu Xia, Yuwei Hu, Shanshan Zhao, Yanqing Ma, Zhichao Wei, Yinglun Li, Lunhao Duan, Jianshan Zhao, Yuxuan Han, Haijun Li, Wanying Chen, Junke Tang, Chengkun Hou, Zhixing Du, Tianli Zhou, Wenjie Zhang, Huping Ding, Jiahe Li, Wen Li, Gui Hu, Yiliang Gu, Siran Yang, Jiamang Wang, Hailong Sun, Yibo Wang, Hui Sun, Jinlong Huang, Yuping He, Shengze Shi, Weihong Zhang, Guodong Zheng, Junpeng Jiang, Sensen Gao, YiFeng Wu, Sijia Chen, Yuhui Chen, Qing-Guo Chen, Zhao Xu, Weihua Luo, and Kaifu Zhang. Ovis2.5 technical report. arXiv:2508.11737, 2025. [20] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5vl technical report. arXiv preprint arXiv:2502.13923, 2025. 4 [21] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, and Aniruddha Kembhavi. Molmo and pixmo: Open weights and open data for state-of-the-art vision-language models, 2024. URL https://arxiv.org/abs/2409. 17146. 4 [22] Joseph Henrich, Steven Heine, and Ara Norenzayan. The weirdest people in the world? Behavioral and brain sciences, 33(2-3):6183, 2010. 5 [23] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-ofthought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35: 2482424837, 2022. 7 [24] Mehar Bhatia, Sahithya Ravi, Aditya Chinchure, EunJeong Hwang, and Vered Shwartz. From local concepts to universals: Evaluating the multicultural understanding of visionlanguage models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 67636782, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlpmain.385. URL https:// aclanthology.org/2024.emnlp-main.385/. [25] Da Yin, Liunian Harold Li, Ziniu Hu, Nanyun Peng, and Kai-Wei Chang. Broaden the vision: Geo-diverse visual commonsense reasoning. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2115 2129, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlpmain.162. URL https:// aclanthology.org/2021.emnlp-main.162/. 8 [26] Ashmal Vayani, Dinura Dissanayake, Hasindri Watawana, Noor Ahsan, Nevasini Sasikumar, Omkar Thawakar, Henok Biadglign Ademtew, Yahya Hmaiti, Amandeep Kumar, Kartik Kukreja, Mykola Maslych, Wafa Al Ghallabi, Mihail Minkov Mihaylov, Chao Qin, Abdelrahman M. Shaker, Mike Zhang, Mahardika Krisna Ihsani, Amiel Gian Esplana, Monil Gokani, Shachar Mirkin, Harsh Singh, Ashay Srivastava, Endre Hamerlik, Fathinah Asma Izzati, Fadillah Adamsyah Maani, Sebastian Cavada, Jenny Chim, Rohit Gupta, Sanjay Manjunath, Kamila Zhumakhanova, Feno Heriniaina Rabevohitra, Azril Hafizi Amirudin, Muhammad Ridzuan, Daniya Najiha Abdul Kareem, Ketan Pravin More, Kunyang Li, Pramesh Shakya, Muhammad Saad, Amirpouya Ghasemaghaei, Amirbek Djanibekov, Dilshod Azizov, Branislava Jankovic, Naman Bhatia, Alvaro Cabrera, Johan Obando-Ceron, Olympiah Otieno, Febian Farestam, Muztoba Rabbani, Sanoojan Ballah, Santosh Sanjeev, Abduragim Shtanchaev, Maheen Fatima, Thao Nguyen, Amrin Kareem, Toluwani Aremu, Nathan Augusto Zacarias Xavier, Amit Bhatkal, Hawau Olamide Toyin, Aman Chadha, Hisham Cholakkal, Rao Muhammad Anwer, Michael Felsberg, Jorma Laaksonen, Thamar Solorio, Monojit Choudhury, Ivan Laptev, Mubarak Shah, Salman Khan, and Fahad Shahbaz Khan. All languages matter: Evaluating lmms on culturally diverse 100 languages. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pages 1956519575, June 2025. 8 [27] David Romero, Chenyang Lyu, Haryo Akbarianto Wibowo, Teresa Lynn, Injy Hamed, Aditya Nanda Kishore, Aishik Mandal, Alina Dragonetti, Artem Abzaliev, Atnafu Lambebo Tonja, Bontu Fufa Balcha, Chenxi Whitehouse, Christian Salamea, Dan John Velasco, David Ifeoluwa Adelani, David Le Meur, Emilio Villa-Cueva, Fajri Koto, Fauzan Farooqui, Frederico Belcavello, Ganzorig Batnasan, Gisela Vallejo, Grainne Caulfield, Guido Ivetta, Haiyue Song, Henok Biadglign Ademtew, HernÃ¡n Maina, Holy Lovenia, Israel Abebe Azime, Jan Christian Blaise Cruz, Jay Gala, Jiahui Geng, Jesus-German Ortiz-Barajas, Jinheon Baek, Jocelyn Dunstan, Laura Alonso Alemany, Kumaranage Ravindu Yasas Nagasinghe, Luciana Benotti, Luis Fernando D' Haro, Marcelo Viridiano, Marcos Estecha-Garitagoitia, Maria Camila Buitrago Cabrera, Mario RodrÃ­guez-Cantelar, MÃ©lanie Jouitteau, Mihail Mihaylov, Naome Etori, Mohamed Fazli Mohamed Imam, Muhammad Farid Adilazuarda, Munkhjargal Gochoo, Munkh-Erdene Otgonbold, Olivier Niyomugisha, Paula MÃ³nica Silva, Pranjal Chitale, Raj Dabre, Rendi Chevi, Ruochen Zhang, Ryandito Diandaru, Samuel Cahyawijaya, Santiago GÃ³ngora, Soyeong Jeong, Sukannya Purkayastha, Tatsuki Kuribayashi, Teresa Clifford, Thanmay Jayakumar, Tiago Timponi Torrent, Toqeer Ehsan, Vladimir Araujo, Yova Kementchedjhieva, Zara Burzo, Zheng Wei Lim, Zheng Xin Yong, Oana Ignat, Joan Nwatu, Rada Mihalcea, Thamar Solorio, and Alham Fikri Aji. Cvqa: Culturally-diverse multilingual visual question answering benchmark. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 1147911505. Curran Associates, Inc., 2024. URL https://proceedings. neurips.cc/paper_files/paper/2024/file/ 1568882ba1a50316e87852542523739c - Paper - Datasets_and_Benchmarks_Track.pdf. 8 [28] Shravan Nayak, Kanishk Jain, Rabiul Awal, Siva Reddy, Sjoerd Van Steenkiste, Lisa Anne Hendricks, Karolina Stanczak, and Aishwarya Agrawal. Benchmarking vision language models for cultural understanding. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 57695790, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlpmain.329. URL https: //aclanthology.org/2024.emnlp-main.329/. 8 [29] Malvina Nikandrou, Georgios Pantazopoulos, Nikolas Vitsakis, Ioannis Konstas, and Alessandro Suglia. CROPE: Evaluating in-context adaptation of vision and language models to culture-specific concepts. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 7917 7936, Albuquerque, New Mexico, April 2025. Association ISBN 979-8-89176-189-6. for Computational Linguistics. the Americas Chapter of the Nations of doi: 10.18653/v1/2025.naacllong.402. URL https: //aclanthology.org/2025.naacl-long.402/. 8 the Nations of [30] Li Zhou, Taelin Karidi, Wanlong Liu, Nicolas Garneau, Yong Cao, Wenyu Chen, Haizhou Li, and Daniel Hershcovich. Does mapo tofu contain coffee? probing LLMs for foodrelated cultural knowledge. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 9840 9867, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-189-6. doi: 10.18653/v1/2025.naacllong.496. URL https: //aclanthology.org/2025.naacl-long.496/. 8 the Americas Chapter of [31] Jun Seong Kim, Kyaw Ye Thu, Javad Ismayilzada, Junyeong Park, Eunsu Kim, Huzama Ahmad, Na Min An, James Thorne, and Alice Oh. WHEN TOM EATS KIMCHI: Evaluating cultural awareness of multimodal large language models in cultural mixture contexts. In Vinodkumar Prabhakaran, Sunipa Dev, Luciana Benotti, Daniel Hershcovich, Yong Cao, Li Zhou, Laura Cabello, and Ife Adebara, editors, Proceedings of the 3rd Workshop on CrossCultural Considerations in NLP (C3NLP 2025), pages 143 154, Albuquerque, New Mexico, May 2025. Association for Computational Linguistics. ISBN 979-8-89176-237doi: 10.18653/v1/2025.c3nlp1.11. URL https: 4. //aclanthology.org/2025.c3nlp-1.11/. 8 [32] Eunsu Kim, Junyeong Park, Juhyun Oh, Kiwoong Park, Seyoung Song, A. Seza DoÄruÃ¶z, Najoung Kim, and Alice Oh. Are they lovers or friends? evaluating llms social reasoning in english and korean dialogues, 2025. URL https://arxiv.org/abs/2510.19028. [33] Andre Ye, Sebastin Santy, Jena D. Hwang, Amy X. Zhang, and Ranjay Krishna. Semantic and expressive variations In Proceedings of in image captions across languages. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2966729679, June 2025. 8 [34] Uri Berger and Edoardo Ponti. Cross-lingual and crosscultural variation in image descriptions. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 9453 9465, Albuquerque, New Mexico, April 2025. Association ISBN 979-8-89176-189-6. for Computational Linguistics. doi: 10.18653/v1/2025.naacllong.478. URL https: //aclanthology.org/2025.naacl-long.478/. 8 [35] Siddhesh Pawar, Junyeong Park, Jiho Jin, Arnav Arora, Junho Myung, Srishti Yadav, Faiz Ghifari Haznitrama, Inhwa Song, Alice Oh, and Isabelle Augenstein. Survey of cultural awareness in language models: Text and beyond. Computational Linguistics, 51(3):9071004, 09 2025. ISSN 0891-2017. doi: 10.1162/COLI.a.14. URL https://doi.org/10. 1162/COLI.a.14. 8 [36] Youssef Mohamed, Runjia Li, Ibrahim Said Ahmad, Kilichbek Haydarov, Philip Torr, Kenneth Church, and Mohamed Elhoseiny. No culture left behind: ArtELingo-28, benchmark of WikiArt with captions in 28 languages. In Yaser AlOnaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2093920962, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.1165. URL https://aclanthology.org/2024.emnlpmain.1165/. 8 [37] Olena Burda-Lassen, Aman Chadha, Shashank Goswami, and Vinija Jain. How culturally aware are vision-language models? In 2025 IEEE 6th International Conference on Image Processing, Applications and Systems (IPAS), volume CFP2540Z-ART, pages 16, 2025. doi: 10.1109/IPAS63548. 2025.10924504. 8 [38] Nithish Kannen, Arif Ahmad, marco Andreetto, Vinodkumar Prabhakaran, Utsav Prabhu, Adji Bousso Dieng, Pushpak Bhattacharyya, and Shachi Dave. Beyond aesthetics: Cultural competence in text-to-image models. In The Thirtyeight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. URL https: //openreview.net/forum?id=4351SumKS9. 8 [39] Zahra Bayramli, Ayhan Suleymanzade, Na Min An, Huzama Ahmad, Eunsu Kim, Junyeong Park, James Thorne, and Alice Oh. Diffusion models through global lens: Are they culturally inclusive? In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3113731155, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-2510. doi: 10.18653/v1/2025.acl-long.1503. URL https: //aclanthology.org/2025.acl-long.1503/. 8 [40] Minh Duc Bui, Katharina Von Der Wense, and Anne Lauscher. Multi3Hate: Multimodal, multilingual, and multicultural hate speech detection with visionlanguage models. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 97149731, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 9798-89176-189-6. doi: 10.18653/v1/2025.naacllong.490. URL https://aclanthology.org/2025.naacllong.490/. [41] Akhila Yerukola, Saadia Gabriel, Nanyun Peng, and Maarten Sap. Mind the gesture: Evaluating AI sensitivity to culturally offensive non-verbal gestures. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2504125080, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176251-0. doi: 10.18653/v1/2025.acl-long.1218. URL https: //aclanthology.org/2025.acl-long.1218/. 8 [42] Mohammed Ali Saleh, AbdElmgeid A. Ali, Kareem Ahmed, and Abeer M. Sarhan. brief analysis of multimodal medical image fusion techniques. Electronics, 12(1), 2023. ISSN 2079-9292. doi: 10.3390/electronics12010097. URL https://www.mdpi.com/2079-9292/12/1/97. 9 [43] Jinyuan Liu, Xin Fan, Zhanbo Huang, Guanyao Wu, Risheng Liu, Wei Zhong, and Zhongxuan Luo. Target-aware dual adversarial learning and multi-scenario multi-modality benchmark to fuse infrared and visible for object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 58025811, 2022. 9 [44] Xiaoshuai Hao, Yi Zhu, Srikar Appalaraju, Aston Zhang, Wanqian Zhang, Bo Li, and Mu Li. Mixgen: new multimodal data augmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) Workshops, pages 379389, January 2023. 9 [45] Chaoya Jiang, Wei Ye, Haiyang Xu, Qinghao Ye, Ming Yan, Ji Zhang, and Shikun Zhang. Timix: text-aware image mixing for effective vision-language pre-training. In Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence, AAAI24/IAAI24/EAAI24. AAAI Press, 2024. ISBN 978-1-57735-887-9. doi: 10.1609/aaai.v38i3.28025. URL https://doi.org/10.1609/aaai.v38i3. 28025. 9 [46] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua Tenenbaum. Compositional visual generation with In Computer VisionECCV composable diffusion models. 2022: 17th European Conference, Tel Aviv, Israel, October 2327, 2022, Proceedings, Part XVII, pages 423439. Springer, 2022. 9 [47] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of textto-image diffusion. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [48] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models, 2023. [49] Jun Hao Liew, Hanshu Yan, Daquan Zhou, and Jiashi Feng. Magicmix: Semantic mixing with diffusion models. arXiv preprint arXiv:2210.16056, 2022. A. Pipeline Details of CultureMix A.1. Country Lists and Statistics Table 4 summarizes, for each country, the number of food dishes and their names. Figure 9 visualizes the distribution of food combinations in MF, showing that the combinations are diverse and well balanced. A.2. Human Validation and Statistics We incorporated human-in-the-loop process throughout all stages of dataset construction. Human annotators validated the generated images, and those that did not meet the quality criteria were regeneratedeither using the same model or more advanced onefollowed by another round of human validation. This iterative process was repeated until the final dataset was completed. The criteria used for validation and the statistics of filtered cases are as follows. Generating Single Food (SF) Images 1. Background removal (a) If the image contains text, hands, people, or tableware, these elements are removed using either diffusion model or manual methods. 2. Human ValidationComparison with the original image (manually checked by the author using an in-house platform) (a) Criteria (Error Types) Image differs from the original Regenerate Food item is cropped Regenerate Tableware appears in the image Regenerate More than one food item appears Regenerate Text appears on the food Manually hide the text Reference food itself is inappropriate (e.g., food placed on cauldron, making it unsuitable for table placement) Replace food image Other cases where the image appears unnatural Replace food image (b) Statistics Regeneration cases: 109 / 295 Food image replacement cases: 12 / 295 Images with text present: 1 / 295 Generating Multiple Food (MF) Images 1. Image Generation Concatenate the two images generated in Step 2 to create MF image. 2. Human Validation (a) Criteria (Error Types) Is the food item cropped? Regenerate Are the two food items excessively unbalanced in size? Regenerate (b) Statistics Regeneration cases: 74 / 948 Single Food with Background (SFB) and Multi-Food with Background (MFB) images 1. Background Image Selection For each category (Street, Landmark), five background images were manually selected from different continents. 2. Input Image Generation Concatenate the MF images beneath the selected background image. 3. Editing Example prompt (the first two versions were used when initial generations failed): 4. Human Validation (a) Criteria Background not generated (showing only the food item) Regenerate Food placed unnaturally (e.g., floating in the air or positioned upright as if it would spill) Regenerate Food or background differs from the original Regenerate (b) Statistics We repeated multiple rounds of refinement. After three attempts, most samples were collected as valid, with only few remaining invalid. These remaining cases were not verified through the platform but were instead visually inspected and regenerated until satisfactory. 1st attempt: Valid/Total = 0.72 2nd attempt: Valid/Total = 0.70 3rd attempt: Valid/Total = 0.88 A.3. Prompts and Model Configurations for Synthetic Dataset Generation We provide details on how the image editing models were used to generate SF, MF, SFB, and MFB images. The specific model names and configurations are summarized in Table 5, and the corresponding prompts are provided below. Note that we used minor prompt variants (e.g., reordering sentences, substituting synonymous verbs) to regenerate images when the initial output failed to satisfy the required criteria of the human annotators. An example of input Images for diffusion model is shown in Figure 10. For transparency, we also provide qualitative examples of image synthesis failures that were filtered out during human validation (Figure 11). These cases illustrate scenarios where the generated images are incomplete or misaligned with the given textual prompt. Based on our results, our synthetic images provide useful insights into the generation pipeline and can help guide future efforts in constructing culturemixing datasets. idx Country Count Food1 Names 1 United States Of America (USA) 2 3 4 5 China France Indonesia Kenya 6 Mexico 7 8 10 11 12 13 14 15 17 18 19 20 21 22 24 25 26 27 28 29 30 31 Philippines Russia South Africa Spain Italy Japan United Kingdom Egypt India Korea Nigeria Portugal Algeria Germany Thailand England Iran Cameroon Croatia Nepal Pakistan Singapore Turkey Poland Sudan 10 10 10 10 10 10 10 10 9 9 8 8 8 8 8 7 7 6 6 5 5 5 5 5 5 4 4 1. Baked beans; 2. Carolina-style pulled pork/barbecue; 3. Chocolate chip cookie; 4. co-shon duh lay; 5. Dutch letter; 6. Eggs Benedict; 7. Hamburger; 8. Jum-buh-lie-ahh; 9. Key Lime Pie; 10. Loco moco; 11. Mochi ice cream; 12. Pecan pie; 13. Pepper jelly; 14. Pie Ã  la Mode; 15. Pumpkin pie; 16. Rainbow cookie; 17. Southern fried chicken; 18. Spaghetti and meatballs; 19. Tootsie Roll 1. Century egg; 2. Cong you bing; 3. Edamame; 4. Lo mai gai; 5. Mantau; 6. Osmanthus cake; 7. Paper wrapped cake; 8. Yangzhou Fried Rice; 9. Yin yang fried rice; 10. Yong tau foo 1. Beef bourguignon; 2. Bouneschlupp; 3. Chouquette; 4. Croissant; 5. GÃ¢teau Basque; 6. Ladyfinger; 7. Nuns puffs; 8. Ratatouille; 9. SablÃ©; 10. Teurgoule 1. Bakpia; 2. Kue mangkok; 3. Kue putu; 4. Laksa; 5. Lakso; 6. Nasi Uduk; 7. Sayur asem; 8. Siomay; 9. Tahu campur; 10. Tahu sumedang 1. Bacella alba; 2. Biegnets; 3. Cha-pa-ti or Cha-poh; 4. Githeri; 5. Karaage; 6. Kenyan kachumbari salad; 7. Maize and beans stew; 8. Matoke; 9. Mukimo; 10. Pound maize flour 1. Chilaquiles; 2. Chile relleno; 3. Huarache; 4. Migas; 5. Paste (pasty); 6. PiÃ±ata cookie; 7. Puchero; 8. Refried beans; 9. Salsa verde; 10. Sope 1. Aparon; 2. Beef Steak; 3. Laing; 4. Oyster omelette; 5. Puto; 6. Rendang; 7. Silog; 8. Sushi Bake; 9. Taho; 10. Uraro 1. Alexandertorte; 2. Bracken fern salad; 3. Mimosa salad; 4. Potato pancake; 5. Pozharsky cutlet; 6. Shchi; 7. Solyanka; 8. Ukha; 9. Vinegret; 10. Zefir 1. African spinach; 2. Bunny chow; 3. Chicken and mushroom pie; 4. Hertzoggie; 5. kota, skhambane; 6. Malva pudding; 7. Melktert; 8. Mopane stew; 9. Potjiekos; 10. tripe 1. Andrajos; 2. ArrÃ²s negre; 3. Cocido lebaniego; 4. Cocido madrileÃ±o; 5. EnsaÃ¯mada; 6. Escudella; 7. Hamin; 8. Hornazo; 9. Panellets; 10. Tortillitas de camarones 1. Cannoli; 2. Casoncelli; 3. Cavallucci; 4. Cotoletta alla milanese; 5. Florentine biscuit; 6. Gelato; 7. Michetta; 8. Piadina romagnola; 9. Piccata 1. Amanatto; 2. Char siu; 3. Dorayaki; 4. Fried ice cream; 5. Melonpan; 6. no; 7. Omurice; 8. Simmered dried strips of daikon radish; 9. Tempura 1. Bedfordshire clanger; 2. Blackberry pie; 3. Empire biscuit; 4. Ham and cheese sandwich; 5. Pease pudding; 6. Saveloy; 7. Spotted dick; 8. Stornoway black pudding; 9. Treacle sponge pudding 1. Custard tart; 2. Falafel; 3. Koshary; 4. Molokhia; 5. mombar; 6. Qatayef; 7. Um ali 1. Bhel puri; 2. Curd rice; 3. Neer dosa; 4. Panta bhat; 5. Pulihora; 6. Rice and curry; 7. Thalassery biryani; 8. Unnakai 1. Bibim guksu; 2. Bibimbap; 3. Japchae; 4. Jeon; 5. Milmyeon; 6. Sundubu-jjigae; 7. Tteokbokki; 8. Tteokguk 1. Bridie; 2. Cooked cassava flakes and Okra soup; 3. E/ku/ru; 4. Ekwang; 5. Lupis; 6. Moin moin; 7. Okra Soup; 8. Vegetable soup with Egusi 1. Aletria; 2. Bacalhau Ã  Gomes de SÃ¡; 3. Cabidela; 4. Cebolada; 5. MocotÃ³; 6. Pastel de TentÃºgal; 7. Polvo Ã  lagareiro; 8. Tripas Ã  moda do Porto 1. Algerian Almond Cookies; 2. Algerian Mekrout; 3. Besbousa; 4. Chermoula; 5. Harira; 6. Tamina; 7. Tikerbabin 1. Edi-kang Ikong; 2. Eisbein; 3. Kaiserschmarrn; 4. Maultasche; 5. Poppy seed roll; 6. Toast Hawaii 1. Khanom bueang; 2. Khao kan chin; 3. Khao soi; 4. Kluai khaek; 5. Mi krop; 6. Pad thai; 7. Sakhu sai mu 1. Bacon and egg pie; 2. Bakewell tart; 3. Bath bun; 4. Curry pie; 5. Fish pie; 6. Steak pie 1. Baghali polo; 2. Chelow; 3. Kashk bademjan; 4. Mirza ghassemi; 5. Reshteh khoshkar; 6. Sajji 1. Achu; 2. leaves of gnetum; 3. Plantain Chips; 4. Puff Puff, beans and pape; 5. rice with beans 1. Brudet; 2. Grahova pretepena juha; 3. MeÄimurska gibanica; 4. soparnik; 5. Zagorski Å¡trukli 1. Chataamari; 2. Chhurpi; 3. Gajar ka halwa; 4. Kwati; 5. Sapu Mhicha 1. Amba; 2. Bun kebab; 3. Momo; 4. Phirni; 5. Zarda 1. Banmian; 2. Bihun Goreng; 3. Chwee kueh; 4. Noodles with tomato egg sauce; 5. Turnip cake 1. Bamia; 2. Cezerye; 3. Kuru fasulye; 4. Qurabiya; 5. ÅÃ¶biyet 1. cabbage rolls; 2. Dumplings; 3. GoÅÄbki; 4. Krumiri 1. Cucumber salad with yogurt; 2. Khachapuri; 3. LoQeymat or zalabia; 4. Vanille cake Table 4. Unique food counts and names per country included in CultureMix. Figure 9. Visualization of food combinations in MF and MFB. Figure 10. An example of input Images for diffusion model based image editing. Figure 11. Sample SF and SFB error images. These images did not meet the human evaluation criteria and illustrate common failure patterns observed during the filtering process. Prompts for SF Image Generation Leave all the food quality the exact same as the original. Modify the background image to pure white. Make the image square. Change the food into top-down view. Convert the food to top-down view. Remove any spoons, chopsticks, and human hands. Add any missing parts of plate or bowl. The plate or bowl should be circle or oval. Prompts for SFB Image Generation (FLUX) Change the white background underneath single food item to table or picnic mat. The table or picnic mat should seamlessly be integrated with the background image. Rotate the food items along the z-axis so they are viewed from natural dining perspective not from the top, but more like how someone sees the plate while sitting at table, and add realistic shadows to the plates. Remove any hands and utensils on the plates. Reconstruct the plate if there isnt any or if its broken. Keep the background and the food quality identical as the original. The generated image should have only one food item. Prompts for SFB Image Generation (QWEN) Leave all the elements the exact same as the original except for the followings: Add table or picnic mat underneath the food item. The table or picnic mat should seamlessly be integrated with the background image. Rotate the food items along the z-axis so they are viewed from natural dining perspective not from the top, but more like how someone sees the plate while sitting at table, and add realistic shadows to the plates. Remove any hands and utensils on the plates. Reconstruct the plate if there isnt any or if its broken. The generated image should have only one food item. Prompts for MFB Image Generation (FLUX) Change the white background underneath two food items to table or picnic mat. The table or picnic mat should seamlessly be integrated with the background image. Rotate the food items along the z-axis so they are viewed from natural dining perspective not from the top, but more like how someone sees the plate while sitting at table, and add realistic shadows to the plates. Remove any hands and utensils on the plates. Reconstruct the plate if there isnt any or if its broken. Keep the background and the food quality identical as the original. The generated image should have only two food items. Prompts for MFB Image Generation (QWEN) Leave all the elements the exact same as the original except for the following: Add table or picnic mat underneath the food item. The table or picnic mat should seamlessly be integrated with the background image. Rotate the food items along the z-axis so they are viewed from natural dining perspective not from the top, but more like how someone sees the plate while sitting at table, and add realistic shadows to the plates. Remove any hands and utensils on the plates. Reconstruct the plate if there isnt any or if its broken. The generated image should have only two food items. Table 5. Models and Configurations. Overview of the models and their specific settings used for image synthesis. Category Details Models Configs black-forest-labs/FLUX.1-Kontext-dev Qwen/Qwen-Image-Edit guidance_scale = 2.5 num_inference_steps = 50 torch.bfloat16 A.4. Sample Dataset Images The complete background image set, consisting of 25 landmark images and 25 street images, is shown in Figure 12 and Figure 13, respectively. Final image generation samples Figure 14 presents examples of generated culture-mixing images from our constructed dataset. These samples illustrate the diversity of cross-cultural compositions produced during the construction of our dataset, illustrating variations in food types and cultural geographic backgrounds. These images reflect the range of visual configurations used to evaluate how LVLMs interpret cultural signals when multiple cultural elements coexist within single image. Real-world image samples To complement generated images, we also collect real-world image set featuring naturally occurring food pairings (Figure 15). We categorize half of the collected images into same-country pairs and the other into cross-country pairs. Images were also collected based on variety of visual complexity, reflecting common sources of variation in real photographs such as mixed lighting, cluttered environments, irregular plating, and inconsistent camera perspectives. Figure 12. Background (Landmark) Images. Figure 13. Background (Street) Images. Figure 14. Image Generation Examples. Each row of composite images shows each food from different countries placed in multiple diverse global backgrounds. Results illustrate the cultural combinations represented in our dataset. Figure 15. Real World Dataset Example. Same-country food pairs (left) and cross-country pairs (right) are shown across spectrum of visual complexity, reflecting the diversity of appearance, plating, and scene structure encountered in real images B.3. Qualitative Analysis on Food Name Prediction"
        },
        {
            "title": "Failure Cases",
            "content": "Figure 16. Similar Food Prediction Examples. Gemini predicted Vanille cake as Gugelhupf, and InternVL3-8B predicted Carolinastyle pulled pork as Pernil. In both cases, the predicted dishes are visually similar to the ground-truth foods yet are distinct items. We sampled 25 instances of incorrect food-name predictions from each subtask (SF, MF, SFB, MFB) for Gemini-2.5pro and InternVL3-8B, resulting in 100 images per model. We then manually checked whether the models predicted food label visually resembled the ground-truth food (i.e., the model confused it with similar-looking dish) or whether it was entirely unrelated, using Google Image search results for the predicted food as reference. Gemini-2.5-pro predicted visually similar food in 25 out of 100 cases, whereas InternVL3-8B did so in only 2 out of 100. This indicates that both models often predict completely different foods, but between the two, Gemini-2.5-pro tended to make closer guesses, whereas InternVL3-8Bs predictions showed little resemblance to the target food. Figure 16 shows examples of similar food prediction for each model. B. Experiments B.1. Results Country and Food Name Identification Accuracy Table 6 reports each models country and food name identification accuracy across subtasks, providing additional detail that complements the radar chart in Figure 3a in the main text. Figure 17 visualizes how prediction correctness shifts from SF to the mixed subtasks (SFB, MF, MFB), showing that culturally mixed contexts often cause models to fail on cases they initially predicted correctly. Also, Figure 18 repeats Figure 3b in the main text with larger size for closer inspection. Additionally, Table 7 compares the models country and food name identification accuracy across landmark and street backgrounds. The performance difference between these two background types was minimal for both SFB and MFB. Figure 19 compares the country, food name identification accuracy, and entropy according to different cultural distances between the target and the distractor, which complements Figure 5 in the main text. Country and Food Name Identification Entropy Table 8 reports each models country and food name identification entropy across subtasks, providing additional detail that complements Figure 6 in the main text. Real World Dataset Table 9 reports each models country and food name identification accuracy of the real-world dataset, providing additional detail that complements the radar chart in Figure 8 in the main text. B.2. Ablations Positional Bias To examine the effect of positional bias with respect to food location on the model performance, we randomly sample 100 multi-food images and compare the predicted labels before and after horizontally flipping each image. As shown in Table 10, the high consistency between the original and flipped predictions indicates that the models outputs are stable under leftright reversals, suggesting minimal to no positional dependence for foodrelated attributes. Size Bias We also investigate the effect of the food item sizes on the model performance by first comparing the relative sizes of food items appearing on the left and right within multi-food images and then evaluating whether differing size ratios lead to changes in the predicted labels. Table 11 demonstrates that the model exhibits minimal sizerelated bias when identifying food items. In other words, even when one food item is noticeably larger than the other, the models identification accuracy remains stable, suggesting that its predictions are largely invariant to object size differences. Table 6. Identification accuracy performance. Most LVLMs show relatively higher accuracy in single food settings (SFB and SF) compared to that of multiple food settings (MFB and MF) for both country and food name identification. Bold and underline indicate the highest and lowest accuracy among subtasks, respectively. Model SF (N=988) MF (N=948) SFB (N=12,350) MFB (N=9,485) (a) Country Identification Gemini-2.5-Pro GPT-5 InternVL3-78B InternVL3-38B InternVL3-14B InternVL3-8B Molmo-72B QwenVL3-32B QwenVL3-8B Ovis-9B Avg. 0.457 0.487 0.234 0.205 0.161 0.152 0.139 0.242 0.252 0.285 0.261 0.499 0.450 0.231 0.199 0.170 0.140 0.129 0.213 0.227 0.263 0.252 0.286 0.250 0.110 0.088 0.075 0.065 0.080 0.077 0.060 0.133 0. 0.313 0.271 0.125 0.112 0.087 0.071 0.081 0.124 0.124 0.148 0.146 Model SF (N=988) MF (N=948) SFB (N=12,350) MFB (N=9,485) (b) Name Identification Gemini-2.5-Pro GPT-5 InternVL3-78B InternVL3-38B InternVL3-14B InternVL3-8B Molmo-72B QwenVL3-32B QwenVL3-8B Ovis-9B Avg. 0.399 0.379 0.128 0.100 0.076 0.070 0.090 0.164 0.145 0.152 0.170 0.435 0.371 0.113 0.093 0.071 0.072 0.093 0.079 0.129 0.112 0. 0.252 0.199 0.065 0.056 0.043 0.035 0.067 0.045 0.034 0.077 0.087 0.268 0.212 0.069 0.062 0.045 0.040 0.058 0.084 0.071 0.075 0.098 Figure 17. Sankey diagram of country prediction for each LVLM. We visualize how prediction correctness shifts from SF to the mixed subtasks (SFB, MF, MFB). Green indicates True True, Red indicates True False, Blue indicates False True, and Gray indicates False False. Although small portion of predictions fall into Blue (False True), much larger portion appears in Red (True False), showing that culturally mixed contexts confuse the models and often cause them to fail on cases they initially predicted correctly. While closed-source models perform better on country identification in SF, they also exhibit substantial True False shifts in the mixed subtasks, resulting in reduced accuracy in culturally mixed settings. Figure 18. Country identification target-prediction heatmaps for each subtask. For every golden country, the plots show the distribution of predicted countries, illustrating both correct predictions and systematic confusions across models. The figure is repeated here in larger size to facilitate closer examination. Table 7. Country and food name identification accuracy by background (SFB and MFB). The models perform similarly for identifying the country and food with the landmark and street background. (a) SFB Accuracy by Background Type Country Food Name Landmark Street Landmark Street 0.285 0.249 0.119 0.086 0.078 0.067 0.080 0.033 0.028 0.139 0.116 0.288 0.252 0.101 0.089 0.073 0.063 0.081 0.122 0.092 0.126 0.129 0.255 0.200 0.068 0.057 0.044 0.037 0.067 0.003 0.000 0. 0.081 0.250 0.199 0.063 0.055 0.041 0.033 0.068 0.086 0.067 0.075 0.094 (b) MFB Accuracy by Background Type Country Food Name Landmark Street Landmark Street 0.311 0.265 0.134 0.116 0.095 0.076 0.082 0.133 0.129 0.156 0.150 0.314 0.277 0.117 0.107 0.080 0.067 0.080 0.115 0.118 0. 0.141 0.264 0.204 0.071 0.064 0.045 0.039 0.057 0.088 0.072 0.078 0.098 0.272 0.220 0.068 0.060 0.044 0.040 0.059 0.079 0.070 0.072 0.098 Model Gemini-2.5-Pro GPT-5 InternVL3-78B InternVL3-38B InternVL3-14B InternVL3-8B Molmo-72B QwenVL3-32B QwenVL3-8B Ovis-9B Avg. Model Gemini-2.5-Pro GPT-5 InternVL3-78B InternVL3-38B InternVL3-14B InternVL3-8B Molmo-72B QwenVL3-32B QwenVL3-8B Ovis-9B Avg. (a) Cultural distance Vs. Accuracy Figure 19. Effect of cultural distance between target and distractor. Models perform best when the target and distractor originate from the same country. Accuracy declines and entropy increases as cultural distance increases, indicating room for improving culture-mixing understanding in LVLMs. (b) Cultural distance Vs. Entropy Table 8. Predicted label entropy by model and subtask. The LVLMs show relatively high prediction uncertainty in single food settings (SFB and SF) compared to that of multiple food settings (MFB and MF) for both country and food name identification. Bold and underline indicate the highest and lowest accuracy among subtasks, respectively. (a) Country Identification Entropy Model SF MF SFB MFB Object Gemini-2.5-Pro 0.1674 0.1406 GPT-5 InternVL3-78B 0.1545 InternVL3-38B 0.1358 InternVL3-14B 0.2109 0.2279 InternVL3-8B Molmo-72B 0.3794 QwenVL3-32B 0.2124 0.1894 QwenVL3-8B 0.1628 Ovis-9B 0.4466 0.3774 0.5471 0.5294 0.6882 0.7483 0.8183 1.0243 0.6087 0.7224 1.9906 1.6294 2.0483 1.5570 2.5861 2.6767 1.7660 2.7046 2.5548 2. 1.7028 1.4498 1.7942 1.4269 2.3096 2.5273 1.9765 1.7793 1.5627 2.2161 0.3127 0.4073 0.4895 0.5136 0.7084 0.7784 1.0114 0.6309 0.6015 0.4315 Average 0.1981 0.6511 2. 1.8745 0.5831 (b) Food Name Identification Entropy Model SF MF SFB MFB Object 0.7720 Gemini-2.5-Pro GPT-5 0.8014 InternVL3-78B 0.7264 InternVL3-38B 0.7515 InternVL3-14B 0.7840 0.8079 InternVL3-8B 0.9270 Molmo-72B QwenVL3-32B 0.7933 0.7914 QwenVL3-8B 0.7685 Ovis-9B 0.9071 0.8144 0.9045 0.8252 0.9458 0.9485 1.0470 1.1436 0.9590 1.0390 2.2122 1.9083 2.2624 1.5961 2.8754 2.9280 1.7244 3.1793 2.7944 2.3858 2.0513 1.7651 1.9843 1.4558 2.5452 2.7138 2.1658 2.0625 1.6854 2. 0.9076 0.8574 0.8797 0.8136 0.9444 0.9707 1.1527 0.9798 0.9032 0.9155 Average 0.7923 0.9534 2.3866 2. 0.9325 Table 9. Real world dataset identification accuracy. LVLMs generally show better performance in identifying multiple foods from the same culture, even in real-world settings. (a) Country Identification Accuracy Model Single Multi (Same) Multi (Diff) Gemini-2.5-Pro GPT-5 InternVL3-78B InternVL3-38B InternVL3-14B InternVL3-8B Molmo-72B QwenVL3-32B QwenVL3-8B Ovis-9B Average 0.868 0.877 0.721 0.667 0.562 0.566 0.539 0.658 0.731 0.749 0.694 0.922 0.904 0.809 0.791 0.609 0.678 0.670 0.600 0.748 0.896 0.763 0.856 0.750 0.654 0.683 0.558 0.663 0.500 0.356 0.375 0. 0.609 (b) Name Identification Accuracy Model Single Multi (Same) Multi (Diff) Gemini-2.5-Pro GPT-5 InternVL3-78B InternVL3-38B InternVL3-14B InternVL3-8B Molmo-72B QwenVL3-32B QwenVL3-8B Ovis-9B Average 0.699 0.635 0.511 0.443 0.365 0.384 0.311 0.489 0.530 0.511 0.488 0.765 0.617 0.583 0.513 0.400 0.391 0.365 0.478 0.461 0.548 0.512 0.692 0.519 0.471 0.471 0.375 0.413 0.298 0.327 0.337 0.500 0. Table 10. The effect of food item location on identification accuracy. We observe almost no shift when comparing predictions on the original multi-food images and their horizontally flipped counterparts, indicating minimal positional bias in both country and name identification. (a) Country Identification Accuracy Model MF (N=350) Location Shift (N=100) Diff (Location MF) InternVL3-8B Ovis-9B QwenVL3-8B 0.111 0.237 0. 0.090 0.250 0.180 0.021 +0.013 0.023 (b) Name Identification Accuracy Model MF (N=350) Location Shift (N=100) Diff (Location MF) InternVL3-8B Ovis-9B QwenVL3-8B 0.049 0.086 0.131 0.020 0.100 0.120 0.029 +0.014 0.011 Table 11. The effect of size on identification accuracy. We observe almost no change in model performance between the original singlefood images and their resized variants, indicating that identification accuracy is largely invariant to object size. (a) Country Identification Model SF (N=247) Size Shift (N=741) Diff InternVL3-8B Ovis-9B QwenVL3-8B 0.158 0.271 0.239 0.139 0.290 0. 0.019 +0.019 0.015 (b) Food Name Identification Model SF (N=247) Size Shift (N=741) Diff InternVL3-8B Ovis-9B QwenVL3-8B 0.061 0.150 0.142 0.066 0.152 0.143 +0.005 +0.003 +0."
        }
    ],
    "affiliations": [
        "KAIST",
        "Meta",
        "Oracle",
        "Stevens Institute of Technology"
    ]
}