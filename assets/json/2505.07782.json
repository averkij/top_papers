{
    "paper_title": "MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine Learning Engineering",
    "authors": [
        "Rushi Qiang",
        "Yuchen Zhuang",
        "Yinghao Li",
        "Dingu Sagar V K",
        "Rongzhi Zhang",
        "Changhao Li",
        "Ian Shu-Hei Wong",
        "Sherry Yang",
        "Percy Liang",
        "Chao Zhang",
        "Bo Dai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement learning, evaluating, and improving autonomous large language model (LLM) agents in iterative machine learning engineering (MLE) workflows. Unlike existing benchmarks that primarily rely on static datasets or single-attempt evaluations, MLE-Dojo provides an interactive environment enabling agents to iteratively experiment, debug, and refine solutions through structured feedback loops. Built upon 200+ real-world Kaggle challenges, MLE-Dojo covers diverse, open-ended MLE tasks carefully curated to reflect realistic engineering scenarios such as data processing, architecture search, hyperparameter tuning, and code debugging. Its fully executable environment supports comprehensive agent training via both supervised fine-tuning and reinforcement learning, facilitating iterative experimentation, realistic data sampling, and real-time outcome verification. Extensive evaluations of eight frontier LLMs reveal that while current models achieve meaningful iterative improvements, they still exhibit significant limitations in autonomously generating long-horizon solutions and efficiently resolving complex errors. Furthermore, MLE-Dojo's flexible and extensible architecture seamlessly integrates diverse data sources, tools, and evaluation protocols, uniquely enabling model-based agent tuning and promoting interoperability, scalability, and reproducibility. We open-source our framework and benchmarks to foster community-driven innovation towards next-generation MLE agents."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 2 8 7 7 0 . 5 0 5 2 : r MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine Learning Engineering Rushi Qiang, Yuchen Zhuang, Yinghao Li, Dingu Sagar K, Rongzhi Zhang, Changhao Li, Ian Shu-Hei Wong, Sherry Yang, Percy Liang, Chao Zhang, Bo Dai Stanford University Georgia Institute of Technology"
        },
        {
            "title": "Abstract",
            "content": "We introduce MLE-Dojo, Gym-style framework for systematically reinforcement learning, evaluating, and improving autonomous large language model (LLM) agents in iterative machine learning engineering (MLE) workflows. Unlike existing benchmarks that primarily rely on static datasets or single-attempt evaluations, MLE-Dojo provides an interactive environment enabling agents to iteratively experiment, debug, and refine solutions through structured feedback loops. Built upon 200+ real-world Kaggle challenges. MLE-Dojo covers diverse, open-ended MLE tasks carefully curated to reflect realistic engineering scenarios such as data processing, architecture search, hyperparameter tuning, and code debugging. Its fully executable environment supports comprehensive agent training via both supervised fine-tuning and reinforcement learning, facilitating iterative experimentation, realistic data sampling, and real-time outcome verification. Extensive evaluations of eight frontier LLMs reveal that while current models achieve meaningful iterative improvements, they still exhibit significant limitations in autonomously generating long-horizon solutions and efficiently resolving complex errors. Furthermore, MLE-Dojos flexible and extensible architecture seamlessly integrates diverse data sources, tools, and evaluation protocols, uniquely enabling model-based agent tuning and promoting interoperability, scalability, and reproducibility. We open-source our framework and benchmarks to foster community-driven innovation towards next-generation MLE agents: https://github.com/MLE-Dojo/MLE-Dojo."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have demonstrated remarkable capabilities across diverse coding tasks, including code generation, debugging, and refactoring [3, 14, 1, 23]. Despite these advances, machine learning engineering (MLE) tasks remain uniquely challenging due to their inherent complexity, specialized domain knowledge, and the extensive iterative experimentation required for developing and optimizing ML algorithms [17, 26, 2]. LLM-based agents hold significant promise for revolutionizing MLE by automating repetitive steps, generating boilerplate code, suggesting suitable algorithms, debugging implementations, and iteratively refining models. Ultimately, advanced LLM-based Figure 1: Benchmark evaluations of eight frontiers LLMs across 50 evaluation tasks in MLE-Dojo. Equal contribution. 2MLE-Dojo webpage and leaderboard available at https://mle-dojo.github.io/MLE-Dojo-page/ and https://huggingface.co/spaces/MLE-Dojo/Leaderboard MLE agents could autonomously perform tasks such as literature search, hypothesis generation, experimental design, method implementation, result analysis, and even scientific dissemination. However, developing robust and autonomous MLE agents remains in its infancy, largely due to the absence of comprehensive benchmarks, interactive and executable training environments, and standardized evaluation frameworks capable of supporting rigorous, iterative experimentation. Several recent benchmarks have emerged to assess and facilitate progress in LLM-based coding agents [22, 34, 37, 17, 21, 26, 29]. However, most existing efforts focus mainly on isolated tasks such as data analysis or visualization, or competitions with narrow, non-interactive scenarios [22, 34, 37]. Such compartmentalized benchmarks fail to capture the inherent complexity and iterative nature of real-world MLE workflows, which require continuous experimentation, iterative debugging, structured feedback incorporation, and efficient resource management. Although broader benchmarks such as MLE-Bench [17] and DSBench [21] provide more diverse MLE tasks, they still lack interactive environments that support iterative experimentation and training paradigms such as fine-tuning or reinforcement learning. Moreover, in contrast to structured software engineering (SWE) tasks [20, 35], which only require structured environment for code-based problem-solving and can be readily sourced from general web resources such as GitHub issues, typical MLE tasks necessitate systematically curated datasets and standardized training data, elements currently unavailable in existing MLE benchmarks or gym-style frameworks [2, 26]. This complexity not only increases storage and computational requirements, but also creates significant challenges when scaling to more comprehensive and diverse problem sets. In this study, we introduce MLE-Dojo, an interactive, large-scale Gym-style environment explicitly designed for developing and benchmarking autonomous LLM agents on real-world MLE workflows. Built upon 200+ real-world Kaggle competitions across critical ML domains, including tabular data analysis, computer vision, natural language processing, and time series forecasting, etc.. MLE-Dojo provides carefully curated, executable tasks that closely replicate real-world engineering challenges. Among these, 150 tasks constitute the initial training dataset for MLE agents and are integrated into an interactive environment, enabling training trajectory sampling for both supervised fine-tuning and reinforcement learning. Moreover, MLE-Dojo incorporates pre-installed dependencies, standardized evaluation scripts, and real-time outcome verification, thus simplifying iterative experimentation and debugging processes. We conduct extensive empirical evaluations involving eight LLMs and multiple agent scaffolds. All evaluation results are publicly accessible via continuously updated leaderboard, promoting transparent comparison, reproducibility, and collaborative research in this emerging field. Additionally, MLE-Dojo features modular architecture that decouples agent capabilities from the underlying environment, facilitating seamless integration with diverse tools and data sources, enhancing interoperability, scalability, and supporting robust ecosystem for developing generalizable MLE agents. Our main contributions are summarized as follows: Comprehensive Framework and Benchmark: We establish MLE-Dojo as comprehensive and large-scale benchmark consisting of over 200 Kaggle MLE competitions, enabling systematic and rigorous evaluations of autonomous LLM agents. Interactive and Executable Environment: MLE-Dojo provides an interactive and fully executable Gym-style environment that facilitates iterative experimentation, including comprehensive training trajectories sampling for supervised fine-tuning and reinforcement learning. Advanced Functionalities and Scalability Supports: MLE-Dojo uniquely facilitates outcome verification, model-agnostic agent tuning, and seamless integration of diverse datasets and tools, significantly accelerating the development of robust, generalizable, and scalable MLE agents. Extensive Empirical Evaluation and Public Leaderboard: We conduct large-scale evaluations across multiple state-of-the-art LLMs and agent scaffolds, with results publicly available through an actively maintained long-term real-time leaderboard to foster community-driven innovation. In the remainder of the paper, we discuss the related work in section 2, detail our data curation process in section 3, and thoroughly describe MLE-Dojo in section 4. We present the experimental setup and results of LLMs as MLE agents in section 5 and conclude the paper in section 6. Datasets Interactive Gym Executable Environment Training Facilities Extensible Tasks Flexible Scaffolds Tabular CV NLP # Training # Eval AutoKaggle [24] MLAgentBench [17] DSBench [21] DSEval [36] MLEBench [2] MLGym [26] MLE-Dojo (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) (cid:34) (cid:37) (cid:37) (cid:37) (cid:34) (cid:34) (cid:34) (cid:34) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) (cid:37) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:37) (cid:37) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) 0 0 0 0 0 0 150 8 13 74 31 75 13 50 Table 1: Summary of existing benchmarks of MLE agents with data resources and sample sizes."
        },
        {
            "title": "2 Related Works",
            "content": "X-Agent (Coding). LLM-based coding agents have significantly advanced automated software development, with impactful applications across two primary domains: SWE and MLE. SWE agents [33, 8, 32] excel at code generation, debugging, and refactoring for improved maintainability and performance. For example, SWE-agent [33] operates within constrained agent-computer interface to facilitate file creation, repository navigation, and code testing; Magentic-One [8] expands the capability of SWE agents with web navigation features, further enhancing their practicality and usability. OpenHands [32] introduces an SWE agent with sandboxed environments to ensure safe command execution and verifiable web browsing, facilitating standardized benchmarking. MLE agents [15, 13, 24, 12, 16, 4, 19, 31], on the other hand, specialize in constructing, optimizing, and deploying comprehensive ML pipelines, including automatic architecture selection and hyperparameter tuning tailored to specific data characteristics. Early contributions focused on automating individual steps in ML engineering pipelines; for instance, CAAFE [15] leverages LLMs to iteratively generate semantically meaningful features for tabular data based on dataset description. More recent works have progressively expanded automation from single steps to encompass end-to-end ML workflows. For example, DS-Agent [13] integrates case-based reasoning approach to retrieve, reuse, evaluate, and refine solutions based on historical experiences. AutoKaggle [24] develops iterative code execution, debugging, and comprehensive unit testing to ensure code correctness and logic consistency. Agent v1.0 [12] provides an end-to-end autonomous data science agent designed to optimize workflows across diverse data science scenarios, while DataInterpreter [16] employs hierarchical graph modeling for complex problem decomposition and programmable node generation that refines and verifies subproblems via code generation. Advanced search and optimization approaches have further enhanced the capabilities of MLE agents. SELA [4] employs Monte Carlo Tree Search to enhance strategic planning during automated ML task-solving. AIDE [19] frames MLE as code optimization problem, formulating trial-and-error as tree search in the space of potential solutions. Here, we focus on evaluating and improving MLE agents. X-Bench. Benchmarks for MLE agents not only provide foundational baselines but also inaugurate public leaderboards. As illustrated in Table 1, early benchmarks primarily targeted isolated actions, such as data visualization and simple data manipulation [22, 34, 37]. Recent benchmarks expand the scope of evaluations to encompass broader project-level tasks, often sourced from real-world Kaggle competitions. For example, AutoKaggle [24] assesses static LLM workflows across 8 tabular Kaggle competitions. MLAgentBench [17] incorporates 13 tasks from Kaggle and bespoke ML challenges, providing baseline solutions and evaluating how often agents can achieve at least 10% improvement over these baselines. However, such benchmarks often exhibit limitations in both task breadth and complexity. Benchmarks such as MLE-Bench [2] and DSBench [21] significantly broaden task diversity to 75 and 74 competitions, respectively, aiming to closely simulate real-world MLE scenarios. However, these platforms still lack robust interactive environments that support iterative experimentation, agent fine-tuning, and realistic training scenarios, critical components for effective development and evaluation of MLE agents. In contrast, MLE-Dojo combines comprehensive MLE task coverage with fully interactive execution environment, offering unparalleled opportunities for developing, benchmarking, and refining advanced MLE agents. X-Gym (Dojo). Interactive environments are crucial for evaluating and refining LLM-based agents. SWE-Gym [28] introduces an interactive environment specifically tailored for software engineering tasks, enabling extensive iterative training and validation. BrowserGym [7] facilitates the evaluation of 3 Figure 2: Overview of task diversity in MLE-Dojo, highlighting representative examples from four major domains: time series, computer vision, tabular data, and natural language processing. web-based navigation and interaction tasks, capturing complex real-world knowledge work scenarios. Similarly, Collaborative-Gym [30] expands agent capabilities by enabling asynchronous, tripartite interaction among agents, humans, and task environments. The most recent and relevant work, ML-Gym [26], further integrates diverse ML research tasks into an interactive gym framework, providing an environment that supports reinforcement learning and iterative experimentation across open-ended research tasks. MLE-Dojo significantly extends both the quantity and complexity of tasks, offering 200+ competitions with an additional dedicated training set of 150 tasks. This enables effective trajectory sampling for supervised fine-tuning and reinforcement learning, thereby setting new standard for comprehensive evaluation and training of MLE agents."
        },
        {
            "title": "3.1 Task Description",
            "content": "MLE-Dojo comprises more than 200 carefully selected tasks spanning diverse ML domains, including tabular data analysis, computer vision, natural language processing, and MLE-Lite3 (Figure 2). Each primary category further encompasses multiple specific task typesfor instance, image classification within computer vision and sentiment analysis within natural language processingyielding total of 15 distinct task types. The majority of these tasks represent complex, practical MLE challenges essential for developing robust LLM-based autonomous agents, sourced from real-world Kaggle competitions. Kaggle4 is widely used online platform that hosts competitive ML tasks. Participants build ML models to solve defined challenges using real-world datasets, with performance publicly ranked based on predefined evaluation metrics. Leveraging extensive task library in Kaggle allows us to ensure task realism and broad relevance to practical MLE workflows."
        },
        {
            "title": "3.2 Dataset Construction",
            "content": "The objective of creating the dataset is to provide effective training data resources and solid evaluation over diverse set of machine learning topics. We select tasks that are light-weight tasks that are neither beginner nor too difficult and completed by large number of human competitors. This 3MLE-Lite is lightweight yet comprehensive subset of MLE-Bench [2], encompassing 22 competitions across diverse domains, including CV, NLP, Tabular and Audio. It is constructed with focus on both coverage and efficiency, and curated with user-friendliness inclusion in our evaluation suite. 4https://www.kaggle.com/ 4 indicates that the selected tasks are representative and also relatively easy to validate and still testing the capabilities we are expecting from machine learning agents. We aggregate tasks from multiple sources, including 68 competitions from MLE-Bench [2] (excluding 7 tasks that are unavailable, excessively large, or tightly coupled with specific packages), 74 from DSBench [21], and 75 additionally scraped and prepared carefully from Kaggles official website. After removing duplicate entries across sources, we obtain diverse collection of over 200 unique tasks. To cover more aspects and data resources, we will continue to extend our dataset to include tasks with more complex data structures and higher difficulty, which require more sophisticated data processing and model training. Following MLE-Bench [2], each task is standardized into consistent data structure (Figure 3 and 15) to create unified benchmark suitable for LLM agents, including: (1) detailed description obtained from the competition websites Overview and Data sections; (2) the original competition dataset, reorganized into clear training and test splits; 5 (3) an evaluation class to locally measure performance against competition-specific metrics and validate submission format; and (4) snapshot of the competition leaderboard, enabling performance comparisons (i.e., ranking) against human participants. MLE-Dojo provides standardized data interface and comprehensive documentation, allowing users to easily integrate new tasks or datasets into our structured, extensible format. We partition MLE-Dojo into training and evaluation subsets with 150 : 50 split. The evaluation set prioritizes tasks commonly referenced in prior benchmarks [2, 21], supplemented by strategically selected additional competitions to ensure domain diversity and representativeness. The larger training set provides extensive interactive experiences for LLM agents, facilitating robust agent training through both supervised fine-tuning and reinforcement learning. Figure 3: Overview of data structure in MLE-Dojo. 4 MLE-Dojo"
        },
        {
            "title": "4.1 Overview",
            "content": "We propose MLE-Dojo, benchmark that provides standardized environment to evaluate MLE agents interacting with task-specific environments. As depicted in Figure 4, MLE-Dojo provides unified interface for MLE agents, i.e., LLM-based assistants that can write code to handle project-level ML tasks, such as completing data science competitions. Each task environment contains essential information, including datasets, evaluation metrics, analysis results, code execution outcomes, error messages, and interaction history, facilitating comprehensive agent-environment interactions. The agent usually takes actions to solve the task, such as requesting task information, executing and writing code, evaluating generated code, retrieving past history, or resetting. From an agents perspective, the environment is typically centered on sampled MLE problem P, where is the task space. Each interaction between the environment and the agent can be formalized as Partially Observable Markov Decision Process (POMDP), where at the time step t, the environment provides the current observation ot and the reward rt R. Depending on the specific agent and environment design, the agent generates the next action at+1 at step + 1 based on the history of prior interactions. Here, O, A, and represent the observation space, action space, and reward space, respectively. The resulting interaction loop6 is shown in Figure 5. 5Given Kaggles potential absence of official test dataset answers. 6For clarity and visual appeal, we present the core logic of the code in an intuitive manner. For detailed implementation, please refer to our code repository. 5 Figure 4: Overview of MLE-Dojo. The framework bridges MLE-Agents with MLE task environments through standardized interfaces for observation and action spaces. Figure 5: Interaction loop in MLE-Dojo with theoretical model (left) and concrete Python API (right)."
        },
        {
            "title": "4.2 Modular and User-Friendly Interface",
            "content": "The MLE-Dojo framework is designed with strong emphasis on modularity, flexibility, and extensibility. Each component within the environment operates both independently and collaboratively and is fully decoupled, allowing seamless integration and extension through unified register mechanism. Specifically, we define the following core modules, each of which encapsulates distinct aspect of the environment: Error. It encodes comprehensive hierarchy of error types, enabling fine-grained debugging and facilitating informative feedback from the environment to the user. Interface. It governs the execution and interaction logic of native environment actions, serving as the backbone of agent-environment communication. Feedback. It translates interaction outcomes into structured, interpretable feedback to guide agent behavior and evaluation. Metric. It defines general metric base class, which can be subclassed to implement competitionspecific evaluation metrics in standardized and reusable manner. Users can interact with the environment either by developing custom modules via the provided APIs or by directly using the pre-designed environment for seamless experimentation, requiring only single call to env.step(action_type, action_args). The minimal and intuitive interaction logic significantly lowers the barrier for agent customization and development. We have also integrated agents of several scaffolds into MLE-Dojo, as detailed in the Appendix I. This design not only ensures clear separation of concerns, but also supports the easy incorporation of new functionalities, promoting both research reproducibility and rapid prototyping across diverse evaluation settings."
        },
        {
            "title": "4.3 Extensible Task Space P",
            "content": "The current task space encompasses all MLE tasks and competitions detailed in Section 3. Each task runs in separate Docker container to isolate its execution environment. The dockerization approach ensures reproducibility and independence across tasks. Within each container, we implement sandbox for executing agent-generated code. This sandbox is configurable with different experimental settings such as time limits and GPU/CPU memory constraints, providing controllable, safe, and unified experimental environment. By separating the environment and the agent into private and public segments, MLE-Dojo ensures that certain directories remain inaccessible (private), while others are shared (public), offering secure and reliable testing playground. To simplify extending MLE-Dojo with new tasks, we standardize the data format for task integration (plug-and-play). unified competition format is maintained for each task, including (1) Basic information: competition description and sample submission, (2) Well-structured datasets: wellsplit datasets through general prepare.py script (some competitions do not originally provide labeled test data), and (3) Public and private leaderboards: both public and private leaderboards assessed using competition-specific evaluation metrics. Following this standardized data format, users can easily incorporate their own competitions for testing purposes. Comprehensive format definitions and detailed guidance are available in Appendix C."
        },
        {
            "title": "4.4 Observation Space O",
            "content": "MLE-Dojo provides rich observation space with five main components: Dataset Information. For each competition, MLE-Dojo supplies comprehensive information required to solve tasks, including (1) Competition background: concise overview of the historical context and evolving research challenges, (2) Goal description: clear definition of specific objectives, desired outcomes, and evaluation metrics, (3) Sample submission: templates exemplifying the expected format and content structure, and (4) Data folder structure: organization and naming conventions of provided datasets for user access and navigation. Evaluation Metric Scores. For each task, MLE-Dojo implements well-defined evaluation functions in Python, providing description-aligned criteria and metrics to validate submission format and method performance. Performance is reported as both raw scores and HumanRank score, offering absolute and relative performance insights. Additionally, MLE-Dojo provides detailed feedback on submission format to guide agents in actively modifying results accordingly. Code Execution Results. To solve MLE tasks, agents generate Python code as solutions. For submission, code is executed in the sandbox environment, generating submission file in predefined directory. Once submission file is successfully generated and follows the requirements specified in the Dataset Information, the environment calls the evaluation metric function to calculate results based on ground truth data and the submission file. Error Messages. When code is executed in the sandbox, flaws may lead to CompileError or RuntimeError. To enable LLM agents to iteratively improve their generated code, MLE-Dojo encapsulates error messages and provides them during agent interactions. Even when code executes successfully, MLE-Dojo also reports additional error messages regarding submission files (e.g., incorrect formats, mismatching columns, etc.). Interaction History. In MLE-Dojo, MLE agents can access interaction histories in two ways: (1) Conversation history: Obtained from the agent side, including all LLM generations and environment observations, and (2) Environment records: Obtained from the environment side, objectively recording every agent action and environment observation. This dual approach accommodates different LLM agents and MLE task implementations, improving compatibility with both dialog-based LLMs and coding-focused agents. For reasoning models, conversation history effectively records extended reasoning procedures, while environment records better suit coding LLMs that may not support natural language interaction."
        },
        {
            "title": "4.5 Expandable Action Space A",
            "content": "The fundamental action space of MLE-Dojo consists of raw executable Python code. Agents can call request_info to query task descriptions and dataset details, then generate Python code to 7 Figure 6: Elo Rankings of eight frontiers LLMs on our proposed MLE-Dojo across four main MLE domains in MLE-Dojo: tabular analysis, computer vision (CV), natural language processing (NLP), and MLE-Lite. solve specific MLE tasks using three core functions: validate_code, execute_code, and reset, which connect the agent to the code interpreter in the sandbox. Additionally, agents can leverage the get_history function to retrieve past interactions between the environment and themselves. MLE-Dojo also provides user-defined action portal, offering an extensible action space for researchers to register additional actions. As long as new actions are registered through the portal and provided to the agent in context, the agent can learn to leverage these new actions to solve problems. By default, MLE-Dojo provides predefined yet extensible set of five basic actions: request_info. When an LLM agent requires necessary information to solve an MLE problem, it can call the request_info function to access task descriptions, sample submissions, data directories, output directories, and data structures. The response includes all necessary competition information without further analysis or information pruning. validate_code. The validate_code action performs basic syntax checking and runtime validation. It provides compilation trial of the code and returns detailed syntax or execution error messages. It also serves as critical interface for running data analysis code and extracting deeper insights from the resulting outputs. This action is more lightweight than execute_code without submission requirements, which may serve as debugging or information printing tool. execute_code. Unlike validate_code, the execute_code action performs complete code execution, submission verification and evaluation. Only through this action can the generated submission of the code be validated and evaluated using the corresponding metric. Each use of the execute_code action corresponds to full competition submission. Restriction on invocations of this action allow for different and flexible methodological and experimental design needs. Only through the invocation of this action can the generated submission be validated and evaluated using the corresponding metric. get_history. Since memory is crucial component in agent design, MLE-Dojo provides the get_history function to access past experiences and enable learning from memory. This flexible design accommodates different agent architectures and scaffolding approaches. reset. MLE-Dojo offers reset function to reset the entire environment and restart from scratch. This action serves as the default mechanism designed to accommodate wide range of use cases."
        },
        {
            "title": "4.6 Reward Space and Environmental Feedback",
            "content": "MLE-Dojo introduces reward mechanism specifically designed to reflect the quality of solutions for coding tasks. This mechanism utilizes quantitative metrics provided by each competition, such as accuracy, F1-score, etc. However, since competitions differ in evaluation metrics and score ranges, directly using absolute performance to evaluate LLM-based agents generated code is not precise. To provide continuous and fine-grained feedback that rewards incremental improvements and innovative solutions, we propose using the relative position of the human leaderboard as reward instead of the coarse-grained medals used in existing benchmarks [2]. 8 Task Categories () MLE-Lite Tabular NLP CV Models () AUP H-Rank (%) Elo AUP H-Rank (%) Elo AUP H-Rank (%) Elo AUP H-Rank (%) Elo gpt-4o-mini [18] gpt-4o [18] o3-mini [27] DeepSeek-v3 [25] DeepSeek-r1 [6] 1.492 1.448 1. 1.825 1.852 Gemini-2.0-Flash [9] 1.696 1.796 Gemini-2.0-Pro [10] 1.919 Gemini-2.5-Pro [11] 21.21 27.85 56.48 44.26 58.43 33.50 48.61 61.95 753 0.724 830 0.691 1108 0. 1004 0.727 1137 0.678 847 0.689 1064 0.787 1257 0.798 15.37 18.97 32.65 37.85 38.13 30.36 37.46 42.64 839 0.837 861 0.842 1019 0. 1015 0.977 1053 0.988 923 0.884 1139 0.970 1150 0.998 13.14 29.97 37.46 28.41 28.48 28.39 30.93 38.45 758 1.172 903 1.418 1056 1. 1028 1.784 1103 1.844 860 1.607 1028 1.651 1266 1.915 10.17 18.76 35.02 26.75 34.26 20.35 23.07 42.83 754 761 1067 1083 978 973 1177 Table 2: Main experiments of LLMs as MLE Agents on MLE tasks in MLE-Dojo. HumanRank Score. We calculate the relative position score of the current submission on the leaderboard of human competitors. Suppose that the submission ranks at position among total of submissions on the leaderboard. Then, the position score is computed as: = 1 . The HumanRank score indicates the percentage of human competitors the agent surpasses on the leaderboard for given competition. higher score reflects stronger performance relative to human participants. To prevent bias between public and private leaderboards, we compute the relative scores on each leaderboard independently and then use their average as the final reward. We adopt the HumanRank Score as the reward in our environment. First, it is fully aligned with the original performance metrics-achieving higher original score is strictly positively correlated with obtaining higher HumanRank Score, regardless of the specific metric used. Moreover, HumanRank is normalized score within the [0, 1] range, which resolves the issue of varying score magnitudes across different tasks and enables it to serve as unified and informative reward."
        },
        {
            "title": "5.1 Experiment Setups",
            "content": "Backbone LLMs. We consider different backbones to test the effectiveness of MLE-Dojo in the evaluation and improvement of LLMs as MLE Agents. MLE Agent leverages native actions and interacts with the environment through straightforward logic. Prompts and implementation details are available in appendix and appendix I.1. Specifically, we consider gpt-4o-mini (2024-07-18) [18], gpt-4o (2024-11-20) [18], o3-mini (2025-01-31) [27] from OpenAI, Gemini-2.0-Flash [9], Gemini-2.0-Pro (exp) [10], and Gemini-2.5-Pro (exp-3-25) [11] from Google, and DeepSeek-v3 (2025-03-24) [25] and DeepSeek-r1 [6] from DeepSeek as evaluation backbone LLMs. For non-reasoning models, we set temperature=0.0 and top-p = 1.0 to ensure reproducible evaluations. We take the best performance of two runs per task per model. Evaluation Metrics. To ensure comprehensive evaluation, we consider Area Under the Performance Profile (AUP) [26], HumanRank Score (H-Rank, %), and Elo ranking [5] together as metrics. Additional implementation details are available in appendix E. Environment Configurations. The total number of steps is set to 15, with agents having full access to their interaction histories. unified, concise prompt with clear instructions is provided, without additional extraneous information. The maximum runtime per session is 12 hours, and GPU memory is limited to 32 GB. The maximum input token length is set at 50,000, while each output round is capped at 8,192 tokens. These prompt, history, time, and memory configurations are designed to rigorously evaluate LLMs capabilities in long-context handling, instruction-following, reasoning, and coding under resource-constrained conditions, closely mirroring realistic Kaggle competition scenarios. We do not restrict the number of submission attempts to enable continuous improvement. To generate valid submission files and scores, agents must explicitly use the \"execute_code\" command; submissions are not automated. We pre-install commonly used Python packages, though agents may install additional packages within their generated code as needed."
        },
        {
            "title": "5.2 Main Results",
            "content": "Table 2 presents comprehensive evaluation of eight LLMs as MLE Agents across four fundamental ML tasks. Reasoning and coding models such as o3-mini, DeepSeek-r1, and Gemini-2.5-Pro consistently achieve high rankings across all metrics, demonstrating strong adaptability, robustness, and overall effectiveness as MLE Agents. Additionally, Figure 8 further illustrates the Performance Profiles along with the corresponding AUP curves. Models like Gemini-2.0-Pro exhibit balanced performance profiles, achieving moderate but consistently reliable results across various tasks. This comprehensive comparative perspective highlights the strengths and limitations of each model, offering practical insights into their suitability for different MLE scenarios. In addition, the HumanRank Score provides an absolute measure of performance incorporating human benchmarks, the Elo Score clarifies competitive relationships through pairwise analyses, and the Performance Profiles with AUP scores assess robustness and consistency across performance variations. Together, these evaluation approaches in MLE-Dojo form robust, multifaceted perspective on the capabilities and limitations of LLMs as MLE agents, enhancing the overall interpretation and reliability of our experimental conclusions. Figure 7: Task difficulty sorted by average HumanRank score. Furthermore, we define the difficulty level of different tasks with the average performance of different models in comparison with the human leaderboard. Figure 7 illustrates the average performance distribution across 8 frontier models on the tasks. As shown in the figure, CV tasks are the most challenging-none of them have an average HumanRank score above 60, and more than half fall below 30. For MLE-Lite tasks, the average HumanRank scores mostly exceed 30. Difficulty levels of tasks in other domains are more evenly distributed. Figure 8: Performance profiles and corresponding AUP curves for evaluating the robustness of LLMs across four ML tasks. The x-axis represents the performance ratio threshold τ , while the y-axis indicates the fraction of tasks for which model achieves performance within factor τ of the best-performing model."
        },
        {
            "title": "5.3 Cost Analysis",
            "content": "Figure 9 illustrates the cost-performance relationship across different LLMs and task categories. Reasoning models (e.g., DeepSeek-r1) typically incur higher costs due to their premium pricing structures and longer solution outputs. Even reasoning models with comparatively lower pricing, such as o3-mini, tend to produce longer outputs due to more complex reasoning processes. These longer outputs significantly increase overall token consumption, contributing to higher cumulative costs. 10 Figure 9: Relationship between average computational cost and performance across evaluated LLMs and task categories. Each point represents the average cost per task, with specific attention given to reasoning vs. non-reasoning model cost dynamics. Note that Gemini-2.0-Pro and Gemini-2.5-Pro are excluded from cost analysis (only for performance reference) due to current free usage. Figure 10: Step-wise HumanRank performance comparisons between reasoning and non-reasoning models, considering only code execution and validation steps. Information-requesting steps are excluded. Notably, tasks involving computer vision and deep neural network training pipelines consistently generate longer codes compared to classical ML tasks (e.g., tabular analysis) executed on CPUs. While cost generally correlates with solution complexity and token consumption, some models, such as DeepSeek-r1, achieve competitive performance with significantly fewer tokens, highlighting potential cost-efficiency opportunities."
        },
        {
            "title": "5.4 Step-wise Performance Dynamics",
            "content": "Figure 10 presents the step-wise performance improvements across different models, illustrating variations in performance trajectories between reasoning and non-reasoning models. Among reasoning models, o3-mini consistently achieves high performance within the initial steps (typically within the first five) and maintains stable scores in subsequent steps. Conversely, DeepSeek-r1 and Gemini-2.5-Pro exhibit gradual improvements, achieving comparable or superior performance in intermediate to later steps. Non-reasoning models occasionally outperform reasoning models at early or intermediate steps but generally show limited improvement as steps progress, resulting in lower final scores."
        },
        {
            "title": "5.5 Action and Error Analysis",
            "content": "Action Strategy and Execution Proportions. Models exhibit distinct strategies regarding the balance between code execution and validation actions (Figure 11). Specifically, o3-mini demonstrates high confidence by frequently proceeding directly to code execution without extensive preliminary validation or output inspections, adopting execution actions in over 90% of instances. Conversely, gpt-4o and gpt-4o-mini employ significantly more conservative strategies, executing code only about 20% of the time and relying heavily on validation steps. Notably, Gemini-2.0-Flash, despite not being among the top-performing models, uses an aggressive execution strategy comparable to stronger models. 11 Figure 11: Proportion of code execution actions relative to combined execution and validation actions. Failure Rate Analysis. Figure 12 the average failure rates shows across tasks, highlighting validation, execution, and overall errors. Gemini-2.5-Pro maintains the lowest overall failure rate, aligning with its consistently high performance. In contrast, DeepSeek-r1, despite achieving strong performance, experiences relatively high failure rates in both execution and validation categories. gpt-4o and gpt-4o-mini, due to their conservative strategies, achieve low execution failure rates but face comparatively high validation failures. Conversely, Gemini-2.0-Flash successfully balances aggressive execution with one of the lowest overall failure rates, second only to Gemini-2.5-Pro. Figure 12: Average failure rates across tasks. Execution Error Types. To further analyze the nature of execution errors, we categorized failures into \"Execution Failed\", \"Submission Not Created\", and \"Submission Invalid\" (Figure 13). Stronger models, generating longer, more complex code, are more susceptible to \"Execution Failed\" errors. However, successful executions from these models frequently yield valid submissions and high performance. Models employing conservative validation strategies (e.g., gpt-4o-mini, gpt-4o) significantly reduce \"Execution Failed\" errors but continue to encounter issues related to submission creation and validity, indicating limitations in the validation approach."
        },
        {
            "title": "5.6 History Length and Solution Length",
            "content": "Figure 13: Execution error types. Figure 14 shows both the total chat history length, including all interaction prompts, actions, and generated codes, and the length of the best solution generated by each model. The total chat history length closely aligns with the best solution length, where both metrics positively correlate with overall model performance. Reasoning models typically generate notably longer solutions compared to non-reasoning models. Moreover, stronger-performing models frequently produce more extended solutions, which often correspond to higher performance scores. Although increased solution length does not inherently ensure superior outcomes, it generally indicates models capability to explore more intricate and sophisticated solution strategies, hallmark predominantly observed in more capable reasoning models. Figure 14: Total chat history length (left) and best solution length (right)."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduced MLE-Dojo, Gym-style framework designed for training, evaluating, and benchmarking autonomous MLE agents. By providing an interactive and realistic environment built upon large-scale collection of real-world Kaggle competitions, MLE-Dojo enables systematic experimentation, rigorous outcome verification, and iterative feedback crucial for advancing LLM-driven MLE workflows. Our extensive empirical evaluations established foundational baselines and highlighted both capabilities and critical limitations of current state-of-the-art models and agent architectures. We publicly release our framework, benchmarks, and leaderboard to encourage transparent comparison, reproducible research, and community-driven progress toward next-generation of fully autonomous MLE agents. Future work includes expanding MLE-Dojo to incorporate domain-specific deep research and support for multi-agent collaborative scenarios."
        },
        {
            "title": "References",
            "content": "[1] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [2] J. S. Chan, N. Chowdhury, O. Jaffe, J. Aung, D. Sherburn, E. Mays, G. Starace, K. Liu, L. Maksin, T. Patwardhan, et al. Mle-bench: Evaluating machine learning agents on machine learning engineering. arXiv preprint arXiv:2410.07095, 2024. [3] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [4] Y. Chi, Y. Lin, S. Hong, D. Pan, Y. Fei, G. Mei, B. Liu, T. Pang, J. Kwok, C. Zhang, et al. Sela: Tree-search enhanced llm agents for automated machine learning. arXiv preprint arXiv:2410.17238, 2024. [5] W.-L. Chiang, L. Zheng, Y. Sheng, A. N. Angelopoulos, T. Li, D. Li, B. Zhu, H. Zhang, M. Jordan, J. E. Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. In Forty-first International Conference on Machine Learning, 2024. [6] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [7] A. Drouin, M. Gasse, M. Caccia, I. H. Laradji, M. D. Verme, T. Marty, D. Vazquez, N. Chapados, and A. Lacoste. Workarena: How capable are web agents at solving common knowledge work tasks? In Forty-first International Conference on Machine Learning, 2024. [8] A. Fourney, G. Bansal, H. Mozannar, C. Tan, E. Salinas, F. Niedtner, G. Proebsting, G. Bassman, J. Gerrits, J. Alber, et al. Magentic-one: generalist multi-agent system for solving complex tasks. arXiv preprint arXiv:2411.04468, 2024. [9] Google. Gemini 2.0 flash: Our powerful workhorse model with low latency and enhanced performance, built to power agentic experiences. Google DeepMind Blog, 2025. [10] Google. Gemini 2.0 pro: Our best model yet for coding performance and complex prompts. Google DeepMind Blog, 2025. [11] Google. Gemini 2.5: Our most intelligent ai model. Google Blog, 2025. [12] A. Grosnit, A. Maraval, J. Doran, G. Paolo, A. Thomas, R. S. H. N. Beevi, J. Gonzalez, K. Khandelwal, I. Iacobacci, A. Benechehab, et al. Large language models orchestrating structured reasoning achieve kaggle grandmaster level. arXiv preprint arXiv:2411.03562, 2024. [13] S. Guo, C. Deng, Y. Wen, H. Chen, Y. Chang, and J. Wang. DS-agent: Automated data science by empowering large language models with case-based reasoning. In Forty-first International Conference on Machine Learning, 2024. 13 [14] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo, C. Burns, S. Puranik, H. He, D. Song, and J. Steinhardt. Measuring coding challenge competence with APPS. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. [15] N. Hollmann, S. Müller, and F. Hutter. Large language models for automated data science: Introducing caafe for context-aware automated feature engineering. Advances in Neural Information Processing Systems, 36, 2024. [16] S. Hong, Y. Lin, B. Liu, B. Liu, B. Wu, C. Zhang, C. Wei, D. Li, J. Chen, J. Zhang, et al. Data interpreter: An llm agent for data science. arXiv preprint arXiv:2402.18679, 2024. [17] Q. Huang, J. Vora, P. Liang, and J. Leskovec. MLAgentbench: Evaluating language agents on machine learning experimentation. In Forty-first International Conference on Machine Learning, 2024. [18] A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [19] Z. Jiang, D. Schmidt, D. Srikanth, D. Xu, I. Kaplan, D. Jacenko, and Y. Wu. Aide: Ai-driven exploration in the space of code. arXiv preprint arXiv:2502.13138, 2025. [20] C. E. Jimenez, J. Yang, A. Wettig, S. Yao, K. Pei, O. Press, and K. Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. [21] L. Jing, Z. Huang, X. Wang, W. Yao, W. Yu, K. Ma, H. Zhang, X. Du, and D. Yu. DSBench: In The Thirteenth How far are data science agents from becoming data science experts? International Conference on Learning Representations, 2025. [22] Y. Lai, C. Li, Y. Wang, T. Zhang, R. Zhong, L. Zettlemoyer, W.-t. Yih, D. Fried, S. Wang, and T. Yu. Ds-1000: natural and reliable benchmark for data science code generation. In International Conference on Machine Learning, pages 1831918345. PMLR, 2023. [23] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):10921097, 2022. [24] Z. Li, Q. Zang, D. Ma, J. Guo, T. Zheng, M. Liu, X. Niu, Y. Wang, J. Yang, J. Liu, et al. Autokaggle: multi-agent framework for autonomous data science competitions. arXiv preprint arXiv:2410.20424, 2024. [25] A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [26] D. Nathani, L. Madaan, N. Roberts, N. Bashlykov, A. Menon, V. Moens, A. Budhiraja, D. Magka, V. Vorotilov, G. Chaurasia, D. Hupkes, R. S. Cabral, T. Shavrina, J. Foerster, Y. Bachrach, W. Y. Wang, and R. Raileanu. Mlgym: new framework and benchmark for advancing ai research agents, 2025. [27] OpenAI. Openai o3-mini: Pushing the frontier of cost-effective reasoning. OpenAI Blog, 2025. [28] J. Pan, X. Wang, G. Neubig, N. Jaitly, H. Ji, A. Suhr, and Y. Zhang. Training software engineering agents and verifiers with swe-gym. arXiv preprint arXiv:2412.21139, 2024. [29] S. Schmidgall and M. Moor. Agentrxiv: Towards collaborative autonomous research. arXiv preprint arXiv:2503.18102, 2025. [30] Y. Shao, V. Samuel, Y. Jiang, J. Yang, and D. Yang. Collaborative gym: framework for enabling and evaluating human-agent collaboration. arXiv preprint arXiv:2412.15701, 2024. [31] P. Trirat, W. Jeong, and S. J. Hwang. Automl-agent: multi-agent llm framework for fullpipeline automl. arXiv preprint arXiv:2410.02958, 2024. [32] X. Wang, B. Li, Y. Song, F. F. Xu, X. Tang, M. Zhuge, J. Pan, Y. Song, B. Li, J. Singh, et al. Openhands: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741, 2024. [33] J. Yang, C. E. Jimenez, A. Wettig, K. Lieret, S. Yao, K. R. Narasimhan, and O. Press. Swe-agent: Agent-computer interfaces enable automated software engineering. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [34] P. Yin, W.-D. Li, K. Xiao, A. Rao, Y. Wen, K. Shi, J. Howland, P. Bailey, M. Catasta, H. Michalewski, et al. Natural language to code generation in interactive data science notebooks. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 126173, 2023. [35] D. Zan, Z. Huang, W. Liu, H. Chen, L. Zhang, S. Xin, L. Chen, Q. Liu, X. Zhong, A. Li, et al. Multi-swe-bench: multilingual benchmark for issue resolving. arXiv preprint arXiv:2504.02605, 2025. [36] Y. Zhang, Q. Jiang, X. Han, N. Chen, Y. Yang, and K. Ren. Benchmarking data science agents. arXiv preprint arXiv:2402.17168, 2024. [37] Y. Zhang, Y. Pan, Y. Wang, and J. Cai. Pybench: Evaluating llm agent on various real-world coding tasks. arXiv preprint arXiv:2407.16732, 2024."
        },
        {
            "title": "A Limitations and Broader Impacts",
            "content": "A.1 Limitations Resource Limitations. While MLE-Dojo provides rich, interactive environment to train and evaluate LLM agents, it also imposes computational and storage demands. Executing full MLE pipelinesincluding data preprocessing, model training, and debugging-across hundreds of competition tasks requires potential API credits, CPU/GPU resources and large-scale disk storage. Although we provide competitions of varying difficulty levels and data sizesorganized in ascending order of size for user claritycomprehensively training or evaluating MLE agents still demands substantial computational and storage resources. Data Privacy and Licensing. MLE-Dojo is constructed atop real-world Kaggle competitions, each of which may be governed by different data usage licenses and privacy policies. Although the framework does not redistribute any proprietary data directly, it facilitates automated downloading and usage of datasets via the official Kaggle API. Users are solely responsible for reviewing, understanding, and adhering to the license agreements associated with each dataset. To assist in compliance, we provide direct links to the license terms and competition rules for all included tasks in https: //github.com/MLE-Dojo/MLE-Dojo/blob/main/prepare/licenses.json. A.2 Broader Impacts Potential Societal Impacts. MLE-Dojo aims to advance the development of intelligent, autonomous agents capable of supporting and accelerating the machine learning engineering process. By enabling automation of repetitive and error-prone engineering tasks, the framework may help reduce the barrier to entry for ML practitioners, democratize access to model development tools, and empower domain experts (e.g., in healthcare, education, and scientific research) to more effectively apply machine learning solutions without requiring deep expertise in engineering workflows. Furthermore, the open-source nature of MLE-Dojo fosters reproducibility, transparency, and community-led innovation in the design and evaluation of LLM agents for real-world MLE tasks. A.3 Ethical Statements Data Usage and Consent. MLE-Dojo does not host or redistribute any proprietary datasets. All tasks are built upon publicly available Kaggle competitions, and datasets are accessed via the official Kaggle API in compliance with their respective licenses. We ensure that no personal or sensitive information is retained within the framework. Users are explicitly informed that it is their responsibility to read and adhere to the terms of use and data handling policies provided by each competition. Where applicable, competitions selected for inclusion in the benchmark are reviewed to ensure they do not contain personally identifiable or ethically sensitive data. Fair Use and Responsible Deployment. MLE-Dojo is intended solely for academic research, educational purposes, and the responsible development of AI systems. We discourage the use of this framework for automating competition participation without appropriate attribution or for circumventing fair use policies. We also emphasize the importance of transparent model development practices when using MLE-Dojo, particularly in scenarios where LLM agents are intended for downstream deployment or integration into critical systems. Bias and Representational Fairness. While MLE-Dojo includes broad spectrum of tasks across multiple ML domains, the selection of tasks is constrained by the availability of open competitions and may not fully represent all application domains or demographic contexts. Moreover, since LLM agents can inherit and amplify biases present in training data or model pretraining corpora, it is crucial that users critically evaluate outputs, particularly in sensitive contexts (e.g., healthcare, hiring, or education). Future versions of MLE-Dojo will explore the inclusion of bias detection and mitigation tools to support ethical agent development. Open-Source Commitment. We commit to maintaining MLE-Dojo as an open-source project under permissive license, enabling transparent inspection, community-driven development, and reproducibility. We welcome contributions that enhance the ethical robustness of the framework, including the addition of fairness metrics, safety guardrails, and improved documentation on responsible usage."
        },
        {
            "title": "B Disclaimer",
            "content": "The dataset is made available exclusively for educational and academic research purposes, with the intention of advancing scholarly investigations in relevant domains. Users of the dataset must adhere to the following terms and conditions: Data Source and Accuracy: Although reasonable efforts have been undertaken to curate and organize the dataset, the providers do not warrant the accuracy, completeness, or currency of the information. Users are strongly encouraged to independently verify the data and bear full responsibility for any analyses, interpretations, or conclusions derived from it. Usage Restrictions: This dataset is strictly limited to non-commercial use. Any commercial exploitation, product development, or profit-oriented application based on this dataset requires prior explicit written authorization from the dataset providers. Privacy and Legal Compliance: Users must ensure that their usage of the dataset complies with all relevant legal frameworks, particularly those concerning data privacy, protection, and security. The dataset providers shall not be held accountable for any legal liabilities or consequences resulting from improper or unauthorized usage. Non-Infringement of Rights: The dataset includes pre-processed content derived from external sources and is distributed solely for non-commercial research purposes. The dataset providers do not assert ownership over the original data and expressly acknowledge the rights of the original creators. It is the users responsibility to ensure that their use of the dataset does not violate any copyright or intellectual property laws. Disclaimer of Liability: The dataset is provided \"as is\" without any express or implied warranties. The dataset providers shall not be liable for any direct, indirect, incidental, or consequential damages arising from the use of the dataset, including but not limited to financial loss, data misinterpretation, or third-party claims."
        },
        {
            "title": "C Unified Data Structure",
            "content": "Users can flexibly incorporate new tasks originating from diverse sources, which can be standardized into unified competition format with minimal overhead. These personalized competitions can then be seamlessly integrated into our environment for evaluation, benchmarking and training. To ensure clarity, modularity, and scalability, each competition is organized under unified and minimalistic directory structure. As shown in Figure 15, the root directory is composed of three major subdirectories: data/, utils/, and an optional info/ folder. The data/ directory is further divided into private/ and public/ subfolders, corresponding respectively to hidden and accessible phases of the competition. Each contains standardized files such as test_answer.csv, leaderboard.csv, and input-output formats (e.g., sample_submission.csv, description.txt), thereby facilitating consistent evaluation procedures. The utils/ folder encapsulates task-specific scripts for data preparation and metric evaluation (i.e., prepare.py, metric.py). Lastly, the optional info/ directory provides supplementary metadata such as web links, original descriptions, and data schema. This design adheres to uniform organizational paradigm that simplifies integration, supports automation, and improves the transparency of competition configurations."
        },
        {
            "title": "D Data Details",
            "content": "To establish unified and flexible framework capable of accommodating diverse and emerging tasks, we develop customized data preparation scripts for each competition. These scripts systematically structure datasets, generate representative example submissions, and enable efficient local testing and evaluation. Given that most Kaggle competitions lack publicly available labels for test datasets, our scripts carefully split the original training data into new, clearly defined training and test subsets. We further implement tailored evaluation metrics specific to each competition, derived from common base metric class. We begin with an initial set of around 600 Kaggle competitions. Weve already manually reviewed them to confirm their suitability based on clarity of descriptions, data availability, and relevance of evaluation metrics, while excluding tasks with excessively large or unwieldy datasets. Our final collection balances challenge complexity with computational feasibility, yielding practical, 17 competition/ data/ private/ test_answer.csv private_leaderboard.csv (Optional) public_leaderboard.csv (Optional) public/ (train/test data) description.txt sample_submission.csv data_structure.txt (Optional) utils/ prepare.py metric.py info (Optional)/ description.txt raw_description.txt web_url.txt data_structure.txt Figure 15: Standardized directory structure for each competition. scalable, and user-friendly benchmark of 200 competitions as our first release. We plan progressive releases of processed datasets to support ongoing benchmarking efforts. Table 3 presents all competitions included in our first release, sorted in ascending order by data size. The associated tags provide information on each tasks metric, category, modality, or domain. Users are encouraged to flexibly select tasks based on specific needs. Table 3: Kaggle Competition Data Overview Competition Name Size Tags playground-series-s3e12 titanic 25.56 kB 93.08 kB playground-series-s3e5 playground-series-s3e13 mercedes-benz-greenermanufacturing icr-identify-age-relatedconditions kaggle-llm-science-exam playground-series-s3e22 225.6 kB 284.43 kB 351.45 kB 356.88 kB 364.21 kB 386.6 kB playground-series-s3e3 455.63 kB playground-series-s3e9 kobe-bryant-shot-selection tabular-playground-seriesjul-2021 bike-sharing-demand home-data-for-ml-course spaceship-titanic covid19-global-forecastingweek-2 covid19-global-forecastingweek485.23 kB 708.3 kB 826.96 kB 1.12 MB 1.15 MB 1.24 MB 1.26 MB 1.41 MB Beginner, Tabular, Binary Classification, Custom Metric Binary Classification, Tabular, Beginner, Categorization Accuracy Beginner, Tabular, Cohen Kappa Score Beginner, Tabular, Multiclass Classification, Health Conditions, MAP{K} Automobiles and Vehicles, Regression, Tabular, R2 score (coefficient of determination) Tabular, Binary Classification, Health, Weighted Multiclass Loss Physics, NLP, MAP{K} Beginner, Tabular, Multiclass Classification, Animals, Health, F1 Score Tabular, Beginner, Binary Classification, Business, Area Under Receiver Operating Characteristic Curve Beginner, Tabular, Regression, Root Mean Squared Error Basketball, Binary Classification, Tabular, Log Loss Tabular, Pollution, Time Series Analysis, Mean Columnwise Root Mean Squared Logarithmic Error Cycling, Tabular, Time Series Analysis, Root Mean Squared Logarithmic Error Mean Absolute Error Beginner, Tabular, Binary Classification, Categorization Accuracy Coronavirus, Tabular, Mean Columnwise Root Mean Squared Logarithmic Error Tabular, Coronavirus, Mean Columnwise Root Mean Squared Logarithmic Error Continued on next page 18 Table 3: Kaggle Competition Data Overview (continued) Competition Name Size Tags nlp-getting-started covid19-global-forecastingweek-1 tabular-playground-seriesjan-2022 playground-series-s3e2 spooky-authoridentification covid19-global-forecastingweek-4 liberty-mutual-groupproperty-inspectionprediction playground-series-s3e25 us-patent-phrase-to-phrasematching movie-review-sentimentanalysis-kernels-only playground-series-s3e6 commonlitreadabilityprize playground-series-s3e14 detecting-insults-in-socialcommentary walmart-recruiting-storesales-forecasting prudential-life-insuranceassessment tweet-sentiment-extraction playground-series-s3e7 llm-detect-ai-generatedtext unimelb playground-series-s4e2 3.86 MB 3.86 MB 4.43 MB 4.53 MB 4.59 MB playground-series-s4e3 5.49 MB tabular-playground-seriessep-2022 nomad2018-predicttransparent-conductors amazon-employee-accesschallenge playground-series-s3e1 tabular-playground-seriesaug-2022 playground-series-s3e18 5.73 MB 6.24 MB 6.39 MB 6.48 MB 7.21 MB 7.63 MB poker-rule-induction 8.3 MB playground-series-s4e4 allstate-purchaseprediction-challenge playground-series-s3e16 8.4 MB 8.97 MB 9.05 MB 1.43 MB 1.63 MB 1.73 MB 1.88 MB 1.9 MB 1.95 MB 2 MB Text, Binary Classification, NLP, Custom Metric Coronavirus, Tabular, Mean Columnwise Root Mean Squared Logarithmic Error Tabular, Time Series Analysis, SMAPE Binary Classification, Tabular, Health Conditions, Beginner, Area Under Receiver Operating Characteristic Curve Multiclass Classification, Literature, Linguistics, Multiclass Loss Tabular, Coronavirus, Mean Columnwise Root Mean Squared Logarithmic Error Housing, Normalized Gini Index 2.11 MB 2.14 MB Beginner, Tabular, Regression, Earth Science, Median Absolute Error NLP, Text, PearsonCorrelationCoefficient 2.44 MB Text, Multiclass Classification, Categorization Accuracy 2.7 MB 2.93 MB 2.99 MB 3.02 MB Beginner, Tabular, Housing, Root Mean Squared Error Text, Regression, Root Mean Squared Error Beginner, Tabular, Regression, Mean Absolute Error Area Under Receiver Operating Characteristic Curve 3.22 MB Time Series Analysis, Weighted Mean Absolute Error 3.4 MB Tabular, QuadraticWeightedKappa Text, Internet, Custom Metric Beginner, Tabular, Binary Classification, Area Under Receiver Operating Characteristic Curve Education, Primary and Secondary Schools, Binary Classification, Text Generation, Roc Auc Score Area Under Receiver Operating Characteristic Curve Beginner, Time Series Analysis, Tabular, Multiclass Classification, Accuracy Score Beginner, Tabular, Multiclass Classification, Binary Classification, Manufacturing, Mean Columnwise Area Under Receiver Operating Characteristic Curve Tabular, SMAPE Chemistry, Mean Columnwise Root Mean Squared Logarithmic Error Area Under Receiver Operating Characteristic Curve Regression, Tabular, Housing, Beginner, Root Mean Squared Error Tabular, Binary Classification, Area Under Receiver Operating Characteristic Curve Beginner, Tabular, Binary Classification, Multilabel Classification, Roc Auc Score Multiclass Classification, Card Games, Tabular, Categorization Accuracy Beginner, Tabular, Regression, Mean Squared Log Error Categorization Accuracy Beginner, Tabular, Regression, Animals, Mean Absolute Error Continued on next page 19 Competition Name feedback-prize-englishlanguage-learning dont-call-me-turkey tabular-playground-seriesapr-2021 playground-series-s3e19 playground-series-s3e17 GiveMeSomeCredit google-quest-challenge forest-cover-type-kernelsonly playground-series-s4e6 novozymes-enzymestability-prediction integer-sequence-learning random-acts-of-pizza playground-series-s3e23 demand-forecastingkernels-only playground-series-s3e8 playground-series-s3e10 playground-series-s4e1 afsis-soil-properties sberbank-russian-housingmarket playground-series-s3e24 crowdflower-weathertwitter predicting-red-hatbusiness-value DontGetKicked battlefin-s-big-datacombine-forecastingchallenge bioresponse tabular-playground-seriesmar-2022 chaii-hindi-and-tamilquestion-answering nbme-score-clinicalpatient-notes leaf-classification learning-agency-labautomated-essay-scoring-2 20-newsgroups-ciphertextchallenge dont-overfit-ii 17.91 MB 17.97 MB 18.63 MB 18.7 MB 18.96 MB 20.94 MB 21.65 MB 21.7 MB 22.71 MB 22.79 MB 25.4 MB 26.74 MB 29.52 MB 30.23 MB 36.05 MB 36.2 MB 36.97 MB 38.6 MB aerial-cactus-identification 25.4 MB rossmann-store-sales 39.85 MB sf-crime imaterialist-challengefurniture-2018 45.24 MB 48.59 MB Table 3: Kaggle Competition Data Overview (continued) Size 9.3 MB 11.04 MB 12.66 MB 12.79 MB 12.86 MB 14.47 MB 14.85 MB 15.38 MB Tags NLP, Education, Primary and Secondary Schools, Custom Metric Binary Classification, Tabular, Animals, Area Under Receiver Operating Characteristic Curve Beginner, Tabular, Binary Classification, Categorization Accuracy Beginner, Tabular, Time Series Analysis, SMAPE Beginner, Tabular, Roc Auc Score Area Under Receiver Operating Characteristic Curve Text, NLP, Mean Columnwise Spearmans (rank correlation coefficient) Tabular, Forestry, Categorization Accuracy 16.2 MB 16.39 MB Beginner, Tabular, Education, Accuracy Score Chemistry, SpearmanR Tabular, Categorization Accuracy Binary Classification, Text, Internet, Area Under Receiver Operating Characteristic Curve Beginner, Tabular, Binary Classification, Roc Auc Score Tabular, SMAPE Beginner, Tabular, Regression, Root Mean Squared Error Beginner, Tabular, Log Loss Beginner, Tabular, Binary Classification, Banking, Roc Auc Score Mean Columnwise Root Mean Squared Error Banking, Housing, Regression, Tabular, Root Mean Squared Logarithmic Error Beginner, Tabular, Binary Classification, Health, Roc Auc Score Earth and Nature, Image, Plants, Area Under Receiver Operating Characteristic Curve Root Mean Squared Error Tabular, Business, Area Under Receiver Operating Characteristic Curve Gini Index Mean Absolute Error 31.06 MB 31.4 MB 31.8 MB Log Loss Tabular, Time Series Analysis, Cities and Urban Areas, Regression, Mean Absolute Error Text, Languages, Custom Metric 35.73 MB Text, Medicine, Education, NLP, Custom Metric Image, Multiclass Classification, Multiclass Loss Education, NLP, Primary and Secondary Schools, Cohen Kappa Score Multiclass Classification, Text, F-Score (Macro) Tabular, Binary Classification, Area Under Receiver Operating Characteristic Curve Tabular, Time Series Analysis, Root Mean Square Percentage Error Multiclass Classification, Tabular, Crime, Multiclass Loss MeanBestErrorAtK Continued on next page 20 Table 3: Kaggle Competition Data Overview (continued) Competition Name Size Tags playground-series-s3e11 word2vec-nlp-tutorial jigsaw-toxic-commentclassification-challenge see-click-predict-fix denoising-dirty-documents homesite-quote-conversion 49.83 MB 54.37 MB 55.18 MB 55.75 MB 58.81 MB 65.13 MB airbnb-recruiting-new-userbookings cat-in-the-dat 67.85 MB 67.96 MB tmdb-box-office-prediction 70.24 MB 72.93 MB 80.86 MB 89.91 MB 95.51 MB Beginner, Tabular, Regression, Mean Squared Log Error Text, Binary Classification, Movies and TV Shows, Area Under Receiver Operating Characteristic Curve Text, Mean Columnwise Area Under Receiver Operating Characteristic Curve Root Mean Squared Logarithmic Error Image, Root Mean Squared Error Binary Classification, Tabular, Area Under Receiver Operating Characteristic Curve Hotels and Accommodations, Recommender Systems, Tabular, NDCG{K} Tabular, Binary Classification, Area Under Receiver Operating Characteristic Curve Tabular, Movies and TV Shows, Root Mean Squared Logarithmic Error Tabular, Root Mean Squared Error Image, Root Mean Squared Error Tabular, Regression, Root Mean Squared Logarithmic Error Linguistics, Languages, Text, Categorization Accuracy home-depot-productsearch-relevance facial-keypoints-detection nyc-taxi-trip-duration text-normalizationchallenge-english-language bnp-paribas-cardif-claimsmanagement playground-series-s4e5 wsdm-cup-multilingualchatbot-arena santander-customersatisfaction playground-series-s3e20 text-normalizationchallenge-russian-language tabular-playground-seriesmar-2021 allstate-claims-severity tabular-playground-seriesjan-2021 cat-in-the-dat-ii liverpool-ion-switching tabular-playground-seriesfeb-2021 yelp-recsys-2013 lmsys-chatbot-arena llm-classificationfinetuning stumbleupon playground-series-s3e4 the-winton-stock-marketchallenge conways-reverse-game-oflife-2020 facebook-recruiting-ivhuman-or-bot the-icml-2013-whalechallenge-right-whaleredux 103.75 MB Banking, Tabular, Binary Classification, Log Loss 104.17 MB Beginner, Tabular, Logistic Regression, R2 Score 113.98 MB Languages, Text Conversation, Accuracy Score 119.04 MB Tabular, Binary Classification, Banking, Area Under Receiver Operating Characteristic Curve 119.66 MB Beginner, Tabular, Time Series Analysis, Root Mean Squared Error 125.87 MB Text, Linguistics, Languages, Categorization Accuracy 131.65 MB Tabular, Logistic Regression, Binary Classification, Area Under Receiver Operating Characteristic Curve 142.87 MB Regression, Tabular, Mean Absolute Error 144.43 MB Tabular, Regression, Root Mean Squared Error 145.84 MB Binary Classification, Area Under Receiver Operating Characteristic Curve 146.08 MB Biology, F-Score (Macro) 154.66 MB Regression, Tabular, Root Mean Squared Error 179.46 MB Root Mean Squared Error 184.19 MB Languages, Text Conversation, Log Loss 184.19 MB Languages, Text Conversation, Log Loss 196.18 MB Text, Tabular, Internet, Area Under Receiver Operating Characteristic Curve 198.09 MB Tabular, Binary Classification, Beginner, Area Under Receiver Operating Characteristic Curve 213.13 MB Finance, Tabular, Weighted Mean Absolute Error 251.11 MB Simulations, Board Games, Custom Metric 260.68 MB Binary Classification, Tabular, Internet, Area Under Receiver Operating Characteristic Curve 293.14 MB Area Under Receiver Operating Characteristic Curve Continued on next page 21 Table 3: Kaggle Competition Data Overview (continued) Competition Name Size Tags porto-seguro-safe-driverprediction statoil-iceberg-classifierchallenge tabular-playground-seriesaug-2021 flavours-of-physics-kernelsonly tgs-salt-identificationchallenge linking-writing-processesto-writing-quality grupo-bimbo-inventorydemand quora-question-pairs tabular-playground-seriesmay-2022 bigquery-geotabintersection-congestion mlsp-2013-birds santander-customertransaction-prediction loan-default-prediction global-wheat-detection tabular-playground-seriesdec-2021 ventilator-pressureprediction whale-categorizationplayground dog-breed-identification plant-pathology-2020fgvc7 dogs-vs-cats-redux-kernelsedition benchmark-bond-tradeprice-challenge pubg-finish-placementprediction instant-gratification tabular-playground-seriesnov-2021 petfinder-pawpularityscore santander-value-predictionchallenge champs-scalar-coupling avazu-ctr-prediction tabular-playground-seriessep-2021 billion-word-imputation plant-seedlingsclassification tabular-playground-seriesfeb-2022 shopee-product-matching AI4Code the-nature-conservancyfisheries-monitoring 300.58 MB Tabular, Binary Classification, Normalized Gini Index 302.1 MB Binary Classification, Weather and Climate, Image, Log Loss 337.38 MB Tabular, Regression, Banking, Root Mean Squared Error 436.14 MB Custom Metric 483.07 MB Geology, Image, Custom Metric 485.71 MB Education, NLP, Primary and Secondary Schools, Mean Squared Error 502.44 MB Tabular, Food, Root Mean Squared Logarithmic Error 523.24 MB Text, Tabular, Linguistics, Internet, Log Loss 574.22 MB Tabular, Binary Classification, Area Under Receiver Operating Characteristic Curve 577.97 MB Tabular, Regression, Cities and Urban Areas, Geospatial Analysis, Root Mean Squared Error Area Under Receiver Operating Characteristic Curve 585.1 MB 606.35 MB Banking, Tabular, Binary Classification, Area Under Receiver Operating Characteristic Curve 611.66 MB Mean Absolute Error 643.57 MB Image, Plants, Custom Metric 693.17 MB Tabular, Multiclass Classification, Categorization Accuracy 698.79 MB Tabular, Medicine, Biology, Mean Absolute Error 726.74 MB Image, Animals, MAP{K} 750.43 MB Multiclass Classification, Animals, Image, Multiclass Loss 823.79 MB Image, Agriculture, Mean Columnwise Area Under Receiver Operating Characteristic Curve 854.51 MB Image, Animals, Binary Classification, Log Loss 910.54 MB Weighted Mean Absolute Error 965.66 MB Video Games, Tabular, Mean Absolute Error 972.6 MB 1.04 GB 1.04 GB Tabular, Binary Classification, Area Under Receiver Operating Characteristic Curve Tabular, Binary Classification, Area Under Receiver Operating Characteristic Curve Image, Root Mean Squared Error 1.08 GB Banking, Finance, Root Mean Squared Logarithmic Error 1.22 GB 1.28 GB 1.37 GB 1.7 GB 1.81 GB 1.87 GB 1.92 GB 2.16 GB 2.27 GB Chemistry, Tabular, Regression, Custom Metric Log Loss Tabular, Binary Classification, Area Under Receiver Operating Characteristic Curve Text, Linguistics, Levenshtein Mean Plants, Image, Multiclass Classification, Custom Metric Tabular, Multiclass Classification, Categorization Accuracy Image, Text, Retail and Shopping, Custom Metric NLP, Text, Computer Science, Custom Metric Image, Multiclass Classification, Multiclass Loss Continued on next page 22 Table 3: Kaggle Competition Data Overview (continued) Competition Name Size Tags jigsaw-unintended-bias-intoxicity-classification uw-madison-gi-tractimage-segmentation ashrae-energy-prediction stanford-covid-vaccine acquire-valued-shopperschallenge facebook-recruiting-iiikeyword-extraction invasive-speciesmonitoring tabular-playground-seriesoct-2021 kuzushiji-recognition nfl-player-contactdetection bengaliai-cv new-york-city-taxi-fareprediction cassava-leaf-diseaseclassification quora-insincere-questionsclassification histopathologic-cancerdetection microsoft-malwareprediction bms-molecular-translation aptos2019-blindnessdetection ranzcr-clip-catheter-lineclassification plant-pathology-2021fgvc8 smartphone-decimeter2022 multi-modal-gesturerecognition osic-pulmonary-fibrosisprogression freesound-audio-tagging2019 hms-harmful-brain-activityclassification hotel-id-2021-fgvc8 imet-2020-fgvc7 predict-volcanic-eruptionsingv-oe airbus-ship-detection alaska2-image-steganalysis hubmap-kidneysegmentation h-and-m-personalizedfashion-recommendations draper-satellite-imagechronology 2.38 GB Text, NLP, Custom Metric 2.47 GB Image, Medicine, Dice3DHausdorff 2.61 GB 2.68 GB 3.07 GB Tabular, Energy, Root Mean Squared Logarithmic Error Biology, Biotechnology, Coronavirus, Public Health, Custom Metric Area Under Receiver Operating Characteristic Curve 3.19 GB Custom Metric 3.35 GB 3.49 GB 4.51 GB 5.01 GB 5.18 GB 5.7 GB 6.19 GB 6.56 GB 7.76 GB 8.47 GB 8.87 GB 10.22 GB 13.13 GB 16.1 GB 22.9 GB 23.23 GB Image, Plants, Area Under Receiver Operating Characteristic Curve Tabular, Binary Classification, Area Under Receiver Operating Characteristic Curve Image, Multiclass Classification, History, Japan, Custom Metric Health, Football, Video, Tabular, Matthews correlation coefficient Image, Multiclass Classification, Weighted Categorization Accuracy Regression, Tabular, Root Mean Squared Error Image, Multiclass Classification, Plants, Categorization Accuracy Text, Binary Classification, Custom Metric Cancer, Medicine, Research, Area Under Receiver Operating Characteristic Curve Area Under Receiver Operating Characteristic Curve Chemistry, Image, Levenshtein Mean Image, Multiclass Classification, Medicine, Healthcare, QuadraticWeightedKappa Image, Multilabel Classification, Mean Columnwise Area Under Receiver Operating Characteristic Curve Image, Plants, Custom Metric Geospatial Analysis, Signal Processing, Research, Tabular, Mobile and Wireless, Custom Metric Custom Metric 23.99 GB Image, Healthcare, Laplace Log Likelihood 26.15 GB Audio, Weighted Label Ranking Average Precision 26.4 GB 26.65 GB 29.46 GB 31.25 GB 31.41 GB 32.27 GB 32.97 GB Tabular, Research, Signal Processing, Healthcare, Kullback Leibler Divergence Image, Public Safety, MAP{K} F-Score Beta (Micro) Signal Processing, Geology, Physics, Mean Absolute Error Image, IntersectionOverUnionObjectSegmentationBeta Custom Metric Image, Health, Biology, Dice 34.56 GB Recommender Systems, Retail and Shopping, MAP{K} 36.07 GB Image, MASpearmanR Continued on next page 23 Table 3: Kaggle Competition Data Overview (continued) Competition Name Size Tags vesuvius-challenge-inkdetection iwildcam-2019-fgvc6 herbarium-2020-fgvc7 cdiscount-imageclassification-challenge inaturalist-2019-fgvc6 icecube-neutrinos-in-deepice iwildcam-2020-fgvc7 siim-covid19-detection rsna-miccai-brain-tumorradiogenomic-classification seti-breakthrough-listen herbarium-2021-fgvc8 herbarium-2022-fgvc9 vinbigdata-chest-xrayabnormalities-detection 37.02 GB 46.68 GB 63.16 GB 78.12 GB Image Segmentation, History, Image Text Recognition, DiceFBeta Image, Multiclass Classification, F-Score (Macro) Image, Plants, F-Score (Macro) Multiclass Classification, Categorization Accuracy 87.76 GB 117.18 GB MeanBestErrorAtK Tabular, Astronomy, Physics, MeanAngularError 118.59 GB Multiclass Classification, Biology, Categorization Accu128.51 GB 136.85 GB 156.02 GB 161.9 GB 163.17 GB 205.96 GB racy Image, Multilabel Classification, Custom Metric Image, Binary Classification, Healthcare, Area Under Receiver Operating Characteristic Curve Astronomy, Signal Processing, Science and Technology, Area Under Receiver Operating Characteristic Curve Image, Plants, F-Score (Macro) Plants, Image, F-Score (Macro) Image, Healthcare, Custom Metric Table 4 presents 50 competitions used for evaluation in our benchmark, encompassing four categories of tasks: MLE-Lite, Tabular, NLP, and CV. Table 4: Competitions for Evaluation Competition Name Size Tags MLE-Lite Competitions spooky-authoridentification detecting-insults-in-socialcommentary nomad2018-predicttransparent-conductors random-acts-of-pizza 1.9 MB 3.02 MB 6.24 MB 17.97 MB aerial-cactus-identification 25.4 MB Multiclass Classification, Literature, Linguistics, Multiclass Loss Area Under Receiver Operating Characteristic Curve Chemistry, Mean Columnwise Root Mean Squared Logarithmic Error Binary Classification, Text, Internet, Area Under Receiver Operating Characteristic Curve Earth and Nature, Image, Plants, Area Under Receiver Operating Characteristic Curve Image, Multiclass Classification, Multiclass Loss Text, Mean Columnwise Area Under Receiver Operating Characteristic Curve Image, Root Mean Squared Error Linguistics, Languages, Text, Categorization Accuracy 36.05 MB 55.18 MB 58.81 MB 95.51 MB leaf-classification jigsaw-toxic-commentclassification-challenge denoising-dirty-documents text-normalizationchallenge-english-language text-normalizationchallenge-russian-language the-icml-2013-whalechallenge-right-whaleredux tabular-playground-seriesmay-2022 mlsp-2013-birds tabular-playground-seriesdec-2021 dog-breed-identification plant-pathology-2020fgvc7 125.87 MB Text, Linguistics, Languages, Categorization Accuracy 293.14 MB Area Under Receiver Operating Characteristic Curve 574.22 MB Tabular, Binary Classification, Area Under Receiver Operating Characteristic Curve Area Under Receiver Operating Characteristic Curve 585.1 MB 693.17 MB Tabular, Multiclass Classification, Categorization Accuracy 750.43 MB Multiclass Classification, Animals, Image, Multiclass Loss 823.79 MB Image, Agriculture, Mean Columnwise Area Under Receiver Operating Characteristic Curve Continued on next page Competition Name Size Tags dogs-vs-cats-redux-kernelsedition new-york-city-taxi-fareprediction histopathologic-cancerdetection aptos2019-blindnessdetection ranzcr-clip-catheter-lineclassification kaggle-llm-science-exam llm-detect-ai-generatedtext 20-newsgroups-ciphertextchallenge lmsys-chatbot-arena stumbleupon linking-writing-processesto-writing-quality quora-question-pairs AI4Code quora-insincere-questionsclassification facial-keypoints-detection whale-categorizationplayground petfinder-pawpularityscore bengaliai-cv19 cassava-leaf-diseaseclassification bms-molecular-translation imet-2020-fgvc7 airbus-ship-detection alaska2-image-steganalysis draper-satellite-imagechronology demand-forecastingkernels-only dont-overfit-ii santander-customersatisfaction liverpool-ion-switching conways-reverse-game-oflife-2020 porto-seguro-safe-driverprediction santander-customertransaction-prediction ventilator-pressureprediction instant-gratification 854.51 MB Image, Animals, Binary Classification, Log Loss 5.7 GB Regression, Tabular, Root Mean Squared Error 7.76 GB 10.22 GB 13.13 GB Cancer, Medicine, Research, Area Under Receiver Operating Characteristic Curve Image, Multiclass Classification, Medicine, Healthcare, QuadraticWeightedKappa Image, Multilabel Classification, Mean Columnwise Area Under Receiver Operating Characteristic Curve NLP Competitions 364.21 kB 4.43 MB 36.97 MB Physics, NLP, MAP@K Education, Primary and Secondary Schools, Binary Classification, Text Generation, Roc Auc Score Multiclass Classification, Text, F-Score (Macro) 184.19 MB Languages, Text Conversation, Log Loss 196.18 MB Text, Tabular, Internet, Area Under Receiver Operating Characteristic Curve 485.71 MB Education, NLP, Primary and Secondary Schools, Mean Squared Error 523.24 MB Text, Tabular, Linguistics, Internet, Log Loss NLP, Text, Computer Science, Custom Metric 2.16 GB Text, Binary Classification, Custom Metric 6.56 GB CV Competitions 80.86 MB 726.74 MB Image, Animals, MAP@K Image, Root Mean Squared Error 1.04 GB Image, Root Mean Squared Error 5.18 GB 6.19 GB 8.87 GB 29.46 GB 31.41 GB 32.27 GB 36.07 GB Image, Multiclass Classification, Weighted Categorization Accuracy Image, Multiclass Classification, Plants, Categorization Accuracy Chemistry, Image, Levenshtein Mean F-Score Beta (Micro) Image, IntersectionOverUnionObjectSegmentationBeta Custom Metric Image, MASpearmanR Tabular Competitions 18.7 MB Tabular, SMAPE 38.6 MB Tabular, Binary Classification, Area Under Receiver Operating Characteristic Curve 119.04 MB Tabular, Binary Classification, Banking, Area Under Receiver Operating Characteristic Curve 146.08 MB Biology, F-Score (Macro) 251.11 MB Simulations, Board Games, Custom Metric 300.58 MB Tabular, Binary Classification, Normalized Gini Index 606.35 MB Banking, Tabular, Binary Classification, Area Under Receiver Operating Characteristic Curve 698.79 MB Tabular, Medicine, Biology, Mean Absolute Error 972.6 MB Tabular, Binary Classification, Area Under Receiver Operating Characteristic Curve Continued on next page 25 Competition Name Size Tags santander-value-predictionchallenge 1.08 GB Banking, Finance, Root Mean Squared Logarithmic Error We ranked the difficulty of competitions based on the average HumanRank scores achieved by all models across each competition. Table presents all competitions for evaluation ranked by difficulty from easiest to hardest. This difficulty ranking provides partial indication of which competitions MLE Agents perform relatively better in comparison to human competitors. However, due to potential biases introduced by factors such as dataset splits, the ranking should be interpreted as reference rather than definitive measure. Table 5: Competition Difficulty Competition Avg. HumanRank tabular-playground-series-dec-2021 detecting-insults-in-social-commentary llm-detect-ai-generated-text santander-customer-satisfaction 20-newsgroups-ciphertext-challenge plant-pathology-2020-fgvc7 dogs-vs-cats-redux-kernels-edition histopathologic-cancer-detection aerial-cactus-identification airbus-ship-detection demand-forecasting-kernels-only nomad2018-predict-transparent-conductors dont-overfit-ii random-acts-of-pizza aptos2019-blindness-detection draper-satellite-image-chronology jigsaw-toxic-comment-classification-challenge the-icml-2013-whale-challenge-right-whaleredux spooky-author-identification facial-keypoints-detection porto-seguro-safe-driver-prediction santander-value-prediction-challenge tabular-playground-series-may-2022 leaf-classification text-normalization-challenge-english-language lmsys-chatbot-arena dog-breed-identification petfinder-pawpularity-score quora-question-pairs alaska2-image-steganalysis cassava-leaf-disease-classification conways-reverse-game-of-life-2020 denoising-dirty-documents stumbleupon imet-2020-fgvc7 whale-categorization-playground mlsp-2013-birds liverpool-ion-switching ventilator-pressure-prediction kaggle-llm-science-exam text-normalization-challenge-russian-language linking-writing-processes-to-writing-quality instant-gratification quora-insincere-questions-classification bms-molecular-translation ranzcr-clip-catheter-line-classification 26 1.000000 0.995000 0.808599 0.777327 0.741197 0.706198 0.704814 0.635389 0.625717 0.578445 0.576934 0.560720 0.532964 0.506494 0.474492 0.472431 0.437596 0.423450 0.405318 0.393214 0.359001 0.337203 0.336664 0.324687 0.315865 0.311389 0.300586 0.283909 0.283346 0.257705 0.223718 0.223404 0.204969 0.182091 0.179036 0.166983 0.154272 0.152072 0.136828 0.135041 0.098380 0.089086 0.071988 0.070380 0.066147 0.036199 Category MLE-Lite MLE-Lite NLP Tabular NLP MLE-Lite MLE-Lite MLE-Lite MLE-Lite CV Tabular MLE-Lite Tabular MLE-Lite MLE-Lite CV MLE-Lite MLE-Lite MLE-Lite CV Tabular Tabular MLE-Lite MLE-Lite MLE-Lite NLP MLE-Lite CV NLP CV CV Tabular MLE-Lite NLP CV CV MLE-Lite Tabular Tabular NLP MLE-Lite NLP Tabular NLP CV MLE-Lite Competition Avg. HumanRank AI4Code bengaliai-cv19 new-york-city-taxi-fare-prediction santander-customer-transaction-prediction 0.025269 0.018395 0.000843 0. Category NLP CV MLE-Lite Tabular"
        },
        {
            "title": "E Implementation Details",
            "content": "E.1 Evaluation Metrics Details We include additional details of evaluation metrics design as follows: AUP Score. We use the AUP score to systematically evaluate and compare multiple methods across diverse tasks. performance profile captures the proportion of tasks where given method performs within certain factor of the best-performing method. Lower performance ratios indicate better results, though we invert this ratio for metrics like accuracy or R2, where higher values represent better performance. If method fails to produce valid solution (infeasible), we assign it penalty relative to the worst feasible performance. Integrating these performance profiles provides the AUP score, offering single robust measure of each methods overall effectiveness across the benchmark. We adopt the performance profile and AUP score from ML-Gym [26] and to compare the effectiveness of different methods across set of benchmark tasks. We make few minor modifications to better align with our framework. For each backbone model and task , the performance ratio is defined as rt,m = ℓt,m , (1) min mM where ℓt,m denotes the performance metric of backbone model on task t, and is the set of all evaluated methods. This formulation assumes that lower metric values correspond to better performance. ℓt,m The performance profile curve of backbone model is then defined as the cumulative distribution of the log-scaled performance ratio: ρm(τ ) = 1 {t log10 rt,m τ } . (2) This function quantifies the proportion of tasks for which backbone model performs within τ -log distance of the best backbone model on that task. To aggregate performance into single scalar score, we compute the Area Under the Profile (AUP) curve: AUPm = ρm(τ ) dτ, (3) (cid:90) τmax 1 where τmax is the smallest value such that ρm(τmax) = 1 for all M. For metrics where higher values indicate better performance (e.g., Accuracy, R2), we invert the ratio computation: rt,m = max mM ℓt,m ℓt,m . (4) If backbone model fails to obtain valid score for task t, its ratio is defined relative to the lowest valid score across all backbone models mbottom as: rt,m = (1 + ε) rt,mbottom, with ε = 1.0. (5) This is relatively reasonable design choice that appropriately penalizes the performance of backbone models that fail to produce valid scores. Additionally, we impose an upper bound of 100 on the value of rt,m to mitigate the risk of biased results caused by extremely large ratios. 27 HumanRank Score. The HumanRank score (Eq. E.1) measures the relative ranking of submission within the competition leaderboard. Submissions receive higher scores if they achieve better rank among all participants. Suppose that the submission ranks at position among total of submissions on the leaderboard. Then, the position score is computed as: = 1 . To prevent bias between public and private leaderboards, we compute the relative scores on each leaderboard independently and then use their average as the final score. Elo ranking. We adopt the Elo ranking calculation algorithm from Chatbot Arena [5]. We utilize Elo ratings to perform systematic pairwise comparisons of methods across competitions. Two complementary Elo calculation methods ensure stability and reliability: an online linear update with low K-factor, providing stable incremental ratings, and Bradley-Terry model using logistic regression, directly fitting ratings to all comparisons for robust estimates. Additionally, we apply bootstrapping techniques to compute confidence intervals, offering clear measures of uncertainty and confidence in the resulting ratings. Each competition can be viewed as battle, in which two backbone models are pitted against each other. Their respective performance scores on the competition determine the outcome-win, loss, or tie-between the two. Based on these pairwise outcomes, we calculate their Elo rankings. In our setting, we assume that the large language models (LLMs) under evaluation are static, allowing us to estimate stable skill rating using the BradleyTerry model. Specifically, we reformulate the pairwise win-loss outcomes between models as logistic regression problem. Let denote the set of all models with cardinality = p, and let ri denote the (logit-transformed) latent skill rating of model i. For every observed battle between model and model j, we construct pair of training samples (xij, yij) and (xji, yji) such that: xij = log (ei ej), yij = 1, xji = log (ej ei), yji = 0, where is the base of the logistic function (default = 10), and ei is p-dimensional one-hot vector indicating model i. The number of duplicate entries per pair is determined by the total number of battles (wins, losses, and ties), with each entry weighted accordingly. The logistic regression is then solved as ˆr = arg min (cid:88) k=1 wk log (cid:0)1 + exp(yk r)(cid:1) , where wk is the sample weight for the k-th comparison. The final Elo score for each model is computed as si = ˆri + R0, where is scaling factor (default = 400), and R0 is the initial Elo offset (default R0 = 1000). To estimate confidence intervals, we apply non-parametric bootstrap procedure. Given the original battle dataset D, we resample with replacement to generate D1, . . . , DR, and re-estimate Elo scores as s(1), . . . , s(R). The final Elo score for each model is reported as the pointwise median across bootstrap samples. All results in our study are reported using = 100 bootstrap rounds with fixed random seed."
        },
        {
            "title": "F Additional Experimental Results",
            "content": "F.1 Additional Analysis of Evaluation Metrics We identify several key observations supported by analytical reasoning regarding the evaluation metrics utilized: HumanRank Score, Elo Score, and Performance Profiles with the AUP score. HumanRank Scores Reflect Comprehensive Absolute Performance. The HumanRank Score (%) converts raw, task-specific metrics into relative rankings against human performance, yielding scores ranging from 0 (lowest) to 1 (highest). high HumanRank score thus indicates superior performance relative to human benchmarks across tasks of diverse difficulty. This uniform normalization allows for comprehensive comparison and mitigates inconsistencies from heterogeneous raw scores. Our main experimental results illustrate that stronger models (e.g., o3-mini, DeepSeek-r1, and Gemini-2.5-Pro) consistently exhibit profiles enclosing those of less capable models, confirming their overall superior performance. Nonetheless, by equally weighting tasks irrespective of complexity, the averaging process may mask task-specific strengths or weaknesses and obscure the direct competitive relationships between individual models. Elo Scores Highlight Pairwise Relative Performance. The Elo Score provides rankings based on explicit win-loss relationships determined through pairwise comparisons on each task. Utilizing the 28 standard Elo rating system, these scores intuitively represent the relative ordering of models based on their competitive outcomes. Elo scores generally reflect pattern where superior models encompass weaker ones, yet variations may arise. Notably, models with comparatively lower absolute average performance scores may still achieve higher Elo scores in certain task categories due to advantageous win-loss records. Elo scores thus effectively reveal relative model strengths and weaknesses but do not quantify the absolute magnitude of performance gaps. Performance Profiles and AUP Scores Demonstrate Robustness. Performance Profiles and AUP scores accumulate performance ratios, which are relative values of raw scores, over varying thresholds. These metrics illustrate model robustness and consistency across multiple evaluation conditions. While these integrated scores offer insightful perspectives into relative performance dynamics, disparities in raw score scales across tasks can lead to comparability challenges. Despite potential biases due to varying raw scales, these metrics successfully capture nuanced relative performances, reflecting the models adaptability across different performance thresholds."
        },
        {
            "title": "G Prompt Details",
            "content": "We design our prompts to be concise while containing all the information necessary for the LLM to perform the task. In doing so, we avoid providing extra assistance beyond what is required, while also ensuring that no essential information is omitted. This careful balance allows our benchmark to serve as fair and comprehensive evaluation of LLMs as MLE Agents. Below, we present the prompts used for the MLE Agents, which consist of four components. The System Instruction is an overall directive provided at the beginning of the task, detailing the task setting, the available actions, the expected output format, and an overview of the environment. During task execution, Error and Reflection as dynamic prompts are supplied to guide the LLM toward the next step, based on the current state: concise prompt is used in the event of an error, while an informative prompt is used upon success, both incorporating feedback from the environment. In cases where the LLM produces outputs in an incorrect format, Parse Error prompt is triggered to prompt the LLM to correct its output. The detailed prompts are as follows: System Instruction You are top-ranked Kaggle grandmaster with extensive competition experience. Your objective is to solve Kaggle competition, with the goal of maximizing the Position Score (Your rank in the leaderboard) in limited steps. You must use Machine Learning/Deep Learning/Computer Vision/NLP/etc. methods to solve the problem, the score of random guess or without any ML/DL/CV/NLP methods will be cancelled finally. You are likely to train models according to specific competition requirements. You have access to GPU and several CPUs for training DL/ML models. Use cuda and PyTorch for faster training whenever needed. You have total of {num_actions} actions available. You have total of {time_left} seconds, including code execution time. You have access to exactly three actions with params, and receive corresponding feedback after each action: 1. request_info: Retrieve specific competition information - params: info_type (str), must be one of: \"overview\", \"sample_submission\", \"data_structure\", \"data_path\", \"output_path\" - feedback: information you requested 2. validate_code: Test (partial) code execution for debugging purposes or \"print\" information in the output - params: code (str) - feedback: execution result (success or failure), error message if failed, code output if success 3. execute_code: Run completed code, generate submission and get evaluation - params: code (str) - feedback: execution result, submission status, evaluation score Code requirements: - Request all information needed first 29 - Read all data files from data_dir - Save all submissions to output_dir, should match test_data length - Don't add, delete, or modify any files in data_dir - Use \"print\" or other functions to output information in the feedback - No plotting or visualization is allowed - Refer to Sample Submission for the output format - Code should be self-contained and not rely on any variables or state outside - Code for submission should be completely runnable, otherwise it will be considered as failed - Optimize your Model/Parameters/Data Processing/Algorithm for continuous improvement Only if \"execute_code\" action taken, code successfully executed and valid submission generated, you'll be able to get Position Score (Your rank in the leaderboard) for this competition. Response format requirements, strictly one of the following: {{ \"action\": \"request_info\", \"params\": {{ \"info_type\": \"<info_type>\" }} \"action\": \"validate_code\", \"params\": {{ \"code\": \"<code>\" }} }} or {{ }} or {{ \"action\": \"execute_code\", \"params\": {{ \"code\": \"<code>\" }} }} - Must be valid JSON format - No additional text or formatting allowed Execution failed, details below: Error #### Error Start #### {observation} #### Error End #### You still have {num_actions} actions available. You still have {time_left} seconds left. Output your next action strictly following Response format requirements. The results of your previous action: Reflection #### Results Start #### {observation} #### Results End #### You still have {num_actions} actions available. You still have {time_left} seconds left. Optimize your Model/Parameters/Data Processing/Algorithm for continuous improvement. 30 Output your next action strictly following Response format requirements. Parse Error The response can't be parsed as valid JSON. Fix the error following the response format requirements. First, only fix the JSON format error, don't change the contents of the response. Then make sure the <code> is in completely runnable format for \"python3 str(<code>)\". Only focus on the format, don't change the meaningful contents. ### Response Start ### {response} ### Response End ### ### Error Start ### {error} ### Error End ### Response format requirements, strictly one of the following: {{ \"action\": \"request_info\", \"params\": {{ \"info_type\": \"<info_type>\" }} \"action\": \"validate_code\", \"params\": {{ \"code\": \"<code>\" }} }} or {{ }} or {{ \"action\": \"execute_code\", \"params\": {{ \"code\": \"<code>\" }} }} - Must be valid JSON format - No additional text or formatting allowed"
        },
        {
            "title": "H Code Analysis",
            "content": "For the same competition, different models exhibit rich diversity in the solutions or code they generate; for different competitions, even the same model is capable of producing specific and targeted solutions tailored to each task. We present the following analyses to illustrate how different models formulate their solutions when interacting with various competitions in MLE-Dojo. H.1 Solution diversity of models When faced with the same competition, different models often produce diverse solutions and code implementations, leading to great diversity of solutions. This diversity arises from variations in the choice of Python packages, the selection of computational devices (CPU or GPU), data preprocessing strategies, and model architectures. Code 1 is code snippet extracted from one of the step actions by Gemini-2.5-Pro on the 20-newsgroups-ciphertext-challenge competition. In this step, the model utilizes sklearn as main python package, CPU as device, employs classical feature engineering techniques, and applies Logistic Regression model to address the task. Code 2 is code snippet extracted from one of the 31 step actions by DeepSeek-r1 on the same competition. In contrast to previous approach, it prioritizes the use of GPU devices, adopts PyTorch as the core framework, and designs and trains the model in more deep learning-oriented manner. This highlights the diverse actions and solution strategies adopted by different models when addressing the same problem. import pandas as pd import os from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline train_df = pd.read_csv(os.path.join(data_dir, 'train.csv')) test_df = pd.read_csv(os.path.join(data_dir, 'test.csv')) sample_submission = pd.read_csv(os.path.join(data_dir, 'sample_submission.csv')) # Define features (X) and target (y) X_train = train_df['ciphertext'] y_train = train_df['target'] X_test = test_df['ciphertext'] # Increased max_features and again for the final attempt pipeline = Pipeline([ ('tfidf', TfidfVectorizer(analyzer='char', ngram_range=(2, 6), max_features=2000)), ('clf', OneVsRestClassifier(LogisticRegression(solver='liblinear', random_state=42, C=70.0))) ]) # Train the model pipeline.fit(X_train, y_train) predictions = pipeline.predict(X_test) submission_df = pd.DataFrame({'Id': test_df['Id'], 'Predicted': predictions}) Code Listing 1: Code snippet of Gemini-2.5-Pro on 20-newsgroups-ciphertext-challenge import pandas as pd import torch import torch.nn as nn from torch.utils.data import Dataset, DataLoader from sklearn.preprocessing import LabelEncoder from pathlib import Path import math device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') ... class CharDataset(Dataset): def __init__(self, texts, labels=None): self.texts = texts self.labels = labels def __len__(self): return len(self.texts) def __getitem__(self, idx): text = self.texts[idx][:max_length].ljust(max_length, ' ') encoded = [char_to_idx.get(c, 0) for in text] tensor = torch.tensor(encoded, dtype=torch.long) if self.labels is not None: label = torch.tensor(self.labels[idx], dtype=torch.long) return tensor, label else: return tensor ... class CipherTransformer(nn.Module): def __init__(self, vocab_size=len(chars)+1, d_model=512, nhead=16, num_layers=8, num_classes=20): super().__init__() self.embed = nn.Embedding(vocab_size, d_model) self.pos_encoder = PositionalEncoding(d_model) encoder_layer = nn.TransformerEncoderLayer( d_model=d_model, nhead=nhead, dim_feedforward=2048, 32 dropout=0.1, batch_first=True ) self.transformer = nn.TransformerEncoder(encoder_layer, num_layers) self.classifier = nn.Sequential( nn.Linear(d_model, 1024), nn.ReLU(), nn.Dropout(0.5), nn.Linear(1024, num_classes) ) def forward(self, x): = self.embed(x) = self.pos_encoder(x) = self.transformer(x) = x.mean(dim=1) return self.classifier(x) ... Code Listing 2: Code snippet of DeepSeek-R1 on 20-newsgroups-ciphertext-challenge H.2 Solution diversity of competitions single model may adopt highly tailored solutions across competitions of varying nature. comparison between Code 1 and Code 3 demonstrates that Gemini-2.5-Pro utilizes markedly distinct approaches in response to the specific demands of each task. This highlights the diverse actions and solution strategies adopted addressing different problems. MLE-Dojo offers broad and flexible space that enables various models or agent scaffolds to fully leverage their respective capabilities without limitation. import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import Dataset, DataLoader from PIL import Image import torchvision.transforms as transforms device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") class CactusDataset(Dataset): def __init__(self, df, img_dir, transform=None, is_test=False, tta_transform=None): self.df = df self.img_dir = img_dir self.transform = transform # Base transform (ToTensor, Normalize) self.tta_transform = tta_transform # Augmentation transform for TTA self.is_test = is_test self.image_files = df['id'].values if not self.is_test: self.labels = df['has_cactus'].values def __len__(self): return len(self.df) def __getitem__(self, idx): img_name = os.path.join(self.img_dir, self.image_files[idx]) image = Image.open(img_name).convert('RGB') if self.is_test and self.tta_transform: # Apply TTA transforms images = [self.transform(self.tta_transform(image, step)) for step in range(TTA_STEPS)] image_stack = torch.stack(images) return image_stack, self.image_files[idx] elif self.transform: # Standard transform for training or non-TTA test image = self.transform(image) if self.is_test: return image, self.image_files[idx] else: label = torch.tensor(float(self.labels[idx])) 33 return image, label ... class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1) self.relu1 = nn.ReLU() self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) self.relu2 = nn.ReLU() self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1) self.relu3 = nn.ReLU() self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) self.fc1 = nn.Linear(128 * 4 * 4, 512) self.relu4 = nn.ReLU() self.dropout = nn.Dropout(0.5) self.fc2 = nn.Linear(512, 1) def forward(self, x): = self.pool1(self.relu1(self.conv1(x))) = self.pool2(self.relu2(self.conv2(x))) = self.pool3(self.relu3(self.conv3(x))) = x.view(x.size(0), -1) = self.relu4(self.fc1(x)) = self.dropout(x) = self.fc2(x) return model = SimpleCNN().to(device) Code Listing 3: Code snippet of Gemini-2.5-Pro on aerial-cactus-identification H.3 Statistics of Method Diversity We further investigate the distribution of model types implemented by different Large Language Models (LLMs) in Figure 16. To facilitate this analysis, the implemented models were systematically categorized based on their architectural complexity and type, utilizing the following mapping for brevity: Adv-NN: Advanced Neural Networks, encompassing contemporary and complex architectures such as Transformers (e.g., BERT, RoBERTa), EfficientNets, and LSTMs. Cls-NN: Classic Neural Networks, referring to well-established architectures like Convolutional Neural Networks (CNNs), ResNet variants, and DenseNets. SD-NN: Self-defined Neural Networks, indicating implementations involving customdesigned network structures or significant modifications to standard architectures, often specified within the solution code itself. Sim-LN: Simple Linear Models, comprising fundamental algorithms like Logistic Regression, Linear Regression, Support Vector Machines (with linear kernels), and Naive Bayes classifiers. Tree: Tree-based Ensemble Models, including algorithms such as Random Forest, Gradient Boosting Machines (GBM), XGBoost, LightGBM, and CatBoost. N/A: Not Applicable, denoting instances where the LLM did not generate specific modeling solution or where the generated code did not contain an identifiable primary model. Analysis of the model distribution across the evaluated LLMs and competition tasks reveals several pertinent observations. Firstly, considerable degree of heterogeneity exists in the model choices preferred by different LLMs for identical tasks. Secondly, the selection of model type strongly correlates with the nature of the competition data; Tree and Sim-LN models are predominantly employed for tabular datasets, reflecting their established effectiveness in such domains, whereas Cls-NN, Adv-NN, and SD-NN are the favored choices for image and natural language processing tasks. Thirdly, certain LLMs exhibit discernible tendencies: for example, some models appear more 34 frequently to propose SD-NN solutions, potentially indicating higher propensity for generating novel or customized architectures, while others might more commonly default to simpler baselines (Sim-LN, Tree) or yield N/A results. The frequent occurrence of N/A across various LLM-task combinations also suggests variability in the LLMs capabilities to successfully generate relevant and complete modeling code for diverse problem specifications. This distribution underscores the varying strategies and potential biases inherent in different LLMs when approaching machine learning problem-solving via code generation. Figure 16: Statistics of method implementation across 50 evaluation tasks of eight frontiers LLM."
        },
        {
            "title": "I Agent Scaffolds",
            "content": "I.1 MLE Agent In MLE-Dojo, the agent scaffold for benchmarking is referred to as the MLE Agent. It leverages the environments natively supported action space and adopts minimalistic yet effective design logic. LLMs are provided with comprehensive and detailed initial instruction at the beginning of each task. This instruction includes the task description, output format, objectives, available actions, and resource constraints. Subsequently, the agent interacts with the environment through concise prompts that guide the LLM to take new actions based on feedback, in order to iteratively improve its performance. The MLE Agent maintains fixed-length interaction history window and utilizes structured, formatted outputs as both actions and their corresponding contents. The MLE Agent serves as lightweight yet representative scaffold that supplies essential guidance without offering additional hints or assistance. It enables systematic benchmarking and evaluation of an LLMs overall capabilities in the MLE-Dojo environment, including context understanding, analytical reasoning, instruction following, and code generation. I.2 AIDE AIDE [19] adopts problem-solving paradigm inspired by how human data scientists approach challenges-through iterative refinement based on performance feedback. Central to its methodology is technique termed Solution Space Tree Search, which enables systematic exploration and optimization over the space of candidate solutions. This framework comprises three core components: (1) Solution Generator, which proposes new candidate solutions either by drafting from scratch or by modifying existing ones (e.g., fixing bugs or introducing improvements); (2) an Evaluator, which runs each candidate and quantitatively assesses its performance against the task objective; and (3) Solution Selector, which identifies the most promising solution to seed the next iteration. Through repeated application of this feedback-driven cycle, AIDE efficiently navigates the solution space 35 and converges towards optimal or near-optimal solutions. This iterative, adaptive process combines algorithmic rigor with human-like creativity, enabling AIDE to solve complex data science problems with remarkable effectiveness. Implementation. We seamlessly integrate AIDE as the agent scaffold within MLE-Dojo, enabling its interaction with the environment through single line of core code. The original score feedback mechanism in AIDE can be effortlessly replaced with either the HumanRank score or the actual raw competition score provided by MLE-Dojo. The interaction process can be fully executed by single call to agent.step(exec_callback=exec_callback), ensuring compatibility without altering AIDEs core logic. Specifically, we make the following modifications: (1) We revise the prompting strategy to emphasize the requirement of generating correctly formatted submission.csv file at the designated location. (2) The feedback mechanism and the criterion for selecting candidate nodes are modified to rely on the true raw score and HumanRank score provided by MLE-Dojo, instead of using the models own validation performance."
        }
    ],
    "affiliations": [
        "Georgia Institute of Technology",
        "Stanford University"
    ]
}